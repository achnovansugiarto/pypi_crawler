{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "<div align=\"center\">\n<img src=docs/source/images/logo1.png width=65% />\n</div>\n\n<h1 align=\"center\"> MARLlib: An Extensive Multi-agent Reinforcement Learning Library </h1>\n\n\n[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)]()\n![test](https://github.com/Replicable-MARL/MARLlib/workflows/test/badge.svg)\n[![Documentation Status](https://readthedocs.org/projects/marllib/badge/?version=latest)](https://marllib.readthedocs.io/en/latest/)\n[![GitHub issues](https://img.shields.io/github/issues/Replicable-MARL/MARLlib)](https://github.com/Replicable-MARL/MARLlib/issues)\n[![PyPI version](https://badge.fury.io/py/marllib.svg)](https://badge.fury.io/py/marllib)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Replicable-MARL/MARLlib/blob/sy_dev/marllib.ipynb)\n[![Awesome](https://awesome.re/badge.svg)](https://marllib.readthedocs.io/en/latest/resources/awesome.html)\n\n**Multi-agent Reinforcement Learning Library ([MARLlib](https://arxiv.org/abs/2210.13708))** is ***a MARL library*** based\non [**Ray**](https://github.com/ray-project/ray) and one of its toolkits [**RLlib**](https://github.com/ray-project/ray/tree/master/rllib). It provides the MARL research community a unified\nplatform for building, training, and evaluating MARL algorithms on almost all diverse tasks and environments.\n\nA simple case of MARLlib usage:\n\n```py\nfrom marllib import marl\n\n# prepare env\nenv = marl.make_env(environment_name=\"mpe\", map_name=\"simple_spread\")\n\n# initialize algorithm with appointed hyper-parameters\nmappo = marl.algos.mappo(hyperparam_source='mpe')\n\n# build agent model based on env + algorithms + user preference\nmodel = marl.build_model(env, mappo, {\"core_arch\": \"gru\", \"encode_layer\": \"128-256\"})\n\n# start training\nmappo.fit(env, model, stop={'timesteps_total': 1000000}, share_policy='group')\n\n# ready to control\nmappo.render(env, model, share_policy='group', restore_path='path_to_checkpoint')\n```\n\n## Why MARLlib?\n\nHere we provide a table for the comparison of MARLlib and existing work.\n\n|   Library   |  Supported Env | Algorithm | Parameter Sharing  | Model \n|:-------------:|:-------------:|:-------------:|:--------------:|:----------------:|\n|     [PyMARL](https://github.com/oxwhirl/pymarl) |        1 cooperative       |       5       |         share        |      GRU           | :x:\n|   [PyMARL2](https://github.com/hijkzzz/pymarl2)|        2 cooperative       |     11   |         share        |  MLP + GRU  | :x:\n| [MAPPO Benchmark](https://github.com/marlbenchmark/on-policy) |       4 cooperative       |      1     |          share + separate        |          MLP + GRU        |         :x:              |\n| [MAlib](https://github.com/sjtu-marl/malib) |  4 self-play  | 10 | share + group + separate | MLP + LSTM | [![Documentation Status](https://readthedocs.org/projects/malib/badge/?version=latest)](https://malib.readthedocs.io/en/latest/?badge=latest)\n|    [EPyMARL](https://github.com/uoe-agents/epymarl)|        4 cooperative      |    9    |        share + separate       |      GRU             |           :x:            |\n|    **[MARLlib](https://github.com/Replicable-MARL/MARLlib)** |       10 **no task mode restriction**     |    18     |   share + group + separate + **customizable**         |         MLP + CNN + GRU + LSTM          |           [![Documentation Status](https://readthedocs.org/projects/marllib/badge/?version=latest)](https://marllib.readthedocs.io/en/latest/) |\n\n|   Library   | Github Stars  | Documentation | Issues Open | Activity | Last Update\n|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n|     [PyMARL](https://github.com/oxwhirl/pymarl) | [![GitHub stars](https://img.shields.io/github/stars/oxwhirl/pymarl)](https://github.com/oxwhirl/pymarl)    |       :x: | ![GitHub opened issue](https://img.shields.io/github/issues/oxwhirl/pymarl.svg) | ![GitHub commit-activity](https://img.shields.io/github/commit-activity/y/oxwhirl/pymarl?label=commit) | ![GitHub last commit](https://img.shields.io/github/last-commit/oxwhirl/pymarl?label=last%20update)  \n|   [PyMARL2](https://github.com/hijkzzz/pymarl2)| [![GitHub stars](https://img.shields.io/github/stars/hijkzzz/pymarl2)](https://github.com/hijkzzz/pymarl2)       |       :x:  | ![GitHub opened issue](https://img.shields.io/github/issues/hijkzzz/pymarl2.svg) | ![GitHub commit-activity](https://img.shields.io/github/commit-activity/y/hijkzzz/pymarl2?label=commit) | ![GitHub last commit](https://img.shields.io/github/last-commit/hijkzzz/pymarl2?label=last%20update)  \n| [MAPPO Benchmark](https://github.com/marlbenchmark/on-policy)| [![GitHub stars](https://img.shields.io/github/stars/marlbenchmark/on-policy)](https://github.com/marlbenchmark/on-policy)   |        :x:              | ![GitHub opened issue](https://img.shields.io/github/issues/marlbenchmark/on-policy.svg) | ![GitHub commit-activity](https://img.shields.io/github/commit-activity/y/marlbenchmark/on-policy?label=commit)| ![GitHub last commit](https://img.shields.io/github/last-commit/marlbenchmark/on-policy?label=last%20update)  \n| [MAlib](https://github.com/sjtu-marl/malib) | [![GitHub stars](https://img.shields.io/github/stars/sjtu-marl/malib)](https://github.com/hijkzzz/sjtu-marl/malib) | [![Documentation Status](https://readthedocs.org/projects/malib/badge/?version=latest)](https://malib.readthedocs.io/en/latest/?badge=latest) | ![GitHub opened issue](https://img.shields.io/github/issues/sjtu-marl/malib.svg) | ![GitHub commit-activity](https://img.shields.io/github/commit-activity/y/sjtu-marl/malib?label=commit) | ![GitHub last commit](https://img.shields.io/github/last-commit/sjtu-marl/malib?label=last%20update)  \n|    [EPyMARL](https://github.com/uoe-agents/epymarl)| [![GitHub stars](https://img.shields.io/github/stars/uoe-agents/epymarl)](https://github.com/uoe-agents/epymarl)        |           :x:            | ![GitHub opened issue](https://img.shields.io/github/issues/uoe-agents/epymarl.svg) | ![GitHub commit-activity](https://img.shields.io/github/commit-activity/y/uoe-agents/epymarl?label=commit) | ![GitHub last commit](https://img.shields.io/github/last-commit/uoe-agents/epymarl?label=last%20update)  \n|    **[MARLlib](https://github.com/Replicable-MARL/MARLlib)** |  [![GitHub stars](https://img.shields.io/github/stars/Replicable-MARL/MARLlib)](https://github.com/Replicable-MARL/MARLlib)  |           [![Documentation Status](https://readthedocs.org/projects/marllib/badge/?version=latest)](https://marllib.readthedocs.io/en/latest/) | ![GitHub opened issue](https://img.shields.io/github/issues/Replicable-MARL/MARLlib.svg) | ![GitHub commit-activity](https://img.shields.io/github/commit-activity/m/Replicable-MARL/MARLlib/sy_dev?label=commit) | ![GitHub last commit](https://img.shields.io/github/last-commit/Replicable-MARL/MARLlib/sy_dev?label=last%20update)  \n\n\n\n[comment]: <> (<div align=\"center\">)\n\n[comment]: <> (<img src=docs/source/images/overview.png width=100% />)\n\n[comment]: <> (</div>)\n\n## key features\n\n:beginner: What **MARLlib** brings to MARL community:\n\n- it unifies diverse algorithm pipelines with agent-level distributed dataflow.\n- it supports all task modes: cooperative, collaborative, competitive, and mixed.\n- it unifies multi-agent environment interfaces with a new interface following Gym.\n- it provides flexible and customizable parameter-sharing strategies.\n\n:rocket: With MARLlib, you can exploit the advantages not limited to:\n\n- **zero knowledge of MARL**: out of the box 18 algorithms with intuitive API!\n- **all task modes available**: support almost all multi-agent environment!\n- **customizable model arch**: pick your favorite one from the model zoo!\n- **customizable policy sharing**: grouped by MARLlib or build your own!\n- more than a thousand experiments are conducted and released!\n\n## Installation\n\n> __Note__:\n> MARLlib supports Linux only.\n\n### Step-by-step  (recommended)\n\n- install dependencies\n- install environments\n- install patches\n\n#### 1. install dependencies (basic)\n\nFirst, install MARLlib dependencies to guarantee basic usage.\nfollowing [this guide](https://marllib.readthedocs.io/en/latest/handbook/env.html), finally install patches for RLlib.\n\n```bash\n$ conda create -n marllib python=3.8\n$ conda activate marllib\n$ git clone https://github.com/Replicable-MARL/MARLlib.git && cd MARLlib\n$ pip install -r requirements.txt\n```\n\n#### 2. install environments (optional)\n\nPlease follow [this guide](https://marllib.readthedocs.io/en/latest/handbook/env.html).\n\n#### 3. install patches (basic)\n\nFix bugs of RLlib using patches by running the following command:\n\n```bash\n$ cd /Path/To/MARLlib/marl/patch\n$ python add_patch.py -y\n```\n\n### PyPI\n\n```bash\n$ pip install --upgrade pip\n$ pip install marllib\n```\n\n## Getting started\n\n<details>\n<summary><b><big>Prepare the configuration</big></b></summary>\n\nThere are four parts of configurations that take charge of the whole training process.\n\n- scenario: specify the environment/task settings\n- algorithm: choose the hyperparameters of the algorithm\n- model: customize the model architecture\n- ray/rllib: change the basic training settings\n\n<div align=\"center\">\n<img src=docs/source/images/configurations.png width=100% />\n</div>\n\nBefore training, ensure all the parameters are set correctly, especially those you don't want to change.\n> __Note__:\n> You can also modify all the pre-set parameters via MARLLib API.*\n\n</details>\n\n<details>\n<summary><b><big>Register the environment</big></b></summary>\n\nEnsure all the dependencies are installed for the environment you are running with. Otherwise, please refer to\n[MARLlib documentation](https://marllib.readthedocs.io/en/latest/handbook/env.html).\n\n\n|   task mode   | api example |\n| :-----------: | ----------- |\n| cooperative | ```marl.make_env(environment_name=\"mpe\", map_name=\"simple_spread\", force_coop=True)``` |\n| collaborative | ```marl.make_env(environment_name=\"mpe\", map_name=\"simple_spread\")``` |\n| competitive | ```marl.make_env(environment_name=\"mpe\", map_name=\"simple_adversary\")``` |\n| mixed | ```marl.make_env(environment_name=\"mpe\", map_name=\"simple_crypto\")``` |\n\nMost of the popular environments in MARL research are supported by MARLlib:\n\n| Env Name | Learning Mode | Observability | Action Space | Observations |\n| :-----------: | :-----------: | :-----------: | :-----------: | :-----------: |\n| **[LBF](https://github.com/semitable/lb-foraging)**  | cooperative + collaborative | Both | Discrete | 1D  |\n| **[RWARE](https://github.com/semitable/robotic-warehouse)**  | cooperative | Partial | Discrete | 1D  |\n| **[MPE](https://github.com/openai/multiagent-particle-envs)**  | cooperative + collaborative + mixed | Both | Both | 1D  |\n| **[SMAC](https://github.com/oxwhirl/smac)**  | cooperative | Partial | Discrete | 1D |\n| **[MetaDrive](https://github.com/decisionforce/metadrive)**  | collaborative | Partial | Continuous | 1D |\n| **[MAgent](https://www.pettingzoo.ml/magent)** | collaborative + mixed | Partial | Discrete | 2D |\n| **[Pommerman](https://github.com/MultiAgentLearning/playground)**  | collaborative + competitive + mixed | Both | Discrete | 2D |\n| **[MAMuJoCo](https://github.com/schroederdewitt/multiagent_mujoco)**  | cooperative | Partial | Continuous | 1D |\n| **[GRF](https://github.com/google-research/football)**  | collaborative + mixed | Full | Discrete | 2D |\n| **[Hanabi](https://github.com/deepmind/hanabi-learning-environment)** | cooperative | Partial | Discrete | 1D |\n\nEach environment has a readme file, standing as the instruction for this task, including env settings, installation, and\nimportant notes.\n</details>\n\n<details>\n<summary><b><big>Initialize the algorithm</big></b></summary>\n\n\n|  running target   | api example |\n| :-----------: | ----------- |\n| train & finetune  | ```marl.algos.mappo(hyperparam_source=$ENV)``` |\n| develop & debug | ```marl.algos.mappo(hyperparam_source=\"test\")``` |\n| 3rd party env | ```marl.algos.mappo(hyperparam_source=\"common\")``` |\n\nHere is a chart describing the characteristics of each algorithm:\n\n| algorithm                                                    | support task mode | discrete action   | continuous action |  policy type        |\n| :------------------------------------------------------------: | :-----------------: | :----------: | :--------------------: | :----------: | \n| *IQL**                                                         | all four               | :heavy_check_mark:   |    |  off-policy |\n| *[PG](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)* | all four                  | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *[A2C](https://arxiv.org/abs/1602.01783)*                      | all four              | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *[DDPG](https://arxiv.org/abs/1509.02971)*                     | all four             |  | :heavy_check_mark:   |  off-policy |\n| *[TRPO](http://proceedings.mlr.press/v37/schulman15.pdf)*      | all four            | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *[PPO](https://arxiv.org/abs/1707.06347)*                      | all four            | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *[COMA](https://ojs.aaai.org/index.php/AAAI/article/download/11794/11653)* | all four                           | :heavy_check_mark:       |   |  on-policy  |\n| *[MADDPG](https://arxiv.org/abs/1706.02275)*                   | all four                     |  | :heavy_check_mark:   |  off-policy |\n| *MAA2C**                                                       | all four                        | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *MATRPO**                                                      | all four                         | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *[MAPPO](https://arxiv.org/abs/2103.01955)*                    | all four                         | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *[HATRPO](https://arxiv.org/abs/2109.11251)*                   | cooperative                     | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *[HAPPO](https://arxiv.org/abs/2109.11251)*                    | cooperative                     | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *[VDN](https://arxiv.org/abs/1706.05296)*                      | cooperative         | :heavy_check_mark:   |    |  off-policy |\n| *[QMIX](https://arxiv.org/abs/1803.11485)*                     | cooperative                    | :heavy_check_mark:   |   |  off-policy |\n| *[FACMAC](https://arxiv.org/abs/2003.06709)*                   | cooperative                    |  | :heavy_check_mark:   |  off-policy |\n| *[VDAC](https://arxiv.org/abs/2007.12306)*                    | cooperative                    | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n| *VDPPO**                                                      | cooperative                | :heavy_check_mark:       | :heavy_check_mark:   |  on-policy  |\n\n***all four**: cooperative collaborative competitive mixed\n\n*IQL* is the multi-agent version of Q learning.\n*MAA2C* and *MATRPO* are the centralized version of A2C and TRPO.\n*VDPPO* is the value decomposition version of PPO.\n\n</details>\n\n<details>\n<summary><b><big>Build the agent model</big></b></summary>\n\nAn agent model consists of two parts, `encoder` and `core arch`. \n`encoder` will be constructed by MARLlib according to the observation space.\nChoose `mlp`, `gru`, or `lstm` as you like to build the complete model.\n\n|  model arch   | api example |\n| :-----------: | ----------- |\n| MLP  | ```marl.build_model(env, algo, {\"core_arch\": \"mlp\")``` |\n| GRU | ```marl.build_model(env, algo, {\"core_arch\": \"gru\"})```  |\n| LSTM | ```marl.build_model(env, algo, {\"core_arch\": \"lstm\"})```  |\n| Encoder Arch | ```marl.build_model(env, algo, {\"core_arch\": \"gru\", \"encode_layer\": \"128-256\"})```  |\n\n\n</details>\n\n<details>\n<summary><b><big>Kick off the training</big></b></summary>\n\n|  setting   | api example |\n| :-----------: | ----------- |\n| train  | ```algo.fit(env, model)``` |\n| debug  | ```algo.fit(env, model, local_mode=True)``` |\n| stop condition | ```algo.fit(env, model, stop={'episode_reward_mean': 2000, 'timesteps_total': 10000000})```  |\n| policy sharing | ```algo.fit(env, model, share_policy='all') # or 'group' / 'individual'```  |\n| save model | ```algo.fit(env, model, checkpoint_freq=100, checkpoint_end=True)```  |\n| GPU accelerate  | ```algo.fit(env, model, local_mode=False, num_gpus=1)``` |\n| CPU accelerate | ```algo.fit(env, model, local_mode=False, num_workers=5)```  |\n\n</details>\n\n<details>\n<summary><b><big>Training & rendering API</big></b></summary>\n\n```py\nfrom marllib import marl\n\n# prepare env\nenv = marl.make_env(environment_name=\"mpe\", map_name=\"simple_spread\")\n# initialize algorithm with appointed hyper-parameters\nmappo = marl.algos.mappo(hyperparam_source=\"mpe\")\n# build agent model based on env + algorithms + user preference\nmodel = marl.build_model(env, mappo, {\"core_arch\": \"mlp\", \"encode_layer\": \"128-256\"})\n# start training\nmappo.fit(\n  env, model, \n  stop={\"timesteps_total\": 1000000}, \n  checkpoint_freq=100, \n  share_policy=\"group\"\n)\n# rendering\nmappo.render(\n  env, model, \n  local_mode=True, \n  restore_path={'params_path': \"checkpoint_000010/params.json\",\n                'model_path': \"checkpoint_000010/checkpoint-10\"}\n)\n```\n</details>\n\n## Benchmark results\n\nAll results are listed [here](https://github.com/Replicable-MARL/MARLlib/tree/main/results).\n\n## Quick examples\n\nMARLlib provides some practical examples for you to refer to.\n\n- [Detailed API usage](https://github.com/Replicable-MARL/MARLlib/blob/sy_dev/examples/api_basic_usage.py): show how to use MARLlib api in\n  detail, e.g. cmd + api combined running.\n- [Policy sharing cutomization](https://github.com/Replicable-MARL/MARLlib/blob/sy_dev/examples/customize_policy_sharing.py):\n  define your group policy-sharing strategy as you like based on current tasks.\n- [Loading model and rendering](https://github.com/Replicable-MARL/MARLlib/blob/sy_dev/examples/load_and_render_model.py):\n  render the environment based on the pre-trained model.\n- [Incorporating new environment](https://github.com/Replicable-MARL/MARLlib/blob/sy_dev/examples/add_new_env.py):\n  add your new environment following MARLlib's env-agent interaction interface.\n- [Incorporating new algorithm](https://github.com/Replicable-MARL/MARLlib/blob/sy_dev/examples/add_new_algorithm.py):\n  add your new algorithm following MARLlib learning pipeline.\n\n## Tutorials\n\nTry MPE + MAPPO examples on Google Colaboratory!\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Replicable-MARL/MARLlib/blob/sy_dev/marllib.ipynb)\n\nMore tutorial documentations are available [here](https://marllib.readthedocs.io/).\n\n## Community\n\n|  Channel   | Link |\n| :----------- | :----------- |\n| Issues | [GitHub Issues](https://github.com/Replicable-MARL/MARLlib/issues) |\n\n\n## Contributing\n\nWe are a small team on multi-agent reinforcement learning, and we will take all the help we can get! \nIf you would like to get involved, here is information on [contribution guidelines and how to test the code locally](https://github.com/Replicable-MARL/MARLlib/blob/sy_dev/CONTRIBUTING.md).\n\nYou can contribute in multiple ways, e.g., reporting bugs, writing or translating documentation, reviewing or refactoring code, requesting or implementing new features, etc.\n\n[comment]: <> (## Paper)\n\n[comment]: <> (If you use MARLlib in your research, please cite the [MARLlib paper]&#40;https://arxiv.org/abs/2210.13708&#41;.)\n\n[comment]: <> (```tex)\n\n[comment]: <> (@article{hu2022marllib,)\n\n[comment]: <> (  title={MARLlib: Extending RLlib for Multi-agent Reinforcement Learning},)\n\n[comment]: <> (  author={Hu, Siyi and Zhong, Yifan and Gao, Minquan and Wang, Weixun and Dong, Hao and Li, Zhihui and Liang, Xiaodan and Chang, Xiaojun and Yang, Yaodong},)\n\n[comment]: <> (  journal={arXiv preprint arXiv:2210.13708},)\n\n[comment]: <> (  year={2022})\n\n[comment]: <> (})\n\n[comment]: <> (```)\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "marllib",
    "package_url": "https://pypi.org/project/marllib/",
    "platform": null,
    "project_url": "https://pypi.org/project/marllib/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/marllib/1.0.2/",
    "requires_dist": null,
    "requires_python": ">=3.8",
    "summary": "",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17377391,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "958d23a93b643d2945b66eb7fcde3a6ddb376bc65df5a8341a86b7d2797c2386",
        "md5": "52fe0d232213fa5fd06541df68cd298e",
        "sha256": "db492e5d4e79034aedeb4a35fd6dd4e7d5b922ca6cb1869b4d879824dde1a0ad"
      },
      "downloads": -1,
      "filename": "marllib-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "52fe0d232213fa5fd06541df68cd298e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 173911,
      "upload_time": "2023-03-21T10:23:40",
      "upload_time_iso_8601": "2023-03-21T10:23:40.171850Z",
      "url": "https://files.pythonhosted.org/packages/95/8d/23a93b643d2945b66eb7fcde3a6ddb376bc65df5a8341a86b7d2797c2386/marllib-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}