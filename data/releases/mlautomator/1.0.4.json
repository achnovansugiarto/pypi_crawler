{
  "info": {
    "author": "Kevin Vecmanis",
    "author_email": "kevinvecmanis@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# ML Automator\n### Author: Kevin Vecmanis\n\nMachine Learning Automator (__ML Automator__) is an automation project that integrates __Sequential Model Based Optimization__ (SMBO) with the main learning algorithms from Python's Sci-kit Learn library to generate a really fast, automated tool for tuning machine learning algorithms.  __MLAutomator__ leverages a library called Hyperopt to accomplish this. [Read more about Hyperopt here](http://hyperopt.github.io/hyperopt/)\n\n## What is SMBO? \n\nSMBO is a form of hyperparameter tuning, like grid search and randomized search.  In contrast to grid and randomized search, however, SMBO used __Bayesian Optimization__ to build a probability model, through trial and error, that is able to better predict what hyperparameters might produce a better model.  The \"sequential\" just means that multiple trials are run, one after another, each time testing better hyper parameters by applying bayesion reasoning and updating the existing probability model.\n\nThe trade-off here is that SMBO models spend more time between each iteration \"selecting\" the next choice of hyperparameters - but this is accepted because the extra time taken to choose the next hyperparameters is typically __signigicantly__ less than each training iteration.  In other words, SMBO results in:\n\n* Reduced time tuning hyperparameters compared to grid and random search methods.\n* Better scores on the testing set.\n\n## Key features:\n\n* Optimizes across data pre-processing and feature selection __in addition__ to hyperparameters.\n* Fast, intelligent scan of parameter space using __Hyperopt__.\n* Optimized parameter search permits scanning a larger cross section of algorithms in the same period of time.\n* An exceptional spot-checking algorithm.\n\n## Usage \n\n__MLAutomator__ accepts a training dataset X, and a target Y.  The user can define their own functions for how these datasets are produced.  Note that MLAutomator is designed to be a highly optimized spot-checking algorithm - you should take care to make sure your data is free from errors and any missing values have been dealt with.   \n\n__MLAutomator__ will find ways of transforming and pre-processing your data to produce a superior model.  Feel free to make your own transformations before passing the data to __MLAutomator__.  \n\n### Optional data utilities\n\nI'm building a suite of data utility functions which can prepare most classification and regression datasets.  These, however, are optional - __MLAutomator__ only requires __X and Y__ inputs in the form of a numpy ndarray.\n\n```Python\nfrom data.utilities import clf_prep\n\nx, y = clf_prep('pima-indians-diabetes.csv')\n```\n\nOnce you have training and target data, this is the main call to use MLAutomator...\n\n### Classification Example: 2-class\n\n```Python\nfrom mlautomator.mlautomator import MLAutomator\n\nautomator = MLAutomator(x, y, iterations = 25)\nautomator.find_best_algorithm()\nautomator.print_best_space()\n```\n\n__MLAutomator__ can typically find a ~ 98th percentile solution in a fraction of the time of __Gridsearch or Randomized search__.  Here it did a comprehensive scan across all hyperparameters for 6 common machine learning algorithms and produced exceptional model performance for the classic Pima Indians Diabetes dataset.\n\n```\nBest Algorithm Configuration:\n    Best algorithm: Logistic Regression\n    Best accuracy : 77.73239917976761%\n    C : 0.02341\n    k_best : 6\n    penalty : l2\n    scaler : RobustScaler(\n        copy=True, \n        quantile_range=(25.0, 75.0), \n        with_centering=True,\n        with_scaling=True)\n    solver : lbfgs\n    Found best solution on iteration 132 of 150\n    Validation used: 10-fold cross-validation\n```\n\n### Classification Example: Multi-class\n\nHere are the results from the classic iris dataset, a multi-class classification problem with three classes\n\n```Python\nfrom data.utilities import from_sklearn\nfrom mlautomator.mlautomator import MLAutomator\n\nx, y = from_sklearn('iris')\nautomator = MLAutomator(x, y, iterations = 30, algo_type = 'classifier', score_metric = 'accuracy')\nautomator.find_best_algorithm()\nautomator.print_best_space()\n```\n\n```\nBest Algorithm Configuration:\n    Best algorithm: Bag of Support Vector Machine Classifiers\n    Best accuracy : 96.67%\n    C : 0.7064\n    degree : 2\n    gamma : auto\n    k_best : 2\n    kernel : rbf\n    n_estimators : 9\n    probability : True\n    scaler : None\n    Found best solution on iteration 3 of 30\n    Validation used: 10-fold cross-validation\n```\n\n\n\n### Regression Example\n\nML Automator supports regression problems as well. In this example we call the Boston Housing dataset from __sklearn.datasets__ using one of our utility functions.\n\n```Python\nfrom data.utilities import from_sklearn\n\nx, y = from_sklearn('boston')\n```\n\n```Python\nfrom mlautomator.mlautomator import MLAutomator\n\nautomator = MLAutomator(x, y, iterations = 30, algo_type = 'regressor', score_metric = 'neg_mean_squared_error')\nautomator.find_best_algorithm()\nautomator.print_best_space()\n```\n\n```\nBest Algorithm Configuration:\n    Best algorithm: K-Neighbor Regressor\n    Best neg_mean_squared_error : 10.41395782834094\n    algorithm : kd_tree\n    k_best : 11\n    n_neighbors : 2\n    scaler : StandardScaler(copy=True, with_mean=True, with_std=True)\n    weights : distance\n    Found best solution on iteration 24 of 30\n    Validation used: 10-fold cross-validation\n```\n\n## Existing Algorithm Support\n\n__MLAutomator__ currently supports the following algorithms:\n\n### Classification:\n* XGBoost Classifier\n* Random Forest Classifier\n* Support Vector Machines\n* Naive Bayes Classifier\n* Stochastic Gradient Descent Classification (SGD)\n* K-Nearest Neighbors Classification\n* Logistic Regression \n\n### Regression: \n* XGBoost Regressor\n* Random Forest Regressor\n* Support Vector Machine Regression\n* SGD Regression\n* K-Nearest Neighbors Regression\n\nUnless otherwise declared using the __specific_algos__ argument, MLAutomator will scan all algorithms to find the best performer.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/VanAurum/ML-automator",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlautomator",
    "package_url": "https://pypi.org/project/mlautomator/",
    "platform": "",
    "project_url": "https://pypi.org/project/mlautomator/",
    "project_urls": {
      "Homepage": "https://github.com/VanAurum/ML-automator"
    },
    "release_url": "https://pypi.org/project/mlautomator/1.0.4/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A fast, simple way to train machine learning algorithms",
    "version": "1.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 5636776,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "868ce90387c8e26735e02ff2d6a2f00b505240c8f664250d9d2e638d4172d47e",
        "md5": "a6202793e4ded2ed04b502cb59874838",
        "sha256": "0a596fd4cfcde56bb2d2afe72c7640298d443a50f0e65242258a8074ae5f7fd0"
      },
      "downloads": -1,
      "filename": "mlautomator-1.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "a6202793e4ded2ed04b502cb59874838",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 13306,
      "upload_time": "2019-05-27T19:54:05",
      "upload_time_iso_8601": "2019-05-27T19:54:05.341787Z",
      "url": "https://files.pythonhosted.org/packages/86/8c/e90387c8e26735e02ff2d6a2f00b505240c8f664250d9d2e638d4172d47e/mlautomator-1.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}