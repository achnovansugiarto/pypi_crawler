{
  "info": {
    "author": "Divine Darkey (teddbug-S)",
    "author_email": "teddbug47@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# DupliCat\n\nA simple utility for finding duplicated files within a specified path.\nIt is intended to be a library but can also be used as a commandline tool,\nit doesn't delete the duplicate files found but returns a list of junk files so that you can choose the ones to delete.\n\n# Usage As A Library:\n   - Import the [dupliCat](https://github.com/teddbug-S/dupliCat/blob/main/src/dupliCat/__init__.py) class and create an object by passing the following arguments,\n       - `path`\n           where the search will be made, defaults to current directory.\n       - `recurse`\n           boolean, set to true if you want it to recurse down to all files in the path including sub-dirs\n           defaults to `False`\n   \n   ## Methods\n\n   - the `generate_secure_hash` method takes a file as first argument and generates a secure-hash for it.\n     Hashing algorithm is blake2b, key is the size of the file, it returns the file with secure_hash attribute\n     set. File must be of type `dupliFile`.\n\n   - `read_chunk` this method reads a default 400 bytes of data from file. It takes the file as first positional\n      argument and size as second argument which defaults to 400. File must be of type `dupliFile`\n\n   - `human_size` this is a static method that converts bytes into human-readable format.\n   ```doctest\n     >>> human_size(nbytes=123456)\n     >>> 120.56 KB\n   ```\n   - `hash_chunk` static method, takes two positional arguments, `text: str` and `key: int`\n      hashes text with key using blake2b.\n  \n   - call the `search_duplicate` method to begin the 🔍 search, search results will be stored in \n       the `duplicates` property of the class. This method is somewhat the main api of the class, it \n       does everything for you, calling other methods instead of this might remove the functionality of\n       using files from `size_index` as input for generating a hash index.\n       \n       the `search_duplicate` method has the following optional arguments\n       - `use_hash`\n           find duplicates using hash table if set to True otherwise uses size_table, using the \n           hash_table is more accurate in conditions where different files have same sizes\n           and quite fast since table is generated using the size table, parameter defaults to `True`.\n           Using the size table is faster but does not guarantee accuracy.\n       - `from_size`\n          it defaults to True, this enables the `search_duplicate` method to generate a hash index\n          using files from the size index. Defaults to `True`.\n       \n    Note: \n        Both parameters are set to True for more accurate search since search will be done using the\n       `size_index` and also the `hash_index`.\n        Junk files set by this method contains all duplicates with one file left over for each.\n \n   - use the `analyse` method to analyse search result, this returns a named tuple of type `Analysis`.\n   It contains \n       the total number of duplicate files accessed through `analysis.total_file_num`, their total size on the disk\n       accessed through `analysis.total_size` and the most occurred file, accessed through `analysis.most_occurrence`.\n  \n  ## Properties\n\n   - `size_index`\n      You can also access the size index using the property. it is a dictionary containing files\n      grouped by their sizes.\n   - `hash_index`\n      You can also access the hash index using this property. It is a dictionary containing files\n      grouped by their secure hashes.\n   -  `fetched_file`\n      access all fetched files from the search path\n   - `path`\n           where the search will be made, defaults to current directory.\n   - `recurse`\n      boolean, set to true if you want it to recurse down to all files in the path including sub-dirs\n      defaults to `False`\n   \n     \n# Usage From Commandline\n   Functionality removed for now.\n\n# Contact\n   - twitter: [teddbug](https://www.twitter.com/teddbug)\n   - facebook: [Tedd Bug](https://www.facebook.com/tedd.bug.79/)\n\n😄 Happy Coding!\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/teddbug-S/dupliCat",
    "keywords": "",
    "license": "MIT",
    "maintainer": "Divine Darkey (teddbug-S)",
    "maintainer_email": "teddbug47@gmail.com",
    "name": "dupliCat",
    "package_url": "https://pypi.org/project/dupliCat/",
    "platform": "",
    "project_url": "https://pypi.org/project/dupliCat/",
    "project_urls": {
      "Homepage": "https://github.com/teddbug-S/dupliCat",
      "Issues": "https://github.com/teddbug-S/dupliCat/issues",
      "Pull Requests": "https://github.com/teddbug-S/dupliCat/pulls"
    },
    "release_url": "https://pypi.org/project/dupliCat/2.2.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A simple package to hunt down file duplicates",
    "version": "2.2.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13898960,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c8dabcb071942935c68f827e6df604ee2ef8821c67a7c36f2b073a6ca0c72730",
        "md5": "e030d773dd85ce7075f4eacc173663db",
        "sha256": "4d0faeffb95f4504f647177ab3ba20a8323bc4f327540f7e1e41f092d08e8c68"
      },
      "downloads": -1,
      "filename": "dupliCat-2.2.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e030d773dd85ce7075f4eacc173663db",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 7103,
      "upload_time": "2022-02-01T17:40:36",
      "upload_time_iso_8601": "2022-02-01T17:40:36.878919Z",
      "url": "https://files.pythonhosted.org/packages/c8/da/bcb071942935c68f827e6df604ee2ef8821c67a7c36f2b073a6ca0c72730/dupliCat-2.2.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "34bbd95ea9c09aa66511f5bf20b8220f1ef62aca3a132836f7d5d99e7f4f283a",
        "md5": "204757dd914f94538188169efd799de9",
        "sha256": "c67e5e6312a8972d2d0fe91d55d2399fe82436823b3d859466184870517ad486"
      },
      "downloads": -1,
      "filename": "dupliCat-2.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "204757dd914f94538188169efd799de9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 6586,
      "upload_time": "2022-02-01T17:40:38",
      "upload_time_iso_8601": "2022-02-01T17:40:38.629385Z",
      "url": "https://files.pythonhosted.org/packages/34/bb/d95ea9c09aa66511f5bf20b8220f1ef62aca3a132836f7d5d99e7f4f283a/dupliCat-2.2.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}