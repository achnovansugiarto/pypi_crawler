{
  "info": {
    "author": "Yuankai Guo, Liang Shi, Yupeng Chen",
    "author_email": "guoyuankai@xiaomi.com, shiliang1@xiaomi.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# MiNLP-Tokenizer\n\n## 1. 工具介绍\n\nMiNLP-Tokenizer是小米AI实验室NLP团队自研的中文分词工具，基于深度学习序列标注模型实现，在公开测试集上取得了SOTA效果。其具备以下特点：\n- **分词效果好**：基于深度学习模型在大规模语料上进行训练，粗、细粒度在SIGHAN 2005 PKU测试集上的F1分别达到95.7%和96.3%[注1]\n- **轻量级模型**：精简模型参数和结构，模型仅有20MB\n- **词典可定制**：灵活、方便的干预机制，根据用户词典对模型结果进行干预\n- **多粒度切分**：提供粗、细粒度两种分词规范，满足各种场景需要\n- **调用更便捷**：一键快速安装，API简单易用\n\n注1：我们结合公司应用场景，制定了粗、细粒度分词规范，并按照规范对PKU测试集重新进行了标注（由于测试集版权限制，未包含在本项目中）。\n\n## 2. 安装\n\npip全自动安装：\n```\npip install minlp-tokenizer\n```\n适用环境：Python 3.5~3.8，TensorFlow>=1.14\n\n## 3. 使用API\n\n```python\nfrom minlptokenizer.tokenizer import MiNLPTokenizer\n\ntokenizer = MiNLPTokenizer(granularity='fine')  # fine：细粒度，coarse：粗粒度，默认为细粒度\nprint(tokenizer.cut('今天天气怎么样？'))\n```\n\n## 4. 自定义用户词典\n\n- 通过用户词典List添加：\n ```python\nfrom minlptokenizer.tokenizer import MiNLPTokenizer\n\ntokenizer = MiNLPTokenizer(['word1', 'word2'], granularity='fine') #用户自定义干预词典传入\n ```\n\n- 通过文件路径方式添加\n ```python\nfrom minlptokenizer.tokenizer import MiNLPTokenizer\n\ntokenizer = MiNLPTokenizer('/path/to/your/lexicon/file', granularity='coarse')  # 构造函数的参数为用户词典路径\n ```\n \n## 5. 未来计划\n\nMiNLP是小米AI实验室NLP团队开发的小米自然语言处理平台，目前已经具备词法、句法、语义等数十个功能模块，在公司业务中得到了广泛应用。\n第一阶段我们开源了MiNLP的中文分词功能，后续我们将陆续开源词性标注、命名实体识别、句法分析等功能，和开发者一起打造功能强大、效果领先的NLP工具集。\n\n## 6. 参与开发\n\n我们欢迎开发者向MiNLP-Tokenizer贡献代码，也欢迎提出各种Issue和反馈意见。\n开发流程详见CONTRIBUTING.md。\n\n## 7.在学术成果中使用\n\n如果您在学术成果中使用了MiNLP中文分词工具，请按如下格式引用：\n  - 中文：郭元凯, 史亮, 陈宇鹏, 孟二利, 王斌. MiNLP-Tokenizer：小米中文分词工具. 2020.\n  - 英文：Yuankai Guo, Liang Shi, Yupeng Chen, Erli Meng, Bin Wang. MiNLP-Tokenzier: XiaoMi Chinese Word Segmenter. 2020.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/XiaoMi/MiNLP",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "minlp-tokenizer",
    "package_url": "https://pypi.org/project/minlp-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/minlp-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/XiaoMi/MiNLP"
    },
    "release_url": "https://pypi.org/project/minlp-tokenizer/3.2.4/",
    "requires_dist": null,
    "requires_python": ">=3.5, <=3.8",
    "summary": "MiNLP-Tokenizer中文分词工具",
    "version": "3.2.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9780123,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d467435a63eb085a95b3aab335e25f58eac017d62d59d9884aa0a4e9ca77faa4",
        "md5": "145a89684775387795e9f1e2db5e3e13",
        "sha256": "f849172e8fe8059520b195f453ac04d69cf6fd1bca656a850a5c2ea720039de4"
      },
      "downloads": -1,
      "filename": "minlp-tokenizer-3.2.4.tar.gz",
      "has_sig": false,
      "md5_digest": "145a89684775387795e9f1e2db5e3e13",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5, <=3.8",
      "size": 37454431,
      "upload_time": "2021-01-11T03:13:18",
      "upload_time_iso_8601": "2021-01-11T03:13:18.019997Z",
      "url": "https://files.pythonhosted.org/packages/d4/67/435a63eb085a95b3aab335e25f58eac017d62d59d9884aa0a4e9ca77faa4/minlp-tokenizer-3.2.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}