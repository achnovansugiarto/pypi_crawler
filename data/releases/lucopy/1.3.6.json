{
  "info": {
    "author": "Redkite",
    "author_email": "luco.support@redkite.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# LucoPy\n\nPython SDK to interact with the Luco API.\n\nContact: support@lu.co\n\n---\n\n## How to Use\n\nInstall the latest version from PyPI using PIP\n\n```bash\npip install LucoPy\n```\n\nOr, specify a version\n\n```bash\npip install LucoPy==x.x.x\n```\n\nImport the LucoApi class from the LucoPy library in your script and create an object of this class by passing in the API base url and the appropriate credentials. E.g.\n\n```bash\nfrom LucoPy import LucoApi\n\napi = LucoApi(base_url, tenant_id, client_id, client_secret, resource_id)\n```\n\n---\n\n## Authentication\n\nIn order to make calls to the API endpoints, LucoPy must be able to generate an authenticated access token. Authentication is managed by the ApiCore class using an identity provided during the instantiation of the LucoApi object.\n\n### Azure Service Principal\n\nIf the Luco instance is hosted on Azure, a service principal can be used to authenticate to the API. In order to use a service principal, an App Registration must be created in the same Azure subscription as the Luco instance. The required credentials must then be passed as arguments to the LucoApi class when instantiating it. These credentials are:\n\n- `tenant_id` - Directory (tenant) ID of the App Registration representing LucoPy\n- `client_id` - Application (client) ID of the App Registration representing LucoPy\n- `client_secret` - Secret value of the App Registration representing LucoPy\n- `resource_id` - Application (client) ID of the target App Registration representing the API\n\n### Other Identities\n\nAny custom identity object can be passed into the LucoApi class via the `identity` kwarg when instantiating the LucoApi class. This idenity object must have a method called `generate_token()` which returns an access token validated to the API and the expiry datetime of this token.\n\n---\n\n### LucoApi Class\n\n`LucoApi(base_url, tenant_id=None, client_id=None, client_secret=None, resource_id=None, identity=None, timeout=20, log=False)`\n\nThis class acts as the gateway to the Luco platform. An instance of this class should be created at the beginning of each script, API calls are then made through the ApiCore which handles the necessary authentication.\n\nThe base URL of the API instance must be passed as a parameter to this object along with the method of authentication.\n\nThe `timeout` option defines the maximum time (seconds) to wait for an HTTPS response from the API before causing a failure.\n\nUse the `log` argument to turn logging on or off. Logs are generated and sent to a log.txt file in the base directory alongside where the script is being run.\n\n1) `find_slot_id(tag, slot_sequence)`\n\n    Find slot id from a tag/date and slot sequence definition. If the slot sequence does not have an active delivery schedule and a new tag is provided - a new slot will be created with this tag and the id of this slot will be returned.\n\n    Args:\n    - tag (str) : Date (YYYY-MM-DD) for scheduled deliveries or Unique tag for unscheduled deliveries\n    - slot_sequence (dict or list of k:v pairs (dicts)) : list slot sequence definitions in form {'key': 'value'}.\n            Order matters - this determines parameter position.\n\n    Returns:\n    - slot_id (int)\n\n2) `get_submission(slot_id, submission_id)`\n\n    Returns a submission object representing an existing submission.\n\n    Args:\n    - slot_id (int)\n    - submission_id (int)\n\n    Returns:\n    - submission (Submission)\n\n3) `create_submission(slot_id)`\n\n    Create a submission against a slot and returns a Submission object representing it.\n\n    Args:\n    - slot_id (int)\n    - stage (string) : None\n    - run_environment (dict or list of dicts) : None\n\n    Returns:\n    - submission (Submission)\n\n4) `find_submission_in_slot_sequence(slotId, submissionId, OnlyCompletedSubmissions=False, TimeDifference=None, FindClosest='historic')`\n\n    Returns a Slot and Submission ID and whether it is an exact match based on the search criteria, and what the relative difference is in terms of time and number of slots.\n\n    Args:\n    - slot_id (int)\n    - submission_id (int)\n    - OnlyCompletedSubmissions (bool)\n    - TimeDifference (str) : d:HH:MM:SS\n    - FindClosest (str) : historic, future, either or exact\n\n    Returns:\n    - Response JSON (dict)\n\n5) `find_submissions_by_slot_sequence(slotSequence, onlyLatestSlot=True, onlyDeliveredSlots=True, onlyCompletedSubmissions=True, onlyLatestSubmission=True, expectedAfterUtc=None, expectedBeforeUtc=None)`\n\n    Returns submissions and their slots for a slot sequence\n\n    Args:\n    - slotSequence (dict or list of k:v pairs (dicts))\n    - onlyLatestSlot (bool)\n    - onlyDeliveredSlots (bool)\n    - onlyCompletedSubmissions (bool)\n    - onlyLatestSubmission (bool)\n    - expectedAfterUtc (str) : YYYY-MM-DD or YYYY-MM-DDThh:mm:ss\n    - expectedBeforeUtc (str) : YYYY-MM-DD or YYYY-MM-DDThh:mm:ss\n\n    Returns:\n    - Response JSON (dict)\n\n6) `find_latest_submission_by_slot_sequence(slotSequence, expectedAfterUtc=None, expectedBeforeUtc=None)`\n\n    Accessory method to find_submissions_by_slot_sequence(). Returns the slot id and submission id of the most recently completed submission on the slot sequence.\n\n    Equivalent to:\n\n    `find_submissions_by_slot_sequence(slotSequence, expectedAfterUtc=expectedAfterUtc, expectedBeforeUtc=expectedBeforeUtc)`\n\n    Where the response JSON is interpreted to only return the slot id and submission id.\n\n    Args:\n    - slotSequence (dict or list of k:v pairs (dicts))\n    - expectedAfterUtc (str) : YYYY-MM-DD or YYYY-MM-DDThh:mm:ss\n    - expectedBeforeUtc (str) : YYYY-MM-DD or YYYY-MM-DDThh:mm:ss\n\n    Returns:\n    - slot_id (int), submission_id (int)\n\n7) `submit_slot_sequences(slot_sequences, allow_overwrites=False, allow_archiving=False, ignore_delivery_config=False)`\n\n    Import new or update existing slot sequences.\n\n    Args:\n    - slot_sequences (dict (JSON))\n    - allow_overwrites (bool) : False\n    - allow_overwrites (bool) : False\n    - ignore_delivery_config (bool) : False\n\n    Retuns:\n    - results (dict)\n\n### Submission Class\n\n`Submission(slot_id, submission_id, core)`\n\nMuch of the functionality is handled at the Submission level. A Submission object is created by the `get_submission` or `create_submission` methods of the LucoApi class. These objects store the definition of the corresponding submission and handle methods relating to it.\n\n1) `params(group=None, key=None)`\n\n    Retrieve slot parameters. The `group` and `key` kwargs can be used to refine the response. Only use `key` in addition to `group`.\n\n    Args:\n    - group (str) : Parameter group to return\n    - key (str) : Key within group to return the value of\n\n    Returns:\n    - result (dict or str)\n\n2) `get_delivery_schedule()`\n\n    Get the currently active delivery schedule. Retuns `None` if there are no active schedules .\n\n    Returns:\n    - delivery_schedule (dict)\n\n3) `get_metrics(stages=None, metrics=None, group_by_stage=False)`\n\n    Retrieve metrics from Submission.\n    \n    Filter by stage and metric by passing strings or lists of strings.\n    Three different return formats:\n        - list of dicts : Default behaviour. All metrics for filtered stages and metrics\n        - dict : Metrics grouped by stage if kwarg group_by_stage=True\n        - metric value : The value of the specified metric if stages and metrics are given as strings\n\n    Args:\n        - stages (string or list of strings)\n        - metrics (string or list of strings)\n        - group_by_stage (bool) : Group metrics by stage. Skips metrics which do not have a stage.\n\n    Returns:\n        - metrics (array, dict or metric value)\n\n4) `get_quality() --> dict`\n\n    Retrieve quality results\n\n    Returns:\n    - quality (dict)\n\n5) `submit_run_environment(stage=None, run_environments=None)`\n\n    Submit run environment details\n\n    Args:\n    - stage (string) : Optional\n    - run_environments (dict or list of dicts) : Required\n\n    Returns:\n    - response status (Bool) : Boolen success or failure\n\n6) `submit_metrics(stage, metric=None, value=None, metrics=None)`\n\n    Submit metrics by passing a dict of metric : value pairs to `metrics`. Option to pass a single metric : value pair using `metric` and `value`. It is recommended to use `metrics`.\n\n    Args:\n    - stage (str)\n    - metric (str) : Metric key\n    - value (str) : Value of metric\n    - metrics (dict) : Dictionary of Metric : Value pairs.\n\n    Returns:\n    - response status (Bool) : Boolen success or failure\n\n7) `submit_quality(stage, tool=None, results=None, dataset=None, action=None)`\n\n    Submit quality results\n\n    Args:\n    - stage (str)\n    - tool (str)\n    - results (str)\n    - dataset (str) : Optional\n    - action (str)\n\n    Returns:\n    - response status (Bool) : Boolen success or failure\n\n8) `submit_status(status, stage=None, type=None, message=None, modified_by=None)`\n\n    Submit the status of the Submission\n\n    Args:\n    - status      (str)\n    - stage       (str) : Optional\n    - type        (str) : Optional\n    - message     (str) : Optional\n    - modified_by (str) : Optional\n\n    Returns:\n    - response status (Bool) : Boolen success or failure\n\n9) `submit_completed_status()`\n\n    Submit a status for a completed submission. Equivalent to:\n    `submit_status('Completed', 'Submission')`\n\n    Returns:\n    - response status (Bool) : Boolen success or failure\n\n---\n\n## Data Quality\n\nThis data quality module provides functionality to support the handling of data quality results. This module will support the conversion of DQ results from a variety of tools into a consistent format which can be submitted to Luco. This generic format is based around the concepts of **checks** and **collections**.\n\nA **collection** is a dictionary object with the following structure:\n\n```json\n{\n    \"name\": \"string\",\n    \"tool\": \"string\",\n    \"toolVersion\": \"1.2.3\",\n    \"referenceUrl\": \"<some-url>\",\n    \"checks\": [\n        {\n            \"check\": \"expect $col1 to not be null\", // Required\n            \"checkArgs\": {\n                \"col1\": \"Id\"\n            },\n            \"success\": true, // Required\n            \"onFail\": {\n                \"ignore\": true,\n                \"failedRecordsLink\": \"<link-to-blob-storage>\"\n            },\n            \"observed\": {\n                \"elementCount\": 10,\n                \"failedPercentage\": \"10.0%\"\n            },\n            \"referenceUrl\": \"<some-url>\",\n            \"tags\": [\n                \"Completeness\"\n            ],\n            \"metadata\": {}\n        }\n    ],\n    \"start\": \"2022-10-04 10:00:00\",\n    \"end\": \"2022-10-04 10:10:00\",\n    \"metadata\": {}\n}\n```\n\nUsing the `Submission.submit_quality()` method, DQ results can be submitted as either a single **check** or as a **collection** of checks.\n\n### Fail conditions\n\nChecks and Collections can have both a `success` and an `action` associated with them. `success` describes the result of the check i.e. did any records fail the check? Or, was the failure rate within an acceptable threshold? Every check must have a `success` value associated with it. \n\n`action` is an optional piece of metadata associated with a DQ check or collection. It describes what happens to the ongoing data process as a result of the success or failure of the data qualtity checks. For example, if the data fails some key checks then the `action` may be to cancel the data delivery process rather than continue with bad data.\n\nThe `onFail` field can be used to define what the `action` should be as a result of the `success` of DQ checks. This is a flexible field and custom logic can used to determine the `action` based on this field. \n\nThe default behaviour is to ignore failed checks and continue the data delivery unless configured otherwise. If there should be a dependency on a check then this can be defined with:\n\n```JSON\n\"onFail\": {\n    \"action\": \"fail\"\n}\n```\n\nThe `CheckResult` and `CollectionResult` objects have methods `is_exception_thrown()` which return True if there is a check which has `\"success\": False` and `\"action\": \"fail\"`. This is the method that is used if the kwarg `auto_determine_action` is `True` when `Submission.submit_quality()` is called.\n\n### Great Expectations\n\nThe `great_expectations` submodule supports the handling of validation results from running expectations and suites against a dataframe.\n\nUtility functions are provided to convert the validations results into a shape supported by Luco:\n\n```python\nimport LucoPy.data_quality.great_expectations.utils as ge_utils\n\ncheck = ge_utils.convert_expectation_to_check(expectation)\n\ncollection = ge_utils.convert_suite_to_collection(suite)\n```\n\nWhere `expectation` and `suite` are the validation results as dict objects.\n\n1) `convert_expectation_to_check(expectation_results, metadata_mappings={})`\n\n    Convert an expectation validation result dict into a `CheckResult` object.\n\n    Args:\n    - expectation_results (dict) : Validation result dict\n    - metadata_mappings (dict) : Custom mappings of key:value pairs in the `meta` field.\n\n    Returns:\n    - CheckResult\n\n2) `convert_suite_to_collection(expectation_suite_results: dict, suite_mappings={}, expectation_mappings={})`\n\n    Convert a suite validation result dict into a `CollectionResult` object.\n\n    Args:\n    - expectation_suite_results (dict) : Validation result dict\n    - suite_mappings (dict) : Custom mappings of key:value pairs in the suite level `meta` field.\n    - expectation_mappings (dict) : Custom mappings of key:value pairs in the expectation level `meta` fields.\n\n    Returns:\n    - CollectionResult\n\nData stored in the `meta` fields of the validation result objects can be mapped and then surfaced in the Luco UI. There are a number of *recognised fields* which Luco will detect and display differently to other metadata. The currently supported fields and their default custom mappings are:\n\nExpectation Meta\n\n| Recognised Field | Default Expected Name | Description |\n|----------|----------|----------|\n| onFail    | onFail    | Behaviour for when an expectation fails |\n| referenceUrl   | referenceUrl    | URL linking to the expectation definition |\n| tags   | tags    | Tags linked to the Data Quality Categories defined in Luco |\n\nExpectation Suite Meta\n\n| Recognised Field | Default Expected Name | Description |\n|----------|----------|----------|\n| toolVersion    | great_expectations_version    | Version of GE used |\n| name    | expectation_suite_name    | Name of the expectation suite |\n| start    | validation_time    | Datetime of when the validation was performed |\n| referenceUrl   | referenceUrl    | URL linking to the expectation suite definition |\n| tags   | tags    | Tags linked to the Data Quality Categories defined in Luco |\n\nIn order to implement custom mappings for these recognised fields, a dictionary of the custom mappings should be provided. E.g.\n\n```python\n\n# Keys should be the recognised fields\n# Values should be the fields present in meta\nexpectation_mapping = {\n    \"onFail\": \"Action\",\n    \"referenceUrl\": \"reference_url\"\n}\n\ncheck = convert_expectation_to_check(expectation_result,\n                                     metadata_mappings=expectation_mapping)\n```\n\n---\n\n## Version History\nLucoPy-1.3.6 : Refactor `Submission.get_metrics()` to remove breaking change. Added `group_by_stage` kwarg to group metrics.\n\nLucoPy-1.3.5 : DQ module improvements. Add options to auto determine `action` and raise exception.\n\nLucoPy-1.3.4 : Bug fix: Default expectation result format to 'BASIC' so it doesn't need to be explicity defined.\n\nLucoPy-1.3.3 : Update return format of Submission.get_metrics() method. ~~Potential breaking change for users of the method~~ (Resolved in 1.3.6).\n\nLucoPy-1.3.2 : DQ module improvements. Bug fixes around string rendering of expectation configurations and observed values.\n\nLucoPy-1.3.1 : DQ module improvements. New method CollectionResult.is_exception_thrown().\n\nLucoPy-1.3.0 : DQ module improvements. Support expectation results in JSON or Expectation format.\n\nLucoPy-1.2.9 : Improvement to great_expectations handing in DQ module. Remove need to convert validation results to dict.\n\nLucoPy-1.2.8 : Bug fix type checking in Submission.submit_quality() method.\n\nLucoPy-1.2.7 : Introduction of a data_quality module with support for great_expectations. Added support for slot sequences defined as a single dict rather than list of dicts.\n\nLucoPy-1.2.6 : Expose submission start and end times as attributes of the Submission object.\n\nLucoPy-1.2.5 : New method to get submission delivery schedule, improved error handling and Custom exceptions. Expose current submission status.\n\nLucoPy-1.2.4 : Bug fix for Submission.submit_metrics to allow a value of zero and catch incorrectly provided metric: value pairs.\n\nLucoPy-1.2.3 : New method to import slot sequences. Some minor quality of life updates.\n\nLucoPy-1.2.2 : Bug fix around unscheduled slots. More informative error handling.\n\nLucoPy-1.2.1 : Updated `find_slot_id` method to use new POST /slots/ endpoint to create unscheduled slots. No change to user.\n\nLucoPy-1.2.0 : First version hosted on PyPI.\n\n---\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "LucoPy",
    "package_url": "https://pypi.org/project/LucoPy/",
    "platform": null,
    "project_url": "https://pypi.org/project/LucoPy/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/LucoPy/1.3.6/",
    "requires_dist": [
      "requests"
    ],
    "requires_python": ">=3.6",
    "summary": "Python SDK to support the Luco data observability tool.",
    "version": "1.3.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17402063,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "43459422dbdb65163b685d0970c82ae4aaa30a3a76a2941a38cccb5af42cda5b",
        "md5": "40957b84cdbb1ad7f1651b616709c478",
        "sha256": "17e32c198d1aec79711687130c1c63f7a5bfc895f7d8d8d5cbc28d611c049da3"
      },
      "downloads": -1,
      "filename": "LucoPy-1.3.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "40957b84cdbb1ad7f1651b616709c478",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 20926,
      "upload_time": "2023-02-02T18:57:28",
      "upload_time_iso_8601": "2023-02-02T18:57:28.483201Z",
      "url": "https://files.pythonhosted.org/packages/43/45/9422dbdb65163b685d0970c82ae4aaa30a3a76a2941a38cccb5af42cda5b/LucoPy-1.3.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9129f832acd6a49ac1df0561fbc5e1e149c1035e7e69f59bd649b0def81c07a9",
        "md5": "ad9dcce3a18fd588d4d0fae51fa12cf7",
        "sha256": "71744a3ff80d79bae3154ca36bc5bea8936cc803ff86bc69484bf00ae82cb3fc"
      },
      "downloads": -1,
      "filename": "LucoPy-1.3.6.tar.gz",
      "has_sig": false,
      "md5_digest": "ad9dcce3a18fd588d4d0fae51fa12cf7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 22956,
      "upload_time": "2023-02-02T18:57:30",
      "upload_time_iso_8601": "2023-02-02T18:57:30.031466Z",
      "url": "https://files.pythonhosted.org/packages/91/29/f832acd6a49ac1df0561fbc5e1e149c1035e7e69f59bd649b0def81c07a9/LucoPy-1.3.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}