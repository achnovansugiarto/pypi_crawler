{
  "info": {
    "author": "Moritz Meister",
    "author_email": "moritz@logicalclocks.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Topic :: Utilities"
    ],
    "description": "<p align=\"center\">\n  <a href=\"https://github.com/logicalclocks/maggy\">\n    <img src=\"https://raw.githubusercontent.com/moritzmeister/maggy/mkdocs/docs/assets/images/maggy.png\" width=\"320\" alt=\"Maggy\">\n  </a>\n</p>\n\n<p align=\"center\">\n  <a href=\"https://community.hopsworks.ai\"><img\n    src=\"https://img.shields.io/discourse/users?label=Hopsworks%20Community&server=https%3A%2F%2Fcommunity.hopsworks.ai\"\n    alt=\"Hopsworks Community\"\n  /></a>\n    <a href=\"https://maggy.ai\"><img\n    src=\"https://img.shields.io/badge/docs-MAGGY-orange\"\n    alt=\"Maggy Documentation\"\n  /></a>\n  <a href=\"https://pypi.org/project/maggy/\"><img\n    src=\"https://img.shields.io/pypi/v/maggy?color=blue\"\n    alt=\"PyPiStatus\"\n  /></a>\n  <a href=\"https://pepy.tech/project/maggy/month\"><img\n    src=\"https://pepy.tech/badge/maggy/month\"\n    alt=\"Downloads\"\n  /></a>\n  <a href=\"https://github.com/psf/black\"><img\n    src=\"https://img.shields.io/badge/code%20style-black-000000.svg\"\n    alt=\"CodeStyle\"\n  /></a>\n  <a><img\n    src=\"https://img.shields.io/pypi/l/maggy?color=green\"\n    alt=\"License\"\n  /></a>\n</p>\n\nMaggy is a framework for **distribution transparent** machine learning experiments on [Apache Spark](https://spark.apache.org/).\nIn this post, we introduce a new unified framework for writing core ML training logic as **oblivious training functions**.\nMaggy enables you to reuse the same training code whether training small models on your laptop or reusing the same code to scale out hyperparameter tuning or distributed deep learning on a cluster.\nMaggy enables the replacement of the current waterfall development process for distributed ML applications, where code is rewritten at every stage to account for the different distribution context.\n\n<p align=\"center\">\n  <figure>\n    <a href=\"https://github.com/logicalclocks/maggy\">\n      <img src=\"https://raw.githubusercontent.com/moritzmeister/maggy/mkdocs/docs/assets/images/firstgraph.png\" alt=\"Maggy\">\n    </a>\n    <figcaption>Maggy uses the same distribution transparent training function in all steps of the machine learning development process.</figcaption>\n  </figure>\n</p>\n\n## Quick Start\n\nMaggy uses PySpark as an engine to distribute the training processes. To get started, install Maggy in the Python environment used by your Spark Cluster, or install Maggy in your local Python environment with the `'spark'` extra, to run on Spark in local mode:\n\n```python\npip install maggy\n```\n\nThe programming model consists of wrapping the code containing the model training\ninside a function. Inside that wrapper function provide all imports and\nparts that make up your experiment.\n\nSingle run experiment:\n\n```python\ndef train_fn():\n    # This is your training iteration loop\n    for i in range(number_iterations):\n        ...\n        # add the maggy reporter to report the metric to be optimized\n        reporter.broadcast(metric=accuracy)\n         ...\n    # Return metric to be optimized or any metric to be logged\n    return accuracy\n\nfrom maggy import experiment\nresult = experiment.lagom(train_fn=train_fn, name='MNIST')\n```\n\n**lagom** is a Swedish word meaning \"just the right amount\". This is how MAggy\nuses your resources.\n\n\n## Documentation\n\nFull documentation is available at [maggy.ai](https://maggy.ai/)\n\n## Contributing\n\nThere are various ways to contribute, and any contribution is welcome, please follow the\nCONTRIBUTING guide to get started.\n\n## Issues\n\nIssues can be reported on the official [GitHub repo](https://github.com/logicalclocks/maggy/issues) of Maggy.\n\n## Citation\n\nPlease see our publications on [maggy.ai](https://maggy.ai/publications) to find out how to cite our work.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/logicalclocks/maggy",
    "keywords": "Hyperparameter,Optimization,Distributed,Training,Keras,PyTorch,TensorFlow,Spark",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "maggy",
    "package_url": "https://pypi.org/project/maggy/",
    "platform": null,
    "project_url": "https://pypi.org/project/maggy/",
    "project_urls": {
      "Homepage": "https://github.com/logicalclocks/maggy"
    },
    "release_url": "https://pypi.org/project/maggy/1.1.0/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Distribution transparent Machine Learning experiments on Apache Spark",
    "version": "1.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14023600,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "30f4182f03e714ebecb432e1794346414bf079324bdec596ae8aa16da72e98a1",
        "md5": "4664b7940d4014f3cb6cac4607d6f701",
        "sha256": "fa9e9c4222eebcf145bb90efd7d0827d187a3d5badeb52de72721f47d39b2be2"
      },
      "downloads": -1,
      "filename": "maggy-1.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "4664b7940d4014f3cb6cac4607d6f701",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 97431,
      "upload_time": "2022-05-06T09:47:23",
      "upload_time_iso_8601": "2022-05-06T09:47:23.352703Z",
      "url": "https://files.pythonhosted.org/packages/30/f4/182f03e714ebecb432e1794346414bf079324bdec596ae8aa16da72e98a1/maggy-1.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}