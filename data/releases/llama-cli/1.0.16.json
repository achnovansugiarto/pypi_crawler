{
  "info": {
    "author": "Teemu Lehtinen",
    "author_email": "teemu.t.lehtinen@aalto.fi",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Llama CLI\n\nA utility program that fetches and preprocesses learning data from supported learning\ntools. Educators and researches have important usecases for accessing the raw data\nthat is generated while learners are using digital learning tools and environments.\nThese stakeholders can aim to e.g. analyse and improve teaching materials, methods,\nand activities.\n\nThe aim of Llama CLI is to support and ease the steps of\n1. connecting to the supported learning data sources\n2. excluding persons and unwanted data tables or columns\n3. fetching partial and complete data sets\n4. anonymizing data before research activities\n5. standardizing/transforming/sharing data\n6. sampling and selecting data for analysis/ML\n\nCurrently supported data sources are\n* A-plus https://apluslms.github.io/\n* JSON log files from https://github.com/acos-server/acos-server\n* Database export from https://docs.mongodb.com/database-tools/mongodump/\n\nTransforming program submissions and events to [ProgSnap 2](https://cssplice.github.io/progsnap2)\nis supported via `llama shell`.\n\n## Etymology\n\nThe name for the project comes from ~ la lumière à Montagne analytique. Pardon my French for ~ light on the mountain of analytics. Also LA is an acronym, that the\npackage author may have used in his thesis more than a decent number of times,\nand that stands for Learning Analytics which is a research field in education\ntechnologies. Llamas are also known from a controversial programming exercise for\ncomputer science majors at Aalto University.\n\n## Installation\n\nLlama CLI is available at PyPI. It has a number of automatically installed\ndependencies, most notably `pandas`, `numpy`, `scipy`, and `requests`.\n\n      % python3 -m pip install llama-cli\n      % llama\n\nOR contained in a virtual environment (directory)\n\n      % python3 -m venv .venv && .venv/bin/pip install llama-cli\n      % .venv/bin/llama\n\n\n## Instructions\n\nLlama CLI operates on the current working directory. The configurations and data\nwill be stored in that directory – little bit like when working with git repositories.\nOne work directory can connect with multiple data sources and one should select\nthe sources that the current research or analysis project requires.\n\n      % llama\n      Llama CLI fetches and preprocesses learning data\n\n      usage: llama <cmd> [<args>]\n\n         status      Show the working tree status\n         source      Manage sources for learning data\n         list        List available data tables and columns\n         privacy     Configure privacy (default: pseudoanonymous)\n         exclude     Exclude selected tables, columns, or persons at fetch\n         fetch       Fetch data from sources\n         anonymize   Export anonymized data\n         shell       Open python REPL with 'llama' instance for exported data\n\n1. Use `llama source add` to interactively connect with data sources.\n   The required addresses and keys will be prompted when required.\n2. Use `llama list` to fetch the available data tables.\n3. Time to consider excluding some uninteresting data or persons who have\n   not consent to the research at hand. See `llama exclude` for examples.\n4. Use `llama fetch rows` to download data tables. Depending on the project\n   it may be necessary to also `llama fetch files` and/or `llama fetch meta`.\n   This step has a delay between internet requests and it may take a long time\n   to complete. The rows can be fetched again to append new data if supported\n   by the data source.\n5. The data in `fetched` directory is pseudoanonymized by default.\n   The pseudo identifiers are required to complete fetching of depended data.\n   With access to the source database the pseudo identifiers can be traced to persons.\n   Use `llama anonymize` to produce `export` directory that can be e.g. stored in\n   research repository, when the security measures and research consent allow it.\n\n\n## Output & Research\n\nThe raw CSV and other files are available in the `export` directory. However,\nthe package also offers a Python interface for programmatic accessors and samplers\nof the exported data. Exports can be opened both in an interactive test via\n`llama shell` or using following constructor in a program or e.g. Jupyter notebook.\n\n      from llama import LlamaApi, LlamaStats\n      llama = LlamaApi('export')\n\nAPI documentation:\n\n> This README documents the `LlamaApi` that in addition to selecting data,\n> offers quick output from statistical methods in `LlamaStats`. When the return\n> values are needed for further processing, the `LlamaStats` must be used directly.\n\n### `llama = LlamaApi(*directories)`\n\nConstructs a standard interface to work with one or multiple Llama export directories.\nIf no directory parameters are given the constructor seeks `./export` directory.\nCalculated distributions are cached in memory for multiple queries.\n* `*directories: str` (optional 0-N paramaters) Llama export directory paths\n* **Returns** an instance of `LlamaApi`\n\n### `llama.list(select)`\n\nLists sources and tables from the data. Subset of data can be selected with\nthe optional select dictionary.\n* `select: dict OR dict[]` (optional) comprised of the following keys\n  * `source: int` (optional) index of a learning data source\n  * `table: str` (optional) text to match table name (or id)\n  * `table_by_id: bool` (optional) True to match `table` with table id\n  * `persons: str[]` (optional) list of person identifiers\n  * `reverse: bool` (optional) True to exclude above matches and include rest\n\n### `llama.get(select)`\n\nReads and iterates over data form tables. This method can be combined with\nmany methods from `LlamaStats`.\n* `select: dict` (optional) see `llama.list`\n* **Returns** `iterator` over `tuples` of\n  `(source: dict, table: dict, rows: pandas.DataFrame)`\n\n### `llama.progsnap2(select, export_dir)`\n\nCreates a [ProgSnap 2](https://cssplice.github.io/progsnap2) compatible\nexport that merges the selected tables to one main event table.\n* `select: dict` see `llama.list`\n* `export_dir: str` a directory where the new export is created\n\n### `llama.overall_description(select)`\n\nCalculates statistical grade and attempt distributions,\nas well as weekly and daily patterns.\n* `select: dict` (optional) see `llama.list`\n\n### `llama.overall_pdf(select, pdf_name)`\n\nRenders a page about overall statistics.\n* `select: dict` (optional) see `llama.list`\n* `pdf_name: str` (optional) a file name for pdf output, else try to plot to window\n![example](img/overall.png)\n\n### `llama.learner_description(select)`\n\nCalculates statistical grade and attempt distributions,\nas well as weekly and daily patterns for the learners.\n* `select: dict` (optional) see `llama.list`\n\n### `llama.learner_pdf(select, pdf_name)`\n\nRenders a statistic page for each learner.\n* `select: dict` (optional) see `llama.list`\n* `pdf_name: str` (optional) a file name for pdf output, else try to plot to window\n![example](img/learner.png)\n\n### `llama.learner_variables(select, csv_name)`\n\nCompresses learner distributions into 21 variables per learner.\n* `select: dict` (optional) see `llama.list`\n* `csv_name: str` (optional) a file name for csv output, else print\n\n### `llama.execise_description(select)`\n\nCalculates statistical distributions for each selected exercise table.\n* `select: dict` (optional) see `llama.list`\n\n### `llama.exercise_pdf(select, pdf_name)`\n\nRenders a statistic page for each exercise.\n* `select: dict` (optional) see `llama.list`\n* `pdf_name: str` (optional) a file name for pdf output, else try to plot to window\n![example](img/exercise.png)\n\n### `llama.exercise_variables(select, csv_name)`\n\nCompresses exercise distributions into 23 variables per exercise.\n* `select: dict` (optional) see `llama.list`\n* `csv_name: str` (optional) a file name for csv output, else print\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/teemulehtinen/llama-cli",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "llama-cli",
    "package_url": "https://pypi.org/project/llama-cli/",
    "platform": null,
    "project_url": "https://pypi.org/project/llama-cli/",
    "project_urls": {
      "Homepage": "https://github.com/teemulehtinen/llama-cli"
    },
    "release_url": "https://pypi.org/project/llama-cli/1.0.16/",
    "requires_dist": [
      "pandas",
      "scipy",
      "matplotlib",
      "requests"
    ],
    "requires_python": ">=3.6",
    "summary": "Llama CLI fetches and preprocesses learning data",
    "version": "1.0.16",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17061857,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eea63a350b9e363d5e5a50456bd8427c07260fbbbc4c575f99cd680e7c9e64b5",
        "md5": "c4e27d05bcf2e004446f802a2c640f3a",
        "sha256": "933e2feb9b1b568731eb398e9117828664fa6388502198d63078c9731493621b"
      },
      "downloads": -1,
      "filename": "llama_cli-1.0.16-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c4e27d05bcf2e004446f802a2c640f3a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 36790,
      "upload_time": "2023-02-27T09:28:38",
      "upload_time_iso_8601": "2023-02-27T09:28:38.957009Z",
      "url": "https://files.pythonhosted.org/packages/ee/a6/3a350b9e363d5e5a50456bd8427c07260fbbbc4c575f99cd680e7c9e64b5/llama_cli-1.0.16-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d48a16f8456b5a8d6e1175d6d7095ddcb3bcb4c0c170cd65466799d90e389e21",
        "md5": "ee694426b1dad0c479e7d0d9c5f3a533",
        "sha256": "03b309878e1577ef8c458df2d541cdcf307d7389bf97cb301c9736c42667bfdb"
      },
      "downloads": -1,
      "filename": "llama-cli-1.0.16.tar.gz",
      "has_sig": false,
      "md5_digest": "ee694426b1dad0c479e7d0d9c5f3a533",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 31320,
      "upload_time": "2023-02-27T09:28:40",
      "upload_time_iso_8601": "2023-02-27T09:28:40.787175Z",
      "url": "https://files.pythonhosted.org/packages/d4/8a/16f8456b5a8d6e1175d6d7095ddcb3bcb4c0c170cd65466799d90e389e21/llama-cli-1.0.16.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}