{
  "info": {
    "author": "drhosseinjavedani",
    "author_email": "h.javedani@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: Other/Proprietary License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# lohrasb\n\nlohrasb is a package built to ease machine learning development. It uses [Optuna](https://optuna.readthedocs.io/en/stable/index.html), [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), and [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) to tune most of the tree-based estimators of sickit-learn. It is compatible with [scikit-learn](https://scikit-learn.org) pipeline.\n\n\n## Introduction\n\nBaseModel of the Lohrasb package can receive various parameters. From an estimator class to its tunning parameters and GridsearchCV, RandomizedSearchCV, or Optuna to their parameters. Samples will be split to train and validation set, and then optimization will estimate optimal related parameters using these optimizing engines.\n\n## Installation\n\nlohrasb package is available on PyPI and can be installed with pip:\n\n```sh\npip install lohrasb\n```\n\n\n## Supported estimators for this package\nAlmost all machine learning estimators for classification and regression supported by Lohrasb.\n\n## Usage\n\n- Tunning best parameters of a machine learning model using [Optuna](https://optuna.readthedocs.io/en/stable/index.html) , [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) or [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html).\n\n## Factories\nFor ease of use of BestModel, some factories are available to build associated instances corresponding to each optimization engine. For example, the following factory can be used for  GridSearchCV:\n\n```\n\nobj = BaseModel().optimize_by_gridsearchcv(\n            estimator=XGBClassifier(),\n            estimator_params={\n                                \"booster\": [\"gbtree\",\"dart\"],\n                                \"eval_metric\": [\"auc\"],\n                                \"max_depth\": [4, 5],\n                                \"gamma\": [0.1, 1.2],\n                                \"subsample\": [0.8],\n                            },\n            fit_params = None,\n            measure_of_accuracy=make_scorer(f1_score, greater_is_better=True),\n            verbose=3,\n            n_jobs=-1,\n            random_state=42,\n            cv=KFold(2),\n        )\n```\n\n## Example 1: Computer Hardware (Part 1: Use BestModel in sklearn pipeline)\n\n#### Import some required libraries\n```\nfrom lohrasb.best_estimator import BaseModel\nimport xgboost\nfrom optuna.pruners import HyperbandPruner\nfrom optuna.samplers._tpe.sampler import TPESampler\nfrom sklearn.model_selection import KFold,train_test_split\nimport pandas as pd\nimport numpy as np\nimport optuna\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine.imputation import (\n    CategoricalImputer,\n    MeanMedianImputer\n    )\nfrom category_encoders import OrdinalEncoder\nfrom sklearn.metrics import (\n    make_scorer)\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import *\nfrom sklearn.svm import *\nfrom xgboost import *\nfrom sklearn.linear_model import *\nfrom lightgbm import *\nfrom sklearn.neural_network import *\nfrom imblearn.ensemble import *\nfrom sklearn.ensemble import *\n\n\n```\n#### Computer Hardware Data Set (a regression problem)\n  \nhttps://archive.ics.uci.edu/ml/datasets/Computer+Hardware\n\n```\nurldata= \"https://archive.ics.uci.edu/ml/machine-learning-databases/cpu-performance/machine.data\"\n# column names\ncol_names=[\n    \"vendor name\",\n    \"Model Name\",\n    \"MYCT\",\n    \"MMIN\",\n    \"MMAX\",\n    \"CACH\",\n    \"CHMIN\",\n    \"CHMAX\",\n    \"PRP\"\n]\n# read data\ndata = pd.read_csv(urldata,header=None,names=col_names,sep=',')\ndata\n```\n#### Train test split\n\n```\nX = data.loc[:, data.columns != \"PRP\"]\ny = data.loc[:, data.columns == \"PRP\"]\ny = y.values.ravel()\n\n\nX_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.33, random_state=42)\n\n```\n#### Find feature types for later use\n\n```\nint_cols =  X_train.select_dtypes(include=['int']).columns.tolist()\nfloat_cols =  X_train.select_dtypes(include=['float']).columns.tolist()\ncat_cols =  X_train.select_dtypes(include=['object']).columns.tolist()\n```\n\n####  Define estimator and set its arguments  \n```\nestimator = LinearRegression()\nestimator_params= {\n        \"fit_intercept\": [True, False],\n    }\n```\n#### Use factory\n\n```\nobj = BaseModel().optimize_by_optuna(\n            estimator=estimator,\n            estimator_params=estimator_params,\n            measure_of_accuracy=\"mean_absolute_error(y_true, y_pred, multioutput='uniform_average')\",\n            with_stratified=False,\n            test_size=.3,\n            verbose=3,\n            n_jobs=-1,\n            random_state=42,\n            # optuna params\n            # optuna study init params\n            study=optuna.create_study(\n                storage=None,\n                sampler=TPESampler(),\n                pruner=HyperbandPruner(),\n                study_name=None,\n                direction=\"minimize\",\n                load_if_exists=False,\n                directions=None,\n            ),\n            # optuna optimization params\n            study_optimize_objective=None,\n            study_optimize_objective_n_trials=10,\n            study_optimize_objective_timeout=600,\n            study_optimize_n_jobs=-1,\n            study_optimize_catch=(),\n            study_optimize_callbacks=None,\n            study_optimize_gc_after_trial=False,\n            study_optimize_show_progress_bar=False,\n        )\n```\n####  Build sklearn pipeline\n```\npipeline =Pipeline([\n            # int missing values imputers\n            ('intimputer', MeanMedianImputer(\n                imputation_method='median', variables=int_cols)),\n            # category missing values imputers\n            ('catimputer', CategoricalImputer(variables=cat_cols)),\n            #\n            ('catencoder', OrdinalEncoder()),\n            # regression model \n            ('obj', obj),\n\n\n ])\n\n\n\n```\n#### Run Pipeline\n\n```\npipeline.fit(X_train,y_train)\ny_pred = pipeline.predict(X_test)\n```\n#### Check performance of the pipeline\n\n```\nprint('r2 score : ')\nprint(r2_score(y_test,y_pred))\n```\n\n## Part 2:  Use BestModel as a standalone estimator \n```\nX_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.33, random_state=42)\n```\n\n#### Transform features to make them ready for model input\n```\ntransform_pipeline =Pipeline([\n            # int missing values imputers\n            ('intimputer', MeanMedianImputer(\n                imputation_method='median', variables=int_cols)),\n            # category missing values imputers\n            ('catimputer', CategoricalImputer(variables=cat_cols)),\n            #\n            ('catencoder', OrdinalEncoder()),\n            # classification model\n\n ])\n```\n#### Transform X_train and X_test\n\n```\nX_train=transform_pipeline.fit_transform(X_train,y_train)\nX_test=transform_pipeline.transform(X_test)\n```\n\n#### Train model and predict\n```\nobj.fit(X_train,y_train)\ny_pred = obj.predict(X_test)\n```\n\n#### Check performance of the model\n\n```\nprint('r2 score : ')\nprint(r2_score(y_test,y_pred))\nprint('mean_absolute_error : ')\nprint(mean_absolute_error(y_test,y_pred))\n\n\nprint(obj.get_best_estimator())\n\nprint(obj.best_estimator)\n\nOptunaObj = obj.get_optimized_object()\nprint(OptunaObj.trials)\n```\n\n## Example 2: XGBoost Survival Embeddings (XGBSEKaplanNeighbors)\nFor more information refer to this link : https://loft-br.github.io/xgboost-survival-embeddings/examples/confidence_interval.html\n\n\n#### Import some required libraries\n```\n! pip3 install torch==1.12.1\nimport sys\nsys.path.append('/usr/local/lib/python3.10/site-packages')\nimport torch\nimport numpy as np\nfrom sklearn.model_selection import KFold, train_test_split\nfrom lohrasb.best_estimator import BaseModel\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import make_scorer\nfrom xgbse.converters import convert_to_structured\nfrom xgbse.metrics import (\n    concordance_index,\n    approx_brier_score\n)\nfrom xgbse import (\n    XGBSEKaplanNeighbors,\n    XGBSEKaplanTree,\n    XGBSEBootstrapEstimator\n)\nfrom pycox.datasets import metabric\n\n```\n\n\n#### Read data metabric\n\n```\ndf = metabric.read_df()\ndf.head()\n```\n\n#### Define labels and train-test split \n\n```\n# splitting to X, T, E format\nX = df.drop(['duration', 'event'], axis=1)\ny = convert_to_structured(df['duration'], df['event'])\n\n\n# splitting between train, and validation \n(X_train, X_test,\n y_train, y_test) = \\\ntrain_test_split(X, y, test_size=0.2, random_state=42)\n```\n\n\n#### Define estimator and set its arguments\n```\nestimator_params = {\n    'n_estimators' :[100,200]\n\n}\n\nPARAMS_TREE = {\n    'objective': 'survival:cox',\n    'eval_metric': 'cox-nloglik',\n    'tree_method': 'hist', \n    'max_depth': 100, \n    'booster':'dart', \n    'subsample': 1.0,\n    'min_child_weight': 50, \n    'colsample_bynode': 1.0\n}\nbase_model = XGBSEKaplanTree(PARAMS_TREE)\n\nTIME_BINS = np.arange(15, 315, 15)\n```\n\n#### Define estimator and fit params\n\n```\nestimator=XGBSEBootstrapEstimator(base_model)\nfit_params = {\"time_bins\":TIME_BINS}\n```\n#### Define BaseModel estimator using random search CV\n\n```\nobj = BaseModel().optimize_by_randomsearchcv(\n            estimator=estimator,\n            fit_params = fit_params,\n            estimator_params=estimator_params,\n            measure_of_accuracy=make_scorer(approx_brier_score, greater_is_better=False),\n            verbose=3,\n            n_jobs=-1,\n            n_iter=2,\n            random_state=42,\n            cv=KFold(2),\n        )\n```\n\n#### Build sklearn pipeline\n\n```\npipeline =Pipeline([\n            ('obj', obj)\n\n ])\n```\n#### Run Pipeline\n```\npipeline.fit(X_train,y_train)\ny_pred = pipeline.predict(X_test)\n```\n\n#### Check performance of the pipeline\n\n```\nprint(f'C-index: {concordance_index(y_test, y_pred)}')\nprint(f'Avg. Brier Score: {approx_brier_score(y_test, y_pred)}')\n```\n\nThere are some examples  available in the [examples](https://github.com/drhosseinjavedani/lohrasb/tree/main/lohrasb/examples). \n\n## License\nLicensed under the [BSD 2-Clause](https://opensource.org/licenses/BSD-2-Clause) License.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/drhosseinjavedani/lohrasb",
    "keywords": "Auto ML,Pipeline,Machine learning",
    "license": "BSD-3-Clause license",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lohrasb",
    "package_url": "https://pypi.org/project/lohrasb/",
    "platform": null,
    "project_url": "https://pypi.org/project/lohrasb/",
    "project_urls": {
      "Homepage": "https://github.com/drhosseinjavedani/lohrasb"
    },
    "release_url": "https://pypi.org/project/lohrasb/2.1.0/",
    "requires_dist": [
      "numpy (>=1.20,<2.0)",
      "scipy (>=1.5,<2.0)",
      "optuna (>=2.10.1,<3.0.0)",
      "xgboost (>=1.6.1,<2.0.0)",
      "imblearn (>=0.0,<0.1)",
      "lightgbm (>=3.3.2,<4.0.0)",
      "nox (>=2022.1.7,<2023.0.0)",
      "feature-engine (>=1.4.1,<2.0.0)",
      "category-encoders (>=2.5.0,<3.0.0)",
      "pandas (>=1.4,<2.0)",
      "xgbse (>=0.2.3,<0.3.0)",
      "pycox (>=0.2.3,<0.3.0)",
      "pyarrow (>=10.0.1,<11.0.0)",
      "scikit-learn (>=1.1.3,<2.0.0)"
    ],
    "requires_python": ">=3.8,<3.11",
    "summary": "Using optuna search optimizer to estimate best tree based estimator compatible with scikit-learn",
    "version": "2.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16972843,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b91ae9b328d28fb8f1908659b71678528c6310b864ecd590521332fb878b15ad",
        "md5": "508a75dc6cf4584a02586e2522a5f2c4",
        "sha256": "0dac461ccb8d9dbb38c0d991b654710c3d5b59bb61c60e967bbc29a03f8ce683"
      },
      "downloads": -1,
      "filename": "lohrasb-2.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "508a75dc6cf4584a02586e2522a5f2c4",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8,<3.11",
      "size": 56103,
      "upload_time": "2022-11-30T12:55:06",
      "upload_time_iso_8601": "2022-11-30T12:55:06.753786Z",
      "url": "https://files.pythonhosted.org/packages/b9/1a/e9b328d28fb8f1908659b71678528c6310b864ecd590521332fb878b15ad/lohrasb-2.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e70260526b33ccf9146a5bececf891049d7d412caf0036fb58f602b54c6436eb",
        "md5": "ff4969772e5011cadbb9a8c2486f0ea2",
        "sha256": "29ae12b0e5f0905d5c7afc43ddf114b8c619122321b099018c50cce6004c6911"
      },
      "downloads": -1,
      "filename": "lohrasb-2.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ff4969772e5011cadbb9a8c2486f0ea2",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8,<3.11",
      "size": 50867,
      "upload_time": "2022-11-30T12:55:08",
      "upload_time_iso_8601": "2022-11-30T12:55:08.865559Z",
      "url": "https://files.pythonhosted.org/packages/e7/02/60526b33ccf9146a5bececf891049d7d412caf0036fb58f602b54c6436eb/lohrasb-2.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}