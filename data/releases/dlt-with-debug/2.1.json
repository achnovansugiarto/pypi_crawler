{
  "info": {
    "author": "Souvik Pratiher",
    "author_email": "souvik.pratiher@databricks.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Programming Language :: Python :: 3"
    ],
    "description": "\n<div id=\"top\"></div>\n<br />\n<div align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/souvik-databricks/random_snaps/main/dlt_logo.png\" alt=\"Delta Live Table\" width=\"450\" height=\"250\">\n  <h3 align=\"center\">DLT with Debug</h3>\n\n  <p align=\"center\">\n    Running DLT workflows from interactive notebooks.\n    <br />\n    <br />\n  </p>\n</div>\n\n## About The Project\n\nDelta Live Tables (DLTs) are a great way to design data pipelines with only focusing on the core business logic.\nIt makes the life of data engineers easy but while the development workflows are streamlined in DLT, when it comes to \n__*debugging and seeing how the data looks after each transformation step*__ in a typical DLT pipeline it becomes very \npainful and cumbersome as we dont have the DLT package available in our interactive environment.\n\nEnter **dlt-with-debug** a lightweight decorator utility which allows developers to do interactive\npipeline development by having a unified source code for both DLT run and Non-DLT interactive notebook run. \n\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Built With\n\n- Python's builtins\n  - [globals()](https://docs.python.org/3/library/functions.html#globals)\n  - [exec()](https://docs.python.org/3/library/functions.html#exec)\n  - [decorator()](https://docs.python.org/3/glossary.html#term-decorator)\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Installation\n\npip install in your Databricks Notebook\n\n_**PyPI**_\n```python\n%pip install dlt_with_debug\n```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n\n### Prerequisites\n\n- [Databricks](https://databricks.com/) \n- [Delta Live Tables](https://databricks.com/product/delta-live-tables)\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n### Usage\n- *In our notebooks containing DLT Jobs the imports changes slightly as below*\n    ```\n    from dlt_with_debug import dltwithdebug, pipeline_id, showoutput\n    \n    if pipeline_id:\n      import dlt\n    else:\n      from dlt_with_debug import dlt\n    ```\n> **Note**: \n> 1. Use the `dlt.create_table()` API instead of `dlt.table()` as `dlt.table()` sometimes gets mixed with `spark.table()` \nin the global namespace.\n> 2. Always pass the `globals()` namespace to `dltwithdebug` decorator like this `@dltwithdebug(globals())`\n\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n---\n\n### Sample `DLT with debug` DLT pipeline example\n\n> **Code**:\n\nCmd 1\n```python\n%pip install -e git+https://github.com/souvik-databricks/dlt-with-debug.git#\"egg=dlt_with_debug\"\n```\nCmd 2\n```python\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# We are importing \n# dltwithdebug as that's the entry point to interactive DLT workflows\n# pipeline_id to ensure we import the dlt package based on environment\n# showoutput is a helper function for seeing the output result along with expectation metrics if any is specified\nfrom dlt_with_debug import dltwithdebug, pipeline_id, showoutput\n\nif pipeline_id:\n  import dlt\nelse:\n  from dlt_with_debug import dlt\n```\nCmd 3\n```python\njson_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n```\nCmd 4\n```python\n# Notice we are using dlt.create_table instead of dlt.table\n\n@dlt.create_table(\n  comment=\"The raw wikipedia click stream dataset, ingested from /databricks-datasets.\",\n  table_properties={\n    \"quality\": \"bronze\"\n  }\n)\n@dltwithdebug(globals())\ndef clickstream_raw():\n  return (\n    spark.read.option(\"inferSchema\", \"true\").json(json_path)\n  )\n```\nCmd 5\n```python\n# for displaying the result of the transformation \n# use showoutput(func_name)\n# for example here we are using showoutput(clickstream_raw) \nshowoutput(clickstream_raw)\n```\n![Alt Text](https://raw.githubusercontent.com/souvik-databricks/random_snaps/main/clck_raw.png)\n\nCmd 6\n```python\n@dlt.create_table(\n  comment=\"Wikipedia clickstream dataset with cleaned-up datatypes / column names and quality expectations.\",\n  table_properties={\n    \"quality\": \"silver\"\n  }\n)\n@dlt.expect(\"valid_current_page\", \"current_page_id IS NOT NULL AND current_page_title IS NOT NULL\")\n@dlt.expect_or_fail(\"valid_count\", \"click_count > 0\")\n@dlt.expect_all({'valid_prev_page_id': \"previous_page_id IS NOT NULL\"})\n@dltwithdebug(globals())\ndef clickstream_clean():\n  return (\n    dlt.read(\"clickstream_raw\")\n      .withColumn(\"current_page_id\", expr(\"CAST(curr_id AS INT)\"))\n      .withColumn(\"click_count\", expr(\"CAST(n AS INT)\"))\n      .withColumn(\"previous_page_id\", expr(\"CAST(prev_id AS INT)\"))\n      .withColumnRenamed(\"curr_title\", \"current_page_title\")\n      .withColumnRenamed(\"prev_title\", \"previous_page_title\")\n      .select(\"current_page_id\", \"current_page_title\", \"click_count\", \"previous_page_id\", \"previous_page_title\")\n  )\n```\nCmd 7\n```python\nshowoutput(clickstream_clean)\n```\n![Alt Text](https://raw.githubusercontent.com/souvik-databricks/random_snaps/main/clck_clean.png)\n\n\n---\n> _Important to note that here you can see we are also **seeing how many records will the expectations affect**._\n---\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Same sample `DLT with debug` DLT pipeline executed as part of a delta live table\n![Alt Text](https://i.ibb.co/VQzZsZR/Screenshot-2022-10-18-at-5-34-14-AM.png)\n\n> Below we can the expectation results also match up with the expectation metrics that we got from dltwithdebug earlier \n> with `showoutput(clickstream_clean)`\n> ![Expectation Results](https://raw.githubusercontent.com/souvik-databricks/random_snaps/main/expectations.png)\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Quick API guide\n\n#### Table syntax\n\n```python\n@dlt.create_table(   # <-- Notice we are using the dlt.create_table() instead of dlt.table()\n  name=\"<name>\",\n  comment=\"<comment>\",\n  spark_conf={\"<key>\" : \"<value\", \"<key\" : \"<value>\"},\n  table_properties={\"<key>\" : \"<value>\", \"<key>\" : \"<value>\"},\n  path=\"<storage-location-path>\",\n  partition_cols=[\"<partition-column>\", \"<partition-column>\"],\n  schema=\"schema-definition\",\n  temporary=False)\n@dlt.expect\n@dlt.expect_or_fail\n@dlt.expect_or_drop\n@dlt.expect_all\n@dlt.expect_all_or_drop\n@dlt.expect_all_or_fail\n@dltwithdebug(globals())    # <-- This dltwithdebug(globals()) needs to be added\ndef <function-name>():\n    return (<query>)\n```\n\n#### View syntax\n\n```python\n@dlt.create_view(    # <-- Notice we are using the dlt.create_view() instead of dlt.view()\n  name=\"<name>\",\n  comment=\"<comment>\")\n@dlt.expect\n@dlt.expect_or_fail\n@dlt.expect_or_drop\n@dlt.expect_all\n@dlt.expect_all_or_drop\n@dlt.expect_all_or_fail\n@dltwithdebug(globals())    # <-- This dltwithdebug(globals()) needs to be added\ndef <function-name>():\n    return (<query>)\n```\n\n#### Getting results syntax\n\n```python\nshowoutput(function_name)  # <-- showoutput(function_name) \n                           # Notice we are only passing the function name\n                           # The name of the function which is wrapped by the dltdecorators\n                           \n                           # For example:\n                           # @dlt.create_table()\n                           # @dltwithdebug(globals())\n                           # def step_one():\n                           #    return spark.read.csv()\n\n                           # showoutput(step_one)\n```\n\n#### Import syntax\n\n```python\n# We are importing \n# dltwithdebug as that's the entry point to interactive DLT workflows\n# pipeline_id to ensure we import the dlt package based on environment\n# showoutput is a helper function for seeing the output result along with expectation metrics if any is specified\nfrom dlt_with_debug import dltwithdebug, pipeline_id, showoutput\n\nif pipeline_id:\n  import dlt\nelse:\n  from dlt_with_debug import dlt\n```\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n\n## Upcoming functionality\n\nAs of now the following DLT APIs are covered for interactive use:\n\n1. **Currently Available:**\n\n   - `dlt.read`\n   - `dlt.read_stream`\n   - `dlt.create_table`\n   - `dlt.create_view`\n   - `dlt.table` <-- This one sometimes gets overridden with `spark.table`\n   - `dlt.view` \n   - `dlt.expect`\n   - `dlt.expect_or_fail`\n   - `dlt.expect_or_drop`\n   - `dlt.expect_all`\n   - `dlt.expect_all_or_drop`\n   - `dlt.expect_all_or_fail`\n\n2. **Will be covered in the upcoming release:**\n   - `dlt.create_target_table`\n   - `dlt.create_streaming_live_table`\n   - `dlt.apply_changes`\n\n## Limitation\n\n`DLT with Debug` is a fully python based utility and as such it doesn't supports `spark.table(\"LIVE.func_name\")` syntax. \n\nSo instead of `spark.table(\"LIVE.func_name\")` use `dlt.read(\"func_name\")` or `dlt.read_stream(\"func_name\")`\n\n## License\n\nDistributed under the MIT License.\n\n<p align=\"right\">(<a href=\"#top\">back to top</a>)</p>\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/souvik-databricks/dlt-with-debug",
    "keywords": "python3,delta live tables",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dlt-with-debug",
    "package_url": "https://pypi.org/project/dlt-with-debug/",
    "platform": null,
    "project_url": "https://pypi.org/project/dlt-with-debug/",
    "project_urls": {
      "Homepage": "https://github.com/souvik-databricks/dlt-with-debug"
    },
    "release_url": "https://pypi.org/project/dlt-with-debug/2.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Utility for running workflows leveraging delta live tables from interactive notebooks",
    "version": "2.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15497118,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "183000d927bfdefa414021810de0f3b4ef6bfcfcfe2ad166621fe9aaf28042f3",
        "md5": "ae4dfec2ce05aa008f6a4d406725079d",
        "sha256": "fdcb255913bde664c4d941a1d169ab4899db18904115127d256ea6bb34cd929a"
      },
      "downloads": -1,
      "filename": "dlt_with_debug-2.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ae4dfec2ce05aa008f6a4d406725079d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 7073,
      "upload_time": "2022-10-22T18:36:54",
      "upload_time_iso_8601": "2022-10-22T18:36:54.566514Z",
      "url": "https://files.pythonhosted.org/packages/18/30/00d927bfdefa414021810de0f3b4ef6bfcfcfe2ad166621fe9aaf28042f3/dlt_with_debug-2.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b0368a3270dd7ec2a705f14c7eddceaaf638d51b02efd0c898262fe50182f7bf",
        "md5": "c556f9d9a94eb5da10b5a1f93592958b",
        "sha256": "c2e07c6eba90c28cc6314ad5cb3aafaf663fe47dac4e6da3117572a8a25aa951"
      },
      "downloads": -1,
      "filename": "dlt_with_debug-2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "c556f9d9a94eb5da10b5a1f93592958b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 6578,
      "upload_time": "2022-10-22T18:36:56",
      "upload_time_iso_8601": "2022-10-22T18:36:56.696048Z",
      "url": "https://files.pythonhosted.org/packages/b0/36/8a3270dd7ec2a705f14c7eddceaaf638d51b02efd0c898262fe50182f7bf/dlt_with_debug-2.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}