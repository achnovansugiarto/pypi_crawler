{
  "info": {
    "author": "Patrick Ryan",
    "author_email": "pjryan126@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "[![Travis](https://travis-ci.com/pjryan126/docscraper.svg?branch=main)](https://travis-ci.com/pjryan126/docscraper.svg?branch=main)\n[![Total alerts](https://img.shields.io/lgtm/alerts/g/pjryan126/docscraper.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/pjryan126/docscraper/alerts/) \n[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/pjryan126/docscraper.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/pjryan126/docscraper/context:python)\n\n# Overview\n\nThe ``docscraper`` package is a ``scrapy`` spider for crawling a given set of\nwebsites and dowloading all available documents with a given set of file\nextensions. The package is intended to be called from a Python script.\n\n# Getting Started\n\nYou can get started by downloading the package with ``pip``:\n\n```\n$ pip install docscraper\n```\n\nOnce the package is installed, you can use it with scrapy directly in your \nPython script to download files from websites as follows:\n\n```\n>>> import docscraper\n>>> allowed_domains = [\"books.toscrape.com\"]\n>>> start_urls = [\"https://books.toscrape.com\"]\n>>> extensions = [\".html\", \".pdf\", \".docx\", \".doc\", \".svg\"]\n>>> docscraper.crawl(allowed_domains, start_urls, extensions=extensions)\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://docscraper.readthedocs.io/en/latest/",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "docscraper",
    "package_url": "https://pypi.org/project/docscraper/",
    "platform": "",
    "project_url": "https://pypi.org/project/docscraper/",
    "project_urls": {
      "Homepage": "https://docscraper.readthedocs.io/en/latest/"
    },
    "release_url": "https://pypi.org/project/docscraper/1.0.1/",
    "requires_dist": [
      "openpyxl",
      "numpy (>=1.16.5)",
      "pandas",
      "scrapy"
    ],
    "requires_python": "",
    "summary": "A web crawler to scrape documents from websites",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9848792,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "385a50ff930c2890c328f120d722848c130a21884494ac8e10118cb4464a8f5c",
        "md5": "354066db6f1465a2fb80ab1e23d173ff",
        "sha256": "529a11df0c492e17cef8fe82228bf6b9d9ab5684e6709d296aeb0c8598ecb47f"
      },
      "downloads": -1,
      "filename": "docscraper-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "354066db6f1465a2fb80ab1e23d173ff",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 3899,
      "upload_time": "2021-03-07T12:51:46",
      "upload_time_iso_8601": "2021-03-07T12:51:46.585891Z",
      "url": "https://files.pythonhosted.org/packages/38/5a/50ff930c2890c328f120d722848c130a21884494ac8e10118cb4464a8f5c/docscraper-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3d79064e1d4190f9508ff9be981076930256ccf2e846d4be81a940f7352099c7",
        "md5": "61100e8b089d4001c4615dcc9542e531",
        "sha256": "7d79461a886db98454442e5d434be194719e293de14c6e81b23ae81354aa83dc"
      },
      "downloads": -1,
      "filename": "docscraper-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "61100e8b089d4001c4615dcc9542e531",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 3530,
      "upload_time": "2021-03-07T12:51:47",
      "upload_time_iso_8601": "2021-03-07T12:51:47.731958Z",
      "url": "https://files.pythonhosted.org/packages/3d/79/064e1d4190f9508ff9be981076930256ccf2e846d4be81a940f7352099c7/docscraper-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}