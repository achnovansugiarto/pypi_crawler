{
  "info": {
    "author": "Patrick Ryan",
    "author_email": "pryan@stoneturn.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "[![Travis](https://travis-ci.com/pjryan126/docscraper.svg?branch=main)](https://travis-ci.com/pjryan126/docscraper.svg?branch=main)\n[![Total alerts](https://img.shields.io/lgtm/alerts/g/pjryan126/docscraper.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/pjryan126/docscraper/alerts/) \n[![Language grade: Python](https://img.shields.io/lgtm/grade/python/g/pjryan126/docscraper.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/pjryan126/docscraper/context:python)\n\n# Overview\n\nThe ``docscraper`` package is a ``scrapy`` spider for crawling a given set of\nwebsites and dowloading all available documents with a given set of file\nextensions. The package is intended to be called from a Python script.\n\n# Getting Started\n\nYou can get started by downloading the package with ``pip``:\n\n```\n$ pip install docscraper\n```\n\nOnce the package is installed, you can use it with scrapy directly in your \nPython script to download files from websites as follows:\n\n```\n>>> import docscraper\n>>> allowed_domains = [\"books.toscrape.com\"]\n>>> start_urls = [\"https://books.toscrape.com\"]\n>>> extensions = [\".html\", \".pdf\", \".docx\", \".doc\", \".svg\"]\n>>> docscraper.crawl(allowed_domains, start_urls, extensions=extensions)\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://docscraper.readthedocs.io/en/latest/",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "docscraper",
    "package_url": "https://pypi.org/project/docscraper/",
    "platform": "",
    "project_url": "https://pypi.org/project/docscraper/",
    "project_urls": {
      "Homepage": "https://docscraper.readthedocs.io/en/latest/"
    },
    "release_url": "https://pypi.org/project/docscraper/1.0.0/",
    "requires_dist": [
      "openpyxl",
      "numpy (>=1.16.5)",
      "pandas",
      "scrapy"
    ],
    "requires_python": "",
    "summary": "a web crawler to scrape documents from websites",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9848792,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a32f81df41a601976c5ebb216d5aac80155d9785da4789422ecd31854ff276a2",
        "md5": "e5fca6d645fcb35db9f521126e4e6383",
        "sha256": "45003e09328a929585a1fb17c78f13ee1e4d0bc1a2887723445a5ecef6115441"
      },
      "downloads": -1,
      "filename": "docscraper-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e5fca6d645fcb35db9f521126e4e6383",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 3904,
      "upload_time": "2021-03-07T12:40:56",
      "upload_time_iso_8601": "2021-03-07T12:40:56.450779Z",
      "url": "https://files.pythonhosted.org/packages/a3/2f/81df41a601976c5ebb216d5aac80155d9785da4789422ecd31854ff276a2/docscraper-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cc2e6926744d9506db9d5fd98d349f15ff2d97e1749258b14591317f29449b14",
        "md5": "2b3a22090751e078dedc51ed7b493dd4",
        "sha256": "0f6240a447466abad8d6db81e9c2b321d4e2a0156e0b4703a58344b71146509e"
      },
      "downloads": -1,
      "filename": "docscraper-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "2b3a22090751e078dedc51ed7b493dd4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 3525,
      "upload_time": "2021-03-07T12:40:57",
      "upload_time_iso_8601": "2021-03-07T12:40:57.625516Z",
      "url": "https://files.pythonhosted.org/packages/cc/2e/6926744d9506db9d5fd98d349f15ff2d97e1749258b14591317f29449b14/docscraper-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}