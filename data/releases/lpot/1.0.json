{
  "info": {
    "author": "Intel MLP/MLPC Team",
    "author_email": "feng.tian@intel.com, chuanqi.wang@intel.com, pengxin.yuan@intel.com, guoming.zhang@intel.com, haihao.shen@intel.com, jiong.gong@intel.com, xi2.chen@intel.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "Intel® Low Precision Optimization Tool\n=========================================\n\nIntel® Low Precision Optimization Tool is an open-source python library which is intended to deliver a unified low-precision inference interface cross multiple Intel optimized DL frameworks on both CPU and GPU. It supports automatic accuracy-driven tuning strategies, along with additional objectives like optimizing for performance, model size and memory footprint. It also provides the easy extension capability for new backends, tuning strategies, metrics and objectives.\n\n> **WARNING**\n>\n> GPU support is under development.\n\n<table>\n  <tr>\n    <td>Infrastructure</td>\n    <td>Workflow</td>\n  </tr>\n  <tr>\n    <td><img src=\"docs/imgs/infrastructure.jpg\" width=640 height=320></td>\n    <td><img src=\"docs/imgs/workflow.jpg\" width=640 height=320></td>\n  </tr>\n </table>\n\nSupported Intel optimized DL frameworks are:\n* [Tensorflow\\*](https://github.com/Intel-tensorflow/tensorflow), including 1.15, 1.15UP1, 2.1, 2.2, 2.3\n* [PyTorch\\*](https://pytorch.org/), including 1.5.0+cpu, 1.6.0+cpu\n* [Apache\\* MXNet](https://mxnet.apache.org), including 1.6.0, 1.7.0\n\nSupported tuning strategies are:\n* [Basic](docs/tuning_strategies.md#basic)\n* [Bayesian](docs/tuning_strategies.md#bayesian)\n* [MSE](docs/tuning_strategies.md#mse)\n* [Exhaustive](docs/tuning_strategies.md#exhaustive)\n* [Random](docs/tuning_strategies.md#random)\n* [TPE - experimental](docs/tuning_strategies.md#TPE)\n\n# Documents\n\n* [Introduction](docs/introduction.md) explains the API of Intel® Low Precision Optimization Tool.\n* [Hello World](examples/helloworld/README.md) demonstrates the simple steps to utilize Intel® Low Precision Optimization Tool for quanitzation, which can help you quick start with the tool.\n* [Tutorial](docs/tutorial.md) provides comprehensive instructions of how to utilize diffrennt features of Intel® Low Precision Optimization Tool. In [examples](examples), there are a lot of examples to demonstrate the usage of Intel® Low Precision Optimization Tool in TensorFlow, PyTorch and MxNet for diffrent categories.\n* [Strategies](docs/tuning_strategies.md) provides comprehensive explanation on the details of how every tuning strategy works.\n* [PTQ and QAT](docs/ptq_qat.md) explains how Intel® Low Precision Optimization Tool works with post-training quantization and quantization-ware training.\n* [Pruning on PyTorch](docs/pruning.md) explains how Intel® Low Precision Optimization Tool works with magnitude pruning on PyTorch.\n* [Tensorboard](docs/tensorboard.md) explains how Intel® Low Precision Optimization Tool helps developer to analyze tensor distribution and the impact to final accuracy during tuning process.\n* [Quantized Model Deployment on PyTorch](docs/pytorch_model_saving.md) explains how Intel® Low Precision Optimization Tool quantizes a FP32 PyTorch model, save and deploy quantized model through lpot utils.\n* [BF16 Mix-Precision on TensorFlow](docs/bf16_convert.md) explains how Intel® Low Precision Optimization Tool supports INT8/BF16/FP32 mix precision model tuning on TensorFlow backend.\n* [Supported Model Types on TensorFlow](docs/tensorflow_model_support.md) explains the TensorFlow model types supported by Intel® Low Precision Optimization Tool.\n\n# Install from source \n\n  ```Shell\n  git clone https://github.com/intel/lpot.git\n  cd lpot\n  python setup.py install\n  ```\n\n# Install from binary\n\n  ```Shell\n  # install from pip\n  pip install lpot\n\n  # install from conda\n  conda install lpot -c intel -c conda-forge\n  ```\n\n# System Requirements\n\nIntel® Low Precision Optimization Tool supports systems based on [Intel 64 architecture or compatible processors](https://en.wikipedia.org/wiki/X86-64), specially optimized for the following CPUs:\n\n* Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, and Cooper Lake)\n* future Intel Xeon Scalable processor (code name Sapphire Rapids)\n\nIntel® Low Precision Optimization Tool requires to install Intel optimized framework version for TensorFlow, PyTorch, and MXNet.\n\n### Validated Hardware/Software Environment\n\n<table>\n<thead>\n  <tr>\n    <th class=\"tg-bobw\">Platform</th>\n    <th class=\"tg-bobw\">OS</th>\n    <th class=\"tg-bobw\">Python</th>\n    <th class=\"tg-bobw\">Framework</th>\n    <th class=\"tg-bobw\">Version</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-nrix\" rowspan=\"8\">Cascade Lake<br><br>Cooper Lake<br><br>Skylake</td>\n    <td class=\"tg-nrix\" rowspan=\"8\">CentOS 7.8<br><br>Ubuntu 18.04</td>\n    <td class=\"tg-nrix\" rowspan=\"8\">3.6<br><br>3.7</td>\n    <td class=\"tg-cly1\" rowspan=\"5\">tensorflow</td>\n    <td class=\"tg-7zrl\">2.2.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.15UP1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">2.3.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">2.1.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.15.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">pytorch</td>\n    <td class=\"tg-7zrl\">1.5.0+cpu</td>\n  </tr>\n  <tr>\n    <td class=\"tg-cly1\" rowspan=\"2\">mxnet</td>\n    <td class=\"tg-7zrl\">1.7.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.6.0</td>\n  </tr>\n</tbody>\n</table>\n\n# Model Zoo\n\nIntel® Low Precision Optimization Tool provides a lot of examples to show promising accuracy loss with best performance gain.\n\n<table>\n<thead>\n  <tr>\n    <th class=\"tg-9wq8\" rowspan=\"2\">Framework</th>\n    <th class=\"tg-9wq8\" rowspan=\"2\">version</th>\n    <th class=\"tg-9wq8\" rowspan=\"2\">model</th>\n    <th class=\"tg-9wq8\" rowspan=\"2\">dataset</th>\n    <th class=\"tg-pb0m\" colspan=\"3\">TOP-1   Accuracy</th>\n    <th class=\"tg-za14\">Performance Speedup</th>\n  </tr>\n  <tr>\n    <td class=\"tg-za14\">INT8 Tuning Accuracy</td>\n    <td class=\"tg-za14\">FP32 Accuracy   Baseline</td>\n    <td class=\"tg-za14\">Acc   Ratio[(INT8-FP32)/FP32]</td>\n    <td class=\"tg-za14\">Realtime Latency Ratio[FP32/INT8]</td>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\" rowspan=\"20\">2.2.0</td>\n    <td class=\"tg-9wq8\">resnet50v1.0</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">73.80%</td>\n    <td class=\"tg-za14\">74.30%</td>\n    <td class=\"tg-9wq8\">-0.67%</td>\n    <td class=\"tg-9wq8\">2.25x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">resnet50v1.5</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">76.80%</td>\n    <td class=\"tg-za14\">76.50%</td>\n    <td class=\"tg-9wq8\">0.39%</td>\n    <td class=\"tg-9wq8\">2.32x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">resnet101</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">77.20%</td>\n    <td class=\"tg-za14\">76.40%</td>\n    <td class=\"tg-9wq8\">1.05%</td>\n    <td class=\"tg-9wq8\">2.75x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">inception_v1</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">70.10%</td>\n    <td class=\"tg-za14\">69.70%</td>\n    <td class=\"tg-9wq8\">0.57%</td>\n    <td class=\"tg-9wq8\">1.56x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">inception_v2</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">74.00%</td>\n    <td class=\"tg-za14\">74.00%</td>\n    <td class=\"tg-9wq8\">0.00%</td>\n    <td class=\"tg-9wq8\">1.68x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">inception_v3</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">77.20%</td>\n    <td class=\"tg-za14\">76.70%</td>\n    <td class=\"tg-9wq8\">0.65%</td>\n    <td class=\"tg-9wq8\">2.05x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">inception_v4</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">80.00%</td>\n    <td class=\"tg-za14\">80.30%</td>\n    <td class=\"tg-9wq8\">-0.37%</td>\n    <td class=\"tg-9wq8\">2.52x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">inception_resnet_v2</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">80.20%</td>\n    <td class=\"tg-za14\">80.40%</td>\n    <td class=\"tg-9wq8\">-0.25%</td>\n    <td class=\"tg-9wq8\">1.75x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">mobilenetv1</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">71.10%</td>\n    <td class=\"tg-za14\">71.00%</td>\n    <td class=\"tg-9wq8\">0.14%</td>\n    <td class=\"tg-9wq8\">1.88x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">ssd_resnet50_v1</td>\n    <td class=\"tg-9wq8\">Coco</td>\n    <td class=\"tg-za14\">37.72%</td>\n    <td class=\"tg-za14\">38.01%</td>\n    <td class=\"tg-9wq8\">-0.76%</td>\n    <td class=\"tg-9wq8\">2.88x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">mask_rcnn_inception_v2</td>\n    <td class=\"tg-9wq8\">Coco</td>\n    <td class=\"tg-za14\">28.75%</td>\n    <td class=\"tg-za14\">29.13%</td>\n    <td class=\"tg-9wq8\">-1.30%</td>\n    <td class=\"tg-9wq8\">4.14x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">wide_deep_large_ds</td>\n    <td class=\"tg-c3ow\">criteo-kaggle</td>\n    <td class=\"tg-za14\">77.61%</td>\n    <td class=\"tg-za14\">77.67%</td>\n    <td class=\"tg-9wq8\">-0.08%</td>\n    <td class=\"tg-9wq8\">1.41x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">vgg16</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.10%</td>\n    <td class=\"tg-za14\">70.90%</td>\n    <td class=\"tg-9wq8\">1.69%</td>\n    <td class=\"tg-9wq8\">3.71x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">vgg19</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.30%</td>\n    <td class=\"tg-za14\">71.00%</td>\n    <td class=\"tg-9wq8\">1.83%</td>\n    <td class=\"tg-9wq8\">3.78x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">resnetv2_50</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">70.20%</td>\n    <td class=\"tg-za14\">69.60%</td>\n    <td class=\"tg-9wq8\">0.86%</td>\n    <td class=\"tg-9wq8\">1.52x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">resnetv2_101</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.50%</td>\n    <td class=\"tg-za14\">71.90%</td>\n    <td class=\"tg-9wq8\">0.83%</td>\n    <td class=\"tg-9wq8\">1.59x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">resnetv2_152</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.70%</td>\n    <td class=\"tg-za14\">72.40%</td>\n    <td class=\"tg-9wq8\">0.41%</td>\n    <td class=\"tg-9wq8\">1.62x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">densenet121</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.60%</td>\n    <td class=\"tg-za14\">72.90%</td>\n    <td class=\"tg-9wq8\">-0.41%</td>\n    <td class=\"tg-9wq8\">1.84x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">densenet161</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">76.10%</td>\n    <td class=\"tg-za14\">76.30%</td>\n    <td class=\"tg-9wq8\">-0.26%</td>\n    <td class=\"tg-9wq8\">1.44x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">tensorflow</td>\n    <td class=\"tg-9wq8\">densenet169</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">74.40%</td>\n    <td class=\"tg-za14\">74.60%</td>\n    <td class=\"tg-9wq8\">-0.27%</td>\n    <td class=\"tg-9wq8\">1.22x</td>\n  </tr>\n</tbody>\n</table>\n\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Framework</th>\n    <th rowspan=\"2\">Version</th>\n    <th rowspan=\"2\">Model</th>\n    <th rowspan=\"2\">Dataset</th>\n    <th colspan=\"3\">TOP-1 Accuracy</th>\n    <th>Performance Speedup</th>\n  </tr>\n  <tr>\n    <td>INT8 Tuning Accuracy</td>\n    <td>FP32 Accuracy Baseline</td>\n    <td>Acc Ratio[(INT8-FP32)/FP32]</td>\n    <td>Realtime Latency Ratio[FP32/INT8]</td>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>mxnet</td>\n    <td rowspan=\"9\">1.7.0</td>\n    <td>resnet50v1</td>\n    <td>ImageNet</td>\n    <td>76.03%</td>\n    <td>76.33%</td>\n    <td>-0.39%</td>\n    <td>3.18x</td>\n  </tr>\n  <tr>\n    <td>mxnet</td>\n    <td>inceptionv3</td>\n    <td>ImageNet</td>\n    <td>77.80%</td>\n    <td>77.64%</td>\n    <td>0.21%</td>\n    <td>2.65x</td>\n  </tr>\n  <tr>\n    <td>mxnet</td>\n    <td>mobilenet1.0</td>\n    <td>ImageNet</td>\n    <td>71.72%</td>\n    <td>72.22%</td>\n    <td>-0.69%</td>\n    <td>2.62x</td>\n  </tr>\n  <tr>\n    <td>mxnet</td>\n    <td>mobilenetv2_1.0</td>\n    <td>ImageNet</td>\n    <td>70.77%</td>\n    <td>70.87%</td>\n    <td>-0.14%</td>\n    <td>2.89x</td>\n  </tr>\n  <tr>\n    <td>mxnet</td>\n    <td>resnet18_v1</td>\n    <td>ImageNet</td>\n    <td>69.99%</td>\n    <td>70.14%</td>\n    <td>-0.21%</td>\n    <td>3.08x</td>\n  </tr>\n  <tr>\n    <td>mxnet</td>\n    <td>squeezenet1.0</td>\n    <td>ImageNet</td>\n    <td>56.88%</td>\n    <td>56.96%</td>\n    <td>-0.14%</td>\n    <td>2.55x</td>\n  </tr>\n  <tr>\n    <td>mxnet</td>\n    <td>ssd-resnet50_v1</td>\n    <td>VOC</td>\n    <td>80.21%</td>\n    <td>80.23%</td>\n    <td>-0.02%</td>\n    <td>4.16x</td>\n  </tr>\n  <tr>\n    <td>mxnet</td>\n    <td>ssd-mobilenet1.0</td>\n    <td>VOC</td>\n    <td>74.94%</td>\n    <td>75.54%</td>\n    <td>-0.79%</td>\n    <td>3.31x</td>\n  </tr>\n  <tr>\n    <td>mxnet</td>\n    <td>resnet152_v1</td>\n    <td>ImageNet</td>\n    <td>78.32%</td>\n    <td>78.54%</td>\n    <td>-0.28%</td>\n    <td>3.16x</td>\n  </tr>\n</tbody>\n</table>\n\n\n# Known Issues\n\n1. MSE tuning strategy doesn't work with PyTorch adaptor layer\n   MSE tuning strategy requires to compare FP32 tensor and INT8 tensor to decide which op has impact on final quantization accuracy. PyTorch adaptor layer doesn't implement this inspect tensor interface. So if the model to tune is a PyTorch model, please do not choose MSE tuning strategy.\n\n# Support\n\nPlease submit your questions, feature requests, and bug reports on the\n[GitHub issues](https://github.com/intel/lpot/issues) page. You may also reach out to lpot.maintainers@intel.com.\n\n# Contributing\n\nWe welcome community contributions to Intel® Low Precision Optimization Tool. If you have an idea on how\nto improve the library:\n\n* For changes impacting the public API, submit an [RFC pull request](CONTRIBUTING.md#RFC_pull_requests).\n* Ensure that the changes are consistent with the [code contribution guidelines](CONTRIBUTING.md#code_contribution_guidelines) and [coding style](CONTRIBUTING.md#coding_style).\n* Ensure that you can run all the examples with your patch.\n* Submit a [pull request](https://github.com/intel/lpot/pulls).\n\nFor additional details, see [contribution guidelines](CONTRIBUTING.md).\n\nThis project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the [Contributor Covenant](CODE_OF_CONDUCT.md) code of conduct.\n\n# License\n\nIntel® Low Precision Optimization Tool is licensed under [Apache License Version 2.0](http://www.apache.org/licenses/LICENSE-2.0). This software includes components with separate copyright notices and license terms. Your use of the source code for these components is subject to the terms and conditions of the following licenses.\n\nApache License Version 2.0:\n* [Intel TensorFlow Quantization Tool](https://github.com/IntelAI/tools)\n\nMIT License:\n* [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)\n\nSee accompanying [LICENSE](LICENSE) file for full license text and copyright notices.\n\n--------\n\n[Legal Information](legal_information.md)\n\n## Citing\n\nIf you use Intel® Low Precision Optimization Tool in your research or wish to refer to the tuning results published in the [Tuning Zoo](#tuning-zoo), please use the following BibTeX entry.\n\n```\n@misc{Intel® Low Precision Optimization Tool,\n  author =       {Feng Tian, Chuanqi Wang, Guoming Zhang, Penghui Cheng, Pengxin Yuan, Haihao Shen, and Jiong Gong},\n  title =        {Intel® Low Precision Optimization Tool},\n  howpublished = {\\url{https://github.com/intel/lpot}},\n  year =         {2020}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/intel/lpot",
    "keywords": "quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training,tuning strategy",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lpot",
    "package_url": "https://pypi.org/project/lpot/",
    "platform": "",
    "project_url": "https://pypi.org/project/lpot/",
    "project_urls": {
      "Homepage": "https://github.com/intel/lpot"
    },
    "release_url": "https://pypi.org/project/lpot/1.0/",
    "requires_dist": [
      "numpy",
      "pyyaml",
      "scikit-learn",
      "schema",
      "py-cpuinfo",
      "hyperopt",
      "pandas"
    ],
    "requires_python": ">=3.5.0",
    "summary": "Repository of Intel® Low Precision Optimization Tool",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11239500,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ca28f42520730479e9a3824721d8b618da3aceb056757d38dcf691c65aa5a3f2",
        "md5": "3563a03860c5f331a8c5620c7aed5bd1",
        "sha256": "ffa79c031f274d299ddad60e14bbd32ac4e0ba40e14f6fc20268cedd07a2a4fb"
      },
      "downloads": -1,
      "filename": "lpot-1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3563a03860c5f331a8c5620c7aed5bd1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5.0",
      "size": 258163,
      "upload_time": "2020-12-02T05:35:30",
      "upload_time_iso_8601": "2020-12-02T05:35:30.321243Z",
      "url": "https://files.pythonhosted.org/packages/ca/28/f42520730479e9a3824721d8b618da3aceb056757d38dcf691c65aa5a3f2/lpot-1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}