{
  "info": {
    "author": "Intel MLP/MLPC Team",
    "author_email": "feng.tian@intel.com, chuanqi.wang@intel.com, pengxin.yuan@intel.com, guoming.zhang@intel.com, haihao.shen@intel.com, jiong.gong@intel.com, xi2.chen@intel.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "Introduction to Intel® LPOT \n===========================\n\nThe Intel® Low Precision Optimization Tool (Intel® LPOT) is an open-source Python library that delivers a unified low-precision inference interface across multiple Intel-optimized Deep Learning (DL) frameworks on both CPUs and GPUs. It supports automatic accuracy-driven tuning strategies, along with additional objectives such as optimizing for performance, model size, and memory footprint. It also provides easy extension capability for new backends, tuning strategies, metrics, and objectives.\n\n> **Note**\n>\n> GPU support is under development.\n\n**Visit the Intel® LPOT online document website at: <https://intel.github.io/lpot>.**\n\n## Architecture\n\nIntel® LPOT features an infrastructure and workflow that aids in increasing performance and faster deployments across architectures. \n\n\n#### Infrastructure\n\n<a target=\"_blank\" href=\"docs/imgs/infrastructure.png\">\n  <img src=\"docs/imgs/infrastructure.png\" alt=\"Infrastructure\" width=800 height=360>\n</a>\n\nClick the image to enlarge it.\n\n#### Workflow\n\n<a target=\"_blank\" href=\"docs/imgs/workflow.png\">\n  <img src=\"docs/imgs/workflow.png\" alt=\"Workflow\" width=800 height=360>\n</a>\n\nClick the image to enlarge it.\n\n#### Supported Frameworks\n\nSupported Intel-optimized DL frameworks are:\n* [TensorFlow\\*](https://github.com/Intel-tensorflow/tensorflow), including [1.15.0 UP2](https://github.com/Intel-tensorflow/tensorflow/tree/v1.15.0up2), [1.15.0 UP1](https://github.com/Intel-tensorflow/tensorflow/tree/v1.15.0up1), [2.1.0](https://github.com/Intel-tensorflow/tensorflow/tree/v2.1.0), [2.2.0](https://github.com/Intel-tensorflow/tensorflow/tree/v2.2.0), [2.3.0](https://github.com/Intel-tensorflow/tensorflow/tree/v2.3.0), [2.4.0](https://github.com/Intel-tensorflow/tensorflow/tree/v2.4.0)\n* [PyTorch\\*](https://pytorch.org/), including [1.5.0+cpu](https://download.pytorch.org/whl/torch_stable.html), [1.6.0+cpu](https://download.pytorch.org/whl/torch_stable.html)\n* [Apache\\* MXNet](https://mxnet.apache.org), including [1.6.0](https://github.com/apache/incubator-mxnet/tree/1.6.0), [1.7.0](https://github.com/apache/incubator-mxnet/tree/1.7.0)\n* [ONNX\\* Runtime](https://github.com/microsoft/onnxruntime), including [1.6.0](https://github.com/microsoft/onnxruntime/tree/v1.6.0)\n\n\n## Installation\n\nSelect the installation based on your operating system.\n\n\n### Linux Installation\n\nYou can install LPOT using one of three options: Install just the LPOT library\nfrom binary or source, or get the Intel-optimized framework together with the\nLPOT library by installing the [Intel® oneAPI AI Analytics Toolkit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html).\n\n#### Option 1 Install from binary\n\n  ```Shell\n  # install stable version from pip\n  pip install lpot\n\n  # install nightly version from pip\n  pip install -i https://test.pypi.org/simple/ lpot\n\n  # install stable version from from conda\n  conda install lpot -c conda-forge -c intel \n  ```\n\n#### Option 2 Install from source\n\n  ```Shell\n  git clone https://github.com/intel/lpot.git\n  cd lpot\n  pip install -r requirements.txt\n  python setup.py install\n  ```\n\n#### Option 3 Install from AI Kit\n\nThe Intel® LPOT library is released as part of the\n[Intel® oneAPI AI Analytics Toolkit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html) (AI Kit).\nThe AI Kit provides a consolidated package of Intel's latest deep learning and\nmachine optimizations all in one place for ease of development. Along with\nLPOT, the AI Kit includes Intel-optimized versions of deep learning frameworks\n(such as TensorFlow and PyTorch) and high-performing Python libraries to\nstreamline end-to-end data science and AI workflows on Intel architectures.\n\nThe AI Kit is distributed through many common channels,\nincluding from Intel's website, YUM, APT, Anaconda, and more.\nSelect and [download](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit/download.html)\nthe AI Kit distribution package that's best suited for you and follow the\n[Get Started Guide](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html)\nfor post-installation instructions.\n\n|[Download AI Kit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit/) |[AI Kit Get Started Guide](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html) |\n|---|---|\n\n### Windows Installation\n\n**Prerequisites**\n\nThe following prerequisites and requirements must be satisfied for a successful installation:\n\n- Python version: 3.6 or 3.7 or 3.8\n\n- Download and install [anaconda](https://anaconda.org/).\n\n- Create a virtual environment named lpot in anaconda:\n\n    ```shell\n    # Here we install python 3.7 for instance. You can also choose python 3.6 & 3.8.\n    conda create -n lpot python=3.7\n    conda activate lpot\n    ```\n**Installation options**\n\n#### Option 1 Install from binary\n\n  ```Shell\n  # install stable version from pip\n  pip install lpot\n\n  # install nightly version from pip\n  pip install -i https://test.pypi.org/simple/ lpot\n\n  # install from conda\n  conda install lpot -c conda-forge -c intel \n  ```\n\n#### Option 2 Install from source\n\n```shell\ngit clone https://github.com/intel/lpot.git\ncd lpot\npip install -r requirements.txt\npython setup.py install\n```\n\n## Documentation\n\n**Get Started**\n\n* [APIs](docs/api-introduction.md) explains Intel® Low Precision Optimization Tool's API.\n* [Transform](docs/transform.md) introduces how to utilize LPOT's built-in data processing and how to develop a custom data processing method. \n* [Dataset](docs/dataset.md) introduces how to utilize LPOT's built-in dataset and how to develop a custom dataset.\n* [Metric](docs/metric.md) introduces how to utilize LPOT's built-in metrics and how to develop a custom metric.\n* [Tutorial](docs/tutorial.md) provides comprehensive instructions on how to utilize LPOT's features with examples. \n* [Examples](/examples) are provided to demonstrate the usage of LPOT in different frameworks: TensorFlow, PyTorch, MXNet, and ONNX Runtime.\n* [UX](docs/ux.md) is a web-based system used to simplify LPOT usage.\n* [Intel oneAPI AI Analytics Toolkit Get Started Guide](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html) explains the AI Kit components, installation and configuration guides, and instructions for building and running sample apps.\n* [AI and Analytics Samples](https://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics) includes code samples for Intel oneAPI libraries.\n\n**Deep Dive**\n\n* [Quantization](docs/Quantization.md) are processes that enable inference and training by performing computations at low-precision data types, such as fixed-point integers. LPOT supports Post-Training Quantization ([PTQ](docs/PTQ.md)) and Quantization-Aware Training ([QAT](docs/QAT.md)). Note that ([Dynamic Quantization](docs/dynamic_quantization.md)) currently has limited support.\n* [Pruning](docs/pruning.md) provides a common method for introducing sparsity in weights and activations.\n* [Benchmarking](docs/benchmark.md) introduces how to utilize the benchmark interface of LPOT.\n* [Mixed precision](docs/mixed_precision.md) introduces how to enable mixed precision, including BFP16 and int8 and FP32, on Intel platforms during tuning.\n* [Graph Optimization](docs/graph_optimization.md) introduces how to enable graph optimization for FP32 and auto-mixed precision.\n* [Model Conversion](docs/model_conversion.md) introduces how to convert TensorFlow QAT model to quantized model running on Intel platforms.\n* [TensorBoard](docs/tensorboard.md) provides tensor histograms and execution graphs for tuning debugging purposes. \n\n**Advanced Topics**\n\n* [Adaptor](docs/adaptor.md) is the interface between LPOT and framework. The method to develop adaptor extension is introduced with ONNX Runtime as example. \n* [Strategy](docs/tuning_strategies.md) can automatically optimized low-precision recipes for deep learning models to achieve optimal product objectives like inference performance and memory usage with expected accuracy criteria. The method to develop a new strategy is introduced.\n\n## System Requirements\n\nIntel® Low Precision Optimization Tool supports systems based on [Intel 64 architecture or compatible processors](https://en.wikipedia.org/wiki/X86-64), specially optimized for the following CPUs:\n\n* Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, and Cooper Lake)\n* future Intel Xeon Scalable processor (code name Sapphire Rapids)\n\nIntel® Low Precision Optimization Tool requires installing the pertinent Intel-optimized framework version for TensorFlow, PyTorch, and MXNet.\n\n### Validated Hardware/Software Environment\n\n<table class=\"docutils\">\n<thead>\n  <tr>\n    <th class=\"tg-bobw\">Platform</th>\n    <th class=\"tg-bobw\">OS</th>\n    <th class=\"tg-bobw\">Python</th>\n    <th class=\"tg-bobw\">Framework</th>\n    <th class=\"tg-bobw\">Version</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-nrix\" rowspan=\"13\">Cascade Lake<br><br>Cooper Lake<br><br>Skylake</td>\n    <td class=\"tg-nrix\" rowspan=\"13\">CentOS 7.8<br><br>Ubuntu 18.04</td>\n    <td class=\"tg-nrix\" rowspan=\"13\">3.6<br><br>3.7<br><br>3.8</td>\n    <td class=\"tg-cly1\" rowspan=\"7\">TensorFlow</td>\n    <td class=\"tg-7zrl\">2.4.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">2.2.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.15.0 UP1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.15.0 UP2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">2.3.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">2.1.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.15.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\" rowspan=\"3\">PyTorch</td>\n    <td class=\"tg-7zrl\">1.5.0+cpu</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.6.0+cpu</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">IPEX</td>\n  </tr>\n  <tr>\n    <td class=\"tg-cly1\" rowspan=\"2\">MXNet</td>\n    <td class=\"tg-7zrl\">1.7.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.6.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ONNX Runtime</td>\n    <td class=\"tg-7zrl\">1.6.0</td>\n  </tr>\n</tbody>\n</table>\n\n### Validated Models\n\nIntel® Low Precision Optimization Tool provides numerous examples to show promising accuracy loss with the best performance gain. A full quantized model list on various frameworks is available in the [Model List](docs/full_model_list.md).\n\n<table class=\"docutils\">\n<thead>\n  <tr>\n    <th rowspan=\"2\">Framework</th>\n    <th rowspan=\"2\">version</th>\n    <th rowspan=\"2\">Model</th>\n    <th rowspan=\"2\">dataset</th>\n    <th colspan=\"3\">Accuracy</th>\n    <th>Performance speed up</th>\n  </tr>\n  <tr>\n    <td>INT8 Tuning Accuracy</td>\n    <td>FP32 Accuracy Baseline</td>\n    <td>Acc Ratio[(INT8-FP32)/FP32]</td>\n    <td>Realtime Latency Ratio[FP32/INT8]</td>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>resnet50v1.5</td>\n    <td>ImageNet</td>\n    <td>76.70%</td>\n    <td>76.50%</td>\n    <td>0.26%</td>\n    <td>3.23x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>Resnet101</td>\n    <td>ImageNet</td>\n    <td>77.20%</td>\n    <td>76.40%</td>\n    <td>1.05%</td>\n    <td>2.42x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>inception_v1</td>\n    <td>ImageNet</td>\n    <td>70.10%</td>\n    <td>69.70%</td>\n    <td>0.57%</td>\n    <td>1.88x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>inception_v2</td>\n    <td>ImageNet</td>\n    <td>74.10%</td>\n    <td>74.00%</td>\n    <td>0.14%</td>\n    <td>1.96x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>inception_v3</td>\n    <td>ImageNet</td>\n    <td>77.20%</td>\n    <td>76.70%</td>\n    <td>0.65%</td>\n    <td>2.36x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>inception_v4</td>\n    <td>ImageNet</td>\n    <td>80.00%</td>\n    <td>80.30%</td>\n    <td>-0.37%</td>\n    <td>2.59x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>inception_resnet_v2</td>\n    <td>ImageNet</td>\n    <td>80.10%</td>\n    <td>80.40%</td>\n    <td>-0.37%</td>\n    <td>1.97x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>Mobilenetv1</td>\n    <td>ImageNet</td>\n    <td>71.10%</td>\n    <td>71.00%</td>\n    <td>0.14%</td>\n    <td>2.88x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>ssd_resnet50_v1</td>\n    <td>Coco</td>\n    <td>37.90%</td>\n    <td>38.00%</td>\n    <td>-0.26%</td>\n    <td>2.97x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>mask_rcnn_inception_v2</td>\n    <td>Coco</td>\n    <td>28.90%</td>\n    <td>29.10%</td>\n    <td>-0.69%</td>\n    <td>2.66x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>vgg16</td>\n    <td>ImageNet</td>\n    <td>72.50%</td>\n    <td>70.90%</td>\n    <td>2.26%</td>\n    <td>3.75x</td>\n  </tr>\n  <tr>\n    <td>tensorflow</td>\n    <td>2.4.0</td>\n    <td>vgg19</td>\n    <td>ImageNet</td>\n    <td>72.40%</td>\n    <td>71.00%</td>\n    <td>1.97%</td>\n    <td>3.79x</td>\n  </tr>\n</tbody>\n</table>\n\n\n<table class=\"docutils\">\n<thead>\n  <tr>\n    <th rowspan=\"2\">Framework</th>\n    <th rowspan=\"2\">version</th>\n    <th rowspan=\"2\">model</th>\n    <th rowspan=\"2\">dataset</th>\n    <th colspan=\"3\">Accuracy</th>\n    <th>Performance speed up</th>\n  </tr>\n  <tr>\n    <td>INT8 Tuning Accuracy</td>\n    <td>FP32 Accuracy Baseline</td>\n    <td>Acc Ratio[(INT8-FP32)/FP32]</td>\n    <td>Realtime Latency Ratio[FP32/INT8]</td>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>pytorch</td>\n    <td>1.5.0+cpu</td>\n    <td>resnet50</td>\n    <td>ImageNet</td>\n    <td>75.96%</td>\n    <td>76.13%</td>\n    <td>-0.23%</td>\n    <td>2.63x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.5.0+cpu</td>\n    <td>resnext101_32x8d</td>\n    <td>ImageNet</td>\n    <td>79.12%</td>\n    <td>79.31%</td>\n    <td>-0.24%</td>\n    <td>2.61x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.6.0a0+24aac32</td>\n    <td>bert_base_mrpc</td>\n    <td>MRPC</td>\n    <td>88.90%</td>\n    <td>88.73%</td>\n    <td>0.19%</td>\n    <td>1.98x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.6.0a0+24aac32</td>\n    <td>bert_base_cola</td>\n    <td>COLA</td>\n    <td>59.06%</td>\n    <td>58.84%</td>\n    <td>0.37%</td>\n    <td>2.19x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.6.0a0+24aac32</td>\n    <td>bert_base_sts-b</td>\n    <td>STS-B</td>\n    <td>88.40%</td>\n    <td>89.27%</td>\n    <td>-0.97%</td>\n    <td>2.28x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.6.0a0+24aac32</td>\n    <td>bert_base_sst-2</td>\n    <td>SST-2</td>\n    <td>91.51%</td>\n    <td>91.86%</td>\n    <td>-0.37%</td>\n    <td>2.30x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.6.0a0+24aac32</td>\n    <td>bert_base_rte</td>\n    <td>RTE</td>\n    <td>69.31%</td>\n    <td>69.68%</td>\n    <td>-0.52%</td>\n    <td>2.15x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.6.0a0+24aac32</td>\n    <td>bert_large_mrpc</td>\n    <td>MRPC</td>\n    <td>87.45%</td>\n    <td>88.33%</td>\n    <td>-0.99%</td>\n    <td>2.73x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.6.0a0+24aac32</td>\n    <td>bert_large_squad</td>\n    <td>SQUAD</td>\n    <td>92.85%</td>\n    <td>93.05%</td>\n    <td>-0.21%</td>\n    <td>2.01x</td>\n  </tr>\n  <tr>\n    <td>pytorch</td>\n    <td>1.6.0a0+24aac32</td>\n    <td>bert_large_qnli</td>\n    <td>QNLI</td>\n    <td>91.20%</td>\n    <td>91.82%</td>\n    <td>-0.68%</td>\n    <td>2.69x</td>\n  </tr>\n</tbody>\n</table>\n\n## Additional Content\n\n* [Release Information](releases_info.md)\n* [Contribution Guidelines](contributions.md)\n* [Legal](legal_information.md)\n* [Security Policy](security_policy.md)\n* [Intel® LPOT Website](https://intel.github.io/lpot)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/intel/lpot",
    "keywords": "quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training,tuning strategy",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lpot",
    "package_url": "https://pypi.org/project/lpot/",
    "platform": "",
    "project_url": "https://pypi.org/project/lpot/",
    "project_urls": {
      "Homepage": "https://github.com/intel/lpot"
    },
    "release_url": "https://pypi.org/project/lpot/1.4/",
    "requires_dist": [
      "numpy",
      "pyyaml",
      "scikit-learn",
      "schema",
      "py-cpuinfo",
      "hyperopt",
      "pandas",
      "pycocotools",
      "opencv-python",
      "requests",
      "Flask-Cors",
      "Flask-SocketIO",
      "Flask",
      "gevent-websocket",
      "gevent",
      "psutil",
      "ruamel.yaml"
    ],
    "requires_python": ">=3.6.0",
    "summary": "Repository of Intel® Low Precision Optimization Tool",
    "version": "1.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11239500,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c411cae13e9e2b0db5cfb57d33bc955638d1ea1f8f04fbb9d1741664f1e150e0",
        "md5": "e1a8889ea14221c62048108bd22d4b0d",
        "sha256": "e511a8450cb6cdfac1762a72cf791e871f822f0a4595919c858b577341a17198"
      },
      "downloads": -1,
      "filename": "lpot-1.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e1a8889ea14221c62048108bd22d4b0d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 2028485,
      "upload_time": "2021-05-31T03:26:35",
      "upload_time_iso_8601": "2021-05-31T03:26:35.881632Z",
      "url": "https://files.pythonhosted.org/packages/c4/11/cae13e9e2b0db5cfb57d33bc955638d1ea1f8f04fbb9d1741664f1e150e0/lpot-1.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c2fdb8fd3b95d3ccc1946e9a36821e0f79edf483cfe04ce47e9c9ef9e6659747",
        "md5": "33fb1b14ff6e0f0012cf20827e0d5ddf",
        "sha256": "e65a24b7a2049a0ddd18de92daaf1d845e06184d2f1b88d739652d45970718ce"
      },
      "downloads": -1,
      "filename": "lpot-1.4.tar.gz",
      "has_sig": false,
      "md5_digest": "33fb1b14ff6e0f0012cf20827e0d5ddf",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 1734845,
      "upload_time": "2021-05-31T03:26:38",
      "upload_time_iso_8601": "2021-05-31T03:26:38.190945Z",
      "url": "https://files.pythonhosted.org/packages/c2/fd/b8fd3b95d3ccc1946e9a36821e0f79edf483cfe04ce47e9c9ef9e6659747/lpot-1.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}