{
  "info": {
    "author": "Intel MLP/MLPC Team",
    "author_email": "feng.tian@intel.com, chuanqi.wang@intel.com, pengxin.yuan@intel.com, guoming.zhang@intel.com, haihao.shen@intel.com, jiong.gong@intel.com, xi2.chen@intel.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "Intel® Low Precision Optimization Tool\n======================================\n\nIntel® Low Precision Optimization Tool (Intel® LPOT) is an open-source Python library that delivers a unified low-precision inference interface across multiple Intel-optimized DL frameworks on both CPUs and GPUs. It supports automatic accuracy-driven tuning strategies, along with additional objectives such as optimizing for performance, model size, and memory footprint. It also provides easy extension capability for new backends, tuning strategies, metrics, and objectives.\n\n> **Note**\n>\n> GPU support is under development.\n\n<table>\n  <tr>\n    <td>Infrastructure</td>\n    <td>Workflow</td>\n  </tr>\n  <tr>\n    <td><img src=\"docs/imgs/infrastructure.jpg\" width=640 height=320></td>\n    <td><img src=\"docs/imgs/workflow.jpg\" width=640 height=320></td>\n  </tr>\n </table>\n\nSupported Intel optimized DL frameworks are:\n* [TensorFlow\\*](https://github.com/Intel-tensorflow/tensorflow), including [1.15.0 UP2](https://github.com/Intel-tensorflow/tensorflow/tree/v1.15.0up2), [1.15.0 UP1](https://github.com/Intel-tensorflow/tensorflow/tree/v1.15.0up1), [2.1.0](https://github.com/Intel-tensorflow/tensorflow/tree/v2.1.0), [2.2.0](https://github.com/Intel-tensorflow/tensorflow/tree/v2.2.0), [2.3.0](https://github.com/Intel-tensorflow/tensorflow/tree/v2.3.0)\n* [PyTorch\\*](https://pytorch.org/), including [1.5.0+cpu](https://download.pytorch.org/whl/torch_stable.html), [1.6.0+cpu](https://download.pytorch.org/whl/torch_stable.html)\n* [Apache\\* MXNet](https://mxnet.apache.org), including [1.6.0](https://github.com/apache/incubator-mxnet/tree/1.6.0), [1.7.0](https://github.com/apache/incubator-mxnet/tree/1.7.0)\n* [ONNX\\* Runtime](https://github.com/microsoft/onnxruntime), including [1.6.0](https://github.com/microsoft/onnxruntime/tree/v1.6.0)\n\n\n# Installation\n## Install for Linux\n### Install from binary\n\n  ```Shell\n  # install from pip\n  pip install lpot\n\n  # install from conda\n  conda install lpot -c conda-forge -c intel \n  ```\n\n### Install from source\n\n  ```Shell\n  git clone https://github.com/intel/lpot.git\n  cd lpot\n  python setup.py install\n  ```\n\n## Install for Windows\n### Install from binary\n  ```Shell\n  # install from pip\n  pip install lpot\n\n  # install from conda\n  conda install lpot -c conda-forge -c intel \n  ```\n### Install from source\n\n#### **Prerequisites**\n\nThe following prerequisites and requirements must be satisfied in order to install successfully：\n\n- Python version: 3.6 or 3.7 or 3.8\n\n- Download and install anaconda: [anaconda](https://anaconda.org/)\n\n- Create a virtual environment named lpot in anaconda:\n    ```shell\n    # Here we install python 3.7 for instance. You can also choose python 3.6 & 3.8.\n    conda create -n lpot python=3.7\n    conda activate lpot\n    ```\n\n#### **Installation Procedure**\n\n```shell\ngit clone https://github.com/intel/lpot.git\ncd lpot\npip install -r requirements.txt\npython setup.py install\n```\n\n# Getting started\n\n* [Introduction](docs/introduction.md) explains Intel® Low Precision Optimization Tool's API.\n* [Tutorial](docs/tutorial.md) provides comprehensive instructions on how to utilize Intel® Low Precision Optimization Tool's features with examples. \n* [Examples](examples) are provided to demonstrate the usage of Intel® Low Precision Optimization Tool in different frameworks: [TensorFlow](examples/tensorflow), [PyTorch](examples/pytorch), [MXNet](examples/mxnet) and [ONNX Runtime](examples/onnxrt).\n\n\n# Deep Dive\n\n* [Quantization](docs/Quantization.md) is the processes that enable inference and training by performing computations at low precision data type, such as fixed point integers. LPOT supports [Post-Training Quantization](docs/PTQ.md) and [Quantization-Aware Training](docs/QAT.md)\n* [Pruning](docs/pruning.md) provides a common method for introducing sparsity in weights and activations.\n* [Benchmarking](docs/benchmark.md) introduces how to utilize the benchmark interface of LPOT.\n* [Mixed precision](docs/mixed_precision.md) introduces how to enable mixed precision, including BFP16 and int8 and FP32, on Intel platforms during tuning.\n* [Transform](docs/transform.md) introduces how to utilize LPOT buildin data processing and how to develop a custom data processing method. \n* [Dataset](docs/dataset.md) introudces how to utilize LPOT buildin dataset and how to develop a custom dataset.\n* [Metric](docs/metric.md) introduces how to utilize LPOT buildin metric and how to develop a custom metric.\n* [TensorBoard](docs/tensorboard.md) provides tensor histogram and execution graph for tuning debugging purpose.\n* [PyTorch Deploy](docs/pytorch_model_saving.md) introduces how LPOT saves and loads quantized PyTorch model.\n\n\n# Advanced Topics\n\n* [Adaptor](docs/adaptor.md) is the interface between LPOT and framework. The method to develop adaptor extension is introduced with ONNX Runtime as example. \n* [Strategy](docs/tuning_strategies.md) can automatically optimized low-precision recipes for deep learning models to achieve optimal product objectives like inference performance and memory usage with expected accuracy criteria. The method to develop a new strategy is introduced.\n\n\n# System Requirements\n\nIntel® Low Precision Optimization Tool supports systems based on [Intel 64 architecture or compatible processors](https://en.wikipedia.org/wiki/X86-64), specially optimized for the following CPUs:\n\n* Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, and Cooper Lake)\n* future Intel Xeon Scalable processor (code name Sapphire Rapids)\n\nIntel® Low Precision Optimization Tool requires installing the pertinent Intel-optimized framework version for TensorFlow, PyTorch, and MXNet.\n\n### Validated Hardware/Software Environment\n\n<table>\n<thead>\n  <tr>\n    <th class=\"tg-bobw\">Platform</th>\n    <th class=\"tg-bobw\">OS</th>\n    <th class=\"tg-bobw\">Python</th>\n    <th class=\"tg-bobw\">Framework</th>\n    <th class=\"tg-bobw\">Version</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-nrix\" rowspan=\"10\">Cascade Lake<br><br>Cooper Lake<br><br>Skylake</td>\n    <td class=\"tg-nrix\" rowspan=\"10\">CentOS 7.8<br><br>Ubuntu 18.04</td>\n    <td class=\"tg-nrix\" rowspan=\"10\">3.6<br><br>3.7</td>\n    <td class=\"tg-cly1\" rowspan=\"6\">TensorFlow</td>\n    <td class=\"tg-7zrl\">2.2.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.15.0 UP1</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.15.0 UP2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">2.3.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">2.1.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.15.2</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">PyTorch</td>\n    <td class=\"tg-7zrl\">1.5.0+cpu</td>\n  </tr>\n  <tr>\n    <td class=\"tg-cly1\" rowspan=\"2\">MXNet</td>\n    <td class=\"tg-7zrl\">1.7.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">1.6.0</td>\n  </tr>\n  <tr>\n    <td class=\"tg-7zrl\">ONNX Runtime</td>\n    <td class=\"tg-7zrl\">1.6.0</td>\n  </tr>\n</tbody>\n</table>\n\n# Model Zoo\n\nIntel® Low Precision Optimization Tool provides numerous examples to show promising accuracy loss with the best performance gain.\n\n<table>\n<thead>\n  <tr>\n    <th class=\"tg-9wq8\" rowspan=\"2\">Framework</th>\n    <th class=\"tg-9wq8\" rowspan=\"2\">Version</th>\n    <th class=\"tg-9wq8\" rowspan=\"2\">Model</th>\n    <th class=\"tg-9wq8\" rowspan=\"2\">Dataset</th>\n    <th class=\"tg-pb0m\" colspan=\"3\">TOP-1   Accuracy</th>\n    <th class=\"tg-za14\">Performance Speedup</th>\n  </tr>\n  <tr>\n    <td class=\"tg-za14\">INT8 Tuning Accuracy</td>\n    <td class=\"tg-za14\">FP32 Accuracy   Baseline</td>\n    <td class=\"tg-za14\">Acc   Ratio[(INT8-FP32)/FP32]</td>\n    <td class=\"tg-za14\">Real-time Latency Ratio[FP32/INT8]</td>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\" rowspan=\"20\">2.2.0</td>\n    <td class=\"tg-9wq8\">resnet50v1.0</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">73.80%</td>\n    <td class=\"tg-za14\">74.30%</td>\n    <td class=\"tg-9wq8\">-0.67%</td>\n    <td class=\"tg-9wq8\">2.25x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">resnet50v1.5</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">76.80%</td>\n    <td class=\"tg-za14\">76.50%</td>\n    <td class=\"tg-9wq8\">0.39%</td>\n    <td class=\"tg-9wq8\">2.32x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">resnet101</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">77.20%</td>\n    <td class=\"tg-za14\">76.40%</td>\n    <td class=\"tg-9wq8\">1.05%</td>\n    <td class=\"tg-9wq8\">2.75x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">inception_v1</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">70.10%</td>\n    <td class=\"tg-za14\">69.70%</td>\n    <td class=\"tg-9wq8\">0.57%</td>\n    <td class=\"tg-9wq8\">1.56x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">inception_v2</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">74.00%</td>\n    <td class=\"tg-za14\">74.00%</td>\n    <td class=\"tg-9wq8\">0.00%</td>\n    <td class=\"tg-9wq8\">1.68x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">inception_v3</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">77.20%</td>\n    <td class=\"tg-za14\">76.70%</td>\n    <td class=\"tg-9wq8\">0.65%</td>\n    <td class=\"tg-9wq8\">2.05x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">inception_v4</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">80.00%</td>\n    <td class=\"tg-za14\">80.30%</td>\n    <td class=\"tg-9wq8\">-0.37%</td>\n    <td class=\"tg-9wq8\">2.52x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">inception_resnet_v2</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">80.20%</td>\n    <td class=\"tg-za14\">80.40%</td>\n    <td class=\"tg-9wq8\">-0.25%</td>\n    <td class=\"tg-9wq8\">1.75x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">mobilenetv1</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">71.10%</td>\n    <td class=\"tg-za14\">71.00%</td>\n    <td class=\"tg-9wq8\">0.14%</td>\n    <td class=\"tg-9wq8\">1.88x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">ssd_resnet50_v1</td>\n    <td class=\"tg-9wq8\">Coco</td>\n    <td class=\"tg-za14\">37.72%</td>\n    <td class=\"tg-za14\">38.01%</td>\n    <td class=\"tg-9wq8\">-0.76%</td>\n    <td class=\"tg-9wq8\">2.88x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">mask_rcnn_inception_v2</td>\n    <td class=\"tg-9wq8\">Coco</td>\n    <td class=\"tg-za14\">28.75%</td>\n    <td class=\"tg-za14\">29.13%</td>\n    <td class=\"tg-9wq8\">-1.30%</td>\n    <td class=\"tg-9wq8\">4.14x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">wide_deep_large_ds</td>\n    <td class=\"tg-c3ow\">criteo-kaggle</td>\n    <td class=\"tg-za14\">77.61%</td>\n    <td class=\"tg-za14\">77.67%</td>\n    <td class=\"tg-9wq8\">-0.08%</td>\n    <td class=\"tg-9wq8\">1.41x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">vgg16</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.10%</td>\n    <td class=\"tg-za14\">70.90%</td>\n    <td class=\"tg-9wq8\">1.69%</td>\n    <td class=\"tg-9wq8\">3.71x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">vgg19</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.30%</td>\n    <td class=\"tg-za14\">71.00%</td>\n    <td class=\"tg-9wq8\">1.83%</td>\n    <td class=\"tg-9wq8\">3.78x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">resnetv2_50</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">70.20%</td>\n    <td class=\"tg-za14\">69.60%</td>\n    <td class=\"tg-9wq8\">0.86%</td>\n    <td class=\"tg-9wq8\">1.52x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">resnetv2_101</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.50%</td>\n    <td class=\"tg-za14\">71.90%</td>\n    <td class=\"tg-9wq8\">0.83%</td>\n    <td class=\"tg-9wq8\">1.59x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">resnetv2_152</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.70%</td>\n    <td class=\"tg-za14\">72.40%</td>\n    <td class=\"tg-9wq8\">0.41%</td>\n    <td class=\"tg-9wq8\">1.62x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">densenet121</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">72.60%</td>\n    <td class=\"tg-za14\">72.90%</td>\n    <td class=\"tg-9wq8\">-0.41%</td>\n    <td class=\"tg-9wq8\">1.84x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">densenet161</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">76.10%</td>\n    <td class=\"tg-za14\">76.30%</td>\n    <td class=\"tg-9wq8\">-0.26%</td>\n    <td class=\"tg-9wq8\">1.44x</td>\n  </tr>\n  <tr>\n    <td class=\"tg-9wq8\">TensorFlow</td>\n    <td class=\"tg-9wq8\">densenet169</td>\n    <td class=\"tg-9wq8\">ImageNet</td>\n    <td class=\"tg-za14\">74.40%</td>\n    <td class=\"tg-za14\">74.60%</td>\n    <td class=\"tg-9wq8\">-0.27%</td>\n    <td class=\"tg-9wq8\">1.22x</td>\n  </tr>\n</tbody>\n</table>\n\n\n<table>\n<thead>\n  <tr>\n    <th rowspan=\"2\">Framework</th>\n    <th rowspan=\"2\">Version</th>\n    <th rowspan=\"2\">Model</th>\n    <th rowspan=\"2\">Dataset</th>\n    <th colspan=\"3\">TOP-1 Accuracy</th>\n    <th>Performance Speedup</th>\n  </tr>\n  <tr>\n    <td>INT8 Tuning Accuracy</td>\n    <td>FP32 Accuracy Baseline</td>\n    <td>Acc Ratio[(INT8-FP32)/FP32]</td>\n    <td>Real-time Latency Ratio[FP32/INT8]</td>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td>MXNet</td>\n    <td rowspan=\"9\">1.7.0</td>\n    <td>resnet50v1</td>\n    <td>ImageNet</td>\n    <td>76.03%</td>\n    <td>76.33%</td>\n    <td>-0.39%</td>\n    <td>3.18x</td>\n  </tr>\n  <tr>\n    <td>MXNet</td>\n    <td>inceptionv3</td>\n    <td>ImageNet</td>\n    <td>77.80%</td>\n    <td>77.64%</td>\n    <td>0.21%</td>\n    <td>2.65x</td>\n  </tr>\n  <tr>\n    <td>MXNet</td>\n    <td>mobilenet1.0</td>\n    <td>ImageNet</td>\n    <td>71.72%</td>\n    <td>72.22%</td>\n    <td>-0.69%</td>\n    <td>2.62x</td>\n  </tr>\n  <tr>\n    <td>MXNet</td>\n    <td>mobilenetv2_1.0</td>\n    <td>ImageNet</td>\n    <td>70.77%</td>\n    <td>70.87%</td>\n    <td>-0.14%</td>\n    <td>2.89x</td>\n  </tr>\n  <tr>\n    <td>MXNet</td>\n    <td>resnet18_v1</td>\n    <td>ImageNet</td>\n    <td>69.99%</td>\n    <td>70.14%</td>\n    <td>-0.21%</td>\n    <td>3.08x</td>\n  </tr>\n  <tr>\n    <td>MXNet</td>\n    <td>squeezenet1.0</td>\n    <td>ImageNet</td>\n    <td>56.88%</td>\n    <td>56.96%</td>\n    <td>-0.14%</td>\n    <td>2.55x</td>\n  </tr>\n  <tr>\n    <td>MXNet</td>\n    <td>ssd-resnet50_v1</td>\n    <td>VOC</td>\n    <td>80.21%</td>\n    <td>80.23%</td>\n    <td>-0.02%</td>\n    <td>4.16x</td>\n  </tr>\n  <tr>\n    <td>MXNet</td>\n    <td>ssd-mobilenet1.0</td>\n    <td>VOC</td>\n    <td>74.94%</td>\n    <td>75.54%</td>\n    <td>-0.79%</td>\n    <td>3.31x</td>\n  </tr>\n  <tr>\n    <td>MXNet</td>\n    <td>resnet152_v1</td>\n    <td>ImageNet</td>\n    <td>78.32%</td>\n    <td>78.54%</td>\n    <td>-0.28%</td>\n    <td>3.16x</td>\n  </tr>\n</tbody>\n</table>\n\n\n# Known Issues\n\nThe MSE tuning strategy does not work with the PyTorch adaptor layer. This strategy requires a comparison between the FP32 and INT8 tensors to decide which op impacts the final quantization accuracy. The PyTorch adaptor layer does not implement this inspect tensor interface. Therefore, do not choose the MSE tuning strategy for PyTorch models.\n\n# Support\n\nSubmit your questions, feature requests, and bug reports to the\n[GitHub issues](https://github.com/intel/lpot/issues) page. You may also reach out to lpot.maintainers@intel.com.\n\n# Contribution\n\nWe welcome community contributions to Intel® Low Precision Optimization\nTool. If you have an idea on how to improve the library, refer to the following:\n\n* For changes impacting the public API, submit an [RFC pull request](CONTRIBUTING.md#RFC_pull_requests).\n* Ensure that the changes are consistent with the [code contribution guidelines](CONTRIBUTING.md#code_contribution_guidelines) and [coding style](CONTRIBUTING.md#coding_style).\n* Ensure that you can run all the examples with your patch.\n* Submit a [pull request](https://github.com/intel/lpot/pulls).\n\nFor additional details, see [contribution guidelines](CONTRIBUTING.md).\n\nThis project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the [Contributor Covenant](CODE_OF_CONDUCT.md) code of conduct.\n\n# License\n\nIntel® Low Precision Optimization Tool is licensed under [Apache License Version 2.0](http://www.apache.org/licenses/LICENSE-2.0). This software includes components that have separate copyright notices and licensing terms. Your use of the source code for these components is subject to the terms and conditions of the following licenses.\n\nApache License Version 2.0:\n* [Intel TensorFlow Quantization Tool](https://github.com/IntelAI/tools)\n\nMIT License:\n* [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)\n\nSee the accompanying [LICENSE](LICENSE) file for full license text and copyright notices.\n\n--------\n\nView [Legal Information](legal_information.md).\n\n## Citation\n\nIf you use Intel® Low Precision Optimization Tool in your research or you wish to refer to the tuning results published in the [Model Zoo](#model-zoo), use the following BibTeX entry.\n\n```\n@misc{Intel® Low Precision Optimization Tool,\n  author =       {Feng Tian, Chuanqi Wang, Guoming Zhang, Penghui Cheng, Pengxin Yuan, Haihao Shen, and Jiong Gong},\n  title =        {Intel® Low Precision Optimization Tool},\n  howpublished = {\\url{https://github.com/intel/lpot}},\n  year =         {2020}\n}\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/intel/lpot",
    "keywords": "quantization,auto-tuning,post-training static quantization,post-training dynamic quantization,quantization-aware training,tuning strategy",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lpot",
    "package_url": "https://pypi.org/project/lpot/",
    "platform": "",
    "project_url": "https://pypi.org/project/lpot/",
    "project_urls": {
      "Homepage": "https://github.com/intel/lpot"
    },
    "release_url": "https://pypi.org/project/lpot/1.1/",
    "requires_dist": [
      "numpy",
      "pyyaml",
      "scikit-learn",
      "schema",
      "py-cpuinfo",
      "hyperopt",
      "pandas (==1.1.5)",
      "pycocotools",
      "scikit-image (==0.17.2)"
    ],
    "requires_python": ">=3.5.0",
    "summary": "Repository of Intel® Low Precision Optimization Tool",
    "version": "1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11239500,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4b53f07aa408333151ba605818a0f774ac2a95154b6462fb9887c8b81c0312e3",
        "md5": "c03b344f2d43a875e4c6a9246278d013",
        "sha256": "ee615e47933e7265db938f3766597a301adf58be3fa008ab7b6aaef17970da3a"
      },
      "downloads": -1,
      "filename": "lpot-1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c03b344f2d43a875e4c6a9246278d013",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5.0",
      "size": 326411,
      "upload_time": "2020-12-31T14:37:40",
      "upload_time_iso_8601": "2020-12-31T14:37:40.907096Z",
      "url": "https://files.pythonhosted.org/packages/4b/53/f07aa408333151ba605818a0f774ac2a95154b6462fb9887c8b81c0312e3/lpot-1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}