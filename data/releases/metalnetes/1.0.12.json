{
  "info": {
    "author": "Jay Johnson",
    "author_email": "jay.p.h.johnson@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: PyPy",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "Metalnetes\n==========\n\nTools for managing multiple Kubernetes **1.14** clusters on KVM (3 CentOS 7 VMs) running on a bare metal Fedora 29 server (also tested on Ubuntu 18.04 until 1.13). Use this repo if you want to create, destroy and manage native Kubernetes clusters. It is a full installer for taking a brand new server up to speed using just bash and environment variables.\n\n.. image:: https://i.imgur.com/awLwim1.png\n\nGetting Started\n---------------\n\nThis `repo <https://github.com/jay-johnson/metalnetes>`__ automates:\n\n- installing many rpms, systems and tools to prepare a bare metal server (on Fedora 29) to host multiple Kubernetes clusters\n- deploying Kubernetes clusters on 3 CentOS 7 VMs\n- each VM has 100 GB, 4 cpu cores, 16 GB ram, and auto-configured for static IP assignment from env vars\n- hosting VMs using KVM which requires access to the server's hypervisor (running this in a vm will not work)\n- deploying a Rook Ceph storage cluster for Kubernetes persistent volumes\n- installs a local dns server (named) with working example for mapping VM static ips to urls that a browser can use with any Kubernetes nginx ingress endpoint\n- ssh access for manually fixing a VM after deployment\n- optional - deploy the `Stock Analysis Engine <https://stock-analysis-engine.readthedocs.io/en/latest/>`__ which includes helm charts for running: Minio (on-premise s3), Redis cluster, Jupyter, Grafana + Prometheus for monitoring (required for ceph cluster monitoring)\n\nFedora Bare Metal Install Guide\n===============================\n\nServer Resource Requirements\n----------------------------\n\nMinimum hardware specs to run 1 cluster:\n\n- 50 GB RAM\n- 12 cpu cores\n- 500 GB hdd space for each cluster (400 GB if you do not want to use base images and slow down each cluster deployment)\n\nClone\n-----\n\n::\n\n    git clone https://github.com/jay-johnson/metalnetes.git\n    cd metalnetes\n\nEdit Cluster Configuration\n==========================\n\nPlease edit the default `Cluster Config k8.env <https://github.com/jay-johnson/metalnetes/blob/master/k8.env>`__ as needed\n\nLaunch Checklist\n----------------\n\nUninstalling and reinstalling clusters is not a slow process, and it helps to take a moment to review the VM's networking, Kubernetes cluster deployment, and KVM configuration before starting or testing a new idea for your next cluster deployment:\n\n#.  `Set a name for the cluster <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L4-L8>`__\n#.  KVM\n\n    #.  `K8_VMS <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L100>`__ - short VM names for showing in ``virsh list`` and must be unique\n    #.  `K8_DOMAIN <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L101>`__ - search domain for cluster ``example.com`` and must work with the dns server records and VM ip addresses\n    #.  `K8_INITIAL_MASTER <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L102>`__ - initial fqdn to set ``m10.example.com``\n    #.  `K8_SECONDARY_MASTERS <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L103>`__ - additional fqdns to set ``m11.example.com m12.example.com`` and space separated\n#.  Networking\n\n    #.  `Confirm VM IP Addresses <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L105-L108>`__\n    #.  `Confirm VM MAC Addresses <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L109-L111>`__\n    #.  `Confirm DNS <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L104>`__\n#.  `Confirm User For Private Docker Registry <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L35-L39>`__\n#.  `Confirm User For SSH Access to VMs <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L118-L119>`__\n#.  `Confirm CPU Per VM (4 cores) <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L116>`__\n#.  `Confirm Memory Per VM (16 GB ram) <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L117>`__\n#.  `Confirm Storage Per VM (100 GB harddrives and qemu raw image format) <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L114>`__\n#.  `Confirm Cluster Storage (rook-ceph by default) <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L57-L60>`__\n#.  `Confirm Ingress (nginx by default) <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L91-L94>`__\n#.  `Confirm Bridge (br0 by default) <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L115>`__\n#.  `Confirm Base VM IP and Mac Address <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L208-L209>`__\n#.  `Confirm Base VM Allow Query DNS CIDR <https://github.com/jay-johnson/metalnetes/blob/master/k8.env#L205>`__\n\nStart Install\n=============\n\nChange to root and start the Fedora bare metal server installer:\n\n::\n\n    sudo su\n    ./fedora/server-install.sh\n\nInstall Bridge\n==============\n\nThis will install a bridge network device called ``br0`` from a network device ``eno1``. This ``br0`` bridge is used by KVM as a shared networking device for all VMs in all Kubernetes clusters.\n\n::\n\n    ./fedora/install-bridge.sh\n\nI am not sure this is required, but I reboot the server at this point. This ensures the OS reboots correctly before creating any VMs, and I can confirm the ``br0`` bridge shows up after a clean restart using ``ifconfig -a | grep br0`` or ``nmcli dev | grep br0``.\n\nStart the Kubernetes Cluster\n============================\n\nBoot your cluster as your user (which should have KVM access). The `boot.sh <https://github.com/jay-johnson/metalnetes/blob/master/boot.sh>`__ uses a base VM to bootstrap and speed up future deployments. Once the base VM is built, it will copy and launch 3 VMs (from the base) and install the latest Kubernetes build in all VMs. Once installed and running the 2nd and 3rd nodes join the 1st node to initialize the cluster. After initializing the cluster, helm and tiller will install and a rook-ceph storage layer will be deployed for persisting your data in volumes:\n\n.. note:: Initial benchmarks take around 30 minutes to build all VMs and bring a new cluster online. Cleaning and restarting the cluster does not take nearly as long as creating VMs for a new cluster. Also the first time running ``./boot.sh`` will take the longest because it builds a shared base VM image to decrease future cluster deploy time.\n\n::\n\n    # go to the base of the repo\n    source k8.env\n    ./boot.sh\n\nFor help with issues please refer to the `FAQ <https://github.com/jay-johnson/metalnetes#faq>`__\n\nView Kubernetes Nodes\n---------------------\n\nOnce it finishes you can view your new cluster nodes with:\n\n::\n\n    ./tools/show-nodes.sh\n\nChanging Between Kubernetes Clusters\n====================================\n\nIf you create a new ``k8.env`` file for each cluster, like ``dev_k8.env`` and ``prod_k8.env`` then you can then quickly toggle between clusters using:\n\n#.  Load ``dev`` Cluster Config file\n\n    ::\n\n        source dev_k8.env\n\n#.  Use the ``metal`` bash function to sync the ``KUBECONFIG`` through the ``dev`` cluster and local host\n\n    ::\n\n        metal\n\n#.  Load ``prod`` Cluster Config file\n\n    ::\n\n        source prod_k8.env\n\n#.  Use the ``metal`` bash function to sync the ``KUBECONFIG`` through the ``prod`` cluster and local host\n\n    ::\n\n        metal\n\nCustomizing the Kubernetes Cluster\n==================================\n\nIf you are looking to swap out parts of the deployment, please ensure the hosting server has a replacement in place for these bare minimum components:\n\n- a dns server that can host the ``example.com`` zone\n- access to a docker-ce daemon (latest stable)\n- a private docker registry\n- KVM (requires **hypervisor** access)\n- a network device that supports static bridging for KVM (please review the ``centos/install-network-device.sh`` for examples)\n- default static network ip assignment from a router or switch that can map a VM's MAC address to a static ip address that the dns server can map to for helping browsers access nginx ingress endpoints\n- access to arp-scan tool for detecting when each VM is ready for ssh scripting using dns name resolution\n\nBefore starting a second cluster there are some deployment sections to change from the default ``k8.env`` Cluster Config file.\n\nPlease review these sections to prevent debugging collision-related issues:\n\nVM and Kubernetes Node Configuration\n------------------------------------\n\n- `VM names, Cluster Nodes, Node Labels, Cluster Tools section <https://github.com/jay-johnson/metalnetes/blob/34c0eabf5f7007056a4823f5c4ea760aea7c8e6e/k8.env#L96-L194>`__\n\n**Considerations and Deployment Constraints**\n\n- ``K8_ENV`` must be a unique name for the cluster (``dev`` vs ``prod`` for example)\n- VM names need to be unique (and on the dns server with fqdn: ``VM_NAME.example.com`` as the default naming convention\n- IPs must be unique (or the dns server will have problems)\n- MAC addressess must be unique\n\nHelm and Tiller Configuration\n-----------------------------\n\n- `Helm and Tiller <https://github.com/jay-johnson/metalnetes/blob/34c0eabf5f7007056a4823f5c4ea760aea7c8e6e/k8.env#L48-L55>`__\n\nCluster Storage Configuation\n----------------------------\n\n**Considerations and Deployment Constraints**\n\n- Operator redundancy\n\n- `Storage (rook-ceph by default) <https://github.com/jay-johnson/metalnetes/blob/34c0eabf5f7007056a4823f5c4ea760aea7c8e6e/k8.env#L57-L65>`__\n- `Additional Block Devices per VM <https://github.com/jay-johnson/metalnetes/blob/34c0eabf5f7007056a4823f5c4ea760aea7c8e6e/k8.env#L178-L188>`__\n\nPrivate Docker Registry\n-----------------------\n\nPlease export the address to your private docker registy before deploying with format:\n\n::\n\n    export DOCKER_REGISTRY_FQDN=REGISTRY_HOST:PORT\n\n- `Registry <https://github.com/jay-johnson/metalnetes/blob/34c0eabf5f7007056a4823f5c4ea760aea7c8e6e/k8.env#L35-L46>`__\n\nManaging a Running Kubernetes Cluster\n=====================================\n\nRun these steps to manage a running kubernetes cluster.\n\nLoad the CLUSTER_CONFIG environment\n-----------------------------------\n\n::\n\n    # from within the repo's root dir:\n    export CLUSTER_CONFIG=$(pwd)/k8.env\n\nFully Clean and Reinitialize the Kubernetes Cluster\n---------------------------------------------------\n\n::\n\n    ./clean.sh\n\nStart Kubernetes Cluster with a Private Docker Registry + Rook Ceph\n-------------------------------------------------------------------\n\n::\n\n    ./start.sh\n\nCheck Kubernetes Nodes\n----------------------\n\n::\n\n    ./tools/show-labels.sh\n\nCluster Join Tool\n=================\n\nIf you want to reboot VMs and have the nodes re-join and rebuild the Kubernetes cluster use:\n\n::\n\n    ./join.sh\n\nDeployment Tools\n================\n\nNginx Ingress\n-------------\n\nDeploy `the nginx ingress <https://github.com/nginxinc/kubernetes-ingress/>`__\n\n::\n\n    ./deploy-nginx.sh\n\nRook-Ceph\n---------\n\nDeploy `rook-ceph <https://rook.io/docs/rook/v0.9/ceph-quickstart.html>`__ using the `Advanced Configuration <https://rook.io/docs/rook/v0.9/advanced-configuration.html>`__\n\n::\n\n    ./deploy-rook-ceph.sh\n\nConfirm Rook-Ceph Operator Started\n\n::\n\n    ./rook-ceph/describe-operator.sh\n\nPrivate Docker Registry\n-----------------------\n\nDeploy a private docker registry for use with the cluster with:\n\n::\n\n    ./deploy-registry.sh\n\nDeploy Helm\n-----------\n\nDeploy `helm <https://helm.sh/docs/>`__\n\n::\n\n    ./deploy-helm.sh\n\nDeploy Tiller\n-------------\n\nDeploy tiller:\n\n::\n\n    ./deploy-tiller.sh\n\n(Optional Validation) - Deploy Stock Analysis Engine\n====================================================\n\nThis repository was created after trying to decouple the `AI Kubernetes cluster for analyzing network traffic <https://github.com/jay-johnson/deploy-to-kubernetes>`__ and the `Stock Analysis Engine (ae) that uses many deep neural networks to predict future stock prices during live-trading hours <https://github.com/AlgoTraders/stock-analysis-engine>`__ from using the same Kubernetes cluster. Additionally with the speed ae is moving, I am looking to keep trying new high availablity solutions and configurations to ensure the intraday data collection never dies (hopefully out of the box too!).\n\nDeploy AE\n---------\n\n- `Configure AE <https://github.com/jay-johnson/metalnetes/blob/34c0eabf5f7007056a4823f5c4ea760aea7c8e6e/k8.env#L67-L89>`__\n\n::\n\n    ./deploy-ae.sh\n\nRedeploying Using Helm\n----------------------\n\n#.  Find the Helm Chart to Remove (this example uses ``ae-grafana``):\n\n    ::\n\n        helm ls ae-grafana\n\n#.  Delete and Purge the Helm Chart Deployment:\n\n    ::\n\n        helm delete --purge ae-grafana\n\n#.  Deploy AE Helm Charts:\n\n    ::\n\n        ./ae/start.sh\n\nMonitoring the Kubernetes Cluster\n---------------------------------\n\n.. note:: Grafana will only deploy if monitoring is enabled when running ``./deploy-ae.sh`` or if you run ``./ae/monitor-start.sh``.\n\nLog in to Grafana from a browser:\n\n- Username: **trex**\n- Password: **123321**\n\nhttps://grafana.example.com\n\nGrafana comes ready-to-go with these starting dashboards:\n\nView Kubernetes Pods in Grafana\n-------------------------------\n\n.. image:: https://i.imgur.com/GHo7dbd.png\n\nView Rook Ceph Cluster in Grafana\n----------------------------------\n\n.. image:: https://i.imgur.com/wptrQW2.png\n\nView Redis Cluster in Grafana\n-----------------------------\n\n.. image:: https://i.imgur.com/kegYzXZ.png\n\nUninstall AE\n------------\n\n::\n\n    ./ae/_uninstall.sh\n\nPlease wait for the Persistent Volume Claims to be deleted\n\n::\n\n    kubetl get pvc -n ae\n\n.. warning:: The Redis pvc ``redis-data-ae-redis-master-0`` must be manually deleted to prevent issues with redeployments after an uninstall\n    ::\n\n        kubectl -n ae delete pvc redis-data-ae-redis-master-0\n\nDelete Cluster VMs\n==================\n\n::\n\n    ./kvm/_uninstall.sh\n\nLicense\n=======\n\nApache 2.0 - Please refer to the `LICENSE <https://github.com/jay-johnson/metalnetes/blob/master/LICENSE>`__ for more details.\n\nFAQ\n===\n\nWhat IP did my VMs get?\n-----------------------\n\nFind VMs by MAC address using the ``K8_VM_BRIDGE`` bridge device using:\n\n::\n\n    ./kvm/find-vms-on-bridge.sh\n\nFind your MAC addresses with a tool that uses ``arp-scan`` to list all ip addresses on the configured bridge device (``K8_VM_BRIDGE``):\n\n::\n\n    ./kvm/list-bridge-ips.sh\n\nWhy Are Not All Rook Ceph Operators Starting?\n---------------------------------------------\n\nRestart the cluster if you see an error like this when looking at the ``rook-ceph-operator``:\n\n::\n\n    # find pods: kubectl get pods -n rook-ceph-system | grep operator\n    kubectl -n rook-ceph-system describe po rook-ceph-operator-6765b594d7-j56mw\n\n::\n\n    Warning  FailedCreatePodSandBox  7m56s                   kubelet, m12.example.com  Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container \"9ab1c663fc53f75fa4f0f79effbb244efa9842dd8257eb1c7dafe0c9bad1ee6c\" network for pod \"rook-ceph-operator-6765b594d7-j56mw\": NetworkPlugin cni failed to set up pod \"rook-ceph-operator-6765b594d7-j56mw_rook-ceph-system\" network: failed to set bridge addr: \"cni0\" already has an IP address different from 10.244.2.1/24\n\n::\n\n    ./clean.sh\n    ./deploy-rook-ceph.sh\n\nHelm fails with connection refused\n----------------------------------\n\nIf you see this:\n\n::\n\n    metalnetes$ helm ls\n    Error: Get http://localhost:8080/api/v1/namespaces/kube-system/pods?labelSelector=app%3Dhelm%2Cname%3Dtiller: dial tcp 127.0.0.1:8080: connect: connection refused\n\nSource the ``k8.env`` Cluster Config file:\n\n::\n\n    metalnetes$ source k8.env\n    metalnetes$ helm ls\n    NAME         \tREVISION\tUPDATED                 \tSTATUS  \tCHART           \tAPP VERSION\tNAMESPACE\n    ae           \t1       \tThu Mar 21 05:49:38 2019\tDEPLOYED\tae-0.0.1        \t0.0.1      \tae\n    ae-grafana   \t1       \tThu Mar 21 05:57:17 2019\tDEPLOYED\tgrafana-2.2.0   \t6.0.0      \tae\n    ae-jupyter   \t1       \tThu Mar 21 05:49:43 2019\tDEPLOYED\tae-jupyter-0.0.1\t0.0.1      \tae\n    ae-minio     \t1       \tThu Mar 21 05:49:40 2019\tDEPLOYED\tminio-2.4.7     \t2019-02-12 \tae\n    ae-prometheus\t1       \tThu Mar 21 05:57:16 2019\tDEPLOYED\tprometheus-8.9.0\t2.8.0      \tae\n    ae-redis     \t1       \tThu Mar 21 05:49:42 2019\tDEPLOYED\tredis-6.4.2     \t4.0.14     \tae\n\nComparing Repo Example Files vs Yours\n-------------------------------------\n\nWhen starting a server from scratch, I like to compare notes from previous builds. I have uploaded the Fedora 29 server's files to help debug common initial installer-type issues. Let me know if you think another one should be added to help others. Please take a moment to compare your server's configured files after the install finishes by looking at the `fedora/etc directory <https://github.com/jay-johnson/metalnetes/tree/master/fedora/etc>`__ with structure and notes:\n\n::\n\n    tree fedora/etc/\n    fedora/etc/\n    ├── dnsmasq.conf # dnsmasq that was conflicting with named later (http://www.thekelleys.org.uk/dnsmasq/doc.html) - dnsqmasq was disabled and stopped on the server using systemctl\n    ├── docker\n    │   └── daemon.json # examples for setting up your private docker registry\n    ├── named.conf\n    ├── NetworkManager\n    │   └── NetworkManager.conf # this is enabled and running using systemctl\n    ├── resolv.conf # locked down with: sudo chattr +i /etc/resolv.conf\n    ├── resolv.dnsmasq\n    ├── ssh\n    │   └── sshd_config # initial ssh config for logging in remotely as fast as possible - please lock this down after install finishes\n    ├── sysconfig\n    │   └── network-scripts\n    │       ├── ifcfg-br0 # bridge network device - required for persisting through a reboot\n    │       └── ifcfg-eno1 # server network device - required for persisting through a reboot\n    └── var\n        └── named\n            └── example.com.zone # dns zone\n\nHow do I know when my VMs have an IP address?\n---------------------------------------------\n\nI use this bash alias in my ``~/.bashrc`` to monitor VMs on the ``br0`` device:\n\n::\n\n    showips() {\n        watch -n1 'sudo arp-scan -q -l --interface br0 | sort'\n    }\n\nThen ``source ~/.bashrc`` and then run: ``showips`` to watch everything on the ``br0`` bridge networking device with each IP's MAC address. (Exit with ``ctrl + c``)\n\nManually Fix Fedora /etc/resolv.conf\n------------------------------------\n\nNetworkManager and dnsmasq had lots of conflicts initially. I used this method to **lock down** ``/etc/resolv.conf`` to ensure the dns routing was stable after reboots.\n\n::\n\n    sudo su\n    nmcli connection modify eth0 ipv4.dns \"192.168.0.100 8.8.8.8 8.8.4.4\"\n    vi /etc/resolv.conf\n    chattr +i /etc/resolv.conf\n    systemctl restart NetworkManager\n\nWhat do I do when Rook-Ceph Uninstall Hangs?\n--------------------------------------------\n\nThe `rook-ceph operator <https://rook.io/docs/rook/v0.9/helm-operator.html>`__ runs outside of Kubernetes on the nodes. Because this runs outside Kubernetes it can get into bad states requiring vm deletes and recreation for issues around server reboots. Please a open a PR if you know how to fix this uninstall issues.\n\nHere's `Rook-Ceph Troubleshooting Guide as well <https://github.com/rook/rook/blob/master/Documentation/ceph-teardown.md>`__\n\nWhen I hit issues like below where there are pids that never die and are outside Kubernetes, I just destroy and recreate the vms with: ``./kvm/_uninstall.sh; sleep 10; ./boot.sh``\n\n::\n\n    root@m11:~# ps auwwx | grep ceph | grep -v grep\n    root     14571  0.0  0.0      0     0 ?        S<   22:59   0:00 [ceph-watch-noti]\n    root     17532  0.0  0.0 123532   844 ?        D    22:59   0:00 /usr/bin/mount -t xfs -o rw,defaults /dev/rbd1 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph-system/mounts/pvc-9aaa30e5-535e-11e9-9fb8-0010019c9110\n    root     19537  0.0  0.0      0     0 ?        S<   22:58   0:00 [ceph-msgr]\n    root@m11:~# kill -9 17532\n    root@m11:~# kill -9 14571\n    root@m11:~# kill -9 19537\n    root@m11:~# kill -9 $(ps auwwx | grep ceph | grep -v grep | awk '{print $2}')\n    root@m11:~# ps auwwx | grep ceph | grep -v grep\n    root     14571  0.0  0.0      0     0 ?        S<   22:59   0:00 [ceph-watch-noti]\n    root     17532  0.0  0.0 123532   844 ?        D    22:59   0:00 /usr/bin/mount -t xfs -o rw,defaults /dev/rbd1 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph-system/mounts/pvc-9aaa30e5-535e-11e9-9fb8-0010019c9110\n    root     19537  0.0  0.0      0     0 ?        S<   22:58   0:00 [ceph-msgr]\n    root@m11:~#\n\nKubeadm Reset or Deleting /var/lib/kubelet Hangs Forever\n--------------------------------------------------------\n\nPlease review the official guide for help:\nhttps://kubernetes.io/docs/setup/independent/troubleshooting-kubeadm/#kubeadm-blocks-when-removing-managed-containers\n\nHere is a stack trace seen running ``dmesg``:\n\n::\n\n    [  841.081661] INFO: task alertmanager:27274 blocked for more than 120 seconds.\n    [  841.086662] \"echo 0 > /proc/sys/kernel/hung_task_timeout_secs\" disables this message.\n    [  841.090801] alertmanager    D ffff9cd1d3e38640     0 27274  27158 0x00000084\n    [  841.094932] Call Trace:\n    [  841.096929]  [<ffffffffae82a621>] ? __switch_to+0x151/0x580\n    [  841.100066]  [<ffffffffaef68c49>] schedule+0x29/0x70\n    [  841.102759]  [<ffffffffaef66721>] schedule_timeout+0x221/0x2d0\n    [  841.105984]  [<ffffffffaeb47414>] ? blk_finish_plug+0x14/0x40\n    [  841.109079]  [<ffffffffaef68ffd>] wait_for_completion+0xfd/0x140\n    [  841.112144]  [<ffffffffae8d67f0>] ? wake_up_state+0x20/0x20\n    [  841.115060]  [<ffffffffc03235a3>] ? _xfs_buf_read+0x23/0x40 [xfs]\n    [  841.118251]  [<ffffffffc03234a9>] xfs_buf_submit_wait+0xf9/0x1d0 [xfs]\n    [  841.121630]  [<ffffffffc03540d1>] ? xfs_trans_read_buf_map+0x211/0x400 [xfs]\n    [  841.125157]  [<ffffffffc03235a3>] _xfs_buf_read+0x23/0x40 [xfs]\n    [  841.128128]  [<ffffffffc03236b9>] xfs_buf_read_map+0xf9/0x160 [xfs]\n    [  841.131806]  [<ffffffffc03540d1>] xfs_trans_read_buf_map+0x211/0x400 [xfs]\n    [  841.135206]  [<ffffffffc0312edd>] xfs_read_agi+0x9d/0x130 [xfs]\n    [  841.138502]  [<ffffffffc0312fa4>] xfs_ialloc_read_agi+0x34/0xd0 [xfs]\n    [  841.141801]  [<ffffffffc0313671>] xfs_ialloc_pagi_init+0x31/0x70 [xfs]\n    [  841.145038]  [<ffffffffc031383f>] xfs_ialloc_ag_select+0x18f/0x220 [xfs]\n    [  841.148463]  [<ffffffffc031395f>] xfs_dialloc+0x8f/0x280 [xfs]\n    [  841.151466]  [<ffffffffc0334131>] xfs_ialloc+0x71/0x520 [xfs]\n    [  841.154291]  [<ffffffffc03438e4>] ? xlog_grant_head_check+0x54/0x100 [xfs]\n    [  841.157607]  [<ffffffffc03366f3>] xfs_dir_ialloc+0x73/0x1f0 [xfs]\n    [  841.160588]  [<ffffffffaef67f32>] ? down_write+0x12/0x3d\n    [  841.163299]  [<ffffffffc0336d08>] xfs_create+0x498/0x750 [xfs]\n    [  841.166212]  [<ffffffffc0333cf0>] xfs_generic_create+0xd0/0x2b0 [xfs]\n    [  841.169452]  [<ffffffffc0333f04>] xfs_vn_mknod+0x14/0x20 [xfs]\n    [  841.172378]  [<ffffffffc0333f43>] xfs_vn_create+0x13/0x20 [xfs]\n    [  841.175338]  [<ffffffffaea4e9b3>] vfs_create+0xd3/0x140\n    [  841.178045]  [<ffffffffaea50a8d>] do_last+0x10cd/0x12a0\n    [  841.180818]  [<ffffffffaeb0278c>] ? selinux_file_alloc_security+0x3c/0x60\n    [  841.184077]  [<ffffffffaea52a67>] path_openat+0xd7/0x640\n    [  841.186864]  [<ffffffffaea5446d>] do_filp_open+0x4d/0xb0\n    [  841.189668]  [<ffffffffaea61af7>] ? __alloc_fd+0x47/0x170\n    [  841.192394]  [<ffffffffaea40597>] do_sys_open+0x137/0x240\n    [  841.195172]  [<ffffffffaef75d15>] ? system_call_after_swapgs+0xa2/0x146\n    [  841.198288]  [<ffffffffaea406d4>] SyS_openat+0x14/0x20\n    [  841.201016]  [<ffffffffaef75ddb>] system_call_fastpath+0x22/0x27\n    [  841.203957]  [<ffffffffaef75d21>] ? system_call_after_swapgs+0xae/0x146\n\nAdditionally there are times where the user is prevented from manually deleting the ``/var/lib/kubelet/`` directory (likely due to some mounted volume) and this hangs the ssh session where even ``ctrl + c`` fails to stop it:\n\n::\n\n    root@m10:~# rm -rf /var/lib/kubelet/*\n    ^C^C^C\n\n\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/jay-johnson/metalnetes",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "metalnetes",
    "package_url": "https://pypi.org/project/metalnetes/",
    "platform": "",
    "project_url": "https://pypi.org/project/metalnetes/",
    "project_urls": {
      "Homepage": "https://github.com/jay-johnson/metalnetes"
    },
    "release_url": "https://pypi.org/project/metalnetes/1.0.12/",
    "requires_dist": [
      "awscli",
      "boto3",
      "bs4",
      "celery",
      "celery[redis]",
      "colorlog",
      "coverage",
      "flake8 (<=3.4.1)",
      "future",
      "mock",
      "pep8 (>=1.7.1)",
      "pycodestyle (<=2.3.1)",
      "pylint",
      "recommonmark",
      "redis (<3)",
      "sphinx",
      "sphinx-autobuild",
      "sphinx-rtd-theme",
      "tabulate",
      "unittest2",
      "urllib3 (<=1.23)",
      "ujson"
    ],
    "requires_python": "",
    "summary": "Tools for managing multiple Kubernetes clusters on KVM (on 3 Centos 7 VMs) running on a bare metal Fedora 29 server (Ubuntu 18.04 was tested until Kubernetes v1.14)",
    "version": "1.0.12",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 5122066,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5d836ee4a37c38a11f52b283e67c90795ea4569aecbb870e5b569f1b87bb4d14",
        "md5": "3b8fa7b00bc96a70446d4c5d13f3a4d2",
        "sha256": "4ad02a4b39d52e2f88f93833cf566838010765ccd8a00f607309ac63b7038a4e"
      },
      "downloads": -1,
      "filename": "metalnetes-1.0.12-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3b8fa7b00bc96a70446d4c5d13f3a4d2",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 14056,
      "upload_time": "2019-04-01T00:16:19",
      "upload_time_iso_8601": "2019-04-01T00:16:19.676055Z",
      "url": "https://files.pythonhosted.org/packages/5d/83/6ee4a37c38a11f52b283e67c90795ea4569aecbb870e5b569f1b87bb4d14/metalnetes-1.0.12-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "509d9c9fe2f0dcdba389300ce5d3f5edee60c90b3c62f4563adaa3f27d420a3d",
        "md5": "5ac0abf5326b46173a43e9af665fc084",
        "sha256": "aad47dcd0fb66d38305a19147f90aab7fbe743f65f2ec2eb547a6b9dbf0ab7a4"
      },
      "downloads": -1,
      "filename": "metalnetes-1.0.12.tar.gz",
      "has_sig": false,
      "md5_digest": "5ac0abf5326b46173a43e9af665fc084",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 19511,
      "upload_time": "2019-04-01T00:16:21",
      "upload_time_iso_8601": "2019-04-01T00:16:21.493830Z",
      "url": "https://files.pythonhosted.org/packages/50/9d/9c9fe2f0dcdba389300ce5d3f5edee60c90b3c62f4563adaa3f27d420a3d/metalnetes-1.0.12.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}