{
  "info": {
    "author": "Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park",
    "author_email": "gwkim.rsrch@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Information Technology",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "<div align=\"center\">\n\n# Donut üç© : Document Understanding Transformer\n\n[![Paper](https://img.shields.io/badge/Paper-arxiv.2111.15664-red)](https://arxiv.org/abs/2111.15664)\n[![Conference](https://img.shields.io/badge/ECCV-2022-blue)](#how-to-cite)\n[![Demo](https://img.shields.io/badge/Demo-Gradio-brightgreen)](#demo)\n[![Demo](https://img.shields.io/badge/Demo-Colab-orange)](#demo)\n[![PyPI](https://img.shields.io/pypi/v/donut-python?color=green&label=pip%20install%20donut-python)](https://pypi.org/project/donut-python)\n[![Downloads](https://static.pepy.tech/personalized-badge/donut-python?period=total&units=international_system&left_color=grey&right_color=green&left_text=downloads)](https://pepy.tech/project/donut-python)\n\nOfficial Implementation of Donut and SynthDoG\n\n</div>\n\n## Introduction\n\n**Donut** üç©, **Do**cume**n**t **u**nderstanding **t**ransformer, is a new method of document understanding that utilizes an OCR-free end-to-end Transformer model. Donut does not require off-the-shelf OCR engines/APIs, yet it shows state-of-the-art performances on various visual document understanding tasks, such as visual document classification or information extraction (a.k.a. document parsing). \nIn addition, we present **SynthDoG** üê∂, **Synth**etic **Do**cument **G**enerator, that helps the model pre-training to be flexible on vairous languages and domains.\n\nOur academic paper, which describes our method in detail and provides full experimental results and analyses, can be found here:<br>\n> [**OCR-free Document Understanding Transformer**](https://arxiv.org/abs/2111.15664).<br>\n> [Geewook Kim](https://geewook.kim), [Teakgyu Hong](https://dblp.org/pid/183/0952.html), [Moonbin Yim](https://github.com/moonbings), [JeongYeon Nam](https://github.com/long8v), [Jinyoung Park](https://github.com/jyp1111), [Jinyeong Yim](https://jinyeong.github.io), [Wonseok Hwang](https://scholar.google.com/citations?user=M13_WdcAAAAJ), [Sangdoo Yun](https://sangdooyun.github.io), [Dongyoon Han](https://dongyoonhan.github.io), [Seunghyun Park](https://scholar.google.com/citations?user=iowjmTwAAAAJ). To appear at ECCV 2022.\n\n<!-- <img width=\"946\" alt=\"image\" src=\"misc/overview.png\"> -->\n\n## Pre-trained Models and Web Demos\n\nGradio web demos are available! [![Demo](https://img.shields.io/badge/Demo-Gradio-brightgreen)](#demo) [![Demo](https://img.shields.io/badge/Demo-Colab-orange)](#demo)\n<!-- |:--:|\n|![image](misc/screenshot_gradio_demos.png)| -->\n- You can run the demo with `./app.py` file.\n- Sample images are available at `./misc` and more receipt images are available at [CORD dataset link](https://huggingface.co/datasets/naver-clova-ix/cord-v2).\n- Web demos are available from the links in the following table.\n\n|Task|Sec/Img|Score|Trained Model|<div id=\"demo\">Demo</div>|\n|---|---|---|---|---|\n| [CORD](https://github.com/clovaai/cord) (Document Parsing)   |   0.7 /<br> 0.7 /<br> 1.2   |  93.9 /<br> 93.6 /<br> 93.5    | [donut-base-finetuned-cord-v2](https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2/tree/official) (1280) /<br> [donut-base-finetuned-cord-v1](https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v1/tree/official) (1280) /<br> [donut-base-finetuned-cord-v1-2560](https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v1-2560/tree/official) | [gradio space web demo](https://huggingface.co/spaces/naver-clova-ix/donut-base-finetuned-cord-v2),<br>[google colab demo](https://colab.research.google.com/drive/1o07hty-3OQTvGnc_7lgQFLvvKQuLjqiw?usp=sharing) |\n| [Train Ticket](https://github.com/beacandler/EATEN) (Document Parsing)   |   0.6   |  98.8    | [donut-base-finetuned-zhtrainticket](https://huggingface.co/naver-clova-ix/donut-base-finetuned-zhtrainticket/tree/official) | [google colab demo](https://colab.research.google.com/drive/16O-hMvGiXrYZnlXA_tfJ9_q760YcLoOj?usp=sharing) |\n| [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip) (Document Classification)     |  0.75   |   95.3      | [donut-base-finetuned-rvlcdip](https://huggingface.co/naver-clova-ix/donut-base-finetuned-rvlcdip/tree/official) | [google colab demo](https://colab.research.google.com/drive/1xUDmLqlthx8A8rWKLMSLThZ7oeRJkDuU?usp=sharing) |\n| [DocVQA Task1](https://rrc.cvc.uab.es/?ch=17) (Document VQA) |  0.78       | 67.5 | [donut-base-finetuned-docvqa](https://huggingface.co/naver-clova-ix/donut-base-finetuned-docvqa/tree/official) | [google colab demo](https://colab.research.google.com/drive/1Z4WG8Wunj3HE0CERjt608ALSgSzRC9ig?usp=sharing) |\n\nThe links to the pre-trained backbones are here:\n- [`donut-base`](https://huggingface.co/naver-clova-ix/donut-base/tree/official): trained with 64 A100 GPUs (~2.5 days), number of layers (encoder: {2,2,14,2}, decoder: 4), input size 2560x1920, swin window size 10, IIT-CDIP (11M) and SynthDoG (ECJK, 0.5M x 4).\n- [`donut-proto`](https://huggingface.co/naver-clova-ix/donut-proto/tree/official): (preliminary model) trained with 8 V100 GPUs (~5 days), number of layers (encoder: {2,2,18,2}, decoder: 4), input size 2048x1536, swin window size 8, and SynthDoG (EJK, 0.4M x 3).\n\nPlease see [our paper](#how-to-cite) for more details.\n\n## SynthDoG datasets\n\n<!-- ![image](misc/sample_synthdog.png) -->\n\nThe links to the SynthDoG-generated datasets are here:\n\n- [`synthdog-en`](https://huggingface.co/datasets/naver-clova-ix/synthdog-en): English, 0.5M.\n- [`synthdog-zh`](https://huggingface.co/datasets/naver-clova-ix/synthdog-zh): Chinese, 0.5M.\n- [`synthdog-ja`](https://huggingface.co/datasets/naver-clova-ix/synthdog-ja): Japanese, 0.5M.\n- [`synthdog-ko`](https://huggingface.co/datasets/naver-clova-ix/synthdog-ko): Korean, 0.5M.\n\nTo generate synthetic datasets with our SynthDoG, please see `./synthdog/README.md` and [our paper](#how-to-cite) for details.\n\n## Updates\n\n**_2022-07-20_** First Commit, We release our code, model weights, synthetic data and generator.\n\n## Software installation\n\n[![PyPI](https://img.shields.io/pypi/v/donut-python?color=green&label=pip%20install%20donut-python)](https://pypi.org/project/donut-python)\n[![Downloads](https://static.pepy.tech/personalized-badge/donut-python?period=total&units=international_system&left_color=grey&right_color=green&left_text=downloads)](https://pepy.tech/project/donut-python)\n\n```bash\npip install donut-python\n```\n\nor clone this repository and install the dependencies:\n```bash\ngit clone https://github.com/clovaai/donut.git\ncd donut/\nconda create -n donut_official python=3.7\nconda activate donut_official\npip install .\n```\n\nWe tested [donut](https://github.com/clovaai/donut) with:\n- [torch](https://github.com/pytorch/pytorch) == 1.11.0+cu113 \n- [torchvision](https://github.com/pytorch/vision) == 0.12.0+cu113\n- [pytorch-lightning](https://github.com/Lightning-AI/lightning) == 1.6.4\n- [transformers](https://github.com/huggingface/transformers) == 4.11.3\n- [timm](https://github.com/rwightman/pytorch-image-models) == 0.5.4\n\n## Getting Started\n\n### Data\n\nThis repository assumes the following structure of dataset:\n```bash\n> tree dataset_name\ndataset_name\n‚îú‚îÄ‚îÄ test\n‚îÇ   ‚îú‚îÄ‚îÄ metadata.jsonl\n‚îÇ   ‚îú‚îÄ‚îÄ {image_path0}\n‚îÇ   ‚îú‚îÄ‚îÄ {image_path1}\n‚îÇ             .\n‚îÇ             .\n‚îú‚îÄ‚îÄ train\n‚îÇ   ‚îú‚îÄ‚îÄ metadata.jsonl\n‚îÇ   ‚îú‚îÄ‚îÄ {image_path0}\n‚îÇ   ‚îú‚îÄ‚îÄ {image_path1}\n‚îÇ             .\n‚îÇ             .\n‚îî‚îÄ‚îÄ validation\n    ‚îú‚îÄ‚îÄ metadata.jsonl\n    ‚îú‚îÄ‚îÄ {image_path0}\n    ‚îú‚îÄ‚îÄ {image_path1}\n              .\n              .\n\n> cat dataset_name/test/metadata.jsonl\n{\"file_name\": {image_path0}, \"ground_truth\": \"{\\\"gt_parse\\\": {ground_truth_parse}, ... {other_metadata_not_used} ... }\"}\n{\"file_name\": {image_path1}, \"ground_truth\": \"{\\\"gt_parse\\\": {ground_truth_parse}, ... {other_metadata_not_used} ... }\"}\n     .\n     .\n```\n\n- The structure of `metadata.jsonl` file is in [JSON Lines text format](https://jsonlines.org), i.e., `.jsonl`. Each line consists of\n  - `file_name` : relative path to the image file.\n  - `ground_truth` : string format (json dumped), the dictionary contains either `gt_parse` or `gt_parses`. Other fields (metadata) can be added to the dictionary but will not be used.\n- `donut` interprets all tasks as a JSON prediction problem. As a result, all `donut` model training share a same pipeline. For training and inference, the only thing to do is preparing `gt_parse` or `gt_parses` for the task in format described below.\n\n#### For Document Classification\nThe `gt_parse` follows the format of `{\"class\" : {class_name}}`, for example, `{\"class\" : \"scientific_report\"}` or `{\"class\" : \"presentation\"}`.\n- Google colab demo is available [here](https://colab.research.google.com/drive/1xUDmLqlthx8A8rWKLMSLThZ7oeRJkDuU?usp=sharing). \n\n#### For Document Information Extraction\nThe `gt_parse` is a JSON object that contains full information of the document image, for example, the JSON object for a receipt may look like `{\"menu\" : [{\"nm\": \"ICE BLACKCOFFEE\", \"cnt\": \"2\", ...}, ...], ...}`.\n- More examples are available at [CORD dataset](https://huggingface.co/datasets/naver-clova-ix/cord-v2).\n- Google colab demo is available [here](https://colab.research.google.com/drive/1o07hty-3OQTvGnc_7lgQFLvvKQuLjqiw?usp=sharing).\n- Gradio web demo is available [here](https://huggingface.co/spaces/naver-clova-ix/donut-base-finetuned-cord-v2).\n\n#### For Document Visual Question Answering\nThe `gt_parses` follows the format of `[{\"question\" : {question_sentence}, \"answer\" : {answer_candidate_1}}, {\"question\" : {question_sentence}, \"answer\" : {answer_candidate_2}}, ...]`, for example, `[{\"question\" : \"what is the model name?\", \"answer\" : \"donut\"}, {\"question\" : \"what is the model name?\", \"answer\" : \"document understanding transformer\"}]`.\n- DocVQA Task1 has multiple answers, hence `gt_parses` should be a list of dictionary that contains a pair of question and answer.\n- Google colab demo is available [here](https://colab.research.google.com/drive/1Z4WG8Wunj3HE0CERjt608ALSgSzRC9ig?usp=sharing).\n\n#### For (Psuedo) Text Reading Task\nThe `gt_parse` looks like `{\"text_sequence\" : \"word1 word2 word3 ... \"}`\n- This task is also a pre-training task of Donut model.\n- You can use our **SynthDoG** üê∂ to generate synthetic images for the text reading task with proper `gt_parse`. See `./synthdog/README.md` for details.\n\n### Training\n\nThis is the configuration of Donut model training on [CORD](https://github.com/clovaai/cord) dataset used in our experiment. \nWe ran this with a single NVIDIA A100 GPU.\n\n```bash\npython train.py --config config/train_cord.yaml \\\n                --pretrained_model_name_or_path \"naver-clova-ix/donut-base\" \\\n                --dataset_name_or_paths '[\"naver-clova-ix/cord-v2\"]' \\\n                --exp_version \"test_experiment\"    \n  .\n  .                                                                                                                                                                                                                                         \nPrediction: <s_menu><s_nm>Lemon Tea (L)</s_nm><s_cnt>1</s_cnt><s_price>25.000</s_price></s_menu><s_total><s_total_price>25.000</s_total_price><s_cashprice>30.000</s_cashprice><s_changeprice>5.000</s_changeprice></s_total>\nAnswer: <s_menu><s_nm>Lemon Tea (L)</s_nm><s_cnt>1</s_cnt><s_price>25.000</s_price></s_menu><s_total><s_total_price>25.000</s_total_price><s_cashprice>30.000</s_cashprice><s_changeprice>5.000</s_changeprice></s_total>\nNormed ED: 0.0\nPrediction: <s_menu><s_nm>Hulk Topper Package</s_nm><s_cnt>1</s_cnt><s_price>100.000</s_price></s_menu><s_total><s_total_price>100.000</s_total_price><s_cashprice>100.000</s_cashprice><s_changeprice>0</s_changeprice></s_total>\nAnswer: <s_menu><s_nm>Hulk Topper Package</s_nm><s_cnt>1</s_cnt><s_price>100.000</s_price></s_menu><s_total><s_total_price>100.000</s_total_price><s_cashprice>100.000</s_cashprice><s_changeprice>0</s_changeprice></s_total>\nNormed ED: 0.0\nPrediction: <s_menu><s_nm>Giant Squid</s_nm><s_cnt>x 1</s_cnt><s_price>Rp. 39.000</s_price><s_sub><s_nm>C.Finishing - Cut</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>B.Spicy Level - Extreme Hot Rp. 0</s_price></s_sub><sep/><s_nm>A.Flavour - Salt & Pepper</s_nm><s_price>Rp. 0</s_price></s_sub></s_menu><s_sub_total><s_subtotal_price>Rp. 39.000</s_subtotal_price></s_sub_total><s_total><s_total_price>Rp. 39.000</s_total_price><s_cashprice>Rp. 50.000</s_cashprice><s_changeprice>Rp. 11.000</s_changeprice></s_total>\nAnswer: <s_menu><s_nm>Giant Squid</s_nm><s_cnt>x1</s_cnt><s_price>Rp. 39.000</s_price><s_sub><s_nm>C.Finishing - Cut</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>B.Spicy Level - Extreme Hot</s_nm><s_price>Rp. 0</s_price><sep/><s_nm>A.Flavour- Salt & Pepper</s_nm><s_price>Rp. 0</s_price></s_sub></s_menu><s_sub_total><s_subtotal_price>Rp. 39.000</s_subtotal_price></s_sub_total><s_total><s_total_price>Rp. 39.000</s_total_price><s_cashprice>Rp. 50.000</s_cashprice><s_changeprice>Rp. 11.000</s_changeprice></s_total>\nNormed ED: 0.039603960396039604                                                                                                                                  \nEpoch 29: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [01:49<00:00,  1.82it/s, loss=0.00327, exp_name=train_cord, exp_version=test_experiment]\n```\n\nSome important arguments:\n\n- `--config` : config file path for model training.\n- `--pretrained_model_name_or_path` : string format, model name in huggingface modelhub or local path.\n- `--dataset_name_or_paths` : string format (json dumped), list of dataset names in huggingface datasets or local paths.\n- `--result_path` : file path to save model outputs/artifacts.\n- `--exp_version` : used for experiment versioning. The output files are saved at `{result_path}/{exp_version}/*`\n\n### Test\n\nWith the trained model, test images and ground truth parses, you can get inference results and accuracy scores.\n\n```bash\npython test.py --dataset_name_or_path naver-clova-ix/cord-v2 --pretrained_model_name_or_path ./result/train_cord/test_experiment --save_path ./result/output.json\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:37<00:00,  2.67it/s]\n{'accuracies': [0.7778, 1.0, {...} , 0.9689], 'mean_accuracy': 0.9388447875172169} length : 100\n```\n\nSome important arguments:\n\n- `--dataset_name_or_path` : string format, the target dataset name in huggingface datasets or local path.\n- `--pretrained_model_name_or_path` : string format, the model name in huggingface modelhub or local path.\n- `--save_path`: file path to save predictions and scores.\n\n## How to Cite\nIf you find this work useful to you, please cite:\n```\n@article{kim2021donut,\n   title={OCR-free Document Understanding Transformer},\n   author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},\n   journal={arXiv preprint arXiv:2111.15664},\n   year={2021}\n}\n```\n\n## License\n\n```\nMIT license\n\nCopyright (c) 2022-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/clovaai/donut",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "donut-python",
    "package_url": "https://pypi.org/project/donut-python/",
    "platform": null,
    "project_url": "https://pypi.org/project/donut-python/",
    "project_urls": {
      "Homepage": "https://github.com/clovaai/donut"
    },
    "release_url": "https://pypi.org/project/donut-python/1.0.5/",
    "requires_dist": [
      "transformers (>=4.11.3)",
      "timm",
      "datasets[vision]",
      "pytorch-lightning (>=1.6.4)",
      "nltk",
      "sentencepiece",
      "zss",
      "sconf (>=0.2.3)"
    ],
    "requires_python": ">=3.7",
    "summary": "OCR-free Document Understanding Transformer",
    "version": "1.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15757952,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "36a20901c2b2bdfa61e1c5babc662e24f95bd90fc9e6d2c9beb6aff18dcc38b1",
        "md5": "24badcaa8b4bc57cf07bf4407c8f40cf",
        "sha256": "3f57cb28856594fc7b4192bb509967c67ac318addac389a1fee5075091778934"
      },
      "downloads": -1,
      "filename": "donut_python-1.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "24badcaa8b4bc57cf07bf4407c8f40cf",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 20495,
      "upload_time": "2022-08-04T14:42:02",
      "upload_time_iso_8601": "2022-08-04T14:42:02.718485Z",
      "url": "https://files.pythonhosted.org/packages/36/a2/0901c2b2bdfa61e1c5babc662e24f95bd90fc9e6d2c9beb6aff18dcc38b1/donut_python-1.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}