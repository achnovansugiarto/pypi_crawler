{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# model-serving\n\nFlask based python wrapper for deploying models as a REST based service based on \n\nðŸ’¡ Flask \n\nðŸ’¡ Gunicorn \n\nðŸ’¡ Protobuf3 (optional for schema validation)\n\nðŸ¥³ flask-caching \n \nðŸ¥³ prometheus metrics \n\nThe repo also contains examples of registering end points and a Makefile to run the service\n\n## Installation \n\n```\n pip install model-serving\n \n```\n\n## Project Structure \n\n```\naylien_model_serving\nâ”‚\n|--- requirements.txt\n|--- Makefile\nâ”‚   \nâ”‚\nâ””â”€â”€â”€app\nâ”‚    |-- app_factory.py\nâ”‚    |-- cached_app_factory.py       \nâ”‚      \nâ”‚   \nâ””â”€â”€â”€examples\n    â”‚-----example_schema.proto\n    |-----example_schema_pb2.py(autogenerated by protoc)\n    â”‚-----example_serving_handler.py\n    |-----example_serving_handler_cached.py\n```\n\n\n## How it works\n\n* It runs a web service on the given port (defaults to `8000`).\n* Any incoming request JSON will be passed to your `ServingHandler.process_request` \n* Your `ServingHandler.process_request` is expected to return a `json` \n* The request and response will be validated with a protobuf schema (optional)\n* This library wraps common service code, monitoring, exception handling, etc.\n \n## Usage\n\n1. Install this library as a dependency for whatever model you want to serve.\n2. Create a `ServingHandler` (see below for interface details).\n3. Run the make target ` make COMMAND_UNCACHED='ServingHandler.run_app()' example-service`   \n\n## Interfaces\n\nThe main interface to flask apps defined in [app_factory](aylien_model_serving/app_factory.py) is the `process_json` function.\nThis function expects to receive json input, optionally perform schema\nvalidation, then call the `callable_handler` function using each of the fields \nin the json object as a keyword argument to the function. The function is expected to \nreturn an object that can be parsed to json and sent as the response.\n\nThis design allows for a very simple but powerful interface that can easily make an endpoint \nout of just about any Python function.\n\n\n### Example Serving Handler\n\nThe example serving handler defined [here](aylien_model_serving/examples/example_serving_handler.py) does the following\n\n1. Defines a method predict_lang. For the purposes of this example, this returns a static prediction. Ideally would be the prediction \n   or classification from your model. \n2. Imports a protobuf3 generated .py schema file(only if you require the json message to be schema validated)\n3. Defines a function process_request that calls the wrapper function process_json with the callable from 1 and schema from 2 \n4. Registers process_request and its route mapping \n5. Repeat 1-4 for a (route, callable) pair if you have more than one service end point. \n\n```python\nimport examples.example_schema_pb2 as schema\nfrom aylien_model_serving.app_factory import FlaskAppWrapper, InvalidRequest\n\n\ndef predict_lang(text):\n    return \"en\", 0.71\n\n\ndef predict(title=None, body=None, enrichments=None):\n    if body is None:\n        body = enrichments[\"extracted\"][\"value\"][\"body\"]\n    if title is None and body is None:\n        raise InvalidRequest(\"Missing text\")\n    article_text = f\"{title} {body}\"\n    detected_lang, confidence = predict_lang(article_text)\n    return {\n        'language': detected_lang,\n        'confidence': confidence,\n        'error': 'Not an error',\n        'version': '0.0.1'\n    }\n\n\ndef process_request():\n    return FlaskAppWrapper.process_json(predict, schema=schema) \n\n\ndef run_app():\n    routes = [\n        {\n            \"endpoint\": \"/\",\n            \"callable\": process_request,\n            \"methods\": [\"POST\"]\n        }\n    ]\n    return FlaskAppWrapper.create_app(routes)\n```\nNote that the FlaskAppWrapper accepts a callable in the process_json , and if you'd like to load a classifier or model.bin in your memory you could modify it like below ðŸ‘‡\n\n```python\nclass ClassifyHandler:\n    def __init__(self):\n        self.classifier = Classifier() #this is the classifier to load , or a binary in local file storage\n\n    def __call__(self, text):\n        return self.classifier.predict(text)\n\n\ndef run_app():\n    classify_handler = ClassifyHandler()\n    routes = [\n        {\n            \"endpoint\": \"/classify\",\n            \"callable\": classify_handler,\n            \"methods\": [\"POST\"]\n        }\n    ]\n    return FlaskAppWrapper.create_app(routes)\n    \n ```   \n  \n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "model-serving",
    "package_url": "https://pypi.org/project/model-serving/",
    "platform": null,
    "project_url": "https://pypi.org/project/model-serving/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/model-serving/2.0.6/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Gunicorn Flask based library for serving ML Models, built by the ml-ops and science team at Aylien",
    "version": "2.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15064638,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8ad7faa7b78c118d8595293ce3193c51946c4eab53fcba454207ff04ed5639b2",
        "md5": "f9ec12a44bdeb58815afdddaf395815b",
        "sha256": "88f67e4b5e0c5bd5e61d07ea82a0790198f7d39bbd429abc87604dd41a428974"
      },
      "downloads": -1,
      "filename": "model-serving-2.0.6.tar.gz",
      "has_sig": false,
      "md5_digest": "f9ec12a44bdeb58815afdddaf395815b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 11744,
      "upload_time": "2022-09-12T08:35:51",
      "upload_time_iso_8601": "2022-09-12T08:35:51.628858Z",
      "url": "https://files.pythonhosted.org/packages/8a/d7/faa7b78c118d8595293ce3193c51946c4eab53fcba454207ff04ed5639b2/model-serving-2.0.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}