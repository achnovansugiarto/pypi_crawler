{
  "info": {
    "author": "Yeonghun Kang, Hyunsoo Park",
    "author_email": "dudgns1675@kaist.ac.kr, phs68660888@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "![](https://raw.githubusercontent.com/hspark1212/MOFTransformer/master/docs/source/assets/fig1.jpg)\n\n<p align=\"center\">\n <a href=\"https://hspark1212.github.io/MOFTransformer/\">\n     <img alt=\"Docs\" src=\"https://img.shields.io/badge/Docs-v1.1.2-brightgreen.svg?style=plastic\">\n </a>\n  <a href=\"https://pypi.org/project/moftransformer/\">\n     <img alt=\"PypI\" src=\"https://img.shields.io/badge/PyPI-v1.1.2-blue.svg?style=plastic&logo=PyPI\">\n </a>\n  <a href=\"https://doi.org/10.6084/m9.figshare.21155506.v2\">\n     <img alt=\"Figshare\" src=\"https://img.shields.io/badge/Figshare-v2-blue.svg?style=plastic&logo=figshare\">\n </a>\n <a href=\"https://chemrxiv.org/engage/chemrxiv/article-details/634fbf8a4a18764f58e9fda5\">\n     <img alt=\"DOI\" src=\"https://img.shields.io/badge/DOI-doi-organge.svg?style=plastic\">\n </a>\n <a href=\"https://pypi.org/project/moftransformer/\">\n     <img alt=\"Lincense\" src=\"https://img.shields.io/badge/License-MIT-lightgrey.svg?style=plastic\">\n </a>\n</p>\n\n# [MOFTransformer](https://hspark1212.github.io/MOFTransformer/index.html)\n\n This package provides universal transfer learing for metal-organic frameworks(MOFs) to construct structure-property relationships. `MOFTransformer` obtains state-of-the-art performance to predict accross various properties that include gas adsorption, diffusion, electronic properties regardless of gas types. Beyond its universal transfer learning capabilityies, it provides feature importance analysis from its attentions scores to capture chemical intution.\n\n## [Install](https://hspark1212.github.io/MOFTransformer/installation.html)\n\n### OS and hardware requirements\nThe package development version is tested on following systems:\n\nLinux : Ubuntu 20.04, 22.04\n\nFor optimal performance, we recommend running with GPUs\n\n### Depedencies\n```\npython>=3.8\n```\nGiven that MOFTransformer is based on pytorch, please install pytorch (>= 1.10.0) according to your environments.\n\n### Installation using PIP \n```\n$ pip install moftransformer\n```\nwhich should install in about 50 seconds.\n\n### Download the pretrained model (ckpt file)\n- you can download the pretrained model with 1 M hMOFs in [figshare](https://figshare.com/articles/dataset/MOFTransformer/21155506)\nor you can download with a command line:\n```\n$ moftransformer download pretrain_model\n```\n### (Optional) Download dataset for CoREMOF, QMOF\n- we've provide the dataset of MOFTransformer (i.e., atom-based graph embeddings and energy-grid embeddings) for CoREMOF, QMOF\n```\n$ moftransformer download coremof\n$ moftransformer download qmof\n```\n\n## [Getting Started](https://hspark1212.github.io/MOFTransformer/tutorial.html)\n\n1. At first, you can run `prepare_data` with 10 cifs in `moftransformer/examples/raw` directory.\n\nIn order to run `prepare_data`, you need to install `GRIDAY` to calculate energy grid.\nYou can download GRIDAY using command-line.\n\n```bash\n$ moftransformer install-griday\n```\n\nExample for running `prepare-data`\n\n```python\nfrom moftransformer.examples import example_path\nfrom moftransformer.utils import prepare_data\n\n# Get example path\nroot_cifs = example_path['root_cif']\nroot_dataset = example_path['root_dataset']\ndownstream = example_path['downstream']\n\ntrain_fraction = 0.7\ntest_fraction = 0.2\n\n# Run prepare data\nprepare_data(root_cifs, root_dataset, downstream=downstream, \n             train_fraciton=train_fraction, test_fraciton=test_fraction)\n```\n\n2. Fine-tune the pretrained MOFTransformer.\n```python\nimport moftransformer\nfrom moftransformer.examples import example_path\n\n# data root and downstream from example\nroot_dataset = example_path['root_dataset']\ndownstream = example_path['downstream']\nlog_dir = './logs/'\n\n# kwargs (optional)\nmax_epochs = 10\nbatch_size = 8\n\nmoftransformer.run(root_dataset, downstream, log_dir=log_dir, \n                   max_epochs=max_epochs, batch_size=batch_size,)\n```\nwhich will run in about 35 seconds.\n\n\n3. Visualize analysis of feature importance for the fine-tuned model.\n\ndownload finetuned-bandgap model before visualize.\n```bash\nmoftransformer download finetuned_model -o ./examples\n```\n\n```python\n%matplotlib widget\nfrom moftransformer.visualize import PatchVisualizer\nfrom moftransformer.examples import visualize_example_path\n\nmodel_path = \"examples/finetuned_bandgap.ckpt\" # or 'examples/finetuned_h2_uptake.ckpt'\ndata_path = visualize_example_path\ncifname = 'MIBQAR01_FSR'\n\nvis = PatchVisualizer.from_cifname(cifname, model_path, data_path)\nvis.draw_graph() # or vis.draw_grid()\n```\n\n## [Architecture](https://hspark1212.github.io/MOFTransformer/introduction.html)\n`MOFTransformer`is a multi-modal Transformer pre-trained with 1 million hypothetical MOFs so that it efficiently capture both local and global feeatures of MOFs.\n\n- `MOFformer` takes two different representations as input\n  - Atom-based Graph Embedding : CGCNN w/o pooling layer -> local features\n  - Energy-grid Embedding : 1D flatten patches of 3D energy grid -> global features\n  \n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/hspark1212/MOFTransformer/master/docs/source/assets/fig2.jpg\" width=\"700\")\n</p>\n\n## [Feature Importance Anaylsis](https://hspark1212.github.io/MOFTransformer/getting_started/visualization.html)\nyou can easily visualize feature importance analysis of atom-based graph embeddings and energy-grid embeddings.\n```python\n%matplotlib widget\nfrom visualize import PatchVisualizer\n\nmodel_path = \"examples/finetuned_bandgap.ckpt\" # or 'examples/finetuned_h2_uptake.ckpt'\ndata_path = 'examples/visualize/dataset/'\ncifname = 'MIBQAR01_FSR'\n\nvis = PatchVisualizer.from_cifname(cifname, model_path, data_path)\nvis.draw_graph()\n```\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hspark1212/MOFTransformer/master/docs/source/getting_started/assets/1.gif\" width=\"400\">\n</p>\n\n```python\nvis.draw_grid()\n```\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hspark1212/MOFTransformer/master/docs/source/getting_started/assets/3.gif\" width=\"400\">\n</p>\n\n## Universal Transfer Learning\n| Property                                 | MOFTransformer | Original Paper | Number of Data | Remarks          | Reference |\n|------------------------------------------|----------------|----------------|----------------|------------------|-----------|\n|N<sub>2</sub> uptake                     | R2: 0.78       | R2: 0.71       | 5,286          | CoRE MOF         | 1         |\n|O<sub>2</sub> uptake                     | R2: 0.83       | R2: 0.74       | 5,286          | CoRE MOF         | 1         |\n|N<sub>2</sub> diffusivity                | R2: 0.77       | R2: 0.76       | 5,286          | CoRE MOF         | 1         |\n|O<sub>2</sub> diffusivity                | R2: 0.78       | R2: 0.74       | 5,286          | CoRE MOF         | 1         |\n|CO<sub>2</sub> Henry coefficient         | MAE : 0.30     | MAE : 0.42     | 8,183          | CoRE MOF         | 2         |\n|Solvent removal stability classification | ACC : 0.76     | ACC : 0.76     | 2,148          | Text-mining data | 3         |\n|Thermal stability regression             | R2 : 0.44      | R2 : 0.46      | 3,098          | Text-mining data | 3         |\n### Reference\n1. [Prediction of O2/N2 Selectivity in Metal−Organic Frameworks via High-Throughput Computational Screening and Machine Learning](https://pubs.acs.org/doi/abs/10.1021/acsami.1c18521)\n2. [Understanding the diversity of the metal-organic framework ecosystem](https://www.nature.com/articles/s41467-020-17755-8)\n3. [Using Machine Learning and Data Mining to Leverage Community Knowledge for the Engineering of Stable Metal–Organic Frameworks](https://pubs.acs.org/doi/10.1021/jacs.1c07217)\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/hspark1212/MOFTransformer",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://hspark1212.github.io/MOFTransformer/",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "moftransformer",
    "package_url": "https://pypi.org/project/moftransformer/",
    "platform": null,
    "project_url": "https://pypi.org/project/moftransformer/",
    "project_urls": {
      "Download": "https://github.com/hspark1212/MOFTransformer",
      "Homepage": "https://hspark1212.github.io/MOFTransformer/"
    },
    "release_url": "https://pypi.org/project/moftransformer/1.1.2/",
    "requires_dist": [
      "wget",
      "pytorch-lightning (==1.7.0)",
      "torchmetrics (>=0.6.0)",
      "transformers (>=4.12.5)",
      "timm (>=0.4.12)",
      "sacred (>=0.8.2)",
      "einops (>=0.4.1)",
      "ase (>=3.22.1)",
      "pymatgen (>=2022.0.16)",
      "matplotlib (>=3.5.0)",
      "seaborn (>=0.12.0)",
      "ipympl (>=0.9.2)",
      "pandas",
      "tqdm",
      "google-auth",
      "jupyter-client",
      "smmap (<6)",
      "emmet-core",
      "sphinx ; extra == 'docs'",
      "livereload ; extra == 'docs'",
      "myst-parser ; extra == 'docs'"
    ],
    "requires_python": ">=3.8",
    "summary": "moftransformer",
    "version": "1.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16938096,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d14d4456ffed4185d67bbb7624c203ed9d21205b080d01e729e89afda693f847",
        "md5": "a26089ec8e5df297c24e4414be5322bf",
        "sha256": "e81e1a522956f3bec38cc52179718edd34b897692e2a7696ef7ba4be2ca30934"
      },
      "downloads": -1,
      "filename": "moftransformer-1.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a26089ec8e5df297c24e4414be5322bf",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 2286880,
      "upload_time": "2023-02-20T05:11:17",
      "upload_time_iso_8601": "2023-02-20T05:11:17.493756Z",
      "url": "https://files.pythonhosted.org/packages/d1/4d/4456ffed4185d67bbb7624c203ed9d21205b080d01e729e89afda693f847/moftransformer-1.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ee2a4ef3625d01aa2fc58c89deb83f4e5c1d6e3802c3d2a846ca5b33ca6433e3",
        "md5": "aa28e1fdb2dc70a7bd8d9869e4ab9885",
        "sha256": "d35a4cfca03695c26f4a3d3a9e7ad20c2e9b463d6f18fc1fd48a318b3e8621b4"
      },
      "downloads": -1,
      "filename": "moftransformer-1.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "aa28e1fdb2dc70a7bd8d9869e4ab9885",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 2278042,
      "upload_time": "2023-02-20T05:11:20",
      "upload_time_iso_8601": "2023-02-20T05:11:20.338528Z",
      "url": "https://files.pythonhosted.org/packages/ee/2a/4ef3625d01aa2fc58c89deb83f4e5c1d6e3802c3d2a846ca5b33ca6433e3/moftransformer-1.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}