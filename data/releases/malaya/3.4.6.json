{
  "info": {
    "author": "huseinzol05",
    "author_email": "husein.zol05@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Topic :: Text Processing"
    ],
    "description": "**Malaya** is a Natural-Language-Toolkit library for bahasa Malaysia, powered by Deep Learning Tensorflow.\n\nDocumentation\n--------------\n\nProper documentation is available at https://malaya.readthedocs.io/\n\nInstalling from the PyPI\n----------------------------------\n\nCPU version\n::\n\n    $ pip install malaya\n\nGPU version\n::\n\n    $ pip install malaya-gpu\n\nOnly **Python 3.6.x and above** and **Tensorflow 1.10 and above but not 2.0** are supported.\n\nFeatures\n--------\n\n-  **Augmentation**\n\n   Augment any text using dictionary of synonym, Wordvector or Transformer-Bahasa.\n-  **Dependency Parsing**\n\n   Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.\n-  **Emotion Analysis**\n\n   Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.\n-  **Entities Recognition**\n\n   Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.\n-  **Generator**\n\n   Generate any texts given a context using GPT2 Bahasa 117M and 345M or Transformer-Bahasa.\n-  **Language Detection**\n\n   using Fast-text and Sparse Deep learning Model to classify Malay (formal and social media), Indonesia (formal and social media), Rojak language and Manglish.\n-  **Normalizer**\n\n   using local Malaysia NLP researches hybrid with Transformer-Bahasa to normalize any bahasa texts.\n-  **Num2Word**\n\n   Convert from numbers to cardinal or ordinal representation.\n-  **Part-of-Speech Recognition**\n\n   Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.\n-  **Relevancy Analysis**\n\n   Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.\n-  **Sentiment Analysis**\n\n   Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.\n-  **Similarity**\n\n   Using deep Encoder, Doc2Vec, BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa and ALXLNET-base-bahasa to build deep semantic similarity models.\n-  **Spell Correction**\n\n   Using local Malaysia NLP researches hybrid with Transformer-Bahasa to auto-correct any bahasa words.\n-  **Stemmer**\n\n   Using BPE LSTM Seq2Seq with attention state-of-art to do Bahasa stemming.\n-  **Subjectivity Analysis**\n\n   Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.\n-  **Summarization**\n\n   Provide Transformer-Bahasa, skip-thought, LDA, LSA and Doc2Vec interface to give precise unsupervised summarization, and TextRank as scoring algorithm.\n-  **Topic Modelling**\n\n   Provide Transformer-Bahasa, LDA2Vec, LDA, NMF and LSA interface for easy topic modelling with topics visualization.\n-  **Toxicity Analysis**\n\n   Transfer learning on BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa.\n-  **Transformer**\n\n   Provide easy interface to load BERT-base-bahasa, Tiny-BERT-bahasa, Albert-base-bahasa, Albert-tiny-bahasa, XLNET-base-bahasa, ALXLNET-base-bahasa, ELECTRA-base-bahasa and ELECTRA-small-bahasa.\n-  **Word2Num**\n\n   Convert from cardinal or ordinal representation to numbers.\n-  **Word2Vec**\n\n   Provide pretrained bahasa wikipedia and bahasa news Word2Vec, with easy interface and visualization.\n\nPretrained Models\n------------------\n\nMalaya also released Bahasa pretrained models, simply check at `Malaya/pretrained-model <https://github.com/huseinzol05/Malaya/tree/master/pretrained-model>`_\n\nOr can try use huggingface ðŸ¤— Transformers library, https://huggingface.co/models?filter=malay\n\nReferences\n-----------\n\nIf you use our software for research, please cite:\n\n::\n\n  @misc{Malaya, Natural-Language-Toolkit library for bahasa Malaysia, powered by Deep Learning Tensorflow,\n    author = {Husein, Zolkepli},\n    title = {Malaya},\n    year = {2018},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/huseinzol05/malaya}}\n  }\n\nAcknowledgement\n----------------\n\nThanks to `Im Big <https://www.facebook.com/imbigofficial/>`_, `LigBlou <https://www.facebook.com/ligblou>`_, `Mesolitica <https://mesolitica.com/>`_ and `KeyReply <https://www.keyreply.com/>`_ for sponsoring AWS, GCP and private cloud to train Malaya models.\n\nContributing\n----------------\n\nThank you for contributing this library, really helps a lot. Feel free to contact me to suggest me anything or want to contribute other kind of forms, we accept everything, not just code!\n\nLicense\n--------\n\n.. |License| image:: https://app.fossa.io/api/projects/git%2Bgithub.com%2Fhuseinzol05%2FMalaya.svg?type=large\n   :target: https://app.fossa.io/projects/git%2Bgithub.com%2Fhuseinzol05%2FMalaya?ref=badge_large\n\n|License|\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "https://github.com/huseinzol05/Malaya/archive/master.zip",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/huseinzol05/Malaya",
    "keywords": "nlp,bm",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "malaya",
    "package_url": "https://pypi.org/project/malaya/",
    "platform": "",
    "project_url": "https://pypi.org/project/malaya/",
    "project_urls": {
      "Download": "https://github.com/huseinzol05/Malaya/archive/master.zip",
      "Homepage": "https://github.com/huseinzol05/Malaya"
    },
    "release_url": "https://pypi.org/project/malaya/3.4.6/",
    "requires_dist": [
      "dateparser",
      "sklearn",
      "scikit-learn",
      "requests",
      "unidecode",
      "tensorflow (<2.0,>=1.14)",
      "numpy",
      "scipy",
      "PySastrawi",
      "ftfy",
      "networkx",
      "sentencepiece",
      "bert-tensorflow",
      "tqdm",
      "herpetologist",
      "youtokentome",
      "albert-tensorflow"
    ],
    "requires_python": ">=3.6.*",
    "summary": "Natural-Language-Toolkit for bahasa Malaysia, powered by Deep Learning Tensorflow.",
    "version": "3.4.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16151054,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "86cdd9af3a18e35275710e55b81bd3b7159ffbd8813a32abef582df43507f694",
        "md5": "69ee37da9a026c897de9c7566f2c9668",
        "sha256": "47da001124ccd11db34eb8308d3b13ea34b60945544f052be0edc07804d78657"
      },
      "downloads": -1,
      "filename": "malaya-3.4.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "69ee37da9a026c897de9c7566f2c9668",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.*",
      "size": 3921301,
      "upload_time": "2020-05-20T14:19:33",
      "upload_time_iso_8601": "2020-05-20T14:19:33.445532Z",
      "url": "https://files.pythonhosted.org/packages/86/cd/d9af3a18e35275710e55b81bd3b7159ffbd8813a32abef582df43507f694/malaya-3.4.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}