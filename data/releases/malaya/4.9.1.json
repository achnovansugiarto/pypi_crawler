{
  "info": {
    "author": "huseinzol05",
    "author_email": "husein.zol05@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Topic :: Text Processing"
    ],
    "description": "**Malaya** is a Natural-Language-Toolkit library for bahasa Malaysia, powered by Tensorflow and PyTorch.\n\nDocumentation\n--------------\n\nProper documentation is available at https://malaya.readthedocs.io/\n\nInstalling from the PyPI\n----------------------------------\n\n::\n\n    $ pip install malaya\n\nIt will automatically install all dependencies except for Tensorflow and PyTorch. So you can choose your own Tensorflow CPU / GPU version and PyTorch CPU / GPU version.\n\nOnly **Python >= 3.6.0**, **Tensorflow >= 1.15.0**, and **PyTorch >= 1.10** are supported.\n\nDevelopment Release\n---------------------------------\n\nInstall from `master` branch,\n\n::\n\n    $ pip install git+https://github.com/huseinzol05/malaya.git\n\n\nWe recommend to use **virtualenv** for development. \n\nDocumentation at https://malaya.readthedocs.io/en/latest/\n\nFeatures\n--------\n\n-  **Alignment**, translation word alignment using Eflomal and pretrained Transformer models.\n-  **Augmentation**, augment any text using dictionary of synonym, Wordvector or Transformer-Bahasa.\n-  **Constituency Parsing**, breaking a text into sub-phrases using finetuned Transformer-Bahasa.  \n-  **Coreference Resolution**, finding all expressions that refer to the same entity in a text using Dependency Parsing models.\n-  **Dependency Parsing**, extracting a dependency parse of a sentence using finetuned Transformer-Bahasa.\n-  **Emotion Analysis**, detect and recognize 6 different emotions of texts using finetuned Transformer-Bahasa.\n-  **Entities Recognition**, seeks to locate and classify named entities mentioned in text using finetuned Transformer-Bahasa.\n-  **Generator**, generate any texts given a context using T5-Bahasa, GPT2-Bahasa or Transformer-Bahasa.\n-  **Jawi-to-Rumi**, convert from Jawi to Rumi using Transformer.\n-  **KenLM**, provide easy interface to load Pretrained KenLM Malaya models.\n-  **Keyword Extraction**, provide RAKE, TextRank and Attention Mechanism hybrid with Transformer-Bahasa.\n-  **Knowledge Graph**, generate Knowledge Graph using T5-Bahasa or parse from Dependency Parsing models.\n-  **Language Detection**, using Fast-text and Sparse Deep learning Model to classify Malay (formal and social media), Indonesia (formal and social media), Rojak language and Manglish.\n-  **Normalizer**, using local Malaysia NLP researches hybrid with Transformer-Bahasa to normalize any bahasa texts.\n-  **Num2Word**, convert from numbers to cardinal or ordinal representation.\n-  **Paraphrase**, provide Abstractive Paraphrase using T5-Bahasa and Transformer-Bahasa.\n-  **Grapheme-to-Phoneme**, convert from Grapheme to Phoneme DBP or IPA using LSTM Seq2Seq with attention state-of-art.\n-  **Part-of-Speech Recognition**, grammatical tagging is the process of marking up a word in a text using finetuned Transformer-Bahasa.\n-  **Question Answer**, reading comprehension using finetuned Transformer-Bahasa.\n-  **Relevancy Analysis**, detect and recognize relevancy of texts using finetuned Transformer-Bahasa.\n-  **Rumi-to-Jawi**, convert from Rumi to Jawi using Transformer.\n-  **Sentiment Analysis**, detect and recognize polarity of texts using finetuned Transformer-Bahasa.\n-  **Text Similarity**, provide interface for lexical similarity deep semantic similarity using finetuned Transformer-Bahasa.\n-  **Spelling Correction**, using local Malaysia NLP researches hybrid with Transformer-Bahasa to auto-correct any bahasa words and NeuSpell using T5-Bahasa.\n-  **Stemmer**, using BPE LSTM Seq2Seq with attention state-of-art to do Bahasa stemming including local language structure.\n-  **Subjectivity Analysis**, detect and recognize self-opinion polarity of texts using finetuned Transformer-Bahasa.\n-  **Kesalahan Tatabahasa**, Fix kesalahan tatabahasa using TransformerTag-Bahasa.\n-  **Summarization**, provide Abstractive T5-Bahasa also Extractive interface using Transformer-Bahasa, skip-thought and Doc2Vec.\n-  **Tokenizer**, provide word, sentence and syllable tokenizers.\n-  **Topic Modelling**, provide Transformer-Bahasa, LDA2Vec, LDA, NMF and LSA interface for easy topic modelling with topics visualization.\n-  **Toxicity Analysis**, detect and recognize 27 different toxicity patterns of texts using finetuned Transformer-Bahasa.\n-  **Transformer**, provide easy interface to load Pretrained Language Malaya models.\n-  **Translation**, provide Neural Machine Translation using Transformer for EN to MS and MS to EN.\n-  **Word2Num**, convert from cardinal or ordinal representation to numbers.\n-  **Word2Vec**, provide pretrained bahasa wikipedia and bahasa news Word2Vec, with easy interface and visualization.\n-  **Zero-shot classification**, provide Zero-shot classification interface using Transformer-Bahasa to recognize texts without any labeled training data.\n-  **Hybrid 8-bit Quantization**, provide hybrid 8-bit quantization for all models to reduce inference time up to 2x and model size up to 4x.\n-  **Longer Sequences Transformer**, provide BigBird, BigBird + Pegasus and Fastformer for longer sequence tasks.\n\nPretrained Models\n------------------\n\nMalaya also released Bahasa pretrained models, simply check at `Malaya/pretrained-model <https://github.com/huseinzol05/Malaya/tree/master/pretrained-model>`_\n\n- **ALBERT**, a Lite BERT for Self-supervised Learning of Language Representations, https://arxiv.org/abs/1909.11942\n- **ALXLNET**, a Lite XLNET, no paper produced.\n- **BERT**, Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805\n- **BigBird**, Transformers for Longer Sequences, https://arxiv.org/abs/2007.14062\n- **ELECTRA**, Pre-training Text Encoders as Discriminators Rather Than Generators, https://arxiv.org/abs/2003.10555\n- **GPT2**, Language Models are Unsupervised Multitask Learners, https://github.com/openai/gpt-2\n- **LM-Transformer**, Exactly like T5, but use Tensor2Tensor instead Mesh Tensorflow with little tweak, no paper produced.\n- **PEGASUS**, Pre-training with Extracted Gap-sentences for Abstractive Summarization, https://arxiv.org/abs/1912.08777\n- **T5**, Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, https://arxiv.org/abs/1910.10683\n- **TinyBERT**, Distilling BERT for Natural Language Understanding, https://arxiv.org/abs/1909.10351\n- **Word2Vec**, Efficient Estimation of Word Representations in Vector Space, https://arxiv.org/abs/1301.3781\n- **XLNET**, Generalized Autoregressive Pretraining for Language Understanding, https://arxiv.org/abs/1906.08237\n- **FNet**, FNet: Mixing Tokens with Fourier Transforms, https://arxiv.org/abs/2105.03824\n- **Fastformer**, Fastformer: Additive Attention Can Be All You Need, https://arxiv.org/abs/2108.09084\n\nReferences\n-----------\n\nIf you use our software for research, please cite:\n\n::\n\n  @misc{Malaya, Natural-Language-Toolkit library for bahasa Malaysia, powered by Deep Learning Tensorflow,\n    author = {Husein, Zolkepli},\n    title = {Malaya},\n    year = {2018},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/huseinzol05/malaya}}\n  }\n\nAcknowledgement\n----------------\n\nThanks to `KeyReply <https://www.keyreply.com/>`_ for sponsoring private cloud to train Malaya models, without it, this library will collapse entirely. \n\nAlso, thanks to `Tensorflow Research Cloud <https://www.tensorflow.org/tfrc>`_ for free TPUs access.\n\nContributing\n----------------\n\nThank you for contributing this library, really helps a lot. Feel free to contact me to suggest me anything or want to contribute other kind of forms, we accept everything, not just code!\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "https://github.com/huseinzol05/Malaya/archive/master.zip",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/huseinzol05/Malaya",
    "keywords": "nlp,bm",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "malaya",
    "package_url": "https://pypi.org/project/malaya/",
    "platform": null,
    "project_url": "https://pypi.org/project/malaya/",
    "project_urls": {
      "Download": "https://github.com/huseinzol05/Malaya/archive/master.zip",
      "Homepage": "https://github.com/huseinzol05/Malaya"
    },
    "release_url": "https://pypi.org/project/malaya/4.9.1/",
    "requires_dist": [
      "dateparser",
      "scikit-learn",
      "requests",
      "unidecode",
      "numpy",
      "scipy",
      "ftfy",
      "networkx (<=2.5.1)",
      "sentencepiece",
      "tqdm",
      "herpetologist",
      "malaya-boilerplate (>=0.0.22)",
      "regex"
    ],
    "requires_python": ">=3.6.*",
    "summary": "Natural-Language-Toolkit for bahasa Malaysia, powered by Tensorflow and PyTorch.",
    "version": "4.9.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16151054,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5b3a31d059ff3ed9d39d187a78c46ec81348759799f9dd4118aff5a4753ff82c",
        "md5": "29cf80bf8c47d34b1b2c57e31989c0cf",
        "sha256": "b678da700fc58ee4d555fdddfab19e8b3a5b67a6076616c9e364994561c4c9c4"
      },
      "downloads": -1,
      "filename": "malaya-4.9.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "29cf80bf8c47d34b1b2c57e31989c0cf",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.*",
      "size": 2724994,
      "upload_time": "2022-09-01T16:54:47",
      "upload_time_iso_8601": "2022-09-01T16:54:47.798562Z",
      "url": "https://files.pythonhosted.org/packages/5b/3a/31d059ff3ed9d39d187a78c46ec81348759799f9dd4118aff5a4753ff82c/malaya-4.9.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}