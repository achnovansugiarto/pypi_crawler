{
  "info": {
    "author": "Lowe's Companies, Inc.",
    "author_email": "lowesopensource@lowes.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License"
    ],
    "description": "L Virtual File System\n=====================\n\nRationale\n---------\nLVFS is a Python interface wrapping several storage APIs, particularly HDFS, to give a generally consistent\nabstraction over multiple APIs.\n\n * LVFS can allow you to shift existing or legacy Python applications from one storage API to another,\n   and has been used to move production applications from HDFS to GCS, and from local files to HDFS.\n * LVFS doesn't require local temporary space; data doesn't need to save on disk when uploading or downloading\n * LVFS handles pretty complicated credential configurations, which can save a lot of time when developing\n * LVFS integrates several big data formats used in Hadoop, so analytics applications usually can begin using\n   Hive or Spark data using only one line of code.\n * LVFS uses Python modules whenever possible and only requires `ssh` be installed if you need to use SSH.\n   (This is because Paramiko does not support complex secured proxies.)\n\nComparison against other solutions\n----------------------------------\nLVFS is similar to some other storage tools:\n\n### rclone\n* `rclone` is a file sync utility that supports almost every imaginable storage API, with an interface very\n  similar to `rsync`, except that `rsync` is older and only supports SSH and it's own custom protocol.\n* Both systems create a common interface for multiple APIs\n* LVFS separates reading from writing so you can read or write without touching local disk.\n  This is important for using LVFS inside a cluster because this reduces disk activity by at least half.\n  It is also important in containers, which often have very little available disk space (only memory)\n* `rclone` supports way more protocols, where LVFS only supports the most common ones.\n* `rclone` is a command line application, but LVFS is a Python library so integrating them into applications\n  is much different.\n\n### hdfs\nThere is a Python package named `hdfs` which supports WebHDFS accesses from Python, and it is the most closely\naligned open source project to LVFS. It is essentially an SDK for WebHDFS and supports everything `lvfs`\nsupports, because LVFS uses `hdfs`.\n\n* LVFS supports more than one protocol, which is important if you are using LVFS to \"lift and shift\" a legacy\n  application between storage APIs\n* `hdfs` supports a more thorough and complete API around HDFS. In particular, it provides file-like objects\n  that can be useful if a file is larger than available memory or if you know you only need part of a file.\n* If you are sure you don't need any other storage APIs, or you need more direct control at the expense\n  of some more difficulty porting to a new API, `hdfs` may be a better choice.\n\n### Alluxio\nAlluxio is a company and their eponymous suite of tools, which includes adapters between several kinds of\nstorage. Alluxio's scope is dramatically larger than LVFS and has many advantages and caveats:\n\n* Alluxio can mount HDFS as a local volume on a Linux machine. This means every Unix command, including\n  `ls` and `sqlite` and `awk` and everything else, will be able to use HDFS or S3 or many other APIs.\n  This is a *really amazing concept*, and it works using the kernel's virtual file system rather than\n  an interface on a user-application level.\n * There are a few open source projects trying to do this for HDFS as well, the completeness of which I\n   have not been able to verify. They deserve some attention as well.\n* Posix file systems and these storage APIs are so much different in the details that actually relying\n  on these adapters in production is super risky, not because of anything wrong with Alluxio but on account\n  of specifics. What happens when one app has a file open and another deletes it, or how `mmap()` should\n  work after a process `fork()`'s? This is why bona-fide file systems take several years to develop.\n\nLVFS is not really an alternative to Alluxio. Alluxio is a massive project connecting so many APIs to\nso many others, and requires a good bit of installation and know-how. LVFS requires a single configuration\nfile and a few lines of Python and effects only the applications that are built for it and import it.\n\n### Minio\nMinio is a specific implementation of the Simple Storage Service protocol (S3). It provides very nice\nS3 clients for several languages, including Python. It also supports an adapter server that exposes an\nS3 endpoint for HDFS clusters. You could reasonably combine these to make a system where you can easily\nmigrate between HDFS and S3 (which many storage APIs support). This could be a good plan, but in our\ncase we needed to directly support GCS buckets with their SDK rather than also using S3 for\nGCS, which we haven't investigated for feasibility.\n\n\nInstall\n=======\n`lvfs` is not yet published to the PyPI repository. When it is, we'll update how to install it here.\nBecause PyPI already has a package named `lvfs`, the package name will likely be different when it\ndoes get published.\n\nConfigure\n=========\nLVFS is designed firstly for HDFS, and sometimes HDFS clusters are firewalled and in a different subnet,\nrequiring an intermediary host to facilitate the communnication. There are also several ways you might\nneed to authenticate, and there are some cases where you even need to authenticate different ways\nsimultaneously. LVFS handles all the common paths encountered in a large corporate environment.\n\nLVFS configuration is stored in `~/.config/lvfs.yml` because we expect that either you are in a container,\nin which case the home directory is specific to an application, or you are using a laptop, where you will\nlikely want to handle development keys or credentials for several projects at once.\n\n> *Please note your company may already have documentation on how to configure LVFS*\n\nConfiguration: Connect to an unauthenticated HDFS cluster behind an SSH tunnel\n------------------------------------------------------------------------------\nThis is probably the most questionable setup but if you find yourself using a\nHadoop cluster that does not support authentication, but is instead protected\nby keeping it on a separate subnet, you will need to connect to the subnet,\nand usually that is done with SSH. LVFS supports this but it is a little involved.\n\n> If you can, avoid doing this in production. SSH tunnels are not great for\neither performance or reliability.\n\n```yaml\n# LVFS only needs credentials but we prefix with this in case we need additional information later\ncredentials:\n  # In Yaml the - indiciates this is a list element.\n  # Any additional stanzas will start with a similar -\n  - realm:\n      # LVFS uses realms to determine which set of credentials to use for each URL.\n      # Every credential stanza gets one realm.\n      # LVFS will use the first stanza that matches the URL.\n      # In this case we match anything using HDFSOverSSH\n      classname: HDFSOverSSH\n    # Note how the indentation dropped by two spaces.\n    # The rest of the stanza configures the connection.\n    ssh_username: your_ssh_user  # This is the username used to log into the jumpbox\n                                 # LVFS will not pass SSH passwords. Use asymmetric keys instead.\n    ssh_jump_host: dns_name      # This is the actual jumpbox\n    # Replace this with your Hadoop username used for HDFS;\n    # this may be the same as your company's SSO.\n    # Do not specify a password, even an empty one. This disables authentication.\n    username: ltorvalds\n    # Hadoop clusters usually use Zookeeper to maintain high availability.\n    # So to handle multiple master nodes, just separate the URLs with spaces.\n    # You may need to ask your admin for these URLs, or they may be available on a corporate wiki or FAQ.\n    webhdfs_root: http://hadoopmaster001.corporate.com:50070;http://hadoopmaster002.corporate.com:50070;http://hadoopmaster003.corporate.com:50070\n```\n\nConfiguration: Connect to an unauthenticated HDFS cluster from within the subnet\n--------------------------------------------------------------------------------\nIf you use an unauthenticated HDFS cluster, then you should avoid poking holes in\nyour firewalls as much as possible, and you can help do that by keeping the data\nprocessing within the cluster. In that case, configuration is a lot easier.\n\n```yaml\ncredentials:\n  - realm:\n      # For historic reasons, all HDFS connections use the HDFSOverSSH connection class\n      # because SSH will be disabled when you don't configure it here.\n      classname: HDFSOverSSH\n    # This is the Hadoop username; there is no jumpbox\n    username: ltorvalds\n    # You may need to ask your admin for these URLs, or they may be available on a corporate wiki or FAQ.\n    webhdfs_root: http://hadoopmaster001.corporate.com:50070;http://hadoopmaster002.corporate.com:50070;http://hadoopmaster003.corporate.com:50070\n```\n\nConfiguration: Connect using AD credentials to an HDFS cluster with open WebHDFS access\n---------------------------------------------------------------------------------------\nConnecting to HDFS using authenticated, encrypted WebHDFS is generally better practice\nthan encrypting using SSH. LVFS is compatible with this setup and you should find it to\nbe quite performant. The password is stored in the clear in this file. For that reason,\n*be careful to control file permissions* and be sure only to use this for development\npurposes. Production applications should probably use Kerberized service principals.\n\n```yaml\ncredentials:\n  - realm:\n      # HDFS is really always HDFSOverSSH. It's a historical artifact.\n      # In this case SSH is not used.\n      classname: HDFSOverSSH\n    username: my_ad_username\n    password: my_ad_password\n    # Set this to the endpoint your cluster exposes.\n    # Keep in mind, this is probably not a master node, and it probably doesn't use Zookeeper\n    # Instead, this is probably a Knox server, the hint being it probably is at port 8443.\n    webhdfs_root: https://theknoxserver.corporate.com:8443/gateway/default\n```\n\nConfiguration: Connect using Kerberos service principals, inside an HDFS cluster\n--------------------------------------------------------------------------------\nThis is probably the most production worthy setup, where authentication is used,\nas well as encryption, but actually nothing is leaving the subnet anyway.\n\n> This does not set up Kerberos authentication, it just uses it.\n> You need to have successfully run `kinit` before running your app.\n\n```yaml\ncredentials:\n  - realm:\n      # HDFS is really always HDFSOverSSH. It's a historical artifact.\n      # In this case SSH is not used.\n      classname: HDFSOverSSH\n    # The username kerberos is special and will trigger Kerberos authentication\n    # be sure to call `kinit` sometime before you run your app so that your\n    # tickets will be set up already. LVFS will not run it for you.\n    username: kerberos\n    webhdfs_root: https://themasterserver.corporate.com:9871\n```\n\nNotes\n-----\nKerberos still requires that you run `kinit` somehow *before* you start using LVFS,\nand preferably before you even start Python. You need to figure out how to make that happen.\nIn an interactive terminal, `kinit` will prompt you for your AD password,\nand if you have a keytab it will not. For this reason, you will need a keytab for non-interactive\nlogins. Your case may be different, but most likely this keytab will be in a file somewhere,\nand only accessible to certain Unix users due to file permissions. Ask your admins where and who.\n\nFinally, check with `klist` that Kerberos is already working on this machine or container.\nAfter `kinit` has been run, check that `klist` shows the principals you expect;\nIf not there's no way LVFS will get it right either.\n\nIt's not clear whether it is practical to authentiate with Kerberos with SSH tunneling,\nbecause you would need to setup Kerberos there, which may require extensive configuration.\nAt any rate, this is not been tested with LVFS.\n\n\nFor later reference, these are the possible modes for HDFSOverSSH:\n\nssh_jump_host | ssh_username | username | password | webhdfs_root | use case\n------------- | ------------ | -------- | -------- | ------------ | --------\n(any)         | (any)        | (any)    | (any)    | None         | Invalid, unconfigured\nnot None      | None         | (any)    | (any)    | (any)        | Invalid\nNone          | not None     | (any)    | (any)    | (any)        | Invalid\n(any)         | (any)        | None     | not None | (any)        | Invalid\nnot None      | not None     | not None | None     | not None     | HDP2 with SSH\nNone          | None         | not None | None     | not None     | HDP2 without SSH\nnot None      | not None     | not None | not None | not None     | HDP3+AD with SSH\nNone          | None         | not None | not None | not None     | HDP3+AD without SSH\nnot None      | not None     | kerberos | None     | not None     | HDP3+Kerberos without SSH &\nNone          | None         | kerberos | None     | not None     | HDP3+Kerberos with SSH\n\n(&) untested\n  \nExample code\n============\n\n## Read a text file as YAML\n```py\n    (URL\n        .to(input(\"Where's the config file? \"))\n        .read_yaml()\n    )\n```\n\n## Read parquet format tables from HDFS\nYou can provide a file or a directory.\nDirectories (as in this case) will have all their shards concatenated into one dataframe.\nPartitions are not concatenated, as they would usually be too large anyway.\n```py\n    df = (URL\n        .to(\"hdfs://hdfsmasternode.example.com/path/to/your/table\")\n        .read_parquet()\n    )\n```\n\n## Copy files recursively from HDFS to local\nLVFS is designed for convenience, not speed or scalability, and each file will be buffered in memory.\nDon't use this method if any of your files are more than a couple gigabytes each.\nYou'll run out of memory if you do.\n\n```py\n    (URL\n        .to(\"hdfs://hdfsmasternode.example.com/path/to/your/table\")\n        .cp(\"/maybe/somewhere/local\")\n    )\n```\n\n### Create a configurable model on the spot\nJust to show how you might want to interact with URLs, here's an example where we load some\nconfiguration files from any location (local or remote!) and then try to load a model from there.\n\n> Keep in mind a lot of this is not about URL, it's just giving the example some context.\n\n```py\ndef load_model(home: URL):\n    \"\"\" Configure and read a model stored in a folder \"\"\"\n    composed_conf = {}\n    for conf in sorted(home.ls(recursive=True)):\n        # Each configuration alphabetically. Use prefixes like \"05-\" to compose them.\n        if conf.basename().endswith(\".yml\"):\n            # URLs are always absolute, so you can use ls() results immediately.\n            composed_conf.update(conf.read_yaml())\n    assert \"latest_model\" in composed_conf\n    model = URL.to(composed_conf).read_pickle()\n    return model.compose_enhanced_sheaf_cohomologies(**composed_conf)\n\nif __name__ == \"__main__\":\n    import os\n    from argparse import ArgumentParser as A\n    parser = A(description=\"Launch sheaf model\")\n    parser.add_argument(\"model\", help=\"Whence come the models\", default=os.getpwd())\n    parser.add_argument(\"version\", help=\"Model version\", default=\"latest\")\n    parser.add_argument(\"data\", help=\"Where the parquets are stored\", default=os.getpwd())\n    args = parser.parse_args()\n    # You can tack on URL segments using `.join(..)`.\n    # The right operand must be a string, not a URL, because URLs are absolute.\n    model = load_model(URL.to(args.model).join(args.version))\n    model.process(URL.to(args.data).read_parquet())\n    print(\"All done!\")\n```\n\nArchitecture\n============\n\nLVFS is based on an abstract URL class with one important static method (`to(..)`).\n`URL.to(..)` checks the protocol and the machine hostname to see which connection implementation\nis most appropriate for the location you're running and where you want to connect to.\nIt will give you an instance of that implementation seamlessly.\n\nThere are a few methods that every implementation needs to implement, like `stat()`, `ls()`,\nand `read_binary()` among a few others. But many methods, like `read_json()` and `cp()` are defined\nin terms of the other methods in order to reduce duplication. That said, GCS is strange and there\nare many sharp edges you hopefully will not be cut by. Consider for example that you must always\nbuffer the fill content of a GCS blob in memory, which is why LVFS does not support streaming.\n\nAPI Documentation\n=================\nYou can generate API documentation for this package after cloning it using `pdoc3` through the\n`generate-docs.sh` script in the project root directory.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/lowes",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lvfs",
    "package_url": "https://pypi.org/project/lvfs/",
    "platform": "",
    "project_url": "https://pypi.org/project/lvfs/",
    "project_urls": {
      "Homepage": "https://github.com/lowes"
    },
    "release_url": "https://pypi.org/project/lvfs/1.1.2/",
    "requires_dist": [
      "pandas ~= 1.0",
      "pyyaml ~= 5.3",
      "hvac ~= 0.10",
      "keyring ~= 21.5",
      "python-dotenv ~= 0.15.0",
      "aiofiles >= 0.5.0",
      "pyarrow >= 0.13.0",
      "pybind11 ~= 2.6.0",
      "google-cloud-storage ~= 1.29 ; extra == \"all\"",
      "minio ~= 6.0 ; extra == \"all\"",
      "hdfs ~= 2.5 ; extra == \"all\"",
      "paramiko ~= 2.7.1 ; extra == \"all\"",
      "PySocks ~= 1.7 ; extra == \"all\"",
      "requests ~= 2.23 ; extra == \"all\"",
      "pyorc ~= 0.4.0 ; extra == \"all\"",
      "gssapi ~= 1.6.12 ; extra == \"all\"",
      "requests-gssapi ~= 1.2.3 ; extra == \"all\"",
      "google-cloud-storage ~= 1.29 ; extra == \"gcs\"",
      "hdfs ~= 2.5 ; extra == \"hdfs\"",
      "paramiko ~= 2.7.1 ; extra == \"hdfs\"",
      "PySocks ~= 1.7 ; extra == \"hdfs\"",
      "requests ~= 2.23 ; extra == \"hdfs\"",
      "minio ~= 6.0 ; extra == \"minio\"",
      "pytest-benchmark ~= 3.0 ; extra == \"test\"",
      "pytest-timeout ~= 1.4 ; extra == \"test\"",
      "pytest-asyncio ~= 0.14.0 ; extra == \"test\"",
      "pytest ~= 6.0 ; extra == \"test\""
    ],
    "requires_python": ">=3.7",
    "summary": "Convenient high level file IO across multiple protocols",
    "version": "1.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12006504,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "12f098c3dd24a10beb94c3f18cc47a3dc72c8a343e80468dee264e46bc5f6bdf",
        "md5": "2334c5f3859cf4aae044c9590a87de0d",
        "sha256": "c93def361e063c4be28e899e1bbae1d6fbc138d2a4273d024cbb1779f2918a17"
      },
      "downloads": -1,
      "filename": "lvfs-1.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "2334c5f3859cf4aae044c9590a87de0d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 50244,
      "upload_time": "2021-11-12T14:40:59",
      "upload_time_iso_8601": "2021-11-12T14:40:59.317152Z",
      "url": "https://files.pythonhosted.org/packages/12/f0/98c3dd24a10beb94c3f18cc47a3dc72c8a343e80468dee264e46bc5f6bdf/lvfs-1.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6af4865c087f07d989944448a81ad70c3e012e92a41fa71f2e381bdb706b1937",
        "md5": "9848c29399baed6b8efcb946a248cad7",
        "sha256": "5609ce2392e6433f87cd7bdb0cc61692fddc24c996a59d5400a48371f87fe7fe"
      },
      "downloads": -1,
      "filename": "lvfs-1.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "9848c29399baed6b8efcb946a248cad7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 48052,
      "upload_time": "2021-11-12T14:41:00",
      "upload_time_iso_8601": "2021-11-12T14:41:00.810510Z",
      "url": "https://files.pythonhosted.org/packages/6a/f4/865c087f07d989944448a81ad70c3e012e92a41fa71f2e381bdb706b1937/lvfs-1.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}