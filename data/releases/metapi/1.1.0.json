{
  "info": {
    "author": "Jie Zhu, Fangming Yang",
    "author_email": "alienchuj@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Environment :: Console",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering :: Bio-Informatics"
    ],
    "description": "[![bioconda-badge](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat)](http://bioconda.github.io)\n[![ohmeta-badge](https://img.shields.io/badge/install%20with-ohmeta-brightgreen.svg?style=flat)](http://anaconda.org/ohmeta)\n[![PyPI version](https://badge.fury.io/py/metapi.svg)](https://badge.fury.io/py/metapi)\n[![star this repo](http://githubbadges.com/star.svg?user=ohmeta&repo=metapi&style=flat)](https://github.com/ohmeta/metapi)\n[![Anaconda-Server Badge](https://anaconda.org/bioconda/metapi/badges/downloads.svg)](https://anaconda.org/bioconda/metapi)\n\n<div align=center><img width=\"500\" height=\"280\" src=\"docs/logo.svg\"/></div>\n\nA general metagenomics data mining system focus on robust microbiome research.\n\n## Overview\n### MAG workflow\n<div align=center><img width=\"600\" height=\"800\" src=\"docs/mag_workflow.svg\"></div>\n\n## Installation\n\nmetapi works with Python 3.6+.\nYou can install it via [bioconda](https://bioconda.github.io/):\n\n```\n$ conda install -c bioconda metapi\n# or\n$ conda install -c ohmeta metapi\n```\n\nOr via pip:\n\n```\n$ pip install metapi\n```\n\n## Run\n\n### help\n\n```\n$ metapi --help\n\n  .___  ___.  _______ .___________.    ___      .______    __\n  |   \\/   | |   ____||           |   /   \\     |   _  \\  |  |\n  |  \\  /  | |  |__   `---|  |----`  /  ^  \\    |  |_)  | |  |\n  |  |\\/|  | |   __|      |  |      /  /_\\  \\   |   ___/  |  |\n  |  |  |  | |  |____     |  |     /  _____  \\  |  |      |  |\n  |__|  |__| |_______|    |__|    /__/     \\__\\ | _|      |__|\n\n            Omics for All, Open Source for All\n\n A general metagenomics data mining system focus on robust microbiome research.\n\n    optional arguments:\n    -h, --help     show this help message and exit\n    -v, --version  print software version and exit\n\n    available subcommands:\n\n    init         init project\n    mag_wf       metagenome-assembly-genome pipeline\n    gene_wf      metagenome-assembly-gene pipeline\n    sync         metapi sync project\n```\n\n### init\n\n```\n$ metapi init --help\n\n  usage: metapi init [-h] [-d WORKDIR] [-s SAMPLES]\n                    [-b {simulate,trimmingrmhost,assembly}]\n\n  arguments:\n      -h, --help            show this help message and exit\n      -d, --workdir WORKDIR\n                            project workdir (default: ./)\n      -s, --samples SAMPLES\n                            desired input:\n                            samples list, tsv format required.\n\n                            if begin from trimming, rmhost, or assembly:\n                                if it is fastq:\n                                    the header is [id, fq1, fq2]\n                                if it is sra:\n                                    the header is [id, sra]\n                            if begin from simulate:\n                                the header is [id, genome, abundance, reads_num, model]\n\n    -b, --begin {simulate,trimming,rmhost,assembly}\n                            pipeline starting point (default: trimming)\n```\n\n### mag_wf\n\n```\n$ metapi mag_wf --help\n\n  usage: metapi mag_wf [-h] [-d WORKDIR] [--config CONFIG] [--cluster CLUSTER]\n                            [--cores CORES] [--jobs JOBS] [--list] [--run] [--debug]\n                            [--dry-run] [--qsub] [--wait WAIT] [--use-conda]\n                            [--conda-prefix CONDA_PREFIX] [--conda-create-envs-only]\n                            [TASK]\n\n  positional arguments:\n  TASK              pipeline end point. Allowed values are \n       simulate_all, prepare_short_reads_all, prepare_long_reads_all,\n       prepare_reads_all,\n       raw_fastqc_all, raw_report_all, raw_all,\n       trimming_oas1_all, trimming_sickle_all,\n       trimming_fastp_all, trimming_report_all,\n       trimming_all,\n       rmhost_soap_all, rmhost_bwa_all, rmhost_bowtie2_all,\n       rmhost_minimap2_all, rmhost_kraken2_all,\n       rmhost_report_all, rmhost_all, qcreport_all,\n       assebmly_megahit_all, assembly_idba_ud_all,\n       assembly_metaspades_all, assembly_spades_all,\n       assembly_plass_all, assembly_opera_ms_all,\n       assembly_metaquast_all, assembly_report_all,\n       single_assembly_all, coassembly_megahit_all,\n       coassembly_all, assembly_all,\n       alignment_base_depth_all, single_alignment_all,\n       coalignment_base_depth_all,\n       coalignment_all, alignment_all,\n       binning_metabat2_coverage_all, binning_metabat2_all,\n       binning_maxbin2_all, binning_concoct_all, binning_graphbin2_all,\n       binning_dastools_all, binning_vamb_all,\n       binning_report_all, single_binning_all,\n       cobinning_metabat2_coverage_all, cobinning_metabat2_all,\n       cobinning_maxbin2_all, cobinning_concoct_all,\n       cobinning_graphbin2_all, cobinning_dastools_all, \n       cobinning_report_all, cobinning_all, binning_all,\n       predict_scaftigs_gene_prodigal_all, predict_scaftigs_gene_prokka_all,\n       predict_bins_gene_prodigal_all, predict_bins_gene_prokka_all,\n       single_predict_scaftigs_gene_all, single_predict_bins_gene_all,\n       copredict_scaftigs_gene_prodigal_all,\n       copredict_scaftigs_gene_prokka_all,\n       copredict_bins_gene_prodigal_all, copredict_bins_gene_prokka_all,\n       copredict_scafitgs_gene_all, copredict_bins_gene_all,\n       predict_scaftigs_gene_all, predict_bins_gene_all,\n       copredict_all, predict_all,\n       single_checkm_all, cocheckm_all, checkm_all,\n       dereplicate_mags_drep_all, dereplicate_mags_all,\n       classify_short_reads_kraken2_all,\n       single_classify_hmq_bins_gtdbtk_all,\n       coclassify_hmq_bins_gtdbtk_all, classify_hmq_bins_gtdbtk_all,\n       single_classify_all, coclassify_all, classify_all,\n       profiling_bgi_soap_all, profiling_bowtie2_all,\n       profiling_metaphlan2_all, profiling_metaphlan3_all,\n       profiling_jgi_all, profiling_bracken_all,\n       profiling_humann2_all, profiling_humann3_all,\n       profiling_all, upload_sequencing_all, upload_assembly_all, upload_all,\n       all (default: all)\n\n  optional arguments:\n  -h, --help            show this help message and exit\n  -d, --workdir WORKDIR\n                        project workdir (default: ./)\n  --config CONFIG       config.yaml (default: ./config.yaml)\n  --cluster CLUSTER     cluster.yaml (default: ./cluster.yaml)\n  --cores CORES         CPU cores (default: 8)\n  --jobs JOBS           qsub job numbers (default: 80)\n  --list                list pipeline rules\n  --run                 run pipeline\n  --debug               debug pipeline\n  --dry-run             dry run pipeline\n  --qsub                qsub pipeline\n  --wait WAIT           wait given seconds (default: 60)\n  --use-conda           use conda environment\n  --conda-prefix CONDA_PREFIX\n                        conda environment prefix\n                        (default: /ldfssz1/ST_META/share/User/zhujie/.conda/envs)\n  --conda-create-envs-only\n    conda create environments only\n```\n\n### Example\n\n```\n# init project\n$ metapi init -d . -s samples.tsv -b trimming\n\n# create conda environments\n$ metapi mag_wf --conda-create-envs-only\n\n# run pipeline with conda\n# metapi mag_wf all --use-conda\n# metapi mag_wf all --use-conda --conda-prefix /path/to/your/default/envs/dir\n\n# run raw_fastqc\n$ metapi mag_wf raw_fastqc_all --run\n\n# run trimming\n$ metapi mag_wf trimming_all --run\n\n# run rmhost\n$ metapi mag_wf rmhost_all --run\n\n# run qc report\n$ metapi mag_wf qcreport_all --run\n\n# run assembly\n$ metapi mag_wf assembly_all --run\n\n# run binning\n$ metapi mag_wf binning_all --run\n\n# run gene predict\n$ metapi mag_wf predict_all --run\n\n# run MAGs checkm\n$ metapi mag_wf checkm_all --run\n\n# run MAGs classify\n$ metapi mag_wf classify_all --run\n\n# run MetaPhlAn2 profiling\n$ metapi mag_wf profiling_metaphlan2_all --run \\\n  --use-conda --conda-prefix /ldfssz1/ST_META/share/User/zhujie/.conda/envs\n\n# run MetaPhlAn3 profiling\n$ metapi mag_wf profiling_metaphlan3_all --run\n\n# run MAGs jgi profling (using jgi_summarize_bam_contig_depths)\n$ metapi mag_wf profiling_jgi_all --run\n\n# run HUMAnN2 profiling\n$ metapi mag_wf profiling_humann2_all --run \\\n  --use-conda --conda-prefix /ldfssz1/ST_META/share/User/zhujie/.conda/envs\n\n# run mag_wf all\n$ metapi mag_wf --run\n\n# run gene_wf all\n$ metapi gene_wf --run\n```\n\n## input requirements\n\nThe input samples file: `samples.tsv` format:\n\nNote: If `id` col contain same id, then the reads of each sample will be merged.\nNote: The fastq need gzip compress.\n\n- begin from trimming, rmhost or assembly:\n\n  - `Paired-end reads`\n\n  |  id   |    fq1     |    fq2     |\n  | :---: | :--------: | :--------: |\n  |  s1   | aa.1.fq.gz | aa.2.fq.gz |\n  |  s2   | bb.1.fq.gz | bb.2.fq.gz |\n  |  s2   | cc.1.fq.gz | cc.2.fq.gz |\n  |  s3   | dd.1.fq.gz | dd.2.fq.gz |\n\n  - `Paired-end reads(interleaved)`\n\n  |  id   |     fq1     |  fq2  |\n  | :---: | :---------: | :---: |\n  |  s1   | aa.12.fq.gz |       |\n  |  s2   | bb.12.fq.gz |       |\n  |  s2   | cc.12.fq.gz |       |\n  |  s3   | dd.12.fq.gz |       |\n\n* `Paired-end reads with long reads`\n\n|  id   |    fq1     |    fq2     |    fq_long    |\n| :---: | :--------: | :--------: | :-----------: |\n|  s1   | aa.1.fq.gz | aa.2.fq.gz | aa.long.fq.gz |\n|  s2   | bb.1.fq.gz | bb.2.fq.gz | bb.long.fq.gz |\n|  s2   | cc.1.fq.gz | cc.2.fq.gz | cc.long.fq.gz |\n|  s3   | dd.1.fq.gz | dd.2.fq.gz | dd.long.fq.gz |\n\n- `Paired-end reads(interleaved) with long reads`\n\n|  id   |     fq1     |  fq2  |    fq_long    |\n| :---: | :---------: | :---: | :-----------: |\n|  s1   | aa.12.fq.gz |       | aa.long.fq.gz |\n|  s2   | bb.12.fq.gz |       | bb.long.fq.gz |\n|  s2   | cc.12.fq.gz |       | cc.long.fq.gz |\n|  s3   | dd.12.fq.gz |       | dd.long.fq.gz |\n\n- `Single-end reads`\n\n|  id   |    fq1     |  fq2  |\n| :---: | :--------: | :---: |\n|  s1   | aa.1.fq.gz |       |\n|  s2   | bb.1.fq.gz |       |\n|  s2   | cc.1.fq.gz |       |\n|  s3   | dd.1.fq.gz |       |\n\n- `SRA (only support paired-end reads)` :\n\nSRA can be dumpped to Paired-end fastq reads\n\n|  id   |  sra   |\n| :---: | :----: |\n|  s1   | aa.sra |\n|  s2   | bb.sra |\n|  s2   | cc.sra |\n|  s3   | dd.sra |\n\n- begin from simulate, only support paired-end reads\n\n  |  id   | genome | abundance | reads_num | model |\n  | :---: | :----: | :-------: | :-------: | :---: |\n  |  s1   | g1.fa  |    1.0    |    1M     | hiseq |\n  |  s2   | g1.fa  |    0.5    |    2M     | hiseq |\n  |  s2   | g2.fa  |    0.5    |    2M     | hiseq |\n  |  s3   | g1.fa  |    0.2    |    3M     | hiseq |\n  |  s3   | g2.fa  |    0.3    |    3M     | hiseq |\n  |  s3   | g3.fa  |    0.5    |    3M     | hiseq |\n\nIt means:\n\nThe sample s1 contain 1M reads which come from g1, the relatative abundance of\nspecies g1 is 1.0.\n\nThe sample s2 contain 2M reads, 1M reads come from g1\nand 1M reads come from g2. the relatative abundance of\nspecies g1 is 0.5, the relatative abundance of\nspecies g2 is 0.5.\n\nThe sample s3 contain 3M reads, 0.6M reads come from g1, 0.9M reads come from\ng2 and 1.5M reads come from g3, the relatative abundance of\nspecies g1 is 0.2, the relatative abundance of\nspecies g2 is 0.3, the relatative abundance of\nspecies g3 is 0.5.\n\nThen metapi will use [InSilicoSeq](https://github.com/HadrienG/InSilicoSeq) to generate metagenomics shutgun reads.\n\n## FAQ\n\n- You know what you want to do, so you know how to configure config.yaml\n- You know snakemake, so you know how to hack metapi\n\n## Getting help\n\nIf you want to report a bug or issue, or have problems with installing or\nrunning the software, please create [a new\nissue](https://github.com/ohmeta/metapi/issues).\nThis is the preferred way of getting support. Alternatively, you can [mail me](mailto:alienchuj@gmail.com).\n\n## Contributing\n\nContributions welcome! Send me a pull request or get in [touch](mailto:alienchuj@gmail.com).\n\nWhen contributing a PR, please use the [dev](https://github.com/ohmeta/metapi/tree/dev) branch.\nFor style, code will be checked using flake8,\n[black](https://github.com/psf/black), and\n[snakefmt](https://github.com/snakemake/snakefmt). These modules can be\ninstalled via conda, `conda install black flake8 flake8-bugbear snakefmt` or via\npip `pip install black flake8 flake8-bugbear snakefmt`.\n\n## Contributors\n\n- Jie Zhu - [@alienzj](https://github.com/alienzj)\n- Fangming Yang - [@yangfangming](https://github.com/yangfangming)\n- Yanmei Ju - [@juyanmei](https://github.com/juyanmei)\n\n## License\n\nThis module is licensed under the terms of the [GPLv3 license](https://opensource.org/licenses/GPL-3.0).\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ohmeta/metapi",
    "keywords": "",
    "license": "GPLv3+",
    "maintainer": "",
    "maintainer_email": "",
    "name": "metapi",
    "package_url": "https://pypi.org/project/metapi/",
    "platform": "",
    "project_url": "https://pypi.org/project/metapi/",
    "project_urls": {
      "Homepage": "https://github.com/ohmeta/metapi"
    },
    "release_url": "https://pypi.org/project/metapi/1.1.0/",
    "requires_dist": [
      "numpy",
      "pandas",
      "openpyxl",
      "snakemake",
      "ruamel.yaml",
      "biopython (>=1.73)"
    ],
    "requires_python": "",
    "summary": "a pipeline to construct a genome catalogue from metagenomics data",
    "version": "1.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14809957,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c8efa83f0860c33b7a1d1de21b61623833c0fba030e2d792c4792efa4bf0f352",
        "md5": "110a77a07a66668457de651bbfccb535",
        "sha256": "084aca6f0c716afb261df1665a67659493a8372bc6178aa8b6ce15932316c322"
      },
      "downloads": -1,
      "filename": "metapi-1.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "110a77a07a66668457de651bbfccb535",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 126386,
      "upload_time": "2021-04-23T07:04:59",
      "upload_time_iso_8601": "2021-04-23T07:04:59.540625Z",
      "url": "https://files.pythonhosted.org/packages/c8/ef/a83f0860c33b7a1d1de21b61623833c0fba030e2d792c4792efa4bf0f352/metapi-1.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ce0ee4d26ea03bea12bc3b9388ea1660c61b71daf16df240e64b018100dcf049",
        "md5": "890c19fb805755144c32d4807f0f5ad6",
        "sha256": "e16e0d414d437269ed8c6a8d79a762e8b3e1dfa2a97d1e9addb3b68df5053a05"
      },
      "downloads": -1,
      "filename": "metapi-1.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "890c19fb805755144c32d4807f0f5ad6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 92793,
      "upload_time": "2021-04-23T07:05:01",
      "upload_time_iso_8601": "2021-04-23T07:05:01.917035Z",
      "url": "https://files.pythonhosted.org/packages/ce/0e/e4d26ea03bea12bc3b9388ea1660c61b71daf16df240e64b018100dcf049/metapi-1.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}