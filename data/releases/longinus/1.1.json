{
  "info": {
    "author": "Hugo Souza",
    "author_email": "mail@hugosouza.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# SaintLonginus\n\n**Version 1.1**\n\n A experiment with web scrapping\n\nSaintLonginus is a web crawler that I developed while learning about web scrapping. It's functionality is to search for given words inside a set of pages.\nWhile it looks for references to the searched words within the page, it also looks for links that lead to other pages, increasing its queue of pages to search in.\n\nIt's built on top of [Selenium](https://www.selenium.dev/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n\nIt's name comes from a popular tradition which says that the catholic saint [Longinus](https://en.wikipedia.org/wiki/Longinus) can help you to find anything you are looking for. \n\nThis project is open-source, so feel free to contribute, report issues or make suggestions.\n\nFor any other cases, feel free to email me at [mail@hugosouza.com](mailto:mail@hugosouza.com)\n\n## Doumentation\n\nHere is how you can this crawler\n\n### Longinus class\n\nThe program's main class.\n\n#### Class Constructor:\n\n##### Longinus(name: str, threads: int = 4, wait_for_page_load_ms: int = 500, when_find: callable = write_to_file, save_bot: bool = False)\n\nThis is the class constructor, it takes some arguments:\n\n* *name (string)*: The name of that crawler instance\n\n* *threads (int)*: How many threads it's going to use while searching\n\n* *wait_for_page_load_ms (int)*:  How many milisseconds the crawler is going to wait for the page to load and run all of its scripts until return the source of that page *(this is important to make sure that pages built with React, Angular, Vue or even vanilla JavaScript can render everything it should before the crawler starts scrapping it)*\n\n* *when_find (callable)*: A function that is going to be executed when there's a reference to a specific keyword on that page. The default function writes the url of that page in a text file called **results.txt**. It must take 3 arguments containing the **url of the page**, **keyword found at the page** and **the whole text of the page**:\n  \n  ```python\n  def write_to_file(url, keyword, full_text):\n      if \"google.com\" in url:\n          return\n      with open(\"results.txt\", \"a+\") as file:\n          file.write(\"{}: {} \\n\".format(url, keyword))\n          file.write(full_text + \"\\n\\n\")\n          file.close()\n  ```\n\n* *save_bot (bool)*: Sets if the bot is going to be periodically saved into a file for further loading. The frequency of the savings can be defined at setup.\n\n#### Instance methods:\n\n##### setup(depth: int = 3, strategy=SHALLOW_LINKS, bonus_when_match: int = 1, saving_frequency = NORMAL)\n\nThis method configures the process of searching that the bot is going to use. It takes the arguments:\n\n* *Depth (int):* The amount of links beyond the root that are going to be scrapped\n* *strategy (STRATEGY CONSTANT)*: The searching strategy that is going to be used (Read the strategy section) \n* *bonus_when_match (int)*: This bonus is a value that is added to the depth when the bot finds a reference to a keyword on a page. It's a way to \"reward\" the page for having a reference to a keyword because it's likely its links also lead to pages with references to a keyword.\n* *saving_frequency (FREQUENCY CONSTANT)*: Defines the frequency the bot is going to the saved (Read the saving frequency section)\n\nIf you try to start the search without calling **setup** first, it will raise a **AssertionError**.\n\n##### set_url(new_urls: list)\n\nChanges the urls that the crawler is going to take as root when it starts searching. The root urls are the first ones to be scrapped, and starting from them, the scrapper is going to find the next urls to be visited.\n\nWhen a Longinus object is instantiated, the urls' default value is *None*, in that case, the crawler is going to perform a Google search for each keyword and gather a list of urls from the results to be used as root.\n\n##### set_callback(new_callback: callable):\n\nSets what function is going to be executed when there's a reference to a specific keyword on that page. The default function writes the url of that page in a text file called **results.txt**. It must take 4 arguments containing the **url of the page**, **keyword found at the page**, **the whole text of the page** and **the title of the HTML document**:\n\n```python\ndef write_to_file(url, keyword, full_text, title):\n    if \"google.com\" in url:\n        return\n    with open(\"results.txt\", \"a+\") as file:\n        file.write(\"{} ({}): {} \\n\".format(title, url, keyword))\n        file.write(full_text + \"\\n\\n\")\n        file.close()\n```\n\n##### set_filter(new_filter: callable)\n\nSets a new filtering function. A filtering function is used to determinate which pages are going to be crawled and which ones aren't based on their url.\n\nThe default function filters pages that contains *\"webcache.googleusercontent.com\"* on their url.\n\nIf you want to create a custom filtering function, follow the structure:\n\n```python\ndef custom_filter(url: str):\n    if condition_to_be_filtered(url):\n        return True     # This page is going to be filtered (not crawled)\n    return False        # This page is going to be crawled (not filtered)\n```\n\n##### start(keywords: list)\n\nStarts the searching looking for the words described in the **keywords** list.\n\nThe search follow the processes configured in the **setup** method so it must be called after the **setup**.\n\n**If the bot as loaded from a save-file, there's no need to speci**\n\n#### Loading a bot\n\nIf you had previously saved a bot and you want to re-load it, you can do it with the:\n\n##### load_bot_from_save(save_filename: str)\n\nThis function acts as a constructor, it reads the bot saved file and constructs another instance of the bot with the same specifications, history and searching queue.\n\nThe **save_filename** argument represents the name of the file generated when the previous bot was saved.\n\n#### Strategies\n\nThere are several constants representing strategies that can be used to setup the crawler:\n\n* *ONLY_ORIGIN_DOMAIN*: Doesn't follow links that lead to a different domain or subdomain\n\n* *ONLY_SUBDOMAINS*: Follow only origin domain and subdomains\n\n* *FOLLOW_ALL_LINKS:* Follows everything that comes up on the page\n\n* *SHALLOW_LINKS:* Follows links that lead to other domains with depth 0\n\n#### Saving frequency\n\nThere are several constants representing the frequency a bot will be saved:\n\n* *ULTRA_FREQUENT*: Will save the bot after each single link searched. (NOT RECOMENDED, IT MIGHT CAUSE THE BOT TO BE EXTREMELLY SLOW)\n\n* *VERY_FREQUENT*: Will save the bot after each 10 links searched.\n\n* *FREQUENT*: Will save the bot after each 25 links searched.\n\n* *NORMAL*: Will save the bot after each 50 links searched.\n\n* *SOMETIMES*: Will save the bot after each 100 links searched.\n\n* *RARELY*: Will save the bot after each 500 links searched.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/fatorius/Longinus",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "longinus",
    "package_url": "https://pypi.org/project/longinus/",
    "platform": null,
    "project_url": "https://pypi.org/project/longinus/",
    "project_urls": {
      "Bug Tracker": "https://github.com/fatorius/Longinus/issues",
      "Homepage": "https://github.com/fatorius/Longinus"
    },
    "release_url": "https://pypi.org/project/longinus/1.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A experimental web crawler",
    "version": "1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13763455,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "56e22e13a0336b2a1a6f993e9935853469d88733360454e9e16f5ba15f3b24bc",
        "md5": "f8dbde82094923841349610c5b59309a",
        "sha256": "40b54df04d27ebfa78a0f81d79b79c3154d3bd3b46ee3c8215f44684c81f78c0"
      },
      "downloads": -1,
      "filename": "longinus-1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f8dbde82094923841349610c5b59309a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 9360,
      "upload_time": "2022-05-09T21:56:33",
      "upload_time_iso_8601": "2022-05-09T21:56:33.565250Z",
      "url": "https://files.pythonhosted.org/packages/56/e2/2e13a0336b2a1a6f993e9935853469d88733360454e9e16f5ba15f3b24bc/longinus-1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "27bd5fd94ac9b5f7a0530e22d154c5eab89bff6527389520faed9ea6b7bac83a",
        "md5": "19224860dee3be74c090e733da94a589",
        "sha256": "064430acfee8708497c16e813923672d4c26991236a838b353f6bb1b43907ac5"
      },
      "downloads": -1,
      "filename": "longinus-1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "19224860dee3be74c090e733da94a589",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 11053,
      "upload_time": "2022-05-09T21:56:36",
      "upload_time_iso_8601": "2022-05-09T21:56:36.933144Z",
      "url": "https://files.pythonhosted.org/packages/27/bd/5fd94ac9b5f7a0530e22d154c5eab89bff6527389520faed9ea6b7bac83a/longinus-1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}