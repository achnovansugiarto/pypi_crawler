{
  "info": {
    "author": "Enoch2090",
    "author_email": "ycgu2090@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "\n- # MAGI Dataset\n\n  ## Install\n\n  ```python\n  pip install magi_dataset\n  ```\n\n  If you plan on using magi_dataset to periodically crawl data, set the following variables in your environment:\n\n  ```shell\n  export GH_TOKEN=\"Your token\"\n  ```\n\n  Read [Creating a personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) for more information on creating GitHub personal access token. If using the default data without crawling new data, you may safely ignore this token. You can either provide the GitHub token using `gh_token` argument when initializing the `GitHubDataset` object, or setting it as an environment variable `GH_TOKEN` in your shell. If neither provided, the GitHub API will be initialized with no token, and the rate limit will be not sufficient for subsequent operations.\n\n  ## Usage\n\n  ###  Initiate Using Defaults\n\n  You may initiate a `GitHubDataset` object directly using source we provided. Currently supported sources can be viewed at [list.json](https://huggingface.co/datasets/Enoch2090/github_semantic_search/blob/main/list.json). For example:\n\n  ```python\n  >>> from magi_dataset import GitHubDataset\n\n  >>> github_dataset = GitHubDataset(\n  ...     empty = False,\n  ...     file_path = 'rust-latest'\n  ... )\n  ```\n\n  Which will download `ghv10_rust-metadata.json`, `ghv10_rust-0.json`and `ghv10_rust-1.json` under `./magi_downloads`, and use them to create a dataset. Downloading from curated sources only cost time of downloading files, which is usually <500MB. \n\n  ### Pull Data by Chunks\n\n  Pulling data from original sources is time-consuming. The recommended way to use `magi_dataset` is to run the collection process in chunked mode. First create an empty dataset and initiate index from GitHub:\n\n  ```python\n  >>> from magi_dataset import GitHubDataset\n\n  >>> github_dataset = GitHubDataset(\n  ...     empty = True\n  ... )\n\n  >>> github_dataset.init_repos(fully_initialize=False)\n  >>> github_dataset.dump('./outputs/gh_data.json')\n  ```\n\n  After this process, the fingerprint `./outputs/gh_data-metadata.json` will be generated, which contains both metadata of this dataset and a fixed index of the repos to pull. Based on this metadata file, you can run multiple instances of `GitHubDataset` to pull data from online sources by chunks. For example:\n\n  ```python\n  # create a new GitHubDataset object in another terminal\n  >>> from magi_dataset import GitHubDataset\n\n  >>> github_dataset = GitHubDataset(\n  ...     empty = True，\n  ...     # use tokens from different accounts to increase throughput\n  ...     gh_token = 'ghp_token1'\n  ... )\n\n  >>> github_dataset.load_fingerprint('./outputs/gh_data-metadata.json')\n\n  >>> github_dataset.update_repos(\n  ...     chunks = range(0, 50）\n  ... )\n\n  >>> github_dataset.dump(\n  ...     './outputs/gh_data.json',\n  ...     chunks = range(0, 50）\n  ... )\n  ```\n\n  Which dumps `gh_data-0.json`, `gh_data-1.json`, ..., `gh_data-49.json` under the `./outputs` directory. You can also copy the fingerprint metadata file to other machines to pull different chunks, in order to relieve some stress on IP address limits of the translation API. Make sure to use tokens from different GitHub accounts in diffenent terminals/on different machines.\n\n  Alternatively, we provide an entry from shell to do this. You can run for each coding language:\n\n  ```bash\n  magi_dataset --lang Python --file ./outputs/gh_data_python-metadata.json --meta_only True --gh_token $GH_TOKEN\n  ```\n\n  And after copying `./outputs/gh_data_python-metadata.json` to other machines, run on them separately:\n\n  ```bash\n  magi_dataset --lang Python --file ./outputs/gh_data_python.json --meta_only False --load_meta ./outputs/gh_data_python-metadata.json --gh_token $GH_TOKEN\n  ```\n\n  ### Pull Data All Together\n\n  If the data is not much (for example, setting `GitHubDataset.MIN_STARS_PER_REPO > 2000`), you can also pull all data together once.To do so, initialize an empty instance and collect data:\n\n  ```python\n  >>> from magi_dataset import GitHubDataset\n\n  >>> github_dataset = GitHubDataset(\n  ...     empty = True\n  ... )\n\n  >>> github_dataset.init_repos(fully_initialize=True)\n  ```\n\n  Or, download the default data (not guranteed to be the newest):\n\n  ```python\n  >>> from magi_dataset import GitHubDataset\n\n  >>> github_dataset3 = GitHubDataset(\n  ...\t    empty = False\n  ... )\n  ```\n\n  The default data may be found at [Enoch2090](https://huggingface.co/Enoch2090)/[github_semantic_search](https://huggingface.co/datasets/Enoch2090/github_semantic_search/blob/main/list.json) on HuggingFace. We will update the data periodically.\n\n  After the dataset is created, access the data with either number index:\n\n  ```python\n  >>> github_dataset[5]\n  GitHubRepo(name='ytdl-org/youtube-dl', stars=114798, description='Command-line program to download videos from YouTube.com and other video sites', _fully_initialized=True)\n  ```\n\n  Or the full name:\n\n  ```python\n  >>> github_dataset['ytdl-org/youtube-dl']\n  GitHubRepo(name='ytdl-org/youtube-dl', stars=114798, description='Command-line program to download videos from YouTube.com and other video sites', _fully_initialized=True)\n  ```\n\n  And you can access the corpus by accessing the `readme` and `hn_comments` attributes of `GitHubRepo` objects.\n\n  ```python\n  >>> github_dataset[5].readme[0:100]\n  '[![Build Status](https://github.com/ytdl-org/youtube-dl/workflows/CI/badge.svg)](https'\n  ```\n\n  ## Future Works\n\n  - The current idle handler design is primordial, will switch to async pipelines to relieve CPU sleep time.\n  - Elasticsearch database builder\n  - Pinecone database builder (wrapper only)\n  - Hash verification of persistence files\n\n  ## Changelogs\n\n  ### v1.0.5 \n\n  Updated default list of files to ghv10. Users may also retrieve default files with keyname in the latest list. For example if the list states\n\n  ```json\n  {\n    \"python-latest\": [\n      \"https://huggingface.co/datasets/Enoch2090/github_semantic_search/resolve/main/ghv10_python-metadata.json\",\n      \"https://huggingface.co/datasets/Enoch2090/github_semantic_search/resolve/main/ghv10_python-0.json\",\n      \"https://huggingface.co/datasets/Enoch2090/github_semantic_search/resolve/main/ghv10_python-1.json\",\n    ]\n  }\n  ```\n\n  User may retrieve these files by simply calling\n\n  ```python\n  >>> github_dataset3 = GitHubDataset(\n  ...\t    empty = False,\n  ...     file_path = \"python-latest\"\n  ... )\n  ```\n\n  Which is similar to Huggingface model initiation.\n\n  ### v1.0.4\n\n  - Added chunked update/dump/loads. Now when saving to files, only `.json` is allowed. Due to the unsafe nature of `.pkl` files and other reasons, `.pkl` files will not be supported in the future. \n  - If saving a `GitHubDataset` with $N$ chunks to file name `data.json`, will create `data-metadata.json`, and `data-0.json`, `data-1.json` ... `data-$(N-1).json`.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Enoch2090/magi_dataset",
    "keywords": "",
    "license": "GPLv3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "magi-dataset",
    "package_url": "https://pypi.org/project/magi-dataset/",
    "platform": null,
    "project_url": "https://pypi.org/project/magi-dataset/",
    "project_urls": {
      "Homepage": "https://github.com/Enoch2090/magi_dataset"
    },
    "release_url": "https://pypi.org/project/magi-dataset/1.0.5/",
    "requires_dist": [
      "Markdown",
      "PyGithub",
      "beautifulsoup4",
      "deep-translator",
      "hn",
      "langdetect",
      "lxml",
      "networkx",
      "numpy (>=1.15.4)",
      "pandas (>=1.2.0)",
      "python-hn",
      "requests",
      "scipy",
      "setuptools",
      "spacy",
      "tqdm",
      "elasticsearch ; extra == 'elasticsearch'"
    ],
    "requires_python": ">=3.7.0",
    "summary": "Convenient access to massive corpus of GitHub repositories",
    "version": "1.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17234876,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2276a02320a80283e9dc6040bff830d2be2049337ddb586d8cf42bca11b1358c",
        "md5": "42e657f0c1dde980343c8a8a5572159f",
        "sha256": "03b9e9eb0735c7e868804e77b7e061809ab12617196feaf8d2b673fbe7753e3b"
      },
      "downloads": -1,
      "filename": "magi_dataset-1.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "42e657f0c1dde980343c8a8a5572159f",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7.0",
      "size": 14122,
      "upload_time": "2023-02-19T21:32:15",
      "upload_time_iso_8601": "2023-02-19T21:32:15.857428Z",
      "url": "https://files.pythonhosted.org/packages/22/76/a02320a80283e9dc6040bff830d2be2049337ddb586d8cf42bca11b1358c/magi_dataset-1.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}