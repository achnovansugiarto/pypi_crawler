{
  "info": {
    "author": "Resero-Labs",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "[![TravisCI](https://api.travis-ci.org/resero-labs/docker-utils.svg?branch=master)](https://travis-ci.org/resero-labs/docker-utils) [![Coverage](https://codecov.io/gh/resero-labs/docker-utils/branch/master/graph/badge.svg)](https://codecov.io/gh/resero-labs/docker-utils) [![PyPi](https://img.shields.io/pypi/v/dockerutils.svg)](https://pypi.org/project/dockerutils/) [![PyPi](https://img.shields.io/pypi/wheel/dockerutils.svg)](https://pypi.org/project/dockerutils/) \n[![Python 3.7](https://img.shields.io/badge/python-3.7-blue.svg)](https://www.python.org/downloads/release/python-370/) \n[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/) \n[![Python 3.5](https://img.shields.io/badge/python-3.5-blue.svg)](https://www.python.org/downloads/release/python-350/) \n[![Python 3.4](https://img.shields.io/badge/python-3.4-blue.svg)](https://www.python.org/downloads/release/python-340/) \n[![Python 2.7](https://img.shields.io/badge/python-2.7-blue.svg)](https://www.python.org/downloads/release/python-270/) \n\n# Docker Utilities/Patterns\n\nDockerutils is a set of utilities and conventions around their use. The intent behind these utilities is to provide a \nvery light layer of abstraction to: simplify interaction with docker; support seamlessly running docker locally or on \nec2 instance in AWS; allow for multiple images per project; etc.\n\nPerhaps the best way to think of dockerutils is that it embodies two entities, commonly used when working\nwith docker, and a set of complementary commands for working with those entities. The two entities are:\n* \"dock\" - the server that is hosing docker (by default, localhost)\n* image - the standard docker image\n\nThe commands used to operate against these entities are:\n\n| CommandSet /  Entity |         Creation         |       Execution      |   Notebook   |            Utility           |\n|----------------------|:------------------------:|:--------------------:|:------------:|:----------------------------:|\n|         Dock         | create-dock <br/> destroy-dock | start-dock <br/> stop-dock | nb-dock      | source dock <br/> ls-dock <br/> ssh-dock |\n|         Image        | build-image              | run-image            | run-notebook | publish-image <br/> transfer-image |\n\nPossible use cases include:\n* seperating development/test dependencies out of production container, e.g. production container vs. dev/test container\n* seperating data science notebook container from execution container\n* environment experimentation\n* cases in which you want to \"freeze\" any external dependencies in one container and use that as a base\nfor containers that are dependent solely on the project\n\n## Conventions\n\n1) Create a docker directory tree at the root of the project\n\n    In this directory tree there should be one sub-directory for each unique docker container type that is desired.\n    Each of these sub-directories would contain the `Dockerfile` that will be used to create the image\n    as well as any source specific to that image.\n2) Use versioneer for project versioning (Optional). \n\n    As part of the image build, a file, `_version.py.bld`, will be generated and placed at the project \n    root. A `Dockerfile` can add that file to the image on creation to prevent the need for including the\n    .git directory tree in the container context (usually quite expensive).\n3) Create a docker/base directory to make use of built in external dependency isolation (optional)\n\n    This capability supports environments where a docker build isn't able to access external dependencies (Docker Hub, \n    pypi, etc.), for instance a server in a \"locked-down\" environment. A base image can be defined to isolate any \n    dependencies that are required. That image can then be built and `transfer-image` used to transfer the base image \n    to the target environment.\n\n    Subsequent images can be built based off of that image that are \"self-contained\" (relying only on source\n    from the project). The remote docker api can then be used to quickly iterate only requiring the more\n    cumbersome transfer-image to be used when external dependencies change.\n4) Building and running images controlled through configuration (<project_dir>/docker/dockerutils.cfg)\n\n    Includes setting most docker parameters, i.e. volume mounts, ports, networks, commands, etc. with\n    replacement varilable support for things like user, project root, etc.\n\n## Command-line Interface\n\n### Image cli\n`build-image` takes the name of one of the sub-directories in the `docker` directory and builds the\nimage defined therein. The image is named \\<project\\>-\\<subdir\\>:\\<user\\>\n\n`run-image` takes the name of one of the sub-directories (or one of the synthetic images defined in `dockerutils.cfg`), \ntogether with any of the configuration for that image defined in `dockerutils.cfg` and starts a docker container\n\n`transfer-image` takes a docker image name and uses docker `save` and `load` to transfer the image to a remote host\n\n`publish-image` takes the name of one of the sub-directories in the `docker` directory and pushes the image built by \nthe docker file to the defined repository (AWS or Docker)\n\n### Notebook cli\n`run-notebook` will start a docker container using either the notebook container found in the `docker/notebook` directory\nif it exists, or [resero-labs/docker-ds](https://github.com/resero-labs/docker-ds) otherwise. The current directory will be mounted\ninto the container for use in the Juypter notebook environment. There are a couple of environment variable to be aware of \nwith this command:\n\n* DOCKER_DS_DONT_PULL - if set, the version of resero-labs/docker-ds currently available will be used rather than pulling \nthe latest version from docker hub.\n* RESERO_JUPYTER_DIFFS - if set, on save, `.py` files and `.html` files for the notebook will be created in a `.diffs` subdirectory.  \n\n### Dock cli\n\nA \"dock\" is a remote system that has a docker daemon running and configured in a secure fashion (generally an EC2 \ninstance). You can \"dock\" your terminal to a remote instance and any docker commands, including image and notebook cli\nabove will be run against the remote docker server. Once a \"dock\" is created, you can dock your terminal by issuing \nthe command `source dock <server IP or moniker>`\n\n`create-dock` will start an ec2 instance that can be used for remote docking. This instance is configured to provide \nsecure interaction with the docker server, as well as to support GPU utliziation (`-g` option with `run-image`)\n\n`destroy-dock` will terminate a remote dock instance and delete any local configuration files\n\n`stop-dock` will change the instances state of a remote dock to `stopped`\n\n`start-dock` will change the instance state of a remote dock to `running`\n\n`ssh-dock` opens a terminal on the remote dock with ssh\n\n`ls-dock` list (including state) any created docks\n\n`nb-dock` run jupyter on the bare AMI of the dock and open a browser window to the notebook server\n\n## `dockerutils.cfg` Format\nConfiguration in `docker/dockerutils.cfg` is used to configure behavior of the `dockerutils` scripts.\n\n### Image Section\nThe `dockerutils.cfg` file format allows for configuration sections named corresponding to the sub-directories in the \ndocker directory tree. Each of these sections may contain one of the following:\n\n* `environment` - in the form of (`-e VAR=value`)+ to pass environment into the container\n* `interactive` - either -e or -it\n*  `gpu` - conforms to NVIDIA Docker 2.0 specification, e.g. `--runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all`, \n`run-image -g` will add this automatically\n* `network` - the name of the network for the container\n* `volumes` - in the form of (`-v <host-dir>:<container-dir>`)+ or (`--mount ...`)+ or both\n* `ports` - in the form of (`-p <host-port>:<container-port>`)+\n* `cmd` - any valid Docker `CMD` specification\n* `pull_FROM_on_force` - defaults to False, if True, add --pull to build command when force building image (or base image)\n* `image_repo` - the repository to publish the image to\n* `publication_tag` - the tag for publication (full image name + tag) \n* `pre_build_script` - A shell command or script to run before a docker build is issued\n* `post_build_script` - A shell command or script to run after a docker build has been compeleted (successfully)\n\n### Synthetic Images\nAdditionally, \"synthetic\" images can be specified by adding a `run-image` section with a `synthetic_images` definition\nthat contains a list of \"synthetic\" images. Each of these may also have a named section as defined for the docker\nsub-directories, but must also contain a `name` value that resolves to one of the docker sub-directories. For example:\n\n```\n[run_image]\nsynthetic_images=shell\n\n[shell]\nname=dev\n...\n```\n\n### Configuration-only Images\nIf there is a docker container that does what you want already, you can create a configuration-only image by \nspecifying `name`, `tag` and `prefix=False` in the configuration section for the image. For example the base notebook \nimage `resero-labs/docker-ds` is often sufficient for running a Jupyter notebook against your code, as it auto detects a \n`setup.py` upon container start and installs the module into the notebook environment.\n\n### Image Tagging\nThe default tag for any image created/run/etc. is the user name in the host environment when running the \nutility. This can be overriden by adding a `tag` value to the desired section. For example:\n\n```\n[dev]\ntag=experiment.2017.12.16\n...\n```\n### Volume Replacement Variables\n\nThe volume specification may contain either environment variables (`$name` and `${name}` formats) as well as specific\nvariable replacement designations of the form `{var}`.  The supported variables include:\n\n* `project_root` - will be replaced with the root directory name of the project\n* `user` - will be replaced with the user name of the user running the command\n* `project` - replace with project namge\n\n### Image Push Replacement Variables\n\nThe `publication_tag` may contain either environment variables (`$name` and `${name}` formats) as well as specific\nvariable replacement designations of the form `{var}`.  The supported variables include:\n\n* `account` - AWS account designation\n* `region` - AWS region\n* `image` - Image name\n* `tag` - Image tag \n* `user` - will be replaced with the user name of the user running the command\n\n### Publish to AWS\nIn order to publish to AWS, the repository will be the image name and the following should\nbe configured for images that are published to AWS\n\n```\nimage_repo=aws\npublication_tag={account}.dkr.ecr.{region}.amazonaws.com/{image}:{tag}\"\n```\n\n## Patterns\n\n### Running your code in container, making live modifications outside container in your editor of choice     \n\nIf you're like me, you have a whole set of tools in your host environment that you use to work with your project.\nOne of the disadvantages of working with Docker can be the difficulty of transplanting those tools into the container\nenvironment. Perhaps there is a way to have your cake and eat it too!\nThe `dev` example does a reasonable job of doing just this. \n\nWith this pattern, you create a `Dockerfile` that has everything in the image *except* for your project source. An\nempty `WORKDIR` is created and then `ENTRYPOINT` even does a `pip install -e` of the contents of the empty `WORKDIR`. \nWe get the desired results by mounting the source project directory into the container's `WORKDIR` (see `dev` section\nof `docker/dockertuils.cfg`).\n\nWith this pattern you can run tests, experiment, etc. in container, make changes to the project in your host \nenvironment toolset and immediately observe the changes that were made.\n\n### Working with a server in a locked-down environment\nYou may find yourself in a situation in which you need to work with a server hosting Docker in an environment that has\nlimited access to the \"outside world\". This pattern can be used to capture all external dependencies in a base \nimage that is built in an environment that is open, use `transfer-image` to send this base image to the server and\nthen utilize a derived image dependent just on project sources and the base image to iterate without requiring open\naccess on the server.\n\n### Adding test frameworks, code analysis tools, etc. to a container for testing/validation\n\n### Tensorflow for both CPU and GPU in the same container\nIt is sometimes useful to try both the CPU and GPU versions of Tensorflow. The example in `docker/tensorflow` provides\na pattern to do so. All tensorflow depdendencies are installed into global python site-packages. Then virtual \nenvironments are created for both cpu and gpu versions and the appropriate version of tensorflow is install into \nthe respective virtual environments.\n\nThe `run-image` script makes use of NVIDIA Docker 2.0 being installed on the host os. When run with the `-g` option,\nthe NVIDIA runtime will be used to make GPUs available in container.\n\nUpon container startup, the appropriate virtual environment (cpu or gpu) will be activated dependant upon the \n`-g` option.\n\nTo switch between virtual envrionments utilized the symbolic links, `/cpu-env` and `/gpu-env`, e.g. \n`source /cpu-env`.\n\n### Versioneer support\nIf you aren't using [versioneer](https://github.com/warner/python-versioneer) to version your Python projects, you \nshould be. Versioneer utilizes .git to determine the project version. Having correct version information in container \nis desireable in many cases, but pushing .git into container usually isn't desireable. To capture version information\nfor container, the `genversion` utility is available and uses capabilities of versioneer to generate a _version.py.bld\nfile in the project root. This file is available for `Dockerfile` to add/overwrite the version.py file within the \nDocker image. An example of this is found in the cicd sub-directory.\n\n### `.dockerignore`\nIn order to minimize the context sent to Docker to build images, please see examples in `.dockerignore` in this\nrepository.\n\n## A Note on Implementation\nWe selected to use the Docker command line from python scripts rather than the [Docker\nAPI available to Python](https://pypi.python.org/pypi/docker/) as it integrates better into my development loop. As each CLI prints \nthe Docker command it's using, if it does something unexpected, it's each to copy and paste\nthe command used, modify it and be on your way without having to debug through CLI utility code, \nallowing bugs/additions to the CLI to be addressed at a later point outside the context of a\nproject development loop.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/resero-labs/docker-utils.git",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dockerutils",
    "package_url": "https://pypi.org/project/dockerutils/",
    "platform": "",
    "project_url": "https://pypi.org/project/dockerutils/",
    "project_urls": {
      "Homepage": "https://github.com/resero-labs/docker-utils.git"
    },
    "release_url": "https://pypi.org/project/dockerutils/2.0.1/",
    "requires_dist": [
      "awscli",
      "boto3",
      "future",
      "ConfigParser ; python_version <= \"2.7\"",
      "wheel (>=0.29) ; extra == 'dev'",
      "pytest (>=3.0) ; extra == 'test'",
      "pytest-cov (>=2.4) ; extra == 'test'",
      "pylint (>=1.8.1) ; extra == 'test'"
    ],
    "requires_python": "",
    "summary": "Docker Utilities/Patterns for Python Projects",
    "version": "2.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9331518,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e927f456dd8630e869b63889a631dd9676d101a40260d0331e604adbe6afcbd2",
        "md5": "bf68521b1968fa9064b5eb806e65a93b",
        "sha256": "dbe323237a7f79c144ee506991058c9332669b33802d1a876baed6777eb42974"
      },
      "downloads": -1,
      "filename": "dockerutils-2.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "bf68521b1968fa9064b5eb806e65a93b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 28485,
      "upload_time": "2019-08-06T20:28:37",
      "upload_time_iso_8601": "2019-08-06T20:28:37.870272Z",
      "url": "https://files.pythonhosted.org/packages/e9/27/f456dd8630e869b63889a631dd9676d101a40260d0331e604adbe6afcbd2/dockerutils-2.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}