{
  "info": {
    "author": "amaury.bignaud@pasteur.fr",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Bio-Informatics",
      "Topic :: Scientific/Engineering :: Visualization"
    ],
    "description": "# MetaTOR\n\n[![PyPI version](https://badge.fury.io/py/metator.svg)](https://badge.fury.io/py/metator)\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/metator.svg)\n[![Build Status](https://github.com/koszullab/metator/actions/workflows/python-package.yml/badge.svg)](https://github.com/koszullab/metaTOR/actions)\n[![codecov](https://codecov.io/gh/koszullab/metator/branch/master/graph/badge.svg)](https://codecov.io/gh/koszullab/metator)\n[![Docker Cloud Build Status](https://img.shields.io/docker/cloud/build/koszullab/metator)](https://hub.docker.com/r/koszullab/metator)\n[![Read the docs](https://readthedocs.org/projects/metator/badge)](https://metator.readthedocs.io)\n[![License: GPLv3](https://img.shields.io/badge/License-GPL%203-0298c3.svg)](https://opensource.org/licenses/bo-3.0)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\nMetagenomic Tridimensional Organisation-based Reassembly - A set of scripts that streamlines the processing and binning of metagenomic metaHiC datasets.\n\n## Table of contents\n\n* [Installation](#Installation)\n  * [Requirements](#Requirements)\n  * [Using pip](#Using-pip)\n  * [Using docker container](#Using-docker-container)\n* [Usage](#Usage)\n* [Output files](#Output-files)\n* [References](#References)\n* [Contact](#Contact)\n\n## Installation\n\n### Requirements\n\n* Python 3.6 or later is required.\n* The following librairies are required but will be automatically installed with the pip installation: `numpy`, `scipy`, `sklearn`, `pandas`, `docopt`, `networkx` `biopython` `pyfastx` and `pysam`.\n* The following software should be installed separately if you used the pip installation:\n  * [bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)\n  * [samtools](http://www.htslib.org/)\n  * [louvain](https://sourceforge.net/projects/louvain/) (original\n        implementation).\n  * [networkanalysis](https://github.com/vtraag/networkanalysis) (not\n    necessary only if you want to use Leiden algorithm to partition the network)\n  * [checkm](https://github.com/Ecogenomics/CheckM)\n  * [pairix](https://github.com/4dn-dcic/pairix) (not necessary if you do not use metator contact map)\n\n### Using pip\n\n```sh\n   pip3 install metator\n```\n\nor, to use the latest version:\n\n```sh\n   pip3 install -e git+https://github.com/koszullab/metator.git@master#egg=metator\n```\n\nIn order to use Louvain or Leiden it's necessary to set a global variable `LOUVAIN_PATH` and `LEIDEN_PATH` depending on which algorithm you wan to use with the absolute path where the executable are.\n\nFor Louvain algorithm in the directory where you have the archive file (available in the external directory of this repository):\n\n```sh\nYOUR_DIRECTORY=$(pwd)\ntar -xvzf louvain-generic.tar.gz\ncd gen-louvain\nmake\nexport LOUVAIN_PATH=$YOUR_DIRECTORY/gen-louvain/\n```\n\nFor Leiden algorithm, clone the networkanalysis repository from github and build the Java script. Then you can export the Leiden path:\n\n```sh\nexport LEIDEN_PATH=/networkanalysis_repository_path/build/libs/networkanalysis-1.2.0.jar\n```\n\n### Using docker container\n\nA dockerfile is also available if that is of interest. You may fetch the image by running the following:\n\n```sh\n    docker pull koszullab/metator\n```\n\n## Usage\n\n```sh\n    metator {network|partition|validation|pipeline} [parameters]\n```\n\nA metaTOR command takes the form `metator action --param1 arg1 --param2\narg2 #etc.`\n\nThere are three actions/steps in the metaTOR pipeline, which must be run in the\nfollowing order:\n\n* `network` : Generate metaHiC contigs network from fastq reads or bam files and normalize it.\n* `partition` : Perform the Louvain or Leiden community detection algorithm many times to bin contigs according to the metaHiC signal between contigs.\n\n* `validation` : Use CheckM to validate the bins, then do a recursive decontamination step to remove contamination.\n\nThere are a number of other, optional, miscellaneous actions:\n\n* `pipeline` : Run all three of the above actions sequentially or only some of them depending on the arguments given. This can take a while.\n* `contactmap` : Generates a contact map from one bin from the final ouptut of metaTOR.\n\n* `version` : display current version number.\n\n* `help` : display help message.\n\nA tutorial is available [here](example/metator_tutorial.md) to explain how to use metaTOR. More advanced tutorials to analyze the output files are also available:\n\n* [Anvio](https://merenlab.org/software/anvio/) manual curation of the contaminated bins. Available [here](example/manual_curation_of_metator_MAGs.md).\n* Visualization and scaffolding of the MAGs with the contactmap modules of MetaTOR. Available [here](example/MAG_visualization_and_scaffolding.md).\n\nPrinciple of MetaTOR pipeline:\n\n![metator_pipeline](example/images/metator_figure.png)\n\n## Output files\n\nThe output files will be in the ouput directory given as parmater or in working directory if no paramater were given. Depending on the command used, different files will be in the ouptut:\n\n| Files/Commands | description | network | partition | validation | pipeline |\n| - | :-: | :-: | :-: | :-: | :-: |\n| alignment_N_for.bam |Bam file of the forward alignment |X|||X|\n| alignment_N_rev.bam |Bam file of the reverse alignment|X|||X|\n| alignment_N.pairs |Pairs of the merge alignment|X|||X|\n| network.txt |Normalized network of the metaHiC library|X|||X|\n| contig_data_network.txt |Information on contigs after network step|X||||\n| clustering_matrix_partition.txt |Matrix of clustering from the partition iterations||X|||\n| contig_data_partition.txt |Information on contigs after partition step||X|||\n| overlapping_checkm_results.txt |CheckM results summary file from the partition step|||X|X|\n| overlapping_checkm_taxonomy.txt |CheckM taxonomy file from the partition step|||X|X|\n| recursive_checkm_results.txt |CheckM results summary file from the recursive step|||X|X|\n| recursive_checkm_taxonomy.txt |CheckM taxonomy file from the recursive step|||X|X|\n| clustering_matrix_validation.txt |Matrix of clustering from the recursive iterations|||X||\n| clustering_matrix.txt |Matrix of clustering from the partition and recursive iterations||||X|\n| contig_data_final.txt |Information on contigs after whole pipeline|||X|X|\n| **bin_summary.txt** |**Information on the final bins**|||**X**|**X**|\n| binning.txt |File with contigs names and their final clustering|||X|X|\n| overlapping_bin |Directory with the fasta of the partition bins||X|X|X|\n| recursive_bin |Directory with the fasta of the recursive bins|||X|X|\n| **final_bin** |**Directory with the fasta of the final bins**|||**X**|**X**|\n\n**Bam alignment files**\nFor the bam alignments files, only the aligned reads are kept and the bam are sorted by name. The N value correspond to the id (order of the given fastq started at 0)\n\n**Pairs aligment files**\nThis format is used to store the relevant information of mapping of the merged alignment. It's a s The N value correspond to the id (order of the given fastq started at 0). It is a tab-separated format holding informations about Hi-C pairs. It has an [official specification](https://github.com/4dn-dcic/pairix/blob/master/pairs_format_specification.md) defined by the 4D Nucleome data coordination and integration center. Here we kept 7 columns readID-chr1-pos1-chr2-pos2-strand1-strand2.\n\n**Network file**\nThis is a tsv file of the network with edgelist form: Id of the first contig, id of the second contig and the weigth of edge normalized.\n\n**Contig data files**\nThese are the files with all the informations from the contigs:\n|ID|Name|Size|GC_content|Hit|Shotgun_coverage|Restriction_site|Core_bin_ID|Core_bin_contigs|Core_bin_size|Overlapping_bin_ID|Overlapping_bin_contigs|Overlapping_bin_size|Recursive_bin_ID|Recursive_bin_contigs|Recursive_bin_size|Final_bin|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|1|NODE_1|642311|38.6876450815882|3837|41.1565|2006|1|65|2175226|1|396|6322353|1|52|2158803|MetaTOR_1_1|\n|2|NODE_2|576356|30.235826468363303|1724|24.509|1256|2|40|1735419|2|401|735419|0|-|-|MetaTOR_2_0|\n|3|NODE_3|540571|42.305266098255366|2188|14.5855|3405|3|127|6409484|3|431|13615480|1|112|6385126|MetaTOR_3_1|\n\nThey have to have the header when they are use as input but the order of the columns nd if they are others columns doesn't matter when they are used as input files.\nDepending on which step of the pipeline have been launch they just have some of these columns:\n\n* `contig_data_network.txt`: columns: ID, Name, Size, GC content, Hit, Shotgun_coverage and Restriction Site only.\n* `contig_data_partition.txt`: The same as the previous with the information of core bins and overlapping bins.\n* `contig_data_final.txt`: All the columns.\n\nThe shotgun coverage will be filled only if the depth.txt file is given, otherwise it will be filled with `-`. This column is only necessarry for the *abundance* and the *theoritical_hit* normalization. The restriction will also be filled with `-` if no enzyme are given. This column is only necessary for the *RS* and the *theoritical_hit* normalization.\nMoreover, if the contig is not binned (no HiC reads mapped on it) all the columns with binning information will be filled with `-`, and if a bin is not recursively decontamined because it's smaller than the size threshold or it doesn't have any contamination the recusive bin information will be filled `0`, `-`, `-`. Finally, if the bin is not in a final bin, it will be annotated `ND` in the last column (for not determined).\n\n**clustering matrix files**\nThe clustering matrix files are at the `.npz` format which is a compresed foramt for sparsed matrix. This sparsed matrix contains the ratio of time each pairs of contigs are clusterize together by the algorithm of clustering (either Louvain or Leiden). The partition matrix contains the information for the partition step, the recursive one for the recursive step and the general one is the mean of both matrices. Be careful the index of the contigs are **zero-based** and not one-based as in the contig data file.\n\nIt's possible to read them in python using the `scipy.sparse.load_npz()` function. If the users wants a tsv file instead, he or she could load the matrix in python using load_npz and make sure to transform the matrix in the `scipy.sparse.coo_matrix` function and used the function from metator `metator.io.save_sparse_matrix` to save it as tsv file.\n\n**CheckM results**\nFiles from [checkM](https://github.com/Ecogenomics/CheckM) output. Two types of files one with the main results of checkM `checkm_results.txt` and one with the taxonomy  `checkm_taxonomy.txt` for both the partition and the recurisve bins.\n\n**binning.txt file**\nThis is a tsv file with two columns: the contig name and the final were the contig is. It only contains contigs which are binned. It could be use a an input to import a binning results in anvio.\n\n**Bin summary file**\nThis is the summary of the data of the final bins build with all the step of metaTOR. The HiC coverage is the number of contacts (intra and inter contigs) per kilobase in the whole bin. The Shotgun coverage is the mean coverage normalized by the size of the shotgun reads from the depth file.\n\n||lineage|completness|contamination|size|contigs|N50|longest_contig|GC|coding_density|taxonomy|Coverage|\n|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n|MetaTOR_8_1|o__Clostridiales|68.29|2.46|1431612|15|116129|291620|26.36|87.97|k__Bacteria;p__Firmicutes;c__Clostridia;o__Clostridiales|146.46719755483332|\n|MetaTOR_8_2|o__Clostridiales|58.42|2.01|1396934|58|41290|174682|28.89|83.70|k__Bacteria;p__Firmicutes;c__Clostridia;o__Clostridiales|22.252416224710686|\n|MetaTOR_8_3|o__Clostridiales|49.37|0.94|1420821|82|33095|89964|30.29|83.24|k__Bacteria;p__Firmicutes;c__Clostridia;o__Clostridiales;f__Peptostreptococcaceae_3;g__Clostridium_3|44.27369196532141|\n\n## References\n\n* [Metagenomic chromosome conformation capture (meta3C) unveils the diversity of chromosome organization in microorganisms](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4381813/), Martial Marbouty, Axel Cournac, Jean-François Flot, Hervé Marie-Nelly, Julien Mozziconacci, and Romain Koszul, eLife, 2014\n* [Meta3C analysis of a mouse gut microbiome](https://www.biorxiv.org/content/early/2015/12/17/034793), Martial Marbouty, Lyam Baudry, Axel Cournac, Romain Koszul, 2015\n* [Scaffolding bacterial genomes and probing host-virus interactions in gut microbiome by proximity ligation (chromosome capture) assay](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5315449/), Martial Marbouty, Lyam Baudry, Axel Cournac, and Romain Koszul, Science Advances, 2017\n\n## Contact\n\n### Authors\n\n* amaury.bignaud@pasteur.fr\n* lyam.baudry@pasteur.fr\n* thfoutel@pasteur.fr\n* martial.marbouty@pasteur.fr\n* romain.koszul@pasteur.fr\n\n### Research lab\n\n[Spatial Regulation of Genomes](https://research.pasteur.fr/en/team/spatial-regulation-of-genomes/) (Institut Pasteur, Paris)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/koszullab/metator",
    "keywords": "",
    "license": "GPLv3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "metator",
    "package_url": "https://pypi.org/project/metator/",
    "platform": "",
    "project_url": "https://pypi.org/project/metator/",
    "project_urls": {
      "Homepage": "https://github.com/koszullab/metator"
    },
    "release_url": "https://pypi.org/project/metator/1.1.3/",
    "requires_dist": [
      "biopython",
      "docopt",
      "hicstuff",
      "networkx",
      "numpy",
      "pandas",
      "pyfastx",
      "pypairix",
      "pysam",
      "requests",
      "scipy",
      "seaborn",
      "sklearn"
    ],
    "requires_python": ">=3.6",
    "summary": "A pipeline for binning metagenomic datasets from metaHiC data.",
    "version": "1.1.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17558126,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6552bf7e8b59ba4c40fe9422092c7829781bbae66133028dea47c14180d49fe1",
        "md5": "b5d17c233f405e2203c5a7584a3a084e",
        "sha256": "7d01773444a0bd28ba3ffed1bed29462fdb055211c29ebd22938e09206c5ada6"
      },
      "downloads": -1,
      "filename": "metator-1.1.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b5d17c233f405e2203c5a7584a3a084e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 67929,
      "upload_time": "2021-07-23T14:06:47",
      "upload_time_iso_8601": "2021-07-23T14:06:47.492136Z",
      "url": "https://files.pythonhosted.org/packages/65/52/bf7e8b59ba4c40fe9422092c7829781bbae66133028dea47c14180d49fe1/metator-1.1.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7446b5e71be5758db548df526ab9750495735c2b28079ea3dae41d74e67050b4",
        "md5": "97d93030091190b4df87c50cea6ead4c",
        "sha256": "3d3f93dbe18bca677cfcb52c4c9fe6f01db7bf13e88140ee482e631c17edc9a3"
      },
      "downloads": -1,
      "filename": "metator-1.1.3.tar.gz",
      "has_sig": false,
      "md5_digest": "97d93030091190b4df87c50cea6ead4c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 64285,
      "upload_time": "2021-07-23T14:06:48",
      "upload_time_iso_8601": "2021-07-23T14:06:48.726245Z",
      "url": "https://files.pythonhosted.org/packages/74/46/b5e71be5758db548df526ab9750495735c2b28079ea3dae41d74e67050b4/metator-1.1.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}