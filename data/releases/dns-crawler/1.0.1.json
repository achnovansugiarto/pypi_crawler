{
  "info": {
    "author": "Jiri Helebrant",
    "author_email": "jiri.helebrant@nic.cz",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3"
    ],
    "description": "# `dns-crawler`\n\n> A crawler for getting info about *(possibly a huge number of)* DNS domains\n\n## Installation\n\nCreate and activate a virtual environment:\n\n```bash\nmkdir dns-crawler\ncd dns-crawler\npython3 -m venv .venv\nsource .venv/bin/activate\n```\n\nInstall `dns-crawler`:\n\n```bash\npip install dns-crawler\n```\n\nThis is enough to make the crawler work, but you will probably get `AttributeError: module 'dns.message' has no attribute 'Truncated'` for a lot of domains. This is because the crawler uses current `dnspython`, but the last release on PyPI is ages behind the current code. It can be fixed easily just by installing `dnspython` from git:\n\n```bash\npip install -U git+https://github.com/rthalley/dnspython.git\n```\n\n(PyPI [doesn't allow us](https://github.com/pypa/pip/issues/6301) to specify the git url it in dependencies unfortunately)\n\n## Basic usage\n\nStart Redis. The exact command depends on your system.\n\nFeed domains into queue and wait for results:\n\n```\n$ dns-crawler domain-list.txt > result.json\n```\n\n(in another shell) Start workers which process the domains and return results to the main process:\n\n```\n$ dns-crawler-workers\n```\n\n## How fast is it anyway?\n\nA single laptop on ~50Mbps connection can crawl the entire *.cz* zone overnight, give or take (with `save_web_content` disabled).\n\nSince the crawler is designed to be parallel, the actual speed depends almost entirely on the worker count. And it can scale accross multiple machines almost infinitely, so should you need a million domains crawled in an hour, you can always just throw more hardware at it.\n\n## Installation\n\n### Requirements\n\n- Python 3.6+\n- [requests](https://python-requests.org/)\n- [pyaml](https://pyyaml.org/) (for config loading)\n- [dnspython](http://www.dnspython.org/) + [pycryptodome](https://pycryptodome.readthedocs.io/) & [python-ecdsa](https://github.com/warner/python-ecdsa) (for DNSSEC validation)\n- [geoip2](https://geoip2.readthedocs.io/en/latest/) + up-to-date Country and ISP (or ASN) databases (mmdb format, works with both free and commercial ones)\n- [rq](https://python-rq.org/) and [redis](https://redis.io/) if you want to process a huge number of domains and even run the crawler across multiple machines\n\n\n### Redis configuration\n\nNo special config needed, but increase the memory limit if you have a lot of domains to process (eg. `maxmemory 1G`). You can also disable disk snapshots to save some I/O time (comment out the `save …` lines).\n\n### Trying it out\n\nCreate a short domain list (one 2nd level domain per line):\n\n```\n$ echo -e \"nic.cz\\nnetmetr.cz\\nroot.cz\" > domains.txt\n```\n\nStart the main process to create job for every domain:\n\n```\n$ dns-crawler domains.txt\n[2019-09-24 07:38:15] Reading domains from …/domains.txt.\n[2019-09-24 07:38:15] Creating job queue using 2 threads.\n[2019-09-24 07:38:15] 0/3 jobs created.\n[2019-09-24 07:38:16] 3/3 jobs created.\n[2019-09-24 07:38:16] Created 3 jobs. Waiting for workers…\n[2019-09-24 07:38:17] 0/3\n```\n\nNow it waits for workers to take the jobs from redis. Run a worker in another shell:\n\n```\n$ dns-crawler-workers 3\n07:38:17 RQ worker 'rq:worker:foo-2' started, version 1.0\n07:38:17 *** Listening on default...\n07:38:17 Cleaning registries for queue: default\n07:38:17 default: crawl.process_domain('nic.cz') (nic.cz)\n07:38:17 RQ worker 'rq:worker:foo-1' started, version 1.0\n07:38:17 RQ worker 'rq:worker:foo-3' started, version 1.0\n07:38:17 *** Listening on default...\n07:38:17 *** Listening on default...\n07:38:17 default: crawl.process_domain('netmetr.cz') (netmetr.cz)\n07:38:17 default: crawl.process_domain('root.cz') (root.cz)\n07:38:17 default: Job OK (netmetr.cz)\n07:38:17 default: Job OK (nic.cz)\n07:38:17 RQ worker 'rq:worker:foo-2' done, quitting\n07:38:17 RQ worker 'rq:worker:foo-3' done, quitting\n07:38:19 default: Job OK (root.cz)\n```\n\n### Output formats\n\nResults are printed to the main process' stdout – JSON for every domain, separated by `\\n`:\n\n```\n…\n[2019-05-03 07:38:17] 2/3\n{\"domain\": \"nic.cz\", \"timestamp\": \"2019-09-24T05:28:06.536991\", \"results\": {…}}\n…\n```\n\nThe progress info with timestamp is printed to stderr, so you can save just the output easily – `dns-crawler list.txt > results`.\n\nA JSON schema for the output JSON is included in this repository: [`result-schema.json`](result-schema.json), and also an example for nic.cz: [`result-example.json`](result-example.json).\n\nThere are several tools for schema validation, viewing, and even code generation.\n\nTo validate a result against schema (CI is set up to do it automatically):\n\n```bash\n$ pip install jsonschema\n$ jsonschema -i result-example.json result-schema.json # if it prints nothing, it's valid\n```\n\nOr, if you don't loathe JS, `ajv` has a much better output:\n\n```bash\n$ npm i -g ajv\n$ ajv validate -s result-schema.json -d result-example.json\n```\n\n#### Formatting the JSON output\n\nIf you want formatted JSONs, just pipe the output through [jq](https://stedolan.github.io/jq/): `dns-crawler list.txt | jq`.\n\n#### SQL output\n\nAn util to create SQL `INSERT`s from the crawler output is included in this repo.\n\n```\n$ dns-crawler list.txt | python output_sql.py table_name\nINSERT INTO table_name VALUES …;\nINSERT INTO table_name VALUES …;\n```\n\nYou can even pipe it right into `psql` or another DB client, which will save the results into DB continually, as they come from the workers:\n\n```\n$ dns-crawler list.txt | python output_sql.py table_name | psql -d db_name …\n```\n\nIt can also generate the table structure (`CREATE TABLE …`), taking the table name as a single argument (without piping anything to stdin):\n\n```\n$ python output_sql.py results\nCREATE TABLE results …;\n```\n\nThe SQL output is tested only with PostgreSQL 11.\n\nThere's also `output_sql.py`, useful for inserting big chunks of resuls (which would be slow to do one by one).\n\n#### Saving to Hadoop\n\nTODO\n\n#### Custom output formats\n\nSince the main process just spits out JSONs to stdout, it's pretty easy to process it with almost anything.\n\nA simple example for YAML output:\n\n```python\nimport json\nimport yaml\nimport sys\n\nfor line in sys.stdin:\n    print(yaml.dump(json.loads(line)))\n```\n\n(and then just `dns-crawler domains.txt | python yaml.py`)\n\nCSV is a different beast, since there's no obvious way to represent arrays…\n\n## Probing just a few domains (or a single one)\n\nIt's possible to run just the crawl function without the rq/redis/worker machinery, which could come handy if you're interested in just a small number of domains. To run it, just import the `process_domain` function.\n\nExample:\n\n```\n$ python\n>>> from crawl import process_domain\n>>> result = process_domain(\"nic.cz\")\n>>> result\n{'domain': 'nic.cz', 'timestamp': '2019-09-13T09:21:10.136303', 'results': { …\n>>>\n>>> result[\"results\"][\"DNS_LOCAL\"][\"DNS_AUTH\"]\n[{'value': 'a.ns.nic.cz.'}, {'value': 'b.ns.nic.cz.'}, {'value': 'd.ns.nic.cz.'}]\n```\n\nFormatted output, inline python code:\n\n```\n$ python -c \"from crawl import process_domain; import json; print(json.dumps(process_domain('nic.cz'), indent=2))\"\n{\n  \"domain\": \"nic.cz\",\n  \"timestamp\": \"2019-09-13T09:24:23.108941\",\n  \"results\": {\n    \"DNS_LOCAL\": {\n      \"DNS_AUTH\": [\n        …\n```\n\n\n## Config file\n\nGeoIP DB paths, DNS resolver IP(s), and timeouts are read from `config.yml` in the working directory, if present.\n\nThe default values are:\n\n```yaml\ngeoip:\n  country: /usr/share/GeoIP/GeoIP2-Country.mmdb\n  isp: /usr/share/GeoIP/GeoIP2-ISP.mmdb\ndns:\n  - 127.0.0.1\njob_timeout: 80  # max. duration for a single domain (seconds) \ndns_timeout: 2 # seconds\nhttp_timeout: 2 # seconds\nsave_web_content: False  # beware, setting to True will output HUGE files\nstrip_html: False # when saving web content, strip HTML tags, scripts, and CSS\n```\n\nUsing free (GeoLite2) Country and ASN DBs instead of commercial ones:\n\n```yaml\ngeoip:\n  country: /usr/share/GeoIP/GeoLite2-Country.mmdb\n  asn: /usr/share/GeoIP/GeoLite2-ASN.mmdb\n```\n\n`ISP` (paid) database is preferred over `ASN` (free), if both are defined. The difference is described on Maxmind's website: https://dev.maxmind.com/faq/what-is-the-difference-between-the-geoip-isp-and-organization-databases/.\n\nThe free `GeoLite2-Country` seems to be a bit inaccurate, especially for IPv6 (it places some CZ.NIC nameservers in Ukraine etc.).\n\nUsing [ODVR](https://blog.nic.cz/2019/04/30/spoustime-nove-odvr/) or other resolvers:\n\n```yaml\ndns:\n  - 193.17.47.1\n  - 185.43.135.1\n```\n\nIf no resolvers are specified (`dns` is missing or empty in the config file), the crawler will attempt to use system settings (handled by `dnspython`, but it usually ends up with `/etc/resolv.conf` on Linux).\n\n## Command line parameters\n\n### dns-crawler\n\n```\ndns-crawler <file>\n       file - plaintext domain list, one domain per line (separated by \\n)\n```\n\nThe main process uses threads (2 for each CPU core) to create the jobs faster. It's *much* faster on (more) modern machines – eg. i7-7600U in a laptop does about 4.2k jobs/s, while server with Xeon X3430 does just about 1.4k (using 8 threads, as they both appear as 4 core to the system).\n\nTo cancel the process, just send a kill signal or hit `Ctrl-C` any time. The process will perform cleanup and exit.\n\n### dns-crawler-workers\n\n```\nUsage: dns-crawler-workers [count] [redis]\n       count - worker count, 8 workers per CPU core by default\n       redis - redis host, localhost:6379 by default\n\nExamples: dns-crawler-workers 8\n          dns-crawler-workers 24 192.168.0.22:4444\n          dns-crawler-workers 16 redis.foo.bar\n```\n\nTrying to use more than 24 workers per CPU core will result in a warning (and countdown before it actually starts the workers):\n\n```\n$ dns-crawler-workers 999\nWhoa. You are trying to run 999 workers on 4 CPU cores. It's easy toscale\nacross multiple machines, if you need to. See README.md for details.\n\nCancel now (Ctrl-C) or have a fire extinguisher ready.\n5 - 4 - 3 -\n```\n\nStopping works the same way as with the main process – `Ctrl-C` (or kill signal) will finish the current job(s) and exit.\n\n> The workers will shout at you that *\"Result will never expire, clean up result key manually\"*. This is perfectly fine, results are continually cleaned by the main process. Unfortunately there's no easy way to disable this message in `rq` without setting a fixed TTL.\n\n## Resuming work\n\nStopping the workers won't delete the jobs from Redis. So, if you stop the `dns-crawler-workers` process and then start a new one (perhaps to use different worker count…), it will pick up the unfinished jobs and continue.\n\nThis can also be used change the worker count if it turns out to be too low or hight for your machine or network:\n\n- to reduce the worker count, just stop the `dns-crawler-workers` process and start a new one with a new count\n- to increase the worker count, either use the same approach, or just start a second `dns-crawler-workers` process in another shell, the worker count will just add up\n- scaling to multiple machines works the same way, see below\n\n## Running on multiple machines\n\nSince all communication between the main process and workers is done through Redis, it's easy to scale the crawler to any number of machines:\n\n```\nmachine-1                     machine-1\n┬──────────────────┐         ┬────────────────────┐\n│    dns-crawler   │         │ dns-crawler-workers│\n│        +         │ ------- │          +         │\n│      redis       │         │    DNS resolver    │\n└──────────────────┘         └────────────────────┘\n\n                             machine-2\n                             ┬─────────────────────┐\n                             │ dns-crawler-workers │\n                     ------- │          +          │\n                             │    DNS resolver     │\n                             └─────────────────────┘\n\n                             …\n                             …\n\n                             machine-n\n                             ┬─────────────────────┐\n                             │ dns-crawler-workers │\n                     _______ │          +          │\n                             │    DNS resolver     │\n                             └─────────────────────┘\n```\n\nJust tell the workers to connect to the shared Redis on the main server, eg.:\n\n```\n$ dns-crawler-workers 24 192.168.0.2:6379\n                    ^            ^\n                    24 threads   redis host\n```\n\n## Monitoring\n\n### Command line\n\n```\n$ rq info\ndefault      |████████████████████ 219458\n1 queues, 219458 jobs total\n\n0 workers, 1 queues\n```\n\n### Web interface\n\n```\n$ pip install rq-dashboard\n$ rq-dashboard\nRQ Dashboard version 0.4.0                                                 \n * Serving Flask app \"rq_dashboard.cli\" (lazy loading)                            \n * Environment: production                                                \n   WARNING: Do not use the development server in a production environment. \n   Use a production WSGI server instead.                                          \n * Debug mode: off                            \n * Running on http://0.0.0.0:9181/ (Press CTRL+C to quit)\n ```\n\n<a href=\"https://i.vgy.me/sk7zWa.png\">\n<img alt=\"RQ Dashboard screenshot\" src=\"https://i.vgy.me/sk7zWa.png\" width=\"40%\">\n</a>\n<a href=\"https://i.vgy.me/4y5Zee.png\">\n<img alt=\"RQ Dashboard screenshot\" src=\"https://i.vgy.me/4y5Zee.png\" width=\"40%\">\n</a>\n\n## Tests\n\nSome basic tests are in the `tests` directory in this repo. If you want to run them manually, take a look at the `test` stage jobs in `.gitlab-ci.yml`. Basically it just downloads free GeoIP DBs, tells the crawler to use them, and crawles some domains, checking values in JSON output. It runs the tests twice – first with the default DNS resolvers (ODVR) and then with system one(s).\n\nIf you're looking into writing some additional tests, be aware that some Docker containers used in GitLab CI don't have IPv6 configured (even if it's working on the host machine), so checking for eg. `WEB6_80_www_VENDOR` will fail without additional setup.\n\n\n## OS support\n\nThe crawler is developed primarily for Linux, but it should work on any OS supported by Python –at least the worker part (but the main process should work too, if you manage to get a Redis server running on your OS).\n\nOne exception is Windows, because it [doesn't support `fork()`](https://github.com/rq/rq/issues/859), but it's possible to get it working under WSL (Windows Subsystem for Linux):\n\n![win10 screenshot](https://i.vgy.me/emJjGN.png)\n\n…so you can turn a gaming machine into an internet crawler quite easily.\n\n\n## Bug reporting\n\nPlease create [issues in this Gitlab repo](https://gitlab.labs.nic.cz/adam/dns-crawler/issues).",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.labs.nic.cz/adam/dns-crawler",
    "keywords": "crawler,dns,http,https",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dns-crawler",
    "package_url": "https://pypi.org/project/dns-crawler/",
    "platform": "",
    "project_url": "https://pypi.org/project/dns-crawler/",
    "project_urls": {
      "Homepage": "https://gitlab.labs.nic.cz/adam/dns-crawler"
    },
    "release_url": "https://pypi.org/project/dns-crawler/1.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A crawler for getting info about DNS domains and services attached to them.",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9855130,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5bd843506e2a74e67506687cd2e3dd9c288a8a8446554657db36c71f7db0dfc9",
        "md5": "2a2d0a0899f39ad2943d96c810ab4a31",
        "sha256": "2f17f259e3facd62cf0297ff2cb4dbcef5cde65ccb6585f155659f064e134d36"
      },
      "downloads": -1,
      "filename": "dns-crawler-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "2a2d0a0899f39ad2943d96c810ab4a31",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 40399,
      "upload_time": "2019-11-29T09:32:04",
      "upload_time_iso_8601": "2019-11-29T09:32:04.062500Z",
      "url": "https://files.pythonhosted.org/packages/5b/d8/43506e2a74e67506687cd2e3dd9c288a8a8446554657db36c71f7db0dfc9/dns-crawler-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}