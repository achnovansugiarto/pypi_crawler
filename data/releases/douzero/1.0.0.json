{
  "info": {
    "author": "Kuai",
    "author_email": "daochen.zha@tamu.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# [ICML 2021] DouZero: Mastering DouDizhu with Self-Play Deep Reinforcement Learning\n<img width=\"500\" src=\"./imgs/douzero_logo.jpg\" alt=\"Logo\" />\n\nDouZero is a reinforcement learning framework for  [DouDizhu](https://en.wikipedia.org/wiki/Dou_dizhu) ([斗地主](https://baike.baidu.com/item/%E6%96%97%E5%9C%B0%E4%B8%BB/177997)), the most popular card game in China. It is a shedding-type game where the player’s objective is to empty one’s hand of all cards before other players. DouDizhu is a very challenging domain with competition, collaboration, imperfect information, large state space, and particularly a massive set of possible actions where the legal actions vary significantly from turn to turn. DouZero is developed by AI Platform, Kwai Inc. (快手).\n\n*   Online Demo: [https://www.douzero.org/](https://www.douzero.org/)\n*   Run the Demo Locally: [https://github.com/datamllab/rlcard-showdown](https://github.com/datamllab/rlcard-showdown)\n*   Paper: \n*   Related Project: [RLCard Project](https://github.com/datamllab/rlcard)\n*   Related Resources: [Awesome-Game-AI](https://github.com/datamllab/awesome-game-ai)\n\n**Community:**\n*  **Slack**: Discuss in [DouZero](https://join.slack.com/t/douzero/shared_invite/zt-rg3rygcw-ouxxDk5o4O0bPZ23vpdwxA) channel.\n*  **QQ Group**: Join our QQ group 819204202. Password: douzeroqqgroup\n\n## Cite this Work\n\n\n## What Makes DouDizhu Challenging?\nIn addition to the challenge of imperfect information, DouDizhu has huge state and action spaces. In particular, the action space of DouDizhu is 10^4 (see [this table](https://github.com/datamllab/rlcard#available-environments)). Unfortunately, most reinforcement learning algorithms can only handle very small action spaces. Moreover, the players in DouDizhu need to both compete and cooperate with others in a partially-observable environment with limited communication, i.e., two Peasants players will play as a team to fight against the Landlord player. Modeling both competing and cooperation is an open research challenge.\n\nIn this work, we propose Deep Monte Carlo (DMC) algorithm with action encoding and parallel actors. This leads to a very simple yet surprisingly effective solution for DouDizhu. Please read our paper for more details.\n\n## Installation\nClone the repo with\n```\ngit clone https://github.com/daochenzha/douzero.git\n```\nMake sure you have python 3.6+ installed. Install dependencies:\n```\ncd douzero\npip3 install -r requirements.txt\n```\nWe recommend installing the stable version of DouZero with\n```\npip3 install douzero\n```\nor install the up-to-date version (it could be not stable) with\n```\npip3 install -e .\n```\n\n## Training\nWe assume you have at least one GPU available. Run\n```\npython3 train.py\n```\nThis will train DouZero on one GPU. To train DouZero on multiple GPUs. Use the following arguments.\n*   `--gpu_devices`: what gpu devices are visible\n*   `--num_actors_devices`: how many of the GPU deveices will be used for simulation, i.e., self-play\n*   `--num_actors`: how many actor processes will be used for each device\n*   `--training_device`: which device will be used for training DouZero\n\nFor example, if we have 4 GPUs, where we want to use the first 3 GPUs to have 15 actors each for simulating and the 4th GPU for training, we can run the following command:\n```\npython3 train.py --gpu_devices 0,1,2,3 --num_actors_devices 3 --num_actors 15 --training_device 3\n```\nFor more customized configuration of training, see the following optional arguments:\n```\n--xpid XPID           Experiment id (default: douzero)\n--save_interval SAVE_INTERVAL\n                      Time interval (in minutes) at which to save the model\n--objective {adp,wp}  Use ADP or WP as reward (default: ADP)\n--gpu_devices GPU_DEVICES\n                      Which GPUs to be used for training\n--num_actor_devices NUM_ACTOR_DEVICES\n                      The number of devices used for simulation\n--num_actors NUM_ACTORS\n                      The number of actors for each simulation device\n--training_device TRAINING_DEVICE\n                      The index of the GPU used for training models\n--load_model          Load an existing model\n--disable_checkpoint  Disable saving checkpoint\n--savedir SAVEDIR     Root dir where experiment data will be saved\n--total_frames TOTAL_FRAMES\n                      Total environment frames to train for\n--exp_epsilon EXP_EPSILON\n                      The probability for exploration\n--batch_size BATCH_SIZE\n                      Learner batch size\n--unroll_length UNROLL_LENGTH\n                      The unroll length (time dimension)\n--num_buffers NUM_BUFFERS\n                      Number of shared-memory buffers\n--num_threads NUM_THREADS\n                      Number learner threads\n--max_grad_norm MAX_GRAD_NORM\n                      Max norm of gradients\n--learning_rate LEARNING_RATE\n                      Learning rate\n--alpha ALPHA         RMSProp smoothing constant\n--momentum MOMENTUM   RMSProp momentum\n--epsilon EPSILON     RMSProp epsilon\n```\n\n## Evaluation\nThe evaluation can be performed with GPU or CPU (GPU will be much faster). Pretrained model is available at [Google Drive](https://drive.google.com/drive/folders/1NmM2cXnI5CIWHaLJeoDZMiwt6lOTV_UB?usp=sharing) or [百度网盘](https://pan.baidu.com/s/18g-JUKad6D8rmBONXUDuOQ), 提取码: 4624. Put pre-trained weights in `baselines/`. The performance is evaluated through self-play. We have provided pre-trained models and some heuristics as baselines:\n*   [random](douzero/evaluation/random_agent.py): agents that play randomly (uniformly)\n*   [rlcard](douzero/evaluation/rlcard/agent.py): the rule-based agent in [RLCard](https://github.com/datamllab/rlcard)\n*   SL (`baselines/sl/`): the pre-trained deep agents on human data\n*   DouZero-ADP (`baselines/douzero_ADP/`): the pretrained DouZero agents with Average Difference Points (ADP) as objective\n*   DouZero-WP (`baselines/douzero_WP/`): the pretrained DouZero agents with Winning Percentage (WP) as objective\n\n### Step 1: Generate evaluation data\n```\npython3 generate_eval_data.py\n```\nSome important hyperparameters are as follows.\n*   `--output`: where the pickled data will be saved\n*   `--num_games`: how many random games will be generated, default 10000\n\n### Step 2: Self-Play\n```\npython3 evaluate.py\n```\nSome important hyperparameters are as follows.\n*   `--landlord`: which agent will play as Landlord, which can be random, rlcard, or the path of the pre-trained model\n*   `--landlord_up`: which agent will play as LandlordUp (the one plays before the Landlord), which can be random, rlcard, or the path of the pre-trained model\n*   `--landlord_down`: which agent will play as LandlordDown (the one plays after the Landlord), which can be random, rlcard, or the path of the pre-trained model\n*   `--eval_data`: the pickle file that contains evaluation data\n\nFor example, the following command evaluates DouZero-ADP in Landlord position against random agents\n```\npython3 evaluate.py --landlord baselines/douzero_ADP/landlord.ckpt --landlord_up random --landlord_down random\n```\nThe following command evaluates DouZero-ADP in Peasants position against RLCard agents\n```\npython3 evaluate.py --landlord rlcard --landlord_up baselines/douzero_ADP/landlord_up.ckpt --landlord_down baselines/douzero_ADP/landlord_down.ckpt\n```\nBy default, our model will be saved in `douzero_checkpoints/douzero` every half an hour. We provide a script to help you identify the most recent checkpoint. Run\n```\nsh get_most_recent.sh douzero_checkpoints/douzero/\n```\nThe most recent model will be in `most_recent_model`.\n\n## Core Team\n*   Algorithm: [Daochen Zha](https://github.com/daochenzha), [Jingru Xie](https://github.com/karoka), Wenye Ma, Sheng Zhang, Xiangru Lian, Xia Hu, Ji Liu\n*   GUI Demo: [Songyi Huang](https://github.com/hsywhu)\n\n## Acknowlegements\n*   The demo is largely based on [RLCard-Showdown](https://github.com/datamllab/rlcard-showdown)\n*   Code implementation is inspired by [TorchBeast](https://github.com/facebookresearch/torchbeast)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "DouDizhu,RL",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "douzero",
    "package_url": "https://pypi.org/project/douzero/",
    "platform": "",
    "project_url": "https://pypi.org/project/douzero/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/douzero/1.0.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "DouZero DouDizhu AI",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11096712,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a7881e8366038e5c84241af1ba9b5948b4db38dcda65950fe2982813e138ee73",
        "md5": "183989ad4f1c40f7f6b8efff5876cd04",
        "sha256": "ed923769ae624a7b0be24a9526fa57010f38913999bfbfe66a40af51f289a529"
      },
      "downloads": -1,
      "filename": "douzero-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "183989ad4f1c40f7f6b8efff5876cd04",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 23393,
      "upload_time": "2021-06-11T03:04:48",
      "upload_time_iso_8601": "2021-06-11T03:04:48.438703Z",
      "url": "https://files.pythonhosted.org/packages/a7/88/1e8366038e5c84241af1ba9b5948b4db38dcda65950fe2982813e138ee73/douzero-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}