{
  "info": {
    "author": "THUIAR",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "[![](https://badgen.net/badge/license/MIT/blue)](#License) [![](https://badgen.net/badge/contact/THUIAR/purple)](https://thuiar.github.io/) ![Python 3.6](https://img.shields.io/badge/python-3.6-green.svg)\n\n# MMSA\n\n> A unified framwork for Multimodal Sentiment Analysis tasks.\n\n**Note:** We strongly recommend browsing the overall structure of our code first. Feel free to contact us if you require any further information.\n\n## Supported Models\n\n|    Type     |                   Model Name                   |                                          From                                          |\n| :---------: | :--------------------------------------------: | :------------------------------------------------------------------------------------: |\n| Single-Task |    [EF_LSTM](models/singleTask/EF_LSTM.py)     |               [MultimodalDNN](https://github.com/rhoposit/MultimodalDNN)               |\n| Single-Task |     [LF_DNN](models/singleTask/LF_DNN.py)      |                                           -                                            |\n| Single-Task |        [TFN](models/singleTask/TFN.py)         |        [Tensor-Fusion-Network](https://github.com/A2Zadeh/TensorFusionNetwork)         |\n| Single-Task |        [LMF](models/singleTask/LMF.py)         | [Low-rank-Multimodal-Fusion](https://github.com/Justin1904/Low-rank-Multimodal-Fusion) |\n| Single-Task |        [MFN](models/singleTask/MFN.py)         |               [Memory-Fusion-Network](https://github.com/pliang279/MFN)                |\n| Single-Task |  [Graph-MFN](models/singleTask/Graph_MFN.py)   |            [Graph-Memory-Fusion-Network](https://github.com/pliang279/MFN)             |\n| Single-Task | [MulT](models/singleTask/MulT.py)(without CTC) |      [Multimodal-Transformer](https://github.com/yaohungt/Multimodal-Transformer)      |\n| Single-Task |   [BERT-MAG](models/singleTask/BERT_MAG.py)    |        [MAG-BERT](https://github.com/WasifurRahman/BERT_multimodal_transformer)        |\n| Single-Task |        [MFM](models/singleTask/MFM.py)         |                                           -                                            |\n| Single-Task |       [MISA](models/singleTask/MISA.py)        |                      [MISA](https://github.com/declare-lab/MISA)                       |\n| Multi-Task  |     [MLF_DNN](models/multiTask/MLF_DNN.py)     |                         [MMSA](https://github.com/thuiar/MMSA)                         |\n| Multi-Task  |        [MTFN](models/multiTask/MTFN.py)        |                         [MMSA](https://github.com/thuiar/MMSA)                         |\n| Multi-Task  |        [MLMF](models/multiTask/MLMF.py)        |                         [MMSA](https://github.com/thuiar/MMSA)                         |\n| Multi-Task  |     [SELF_MM](models/multiTask/SELF_MM.py)     |                      [Self-MM](https://github.com/thuiar/Self-MM)                      |\n\n## Results\n\n> Detailed results are shown in [results/result-stat.md](results/result-stat.md)\n\n## Usage\n\n### Clone codes\n\n- Clone this repo and install requirements. Create virtual environments if needed.\n\n```\ngit clone https://github.com/thuiar/MMSA\ncd MMSA\n# conda create -n mmsa python=3.6\npip install -r requirements.txt\n```\n\n### Datasets and pre-trained berts\n\nDownload dataset features and pre-trained berts from the following links.\n\n- [Baidu Cloud Drive](https://pan.baidu.com/s/1oksuDEkkd3vGg2oBMBxiVw) with code: `ctgs`\n- [Google Cloud Drive](https://drive.google.com/drive/folders/1E5kojBirtd5VbfHsFp6FYWkQunk73Nsv?usp=sharing)\n\nFor all features, you can use `SHA-1 Hash Value` to check the consistency.\n\n> `MOSI/unaligned_50.pkl`: `5da0b8440fc5a7c3a457859af27458beb993e088`  \n> `MOSI/aligned_50.pkl`: `5c62b896619a334a7104c8bef05d82b05272c71c`  \n> `MOSEI/unaligned_50.pkl`: `db3e2cff4d706a88ee156981c2100975513d4610`  \n> `MOSEI/aligned_50.pkl`: `ef49589349bc1c2bc252ccc0d4657a755c92a056`  \n> `SIMS/unaligned_39.pkl`: `a00c73e92f66896403c09dbad63e242d5af756f8`\n\nDue to the size limitations, the MOSEI features and SIMS raw videos are available in `Baidu Cloud Drive` only. All dataset features are organized as:\n\n```python\n{\n    \"train\": {\n        \"raw_text\": [],\n        \"audio\": [],\n        \"vision\": [],\n        \"id\": [], # [video_id$_$clip_id, ..., ...]\n        \"text\": [],\n        \"text_bert\": [],\n        \"audio_lengths\": [],\n        \"vision_lengths\": [],\n        \"annotations\": [],\n        \"classification_labels\": [], # Negative(< 0), Neutral(0), Positive(> 0)\n        \"regression_labels\": []\n    },\n    \"valid\": {***}, # same as the \"train\"\n    \"test\": {***}, # same as the \"train\"\n}\n```\n\nFor MOSI and MOSEI, the pre-extracted text features are from BERT, different from the original glove features in the [CMU-Multimodal-SDK](http://immortal.multicomp.cs.cmu.edu/raw_datasets/processed_data/).\n\nFor SIMS, if you want to extract features from raw videos, you need to install [Openface Toolkits](https://github.com/TadasBaltrusaitis/OpenFace/wiki) first, and then refer our codes in the `data/DataPre.py`.\n\n```\npython data/DataPre.py --data_dir [path_to_Dataset] --language ** --openface2Path  [path_to_FeatureExtraction]\n```\n\nFor bert models, you also can download [Bert-Base, Chinese](https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip) from [Google-Bert](https://github.com/google-research/bert). And then, convert tensorflow into pytorch using [transformers-cli](https://huggingface.co/transformers/converting_tensorflow_models.html)\n\nThen, modify `config/config_*.py` to update dataset pathes.\n\n### Run\n\n```\npython run.py --modelName *** --datasetName ***\n```\n\n## Paper\n\n- [CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality](https://www.aclweb.org/anthology/2020.acl-main.343/)\n- [Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis](https://arxiv.org/abs/2102.04830)\n\nPlease cite our paper if you find our work useful for your research:\n\n```\n@inproceedings{yu2020ch,\n  title={CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality},\n  author={Yu, Wenmeng and Xu, Hua and Meng, Fanyang and Zhu, Yilin and Ma, Yixiao and Wu, Jiele and Zou, Jiyun and Yang, Kaicheng},\n  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},\n  pages={3718--3727},\n  year={2020}\n}\n```\n\n```\n@article{yu2021learning,\n  title={Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis},\n  author={Yu, Wenmeng and Xu, Hua and Yuan, Ziqi and Wu, Jiele},\n  journal={arXiv preprint arXiv:2102.04830},\n  year={2021}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/thuiar/MMSA",
    "keywords": "multimodal,sentiment analysis,mosi,mosei",
    "license": "",
    "maintainer": "iyuge2, FlameSky, Columbine21",
    "maintainer_email": "",
    "name": "MMSA",
    "package_url": "https://pypi.org/project/MMSA/",
    "platform": "",
    "project_url": "https://pypi.org/project/MMSA/",
    "project_urls": {
      "Documentation": "https://github.com/thuiar/MMSA/",
      "Homepage": "https://github.com/thuiar/MMSA",
      "Tracker": "https://github.com/thuiar/MMSA/issues"
    },
    "release_url": "https://pypi.org/project/MMSA/2.0.0/",
    "requires_dist": [
      "torch (>=1.9.1)",
      "transformers (>=4.4.0)",
      "numpy (>=1.20.3)",
      "pandas (>=1.2.5)",
      "tqdm (>=4.62.2)",
      "nvidia-ml-py3 (>=7.352.0)",
      "scikit-learn (>=0.24.2)",
      "easydict (>=1.9)"
    ],
    "requires_python": ">=3.8",
    "summary": "Multimodal Sentiment Analysis Framework",
    "version": "2.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15518193,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "774c983eafec7b0291b3b9c3696deb62c30aef25bad694fae5c344a20ec2c6a5",
        "md5": "77880c8741d1dd6fc0c8e59c957e3a1d",
        "sha256": "c335ff78370fbdd4e5d513120a40f447a4916307f92bbbe7ba1183c2eb6b42ff"
      },
      "downloads": -1,
      "filename": "MMSA-2.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "77880c8741d1dd6fc0c8e59c957e3a1d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 85535,
      "upload_time": "2022-01-17T06:56:29",
      "upload_time_iso_8601": "2022-01-17T06:56:29.674125Z",
      "url": "https://files.pythonhosted.org/packages/77/4c/983eafec7b0291b3b9c3696deb62c30aef25bad694fae5c344a20ec2c6a5/MMSA-2.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a17d074b59ffdcb9cfd96f7723c0f2ddce720b14fbf83b7d15a0ff3604322789",
        "md5": "a8050206a2c2c80276717529dbbc6994",
        "sha256": "908b5ff85a310f849ee83561930c88aeb409a50c3efa3a641bd1040f006d317a"
      },
      "downloads": -1,
      "filename": "MMSA-2.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "a8050206a2c2c80276717529dbbc6994",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 54973,
      "upload_time": "2022-01-17T06:56:31",
      "upload_time_iso_8601": "2022-01-17T06:56:31.496479Z",
      "url": "https://files.pythonhosted.org/packages/a1/7d/074b59ffdcb9cfd96f7723c0f2ddce720b14fbf83b7d15a0ff3604322789/MMSA-2.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}