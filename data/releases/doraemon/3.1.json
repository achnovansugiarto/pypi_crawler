{
  "info": {
    "author": "Andy Wong",
    "author_email": "wangyucheng@iie.ac.cn",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "\n\n# Doraemon\n**Doraemon is a set of crawlers.**\n\n**Tools**\n***\n1. Robust Requests\n2. Proxy Kit\n3. User-friendly Chrome\n***\n\n**Crawlers**\n***\n1. Google Knowledge Graph [Invalid]\n2. Google Translator\n3. Dianping [Invalid] # 大众点评 \n4. QQ music lyrics\n5. whois\n6. NetEase music comments\n***\n\n## Tools\n### 1. Robust Requests\n```python\nfrom Doraemon import requests_dora\nurl = \"https://www.baidu.com\"\n\nheaders = requests_dora.get_default_headers()\nheaders[\"User-Agent\"] = requests_dora.get_random_user_agent()\n\ndef get_proxies():\n    proxy_str = \"127.0.0.1:1080\"\n    proxies = {\"http\": \"http://%s\" % proxy_str,\n               \"https\": \"http://%s\" % proxy_str, }\n    return proxies\n\n# max_times, get_proxies_fun, and invoked_by are optional parameters, others are the same as the requests.get() and requests.post()\nres1 = requests_dora.try_best_2_get(url, max_times=5, get_proxies_fun=get_proxies, invoked_by=\"parent_fun_name\") \nres2 = requests_dora.try_best_2_post(url, max_times=5, get_proxies_fun=get_proxies)\nprint(res1.status_code)\nprint(res2.status_code)\n```\n### 2. Proxy Kit\n```python\nfrom Doraemon import proxies_dora\nproxies1 = proxies_dora.get_proxies(\"127.0.0.1:223\") # get a self-defined proxies dict\nproxies2 = proxies_dora.get_data5u_proxies(\"your data5u api key\") # input api key for crawling, get a proxies dict\n\npool = [\n\"127.0.0.1:233\",\n\"123.123.123.123:123\",\n\"...\",\n]\n\nproxies_dora.set_pool(pool) # set a self-defined proxy pool\nproxies3 = proxies_dora.get_proxies_fr_pool() # get a proxies dict from the pool\n\nloc_info1 = proxies_dora.loc_proxy_ipv4(proxies1) # get location info of a given proxy, ipv4 only\nloc_info2 = proxies_dora.loc_proxy(proxies2) # get location info of a given proxy, for both ipv4 and ipv6\n\n```\n### 3. User-friendly Chrome\n```python\nfrom Doraemon import chrome_dora\n\nproxy = \"127.0.0.1:1080\"\nbaidu_url = \"https://www.baidu.com\"\n# no_images: do not load images(response more quickly)\n# headless: make the chrome invisible\n# proxy: set if you need\n# they are all optional\nchrome = chrome_dora.MyChrome(headless=False, proxy=\"127.0.0.1:1080\", no_images=True) \nchrome.get(baidu_url)\nprint(chrome.page_source)\n```\n\n## Crawlers\n### 1. Google Knowledge Graph [invalid]\n```python\nfrom Doraemon import google_KG\n\ndef get_proxies():\n    proxy_str = \"127.0.0.1:1080\"\n    proxies = {\"http\": \"http://%s\" % proxy_str,\n               \"https\": \"http://%s\" % proxy_str, }\n    return proxies\n\nres = google_KG.get_entity(\"alibaba\", get_proxies_fun=get_proxies)\nprint(res)\n```\n\n### 2. Google Translator\n```python\nfrom Doraemon import google_translator, proxies_dora\n\ndef get_proxies():\n    proxy_str = \"127.0.0.1:1080\"\n    proxies = {\"http\": \"http://%s\" % proxy_str,\n               \"https\": \"http://%s\" % proxy_str, }\n    return proxies\n\nori_text = \"中华民国\"\n# sl, tl and get_proxies_fun are optional, the default values are \"auto\", \"en\", None\nres1 = google_translator.trans(ori_text,sl=\"auto\", tl=\"zh-TW\", get_proxies_fun=get_proxies) \n# replace the function get_proxies with proxies_dora.get_proxies(\"127.0.0.1:1080\")\nres2 = google_translator.trans(ori_text,sl=\"auto\", tl=\"zh-TW\", get_proxies_fun=lambda: proxies_dora.get_proxies(\"127.0.0.1:1080\")) \n\nlong_text = ori_text * 2500 # 10000 characters\nres3 = google_translator.trans_long(long_text)# if len(text) > 5000\n\nprint(res1)\nprint(res2)\n```\n\n**Language Code:**\n```angular2html\n检测语言: auto\n阿尔巴尼亚语: sq\n阿拉伯语: ar\n阿姆哈拉语: am\n阿塞拜疆语: az\n爱尔兰语: ga\n爱沙尼亚语: et\n巴斯克语: eu\n白俄罗斯语: be\n保加利亚语: bg\n冰岛语: is\n波兰语: pl\n波斯尼亚语: bs\n波斯语: fa\n布尔语(南非荷兰语): af\n丹麦语: da\n德语: de\n俄语: ru\n法语: fr\n菲律宾语: tl\n芬兰语: fi\n弗里西语: fy\n高棉语: km\n格鲁吉亚语: ka\n古吉拉特语: gu\n哈萨克语: kk\n海地克里奥尔语: ht\n韩语: ko\n豪萨语: ha\n荷兰语: nl\n吉尔吉斯语: ky\n加利西亚语: gl\n加泰罗尼亚语: ca\n捷克语: cs\n卡纳达语: kn\n科西嘉语: co\n克罗地亚语: hr\n库尔德语: ku\n拉丁语: la\n拉脱维亚语: lv\n老挝语: lo\n立陶宛语: lt\n卢森堡语: lb\n罗马尼亚语: ro\n马尔加什语: mg\n马耳他语: mt\n马拉地语: mr\n马拉雅拉姆语: ml\n马来语: ms\n马其顿语: mk\n毛利语: mi\n蒙古语: mn\n孟加拉语: bn\n缅甸语: my\n苗语: hmn\n南非科萨语: xh\n南非祖鲁语: zu\n尼泊尔语: ne\n挪威语: no\n旁遮普语: pa\n葡萄牙语: pt\n普什图语: ps\n齐切瓦语: ny\n日语: ja\n瑞典语: sv\n萨摩亚语: sm\n塞尔维亚语: sr\n塞索托语: st\n僧伽罗语: si\n世界语: eo\n斯洛伐克语: sk\n斯洛文尼亚语: sl\n斯瓦希里语: sw\n苏格兰盖尔语: gd\n宿务语: ceb\n索马里语: so\n塔吉克语: tg\n泰卢固语: te\n泰米尔语: ta\n泰语: th\n土耳其语: tr\n威尔士语: cy\n乌尔都语: ur\n乌克兰语: uk\n乌兹别克语: uz\n西班牙语: es\n希伯来语: iw\n希腊语: el\n夏威夷语: haw\n信德语: sd\n匈牙利语: hu\n修纳语: sn\n亚美尼亚语: hy\n伊博语: ig\n意大利语: it\n意第绪语: yi\n印地语: hi\n印尼巽他语: su\n印尼语: id\n印尼爪哇语: jw\n英语: en\n约鲁巴语: yo\n越南语: vi\n中文(繁体): zh-TW\n中文(简体): zh-CN\n```\n### 3. Dianping [partly invalid: character decoding]\n```python\nfrom Doraemon import dianping, proxies_dora\nimport json\n\n# get_proxies_fun is optional, set if you want to use a proxy\nshop_list = dianping.search_shops(\"2\", \"4s店\", 1, get_proxies_fun=lambda: proxies_dora.get_proxies(\"127.0.0.1:1080\")) # args: city id, keyword, page index\nprint(json.dumps(shop_list, indent=2, ensure_ascii=False))\n# [{\"name\": \"shopname1\", \"shop_id\": \"1245587}, ...]\n\n# get_proxies_fun is optional, set if you want to use a proxy, this example use data5u proxy, \n# the website is :http://www.data5u.com/api/doc-dynamic.html \nshop_list_around = dianping.get_around(\"2\", \"5724615\", 2000, 1, get_proxies_fun=lambda: proxies_dora.get_data5u_proxies(\"your data5u api key\")) # args: city id, shop id, max distance, page index\nprint(json.dumps(shop_list_around, indent=2, ensure_ascii=False))\n'''\nshop_list_around is like this:\n    [\n      {\n        \"img_src\": \"https://img.meituan.net/msmerchant/2e5787325ba4579ec2e2e3f45038ade1149446.jpg%40340w_255h_1e_1c_1l%7Cwatermark%3D1%26%26r%3D1%26p%3D9%26x%3D2%26y%3D2%26relative%3D1%26o%3D20\",\n        \"title\": \"速度披萨(华贸城店)\",\n        \"star_level\": 4.5,\n        \"review_num\": 30,\n        \"mean_price\": 89,\n        \"cat\": \"西餐\",\n        \"region\": \"北苑家园\",\n        \"addr\": \"清苑路13号\",\n        \"rec_dish\": [\n          \"黑芝麻沙拉\",\n          \"蟹肉意面\",\n          \"火腿榴莲披萨双拼\"\n        ],\n        \"score\": {\n          \"taste\": 8.5,\n          \"env\": 8.4,\n          \"service\": 8.4\n        }\n      },\n    ]\n'''\n```\n\n### 4. QQ music lyrics\n```python\n# crawl songs by areas\narea_list = [\"港台\", \"内地\"] # {'全部': -100, '内地': 200, '港台': 2, '欧美': 5, '日本': 4, '韩国': 3, '其他': 6}\nsave_path = \"./qq_music_songs_by_area\"\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ncrawl_songs(area_list, save_path)\n```\n\n### 5. whois\n```python\nip_list = [\"154.17.24.36\", \"154.17.24.37\", \"154.17.24.39\", \"154.17.21.36\"] * 100\n# friendly\nres = get_org_name_by_reg_db_friendly(ip_list, block_size = 100, sleep = 2)\n# no limited\nres = get_org_name_by_registration_db_for_list(ip_list)\nprint(len(res))\n```\n\n### 6. NetEase music comments \nrun under `netease_music`\n```bash\nscrapy crawl comments\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/131250208/Doraemon",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "Doraemon",
    "package_url": "https://pypi.org/project/Doraemon/",
    "platform": "",
    "project_url": "https://pypi.org/project/Doraemon/",
    "project_urls": {
      "Homepage": "https://github.com/131250208/Doraemon"
    },
    "release_url": "https://pypi.org/project/Doraemon/3.1/",
    "requires_dist": [
      "selenium",
      "requests",
      "bs4",
      "aiohttp",
      "tqdm",
      "rsa",
      "PyExecJS"
    ],
    "requires_python": ">=3.5.0",
    "summary": "The pocket of Doraemon: many crawlers",
    "version": "3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8966841,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "863fe4de085d04a89134deb528ae59fe67e3cd4dfcfb48937a34ba6474278cb8",
        "md5": "b62172c2027ab7500750978d24c92e72",
        "sha256": "8c3bfbf5c83c12111d8c8ab4c8b1b91081fc9d1677928f73def3adf74292974d"
      },
      "downloads": -1,
      "filename": "Doraemon-3.1-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b62172c2027ab7500750978d24c92e72",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": ">=3.5.0",
      "size": 48365,
      "upload_time": "2020-11-16T04:10:54",
      "upload_time_iso_8601": "2020-11-16T04:10:54.533854Z",
      "url": "https://files.pythonhosted.org/packages/86/3f/e4de085d04a89134deb528ae59fe67e3cd4dfcfb48937a34ba6474278cb8/Doraemon-3.1-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b4ae62c2eea6c8d9c041ad9dd65a3768c0e4ad5ed98dcfc372deba256e7ad0b8",
        "md5": "21829058e9a6cba14b86b0b04946c20a",
        "sha256": "5d72e088a0cb89ae281e59f00ffe5fa55f21478273ec246fb02f985f29fc0a03"
      },
      "downloads": -1,
      "filename": "Doraemon-3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "21829058e9a6cba14b86b0b04946c20a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5.0",
      "size": 29856,
      "upload_time": "2020-11-16T04:10:56",
      "upload_time_iso_8601": "2020-11-16T04:10:56.148543Z",
      "url": "https://files.pythonhosted.org/packages/b4/ae/62c2eea6c8d9c041ad9dd65a3768c0e4ad5ed98dcfc372deba256e7ad0b8/Doraemon-3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}