{
  "info": {
    "author": "Seldon Technologies Ltd.",
    "author_email": "hello@seldon.io",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# MLServer\n\nAn open source inference server for your machine learning models.\n\n![](https://raw.githubusercontent.com/SeldonIO/MLServer/master/docs/assets/mlserver-logo.png)\n\n## Overview\n\nMLServer aims to provide an easy way to start serving your machine learning\nmodels through a REST and gRPC interface, fully compliant with [KFServing's V2\nDataplane](https://kserve.github.io/website/modelserving/inference_api/)\nspec.\n\n- Multi-model serving, letting users run multiple models within the same\n  process.\n- Ability to run [inference in parallel for vertical\n  scaling](https://mlserver.readthedocs.io/en/latest/user-guide/parallel-inference.html)\n  across multiple models through a pool of inference workers.\n- Support for [adaptive\n  batching](https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html),\n  to group inference requests together on the fly.\n- Scalability with deployment in Kubernetes native frameworks, including\n  [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html#v2-kfserving-protocol) and\n  [KServe (formerly known as KFServing)](https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/), where\n  MLServer is the core Python inference server used to serve machine learning\n  models.\n- Support for the standard [V2 Inference Protocol](https://kserve.github.io/website/modelserving/inference_api/) on\n  both the gRPC and REST flavours, which has been standardised and adopted by\n  various model serving frameworks.\n\nYou can read more about the goals of this project on the [inital design\ndocument](https://docs.google.com/document/d/1C2uf4SaAtwLTlBCciOhvdiKQ2Eay4U72VxAD4bXe7iU/edit?usp=sharing).\n\n## Usage\n\nYou can install the `mlserver` package running:\n\n```bash\npip install mlserver\n```\n\nNote that to use any of the optional [inference runtimes](#Inference-Runtimes),\nyou'll need to install the relevant package.\nFor example, to serve a `scikit-learn` model, you would need to install the\n`mlserver-sklearn` package:\n\n```bash\npip install mlserver-sklearn\n```\n\nFor further information on how to use MLServer, you can check any of the\n[available examples](#examples).\n\n## Inference Runtimes\n\nInference runtimes allow you to define how your model should be used within\nMLServer.\nYou can think of them as the **backend glue** between MLServer and your machine\nlearning framework of choice.\nYou can read more about [inference runtimes in their documentation\npage](./docs/runtimes/index.md).\n\nOut of the box, MLServer comes with a set of pre-packaged runtimes which let\nyou interact with a subset of common frameworks.\nThis allows you to start serving models saved in these frameworks straight\naway.\nHowever, it's also possible to **write [custom\nruntimes](./docs/runtimes/custom)**.\n\nOut of the box, MLServer provides support for:\n\n| Framework    | Supported | Documentation                                                    |\n| ------------ | --------- | ---------------------------------------------------------------- |\n| Scikit-Learn | ✅        | [MLServer SKLearn](./runtimes/sklearn)                           |\n| XGBoost      | ✅        | [MLServer XGBoost](./runtimes/xgboost)                           |\n| Spark MLlib  | ✅        | [MLServer MLlib](./runtimes/mllib)                               |\n| LightGBM     | ✅        | [MLServer LightGBM](./runtimes/lightgbm)                         |\n| Tempo        | ✅        | [`github.com/SeldonIO/tempo`](https://github.com/SeldonIO/tempo) |\n| MLflow       | ✅        | [MLServer MLflow](./runtimes/mlflow)                             |\n\n## Examples\n\nTo see MLServer in action, check out [our full list of\nexamples](./docs/examples/index.md).\nYou can find below a few selected examples showcasing how you can leverage\nMLServer to start serving your machine learning models.\n\n- [Serving a `scikit-learn` model](./docs/examples/sklearn/README.md)\n- [Serving a `xgboost` model](./docs/examples/xgboost/README.md)\n- [Serving a `lightgbm` model](./docs/examples/lightgbm/README.md)\n- [Serving a `tempo` pipeline](./docs/examples/tempo/README.md)\n- [Serving a custom model](./docs/examples/custom/README.md)\n- [Multi-Model Serving with multiple frameworks](./docs/examples/mms/README.md)\n- [Loading / unloading models from a model repository](./docs/examples/model-repository/README.md)\n\n## Developer Guide\n\n### Versioning\n\nBoth the main `mlserver` package and the [inference runtimes\npackages](./docs/runtimes/index.md) try to follow the same versioning schema.\nTo bump the version across all of them, you can use the\n[`./hack/update-version.sh`](./hack/update-version.sh) script.\nFor example:\n\n```bash\n./hack/update-version.sh 0.2.0.dev1\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/SeldonIO/MLServer.git",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlserver",
    "package_url": "https://pypi.org/project/mlserver/",
    "platform": "",
    "project_url": "https://pypi.org/project/mlserver/",
    "project_urls": {
      "Homepage": "https://github.com/SeldonIO/MLServer.git"
    },
    "release_url": "https://pypi.org/project/mlserver/1.0.0/",
    "requires_dist": [
      "click",
      "fastapi",
      "grpcio",
      "numpy",
      "pandas",
      "protobuf",
      "uvicorn",
      "starlette-exporter",
      "py-grpc-prometheus",
      "importlib-metadata ; python_version < \"3.8\"",
      "orjson ; extra == 'all'"
    ],
    "requires_python": "",
    "summary": "ML server",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17395818,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "de8fef9c6d667ade692ce55045004f957b37334ebca4a47b8866d4b7b380bf43",
        "md5": "8e6cbe45b1cf3e734a8d62dc0e215164",
        "sha256": "219fba4169be11f43010435baaf38f02daa82165bce4b18653c2e0a26ea04984"
      },
      "downloads": -1,
      "filename": "mlserver-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8e6cbe45b1cf3e734a8d62dc0e215164",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 64103,
      "upload_time": "2022-02-11T11:50:40",
      "upload_time_iso_8601": "2022-02-11T11:50:40.312712Z",
      "url": "https://files.pythonhosted.org/packages/de/8f/ef9c6d667ade692ce55045004f957b37334ebca4a47b8866d4b7b380bf43/mlserver-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fb515eb44566be128751f0c5539b0176bf57d7120b92914034f41ec7403bd41f",
        "md5": "26de6bdbba6dadf9442de9fe1c783657",
        "sha256": "a7cb393a04b7554dec37ec4365d97c6a1e1eb594dfe9060a13f88b49bd4de6cc"
      },
      "downloads": -1,
      "filename": "mlserver-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "26de6bdbba6dadf9442de9fe1c783657",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 50645,
      "upload_time": "2022-02-11T11:50:52",
      "upload_time_iso_8601": "2022-02-11T11:50:52.314082Z",
      "url": "https://files.pythonhosted.org/packages/fb/51/5eb44566be128751f0c5539b0176bf57d7120b92914034f41ec7403bd41f/mlserver-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}