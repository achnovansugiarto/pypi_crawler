{
  "info": {
    "author": "Seldon Technologies Ltd.",
    "author_email": "hello@seldon.io",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# MLServer\n\nAn open source inference server for your machine learning models.\n\n![](https://raw.githubusercontent.com/SeldonIO/MLServer/master/docs/assets/mlserver-logo.png)\n\n## Overview\n\nMLServer aims to provide an easy way to start serving your machine learning\nmodels through a REST and gRPC interface, fully compliant with [KFServing's V2\nDataplane](https://kserve.github.io/website/modelserving/inference_api/)\nspec.\n\n- Multi-model serving, letting users run multiple models within the same\n  process.\n- Ability to run [inference in parallel for vertical\n  scaling](https://mlserver.readthedocs.io/en/latest/user-guide/parallel-inference.html)\n  across multiple models through a pool of inference workers.\n- Support for [adaptive\n  batching](https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html),\n  to group inference requests together on the fly.\n- Scalability with deployment in Kubernetes native frameworks, including\n  [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html#v2-kfserving-protocol) and\n  [KServe (formerly known as KFServing)](https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/), where\n  MLServer is the core Python inference server used to serve machine learning\n  models.\n- Support for the standard [V2 Inference Protocol](https://kserve.github.io/website/modelserving/inference_api/) on\n  both the gRPC and REST flavours, which has been standardised and adopted by\n  various model serving frameworks.\n\nYou can read more about the goals of this project on the [inital design\ndocument](https://docs.google.com/document/d/1C2uf4SaAtwLTlBCciOhvdiKQ2Eay4U72VxAD4bXe7iU/edit?usp=sharing).\n\n## Usage\n\nYou can install the `mlserver` package running:\n\n```bash\npip install mlserver\n```\n\nNote that to use any of the optional [inference runtimes](#Inference-Runtimes),\nyou'll need to install the relevant package.\nFor example, to serve a `scikit-learn` model, you would need to install the\n`mlserver-sklearn` package:\n\n```bash\npip install mlserver-sklearn\n```\n\nFor further information on how to use MLServer, you can check any of the\n[available examples](#examples).\n\n## Inference Runtimes\n\nInference runtimes allow you to define how your model should be used within\nMLServer.\nYou can think of them as the **backend glue** between MLServer and your machine\nlearning framework of choice.\nYou can read more about [inference runtimes in their documentation\npage](./docs/runtimes/index.md).\n\nOut of the box, MLServer comes with a set of pre-packaged runtimes which let\nyou interact with a subset of common frameworks.\nThis allows you to start serving models saved in these frameworks straight\naway.\nHowever, it's also possible to **write [custom\nruntimes](./docs/runtimes/custom)**.\n\nOut of the box, MLServer provides support for:\n\n| Framework    | Supported | Documentation                                                    |\n| ------------ | --------- | ---------------------------------------------------------------- |\n| Scikit-Learn | ✅        | [MLServer SKLearn](./runtimes/sklearn)                           |\n| XGBoost      | ✅        | [MLServer XGBoost](./runtimes/xgboost)                           |\n| Spark MLlib  | ✅        | [MLServer MLlib](./runtimes/mllib)                               |\n| LightGBM     | ✅        | [MLServer LightGBM](./runtimes/lightgbm)                         |\n| Tempo        | ✅        | [`github.com/SeldonIO/tempo`](https://github.com/SeldonIO/tempo) |\n| MLflow       | ✅        | [MLServer MLflow](./runtimes/mlflow)                             |\n\n## Examples\n\nTo see MLServer in action, check out [our full list of\nexamples](./docs/examples/index.md).\nYou can find below a few selected examples showcasing how you can leverage\nMLServer to start serving your machine learning models.\n\n- [Serving a `scikit-learn` model](./docs/examples/sklearn/README.md)\n- [Serving a `xgboost` model](./docs/examples/xgboost/README.md)\n- [Serving a `lightgbm` model](./docs/examples/lightgbm/README.md)\n- [Serving a `tempo` pipeline](./docs/examples/tempo/README.md)\n- [Serving a custom model](./docs/examples/custom/README.md)\n- [Multi-Model Serving with multiple frameworks](./docs/examples/mms/README.md)\n- [Loading / unloading models from a model repository](./docs/examples/model-repository/README.md)\n\n## Developer Guide\n\n### Versioning\n\nBoth the main `mlserver` package and the [inference runtimes\npackages](./docs/runtimes/index.md) try to follow the same versioning schema.\nTo bump the version across all of them, you can use the\n[`./hack/update-version.sh`](./hack/update-version.sh) script.\nFor example:\n\n```bash\n./hack/update-version.sh 0.2.0.dev1\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/SeldonIO/MLServer.git",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlserver",
    "package_url": "https://pypi.org/project/mlserver/",
    "platform": null,
    "project_url": "https://pypi.org/project/mlserver/",
    "project_urls": {
      "Homepage": "https://github.com/SeldonIO/MLServer.git"
    },
    "release_url": "https://pypi.org/project/mlserver/1.0.1/",
    "requires_dist": [
      "click",
      "fastapi",
      "grpcio",
      "numpy",
      "pandas",
      "protobuf",
      "uvicorn",
      "starlette-exporter",
      "py-grpc-prometheus",
      "importlib-metadata ; python_version < \"3.8\"",
      "orjson ; extra == 'all'"
    ],
    "requires_python": "",
    "summary": "ML server",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17395818,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "62e9a449e2fc5db0f90b55f660decb53983bb55896afae86a89010c4daa5a4ef",
        "md5": "c5780346ef3930ee05a4da7c222da2c4",
        "sha256": "c4ea369976012e306eb69104db17f70bd89086d90adda17c8e3f06c47d4747ff"
      },
      "downloads": -1,
      "filename": "mlserver-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c5780346ef3930ee05a4da7c222da2c4",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 65869,
      "upload_time": "2022-03-08T18:01:19",
      "upload_time_iso_8601": "2022-03-08T18:01:19.757381Z",
      "url": "https://files.pythonhosted.org/packages/62/e9/a449e2fc5db0f90b55f660decb53983bb55896afae86a89010c4daa5a4ef/mlserver-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e9ccdc6004da0edb8fc38737121b72ad361fb2d53e0d3e95be7a1844cb944e4f",
        "md5": "0c42dd56770135e35a377f501ae5b391",
        "sha256": "491be6210776fccdb5ba22545c8c2833bbf61df486c4d965e43a55ec3da0c913"
      },
      "downloads": -1,
      "filename": "mlserver-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "0c42dd56770135e35a377f501ae5b391",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 52029,
      "upload_time": "2022-03-08T18:01:35",
      "upload_time_iso_8601": "2022-03-08T18:01:35.888887Z",
      "url": "https://files.pythonhosted.org/packages/e9/cc/dc6004da0edb8fc38737121b72ad361fb2d53e0d3e95be7a1844cb944e4f/mlserver-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}