{
  "info": {
    "author": "Seldon Technologies Ltd.",
    "author_email": "hello@seldon.io",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# MLServer\n\nAn open source inference server for your machine learning models.\n\n[![video_play_icon](https://user-images.githubusercontent.com/10466106/151803854-75d17c32-541c-4eee-b589-d45b07ea486d.png)](https://www.youtube.com/watch?v=aZHe3z-8C_w)\n\n## Overview\n\nMLServer aims to provide an easy way to start serving your machine learning\nmodels through a REST and gRPC interface, fully compliant with [KFServing's V2\nDataplane](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html)\nspec. Watch a quick video introducing the project [here](https://www.youtube.com/watch?v=aZHe3z-8C_w).\n\n- Multi-model serving, letting users run multiple models within the same\n  process.\n- Ability to run [inference in parallel for vertical\n  scaling](https://mlserver.readthedocs.io/en/latest/user-guide/parallel-inference.html)\n  across multiple models through a pool of inference workers.\n- Support for [adaptive\n  batching](https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html),\n  to group inference requests together on the fly.\n- Scalability with deployment in Kubernetes native frameworks, including\n  [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html#v2-kfserving-protocol) and\n  [KServe (formerly known as KFServing)](https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/), where\n  MLServer is the core Python inference server used to serve machine learning\n  models.\n- Support for the standard [V2 Inference Protocol](https://docs.seldon.io/projects/seldon-core/en/latest/reference/apis/v2-protocol.html) on\n  both the gRPC and REST flavours, which has been standardised and adopted by\n  various model serving frameworks.\n\nYou can read more about the goals of this project on the [inital design\ndocument](https://docs.google.com/document/d/1C2uf4SaAtwLTlBCciOhvdiKQ2Eay4U72VxAD4bXe7iU/edit?usp=sharing).\n\n## Usage\n\nYou can install the `mlserver` package running:\n\n```bash\npip install mlserver\n```\n\nNote that to use any of the optional [inference runtimes](#inference-runtimes),\nyou'll need to install the relevant package.\nFor example, to serve a `scikit-learn` model, you would need to install the\n`mlserver-sklearn` package:\n\n```bash\npip install mlserver-sklearn\n```\n\nFor further information on how to use MLServer, you can check any of the\n[available examples](#examples).\n\n## Inference Runtimes\n\nInference runtimes allow you to define how your model should be used within\nMLServer.\nYou can think of them as the **backend glue** between MLServer and your machine\nlearning framework of choice.\nYou can read more about [inference runtimes in their documentation\npage](./docs/runtimes/index.md).\n\nOut of the box, MLServer comes with a set of pre-packaged runtimes which let\nyou interact with a subset of common frameworks.\nThis allows you to start serving models saved in these frameworks straight\naway.\nHowever, it's also possible to **[write custom\nruntimes](./docs/runtimes/custom.md)**.\n\nOut of the box, MLServer provides support for:\n\n| Framework     | Supported | Documentation                                                    |\n| ------------- | --------- | ---------------------------------------------------------------- |\n| Scikit-Learn  | ✅        | [MLServer SKLearn](./runtimes/sklearn)                           |\n| XGBoost       | ✅        | [MLServer XGBoost](./runtimes/xgboost)                           |\n| Spark MLlib   | ✅        | [MLServer MLlib](./runtimes/mllib)                               |\n| LightGBM      | ✅        | [MLServer LightGBM](./runtimes/lightgbm)                         |\n| Tempo         | ✅        | [`github.com/SeldonIO/tempo`](https://github.com/SeldonIO/tempo) |\n| MLflow        | ✅        | [MLServer MLflow](./runtimes/mlflow)                             |\n| Alibi-Detect  | ✅        | [MLServer Alibi Detect](./runtimes/alibi-detect)                 |\n| Alibi-Explain | ✅        | [MLServer Alibi Explain](./runtimes/alibi-explain)               |\n| HuggingFace   | ✅        | [MLServer HuggingFace](./runtimes/huggingface)                   |\n\n## Examples\n\nTo see MLServer in action, check out [our full list of\nexamples](./docs/examples/index.md).\nYou can find below a few selected examples showcasing how you can leverage\nMLServer to start serving your machine learning models.\n\n- [Serving a `scikit-learn` model](./docs/examples/sklearn/README.md)\n- [Serving a `xgboost` model](./docs/examples/xgboost/README.md)\n- [Serving a `lightgbm` model](./docs/examples/lightgbm/README.md)\n- [Serving a `tempo` pipeline](./docs/examples/tempo/README.md)\n- [Serving a custom model](./docs/examples/custom/README.md)\n- [Serving an `alibi-detect` model](./docs/examples/alibi-detect/README.md)\n- [Serving a `HuggingFace` model](./docs/examples/huggingface/README.md)\n- [Multi-Model Serving with multiple frameworks](./docs/examples/mms/README.md)\n- [Loading / unloading models from a model repository](./docs/examples/model-repository/README.md)\n\n## Developer Guide\n\n### Versioning\n\nBoth the main `mlserver` package and the [inference runtimes\npackages](./docs/runtimes/index.md) try to follow the same versioning schema.\nTo bump the version across all of them, you can use the\n[`./hack/update-version.sh`](./hack/update-version.sh) script.\n\nFor example:\n\n```bash\n./hack/update-version.sh 0.2.0.dev1\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/SeldonIO/MLServer.git",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlserver",
    "package_url": "https://pypi.org/project/mlserver/",
    "platform": null,
    "project_url": "https://pypi.org/project/mlserver/",
    "project_urls": {
      "Homepage": "https://github.com/SeldonIO/MLServer.git"
    },
    "release_url": "https://pypi.org/project/mlserver/1.2.3/",
    "requires_dist": [
      "click",
      "fastapi (!=0.89.0,<=0.89.1)",
      "python-dotenv",
      "grpcio",
      "numpy",
      "pandas",
      "protobuf",
      "uvicorn",
      "starlette-exporter",
      "py-grpc-prometheus",
      "aiokafka",
      "tritonclient[http] (>=2.24)",
      "aiofiles",
      "orjson",
      "importlib-metadata ; python_version < \"3.8\"",
      "uvloop ; sys_platform != \"win32\" and (sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\")"
    ],
    "requires_python": "",
    "summary": "ML server",
    "version": "1.2.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17395818,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cdb68290938ee88434526162460ad6b5aae6e9a3844c4369b04177d0d329bb03",
        "md5": "ba10f0a94b946eae7eb34a249064fcca",
        "sha256": "17ec7fea481c8f7873c39a292a4bc1ab2dd677e104c12a314ecfc70664091354"
      },
      "downloads": -1,
      "filename": "mlserver-1.2.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ba10f0a94b946eae7eb34a249064fcca",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 95996,
      "upload_time": "2023-01-16T09:32:57",
      "upload_time_iso_8601": "2023-01-16T09:32:57.927097Z",
      "url": "https://files.pythonhosted.org/packages/cd/b6/8290938ee88434526162460ad6b5aae6e9a3844c4369b04177d0d329bb03/mlserver-1.2.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e6a9db430fe821747a5641dc1e6f9c4eeb509bf271bda7347ebcb46619ae2b67",
        "md5": "503c56a27e7be956969b908ccb0e546d",
        "sha256": "e3c9da38f5c3ed7f6af0c7fb8ee7086192fe23a034426d12c4bfd0b48dfa0c8a"
      },
      "downloads": -1,
      "filename": "mlserver-1.2.3.tar.gz",
      "has_sig": false,
      "md5_digest": "503c56a27e7be956969b908ccb0e546d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 74017,
      "upload_time": "2023-01-16T09:32:59",
      "upload_time_iso_8601": "2023-01-16T09:32:59.552665Z",
      "url": "https://files.pythonhosted.org/packages/e6/a9/db430fe821747a5641dc1e6f9c4eeb509bf271bda7347ebcb46619ae2b67/mlserver-1.2.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}