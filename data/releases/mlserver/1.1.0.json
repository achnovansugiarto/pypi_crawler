{
  "info": {
    "author": "Seldon Technologies Ltd.",
    "author_email": "hello@seldon.io",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# MLServer\n\nAn open source inference server for your machine learning models.\n\n[![video_play_icon](https://user-images.githubusercontent.com/10466106/151803854-75d17c32-541c-4eee-b589-d45b07ea486d.png)](https://www.youtube.com/watch?v=aZHe3z-8C_w)\n\n## Overview\n\nMLServer aims to provide an easy way to start serving your machine learning\nmodels through a REST and gRPC interface, fully compliant with [KFServing's V2\nDataplane](https://kserve.github.io/website/modelserving/inference_api/)\nspec. Watch a quick video introducing the project [here](https://www.youtube.com/watch?v=aZHe3z-8C_w).\n\n- Multi-model serving, letting users run multiple models within the same\n  process.\n- Ability to run [inference in parallel for vertical\n  scaling](https://mlserver.readthedocs.io/en/latest/user-guide/parallel-inference.html)\n  across multiple models through a pool of inference workers.\n- Support for [adaptive\n  batching](https://mlserver.readthedocs.io/en/latest/user-guide/adaptive-batching.html),\n  to group inference requests together on the fly.\n- Scalability with deployment in Kubernetes native frameworks, including\n  [Seldon Core](https://docs.seldon.io/projects/seldon-core/en/latest/graph/protocols.html#v2-kfserving-protocol) and\n  [KServe (formerly known as KFServing)](https://kserve.github.io/website/modelserving/v1beta1/sklearn/v2/), where\n  MLServer is the core Python inference server used to serve machine learning\n  models.\n- Support for the standard [V2 Inference Protocol](https://kserve.github.io/website/modelserving/inference_api/) on\n  both the gRPC and REST flavours, which has been standardised and adopted by\n  various model serving frameworks.\n\nYou can read more about the goals of this project on the [inital design\ndocument](https://docs.google.com/document/d/1C2uf4SaAtwLTlBCciOhvdiKQ2Eay4U72VxAD4bXe7iU/edit?usp=sharing).\n\n## Usage\n\nYou can install the `mlserver` package running:\n\n```bash\npip install mlserver\n```\n\nNote that to use any of the optional [inference runtimes](#Inference-Runtimes),\nyou'll need to install the relevant package.\nFor example, to serve a `scikit-learn` model, you would need to install the\n`mlserver-sklearn` package:\n\n```bash\npip install mlserver-sklearn\n```\n\nFor further information on how to use MLServer, you can check any of the\n[available examples](#examples).\n\n## Inference Runtimes\n\nInference runtimes allow you to define how your model should be used within\nMLServer.\nYou can think of them as the **backend glue** between MLServer and your machine\nlearning framework of choice.\nYou can read more about [inference runtimes in their documentation\npage](./docs/runtimes/index.md).\n\nOut of the box, MLServer comes with a set of pre-packaged runtimes which let\nyou interact with a subset of common frameworks.\nThis allows you to start serving models saved in these frameworks straight\naway.\nHowever, it's also possible to **[write custom\nruntimes](./docs/runtimes/custom.md)**.\n\nOut of the box, MLServer provides support for:\n\n| Framework    | Supported | Documentation                                                    |\n| ------------ | --------- | ---------------------------------------------------------------- |\n| Scikit-Learn | ✅        | [MLServer SKLearn](./runtimes/sklearn)                           |\n| XGBoost      | ✅        | [MLServer XGBoost](./runtimes/xgboost)                           |\n| Spark MLlib  | ✅        | [MLServer MLlib](./runtimes/mllib)                               |\n| LightGBM     | ✅        | [MLServer LightGBM](./runtimes/lightgbm)                         |\n| Tempo        | ✅        | [`github.com/SeldonIO/tempo`](https://github.com/SeldonIO/tempo) |\n| MLflow       | ✅        | [MLServer MLflow](./runtimes/mlflow)                             |\n| Alibi-Detect | ✅        | [MLServer Alibi Detect](./runtimes/alibi-detect)                 |\n\n## Examples\n\nTo see MLServer in action, check out [our full list of\nexamples](./docs/examples/index.md).\nYou can find below a few selected examples showcasing how you can leverage\nMLServer to start serving your machine learning models.\n\n- [Serving a `scikit-learn` model](./docs/examples/sklearn/README.md)\n- [Serving a `xgboost` model](./docs/examples/xgboost/README.md)\n- [Serving a `lightgbm` model](./docs/examples/lightgbm/README.md)\n- [Serving a `tempo` pipeline](./docs/examples/tempo/README.md)\n- [Serving a custom model](./docs/examples/custom/README.md)\n- [Serving an `alibi-detect` model](./docs/examples/alibi-detect/README.md)\n- [Multi-Model Serving with multiple frameworks](./docs/examples/mms/README.md)\n- [Loading / unloading models from a model repository](./docs/examples/model-repository/README.md)\n\n## Developer Guide\n\n### Versioning\n\nBoth the main `mlserver` package and the [inference runtimes\npackages](./docs/runtimes/index.md) try to follow the same versioning schema.\nTo bump the version across all of them, you can use the\n[`./hack/update-version.sh`](./hack/update-version.sh) script.\n\nFor example:\n\n```bash\n./hack/update-version.sh 0.2.0.dev1\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/SeldonIO/MLServer.git",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlserver",
    "package_url": "https://pypi.org/project/mlserver/",
    "platform": null,
    "project_url": "https://pypi.org/project/mlserver/",
    "project_urls": {
      "Homepage": "https://github.com/SeldonIO/MLServer.git"
    },
    "release_url": "https://pypi.org/project/mlserver/1.1.0/",
    "requires_dist": [
      "click",
      "fastapi",
      "python-dotenv",
      "grpcio",
      "numpy",
      "pandas",
      "protobuf",
      "uvicorn",
      "starlette-exporter",
      "py-grpc-prometheus",
      "aiokafka",
      "importlib-metadata ; python_version < \"3.8\"",
      "uvloop ; sys_platform != \"win32\" and (sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\")",
      "orjson ; extra == 'all'"
    ],
    "requires_python": "",
    "summary": "ML server",
    "version": "1.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17395818,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2cb480e7b83ff05ce2b9c0aca150b7e3453aaee2c198eee26d13380e515fd4e1",
        "md5": "059121757425010cfff104203a0ddda1",
        "sha256": "bb183567f2ffd2678b1ea8ceec5c0b5e1c6dc8f83cc8e3f7a91a91ee5f3d472e"
      },
      "downloads": -1,
      "filename": "mlserver-1.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "059121757425010cfff104203a0ddda1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 78779,
      "upload_time": "2022-06-15T10:44:07",
      "upload_time_iso_8601": "2022-06-15T10:44:07.869875Z",
      "url": "https://files.pythonhosted.org/packages/2c/b4/80e7b83ff05ce2b9c0aca150b7e3453aaee2c198eee26d13380e515fd4e1/mlserver-1.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f5b98f2f8cac69f59d0a91fac59065483437b7201413f929d591887423449a19",
        "md5": "86a73e435179a5ecf5e33e45b91a3483",
        "sha256": "132755317bae119e9020b87783616ca46a532c710dad7016f0bfbc44b1002cd2"
      },
      "downloads": -1,
      "filename": "mlserver-1.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "86a73e435179a5ecf5e33e45b91a3483",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 60580,
      "upload_time": "2022-06-15T10:44:25",
      "upload_time_iso_8601": "2022-06-15T10:44:25.461050Z",
      "url": "https://files.pythonhosted.org/packages/f5/b9/8f2f8cac69f59d0a91fac59065483437b7201413f929d591887423449a19/mlserver-1.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}