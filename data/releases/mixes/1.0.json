{
  "info": {
    "author": "",
    "author_email": "Andriy Dmytruk <Andr.Dmytruk@gmail.com>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3"
    ],
    "description": "# **Mixes** - repository of mixture models\n\nThis repository was created as part of Research paper \"Estimation of Gaussian Mixture Networks\"\nrequired as part of Master of Mathematics degree in Computational Mathematics at the University of Waterloo authored\nby Andriy Dmytruk and supervised by Ryan Browne.\n\nThe repository includes implementation of the following mixture models:\n* Gaussian Mixture Model ([GMM](mixes/GMM.py))\n* Skew Gaussian Mixture Model ([Skew GMM](mixes/SkewGMM.py))\n* Deep Gaussian Misture Modle ([DGMM](mixes/DGMM.py))\n* Gaussian Mixture Network ([GMN](mixes/GMN.py))\n\n## Usage\n\nThe implementation is present in the [mixes/](mixes/) folder.\nYou can see an example of usage in the [experiments/example.ipynb](experiments/example.ipynb) jupyter notebook.\n\nAll the experiments that were performed as part of the research paper can also be found inside the [experiments/](experiments/) folder.\n\n## Models description\n\n### Skew Gaussian Mixture Model\n\nSkew GMM was implemented based on paper \n[\"Maximum likelihood estimation for multivariate skew normal mixture models\"](https://www.sciencedirect.com/science/article/pii/S0047259X08001152)\nby Tsung I. Lin (2006).\n\n### Gaussian Mixture Network\n\nGMN was proposed in the author's research paper. The model creates a network of gaussian distributions\nwhere next layers in the model have conditional probability distribution based on the previous layer.\nEach layer is a mixture of components, therefore the whole model creates a network of gaussian nodes.\n\nThe most important parameters are: \n* `layer_sizes` - these are the sizes of layers. The first layer will be used for clusterization\nand therefore its size should correspond to the desired number of clusters.\n* `layer_dims` - the input dimensions of each layer. Each layer has an input and output dimension. \nThe output of the first layer is automatically set to the dimensionality of the data, and output of prevous layer\nis considered the input of the next one. By reducing dimensionality of the deeper layers have fewer parameters,\nand each layer becomes similar to Mixtures of Factor Analyzers. You would probably want to set the \ndimensions in a non-increasing order.\n* `init` - determines how the model is initialized. Use `kmeans` (default) for initialization by `K-Means` and \nfactor analysis on each layer. Use `random` for a completely random initialization.\n\n\n\n### Deep Gaussian Mixture Model\n\nDGMM is based on papers [\"Deep Gaussian mixture models\"](https://link.springer.com/article/10.1007/s11222-017-9793-z) \nby Cinzia Viroli, Geoffrey J. McLachlan (2019) and\n[\"Factoring Variations in Natural Images with Deep Gaussian Mixture Models\"](https://proceedings.neurips.cc/paper/2014/hash/8c3039bd5842dca3d944faab91447818-Abstract.html)\nby Aaron van den Oord, Benjamin Schrauwen (2014).\n\nThe parameters are similar to GMN model, as is the implementation in this repository.\n\nThe difference between DGMM and GMN is that GMN gives probabilities to layer's components conditional\non the previous layer, while DGMM has them independent.\n\n### Annealing\n\nWe implemented deterministic annealing for mixture models as described in the paper\n[\"On the Bumpy Road to the Dominant Mode\"](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2009.00681.x?casa_token=ntehyQT23A0AAAAA:pHs1_s24ZAQvg36cwjxJTcAqgH4QW-VHwOq2p-wyHNCNSeymbOR9xEdp30sfbmjI-jxdeqrvaWr6mr8)\nby Hua Zhou, Kenneth L. Lange (2010).\n\nSince the log-likelihood functions is frequently non-concave, the EM algorithm can end up\nin suboptimal modes. The idea of annealing is to flatten the objective function and therefore \nincrease the chances of terminating in a dominant mode. \n\nThe parameter `use_annealing` determines whether to use annealing, while the parameter \n`annealing_start_v` determines the intial value for annealing. The value must be between `0` and `1`.\nLower values correspond to a more flattened objective function, while `1` corresponds to no\nannealing. Starting for the `annealing_start_v`, the annealing value will be increased to `1` during model fitting\nif `use_annealing` is set to true.\n\n### Regularizatoin\n\nGMM, GMN and DGMM models have the variance regularization parameter `var_regularization`. \nRegularization makes the covariances larger on each step. This keeps the covariance matrix from becoming\nclose to singular, which would greatly degrade optimization for it. The parameter can also be used\nfor restricting the model to larger covariances and avoid overfitting.\n\n## Stopping criterion\n\nUse the `stopping_criterion` parameter in models to specify a stopping criterion. Specified function\nmust have the same signature as functions in the [mixes/stopping_criterion.py](mixes/stopping_criterion.py) file.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "mixture models,GMN,DGMM,Skew GMM,GMM",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mixes",
    "package_url": "https://pypi.org/project/mixes/",
    "platform": null,
    "project_url": "https://pypi.org/project/mixes/",
    "project_urls": {
      "Homepage": "https://github.com/KangaroosInAntarcitica/mixes"
    },
    "release_url": "https://pypi.org/project/mixes/1.0/",
    "requires_dist": [
      "scikit-learn (>=0.24.1)",
      "numpy (>=1.20.1)",
      "scipy (>=1.6.2)",
      "pandas (>=1.2.4) ; extra == 'dataframes'",
      "matplotlib (>=3.3.4) ; extra == 'plotting'"
    ],
    "requires_python": ">=3.0",
    "summary": "A package of mixture models including Skew GMM, GMN and DGMM",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14691079,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2ae335a2d41d7436d9dee1a775f9a2587361ea9235ee5b3cb527268f118027d7",
        "md5": "0d54baf23497800dab74ef2bff6fa5df",
        "sha256": "2605dbc23c07eb1c6da657bcaaa33f1a64207440fe9305c7b39cf63ad54b13a0"
      },
      "downloads": -1,
      "filename": "mixes-1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0d54baf23497800dab74ef2bff6fa5df",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.0",
      "size": 24013,
      "upload_time": "2022-08-08T16:10:09",
      "upload_time_iso_8601": "2022-08-08T16:10:09.316248Z",
      "url": "https://files.pythonhosted.org/packages/2a/e3/35a2d41d7436d9dee1a775f9a2587361ea9235ee5b3cb527268f118027d7/mixes-1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "45810b09194f17947e50160ef2123f59c3d5cb75ef6a3bef1cd546d9606b99fc",
        "md5": "1474b936ec1f081f9be1e6639427cce8",
        "sha256": "a374c438693ad31390e34c85b8316bcc5f65953ebeb080dec8a5329a2251b4a0"
      },
      "downloads": -1,
      "filename": "mixes-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "1474b936ec1f081f9be1e6639427cce8",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.0",
      "size": 23048,
      "upload_time": "2022-08-08T16:10:11",
      "upload_time_iso_8601": "2022-08-08T16:10:11.403199Z",
      "url": "https://files.pythonhosted.org/packages/45/81/0b09194f17947e50160ef2123f59c3d5cb75ef6a3bef1cd546d9606b99fc/mixes-1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}