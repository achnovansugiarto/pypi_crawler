{
  "info": {
    "author": "Marton Szemenyei",
    "author_email": "szemenyei@iit.bme.hu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# DynEnv\nDynamic Simulation Environments for Reinforcement Learning\n\nThis project contains two reinforcement learning environments based on 2D physics simulation via [pymunk](https://www.pymunk.org). The environments support different observation modalities and also noisy observations. The current environments are the following:\n\n- **Robot Soccer SPL League (RoboCupEnvironment):** Here, two teams of robots are competing to play soccer.\n- **Autonomous driving environment (DrivingEnvironment):** Here, two teams of cars try to get to their unique destinations as quickly as possible without crashing or hitting pedestrians. The teams are not competing here, but only cars on the same team are allowed to share information (to model human drivers).\n\n## Requirements\n\n- Python 3.6+\n- PyMunk\n- OpenCV\n- PyGame\n- PyTorch (optional)\n\n## Installation\n\nYou can install simply using pip:\n\n`pip install DynEnv`\n\nOr build from source:\n\n```\ngit clone https://github.com/szemenyeim/DynEnv.git\ncd DynEnv\npip install -e .\n```\n\n## Usage\n\nYou can simply use the environments the following way:\n\n```python\nfrom DynEnv import *\n\nmyEnv = RoboCupEnvironment(nPlayers)\nmyEnv = DrivingEnvironment(nPlayers)\n\nret = myEnv.step(actions)\n```\n\nOr create vectorized environments by using:\n\n```python\nmake_dyn_env(env, num_envs, num_players, render, observationType, noiseType, noiseMagnitude, use_continuous_actions)\nenv = DynEnvType.ROBO_CUP or DynEnvType.DRIVE\n```\n\nMore complex examples including \n- neural networks tailored for the special output format (i.e. the number of observations can vary through time),\n- logging and\n- plotting the results.\n\nFor the above, confer the `DynEnv/examples` directory. The `main.py` file consists a full example, while if you would like to try out how the environments work by hand, `play.py` is there for you as well.\n\n### Model structure\nThe most important part from the point of view of the neural network is the `DynEnv/models` directory, which exposes you the following classes:\n- _ICMAgent_: the top-level agent consisting of an A2C and an Intrinsic Curiosity Module (and its variant, [Rational Curiosity Module](https://github.com/rpatrik96/AttA2C))\n- _InOutArranger_: helper class to rearrange observations for simple NN forwarding\n- _EmbedBlock_: the embedding network used for an object\n- _InputLayer_: a complex network which convert all observations into a unified feature space\n- _ActorBlock_: a neural network predicting actions for a given action type\n- _ActorLayer_: an ensemble of _ActorBlock_ to predict every action\n- _AttentionLayer_:\n- _DynEnvFeatureExtractor_: a wrapper for the input transform by _InputLayer_, collapsing the time dimension with Recurrent Temporal Attention and running an LSTM\n\n\n### Parameters\n\nHere are some of the important settings of the environments\n\n- **nPlayers [1-5/10]**: Number of total players in the environment (in the RoboCup env this is per team). The limit is 10 in the Driving, 5 in the RoboCup env.\n- **render [bool]**: Whether to visualize the environment.\n- **observationType [Full, Partial, Image]**: Image observation only supported for the RoboCup environment.\n- **noiseType [Random, Realistic]**: Realistic noise: noise magnitude and false negative rate depends on distance, proximity of other objects and sighting type. False positives and misclassifications are more likely to occur in certain situations.\n- **noiseMagnitude [0-5]**: Variable to control noise\n- **continuousActions [bool]**: Whether the driving env actions are understood as categorical or continuous. (Driving env only)\n- **allowHeadturn [bool]**: Enables head turining actions. (RoboCup env only)\n\nHere are some examples of different noise and observation types\n\n#### Random Noise\n\nFull Observation            |  Partial Observation\n:-------------------------:|:-------------------------:\n![](/images/randNoise/game.gif)  |  ![](/images/randNoise/obs.gif)\n\nTop Camera            |  Bottom Camera\n:-------------------------:|:-------------------------:\n![](/images/randNoise/top.gif)  |  ![](/images/randNoise/bottom.gif)\n\n\n#### Realistic Noise\n\n\nFull Observation            |  Partial Observation\n:-------------------------:|:-------------------------:\n![](/images/realNoise/game.gif)  |  ![](/images/realNoise/obs.gif)\n\nTop Camera            |  Bottom Camera\n:-------------------------:|:-------------------------:\n![](/images/realNoise/top.gif)  |  ![](/images/realNoise/bottom.gif)\n\n#### Large, Realistic Noise\n\n\nFull Observation            |  Partial Observation\n:-------------------------:|:-------------------------:\n![](/images/bigNoise/game.gif)  |  ![](/images/bigNoise/obs.gif)\n\nTop Camera            |  Bottom Camera\n:-------------------------:|:-------------------------:\n![](/images/bigNoise/top.gif)  |  ![](/images/bigNoise/bottom.gif)\n\n#### Driving, Realistic Noise\n\n\nFull Observation            |  Partial Observation\n:-------------------------:|:-------------------------:\n![](/images/drive/game.gif)  |  ![](/images/drive/obs.gif)\n\n### Important functions and members\n\n- `reset()` Resets the environment to a new game and returns initial observations.\n- `setRandomSeed(seed)` Sets the environment seed, resets the environment and returns initial observations.\n- `observationSpace` Returns information about the observations returned by the environrment. (**Note: These are not Gym Compatible, see the folowing section**)\n- `actionSpace` Returns information about the actions the environment expects.\n- `step(actions)` Performs one step. This consists of several simulation steps (10 for the Driving and 50 for the RoboCup environments). It returns observations for every 10 simulation steps and full state for the last step.\n- `renderMode` Whether to render to a display (`'human'`) or to a memory array (`'memory'`).\n- `agentVisID` With this, you can visualize the observation of an agent during rendering.\n- `render()` Returns rendered images if the render mode is `'memory'`. Does nothing otherwise, as the rendering is done by the step function due to the multi-timestep feature.\n\n### So, what are the actions?\n\nThe environments expect an iterable object containing the actions for every player. Each player action must contain the following:\n\n#### RoboCup:\n- **Movement direction:** Categorical (5)\n- **Turn:** Categorical (3)\n- **Turn head:** Continuous [-6 +6]\n- **Kick:** Categorical (3) (this is exclusive with moving or turning)\n\n#### Driving:\n- **Gas/break:** Continuous [-3 +3] or Categorical (3)\n- **Turn:** Continuous [-3 +3] or Categorical (3)\n\n### What is returned?\n\nBoth environments return the following variables in the step function:\n\n- **Observations:** Observations for every agent. What this is exactly depends on the observationType variable.\n- **Rewards:** Rewards for each agent.\n  - **Team rewards:** Shared rewards for every team. These are added to the agent reward variables, and are not returned.\n- **Finished:** Game over flag\n- **Info:** Other important data\n  - **Full State:** The full state of the env\n  - **episode_r**: Cumulative rewards for the episode (Returned only at the end of an episode)\n  - **episode_p_r**: Cumulative positive-only rewards for the episode (Returned only at the end of an episode)\n  - **episode_g**: Goals in these episode (RoboCup env only) (Returned only at the end of an episode)\n\nPosition information is normalized in both the observations and the full state.\n\n#### The Observation Space\n\nDue to limitations in the OpenAI gym, this part of the env is not fully compatible. The `observationSpace` variable is a list containing the following variables:\n\n`[nTimeSteps, nAgents, nObjectType, [<number of features for every object type>]]`\n\nThe observations returned are arranged as follows:\n\n`[nParallelEnvs x nTimeSteps x nAgents x nObjectType]`\n\nEach element of the above list is a NumPy array containing all the observations by a single agent in a single timestep. To help contructing input layers a custom class `DynEnv.models.InOutArranger` is provided with the following two functions:\n\n- `inputs, counts = rearrange_inputs(x)`: Creates a single list of NumPy arrays. Each element of this list contains a single numpy array of all the observations for a given object type. (Warning: in some cases this might be an empty list!)\n- `outputs, masks = rearrange_outputs(inputs, counts)`: Takes a list of Torch Tensors and the counts output by the previous function, and creates a single tensor shaped [TimeSteps x maxObjCnt x nPlayers x featureCnt] by padding the second dimension to the largest number of objects seen for every robot. The masks variable is binary array shaped [TimeSteps x maxObjCnt x nPlayers], which is True for padded elements (this is in line with PyTorch's MultiHeadedAttention layer). (Warning: This assumes that the featureCnt is the same for every object time.)\n\nHere is a more comprehensive example:\n\n```python\nfrom DynEnv.models import *\nfrom torch import nn\n\nmyEnv = ...\nobsSpace = myEnv.observationSpace\nnTime = obsSpace[0]\nnPlayers = obsSpace[1]\nnObj = obsSpace[2]\nfeaturesPerObject = obsSpace[3]\n\ndevice = <CUDA or CPU>\nmyNeuralNets = [nn.Linear(objfeat,128).to(device) for objFeat in featuresPerObject]\nmyArranger = models.InOutArranger(nObj,nPlayers,nTime)\n\n...\n\nobs, _ = myEnv.step(actions)\n\nnetInputs, counts = myArranger.rearrange_inputs(obs)\nnetOutputs = [myNet(torch.tensor(netInput).to(device)) for myNet,netInput in zip(myNeuralNets,netInputs)]\noutputs,masks = myArranger.rearrange_outputs(netOutputs,counts)\n\n```\n\n#### RoboCup\n\nThe full state contains the following:\n\n- Robots **[x, y, cos(angle), sin(angle), team, fallen or penalized]**\n- Balls **[x, y, ball owned team ID]**\n\nIf the observation is full state, the robot's own position is returned in a separate list, and both axes are flipped and angles rotated 180 degrees for team -1. Moreover, in this case the ball owned flag indicates whether the ball is owned by the robot's team, or the opponent.\n\nThe partial observation contains the following for each robot:\n\n- Balls: **[x, y, radius, ball owned status]**\n- Robots (self not included): **[x, y, radius, cos(angle), sin(angle), team, fallen or penalized]**\n- Goalposts: **[x, y, radius]**\n- Crosses: **[x, y, radius]**\n- Lines: **[x1, y1, x2, y2]**\n- Center circle: **[x, y, radius]**\n\nsigthingType can be Normal, Distant or Partial. In this case, the positions and angles are returned relative to the robot's position and head angle.\n\nThe image observations contain 2D images of semantic labels. The images have 4 binary channels:\n\n- 0: Ball\n- 1: Robot\n- 2: Goalpost\n- 3: Line\n\n#### Driving\n\nThe full state contains the following:\n\n- Cars: **[x, y, cos(angle), sin(angle), width, height, finished]**\n- Obstacles: **[x, y, cos(angle), sin(angle), width, height]**\n- Pedestrians: **[x, y]**\n- Lanes: **[x1, y1, x2, y2, type]**\n\nIf the observation is full state, the car's own position is returned in a separate list, identical to the Self entry below.\n\nThe partial observation contains the following for each car:\n\n- Self: **[x, y, cos(angle), sin(angle), width, height, goal_x, goal_y, finished]**\n- Cars: **[x, y, cos(angle), sin(angle), width, height]**\n- Obstacles: **[x, y, cos(angle), sin(angle), width, height]**\n- Pedestrians: **[x, y]**\n- Lanes: **[signed distance, cos(angle), sin(angle), type]**\n\nWidths and heights are also normalized.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/szemenyeim/DynEnv",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "DynEnv",
    "package_url": "https://pypi.org/project/DynEnv/",
    "platform": "",
    "project_url": "https://pypi.org/project/DynEnv/",
    "project_urls": {
      "Homepage": "https://github.com/szemenyeim/DynEnv"
    },
    "release_url": "https://pypi.org/project/DynEnv/1.0/",
    "requires_dist": [
      "opencv-python",
      "numpy",
      "pymunk",
      "pygame",
      "gym",
      "stable-baselines",
      "torch",
      "pandas",
      "matplotlib"
    ],
    "requires_python": ">=3.5",
    "summary": "Dynamic RL Environments for Autonomous Driving and Robot Soccer",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7396464,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d0e0dbee564834e86b0182400e1166cfef102b4fe15ec0024078d7bd3552c96e",
        "md5": "438276bcce29f510bfec3d39e0610029",
        "sha256": "ffd754f57fdd3b8760f6780b2a38c61dfb607d5fbe1da7ff85059708ad8109ac"
      },
      "downloads": -1,
      "filename": "DynEnv-1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "438276bcce29f510bfec3d39e0610029",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5",
      "size": 98294,
      "upload_time": "2019-11-30T11:19:04",
      "upload_time_iso_8601": "2019-11-30T11:19:04.762834Z",
      "url": "https://files.pythonhosted.org/packages/d0/e0/dbee564834e86b0182400e1166cfef102b4fe15ec0024078d7bd3552c96e/DynEnv-1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9cece08c6b2895234c209be2c1f0415106974c80ccc49931bb42bf4a7e94dd5d",
        "md5": "f8b68cf3acee4b088da8d29f26768801",
        "sha256": "af1bce1f34d2caaecbabaaad9e87f06eb9e8f0029feaaa589e6d3e6bfd4d04c6"
      },
      "downloads": -1,
      "filename": "DynEnv-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "f8b68cf3acee4b088da8d29f26768801",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 59828,
      "upload_time": "2019-11-30T11:19:07",
      "upload_time_iso_8601": "2019-11-30T11:19:07.505971Z",
      "url": "https://files.pythonhosted.org/packages/9c/ec/e08c6b2895234c209be2c1f0415106974c80ccc49931bb42bf4a7e94dd5d/DynEnv-1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}