{
  "info": {
    "author": "Marton Szemenyei",
    "author_email": "szemenyei@iit.bme.hu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# DynEnv\nDynamic Simulation Environments for Reinforcement Learning\n\nThis project contains two reinforcement learning environments based on 2D physics simulation via [pymunk](https://www.pymunk.org). The environments support different observation modalities and also noisy observations. The current environments are the following:\n\n- **Robot Soccer SPL League (RoboCupEnvironment):** Here, two teams of robots are competing to play soccer.\n- **Autonomous driving environment (DrivingEnvironment):** Here, two teams of cars try to get to their unique destinations as quickly as possible without crashing or hitting pedestrians. The teams are not competing here, but only cars on the same team are allowed to share information (to model human drivers).\n\n\n## Table of contents\n* [Requirements](#requirements)\n* [Installation](#installation)\n* [Usage](#usage)\n    * [Model structure](#model-structure)\n    * [Parameters](#parameters)\n    * [Important functions and members](#important-functions-and-members)\n    * [So, what are the actions?](#so-what-are-the-actions)\n    * [What is returned?](#what-is-returned)\n\n\n## Requirements\n\n- Python 3.6+\n- PyMunk\n- OpenCV\n- PyGame\n- PyTorch (optional)\n\n## Installation\n\nYou can install simply using pip:\n\n`pip install DynEnv`\n\nOr build from source:\n\n```\ngit clone https://github.com/szemenyeim/DynEnv.git\ncd DynEnv\npip install -e .\n```\n\n## Usage\n\nYou can simply use the environments the following way:\n\n```python\nfrom DynEnv import *\n\nmyEnv = RoboCupEnvironment(nPlayers)\nmyEnv = DrivingEnvironment(nPlayers)\n\nret = myEnv.step(actions)\n```\n\nOr create vectorized environments by using:\n\n```python\nmake_dyn_env(env, num_envs, num_players, render, observationType, noiseType, noiseMagnitude, use_continuous_actions)\nenv = DynEnvType.ROBO_CUP or DynEnvType.DRIVE\n```\n\nMore complex examples including \n- neural networks tailored for the special output format (i.e. the number of observations can vary through time),\n- logging and\n- plotting the results.\n\nFor the above, confer the `DynEnv/examples` directory. The `main.py` file consists a full example, while if you would like to try out how the environments work by hand, `play.py` is there for you as well.\n\n### Model structure\nThe most important part from the point of view of the neural network is the `DynEnv/models` directory, which exposes you the following classes:\n- _ICMAgent_: the top-level agent consisting of an A2C and an Intrinsic Curiosity Module (and its variant, [Rational Curiosity Module](https://github.com/rpatrik96/AttA2C))\n- _InOutArranger_: helper class to rearrange observations for simple NN forwarding\n- _EmbedBlock_: the embedding network used for an object\n- _InputLayer_: a complex network which convert all observations into a unified feature space\n- _ActorBlock_: a neural network predicting actions for a given action type\n- _ActorLayer_: an ensemble of _ActorBlock_ to predict every action\n- _AttentionLayer_:\n- _DynEnvFeatureExtractor_: a wrapper for the input transform by _InputLayer_, collapsing the time dimension with Recurrent Temporal Attention and running an LSTM\n\n\n### Parameters\n\nHere are some of the important settings of the environments\n\n- **nPlayers [1-5/10]**: Number of total players in the environment (in the RoboCup env this is per team). The limit is 10 in the Driving, 5 in the RoboCup env.\n- **render [bool]**: Whether to visualize the environment.\n- **observationType [Full, Partial, Image]**: Image observation only supported for the RoboCup environment.\n- **noiseType [Random, Realistic]**: Realistic noise: noise magnitude and false negative rate depends on distance, proximity of other objects and sighting type. False positives and misclassifications are more likely to occur in certain situations.\n- **noiseMagnitude [0-5]**: Variable to control noise\n- **continuousActions [bool]**: Whether the driving env actions are understood as categorical or continuous. (Driving env only)\n- **allowHeadturn [bool]**: Enables head turining actions. (RoboCup env only)\n\nHere are some examples of different noise and observation types\n\n#### Random Noise\n\nFull Observation            |  Partial Observation\n:-------------------------:|:-------------------------:\n![](/images/randNoise/game.gif)  |  ![](/images/randNoise/obs.gif)\n\nTop Camera            |  Bottom Camera\n:-------------------------:|:-------------------------:\n![](/images/randNoise/top.gif)  |  ![](/images/randNoise/bottom.gif)\n\n\n#### Realistic Noise\n\n\nFull Observation            |  Partial Observation\n:-------------------------:|:-------------------------:\n![](/images/realNoise/game.gif)  |  ![](/images/realNoise/obs.gif)\n\nTop Camera            |  Bottom Camera\n:-------------------------:|:-------------------------:\n![](/images/realNoise/top.gif)  |  ![](/images/realNoise/bottom.gif)\n\n#### Large, Realistic Noise\n\n\nFull Observation            |  Partial Observation\n:-------------------------:|:-------------------------:\n![](/images/bigNoise/game.gif)  |  ![](/images/bigNoise/obs.gif)\n\nTop Camera            |  Bottom Camera\n:-------------------------:|:-------------------------:\n![](/images/bigNoise/top.gif)  |  ![](/images/bigNoise/bottom.gif)\n\n#### Driving, Realistic Noise\n\n\nFull Observation            |  Partial Observation\n:-------------------------:|:-------------------------:\n![](/images/drive/game.gif)  |  ![](/images/drive/obs.gif)\n\n### Important functions and members\n\n- `reset()` Resets the environment to a new game and returns initial observations.\n- `setRandomSeed(seed)` Sets the environment seed, resets the environment and returns initial observations.\n- `observation_space` Returns information about the observations returned by the environrment.\n- `action_space` Returns information about the actions the environment expects.\n- `step(actions)` Performs one step. This consists of several simulation steps (10 for the Driving and 50 for the RoboCup environments). It returns observations for every 10 simulation steps and full state for the last step.\n- `renderMode` Whether to render to a display (`'human'`) or to a memory array (`'memory'`).\n- `agentVisID` With this, you can visualize the observation of an agent during rendering.\n- `render()` Returns rendered images if the render mode is `'memory'`. Does nothing otherwise, as the rendering is done by the step function due to the multi-timestep feature.\n\n### So, what are the actions?\n\nThe environments expect an iterable object containing the actions for every player. Each player action must contain the following:\n\n#### RoboCup:\n- **Movement direction:** Categorical (5)\n- **Turn:** Categorical (3)\n- **Turn head:** Continuous [-6 +6]\n- **Kick:** Categorical (3) (this is exclusive with moving or turning)\n\n#### Driving:\n- **Gas/break:** Continuous [-3 +3] or Categorical (3)\n- **Turn:** Continuous [-3 +3] or Categorical (3)\n\n### What is returned?\n\nBoth environments return the following variables in the step function:\n\n- **Observations:** Observations for every agent. What this is exactly depends on the observationType variable.\n- **Rewards:** Rewards for each agent.\n  - **Team rewards:** Shared rewards for every team. These are added to the agent reward variables, and are not returned.\n- **Finished:** Game over flag\n- **Info:** Other important data\n  - **Full State:** The full state of the env\n  - **episode_r**: Cumulative rewards for the episode (Returned only at the end of an episode)\n  - **episode_p_r**: Cumulative positive-only rewards for the episode (Returned only at the end of an episode)\n  - **episode_g**: Goals in these episode. For RoboCup this is goals per team, for the Driving env the first value is the number of cars that reached their destination without crashing, the second is the number of crashed cars. (Returned only at the end of an episode)\n\nPosition information is normalized in both the observations and the full state.\n\n#### The Observation Space\n\nDue to limitations in the OpenAI gym, this part of the environment is not fully compatible. The `observation_space` variable is an instance of `gym.space.Space`, however, the meaning is slightly different.\nI.e. querying the `observation_space` variable and calling `.sample()` on it will get you a fully valid observation format, it does not cover every form of observations an environment can produce. Let us elaborate on that!\n\n##### Gym-like observation space descriptor\n\nDue to the fact that in every time step each agent can see different number of objects (such as cars in the _Driving_ environment), including 0 as a valid number for each object type (not to mention false positive sightings or misclassifications), we cannot give an observation space format which covers all possibilities. However, what we can do is to _assume_ that each object type is present in the observation with a single instance, thus including every necessary information about the object space (but be aware that multiple observations from the same object type can be in the list of observations).\n\nHere is an example for the Driving environment how the observation space looks like (we use extensively the `Dict` gym space, as it enables to describe what is contained, note that prefixes are present as for our network the order in the `featuresPerObject` - see below example - matters):\n\n```python\n...\n# subspace for cars\n car_space = Dict({\n            \"position\": Box(-self.mean * 2, +self.mean * 2, shape=(2,)),\n            \"orientation\": Box(-1, 1, shape=(2,)),\n            \"width_height\": Box(-10, 10, shape=(2,)),\n            \"finished\": MultiBinary(1)\n        })\n...\n\n# assemble observation space\nself.observation_space = Dict({\n                \"0_self\": self_space,\n                \"1_car\": car_space,\n                \"2_obstacle\": obstacle_space,\n                \"3_pedestrian\": pedestrian_space,\n                \"4_lane\": lane_space\n            })\n\n```\n\n ##### List of observations\n\nThe observations returned are arranged as follows:\n\n`[nParallelEnvs x nTimeSteps x nAgents x nObjectType]`\n\nEach element of the above list is a NumPy array containing all the observations by a single agent in a single timestep. To help contructing input layers a custom class `DynEnv.models.InOutArranger` is provided with the following two functions:\n\n- `inputs, counts = rearrange_inputs(x)`: Creates a single list of NumPy arrays. Each element of this list contains a single numpy array of all the observations for a given object type. (Warning: in some cases this might be an empty list!)\n- `outputs, masks = rearrange_outputs(inputs, counts, device)`: Takes a list of Torch Tensors and the counts output by the previous function, and creates a single tensor shaped [TimeSteps x maxObjCnt x nPlayers x featureCnt] by padding the second dimension to the largest number of objects seen for every robot. The masks variable is binary array shaped [TimeSteps x maxObjCnt x nPlayers], which is True for padded elements (this is in line with PyTorch's MultiHeadedAttention layer). (Warning: This assumes that the featureCnt is the same for every object time.)\n\nHere is a more comprehensive example:\n\n```python\nfrom DynEnv.models import *\nfrom torch import nn\n\n# setup environment, query all required variables\nmyEnv = ...\nobsSpace = myEnv.observation_space\nnTime =  5 if env is DynEnvType.ROBO_CUP else 1\nnPlayers = ...\nnObjectTypes = len(obsSpace.spaces.keys())\nfeaturesPerObject = [flatdim(s) for s in obsSpace.spaces.values()]\n\n# create neural network and rearrange inputs\ndevice = <CUDA or CPU>\nmyNeuralNets = [nn.Linear(objfeat,128).to(device) for objFeat in featuresPerObject]\nmyArranger = models.InOutArranger(nObjectTypes,nPlayers,nTime)\n\n...\n# create sample action and step\nactions = torch.stack([action_space.sample() for _ in range(nPlayers)]\nobs, _ = myEnv.step(actions)\n\n# summary\n# rearrange inputs - forward - rearrange outputs\nnetInputs, counts = myArranger.rearrange_inputs(obs)\nnetOutputs = [myNet(torch.tensor(netInput).to(device)) for myNet,netInput in zip(myNeuralNets,netInputs)]\noutputs,masks = myArranger.rearrange_outputs(netOutputs,counts,device)\n\n```\n\n#### RoboCup\n\nThe full state contains the following:\n\n- Robots **[x, y, cos(angle), sin(angle), team ID, fallen or penalized]**\n- Balls **[x, y, ball owned by team ID, closest robot status]**\n\n'Team ID' is +/-1. 'Fallen or penalized' and 'closest robot status' are binary numbers. The latter is 1 for the robot closest to the ball from each team.\n\nIf the observation is full state, the robot's own position is returned in a separate list, and both axes are flipped and angles rotated 180 degrees for team -1. Moreover, in this case the ball owned flag indicates whether the ball is owned by the robot's team, or the opponent.\n\nThe partial observation contains the following for each robot:\n\n- Balls: **[x, y, radius, ball owned status, closest robot status]**\n- Robots (self not included): **[x, y, cos(angle), sin(angle), team, fallen or penalized]**\n- Goalposts: **[x, y, radius]**\n- Crosses: **[x, y, radius]**\n- Lines: **[x1, y1, x2, y2]**\n- Center circle: **[x, y, radius]**\n\nBall owned status is 0 if the ball is not owned, +1 if the ball is owned by the robot's team and -1 if owned by the opposite team.\n\nIn the partial sighting case, the positions and angles are returned relative to the robot's position and head angle.\n\nThe image observations contain 2D images of semantic labels. The images have 4 binary channels:\n\n- 0: Ball\n- 1: Robot\n- 2: Goalpost\n- 3: Line\n\n#### Driving\n\nThe full state contains the following:\n\n- Cars: **[x, y, cos(angle), sin(angle), width, height, finished]**\n- Obstacles: **[x, y, cos(angle), sin(angle), width, height]**\n- Pedestrians: **[x, y]**\n- Lanes: **[x1, y1, x2, y2, type]**\n\nLane type is 0 for standard lanes, 1 for the middle lane and -1 for the edge of the road.\n\nIf the observation is full state, the car's own position is returned in a separate list, identical to the Self entry below.\n\nThe partial observation contains the following for each car:\n\n- Self: **[x, y, cos(angle), sin(angle), width, height, goal_x, goal_y, finished]**\n- Cars: **[x, y, cos(angle), sin(angle), width, height]**\n- Obstacles: **[x, y, cos(angle), sin(angle), width, height]**\n- Pedestrians: **[x, y]**\n- Lanes: **[signed distance, cos(angle), sin(angle), type]**\n\nWidths and heights are also normalized.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/szemenyeim/DynEnv",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "DynEnv",
    "package_url": "https://pypi.org/project/DynEnv/",
    "platform": "",
    "project_url": "https://pypi.org/project/DynEnv/",
    "project_urls": {
      "Homepage": "https://github.com/szemenyeim/DynEnv"
    },
    "release_url": "https://pypi.org/project/DynEnv/1.1/",
    "requires_dist": [
      "opencv-python",
      "numpy",
      "pymunk",
      "pygame",
      "gym",
      "stable-baselines",
      "torch",
      "pandas",
      "matplotlib"
    ],
    "requires_python": ">=3.5",
    "summary": "Dynamic RL Environments for Autonomous Driving and Robot Soccer",
    "version": "1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7396464,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dfff2285c921d1b841d073d8a41030087713c7e663121a70bc16915955ce27b9",
        "md5": "0acf781093c87ea501fccc25484a181a",
        "sha256": "5ff8f3297b58975e85f40f0261c77a73acb4a9966d4d49dbf400c08514c152fa"
      },
      "downloads": -1,
      "filename": "DynEnv-1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0acf781093c87ea501fccc25484a181a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5",
      "size": 87245,
      "upload_time": "2019-12-01T14:11:47",
      "upload_time_iso_8601": "2019-12-01T14:11:47.651166Z",
      "url": "https://files.pythonhosted.org/packages/df/ff/2285c921d1b841d073d8a41030087713c7e663121a70bc16915955ce27b9/DynEnv-1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f4f1ce23f2640daa9cec9465cf1db85172c7c6f6cb248a72b0c71ec4059ac1fa",
        "md5": "a6bcbc806be1f8f63e5b1bd50abf4ae9",
        "sha256": "163753c07aa78495b38dfc916da8f98312d8203082f0742cea51aaab6265b7d5"
      },
      "downloads": -1,
      "filename": "DynEnv-1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "a6bcbc806be1f8f63e5b1bd50abf4ae9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 66169,
      "upload_time": "2019-12-01T14:11:49",
      "upload_time_iso_8601": "2019-12-01T14:11:49.687275Z",
      "url": "https://files.pythonhosted.org/packages/f4/f1/ce23f2640daa9cec9465cf1db85172c7c6f6cb248a72b0c71ec4059ac1fa/DynEnv-1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}