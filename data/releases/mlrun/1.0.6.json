{
  "info": {
    "author": "Yaron Haviv",
    "author_email": "yaronh@iguazio.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "<a id=\"top\"></a>\n[![Build Status](https://github.com/mlrun/mlrun/workflows/CI/badge.svg)](https://github.com/mlrun/mlrun/actions)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![PyPI version fury.io](https://badge.fury.io/py/mlrun.svg)](https://pypi.python.org/pypi/mlrun/)\n[![Documentation](https://readthedocs.org/projects/mlrun/badge/?version=latest)](https://mlrun.readthedocs.io/en/latest/?badge=latest)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n<p align=\"left\"><img src=\"docs/_static/images/MLRun-logo.png\" alt=\"MLRun logo\" width=\"150\"/></p>\n\n*The Open-Source MLOps Orchestration Framework*\n\nMLRun is an open-source MLOps framework that offers an integrative approach to managing your machine-learning pipelines from early development through model development to full pipeline deployment in production.\nMLRun offers a convenient abstraction layer to a wide variety of technology stacks while empowering data engineers and data scientists to define the feature and models.\n\n<a id=\"mlrun-arch\"></a>\n#### The MLRun Architecture <!-- omit in toc -->\n\n<p align=\"left\"><img src=\"docs/_static/images/mlrun-architecture.png\" alt=\"MLRun architecture\" /></p>\n\nMLRun is composed of the following layers:\n\n- **Feature and Artifact Store** &mdash;\n    handles the ingestion, processing, metadata, and storage of data and features across multiple repositories and technologies.\n- **Elastic Serverless Runtimes** &mdash;\n    converts simple code to scalable and managed microservices with workload-specific runtime engines (such as Kubernetes jobs, Nuclio, Dask, Spark, and Horovod).\n- **ML Pipeline Automation** &mdash;\n    automates data preparation, model training and testing, deployment of real-time production pipelines, and end-to-end monitoring.\n- **Central Management** &mdash;\n    provides a unified portal for managing the entire MLOps workflow.\n    The portal includes a UI, a CLI, and an SDK, which are accessible from anywhere.\n\n<a id=\"key-benefits\"></a>\n#### Key Benefits <!-- omit in toc -->\n\nMLRun provides the following key benefits:\n\n- **Rapid deployment** of code to production pipelines\n- **Elastic scaling** of batch and real-time workloads\n- **Feature management** &mdash; ingestion, preparation, and monitoring\n- **Works anywhere** &mdash; your local IDE, multi-cloud, or on-prem\n\n&#x25B6; For more information, see the [MLRun Python package documentation](https://mlrun.readthedocs.io).\n\n#### In This Document <!-- omit in toc -->\n\n- [General Concept and Motivation](#general-concept-and-motivation)\n  - [The Challenge](#the-challenge)\n  - [Why MLRun?](#why-mlrun)\n- [Installation](#installation)\n  - [Installation on the Iguazio Data Science Platform](#installation-on-the-iguazio-data-science-platform)\n- [Examples and Tutorial Notebooks](#examples-and-tutorial-notebooks)\n  - [Additional Examples](#additional-examples)\n- [Quick-Start Tutorial &mdash; Architecture and Usage Guidelines](#quick-start-tutorial--architecture-and-usage-guidelines)\n  - [Basic Components](#basic-components)\n  - [Managed and Portable Execution](#managed-and-portable-execution)\n  - [Automated Parameterization, Artifact Tracking, and Logging](#automated-parameterization-artifact-tracking-and-logging)\n    - [Example](#example)\n  - [Using Hyperparameters for Job Scaling](#using-hyperparameters-for-job-scaling)\n  - [Automated Code Deployment and Containerization](#automated-code-deployment-and-containerization)\n  - [Running an ML Workflow with Kubeflow Pipelines](#running-an-ml-workflow-with-kubeflow-pipelines)\n  - [Viewing Run Data and Performing Database Operations](#viewing-run-data-and-performing-database-operations)\n    - [The MLRun Dashboard](#the-mlrun-dashboard)\n    - [MLRun Database Methods](#mlrun-database-methods)\n  - [Additional Information and Examples](#additional-information-and-examples)\n    - [Replacing Runtime Context Parameters from the CLI](#replacing-runtime-context-parameters-from-the-cli)\n    - [Remote Execution](#remote-execution)\n      - [Nuclio Example](#nuclio-example)\n  - [Running an MLRun Service](#running-an-mlrun-service)\n    - [Using the MLRun CLI to Run an MLRun Service](#using-the-mlrun-cli-to-run-an-mlrun-service)\n\n<a id=\"concepts-n-motivation\"></a>\n## General Concept and Motivation\n\n<a id=\"the-challenge\"></a>\n### The Challenge\n\nAs an ML developer or data scientist, you typically want to write code in your preferred local development environment (IDE) or web notebook, and then run the same code on a larger cluster using scale-out containers or functions.\nWhen you determine that the code is ready, you or someone else need to transfer the code to an automated ML workflow (for example, using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/pipelines-quickstart/)).\nThis pipeline should be secure and include capabilities such as logging and monitoring, as well as allow adjustments to relevant components and easy redeployment.\n\nHowever, the implementation is challenging: various environments (\"runtimes\") use different configurations, parameters, and data sources.\nIn addition, multiple frameworks and platforms are used to focus on different stages of the development life cycle.\nThis leads to constant development and DevOps/MLOps work.\n\nFurthermore, as your project scales, you need greater computation power or GPUs, and you need to access large-scale data sets.\nThis cannot work on laptops.\nYou need a way to seamlessly run your code on a remote cluster and automatically scale it out.\n\n<a id=\"why-mlrun\"></a>\n### Why MLRun?\n\nWhen running ML experiments, you should ideally be able to record and version your code, configuration, outputs, and associated inputs (lineage), so you can easily reproduce and explain your results.\nThe fact that you probably need to use different types of storage (such as files and AWS S3 buckets) and various databases, further complicates the implementation.\n\nWouldn't it be great if you could write the code once, using your preferred development environment and simple \"local\" semantics, and then run it as-is on different platforms?\nImagine a layer that automates the build process, execution, data movement, scaling, versioning, parameterization, outputs tracking, and more.\nA world of easily developed, published, or consumed data or ML \"functions\" that can be used to form complex and large-scale ML pipelines.\n\nIn addition, imagine a marketplace of ML functions that includes both open-source templates and your internally developed functions, to support code reuse across projects and companies and thus further accelerate your work.\n\n<b>This is the goal of MLRun.</b>\n\n> **Note:** The code is in early development stages and is provided as a reference.\n> The hope is to foster wide industry collaboration and make all the resources pluggable, so that developers can code to a single API and use various open-source projects or commercial products.\n\n[Back to top](#top)\n\n<a id=\"installation\"></a>\n## Installation\n\nRun the following command from your Python development environment (such as Jupyter Notebook) to install the MLRun package (`mlrun`), which includes a Python API library and the `mlrun` command-line interface (CLI):\n```python\npip install mlrun\n```\n\nMLRun requires separate containers for the API and the dashboard (UI).\nYou can also select to use the pre-baked JupyterLab image.\n\nTo install and run MLRun locally using Docker or Kubernetes, see the instructions in the [**MLRun documentation**](https://mlrun.readthedocs.io/en/latest/install.html).\n\n<a id=\"installation-iguazio-platform\"></a>\n### Installation on the Iguazio Data Science Platform\n\nMLRun runs as a service on the [Iguazio Data Science Platform](https://www.iguazio.com) (version 2.8 and above) &mdash;\n\nTo access MLRun UI select it from the services screen, consult with Iguazio support for further details.\n\n[Back to top](#top)\n\n<a id=\"examples-n-tutorial-notebooks\"></a>\n## Examples and Tutorial Notebooks\n\nMLRun has many code examples and tutorial Jupyter notebooks with embedded documentation, ranging from examples of basic tasks to full end-to-end use-case applications, including the following; note that some of the examples are found in other mlrun GitHub repositories:\n\n- Learn MLRun basics &mdash; [**examples/mlrun_basics.ipynb**](examples/mlrun_basics.ipynb)\n- Convert local runs to Kubernetes jobs and create automated pipelines &mdash; [**examples/mlrun_jobs.ipynb**](examples/mlrun_jobs.ipynb)\n  - build and end to end pipeline in a single notebook\n  - build custom containers and work with shared files and objects\n  - use model management APIs (log_model, get_model, update_model)\n- End-to-end ML pipeline&mdash; [**demos/scikit-learn**](https://github.com/mlrun/demos/tree/master/scikit-learn-pipeline), including:\n  - Data ingestion and analysis \n  - Model training\n  - Verification\n  - Model deployment\n- MLRun with scale-out runtimes &mdash;\n  - Distributed TensorFlow with Horovod and MPIJob, including data collection and labeling, model training and serving, and implementation of an automated workflow &mdash; [**demos/image-classification-with-distributed-training**](https://github.com/mlrun/demos/tree/master/image-classification-with-distributed-training)\n  - Serverless model serving with Nuclio &mdash; [**examples/xgb_serving.ipynb**](examples/xgb_serving.ipynb)\n  - Dask &mdash; [**examples/mlrun_dask.ipynb**](examples/mlrun_dask.ipynb)\n  - Spark &mdash; [**examples/mlrun_sparkk8s.ipynb**](examples/mlrun_sparkk8s.ipynb)\n- MLRun project and Git life cycle &mdash;\n  - Load a project from a remote Git location and run pipelines &mdash; [**examples/load-project.ipynb**](examples/load-project.ipynb)\n  - Create a new project, functions, and pipelines, and upload to Git &mdash; [**examples/new-project.ipynb**](examples/new-project.ipynb)\n- Import and export functions using different modes &mdash; [**examples/mlrun_export_import.ipynb**](examples/mlrun_export_import.ipynb)\n  - save, auto-document, and upload functions \n  - import functions and run as: module, local-run, and clusterd job\n- Query the MLRun DB &mdash; [**examples/mlrun_db.ipynb**](examples/mlrun_db.ipynb)\n\n<a id=\"additional-examples\"></a>\n### Additional Examples\n\n- Additional end-to-end use-case applications &mdash; [mlrun/demos](https://github.com/mlrun/demos) repo\n- MLRun functions Library &mdash; [mlrun/functions](https://github.com/mlrun/functions) repo \n\n[Back to top](#top)\n\n<a id=\"qs-tutorial\"></a>\n## Quick-Start Tutorial &mdash; Architecture and Usage Guidelines\n<!-- TODO: Move this to a separate docs/quick-start.md file, add an opening\n  paragraph, update the heading levels, add a `top` anchor, and remove the\n  \"Back to quick-start TOC\" links (leaving only the \"Back to top\" links). -->\n\n<a id=\"basic-components\"></a>\n### Basic Components\n\nMLRun has the following main components:\n\n- <a id=\"def-project\"></a>**Project** &mdash; a container for organizing all of your work on a particular activity.\n    Projects consist of metadata, source code, workflows, data and artifacts, models, triggers, and member management for user collaboration.\n\n- <a id=\"def-function\"></a>**Function** &mdash; a software package with one or more methods and runtime-specific attributes (such as image, command, arguments, and environment).\n\n- <a id=\"def-run\"></a>**Run** &mdash; an object that contains information about an executed function.\n    The run object is created as a result of running a function, and contains the function attributes (such as arguments, inputs, and outputs), as well the execution status and results (including links to output artifacts).\n\n- <a id=\"def-artifact\"></a>**Artifact** &mdash; versioned data artifacts (such as data sets, files and models) that are produced or consumed by functions, runs, and workflows.\n\n- <a id=\"def-workflow\"></a>**Workflow** &mdash; defines a functions pipeline or a directed acyclic graph (DAG) to execute using [Kubeflow Pipelines](https://www.kubeflow.org/docs/pipelines/pipelines-quickstart/).\n\n- <a id=\"def-ui\"></a>**UI** &mdash; a graphical user interface (dashboard) for displaying and managing projects and their contained experiments, artifacts, and code.\n\n<a id=\"managed-and-portable-execution\"></a>\n### Managed and Portable Execution\n\n<a id=\"def-runtime\"></a>MLRun supports various types of **\"runtimes\"** &mdash; computation frameworks such as local, Kubernetes job, Dask, Nuclio, Spark, or MPI job (Horovod).\nRuntimes may support parallelism and clustering to distribute the work among multiple workers (processes/containers).\n\nThe following code example creates a task that defines a run specification &mdash; including the run parameters, inputs, and secrets.\nYou run the task on a \"job\" function, and print the result output (in this case, the \"model\" artifact) or watch the run's progress.\nFor more information and examples, see the [**examples/mlrun_basics.ipynb**](examples/mlrun_basics.ipynb) notebook.\n```python\n# Create a task and set its attributes\ntask = NewTask(handler=handler, name='demo', params={'p1': 5})\ntask.with_secrets('file', 'secrets.txt').set_label('type', 'demo')\n\nrun = new_function(command='myfile.py', kind='job').run(task)\nrun.logs(watch=True)\nrun.show()\nprint(run.artifact('model'))\n```\n\nYou can run the same [task](#def-task) on different functions &mdash; enabling code portability, re-use, and AutoML.\nYou can also use the same [function](#def-function) to run different tasks or parameter combinations with minimal coding effort.\n\nMoving from local notebook execution to remote execution &mdash; such as running a container job, a scaled-out framework, or an automated workflow engine like Kubeflow Pipelines &mdash; is seamless: just swap the runtime function or wire functions in a graph.\nContinuous build integration and deployment (CI/CD) steps can also be configured as part of the workflow, using the `deploy_step` function method.\n\nFunctions (function objects) can be created by using any of the following methods:\n\n- **`new_function`** &mdash; creates a function \"from scratch\" or from another function.\n- **`code_to_function`** &mdash; creates a function from local or remote source code or from a web notebook.\n- **`import_function`** &mdash; imports a function from a local or remote YAML function-configuration file or from a function object in the MLRun database (using a DB address of the format `db://<project>/<name>[:<tag>]`).\n- **`function_to_module`** &mdash; import MLRun function or code as a local python module (can also be used inside another parent function) \nYou can use the `save` function method to save a function object in the MLRun database, or the `export` method to save a YAML function-configuration function to your preferred local or remote location.\nFor function-method details and examples, see the embedded documentation/help text.\n\n[Back to top](#top) / [Back to quick-start TOC](#qs-tutorial)\n\n<a id=\"auto-parameterization-artifact-tracking-n-logging\"></a>\n### Automated Parameterization, Artifact Tracking, and Logging\n\nAfter running a job, you need to be able to track it, including viewing the run parameters, inputs, and outputs.\nTo support this, MLRun introduces a concept of a runtime **\"context\"**: the code can be set up to get parameters and inputs from the context, as well as log run outputs, artifacts, tags, and time-series metrics in the context.\n\n<a id=\"auto-parameterization-artifact-tracking-n-logging-example\"></a>\n#### Example\n\nThe following code example from the [**train-xgboost.ipynb**](https://github.com/mlrun/demo-xgb-project/blob/master/notebooks/train-xgboost.ipynb) notebook of the MLRun XGBoost demo (**demo-xgboost**) defines two functions:\nthe `iris_generator` function loads the Iris data set and saves it to the function's context object; the `xgb_train` function uses XGBoost to train an ML model on a data set and saves the log results in the function's context:\n\n```python\nimport xgboost as xgb\nimport os\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom mlrun.artifacts import PlotArtifact\nimport pandas as pd\n\n\ndef iris_generator(context):\n    iris = load_iris()\n    iris_dataset = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n    iris_labels = pd.DataFrame(data=iris.target, columns=['label'])\n    iris_dataset = pd.concat([iris_dataset, iris_labels], axis=1)\n    context.logger.info('Saving Iris data set to \"{}\"'.format(context.out_path))\n    context.log_dataset('iris_dataset', df=iris_dataset)\n\n\ndef xgb_train(context,\n              dataset='',\n              model_name='model.bst',\n              max_depth=6,\n              num_class=10,\n              eta=0.2,\n              gamma=0.1,\n              steps=20):\n\n    df = pd.read_csv(dataset)\n    X = df.drop(['label'], axis=1)\n    y = df['label']\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\n    dtrain = xgb.DMatrix(X_train, label=Y_train)\n    dtest = xgb.DMatrix(X_test, label=Y_test)\n\n    # Get parameters from event\n    param = {\"max_depth\": max_depth,\n             \"eta\": eta, \"nthread\": 4,\n             \"num_class\": num_class,\n             \"gamma\": gamma,\n             \"objective\": \"multi:softprob\"}\n\n    xgb_model = xgb.train(param, dtrain, steps)\n\n    preds = xgb_model.predict(dtest)\n    best_preds = np.asarray([np.argmax(line) for line in preds])\n\n    context.log_result('accuracy', float(accuracy_score(Y_test, best_preds)))\n    context.log_model('model', body=bytes(xgb_model.save_raw()), \n                      model_file='model.txt', \n                      metrics=context.results, parameters={'xx':'abc'},\n                      labels={'framework': 'xgboost'},\n                      artifact_path=context.artifact_subpath('models'))\n```\n\nThe example training function can be executed locally with parameters, and the run results and artifacts can be logged automatically into a database by using a single command, as demonstrated in the following example; the example sets the function's `eta` parameter:\n```python\ntrain_run = run_local(handler=xgb_train, pramas={'eta': 0.3})\n```\n\nAlternatively, you can replace the function with a serverless runtime to run the same code on a remote cluster, which could result in a ~10x performance boost.\nYou can find examples for different runtimes &mdash; such as a Kubernetes job, Nuclio, Dask, Spark, or an MPI job &mdash; in the MLRun [**examples**](examples) directory.\n\nIf you run your code from the `main` function, you can get the runtime context by calling the `get_or_create_ctx` method, as demonstrated in the following code from the MLRun [**training.py**](examples/training.py) example application.\nThe code also demonstrates how you can use the context object to read and write execution metadata, parameters, secrets, inputs, and outputs:\n\n```python\nfrom mlrun import get_or_create_ctx\nfrom mlrun.artifacts import ChartArtifact\nimport pandas as pd\n\n\ndef my_job(context, p1=1, p2='x'):\n    # load MLRUN runtime context (will be set by the runtime framework e.g. KubeFlow)\n\n    # get parameters from the runtime context (or use defaults)\n\n    # access input metadata, values, files, and secrets (passwords)\n    print(f'Run: {context.name} (uid={context.uid})')\n    print(f'Params: p1={p1}, p2={p2}')\n    print('accesskey = {}'.format(context.get_secret('ACCESS_KEY')))\n    print('file\\n{}\\n'.format(context.get_input('infile.txt', 'infile.txt').get()))\n\n    # Run some useful code e.g. ML training, data prep, etc.\n\n    # log scalar result values (job result metrics)\n    context.log_result('accuracy', p1 * 2)\n    context.log_result('loss', p1 * 3)\n    context.set_label('framework', 'sklearn')\n\n    # log various types of artifacts (file, web page, table), will be versioned and visible in the UI\n    context.log_artifact('model', body=b'abc is 123', local_path='model.txt', labels={'framework': 'xgboost'})\n    context.log_artifact('html_result', body=b'<b> Some HTML <b>', local_path='result.html')\n\n    # create a chart output (will show in the pipelines UI)\n    chart = ChartArtifact('chart')\n    chart.labels = {'type': 'roc'}\n    chart.header = ['Epoch', 'Accuracy', 'Loss']\n    for i in range(1, 8):\n        chart.add_row([i, i/20+0.75, 0.30-i/20])\n    context.log_artifact(chart)\n\n    raw_data = {'first_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n                'last_name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze'],\n                'age': [42, 52, 36, 24, 73],\n                'testScore': [25, 94, 57, 62, 70]}\n    df = pd.DataFrame(raw_data, columns=[\n        'first_name', 'last_name', 'age', 'testScore'])\n    context.log_dataset('mydf', df=df, stats=True)\n\n\nif __name__ == \"__main__\":\n    context = get_or_create_ctx('train')\n    p1 = context.get_param('p1', 1)\n    p2 = context.get_param('p2', 'a-string')\n    my_job(context, p1, p2)\n```\n\nThe example **training.py** application can be invoked as a local task, as demonstrated in the following code from the MLRun [**mlrun_basics.ipynb**](examples/mlrun_basics.ipynb) example notebook:\n```python\nrun = run_local(task, command='training.py')\n```\nAlternatively, you can invoke the application by using the `mlrun` CLI; edit the parameters, inputs, and/or secret information, as needed, and ensure that **training.py** is found in the execution path or edit the file path in the command:\n```sh\nmlrun run --name train -p p2=5 -i infile.txt=s3://my-bucket/infile.txt -s file=secrets.txt training.py\n```\n\n[Back to top](#top) / [Back to quick-start TOC](#qs-tutorial)\n\n<a id=\"using-hyperparameters-for-job-scaling\"></a>\n### Using Hyperparameters for Job Scaling\n\nData science involves long computation times and data-intensive tasks.\nTo ensure efficiency and scalability, you need to implement parallelism whenever possible.\nMLRun supports this by using two mechanisms:\n\n1. Clustering &mdash; run the code on a distributed processing engine (such as Dask, Spark, or Horovod).\n2. Load-balancing/partitioning &mdash; split (partition) the work across multiple workers.\n\nMLRun functions and tasks can accept hyperparameters or parameter lists, deploy many parallel workers, and partition the work among the deployed workers.\nThe parallelism implementation is left to the runtime.\nEach runtime may have its own method of concurrent tasks execution.\nFor example, the Nuclio serverless engine manages many micro threads in the same process, which can run multiple tasks in parallel.\nIn a containerized system like Kubernetes, you can launch multiple containers, each processing a different task.\n\nMLRun supports parallelism.\nFor example, the following code demonstrates how to use hyperparameters to run the XGBoost model-training task from the example in the previous section (`xgb_train`) with different parameter combinations:\n```python\n    parameters = {\n         \"eta\":       [0.05, 0.10, 0.20, 0.30],\n         \"max_depth\": [3, 4, 5, 6, 8, 10],\n         \"gamma\":     [0.0, 0.1, 0.2, 0.3],\n         }\n\n    task = NewTask(handler=xgb_train, out_path='/User/mlrun/data').with_hyper_params(parameters, 'max.accuracy')\n    run = run_local(task)\n```\n\nThis code demonstrates how to instruct MLRun to run the same task while choosing the parameters from multiple lists (grid search).\nMLRun then records all the runs, but marks only the run with minimal loss as the selected result.\nFor parallelism, it would be better to use runtimes like Dask, Nuclio, or jobs.\n\nAlternatively, you can run a similar task (with hyperparameters) by using the MLRun CLI (`mlrun`); ensure that **training.py** is found in the execution path or edit the file path in the command:\n```sh\nmlrun run --name train_hyper -x p1=\"[3,7,5]\" -x p2=\"[5,2,9]\" --out-path '/User/mlrun/data' training.py\n```\n\nYou can also use a parameters file if you want to control the parameter combinations or if the parameters are more complex.\nThe following code from the example [**mlrun_basics.ipynb**](examples/mlrun_basics.ipynb) notebook demonstrates how to run a task that uses a CSV parameters file (**params.csv** in the current directory):\n```python\n    task = NewTask(handler=xgb_train).with_param_file('params.csv', 'max.accuracy')\n    run = run_local(task)\n```\n\n> **Note:** Parameter lists can be used in various ways.\n> For example, you can pass multiple parameter files and use multiple workers to process the files simultaneously instead of one at a time.\n\n[Back to top](#top) / [Back to quick-start TOC](#qs-tutorial)\n\n<a id=\"auto-code-deployment-n-containerization\"></a>\n### Automated Code Deployment and Containerization\n\nMLRun adopts Nuclio serverless technologies for automatically packaging code and building containers.\nThis enables you to provide code with some package requirements and let MLRun build and deploy your software.\n\nTo build or deploy a function, all you need is to call the function's `deploy` method, which initiates a build or deployment job.\nDeployment jobs can be incorporated in pipelines just like regular jobs (using the `deploy_step` method of the function or Kubernetes-job runtime), thus enabling full automation and CI/CD.\n\nA functions can be built from source code or from a function specification, web notebook, Git repo, or TAR archive.\n\nA function can also be built by using the `mlrun` CLI and providing it with the path to a YAML function-configuration file.\nYou can generate such a file by using the `to_yaml` or `export` function method.\nFor example, the following CLI code builds a function from a **function.yaml** file in the current directory:\n```sh\nmlrun build function.yaml\n```\nFollowing is an example **function.yaml** configuration file:\n```yaml\nkind: job\nmetadata:\n  name: remote-git-test\n  project: default\n  tag: latest\nspec:\n  command: 'myfunc.py'\n  args: []\n  image_pull_policy: Always\n  build:\n    commands: ['pip install pandas']\n    base_image: mlrun/mlrun:dev\n    source: git://github.com/mlrun/ci-demo.git\n```\n\nFor more examples of building and running functions remotely using the MLRun CLI, see the [**remote**](examples/remote.md) example.\n\nYou can also convert your web notebook to a containerized job, as demonstrated in the following sample code; for a similar example with more details, see the [**mlrun_jobs.ipynb**](examples/mlrun_jobs.ipynb) example:\n\n```python\n# Create an ML function from the notebook code and annotations, and attach a\n# v3io Iguazio Data Science Platform data volume to the function\nfn = code_to_function(kind='job').apply(mount_v3io())\n\n# Prepare an image from the dependencies to allow updating the code and\n# parameters per run without the need to build a new image\nfn.build(image='mlrun/nuctest:latest')\n```\n\n[Back to top](#top)\n\n<a id=\"run-ml-workflow-w-kubeflow-pipelines\"></a>\n### Running an ML Workflow with Kubeflow Pipelines\n\nML pipeline execution with MLRun is similar to CLI execution.\nA pipeline is created by running an MLRun workflow.\nMLRun automatically saves outputs and artifacts in a way that is visible to [Kubeflow Pipelines](https://github.com/kubeflow/pipelines), and allows interconnecting steps.\n\nFor an example of a full ML pipeline that's implemented in a web notebook, see the Sklearn MLRun demo ([**demos/scikit-learn**](https://github.com/mlrun/demos/tree/master/scikit-learn-pipeline)).\nThe  [**sklearn-project.ipynb**](https://github.com/mlrun/demos/blob/master/scikit-learn-pipeline/sklearn-project.ipynb) demo notebook includes the following code for implementing an ML-training pipeline:\n```python\nfrom kfp import dsl\nfrom mlrun import mount_v3io\n\nfuncs = {}\nDATASET = 'iris_dataset'\nLABELS  = \"label\"\n\ndef init_functions(functions: dict, project=None, secrets=None):\n    for f in functions.values():\n        f.apply(mount_v3io())\n        f.spec.image_pull_policy = 'Always'\n\n@dsl.pipeline(\n    name=\"My XGBoost training pipeline\",\n    description=\"Shows how to use mlrun.\"\n)\ndef kfpipeline():\n\n    # build our ingestion function (container image)\n    builder = funcs['gen-iris'].deploy_step(skip_deployed=True)\n\n    # run the ingestion function with the new image and params\n    ingest = funcs['gen-iris'].as_step(\n        name=\"get-data\",\n        handler='iris_generator',\n        image=builder.outputs['image'],\n        params={'format': 'pq'},\n        outputs=[DATASET])\n\n    # analyze our dataset\n    describe = funcs[\"describe\"].as_step(\n        name=\"summary\",\n        params={\"label_column\": LABELS},\n        inputs={\"table\": ingest.outputs[DATASET]})\n\n    # train with hyper-paremeters \n    train = funcs[\"train\"].as_step(\n        name=\"train-skrf\",\n        params={\"model_pkg_class\" : \"sklearn.ensemble.RandomForestClassifier\",\n                \"sample\"          : -1, \n                \"label_column\"    : LABELS,\n                \"test_size\"       : 0.10},\n        hyperparams={'CLASS_n_estimators': [100, 300, 500]},\n        selector='max.accuracy',\n        inputs={\"dataset\"         : ingest.outputs[DATASET]},\n        outputs=['model', 'test_set'])\n\n    # test and visualize our model\n    test = funcs[\"test\"].as_step(\n        name=\"test\",\n        params={\"label_column\": LABELS},\n        inputs={\"models_path\" : train.outputs['model'],\n                \"test_set\"    : train.outputs['test_set']})\n\n    # deploy our model as a serverless function\n    deploy = funcs[\"serving\"].deploy_step(models={f\"{DATASET}_v1\": train.outputs['model']})\n```\n\n[Back to top](#top) / [Back to quick-start TOC](#qs-tutorial)\n\n<a id=\"db-operations\"></a>\n### Viewing Run Data and Performing Database Operations\n\nWhen you configure an MLRun database, the results, parameters, and input and output artifacts of each run are recorded in the database.\nYou can view the results and perform operations on the database by using either of the following methods:\n\n- Using [the MLRun dashboard](#mlrun-ui)\n- Using [DB methods](#mlrun-db-methods) from your code\n\n[Back to top](#top) / [Back to quick-start TOC](#qs-tutorial)\n\n<a id=\"mlrun-ui\"></a>\n#### The MLRun Dashboard\n\nThe MLRun dashboard is a graphical user interface (GUI) for working with MLRun and viewing run data.\n\n<br><p align=\"center\"><img src=\"docs/_static/images/mlrunui.png\" width=\"800\"/></p><br>\n\n[Back to top](#top) / [Back to quick-start TOC](#qs-tutorial)\n\n<a id=\"mlrun-db-methods\"></a>\n#### MLRun Database Methods\n\nYou can use the `get_run_db` DB method to get an MLRun DB object for a configured MLRun database or API service.\nThen, use the DB object's `connect` method to connect to the database or API service, and use additional methods to perform different operations, such as listing run artifacts or deleting completed runs.\nFor more information and examples, see the [**mlrun_db.ipynb**](examples/mlrun_db.ipynb) example notebook, which includes the following sample DB method calls:\n```python\nfrom mlrun import get_run_db\n\n# Get an MLRun DB object and connect to an MLRun database/API service.\n# Specify the DB path (for example, './' for the current directory) or\n# the API URL ('http://mlrun-api:8080' for the default configuration).\ndb = get_run_db('./')\n\n# List all runs\ndb.list_runs('').show()\n\n# List all artifacts for version 'latest' (default)\ndb.list_artifacts('', tag='').show()\n\n# Check different artifact versions\ndb.list_artifacts('ch', tag='*').show()\n\n# Delete completed runs\ndb.del_runs(state='completed')\n```\n\n[Back to top](#top) / [Back to quick-start TOC](#qs-tutorial)\n\n<a id=\"additional-info-n-examples\"></a>\n### Additional Information and Examples\n\n- [Replacing Runtime Context Parameters from the CLI](#replace-runtime-context-param-from-cli)\n- [Remote Execution](#remote-execution)\n  - [Nuclio Example](#remote-execution-nuclio-example)\n- [Running the MLRun Database/API Service](#run-mlrun-db-service)\n\n<a id=\"replace-runtime-context-param-from-cli\"></a>\n#### Replacing Runtime Context Parameters from the CLI\n\nYou can use the MLRun CLI (`mlrun`) to run MLRun functions or code and change the parameter values.\n\nFor example, the following CLI command runs the example XGBoost training code from the previous tutorial examples:\n```sh\npython -m mlrun run -p p1=5 -s file=secrets.txt -i infile.txt=s3://mybucket/infile.txt training.py\n```\n\nWhen running this sample command, the CLI executes the code in the **training.py** application using the provided run information:\n- The value of parameter `p1` is set to `5`, overwriting the current parameter value in the run context.\n- The file **infile.txt** is downloaded from a remote \"mybucket\" AWS S3 bucket.\n- The credentials for the S3 download are retrieved from a **secrets.txt** file in the current directory.\n\n<a id=\"remote-execution\"></a>\n#### Remote Execution\n\nYou can also run the same MLRun code that you ran locally as a remote HTTP endpoint.\n\n<a id=\"remote-execution-nuclio-example\"></a>\n##### Nuclio Example\n\nFor example, you can wrap the XGBoost training code from the previous tutorial examples within a serverless [Nuclio](https://nuclio.io) handler function, and execute the code remotely using a similar CLI command to the one that you used locally.\n\nYou can run the following code from a Jupyter Notebook to create a Nuclio function from the notebook code and annotations, and deploy the function to a remote cluster.\n\n> **Note:**\n> - Before running the code, install the [`nuclio-jupyter`](https://github.com/nuclio/nuclio-jupyter) package for using Nuclio from Jupyter Notebook.\n> - The example uses `apply(mount_v3io()`to attach a v3io Iguazio Data Science Platform data-store volume to the function.\n>   By default, the v3io mount mounts the home directory of the platform's running user into the `\\\\User` function path.\n\n```python\n# Create an `xgb_train` Nuclio function from the notebook code and annotations;\n# add a v3io data volume and a multi-worker HTTP trigger for parallel execution\nfn = code_to_function('xgb_train', runtime='nuclio:mlrun')\nfn.apply(mount_v3io()).with_http(workers=32)\n\n# Deploy the function\nrun = fn.run(task, handler='xgb_train')\n```\n\nTo execute the code remotely, run the same CLI command as in the previous tutorial examples and just substitute the code file name at the end with your function's URL.\nFor example, run the following command and replace `<function endpoint>` with your remote function endpoint:\n```sh\nmlrun run -p p1=5 -s file=secrets.txt -i infile.txt=s3://mybucket/infile.txt http://<function-endpoint>\n```\n\n[Back to top](#top) / [Back to quick-start TOC](#qs-tutorial)\n\n<a id=\"run-mlrun-service\"></a>\n### Running an MLRun Service\n\nAn MLRun service is a web service that manages an MLRun database for tracking and logging MLRun run information, and exposes an HTTP API for working with the database and performing MLRun operations.\n\nYou can create and run an MLRun service by using either of the following methods:\n- [Using Docker](#run-mlrun-service-docker)\n- [Using the MLRun CLI](#run-mlrun-service-cli)\n\n> **Note:** For both methods, you can optionally configure the service port and/or directory path by setting the `MLRUN_httpdb__port` and `MLRUN_httpdb__dirpath` environment variables instead of the respective run parameters or CLI options.\n\n<a id=\"run-mlrun-service-cli\"></a>\n#### Using the MLRun CLI to Run an MLRun Service\n\nUse the `db` command of the MLRun CLI (`mlrun`) to create and run an instance of the MLRun service from the command line:\n```sh\nmlrun db [OPTIONS]\n```\n\nTo see the supported options, run `mlrun db --help`:\n```\nOptions:\n  -p, --port INTEGER  HTTP port for serving the API\n  -d, --dirpath TEXT  Path to the MLRun service directory\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/mlrun/mlrun",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlrun",
    "package_url": "https://pypi.org/project/mlrun/",
    "platform": null,
    "project_url": "https://pypi.org/project/mlrun/",
    "project_urls": {
      "Homepage": "https://github.com/mlrun/mlrun"
    },
    "release_url": "https://pypi.org/project/mlrun/1.0.6/",
    "requires_dist": [
      "urllib3 (<1.27,>=1.25.4)",
      "chardet (<4.0,>=3.0.2)",
      "GitPython (~=3.0)",
      "aiohttp (~=3.8)",
      "click (~=8.0.0)",
      "typing-extensions (<5,>=3.10.0)",
      "kfp (~=1.8.0)",
      "nest-asyncio (~=1.0)",
      "ipython (~=7.0)",
      "nuclio-jupyter (~=0.9.1)",
      "numpy (<1.23.0,>=1.16.5)",
      "pandas (~=1.2)",
      "pyarrow (<6,>=1)",
      "pyyaml (~=5.1)",
      "requests (~=2.22)",
      "sqlalchemy (~=1.3)",
      "tabulate (~=0.8.6)",
      "v3io (~=0.5.13)",
      "pydantic (~=1.5)",
      "orjson (~=3.3)",
      "alembic (<1.6.0,~=1.4)",
      "mergedeep (~=1.3)",
      "v3io-frames (~=0.10.2)",
      "semver (~=2.13)",
      "dask (~=2021.11.2)",
      "distributed (~=2021.11.2)",
      "kubernetes (~=12.0)",
      "humanfriendly (~=8.2)",
      "fastapi (~=0.78.0)",
      "fsspec (~=2021.8.1)",
      "v3iofs (~=0.1.7)",
      "cryptography (<3.4,~=3.0)",
      "storey (~=1.0.6)",
      "deepdiff (~=5.0)",
      "pymysql (~=1.0)",
      "inflection (~=0.5.0)",
      "python-dotenv (~=0.17.0)",
      "uvicorn (~=0.17.0) ; extra == 'api'",
      "dask-kubernetes (~=0.11.0) ; extra == 'api'",
      "apscheduler (~=3.6) ; extra == 'api'",
      "sqlite3-to-mysql (~=1.4) ; extra == 'api'",
      "msrest (~=0.6.21) ; extra == 'azure-blob-storage'",
      "azure-core (~=1.23) ; extra == 'azure-blob-storage'",
      "azure-storage-blob (~=12.13) ; extra == 'azure-blob-storage'",
      "adlfs (~=2021.8.1) ; extra == 'azure-blob-storage'",
      "azure-identity (~=1.5) ; extra == 'azure-key-vault'",
      "azure-keyvault-secrets (~=4.2) ; extra == 'azure-key-vault'",
      "bokeh (>=2.4.2,~=2.4) ; extra == 'bokeh'",
      "adlfs (~=2021.8.1) ; extra == 'complete'",
      "aiobotocore (~=1.4.0) ; extra == 'complete'",
      "azure-core (~=1.23) ; extra == 'complete'",
      "azure-identity (~=1.5) ; extra == 'complete'",
      "azure-keyvault-secrets (~=4.2) ; extra == 'complete'",
      "azure-storage-blob (~=12.13) ; extra == 'complete'",
      "boto3 (<1.17.107,~=1.9) ; extra == 'complete'",
      "botocore (<1.20.107,>=1.20.106) ; extra == 'complete'",
      "gcsfs (~=2021.8.1) ; extra == 'complete'",
      "msrest (~=0.6.21) ; extra == 'complete'",
      "plotly (~=5.4) ; extra == 'complete'",
      "s3fs (~=2021.8.1) ; extra == 'complete'",
      "adlfs (~=2021.8.1) ; extra == 'complete-api'",
      "aiobotocore (~=1.4.0) ; extra == 'complete-api'",
      "apscheduler (~=3.6) ; extra == 'complete-api'",
      "azure-core (~=1.23) ; extra == 'complete-api'",
      "azure-identity (~=1.5) ; extra == 'complete-api'",
      "azure-keyvault-secrets (~=4.2) ; extra == 'complete-api'",
      "azure-storage-blob (~=12.13) ; extra == 'complete-api'",
      "boto3 (<1.17.107,~=1.9) ; extra == 'complete-api'",
      "botocore (<1.20.107,>=1.20.106) ; extra == 'complete-api'",
      "dask-kubernetes (~=0.11.0) ; extra == 'complete-api'",
      "gcsfs (~=2021.8.1) ; extra == 'complete-api'",
      "msrest (~=0.6.21) ; extra == 'complete-api'",
      "plotly (~=5.4) ; extra == 'complete-api'",
      "s3fs (~=2021.8.1) ; extra == 'complete-api'",
      "sqlite3-to-mysql (~=1.4) ; extra == 'complete-api'",
      "uvicorn (~=0.17.0) ; extra == 'complete-api'",
      "gcsfs (~=2021.8.1) ; extra == 'google-cloud-storage'",
      "plotly (~=5.4) ; extra == 'plotly'",
      "boto3 (<1.17.107,~=1.9) ; extra == 's3'",
      "botocore (<1.20.107,>=1.20.106) ; extra == 's3'",
      "aiobotocore (~=1.4.0) ; extra == 's3'",
      "s3fs (~=2021.8.1) ; extra == 's3'"
    ],
    "requires_python": "",
    "summary": "Tracking and config of machine learning runs",
    "version": "1.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17396778,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "03c2d59ea58e05a9884fe3e498b9d501d5c930a29f437ac1dabd550882f83396",
        "md5": "2175e0abcf01af6e864e6006f2cb6a5a",
        "sha256": "2a6e30950a80767e5c04edc9f96a6782ba6b44c049c22df469451c3b6374d6ca"
      },
      "downloads": -1,
      "filename": "mlrun-1.0.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "2175e0abcf01af6e864e6006f2cb6a5a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 850157,
      "upload_time": "2022-08-15T08:53:33",
      "upload_time_iso_8601": "2022-08-15T08:53:33.648804Z",
      "url": "https://files.pythonhosted.org/packages/03/c2/d59ea58e05a9884fe3e498b9d501d5c930a29f437ac1dabd550882f83396/mlrun-1.0.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}