{
  "info": {
    "author": "Thomas Meißner",
    "author_email": "meissnercorporation@gmx.de",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# e2e ML\n> An end to end solution for automl. .\n\nPass in your data, add some information about it and get a full pipelines in return. Data preprocessing,\nfeature creation, modelling and evaluation with just a few lines of code.\n\n![](header.png)\n\n## Installation\n\nFrom Pypi:\n\n```sh\npip install e2eml\n```\nWe highly recommend to create a new virtual environment first. Then install e2e-ml into it. In the environment also download\nthe pretrained spacy model with. Otherwise e2eml will do this automatically during runtime.\n```sh\npython3 -m spacy download en\n```\nor\n```sh\npython -m spacy download en\n```\n(depending on your operating system.)\n\n## Usage example\n\ne2e has been designed to create state-of-the-art machine learning pipelines with a few lines of code. Basic example of usage:\n```sh\nimport e2eml\nfrom e2eml.classification import classification_blueprints\nimport pandas as pd\n# import data\ndf = pd.read_csv(\"Your.csv\")\n\n# split into a test/train & holdout set (holdout for prediction illustration here, but not required at all)\ntrain_df = df.head(1000).copy()\nholdout_df = df.tail(200).copy() # make sure\n# saving the holdout dataset's target for later and delete it from holdout dataset\ntarget = \"target_column\"\nholdout_target = holdout_df[target].copy()\ndel holdout_df[target]\n\n# instantiate the needed blueprints class\nfrom classification import classification_blueprints # regression bps are available with from regression import regression_blueprints\ntest_class = classification_blueprints.ClassificationBluePrint(datasource=train_df, \n                        target_variable=target,\n                        train_split_type='cross',\n                        preferred_training_mode='auto', # Auto will automatically identify, if LGBM & Xgboost can use GPU acceleration*\n                        tune_mode='accurate' # hyperparameter sets will be validated with 10-fold CV Set this to 'simple' for 1-fold CV\n                        #categorical_columns=cat_columns # you can define categorical columns, otherwise e2e does this automatically\n                        #date_columns=date_columns # you can also define date columns (expected is YYYY-MM-DD format)\n                                                               )\n                                                                 \n\"\"\"\n*\n'Auto' is recommended for preferred_training_mode parameter, but with 'CPU' and 'GPU' it can also be controlled manually.\nIf you install Xgboost & LGBM into the same environment as GPU accelerated versions, you can set preferred_training_mode='gpu'.\nThis will massively improve training times and speed up SHAP feature importance for LGBM and Xgboost related tasks.\nFor Xgboost this should work out of the box, if installed into a RAPIDS environment.\n\"\"\"\n# run actual blueprint\ntest_class.ml_bp01_multiclass_full_processing_xgb_prob(preprocessing_type='nlp') \n# you could also change the preprocessing blueprint with the parameter \"preprocess_bp='bp_01' (or bp_02 or bp_03)\"\n\"\"\"\nWhen choosing blueprints several options are available:\n\nMulticlass blueprints can handle binary and multiclass tasks:\n- ml_bp00_train_test_binary_full_processing_log_reg_prob()\n- ml_bp01_multiclass_full_processing_xgb_prob()\n- ml_bp02_multiclass_full_processing_lgbm_prob()\n- ml_bp03_multiclass_full_processing_sklearn_stacking_ensemble()\n- ml_bp04_multiclass_full_processing_ngboost()\n- ml_special_binary_full_processing_boosting_blender()\n- ml_special_multiclass_auto_model_exploration()\n\nThere are regression blueprints as well (in regression module):\n- ml_bp10_train_test_regression_full_processing_linear_reg()\n- ml_bp11_regression_full_processing_xgboost()\n- ml_bp12_regressions_full_processing_lgbm()\n- ml_bp13_regression_full_processing_sklearn_stacking_ensemble()\n- ml_bp14_regressions_full_processing_ngboost()\n- ml_special_regression_full_processing_boosting_blender()\n- ml_special_regression_auto_model_exploration()\n\nThe preproccesing_type has 2 modes as of now:\n- full (default), which runs all steps except NLP specific ones\n- nlp: Adds some NLP related feature enginering steps.\n\"\"\"\n# After running the blueprint the pipeline is done. I can be saved with:\nsave_to_production(test_class, file_name='automl_instance')\n\n# The blueprint can be loaded with\nloaded_test_class = load_for_production(file_name='automl_instance')\n\n# predict on new data (in this case our holdout) with loaded blueprint\nloaded_test_class.ml_bp01_multiclass_full_processing_xgb_prob(holdout_df, preprocessing_type='nlp')\n\n# predictions can be accessed via a class attribute\nprint(churn_class.predicted_classes['xgboost'])\n```\n# Disclaimer\ne2e is not designed to quickly iterate over several algorithms and suggest you the best. It is made to deliver\nstate-of-the-art performance as ready-to-go blueprints. e2e-ml blueprints contain:\n- preprocessing (outlier, rare feature, datetime, categorical and NLP handling)\n- feature creation (binning, clustering, categorical and NLP features)\n- automated feature selection\n- model training with crossfold validation\n- automated hyperparameter tuning\n- model evaluation\n  This comes at the cost of runtime. Depending on your data we recommend strong hardware.\n\n## Release History\n\n\n* 1.0.1\n  - Optimized package requirements\n  - Pinned LGBM requirement to version 3.1.0 due to the bug \"LightGBMError: bin size 257 cannot run on GPU #3339\"\n* 0.9.9\n  * Enabled tune_mode parameter during class instantiation.\n  * Updated docstings across all functions and changed model defaults.\n  * Multiple bug fixes (LGBM regression accurate mode, label encoding and permutation tests).\n  * Enhanced user information & better ROC_AUC display\n  * Added automated GPU detection for LGBM and Xgboost.\n  * Added functions to save and load blueprints\n  * architectural changes (preprocessing organized in blueprints as well)\n* 0.9.4\n  * First release with classification and regression blueprints. (not available anymore)\n\n## Meta\n\nCreator: Thomas Meißner – [LinkedIn](https://www.linkedin.com/in/thomas-mei%C3%9Fner-m-a-3808b346)\n\nConsultant: Gabriel Stephen Alexander – [Github](https://github.com/bitsofsteve)\n\n\n[e2e-ml Github repository](https://github.com/ThomasMeissnerDS/e2e_ml)\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ThomasMeissnerDS/e2e_ml",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "e2eml",
    "package_url": "https://pypi.org/project/e2eml/",
    "platform": "",
    "project_url": "https://pypi.org/project/e2eml/",
    "project_urls": {
      "Homepage": "https://github.com/ThomasMeissnerDS/e2e_ml"
    },
    "release_url": "https://pypi.org/project/e2eml/1.0.1/",
    "requires_dist": [
      "ipython (>=6)",
      "boostaroota (>=1.3)",
      "category-encoders (==2.2.2)",
      "imblearn (>=0.0)",
      "ipython (>=7.10.0)",
      "jupyter-core (>=4.7.0)",
      "lightgbm (==3.1.0)",
      "matplotlib (>=3.3.4)",
      "ngboost (>=0.3.1)",
      "nltk (>=3.2.4)",
      "numpy (>=1.19.4)",
      "optuna (>=2.5.0)",
      "pandas (==1.1.5)",
      "pip (>=21.0.1)",
      "plotly (>=4.14.3)",
      "psutil (==5.8.0)",
      "seaborn (>=0.11.1)",
      "scikit-learn (==0.23.1)",
      "scipy (==1.6.3)",
      "setuptools (>=51.1.0)",
      "shap (>=0.39.0)",
      "spacy (>=3.0.6)",
      "wheel (==0.36.2)",
      "xgboost (>=1.3.3)"
    ],
    "requires_python": "",
    "summary": "An end to end solution for automl.",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14314192,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7f04280cb637e006dbd11f6835ea96cffdabebb174c427fde0e256d972afb279",
        "md5": "2e9f4fbdf6dc696c357475c0edbd3afe",
        "sha256": "46c236c5b55d816a2665681617378f119d4bb78ecf19822e850d05c92354308d"
      },
      "downloads": -1,
      "filename": "e2eml-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "2e9f4fbdf6dc696c357475c0edbd3afe",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 91505,
      "upload_time": "2021-07-21T11:31:25",
      "upload_time_iso_8601": "2021-07-21T11:31:25.608127Z",
      "url": "https://files.pythonhosted.org/packages/7f/04/280cb637e006dbd11f6835ea96cffdabebb174c427fde0e256d972afb279/e2eml-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "14c32322b788bf4d0df59503883bffa771b0479746186a940b5aa41cf9a04a66",
        "md5": "00e648db5da23cf4f89a08fcc7014bc7",
        "sha256": "68625bf5ffe187b96f4f6538f0dcf2f30e8bc32a9a698b5a56d7859f8e3f0285"
      },
      "downloads": -1,
      "filename": "e2eml-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "00e648db5da23cf4f89a08fcc7014bc7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 46891,
      "upload_time": "2021-07-21T11:31:27",
      "upload_time_iso_8601": "2021-07-21T11:31:27.184921Z",
      "url": "https://files.pythonhosted.org/packages/14/c3/2322b788bf4d0df59503883bffa771b0479746186a940b5aa41cf9a04a66/e2eml-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}