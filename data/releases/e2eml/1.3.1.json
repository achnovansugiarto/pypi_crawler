{
  "info": {
    "author": "Thomas Meißner",
    "author_email": "meissnercorporation@gmx.de",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# e2e ML\n> An end to end solution for automl. .\n\nPass in your data, add some information about it and get a full pipelines in return. Data preprocessing,\nfeature creation, modelling and evaluation with just a few lines of code.\n\n![](header.png)\n\n## Installation\n\nFrom Pypi:\n\n```sh\npip install e2eml\n```\nWe highly recommend to create a new virtual environment first. Then install e2e-ml into it. In the environment also download\nthe pretrained spacy model with. Otherwise e2eml will do this automatically during runtime.\n```sh\npython3 -m spacy download en\n```\nor\n```sh\npython -m spacy download en\n```\n(depending on your operating system.)\n\n## Usage example\n\ne2e has been designed to create state-of-the-art machine learning pipelines with a few lines of code. Basic example of usage:\n```sh\nimport e2eml\nfrom e2eml.classification import classification_blueprints\nimport pandas as pd\n# import data\ndf = pd.read_csv(\"Your.csv\")\n\n# split into a test/train & holdout set (holdout for prediction illustration here, but not required at all)\ntrain_df = df.head(1000).copy()\nholdout_df = df.tail(200).copy() # make sure\n# saving the holdout dataset's target for later and delete it from holdout dataset\ntarget = \"target_column\"\nholdout_target = holdout_df[target].copy()\ndel holdout_df[target]\n\n# instantiate the needed blueprints class\nfrom classification import classification_blueprints # regression bps are available with from regression import regression_blueprints\ntest_class = classification_blueprints.ClassificationBluePrint(datasource=train_df, \n                        target_variable=target,\n                        train_split_type='cross',\n                        preferred_training_mode='auto', # Auto will automatically identify, if LGBM & Xgboost can use GPU acceleration*\n                        tune_mode='accurate' # hyperparameter sets will be validated with 10-fold CV Set this to 'simple' for 1-fold CV\n                        #categorical_columns=cat_columns # you can define categorical columns, otherwise e2e does this automatically\n                        #date_columns=date_columns # you can also define date columns (expected is YYYY-MM-DD format)\n                                                               )\n                                                                 \n\"\"\"\n*\n'Auto' is recommended for preferred_training_mode parameter, but with 'CPU' and 'GPU' it can also be controlled manually.\nIf you install Xgboost & LGBM into the same environment as GPU accelerated versions, you can set preferred_training_mode='gpu'.\nThis will massively improve training times and speed up SHAP feature importance for LGBM and Xgboost related tasks.\nFor Xgboost this should work out of the box, if installed into a RAPIDS environment.\n\"\"\"\n# run actual blueprint\ntest_class.ml_bp01_multiclass_full_processing_xgb_prob(preprocessing_type='nlp') \n# you could also change the preprocessing blueprint with the parameter \"preprocess_bp='bp_01' (or bp_02 or bp_03)\"\n\"\"\"\nWhen choosing blueprints several options are available:\n\nMulticlass blueprints can handle binary and multiclass tasks:\n- ml_bp00_train_test_binary_full_processing_log_reg_prob()\n- ml_bp01_multiclass_full_processing_xgb_prob()\n- ml_bp02_multiclass_full_processing_lgbm_prob()\n- ml_bp03_multiclass_full_processing_sklearn_stacking_ensemble()\n- ml_bp04_multiclass_full_processing_ngboost()\n- ml_special_binary_full_processing_boosting_blender()\n- ml_special_multiclass_auto_model_exploration()\n\nThere are regression blueprints as well (in regression module):\n- ml_bp10_train_test_regression_full_processing_linear_reg()\n- ml_bp11_regression_full_processing_xgboost()\n- ml_bp12_regressions_full_processing_lgbm()\n- ml_bp13_regression_full_processing_sklearn_stacking_ensemble()\n- ml_bp14_regressions_full_processing_ngboost()\n- ml_special_regression_full_processing_boosting_blender()\n- ml_special_regression_auto_model_exploration()\n\nThe preproccesing_type has 2 modes as of now:\n- full (default), which runs all steps except NLP specific ones\n- nlp: Adds some NLP related feature enginering steps.\n\"\"\"\n# After running the blueprint the pipeline is done. I can be saved with:\nsave_to_production(test_class, file_name='automl_instance')\n\n# The blueprint can be loaded with\nloaded_test_class = load_for_production(file_name='automl_instance')\n\n# predict on new data (in this case our holdout) with loaded blueprint\nloaded_test_class.ml_bp01_multiclass_full_processing_xgb_prob(holdout_df, preprocessing_type='nlp')\n\n# predictions can be accessed via a class attribute\nprint(churn_class.predicted_classes['xgboost'])\n```\n# Disclaimer\ne2e is not designed to quickly iterate over several algorithms and suggest you the best. It is made to deliver\nstate-of-the-art performance as ready-to-go blueprints. e2e-ml blueprints contain:\n- preprocessing (outlier, rare feature, datetime, categorical and NLP handling)\n- feature creation (binning, clustering, categorical and NLP features)\n- automated feature selection\n- model training with crossfold validation\n- automated hyperparameter tuning\n- model evaluation\n  This comes at the cost of runtime. Depending on your data we recommend strong hardware.\n\n## Release History\n\n* 1.3.0\n  - Fixed issue with automated GPU-acceleration detection and flagging\n  - Fixed avg regression blueprint where eval function tried to call classification evaluation\n  - Moved POS tagging + PCA step into non-NLP pipeline as it showed good results in general\n  - improved NLP part (more and better feature engineering and preprocessing) of blueprints for better performance\n  - Added Vowpal Wabbit for classification and regression and replaced stacking ensemble in automated model exploration\n    by Vowpal Wabbit as well\n  - Set random_state for train_test splits for consistency\n* 1.0.1\n  - Optimized package requirements\n  - Pinned LGBM requirement to version 3.1.0 due to the bug \"LightGBMError: bin size 257 cannot run on GPU #3339\"\n* 0.9.9\n  * Enabled tune_mode parameter during class instantiation.\n  * Updated docstings across all functions and changed model defaults.\n  * Multiple bug fixes (LGBM regression accurate mode, label encoding and permutation tests).\n  * Enhanced user information & better ROC_AUC display\n  * Added automated GPU detection for LGBM and Xgboost.\n  * Added functions to save and load blueprints\n  * architectural changes (preprocessing organized in blueprints as well)\n* 0.9.4\n  * First release with classification and regression blueprints. (not available anymore)\n\n## Meta\n\nCreator: Thomas Meißner – [LinkedIn](https://www.linkedin.com/in/thomas-mei%C3%9Fner-m-a-3808b346)\n\nConsultant: Gabriel Stephen Alexander – [Github](https://github.com/bitsofsteve)\n\n\n[e2e-ml Github repository](https://github.com/ThomasMeissnerDS/e2e_ml)\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ThomasMeissnerDS/e2e_ml",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "e2eml",
    "package_url": "https://pypi.org/project/e2eml/",
    "platform": "",
    "project_url": "https://pypi.org/project/e2eml/",
    "project_urls": {
      "Homepage": "https://github.com/ThomasMeissnerDS/e2e_ml"
    },
    "release_url": "https://pypi.org/project/e2eml/1.3.1/",
    "requires_dist": [
      "ipython (>=6)",
      "boostaroota (>=1.3)",
      "category-encoders (==2.2.2)",
      "imblearn (>=0.0)",
      "ipython (>=7.10.0)",
      "jupyter-core (>=4.7.0)",
      "lightgbm (==3.1.0)",
      "matplotlib (>=3.3.4)",
      "ngboost (>=0.3.1)",
      "nltk (>=3.2.4)",
      "numpy (>=1.19.4)",
      "optuna (>=2.5.0)",
      "pandas (==1.1.5)",
      "pip (>=21.0.1)",
      "plotly (>=4.14.3)",
      "psutil (==5.8.0)",
      "seaborn (>=0.11.1)",
      "scikit-learn (==0.23.1)",
      "scipy (==1.6.3)",
      "setuptools (>=51.1.0)",
      "shap (>=0.39.0)",
      "spacy (>=3.0.6)",
      "vowpalwabbit (>=8.11.0wheel==0.36.2)",
      "xgboost (>=1.3.3)"
    ],
    "requires_python": "",
    "summary": "An end to end solution for automl.",
    "version": "1.3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14314192,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2380b78f41dad6cdae52cb8abe51f65366dbf30c01000d643eba931197544092",
        "md5": "a8847b14f80207ed63f26b69d132a4bc",
        "sha256": "8238dce9bb35ce6c2d4ddbb58f0cc7c425b8c4d7e70cef28e494442b00c7ba8d"
      },
      "downloads": -1,
      "filename": "e2eml-1.3.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a8847b14f80207ed63f26b69d132a4bc",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 95841,
      "upload_time": "2021-07-25T20:25:46",
      "upload_time_iso_8601": "2021-07-25T20:25:46.948070Z",
      "url": "https://files.pythonhosted.org/packages/23/80/b78f41dad6cdae52cb8abe51f65366dbf30c01000d643eba931197544092/e2eml-1.3.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5985446cfdabf088e96c99194980d7bc2c4f0512ff94f04493d0c0a80c019f40",
        "md5": "19da8bff3c1a83920e15326790e6364e",
        "sha256": "d72870705173e7e9536b10f28949a32b5239d29820ee3c23ad89dc3d9d14d7f5"
      },
      "downloads": -1,
      "filename": "e2eml-1.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "19da8bff3c1a83920e15326790e6364e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 51491,
      "upload_time": "2021-07-25T20:25:48",
      "upload_time_iso_8601": "2021-07-25T20:25:48.674899Z",
      "url": "https://files.pythonhosted.org/packages/59/85/446cfdabf088e96c99194980d7bc2c4f0512ff94f04493d0c0a80c019f40/e2eml-1.3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}