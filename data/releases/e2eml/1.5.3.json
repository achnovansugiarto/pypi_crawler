{
  "info": {
    "author": "Thomas Meißner",
    "author_email": "meissnercorporation@gmx.de",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# e2e ML\n> An end to end solution for automl. .\n\nPass in your data, add some information about it and get a full pipelines in return. Data preprocessing,\nfeature creation, modelling and evaluation with just a few lines of code.\n\n![](header.png)\n\n## Installation\n\nFrom Pypi:\n\n```sh\npip install e2eml\n```\nWe highly recommend to create a new virtual environment first. Then install e2e-ml into it. In the environment also download\nthe pretrained spacy model with. Otherwise e2eml will do this automatically during runtime.\n\ne2eml can also be installed into a RAPIDS environment. For this we recommend to create a fresh environment following\n[RAPIDS](https://rapids.ai/start.html) instructions. After environment installation and activation, a special installation is needed to not run into installation issues.\nJust run:\n```sh\npip install e2eml[rapids]\n```\nThis will additionally install cupy and cython to prevent issues. Additionally it is needed to run:\n```sh\npip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n```\nOtherwise Pytorch will fail trying to run on GPU.\nIf e2eml shall e installed together with Jupyter core and ipython, please install with:\n```sh\npip install e2eml[full]\n```\ninstead.\n\n## Usage example\n\ne2e has been designed to create state-of-the-art machine learning pipelines with a few lines of code. Basic example of usage:\n```sh\nimport e2eml\nfrom e2eml.classification import classification_blueprints\nimport pandas as pd\n# import data\ndf = pd.read_csv(\"Your.csv\")\n\n# split into a test/train & holdout set (holdout for prediction illustration here, but not required at all)\ntrain_df = df.head(1000).copy()\nholdout_df = df.tail(200).copy() # make sure\n# saving the holdout dataset's target for later and delete it from holdout dataset\ntarget = \"target_column\"\nholdout_target = holdout_df[target].copy()\ndel holdout_df[target]\n\n# instantiate the needed blueprints class\nfrom classification import classification_blueprints # regression bps are available with from regression import regression_blueprints\ntest_class = classification_blueprints.ClassificationBluePrint(datasource=train_df, \n                        target_variable=target,\n                        train_split_type='cross',\n                        preferred_training_mode='auto', # Auto will automatically identify, if LGBM & Xgboost can use GPU acceleration*\n                        tune_mode='accurate' # hyperparameter sets will be validated with 10-fold CV Set this to 'simple' for 1-fold CV\n                        #categorical_columns=cat_columns # you can define categorical columns, otherwise e2e does this automatically\n                        #date_columns=date_columns # you can also define date columns (expected is YYYY-MM-DD format)\n                                                               )\n                                                                 \n\"\"\"\n*\n'Auto' is recommended for preferred_training_mode parameter, but with 'CPU' and 'GPU' it can also be controlled manually.\nIf you install Xgboost & LGBM into the same environment as GPU accelerated versions, you can set preferred_training_mode='gpu'.\nThis will massively improve training times and speed up SHAP feature importance for LGBM and Xgboost related tasks.\nFor Xgboost this should work out of the box, if installed into a RAPIDS environment.\n\"\"\"\n# run actual blueprint\ntest_class.ml_bp01_multiclass_full_processing_xgb_prob(preprocessing_type='nlp') \n# you could also change the preprocessing blueprint with the parameter \"preprocess_bp='bp_01' (or bp_02 or bp_03)\"\n\"\"\"\nWhen choosing blueprints several options are available:\n\nMulticlass blueprints can handle binary and multiclass tasks:\n- ml_bp00_train_test_binary_full_processing_log_reg_prob()\n- ml_bp01_multiclass_full_processing_xgb_prob()\n- ml_bp02_multiclass_full_processing_lgbm_prob()\n- ml_bp03_multiclass_full_processing_sklearn_stacking_ensemble()\n- ml_bp04_multiclass_full_processing_ngboost()\n- ml_bp05_multiclass_full_processing_vowpal_wabbit\n- ml_bp_06_multiclass_full_processing_bert_transformer() # for NLP specifically\n- ml_special_binary_full_processing_boosting_blender()\n- ml_special_multiclass_auto_model_exploration()\n\nThere are regression blueprints as well (in regression module):\n- ml_bp10_train_test_regression_full_processing_linear_reg()\n- ml_bp11_regression_full_processing_xgboost()\n- ml_bp12_regressions_full_processing_lgbm()\n- ml_bp13_regression_full_processing_sklearn_stacking_ensemble()\n- ml_bp14_regressions_full_processing_ngboost()\n- ml_bp15_regression_full_processing_vowpal_wabbit_reg()\n- ml_16_regressions_full_processing_bert_transformer()\n- ml_special_regression_full_processing_boosting_blender()\n- ml_special_regression_auto_model_exploration()\n\nThe preproccesing_type has 2 modes as of now:\n- full (default), which runs all steps except NLP specific ones\n- nlp: Adds some NLP related feature enginering steps.\n\"\"\"\n# After running the blueprint the pipeline is done. I can be saved with:\nsave_to_production(test_class, file_name='automl_instance')\n\n# The blueprint can be loaded with\nloaded_test_class = load_for_production(file_name='automl_instance')\n\n# predict on new data (in this case our holdout) with loaded blueprint\nloaded_test_class.ml_bp01_multiclass_full_processing_xgb_prob(holdout_df, preprocessing_type='nlp')\n\n# predictions can be accessed via a class attribute\nprint(churn_class.predicted_classes['xgboost'])\n```\n# Disclaimer\ne2e is not designed to quickly iterate over several algorithms and suggest you the best. It is made to deliver\nstate-of-the-art performance as ready-to-go blueprints. e2e-ml blueprints contain:\n- preprocessing (outlier, rare feature, datetime, categorical and NLP handling)\n- feature creation (binning, clustering, categorical and NLP features)\n- automated feature selection\n- model training (with crossfold validation)\n- automated hyperparameter tuning\n- model evaluation\n  This comes at the cost of runtime. Depending on your data we recommend strong hardware.\n\n## Release History\n\n* 1.5.3\n  - Added transformer blueprints for NLP classification and regression\n  - renamed Vowpal Wabbit blueprint to fit into blueprint naming convention\n  - Created \"extras\" options for library installation: 'rapids' installs extras, so e2eml can be installed into\n    into a rapids environment while 'jupyter' adds jupyter core and ipython. 'full' installs all of them.\n* 1.3.9\n  - Fixed issue with automated GPU-acceleration detection and flagging\n  - Fixed avg regression blueprint where eval function tried to call classification evaluation\n  - Moved POS tagging + PCA step into non-NLP pipeline as it showed good results in general\n  - improved NLP part (more and better feature engineering and preprocessing) of blueprints for better performance\n  - Added Vowpal Wabbit for classification and regression and replaced stacking ensemble in automated model exploration\n    by Vowpal Wabbit as well\n  - Set random_state for train_test splits for consistency\n  - Fixed sklearn dependency to 0.22.0 due to six import error\n* 1.0.1\n  - Optimized package requirements\n  - Pinned LGBM requirement to version 3.1.0 due to the bug \"LightGBMError: bin size 257 cannot run on GPU #3339\"\n* 0.9.9\n  * Enabled tune_mode parameter during class instantiation.\n  * Updated docstings across all functions and changed model defaults.\n  * Multiple bug fixes (LGBM regression accurate mode, label encoding and permutation tests).\n  * Enhanced user information & better ROC_AUC display\n  * Added automated GPU detection for LGBM and Xgboost.\n  * Added functions to save and load blueprints\n  * architectural changes (preprocessing organized in blueprints as well)\n* 0.9.4\n  * First release with classification and regression blueprints. (not available anymore)\n\n## Meta\n\nCreator: Thomas Meißner – [LinkedIn](https://www.linkedin.com/in/thomas-mei%C3%9Fner-m-a-3808b346)\n\nConsultant: Gabriel Stephen Alexander – [Github](https://github.com/bitsofsteve)\n\n\n[e2e-ml Github repository](https://github.com/ThomasMeissnerDS/e2e_ml)\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ThomasMeissnerDS/e2e_ml",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "e2eml",
    "package_url": "https://pypi.org/project/e2eml/",
    "platform": "",
    "project_url": "https://pypi.org/project/e2eml/",
    "project_urls": {
      "Homepage": "https://github.com/ThomasMeissnerDS/e2e_ml"
    },
    "release_url": "https://pypi.org/project/e2eml/1.5.3/",
    "requires_dist": [
      "babel (>=2.9.1)",
      "boostaroota (>=1.3)",
      "category-encoders (==2.2.2)",
      "imblearn (>=0.0)",
      "lightgbm (==3.1.0)",
      "matplotlib (>=3.3.4)",
      "ngboost (>=0.3.1)",
      "nltk (>=3.2.4)",
      "numpy (>=1.19.4)",
      "optuna (>=2.5.0)",
      "pandas (==1.1.5)",
      "pip (>=21.0.1)",
      "plotly (>=4.14.3)",
      "psutil (==5.8.0)",
      "seaborn (>=0.11.1)",
      "scikit-learn (==0.22.2)",
      "scipy (==1.6.3)",
      "setuptools (>=51.1.0)",
      "shap (>=0.39.0)",
      "spacy (>=3.0.6)",
      "textblob (>=0.15.3)",
      "torch (>=1.7.0)",
      "transformers (>=4.0.0)",
      "vowpalwabbit (>=8.11.0)",
      "wheel (==0.36.2)",
      "xgboost (>=1.3.3)"
    ],
    "requires_python": "",
    "summary": "An end to end solution for automl.",
    "version": "1.5.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14314192,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5f52c106cc68adbb7ebccf44afdcc34d0460159d71049f4c8a2b991f6d5e05c7",
        "md5": "1245cabd7c47dc1de8da51536ab5bdc1",
        "sha256": "18ac26e66f0062daa5daea0cc0134cb122978c6c340d90ccc04dbd2d5f17a837"
      },
      "downloads": -1,
      "filename": "e2eml-1.5.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1245cabd7c47dc1de8da51536ab5bdc1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 108800,
      "upload_time": "2021-08-01T20:48:45",
      "upload_time_iso_8601": "2021-08-01T20:48:45.609142Z",
      "url": "https://files.pythonhosted.org/packages/5f/52/c106cc68adbb7ebccf44afdcc34d0460159d71049f4c8a2b991f6d5e05c7/e2eml-1.5.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8c7d0a9b27c094be1027bd2a452503da842756a9b8c79d4d9f41f65a88e704bf",
        "md5": "66f2c74f8f0d92ae6824ff2a7aa79a91",
        "sha256": "f0df5f6f05832241a1adf7340e8e0754663ee8a500336fd9f4a04b8455914d34"
      },
      "downloads": -1,
      "filename": "e2eml-1.5.3.tar.gz",
      "has_sig": false,
      "md5_digest": "66f2c74f8f0d92ae6824ff2a7aa79a91",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 62170,
      "upload_time": "2021-08-01T20:48:47",
      "upload_time_iso_8601": "2021-08-01T20:48:47.110200Z",
      "url": "https://files.pythonhosted.org/packages/8c/7d/0a9b27c094be1027bd2a452503da842756a9b8c79d4d9f41f65a88e704bf/e2eml-1.5.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}