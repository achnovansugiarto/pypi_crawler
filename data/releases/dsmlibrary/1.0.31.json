{
  "info": {
    "author": "DigitalStoreMesh Co.,Ltd",
    "author_email": "contact@storemesh.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# DSM Library\n\n## DataNode\n0. init DataNode\n```python\nfrom dsmlibrary.datanode import DataNode \n\ndata = DataNode(token)\n```\n1. upload file\n```python\ndata.upload_file(directory_id=<directory_id>, file_path='<file_path>', description=\"<description(optional)>\")\n```\n\n2. download file\n```python\ndata.download_file(file_id=<file_id>, download_path=\"<place download file save> (default ./dsm.tmp)\")\n```\n3. get file\n```python\nmeta, file = data.get_file(file_id=\"<file_id>\")\n# meta -> dict\n# file -> io bytes\n```\n```python\n# example read csv pandas\n \nmeta, file = data.get_file(file_id=\"<file_id>\")\ndf = pd.read_csv(file)\n...\n``` \n4. read df\n```python\ndf = data.read_df(file_id=\"<file_id>\")\n# df return as pandas dataframe\n```\n\n6. read ddf\n\n* ```.parquet must use this function```\n\n```python\nddf = data.read_ddf(file_id=\"<file_id>\")\n# ddf return as dask dataframe\n```\n\n7. write parquet file\n```python\ndf = ... # pandas dataframe or dask dataframe\n\ndata.write(df=df, directory=<directory_id>, name=\"<save_file_name>\", description=\"<description>\", replace=<replace if file exists. default False>, profiling=<True or False default False>, lineage=<list of file id. eg [1,2,3]>)\n```\n\n8. writeListDataNode\n\n```python\ndf = ... # pandas dataframe or dask dataframe\ndata.writeListDataNode(df=df, directory_id=<directory_id>, name=\"<save_file_name>\", description=\"<description>\", replace=<replace if file exists. default False>, profiling=<True or False default False>, lineage=<list of file id. eg [1,2,3]>)\n```\n\n9. get file id\n```python\nfile_id = data.get_file_id(name=<file name>, directory_id=<directory id>)\n# file_id return int fileID\n```\n\n10. get directory id\n```\ndirectory_id = data.get_directory_id(parent_dir_id=<directory id>, name=<file name>)\n# directory_id return int directoryID\n```\n\n11. get get_file_version\n```use for listDataNode```\n\n```python\nfileVersion = data.get_file_version(file_id=<file id>)\n# return dict `file_id` and `timestamp`\n```\n\n\n## Clickhouse\n1. imoprt data to clickhouse\n\n```python\nfrom dsmlibrary.clickhouse import ClickHouse\n\nddf = ... # pandas dataframe or dask dataframe\n\n## to warehouse\ntable_name = <your_table_name>\npartition_by = <your_partition_by>\n\nconnection = { \n  'host': '', \n  'port': , \n  'database': '', \n  'user': '', \n  'password': '', \n  'settings':{ \n     'use_numpy': True \n  }, \n  'secure': False \n}\n\nwarehouse = ClickHouse(connection=connection)\n\ntableName = warehouse.get_or_createTable(ddf=ddf, tableName=table_name, partition_by=partition_by)\nwarehouse.write(ddf=ddf, tableName=tableName)\n```\n\n2. query data from clickhouse\n```python\nquery = f\"\"\" \n    SELECT * FROM {tableName} LIMIT 10 \n\"\"\" \nwarehouse.read(sqlQuery=query)\n\n```\n\n3. drop table\n```python\nwarehouse.dropTable(tableName=table_name)\n```\n\n- optional\n```use for custom config insert data to clickhouse```\n```python\nconfig = {\n  'n_partition_per_block': 10,\n  'n_row_per_loop': 1000\n}\nwarehouse = ClickHouse(connection=connection, config=config)\n```\n\n4. truncate table\n```\nwarehouse.truncateTable(tableName=table_name)\n```\n\n# API\n## dsmlibrary\n### dsmlibrary.datanode.DataNode\n- upload_file\n- download_file\n- read_df\n- read_ddf\n- write\n- get_file_id\n\n### dsmlibrary.clickhouse.ClickHouse\n- get_or_createTable\n- write\n- read\n- dropTable\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.com/public-project2/dsm-library",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dsmlibrary",
    "package_url": "https://pypi.org/project/dsmlibrary/",
    "platform": null,
    "project_url": "https://pypi.org/project/dsmlibrary/",
    "project_urls": {
      "Homepage": "https://gitlab.com/public-project2/dsm-library"
    },
    "release_url": "https://pypi.org/project/dsmlibrary/1.0.31/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A simple way to use Dataset. for dsm",
    "version": "1.0.31",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17413320,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "336f715438cac768bf154f511c4a95a44882fc03e1ff760a5dfea78ba5ccd49e",
        "md5": "698d2cd583cee8134ac27c4f4bb2e9cf",
        "sha256": "257b0786558b1aa66962bda62bbd7e87f975a78c82bc82353690ef79834b8c75"
      },
      "downloads": -1,
      "filename": "dsmlibrary-1.0.31.tar.gz",
      "has_sig": false,
      "md5_digest": "698d2cd583cee8134ac27c4f4bb2e9cf",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 16307,
      "upload_time": "2023-02-17T11:42:02",
      "upload_time_iso_8601": "2023-02-17T11:42:02.927508Z",
      "url": "https://files.pythonhosted.org/packages/33/6f/715438cac768bf154f511c4a95a44882fc03e1ff760a5dfea78ba5ccd49e/dsmlibrary-1.0.31.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}