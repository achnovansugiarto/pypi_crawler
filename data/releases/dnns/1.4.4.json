{
  "info": {
    "author": "Kevin Ryczko",
    "author_email": "kevin.ryczko@uottawa.ca",
    "bugtrack_url": null,
    "classifiers": [
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# dnns - general purpose package for training deep neural networks with custom HDF5 datasets. \n\nAfter many iterations of deep learning scripts, I have finally decided to have one package - with data loading tools to do deep learning. For more documentation go to https://cleanit.github.io/dnns.\n\n## Some information about the package:\n  1. This package utilizes pytorch.\n  2. This package can also utilize mixed-precision training using NVIDIA's apex package.\n  3. There is an executable titled 'worker.py' that is installed with this package. If you are not interested in going through the process of writing a script that does training (or you have done this already and don't want to do it again), then you can just use it (see steps of how to use it below).\n  4. This script can be used for multi-node, multi-gpu training (see below for more information).\n\n## Quickstart:\nYou **must** have 2 things (1 other thing is recommended). \n\n  1) A network (using the ordinary class structure that pytorch uses) written to a file (default is dnn.py) with a network class defined to be DNN. \n  2) A train and test directory with your HDF5 data sitting in it. The default dataset labels that the loader will read are 'X' and 'Y', which represent input and output data.\n\n  3) (Recommended) A YAML file (default name is input.yaml) where you can configure the training protocol. \n\nTo run, you can simply run:\n```\npython -m torch.distributed.launch -nnodes <n_nodes> --nproc_per_node <n_gpus_per_node> worker.py\n```\n\n## Slowstart:\nLet's say you are in some directory called `some_dir`. We type `ls` and see:\n```\n~some_dir $ ls\ninput.yaml  dnn.py  train/  test/\n```\nLet's say you have HDF5 files called `training.h5` and `testing.h5` located in `train/` and `test/` with dataset labels `input_data` and `output_data`. Our configuration file `input.yaml` could look something like this:\n```\n~some_dir $ more input.yaml\n# the number of epochs to run\nn_epochs: 2000\n\n# batch size of images\nbatch_size: 512\n\n# learning rate for model\nlearning_rate: 0.00001\n\n# number of threads for each GPU to use for data queuing\nn_workers: 6 \n\n# labels in the HDF5 files\nx_label: 'input_data'\ny_label: 'output_data'\n\nmixed_precision: false\n```\nHere we have defined the number of epochs, batch size, learning rate, the number of worker threads, dataset labels, and we have turned mixed precision off. Now let's look at `dnn.py`. \n```\n~some_dir $ more dnn.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom collections import OrderedDict\n\nclass DNN(nn.Module):\n\n    def __init__(self, input_shape):\n        super().__init__()\n        layers = OrderedDict()\n        layers['conv_red_1'] = nn.Conv3d(1, 64, 5, padding=2, stride=2)\n        layers['conv_red_1_elu'] = nn.ELU()\n        layers['conv_red_2'] = nn.Conv3d(64, 64, 5, padding=2, stride=1)\n        layers['conv_red_2_elu'] = nn.ELU()\n\n        layers['conv_nonred_3'] = nn.Conv3d(64, 16, 5, padding=2)\n        layers['conv_nonred_3_elu'] = nn.ELU()\n        for i in range(4, 9):\n            layers['conv_nonred_' + str(i)] = nn.Conv3d(16, 16, 5, padding=2)\n            layers['conv_nonred_' + str(i) + '_elu'] = nn.ELU()\n\n        layers['conv_red_3'] = nn.Conv3d(16, 64, 5, padding=2, stride=1)\n        layers['conv_red_3_elu'] = nn.ELU()\n\n        layers['conv_nonred_9'] = nn.Conv3d(64, 32, 5, padding=2, stride=1)\n        layers['conv_red_9_elu'] = nn.ELU()\n        for i in range(10, 14):\n            layers['conv_nonred_' + str(i)] = nn.Conv3d(32, 32, 5, padding=2)\n            layers['conv_nonred_' + str(i) + '_elu'] = nn.ELU()\n\n        layers['flatten'] = nn.Flatten()\n        layers['fc1'] = nn.Linear((input_shape[0] //2 + 1) * (input_shape[1] //2 + 1) * (input_shape[2] //2 + 1) * input_shape[3] * 32, 1024 )\n        layers['fc1_relu'] = nn.ELU()\n        layers['fc2'] = nn.Linear(1024, 1)\n        self.model = nn.Sequential(layers)\n\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[-1], x.shape[1], x.shape[2], x.shape[3])  \n        return self.model(x)\n```\nWith these defined you can simply run:\n```\npython -m torch.distributed.launch -nnodes <n_nodes> --nproc_per_node <n_gpus_per_node> worker.py\n```\nAfterwards a checkpoint file `checkpoint.torch`, and a data file `loss_vs_epoch.dat` is created. \n\n## Limitations\n1) Currently, you can only have one HDF5 file for training/testing.\n2) There has not been a lot of testing of this repo. I expect there will be critical issues that must be taken care of in future releases.\n\n## Contact\nPlease contact me at `kevin.ryczko@uottawa.ca` for any issues.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/CLEANit/dnns.git",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dnns",
    "package_url": "https://pypi.org/project/dnns/",
    "platform": "",
    "project_url": "https://pypi.org/project/dnns/",
    "project_urls": {
      "Homepage": "https://github.com/CLEANit/dnns.git"
    },
    "release_url": "https://pypi.org/project/dnns/1.4.4/",
    "requires_dist": [
      "torch"
    ],
    "requires_python": "",
    "summary": "A deep learning package for using HDF5 and Pytorch (Distributed Data Parallel with NVIDIA mixed-precision) with ease.",
    "version": "1.4.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8642290,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "62081ca02c3ebdd1bdf11c7e9229fc8e89af067e32568170171aceee5eb6869c",
        "md5": "65562ae892a7ed50191cb0faa8d27cd2",
        "sha256": "444851806d74840971e94f2d078ae853247063d2e8f3db53fea1c166a1af20aa"
      },
      "downloads": -1,
      "filename": "dnns-1.4.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "65562ae892a7ed50191cb0faa8d27cd2",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 26217,
      "upload_time": "2020-11-12T13:25:03",
      "upload_time_iso_8601": "2020-11-12T13:25:03.412965Z",
      "url": "https://files.pythonhosted.org/packages/62/08/1ca02c3ebdd1bdf11c7e9229fc8e89af067e32568170171aceee5eb6869c/dnns-1.4.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "02833408e35ba2fde3ebba2cdccc8186b3988bd147ddf59648be9abe4b00fb2e",
        "md5": "4a7d09bb26470e693477131716d2d1d0",
        "sha256": "3ac5efd541ae746e5f098831b444427c0ffa546962137c7919e11f395e30462e"
      },
      "downloads": -1,
      "filename": "dnns-1.4.4.tar.gz",
      "has_sig": false,
      "md5_digest": "4a7d09bb26470e693477131716d2d1d0",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 14132,
      "upload_time": "2020-11-12T13:25:04",
      "upload_time_iso_8601": "2020-11-12T13:25:04.532783Z",
      "url": "https://files.pythonhosted.org/packages/02/83/3408e35ba2fde3ebba2cdccc8186b3988bd147ddf59648be9abe4b00fb2e/dnns-1.4.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}