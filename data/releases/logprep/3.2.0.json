{
  "info": {
    "author": "Logprep Team",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "<h1 align=\"center\">Logprep</h1>\n<h3 align=\"center\">\n\n![GitHub release (latest by date)](https://img.shields.io/github/v/release/fkie-cad/Logprep)\n![GitHub Workflow Status (branch)](https://img.shields.io/github/workflow/status/fkie-cad/logprep/CI/main)\n[![Documentation Status](https://readthedocs.org/projects/logprep/badge/?version=latest)](http://logprep.readthedocs.io/?badge=latest)\n![GitHub contributors](https://img.shields.io/github/contributors/fkie-cad/Logprep)\n<a href=\"https://codecov.io/github/fkie-cad/Logprep\" target=\"_blank\">\n    <img src=\"https://img.shields.io/codecov/c/github/fkie-cad/Logprep?color=%2334D058\" alt=\"Coverage\">\n</a>\n![GitHub Repo stars](https://img.shields.io/github/stars/fkie-cad/logprep?style=social)\n</h3>\n\n## Introduction\n\nLogprep allows to collect, process and forward log messages from various data sources.\nLog messages are being read and written by so-called connectors.\nCurrently, connectors for Kafka and JSON files exist.\n\nThe log messages are processed step-by-step by a pipeline of processors,\nwhere each processor modifies an event that is being passed through.\nThe main idea is that each processor performs a simple task that is easy to carry out.\nOnce the log massage is passed through all processors in the pipeline the resulting\nmessage is sent to a configured output connector.\n\nLogprep is designed to be expandable with new connectors and processors.\n\nLogprep is primarily designed to process log messages. Generally, Logprep can handle JSON messages,\nallowing further applications besides log handling.\n\nThis readme provides basic information about the following topics:\n- [About Logprep](#about-logprep)    \n- [Getting Started](#getting-started)\n- [Docker Quickstart](#docker-quickstart-environment)\n- [Documentation](#documentation)\n- [Contributing](#contributing)\n- [License](#license)\n\nMore detailed information can be found in the \n[Documentation](https://logprep.readthedocs.io/en/latest/).\n\n## About Logprep\n\n### Pipelines\n\nLogprep processes incoming log messages with a configured pipeline that can be spawned\nmultiple times via multiprocessing.\nThe following chart shows a basic setup that represents this behaviour.\nThe pipeline consists of three processors the `Normalizer`, `Geo-IP Enricher` and the \n`Dropper`. \nEach pipeline runs concurrently and takes one event from the `Input Connector`.\nOnce the log messages is fully processed the result will be forwarded to the `Output Connector`,\nafter which the pipeline will take the next message, repeating the processing cycle.\n\n```mermaid\nflowchart LR\nA[Input\\nConnector] --> B\nA[Input\\nConnector] --> C\nA[Input\\nConnector] --> D\nsubgraph Pipeline 1\nB[Normalizer] --> E[Geo-IP Enricher]\nE --> F[Dropper] \nend\nsubgraph Pipeline 2\nC[Normalizer] --> G[Geo-IP Enricher]\nG --> H[Dropper] \nend\nsubgraph Pipeline n\nD[Normalizer] --> I[Geo-IP Enricher]\nI --> J[Dropper] \nend\nF --> K[Output\\nConnector]\nH --> K[Output\\nConnector]\nJ --> K[Output\\nConnector]\n```\n\n### Processors\n\nEvery processor has one simple task to fulfill.\nFor example, the `Normalizer` can split up long message fields into multiple subfields or copy \ncontent from one field into another one, leading to a normalized message pattern.\nThe `Geo-IP Enricher`, for example, takes an ip-address and adds the geolocation of it to the \nlog message, based on a configured geo-ip database. \nOr the `Dropper` deletes fields from the log message.\n\nAs detailed overview of all processors can be found in the\n[processor documentation](https://logprep.readthedocs.io/en/latest/user_manual/configuration/processor.html).\n\nTo influence the behaviour of those processors, each can be configured with a set of rules.\nThese rules define two things.\nFirstly, they specify when the processor should process a log message\nand secondly they specify how to process the message. \nFor example which fields should be deleted or to which IP-address the geolocation should be \nretrieved.\n\nFor performance reasons on startup all rules per processor are aggregated to a generic and a specific rule tree, respectively. \nInstead of evaluating all rules independently for each log message the message is checked against \nthe rule tree.\nEach node in the rule tree represents a condition that has to be meet,\nwhile the leafs represent changes that the processor should apply.\nIf no condition is met, the processor will just pass the log event to the next processor.\n\nThe following chart gives an example of such a rule tree:\n\n```mermaid\nflowchart TD\nA[root]\nA-->B[Condition 1]\nA-->C[Condition 2]\nA-->D[Condition 3]\nB-->E[Condition 4]\nB-->H(Rule 1)\nC-->I(Rule 2)\nD-->J(rule 3)\nE-->G(Rule 4)\n```\n\nTo further improve the performance, it is possible to prioritize specific nodes of the rule tree,\nsuch that broader conditions are higher up in the tree. \nAnd specific conditions can be moved further down.\nFollowing json gives an example of such a rule tree configuration. \nThis configuration will lead to the prioritization of `tags` and `message` in the rule tree.\n\n```json\n{\n  \"priority_dict\": {\n    \"category\": \"01\",\n    \"message\": \"02\"\n  },\n  \"tag_map\": {\n    \"check_field_name\": \"check-tag\"\n  }\n}\n```\n\nInstead of writing very specific rules that apply to single log messages, it is also possible\nto define generic rules that apply to multiple messages. \nIt is possible to define a set of generic and specific rules for each processor, resulting \nin two rule trees. \n\n### Connectors\n\nConnectors are responsible for reading the input and writing the result to a desired output. \nThe main connector that is currently used and implemented is a kafka-connector allowing to \nreceive messages from a kafka-topic and write messages into a kafka-topic. \n\nThe details regarding the connectors can be found in the\n[connector documentation](https://logprep.readthedocs.io/en/latest/user_manual/configuration/connector.html).\n\n### Configuration\n\nTo run Logprep, certain configurations have to be provided. \nFirst, a general configuration is given that describes the pipeline and the connectors, \nand lastly, the processors need rules in order to process messages correctly.\n\nThe following yaml configuration shows an example configuration for the pipeline shown \nin the graph above:\n\n```yaml\nprocess_count: 3\ntimeout: 0.1\n\npipeline:\n  - normalizer:\n      type: normalizer\n      specific_rules:\n        - rules/01_normalizer/specific/\n      generic_rules:\n        - rules/01_normalizer/generic/\n      regex_mapping: artifacts/regex_mapping.yml\n      grok_patterns: artifacts/grok_patterns/\n  - geoip_enricher:\n      type: geoip_enricher\n      specific_rules:\n        - rules/02_geoip_enricher/specific/\n      generic_rules:\n        - rules/02_geoip_enricher/generic/\n      tree_config: artifacts/tree_config.json\n      db_path: artifacts/GeoDB.mmdb\n  - dropper:\n      type: dropper\n      specific_rules:\n        - rules/03_dropper/specific/\n      generic_rules:\n        - rules/03_dropper/generic/\n\nconnector:\n  type: confluentkafka\n  bootstrapservers: [127.0.0.1:9092]\n  consumer:\n    topic: consumer\n    group: cgroup\n    auto_commit: on\n    session_timeout: 6000\n    offset_reset_policy: smallest\n  producer:\n    topic: producer\n    error_topic: producer_error\n    ack_policy: all\n    compression: gzip\n    maximum_backlog: 10000\n    linger_duration: 0\n    flush_timeout: 30\n    send_timeout: 2\n```\n\nThe following yaml represents a dropper rule which according to the previous configuration \nshould be in the `rules/03_dropper/generic/` directory.\n\n```yaml\nfilter: \"message\"\ndrop:\n  - message\ndescription: \"Drops the message field\"\n```\n\nThe condition of this rule would check if the field `message` exists in the log.\nIf it does exist then the dropper would delete this field from the log message.\n\nDetails about the rule language and how to write rules for the processors can be found in the\n[rule language documentation](https://logprep.readthedocs.io/en/latest/user_manual/rule_language.html).\n\n## Getting Started\n\n### Installation\n\nPython should be present on the system, currently supported are the versions 3.6 - 3.9.\nTo install Logprep you have three options:\n\n**1. Option:** Installation via PyPI: \n\nThis option is recommended if you just want to use the latest release of logprep.\n```\npip install logprep\n```\n\n**2. Option:** Installation via Git Repository:\n\nThis option is recommended if you are interested in the latest developments and might want to \ncontribute to them.\n```\ngit clone https://github.com/fkie-cad/Logprep.git\ncd Logprep\npip install -r requirements.txt\n```\n\n**3. Option:** Installation via Github Release\n\nThis option is recommended if you just want to try out the latest developments.\n```\npip install git+https://github.com/fkie-cad/Logprep.git@latest\n```\n\n### Testing\n\nTox can be used to perform unit and acceptance tests (install tox via `pip3 install tox`).\nTests are started by executing `tox` in the project root directory.\nThis creates a virtual test environment and executes tests within it.\n\nMultiple different test environments were defined for tox.\nThose can be executed via: `tox -e [name of the test environment]`.\nFor Example:\n\n```\ntox -e py36-all\n```\n\nThis runs all tests, calculates the test coverage and evaluates the code quality for the python \n3.6 version.\n\nMultiple environments can be tested within one call: \n\n```\ntox -e py36-all -e py37-all -e py38-all -e py39-all\n```\n\nIf you want to run them in parallel attach the option `-p`.\nThis can lead to side effects in I/O operations though, like concurrences in writing or reading\nfiles.\n\nAn overview of the test environments can be obtained by executing:\n\n```\ntox -av\n```\n\nIn case the requirements change, the test environments must be rebuilt with the parameter `-r`:\n\n```\ntox -e all -r\n```\n\n### Semgrep\n\nTo run the semgrep rules against the semgrep python registry at least python 3.7 is required. \nBecause of that and the default logprep support for python 3.6 semgrep is not part of the \nrequirements_dev.txt. \nIf you want to run semgrep rules use a python environment with version higher than 3.6 and run\n\n```\npip install semgrep\n```\n\nAfterwards you can just call the tox environment with for example \n\n```\ntox -e py37-semgrep\n```\n\n### Running Logprep\n\nDepending on how you have installed Logprep you have different choices to run Logprep as well.  \nIf you have installed it via PyPI or the Github Development release just run:\n\n```\nlogprep $CONFIG\n```\n\nIf you have installed Logprep via cloning the repository then you should run it via:\n\n```\nPYTHONPATH=\".\" python3 logprep/run_logprep.py $CONFIG\n```\n\nWhere `$CONFIG` is the path to a configuration file (see the documentation about the \n[configuration](https://logprep.readthedocs.io/en/latest/user_manual/configuration/index.html)).\nThe next sections all assume an installation via pip\n\n### Verifying Configuration\n\nThe following command can be executed to verify the configuration file without having to run Logprep:\n\n```\nlogprep --verify-config $CONFIG\n```\n\nWhere `$CONFIG` is the path to a configuration file (see the documentation about the \n[configuration](https://logprep.readthedocs.io/en/latest/user_manual/configuration/index.html)).\n\n### Validating Labeling-Schema and Rules\n\nThe following command can be executed to validate the schema and the rules:\n\n```\nlogprep --validate-rules $CONFIG\n```\n\nWhere `$CONFIG` is the path to a configuration file (see the documentation about the \n[configuration](https://logprep.readthedocs.io/en/latest/user_manual/configuration/index.html)).\n\nAlternatively, the validation can be performed directly. Assuming you have cloned the repository \nfrom git. \n\n```\nPYTHONPATH=\".\" python3 logprep/util/schema_and_rule_checker.py --labeling-schema $LABELING_SCHEMA --labeling-rules $LABELING_RULES\n```\n\nWhere `$LABELING_SCHEMA` is the path to a labeling-schema (JSON file) and `$LABELING_RULES` is \nthe path to a directory with rule files (JSON/YML files, see Rules.md, subdirectories \nare permitted)\n\nAnalogously, `--normalization-rules` and `--pseudonymizer-rules` can be used.\n\nValidation does also perform a verification of the pipeline section of the Logprep configuration.\n\n### Reload the Configuration\n\nTo change the configuration of Logprep it is not needed to restart Logprep entirely.\nInstead, it can be issued to reload the configuration. \nFor this, the signal `SIGUSR1` must be send to the Logprep process.\n\nIf the configuration does not pass a consistency check, then an error message is logged and \nLogprep keeps running with the previous configuration.\nThe configuration should be then checked and corrected on the basis of the error message.\n\n## Docker Quickstart Environment\n\nLogprep was designed to work with the Elastic Stack and Kafka.\nThis repository comes with a docker-compose file that builds a pre-configured Elastic Stack with \nKafka and Logprep.\nTo get it running docker and docker-compose (version >= 1.28) must be first installed.\nThe docker-compose file is located in the directory quickstart.\n\n### Running the Test Environment\n\nBefore running, docker-compose `sysctl -w vm.max_map_count=262144` must be executed.\nOtherwise, Elasticsearch is not properly started.\nThe environment can either be started with a Logprep container or without one.\nTo start it without Logprep, the command `docker-compose up -d` can be executed from within \nthe quickstart directory.\nIt starts and connects Kafka, ZooKeeper, Logstash, Elasticsearch and Kibana.\nLogprep can be then started from the host once the quickstart environment is running and \nfully loaded.\nThe Logprep configuration file `quickstart/exampledata/config/pipeline.yml` can be used to \nconnect to the quickstart environment.\nTo start the whole environment including a Logprep container, the \ncommand `docker-compose --profile logprep up -d` can be executed.\n\n### Interacting with the Quickstart Environment\n\nIt is now possible to write JSON events into Kafka and read the processed events in Kibana.\n\nOnce everything has started, Kibana can be accessed by a web-browser with the \naddress `127.0.0.1:5601`.\nKafka can be accessed with the console producer and consumer from Kafka with the \naddress `127.0.0.1:9092` or from within the docker container `Kafka`.\nThe table below shows which ports have been exposed on localhost for the services.\n\n#### Table of Ports for Services\n\n|          | Kafka | ZooKeeper | Elasticsearch | Kibana |\n| ---      | ---   | ---       | ---           | ---    |\n| **Port** | 9092  | 2181      | 9200          | 5601   |\n\nThe example rules that are used in the docker instance of Logprep can be found \nin `quickstart/exampledata/rules`.\nExample events that trigger for the example rules can be found in \n`quickstart/exampledata/input_logdata/test_input.jsonl`.\nThese events can be added to Kafka with the Kafka console producer within the Kafka container by \nexecuting the following command:\n\n`(docker exec -i kafka /opt/kafka/bin/kafka-console-producer.sh --bootstrap-server 127.0.0.1:9092 --topic consumer) < exampledata/input_logdata/test_input.jsonl`\n\nOnce the events have been processed for the first time, the new indices *processed*, *sre* \nand *pseudonyms* should be available in Kibana.\n\nThe environment can be stopped via `docker-compose down`.\n\n## Documentation\n\nThe documentation for Logprep is online at https://logprep.readthedocs.io/en/latest/ or it can \nbe built locally via tox (install via `pip3 install tox`). \nBuilding the documentation is done by executing the following command from within \nthe project root directory:\n\n```\ntox -e py36-docs\n```\n\nA HTML documentation can be then found in `doc/_build/html/index.html`.\n\n## Contributing\n\nEvery contribution is highly appreciated.\nIf you have ideas or improvements feel free to create a fork and open a pull requests. \nIssues and engagement in open discussions are also welcome.\n\n## License\n\nLogprep is distributed under the LGPL-2.1 License. See LICENSE file for more information.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/fkie-cad/Logprep",
    "keywords": "",
    "license": "LGPL-2.1 license",
    "maintainer": "",
    "maintainer_email": "",
    "name": "logprep",
    "package_url": "https://pypi.org/project/logprep/",
    "platform": null,
    "project_url": "https://pypi.org/project/logprep/",
    "project_urls": {
      "Documentation": "https://logprep.readthedocs.io/en/latest/",
      "Homepage": "https://github.com/fkie-cad/Logprep"
    },
    "release_url": "https://pypi.org/project/logprep/3.2.0/",
    "requires_dist": [
      "setuptools",
      "luqum",
      "jsonref",
      "pyyaml",
      "ruamel.yaml",
      "confluent-kafka (==1.8.2)",
      "pycryptodome",
      "urlextract",
      "tldextract",
      "geoip2",
      "pygrok",
      "colorama",
      "python-dateutil",
      "pytz",
      "numpy",
      "arrow",
      "prometheus-client",
      "mysql-connector-python",
      "backports.cached-property ; python_version < \"3.8\"",
      "hyperscan (==0.1.5) ; python_version >= \"3.6\" and python_version <= \"3.8\" and sys_platform == \"linux\" and platform_machine == \"x86_64\"",
      "hyperscan (>=0.2.0) ; python_version >= \"3.9\" and sys_platform == \"linux\" and platform_machine == \"x86_64\""
    ],
    "requires_python": ">=3.6",
    "summary": "Logprep allows to collect, process and forward log messages from various data sources.",
    "version": "3.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17415259,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bf6d8fd3e063bea8b23b1d7f2635e8e44695ae6c64b0dae8ff4ff5db05aba051",
        "md5": "9410b2a95a9d6e2bf904ebe6d2c5cc6c",
        "sha256": "5cca5f97da1f1c69e9154f6e4a11172eaf3187ced75d51e5daabc95e362fafdf"
      },
      "downloads": -1,
      "filename": "logprep-3.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "9410b2a95a9d6e2bf904ebe6d2c5cc6c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 298263,
      "upload_time": "2022-08-08T11:16:24",
      "upload_time_iso_8601": "2022-08-08T11:16:24.021748Z",
      "url": "https://files.pythonhosted.org/packages/bf/6d/8fd3e063bea8b23b1d7f2635e8e44695ae6c64b0dae8ff4ff5db05aba051/logprep-3.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c5158b105681071d9db39e13720ff52d4be02fef0dd0082ecf3a1452240e8dc2",
        "md5": "b119b177ea043c7c817a70dddfdec518",
        "sha256": "51188b1c92ddb485581bc9f7c61a583bb97075b2cc2946944701a4b93675e216"
      },
      "downloads": -1,
      "filename": "logprep-3.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "b119b177ea043c7c817a70dddfdec518",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 224500,
      "upload_time": "2022-08-08T11:16:25",
      "upload_time_iso_8601": "2022-08-08T11:16:25.487426Z",
      "url": "https://files.pythonhosted.org/packages/c5/15/8b105681071d9db39e13720ff52d4be02fef0dd0082ecf3a1452240e8dc2/logprep-3.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}