{
  "info": {
    "author": "NVIDIA Corporation",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Software Development :: Libraries"
    ],
    "description": "# [Merlin Systems](https://github.com/NVIDIA-Merlin/systems)\n\n![PyPI - Python Version](https://img.shields.io/pypi/pyversions/merlin-systems)\n[![PyPI version shields.io](https://img.shields.io/pypi/v/merlin-systems.svg)](https://pypi.python.org/pypi/merlin-systems/)\n![GitHub License](https://img.shields.io/github/license/NVIDIA-Merlin/systems)\n[![Documentation](https://img.shields.io/badge/documentation-blue.svg)](https://nvidia-merlin.github.io/systems/main/README.html)\n\nMerlin Systems provides tools for combining recommendation models with other elements of production recommender systems like feature stores, nearest neighbor search, and exploration strategies into end-to-end recommendation pipelines that can be served with [Triton Inference Server](https://github.com/triton-inference-server/server).\n\n## Quickstart\n\nMerlin Systems uses the Merlin Operator DAG API, the same API used in [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular) for feature engineering, to create serving ensembles. To combine a feature engineering workflow and a Tensorflow model into an inference pipeline:\n\n```python\nimport tensorflow as tf\nfrom nvtabular.workflow import Workflow\nfrom merlin.systems.dag import Ensemble, PredictTensorflow, TransformWorkflow\n\n# Load saved NVTabular workflow and TensorFlow model\nworkflow = Workflow.load(nvtabular_workflow_path)\nmodel = tf.keras.models.load_model(tf_model_path)\n\n# Remove target/label columns from feature processing workflowk\nworkflow = workflow.remove_inputs([<target_columns>])\n\n# Define ensemble pipeline\npipeline = (\n\tworkflow.input_schema.column_names >>\n\tTransformWorkflow(workflow) >>\n\tPredictTensorflow(model)\n)\n\n# Export artifacts to disk\nensemble = Ensemble(pipeline, workflow.input_schema)\nensemble.export(export_path)\n```\n\nAfter you export your ensemble, you reference the directory to run an instance of Triton Inference Server to host your ensemble.\n\n```shell\ntritonserver --model-repository=/export_path/\n```\n\nRefer to the [Merlin Systems Example Notebooks](./examples/) for a notebook that serves a ranking models ensemble.\nThe notebook shows how to deploy the ensemble and demonstrates sending requests to Triton Inference Server.\n\n## Building a Four-Stage Recommender Pipeline\n\nMerlin Systems can also build more complex serving pipelines that integrate multiple models and external tools (like feature stores and nearest neighbor search):\n\n```python\n# Load artifacts for the pipeline\nretrieval_model = tf.keras.models.load_model(retrieval_model_path)\nranking_model = tf.keras.models.load_model(ranking_model_path)\nfeature_store = feast.FeatureStore(feast_repo_path)\n\n# Define the fields expected in requests\nrequest_schema = Schema([\n    ColumnSchema(\"user_id\", dtype=np.int32),\n])\n\n# Fetch user features, use them to a compute user vector with retrieval model,\n# and find candidate items closest to the user vector with nearest neighbor search\nuser_features = request_schema.column_names >> QueryFeast.from_feature_view(\n    store=feature_store, view=\"user_features\", column=\"user_id\"\n)\n\nretrieval = (\n    user_features\n    >> PredictTensorflow(retrieval_model_path)\n    >> QueryFaiss(faiss_index_path, topk=100)\n)\n\n# Filter out candidate items that have already interacted with\n# in the current session and fetch item features for the rest\nfiltering = retrieval[\"candidate_ids\"] >> FilterCandidates(\n    filter_out=user_features[\"movie_ids\"]\n)\n\nitem_features = filtering >> QueryFeast.from_feature_view(\n    store=feature_store, view=\"movie_features\", column=\"filtered_ids\",\n)\n\n# Join user and item features for the candidates and use them to predict relevance scores\ncombined_features = item_features >> UnrollFeatures(\n    \"movie_id\", user_features, unrolled_prefix=\"user\"\n)\n\nranking = combined_features >> PredictTensorflow(ranking_model_path)\n\n# Sort candidate items by relevance score with some randomized exploration\nordering = combined_features[\"movie_id\"] >> SoftmaxSampling(\n    relevance_col=ranking[\"output\"], topk=10, temperature=20.0\n)\n\n# Create and export the ensemble\nensemble = Ensemble(ordering, request_schema)\nensemble.export(\"./ensemble\")\n```\n\n## Installation\n\nMerlin Systems requires Triton Inference Server and Tensorflow. The simplest setup is to use the [Merlin Tensorflow Inference Docker container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow-inference), which has both pre-installed.\n\n### Installing Merlin Systems Using Pip\n\nYou can install Merlin Systems with `pip`:\n\n```shell\npip install merlin-systems\n```\n\n### Installing Merlin Systems from Source\n\nMerlin Systems can be installed from source by cloning the GitHub repository and running `setup.py`\n\n```shell\ngit clone https://github.com/NVIDIA-Merlin/systems.git\ncd systems && python setup.py develop\n```\n\n### Running Merlin Systems from Docker\n\nMerlin Systems is installed on multiple Docker containers that are available from the NVIDIA GPU Cloud (NGC) catalog.\nThe following table lists the containers that include Triton Inference Server for use with Merlin.\n\n| Container Name      | Container Location                                                                     | Functionality                                                                      |\n| ------------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |\n| `merlin-hugectr`    | <https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-hugectr>    | Merlin frameworks, HugeCTR, and Triton Inference Server                            |\n| `merlin-tensorflow` | <https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow> | Merlin frameworks selected for only Tensorflow support and Triton Inference Server |\n\nIf you want to add support for GPU-accelerated workflows, you will first need to install the [NVIDIA Container Toolkit](https://github.com/NVIDIA/nvidia-docker) to provide GPU support for Docker. You can use the NGC links referenced in the table above to obtain more information about how to launch and run these containers.\n\n## Feedback and Support\n\nTo report bugs or get help, please [open an issue](https://github.com/NVIDIA-Merlin/NVTabular/issues/new/choose).\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/NVIDIA-Merlin/systems",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "merlin-systems",
    "package_url": "https://pypi.org/project/merlin-systems/",
    "platform": null,
    "project_url": "https://pypi.org/project/merlin-systems/",
    "project_urls": {
      "Homepage": "https://github.com/NVIDIA-Merlin/systems"
    },
    "release_url": "https://pypi.org/project/merlin-systems/23.2.0/",
    "requires_dist": null,
    "requires_python": ">=3.8",
    "summary": "",
    "version": "23.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17209351,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "04f17300f42bd28928b3157fc7f81c0d67f3cff0a1e1ea4dd79e62d73baecd9b",
        "md5": "b290a99456e5e2105efc908ec9a5c636",
        "sha256": "f0cad61a3726831793e1f338b9abbc0a1fa796f54b619fab07e57ee5e0fd5ff2"
      },
      "downloads": -1,
      "filename": "merlin-systems-23.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "b290a99456e5e2105efc908ec9a5c636",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 80945,
      "upload_time": "2023-03-08T16:29:58",
      "upload_time_iso_8601": "2023-03-08T16:29:58.055328Z",
      "url": "https://files.pythonhosted.org/packages/04/f1/7300f42bd28928b3157fc7f81c0d67f3cff0a1e1ea4dd79e62d73baecd9b/merlin-systems-23.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}