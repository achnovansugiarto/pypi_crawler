{
  "info": {
    "author": "Muhammad N. Fawi",
    "author_email": "m.noor.fawi@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# lzhw\n##### Compression library for data frames and tabular data files, csv, excel etc.\n\n![lzhw logo](./img/lzhw_logo.jpg)\n\n[![](https://img.shields.io/badge/docs-latest-blue.svg)](https://mnoorfawi.github.io/lzhw/) \n[![Build Status](https://travis-ci.com/MNoorFawi/lzhw.svg?branch=master)](https://travis-ci.com/MNoorFawi/lzhw)\n\n**Compression** library to compress big lists and/or pandas dataframes using an **optimized algorithm (lzhw)** developed from Lempel-Ziv, Huffman and LZ-Welch techniques.\n\n**lzhw** has a command line tool that can be downloaded from [here](https://drive.google.com/file/d/1CBu7Adb5CHZUwhANa_i8Es0-8jSWAmiC/view?usp=sharing) and can work from command line with no prior python installation.\n\n**Manual on how to use it available [here](https://mnoorfawi.github.io/lzhw/5%20Using%20the%20lzhw%20command%20line%20tool/)**.\n\nIt works on Windows and soon a Mac version will be available.\n\n#### Full documentation can be found [here](https://mnoorfawi.github.io/lzhw/)\n\n**Data Frames compression and decompression works in parallel**. \n## How lzhw Works\nThe library's main goal is to compress data frames, excel and csv files so that they consume less space to overcome memory errors.\nAlso to enable dealing with large files that can cause memory errors when reading them in python or that cause slow operations.\nWith **lzhw**, we can read compressed files and do operations **column by column** only on columns that we are interesred in. \n\nThe algorithm is a mix of the famous **lempel-ziv and huffman coding** algorithm with some use of **lempel-ziv-welch** algorithm.\nThe algorithm starts with an input stream for example this one:\n```python\nexample = [\"to\", \"be\", \"or\", \"not\", \"to\", \"be\", \"or\", \"to\", \"be\", \"or\", \"not\"] * 2\nprint(\"\".join(example))\n# tobeornottobeortobeornottobeornottobeortobeornot\n```\n**lzhw** uses [**lempel-ziv77**](https://en.wikipedia.org/wiki/LZ77_and_LZ78) to discover repeated sequences in the stream and construct *triplets*, in that format **<offset,length,literal>**. \nWhere *offset* is how many steps should we return back word to find the beginning of the current sequence and *length* is how many steps should we move and *literal* is the next value after the sequence.\n\nThen we will have 3 shorter lists representing the stream, where [**Huffman Coding**](https://en.wikipedia.org/wiki/Huffman_coding) can come to the game encoding them.\n\nThe function that performs lempel-ziv and returning the triplets called **lz77_compress**.\n```python\nimport lzhw\nlz77_ex = lzhw.lz77_compress(example)\nprint(lz77_ex)\n# [(None, None, 'to'), (None, None, 'be'), (None, None, 'or'), \n# (None, None, 'not'), (4, 3, 'to'), (7, 6, 'not'), (11, 6, 'not')]\n```\nHere all the **None**s values are originally \"0s\" but converted to None to save more space.\n\nNow huffman coding will take the offsets list, lengths list and literal list and encode them based on most occurring values to give:\n```python\nlz77_lists = list(zip(*lz77_ex))\nprint(lz77_lists)\n# [(None, None, None, None, 4, 7, 11), \n#  (None, None, None, None, 3, 6, 6), \n#  ('to', 'be', 'or', 'not', 'to', 'not', 'not')]\n\nhuffs = []\nfrom collections import Counter\nfor i in range(len(lz77_lists)):\n    huff = lzhw.huffman_coding(Counter(lz77_lists[i]))\n    huffs.append(huff)\nprint(huffs)\n# [{None: '1', 4: '010', 7: '011', 11: '00'}, {None: '1', 3: '00', 6: '01'}, \n#  {'to': '11', 'be': '100', 'or': '101', 'not': '0'}]\n```\nNow if we encode each value in the triplets with its corresponding value from the huffman dictionary and append everything together we will have:\n```python\nbits = []\nfor i in range(len(huffs)):\n    bit = \"\".join([huffs[i].get(k) for k in lz77_lists[i]])\n    bits.append(bit)\nprint(bits)\n# ['111101001100', '1111000101', '1110010101100']\n\nprint(len(\"\".join(bits)))\n# 35\n```\nWhich has a length of **35** bits only!\n\nThen [Lempel-Ziv-Welch](https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch), **lzw_compress()**, is used to further compress the dictionaries produces by Huffman. \n\nUsing each algorithm alone can give us bigger number of bits, for example, using only huffman coding will give us:\n```python\nhuff_alone = lzhw.huffman_coding(Counter(example))\nprint(huff_alone)\n# {'to': '11', 'be': '01', 'or': '10', 'not': '00'}\n\nhuff_bit = \"\".join([huff_alone.get(k) for k in example])\nprint(huff_bit)\n# 11011000110110110110001101100011011011011000\n\nprint(len(huff_bit))\n# 44\n```\n44 bits, 9 more bit!!! Big deal when dealing with bigger data.\n\nThe techniques may seem similar to the [**DEFLATE**](https://en.wikipedia.org/wiki/DEFLATE) algorithm which uses both lempel-ziv77 and huffman coding, but I am not sure how the huffman coding further compresses the triplets. And also it doesn't use the lempel-ziv-welch for further compression.\n\nAll of the steps can be done at once using **LZHW** class as follows and as shown in the Quick Start section:\n```python\nlzhw_comp = lzhw.LZHW(example)\nprint(lzhw_comp.compressed)\n# (8012, 1989, 15532) # this is how the compressed data looks like and stored\n\nprint(lzhw_comp.sequences) \n# {'offset': {3: None, 10: 4, 11: 7, 4: 11}, \n#  'length': {3: None, 4: 3, 5: 6}, \n#  'literal_str': {7: 321647, 12: 312421, 13: 319090, 2: 163110516}}\n```\n\n## Quick Start\n\n```bash\npip install lzhw\n```\n\n```python\nimport lzhw\n\nsample_data = [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \n               \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \n               \"Rain\", \"Rain\", \"Sunny\", \"Sunny\", \"Overcaste\"]\n\ncompressed = lzhw.LZHW(sample_data)\n## let's see how the compressed object looks like:\nprint(compressed.compressed)\n# (506460, 128794, 112504)\n\n## its size\nprint(compressed.size())\n# 72\n\n## size of original\nfrom sys import getsizeof\nprint(getsizeof(sample_data))\n# 216\n\nprint(compressed.space_saving())\n# space saving from original to compressed is 67%\n\n## Let's decompress and check whether there is any information loss\ndecomp = compressed.decompress()\nprint(decomp == sample_data)\n# True\n```\n\nAs we saw, the LZHW class has saved 67% of the space used to store the original list without any loss. This percentage can get better with bigger data that may have repeated sequences.\nThe class has also some useful helper methods as **space_saving**, **size**, and **decompress()** to revert back to original.\n\nAnother example with numeric data.\n\n```python\nfrom random import sample, choices\n\nnumbers = choices(sample(range(0, 5), 5), k = 20)\ncomp_num = lzhw.LZHW(numbers)\n\nprint(getsizeof(numbers) > comp_num.size())\n# True\n\nprint(numbers == list(map(int, comp_num.decompress()))) ## make it int again\n# True\n\nprint(comp_num.space_saving())\n# space saving from original to compressed is 73%\n```\n\nLet's look at how the compressed object is stored and how it looks like when printed:\nLZHW class has an attribute called **compressed** which is a tuple of integers representing the encoded triplets.\n\n```python\nprint(comp_num.compressed) # how the compressed is saved (as tuple of 3 integers)\n# (8198555, 620206, 3059308)\n```\n\nWe can also write the compressed data to files using **save_to_file** method, \nand read it back and decompress it using **decompress_from_file** function.\n\n```python\nstatus = [\"Good\", \"Bad\", \"Bad\", \"Bad\", \"Good\", \"Good\", \"Average\", \"Average\", \"Good\",\n          \"Average\", \"Average\", \"Bad\", \"Average\", \"Good\", \"Bad\", \"Bad\", \"Good\"]\ncomp_status = lzhw.LZHW(status)\ncomp_status.save_to_file(\"status.txt\")\ndecomp_status = lzhw.decompress_from_file(\"status.txt\")\nprint(status == decomp_status)\n# True\n```\n\n## Compressing DataFrames in Parallel\n\nlzhw doesn't work only on lists, it also compress pandas dataframes and save it into compressed files to decompress them later.\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 1, 2, 2, 1, 3, 4, 4],\n                   \"b\": [\"A\", \"A\", \"B\", \"B\", \"A\", \"C\", \"D\", \"D\"]})\ncomp_df = lzhw.CompressedDF(df)\n# 100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2003.97it/s]\n```\n\nLet's check space saved by compression\n```python\ncomp_space = 0\nfor i in range(len(comp_df.compressed)):\n\tcomp_space += comp_df.compressed[i].size()\n\nprint(comp_space, getsizeof(df))\n# 144 712\n\n## Test information loss\nprint(comp_df.compressed[0].decompress() == list(map(str, df.a)))\n# True\n```\n\n#### Saving and Loading Compressed DataFrames\n\nWith lzhw we can save a data frame into a compressed file and then read it again \nusing **save_to_file** method and **decompress_df_from_file** function.\n\n```python\n## Save to file\ncomp_df.save_to_file(\"comp_df.txt\")\n\n## Load the file\noriginal = lzhw.decompress_df_from_file(\"comp_df.txt\")\n# 100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2004.93it/s]\n\nprint(original)\n#   a  b\n#0  1  A\n#1  1  A\n#2  2  B\n#3  2  B\n#4  1  A\n#5  3  C\n#6  4  D\n#7  4  D\n```\n\n#### Compressing Bigger DataFrames\n\nLet's try to compress a real-world dataframe **german_credit.xlsx** file from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) [1].\n\nOriginal txt file is **219 KB** on desk.\n\n```python\ngc_original = pd.read_excel(\"examples/german_credit.xlsx\")\ncomp_gc = lzhw.CompressedDF(gc_original)\n# 100%|█████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 257.95it/s]\n\n## Compare sizes in Python:\ncomp_space = 0\nfor i in range(len(comp_gc.compressed)):\n\tcomp_space += comp_gc.compressed[i].size()\n\nprint(comp_space, getsizeof(gc_original))\n# 4504 548852\n\nprint(comp_gc.compressed[0].decompress() == list(map(str, gc_original.iloc[:, 0])))\n# True\n```\n\n**Huge space saving, 99%, with no information loss!**\n\nLet's now write the compressed dataframe into a file and compare the sizes of the files.\n\n```python\ncomp_gc.save_to_file(\"gc_compressed.txt\")\n``` \n\nChecking the size of the compressed file, it is **44 KB**. Meaning that in total we saved around **79%**.\nFuture versions will be optimized to save more space.\n\nLet's now check when we reload the file, will we lose any information or not.\n\n```python\n## Load the file\ngc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\")\n# 100%|█████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 259.46it/s]\n\nprint(list(gc_original2.iloc[:, 13]) == list(map(str, gc_original.iloc[:, 13])))\n# True\n\nprint(gc_original.shape == gc_original2.shape)\n# True\n```\n\n**Perfect! There is no information loss at all.**\n\n## (De)Compressing specific columns or rows from a dataframe\n\nWith **lzhw** you can choose what columns you are interested in compressing from a data frame.\n**CompressedDF** class has an argument **selected_cols**.\n```python\ngc_original = pd.read_excel(\"examples/german_credit.xlsx\")\ncomp_gc = lzhw.CompressedDF(gc_original, selected_cols = [0, 3, 4, 7])\n# 100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 401.11it/s]\n``` \nAlso when you have a compressed file that you want to decompress, you don't have to decompress it all, you can choose only specific columns and/or rows to decompress.\nBy this you can deal with large compressed files and do operations **column by column** quickly and **avoid memory errors**\n**decompress_df_from_file** function has the same argument **selected_cols**.\n```python\ngc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\", selected_cols = [0, 4])\n# 100%|████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 3348.53it/s]\n\ngc_original2.head()\n#\tDuration\tAge\n#0\t       6\t67\n#1\t      48\t22\n#2\t      12\t49\n#3\t      42\t45\n#4\t      24\t53\n```\nLet's compare this subset with the original df.\n```python\ngc_original.iloc[:, [0, 4]].head()\n#\tDuration\tAge\n#0\t       6\t67\n#1\t      48\t22\n#2\t      12\t49\n#3\t      42\t45\n#4\t      24\t53\n```\nPerfect!\n\n*selected_cols* has \"all\" as its default value.\n\n**decompress_df_from_file** has another argument which is **n_rows** to specify the number of rows we would like to decompress only.\n\nThe argument's default value is **0** to decompress all data frame, if specified it will decompress from start until desired number of rows.\n```python\ngc_original_subset = lzhw.decompress_df_from_file(\"gc_compressed.txt\", n_rows = 6)\n# 100%|████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 914.21it/s]\n\nprint(gc_original_subset.shape)\n# (6, 62)\n```\n\nThis can be very helpful when reading very big data in chunks of rows and columns to avoid **MemoryError** and to apply **operations** and **online algorithms** **faster**.\n\n```python\ngc_original_subset_smaller = lzhw.decompress_df_from_file(\"gc_compressed.txt\", \n                                                  selected_cols = [1, 4, 8, 9], \n                                                  n_rows = 6)\n# 100%|████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 3267.86it/s]\n\nprint(gc_original_subset_smaller.shape)\n# (6, 4)\n\nprint(gc_original_subset_smaller)\n#   Amount Age ForeignWorker Class\n# 0   1169  67             1  Good\n# 1   5951  22             1   Bad\n# 2   2096  49             1  Good\n# 3   7882  45             1  Good\n# 4   4870  53             1   Bad\n# 5   9055  35             1  Good\n```\n\n## Using the lzhw Command Line Interface\n\nIn **lzhw_cli** folder, there is a python script that can work on command line to compress and decompress files.\n\nHere is the file:\n```bash\n$python lzhw_cli.py\n\nusage: lzhw_cli.py [-h] [-d] -f INPUT -o OUTPUT [-c COLUMNS [COLUMNS ...]]\n                   [-nh]\nlzhw_cli.py: error: the following arguments are required: -f/--input, -o/--output\n```\n\nGetting help to see what it does and its arguments:\n```bash\n$python lzhw_cli.py -h\n\nusage: lzhw_cli.py [-h] [-d] -f INPUT -o OUTPUT [-c COLUMNS [COLUMNS ...]]\n                   [-r ROWS] [-nh]\n\nLZHW is a tabular data compression tool. It is used to compress excel, csv and\nany flat file. Version: 0.0.7\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -d, --decompress      decompress input into output\n  -f INPUT, --input INPUT\n                        input file to be (de)compressed\n  -o OUTPUT, --output OUTPUT\n                        output where to save result\n  -c COLUMNS [COLUMNS ...], --columns COLUMNS [COLUMNS ...]\n                        select specific columns by names or indices (1-based)\n                        to compress or decompress\n  -r ROWS, --rows ROWS  select specific rows to decompress (1-based)\n  -nh, --no-header      skip header / data to be compressed has no header\n```\n\nHow to compress:\n```bash\n$python lzhw_cli.py -f \"file_to_compress\" -o \"output\"\n\ncompressed successfully\n```\n\nHow to decompress:\n```bash\n$python lzhw_cli.py -d -f \"file_to_decompress\" -o \"output\"\n\ndecompressed successfully\n```\n## Helper Functions\n\nAside from the functions and classes discussed, the library also has some more compression functions that can be used as standalone.\n\n#### lz78()\n\n**lz78** which performs the famous **lempel-ziv78** algorithm which differs from lempel-ziv77 in that instead of triplets it creates a dictionary for the previously seen sequences:\n```python\nimport random\nrandom.seed(1311)\nexample = random.choices([\"A\", \"B\", \"C\"], k = 20)\nprint(example)\n#['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', \n# 'A', 'B', 'B', 'C', 'C', 'B', 'C', 'C', 'B']\n\nlz78_comp, symb_dict = lzhw.lz78(example)\nprint(lz78_comp)\n# ['1', '1', 'C', '3', '1', 'A', '3', 'C', '3', 'B', \n#  '7', '1', 'B', '7', 'C', '6', 'C', 'C B']\n\nprint(symb_dict)\n# {'A': '1', 'A C': '2', 'C': '3', 'A A': '4', 'C C': '5', \n#  'C B': '6', 'B': '7', 'A B': '8', 'B C': '9', 'C B C': '10'}\n```\n\n#### huffman_coding()\n\nHuffman Coding function which takes a Counter object and encodes each symbol accordingly.\n```python\nfrom collections import Counter\nhuffs = lzhw.huffman_coding(Counter(example))\nprint(huffs)\n# {'A': '10', 'C': '0', 'B': '11'}\n```\n\n#### lzw_compress() and lzw_decompress()\n\nThey perform [lempel-ziv-welch](https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch) compressing and decompressing\n```python\nprint(lzhw.lzw_compress(\"Hello World\"))\n# 723201696971929295664359987300\n\nprint(lzhw.lzw_decompress(lzhw.lzw_compress(\"Hello World\")))\n# Hello World\n```\n\n#### lz20()\nI wanted to modify the lempel-ziv78 and instead of creating a dictionary and returing the codes in the output compressed stream, I wanted to glue the repeated sequences together to get a shorter list with more repeated sequences to further use it with huffman coding.\n\nI named this function lempel-ziv20 :D:\n```python\nlz20_ex = lzhw.lz20(example)\nprint(lz20_ex)\n# ['A', 'A', 'C', 'C', 'A', 'A', 'C', 'C', 'C', 'B', 'B', \n#  'A', 'B', 'B', 'C', 'C B', 'C', 'C B']\n\nhuff20 = lzhw.huffman_coding(Counter(lz20_ex))\nprint(huff20)\n# {'A': '10', 'C': '0', 'B': '111', 'C B': '110'}\n```\n\nIn data with repeated sequences it will give better huffman dictionaries.\n\n#### lz77_compress() and lz77_decompress()\n\nThe main two functions in the library which apply the lempel-ziv77 algorithm:\n\n```python\nlz77_ex = lzhw.lz77_compress(example)\nprint(lz77_ex)\n# [(None, None, 'A'), (1, 1, 'C'), (1, 1, 'A'), (4, 3, 'C'), \n#  (None, None, 'B'), (1, 1, 'A'), (3, 2, 'C'), (7, 2, 'C'), (1, 1, 'B')]\n\nlz77_decomp = lzhw.lz77_decompress(lz77_ex)\nprint(lz77_decomp == example)\n# True\n```\n\n##### Reference\n###### \t\t[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/MNoorFawi/lzhw",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lzhw",
    "package_url": "https://pypi.org/project/lzhw/",
    "platform": "",
    "project_url": "https://pypi.org/project/lzhw/",
    "project_urls": {
      "Homepage": "https://github.com/MNoorFawi/lzhw"
    },
    "release_url": "https://pypi.org/project/lzhw/1.1.14/",
    "requires_dist": [
      "pandas",
      "tqdm",
      "joblib"
    ],
    "requires_python": "",
    "summary": "Compression suite for data frames and tabular data files, csv, excel etc.",
    "version": "1.1.14",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7625775,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3d8dec3682e35b2614f39d300083842c11d16ca7d1a8f93cc36dd8b6fbcfd451",
        "md5": "d013dae25105d5604c8ea00145283300",
        "sha256": "42d1624cea870af36f768fbef6cf2662d5b70c57ca2dc48023c81eaae1ec7276"
      },
      "downloads": -1,
      "filename": "lzhw-1.1.14-cp37-cp37m-win_amd64.whl",
      "has_sig": false,
      "md5_digest": "d013dae25105d5604c8ea00145283300",
      "packagetype": "bdist_wheel",
      "python_version": "cp37",
      "requires_python": null,
      "size": 159209,
      "upload_time": "2020-06-19T14:48:17",
      "upload_time_iso_8601": "2020-06-19T14:48:17.758291Z",
      "url": "https://files.pythonhosted.org/packages/3d/8d/ec3682e35b2614f39d300083842c11d16ca7d1a8f93cc36dd8b6fbcfd451/lzhw-1.1.14-cp37-cp37m-win_amd64.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f57a28322a0f6dd8174fbd3b0dcc8b96d6bd47cc7f727df679f2a7d0b5d873c4",
        "md5": "aec33b17033ca6410a59e43ca8e85cd1",
        "sha256": "873990b244b36a4f44bf8aad900c4a66dc4ecc994464fb186506a3fdaa806675"
      },
      "downloads": -1,
      "filename": "lzhw-1.1.14.tar.gz",
      "has_sig": false,
      "md5_digest": "aec33b17033ca6410a59e43ca8e85cd1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 563689,
      "upload_time": "2020-06-19T14:48:19",
      "upload_time_iso_8601": "2020-06-19T14:48:19.665668Z",
      "url": "https://files.pythonhosted.org/packages/f5/7a/28322a0f6dd8174fbd3b0dcc8b96d6bd47cc7f727df679f2a7d0b5d873c4/lzhw-1.1.14.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}