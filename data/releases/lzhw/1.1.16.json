{
  "info": {
    "author": "Muhammad N. Fawi",
    "author_email": "m.noor.fawi@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# lzhw\n##### Compression library for data frames and tabular data files, csv, excel etc.\n\n![lzhw logo](./img/lzhw_logo.jpg)\n\n[![](https://img.shields.io/badge/docs-latest-blue.svg)](https://mnoorfawi.github.io/lzhw/) \n[![Build Status](https://travis-ci.com/MNoorFawi/lzhw.svg?branch=master)](https://travis-ci.com/MNoorFawi/lzhw)\n\n**Compression** library to compress big lists and/or pandas dataframes using an **optimized algorithm (lzhw)** developed from Lempel-Ziv, Huffman and LZ-Welch techniques.\n\n**lzhw** has a command line tool that can be downloaded from [here](https://github.com/MNoorFawi/lzhw/releases/download/v0.0.10/lzhw.exe) and can work from command line with no prior python installation.\n\n**Manual on how to use it available [here](https://mnoorfawi.github.io/lzhw/5%20Using%20the%20lzhw%20command%20line%20tool/)**.\n\nIt works on Windows and soon a Mac version will be available.\n\n## Full documentation can be found [here](https://mnoorfawi.github.io/lzhw/)\n\n**Data Frames compression and decompression can work in parallel**. \n\n## Quick Start\n\n```bash\npip install lzhw\n```\n\n```python\nimport lzhw\n\nsample_data = [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \n               \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \n               \"Rain\", \"Rain\", \"Sunny\", \"Sunny\", \"Overcaste\"]\n\ncompressed = lzhw.LZHW(sample_data)\n## let's see how the compressed object looks like:\nprint(compressed.compressed)\n# (506460, 128794, 112504)\n\n## its size\nprint(compressed.size())\n# 72\n\n## size of original\nfrom sys import getsizeof\nprint(getsizeof(sample_data))\n# 216\n\nprint(compressed.space_saving())\n# space saving from original to compressed is 67%\n\n## Let's decompress and check whether there is any information loss\ndecomp = compressed.decompress()\nprint(decomp == sample_data)\n# True\n```\n\nAs we saw, the LZHW class has saved 67% of the space used to store the original list without any loss. This percentage can get better with bigger data that may have repeated sequences.\nThe class has also some useful helper methods as **space_saving**, **size**, and **decompress()** to revert back to original.\n\nAnother example with numeric data.\n\n```python\nfrom random import sample, choices\n\nnumbers = choices(sample(range(0, 5), 5), k = 20)\ncomp_num = lzhw.LZHW(numbers)\n\nprint(getsizeof(numbers) > comp_num.size())\n# True\n\nprint(numbers == list(map(int, comp_num.decompress()))) ## make it int again\n# True\n\nprint(comp_num.space_saving())\n# space saving from original to compressed is 73%\n```\n\nLet's look at how the compressed object is stored and how it looks like when printed:\nLZHW class has an attribute called **compressed** which is a tuple of integers representing the encoded triplets.\n\n```python\nprint(comp_num.compressed) # how the compressed is saved (as tuple of 3 integers)\n# (8198555, 620206, 3059308)\n```\n\nWe can also write the compressed data to files using **save_to_file** method, \nand read it back and decompress it using **decompress_from_file** function.\n\n```python\nstatus = [\"Good\", \"Bad\", \"Bad\", \"Bad\", \"Good\", \"Good\", \"Average\", \"Average\", \"Good\",\n          \"Average\", \"Average\", \"Bad\", \"Average\", \"Good\", \"Bad\", \"Bad\", \"Good\"]\ncomp_status = lzhw.LZHW(status)\ncomp_status.save_to_file(\"status.txt\")\ndecomp_status = lzhw.decompress_from_file(\"status.txt\")\nprint(status == decomp_status)\n# True\n```\n\n## Compressing DataFrames (in Parallel)\n\nlzhw doesn't work only on lists, it also compress pandas dataframes and save it into compressed files to decompress them later.\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 1, 2, 2, 1, 3, 4, 4],\n                   \"b\": [\"A\", \"A\", \"B\", \"B\", \"A\", \"C\", \"D\", \"D\"]})\ncomp_df = lzhw.CompressedDF(df)\n# 100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2003.97it/s]\n```\n\nLet's check space saved by compression\n```python\ncomp_space = 0\nfor i in range(len(comp_df.compressed)):\n\tcomp_space += comp_df.compressed[i].size()\n\nprint(comp_space, getsizeof(df))\n# 144 712\n\n## Test information loss\nprint(comp_df.compressed[0].decompress() == list(map(str, df.a)))\n# True\n```\n\n#### Saving and Loading Compressed DataFrames\n\nWith lzhw we can save a data frame into a compressed file and then read it again \nusing **save_to_file** method and **decompress_df_from_file** function.\n\n```python\n## Save to file\ncomp_df.save_to_file(\"comp_df.txt\")\n\n## Load the file\noriginal = lzhw.decompress_df_from_file(\"comp_df.txt\")\n# 100%|██████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2004.93it/s]\n\nprint(original)\n#   a  b\n#0  1  A\n#1  1  A\n#2  2  B\n#3  2  B\n#4  1  A\n#5  3  C\n#6  4  D\n#7  4  D\n```\n\n#### Compressing Bigger DataFrames\n\nLet's try to compress a real-world dataframe **german_credit.xlsx** file from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) [1].\n\nOriginal txt file is **219 KB** on desk.\n\nLet's have a look at how to use parallelism in this example:\n\n```python\ngc_original = pd.read_excel(\"examples/german_credit.xlsx\")\ncomp_gc = lzhw.CompressedDF(gc_original, parallel = True, n_jobs = 2) # two CPUs\n# 100%|█████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 257.95it/s]\n\n## Compare sizes in Python:\ncomp_space = 0\nfor i in range(len(comp_gc.compressed)):\n\tcomp_space += comp_gc.compressed[i].size()\n\nprint(comp_space, getsizeof(gc_original))\n# 4504 548852\n\nprint(comp_gc.compressed[0].decompress() == list(map(str, gc_original.iloc[:, 0])))\n# True\n```\n\n**Huge space saving, 99%, with no information loss!**\n\nLet's now write the compressed dataframe into a file and compare the sizes of the files.\n\n```python\ncomp_gc.save_to_file(\"gc_compressed.txt\")\n``` \n\nChecking the size of the compressed file, it is **44 KB**. Meaning that in total we saved around **79%**.\nFuture versions will be optimized to save more space.\n\nLet's now check when we reload the file, will we lose any information or not.\n\n```python\n## Load the file\ngc_original2 = lzhw.decompress_df_from_file(\"gc_compressed.txt\")\n# 100%|█████████████████████████████████████████████████████████████████████████████████| 62/62 [00:00<00:00, 259.46it/s]\n\nprint(list(gc_original2.iloc[:, 13]) == list(map(str, gc_original.iloc[:, 13])))\n# True\n\nprint(gc_original.shape == gc_original2.shape)\n# True\n```\n\n**Perfect! There is no information loss at all.**\n\nWith **lzhw** also you can choose what columns you are interested in compressing from a data frame.\n**CompressedDF** class has an argument **selected_cols**. And how many rows you want to decompress with **n_rows** argument. \n\nPlease see [documentation](https://mnoorfawi.github.io/lzhw/) for deeper look\n\n## LZHW Comparison with joblib algorithms\n\nI love [joblib](https://joblib.readthedocs.io/en/latest/index.html). I usually use it for **parallelism** for its great performance coming with a smooth simplicity.\n\nI once saw this [article](https://joblib.readthedocs.io/en/latest/auto_examples/compressors_comparison.html#sphx-glr-auto-examples-compressors-comparison-py) in its documentation and it is about measuring the performance between different compressors available in it.\n\nBecause I am developing a compression library, I wanted to extend the code available in this article adding **lzhw** to the comparison, just to know where my library stands.\n\njoblib uses three main techniques in this article **Zlib, LZMA and LZ4**.\n\nI will use [1500000 Sales Records Data](http://eforexcel.com/wp/wp-content/uploads/2017/07/1500000%20Sales%20Records.zip).\n\n**We will look at Compression and Decompression Duration and The compressed file sizes.**\n\n*The downloaded compressed file is 53MB on the websites*\n\nI will reproduce the code in joblib documentation\n```python\ndata = pd.read_csv(\"1500000 Sales Records.csv\")\nprint(data.shape)\n\npickle_file = './pickle_data.joblib'\nstart = time.time()\nwith open(pickle_file, 'wb') as f:\n    dump(data, f)\nraw_dump_duration = time.time() - start\nprint(\"Raw dump duration: %0.3fs\" % raw_dump_duration)\n\nraw_file_size = os.stat(pickle_file).st_size / 1e6\nprint(\"Raw dump file size: %0.3fMB\" % raw_file_size)\n\nstart = time.time()\nwith open(pickle_file, 'rb') as f:\n    load(f)\nraw_load_duration = time.time() - start\nprint(\"Raw load duration: %0.3fs\" % raw_load_duration)\n\n## ZLIB\nstart = time.time()\nwith open(pickle_file, 'wb') as f:\n    dump(data, f, compress='zlib')\nzlib_dump_duration = time.time() - start\nprint(\"Zlib dump duration: %0.3fs\" % zlib_dump_duration)\n\nzlib_file_size = os.stat(pickle_file).st_size / 1e6\nprint(\"Zlib file size: %0.3fMB\" % zlib_file_size)\n\nstart = time.time()\nwith open(pickle_file, 'rb') as f:\n    load(f)\nzlib_load_duration = time.time() - start\nprint(\"Zlib load duration: %0.3fs\" % zlib_load_duration)\n\n## LZMA\nstart = time.time()\nwith open(pickle_file, 'wb') as f:\n    dump(data, f, compress=('lzma', 3))\nlzma_dump_duration = time.time() - start\nprint(\"LZMA dump duration: %0.3fs\" % lzma_dump_duration)\n\nlzma_file_size = os.stat(pickle_file).st_size / 1e6\nprint(\"LZMA file size: %0.3fMB\" % lzma_file_size)\n\nstart = time.time()\nwith open(pickle_file, 'rb') as f:\n    load(f)\nlzma_load_duration = time.time() - start\nprint(\"LZMA load duration: %0.3fs\" % lzma_load_duration)\n\n## LZ4\nstart = time.time()\nwith open(pickle_file, 'wb') as f:\n    dump(data, f, compress='lz4')\nlz4_dump_duration = time.time() - start\nprint(\"LZ4 dump duration: %0.3fs\" % lz4_dump_duration)\n\nlz4_file_size = os.stat(pickle_file).st_size / 1e6\nprint(\"LZ4 file size: %0.3fMB\" % lz4_file_size)\n\nstart = time.time()\nwith open(pickle_file, 'rb') as f:\n    load(f)\nlz4_load_duration = time.time() - start\nprint(\"LZ4 load duration: %0.3fs\" % lz4_load_duration)\n\n## LZHW\nstart = time.time()\nlzhw_data = lzhw.CompressedDF(data)\nlzhw_data.save_to_file(\"lzhw_data.txt\")\nlzhw_compression_duration = time.time() - start\nprint(\"LZHW compression duration: %0.3fs\" % lzhw_compression_duration)\n\nlzhw_file_size = os.stat(\"lzhw_data.txt\").st_size / 1e6\nprint(\"LZHW file size: %0.3fMB\" % lzhw_file_size)\n\nstart = time.time()\nlzhw_d = lzhw.decompress_df_from_file(\"lzhw_data.txt\", parallel = True, n_jobs = -3)  \n# decompression is slower than compression\nlzhw_d_duration = time.time() - start\nprint(\"LZHW decompression duration: %0.3fs\" % lzhw_d_duration)\n\n# (1500000, 14)\n# Raw dump duration: 1.294s\n# Raw dump file size: 267.591MB\n# Raw load duration: 1.413s\n# Zlib dump duration: 6.583s\n# Zlib file size: 96.229MB\n# Zlib load duration: 2.430s\n# LZMA dump duration: 76.526s\n# LZMA file size: 72.476MB\n# LZMA load duration: 9.240s\n# LZ4 dump duration: 1.984s\n# LZ4 file size: 152.374MB\n# LZ4 load duration: 2.135s\n# LZHW compression duration: 53.958s\n# LZHW file size: 41.816MB\n# LZHW decompression duration: 56.687s\n```\n\nNow let's visualize the new results:\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN = 5\nload_durations = (raw_load_duration, zlib_load_duration,\n                  lzma_load_duration, lz4_load_duration, lzhw_d_duration)\ndump_durations = (raw_dump_duration, zlib_dump_duration,\n                  lzma_dump_duration, lz4_dump_duration, lzhw_compression_duration)\nfile_sizes = (raw_file_size, zlib_file_size, lzma_file_size, lz4_file_size, lzhw_file_size)\nind = np.arange(N)\nwidth = 0.5\n\nplt.figure(1, figsize=(5, 4))\np1 = plt.bar(ind, dump_durations, width)\np2 = plt.bar(ind, load_durations, width, bottom=dump_durations)\nplt.ylabel('Time in seconds')\nplt.title('Compression & Decompression durations\\nof different algorithms')\nplt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\"))\nplt.legend((p1[0], p2[0]), ('Compression duration', 'Decompression duration'))\n```\n\n![](./img/lzhw_duration2.jpg)\n\n```python\nplt.figure(2, figsize=(5, 4))\nplt.bar(ind, file_sizes, width, log=True)\nplt.ylabel('File size in MB')\nplt.xticks(ind, ('Raw', 'Zlib', 'LZMA', \"LZ4\", \"LZHW\"))\nplt.title('Compressed data size\\nof different algorithms')\nfor index, value in enumerate(file_sizes):\n    plt.text(index, value, str(round(value)) + \"MB\")\n```\n\n![](./img/lzhw_size2.jpg)\n\n**By far LZHW outperforms others with acceptable time difference**, especially with all other functionalities it enables to deal with compressed data.\n\n##### Reference\n \t\t[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/MNoorFawi/lzhw",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lzhw",
    "package_url": "https://pypi.org/project/lzhw/",
    "platform": "",
    "project_url": "https://pypi.org/project/lzhw/",
    "project_urls": {
      "Homepage": "https://github.com/MNoorFawi/lzhw"
    },
    "release_url": "https://pypi.org/project/lzhw/1.1.16/",
    "requires_dist": [
      "pandas",
      "tqdm",
      "numpy",
      "joblib"
    ],
    "requires_python": "",
    "summary": "Compression suite for data frames and tabular data files, csv, excel etc.",
    "version": "1.1.16",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7625775,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ea4753677ada8b692d519dad8f6084c85cba5f9fdb63c27ae746da4e877c2e0a",
        "md5": "f98f47bc636f44e1ecc32d1bc0b1521f",
        "sha256": "594f759a52f5e2b88a107396d86119332553c9893665b6534b1a541fb712e876"
      },
      "downloads": -1,
      "filename": "lzhw-1.1.16-cp37-cp37m-win_amd64.whl",
      "has_sig": false,
      "md5_digest": "f98f47bc636f44e1ecc32d1bc0b1521f",
      "packagetype": "bdist_wheel",
      "python_version": "cp37",
      "requires_python": null,
      "size": 227960,
      "upload_time": "2020-06-26T22:59:40",
      "upload_time_iso_8601": "2020-06-26T22:59:40.660695Z",
      "url": "https://files.pythonhosted.org/packages/ea/47/53677ada8b692d519dad8f6084c85cba5f9fdb63c27ae746da4e877c2e0a/lzhw-1.1.16-cp37-cp37m-win_amd64.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4253f2b046f194117b9797d5e64964ba3ac879532b49f01c3e9c81d2bd015231",
        "md5": "162fe8ebd0f9234ee7e0b0ef09a644d9",
        "sha256": "9d27a2c12cea2f854973d42035269802dbc607e4ee6420ea3af111422c83154a"
      },
      "downloads": -1,
      "filename": "lzhw-1.1.16.tar.gz",
      "has_sig": false,
      "md5_digest": "162fe8ebd0f9234ee7e0b0ef09a644d9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 389437,
      "upload_time": "2020-06-26T22:59:43",
      "upload_time_iso_8601": "2020-06-26T22:59:43.084558Z",
      "url": "https://files.pythonhosted.org/packages/42/53/f2b046f194117b9797d5e64964ba3ac879532b49f01c3e9c81d2bd015231/lzhw-1.1.16.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}