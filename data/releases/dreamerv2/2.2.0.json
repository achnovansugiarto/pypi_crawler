{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Topic :: Games/Entertainment",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "**Status:** Stable release\n\n[![PyPI](https://img.shields.io/pypi/v/dreamerv2.svg)](https://pypi.python.org/pypi/dreamerv2/#history)\n\n# Mastering Atari with Discrete World Models\n\nImplementation of the [DreamerV2][website] agent in TensorFlow 2. Training\ncurves for all 55 games are included.\n\n<p align=\"center\">\n<img width=\"90%\" src=\"https://imgur.com/gO1rvEn.gif\">\n</p>\n\nIf you find this code useful, please reference in your paper:\n\n```\n@article{hafner2020dreamerv2,\n  title={Mastering Atari with Discrete World Models},\n  author={Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},\n  journal={arXiv preprint arXiv:2010.02193},\n  year={2020}\n}\n```\n\n[website]: https://danijar.com/dreamerv2\n\n## Method\n\nDreamerV2 is the first world model agent that achieves human-level performance\non the Atari benchmark. DreamerV2 also outperforms the final performance of the\ntop model-free agents Rainbow and IQN using the same amount of experience and\ncomputation. The implementation in this repository alternates between training\nthe world model, training the policy, and collecting experience and runs on a\nsingle GPU.\n\n![World Model Learning](https://imgur.com/GRC9QAw.png)\n\nDreamerV2 learns a model of the environment directly from high-dimensional\ninput images. For this, it predicts ahead using compact learned states. The\nstates consist of a deterministic part and several categorical variables that\nare sampled. The prior for these categoricals is learned through a KL loss. The\nworld model is learned end-to-end via straight-through gradients, meaning that\nthe gradient of the density is set to the gradient of the sample.\n\n![Actor Critic Learning](https://imgur.com/wH9kJ2O.png)\n\nDreamerV2 learns actor and critic networks from imagined trajectories of latent\nstates. The trajectories start at encoded states of previously encountered\nsequences. The world model then predicts ahead using the selected actions and\nits learned state prior. The critic is trained using temporal difference\nlearning and the actor is trained to maximize the value function via reinforce\nand straight-through gradients.\n\nFor more information:\n\n- [Google AI Blog post](https://ai.googleblog.com/2021/02/mastering-atari-with-discrete-world.html)\n- [Project website](https://danijar.com/dreamerv2/)\n- [Research paper](https://arxiv.org/pdf/2010.02193.pdf)\n\n## Using the Package\n\nThe easiest way to run DreamerV2 on new environments is to install the package\nvia `pip3 install dreamerv2`. The code automatically detects whether the\nenvironment uses discrete or continuous actions. Here is a usage example that\ntrains DreamerV2 on the MiniGrid environment:\n\n```python\nimport gym\nimport gym_minigrid\nimport dreamerv2.api as dv2\n\nconfig = dv2.defaults.update({\n    'logdir': '~/logdir/minigrid',\n    'log_every': 1e3,\n    'train_every': 10,\n    'prefill': 1e5,\n    'actor_ent': 3e-3,\n    'loss_scales.kl': 1.0,\n    'discount': 0.99,\n}).parse_flags()\n\nenv = gym.make('MiniGrid-DoorKey-6x6-v0')\nenv = gym_minigrid.wrappers.RGBImgPartialObsWrapper(env)\ndv2.train(env, config)\n```\n\n## Manual Instructions\n\nTo modify the DreamerV2 agent, clone the repository and follow the instructions\nbelow. There is also a Dockerfile available, in case you do not want to install\nthe dependencies on your system.\n\nGet dependencies:\n\n```sh\npip3 install tensorflow==2.6.0 tensorflow_probability ruamel.yaml 'gym[atari]' dm_control\n```\n\nTrain on Atari:\n\n```sh\npython3 dreamerv2/train.py --logdir ~/logdir/atari_pong/dreamerv2/1 \\\n  --configs atari --task atari_pong\n```\n\nTrain on DM Control:\n\n```sh\npython3 dreamerv2/train.py --logdir ~/logdir/dmc_walker_walk/dreamerv2/1 \\\n  --configs dmc --task dmc_walker_walk\n```\n\nMonitor results:\n\n```sh\ntensorboard --logdir ~/logdir\n```\n\nGenerate plots:\n\n```sh\npython3 common/plot.py --indir ~/logdir --outdir ~/plots \\\n  --xaxis step --yaxis eval_return --bins 1e6\n```\n\n## Docker Instructions\n\nThe [Dockerfile](https://github.com/danijar/dreamerv2/blob/main/Dockerfile)\nlets you run DreamerV2 without installing its dependencies in your system. This\nrequires you to have Docker with GPU access set up.\n\nCheck your setup:\n\n```sh\ndocker run -it --rm --gpus all tensorflow/tensorflow:2.4.2-gpu nvidia-smi\n```\n\nTrain on Atari:\n\n```sh\ndocker build -t dreamerv2 .\ndocker run -it --rm --gpus all -v ~/logdir:/logdir dreamerv2 \\\n  python3 dreamerv2/train.py --logdir /logdir/atari_pong/dreamerv2/1 \\\n    --configs atari --task atari_pong\n```\n\nTrain on DM Control:\n\n```sh\ndocker build -t dreamerv2 . --build-arg MUJOCO_KEY=\"$(cat ~/.mujoco/mjkey.txt)\"\ndocker run -it --rm --gpus all -v ~/logdir:/logdir dreamerv2 \\\n  python3 dreamerv2/train.py --logdir /logdir/dmc_walker_walk/dreamerv2/1 \\\n    --configs dmc --task dmc_walker_walk\n```\n\n## Tips\n\n- **Efficient debugging.** You can use the `debug` config as in `--configs\natari debug`. This reduces the batch size, increases the evaluation\nfrequency, and disables `tf.function` graph compilation for easy line-by-line\ndebugging.\n\n- **Infinite gradient norms.** This is normal and described under loss scaling in\nthe [mixed precision][mixed] guide. You can disable mixed precision by passing\n`--precision 32` to the training script. Mixed precision is faster but can in\nprinciple cause numerical instabilities.\n\n- **Accessing logged metrics.** The metrics are stored in both TensorBoard and\nJSON lines format. You can directly load them using `pandas.read_json()`. The\nplotting script also stores the binned and aggregated metrics of multiple runs\ninto a single JSON file for easy manual plotting.\n\n[mixed]: https://www.tensorflow.org/guide/mixed_precision",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://github.com/danijar/dreamerv2",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dreamerv2",
    "package_url": "https://pypi.org/project/dreamerv2/",
    "platform": "",
    "project_url": "https://pypi.org/project/dreamerv2/",
    "project_urls": {
      "Homepage": "http://github.com/danijar/dreamerv2"
    },
    "release_url": "https://pypi.org/project/dreamerv2/2.2.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Mastering Atari with Discrete World Models",
    "version": "2.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12303995,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "54f904ced18ef912b55f267bacc069b9a6b04801f4e77df33ace6afc62351037",
        "md5": "3a094b267d5657fd07261fed6f3c810e",
        "sha256": "e186f0ae460a11c7f934a1cf6d55766988a0287538e73f3648fb16c7d6a0adf7"
      },
      "downloads": -1,
      "filename": "dreamerv2-2.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "3a094b267d5657fd07261fed6f3c810e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 41862,
      "upload_time": "2021-12-14T20:46:34",
      "upload_time_iso_8601": "2021-12-14T20:46:34.719368Z",
      "url": "https://files.pythonhosted.org/packages/54/f9/04ced18ef912b55f267bacc069b9a6b04801f4e77df33ace6afc62351037/dreamerv2-2.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}