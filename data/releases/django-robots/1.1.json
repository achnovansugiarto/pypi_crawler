{
  "info": {
    "author": "Jannis Leidel",
    "author_email": "jannis@leidel.info",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: Web Environment",
      "Framework :: Django",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python"
    ],
    "description": "=======================================\nRobots exclusion application for Django\n=======================================\n\nThis is a basic Django application to manage robots.txt files following the\n`robots exclusion protocol`_, complementing the Django_ `Sitemap contrib app`_.\n\nThe robots exclusion application consists of two database models which are\ntied together with a m2m relationship:\n\n* Rules_\n* URLs_\n\n.. _Django: http://www.djangoproject.com/\n\nInstallation\n============\n\nUse your favorite Python installer to install it from PyPI::\n\n    pip install django-robots\n\nOr get the source from the application site at::\n\n    http://github.com/jezdez/django-robots/\n\nTo install the sitemap app, then follow these steps:\n\n1. Add ``'robots'`` to your INSTALLED_APPS_ setting.\n2. Make sure ``'django.template.loaders.app_directories.Loader'``\n   is in your TEMPLATE_LOADERS_ setting. It's in there by default, so\n   you'll only need to change this if you've changed that setting.\n3. Make sure you've installed the `sites framework`_.\n4. Run the ``syncdb`` or ``migrate`` management command (depening if you're\n   using South or not)\n\n.. _INSTALLED_APPS: http://docs.djangoproject.com/en/dev/ref/settings/#installed-apps\n.. _TEMPLATE_LOADERS: http://docs.djangoproject.com/en/dev/ref/settings/#template-loaders\n.. _sites framework: http://docs.djangoproject.com/en/dev/ref/contrib/sites/\n\nSitemaps\n--------\n\nBy default a ``Sitemap`` statement is automatically added to the resulting\nrobots.txt by reverse matching the URL of the installed `Sitemap contrib app`_.\nThis is especially useful if you allow every robot to access your whole site,\nsince it then gets URLs explicitly instead of searching every link.\n\nTo change the default behaviour to omit the inclusion of a sitemap link,\nchange the ``ROBOTS_USE_SITEMAP`` setting in your Django settings file to::\n\n    ROBOTS_USE_SITEMAP = False\n\nIn case you want to use specific sitemap URLs instead of the one that is\nautomatically discovered, change the ``ROBOTS_SITEMAP_URLS`` setting to::\n\n    ROBOTS_SITEMAP_URLS = [\n        'http://www.example.com/sitemap.xml',\n    ]\n\n.. _Sitemap contrib app: http://docs.djangoproject.com/en/dev/ref/contrib/sitemaps/\n\nInitialization\n==============\n\nTo activate robots.txt generation on your Django site, add this line to your\nURLconf_::\n\n    (r'^robots\\.txt$', include('robots.urls')),\n\nThis tells Django to build a robots.txt when a robot accesses ``/robots.txt``.\nThen, please sync your database to create the necessary tables and create\n``Rule`` objects in the admin interface or via the shell.\n\n.. _URLconf: http://docs.djangoproject.com/en/dev/topics/http/urls/\n.. _sync your database: http://docs.djangoproject.com/en/dev/ref/django-admin/#syncdb\n\nRules\n=====\n\n``Rule`` - defines an abstract rule which is used to respond to crawling web\nrobots, using the `robots exclusion protocol`_, a.k.a. robots.txt.\n\nYou can link multiple URL pattern to allows or disallows the robot identified\nby its user agent to access the given URLs.\n\nThe crawl delay field is supported by some search engines and defines the\ndelay between successive crawler accesses in seconds. If the crawler rate is a\nproblem for your server, you can set the delay up to 5 or 10 or a comfortable\nvalue for your server, but it's suggested to start with small values (0.5-1),\nand increase as needed to an acceptable value for your server. Larger delay\nvalues add more delay between successive crawl accesses and decrease the\nmaximum crawl rate to your web server.\n\nThe `sites framework`_ is used to enable multiple robots.txt per Django instance.\nIf no rule exists it automatically allows every web robot access to every URL.\n\nPlease have a look at the `database of web robots`_ for a full list of\nexisting web robots user agent strings.\n\n.. _robots exclusion protocol: http://en.wikipedia.org/wiki/Robots_exclusion_standard\n.. _'sites' framework: http://www.djangoproject.com/documentation/sites/\n.. _database of web robots: http://www.robotstxt.org/db.html\n\nURLs\n====\n\n``Url`` - defines a case-sensitive and exact URL pattern which is used to\nallow or disallow the access for web robots. Case-sensitive.\n\nA missing trailing slash does also match files which start with the name of\nthe given pattern, e.g., ``'/admin'`` matches ``/admin.html`` too.\n\nSome major search engines allow an asterisk (``*``) as a wildcard to match any\nsequence of characters and a dollar sign (``$``) to match the end of the URL,\ne.g., ``'/*.jpg$'`` can be used to match all jpeg files.\n\nCaching\n=======\n\nYou can optionally cache the generation of the ``robots.txt``. Add or change\nthe ``ROBOTS_CACHE_TIMEOUT`` setting with a value in seconds in your Django\nsettings file::\n\n    ROBOTS_CACHE_TIMEOUT = 60*60*24\n\nThis tells Django to cache the ``robots.txt`` for 24 hours (86400 seconds).\nThe default value is ``None`` (no caching).\n\nChangelog\n=========\n\n1.1 (05/12/2015)\n----------------\n\n- Fixed compatibility to Django 1.7 and 1.8.\n\n- Moved South migrations into different subdirectory so South>=1.0 is needed.\n\n1.0 (01/16/2014)\n----------------\n\n- *BACKWARDS-INCOMPATIBLE* change: The default behaviour of this app has\n  changed to **allow all bots** from the previous opposite behavior.\n\n- Fixed some backward compatibility issues.\n\n- Updated existing translations (Danish, German, French,\n  Portugese (Brasil), Russian).\n\n- Added Greek, Spanish (Spain), Japanese, Dutch, Slovak and Ukrainian\n  translations.\n\n0.9.2 (03/24/2013)\n------------------\n\n- Fixed compatibility with Django 1.5. Thanks, Russell Keith-Magee.\n\n0.9.1 (11/23/2012)\n------------------\n\n- Fixed argument signature in new class based view. Thanks, mkai.\n\n0.9 (11/21/2012)\n----------------\n\n- Deprecated ``ROBOTS_SITEMAP_URL`` setting. Use ``ROBOTS_SITEMAP_URLS``\n  instead.\n\n- Refactored ``rule_list`` view to be class based. django-robots now\n  requires Django >= 1.3.\n\n- Stop returning 404 pages if there are no Rules setup on the site. Instead\n  dissallow access for all robots.\n\n- Added an initial South migration. If you're using South you have to \"fake\"\n  the initial database migration::\n\n     python manage.py migrate --fake robots 0001\n\n- Added initial Sphinx docs.\n\nBugs and feature requests\n=========================\n\nAs always your mileage may vary, so please don't hesitate to send feature\nrequests and bug reports:\n\n    https://github.com/jezdez/django-robots/issues\n\nThanks!",
    "description_content_type": null,
    "docs_url": null,
    "download_url": null,
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/jezdez/django-robots/",
    "keywords": null,
    "license": null,
    "maintainer": null,
    "maintainer_email": null,
    "name": "django-robots",
    "package_url": "https://pypi.org/project/django-robots/",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/django-robots/",
    "project_urls": {
      "Homepage": "https://github.com/jezdez/django-robots/"
    },
    "release_url": "https://pypi.org/project/django-robots/1.1/",
    "requires_dist": null,
    "requires_python": null,
    "summary": "Robots exclusion application for Django, complementing Sitemaps.",
    "version": "1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15421801,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "61b0b293ed5f0e3489be039e31c5fdea262517147250585a125b9be989cb53df",
        "md5": "ea7bcdc78ac6994f4aae3c70663240af",
        "sha256": "11b7ace22d6905513d13d70330c09d674c6883c2fe49af1e8cafec2e0529bfe4"
      },
      "downloads": -1,
      "filename": "django_robots-1.1-py2-none-any.whl",
      "has_sig": true,
      "md5_digest": "ea7bcdc78ac6994f4aae3c70663240af",
      "packagetype": "bdist_wheel",
      "python_version": "py2",
      "requires_python": null,
      "size": 58741,
      "upload_time": "2015-05-12T11:13:30",
      "upload_time_iso_8601": "2015-05-12T11:13:30.187952Z",
      "url": "https://files.pythonhosted.org/packages/61/b0/b293ed5f0e3489be039e31c5fdea262517147250585a125b9be989cb53df/django_robots-1.1-py2-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3f8606df61c727c77b0616a93bdeace1825b56b9bb938d8096a1d2a571be678c",
        "md5": "534d25defea122cf9eb262fde5a608d8",
        "sha256": "cdb541184ed2d50f2e885b755635c94825e1d48bbb78fd37405f31878300d49e"
      },
      "downloads": -1,
      "filename": "django-robots-1.1.tar.gz",
      "has_sig": true,
      "md5_digest": "534d25defea122cf9eb262fde5a608d8",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 28570,
      "upload_time": "2015-05-12T11:13:26",
      "upload_time_iso_8601": "2015-05-12T11:13:26.705549Z",
      "url": "https://files.pythonhosted.org/packages/3f/86/06df61c727c77b0616a93bdeace1825b56b9bb938d8096a1d2a571be678c/django-robots-1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}