{
  "info": {
    "author": "Timoth√© Boulet",
    "author_email": "timothe.boulet0@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Local Python performance measurement\r\n\r\nThis project aims to create an easy-to-use package for measuring the performance of python in any machine, in terms of CPU, multiprocessing and GPU (pytorch with CUDA), and also verify that the GPU is used.\r\n\r\n# Installation\r\n\r\nInstall the requirements in requirements.txt, then install the package:\r\n\r\n```bash\r\npip install -r requirements.txt\r\npip install localperf\r\n```\r\n\r\n\r\n# Usage\r\n\r\nYou can measure the performance of your machine in terms of CPU, multiprocessing and GPU (pytorch only for now) by running the commands below.\r\n\r\nRelevant arguments for visualization are:\r\n- `--plot` : plot the results (default False)\r\n- `--log_dir` [log directory] : directory where to save the results (default no logging)\r\n- `--image_dir` [image directory]: directory where to save the images (default no image saving)\r\n- `--no-progess` : do not show progress bar (default behavior is to show)\r\n\r\n# CPU\r\n\r\n<p align=\"center\">\r\n  <img src=\"assets/cpu.png\" alt=\"CPU performances\" width=\"60%\"/>\r\n</p>\r\n\r\nTo measure the performance of your machine in terms of CPU, run the following command:\r\n```bash\r\npython -m localperf.cpu\r\n```\r\n\r\nRelevant arguments for the benchmark are:\r\n- `log_n_data` [log n data] : maximum number of data to do the benchmark (in log10 scale). The treatment of 1 data is defined as the sum of integers from 1 to 1000 (with a for loop), it is used as a base unit of computation.\r\n- `n_measures` [n measures] : number of measures to do for each data size\r\n\r\n# Parallelization\r\n\r\n<p align=\"center\">\r\n  <img src=\"assets/multiprocessing.png\" alt=\"Parallelization performances\" width=\"60%\"/>\r\n</p>\r\n\r\nTo measure the performance of your machine in terms of parallelization, run the following command:\r\n```bash\r\npython -m localperf.parallel\r\n```\r\n\r\nRelevant arguments for the benchmark are:\r\n- `log_n_data` [log n data] : maximum number of data to do the benchmark (in log10 scale)\r\n- `log2_n_process` [log2 n process] : maximum number of processes to do the benchmark (in log2 scale)\r\n- `n_measures` [n measures] : number of measures to do for each data size\r\n- `lib` [lib] : library to use for parallelization. Default is joblib. Currently supported libraries are multiprocessing (`mp`), joblib (`joblib`) and ray (`ray`). For ray you will need to install it with pip before running the benchmark.\r\n\r\n## Compare parallelization libraries\r\n\r\nTo compare the performances of the different libraries, run the following command:\r\n```bash\r\npython -m localperf.parallel_benchmark\r\n```\r\nThis will compare the performances of multiprocessing, joblib and ray. Relevant arguments are:\r\n- `log_n_data` [log n data] : maximum number of data to do the benchmark (in log10 scale)\r\n- `n_process` [n process] : number of processes to do the benchmark. Default behavior is to use your number of CPUs, given by `multiprocessing.cpu_count()`\r\n- `n_measures` [n measures] : number of measures to do for each data size\r\n\r\n\r\n# GPU (pytorch)\r\n\r\n## Install CUDA for pytorch\r\n\r\n<p align=\"center\">\r\n  <img src=\"assets/gpu_torch.png\" alt=\"GPU with torch performances\" width=\"60%\"/>\r\n</p>\r\n\r\nFirst, install pytorch with CUDA following the instructions on the [pytorch website](https://pytorch.org/get-started/locally/). \r\nIf this code returns `True`, it means pytorch and CUDA are installed and its a good sign your GPU is used but this is not a guarantee:\r\n```python\r\nimport torch\r\nprint(torch.cuda.is_available())\r\n```\r\nYou may use the command `nvidia-smi` to check if your GPU is recognized by the system.\r\n\r\n    nvidia-smi\r\n<p align=\"center\">\r\n  <img src=\"assets/nvidia_smi.png\" alt=\"nvidia-smi sceenshot\" width=\"60%\"/>\r\n</p>\r\nThis give you information about each GPU recognized by the system : the name, the VRAM used. You can run this command in a separate terminal to see the GPU usage during your code :\r\n\r\n    watch -n 0.1 nvidia-smi\r\n\r\n## Measure performance\r\n\r\nTo measure the performance of your machine in terms of GPU, run the following command:\r\n```bash\r\npython -m localperf.gpu_torch\r\n```\r\nRelevant arguments for the benchmark are:\r\n- `log_n_data` [log n data] : maximum number of data to do the benchmark (in log10 scale)\r\n- `n_measures` [n measures] : number of measures to do for each data size\r\n- `n_measures_gpu` [n measures gpu] : number of measures to do for each data size, on the GPU. If not specified, the same number of measures as on the CPU is done.\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/tboulet/Local-Python-Performance",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "localperf",
    "package_url": "https://pypi.org/project/localperf/",
    "platform": null,
    "project_url": "https://pypi.org/project/localperf/",
    "project_urls": {
      "Homepage": "https://github.com/tboulet/Local-Python-Performance"
    },
    "release_url": "https://pypi.org/project/localperf/1.4.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Measure of python performance in local.",
    "version": "1.4.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17059439,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0813802af3027bfd15fed79223dcc14103a3e23e63411101a9288f45db37b0d9",
        "md5": "ac62447350563229adf27431de6eed3b",
        "sha256": "f4142c0542bf3e0bb34e077368566ee71d4ceede5244062606287c2b7cd6e461"
      },
      "downloads": -1,
      "filename": "localperf-1.4.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ac62447350563229adf27431de6eed3b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 13792,
      "upload_time": "2023-02-27T04:18:26",
      "upload_time_iso_8601": "2023-02-27T04:18:26.151767Z",
      "url": "https://files.pythonhosted.org/packages/08/13/802af3027bfd15fed79223dcc14103a3e23e63411101a9288f45db37b0d9/localperf-1.4.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "59182dd5c192e5c48cbd0c63c3df1a3d8fb367dedd0fd21e34ba8bd417878f99",
        "md5": "164c975bb2f60040f2e9aa7d446f6bd7",
        "sha256": "6d29d5a873656deb4f07767a4350b424f9bc50d358063e039651f9927afb26be"
      },
      "downloads": -1,
      "filename": "localperf-1.4.0.tar.gz",
      "has_sig": false,
      "md5_digest": "164c975bb2f60040f2e9aa7d446f6bd7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 9927,
      "upload_time": "2023-02-27T04:18:29",
      "upload_time_iso_8601": "2023-02-27T04:18:29.369441Z",
      "url": "https://files.pythonhosted.org/packages/59/18/2dd5c192e5c48cbd0c63c3df1a3d8fb367dedd0fd21e34ba8bd417878f99/localperf-1.4.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}