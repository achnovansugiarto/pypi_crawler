{
  "info": {
    "author": "Hao Wang and Bas van Stein",
    "author_email": "wangronin@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "![Python](https://img.shields.io/pypi/pyversions/mipego.svg)\n\n![Python](https://img.shields.io/pypi/status/mipego.svg)\n\n![Dependencies](https://img.shields.io/badge/dependencies-up%20to%20date-brightgreen.svg)\n\n[![GitHub Issues](https://img.shields.io/github/issues/wangronin/MIP-EGO.svg)](https://github.com/wangronin/MIP-EGO/issues)\n\n![Contributions welcome](https://img.shields.io/badge/contributions-welcome-orange.svg)\n\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n\n\n\n## Citation\n\n\n\nYou can find our paper on [IEEE Explore](https://ieeexplore.ieee.org/abstract/document/8851720/) and on [Arxiv](https://arxiv.org/abs/1810.05526).  \n\nWhen using MiP-EGO for your research, please cite us as follows:\n\n\n\n    @inproceedings{van2019automatic,\n\n      title={Automatic Configuration of Deep Neural Networks with Parallel Efficient Global Optimization},\n\n      author={van Stein, Bas and Wang, Hao and B{\\\"a}ck, Thomas},\n\n      booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},\n\n      pages={1--7},\n\n      year={2019},\n\n      organization={IEEE}\n\n    }\n\n\n\n## Overview\n\n\n\n**MiP-EGO** *(Mixed integer, Parallel - Efficient Global Optimization)* is an optimization package that can be used to optimize Mixed integer optimization problems. A mixed-integer problem is one where some of the decision variables are constrained to be integer values or categorical values.  \n\nNext to the classical mixed integer problems, Algorithm selection or algorithm parameter optimization can also be seen as a complex mixed-integer problem.\n\n\n\nThe advantage of MiP-EGO is that it uses a surrogate model (the EGO part) to learn from the evaluations it has made so far. Instead of *Gaussian Process Regression* like in standard *EGO*, the MiP-EGO uses Random Forests instead, since Random Forests can handle mixed integer data by default.  \n\nThe P in MiP-EGO stands for parallel, as this implementation has the additional feature that it can evaluate several solutions in parallel, which is extremely handy when an evaluation takes a long time and several machines are available.\n\n\n\nFor example, one use case would be to optimize an expensive (in time) simulation. There are four simulation licenses, so four simulations can be run at the same time. With MiP-EGO, all these four licenses can be fully utilized, speeding up the optimization procedure. Using a novel infill-criteria, the Moment Generating Function Based criterium, multiple points can be selected as candidate solutions. See the following paper for more detail about this criterium:  \n\nWANG, Hao, et al. *A new acquisition function for Bayesian optimization based on the moment-generating function.* In: Systems, Man, and Cybernetics (SMC), 2017 IEEE International Conference on. IEEE, 2017. p. 507-512.\n\n\n\n\n\n#### Async Parallel Optimization of Neural Network Architectures\n\n\n\nMiP-EGO also supports asynchronous parallel optimization to optimize the architecture and parameters of deep neural networks. See Example 2 for more details.\n\n\n\n\n\n## Install\n\n```python\n\npip install mipego\n\n```\n\n\n\n## Usage\n\n\n\nTo use the optimizer you need to define an objective function, the search space and configure the optimizer. Below are two examples that describe most of the functionality.\n\n\n\n### Example - Optimizing A Black-Box Function\n\nIn this example we optimize a mixed integer black box problem.\n\n\n\n```python\n\nimport os\n\nimport numpy as np\n\n\n\n#import our package, the surrogate model and the search space classes\n\nfrom mipego import mipego\n\nfrom mipego.Surrogate import RandomForest\n\nfrom mipego.SearchSpace import ContinuousSpace, NominalSpace, OrdinalSpace\n\n\n\n# The \"black-box\" objective function\n\ndef obj_func(x):\n\n   x_r, x_i, x_d = np.array([x['C_0'],x['C_1']]), x['I'], x['N']\n\n   if x_d == 'OK':\n\n       tmp = 0\n\n   else:\n\n       tmp = 1\n\n   return np.sum(x_r ** 2.) + abs(x_i - 10) / 123. + tmp * 2.\n\n\n\n\n\n# First we need to define the Search Space\n\n# the search space consists of two continues variable\n\n# one ordinal (integer) variable\n\n# and one categorical.\n\nC = ContinuousSpace([-5, 5],'C') * 2 \n\n#here we defined two variables at once using the same lower and upper bounds.\n\n#One with label C_0, and the other with label C_1\n\nI = OrdinalSpace([-100, 100],'I') # one integer variable with label I\n\nN = NominalSpace(['OK', 'A', 'B', 'C', 'D', 'E'], 'N')\n\n\n\n#the search space is simply the product of the above variables\n\nsearch_space = C * I * N\n\n\n\n#next we define the surrogate model and the optimizer.\n\nmodel = RandomForest(levels=search_space.levels)\n\nopt = mipego(search_space, obj_func, model, \n\n                 minimize=True,     #the problem is a minimization problem.\n\n                 max_eval=500,      #we evaluate maximum 500 times\n\n                 infill='EI',       #Expected improvement as criteria\n\n                 n_init_sample=10,  #We start with 10 initial samples\n\n                 n_job=1,         #We evaluate 1 sample using 1 process.\n\n                 optimizer='MIES',  #We use the MIES internal optimizer.\n\n                 verbose=False, random_seed=None)\n\n\n\n\n\n#and we run the optimization.\n\nincumbent, stop_dict = opt.run()\n\n```\n\n\n\n\n\n\n\n### Example 2 - Optimizing A Neural Network\n\nIn this example we optimize a neural network architecture on the MNIST dataset.\n\nThe objective function in this case is [this file](https://github.com/wangronin/BayesianOptimization/blob/master/all-cnn.py) from the root repository directory.  \n\nIn the objective file the neural network architecture is defined and evaluated on the MNIST dataset.   \n\nThe code below shows how to set up the optimizer for this purpose using 4 GPUs asynchronously. \n\n\n\n```python\n\nimport os\n\nimport numpy as np\n\nimport subprocess, sys\n\nfrom subprocess import STDOUT, check_output\n\n\n\n#import our package, the surrogate model and the search space classes\n\nfrom mipego import mipego\n\nfrom mipego.Surrogate import RandomForest\n\nfrom mipego.SearchSpace import ContinuousSpace, NominalSpace, OrdinalSpace\n\n\n\n#some help packages\n\nimport re\n\nimport traceback\n\nimport time\n\n\n\n#first lets define our objective function, \n\n#this is basically calling the file (all-cnn.py) and processes its output.\n\nclass obj_func(object):\n\n    def __init__(self, program):\n\n        self.program = program\n\n        \n\n    def __call__(self, cfg, gpu_no):\n\n        print(\"calling program with gpu \"+str(gpu_no))\n\n        cmd = ['python3', self.program, '--cfg', str(cfg), str(gpu_no)]\n\n        outs = \"\"\n\n        outputval = 0\n\n        try:\n\n            #we use a timeout to cancel very long evaluations.\n\n            outs = str(check_output(cmd,stderr=STDOUT, timeout=40000)) \n\n            outs = outs.split(\"\\\\n\")\n\n            \n\n            outputval = 0\n\n            for i in range(len(outs)-1,1,-1):\n\n                if re.match(\"^\\d+?\\.\\d+?$\", outs[-i]) is not None:\n\n                    print(outs[-i])\n\n                    outputval = -1 * float(outs[-i])\n\n            if np.isnan(outputval):\n\n                outputval = 0 #default to 0.\n\n        except subprocess.CalledProcessError as e:\n\n            #exception handling\n\n            traceback.print_exc()\n\n            print (e.output)\n\n        except:\n\n            print (\"Unexpected error:\")\n\n            traceback.print_exc()\n\n            outputval = 0\n\n        return outputval\n\n\n\n\n\n\n\nobjective = obj_func('./all-cnn.py')\n\nactivation_fun = [\"softmax\",\"sigmoid\"] #activation function of the last layer.\n\nactivation_fun_conv = [\"elu\",\"relu\",\"tanh\",\"sigmoid\",\"selu\"]\n\n\n\n#Next we define the search space.\n\nfilters = OrdinalSpace([10, 600], 'filters') * 7\n\nkernel_size = OrdinalSpace([1, 6], 'k') * 7\n\nstrides = OrdinalSpace([1, 5], 's') * 3\n\nstack_sizes = OrdinalSpace([1, 5], 'stack') * 3\n\nactivation = NominalSpace(activation_fun_conv, \"activation\")  \n\nactivation_dense = NominalSpace(activation_fun, \"activ_dense\") \n\n\n\n# to use step decay or not\n\nstep = NominalSpace([True, False], \"step\")  \n\n#to use global pooling in the end or not.\n\nglobal_pooling = NominalSpace([True, False], \"global_pooling\")\n\n\n\ndrop_out = ContinuousSpace([1e-5, .9], 'dropout') * 4 \n\nlr_rate = ContinuousSpace([1e-4, 1.0e-0], 'lr')      #learning rate\n\nl2_regularizer = ContinuousSpace([1e-5, 1e-2], 'l2') # l2_regularizer\n\n\n\nsearch_space =  stack_sizes * strides * filters *  kernel_size * activation * activation_dense * drop_out * lr_rate * l2_regularizer * step * global_pooling \n\n \n\n#We will use the first 4 GPU's of the system.\n\navailable_gpus = [0,1,2,3] \n\n\n\n# use random forest as the surrogate model \n\nmodel = RandomForest(levels=search_space.levels)\n\n\n\n#now define the optimizer.\n\nopt = mipego(search_space, objective, model, \n\n                 minimize=True, max_eval=500,\n\n                 infill='MGFI', n_init_sample=10, \n\n                 n_job=4, \n\n                 #4 GPU's, all evaluating 1 point at a time.\n\n                 wait_iter=3, optimizer='MIES', \n\n                 verbose=False, random_seed=None,\n\n                 available_gpus=available_gpus)\n\n\n\n\n\n#run\n\nincumbent, stop_dict = opt.run()\n\n```\n\n\n\n\n\n\n\n## Contributing\n\nPlease take a look at our [contributing](https://github.com/wangronin/BayesianOptimization/blob/master/CONTRIBUTING.md) guidelines if you're interested in helping!\n\n\n\n#### Beta Features\n\n- Async GPU execution\n\n- Intermediate files to support restarts / resumes.\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/wangronin/MIP-EGO",
    "keywords": "bayesian optimization ego mixed-integer",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mipego",
    "package_url": "https://pypi.org/project/mipego/",
    "platform": "",
    "project_url": "https://pypi.org/project/mipego/",
    "project_urls": {
      "Homepage": "https://github.com/wangronin/MIP-EGO"
    },
    "release_url": "https://pypi.org/project/mipego/1.0.3/",
    "requires_dist": [
      "pandas",
      "numpy",
      "scipy",
      "scikit-learn",
      "joblib",
      "dill",
      "pyDOE"
    ],
    "requires_python": "",
    "summary": "Mixed Integer Parallel - Efficient Global Optimization with GPU support",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11755238,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b9bf2e411f3fd95b5bcb2cbd567f76e2740a8a09d10dca2216287d57d11755ef",
        "md5": "0c87c709c5223b16b1f2127510480882",
        "sha256": "5f5ebb6660fec1163e3ef4b911d253b7e93db1a0c739a85660382ce26f3816ed"
      },
      "downloads": -1,
      "filename": "mipego-1.0.3-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0c87c709c5223b16b1f2127510480882",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 33165,
      "upload_time": "2021-06-08T14:17:20",
      "upload_time_iso_8601": "2021-06-08T14:17:20.928787Z",
      "url": "https://files.pythonhosted.org/packages/b9/bf/2e411f3fd95b5bcb2cbd567f76e2740a8a09d10dca2216287d57d11755ef/mipego-1.0.3-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e1f774c667a5bf4c10c10aace3be68a342ac69422245b2572063a97a96be6cfe",
        "md5": "18906bd202bebc3795f502d46495a259",
        "sha256": "b13c235c44a704d07029c54f69b69a163ab508e205db5be0d5f2d6faa46d9394"
      },
      "downloads": -1,
      "filename": "mipego-1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "18906bd202bebc3795f502d46495a259",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 36144,
      "upload_time": "2021-06-08T14:17:22",
      "upload_time_iso_8601": "2021-06-08T14:17:22.510621Z",
      "url": "https://files.pythonhosted.org/packages/e1/f7/74c667a5bf4c10c10aace3be68a342ac69422245b2572063a97a96be6cfe/mipego-1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}