{
  "info": {
    "author": "",
    "author_email": "\"Sterling G. Baird\" <sterling.baird@utah.edu>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "<!-- TODO: add buttons for code ocean and Zenodo DOI [![Open in Code Ocean](https://codeocean.com/codeocean-assets/badge/open-in-code-ocean.svg)](https://codeocean.com/capsule/3904426/tree)-->\n# DiSCoVeR\n\n[![Open In Colab (PyPI)](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1MgV_ZewS6gLm1a3Vyhg33pFHi5uTld_2?usp=sharing)\n[![Open in Code Ocean](https://codeocean.com/codeocean-assets/badge/open-in-code-ocean.svg)](https://doi.org/10.24433/CO.8463578.v1)\n[![Interactive Figures](https://img.shields.io/static/v1?message=Open%20interactive%20figures&logo=github&labelColor=5c5c5c&color=blueviolet&logoColor=white&label=%20)](https://mat-discover.readthedocs.io/en/latest/figures.html)\n[![Read the Docs](https://img.shields.io/readthedocs/mat-discover?label=Open%20the%20documentation&logo=readthedocs)](https://mat-discover.readthedocs.io/en/latest/)\n\n[![PyPI version](https://img.shields.io/pypi/v/mat_discover.svg)](https://pypi.org/project/mat_discover/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Coverage Status](https://coveralls.io/repos/github/sparks-baird/mat_discover/badge.svg?service=github)](https://coveralls.io/github/sparks-baird/mat_discover)\n[![Lines of code](https://img.shields.io/tokei/lines/github/sparks-baird/mat_discover)](https://img.shields.io/tokei/lines/github/sparks-baird/mat_discover)\n[![License](https://img.shields.io/github/license/sparks-baird/mat_discover?service=github)](https://img.shields.io/github/license/sparks-baird/mat_discover)\n[![DOI](https://zenodo.org/badge/392897894.svg?service=github)](https://zenodo.org/badge/latestdoi/392897894)\n<!-- ![PyPI - License](https://img.shields.io/pypi/l/mat_discover) -->\n<!-- [![Coverage Status](https://coveralls.io/repos/github/sparks-baird/mat_discover/badge.svg?branch=main)](https://coveralls.io/github/sparks-baird/mat_discover?branch=main) -->\n<!-- ![Coveralls](https://img.shields.io/coveralls/github/sparks-baird/mat_discover) -->\n\n[![Conda](https://img.shields.io/conda/v/sgbaird/mat_discover)](https://anaconda.org/sgbaird/mat_discover)\n[![Conda](https://img.shields.io/conda/pn/sgbaird/mat_discover)](https://anaconda.org/sgbaird/mat_discover)\n[![Conda](https://img.shields.io/conda/dn/sgbaird/mat_discover?label=conda%7Cdownloads)](https://anaconda.org/sgbaird/mat_discover)\n[![Anaconda-Server Badge](https://anaconda.org/sgbaird/mat_discover/badges/latest_release_relative_date.svg)](https://anaconda.org/sgbaird/mat_discover)\n<!-- ![Conda](https://img.shields.io/conda/dn/sgbaird/mat_discover) -->\n<!-- [![Anaconda-Server Downloads](https://anaconda.org/sgbaird/mat_discover/badges/downloads.svg)](https://anaconda.org/sgbaird/mat_discover) -->\n<!-- [![Anaconda-Server Downloads](https://anaconda.org/sgbaird/mat_discover/badges/downloads.svg?service=github)](https://anaconda.org/sgbaird/mat_discover) -->\n<!-- ![PyPI - Downloads](https://img.shields.io/pypi/dm/mat_discover?label=PyPI%20downloads) -->\n\nA materials discovery algorithm geared towards exploring high performance candidates in new chemical spaces using composition-only.\n\n<img src=https://user-images.githubusercontent.com/45469701/139520031-bf4fda18-9be7-4c54-b70b-c9be8e974cea.png width=500>  \n\n<sup>Bulk modulus values overlaid on DensMAP densities (cropped).</sup>\n\nWe describe the DiSCoVeR algorithm, how to install `mat_discover`, and basic usage (e.g.\n`fit`/`predict`, custom or built-in datasets, adaptive design). [Interactive plots](https://mat-discover.readthedocs.io/en/latest/figures.html) for several types of\nPareto front plots are available via [the `mat_discover` documentation](https://mat-discover.readthedocs.io/en/latest/). We also describe how\nto contribute, what to do if you run into bugs or have questions, and citation information. The [`mat_discover` docs](https://mat-discover.readthedocs.io/en/latest/) have more, such as [examples](https://mat-discover.readthedocs.io/en/latest/examples.html) (including a [teaching example](https://mat-discover.readthedocs.io/en/latest/examples.html#bare-bones)), the [interactive figures](https://mat-discover.readthedocs.io/en/latest/figures.html#figures) mentioned, and the [Python API](https://mat-discover.readthedocs.io/en/latest/modules.html).\n\nThe article ([ChemRxiv](https://dx.doi.org/10.33774/chemrxiv-2021-5l2f8-v3)) has been accepted at [Digital Discovery](https://www.rsc.org/journals-books-databases/about-journals/digital-discovery/) (2021-02-03). See [Citing](README.md#citing).\n\n## DiSCoVeR Workflow\n\nWhy you'd want to use this tool, whether it's \"any good\", alternative tools, and summaries of the workflow.\n\n### Why DiSCoVeR?\n\nThe primary anticipated use-case of DiSCoVeR is that you have some training data (chemical formulas and target property), and you would like to determine the \"next best experiment\" to perform based on a user-defined relative importance of performance vs. chemical novelty. You can even run the model without any training targets which is equivalent to setting the target weight as 0.\n\n### Is it any good?\n\nTake an initial training set of 100 chemical formulas and associated Materials Project bulk moduli followed by 900 adaptive design iterations (x-axis) using random search, novelty-only (performance weighted at 0), a 50/50 weighting split, and performance-only (novelty weighted at 0). These are the columns. The rows are the total number of observed \"extraordinary\" compounds (top 2%), the total number of _additional_ unique atoms, and total number of additional unique chemical formulae templates. In other words:\n1. How many \"extraordinary\" compounds have been observed so far?\n1. How many unique atoms have been explored so far? (not counting atoms already in the starting 100 formulas)\n1. How many unique chemical templates (e.g. A2B3, ABC, ABC2) have been explored so far? (not counting templates already in the starting 100 formulas)\n\nThe 50/50 weighting split offers a good trade-off between performance and novelty. Click the image to navigate to the interactive figure which includes two additional rows: best so far and current observed.\n\n<a href=https://mat-discover.readthedocs.io/en/latest/figures.html#adaptive-design-comparison>\n  <img src=https://user-images.githubusercontent.com/45469701/146947278-6399e996-ce9a-46bd-bda7-e3b4feedc525.png width=675>\n</a>\n\nWe also ran some benchmarking against `sklearn.neighbors.LocalOutlierFactor` (novelty detection algorithm) using `mat2vec` and `mod_petti` featurizations. The interactive results are available [here](https://mat-discover.readthedocs.io/en/latest/figures.html#adaptive-design-comparison).\n\n### Alternatives\n\nThis approach is similar to what you will find with Bayesian optimization\n(BO), but with explicit emphasis on chemical novelty. If you're interested in doing\nBayesian optimization, I recommend using [Facebook/Ax](https://ax.dev/docs/bayesopt.html) (not affiliated). I am\nworking on an [implementation of composition-based Bayesian optimization\nusing Ax](https://github.com/facebook/Ax/issues/727) (2021-12-10).\n\nFor alternative \"suggest next experiment\" materials discovery tools,\nsee the [Citrine Platform](https://citrination.com/) (free for non-commercial use), [CAMD](https://github.com/TRI-AMDD/CAMD) ([trihackathon2020 tutorial notebooks](https://github.com/TRI-AMDD/tri-hackathon-2020)), [PyChemia](https://github.com/MaterialsDiscovery/PyChemia),\n[Heteroscedastic-BO](https://github.com/Ryan-Rhys/Heteroscedastic-BO), and\n[thermo](https://github.com/janosh/thermo).\n\nFor materials informatics (MI) and other relevant codebases/links, see:\n\n- [my lists of (total ~200) MI codebases](https://github.com/sgbaird?tab=stars),\n  in particular:\n  - [materials discovery](https://github.com/stars/sgbaird/lists/materials-discovery)\n  - [composition](https://github.com/stars/sgbaird/lists/%EF%B8%8F-composition-predictions)-,\n    [crystal structure](https://github.com/stars/sgbaird/lists/structural-predictions)-,\n    and [molecule](https://github.com/stars/sgbaird/lists/molecule-predictions)-based predictions\n  - [MI databases](https://github.com/stars/sgbaird/lists/materials-databases), especially [NOMAD](https://nomad-lab.eu/) and [MPDS](https://mpds.io/)\n  - [MI materials synthesis](https://github.com/stars/sgbaird/lists/materials-synthesis)\n  - [MI natural language processing](https://github.com/stars/sgbaird/lists/materials-nlp)\n  - [physics-based MI simulations](https://github.com/stars/sgbaird/lists/materials-synthesis)\n- Other lists of MI-relevant codebases:\n  - [general machine learning codebases](https://github.com/stars/sgbaird/lists/machine-learning-general)\n  - [tools to help with scientific publishing](https://github.com/stars/sgbaird/lists/scientific-publishing)\n  - [tools to help with your Python coding efforts](https://github.com/stars/sgbaird/lists/python-enhancements)\n- [this curated list of \"Awesome\" materials\n  informatics](https://github.com/tilde-lab/awesome-materials-informatics) (~100 as of 2021-12-10)\n\n### Visualization\nThe DiSCoVeR workflow is visualized as follows:\n\n<img src=\"https://github.com/sparks-baird/mat_discover/raw/main/figures/discover-workflow.png\" alt=\"DiSCoVeR Workflow\" width=600>\n\n<sup>Figure 1: DiSCoVeR workflow to create chemically homogeneous clusters.  (a) Training and validation data are obtained inthe form of chemical formulas and target properties (i.e.  performance).  (b) The training and validation chemical formulasare combined and used to compute ElMD pairwise distances.  (c) ElMD pairwise distance matrices are used to computeDensMAP embeddings and DensMAP densities.  (d) DensMAP embeddings are used to compute HDBSCAN\\* clusters.(e) Validation target property predictions are made via CrabNet and plotted against the uniqueness proxy (e.g.  densityproxy) in the form of a Pareto front plot.  Discovery scores are assigned based on the (arbitrarily) weighted sum of scaledperformance and uniqueness proxy.  Higher scores are better.  (f) HDBSCAN* clustering results can be used to obtain acluster-wise performance (e.g.  average target property) plotted against a cluster-wise uniqueness proxy (e.g.  fraction ofvalidation compounds vs.  total compounds within a cluster).</sup>\n\n### Tabular Summary\nA summary of the DiSCoVeR methods are given in the following table:\n\n<sup>Table 1: A description of methods used in this work and each methodâ€™s role in DiSCoVeR. âˆ—A Pareto front is more information-dense than a proxy score in that there are no predefined relative weights for performance vs. uniqueness proxy. Compounds that are closer to the Pareto front are better. The upper areas of the plot represent a higher weight towards performance while the right-most areas of the plot represent a higher weight towards uniqueness.</sup>\n| Method                                                                   | What is it?                                   | What is its role in DiSCoVeR?           |\n| ------------------------------------------------------------------------ | --------------------------------------------- | --------------------------------------- |\n| [CrabNet](https://github.com/anthony-wang/CrabNet)                       | Composition-based property regression         | Predict performance for proxy scores    |\n| [ElMD](https://github.com/lrcfmd/ElMD)                                   | Composition-based distance metric             | Supply distance matrix to DensMAP       |\n| [DensMAP](https://umap-learn.readthedocs.io/en/latest/densmap_demo.html) | Density-aware dimensionality reduction        | Obtain densities for density proxy      |\n| [HDBSCAN*](https://hdbscan.readthedocs.io/en/latest/index.html)          | Density-aware clustering                      | Create chemically homogeneous clusters  |\n| Peak proxy                                                               | High performance relative to nearby compounds | Proxy for \"surprising\" high performance |\n| Density proxy                                                            | Sparsity relative to nearby compounds         | Proxy for chemical novelty              |\n| Peak proxy score                                                         | Weighted sum of performance and peak proxy    | Used to rank compounds                  |\n| Density proxy score                                                      | Weighted sum of performance and density proxy | Used to rank compounds                  |\n| [Pareto front](https://en.wikipedia.org/wiki/Pareto_front)               | Optimal performance/uniqueness trade-offs     | Visually screen compounds (no weights*) |\n\n## Installation\n\nI recommend that you run `mat_discover` in a separate conda environment, at least for\ninitial testing. After installing\n[Anaconda](https://docs.anaconda.com/anaconda/navigator/index.html) or\n[Miniconda](https://docs.conda.io/en/latest/miniconda.html), you can [create a new\nenvironment](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands)\nin Python `3.9` (`mat_discover` is also tested on `3.7` and `3.8`) via:\n\n```python\nconda create --name mat_discover python==3.9.*\n```\n\nThere are three ways to install `mat_discover`: Anaconda (`conda`), PyPI (`pip`), and from source. Anaconda is the preferred method.\n\n### Anaconda\n\nTo install `mat_discover` using `conda`, first, update `conda` via:\n\n```python\nconda update conda\n```\n\nThe Anaconda `mat_discover` package is hosted on the [@sgbaird channel](https://anaconda.org/sgbaird/repo) and can be installed via:\n\n```python\nconda install -c sgbaird mat_discover\n```\n\n### Pip\n\nTo install via `pip`, first update `pip` via:\n\n```python\npip install -U pip\n```\n\nDue to limitations of PyPI distributions of CUDA/PyTorch, you will need to install PyTorch separately via the command that's most relevant to you ([PyTorch Getting Started](https://pytorch.org/get-started/locally/)). For example, for Stable/Windows/Pip/Python/CUDA-11.3:\n\n```python\npip install torch==1.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n```\n\n<!--- ```python\nconda install pytorch cudatoolkit=11.1 -c pytorch -c conda-forge\n``` --->\n\nFinally, install `mat_discover`:\n\n```python\npip install mat_discover\n```\n\n### From Source\n\nTo install from source, clone the `mat_discover` repository:\n\n```python\ngit clone https://github.com/sparks-baird/mat_discover.git\ncd mat_discover\n```\n\nTo perform the local installation, you can use `pip`, `conda`, or `flit`. If using `flit`, make sure to install it first via `conda install flit` or `pip install flit`.\n| **pip**            | **conda**                                 | **flit**                  |\n| ------------------ | ----------------------------------------- | ------------------------- |\n| `pip install -e .` | `conda env create --file environment.yml` | `flit install --pth-file` |\n\n<!-- conda install torch cudatoolkit=11.1 -c pytorch -c conda-forge # or use pip command specific to you from https://pytorch.org/get-started/locally/ -->\n\n## Basic Usage\nHow to `fit`/`predict`, use custom or built-in datasets, and perform adaptive design.\n\n### Fit/Predict\n\n```python\nfrom mat_discover.mat_discover_ import Discover\ndisc = Discover()\ndisc.fit(train_df) # DataFrames should have at minimum \"formula\" and \"target\" columns\nscores = disc.predict(val_df)\ndisc.plot()\ndisc.save()\nprint(disc.dens_score_df.head(10), disc.peak_score_df.head(10))\n```\n\n> âš ï¸ ignore the \"validation\" mean absolute error (MAE) command line output during `disc.fit(train_df)` âš ï¸\n\nSee\n[mat_discover_example.py](https://github.com/sparks-baird/examples/mat_discover_example.py),\n[![Open In Colab\n(PyPI)](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1MgV_ZewS6gLm1a3Vyhg33pFHi5uTld_2?usp=sharing),\nor\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/sparks-baird/mat_discover/main?labpath=mat_discover_pypi.ipynb).\nOn Google Colab and Binder, this may take a few minutes to install and load,\nrespectively. During training and prediction, Google Colab will be faster than Binder\nsince Google Colab has access to a GPU while Binder does not. Sometimes Binder takes a long time to load, so please consider using Open In Colab or the normal installation instructions instead.\n\n### Load Data\n\nIf you're using your own dataset, you will need to supply a Pandas DataFrame that\ncontains `formula` (string) and `target` (numeric) columns. If you have a `train.csv` file\n(located in current working directory) with these two columns, this can be converted to\na DataFrame via:\n\n```python\nimport pandas as pd\ntrain_df = pd.read_csv(\"train.csv\")\n```\n\nFor validation data without known property values to be used with `predict`, dummy\nvalues (all zeros) are assigned internally. In this case, you can read in a CSV file\nthat contains only the `formula` (string) column:\n\n```python\nval_df = pd.read_csv(\"val.csv\")\n```\n\nNote that you can load any of the datasets within `CrabNet/data/`, which includes `matbench` data, other datasets from the CrabNet paper, and a recent (as of Oct 2021) snapshot of `K_VRH` bulk modulus data from Materials Project. For example, to load the bulk modulus snapshot:\n\n```python\nfrom crabnet.data.materials_data import elasticity\ntrain_df, val_df = disc.data(elasticity, \"train.csv\") # note that `val.csv` within `elasticity` is every other Materials Project compound (i.e. \"target\" column filled with zeros)\n```\n\nThe built-in data directories are as follows:\n>\n> ```python\n> {'benchmark_data',\n>  'benchmark_data.CritExam__Ed',\n>  'benchmark_data.CritExam__Ef',\n>  'benchmark_data.OQMD_Bandgap',\n>  'benchmark_data.OQMD_Energy_per_atom',\n>  'benchmark_data.OQMD_Formation_Enthalpy',\n>  'benchmark_data.OQMD_Volume_per_atom',\n>  'benchmark_data.aflow__Egap',\n>  'benchmark_data.aflow__ael_bulk_modulus_vrh',\n>  'benchmark_data.aflow__ael_debye_temperature',\n>  'benchmark_data.aflow__ael_shear_modulus_vrh',\n>  'benchmark_data.aflow__agl_thermal_conductivity_300K',\n>  'benchmark_data.aflow__agl_thermal_expansion_300K',\n>  'benchmark_data.aflow__energy_atom',\n>  'benchmark_data.mp_bulk_modulus',\n>  'benchmark_data.mp_e_hull',\n>  'benchmark_data.mp_elastic_anisotropy',\n>  'benchmark_data.mp_mu_b',\n>  'benchmark_data.mp_shear_modulus',\n>  'element_properties',\n>  'matbench',\n>  'materials_data',\n>  'materials_data.elasticity',\n>  'materials_data.example_materials_property'}\n> ```\n\nTo see what `.csv` files are available (e.g. `train.csv`), you will probably need to navigate to [CrabNet/data/](https://github.com/sgbaird/CrabNet/tree/master/crabnet/data) and explore. For example, to use a snapshot of the Materials Project `e_above_hull` dataset ([`mp_e_hull`](https://github.com/sgbaird/CrabNet/tree/master/crabnet/data/benchmark_data/mp_e_hull)):\n```python\nfrom crabnet.data.benchmark_data import mp_e_hull\ntrain_df = disc.data(mp_e_hull, \"train.csv\", split=False)\nval_df = disc.data(mp_e_hull, \"val.csv\", split=False)\ntest_df = disc.data(mp_ehull, \"test.csv\", split=False)\n```\n\nFinally, to download data from Materials Project directly, see [generate_elasticity_data.py](https://github.com/sparks-baird/mat_discover/blob/main/mat_discover/utils/generate_elasticity_data.py).\n\n### Adaptive Design\nThe anticipated end-use of `mat_discover` is in an adaptive design scheme where the objective function (e.g. wetlab synthesis and characterization) is expensive. After loading some data for a validation scenario (or your own data)\n```python\nfrom crabnet.data.materials_data import elasticity\nfrom mat_discover.utils.data import data\nfrom mat_discover.adaptive_design import Adapt\ntrain_df, val_df = data(elasticity, \"train.csv\", dummy=False, random_state=42)\ntrain_df, val_df, extraordinary_thresh = extraordinary_split(\n    train_df, val_df, train_size=100, extraordinary_percentile=0.98, random_state=42\n)\n```\nyou can then predict your first additional experiment to run via:\n```python\nadapt = Adapt(train_df, val_df, timed=False)\nfirst_experiment = adapt.suggest_first_experiment() # fit Discover() to train_df, then move top-ranked from val_df to train_df\n```\nSubsequent experiments are suggested as follows:\n```python\nsecond_experiment = adapt.suggest_next_experiment() # refit CrabNet, use existing DensMAP data, move top-ranked from val to train\nthird_experiment = adapt.suggest_next_experiment()\n```\n\nAlternatively, you can do this in a closed loop via:\n```python\nn_iter = 100\nadapt.closed_loop_adaptive_design(n_experiments=n_iter, print_experiment=False)\n```\nHowever, as the name suggests, the closed loop approach does not allow you to input data after each suggested experiment.\n\n## Developing and Contributing\n\nThis project was developed primarily in [Python in Visual Studio Code](https://code.visualstudio.com/docs/languages/python) using `black`, `mypy`, `pydocstyle`, `kite`, other tools, and various community extensions. Some other notable tools used in this project are:\n\n- Miniconda\n- `pipreqs` was used as a starting point for `requirements.txt`\n- `flit` is used to create `pyproject.toml` to publish to PyPI\n- `conda env export --from-history -f environment.yml` was used as a starting point for `environment.yml`\n- `grayskull` and `conda-souschef` are used to generate and tweak `meta.yaml`,\n  respectively, for publishing to Anaconda (if you know how to get this up on\n  conda-forge, help is welcome ðŸ˜‰)\n- A variety of GitHub actions are used (see [workflows](https://github.com/sparks-baird/.github/workflows))\n- `pytest` is used for testing\n- `numba` is used to accelerate the Wasserstein distance matrix computations via CPU or GPU\n\n<!-- - `conda-smithy` is used to create a feedstock for `conda-forge` -->\n\nFor simple changes, navigate to github.com/sparks-baird/mat_discover, click on the\nrelevant file (e.g. `README.md`), and look for the pencil (âœï¸). GitHub will walk you\nthrough the rest.\n\nTo help with in-depth development, you will need to [install from\nsource](README.md#from-source). Note that when using a `conda` environment\n(recommended), you may avoid certain issues down the road by opening VS Code via an\nAnaconda command prompt and entering the command `code` (at least until the VS Code devs\nfix some of the issues associated with opening it \"normally\"). For example, in Windows,\npress the \"Windows\" key, type \"anaconda\", and open \"Anaconda Powershell Prompt\n(miniconda3)\" or similar. Then type `code` and press enter. To build the docs, first install `sphinx` and `sphinx_rtd_theme`. Then run:\n\n```bash\ncd docs/\nmake html\n```\n\nAnd open `docs/build/index.html` (e.g. via `start index.html` on Windows)\n\n## Bugs, Questions, and Suggestions\n\nIf you find a bug or have suggestions for documentation please [open an\nissue](https://github.com/sparks-baird/mat_discover/issues/new/choose). If you're\nreporting a bug, please include a simplified reproducer. If you have questions, have\nfeature suggestions/requests, or are interested in extending/improving `mat_discover`\nand would like to discuss, please use the Discussions tab and use the appropriate\ncategory (\"Ideas\", \"Q&A\", etc.). If you have a\nquestion, please ask! I won't bite. Pull requests are welcome and encouraged.\n\n## Citing\n\nThe preprint is hosted on ChemRxiv:\n> Baird S, Diep T, Sparks T. DiSCoVeR: a Materials Discovery Screening Tool for High Performance, Unique Chemical Compositions. ChemRxiv 2021. [doi:10.33774/chemrxiv-2021-5l2f8-v3](https://dx.doi.org/10.33774/chemrxiv-2021-5l2f8-v3). This content is a preprint and has not been peer-reviewed.\n\nThe BibTeX citation is as follows:\n\n```bib\n@article{baird_diep_sparks_2021,\nplace={Cambridge},\ntitle={DiSCoVeR: a Materials Discovery Screening Tool for High Performance, Unique Chemical Compositions},\nDOI={10.33774/chemrxiv-2021-5l2f8-v3},\njournal={ChemRxiv},\npublisher={Cambridge Open Engage},\nauthor={Baird, Sterling and Diep, Tran and Sparks, Taylor},\nyear={2021}\n}\n```\n\nThe article is under review at [Digital Discovery](https://www.rsc.org/journals-books-databases/about-journals/digital-discovery/).\n\n## Looking for more?\nSee [examples](https://mat-discover.readthedocs.io/en/latest/examples.html), including [a teaching example](https://mat-discover.readthedocs.io/en/latest/examples.html#bare-bones), and the [Python API](https://mat-discover.readthedocs.io/en/latest/modules.html).\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mat_discover",
    "package_url": "https://pypi.org/project/mat_discover/",
    "platform": "",
    "project_url": "https://pypi.org/project/mat_discover/",
    "project_urls": {
      "Source": "https://github.com/sparks-baird/mat_discover"
    },
    "release_url": "https://pypi.org/project/mat_discover/2.1.1/",
    "requires_dist": [
      "plotly",
      "tqdm",
      "pqdm",
      "seaborn",
      "scipy",
      "hdbscan",
      "cython",
      "numba >=0.53.1",
      "kaleido",
      "pandas",
      "matplotlib >=3.4.3",
      "scikit_learn",
      "umap-learn",
      "dill",
      "crabnet >=1.2.2",
      "chem_wasserstein >=1.0.8",
      "composition_based_feature_vector",
      "pytest ; extra == \"test\"",
      "ElM2D==0.4.1 ; extra == \"test\"",
      "pytest-cov ; extra == \"test\"",
      "pre-commit ; extra == \"test\"",
      "sphinx==4.2.0 ; extra == \"test\"",
      "myst-parser==0.15.2 ; extra == \"test\"",
      "nbformat >=4.2.0 ; extra == \"test\"",
      "black >=22.1.0 ; extra == \"test\""
    ],
    "requires_python": "",
    "summary": "Data-driven materials discovery based on composition.",
    "version": "2.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14749219,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "502aea1d7cd0f450069b9f94446553f9d7ab83aad8a2003d8a03fe929a16b269",
        "md5": "16e9083019ba366d3b522d3c073dfbe4",
        "sha256": "3db33248a6a39546080519b67ea8a426163d3e1fee8771bf85f969b69e6cc9a6"
      },
      "downloads": -1,
      "filename": "mat_discover-2.1.1-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "16e9083019ba366d3b522d3c073dfbe4",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 41900,
      "upload_time": "2022-02-12T08:36:57",
      "upload_time_iso_8601": "2022-02-12T08:36:57.349109Z",
      "url": "https://files.pythonhosted.org/packages/50/2a/ea1d7cd0f450069b9f94446553f9d7ab83aad8a2003d8a03fe929a16b269/mat_discover-2.1.1-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f1018abab54691e9fa8ca0a2d44af91dbabe6de93c9d950a7f653eac78598ff8",
        "md5": "b6da371bd9a05849042a28063e3ed296",
        "sha256": "8deb7dc2b5f010b24122da58ed4be01187242525e1fb1ed2b31b2b3af6fe3eda"
      },
      "downloads": -1,
      "filename": "mat_discover-2.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "b6da371bd9a05849042a28063e3ed296",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 16833700,
      "upload_time": "2022-02-12T08:37:00",
      "upload_time_iso_8601": "2022-02-12T08:37:00.516311Z",
      "url": "https://files.pythonhosted.org/packages/f1/01/8abab54691e9fa8ca0a2d44af91dbabe6de93c9d950a7f653eac78598ff8/mat_discover-2.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}