{
  "info": {
    "author": "lrabbit",
    "author_email": "709343607@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "lrabbit_scrapy\n=====\n\nthis is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some\nrepeated code every time, using this small framework you can quickly crawl data into a file or database.\n\n\nInstalling\n----------\n\n    $ pip install lrabbit_scrapy\n\nquick start\n----------------\n\n* create blog_spider.py\n\n```python\nfrom lrabbit_scrapy.spider import LrabbitSpider\nfrom lrabbit_scrapy.common_utils.network_helper import RequestSession\nfrom lrabbit_scrapy.common_utils.print_log_helper import LogUtils\nfrom lrabbit_scrapy.common_utils.all_in_one import FileStore\nimport os\nfrom parsel import Selector\n\n\nclass Spider(LrabbitSpider):\n    \"\"\"\n        spider_name : lrabbit blog spider\n    \"\"\"\n    # unique spider name\n    spider_name = \"lrabbit_blog\"\n    # max thread worker numbers\n    max_thread_num = 10\n    # reset all task_list\n    reset_task_config = True\n    # open loop init_task_list\n    loop_task_config = False\n    # remove config option\n    remove_confirm_config = False\n    # config_path_name\n    config_env_name = \"config_path\"\n    # redis db_num\n    redis_db_config = 0\n\n    def __init__(self):\n        super().__init__()\n        self.session = RequestSession()\n        self.proxy_session = RequestSession(proxies=None)\n        csv_path = os.path.join(os.path.abspath(os.getcwd()), f\"{self.spider_name}.csv\")\n        self.field_names = ['id', 'title', 'datetime']\n        self.blog_file = FileStore(file_path=csv_path, filed_name=self.field_names)\n\n    def worker(self, task):\n        LogUtils.log_info(task)\n        html = self.session.send_request(method='GET', url=f'http://www.lrabbit.life/post_detail/?id={task}')\n        selector = Selector(html)\n        title = selector.css(\".detail-title h1::text\").get()\n        datetime = selector.css(\".detail-info span::text\").get()\n        if title:\n            post_data = {\"id\": task, \"title\": title, 'datetime': datetime}\n            self.blog_file.write(post_data)\n            # when you succes get content update redis stat\n            self.update_stat_redis()\n        self.task_list.remove(task)\n        LogUtils.log_finish(task)\n\n    def init_task_list(self):\n        # res = self.mysql_client.query(\"select id from rookie limit 100 \")\n        # return [item['id'] for item in res]\n        return [i for i in range(100)]\n\n\nif __name__ == '__main__':\n    spider = Spider()\n    spider.run()\n\n```\n\n* set config.ini and config env variable\n    * create crawl.ini forexam this file path is /root/crawl.ini\n    ```ini\n  [server]\n  mysql_user = root\n  mysql_password = 123456\n  mysql_database = test\n  mysql_host = 192.168.1.1\n  redis_user = lrabbit\n  redis_host = 192.168.1.1\n  redis_port = 6379\n  redis_password = love20100001314\n\n  [test]\n  mysql_user = root\n  mysql_password = 123456\n  mysql_database = test\n  mysql_host = 192.168.1.1\n  redis_user = lrabbit\n  redis_host = 192.168.1.1\n  redis_port = 6379\n  redis_password = 123456\n  ```\n    * set config env\n        * windows power shell\n        * $env:config_path = \"/root/crawl.ini\"\n        * linux\n        * export config_path=\"/root/crawl.ini\"\n      \n\n* python3 blog_spider.py\n\nLinks\n-----\n\n- author: https://www.lrabbit.life/\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/litter-rabbit/lrabbit_scrapy",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lrabbit-scrapy",
    "package_url": "https://pypi.org/project/lrabbit-scrapy/",
    "platform": "",
    "project_url": "https://pypi.org/project/lrabbit-scrapy/",
    "project_urls": {
      "Bug Tracker": "https://github.com/litter-rabbit/lrabbit_scrapy/issues",
      "Homepage": "https://github.com/litter-rabbit/lrabbit_scrapy"
    },
    "release_url": "https://pypi.org/project/lrabbit-scrapy/2.0.1/",
    "requires_dist": [
      "parsel (==1.6.0)",
      "requests (>=2.26.0)",
      "PyMySQL (>=0.9.3)",
      "redispy (>=3.0.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "this is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some repeated code every time, using this small framework you can quickly crawl data into a file or database.",
    "version": "2.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12262992,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cb3f0df4590a5dd49a181a72cc16493a7f039bbc8290a3e47b1fe76cb953626c",
        "md5": "d5d2f05d6e275054c4cf6d8378ae5717",
        "sha256": "4a85828f55fbe4240247b518719ee95e15175c726534e49390fd933b8a8239bb"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d5d2f05d6e275054c4cf6d8378ae5717",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 21421,
      "upload_time": "2021-11-24T02:26:37",
      "upload_time_iso_8601": "2021-11-24T02:26:37.984171Z",
      "url": "https://files.pythonhosted.org/packages/cb/3f/0df4590a5dd49a181a72cc16493a7f039bbc8290a3e47b1fe76cb953626c/lrabbit_scrapy-2.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6e62630e0f602c74eff9aff1f36920c751416f56c28ddb7f2d30a7bbcc9da1c9",
        "md5": "c7f8e17c2797bc2caa9fa16a18ee42ac",
        "sha256": "1c9ac1ecca1c6427cd94dd8881d8fe7d2cb9468d0f82c668a0f08df477430e33"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "c7f8e17c2797bc2caa9fa16a18ee42ac",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 17489,
      "upload_time": "2021-11-24T02:26:40",
      "upload_time_iso_8601": "2021-11-24T02:26:40.523533Z",
      "url": "https://files.pythonhosted.org/packages/6e/62/630e0f602c74eff9aff1f36920c751416f56c28ddb7f2d30a7bbcc9da1c9/lrabbit_scrapy-2.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}