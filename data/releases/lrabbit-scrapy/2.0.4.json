{
  "info": {
    "author": "lrabbit",
    "author_email": "709343607@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "lrabbit_scrapy\n=====\n\nthis is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some\nrepeated code every time, using this small framework you can quickly crawl data into a file or database.\n\n\nInstalling\n----------\n\n    $ pip install lrabbit_scrapy\n\nquick start\n----------------\n\n* create blog_spider.py\n\n```python\nfrom lrabbit_scrapy.spider import LrabbitSpider\nfrom lrabbit_scrapy.common_utils.network_helper import RequestSession\nfrom lrabbit_scrapy.common_utils.print_log_helper import LogUtils\nfrom lrabbit_scrapy.common_utils.all_in_one import FileStore\nimport os\nfrom lrabbit_scrapy.common_utils.mysql_helper import MysqlClient\nfrom parsel import Selector\n\n\nclass Spider(LrabbitSpider):\n    \"\"\"\n        spider_name : lrabbit blog spider\n    \"\"\"\n    # unique spider name\n    spider_name = \"lrabbit_blog\"\n    # max thread worker numbers\n    max_thread_num = 2\n    # is open for every thread a mysql connection,if your max_thread_num overpass 10 and  in code need mysql query ,you need open this config\n    thread_mysql_open = True\n    # reset all task_list,every restart program will init task list\n    reset_task_config = False\n    # open loop init_task_list ,when your task is all finish,and you want again ,you can open it\n    loop_task_config = False\n    # remove config option,if open it,then confirm option when you init task\n    remove_confirm_config = False\n    # config_path_name, this is env name ,is this code ,you need in linux to execute: export config_path=\"crawl.ini\"\n    config_env_name = \"config_path\"\n    # redis db_num\n    redis_db_config = 0\n    # debug log ,open tracback log\n    debug_config = False\n\n    def __init__(self):\n        super().__init__()\n        self.session = RequestSession()\n        self.proxy_session = RequestSession(proxies=None)\n        csv_path = os.path.join(os.path.abspath(os.getcwd()), f\"{self.spider_name}.csv\")\n        self.field_names = ['id', 'title', 'datetime']\n        self.blog_file = FileStore(file_path=csv_path, filed_name=self.field_names)\n\n    def worker(self, *args):\n        task = args[0]\n        mysql_client: MysqlClient\n        if len(args) == 2:\n            mysql_client = args[1]\n            # mysql_client.execute(\"\")\n        res = self.session.send_request(method='GET', url=f'http://www.lrabbit.life/post_detail/?id={task}')\n        selector = Selector(res.text)\n        title = selector.css(\".detail-title h1::text\").get()\n        datetime = selector.css(\".detail-info span::text\").get()\n        if title:\n            post_data = {\"id\": task, \"title\": title, 'datetime': datetime}\n            self.blog_file.write(post_data)\n            # when you succes get content update redis stat\n            self.update_stat_redis()\n        LogUtils.log_finish(task)\n\n    def init_task_list(self):\n\n        # you can get init task from mysql\n        # res = self.mysql_client_pool.execute_query(\"select id from rookie limit 100 \")\n        # return [task['id'] for task in res]\n        return [i for i in range(100)]\n\n\nif __name__ == '__main__':\n    spider = Spider()\n    spider.run()\n\n```\n\n* set config.ini and config env variable\n    * create crawl.ini,for example this file path is /root/crawl.ini\n    ```ini\n  [server]\n  mysql_user = root\n  mysql_password = 123456\n  mysql_database = test\n  mysql_host = 192.168.1.1\n  redis_user = lrabbit\n  redis_host = 192.168.1.1\n  redis_port = 6379\n  redis_password = 123456\n\n  [test]\n  mysql_user = root\n  mysql_password = 123456\n  mysql_database = test\n  mysql_host = 192.168.1.1\n  redis_user = lrabbit\n  redis_host = 192.168.1.1\n  redis_port = 6379\n  redis_password = 123456\n  ```\n    * set config env\n        * windows power shell\n        * $env:config_path = \"/root/crawl.ini\"\n        * linux\n        * export config_path=\"/root/crawl.ini\"\n\n\n* python3 blog_spider.py\n* python3 blog_spider.py stat\n    * show task stat\n\nLinks\n-----\n\n- author: https://www.lrabbit.life/\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/litter-rabbit/lrabbit_scrapy",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lrabbit-scrapy",
    "package_url": "https://pypi.org/project/lrabbit-scrapy/",
    "platform": "",
    "project_url": "https://pypi.org/project/lrabbit-scrapy/",
    "project_urls": {
      "Bug Tracker": "https://github.com/litter-rabbit/lrabbit_scrapy/issues",
      "Homepage": "https://github.com/litter-rabbit/lrabbit_scrapy"
    },
    "release_url": "https://pypi.org/project/lrabbit-scrapy/2.0.4/",
    "requires_dist": [
      "parsel (==1.6.0)",
      "requests (>=2.26.0)",
      "PyMySQL (>=0.9.3)",
      "redispy (>=3.0.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "this is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some repeated code every time, using this small framework you can quickly crawl data into a file or database.",
    "version": "2.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12262992,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d80b25aa9e22c76ccfc96a413d9f57944c2d163400413b7d5b5ba3671233c8ec",
        "md5": "4de0c5e5aa765f9238e028589cbc317a",
        "sha256": "e15864c1991e25a9598daed00797bc7487885c1571fec3d51dc52d95eee4c597"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4de0c5e5aa765f9238e028589cbc317a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 22982,
      "upload_time": "2021-11-29T02:13:13",
      "upload_time_iso_8601": "2021-11-29T02:13:13.945794Z",
      "url": "https://files.pythonhosted.org/packages/d8/0b/25aa9e22c76ccfc96a413d9f57944c2d163400413b7d5b5ba3671233c8ec/lrabbit_scrapy-2.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cea0ea46f5fec47d1d911d422ed596c32c2f70ef85a33c4324543456065ec665",
        "md5": "29817a1c2d00edc7698f31d76f8d8e39",
        "sha256": "b52077539740db60dfa5a76b3da055b41e79ea779109c4c03f4999d3b86d4646"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "29817a1c2d00edc7698f31d76f8d8e39",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 19324,
      "upload_time": "2021-11-29T02:13:15",
      "upload_time_iso_8601": "2021-11-29T02:13:15.348682Z",
      "url": "https://files.pythonhosted.org/packages/ce/a0/ea46f5fec47d1d911d422ed596c32c2f70ef85a33c4324543456065ec665/lrabbit_scrapy-2.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}