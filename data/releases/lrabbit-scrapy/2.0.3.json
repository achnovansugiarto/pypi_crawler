{
  "info": {
    "author": "lrabbit",
    "author_email": "709343607@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "lrabbit_scrapy\n=====\n\nthis is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some\nrepeated code every time, using this small framework you can quickly crawl data into a file or database.\n\n\nInstalling\n----------\n\n    $ pip install lrabbit_scrapy\n\nquick start\n----------------\n\n* create blog_spider.py\n\n```python\nfrom lrabbit_scrapy.spider import LrabbitSpider\nfrom lrabbit_scrapy.common_utils.network_helper import RequestSession\nfrom lrabbit_scrapy.common_utils.print_log_helper import LogUtils\nfrom lrabbit_scrapy.common_utils.all_in_one import FileStore\nimport os\nfrom lrabbit_scrapy.common_utils.mysql_helper import MysqlClient\nfrom parsel import Selector\n\n\nclass Spider(LrabbitSpider):\n    \"\"\"\n        spider_name : lrabbit blog spider\n    \"\"\"\n    # unique spider name\n    spider_name = \"lrabbit_blog\"\n    # max thread worker numbers\n    thread_mysql_open = True\n    max_thread_num = 5\n    # reset all task_list\n    reset_task_config = True\n    # open loop init_task_list\n    loop_task_config = True\n    # remove config option\n    remove_confirm_config = True\n    # config_path_name\n    config_env_name = \"config_path\"\n    # redis db_num\n    redis_db_config = 0\n    # debug log\n    debug_config = False\n\n    def __init__(self):\n        super().__init__()\n        self.session = RequestSession()\n        self.proxy_session = RequestSession(proxies=None)\n        csv_path = os.path.join(os.path.abspath(os.getcwd()), f\"{self.spider_name}.csv\")\n        self.field_names = ['id', 'title', 'datetime']\n        self.blog_file = FileStore(file_path=csv_path, filed_name=self.field_names)\n\n    def worker(self, *args):\n        task = args[0]\n        # mysql_client: MysqlClient\n        # if len(args) == 2:\n        #     mysql_client = args[1]\n\n        res = self.session.send_request(method='GET', url=f'http://www.lrabbit.life/post_detail/?id={task}')\n        selector = Selector(res.text)\n        title = selector.css(\".detail-title h1::text\").get()\n        datetime = selector.css(\".detail-info span::text\").get()\n        if title:\n            post_data = {\"id\": task, \"title\": title, 'datetime': datetime}\n            self.blog_file.write(post_data)\n            # when you succes get content update redis stat\n            self.update_stat_redis()\n        LogUtils.log_finish(task)\n        self.redis_client.redis_executor.srem(self.current_task_key, task)\n\n    def init_task_list(self):\n        # res = self.mysql_client.query(\"select id from rookie limit 100 \")\n        # return [item['id'] for item in res]\n        return [i for i in range(100)]\n\n\nif __name__ == '__main__':\n    spider = Spider()\n    spider.run()\n\n\n```\n\n* set config.ini and config env variable\n    * create crawl.ini forexam this file path is /root/crawl.ini\n    ```ini\n  [server]\n  mysql_user = root\n  mysql_password = 123456\n  mysql_database = test\n  mysql_host = 192.168.1.1\n  redis_user = lrabbit\n  redis_host = 192.168.1.1\n  redis_port = 6379\n  redis_password = love20100001314\n\n  [test]\n  mysql_user = root\n  mysql_password = 123456\n  mysql_database = test\n  mysql_host = 192.168.1.1\n  redis_user = lrabbit\n  redis_host = 192.168.1.1\n  redis_port = 6379\n  redis_password = 123456\n  ```\n    * set config env\n        * windows power shell\n        * $env:config_path = \"/root/crawl.ini\"\n        * linux\n        * export config_path=\"/root/crawl.ini\"\n\n\n* python3 blog_spider.py\n\nLinks\n-----\n\n- author: https://www.lrabbit.life/\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/litter-rabbit/lrabbit_scrapy",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lrabbit-scrapy",
    "package_url": "https://pypi.org/project/lrabbit-scrapy/",
    "platform": "",
    "project_url": "https://pypi.org/project/lrabbit-scrapy/",
    "project_urls": {
      "Bug Tracker": "https://github.com/litter-rabbit/lrabbit_scrapy/issues",
      "Homepage": "https://github.com/litter-rabbit/lrabbit_scrapy"
    },
    "release_url": "https://pypi.org/project/lrabbit-scrapy/2.0.3/",
    "requires_dist": [
      "parsel (==1.6.0)",
      "requests (>=2.26.0)",
      "PyMySQL (>=0.9.3)",
      "redispy (>=3.0.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "this is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some repeated code every time, using this small framework you can quickly crawl data into a file or database.",
    "version": "2.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12262992,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2a545e51c2db0df485aa62686082d4899a32bd3a54df57a4b79c0d329d825c86",
        "md5": "915f145038ca246d84f7957bb5a0bc5c",
        "sha256": "9aaa76ab776f87626a039bf0810ab03c49c8f09e3ff2fc7f8c4fc80a80e5f397"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "915f145038ca246d84f7957bb5a0bc5c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 22800,
      "upload_time": "2021-11-25T02:47:21",
      "upload_time_iso_8601": "2021-11-25T02:47:21.835282Z",
      "url": "https://files.pythonhosted.org/packages/2a/54/5e51c2db0df485aa62686082d4899a32bd3a54df57a4b79c0d329d825c86/lrabbit_scrapy-2.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4ff6719e415bbebcb1a426784244abe8ffa7720a85adb61e0bbfba8d09ece8a7",
        "md5": "47e25b39b0f4db8f44750768ec504db5",
        "sha256": "b9d9b6c0da8ad74936bf968c83dd840754f2911ac4c8547516758b586b755e7b"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "47e25b39b0f4db8f44750768ec504db5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 18905,
      "upload_time": "2021-11-25T02:47:23",
      "upload_time_iso_8601": "2021-11-25T02:47:23.387583Z",
      "url": "https://files.pythonhosted.org/packages/4f/f6/719e415bbebcb1a426784244abe8ffa7720a85adb61e0bbfba8d09ece8a7/lrabbit_scrapy-2.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}