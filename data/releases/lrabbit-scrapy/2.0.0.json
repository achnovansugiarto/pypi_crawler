{
  "info": {
    "author": "lrabbit",
    "author_email": "709343607@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "lrabbit_scrapy\n=====\n\nthis is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some\nrepeated code every time, using this small framework you can quickly crawl data into a file or database.\n\n\nInstalling\n----------\n\n    $ pip3 install lrabbit_scrapy\n\nA Simple Example\n----------------\n\n```python\nclass Spider(LrabbitSpider):\n    \"\"\"\n     # spider_name \n     redis_key:\n        list:spider_name 任务队列\n        success:count:spider_name 记录成功数\n        list:error:excepiton404\n    \"\"\"\n    spider_name = \"test\"\n    # 最大线程数\n    max_thread_num = 10\n    # 重置任务队列\n    reset_task_config = False\n    # 开启循环模式\n    loop_task_config = False\n    # 去除确认信息\n    remove_confirm_config = False\n\n    def __init__(self):\n        super().__init__()\n        self.session = RequestSession()\n        self.proxy_session = RequestSession(proxies=None)\n\n    def worker(self, task):\n        LogUtils.log_info(task)\n        self.session.send_request(method='GET', url=\"https://www.lrabbit.life/233333333333333333/\")\n        # when you keyboraderror you can't lost you task\n        self.task_list.remove(task)\n        # update stat\n        self.update_stat_redis()\n        LogUtils.log_finish(task)\n\n    def init_task_list(self):\n        res = self.mysql_client.query(\"select id from rookie limit 100 \")\n        return [item['id'] for item in res]\n\n    \nif __name__ == '__main__':\n    spider = Spider()\n    spider.run()\n\n```\n\nLinks\n-----\n\n- author: https://www.lrabbit.life/\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/litter-rabbit/lrabbit_scrapy",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lrabbit-scrapy",
    "package_url": "https://pypi.org/project/lrabbit-scrapy/",
    "platform": "",
    "project_url": "https://pypi.org/project/lrabbit-scrapy/",
    "project_urls": {
      "Bug Tracker": "https://github.com/litter-rabbit/lrabbit_scrapy/issues",
      "Homepage": "https://github.com/litter-rabbit/lrabbit_scrapy"
    },
    "release_url": "https://pypi.org/project/lrabbit-scrapy/2.0.0/",
    "requires_dist": [
      "PyMySQL (>=0.9.3)",
      "redispy (>=3.0.0)",
      "aiomysql (>=0.0.21)",
      "aiohttp (>=3.7.3)",
      "sqlalchemy (>=1.4.0)",
      "parsel (==1.6.0)",
      "requests (>=2.26.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "this is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some repeated code every time, using this small framework you can quickly crawl data into a file or database.",
    "version": "2.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12262992,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8e8933b4abcd1d43168a50ae89e02a666c498132dca8f403dd1bfe898a3540b8",
        "md5": "c738a39b0412078e3ce219fa340129f0",
        "sha256": "c4c99ecbf131bf7d08fd8b3364fc0c9f19ce8dea897847fd037515e130ae2dc1"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c738a39b0412078e3ce219fa340129f0",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 21135,
      "upload_time": "2021-11-23T08:20:36",
      "upload_time_iso_8601": "2021-11-23T08:20:36.539914Z",
      "url": "https://files.pythonhosted.org/packages/8e/89/33b4abcd1d43168a50ae89e02a666c498132dca8f403dd1bfe898a3540b8/lrabbit_scrapy-2.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f08233f6c7c9bbf9c49f63c70d46e9c1113bd0ceaebfa46eed89a248b4e436cd",
        "md5": "5e94b06afcf09cfeb5d53f0fb063ba25",
        "sha256": "6d194a0a4d9e6bd10a27e0680f94ce8e9ba20ad801552a95c534d3300e565356"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "5e94b06afcf09cfeb5d53f0fb063ba25",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 16620,
      "upload_time": "2021-11-23T08:20:38",
      "upload_time_iso_8601": "2021-11-23T08:20:38.141092Z",
      "url": "https://files.pythonhosted.org/packages/f0/82/33f6c7c9bbf9c49f63c70d46e9c1113bd0ceaebfa46eed89a248b4e436cd/lrabbit_scrapy-2.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}