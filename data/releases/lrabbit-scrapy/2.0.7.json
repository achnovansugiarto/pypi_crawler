{
  "info": {
    "author": "lrabbit",
    "author_email": "709343607@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "lrabbit_scrapy\n=====\n\nthis is a small spider,you can easy running. When you often need to crawl a single site, you don't have to redo some\nrepeated code every time, using this small framework you can quickly crawl data into a file or database.\n\n\nInstalling\n----------\n\n    $ pip install lrabbit_scrapy\n\nquick start\n----------------\n\n* create blog_spider.py\n* python3 -m lrabbit_scrapy new_scrapy blog\n    * this is file content\n```python\nfrom lrabbit_scrapy.spider import LrabbitSpider\nfrom lrabbit_scrapy.common_utils.network_helper import RequestSession\nfrom lrabbit_scrapy.common_utils.print_log_helper import LogUtils\nfrom lrabbit_scrapy.common_utils.all_in_one import FileStore\nimport os\nfrom lrabbit_scrapy.common_utils.mysql_helper import MysqlClient\nfrom parsel import Selector\n\n\nclass Spider(LrabbitSpider):\n    \"\"\"\n        spider_name : lrabbit blog spider\n    \"\"\"\n    # unique spider name\n    spider_name = \"lrabbit_blog\"\n    # max thread worker numbers\n    max_thread_num = 2\n    # is open for every thread a mysql connection,if your max_thread_num overpass 10 and  in code need mysql query ,you need open this config\n    thread_mysql_open = True\n    # reset all task_list,every restart program will init task list\n    reset_task_config = False\n    # open loop init_task_list ,when your task is all finish,and you want again ,you can open it\n    loop_task_config = False\n    # remove config option,if open it,then confirm option when you init task\n    remove_confirm_config = False\n    # config_path_name, this is env name ,is this code ,you need in linux to execute: export config_path=\"crawl.ini\"\n    config_env_name = \"config_path\"\n    # redis db_num\n    redis_db_config = 0\n    # debug log ,open tracback log\n    debug_config = False\n\n    def __init__(self):\n        super().__init__()\n        self.session = RequestSession()\n        self.proxy_session = RequestSession(proxies=None)\n        csv_path = os.path.join(os.path.abspath(os.getcwd()), f\"{self.spider_name}.csv\")\n        self.field_names = ['id', 'title', 'datetime']\n        self.blog_file = FileStore(file_path=csv_path, filed_name=self.field_names)\n\n    def worker(self, *args):\n        task = args[0]\n        mysql_client: MysqlClient\n        if len(args) == 2:\n            mysql_client = args[1]\n            # mysql_client.execute(\"\")\n        res = self.session.send_request(method='GET', url=f'http://www.lrabbit.life/post_detail/?id={task}')\n        selector = Selector(res.text)\n        title = selector.css(\".detail-title h1::text\").get()\n        datetime = selector.css(\".detail-info span::text\").get()\n        if title:\n            post_data = {\"id\": task, \"title\": title, 'datetime': datetime}\n            self.blog_file.write(post_data)\n            # when you succes get content update redis stat\n            self.update_stat_redis()\n        LogUtils.log_finish(task)\n\n    def init_task_list(self):\n\n        # you can get init task from mysql\n        # res = self.mysql_client.query(\"select id from rookie limit 100 \")\n        # return [task['id'] for task in res]\n        return [i for i in range(100)]\n\n\nif __name__ == '__main__':\n    spider = Spider()\n    spider.run()\n\n```\n\n* set config.ini and config env variable\n    * create crawl.ini,for example this file path is /root/crawl.ini\n    ```ini\n  [server]\n  mysql_user = root\n  mysql_password = 123456\n  mysql_database = test\n  mysql_host = 192.168.1.1\n  redis_user = lrabbit\n  redis_host = 192.168.1.1\n  redis_port = 6379\n  redis_password = 123456\n\n  [test]\n  mysql_user = root\n  mysql_password = 123456\n  mysql_database = test\n  mysql_host = 192.168.1.1\n  redis_user = lrabbit\n  redis_host = 192.168.1.1\n  redis_port = 6379\n  redis_password = 123456\n  ```\n    * set config env\n        * windows power shell\n        * $env:config_path = \"/root/crawl.ini\"\n        * linux\n        * export config_path=\"/root/crawl.ini\"\n\n\n* python3 blog_spider.py\n\n## other function\n* python3 blog_spider.py stat\n    * show task stat\n* python3 -m lrabbit-scrapy sslpass\n  * pass android ssl \nLinks\n-----\n\n- author: https://www.lrabbit.life/\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/litter-rabbit/lrabbit_scrapy",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lrabbit-scrapy",
    "package_url": "https://pypi.org/project/lrabbit-scrapy/",
    "platform": "",
    "project_url": "https://pypi.org/project/lrabbit-scrapy/",
    "project_urls": {
      "Bug Tracker": "https://github.com/litter-rabbit/lrabbit_scrapy/issues",
      "Homepage": "https://github.com/litter-rabbit/lrabbit_scrapy"
    },
    "release_url": "https://pypi.org/project/lrabbit-scrapy/2.0.7/",
    "requires_dist": [
      "parsel (==1.6.0)",
      "requests (>=2.26.0)",
      "PyMySQL (>=0.9.3)",
      "redispy (>=3.0.0)",
      "frida (>=15.0.0)",
      "frida-tools (>=10.4.1)"
    ],
    "requires_python": ">=3.6.8",
    "summary": "this is a small spider,you can easy running. When you often need to crawl a single site, you can reduce some repeated code every time, using this small framework you can quickly crawl data into a file or database.",
    "version": "2.0.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12262992,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dd40313e1383e840c51ce50e038f89accd8ed898195ba29efb6ea6e74880524a",
        "md5": "e6095f3fc24228deda1f53ba939913b5",
        "sha256": "586139ad0e4554814e25e2ce97489fd3fbe1635a92c307337561adba547b9d51"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e6095f3fc24228deda1f53ba939913b5",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.8",
      "size": 26972,
      "upload_time": "2021-12-07T09:04:43",
      "upload_time_iso_8601": "2021-12-07T09:04:43.886054Z",
      "url": "https://files.pythonhosted.org/packages/dd/40/313e1383e840c51ce50e038f89accd8ed898195ba29efb6ea6e74880524a/lrabbit_scrapy-2.0.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c729312dccbb607f9e1cc875f1c3dd29534ae6e88b2ee333b189d512e2fb5050",
        "md5": "17fbee5c5fbfd21e75c2f577c0149ee7",
        "sha256": "ec4785f9b4cc577134bfa40124f544ec2d5c0e9b4a2e86ca6bf10d4e9e9253a9"
      },
      "downloads": -1,
      "filename": "lrabbit_scrapy-2.0.7.tar.gz",
      "has_sig": false,
      "md5_digest": "17fbee5c5fbfd21e75c2f577c0149ee7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.8",
      "size": 22254,
      "upload_time": "2021-12-07T09:04:45",
      "upload_time_iso_8601": "2021-12-07T09:04:45.285110Z",
      "url": "https://files.pythonhosted.org/packages/c7/29/312dccbb607f9e1cc875f1c3dd29534ae6e88b2ee333b189d512e2fb5050/lrabbit_scrapy-2.0.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}