{
  "info": {
    "author": "Simon Descarpentries",
    "author_email": "contact@acoeuro.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: End Users/Desktop",
      "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Internet :: WWW/HTTP"
    ],
    "description": "`doc_crawler` can explore a website recursively from a given URL and retrieve, in the\ndescendant pages, the encountered document files (by default: PDF, ODT, CSV, RTF, DOC and XLS)\nbased on regular expression matching (typically against their extension).\n\n## Features\nDocuments can be listed to the output or downloaded (with the `--download` argument).\n\nTo address real life situations, one can log activity and follow the progress (with `--verbose`). \\\nAlso, the search can be limited to one page (with the `--single-page` argument).\n\nElse, documents can be downloaded from a given list of URL, that one may have previously\nproduced using default options of `doc_crawler` and an output redirection such as: \\\n`./doc_crawler.py http://… > url.lst`.\n\nTo finish the work, documents can be downloaded one by one if necessary, using the `--download-file`\nargument, which makes `doc_crawler` a tool sufficient by itself to assist you at every steps.\n\nBy default, the program waits a randomly-pick amount of seconds, between 1 and 5, before each\ndownload to avoid being rude toward the webserver it interacts with (and so avoid to be black-listed).\nThis behavior can be disabled (with a `--no-random-wait` and/or a `--wait=0` argument).\n\n## Options\n`--accept` optional regular expression (case insensitive) to keep matching document names. \\\n Example : `--accept=jpe?g$` will hopefully keep all : .JPG, .JPEG, .jpg, .jpeg \\\n`--download` directly downloads found documents if set, output their URL if not. \\\n`--single-page` limits the search for documents to download to the giver URL. \\\n`--verbose` creates a log file to keep trace of what was done. \\\n`--wait=x` change the default waiting time before each download (page or document). \\\n Example : `--wait=3` will wait between 1 and 3s before each download. Default is 5.\\\n`--no-random-wait` stops the random pick up of waiting times. `--wait=` or default is used.\\\n`--download-files` will download each documents which URL are listed in the given file. \\\n Example : `--download-files url.lst`\n`--download-file` will directly save in the current folder the URL-pointed document. \\\n Example : `--download-file http://…` \\\n\n## Usages\n`doc_crawler.py [--accept=jpe?g] [--download] [--single-page] [--verbose] http://…` \\\n`doc_crawler.py [--wait=3] [--no-random-wait] --download-files url.lst` \\\n`doc_crawler.py [--wait=0] --download-file http://…`\n\n`--wait=` and `--no-random-wait` arguments can be used with every form. \\\n`doc_crawler.py` works great with Tor : `torsocks doc_crawler.py http://…`\\\nFor a `pip` installed `doc_crawler`, the command can be invoked as follow: `python3 -m doc_crawler […] http://…`\n\n## Tests\nAround 20 doctests are included in `doc_crawler.py`. You can run them with the following\ncommand in the cloned repository root: \\\n`python3 -m doctest doc_crawler.py`\n\nTests can also be launched one by one using the `--test=XXX` argument:\\\n`python3 -m doc_crawler --test=download_file`\n\nTests are successfully passed if nothing is output.\n\n## Requirements\n* requests\n* yaml\n\nOne can install them Under Debian using the following command : `apt install python3-requests python3-yaml`\n\n## Licence\nGNU General Public License v3.0. See LICENCE file for more information.\n",
    "description_content_type": null,
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Siltaar/doc_crawler.py",
    "keywords": "crawler downloader recursive pdf-extractor web-crawler web-crawler-python file-download",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "doc_crawler",
    "package_url": "https://pypi.org/project/doc_crawler/",
    "platform": "",
    "project_url": "https://pypi.org/project/doc_crawler/",
    "project_urls": {
      "Homepage": "https://github.com/Siltaar/doc_crawler.py"
    },
    "release_url": "https://pypi.org/project/doc_crawler/1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Explore a website recursively and download all the wanted documents (PDF, ODT…)",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 3648620,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b48752d7f95d1cd72db0b1877dc4363f1f8f3227e4272d8f3edd89055fce27d4",
        "md5": "3de98da7d87403126c1095f60120a450",
        "sha256": "2ffe66eadb16480fc72ef28186b54157ed7144a11ccbbc8a0f90c58f050c3afe"
      },
      "downloads": -1,
      "filename": "doc_crawler-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "3de98da7d87403126c1095f60120a450",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 5347,
      "upload_time": "2017-08-29T16:36:54",
      "upload_time_iso_8601": "2017-08-29T16:36:54.145057Z",
      "url": "https://files.pythonhosted.org/packages/b4/87/52d7f95d1cd72db0b1877dc4363f1f8f3227e4272d8f3edd89055fce27d4/doc_crawler-1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}