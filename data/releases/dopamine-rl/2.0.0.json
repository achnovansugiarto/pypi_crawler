{
  "info": {
    "author": "The Dopamine Team",
    "author_email": "opensource@google.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Scientific/Engineering :: Mathematics",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# Dopamine\n\n<div align=\"center\">\n  <img src=\"https://google.github.io/dopamine/images/dopamine_logo.png\"><br><br>\n</div>\n\nDopamine is a research framework for fast prototyping of reinforcement learning\nalgorithms. It aims to fill the need for a small, easily grokked codebase in\nwhich users can freely experiment with wild ideas (speculative research).\n\nOur design principles are:\n\n* _Easy experimentation_: Make it easy for new users to run benchmark\n                          experiments.\n* _Flexible development_: Make it easy for new users to try out research ideas.\n* _Compact and reliable_: Provide implementations for a few, battle-tested\n                          algorithms.\n* _Reproducible_: Facilitate reproducibility in results. In particular, our\n                  setup follows the recommendations given by\n                  [Machado et al. (2018)][machado].\n\nIn the spirit of these principles, this first version focuses on supporting the\nstate-of-the-art, single-GPU *Rainbow* agent ([Hessel et al., 2018][rainbow])\napplied to Atari 2600 game-playing ([Bellemare et al., 2013][ale]).\nSpecifically, our Rainbow agent implements the three components identified as\nmost important by [Hessel et al.][rainbow]:\n\n* n-step Bellman updates (see e.g. [Mnih et al., 2016][a3c])\n* Prioritized experience replay ([Schaul et al., 2015][prioritized_replay])\n* Distributional reinforcement learning ([C51; Bellemare et al., 2017][c51])\n\nFor completeness, we also provide an implementation of DQN\n([Mnih et al., 2015][dqn]).\nFor additional details, please see our\n[documentation](https://github.com/google/dopamine/tree/master/docs).\n\nThis is not an official Google product.\n\n## What's new\n*  **30/01/2019:** Dopamine 2.0 now supports general discrete-domain gym\n   environments.\n*  **01/11/2018:** Download links for each individual checkpoint, to avoid\n   having to download all of the checkpoints.\n*  **29/10/2018:** Graph definitions now show up in Tensorboard.\n*  **16/10/2018:** Fixed a subtle bug in the IQN implementation and upated\n   the colab tools, the JSON files, and all the downloadable data.\n*  **18/09/2018:** Added support for double-DQN style updates for the\n   `ImplicitQuantileAgent`.\n   *  Can be enabled via the `double_dqn` constructor parameter.\n*  **18/09/2018:** Added support for reporting in-iteration losses directly from\n   the agent to Tensorboard.\n   *  Include the flag `--debug_mode` in your command line to enable it.\n   *  Control frequency of writes with the `summary_writing_frequency`\n      agent constructor parameter (defaults to `500`).\n*  **27/08/2018:** Dopamine launched!\n\n## Instructions\n### Install via source\nInstalling from source allows you to modify the agents and experiments as\nyou please, and is likely to be the pathway of choice for long-term use.\nThese instructions assume that you've already set up your favourite package\nmanager (e.g. `apt` on Ubuntu, `homebrew` on Mac OS X), and that a C++ compiler\nis available from the command-line (almost certainly the case if your favourite\npackage manager works).\n\nThe instructions below assume that you will be running Dopamine in a *virtual\nenvironment*. A virtual environment lets you control which dependencies are\ninstalled for which program; however, this step is optional and you may choose\nto ignore it.\n\nDopamine is a Tensorflow-based framework, and we recommend you also consult\nthe [Tensorflow documentation](https://www.tensorflow.org/install)\nfor additional details.\n\nFinally, these instructions are for Python 2.7. While Dopamine is Python 3\ncompatible, there may be some additional steps needed during installation.\n\n#### Ubuntu\n\nFirst set up the virtual environment:\n\n```\nsudo apt-get update && sudo apt-get install virtualenv\nvirtualenv --python=python2.7 dopamine-env\nsource dopamine-env/bin/activate\n```\n\nThis will create a directory called `dopamine-env` in which your virtual\nenvironment lives. The last command activates the environment.\n\nThen, install the dependencies to Dopamine. If you don't have access to a\nGPU, then replace `tensorflow-gpu` with `tensorflow` in the line below\n(see [Tensorflow instructions](https://www.tensorflow.org/install/install_linux)\nfor details).\n\n```\nsudo apt-get update && sudo apt-get install cmake zlib1g-dev\npip install absl-py atari-py gin-config gym opencv-python tensorflow-gpu\n```\n\nDuring installation, you may safely ignore the following error message:\n*tensorflow 1.10.1 has requirement numpy<=1.14.5,>=1.13.3, but you'll have\nnumpy 1.15.1 which is incompatible*.\n\nFinally, download the Dopamine source, e.g.\n\n```\ngit clone https://github.com/google/dopamine.git\n```\n\n#### Mac OS X\n\nFirst set up the virtual environment:\n\n```\npip install virtualenv\nvirtualenv --python=python2.7 dopamine-env\nsource dopamine-env/bin/activate\n```\n\nThis will create a directory called `dopamine-env` in which your virtual\nenvironment lives. The last command activates the environment.\n\nThen, install the dependencies to Dopamine:\n\n```\nbrew install cmake zlib\npip install absl-py atari-py gin-config gym opencv-python tensorflow\n```\n\nDuring installation, you may safely ignore the following error message:\n*tensorflow 1.10.1 has requirement numpy<=1.14.5,>=1.13.3, but you'll have\nnumpy 1.15.1 which is incompatible*.\n\nFinally, download the Dopamine source, e.g.\n\n```\ngit clone https://github.com/google/dopamine.git\n```\n\n#### Running tests\n\nYou can test whether the installation was successful by running the following:\n\n```\nexport PYTHONPATH=${PYTHONPATH}:.\npython tests/dopamine/atari_init_test.py\n```\n\nThe entry point to the standard Atari 2600 experiment is\n[`dopamine/discrete_domains/train.py`](https://github.com/google/dopamine/blob/master/dopamine/discrete_domains/train.py).\nTo run the basic DQN agent,\n\n```\npython -um dopamine.discrete_domains.train \\\n  --base_dir=/tmp/dopamine \\\n  --gin_files='dopamine/agents/dqn/configs/dqn.gin'\n```\n\nBy default, this will kick off an experiment lasting 200 million frames.\nThe command-line interface will output statistics about the latest training\nepisode:\n\n```\n[...]\nI0824 17:13:33.078342 140196395337472 tf_logging.py:115] gamma: 0.990000\nI0824 17:13:33.795608 140196395337472 tf_logging.py:115] Beginning training...\nSteps executed: 5903 Episode length: 1203 Return: -19.\n```\n\nTo get finer-grained information about the process,\nyou can adjust the experiment parameters in\n[`dopamine/agents/dqn/configs/dqn.gin`](https://github.com/google/dopamine/blob/master/dopamine/agents/dqn/configs/dqn.gin),\nin particular by reducing `Runner.training_steps` and `Runner.evaluation_steps`,\nwhich together determine the total number of steps needed to complete an\niteration. This is useful if you want to inspect log files or checkpoints, which\nare generated at the end of each iteration.\n\nMore generally, the whole of Dopamine is easily configured using the\n[gin configuration framework](https://github.com/google/gin-config).\n\n#### Non-Atari discrete environments\n\nWe provide sample configuration files for training an agent on Cartpole and\nAcrobot. For example, to train C51 on Cartpole with default settings, run the\nfollowing command:\n\n```\npython -um dopamine.discrete_domains.train \\\n  --base_dir=/tmp/dopamine \\\n  --gin_files='dopamine/agents/rainbow/configs/c51_cartpole.gin'\n```\n\nYou can train Rainbow on Acrobot with the following command:\n\n```\npython -um dopamine.discrete_domains.train \\\n  --base_dir=/tmp/dopamine \\\n  --gin_files='dopamine/agents/rainbow/configs/rainbow_acrobot.gin'\n```\n\n\n### Install as a library\nAn easy, alternative way to install Dopamine is as a Python library:\n\n```\n# Alternatively brew install, see Mac OS X instructions above.\nsudo apt-get update && sudo apt-get install cmake\npip install dopamine-rl\npip install atari-py\n```\n\nDepending on your particular system configuration, you may also need to install\nzlib (see \"Install via source\" above).\n\n#### Running tests\n>From the root directory, tests can be run with a command such as:\n\n```\npython -um tests.agents.rainbow.rainbow_agent_test\n```\n\n### References\n\n[Bellemare et al., *The Arcade Learning Environment: An evaluation platform for\ngeneral agents*. Journal of Artificial Intelligence Research, 2013.][ale]\n\n[Machado et al., *Revisiting the Arcade Learning Environment: Evaluation\nProtocols and Open Problems for General Agents*, Journal of Artificial\nIntelligence Research, 2018.][machado]\n\n[Hessel et al., *Rainbow: Combining Improvements in Deep Reinforcement Learning*.\nProceedings of the AAAI Conference on Artificial Intelligence, 2018.][rainbow]\n\n[Mnih et al., *Human-level Control through Deep Reinforcement Learning*. Nature,\n2015.][dqn]\n\n[Mnih et al., *Asynchronous Methods for Deep Reinforcement Learning*. Proceedings\nof the International Conference on Machine Learning, 2016.][a3c]\n\n[Schaul et al., *Prioritized Experience Replay*. Proceedings of the International\nConference on Learning Representations, 2016.][prioritized_replay]\n\n### Giving credit\n\nIf you use Dopamine in your work, we ask that you cite our\n[white paper][dopamine_paper]. Here is an example BibTeX entry:\n\n```\n@article{castro18dopamine,\n  author    = {Pablo Samuel Castro and\n               Subhodeep Moitra and\n               Carles Gelada and\n               Saurabh Kumar and\n               Marc G. Bellemare},\n  title     = {Dopamine: {A} {R}esearch {F}ramework for {D}eep {R}einforcement {L}earning},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1812.06110},\n  archivePrefix = {arXiv}\n}\n```\n\n\n\n[machado]: https://jair.org/index.php/jair/article/view/11182\n[ale]: https://jair.org/index.php/jair/article/view/10819\n[dqn]: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\n[a3c]: http://proceedings.mlr.press/v48/mniha16.html\n[prioritized_replay]: https://arxiv.org/abs/1511.05952\n[c51]: http://proceedings.mlr.press/v70/bellemare17a.html\n[rainbow]: https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/download/17204/16680\n[iqn]: https://arxiv.org/abs/1806.06923\n[dopamine_paper]: https://arxiv.org/abs/1812.06110\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/google/dopamine",
    "keywords": "dopamine reinforcement-learning python machine learning",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dopamine-rl",
    "package_url": "https://pypi.org/project/dopamine-rl/",
    "platform": "",
    "project_url": "https://pypi.org/project/dopamine-rl/",
    "project_urls": {
      "Bug Reports": "https://github.com/google/dopamine/issues",
      "Documentation": "https://github.com/google/dopamine",
      "Homepage": "https://github.com/google/dopamine",
      "Source": "https://github.com/google/dopamine"
    },
    "release_url": "https://pypi.org/project/dopamine-rl/2.0.0/",
    "requires_dist": [
      "absl-py (>=0.2.2)",
      "gin-config (>=0.1.1)",
      "gym (>=0.10.5)",
      "opencv-python (>=3.4.1.15)"
    ],
    "requires_python": "",
    "summary": "Dopamine: A framework for flexible Reinforcement Learning research",
    "version": "2.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15170036,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "61fd2b383db89f761b9e6ff33b2e29d742762024f45a0bdcdfc1a125672c23fb",
        "md5": "6ff2875b5c1d0083d12e5e434ebc50b6",
        "sha256": "4c8171a00b2e299dd6e68e2ed87c85acd575d8e30d4ac56fe3e869a3e7eb1797"
      },
      "downloads": -1,
      "filename": "dopamine_rl-2.0.0-py2-none-any.whl",
      "has_sig": false,
      "md5_digest": "6ff2875b5c1d0083d12e5e434ebc50b6",
      "packagetype": "bdist_wheel",
      "python_version": "py2",
      "requires_python": null,
      "size": 71879,
      "upload_time": "2019-01-31T17:43:18",
      "upload_time_iso_8601": "2019-01-31T17:43:18.444665Z",
      "url": "https://files.pythonhosted.org/packages/61/fd/2b383db89f761b9e6ff33b2e29d742762024f45a0bdcdfc1a125672c23fb/dopamine_rl-2.0.0-py2-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0135fd899ffc528f302660109fce0af54d9e6d601fb384d8f143f92cc2de9dff",
        "md5": "ae8cffb1b5dd35776c1a23d4f46e58b1",
        "sha256": "04d706685dfb9622efaddf7bc5839059bbf64ae0500c75a49eac7481cbfa4098"
      },
      "downloads": -1,
      "filename": "dopamine_rl-2.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ae8cffb1b5dd35776c1a23d4f46e58b1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 48793,
      "upload_time": "2019-01-31T17:43:20",
      "upload_time_iso_8601": "2019-01-31T17:43:20.302801Z",
      "url": "https://files.pythonhosted.org/packages/01/35/fd899ffc528f302660109fce0af54d9e6d601fb384d8f143f92cc2de9dff/dopamine_rl-2.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}