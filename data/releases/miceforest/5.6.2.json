{
  "info": {
    "author": "Samuel Wilson",
    "author_email": "samwilson303@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "\n[![tests +\nmypy](https://github.com/AnotherSamWilson/miceforest/actions/workflows/run_tests.yml/badge.svg)](https://github.com/AnotherSamWilson/miceforest/actions/workflows/run_tests.yml)\n[![Documentation\nStatus](https://readthedocs.org/projects/miceforest/badge/?version=latest)](https://miceforest.readthedocs.io/en/latest/?badge=latest)\n[![CodeCov](https://codecov.io/gh/AnotherSamWilson/miceforest/branch/master/graphs/badge.svg?branch=master&service=github)](https://codecov.io/gh/AnotherSamWilson/miceforest)\n[![Downloads](https://static.pepy.tech/badge/miceforest)](https://pepy.tech/project/miceforest)  \n[![Pypi](https://img.shields.io/pypi/v/miceforest.svg)](https://pypi.python.org/pypi/miceforest)\n[![Conda\nVersion](https://img.shields.io/conda/vn/conda-forge/miceforest.svg)](https://anaconda.org/conda-forge/miceforest)\n[![PyVersions](https://img.shields.io/pypi/pyversions/miceforest.svg?logo=python&logoColor=white)](https://pypi.org/project/miceforest/)\n<!-- [![MIT license](http://img.shields.io/badge/license-MIT-brightgreen.svg)](http://opensource.org/licenses/MIT) -->\n<!-- [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)   -->\n<!-- [![DEV_Version_Badge](https://img.shields.io/badge/Dev-5.6.1-blue.svg)](https://pypi.org/project/miceforest/) -->\n\n## miceforest: Fast, Memory Efficient Imputation with LightGBM\n\n<a href='https://github.com/AnotherSamWilson/miceforest'><img src='https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/icon.png' align=\"right\" height=\"300\" /></a>\n\nFast, memory efficient Multiple Imputation by Chained Equations (MICE)\nwith lightgbm. The R version of this package may be found\n[here](https://github.com/FarrellDay/miceRanger).\n\n`miceforest` was designed to be:\n\n  - **Fast**\n      - Uses lightgbm as a backend\n      - Has efficient mean matching solutions.\n      - Can utilize GPU training\n  - **Flexible**\n      - Can impute pandas dataframes and numpy arrays\n      - Handles categorical data automatically\n      - Fits into a sklearn pipeline\n      - User can customize every aspect of the imputation process\n  - **Production Ready**\n      - Can impute new, unseen datasets quickly\n      - Kernels are efficiently compressed during saving and loading\n      - Data can be imputed in place to save memory\n      - Can build models on non-missing data\n\nThis document contains a thorough walkthrough of the package,\nbenchmarks, and an introduction to multiple imputation. More information\non MICE can be found in Stef van Buuren’s excellent online book, which\nyou can find\n[here](https://stefvanbuuren.name/fimd/ch-introduction.html).\n\n#### Table of Contents:\n\n  - [Package\n    Meta](https://github.com/AnotherSamWilson/miceforest#Package-Meta)\n  - [The\n    Basics](https://github.com/AnotherSamWilson/miceforest#The-Basics)\n      - [Basic\n        Examples](https://github.com/AnotherSamWilson/miceforest#Basic-Examples)\n      - [Customizing LightGBM\n        Parameters](https://github.com/AnotherSamWilson/miceforest#Customizing-LightGBM-Parameters)\n      - [Available Mean Match\n        Schemes](https://github.com/AnotherSamWilson/miceforest#Controlling-Tree-Growth)\n      - [Imputing New Data with Existing\n        Models](https://github.com/AnotherSamWilson/miceforest#Imputing-New-Data-with-Existing-Models)\n      - [Saving and Loading\n        Kernels](https://github.com/AnotherSamWilson/miceforest#Saving-and-Loading-Kernels)\n      - [Implementing sklearn\n        Pipelines](https://github.com/AnotherSamWilson/miceforest#Implementing-sklearn-Pipelines)\n  - [Advanced\n    Features](https://github.com/AnotherSamWilson/miceforest#Advanced-Features)\n      - [Customizing the Imputation\n        Process](https://github.com/AnotherSamWilson/miceforest#Customizing-the-Imputation-Process)\n      - [Building Models on Nonmissing\n        Data](https://github.com/AnotherSamWilson/miceforest#Building-Models-on-Nonmissing-Data)\n      - [Tuning\n        Parameters](https://github.com/AnotherSamWilson/miceforest#Tuning-Parameters)\n      - [On\n        Reproducibility](https://github.com/AnotherSamWilson/miceforest#On-Reproducibility)\n      - [How to Make the Process\n        Faster](https://github.com/AnotherSamWilson/miceforest#How-to-Make-the-Process-Faster)\n      - [Imputing Data In\n        Place](https://github.com/AnotherSamWilson/miceforest#Imputing-Data-In-Place)\n  - [Diagnostic\n    Plotting](https://github.com/AnotherSamWilson/miceforest#Diagnostic-Plotting)\n      - [Imputed\n        Distributions](https://github.com/AnotherSamWilson/miceforest#Distribution-of-Imputed-Values)\n      - [Correlation\n        Convergence](https://github.com/AnotherSamWilson/miceforest#Convergence-of-Correlation)\n      - [Variable\n        Importance](https://github.com/AnotherSamWilson/miceforest#Variable-Importance)\n      - [Mean\n        Convergence](https://github.com/AnotherSamWilson/miceforest#Variable-Importance)\n  - [Benchmarks](https://github.com/AnotherSamWilson/miceforest#Benchmarks)\n  - [Using the Imputed\n    Data](https://github.com/AnotherSamWilson/miceforest#Using-the-Imputed-Data)\n  - [The MICE\n    Algorithm](https://github.com/AnotherSamWilson/miceforest#The-MICE-Algorithm)\n      - [Introduction](https://github.com/AnotherSamWilson/miceforest#The-MICE-Algorithm)\n      - [Common Use\n        Cases](https://github.com/AnotherSamWilson/miceforest#Common-Use-Cases)\n      - [Predictive Mean\n        Matching](https://github.com/AnotherSamWilson/miceforest#Predictive-Mean-Matching)\n      - [Effects of Mean\n        Matching](https://github.com/AnotherSamWilson/miceforest#Effects-of-Mean-Matching)\n\n## Package Meta\n\n### Installation\n\nThis package can be installed using either pip or conda, through\nconda-forge:\n\n``` bash\n# Using pip\n$ pip install miceforest --no-cache-dir\n\n# Using conda\n$ conda install -c conda-forge miceforest\n```\n\nYou can also download the latest development version from this\nrepository. If you want to install from github with conda, you must\nfirst run `conda install pip git`.\n\n``` bash\n$ pip install git+https://github.com/AnotherSamWilson/miceforest.git\n```\n\n### Classes\n\nmiceforest has 3 main classes which the user will interact with:\n\n  - [`ImputationKernel`](https://miceforest.readthedocs.io/en/latest/ik/miceforest.ImputationKernel.html#miceforest.ImputationKernel)\n    - This class contains the raw data off of which the `mice` algorithm\n    is performed. During this process, models will be trained, and the\n    imputed (predicted) values will be stored. These values can be used\n    to fill in the missing values of the raw data. The raw data can be\n    copied, or referenced directly. Models can be saved, and used to\n    impute new datasets.\n  - [`ImputedData`](https://miceforest.readthedocs.io/en/latest/ik/miceforest.ImputedData.html#miceforest.ImputedData)\n    - The result of `ImputationKernel.impute_new_data(new_data)`. This\n    contains the raw data in `new_data` as well as the imputed values.  \n  - [`MeanMatchScheme`](https://miceforest.readthedocs.io/en/latest/ik/miceforest.MeanMatchScheme.html#miceforest.MeanMatchScheme)\n    - Determines how mean matching should be carried out. There are 3\n    built-in mean match schemes available in miceforest, discussed\n    below.\n\n## The Basics\n\nWe will be looking at a few simple examples of imputation. We need to\nload the packages, and define the data:\n\n``` python\nimport miceforest as mf\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\n# Load data and introduce missing values\niris = pd.concat(load_iris(as_frame=True,return_X_y=True),axis=1)\niris.rename({\"target\": \"species\"}, inplace=True, axis=1)\niris['species'] = iris['species'].astype('category')\niris_amp = mf.ampute_data(iris,perc=0.25,random_state=1991)\n```\n\n### Basic Examples\n\nIf you only want to create a single imputed dataset, you can use\n[`ImputationKernel`](https://miceforest.readthedocs.io/en/latest/ik/miceforest.ImputationKernel.html#miceforest.ImputationKernel)\nwith some default settings:\n\n``` python\n# Create kernel. \nkds = mf.ImputationKernel(\n  iris_amp,\n  save_all_iterations=True,\n  random_state=1991\n)\n\n# Run the MICE algorithm for 2 iterations\nkds.mice(2)\n\n# Return the completed dataset.\niris_complete = kds.complete_data()\n```\n\nThere are also an array of plotting functions available, these are\ndiscussed below in the section [Diagnostic\nPlotting](https://github.com/AnotherSamWilson/miceforest#Diagnostic-Plotting).\n\nWe usually don’t want to impute just a single dataset. In statistics,\nmultiple imputation is a process by which the uncertainty/other effects\ncaused by missing values can be examined by creating multiple different\nimputed datasets.\n[`ImputationKernel`](https://miceforest.readthedocs.io/en/latest/ik/miceforest.ImputationKernel.html#miceforest.ImputationKernel)\ncan contain an arbitrary number of different datasets, all of which have\ngone through mutually exclusive imputation processes:\n\n``` python\n# Create kernel. \nkernel = mf.ImputationKernel(\n  iris_amp,\n  datasets=4,\n  save_all_iterations=True,\n  random_state=1\n)\n\n# Run the MICE algorithm for 2 iterations on each of the datasets\nkernel.mice(2)\n\n# Printing the kernel will show you some high level information.\nprint(kernel)\n```\n\n    ## \n    ##               Class: ImputationKernel\n    ##            Datasets: 4\n    ##          Iterations: 2\n    ##        Data Samples: 150\n    ##        Data Columns: 5\n    ##   Imputed Variables: 5\n    ## save_all_iterations: True\n\nAfter we have run mice, we can obtain our completed dataset directly\nfrom the kernel:\n\n``` python\ncompleted_dataset = kernel.complete_data(dataset=2)\nprint(completed_dataset.isnull().sum(0))\n```\n\n    ## sepal length (cm)    0\n    ## sepal width (cm)     0\n    ## petal length (cm)    0\n    ## petal width (cm)     0\n    ## species              0\n    ## dtype: int64\n\n### Customizing LightGBM Parameters\n\nParameters can be passed directly to lightgbm in several different ways.\nParameters you wish to apply globally to every model can simply be\npassed as kwargs to `mice`:\n\n``` python\n# Run the MICE algorithm for 1 more iteration on the kernel with new parameters\nkernel.mice(iterations=1,n_estimators=50)\n```\n\nYou can also pass pass variable-specific arguments to\n`variable_parameters` in mice. For instance, let’s say you noticed the\nimputation of the `[species]` column was taking a little longer, because\nit is multiclass. You could decrease the n\\_estimators specifically for\nthat column with:\n\n``` python\n# Run the MICE algorithm for 2 more iterations on the kernel \nkernel.mice(\n  iterations=1,\n  variable_parameters={'species': {'n_estimators': 25}},\n  n_estimators=50\n)\n\n# Let's get the actual models for these variables:\nspecies_model = kernel.get_model(dataset=0,variable=\"species\")\nsepalwidth_model = kernel.get_model(dataset=0,variable=\"sepal width (cm)\")\n\nprint(\nf\"\"\"Species used {str(species_model.params[\"num_iterations\"])} iterations\nSepal Width used {str(sepalwidth_model.params[\"num_iterations\"])} iterations\n\"\"\"\n)\n```\n\n    ## Species used 25 iterations\n    ## Sepal Width used 50 iterations\n\nIn this scenario, any parameters specified in `variable_parameters`\ntakes presidence over the kwargs.\n\nSince we can pass any parameters we want to LightGBM, we can completely\ncustomize how our models are built. That includes how the data should be\nmodeled. If your data contains count data, or any other data which can\nbe parameterized by lightgbm, you can simply specify that variable to be\nmodeled with the corresponding objective function.\n\nFor example, let’s pretend `sepal width (cm)` is a count field which can\nbe parameterized by a Poisson distribution. Let’s also change our\nboosting method to gradient boosted trees:\n\n``` python\n# Create kernel. \ncust_kernel = mf.ImputationKernel(\n  iris_amp,\n  datasets=1,\n  random_state=1\n)\n\ncust_kernel.mice(\n  iterations=1, \n  variable_parameters={'sepal width (cm)': {'objective': 'poisson'}},\n  boosting = 'gbdt',\n  min_sum_hessian_in_leaf=0.01\n)\n```\n\nOther nice parameters like `monotone_constraints` can also be passed.\nSetting the parameter `device: 'gpu'` will utilize GPU learning, if\nLightGBM is set up to do this on your machine.\n\n### Available Mean Match Schemes\n\nNote: It is probably a good idea to read [this\nsection](https://github.com/AnotherSamWilson/miceforest#Predictive-Mean-Matching)\nfirst, to get some context on how mean matching works.\n\nThe class `miceforest.MeanMatchScheme` contains information about how\nmean matching should be performed, such as:\n\n1)  Mean matching functions  \n2)  Mean matching candidates  \n3)  How to get predictions from a lightgbm model  \n4)  The datatypes predictions are stored as\n\nThere are three pre-built mean matching schemes that come with\n`miceforest`:\n\n``` python\nfrom miceforest import (\n  mean_match_default,\n  mean_match_fast_cat,\n  mean_match_shap\n)\n\n# To get information for each, use help()\n# help(mean_match_default)\n```\n\nThese schemes mostly differ in their strategy for performing mean\nmatching\n\n  - **mean\\_match\\_default** - medium speed, medium imputation quality\n      - Categorical: perform a K Nearest Neighbors search on the\n        candidate class probabilities, where K = mmc. Select 1 at\n        random, and choose the associated candidate value as the\n        imputation value.  \n      - Numeric: Perform a K Nearest Neighbors search on the candidate\n        predictions, where K = mmc. Select 1 at random, and choose the\n        associated candidate value as the imputation value.  \n  - **mean\\_match\\_fast\\_cat** - fastest speed, lowest imputation\n    quality\n      - Categorical: return class based on random draw weighted by class\n        probability for each sample.  \n      - Numeric: perform a K Nearest Neighbors search on the candidate\n        class probabilities, where K = mmc. Select 1 at random, and\n        choose the associated candidate value as the imputation value.  \n  - **mean\\_match\\_shap** - slowest speed, highest imputation quality\n    for large datasets\n      - Categorical: perform a K Nearest Neighbors search on the\n        candidate prediction shap values, where K = mmc. Select 1 at\n        random, and choose the associated candidate value as the\n        imputation value.  \n      - Numeric: perform a K Nearest Neighbors search on the candidate\n        prediction shap values, where K = mmc. Select 1 at random, and\n        choose the associated candidate value as the imputation value.\n\nAs a special case, if the mean\\_match\\_candidates is set to 0, the\nfollowing behavior is observed for all schemes:\n\n  - Categorical: the class with the highest probability is chosen.  \n  - Numeric: the predicted value is used\n\nThese mean matching schemes can be updated and customized, we show an\nexample below in the advanced section.\n\n### Imputing New Data with Existing Models\n\nMultiple Imputation can take a long time. If you wish to impute a\ndataset using the MICE algorithm, but don’t have time to train new\nmodels, it is possible to impute new datasets using a `ImputationKernel`\nobject. The `impute_new_data()` function uses the models collected by\n`ImputationKernel` to perform multiple imputation without updating the\nmodels at each iteration:\n\n``` python\n# Our 'new data' is just the first 15 rows of iris_amp\nfrom datetime import datetime\n\n# Define our new data as the first 15 rows\nnew_data = iris_amp.iloc[range(15)]\n\n# Imputing new data can often be made faster by \n# first compiling candidate predictions\nkernel.compile_candidate_preds()\n\nstart_t = datetime.now()\nnew_data_imputed = kernel.impute_new_data(new_data=new_data)\nprint(f\"New Data imputed in {(datetime.now() - start_t).total_seconds()} seconds\")\n```\n\n    ## New Data imputed in 0.479109 seconds\n\nAll of the imputation parameters (variable\\_schema,\nmean\\_match\\_candidates, etc) will be carried over from the original\n`ImputationKernel` object. When mean matching, the candidate values are\npulled from the original kernel dataset. To impute new data, the\n`save_models` parameter in `ImputationKernel` must be \\> 0. If\n`save_models == 1`, the model from the latest iteration is saved for\neach variable. If `save_models > 1`, the model from each iteration is\nsaved. This allows for new data to be imputed in a more similar fashion\nto the original mice procedure.\n\n### Saving and Loading Kernels\n\nKernels can be saved using the `.save_kernel()` method, and then loaded\nagain using the `utils.load_kernel()` function. Internally, this\nprocedure uses `blosc` and `dill` packages to do the following:\n\n1.  Convert working data to parquet bytes (if it is a pandas dataframe)\n2.  Serialize the kernel  \n3.  Compress this serialization  \n4.  Save to a file\n\n### Implementing sklearn Pipelines\n\nkernels can be fit into sklearn pipelines to impute training and scoring\ndatasets:\n\n``` python\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport miceforest as mf\n\n# Define our data\nX, y = make_classification(random_state=0)\n\n# Ampute and split the training data\nX = mf.utils.ampute_data(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Initialize our miceforest kernel. datasets parameter should be 1,\n# we don't want to return multiple datasets.\npipe_kernel = mf.ImputationKernel(X_train, datasets=1)\n\n# Define our pipeline\npipe = Pipeline([\n    ('impute', pipe_kernel),\n    ('scaler', StandardScaler()),\n])\n\n# Fit on and transform our training data.\n# Only use 2 iterations of mice.\nX_train_t = pipe.fit_transform(\n    X_train,\n    y_train,\n    impute__iterations=2\n)\n\n# Transform the test data as well\nX_test_t = pipe.transform(X_test)\n\n# Show that neither now have missing values.\nassert not np.any(np.isnan(X_train_t))\nassert not np.any(np.isnan(X_test_t))\n```\n\n## Advanced Features\n\nMultiple imputation is a complex process. However, `miceforest` allows\nall of the major components to be switched out and customized by the\nuser.\n\n### Customizing the Imputation Process\n\nIt is possible to heavily customize our imputation procedure by\nvariable. By passing a named list to `variable_schema`, you can specify\nthe predictor variables for each imputed variable. You can also specify\n`mean_match_candidates` and `data_subset` by variable by passing a dict\nof valid values, with variable names as keys. You can even replace the\nentire default mean matching function for certain objectives if desired.\nBelow is an *extremely* convoluted setup, which you would probably never\nwant to use. It simply shows what is possible:\n\n``` python\n# Use the default mean match schema as our base\nfrom miceforest import mean_match_default\nmean_match_custom = mean_match_default.copy()\n\n# Define a mean matching function that \n# just randomly shuffles the predictions\ndef custom_mmf(bachelor_preds):\n    np.random.shuffle(bachelor_preds)\n    return bachelor_preds\n\n# Specify that our custom function should be\n# used to perform mean matching on any variable\n# that was modeled with a poisson objective:\nmean_match_custom.set_mean_match_function(\n  {\"poisson\": custom_mmf}\n)\n\n# Set the mean match candidates by variable\nmean_match_custom.set_mean_match_candidates(\n  {\n      'sepal width (cm)': 3,\n      'petal width (cm)': 0\n  }\n)\n\n# Define which variables should be used to model others\nvariable_schema = {\n    'sepal width (cm)': ['species','petal width (cm)'],\n    'petal width (cm)': ['species','sepal length (cm)']\n}\n\n# Subset the candidate data to 50 rows for sepal width (cm).\nvariable_subset = {\n  'sepal width (cm)': 50\n}\n\n# Specify that petal width (cm) should be modeled by the\n# poisson objective. Our custom mean matching function\n# above will be used for this variable.\nvariable_parameters = {\n  'petal width (cm)': {\"objective\": \"poisson\"}\n}\n\ncust_kernel = mf.ImputationKernel(\n    iris_amp,\n    datasets=3,\n    mean_match_scheme=mean_match_custom,\n    variable_schema=variable_schema,\n    data_subset=variable_subset\n)\ncust_kernel.mice(iterations=1, variable_parameters=variable_parameters)\n```\n\nThe mean matching function can take any number of the following\narguments. If a function does not take one of these arguments, then the\nprocess will not prepare that data for mean matching.\n\n``` python\nfrom miceforest.MeanMatchScheme import AVAILABLE_MEAN_MATCH_ARGS\nprint(\"\\n\".join(AVAILABLE_MEAN_MATCH_ARGS))\n```\n\n    ## mean_match_candidates\n    ## lgb_booster\n    ## bachelor_preds\n    ## bachelor_features\n    ## candidate_values\n    ## candidate_features\n    ## candidate_preds\n    ## random_state\n    ## hashed_seeds\n\n### Building Models on Nonmissing Data\n\nThe MICE process itself is used to impute missing data in a dataset.\nHowever, sometimes a variable can be fully recognized in the training\ndata, but needs to be imputed later on in a different dataset. It is\npossible to train models to impute variables even if they have no\nmissing values by setting `train_nonmissing=True`. In this case,\n`variable_schema` is treated as the list of variables to train models\non. `imputation_order` only affects which variables actually have their\nvalues imputed, it does not affect which variables have models trained:\n\n``` python\norig_missing_cols = [\"sepal length (cm)\", \"sepal width (cm)\"]\nnew_missing_cols = [\"sepal length (cm)\", \"sepal width (cm)\", \"species\"]\n\n# Training data only contains 2 columns with missing data\niris_amp2 = iris.copy()\niris_amp2[orig_missing_cols] = mf.ampute_data(\n  iris_amp2[orig_missing_cols],\n  perc=0.25,\n  random_state=1991\n)\n\n# Specify that models should also be trained for species column\nvar_sch = new_missing_cols\n\ncust_kernel = mf.ImputationKernel(\n    iris_amp2,\n    datasets=1,\n    variable_schema=var_sch,\n    train_nonmissing=True\n)\ncust_kernel.mice(1)\n\n# New data has missing values in species column\niris_amp2_new = iris.iloc[range(10),:].copy()\niris_amp2_new[new_missing_cols] = mf.ampute_data(\n  iris_amp2_new[new_missing_cols],\n  perc=0.25,\n  random_state=1991\n)\n\n# Species column can still be imputed\niris_amp2_new_imp = cust_kernel.impute_new_data(iris_amp2_new)\niris_amp2_new_imp.complete_data(0).isnull().sum()\n```\n\n    ## sepal length (cm)    0\n    ## sepal width (cm)     0\n    ## petal length (cm)    0\n    ## petal width (cm)     0\n    ## species              0\n    ## dtype: int64\n\nHere, we knew that the species column in our new data would need to be\nimputed. Therefore, we specified that a model should be built for all 3\nvariables in the `variable_schema` (passing a dict of target - feature\npairs would also have worked).\n\n### Tuning Parameters\n\n`miceforest` allows you to tune the parameters on a kernel dataset.\nThese parameters can then be used to build the models in future\niterations of mice. In its most simple invocation, you can just call the\nfunction with the desired optimization steps:\n\n``` python\n# Using the first ImputationKernel in kernel to tune parameters\n# with the default settings.\noptimal_parameters, losses = kernel.tune_parameters(\n  dataset=0,\n  optimization_steps=5\n)\n\n# Run mice with our newly tuned parameters.\nkernel.mice(1, variable_parameters=optimal_parameters)\n\n# The optimal parameters are kept in ImputationKernel.optimal_parameters:\nprint(optimal_parameters)\n```\n\n    ## {0: {'boosting': 'gbdt', 'num_iterations': 165, 'max_depth': 8, 'num_leaves': 20, 'min_data_in_leaf': 1, 'min_sum_hessian_in_leaf': 0.1, 'min_gain_to_split': 0.0, 'bagging_fraction': 0.2498838792503861, 'feature_fraction': 1.0, 'feature_fraction_bynode': 0.6020460898858531, 'bagging_freq': 1, 'verbosity': -1, 'objective': 'regression', 'learning_rate': 0.02, 'cat_smooth': 17.807024990062555}, 1: {'boosting': 'gbdt', 'num_iterations': 94, 'max_depth': 8, 'num_leaves': 14, 'min_data_in_leaf': 4, 'min_sum_hessian_in_leaf': 0.1, 'min_gain_to_split': 0.0, 'bagging_fraction': 0.7802435334180599, 'feature_fraction': 1.0, 'feature_fraction_bynode': 0.6856668707631843, 'bagging_freq': 1, 'verbosity': -1, 'objective': 'regression', 'learning_rate': 0.02, 'cat_smooth': 4.802568893662679}, 2: {'boosting': 'gbdt', 'num_iterations': 229, 'max_depth': 8, 'num_leaves': 4, 'min_data_in_leaf': 8, 'min_sum_hessian_in_leaf': 0.1, 'min_gain_to_split': 0.0, 'bagging_fraction': 0.9565982004313843, 'feature_fraction': 1.0, 'feature_fraction_bynode': 0.6065024947204825, 'bagging_freq': 1, 'verbosity': -1, 'objective': 'regression', 'learning_rate': 0.02, 'cat_smooth': 17.2138799939537}, 3: {'boosting': 'gbdt', 'num_iterations': 182, 'max_depth': 8, 'num_leaves': 20, 'min_data_in_leaf': 4, 'min_sum_hessian_in_leaf': 0.1, 'min_gain_to_split': 0.0, 'bagging_fraction': 0.7251674145835884, 'feature_fraction': 1.0, 'feature_fraction_bynode': 0.9262368919526676, 'bagging_freq': 1, 'verbosity': -1, 'objective': 'regression', 'learning_rate': 0.02, 'cat_smooth': 5.780326477879999}, 4: {'boosting': 'gbdt', 'num_iterations': 208, 'max_depth': 8, 'num_leaves': 4, 'min_data_in_leaf': 7, 'min_sum_hessian_in_leaf': 0.1, 'min_gain_to_split': 0.0, 'bagging_fraction': 0.6746301598613926, 'feature_fraction': 1.0, 'feature_fraction_bynode': 0.20999114041328495, 'bagging_freq': 1, 'verbosity': -1, 'objective': 'multiclass', 'num_class': 3, 'learning_rate': 0.02, 'cat_smooth': 8.604908973256704}}\n\nThis will perform 10 fold cross validation on random samples of\nparameters. By default, all variables models are tuned. If you are\ncurious about the default parameter space that is searched within, check\nout the `miceforest.default_lightgbm_parameters` module.\n\nThe parameter tuning is pretty flexible. If you wish to set some model\nparameters static, or to change the bounds that are searched in, you can\nsimply pass this information to either the `variable_parameters`\nparameter, `**kwbounds`, or both:\n\n``` python\n# Using a complicated setup:\noptimal_parameters, losses = kernel.tune_parameters(\n  dataset=0,\n  variables = ['sepal width (cm)','species','petal width (cm)'],\n  variable_parameters = {\n    'sepal width (cm)': {'bagging_fraction': 0.5},\n    'species': {'bagging_freq': (5,10)}\n  },\n  optimization_steps=5,\n  extra_trees = [True, False]\n)\n\nkernel.mice(1, variable_parameters=optimal_parameters)\n```\n\nIn this example, we did a few things - we specified that only `sepal\nwidth (cm)`, `species`, and `petal width (cm)` should be tuned. We also\nspecified some specific parameters in `variable_parameters.` Notice that\n`bagging_fraction` was passed as a scalar, `0.5`. This means that, for\nthe variable `sepal width (cm)`, the parameter `bagging_fraction` will\nbe set as that number and not be tuned. We did the opposite for\n`bagging_freq`. We specified bounds that the process should search in.\nWe also passed the argument `extra_trees` as a list. Since it was passed\nto \\*\\*kwbounds, this parameter will apply to all variables that are\nbeing tuned. Passing values as a list tells the process that it should\nrandomly sample values from the list, instead of treating them as set of\ncounts to search within.\n\nThe tuning process follows these rules for different parameter values it\nfinds:\n\n  - Scalar: That value is used, and not tuned.  \n  - Tuple: Should be length 2. Treated as the lower and upper bound to\n    search in.  \n  - List: Treated as a distinct list of values to try randomly.\n\n### On Reproducibility\n\n`miceforest` allows for different “levels” of reproducibility, global\nand record-level.\n\n##### **Global Reproducibility**\n\nGlobal reproducibility ensures that the same values will be imputed if\nthe same code is run multiple times. To ensure global reproducibility,\nall the user needs to do is set a `random_state` when the kernel is\ninitialized.\n\n##### **Record-Level Reproducibility**\n\nSometimes we want to obtain reproducible imputations at the record\nlevel, without having to pass the same dataset. This is possible by\npassing a list of record-specific seeds to the `random_seed_array`\nparameter. This is useful if imputing new data multiple times, and you\nwould like imputations for each row to match each time it is imputed.\n\n``` python\n# Define seeds for the data, and impute iris\nrandom_seed_array = np.random.randint(9999, size=150)\niris_imputed = kernel.impute_new_data(\n    iris_amp,\n    random_state=4,\n    random_seed_array=random_seed_array\n)\n\n# Select a random sample\nnew_inds = np.random.choice(150, size=15)\nnew_data = iris_amp.loc[new_inds]\nnew_seeds = random_seed_array[new_inds]\nnew_imputed = kernel.impute_new_data(\n    new_data,\n    random_state=4,\n    random_seed_array=new_seeds\n)\n\n# We imputed the same values for the 15 values each time,\n# because each record was associated with the same seed.\nassert new_imputed.complete_data(0).equals(iris_imputed.complete_data(0).loc[new_inds])\n```\n\nNote that record-level reproducibility is only possible in the\n`impute_new_data` function, there are no guarantees of record-level\nreproducibility in imputations between the kernel and new data.\n\n### How to Make the Process Faster\n\nMultiple Imputation is one of the most robust ways to handle missing\ndata - but it can take a long time. There are several strategies you can\nuse to decrease the time a process takes to run:\n\n  - Decrease `data_subset`. By default all non-missing datapoints for\n    each variable are used to train the model and perform mean matching.\n    This can cause the model training nearest-neighbors search to take a\n    long time for large data. A subset of these points can be searched\n    instead by using `data_subset`.  \n  - If categorical columns are taking a long time, you can use the\n    `mean_match_fast_cat` scheme. You can also set different parameters\n    specifically for categorical columns, like smaller\n    `bagging_fraction` or `num_iterations`.\n  - If you need to impute new data faster, compile the predictions with\n    the `compile_candidate_preds` method. This stores the predictions\n    for each model, so it does not need to be re-calculated at each\n    iteration.  \n  - Convert your data to a numpy array. Numpy arrays are much faster to\n    index. While indexing overhead is avoided as much as possible, there\n    is no getting around it. Consider comverting to `float32` datatype\n    as well, as it will cause the resulting object to take up much less\n    memory.\n  - Decrease `mean_match_candidates`. The maximum number of neighbors\n    that are considered with the default parameters is 10. However, for\n    large datasets, this can still be an expensive operation. Consider\n    explicitly setting `mean_match_candidates` lower.\n  - Use different lightgbm parameters. lightgbm is usually not the\n    problem, however if a certain variable has a large number of\n    classes, then the max number of trees actually grown is (\\# classes)\n    \\* (n\\_estimators). You can specifically decrease the bagging\n    fraction or n\\_estimators for large multi-class variables, or grow\n    less trees in general.  \n  - Use a faster mean matching function. The default mean matching\n    function uses the scipy.Spatial.KDtree algorithm. There are faster\n    alternatives out there, if you think mean matching is the holdup.\n\n### Imputing Data In Place\n\nIt is possible to run the entire process without copying the dataset. If\n`copy_data=False`, then the data is referenced directly:\n\n``` python\nkernel_inplace = mf.ImputationKernel(\n  iris_amp,\n  datasets=1,\n  copy_data=False\n)\nkernel_inplace.mice(2)\n```\n\nNote, that this probably won’t (but could) change the original dataset\nin undesirable ways. Throughout the `mice` procedure, imputed values are\nstored directly in the original data. At the end, the missing values are\nput back as `np.NaN`.\n\nWe can also complete our original data in place:\n\n``` python\nkernel_inplace.complete_data(dataset=0, inplace=True)\nprint(iris_amp.isnull().sum(0))\n```\n\n    ## sepal length (cm)    0\n    ## sepal width (cm)     0\n    ## petal length (cm)    0\n    ## petal width (cm)     0\n    ## species              0\n    ## dtype: int64\n\nThis is useful if the dataset is large, and copies can’t be made in\nmemory.\n\n## Diagnostic Plotting\n\nAs of now, miceforest has four diagnostic plots available.\n\n### Distribution of Imputed-Values\n\nWe probably want to know how the imputed values are distributed. We can\nplot the original distribution beside the imputed distributions in each\ndataset by using the `plot_imputed_distributions` method of an\n`ImputationKernel` object:\n\n``` python\nkernel.plot_imputed_distributions(wspace=0.3,hspace=0.3)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/distributions.png\" width=\"600px\" />\n\nThe red line is the original data, and each black line are the imputed\nvalues of each dataset.\n\n### Convergence of Correlation\n\nWe are probably interested in knowing how our values between datasets\nconverged over the iterations. The `plot_correlations` method shows you\na boxplot of the correlations between imputed values in every\ncombination of datasets, at each iteration. This allows you to see how\ncorrelated the imputations are between datasets, as well as the\nconvergence over iterations:\n\n``` python\nkernel.plot_correlations()\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/plot_corr.png\" width=\"600px\" />\n\n### Variable Importance\n\nWe also may be interested in which variables were used to impute each\nvariable. We can plot this information by using the\n`plot_feature_importance` method.\n\n``` python\nkernel.plot_feature_importance(dataset=0, annot=True,cmap=\"YlGnBu\",vmin=0, vmax=1)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/var_imp.png\" width=\"600px\" />\n\nThe numbers shown are returned from the\n`lightgbm.Booster.feature_importance()` function. Each square represents\nthe importance of the column variable in imputing the row variable.\n\n### Mean Convergence\n\nIf our data is not missing completely at random, we may see that it\ntakes a few iterations for our models to get the distribution of\nimputations right. We can plot the average value of our imputations to\nsee if this is occurring:\n\n``` python\nkernel.plot_mean_convergence(wspace=0.3, hspace=0.4)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/mean_convergence.png\" width=\"600px\" />\n\nOur data was missing completely at random, so we don’t see any\nconvergence occurring here.\n\n## Using the Imputed Data\n\nTo return the imputed data simply use the `complete_data` method:\n\n``` python\ndataset_1 = kernel.complete_data(0)\n```\n\nThis will return a single specified dataset. Multiple datasets are\ntypically created so that some measure of confidence around each\nprediction can be created.\n\nSince we know what the original data looked like, we can cheat and see\nhow well the imputations compare to the original data:\n\n``` python\nacclist = []\nfor iteration in range(kernel.iteration_count()+1):\n    species_na_count = kernel.na_counts[4]\n    compdat = kernel.complete_data(dataset=0,iteration=iteration)\n\n    # Record the accuract of the imputations of species.\n    acclist.append(\n      round(1-sum(compdat['species'] != iris['species'])/species_na_count,2)\n    )\n\n# acclist shows the accuracy of the imputations\n# over the iterations.\nprint(acclist)\n```\n\n    ## [0.35, 0.81, 0.84, 0.84, 0.89, 0.92, 0.89]\n\nIn this instance, we went from a low accuracy (what is expected with\nrandom sampling) to a much higher accuracy.\n\n## The MICE Algorithm\n\nMultiple Imputation by Chained Equations ‘fills in’ (imputes) missing\ndata in a dataset through an iterative series of predictive models. In\neach iteration, each specified variable in the dataset is imputed using\nthe other variables in the dataset. These iterations should be run until\nit appears that convergence has been met.\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/MICEalgorithm.png\" style=\"display: block; margin: auto;\" />\n\nThis process is continued until all specified variables have been\nimputed. Additional iterations can be run if it appears that the average\nimputed values have not converged, although no more than 5 iterations\nare usually necessary.\n\n### Common Use Cases\n\n##### **Data Leakage:**\n\nMICE is particularly useful if missing values are associated with the\ntarget variable in a way that introduces leakage. For instance, let’s\nsay you wanted to model customer retention at the time of sign up. A\ncertain variable is collected at sign up or 1 month after sign up. The\nabsence of that variable is a data leak, since it tells you that the\ncustomer did not retain for 1 month.\n\n##### **Funnel Analysis:**\n\nInformation is often collected at different stages of a ‘funnel’. MICE\ncan be used to make educated guesses about the characteristics of\nentities at different points in a funnel.\n\n##### **Confidence Intervals:**\n\nMICE can be used to impute missing values, however it is important to\nkeep in mind that these imputed values are a prediction. Creating\nmultiple datasets with different imputed values allows you to do two\ntypes of inference:\n\n  - Imputed Value Distribution: A profile can be built for each imputed\n    value, allowing you to make statements about the likely distribution\n    of that value.  \n  - Model Prediction Distribution: With multiple datasets, you can build\n    multiple models and create a distribution of predictions for each\n    sample. Those samples with imputed values which were not able to be\n    imputed with much confidence would have a larger variance in their\n    predictions.\n\n### Predictive Mean Matching\n\n`miceforest` can make use of a procedure called predictive mean matching\n(PMM) to select which values are imputed. PMM involves selecting a\ndatapoint from the original, nonmissing data (candidates) which has a\npredicted value close to the predicted value of the missing sample\n(bachelors). The closest N (`mean_match_candidates` parameter) values\nare selected, from which a value is chosen at random. This can be\nspecified on a column-by-column basis. Going into more detail from our\nexample above, we see how this works in practice:\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/PMM.png\" style=\"display: block; margin: auto;\" />\n\nThis method is very useful if you have a variable which needs imputing\nwhich has any of the following characteristics:\n\n  - Multimodal  \n  - Integer  \n  - Skewed\n\n### Effects of Mean Matching\n\nAs an example, let’s construct a dataset with some of the above\ncharacteristics:\n\n``` python\nrandst = np.random.RandomState(1991)\n# random uniform variable\nnrws = 1000\nuniform_vec = randst.uniform(size=nrws)\n\ndef make_bimodal(mean1,mean2,size):\n    bimodal_1 = randst.normal(size=nrws, loc=mean1)\n    bimodal_2 = randst.normal(size=nrws, loc=mean2)\n    bimdvec = []\n    for i in range(size):\n        bimdvec.append(randst.choice([bimodal_1[i], bimodal_2[i]]))\n    return np.array(bimdvec)\n\n# Make 2 Bimodal Variables\nclose_bimodal_vec = make_bimodal(2,-2,nrws)\nfar_bimodal_vec = make_bimodal(3,-3,nrws)\n\n\n# Highly skewed variable correlated with Uniform_Variable\nskewed_vec = np.exp(uniform_vec*randst.uniform(size=nrws)*3) + randst.uniform(size=nrws)*3\n\n# Integer variable correlated with Close_Bimodal_Variable and Uniform_Variable\ninteger_vec = np.round(uniform_vec + close_bimodal_vec/3 + randst.uniform(size=nrws)*2)\n\n# Make a DataFrame\ndat = pd.DataFrame(\n    {\n    'uniform_var':uniform_vec,\n    'close_bimodal_var':close_bimodal_vec,\n    'far_bimodal_var':far_bimodal_vec,\n    'skewed_var':skewed_vec,\n    'integer_var':integer_vec\n    }\n)\n\n# Ampute the data.\nampdat = mf.ampute_data(dat,perc=0.25,random_state=randst)\n\n# Plot the original data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ng = sns.PairGrid(dat)\ng.map(plt.scatter,s=5)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/dataset.png\" width=\"600px\" style=\"display: block; margin: auto;\" />\nWe can see how our variables are distributed and correlated in the graph\nabove. Now let’s run our imputation process twice, once using mean\nmatching, and once using the model prediction.\n\n``` python\nfrom miceforest import mean_match_default\nscheme_mmc_0 = mean_match_default.copy()\nscheme_mmc_5 = mean_match_default.copy()\n\nscheme_mmc_0.set_mean_match_candidates(0)\nscheme_mmc_5.set_mean_match_candidates(5)\n\nkernelmeanmatch = mf.ImputationKernel(ampdat, mean_match_scheme=scheme_mmc_5, datasets=1)\nkernelmodeloutput = mf.ImputationKernel(ampdat, mean_match_scheme=scheme_mmc_0, datasets=1)\n\nkernelmeanmatch.mice(2)\nkernelmodeloutput.mice(2)\n```\n\nLet’s look at the effect on the different variables.\n\n##### With Mean Matching\n\n``` python\nkernelmeanmatch.plot_imputed_distributions(wspace=0.2,hspace=0.4)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/meanmatcheffects.png\" width=\"600px\" style=\"display: block; margin: auto;\" />\n\n##### Without Mean Matching\n\n``` python\nkernelmodeloutput.plot_imputed_distributions(wspace=0.2,hspace=0.4)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/nomeanmatching.png\" width=\"600px\" style=\"display: block; margin: auto;\" />\n\nYou can see the effects that mean matching has, depending on the\ndistribution of the data. Simply returning the value from the model\nprediction, while it may provide a better ‘fit’, will not provide\nimputations with a similair distribution to the original. This may be\nbeneficial, depending on your goal.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/AnotherSamWilson/miceforest",
    "keywords": "MICE,Imputation,Missing Values,Missing,Random Forest",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "miceforest",
    "package_url": "https://pypi.org/project/miceforest/",
    "platform": null,
    "project_url": "https://pypi.org/project/miceforest/",
    "project_urls": {
      "Homepage": "https://github.com/AnotherSamWilson/miceforest"
    },
    "release_url": "https://pypi.org/project/miceforest/5.6.2/",
    "requires_dist": [
      "lightgbm (>=3.3.1)",
      "numpy",
      "blosc",
      "dill",
      "scipy (>=1.6.0) ; extra == 'default_mm'",
      "seaborn (>=0.11.0) ; extra == 'plotting'",
      "matplotlib (>=3.3.0) ; extra == 'plotting'",
      "pandas ; extra == 'testing'",
      "sklearn ; extra == 'testing'"
    ],
    "requires_python": ">=3.7",
    "summary": "Missing Value Imputation using LightGBM",
    "version": "5.6.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16075390,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1572ba16bde67a11f9b2176d4cba038f284b325293d40f5ab569df11d8f9ed1c",
        "md5": "26de054be00930d96f07833efd4f96cd",
        "sha256": "71b21ed58120fee6d6138b8cf371dfb65ab82084985077e7b31aae012cc0368a"
      },
      "downloads": -1,
      "filename": "miceforest-5.6.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "26de054be00930d96f07833efd4f96cd",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 57888,
      "upload_time": "2022-08-13T21:50:29",
      "upload_time_iso_8601": "2022-08-13T21:50:29.380344Z",
      "url": "https://files.pythonhosted.org/packages/15/72/ba16bde67a11f9b2176d4cba038f284b325293d40f5ab569df11d8f9ed1c/miceforest-5.6.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "224759218eda39acdc2b1648770fbd07af5a4239a7ed29d73c2247e9fcda8f51",
        "md5": "ead90e6c107d5b5d8e68f744fb0e9224",
        "sha256": "a81a03e69d97273a77d83b0a14c61ce33578d2704d9f86f40102f54e1b8540dc"
      },
      "downloads": -1,
      "filename": "miceforest-5.6.2.tar.gz",
      "has_sig": false,
      "md5_digest": "ead90e6c107d5b5d8e68f744fb0e9224",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 75081,
      "upload_time": "2022-08-13T21:50:32",
      "upload_time_iso_8601": "2022-08-13T21:50:32.050351Z",
      "url": "https://files.pythonhosted.org/packages/22/47/59218eda39acdc2b1648770fbd07af5a4239a7ed29d73c2247e9fcda8f51/miceforest-5.6.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}