{
  "info": {
    "author": "Samuel Wilson",
    "author_email": "samwilson303@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "\n[![Build\nStatus](https://travis-ci.org/AnotherSamWilson/miceforest.svg?branch=master)](https://travis-ci.org/AnotherSamWilson/miceforest)\n[![MyPy](https://img.shields.io/badge/MyPy-passing-success.svg)](https://pypi.org/project/miceforest/)\n[![Documentation\nStatus](https://readthedocs.org/projects/miceforest/badge/?version=latest)](https://miceforest.readthedocs.io/en/latest/?badge=latest)\n[![MIT\nlicense](http://img.shields.io/badge/license-MIT-brightgreen.svg)](http://opensource.org/licenses/MIT)\n[![CodeCov](https://codecov.io/gh/AnotherSamWilson/miceforest/branch/master/graphs/badge.svg?branch=master&service=github)](https://codecov.io/gh/AnotherSamWilson/miceforest)\n[![Code style:\nblack](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)  \n[![DEV\\_Version\\_Badge](https://img.shields.io/badge/Dev-2.0.6-blue.svg)](https://pypi.org/project/miceforest/)\n[![Pypi](https://img.shields.io/pypi/v/miceforest.svg)](https://pypi.python.org/pypi/miceforest)\n[![PyVersions](https://img.shields.io/pypi/pyversions/miceforest.svg?logo=python&logoColor=white)](https://pypi.org/project/miceforest/)\n[![Downloads](https://pepy.tech/badge/miceforest/month)](https://pepy.tech/project/miceforest)\n\n## miceforest: Fast Imputation with Random Forests in Python\n\n<a href='https://github.com/AnotherSamWilson/miceforest'><img src='https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/icon.png' align=\"right\" height=\"300\" /></a>\n\nFast, memory efficient Multiple Imputation by Chained Equations (MICE)\nwith random forests. It can impute categorical and numeric data without\nmuch setup, and has an array of diagnostic plots available. The R\nversion of this package may be found\n[here](https://github.com/FarrellDay/miceRanger).\n\nThis document contains a thorough walkthrough of the package,\nbenchmarks, and an introduction to multiple imputation. More information\non MICE can be found in Stef van Buurenâ€™s excellent online book, which\nyou can find\n[here](https://stefvanbuuren.name/fimd/ch-introduction.html).\n\n#### Table of Contents:\n\n  - [Package\n    Meta](https://github.com/AnotherSamWilson/miceforest#Package-Meta)\n  - [Using\n    miceforest](https://github.com/AnotherSamWilson/miceforest#Using-miceforest)\n      - [Single\n        Imputation](https://github.com/AnotherSamWilson/miceforest#Imputing-A-Single-Dataset)\n      - [Multiple\n        Imputation](https://github.com/AnotherSamWilson/miceforest#Simple-Example-Of-Multiple-Imputation)\n      - [Controlling Tree\n        Growth](https://github.com/AnotherSamWilson/miceforest#Controlling-Tree-Growth)\n      - [Custom Imputation\n        Schemas](https://github.com/AnotherSamWilson/miceforest#Creating-a-Custom-Imputation-Schema)\n      - [Imputing New Data with Existing\n        Models](https://github.com/AnotherSamWilson/miceforest#Imputing-New-Data-with-Existing-Models)\n  - [Diagnostic\n    Plotting](https://github.com/AnotherSamWilson/miceforest#Diagnostic-Plotting)\n      - [Imputed\n        Distributions](https://github.com/AnotherSamWilson/miceforest#Distribution-of-Imputed-Values)\n      - [Correlation\n        Convergence](https://github.com/AnotherSamWilson/miceforest#Convergence-of-Correlation)\n      - [Variable\n        Importance](https://github.com/AnotherSamWilson/miceforest#Variable-Importance)\n      - [Mean\n        Convergence](https://github.com/AnotherSamWilson/miceforest#Variable-Importance)\n  - [Using the Imputed\n    Data](https://github.com/AnotherSamWilson/miceforest#Using-the-Imputed-Data)\n  - [The MICE\n    Algorithm](https://github.com/AnotherSamWilson/miceforest#The-MICE-Algorithm)\n      - [Introduction](https://github.com/AnotherSamWilson/miceforest#The-MICE-Algorithm)\n      - [Common Use\n        Cases](https://github.com/AnotherSamWilson/miceforest#Common-Use-Cases)\n      - [Predictive Mean\n        Matching](https://github.com/AnotherSamWilson/miceforest#Predictive-Mean-Matching)\n      - [Effects of Mean\n        Matching](https://github.com/AnotherSamWilson/miceforest#Effects-of-Mean-Matching)\n\n### Package Meta\n\nmiceforest has 4 main classes which the user will interact with:\n\n  - `KernelDataSet` - a kernel data set is a dataset on which the mice\n    algorithm is performed. Models are saved inside the instance, which\n    can also be called on to impute new data. Several plotting methods\n    are included to run diagnostics on the imputed data.  \n  - `MultipleImputedKernel` - a collection of `KernelDataSet`s. Has\n    additional methods for accessing and comparing multiple kernel\n    datasets together.  \n  - `ImputedDataSet` - a single dataset that has been imputed. These are\n    returned after `impute_new_data()` is called.\n  - `MultipleImputedDataSet` - A collection of datasets that have been\n    imputed. Has additional methods for comparing the imputations\n    between datasets.\n\nYou can download the latest stable version from PyPi:\n\n``` bash\n$ pip install miceforest\n```\n\nYou can also download the latest development version from this\nrepository:\n\n``` bash\n$ pip install git+https://github.com/AnotherSamWilson/miceforest.git\n```\n\n## Using miceforest\n\nIn these examples we will be looking at a few simple examples of\nimputation. We need to load the packages, and define the data:\n\n``` python\nimport miceforest as mf\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n\n# Load data and introduce missing values\niris = pd.concat(load_iris(as_frame=True,return_X_y=True),axis=1)\niris['target'] = iris['target'].astype('category')\niris_amp = mf.ampute_data(iris,perc=0.25,random_state=1991)\n```\n\n### Imputing a Single Dataset\n\nIf you only want to create a single imputed dataset, you can use\n`KernelDataSet`:\n\n``` python\n# Create kernel. \nkds = mf.KernelDataSet(\n  iris_amp,\n  save_all_iterations=True,\n  random_state=1991\n)\n\n# Run the MICE algorithm for 3 iterations\nkds.mice(3)\n\n# Return the completed kernel data\ncompleted_data = kds.complete_data()\n```\n\nThere are also an array of plotting functions available, these are\ndiscussed below in the section [Diagnostic\nPlotting](https://github.com/AnotherSamWilson/miceforest#Diagnostic-Plotting).\nThe plotting behavior between single imputed datasets and multi-imputed\ndatasets is slightly different.\n\n### Simple Example of Multiple Imputation\n\nWe can also create a class which contains multiple `KernelDataSet`s,\nalong with easy ways to compare them:\n\n``` python\n# Create kernel. \nkernel = mf.MultipleImputedKernel(\n  iris_amp,\n  datasets=4,\n  save_all_iterations=True,\n  random_state=1991\n)\n\n# Run the MICE algorithm for 3 iterations on each of the datasets\nkernel.mice(3)\n```\n\nPrinting the `MultipleImputedKernel` object will tell you some high\nlevel information:\n\n``` python\nprint(kernel)\n```\n\n    ##               Class: MultipleImputedKernel\n    ##        Models Saved: Last Iteration\n    ##            Datasets: 4\n    ##          Iterations: 3\n    ##   Imputed Variables: 5\n    ## save_all_iterations: True\n\n### Controlling Tree Growth\n\nA *very* nice thing about random forests is that they are trivially\nparallelizable. We can save a lot of time by setting the `n_jobs`\nparameter in both the fit and predict methods for the random forests:\n\n``` python\n# Run the MICE algorithm for 2 more iterations on the kernel, \nkernel.mice(2,n_jobs=2)\n```\n\nAny other arguments may be passed to either class\n(`RandomForestClassifier`,`RandomForestRegressor`). In our example, we\nmay not have saved much (if any) time. This is because there is overhead\nwith using multiple cores, and our data is very small.\n\n### Creating a Custom Imputation Schema\n\nIt is possible to customize our imputation procedure by variable. By\npassing a named list to `variable_schema`, you can specify the\npredictors for each variable to impute. You can also select which\nvariables should be imputed using mean matching, as well as the mean\nmatching candidates, by passing a dict to`mean_match_candidates`:\n\n``` python\nvar_sch = {\n    'sepal width (cm)': ['target','petal width (cm)'],\n    'petal width (cm)': ['target','sepal length (cm)']\n}\nvar_mmc = {\n    'sepal width (cm)': 5,\n    'petal width (cm)': 0\n}\n\ncust_kernel = mf.MultipleImputedKernel(\n    iris_amp,\n    datasets=3,\n    variable_schema=var_sch,\n    mean_match_candidates=var_mmc\n)\ncust_kernel.mice(2)\n```\n\n### Imputing New Data with Existing Models\n\nMultiple Imputation can take a long time. If you wish to impute a\ndataset using the MICE algorithm, but donâ€™t have time to train new\nmodels, it is possible to impute new datasets using a\n`MultipleImputedKernel` object. The `impute_new_data()` function uses\nthe random forests collected by `MultipleImputedKernel` to perform\nmultiple imputation without updating the random forest at each\niteration:\n\n``` python\n# Our 'new data' is just the first 15 rows of iris_amp\nnew_data = iris_amp.iloc[range(15)]\nnew_data_imputed = kernel.impute_new_data(new_data=new_data)\nprint(new_data_imputed)\n```\n\n    ##               Class: MultipleImputedDataSet\n    ##            Datasets: 4\n    ##          Iterations: 5\n    ##   Imputed Variables: 5\n    ## save_all_iterations: False\n\nAll of the imputation parameters (variable\\_schema,\nmean\\_match\\_candidates, etc) will be carried over from the original\n`MultipleImputedKernel` object. When mean matching, the candidate values\nare pulled from the original kernel dataset. To impute new data, the\n`save_models` parameter in `MultipleImputedKernel` must be \\> 0. If\n`save_models == 1`, the model from the latest iteration is saved for\neach variable. If `save_models > 1`, the model from each iteration is\nsaved. This allows for new data to be imputed in a more similar fashion\nto the original mice procedure.\n\n## Diagnostic Plotting\n\nAs of now, miceforest has four diagnostic plots available.\n\n### Distribution of Imputed-Values\n\nWe probably want to know how the imputed values are distributed. We can\nplot the original distribution beside the imputed distributions in each\ndataset by using the `plot_imputed_distributions` method of an\n`MultipleImputedKernel` object:\n\n``` python\nkernel.plot_imputed_distributions(wspace=0.3,hspace=0.3)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/distributions.png\" width=\"600px\" />\n\nThe red line is the original data, and each black line are the imputed\nvalues of each dataset.\n\n### Convergence of Correlation\n\nWe are probably interested in knowing how our values between datasets\nconverged over the iterations. The `plot_correlations` method shows you\na boxplot of the correlations between imputed values in every\ncombination of datasets, at each iteration. This allows you to see how\ncorrelated the imputations are between datasets, as well as the\nconvergence over iterations:\n\n``` python\nkernel.plot_correlations()\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/plot_corr.png\" width=\"600px\" />\n\n### Variable Importance\n\nWe also may be interested in which variables were used to impute each\nvariable. We can plot this information by using the\n`plot_feature_importance` method.\n\n``` python\nkernel.plot_feature_importance(annot=True,cmap=\"YlGnBu\",vmin=0, vmax=1)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/var_imp.png\" width=\"600px\" />\n\nThe numbers shown are returned from the sklearn random forest\n`_feature_importance` attribute. Each square represents the importance\nof the column variable in imputing the row variable.\n\n### Mean Convergence\n\nIf our data is not missing completely at random, we may see that it\ntakes a few iterations for our models to get the distribution of\nimputations right. We can plot the average value of our imputations to\nsee if this is occurring:\n\n``` python\nkernel.plot_mean_convergence(wspace=0.3, hspace=0.4)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/mean_convergence.png\" width=\"600px\" />\n\nOur data was missing completely at random, so we donâ€™t see any\nconvergence occurring here.\n\n## Using the Imputed Data\n\nTo return the imputed data simply use the `complete_data` method:\n\n``` python\ndataset_1 = kernel.complete_data(0)\n```\n\nThis will return a single specified dataset. Multiple datasets are\ntypically created so that some measure of confidence around each\nprediction can be created.\n\nSince we know what the original data looked like, we can cheat and see\nhow well the imputations compare to the original data:\n\n``` python\nacclist = []\nfor iteration in range(kernel.iteration_count()+1):\n    target_na_count = kernel.na_counts['target']\n    compdat = kernel.complete_data(dataset=0,iteration=iteration)\n\n    # Record the accuract of the imputations of target.\n    acclist.append(\n      round(1-sum(compdat['target'] != iris['target'])/target_na_count,2)\n    )\n\n# acclist shows the accuracy of the imputations\n# over the iterations.\nprint(acclist)\n```\n\n    ## [0.32, 0.76, 0.78, 0.81, 0.86, 0.86]\n\nIn this instance, we went from a \\~32% accuracy (which is expected with\nrandom sampling) to an accuracy of \\~86%. We managed to replace the\nmissing `target` values with a pretty high degree of accuracy\\!\n\n## The MICE Algorithm\n\nMultiple Imputation by Chained Equations â€˜fills inâ€™ (imputes) missing\ndata in a dataset through an iterative series of predictive models. In\neach iteration, each specified variable in the dataset is imputed using\nthe other variables in the dataset. These iterations should be run until\nit appears that convergence has been met.\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/MICEalgorithm.png\" style=\"display: block; margin: auto;\" />\n\nThis process is continued until all specified variables have been\nimputed. Additional iterations can be run if it appears that the average\nimputed values have not converged, although no more than 5 iterations\nare usually necessary.\n\n### Common Use Cases\n\n##### **Data Leakage:**\n\nMICE is particularly useful if missing values are associated with the\ntarget variable in a way that introduces leakage. For instance, letâ€™s\nsay you wanted to model customer retention at the time of sign up. A\ncertain variable is collected at sign up or 1 month after sign up. The\nabsence of that variable is a data leak, since it tells you that the\ncustomer did not retain for 1 month.\n\n##### **Funnel Analysis:**\n\nInformation is often collected at different stages of a â€˜funnelâ€™. MICE\ncan be used to make educated guesses about the characteristics of\nentities at different points in a funnel.\n\n##### **Confidence Intervals:**\n\nMICE can be used to impute missing values, however it is important to\nkeep in mind that these imputed values are a prediction. Creating\nmultiple datasets with different imputed values allows you to do two\ntypes of inference:\n\n  - Imputed Value Distribution: A profile can be built for each imputed\n    value, allowing you to make statements about the likely distribution\n    of that value.  \n  - Model Prediction Distribution: With multiple datasets, you can build\n    multiple models and create a distribution of predictions for each\n    sample. Those samples with imputed values which were not able to be\n    imputed with much confidence would have a larger variance in their\n    predictions.\n\n### Predictive Mean Matching\n\n`miceforest` can make use of a procedure called predictive mean matching\n(PMM) to select which values are imputed. PMM involves selecting a\ndatapoint from the original, nonmissing data which has a predicted value\nclose to the predicted value of the missing sample. The closest N\n(`mean_match_candidates` parameter) values are chosen as candidates,\nfrom which a value is chosen at random. This can be specified on a\ncolumn-by-column basis. Going into more detail from our example above,\nwe see how this works in practice:\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/PMM.png\" style=\"display: block; margin: auto;\" />\n\nThis method is very useful if you have a variable which needs imputing\nwhich has any of the following characteristics:\n\n  - Multimodal  \n  - Integer  \n  - Skewed\n\n### Effects of Mean Matching\n\nAs an example, letâ€™s construct a dataset with some of the above\ncharacteristics:\n\n``` python\nrandst = np.random.RandomState(1991)\n# random uniform variable\nnrws = 1000\nuniform_vec = randst.uniform(size=nrws)\n\ndef make_bimodal(mean1,mean2,size):\n    bimodal_1 = randst.normal(size=nrws, loc=mean1)\n    bimodal_2 = randst.normal(size=nrws, loc=mean2)\n    bimdvec = []\n    for i in range(size):\n        bimdvec.append(randst.choice([bimodal_1[i], bimodal_2[i]]))\n    return np.array(bimdvec)\n\n# Make 2 Bimodal Variables\nclose_bimodal_vec = make_bimodal(2,-2,nrws)\nfar_bimodal_vec = make_bimodal(3,-3,nrws)\n\n\n# Highly skewed variable correlated with Uniform_Variable\nskewed_vec = np.exp(uniform_vec*randst.uniform(size=nrws)*3) + randst.uniform(size=nrws)*3\n\n# Integer variable correlated with Close_Bimodal_Variable and Uniform_Variable\ninteger_vec = np.round(uniform_vec + close_bimodal_vec/3 + randst.uniform(size=nrws)*2)\n\n# Make a DataFrame\ndat = pd.DataFrame(\n    {\n    'uniform_var':uniform_vec,\n    'close_bimodal_var':close_bimodal_vec,\n    'far_bimodal_var':far_bimodal_vec,\n    'skewed_var':skewed_vec,\n    'integer_var':integer_vec\n    }\n)\n\n# Ampute the data.\nampdat = mf.ampute_data(dat,perc=0.25,random_state=randst)\n\n# Plot the original data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ng = sns.PairGrid(dat)\ng.map(plt.scatter,s=5)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/dataset.png\" width=\"600px\" style=\"display: block; margin: auto;\" />\nWe can see how our variables are distributed and correlated in the graph\nabove. Now letâ€™s run our imputation process twice, once using mean\nmatching, and once using the model prediction.\n\n``` r\nkernelmeanmatch <- mf.MultipleImputedKernel(ampdat,mean_match_candidates=5)\nkernelmodeloutput <- mf.MultipleImputedKernel(ampdat,mean_match_candidates=0)\n\nkernelmeanmatch.mice(5)\nkernelmodeloutput.mice(5)\n```\n\nLetâ€™s look at the effect on the different variables.\n\n##### With Mean Matching\n\n``` python\nkernelmeanmatch.plot_imputed_distributions(wspace=0.2,hspace=0.4)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/meanmatcheffects.png\" width=\"600px\" style=\"display: block; margin: auto;\" />\n\n##### Without Mean Matching\n\n``` python\nkernelmodeloutput.plot_imputed_distributions(wspace=0.2,hspace=0.4)\n```\n\n<img src=\"https://raw.githubusercontent.com/AnotherSamWilson/miceforest/master/examples/nomeanmatching.png\" width=\"600px\" style=\"display: block; margin: auto;\" />\n\nYou can see the effects that mean matching has, depending on the\ndistribution of the data. Simply returning the value from the model\nprediction, while it may provide a better â€˜fitâ€™, will not provide\nimputations with a similair distribution to the original. This may be\nbeneficial, depending on your goal.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/AnotherSamWilson/miceforest",
    "keywords": "MICE,Imputation,Missing Values,Missing,Random Forest",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "miceforest",
    "package_url": "https://pypi.org/project/miceforest/",
    "platform": "",
    "project_url": "https://pypi.org/project/miceforest/",
    "project_urls": {
      "Homepage": "https://github.com/AnotherSamWilson/miceforest"
    },
    "release_url": "https://pypi.org/project/miceforest/2.0.6/",
    "requires_dist": [
      "scikit-learn",
      "numpy",
      "pandas",
      "seaborn (>=0.11.0)",
      "matplotlib (>=3.3.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "Imputes missing data with MICE + random forests",
    "version": "2.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16075390,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c5a23e132eab20b7db37ca35ff25a5d778000b14aecc104906a552ac3dd7ff11",
        "md5": "eff0e38899fc2366d6024934f3cc8b45",
        "sha256": "72e547a25027e1a1ee02344c3c4848ce2b7e628f4fb293a2e0a63fd844210e59"
      },
      "downloads": -1,
      "filename": "miceforest-2.0.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "eff0e38899fc2366d6024934f3cc8b45",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 27206,
      "upload_time": "2021-07-18T23:00:55",
      "upload_time_iso_8601": "2021-07-18T23:00:55.660248Z",
      "url": "https://files.pythonhosted.org/packages/c5/a2/3e132eab20b7db37ca35ff25a5d778000b14aecc104906a552ac3dd7ff11/miceforest-2.0.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fa1636e8f3d026cfa1d3e4f1443df054806e42143415131b7394cf78a95e8f81",
        "md5": "9e64b706150c9be6142fa5f781eb6825",
        "sha256": "1afe1f5cb598ee4dab5f61817907e4fa692184a84d02cb0edd46b5af6d9eb224"
      },
      "downloads": -1,
      "filename": "miceforest-2.0.6.tar.gz",
      "has_sig": false,
      "md5_digest": "9e64b706150c9be6142fa5f781eb6825",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 30478,
      "upload_time": "2021-07-18T23:00:57",
      "upload_time_iso_8601": "2021-07-18T23:00:57.338788Z",
      "url": "https://files.pythonhosted.org/packages/fa/16/36e8f3d026cfa1d3e4f1443df054806e42143415131b7394cf78a95e8f81/miceforest-2.0.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}