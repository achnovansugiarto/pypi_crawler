{
  "info": {
    "author": "Christos Hadjinikolis, Radu Ghitescu",
    "author_email": "christos.hadjinikolis@gmail.com, radu.ghitescu@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "[![Coverage Status](./docs/coverage_report/coverage-badge.svg?dummy=8484744)]()\n\n<img src=\"https://github.com/VorTECHsa/dynamicio/blob/master/docs/images/logo-transparent.png\" width=\"500\"> <img src=\"https://github.com/VorTECHsa/dynamicio/blob/master/docs/images/wrapped-panda.png\" width=\"100\">\n\n\nA repository for hosting the `dynamicio` library, used as a wrapper for `pandas` i/o operations.\n\n-- Logo illustrated by [Nick Loucas](https://www.linkedin.com/in/nickloucas/)\n\n## Why wrap your i/o\n\nWith the growing use of microservices&ndash;a norm in today's application deployment patterns&ndash;\ndevelopers were enabled to leverage the isolated nature of a microservice to use whatever language, library or\nframework they saw fit for their requirements. Though this is a convenient outcome, it is also one that\nincreases the complexity of a software tech-stack within an organisation's ecosystem.\n\n### The Problem\n\nThis trade-off is inevitable, and the \"negative\" consequences don't stop there. More than the need to\nsupport multiple languages and processing frameworks, data services teams (DST) often end up being driven by the\nrequirements dictated by the various processing frameworks used by different developers, which may have\nlimitations in terms of the format of data input they can accept or be less optimised to deal with specific\ndata types. Therefore, rather than the focus of a DST to be on **configuring access authorisation**, optimisation\nof read **latency** and **throughput**, increasing **fault tolerance** and **high availability**, they end up dealing\nwith what format works with, e.g., either `pandas` either `numpy`, `SciPy` or `tensorflow`.\n\n### As far as ML Systems are concerned...\n\nThis problem is highlighted as an **ML-System Anti-Pattern** in\n[Hidden Technical Debt in Machine Learning Systems](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf),\nreferred to as **Glue Code**:\n\n> ML researchers tend to develop general purpose solutions as self-contained packages. A wide variety of these are\n> available as open-source packages at places like `ml-oss.org`, or from in-house code, proprietary packages, and cloud-based platforms.\n>\n> Using generic packages often results in a **glue code** system design pattern, in which a massive amount of supporting\n> code is written to get data into and out of general-purpose packages. **Glue code** is costly in the long\n> term because it tends to freeze a system to the peculiarities of a specific package; testing alternatives\n> may become prohibitively expensive. In this way, using a generic package can inhibit improvements, because it\n> makes it harder to take advantage of domain-specific properties or to tweak the objective function to\n> achieve a domain-specific goal. Because a mature system might end up being (at most) 5% machine learning\n> code and (at least) 95% glue code, it may be less costly to create a clean native solution rather\n> than re-use a generic package.\n\n### The Solution\n\nQuoting from the same paper:\n\n> An important strategy for combating glue-code is to wrap black-box packages into common API's. This allows supporting\n> infrastructure to be more reusable and reduces the cost of changing packages.\n\nSo, as far as a single language is concerned, in this case `Python`, this can be addressed with the use of a wrapper\nwhich can increase re-usability and decouple processing from the i/o layer.\n\n`dynamic(i/o)` serves exactly that. In addition, it also serves as a convenient abstraction for defining the\n**input** and **output** sources of your `ETL` pipelines in an amenable way. Furthermore, it is configured in a way that\nallows it to \"choose\" the appropriate sources to load depending on the environment it is called from (e.g. `local` or\n`cloud`). The latter allows developers to quickly test their pipelines, by directing it to local mock/sample data,\nlifting the burden of having to mock i/o function returns that would otherwise interact with cloud resources.\n\n### Features:\n`dynamic(i/o)` supports:\n* seamless transition between environments; \n* abstracting away from resource and data types through `resource definitions`; \n* honouring your expectations on data through `schema definitions`;\n* metrics auto-generation (logging) for monitoring purposes.\n\n## Supported sources and data formats:\n\n<img src=\"https://github.com/VorTECHsa/dynamicio/blob/master/docs/images/supported_sources.png\" width=\"600\">\n\n- **S3** (or local) Input & Output:\n  - `parquet`\n  - `h5`\n  - `json`\n  - `csv`\n- **Postgres** Input & Output\n- **Kafka** Output\n\n### Coming soon:\n- **Athena** (pending)\n- **Delta Tables** (pending)\n- **GCS** (pending)\n- **BigQuery** (pending)\n\n## Installation\n\nTo install `dynamic(i/o)` you need to first authenticate with AWS Code Artifact. Just follow the below steps:\n\n```shell\n>> pip install dynamicio\n```\n\n## API Documentation\nRead our docs here: https://vortechsa.github.io/dynamicio/\n\n## How to use\n\nWe will go over an end-to-end example for reading and writing a single dataset, covering:\n\n1. all components involved and how they are configured, and;\n2. how these components are pieced together\n\nYou can find this example under the demo directory fo this repo.\n\n### Keywords:\n\n- **source configs**\n- **resource definitions**\n- **schema definitions**\n\n### Let's start\n\nSuppose you want to ingest the `foo` and `bar` datasets, respectively from `S3` and `Postgres` and stage them\nto S3 for further processing.\n\nAssume you want to build a pipeline that looks something like the image below:\n\n<img src=\"https://github.com/VorTECHsa/dynamicio/blob/master/docs/images/sample-pipeline.png\" width=\"600\">\n\nAssume the below repository structure, which implements this pipeline, for the purpose of this tutorial:\n\n```shell\ndemo\n.\n├── __init__.py\n├── resources\n│   ├── definitions\n│   │   ├── input.yaml\n│   │   ├── processed.yaml\n│   │   └── raw.yaml\n│   └── schemas\n│       ├── input\n│       │   ├── bar.yaml\n│       │   └── foo.yaml\n│       └── processed\n│           ├── final_bar.yaml\n│           └── final_foo.yaml\n├── src\n│   ├── __init__.py\n│   ├── __main__.py\n│   ├── constants.py\n│   ├── environment.py\n│   ├── io.py\n│   ├── runner_selection.py\n│   └── runners\n│       ├── __init__.py\n│       ├── staging.py\n│       └── transform.py\n└── tests\n    ├── __init__.py\n    ├── conftest.py\n    ├── constants.py\n    ├── data\n    │   ├── input\n    │   │   ├── bar.parquet\n    │   │   └── foo.csv\n    │   ├── processed\n    │   │   └── expected\n    │   │       ├── final_bar.parquet\n    │   │       └── final_foo.parquet\n    │   └── raw\n    │       └── expected\n    │           ├── staged_bar.parquet\n    │           └── staged_foo.parquet\n    ├── runners\n    │   ├── __init__.py\n    │   ├── conftest.py\n    │   ├── test_staging.py\n    │   └── test_transform.py\n    ├── test_pipeline.py\n    └── test_runner_selection.py\n```\n\n#### Step 1: Resource Definitions\n\nWe will start with defining our input and output resources as yaml files. These need to be defined under `resources/definitions`:\n\n```shell\nresources\n├── __init__.py\n├── definitions\n│   ├── input.yaml\n│   ├── processed.yaml\n│   └── raw.yaml\n└── schemas\n    ├── input\n    │   ├── bar.yaml\n    │   └── foo.yaml\n    └── processed\n        ├── final_bar.yaml\n        └── final_foo.yaml\n\n```\n\nYou will need to define your pipeline's resources by creating three `yaml` files. The first is:\n\n- `input.yaml` which concerns data read by the **staging** task;\n\n```yaml\n---\nFOO:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/input/foo.csv\"\n      file_type: \"csv\"\n  actual:\n    type: \"s3\"\n    s3:\n      bucket: \"[[ S3_YOUR_INPUT_BUCKET ]]\"\n      file_path: \"data/foo.h5\"\n      file_type: \"hdf\"\n  schema:\n    file_path: \"[[ RESOURCES ]]/schemas/input/foo.yaml\"\n\nBAR:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/input/bar.parquet\"\n      file_type: \"parquet\"\n  actual:\n    type: \"postgres\"\n    postgres:\n      db_host: \"[[ DB_HOST ]]\"\n      db_port: \"[[ DB_PORT ]]\"\n      db_name: \"[[ DB_NAME ]]\"\n      db_user: \"[[ DB_USER ]]\"\n      db_password: \"[[ DB_PASS ]]\"\n  schema:\n    file_path: \"[[ RESOURCES ]]/schemas/input/bar.yaml\"\n```\n\n- the `raw.yaml`, which concerns data coming out of the **staging** task and go into the **transform** task:\n\n```yaml\n---\nSTAGED_FOO:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/raw/staged_foo.parquet\"\n      file_type: \"parquet\"\n  actual:\n    type: \"s3\"\n    s3:\n      bucket: \"[[ S3_YOUR_OUTPUT_BUCKET ]]\"\n      file_path: \"live/data/raw/staged_foo.parquet\"\n      file_type: \"parquet\"\n\nSTAGED_BAR:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/raw/staged_bar.parquet\"\n      file_type: \"parquet\"\n  actual:\n    type: \"s3\"\n    s3:\n      bucket: \"[[ S3_YOUR_OUTPUT_BUCKET ]]\"\n      file_path: \"live/data/raw/staged_bar.parquet\"\n      file_type: \"parquet\"\n```\n- and the `processed.yaml`, which concerns data coming out of the **transform* task:\n```yaml\n---\nFINAL_FOO:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/processed/final_foo.parquet\"\n      file_type: \"parquet\"\n  actual:\n    type: \"s3\"\n    s3:\n      bucket: \"[[ S3_YOUR_OUTPUT_BUCKET ]]\"\n      file_path: \"live/data/processed/final_foo.parquet\"\n      file_type: \"parquet\"\n  schema:\n    file_path: \"[[ RESOURCES ]]/schemas/processed/final_foo.yaml\"\n\nFINAL_BAR:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/processed/final_bar.parquet\"\n      file_type: \"parquet\"\n    options:\n      use_deprecated_int96_timestamps: true\n      coerce_timestamps: \"ms\"\n      allow_truncated_timestamps: false\n      row_group_size: 1000000\n  actual:\n    type: \"kafka\"\n    kafka:\n      kafka_server: \"[[ KAFKA_SERVER ]]\"\n      kafka_topic: \"[[ KAFKA_TOPIC ]]\"\n    options:\n      compression_type: \"snappy\"\n      max_in_flight_requests_per_connection: 10\n      batch_size: 262144\n      request_timeout_ms: 60000 # 60s\n      buffer_memory: 134217728  # 128MB\n  schema:\n    file_path: \"[[ RESOURCES ]]/schemas/processed/final_bar.yaml\"\n\n```\n\nWe will hence refer to these files as **\"resource definitions\"**.\n\nThe first, `input.yaml` defines the input sources for the **staging** task, handled by the respective module (`runners/staging.py`) while the\nsecond one, defines its output; similarly for **transform**. These files are parsed by `dynamicio.config.IOConfig` to generated configuration i/o\ninstances referred to as \"source configs\" (see `demo/src/__init__.py`).\n\nNotice that under every source there are three layers: `sample`, `actual` and `schema`. The first two point to the variants of the same\ndataset, depending on whether it is called from the local environment or from the cloud (we will showcase how this\ndistinction takes place later).\n\nThe third refers your source config to a **\"schema definition\"** for your dataset (we will cover this in detail later).\n\n#### Step 2: Defining your environment variables\n\nAlso notice that paths to datasets are embedded with dynamic values identified with double squared brackets, e.g.\n`[[ S3_YOUR_OUTPUT_BUCKET ]]`. These can be defined in a module in your repository.\n\nResource definitions (`*.yaml` files) work in conjunction with `global` and `environment` variables:\n\n- `environment.py`\n\n```shell\n├── __init__.py\n├── src\n    └── environment.py\n...\n```\n\nLet's have a look inside.\n\n```python\n\"\"\"A module for configuring all environment variables.\"\"\"\nimport os\n\n# Let's keep type checkers happy\nENVIRONMENT: str\nCLOUD_ENV: str\nRESOURCES: str\nS3_YOUR_INPUT_BUCKET: str\nS3_YOUR_OUTPUT_BUCKET: str\nDB_PASS: str\nDB_HOST: str\nDB_PORT: str\nDB_NAME: str\n\n# Keys are environment variable names, values are default values. Pass None for no default.\n__REQUIRED_ENVIRONMENT_VARIABLES__ = {\n    \"ENVIRONMENT\": \"sample\",\n    \"CLOUD_ENV\": \"DEV\",\n    \"RESOURCES\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"../resources\"),\n    \"TEST_RESOURCES\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"../tests\"),\n    \"S3_YOUR_INPUT_BUCKET\": None,\n    \"S3_YOUR_OUTPUT_BUCKET\": None,\n    \"KAFKA_SERVER\": None,\n    \"KAFKA_TOPIC\": None,\n    \"DB_HOST\": None,\n    \"DB_PORT\": None,\n    \"DB_NAME\": None,\n    \"DB_USER\": None,\n    \"DB_PASS\": None,\n    \"REFERENCE_DATA_STATE_KEY\": None,\n}\n\n# Let's dynamically fetch those values from the environment and add them to the local scope\nfor required_variable, default_value in __REQUIRED_ENVIRONMENT_VARIABLES__.items():\n    locals()[required_variable] = os.getenv(required_variable, default_value)\n\n```\n\nThis module will be passed as an input parameter to instances of the `dynamicio.config.IOConfig` class. Let's cover\nsome of its variables:\n\n- ```python\n  \"ENVIRONMENT\": \"sample\",\n  ```\n  used to distinguish between local and cloud runs of your module. It assumes that this environment variable is\n  defined in the cloud environment where your module is executed from.\n- ```python\n  \"TEST_RESOURCES\": os.path.join(os.path.dirname(os.path.realpath(__file__)), \"../tests\"),\n  ```\n  It is defined in the resource definitions, e.g.:\n\n```yaml\n---\nFOO:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/input/foo.csv\"\n      file_type: \"csv\"\n  actual:\n    type: \"s3\"\n    s3:\n      bucket: \"[[ S3_YOUR_INPUT_BUCKET ]]\"\n      file_path: \"data/foo.h5\"\n      file_type: \"hdf\"\n  schema:\n    file_path: \"[[ RESOURCES ]]/schemas/input/foo.yaml\"\n```\n\nand therefore needs to be defined here as well.\n\nAny other dynamic variable (identified with the doubly squared brackets) defined in the resource definitions needs to\nalso be defined here and can be either statically or dynamically defined (i.e. hardcoded or defined as an environment\nvalue).\n\n#### Step 3: Read in your resource definitions\n\nReading in the resources definitions can be done by means of instantiating instances of the `dynamicio.config.IOConfig`\nclass (the, so called, \"source configs\"). This is done in:\n\n```shell\nsrc\n├── __init__.py\n```\n\nwhich allows it to be automatically loaded on call of any module within the `pipeline` package.\n\n```python\n\"\"\"Set config IOs\"\"\"\n__all__ = [\"input_config\", \"raw_config\", \"processed_config\"]\n\nimport logging\nimport os\n\nfrom demo.src import environment\nfrom demo.src.environment import ENVIRONMENT, RESOURCES\nfrom dynamicio.config import IOConfig\n\nlogging.basicConfig(level=logging.INFO)\nlogging.getLogger(\"kafka\").setLevel(logging.WARNING)\n\n\ninput_config = IOConfig(\n    path_to_source_yaml=(os.path.join(RESOURCES, \"definitions/input.yaml\")),\n    env_identifier=ENVIRONMENT,\n    dynamic_vars=environment,\n)\nraw_config = IOConfig(\n    path_to_source_yaml=(os.path.join(RESOURCES, \"definitions/raw.yaml\")),\n    env_identifier=ENVIRONMENT,\n    dynamic_vars=environment,\n)\nprocessed_config = IOConfig(\n    path_to_source_yaml=(os.path.join(RESOURCES, \"definitions/processed.yaml\")),\n    env_identifier=ENVIRONMENT,\n    dynamic_vars=environment,\n)\n\n```\n\nOn loading, `IOConfig` will load the respective configs for all resources in the form of a multi-layered dictionary, e.g.,\nfor `actual`:\n\n```python\nimport demo.src.environment\n\n{\n    \"FOO\": {\n        \"sample\": {\n            \"type\": \"local\",\n            \"local\": {\n                \"file_path\": f\"{demo.src.environment.TEST_RESOURCES}/data/input/foo.csv\",\n                \"file_type\": \"csv\",\n            },\n        },\n        \"actual\": {\n            \"type\": \"s3\",\n            \"s3\": {\n                \"bucket\": f\"{demo.src.environment.S3_YOUR_INPUT_BUCKET}\",\n                \"file_path\": \"data/foo.h5\",\n                \"file_type\": \"hdf\"\n            }\n        },\n    }\n}\n```\n\nThen, depending on the value of the `env_identifier` parameter, the respective sub-dictionary is returned. For example,\nwith:\n\n```python\nfoo_io = input_config.get(source_key=\"FOO\")\n```\n\nand with `env_identifier=\"actual\"`, the output would be:\n\n```python\n\"type\": \"s3\",\n\"s3\": {\n    \"bucket\": f\"{demo.src.environment.S3_YOUR_INPUT_BUCKET}\",\n    \"file_path\": \"data/foo.h5\",\n    \"file_type\": \"hdf\"\n}\n```\n\n#### Step 4: Loading the data resources\n\nTo load a resource, you will need to generate instances of subclasses of `from dynamicio import UnifiedIO` class. Note\nthat the `UnifiedIO` class operates as an abstract class and cannot be used for instantiating objects.\n\nYou will need to implement your own subclasses for each of the inputs you care to load. You can do this in the `io.py`\nmodule, under:\n\n```shell\n.\n├── src\n│   ├── __init__.py\n│   ├── io.py\n\n```\n\nThe file looks like this:\n\n```python\n\"\"\"Responsible for configuring io operations for input data.\"\"\"\n# pylint: disable=too-few-public-methods\n__all__ = [\"Foo\", \"Bar\", \"StagedFoo\", \"StagedBar\", \"BarDataModel\", \"FinalFoo\", \"FinalBar\"]\n\nfrom sqlalchemy import Column, Float, String\nfrom sqlalchemy.ext.declarative import declarative_base\n\nfrom dynamicio import UnifiedIO, WithKafka, WithLocal, WithPostgres, WithS3File\nfrom dynamicio.core import SCHEMA_FROM_FILE, DynamicDataIO\n\nBase = declarative_base()\n\n\nclass Foo(UnifiedIO):\n    \"\"\"UnifiedIO subclass for V6 data.\"\"\"\n\n    schema = SCHEMA_FROM_FILE\n\n\nclass Bar(UnifiedIO):\n    \"\"\"UnifiedIO subclass for cargo movements volumes data.\"\"\"\n\n    schema = SCHEMA_FROM_FILE\n\n\nclass StagedFoo(WithS3File, WithLocal, DynamicDataIO):\n    \"\"\"UnifiedIO subclass for staged foos6.\"\"\"\n\n    schema = {\n        \"column_a\": \"object\",\n        \"column_b\": \"object\",\n        \"column_c\": \"int64\",\n        \"column_d\": \"int64\",\n    }\n\n\nclass StagedBar(WithLocal, WithPostgres, DynamicDataIO):\n    \"\"\"UnifiedIO subclass for cargo movements volumes data.\"\"\"\n\n    schema = {\n        \"column_a\": \"object\",\n        \"column_b\": \"object\",\n        \"column_c\": \"int64\",\n        \"column_d\": \"int64\",\n    }\n\n\nclass FinalFoo(UnifiedIO):\n    \"\"\"UnifiedIO subclass for V6 data.\"\"\"\n\n    schema = SCHEMA_FROM_FILE\n\n\nclass FinalBar(WithLocal, WithKafka, DynamicDataIO):\n    \"\"\"UnifiedIO subclass for cargo movements volumes data.\"\"\"\n\n    schema = SCHEMA_FROM_FILE\n\n\nclass BarDataModel(Base):\n    \"\"\"Sql_alchemy model for Bar table.\"\"\"\n\n    __tablename__ = \"bar\"\n\n    column_a = Column(String(64), primary_key=True)\n    column_b = Column(String(64))\n    column_c = Column(Float)\n    column_d = Column(Float)\n\n```\n\nInstances of the `DynamicDataIO` class **must** define a class `schema`. The schema has the form of a dictionary, associating columns (keys) with `dtypes` (values).\n\n##### Step 4.1. `SCHEMA_FROM_FILE`\n\n`from dynamicio.core import SCHEMA_FROM_FILE` is a unique dynamic(i/o) object used as a placeholder.\nIt is used to indicate that a schema is provided as part of a _resource definition_.\n\nFor example:\n\n```yaml\n---\nFOO:\n  sample:\n  ...\n  actual:\n  ...\n  schema:\n    file_path: \"[[ RESOURCES ]]/schemas/input/foo.yaml\"\n```\n\n`foo.yaml` is effectively a schema definition and looks like this:\n\n```yaml\n---\n---\nname: foo\ncolumns:\n  column_a:\n    type: \"object\"\n    validations:\n      has_unique_values:\n        apply: true\n        options: {}\n    metrics:\n      - Counts\n  column_b:\n    type: \"object\"\n    validations:\n      has_no_null_values:\n        apply: true\n        options: {}\n    metrics:\n      - CountsPerLabel\n  column_c:\n    type: float64\n    validations:\n      is_greater_than:\n        apply: true\n        options:\n          threshold: 1000\n    metrics: []\n  column_d:\n    type: float64\n    validations:\n      is_lower_than:\n        apply: true\n        options:\n          threshold: 1000\n    metrics:\n      - Min\n      - Max\n      - Mean\n      - Std\n      - Variance\n```\n\nThe file is quite self-explanatory. The format is:\n\n`DataSet`:\n\n- `Column`\n  - `type`\n  - `validations`\n  - `metrics`\n\nFor a dataset, each of the desired columns are dictated here, along with their\ndesignated `dtypes`. The `columns` are used to filter out undesired columns in an optimal manner.\nThis means that it will happen on loading for `*.csv` and `*.parquet` files as well as when\ninteracting with a database, but will happen post-loading in the case of `*.h5` or `*.json`.\n\n`dtypes` are then used to validate the types of the columns. If types don't match, `dynamic(i/o)`\nwill attempt to cast them and will issue a `WARNING`. If casting does not work either, it will\nthrow a `ValueError` exception.\n\n`validations` and `metrics` are there to document the user's expectations of the quality of the dataset.\nThey can be automatically applied on loading or on writing out.\n\nSpecifically, you can use the following **validations**:\n\n- `has_unique_values`\n- `has_no_null_values`\n- `has_acceptable_percentage_of_nulls`\n- `has_acceptable_categorical_values`\n- `is_greater_than`\n- `is_greater_than_or_equal`\n- `is_lower_than`\n- `is_lower_than_or_equal`\n- `is_between`\n\nand **metrics**:\n\n- `Min`\n- `Max`\n- `Mean`\n- `Std`\n- `Variance`\n- `Counts`\n- `UniqueCounts`\n- `CountsPerLabel`\n\nThe `dynamicio` cli can be used to automatically generate schema definitions for you, provided either a path to\na dataset (`json`, `parquet`, `hdf`, `csv`) or to a directory. Here is how you can use it:\n\n```shell\nusage: dynamicio [-h] (--batch | --single) -p PATH -o OUTPUT\n\nGenerate dataset schemas\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --batch               used to generate multiple schemas provided a datasets directory.\n  --single              used to generate a schema provided a single dataset.\n  -p PATH, --path PATH  the path to the dataset/datasets-directory.\n  -o OUTPUT, --output OUTPUT\n                        the path to the schemas output directory.\n```\n\nThe generated schema definitions will not have any validations or metrics automatically selected for you.\n\n##### Step 4.2: Loading from `S3`\n\nTo then load from `S3` you simply do:\n\n```python\n    foo_df = Foo(source_config=input_config.get(source_key=\"FOO\"), apply_schema_validations=True, log_schema_metrics=True).read()\n```\n\nwhich will load the `foo.csv` file as a dataframe.\n\n##### Step 4.3: Loading from `Postgres`\n\nIn contrast to `S3` resources, `postgres` resources need additional options to be\ndefined for their loading.\n\nSpecifically, you need to define a data model, defining the table, the columns and\ntheir respective SQL types. This is necessary as a different reader is utilised in\nthe case of postgres (this need will be addressed in future releases).\n\nThe data model definition is also defined in `io.py` and looks like:\n\n```python\n\"\"\"\nA module for defining sql_alchemy models.\n\"\"\"\n__all__ = [\"BarDataModel\"]\n\nfrom sqlalchemy import Column, Float, String\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\n\nclass BarDataModel(Base):\n    \"\"\"\n    Sql_alchemy model for Bar table\n    \"\"\"\n\n    __tablename__ = \"bar\"\n\n    column_a = Column(String(64), primary_key=True)\n    column_b = Column(String(64))\n    column_c = Column(Float)\n    column_d = Column(Float)\n\n```\n\nTo, then, load from `postgres` you simply do:\n\n```python\n    bar_df = Bar(source_config=input_config.get(source_key=\"BAR\"), apply_schema_validations=True, log_schema_metrics=True, model=BarDataModel).read()\n```\n\nwhich will load the cargo the movements table as a dataframe.\n\n#### Step 5: Writing out\n\nSinking data is done in a very similar way. You need to:\n\n1. Define your output resource definitions, in our case in `raw.yaml`\n\n```yaml\n---\nSTAGED_FOO:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/raw/staged_foo.parquet\"\n      file_type: \"parquet\"\n  actual:\n    type: \"s3\"\n    s3:\n      bucket: \"[[ S3_YOUR_OUTPUT_BUCKET ]]\"\n      file_path: \"live/data/raw/staged_foo.parquet\"\n      file_type: \"parquet\"\n\nSTAGED_BAR:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/raw/staged_bar.parquet\"\n      file_type: \"parquet\"\n  actual:\n    type: \"s3\"\n    s3:\n      bucket: \"[[ S3_YOUR_OUTPUT_BUCKET ]]\"\n      file_path: \"live/data/raw/staged_bar.parquet\"\n      file_type: \"parquet\"\n```\n\n2. You need to define the respective dynamic values found in your resource definitions\n   in your `src/environment.py`\n3. You need to create an instance of the `IOConfig` class for the `raw.yaml` in the\n   `__init__.py` file (we already did this).\n4. Define the additional `DynamicDataIO` subclasses in the `src/io.py` module,\n   dictating through the schema the list of columns, and their types (also used for schema validation).\n5. Finally, instantiate instances of those subclasses and call the `.write()` method,\n   passing in the dataframe you want to write out, e.g.\n\n`demo/src/runners/staging.py`:\n```python\n    ...\n    StagedFoo(source_config=raw_config.get(source_key=\"STAGED_FOO\"), **constants.TO_PARQUET_KWARGS).write(foo_df)\n    StagedBar(source_config=raw_config.get(source_key=\"STAGED_BAR\")).write(bar_df)\n```\n\nNotice that you can pass all `pandas` options to write out, when for instance you are writing out `parquet`.\n\n`demo/src/constants.py`:\n```python\n# Parquet\nTO_PARQUET_KWARGS = {\n    \"use_deprecated_int96_timestamps\": False,\n    \"coerce_timestamps\": \"ms\",\n    \"allow_truncated_timestamps\": True,\n}\n```\n\nOf, course this is not a problem as parquet is the format used by both resources in either environment. This not always the case however. See in \n`demo/resources/definitions/processed.yaml`:\n\n```yaml\n---\n...\n\nFINAL_BAR:\n  sample:\n    type: \"local\"\n    local:\n      file_path: \"[[ TEST_RESOURCES ]]/data/processed/final_bar.parquet\"\n      file_type: \"parquet\"\n    options:                                      <---- Options for Local writing as parquet\n      use_deprecated_int96_timestamps: true\n      coerce_timestamps: \"ms\"\n      allow_truncated_timestamps: false\n      row_group_size: 1000000\n  actual:\n    type: \"kafka\"\n    kafka:\n      kafka_server: \"[[ KAFKA_SERVER ]]\"\n      kafka_topic: \"[[ KAFKA_TOPIC ]]\"\n    options:                                      <---- Options for writting to a Kafka Topic\n      compression_type: \"snappy\"\n      max_in_flight_requests_per_connection: 10\n      batch_size: 262144\n      request_timeout_ms: 60000 # 60s\n      buffer_memory: 134217728  # 128MB\n  schema:\n    file_path: \"[[ RESOURCES ]]/schemas/processed/final_bar.yaml\"\n```\n\nHere, we have a case where different options need to be used for each environment as it deals with a different source. This is gracefully managed through resource \ndefinitions passing these arguments in the `options` key per environment.    \n\n### Step 6: Full Code\n\nThe full code for the loading module in our example would live under:\n\n```shell\n├── __init__.py\n├── src\n...\n│   ├── runners\n│   │   └── staging.py\n```\n\nand looks like:\n\n```python\n\"\"\"Add module docstring....\"\"\"\nimport logging\n\nfrom demo.src import constants, input_config, raw_config\nfrom demo.src.io import Bar, BarDataModel, Foo, StagedBar, StagedFoo\n\nlogger = logging.getLogger(__name__)\n\n\ndef main() -> None:\n    \"\"\"The entry point for the Airflow Staging task.\n\n    Returns:\n        Void function.\n    \"\"\"\n    # LOAD DATA\n    logger.info(\"Loading data from live sources...\")\n\n    bar_df = Bar(source_config=input_config.get(source_key=\"BAR\"), apply_schema_validations=True, log_schema_metrics=True, model=BarDataModel).read()\n    foo_df = Foo(source_config=input_config.get(source_key=\"FOO\"), apply_schema_validations=True, log_schema_metrics=True).read()\n\n    logger.info(\"Data successfully loaded from live sources...\")\n\n    # TRANSFORM  DATA\n    logger.info(\"Apply transformations...\")\n\n    # TODO: Apply your transformations\n\n    logger.info(\"Transformations applied successfully...\")\n\n    # SINK DATA\n    logger.info(\"Begin sinking data to staging area:\")\n    StagedFoo(source_config=raw_config.get(source_key=\"STAGED_FOO\"), **constants.TO_PARQUET_KWARGS).write(foo_df)\n    StagedBar(source_config=raw_config.get(source_key=\"STAGED_BAR\")).write(bar_df)\n    logger.info(\"Data staging is complete...\")\n\n```\n\n## Testing Locally\n\nAfter following the above documentation, at this point it should be clear that `dynamic(i/o)` is optimised for enabling\nseamless local testing for your pipelines.\n\nSimply by configuring your `ENVIRONMENT`'s default value to `sample` and provided that you have the required tests data\nsources in the necessary directories, it becomes very simple to test your pipelines end-to-end in seconds, eliminating\nthe need to deploy your dags and wait for their tasks to be provided access to processing resources.\n\nAll you need to do is mimic the order of execution of your tasks, running them in procedural order.\n\nIn the case of our example, you would have to:\n\n1. Add the necessary data under `tests/data`:\n\n```shell\n└── tests\n    ├── __init__.py\n    ├── conftest.py\n    ├── constants.py\n    ├── data\n    │   ├── input\n    │   │   ├── bar.parquet\n    │   │   └── foo.csv\n    │   ├── processed\n    │   │   └── expected\n    │   │       ├── final_bar.parquet\n    │   │       └── final_foo.parquet\n    │   └── raw\n    │       └── expected\n    │           ├── staged_bar.parquet\n    │           └── staged_foo.parquet\n    ├── runners\n    │   ├── __init__.py\n    │   ├── conftest.py\n    │   ├── test_staging.py\n    │   └── test_transform.py\n    ├── test_pipeline.py\n    └── test_runner_selection.py\n```\n\n2. Implement an end-to-end, black-box style test that simply generates the expected data output given a\n   specific input (deleting the output after the assertion)\n\nAn example end-to-end test in this case, for a single airflow task would look like:\n\n```python\n\"\"\"An example pipeline to showcase how dynamicio can bt used for setting up a local e2e testing!\"\"\"\n# pylint: disable=missing-module-docstring, missing-class-docstring, missing-function-docstring, unused-argument, too-few-public-methods\n# noqa\nimport os\n\nimport pandas as pd\nimport pytest\n\nfrom demo.src import processed_config, raw_config\nfrom demo.src.runners import staging, transform\n\n\nclass TestPipeline:\n    \"\"\"Example e2e test.\"\"\"\n\n    @pytest.mark.end_to_end\n    def test_dag_with_mock_sample_input_data(\n        self,\n        expected_staged_foo_df,\n        expected_staged_bar_df,\n        expected_final_foo_df,\n        expected_final_bar_df,\n    ):\n        \"\"\"Showcases how you can leverage dynamicio to read local data for fast feedback when you want to run your pipelines locally.\"\"\"\n        # Given\n        # The pipeline/src/resources/input.yaml\n\n        # When\n        staging.main()\n        transform.main()\n\n        # Then\n        try:\n            assert expected_staged_foo_df.equals(pd.read_parquet(raw_config.get(source_key=\"STAGED_FOO\")[\"local\"][\"file_path\"]))\n            assert expected_staged_bar_df.equals(pd.read_parquet(raw_config.get(source_key=\"STAGED_BAR\")[\"local\"][\"file_path\"]))\n            assert expected_final_foo_df.equals(pd.read_parquet(processed_config.get(source_key=\"FINAL_FOO\")[\"local\"][\"file_path\"]))\n            assert expected_final_bar_df.equals(pd.read_parquet(processed_config.get(source_key=\"FINAL_BAR\")[\"local\"][\"file_path\"]))\n        finally:\n            os.remove(raw_config.get(source_key=\"STAGED_FOO\")[\"local\"][\"file_path\"])\n            os.remove(raw_config.get(source_key=\"STAGED_BAR\")[\"local\"][\"file_path\"])\n            os.remove(processed_config.get(source_key=\"FINAL_FOO\")[\"local\"][\"file_path\"])\n            os.remove(processed_config.get(source_key=\"FINAL_BAR\")[\"local\"][\"file_path\"])\n\n```\n\n# Last notes\n\nHope this was helpful. \n\nPlease do reach out with comments and your views about how the library or the docs can be improved, and by all means, come along and contribute to our project!\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dynamicio",
    "package_url": "https://pypi.org/project/dynamicio/",
    "platform": null,
    "project_url": "https://pypi.org/project/dynamicio/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/dynamicio/3.0.38/",
    "requires_dist": [
      "awscli (>=1.22.24)",
      "boto3 (>=1.20.24)",
      "fastparquet (==0.8.0)",
      "fsspec (==2022.3.0)",
      "kafka-python (~=2.0.2)",
      "logzero (>=1.7.0)",
      "magic-logger (>=1.0.2)",
      "pandas (>=1.2.4)",
      "psycopg2-binary (~=2.8.6)",
      "pyarrow (==8.0.0)",
      "python-json-logger (~=2.0.1)",
      "PyYAML (~=5.4.1)",
      "s3fs (==0.4.2)",
      "simplejson (~=3.17.2)",
      "SQLAlchemy (>=1.4.11)",
      "tables (~=3.6.1)"
    ],
    "requires_python": ">=3.8",
    "summary": "Panda's wrapper for IO operations",
    "version": "3.0.38",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17478135,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1ef2bc9f9cddb7cfb2ce0e7738371f2d3fc16950eb28d0c1d03c7f918b2779ff",
        "md5": "22b5315930cd2e5219c58879a72db986",
        "sha256": "ecdf9266990dd6ce5a37fecc1bc7c0cf0e79bbcafd95eced316813c60d894d42"
      },
      "downloads": -1,
      "filename": "dynamicio-3.0.38-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "22b5315930cd2e5219c58879a72db986",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 80376,
      "upload_time": "2022-06-15T15:05:10",
      "upload_time_iso_8601": "2022-06-15T15:05:10.351007Z",
      "url": "https://files.pythonhosted.org/packages/1e/f2/bc9f9cddb7cfb2ce0e7738371f2d3fc16950eb28d0c1d03c7f918b2779ff/dynamicio-3.0.38-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "335e8da4fe0d43ffcd872af117eb0b573e1cf9e05a31a0af15ee147362500961",
        "md5": "6043af3a78260c9eb58f93ea3dd7b7c8",
        "sha256": "aac8abd8dadca29671259da648f56525b38b2d5fa98d8f965abc6dd95c05611a"
      },
      "downloads": -1,
      "filename": "dynamicio-3.0.38.tar.gz",
      "has_sig": false,
      "md5_digest": "6043af3a78260c9eb58f93ea3dd7b7c8",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 86169,
      "upload_time": "2022-06-15T15:05:12",
      "upload_time_iso_8601": "2022-06-15T15:05:12.875434Z",
      "url": "https://files.pythonhosted.org/packages/33/5e/8da4fe0d43ffcd872af117eb0b573e1cf9e05a31a0af15ee147362500961/dynamicio-3.0.38.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}