{
  "info": {
    "author": "Siddesh Sambasivam Suseela",
    "author_email": "plutocrat45@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/SiddeshSambasivam/MatterIx/master/assets/Logo.png?token=AKHFPP5DPO3RQLN3NBTHJGDA4DBL6\" />\n</p>\n<p align=\"center\">\n    <img alt=\"GitHub Pipenv locked Python version\" src=\"https://img.shields.io/github/pipenv/locked/python-version/SiddeshSambasivam/MatterIx\">\n    <img alt=\"PyPI\" src=\"https://img.shields.io/pypi/v/Matterix\">\n    <img alt=\"GitHub Pipenv locked dependency version (branch)\" src=\"https://img.shields.io/github/pipenv/locked/dependency-version/SiddeshSambasivam/MatterIx/black/master\">\n    <img alt=\"PyPI - License\" src=\"https://img.shields.io/pypi/l/Matterix\">\n</p>\n\n<p align=\"center\">\n  <a style=\"padding: 0 10px;\" target=\"#\" href=\"#ins\">Installation</a> • \n  <a style=\"padding: 0 10px;\" target=\"#\" href=\"#releases\">Releases</a> • \n  <a style=\"padding: 0 10px;\" href=\"#contributing\">Contributing</a> • \n  <a style=\"padding: 0 10px;\" href=\"#features\">Features</a>\n</p>\n\nMatterIx is a simple deep learning framework built to understand the fundamental concepts of <b>autodiff</b>, <b>optimizers</b>, <b>loss functions</b> from a first principle basis. It provide features such as automatic differentiation (autodiff), optimizers, loss functions and basic modules to create your own neural networks.\n\n<table align=\"center\" width=\"600px\">\n <tr>\n    <th>Feature</th>\n    <th>Description</th>\n    <th>Function/Specs</th>\n </tr>\n <tr>\n    <td><a href=\"#autodiff\">Autodiff</a></td>\n    <td>Allows to compute gradients for tensors.</td>\n    <td>First-order derivative</td>\n  </tr>\n  <tr>\n    <td><a href=\"#loss\">Loss functions</a></td>\n    <td>Provides a metric to evaluate the model or function</td>\n    <td>Mean squared error (MSE), Root mean squared error (RMSE)</td>\n  </tr>\n  <tr>\n    <td><a href=\"#opt\">Optimizers</a></td>\n    <td>Updates the parameters of a model for a specific optimization problem</td>\n    <td>Stochastic gradient descent (SGD)</td>\n  </tr>\n  <tr>\n    <td><a href=\"#act\">Activation functions</a></td>\n    <td>Provides the necessary non-linearity to model. It acts as a switch for the nodes</td>\n    <td>Sigmoid, tanh, ReLU</td>\n  </tr>\n  <tr>\n    <td><a href=\"#mod\">Module</a></td>\n    <td>Serves as a base class to design your own neural networks</td>\n    <td>NIL</td>\n  </tr>\n</table>\n\n<br/>\n\nThe <b>core value of matterix</b> is that it is a distilled version of pytorch so it is easier to understand what is happening under the hood.\n\nAt its core, matterix\n\n<h3 style=\"font-weight:bold\" id=\"#ins\">Installation</h3>\na. Install it from github\n\n```bash\n# Install either with option-1 or option-2\n\n# Option-1 (Preferred)\npip install git+https://github.com/SiddeshSambasivam/MatterIx.git#egg=MatterIx\n\n# Option-2\ngit clone https://github.com/SiddeshSambasivam/MatterIx.git\n\npython setup.py install\n\n```\n\nb. Install from PyPI\n\n```bash\n# Install directly from PyPI repository\npip install --upgrade matterix\n```\n\n<h2 style=\"font-weight:bold\" id=\"features\">Features</h2>\n\n<h3 style=\"font-weight:bold\" id=\"autodiff\">1. Autodiff</h3>\n\nGradients are computed using reverse-mode autodiff. All computations are representated as a graph of tensors with each tensor holding a reference to a function which can compute the local gradient of that tensor. The calculation of the partial derivative for each tensor is completed when the entire graph is traversed.\n\nThe fundamental idea behind **`autodiff`** is that it calculates the local derivative for each variable rather than its partial derivative. This way traversing through the computational graph is simple and modular, i.e we could calculate the partial derivative of any variable with respect to the output with just one traversal, with a complexity of <img src=\"https://latex.codecogs.com/gif.latex?O(n)\"/>.\n\nThe difference between **partial** and **local derivative** is the way each variable is treated in each equation. When calculating the partial derivative of a function, the expression is broken down into variables, for example <img src=\"https://latex.codecogs.com/gif.latex?c = a*b\"/> and <img src=\"https://latex.codecogs.com/gif.latex?d=a+b+c\"/>, instead of using <img src=\"https://latex.codecogs.com/gif.latex?c\"/>, we say <img src=\"https://latex.codecogs.com/gif.latex?a*b\"/> in the <img src=\"https://latex.codecogs.com/gif.latex?d=a*b*a*b\"/>. On the other hand, when calculating the local derivative of a function, each element in the expression is considered a variable. I understand this might not be clear, so refer to the following example:\n\n### Example\n\n---\n\n```python\n# Lets consider two variables `a` and `b`\na = 2\nb = 3\n\n# Function c which is a multiplied by b\nc = a*b\n\n# where d is a function of `a`, `b`, `c` but infact it is just a function of two variables `a` and `b`\nd = a+c+b\n\n```\n\nAnd if we work out the partial derivative of <img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a}\"/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a} = \\frac{\\partial a}{\\partial a} + \\frac{\\partial c}{\\partial a} + \\frac{\\partial b}{\\partial a}\" /><br/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a} = 1 + \\frac{\\partial (a*b)}{\\partial a} + 0\"/><br/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a} = 1 + a\\frac{\\partial (b)}{\\partial a} + b\\frac{\\partial (a)}{\\partial a}\"/><br/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a} = 1 + a*0 + b*1\"/><br/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a} = 1 + b = 1 + 3 = 4\"/><br/>\n\nThe important part to note is that `c` is written as `a*b` when calculating the derivative. Now, let's try the same computation using the local derivative method.\n\nLocal derivative of `d` w.r.t `a` is expressed as <img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial \\bar{d}}{\\partial a}\"/>, note the bar over the `d`.\n\n#### What does this really mean?\n\nLocal derivative treats `c` as a variable instead of a function of `a` and `b`. Therefore when calculating <img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial \\bar{d}}{\\partial a}\" />, we treat `c` as a constant rather than a function of `a` and `b`. This gives us the <img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial \\bar{d}}{\\partial a} = 1\" /><br/>\n\nSimilarly,\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial \\bar{d}}{\\partial b} = 1\" /><br/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial \\bar{d}}{\\partial c} = 1\" /><br/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial \\bar{c}}{\\partial a} = b\" /><br/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial \\bar{c}}{\\partial b} = a\" /><br/>\n\nTherefore, to calculate <img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a}\" /> we have to multiply edges of a path and add the different path to a node.\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a} = \\frac{\\partial \\bar{d}}{\\partial a} + \\frac{\\partial \\bar{c}}{\\partial a}\" /><br/>\n\n<img src=\"https://latex.codecogs.com/gif.latex?\\frac{\\partial d}{\\partial a} = 1 + b = 1 + 3 = 4\" /><br/>\n\n<h3 style=\"font-weight:bold\" id=\"loss\">2. Loss functions</h3>\n\n2.1 Mean squared error. Example\n\n```python\nfrom matterix.loss import MSE\n\ny_train = ... # Actual/true value\ny_pred = ... # model prediction\n\nloss = MSE(y_train, y_pred)\n\n```\n\n2.2 Root Mean squared error\n\n```python\nfrom matterix.loss import RMSE\n\ny_train = ... # Actual/true value\ny_pred = ... # model prediction\n\nloss = RMSE(y_train, y_pred)\n\n```\n\n<h3 style=\"font-weight:bold\" id=\"opt\">3. Optimizers</h3>\n\n3.1 **Stochastic gradient descent**\n\n```python\nfrom matterix.optimizer import SGD\n\noptimizer = SGD(model, model.parameters(), lr=0.001) # model, parameters to optimize, learning rate\n\n# To set the gradient of the parameters to zero\noptimizer.zero_grad()\n\n# To update the parameters\noptimizer.step()\n\n```\n\n<h3 style=\"font-weight:bold\" id=\"act\">4. Activation functions</h3>\n\n**Functions:** sigmoid, tanh, relu.\n\nAll the activation functions are available from `matterix.functions`. Example,\n\n```python\nfrom matterix.functions import sigmoid\n```\n\n<h3 style=\"font-weight:bold\" id=\"mod\">5. Module</h3>\n\nModule provides the necessary functions to design your own neural network. It has methods to set all the gradients of the parameters to zero, get all the parameters of the network.\n\n1. Create a class which inherits from `nn.Module` to define for network\n2. Initiate your parameters\n3. Write a forward function\n\nSee the example below.\n\n```python\nfrom matterix import Tensor\nimport matterix.nn as nn\n\n# To define a neural network, just inherit `Module` from `nn`\nclass SampleModel(nn.Module):\n\n    def __init__(self) -> None:\n        # Initilalize your parameters\n        self.w1 = Tensor.randn(5, requires_grad=True)\n        self.w2 = Tensor.randn(14, requires_grad=True)\n        ...\n\n    def forward(self, x) -> Tensor:\n\n        out_1 = x @ self.w1\n        ...\n\n        return output\n\nmodel = SampleModel()\n\nmodel.zero_grad() # Sets the gradient of all the parameters to zero\nmodel.parameters() # Gets all the parameters\n```\n\n<h2 style=\"font-weight:bold\">Example</h2>\n\nThe following is a simple example\n\n```python\n# Simple linear regression\nfrom matterix import Tensor\nfrom matterix.nn import Module\nfrom matterix.optim import SGD\nfrom matterix.loss import MSE\n\nfrom tqdm import trange\n\nx_data = Tensor.randn(100,5)\ncoef = Tensor([-1,3,-2, 8, 6])\ny_data = x_data @ coef + 5.0\n\nclass Model(Module):\n\n    def __init__(self):\n        self.w = Tensor.randn(5, requires_grad=True)\n        self.b = Tensor.randn(requires_grad=True)\n\n    def forward(self, x) -> Tensor:\n        return x @ self.w + self.b\n\nmodel = Model()\noptimizer = SGD(model, model.parameters(), lr=0.001)\n\nepochs = 100\n\nfor epoch in (t:=trange(epochs)):\n\n    optimizer.zero_grad()\n\n    y_pred = model(x_data)\n    error = (y_data-y_pred)\n\n    loss = (error*error).sum()\n\n    loss.backward()\n\n    optimizer.step()\n\n    t.set_description(\"Epoch: %.0f Loss: %.10f\" % (epoch, loss.data))\n    t.refresh()\n\nprint(w) # Tensor([-1.000003 3.00000593 -1.99999385 7.99999544 6.0000062 ], shape=(5,))\nprint(b) # Tensor(5.000001599010044, shape=(1,))\n\n```\n\n<h2 style=\"font-weight:bold\">Development setup</h2>\n\nInstall the necessary dependecies in a seperate virtual environment\n\n```bash\n# Create a virtual environment during development to avoid dependency issues\npip install -r requirements.txt\n\n# Before submitting a PR, run the unittests locally\npytest -v\n```\n\n<h2 style=\"font-weight:bold\" id=\"releases\">Release history</h2>\n\n-   **1.0.1**\n\n    -   Used 1.0.0 for testing\n    -   **ADD:** Tanh function, RMSE loss, randn and randint\n\n-   **0.1.1**\n\n    -   **ADD:** Optimizer: SGD\n    -   **ADD:** Functions: Relu\n    -   **ADD:** Loss functions: RMSE, MSETensor\n    -   **ADD:** Module: For defining neural networks\n    -   **FIX:** Floating point precision issue when calculating gradient\n\n-   **0.1.0**\n\n    -   First stable release\n    -   **ADD:** Tensor, tensor operations, sigmoid functions\n    -   **FIX:** Inaccuracies with gradient computation\n\n<h2 style=\"font-weight:bold\" id=\"contributing\">Contributing</h2>\n\n1. Fork it\n\n2. Create your feature branch\n\n    ```bash\n    git checkout -b feature/new_feature\n    ```\n\n3. Commit your changes\n\n    ```\n    git commit -m 'add new feature'\n    ```\n\n4. Push to the branch\n\n    ```\n    git push origin feature/new_feature\n    ```\n\n5. Create a new pull request (PR)\n\n---\n\nSiddesh Sambasivam Suseela - [@ssiddesh45](https://twitter.com/ssiddesh45) - plutocrat45@gmail.com\n\nDistributed under the MIT license. See `LICENSE` for more information.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/SiddeshSambasivam/MatterIx",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "MatterIx",
    "package_url": "https://pypi.org/project/MatterIx/",
    "platform": "",
    "project_url": "https://pypi.org/project/MatterIx/",
    "project_urls": {
      "Bug Tracker": "https://github.com/SiddeshSambasivam/MatterIx/issues",
      "Homepage": "https://github.com/SiddeshSambasivam/MatterIx"
    },
    "release_url": "https://pypi.org/project/MatterIx/1.0.1/",
    "requires_dist": [
      "appdirs (==1.4.4)",
      "attrs (==21.2.0)",
      "black (==21.6b0)",
      "certifi (==2021.5.30)",
      "click (==8.0.1)",
      "iniconfig (==1.1.1)",
      "mypy-extensions (==0.4.3)",
      "numpy (==1.20.3)",
      "packaging (==20.9)",
      "pathspec (==0.8.1)",
      "pluggy (==0.13.1)",
      "py (==1.10.0)",
      "pyparsing (==2.4.7)",
      "pytest (==6.2.4)",
      "regex (==2021.4.4)",
      "toml (==0.10.2)"
    ],
    "requires_python": ">=3.8.0",
    "summary": "Just another deep learning framework",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10820111,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "977fec1c1e53396f6701a3fceba9dec92db7866f8748f294379c6579fbda086d",
        "md5": "72cf435424f49e9bfbc42019247ad32b",
        "sha256": "4538b9a0fbb966af245cfc0b23ebad4937f376c2d4f4afb56341c5876a38371c"
      },
      "downloads": -1,
      "filename": "MatterIx-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "72cf435424f49e9bfbc42019247ad32b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8.0",
      "size": 17465,
      "upload_time": "2021-06-28T17:30:56",
      "upload_time_iso_8601": "2021-06-28T17:30:56.357778Z",
      "url": "https://files.pythonhosted.org/packages/97/7f/ec1c1e53396f6701a3fceba9dec92db7866f8748f294379c6579fbda086d/MatterIx-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b9a1eaea40141d817283204a8652d0f786d96556c83da73c3b67aa116487b3a0",
        "md5": "17906ebf054983006c2c84875bb924f3",
        "sha256": "8907df4a3a8dc71893e126d7f3b3f698476cb49761b03ad60f14b8624636268c"
      },
      "downloads": -1,
      "filename": "MatterIx-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "17906ebf054983006c2c84875bb924f3",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8.0",
      "size": 13323,
      "upload_time": "2021-06-28T17:31:00",
      "upload_time_iso_8601": "2021-06-28T17:31:00.733368Z",
      "url": "https://files.pythonhosted.org/packages/b9/a1/eaea40141d817283204a8652d0f786d96556c83da73c3b67aa116487b3a0/MatterIx-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}