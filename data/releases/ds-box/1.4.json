{
  "info": {
    "author": "Vincent Levorato",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "[![PyPI version](https://badge.fury.io/py/ds-box.svg)](https://badge.fury.io/py/ds-box)\n\n# DS Box\n<p align=\"center\">\n<img width=\"10%\" src=\"https://user-images.githubusercontent.com/17388898/81501373-51baa880-92d8-11ea-8b96-d461bee1d21e.png\">\n</p>\n\nPackage made by <a href=\"https://www.linkedin.com/in/vlevorato/\">V. Levorato</a>.\n\nThis package provides:\n* specific **Operators for Apache Airflow** in order to manage Data Science workflows\n* propose a **bunch of algorithms and routines** complementary to Data Science usual packages\n\n## Install\n\nDSbox can be installed from PyPI or by cloning this repo, and adding it to your project path.\n```\npip install ds-box\n```\n\n## Airflow Operators and Data Units\n\nTwo specifics Airflow operators are given here: **DataOperator** and **KubeDataOperator**\n\n### Airflow DataOperator\nThis operator is able to add input and/or output data during the operator execution. As Airflow operator are stateless, passing simple datasets to be read or written through, for instance, a PythonOperator, needs to write all the code inside the operator to achieve that. With a DataOperator, the only contract you have is to manipulate Pandas dataframe(s) inside the function passed to it. Here is the code structure:\n```python\nfrom dsbox.operators.data_operator import DataOperator\n\nmy_dag_task = DataOperator(operation_function=my_function,\n                           input_unit=reading_some_csv_unit,\n                           output_unit=writing_some_parquet_unit,\n                           dag=dag, task_id='My_process')\n```\n\nWith this example, I just need to define ```my_function```, the ```input_unit``` and the ```output_unit```.\n\n```python\nfrom dsbox.operators.data_unit import DataInputFileUnit, DataOutputFileUnit\n\ndef my_function(dataframe):\n  # useless function sorting dataframe by index\n  return dataframe.sort_index()\n\nreading_some_csv_unit = DataInputFileUnit('path/to/dataset.csv',\n                                          pandas_read_function_name='read_csv',\n                                          sep=';')\n\nwriting_some_parquet_unit = DataOutputFileUnit('path/to/index_sorted_dataset.parquet',\n                                               pandas_write_function_name='to_parquet')                                     \n\n```\n\nSeveral pre-built ```DataUnit``` are avalaible and you can easily build yours. Some examples gives you a preview of complete dags workflow for data science.\n`DataGlobalInputUnit` and `DataGlobalOutputUnit` can now be used with **any dataframe backend API (Pandas, Dask, Vaex, ...)** as you define explicitly which function you are using. \n\n\n### Airflow KubeDataOperator (KDO)\n\nUsing ```DataOperator``` can lead to some architecture issues: tasks must run on a machine or VM having the airflow worker service, and even if it should be moved to specific instance(s), it can be tricky to set auto-scaling mechanism, and to well manage your ressources.\n\nTo avoid that, you can use containers to execute your tasks in a Kubernetes cluster, with the same definition as ```DataOperator```, using ```KubeDataOperator```. This operator is based on the ```\nKubernetesPodOperator```, and thought for data scientists.\n\nThe difference is that the operation definition is made in a YAML description file, and all you have to do is call the definition with this operator. It takes also all the Kubernetes configuration in one parameter (could be done with ```**kwargs``` but explicit is better). Same example as above with KDO:\n\n```python\nfrom dsbox.operators.kube_data_operator import KubeDataOperator\n\nmy_dag_task  = KubeDataOperator(operation='My_process',\n                                kube_conf=build_kpo_config(),\n                                dag=dag)\n```\n\nThe ```build_kpo_config()``` function should be defined based on the project specificity and should explicitly set the ```cmds``` and ```arguments``` parameters needed by the underlying ```\nKubernetesPodOperator```, in addition to the cluster configuration parameters. The ```cmds``` should call a function which will execute the task by using a class called ```Dataoperations``` containing all the datasets definitions with data units and operation functions. Example of a function called by ```cmds```:\n\n```python\nfrom dsbox.kube.data_operations import Dataoperations\n\ndef my_run_function(operation_name, data_root_path, data_operations_file_path):\n\n    project_data_ops = Dataoperations(path=data_root_path)\n    project_data_ops.load_datasets(data_operations_file_path)\n    project_data_ops.run(operation_name)\n```\n\nand an example of the YAML data operations file that should be passed as ```data_operations_file_path```:\n```yaml\nMy_process:\n  operation_function:\n    module: myprojectmodule.process\n    name: my_function\n  input_unit:\n    type: DataInputFileUnit\n    input_path: 'path/to/dataset.csv'\n    pandas_read_function_name: read_csv\n    sep: ';'\n  output_unit:\n    type: DataOutputFileUnit\n    output_path: 'path/to/index_sorted_dataset.parquet'\n    pandas_write_function_name: to_parquet\n```\n\n## Machine Learning part\n\nThis part is a humble contribution to accelerate Data Scientists recurrent operations, like for instance building lagged features for times series or exporting model features contribution. All the code produced here is Pandas and Scikit-learn friendly, as nearly everything is defined as a sklearn estimator.\n\n### Tutorials ðŸŽ“\n#### Neural Networks \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vlevorato/dsbox/blob/develop/notebooks/NeuralNets-AutoEncoder.ipynb) <a href=\"https://github.com/vlevorato/dsbox/blob/develop/notebooks/NeuralNets-AutoEncoder.ipynb\">Classic Auto-encoders architectures</a>\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vlevorato/dsbox/blob/develop/notebooks/NeuralNets-TextClassification.ipynb) <a href=\"https://github.com/vlevorato/dsbox/blob/develop/notebooks/NeuralNets-TextClassification.ipynb\">\nLSTM and CNN for text classification - Sentiment analysis applied to tweets in French</a>\n\n\n\n### Subpackages list\n\n Subpackage | Description\n------------ | -------------\nCalibration | calibration model techniques\nEnsemble | some ensemble models\nExplain | model explanation methods\nFeature_engineering | some categorical and time series pre-built feature engineering\nFeature_selection | some feature selection methods\nMarkov | simple markov chains, supervised and unsupervised Hidden Markov Models\nNeural_networks | end-to-end NN architectures\nOutliers | some outlier detections techniques (MAD, FFT, Gaussian Process, etc.)\nText | some NLP models\nVisualization | some pre-built function to display common graphics\n\n\n_About copyrights on ML part_: some code has been re-packaged from existing libraries which are not or fewly maintained, and for which I could have been involved in the past. All licences are respected, all original authors and repos are quoted.\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/vlevorato/dsbox",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ds-box",
    "package_url": "https://pypi.org/project/ds-box/",
    "platform": "",
    "project_url": "https://pypi.org/project/ds-box/",
    "project_urls": {
      "Homepage": "https://github.com/vlevorato/dsbox"
    },
    "release_url": "https://pypi.org/project/ds-box/1.4/",
    "requires_dist": null,
    "requires_python": ">=3.5",
    "summary": "Package made to accelerate data science workflows in production.",
    "version": "1.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17379814,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1ac0091a023a892b077b558d064c40dcf4ff831fecca4790c17c62a92594d0b0",
        "md5": "3a5c1644983ab8bd8acb0cb6031404db",
        "sha256": "f1a85ec719671b4e285b9e9bd2d46e11ef0c73b35cba481adb535a891a018bb0"
      },
      "downloads": -1,
      "filename": "ds_box-1.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3a5c1644983ab8bd8acb0cb6031404db",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5",
      "size": 150761,
      "upload_time": "2020-06-07T14:12:29",
      "upload_time_iso_8601": "2020-06-07T14:12:29.751892Z",
      "url": "https://files.pythonhosted.org/packages/1a/c0/091a023a892b077b558d064c40dcf4ff831fecca4790c17c62a92594d0b0/ds_box-1.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a0f44fdbc4c0f3983c219ec5899ea4d47cbec2b7f2eaed78c23c60becb3b6c2c",
        "md5": "28d596e4d4acecf48679829ede2921b5",
        "sha256": "c407acbc5e425da685fe0e96ae1931f77d56c32534765ff2c57ce4df72ec0a36"
      },
      "downloads": -1,
      "filename": "ds-box-1.4.tar.gz",
      "has_sig": false,
      "md5_digest": "28d596e4d4acecf48679829ede2921b5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 6754368,
      "upload_time": "2020-06-07T14:12:32",
      "upload_time_iso_8601": "2020-06-07T14:12:32.373359Z",
      "url": "https://files.pythonhosted.org/packages/a0/f4/4fdbc4c0f3983c219ec5899ea4d47cbec2b7f2eaed78c23c60becb3b6c2c/ds-box-1.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}