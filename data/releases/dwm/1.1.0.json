{
  "info": {
    "author": "Jeremiah Coleman",
    "author_email": "colemanja91@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Environment :: Web Environment",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Topic :: Software Development :: Libraries :: Application Frameworks",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# dwm (Data Washing Machine)\nRed Hat's business logic for maintaining marketing data quality\n\n# Introduction\n\nDatabase quality is a problem for many companies. Often there is a mad dash to collect as much data as possible before a single thought is given to keeping that data high quality. Some examples of bad input data include:\n\n - Data collection tools (such as interest or freemium forms) that require manual input, rather than OAuth or picklists\n - Ill-trained data entry or office staff\n - Data purchased from outside sources that does not conform with company standards\n\nBad data introduced through these sources can lead to significant amounts of lost time invested in manual correction, or directly to lost opportunities and revenue due to not being able to query on clean data. Obviously, one solution is to make sure that all data collection sources conform with your database standards, but if you can make that happen, then I'll just be over here flying on my unicorn.\n\nThis package was originally developed for use by Red Hat's Marketing Operations group to maintain quality of contact data, although the principles are sound enough to apply to many types of databases.\n\n# Business logic\n\nThe following are what we have determined to be (as a best practice) the general rules available for cleaning a set of data. Theoretically, any string field can have these rules applied to them; however, when configuring DWM, one should evaluate whether or not a rule is appropriate for a given field.\n\n## Validation\n\nValidation is the removal of data that is straight-up junk and provides no business value whatsoever. This data is usually the result of spam-bots, errors in collection tools (such as posting bad html strings), or someone uploading the wrong spreadsheet. We've split validation into two pieces: generic and field-specific.\n\n### Generic\n\nGeneric validation is the removal of data that has no place in a given database, no matter what field it is found in. The following are examples of generic bad data:\n\n```\naaaaaaaaaaaaaaa ## one repeated character\nfdsafdasfdfdsafdafdsa ## all typed with left hand on the home line\nbuy levitra cialis ## spam data\nwww.buymystuff.com ## marketing database-specific example; our processes do not try to clean any website/URL related fields, so if this appears in one of the fields we are cleaning, it's probably bad data\n```\n\nDWM uses two types of generic validation:\n\n- `genericLookup`: remove 'bad' values based on a known list of bad data (previously observed)\n- `genericRegex`: remove 'bad' values based on regular expressions; i.e., any word longer than 4 characters which is all the same character; or a string containing 'viagra'\n\n### Field-Specific\n\nField-Specific validation is the removal of data that is junk in one field, but good data in another. An example is a string of nine numbers: ```9493020093```. In a phone number field, this is probably good data. In the First Name field, it's junk. Conversely, ```hi, this is a string of letters``` may have a purpose in a text-based field, but it provides no reasonable data in a phone number field.\n\nDWM uses two types of field-specific validation:\n\n- `fieldSpecificLookup`: remove 'bad' values based on a known list of bad data (previously observed)\n- `fieldSpecificRegex`: remove 'bad' values based on regular expressions; i.e., for the field `firstName`, remove any value containing all numbers\n\n## Normalization\n\nNormalization is the correction of data to conform with a certain expected set of values. For example, `Proggrrammer/Developer` is almost a valid \"Job Role\" value `Programmer/Developer`, but is mis-spelled. Another example is `Programmer`, which obviously would fall into the previous category, but is not an exact value match.\n\nNote that normalization usually cannot be applied to fields that are expected to be free-text, such as \"First Name\" or \"Company Name\". If certain rules need to be applied to those fields, use of the __User-Defined Functions__ is recommended.\n\nDWM uses two types of normalization:\n\n- `normLookup`: replace 'almost' values based on a known list of data (previously observed); i.e., common mis-spellings\n- `normRegex`: replace 'almost' values based on regular expressions; i.e., for the field `jobRole`, replace any value that contains `programmer` but not `manager` with `Programmer/Developer`\n- `normIncludes`: replace 'almost' values based on at least one of the following: includes strings, excludes strings, starts with string, ends with string\n\n## Derivation\n\nDerivation is the management of fields designed to help business users more easily make decisions, but are not explicitly collected. One example is a \"Super Region\" field. Although \"Country\" is a value that is likely collected, users of the database may only need to filter to a general region to do their jobs.\n\nDWM uses three types of derivation:\n\n- `deriveValue`: given input values from one or more fields, find the corresponding output value; i.e., for `jobRole='Manager'` and `department='IT'`, then set `persona='IT Decision Maker'`\n- `copyValue`: given an input value from one field, copy that value to the target field\n- `deriveRegex`: given an input value from one field, derive target field value using regular expressions\n- `deriveIncludes`: given an input value from one field, derive target field based on at least one of the following: includes strings, excludes strings, starts with string, ends with string\n\nWithin the runtime configuration, derivation rules are ordered within a dictionary to maintain rule-hierarchy. So, if Rule 1 does not yield a result, then Rule 2 would be tried. The process will exit after one of the derivation rules produces a new value.\n\n## User-Defined Functions\n\nThe above three processes are the most common processes that need to be applied to data, but not everything can be planned for. User-Defined functionality is designed to fill this gap. For instance, US Zipcodes can have some fairly basic and consistent transformations applied to make the data easier to work with (i.e., strip off trailing hypen/number combos, left pad with 0s in case of bad spreadsheet formatting), but those rules don't fall into any of the above categories.\n\nAlso included may be third-party data enrichment. For example, if you have an API contract with a company that provides IP address geolocation, or provides additional company info based on email domain, you can define a function to interact with that API and pull additional data into the fields of interest.\n\n## Order\n\nWe've found this to be the most efficient order in which to run the above cleaning types.\n\n1. Generic Validation\n2. Field-specific Validation\n3. Normalization\n4. Derive Data (aka \"Fill-in-the-gaps\", depending on field type)\n\n## Audit History\n\nRecord-level audit history is a record of what changes were made to which data fields. This includes what the previous value was, what the new/replacement value was, and what rule caused the change. The record is somewhat akin to a git commit, in that it only records where changes were made, and does not keep a record of anything that remained unchanged. Although it is optional in this package, it is recommended for any automation of these processes to provide both a record for troubleshooting and transparency for the business users of the database.\n\n# Useage\n\n```python\nfrom pymongo import MongoClient\nfrom dwm import DWM\n\n# Initialize mongo connection where rules are stored\nmy_mongo = MongoClient()\n\nfield_config = {\n  'field1': {\n    'lookup': ['genericLookup', 'genericRegex', 'fieldSpecificLookup',\n               'fieldSpecificRegex', 'normLookup', 'normRegex', 'normIncludes'],\n    'derive': [\n      {\n        'type': 'deriveIncludes',\n        'fieldSet': ['field2'],\n        'options': ['overwrite', 'blankIfNoMatch']\n      }\n    ]\n  }\n}\n\n# Initialize DWM instance\ndwm_instance = (\n  name='MyDWM',\n  mongo=my_mongo,\n  fields=field_config\n)\n\n# Create a test record to run through DWM\ntest_record = {\n  'field1': 'potentiallybaddata',\n  'field2': 'potentialderivematch'\n}\n\n# Run through DWM and return cleaned record = history/changes\nclean_record, hist = dwm_instance.run(test_record)\n\n```\n\n## Lookups, Derivation, and Regex rules\n\nA complete schema for these items is in the DataDictionary.md file. Also included is a recommendation for indexes to improve performance.\n\n## User-Defined Functions\n\nUser-Defined functions must take exactly two inputs, `data` (a single dictionary of data to which transformations are applied) and `histObj` (a dictionary object used to record field-level changes), and output the same two (with changes applied to `data` and any relevant updates made to `histObj`). Helper functions for recording history are included in the dwm package.\n\nUDFs should ideally be defined in a file separate from the script calling the DWM functions, then loaded in independently.\n\n__Example:__\n\n__udf.py__\n\n```python\nfrom dwm import _CollectHistory_, _CollectHistoryAgg_\ndef myFunction(data, histObj):\n\n  fieldOld = data['myField']\n\n  fieldNew = 'Hi! This is a data change'\n\n  data['myField'] = fieldNew\n\n  change = _CollectHistory_(lookupType='UDF-myFunction', fromVal=fieldOld, toVal=fieldNew) ## recommended format for lookupType: \"UDF-nameOfFunction\"\n\n  histObjUpd = _CollectHistoryAgg_(contactHist=histObj, fieldHistObj=change, fieldName='myField')\n\n  return data, histObj\n\n```\n\n__example.py__\n\n```python\nfrom dwm import Dwm\nfrom udf import myFunction\n\n# Set up UDF config to pass to Dwm\nudf_config = {\n  'beforeDerive': [myFunction]\n}\n\n# Initialize DWM instance\ndwm_instance = (\n  name='MyDWM',\n  mongo=my_mongo,\n  udfs=udf_config\n)\n\n```\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/rh-marketingops/dwm",
    "keywords": "marketing automation data quality cleanse washing cleaning",
    "license": "GNU General Public License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dwm",
    "package_url": "https://pypi.org/project/dwm/",
    "platform": "any",
    "project_url": "https://pypi.org/project/dwm/",
    "project_urls": {
      "Homepage": "https://github.com/rh-marketingops/dwm"
    },
    "release_url": "https://pypi.org/project/dwm/1.1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Best practices for marketing data quality management",
    "version": "1.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 3841978,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a18906d41fd21f602e8ac95fd38259b5d6edf7f3be177d0f88b1f3c2884b84ce",
        "md5": "108033d073ef32195479624479a354ce",
        "sha256": "a77e77000e5fcc87873f306bf97bc60e8964dea16d6d1d022ceaf0a095ae5d2b"
      },
      "downloads": -1,
      "filename": "dwm-1.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "108033d073ef32195479624479a354ce",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 23082,
      "upload_time": "2018-05-07T19:59:02",
      "upload_time_iso_8601": "2018-05-07T19:59:02.758795Z",
      "url": "https://files.pythonhosted.org/packages/a1/89/06d41fd21f602e8ac95fd38259b5d6edf7f3be177d0f88b1f3c2884b84ce/dwm-1.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}