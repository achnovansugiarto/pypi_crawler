{
  "info": {
    "author": "Tommaso Carraro",
    "author_email": "tommaso.carraro@studenti.unipd.it",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "![build status](https://github.com/tommasocarraro/LTNtorch/actions/workflows/build.yml/badge.svg)\n[![coverage status](https://coveralls.io/repos/github/tommasocarraro/LTNtorch/badge.svg?branch=main)](https://coveralls.io/github/tommasocarraro/LTNtorch?branch=main)\n[![PyPi](https://img.shields.io/pypi/v/LTNtorch.svg)](https://pypi.python.org/pypi/LTNtorch)\n[![docs link](https://img.shields.io/badge/docs-github.io-blue)](https://tommasocarraro.github.io/LTNtorch/)\n[![MIT license](https://img.shields.io/github/license/Naereen/StrapDown.js.svg)](https://github.com/Naereen/StrapDown.js/blob/master/LICENSE)\n![python version](https://img.shields.io/badge/python-3.7|3.8|3.9-blue)\n[![DOI BADGE](https://zenodo.org/badge/DOI/10.5281/zenodo.6394282.svg)](https://doi.org/10.5281/zenodo.6394282)\n\n# LTNtorch: PyTorch implementation of Logic Tensor Networks\n\nWelcome to the PyTorch's implementation of [Logic Tensor Networks](https://arxiv.org/abs/2012.13635)!\n\n### Basic idea of the framework\n\nLogic Tensor Network (LTN) is a Neural-Symbolic (NeSy) framework which supports learning of neural networks using the\nsatisfaction of a first-order logic knowledge base as an objective. In other words, LTN uses logical reasoning on the\nknowledge base to guide the learning of a potentially deep neural network. \n\nThe idea of the framework is simple: \n- we have a first-order logic knowledge base containing a set of axioms;\n- we have some predicates, functions, or logical constants appearing in these axioms that we want to learn;\n- we have some data available that we can use to learn the parameters of those symbols.\n\nThe idea is to use the logical axioms as a loss function for our Logic Tensor Network. The objective is to find solutions\nin the hypothesis space that maximally satisfy all the axioms contained in our knowledge base.\n\n### Learning in LTN\n\nIn LTN, the learnable parameters are contained in the predicates, functions, and possibly learnable logical constants\nthat appear in the logical axioms of the knowledge base. \n\nDuring the forward step of the back-propagation algorithm, LTN\ncomputes the truth values of the logical formulas contained in the knowledge base, using the available data to ground\n(or instantiate) the logical formulas. As we have already said, these formulas \nwill contain some predicates and functions which are represented as learnable models. \n\nAt the end of the forward phase,\nthe truth values computed for the formulas are aggregated and used in the loss function. Our objective is to maximize\nthe aggregated truth value, namely maximally satisfy all the axioms.\n\nDuring the backward step, the\nlearnable parameters of predicates, functions, and possibly learnable logical constants are changed in such a way to move\ntowards a solution in the hypothesis space which better satisfies all the axioms in the knowledge base.\n\nAt the end of the training, the parameters of predicates, functions, and constants will have been updated in such a way the \nlogical formulas in the knowledge base are maximally satisfied. In particular, the parameters will have been learned by using both\ndata (to ground the formulas) and logical reasoning (at the loss function).\n\nAfter learning, it is possible to query predicates and functions on new data which was not available during training. Also,\nit is possible to query the truth values of new formulas which were not included in the knowledge base during training. In addition,\nif some logical constants have been learned, their parameters can be interpreted as embeddings.\n\n### Real Logic logical language\n\nTo make this learning possible, LTN uses a differentiable first-order logic language, called Real Logic, which enables \nthe incorporation of data and logic.\n\nReal Logic defines the concept of `grounding` (different from the grounding of logic), which is a mapping from the logical domain (i.e., constants, variables, and logical symbols)\nto tensors in the Real field or operations based on tensors. These operations could be, for instance, mathematical functions or learnable neural networks. In other words,\na `grounding`, denoted as ùí¢, is a function which maps a logical symbol into a real tensor or an operation on tensors.\n\nIn particular, the grounding is defined as follows. Let us assume that *c* is a constant, *x* is a logical \nvariable, *P* is a predicate, and *f* is a logical function:\n![Grounding_in LTN](https://github.com/tommasocarraro/LTNtorch/blob/main/images/grounding.png?raw=true)\n\nThe `grounding` defines also how the logical connectives (‚àß, ‚à®, ¬¨, ‚áí, ‚Üî) and quantifiers\n(‚àÄ, ‚àÉ) are mapped in the Real domain. In particular, logical connectives are grounded using fuzzy logic semantics, while\nquantifiers are grounded using fuzzy aggregators. Please, **carefully** read the paper if you have some doubts on these notions.\n\nExamples of possible groundings are showed in the following figure. In the figure, `friend(Mary, John)` is an\natomic formula (predicate), while `‚àÄx(friend(John, x) ‚áí friend(Mary, x))` is a closed formula (all the variables are\nquantified). The letter ùí¢, again, is the grounding, the function which maps the logical domain into the Real domain.\n\n![Grounding_illustration](https://github.com/tommasocarraro/LTNtorch/blob/main/images/framework_grounding.png?raw=true)\n\n### LTN as PyTorch computational graphs\n\nIn practice, LTN converts Real Logic formulas (e.g., `‚àÄx‚àÉy(friend(x,y) ‚àß italian(y))`, which states that everybody has a friend that is Italian) into [PyTorch](https://www.pytorch.org/) \ncomputational graphs. Such formulas can express complex queries about the data, prior knowledge to satisfy during \nlearning, statements to prove, etc. The following figure shows an example of how LTN converts such formulas into PyTorch \ncomputational graphs.\n\n![Computational_graph_illustration](https://github.com/tommasocarraro/LTNtorch/blob/main/images/framework_computational_graph.png?raw=true)\n\nLet us assume we have 6 people which are denoted using 4 real-valued features.\nThe previous figure illustrates the following:\n![Computational_graph_explanation](https://github.com/tommasocarraro/LTNtorch/blob/main/images/computational_graph_explanation.png?raw=true)\n\n### Conclusion\n\nUsing LTN, one can represent and effectively compute some of the most important tasks of deep learning. Examples of such \ntasks are classification, regression, clustering, and so on.\n\nThe [Getting Started](#getting-started) section of the README links to tutorials and examples to learn how to code Logic\nTensor Networks in PyTorch.\n\nHowever, we suggest to carefully read the [paper](https://arxiv.org/pdf/2012.13635.pdf) before going through the tutorials and examples.\n\n\n# Installation\n\nIt is possible to install LTNtorch using `pip`.\n\n`pip install LTNtorch`\n\nAlternatively, it is possible to install LTNtorch by cloning this repository. In this case, make sure to install all the \nrequirements.\n\n`pip3 install -r requirements.txt`\n\n# Structure of repository\n\n- `ltn/core.py`: this module contains the implementation of the LTN framework. In particular, it contains the definition \nof constants, variables, predicates, functions, connectives and quantifiers;\n- `ltn/fuzzy_ops.py`: this module contains the implementation of common fuzzy logic semantics using PyTorch primitives;\n- `tutorials/`: this folder contains some important tutorials to getting started with coding in LTN;\n- `examples/`: this folder contains various problems approached using LTN and presented in the \"Reach of Logic Tensor \nNetworks\" section of the paper;\n- `tests/`: this folder contains unit tests that have been used to test the `core` and `fuzzy_ops` modules. The coverage \nis 100%.\n\n# Tests\n\nThe `core` and `fuzzy_ops` modules of this repository have been entirely tested using `pytest`, with a coverage of 100%.\nThe examples included in the documentation have also been tested, using `doctest`.\n\n# Documentation\n\nThe [documentation](https://tommasocarraro.github.io/LTNtorch/) has been created with \n[Sphinx](https://www.sphinx-doc.org/en/master/index.html), using the \n[Read the Docs Sphinx Theme](https://sphinx-rtd-theme.readthedocs.io/en/stable/).\n\n# Getting Started\n\n## Tutorials\n\n`tutorials/` contains some important tutorials to getting started with coding in LTN. We suggest completing the tutorials in order.\nThe tutorials cover the following topics:\n1. [Grounding in LTN (part 1)](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/tutorials/1-grounding_non_logical_symbols.ipynb): Real Logic, constants, predicates, functions, variables;\n2. [Grounding in LTN (part 2)](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/tutorials/2-grounding_connectives.ipynb): connectives and quantifiers (+ [complement](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/tutorials/2b-operators-and-gradients.ipynb): choosing appropriate operators for learning);\n3. [Learning in LTN](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/tutorials/3-knowledgebase-and-learning.ipynb): using satisfiability of LTN formulas as a training objective.\n\nThe tutorials are implemented using jupyter notebooks.\n\n## Examples\n\n`examples/` contains the series of examples included in the \"Reach of Logic Tensor Networks\" section of the paper. Their objective \nis to show how the language of Real Logic can be used to specify a number of tasks that involve learning from data and \nreasoning about logical knowledge. \n\nThe examples covered are the following:\n1. [Binary classification](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/examples/1-binary_classification.ipynb): illustrates, in the simplest setting, how to ground a binary classifier as a predicate in LTN;\n2. [Multi-class single-label classification](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/examples/2-multi_class_single_label_classification.ipynb): illustrate how to ground predicates that can classify samples in several mutually-exclusive classes;\n3. [Multi-class multi-label classification](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/examples/3-multi_class_multi_label_classification.ipynb): illustrate how to ground predicates that can classify samples in several classes which are not mutually-exclusive;\n4. [Semi-supervised pattern recognition](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/examples/4-semi-supervised_pattern_recognition.ipynb): showcases the power of LTN in dealing with semi-supervised learning tasks;\n5. [Regression](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/examples/5-regression.ipynb): illustrates how to ground a regressor as a function symbol in LTN;\n6. [Clustering](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/examples/6-clustering.ipynb): illustrates how LTN can solve a unsupervised tasks using first-order logical constraints;\n7. [Learning embeddings with LTN](https://nbviewer.jupyter.org/github/tommasocarraro/LTNtorch/blob/main/examples/7-learning_embeddings_with_LTN.ipynb): illustrates how LTN can learn embeddings using learnable logical constants.\n\nThe examples are presented using jupyter notebooks.\n\n# License\n\nThis project is licensed under the MIT License - see the [LICENSE](https://nbviewer.jupyter.org/github/bmxitalia/LTNtorch/blob/main/LICENSE) file for details.\n\n# Acknowledgements\n\nLTN has been developed thanks to active contributions and discussions with the following people (in alphabetical order):\n- Alessandro Daniele (FBK)\n- Artur d‚ÄôAvila Garcez (City)\n- Benedikt Wagner (City)\n- Emile van Krieken (VU Amsterdam)\n- Francesco Giannini (UniSiena)\n- Giuseppe Marra (UniSiena)\n- Ivan Donadello (FBK)\n- Lucas Bechberger (UniOsnabruck)\n- Luciano Serafini (FBK)\n- Marco Gori (UniSiena)\n- Michael Spranger (Sony AI)\n- Michelangelo Diligenti (UniSiena)\n- Samy Badreddine (Sony AI)\n- Tommaso Carraro (FBK)\n\n# Citing this repo\n\nIf you are using **LTNtorch** in your work, please consider citing this repository.\n\n```\n@software{LTNtorch,\n  author = {Tommaso Carraro},\n  title = {{LTNtorch: PyTorch implementation of Logic Tensor Networks}},\n  month = {mar},\n  year = {2022},\n  publisher = {Zenodo},\n  version = {1.0.0},\n  doi = {10.5281/zenodo.6394282},\n  url = {https://doi.org/10.5281/zenodo.6394282}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/bmxitalia/LTNtorch",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/bmxitalia/LTNtorch",
    "keywords": "pytorch,machine-learning,framework,neural-symbolic-computing,fuzzy-logic",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "LTNtorch",
    "package_url": "https://pypi.org/project/LTNtorch/",
    "platform": null,
    "project_url": "https://pypi.org/project/LTNtorch/",
    "project_urls": {
      "Download": "https://github.com/bmxitalia/LTNtorch",
      "Homepage": "https://github.com/bmxitalia/LTNtorch"
    },
    "release_url": "https://pypi.org/project/LTNtorch/1.0.1/",
    "requires_dist": [
      "numpy",
      "torch"
    ],
    "requires_python": ">=3.7",
    "summary": "LTNtorch: PyTorch implementation of Logic Tensor Networks",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17477764,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "73111cc312c068f86c5408d9b71842634d8a89415c4419e599086a1d2e13503a",
        "md5": "36e165c610dc8bd5e4644845c08c12e1",
        "sha256": "028e3fd652ae7996735d0b35691924b11abdf22eed00ab730d53adf863f65284"
      },
      "downloads": -1,
      "filename": "LTNtorch-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "36e165c610dc8bd5e4644845c08c12e1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 29256,
      "upload_time": "2023-03-28T12:38:33",
      "upload_time_iso_8601": "2023-03-28T12:38:33.488992Z",
      "url": "https://files.pythonhosted.org/packages/73/11/1cc312c068f86c5408d9b71842634d8a89415c4419e599086a1d2e13503a/LTNtorch-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ea221100e3c81b27316a71ae79865d907c355684bfe45b5590c3848a1612114f",
        "md5": "605ebfca5ac16bc4438bfa792da73b67",
        "sha256": "b485dc518665898a2483c074111b725185274738b70fbc79ee829c59b5ad55f3"
      },
      "downloads": -1,
      "filename": "LTNtorch-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "605ebfca5ac16bc4438bfa792da73b67",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 28633,
      "upload_time": "2023-03-28T12:38:35",
      "upload_time_iso_8601": "2023-03-28T12:38:35.165020Z",
      "url": "https://files.pythonhosted.org/packages/ea/22/1100e3c81b27316a71ae79865d907c355684bfe45b5590c3848a1612114f/LTNtorch-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}