{
  "info": {
    "author": "Maciej Brencz",
    "author_email": "maciej.brencz@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Topic :: Text Processing :: Markup :: XML"
    ],
    "description": "# mediawiki-dump\n[![PyPI](https://img.shields.io/pypi/v/mediawiki_dump.svg)](https://pypi.python.org/pypi/mediawiki_dump)\n[![Downloads](https://pepy.tech/badge/mediawiki_dump)](https://pepy.tech/project/mediawiki_dump)\n[![CI](https://github.com/macbre/mediawiki-dump/actions/workflows/tests.yml/badge.svg)](https://github.com/macbre/mediawiki-dump/actions/workflows/tests.yml)\n[![Coverage Status](https://coveralls.io/repos/github/macbre/mediawiki-dump/badge.svg?branch=master)](https://coveralls.io/github/macbre/mediawiki-dump?branch=master)\n\n```\npip install mediawiki_dump\n```\n\n[Python3 package](https://pypi.org/project/mediawiki_dump/) for working with [MediaWiki XML content dumps](https://www.mediawiki.org/wiki/Manual:Backing_up_a_wiki#Backup_the_content_of_the_wiki_(XML_dump)).\n\n[Wikipedia](https://dumps.wikimedia.org/) (bz2 compressed) and [Wikia](https://community.fandom.com/wiki/Help:Database_download) (7zip) content dumps are supported.\n\n## Dependencies\n\nIn order to read 7zip archives (used by Wikia's XML dumps) you need to install [`libarchive`](http://libarchive.org/):\n\n```\nsudo apt install libarchive-dev\n```\n\n## API\n\n### Tokenizer\n\nAllows you to clean up the wikitext:\n\n```python\nfrom mediawiki_dump.tokenizer import clean\nclean('[[Foo|bar]] is a link')\n'bar is a link'\n```\n\nAnd then tokenize the text:\n\n```python\nfrom mediawiki_dump.tokenizer import tokenize\ntokenize('11. juni 2007 varð kunngjørt, at Svínoyar kommuna verður løgd saman við Klaksvíkar kommunu eftir komandi bygdaráðsval.')\n['juni', 'varð', 'kunngjørt', 'at', 'Svínoyar', 'kommuna', 'verður', 'løgd', 'saman', 'við', 'Klaksvíkar', 'kommunu', 'eftir', 'komandi', 'bygdaráðsval']\n```\n\n### Dump reader\n\nFetch and parse dumps (using a local file cache):\n\n```python\nfrom mediawiki_dump.dumps import WikipediaDump\nfrom mediawiki_dump.reader import DumpReader\n\ndump = WikipediaDump('fo')\npages = DumpReader().read(dump)\n\n[page.title for page in pages][:10]\n\n['Main Page', 'Brúkari:Jon Harald Søby', 'Forsíða', 'Ormurin Langi', 'Regin smiður', 'Fyrimynd:InterLingvLigoj', 'Heimsyvirlýsingin um mannarættindi', 'Bólkur:Kvæði', 'Bólkur:Yrking', 'Kjak:Forsíða']\n```\n\n`read` method yields the `DumpEntry` object for each revision.\n\nBy using `DumpReaderArticles` class you can read article pages only:\n\n```python\nimport logging; logging.basicConfig(level=logging.INFO)\n\nfrom mediawiki_dump.dumps import WikipediaDump\nfrom mediawiki_dump.reader import DumpReaderArticles\n\ndump = WikipediaDump('fo')\nreader = DumpReaderArticles()\npages = reader.read(dump)\n\nprint([page.title for page in pages][:25])\n\nprint(reader.get_dump_language())  # fo\n```\n\nWill give you:\n\n```\nINFO:DumpReaderArticles:Parsing XML dump...\nINFO:WikipediaDump:Checking /tmp/wikicorpus_62da4928a0a307185acaaa94f537d090.bz2 cache file...\nINFO:WikipediaDump:Fetching fo dump from <https://dumps.wikimedia.org/fowiki/latest/fowiki-latest-pages-meta-current.xml.bz2>...\nINFO:WikipediaDump:HTTP 200 (14105 kB will be fetched)\nINFO:WikipediaDump:Cache set\n...\n['WIKIng', 'Føroyar', 'Borðoy', 'Eysturoy', 'Fugloy', 'Forsíða', 'Løgmenn í Føroyum', 'GNU Free Documentation License', 'GFDL', 'Opið innihald', 'Wikipedia', 'Alfrøði', '2004', '20. juni', 'WikiWiki', 'Wiki', 'Danmark', '21. juni', '22. juni', '23. juni', 'Lívfrøði', '24. juni', '25. juni', '26. juni', '27. juni']\n```\n\n## Reading Wikia's dumps\n\n ```python\nimport logging; logging.basicConfig(level=logging.INFO)\n\nfrom mediawiki_dump.dumps import WikiaDump\nfrom mediawiki_dump.reader import DumpReaderArticles\n\ndump = WikiaDump('plnordycka')\npages = DumpReaderArticles().read(dump)\n\nprint([page.title for page in pages][:25])\n```\n\nWill give you:\n\n```\nINFO:DumpReaderArticles:Parsing XML dump...\nINFO:WikiaDump:Checking /tmp/wikicorpus_f7dd3b75c5965ee10ae5fe4643fb806b.7z cache file...\nINFO:WikiaDump:Fetching plnordycka dump from <https://s3.amazonaws.com/wikia_xml_dumps/p/pl/plnordycka_pages_current.xml.7z>...\nINFO:WikiaDump:HTTP 200 (129 kB will be fetched)\nINFO:WikiaDump:Cache set\nINFO:WikiaDump:Reading wikicorpus_f7dd3b75c5965ee10ae5fe4643fb806b file from dump\n...\nINFO:DumpReaderArticles:Parsing completed, entries found: 615\n['Nordycka Wiki', 'Strona główna', '1968', '1948', 'Ormurin Langi', 'Mykines', 'Trollsjön', 'Wyspy Owcze', 'Nólsoy', 'Sandoy', 'Vágar', 'Mørk', 'Eysturoy', 'Rakfisk', 'Hákarl', '1298', 'Sztokfisz', '1978', '1920', 'Najbardziej na północ', 'Svalbard', 'Hamferð', 'Rok w Skandynawii', 'Islandia', 'Rissajaure']\n```\n\n## Fetching full history\n\nPass `full_history` to `BaseDump` constructor to fetch the XML content dump with full history:\n\n```python\nimport logging; logging.basicConfig(level=logging.INFO)\n\nfrom mediawiki_dump.dumps import WikiaDump\nfrom mediawiki_dump.reader import DumpReaderArticles\n\ndump = WikiaDump('macbre', full_history=True)  # fetch full history, including old revisions\npages = DumpReaderArticles().read(dump)\n\nprint('\\n'.join([repr(page) for page in pages]))\n```\n\nWill give you:\n\n```\nINFO:DumpReaderArticles:Parsing completed, entries found: 384\n<DumpEntry \"Macbre Wiki\" by Default at 2016-10-12T19:51:06+00:00>\n<DumpEntry \"Macbre Wiki\" by Wikia at 2016-10-12T19:51:05+00:00>\n<DumpEntry \"Macbre Wiki\" by Macbre at 2016-11-04T10:33:20+00:00>\n<DumpEntry \"Macbre Wiki\" by FandomBot at 2016-11-04T10:37:17+00:00>\n<DumpEntry \"Macbre Wiki\" by FandomBot at 2017-01-25T14:47:37+00:00>\n<DumpEntry \"Macbre Wiki\" by Ryba777 at 2017-04-10T11:20:25+00:00>\n<DumpEntry \"Macbre Wiki\" by Ryba777 at 2017-04-10T11:21:20+00:00>\n<DumpEntry \"Macbre Wiki\" by Macbre at 2018-03-07T12:51:12+00:00>\n<DumpEntry \"Main Page\" by Wikia at 2016-10-12T19:51:05+00:00>\n<DumpEntry \"FooBar\" by Anonymous at 2016-11-08T10:15:33+00:00>\n<DumpEntry \"FooBar\" by Anonymous at 2016-11-08T10:15:49+00:00>\n...\n<DumpEntry \"YouTube tag\" by FANDOMbot at 2018-06-05T11:45:44+00:00>\n<DumpEntry \"Maps\" by Macbre at 2018-06-06T08:51:24+00:00>\n<DumpEntry \"Maps\" by Macbre at 2018-06-07T08:17:13+00:00>\n<DumpEntry \"Maps\" by Macbre at 2018-06-07T08:17:36+00:00>\n<DumpEntry \"Scary transclusion\" by Macbre at 2018-07-24T14:52:20+00:00>\n<DumpEntry \"Lua\" by Macbre at 2018-09-11T14:04:15+00:00>\n<DumpEntry \"Lua\" by Macbre at 2018-09-11T14:14:24+00:00>\n<DumpEntry \"Lua\" by Macbre at 2018-09-11T14:14:37+00:00>\n```\n\n## Reading dumps of selected articles\n\nYou can use [`mwclient` Python library](https://mwclient.readthedocs.io/en/latest/index.html)\nand fetch \"live\" dumps of selected articles from any MediaWiki-powered site.\n\n```python\nimport mwclient\nsite = mwclient.Site('vim.fandom.com', path='/')\n\nfrom mediawiki_dump.dumps import MediaWikiClientDump\nfrom mediawiki_dump.reader import DumpReaderArticles\n\ndump = MediaWikiClientDump(site, ['Vim documentation', 'Tutorial'])\n\npages = DumpReaderArticles().read(dump)\n\nprint('\\n'.join([repr(page) for page in pages]))\n```\n\nWill give you:\n\n```\n<DumpEntry \"Vim documentation\" by Anonymous at 2019-07-05T09:39:47+00:00>\n<DumpEntry \"Tutorial\" by Anonymous at 2019-07-05T09:41:19+00:00>\n```\n\n## Finding pages with a specific [parser tag](https://www.mediawiki.org/wiki/Manual:Tag_extensions)\n\nLet's find pages where no longer supported `<place>` tag is still used:\n\n```python\nimport logging; logging.basicConfig(level=logging.INFO)\n\nfrom mediawiki_dump.dumps import WikiaDump\nfrom mediawiki_dump.reader import DumpReader\n\ndump = WikiaDump('plpoznan')\npages = DumpReader().read(dump)\n\nwith_places_tag = [\n    page.title\n    for page in pages\n    if '<place ' in page.content\n]\n\nlogging.info('Pages found: %d', len(with_places_tag))\n\nwith open(\"pages.txt\", mode=\"wt\", encoding=\"utf-8\") as fp:\n    for entry in with_places_tag:\n        fp.write(entry + \"\\n\")\n\nlogging.info(\"pages.txt file created\")\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/macbre/mediawiki-dump",
    "keywords": "dump fandom mediawiki wikipedia wikia",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mediawiki-dump",
    "package_url": "https://pypi.org/project/mediawiki-dump/",
    "platform": null,
    "project_url": "https://pypi.org/project/mediawiki-dump/",
    "project_urls": {
      "Homepage": "https://github.com/macbre/mediawiki-dump"
    },
    "release_url": "https://pypi.org/project/mediawiki-dump/1.1.0/",
    "requires_dist": [
      "libarchive-c (==4.0)",
      "requests (>=2.26.0)",
      "mwclient (>=0.10.1)",
      "black (==23.1.0) ; extra == 'dev'",
      "coveralls (==3.3.1) ; extra == 'dev'",
      "pylint (==2.17.0) ; extra == 'dev'",
      "pytest (==7.2.2) ; extra == 'dev'",
      "pytest-cov (==4.0.0) ; extra == 'dev'",
      "responses (==0.23.1) ; extra == 'dev'"
    ],
    "requires_python": ">=3.7",
    "summary": "Python package for working with MediaWiki XML content dumps",
    "version": "1.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17308155,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "253aaae76a6838a5755e73bc918d7a4a7516d6af0321f281c50dd56c341329fc",
        "md5": "d0e109efb42d4ec52d6aab5429fbc458",
        "sha256": "400f708db55fc1252b82ea223ad0f9540224291d34225841e2b0388b5663c276"
      },
      "downloads": -1,
      "filename": "mediawiki_dump-1.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d0e109efb42d4ec52d6aab5429fbc458",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 18923,
      "upload_time": "2023-03-15T19:23:15",
      "upload_time_iso_8601": "2023-03-15T19:23:15.622531Z",
      "url": "https://files.pythonhosted.org/packages/25/3a/aae76a6838a5755e73bc918d7a4a7516d6af0321f281c50dd56c341329fc/mediawiki_dump-1.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9160a9a5068a51823feeb79319fa95256db30c9e1a01d773f5c3891e4857cdc0",
        "md5": "d791e9b1a0f3e640b4e98743d47bcb61",
        "sha256": "1a4ec70686a880306cfd23a951f7d3011040abb1d6105976fc7245dec8adc079"
      },
      "downloads": -1,
      "filename": "mediawiki_dump-1.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "d791e9b1a0f3e640b4e98743d47bcb61",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 14054,
      "upload_time": "2023-03-15T19:23:17",
      "upload_time_iso_8601": "2023-03-15T19:23:17.470761Z",
      "url": "https://files.pythonhosted.org/packages/91/60/a9a5068a51823feeb79319fa95256db30c9e1a01d773f5c3891e4857cdc0/mediawiki_dump-1.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}