{
  "info": {
    "author": "Balbir",
    "author_email": "Balbir250894@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "### dq-module\ndq-module is a tool which can be used to perform validations and profiling on the datasets.This tool is compatible with two run_engines `pyspark` and `polars`.\n### Features\n## 1. Data Validation \nThis library contains a `SingleDatasetQualityCheck()` class which can used to validate the dataset against a defined set of rules.\nThis class contains the following rules which can be used to validate the dataset: \n - null_check\n - schema_check\n - range_min_check\n - range_max_check\n - length_check\n - unique_records_check\n - unique_keys_check\n - allowed_values_check\n - min_length_check\n - column_count_check\n - not_allowed_values_check\n -  date_range_check\n - regex_check\n - row_count_check\n\n### How To Use\n```sh\nimport dataqualitycheck as dq\nfrom datetime import date\nimport time\n```\n##### Step 1: Adding a datasource\nWe have 4 classes which can be used to connect to a datasource:\n1. `AzureBlobDF()` - This class can be used to interact with the datasources on azure blob storage . \n2. `DatabricksFileSystemDF()` - This class can be used to interect with the datasources on  databricks filesystem.\n3. `DatabricksSqlDF()` - This class can be used to interact with the\ndatasources on  databricks databases.\n4. `LocalFileSystemDF()` - This class can be used to interact with the datasources on your local filesystem.\n\nEach of the above class provides the functionalities to read and write from the respective datasources.\n\n\n**Example:**\nPass the configuration of the  blob connector  in `blob_connector_config`and \nadd a datasource by defining a `data_read_ob` and `data_write_ob`.\n```sh\nblob_connector_config = {\"storage_account_name\" : \"rgmdemostorage\", \"container_name\" : \"cooler-images\", \"sas_token\" : <vaild-sas-token>}\n```\n```sh\ndata_read_ob = dq.AzureBlobDF(storage_name=blob_connector_config[\"storage_account_name\"], sas_token=blob_connector_config[\"sas_token\"])\ndata_write_ob = dq.DatabricksFileSystemDF()\n```\n\n `tables_list` is a dictionary that contains the list of sources along with the container_name , source_type , layer , source_name , filename , read_connector_method and latest_file_path  for the tables on which the validations has to be applied . \n```sh\n#This is optional.It is required when you are calling individual rules.\ntables_list={}\n```\n##### Step 2: Add a DataContext \nInstantiate a DataContext by passing `tables_list`,`interaction_between_tables`,`data_read_ob`,`data_write_ob`, `data_right_structure`,`job_id`,`time_zone`,`no_of_partition` and `output_db_name `.\nYou can also pass the `run_engine` with which you want to apply the quality checks. By default, the run_engine is `pyspark`.\n```sh\ndq_ob = dq.SingleDatasetQualityCheck(tables_list={}, \n                                  interaction_between_tables=[],  \n                                  data_read_ob=data_read_ob, \n                                  data_write_ob=data_write_ob, \n                                  data_right_structure='file',\n                                  job_id=<pipeline_run_id>,\n                                  time_zone=None,\n            output_db_name=<database_name_where_report_has_to_be_written>,\n                                  no_of_partition=4)\n```\n##### Step 3:\nPassing a `rules_diagnosys_summery_file_path` and `config_df` as an input and apply validations on various columns of respective table defined in the `config_df`.\n\nHere is an sample of the `config_df`.\n\n![config_df](https://i.imgur.com/p5348Jd.png)\n\n**Example:**\n```sh\nrules_diagnosys_summery_folder_path = <folder-path-for-the-output-report>\n \nconfig_df = spark.read.option(\"header\",True).csv(<path-to-the-config>)\n\ndq_ob.apply_validation(config_df, write_summary_on_database=True, failed_schema_source_list=[], output_summary_folder_path=rules_diagnosys_summery_folder_path)\n```\n## 2. Data Profiling\n- We can generate a detailed summary statistics such as mean, median, mode, list of uniques, missing count etc. of a dataset using the `DataProfile()`class.\n- This class can also be used to recommend some  data quality rules  based on the profiling report generated on the dataset.\n### How To Use\n\n**Step 1: Add a Datasource**\n```sh\ndata_write_ob = dq.DatabricksSqlDF()\ndata_write_ob = dq.DatabricksSqlDF()\n```\n**Step 2: Add a DataContext**\n```sh\nimport pytz\ntime_zone = pytz.timezone('US/Central')\ndq_ob = dq.DataProfile(tables_list=tables_list,\n                     interaction_between_tables=[],\n                     data_read_ob=data_read_ob,\n                     data_write_ob=data_write_ob,\n                     data_right_structure='table',\n                     job_id=<pipeline_run_id>,\n                     time_zone=time_zone,\n                     no_of_partition=4,\n            output_db_name=<database_name_where_report_has_to_be_written>,\n                     run_engine='polars')\n```\n**Step 3:**\nPass `config_df` as an input and apply data profiling on various columns of respective table defined in the `config_df`.\n``` sh\n# You can create a config_df in pyspark/polars run_engine directly also rather than reading as a csv.\nconfig_df = spark.createDataFrame([{\"container_name\" : \"rgm-data-quality\",\n                                 \"source_type\" : \"sharepoint\",\n                                 \"layer\" : \"raw\",\n                                 \"source_name\" : \"data_quality_db\",\n                                 \"filename\" : \"neilsen_data_parquet\",\n            \"latest_file_path\" : \"data_quality_db.neilsen_data_parquet\",  \n                            \"read_connector_method\" : \"databricks sql\",\n         \"output_folder_structure\" : \"processed/data_profiling_test/\"}])\n```\n```sh\n# Generating a data profiling report.\ndq_ob.apply_data_profiling(source_config_df=config_df,\n                           write_consolidated_report=True)\n```\n```sh\n# Generating a data profiling report as well as recommending the quality rules based on the profiling report.\nrules_config = dq_ob.data_profiling_based_quality_rules(config_df, list_of_columns_to_be_ignored)\n```\n## 3. Consistency Check\n You can check the consistency of common columns between two tables using the `ConsistencyCheck()` class.\n### How To Use\n\n**Step 1: Add a datasource.**\n```sh\ndata_read_ob = dq.DatabricksFileSystemDF()\ndata_write_ob = dq.AzureBlobDF(storage_name=blob_connector_config[\"storage_account_name\"], sas_token=blob_connector_config[\"sas_token\"])\n```\n**Step 2: Add a DataContext**\n```sh\ndq_ob = dq.ConsistencyCheck(tables_list={}, \n                   interaction_between_tables=[],\n                   data_read_ob=data_read_ob,\n                   data_write_ob=data_write_ob, \n                   data_right_structure='file',\n                   job_id=<pipeline_run_id>,\n                   time_zone=None,\n                   no_of_partition=4,\n         output_db_name=<database_name_where_report_has_to_be_written>)\n```\n**Step 3:**\n Pass `config_df` and `output_report_folder_path` as an input and apply consistency check.\n Here is a sample consistency check config.\n\n![config_df](https://i.imgur.com/lFMdGoj.png)\n\n```sh\nconfig_df = spark.read.option(\"header\",True).csv(<path-to-the-consistency-check-config>)\noutput_report_folder_path = <folder-path-for-the-output-report>\n\ndq_ob.apply_consistency_check(config_df, output_report_folder_path)\n\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "module-dataquality",
    "package_url": "https://pypi.org/project/module-dataquality/",
    "platform": null,
    "project_url": "https://pypi.org/project/module-dataquality/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/module-dataquality/1.0.6/",
    "requires_dist": [
      "polars"
    ],
    "requires_python": ">=3.8",
    "summary": "data profiling and basic data quality rules check",
    "version": "1.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17299622,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "484dd496411ef75dd3232f79900046efb2a1bfb84e0d677886d1e7b8e7d5e622",
        "md5": "beb042ac72dbc9a2c72361da5d6f7063",
        "sha256": "ed88e8b424d994497d1ce84c99ec72121aa8b66a664d0fd030bb8c4867454f10"
      },
      "downloads": -1,
      "filename": "module_dataquality-1.0.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "beb042ac72dbc9a2c72361da5d6f7063",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 31814,
      "upload_time": "2023-03-14T12:39:22",
      "upload_time_iso_8601": "2023-03-14T12:39:22.707977Z",
      "url": "https://files.pythonhosted.org/packages/48/4d/d496411ef75dd3232f79900046efb2a1bfb84e0d677886d1e7b8e7d5e622/module_dataquality-1.0.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}