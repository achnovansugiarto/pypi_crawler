{
  "info": {
    "author": "Balbir",
    "author_email": "Balbir250894@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "===================\nWorking Example\n===================\n\n.. code:: sh\n\n   import dataqualitycheck as dq\n   from datetime import date\n   import time\n\nApplying SingleDatasetQualityCheck\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n**Step-1:** Pass the configuration of the blob connector in\n``blob_connector_config`` and add a datasource by defining a\n``data_read_ob`` and ``data_write_ob``.\n\n.. code:: sh\n\n   blob_connector_config = {\"storage_account_name\": \"rgmdemostorage\", \"storage_account_access_key\": \"Yi0oL/FTXMVT1GqmKEtg57gshyWxIw15o+AyhcC27qnHfk9ljLPzzG4Fw+Z6u1yp3tfNqYEZ+wln+AStEYJGug==\" , \"container_name\":\"cooler-images\", \"sas_token\":\"?sv=2021-06-08&ss=bfqt&srt=co&sp=rwdlacupytf&se=2024-01-31T19:10:46Z&st=2022-12-16T11:10:46Z&spr=https&sig=3dzIPEHiPRohQpJn90XpaEKuER7D5TY5lvWZGm0yvbk%3D\"}\n\n.. code:: sh\n\n   data_read_ob = dq.AzureBlobDF(storage_name = blob_connector_config[\"storage_account_name\"], sas_token = blob_connector_config[\"sas_token\"])\n   data_write_ob =dq.AzureBlobDF(storage_name = blob_connector_config[\"storage_account_name\"], sas_token = blob_connector_config[\"sas_token\"])\n\n**Step-2:** ``tables_list`` is a dictionary that contains the list of\nsources along with the container_name , source_type , layer ,\nsource_name , filename , read_connector_method and latest_file_path for\nthe tables on which the validations has to be applied .\n\n.. code:: sh\n\n   tables_list={}\n\n**Step-3:** Instantiate a DataContext by passing\n``tables_list``,\\ ``interaction_between_tables``,\\ ``data_read_ob``,\\ ``data_write_ob``,\n``data_right_structure``,\\ ``job_id``,\\ ``time_zone``,\\ ``no_of_partition``\nand ``output_db_name``. You can also pass the ``run_engine`` with which\nyou want to apply the quality checks. There are two run_engines\navailable: - ``pyspark`` - ``polars``\n\nBy default, the run_engine is ``pyspark``.\n\n.. code:: sh\n\n   dq_ob =dq.SingleDatasetQualityCheck(tables_list={}, \n                                     interaction_between_tables=[],  \n                                     data_read_ob  = data_read_ob, \n                                     data_write_ob = data_write_ob, \n                                     data_right_structure = 'file',\n                                     job_id='blob_1',\n                                     time_zone=None,\n                                     output_db_name=\"data_quality_output\",\n                                     no_of_partition=4)\n\n**Step-4:** Passing a ``rules_diagnosys_summery_file_path`` and\n``config_df`` as an input and apply validations on various columns of\nrespective table defined in the ``config_df``.\n\n.. code:: sh\n\n    rules_diagnosys_summery_folder_path = \"abfss://%s@%s.dfs.core.windows.net/processed/data_quality/summary3/\" %(blob_connector_config[\"container_name\"], blob_connector_config[\"storage_account_name\"])\n\n   config_df = spark.read.option(\"header\",True).csv(\"dbfs:/FileStore/shared_uploads/vaishali.garg@decisionpoint.in/quality_checks_config.csv\")\n\n   dq_ob.apply_validation(config_df, write_summary_on_database = True, failed_schema_source_list = [], output_summary_folder_path = rules_diagnosys_summery_folder_path)\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "module-dataquality",
    "package_url": "https://pypi.org/project/module-dataquality/",
    "platform": null,
    "project_url": "https://pypi.org/project/module-dataquality/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/module-dataquality/1.0.3/",
    "requires_dist": [
      "polars"
    ],
    "requires_python": ">=3.8",
    "summary": "data profiling and basic data quality rules check",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17299622,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3f5129bb2dab5c6162ebb925e1197416ac28527a21d5c93f2f95519c6a2af44d",
        "md5": "1878c5a521f2657139d732537ddbd110",
        "sha256": "f313e7cffbe732c7b97859158978a83225951fd5d73bdb420bbd5a12e20095e2"
      },
      "downloads": -1,
      "filename": "module_dataquality-1.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1878c5a521f2657139d732537ddbd110",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 28697,
      "upload_time": "2023-03-13T14:01:28",
      "upload_time_iso_8601": "2023-03-13T14:01:28.061568Z",
      "url": "https://files.pythonhosted.org/packages/3f/51/29bb2dab5c6162ebb925e1197416ac28527a21d5c93f2f95519c6a2af44d/module_dataquality-1.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ee55858f0ab5cda3e6f9a8030e3b4299cd717f2765f2fcb0555f5083d2545af5",
        "md5": "9ae1b82ef0f7a454d1460d6332136e44",
        "sha256": "d321df6986b0f6bb26d9e7cf2751c16f039db12355ef995321770a7df6bf26d7"
      },
      "downloads": -1,
      "filename": "module_dataquality-1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "9ae1b82ef0f7a454d1460d6332136e44",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 23761,
      "upload_time": "2023-03-13T14:01:31",
      "upload_time_iso_8601": "2023-03-13T14:01:31.134282Z",
      "url": "https://files.pythonhosted.org/packages/ee/55/858f0ab5cda3e6f9a8030e3b4299cd717f2765f2fcb0555f5083d2545af5/module_dataquality-1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}