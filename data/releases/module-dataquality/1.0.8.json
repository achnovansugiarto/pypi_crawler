{
  "info": {
    "author": "Sweta",
    "author_email": "sweta.swami@decisionpoint.in",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "### dq-module\ndq-module is a tool which can be used to perform validations and profiling on the datasets.This tool is compatible with two run_engines `pyspark` and `polars`.\n### Features\n## 1. Data Validation \nThis library contains a `SingleDatasetQualityCheck()` class which can used to validate the dataset against a defined set of rules.\nThis class contains the following rules which can be used to validate the dataset: \n - null_check\n - schema_check\n - range_min_check\n - range_max_check\n - length_check\n - unique_records_check\n - unique_keys_check\n - allowed_values_check\n - min_length_check\n - column_count_check\n - not_allowed_values_check\n -  date_range_check\n - regex_check\n - row_count_check\n\n### How To Use\n\n```sh\nimport dataqualitycheck as dq\nfrom datetime import date\nimport time\n```\n##### Step 1: Adding a datasource\nWe have 4 classes which can be used to connect to a datasource:\n1. `AzureBlobDF()` - This class can be used to interact with the datasources on azure blob storage . \n2. `DatabricksFileSystemDF()` - This class can be used to interect with the datasources on  databricks filesystem.\n3. `DatabricksSqlDF()` - This class can be used to interact with the datasources on  databricks databases.\n4. `LocalFileSystemDF()` - This class can be used to interact with the datasources on your local filesystem.\n\nEach of the above class provides the functionalities to read and write from the respective datasources.\n\n\n**Example:**\nPass the configuration of the  blob connector  in `blob_connector_config`and \nadd a datasource by defining a `data_read_ob` and `data_write_ob`.\n```sh\nblob_connector_config = {\"storage_account_name\" : \"rgmdemostorage\", \"container_name\" : \"cooler-images\", \"sas_token\" : <vaild-sas-token>}\n```\n```sh\ndata_read_ob = dq.AzureBlobDF(storage_name=blob_connector_config[\"storage_account_name\"], sas_token=blob_connector_config[\"sas_token\"])\ndata_write_ob = dq.DatabricksFileSystemDF()\n```\n\n `tables_list` is a dictionary that contains the list of sources along with the container_name , source_type , layer , source_name , filename , read_connector_method and latest_file_path  for the tables on which the validations has to be applied . \n```sh\n#This is optional.It is required when you are calling individual rules.\ntables_list={}\n```\n##### Step 2: Add a DataContext \nInstantiate a DataContext by passing `tables_list`,`interaction_between_tables`,`data_read_ob`,`data_write_ob`, `data_right_structure`,`job_id`,`time_zone`,`no_of_partition` and `output_db_name `.\nYou can also pass the `run_engine` with which you want to apply the quality checks. By default, the run_engine is `pyspark`.\n\n```sh\ndq_ob = dq.SingleDatasetQualityCheck(tables_list={}, \n                                  interaction_between_tables=[],  \n                                  data_read_ob=data_read_ob, \n                                  data_write_ob=data_write_ob, \n                                  data_right_structure='file',\n                                  job_id=<pipeline_run_id>,\n                                  time_zone=None,\n            output_db_name=<database_name_where_report_has_to_be_written>,\n                                  no_of_partition=4)\n```\n\n##### Step 3:\nPassing a `rules_diagnosys_summery_file_path` and `config_df` as an input and apply validations on various columns of respective table defined in the `config_df`.\n\nHere is an sample of the `config_df`.\n| container_name   | source_type | layer     | source_name         | filename     | rule_name       | column_to_be_checked | value | date_column_config | date_format_dictionary | ruletype      | active | read_connector_method | latest_file_path                                                                                  | output_folder_structure | failed_schema_source_list |\n|------------------|-------------|-----------|---------------------|--------------|-----------------|----------------------|-------|--------------------|------------------------|---------------|--------|-----------------------|---------------------------------------------------------------------------------------------------|-------------------------|---------------------------|\n| rgm-data-quality | sharepoint  | processed | neilsen_data_folder | neilsen_data | null_check      | prod_brand           | null  | null               | null                   | Mandatory     | 1      | blob                  | path-to-file | processed/              |                           |\n| rgm-data-quality | sharepoint  | processed | neilsen_data_folder | neilsen_data | range_min_check | inventory_kg_avg     | 10    | null               | null                   | Not Mandatory | 1      | blob                  | path-to-file | processed/              |                           |\n| rgm-data-quality | sharepoint  | processed | neilsen_data_folder | neilsen_data | range_max_check | inventory_kg_avg     | 1000  | null               | null                   | Not Mandatory | 1      | blob                  | path-to-file | processed/              |                           |\n\n**Example:**\n```sh\nrules_diagnosys_summery_folder_path = <folder-path-for-the-output-report>\n \nconfig_df = spark.read.option(\"header\",True).csv(<path-to-the-config>)\n\ndq_ob.apply_validation(config_df, write_summary_on_database=True, failed_schema_source_list=[], output_summary_folder_path=rules_diagnosys_summery_folder_path)\n```\n## 2. Data Profiling\n- We can generate a detailed summary statistics such as mean, median, mode, list of uniques, missing count etc. of a dataset using the `DataProfile()` class.\n- This class can also be used to recommend some  data quality rules  based on the profiling report generated on the dataset.\n### How To Use\n\n**Step 1: Add a Datasource**\n```sh\ndata_write_ob = dq.DatabricksSqlDF()\ndata_write_ob = dq.DatabricksSqlDF()\n```\n**Step 2: Add a DataContext**\n```sh\nimport pytz\ntime_zone = pytz.timezone('US/Central')\ndq_ob = dq.DataProfile(tables_list=tables_list,\n                     interaction_between_tables=[],\n                     data_read_ob=data_read_ob,\n                     data_write_ob=data_write_ob,\n                     data_right_structure='table',\n                     job_id=<pipeline_run_id>,\n                     time_zone=time_zone,\n                     no_of_partition=4,\n            output_db_name=<database_name_where_report_has_to_be_written>,\n                     run_engine='polars')\n```\n**Step 3:**\nPass `config_df` as an input and apply data profiling on various columns of respective table defined in the `config_df`.\n``` sh\n# You can create a config_df in pyspark/polars run_engine directly also rather than reading as a csv.\nconfig_df = spark.createDataFrame([{\"container_name\" : \"rgm-data-quality\",\n                                 \"source_type\" : \"sharepoint\",\n                                 \"layer\" : \"raw\",\n                                 \"source_name\" : \"data_quality_db\",\n                                 \"filename\" : \"neilsen_data_parquet\",\n            \"latest_file_path\" : \"data_quality_db.neilsen_data_parquet\",  \n                            \"read_connector_method\" : \"databricks sql\",\n         \"output_folder_structure\" : \"processed/data_profiling_test/\"}])\n```\n```sh\n# Generating a data profiling report.\ndq_ob.apply_data_profiling(source_config_df=config_df,\n                           write_consolidated_report=True)\n```\n```sh\n# Generating a data profiling report as well as recommending the quality rules based on the profiling report.\nrules_config = dq_ob.data_profiling_based_quality_rules(config_df, list_of_columns_to_be_ignored)\n```\n\n## 3. Consistency Check\n You can check the consistency of common columns between two tables using the `ConsistencyCheck()` class.\n### How To Use\n\n**Step 1: Add a datasource.**\n```sh\ndata_read_ob = dq.DatabricksFileSystemDF()\ndata_write_ob = dq.AzureBlobDF(storage_name=blob_connector_config[\"storage_account_name\"], sas_token=blob_connector_config[\"sas_token\"])\n```\n**Step 2: Add a DataContext**\n```sh\ndq_ob = dq.ConsistencyCheck(tables_list={}, \n                   interaction_between_tables=[],\n                   data_read_ob=data_read_ob,\n                   data_write_ob=data_write_ob, \n                   data_right_structure='file',\n                   job_id=<pipeline_run_id>,\n                   time_zone=None,\n                   no_of_partition=4,\n         output_db_name=<database_name_where_report_has_to_be_written>)\n```\n**Step 3:**\n Pass `config_df` and `output_report_folder_path` as an input and apply consistency check.\nHere is a sample consistency check config.\n| container_name   | base_table_source_type | base_table_layer | base_table_source_name | base_table_filename    | base_table_col_name | base_table_file_path                                                                                                | mapped_table_source_type | mapped_table_layer | mapped_table_source_name | mapped_table_filename    | mapped_table_col_name | mapped_table_file_path                                                                                                  | read_connector_method | output_folder_structure          |\n|------------------|------------------------|------------------|------------------------|------------------------|---------------------|---------------------------------------------------------------------------------------------------------------------|--------------------------|--------------------|--------------------------|--------------------------|-----------------------|-------------------------------------------------------------------------------------------------------------------------|-----------------------|----------------------------------|\n| rgm-data-quality | sharepoint             | processed        | scantrack              | butters_margarins_base | BARCODE             | dbfs:/FileStore/Tables/sharepoint/scantrack/butters_margarins_base/2023/01/12/butters_margarins_base_01_12_2023.csv | sharepoint               | processed          | scantrack                | butters_margarins_master | BARCODE               | dbfs:/FileStore/Tables/sharepoint/scantrack/butters_margarins_master/2023/01/12/butters_margarins_master_01_12_2023.csv | dbfs                  | processed/data_consistenct_test/ |\n| rgm-data-quality | sharepoint             | processed        | scantrack              | cheese_base            | BARCODE             | dbfs:/FileStore/Tables/sharepoint/scantrack/cheese_base/2023/01/12/cheese_base_01_12_2023.csv                       | sharepoint               | processed          | scantrack                | cheese_master            | BARCODE               | dbfs:/FileStore/Tables/sharepoint/scantrack/cheese_master/2023/01/12/cheese_master_01_12_2023.csv                       | dbfs                  | processed/data_consistenct_test/ |\n\n\n```sh\nconfig_df = spark.read.option(\"header\",True).csv(<path-to-the-consistency-check-config>)\n\noutput_report_folder_path = <folder-path-for-the-output-report>\n\ndq_ob.apply_consistency_check(config_df, output_report_folder_path)\n\n```\n\n\n        \n                          \n                           \n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "module-dataquality",
    "package_url": "https://pypi.org/project/module-dataquality/",
    "platform": null,
    "project_url": "https://pypi.org/project/module-dataquality/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/module-dataquality/1.0.8/",
    "requires_dist": [
      "polars"
    ],
    "requires_python": ">=3.8",
    "summary": "data profiling and basic data quality rules check",
    "version": "1.0.8",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17299622,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4cd4f237a64c1d02081b0e97b4a8222b4b1a9b81a0b8e25449ee42abafcb1a21",
        "md5": "d89dc41508b809b5dfb7ac9669685703",
        "sha256": "a2b07ba16890159539141381289f9a2d9723bba64ee109b121f6cc5f203dea30"
      },
      "downloads": -1,
      "filename": "module_dataquality-1.0.8-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d89dc41508b809b5dfb7ac9669685703",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 5624,
      "upload_time": "2023-03-15T06:00:57",
      "upload_time_iso_8601": "2023-03-15T06:00:57.892732Z",
      "url": "https://files.pythonhosted.org/packages/4c/d4/f237a64c1d02081b0e97b4a8222b4b1a9b81a0b8e25449ee42abafcb1a21/module_dataquality-1.0.8-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "07e4603bdb8bf7234901891f59cc189145b04e5457dfd3b2680237532bcbcb3f",
        "md5": "8819a6080ff501f5bc4b5deb250b6049",
        "sha256": "1857e24ed240bc90284f19340b63ef0ce2637b13ed41d604a0d652cbadffee8f"
      },
      "downloads": -1,
      "filename": "module_dataquality-1.0.8.tar.gz",
      "has_sig": false,
      "md5_digest": "8819a6080ff501f5bc4b5deb250b6049",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 5387,
      "upload_time": "2023-03-15T06:01:00",
      "upload_time_iso_8601": "2023-03-15T06:01:00.898579Z",
      "url": "https://files.pythonhosted.org/packages/07/e4/603bdb8bf7234901891f59cc189145b04e5457dfd3b2680237532bcbcb3f/module_dataquality-1.0.8.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}