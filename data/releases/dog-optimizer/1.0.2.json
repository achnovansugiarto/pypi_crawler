{
  "info": {
    "author": "Maor Ivgi, Oliver Hinder, Yair Carmon",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# DoG Optimizer\n\nThis repository contains the implementation of the algorithms in the paper \n[DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule](https://arxiv.org/abs/2302.12022)\nby Maor Ivgi, Oliver Hinder and Yair Carmon.\n\n## Algorithm\nDoG (\"Distance over Gradients\") is a parameter-free stochastic optimizer. \nDoG updates parameters $x_t$ with stochastic gradients $g_t$ according to:\n```math\n\\begin{aligned}\n   \\eta_t & = \\frac{ \\bar{r}_t }{ \\sqrt{\\sum_{i \\le t }{\\lVert g_i\\rVert ^2 + \\epsilon}} } \\\\   \n   x_{t+1} & = x_{t} - \\eta_t \\cdot g_t\n  \\end{aligned}\n```\nwhere\n```math\n\\begin{equation*}\n\\bar{r}_t = \\begin{cases}\n\\text{max}_{i \\le t}{\\lVert x_i - x_0 \\rVert} & t \\ge 1 \\\\\nr_{\\epsilon} & t=0.\n\\end{cases}\n\\end{equation*}\n```\nThe initial movement parameter $r_{\\epsilon}$ should be chosen small relative to the distance between $x_0$ and the nearest optimum $x^\\star$ (see additional discussion below).\n\nLDoG (layerwise DoG) is a variant of DoG that applies the above update rule separately to every element in the list of parameters provided to the optimizer object.\n\n## Installation\nTo install the package, simply run `pip install dog-optimizer`.\n\n## Usage\nDoG and LDoG are implemented using the standard pytorch optimizer interface. After installing the pacakge with `pip install dog-optimizer`,\nAll you need to do is replace the line that creates your optimizer with \n```python\nfrom dog import DoG\noptimizer = DoG(optimizer args)\n```\nfor DoG, or\n```python\nfrom dog import LDoG\noptimizer = LDoG(optimizer args)\n```\nfor LDoG, \nwhere `optimizer args` follows the standard pytorch optimizer syntex. \nTo see the list of all available parameters, run `help(DoG)` or `help(LDoG)`.\n\n\nUsing the polynomial decay averager is also easy. Simply create it with \n```python\nfrom dog import PolynomialDecayAverager\naverager = PolynomialDecayAverager(model)\n```\nthen, after each `optimizer.step()`, call `averager.step()` as well.\nYou can then get both the current model and the averaged model with `averager.base_model` and `averager.averaged_model` respectively.\n\nAn example of how to use the above to train a simple CNN on MNIST can be found in `examples/mnist.py` \n(based on this [pytorch example](https://github.com/pytorch/examples/blob/main/mnist/main.py)).\n\n### Choosing `reps_rel`\nDoG is parameter-free by design, so there is no need to tune a learning rate parameter. \nHowever, as discussed in the paper, DoG has an initial step movement parameter \n$r_{\\epsilon}$ that must be small enough to avoid destructively updates that cause divergence, \nbut an extremely small value of $r_{\\epsilon}$ would slow down training. \nWe recommend choosing $r_{\\epsilon}$ relative to the norm of the initial weights $x_0$. In particular, we set \n$r_{\\epsilon}$ to be `reps_rel` $\\times (1+\\rVert x_0 \\lVert)$, where `reps_rel` is a configurable parameter of the optimizer. The default value \nof `reps_rel` is 1e-6, and we have found it to work well most of the time. However, in our experiments we did encounter \nsome situations that required different values of `reps_rel`:\n- If optimization diverges early, it is likely that `reps_rel` (and hence $r_{\\epsilon}$) is too large: \ntry decreasing it by factors 100 until divergence no longer occurs. This happened when applying LDoG to fine-tune T5, \nwhich had large pre-trained weights; setting `reps_rel` to 1e-8 eliminated the divergence.\n- If the DoG step size (`eta`) does not substantially increase from its initial value for a few hundred steps, it could be that `reps_rel` is too small: \ntry increasing it by factors of 100 until you see `eta` starting to increase in the first few steps. \nThis happened when training models with batch normalization; setting `reps_rel` to 1e-4 eliminated the problem.\n\n\n## Citation\n```\n@article{ivgi2023dog,\n  title={{D}o{G} is {SGD}'s Best Friend: A Parameter-Free Dynamic Step Size Schedule}, \n  author={Maor Ivgi and Oliver Hinder and Yair Carmon}, \n  journal={arXiv:2302.12022}, \n  year={2023},\n}  \n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dog-optimizer",
    "package_url": "https://pypi.org/project/dog-optimizer/",
    "platform": null,
    "project_url": "https://pypi.org/project/dog-optimizer/",
    "project_urls": {
      "Homepage": "https://github.com/formll/dog",
      "Paper": "https://arxiv.org/abs/2302.12022"
    },
    "release_url": "https://pypi.org/project/dog-optimizer/1.0.2/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "implementation of the algorithms in the paper DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17052261,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2300e84ed82a30bde144579f2ab5695e8b636e11595b2f96ea625e7248272d7f",
        "md5": "45530058790d4914b937348a9d96a349",
        "sha256": "977c24a2291858c8fdc51cd0d99a9bd2179d13d9d66783b8babd99b4e72591b2"
      },
      "downloads": -1,
      "filename": "dog_optimizer-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "45530058790d4914b937348a9d96a349",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 8214,
      "upload_time": "2023-02-26T18:47:43",
      "upload_time_iso_8601": "2023-02-26T18:47:43.217818Z",
      "url": "https://files.pythonhosted.org/packages/23/00/e84ed82a30bde144579f2ab5695e8b636e11595b2f96ea625e7248272d7f/dog_optimizer-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "654b74fb0c2ce916fedb8210f9b63508a1d0bac98ff7848da8df897f38c1b2ff",
        "md5": "192e233da7bfbec5823ac9a3743994db",
        "sha256": "a40e0c56fd8ea4c41ffe95f66d9f0aa9af6549d4f212e3f41adef8d6002e5d37"
      },
      "downloads": -1,
      "filename": "dog-optimizer-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "192e233da7bfbec5823ac9a3743994db",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 8971,
      "upload_time": "2023-02-26T18:47:46",
      "upload_time_iso_8601": "2023-02-26T18:47:46.827677Z",
      "url": "https://files.pythonhosted.org/packages/65/4b/74fb0c2ce916fedb8210f9b63508a1d0bac98ff7848da8df897f38c1b2ff/dog-optimizer-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}