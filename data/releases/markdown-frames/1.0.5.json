{
  "info": {
    "author": "Exacaster",
    "author_email": "support@exacaster.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Software Development :: Testing"
    ],
    "description": "# Markdown Frames\n\nHelper package for testing Apache Spark and Pandas DataFrames.\nIt makes your data-related unit tests more readable.\n\n## History\n\nWhile working at [Exacaster](https://exacaster.com/) [Vaidas Armonas](https://github.com/Va1da2) came up with the idea to make testing data more representable. And with the help of his team, he implemented the initial version of this package.\n\nBefore that, we had to define our testing data as follows:\n```python\nschema = [\"user_id\", \"even_type\", \"item_id\", \"event_time\", \"country\", \"dt\"]\ninput_df = spark.createDataFrame([\n    (123456, 'page_view', None, datetime(2017,12,31,23,50,50), \"uk\", \"2017-12-31\"),\n    (123456, 'item_view', 68471513, datetime(2017,12,31,23,50,55), \"uk\", \"2017-12-31\")], \n    schema)\n```\n\nAnd with this library you can define same data like this:\n```python\ninput_data = \"\"\" \n    |  user_id   |  even_type  | item_id  |    event_time       | country  |     dt      |\n    |   bigint   |   string    |  bigint  |    timestamp        |  string  |   string    |\n    | ---------- | ----------- | -------- | ------------------- | -------- | ----------- |\n    |   123456   |  page_view  |   None   | 2017-12-31 23:50:50 |   uk     | 2017-12-31  |\n    |   123456   |  item_view  | 68471513 | 2017-12-31 23:50:55 |   uk     | 2017-12-31  |\n\"\"\"\ninput_df = spark_df(input_data, spark)\n```\n\n## Installation\nTo install this package, run this command on your python environment:\n```bash\npip install markdown_frames[pyspark]\n```\n\n## Usage\n\nWhen you have this package installed, you can use it in your unit tests as follows (assuming you are using `pytest-spark` ang have Spark Session available):\n\n```python\nfrom pyspark.sql import SparkSession\nfrom markdown_frames.spark_dataframe import spark_df\n\ndef test_your_use_case(spark: SpakSession): -> None\n    expected_data = \"\"\"\n        | column1 | column2 | column3 | column4 |\n        |   int   |  string |  float  |  bigint |\n        | ------- | ------- | ------- | ------- |\n        |   1     |   user1 |   3.14  |  111111 |\n        |   2     |   None  |   1.618 |  222222 |\n        |   3     |   ''    |   2.718 |  333333 |\n        \"\"\"\n    expected_df = spark_df(expected_data, spark)\n\n    actaual_df = your_use_case(spark)\n\n    assert expected_df.collect()) == actaual_df.collect())\n```\n\n## Supported data types\n\nThis package supports all major datatypes, use these type names in your table definitions:\n- `int`\n- `bigint`\n- `float`\n- `double`\n- `string`\n- `boolean`\n- `timestamp`\n- `decimal(precision,scale)` (scale and precision must be integers)\n- `array<int>` (int can be replaced by  any of mentioned types)\n- `map<string,int>` (string and int can be replaced by any of mentioned types)\n\nFor `null` values use `None` keyword.\n\n## License\n\nThis project is [MIT](./LICENSE) licensed.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/exacaster/markdown_frames",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "markdown-frames",
    "package_url": "https://pypi.org/project/markdown-frames/",
    "platform": "",
    "project_url": "https://pypi.org/project/markdown-frames/",
    "project_urls": {
      "Homepage": "https://github.com/exacaster/markdown_frames"
    },
    "release_url": "https://pypi.org/project/markdown-frames/1.0.5/",
    "requires_dist": [
      "pandas ; extra == 'pandas'",
      "pyspark ; extra == 'pyspark'"
    ],
    "requires_python": "",
    "summary": "Markdown tables parsing to pyspark / pandas DataFrames",
    "version": "1.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15067536,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "894244208a2f10c0bb2768b606202d6b67ee4dd2ea1da6c05798002f0bfb3044",
        "md5": "5e9f05a0e0d53b3beaadd653cc7c468a",
        "sha256": "4fcad5d08a190a13e2835e4cab2d9134fa37e87a9a9aba8ffdbe957d5efe4f9e"
      },
      "downloads": -1,
      "filename": "markdown_frames-1.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "5e9f05a0e0d53b3beaadd653cc7c468a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 8336,
      "upload_time": "2022-02-04T17:49:41",
      "upload_time_iso_8601": "2022-02-04T17:49:41.940546Z",
      "url": "https://files.pythonhosted.org/packages/89/42/44208a2f10c0bb2768b606202d6b67ee4dd2ea1da6c05798002f0bfb3044/markdown_frames-1.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9cca3e2cd670f811311333ad7ad1fb5d05e349b2bc2ab2e04beb63d7bbcb5377",
        "md5": "93ee6771754346f8cb7fad7acac3ddcf",
        "sha256": "931303188e5b1111074ec8240dd014370af13d69936ce1df0aae8954218b8e3a"
      },
      "downloads": -1,
      "filename": "markdown_frames-1.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "93ee6771754346f8cb7fad7acac3ddcf",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 6294,
      "upload_time": "2022-02-04T17:49:43",
      "upload_time_iso_8601": "2022-02-04T17:49:43.125945Z",
      "url": "https://files.pythonhosted.org/packages/9c/ca/3e2cd670f811311333ad7ad1fb5d05e349b2bc2ab2e04beb63d7bbcb5377/markdown_frames-1.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}