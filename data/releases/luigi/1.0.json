{
  "info": {
    "author": "Erik Bernhardsson",
    "author_email": "erikbern@spotify.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "Luigi is a Python package that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization, handling failures, command line integration, and much more.\n\nThe purpose of Luigi is to address all the plumbing typically associated with long-running batch processes. You want to chain many tasks, automate them, and failures *will* happen. These tasks can be anything, but typically long running things like [Hadoop](http://hadoop.apache.org/) jobs, dumping data to/from databases, running machine learning algorithms, or anything else. \n\nThere are other software packages that focus on lower level aspects of data processing, like [Hive](http://hive.apache.org/), [Pig](http://pig.apache.org/), [Cascading](http://www.cascading.org/). Luigi is not a framework to replace these. Instead it helps you stitch many tasks together into long-running pipelines that can comprise thousands of tasks and take weeks to complete. Luigi takes care of a lot of the workflow management so that you can focus on the concrete tasks and their dependencies.\n\nLuigi comes with a toolbox of several common tasks that you use. It includes native Python support for running mapreduce jobs in Hadoop. It also comes with file system abstractions for HDFS and local files that also ensures all file system operations are atomic. This is important because it means your data pipeline will not crash in a state containing partial data.\n\n## Dependency graph example\n\nJust to give you an idea of what Luigi does, this is a screen shot from something we are running in production. Using Luigi's visualizer, we get a nice visual overview of the dependency graph of the workflow. At the top of the graph are two data sets containing external data dumps. Each node represents a task which has to be run. Green tasks are already completed whereas white tasks are yet to be run. Most of these tasks are Hadoop job, but at the end of the graph is a task ingesting data from HDFS into Cassandra.\n\n\n## Background\n\nWe use Luigi internally at [Spotify](http://www.spotify.com/) to run 1000s of tasks every day, organized in complex dependency graphs. Most of these tasks are Hadoop job. Luigi provides an infrastructure that powers all kinds of stuff including recommendations, toplists, A/B test analysis, external reports, internal dashboards, etc. Luigi grew out of the realization that powerful abstractions for the batch processing can help programmers focus on the most important bits and leave the rest (the boilerplate) to the framework.\n\nConceptually, Luigi similar to [GNU Make](http://www.gnu.org/software/make/) where you have certain tasks and these tasks in turn may have dependencies on other tasks. There are also some similarities to [Oozie](http://incubator.apache.org/oozie/) and [Azkaban](http://data.linkedin.com/opensource/azkaban). One major difference is that Luigi is not just built specifically for Hadoop, and it's easy to extend it with other kinds of tasks.\n\nEverything in Luigi is in Python. Instead of XML configuration or similar external data files, the dependency graph is specified *within Python*. This makes it easy to build up complex dependency graphs of tasks, where the dependencies can involve date algebra or recursive references to other versions of the same task.\n\n## Installing\n\nDownloading and running *python setup.py install* should be enough. Note that you probably want [Tornado](http://www.tornadoweb.org/) and [Pygraphviz](http://networkx.lanl.gov/pygraphviz/). Also [Mechanize](http://wwwsearch.sourceforge.net/mechanize/) is optional if you want to run Hadoop jobs since it makes debugging easier. If you want to run Hadoop you should also make sure to edit /etc/luigi/client.cfg and add something like this\n\n    [hadoop]\n    jar: /usr/lib/hadoop-xyz/hadoop-streaming-xyz-123.jar\n\n## Examples\n\nLet's begin with the classic WordCount example. We'll show a non-Hadoop version then later show how it can be implemented as a Hadoop job. The examples are all available in the examples/ directory.\n\n### Example 1 - Simple wordcount\n\nAssume you have a bunch of text files dumped onto disk every night by some external process. These text files contain English text, and we want to monitor the top few thousand words over arbitrary date periods. Hopefully this doesn't sound too contrived - you could imagine mapping \"words\" to \"artists\" to get an idea of a real application at Spotify.\n\nimport luigi\nimport datetime\n\nclass InputText(luigi.ExternalTask):\n    ''' This class represents something that was created elsewhere by an external process, so all we want to do is to implement the output method\n    '''\n    date = luigi.DateParameter()\n\n    def output(self):\n        return luigi.LocalTarget(self.date.strftime('/var/tmp/text/%Y-%m-%d.txt'))\n\nclass WordCount(luigi.Task):\n    date_interval = luigi.DateIntervalParameter()\n\n    def requires(self):\n        return [InputText(date) for date in self.date_interval.dates()]\n\n    def output(self):\n        return luigi.LocalTarget('/var/tmp/text-count/%s' % (self.date_interval))\n\n    def run(self):\n        count = {}\n        for file in self.input(): # The input() method is a wrapper around requires() that returns Target objects\n            for line in file.open('r'): # Target objects are a file system/format abstraction and this will return a file stream object\n                for word in line.strip().split():\n                    count[word] = count.get(word, 0) + 1\n\n        # output data\n        f = self.output().open('w')\n        for word, count in count.iteritems():\n            f.write(\"%s\\t%d\\n\" % (word, count))\n        f.close() # Note that this is essential because file system operations are atomic\n\nif __name__ == '__main__':\n    luigi.run()\n\nNow, provided you have a bunch of input files in /var/tmp/text/ (you can generate them using examples/generate\\_input.py), try running this using eg\n\n    $ python wordcount.py WordCount --local-scheduler --date 2012-08-01\n\nYou can also try to view the manual using --help which will give you an overview of the options:\n\n    usage: wordcount.py [-h] [--local-scheduler] [--scheduler-host SCHEDULER_HOST]\n                        [--lock] [--lock-pid-dir LOCK_PID_DIR] [--workers WORKERS]\n                        [--date-interval DATE_INTERVAL]\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      --local-scheduler     Use local scheduling\n      --scheduler-host SCHEDULER_HOST\n                            Hostname of machine running remote scheduler [default:\n                            localhost]\n      --lock                Do not run if the task is already running\n      --lock-pid-dir LOCK_PID_DIR\n                            Directory to store the pid file [default:\n                            /var/tmp/luigi]\n      --workers WORKERS     Maximum number of parallel tasks to run [default: 1]\n      --date-interval DATE_INTERVAL\n                            WordCount.date_interval\n\nRunning the command again will do nothing because the output file is already created. Note that unlike Makefile, the output will not be recreated when any of the input files is modified. You need to delete the output file manually.\n\n### Using the central planner\n\nThe --local-scheduler flag tells Luigi not to connect to a central scheduler. This is recommended in order to get started and or for development purposes. At the point where you start putting things in production we strongly recommend running the central scheduler server. In addition to provide locking so the same task is not run by multiple processes at the same time, this server also provides a pretty nice visualization of your current work flow.\n\nIf you drop the *--local-scheduler* flag, your script will try to connect to the central planner, by default at localhost port 8081. If you run\n\n    PYTHONPATH=. python bin/luigid\n\nin the background and then run\n\n    $ python wordcount.py --date 2012-W03\n\nthen in fact your script will now do the scheduling through a centralized server. You need [Tornado](http://www.tornadoweb.org/) and [Pygraphviz](http://networkx.lanl.gov/pygraphviz/) for this to work. These are available in Debian as *python-tornado* and *python-pygraphviz*, respectively.\n\nLaunching *http://localhost:8081* should show something like this:\n\n\nThe green boxes mean that the job is already done. If you keep invoking the script with a bunch of different date intervals it might look like this after a while:\n\n\nYou can drag and scroll to re-center and zoom. The visualizer will automatically prune all done tasks after a while.\n\n### Example 2 - Hadoop WordCount\n\nLuigi also provides support for Hadoop jobs straight out of the box. The interface is similar to mrjob but each job class is now a Luigi Task that can also define their dependencies and output files.\n\nEC2 is unfortunately not supported at this point. We have some old code for this (using Python [boto](http://github.com/boto/boto)) and would love to help anyone interested in getting it running.\n\nimport luigi, luigi.hadoop, luigi.hdfs\nimport datetime\n\n# To make this run, you probably want to edit /etc/luigi/client.cfg and add something like:\n#\n# [hadoop]\n# jar: /usr/lib/hadoop-xyz/hadoop-streaming-xyz-123.jar\n\nclass InputText(luigi.ExternalTask):\n    date = luigi.DateParameter()\n    def output(self):\n        return luigi.hdfs.HdfsTarget(self.date.strftime('/tmp/text/%Y-%m-%d.txt'))\n\nclass WordCount(luigi.hadoop.JobTask):\n    date_interval = luigi.DateIntervalParameter()\n\n    def requires(self):\n        return [InputText(date) for date in self.date_interval.dates()]\n\n    def output(self):\n        return luigi.hdfs.HdfsTarget('/tmp/text-count/%s' % self.date_interval)\n\n    def mapper(self, line):\n        for word in line.strip().split():\n            yield word, 1\n\n    def reducer(self, key, values):\n        yield key, sum(values)\n\nif __name__ == '__main__':\n    luigi.run()\n\nLuigi also has support for combiners and counters. If you want to bundle modules with your job, you can use that either by overriding the *extra_packages* method, or by invoking the *luigi.hadoop.attach* function anywhere in your code.\n\nRun the example using\n\n    $ python wordcount_hadoop.py WordCount --date 2012-W03\n\nThis will yield a familiar overview\n\n\nThe blue box means that the job is currently running. If it fails, it will become red:\n\n\nIn case your job crashes remotely due to any Python exception, Luigi will try to fetch the traceback and print it on standard output. You need [Mechanize](http://wwwsearch.sourceforge.net/mechanize/) for it to work and you also need connectivity to your tasktrackers.\n\n## Conceptual overview\n\nThere are two fundamental building blocks of Luigi - the *Task* class and the *Target* class. Both are abstract classes and expect a few methods to be implemented. In addition to those two concepts, the *Parameter* class is an important concept that governs how a Task is run.\n\n### Target\n\nBroadly speaking, the Target class corresponds to a file on a disk. Or a file on HDFS. Or some kind of a checkpoint, like an entry in a database. Actually, the only method that Targets have to implement is the *exists* method which returns True if and only if the Target exists.\n\nIn practice, implementing Target subclasses is rarely needed. You can probably get pretty far with the *LocalTarget* and *hdfs.HdfsTarget* classes that are available out of the box. These directly map to a file on the local drive, or a file in HDFS, respectively. In addition these also wrap the underlying operations to make them atomic. They both implement the *open(flag)* method which returns a stream object that could be read (flag = 'r') from or written to (flag = 'w'). Both LocalTarget and hdfs.HdfsTarget also optionally take a format parameter. Luigi comes with Gzip support by providing *format=format.Gzip* . Adding support for other formats is pretty simple.\n\n### Task\n\nThe *Task* class is a bit more conceptually interesting because this is where computation is done. There is a few methods that can be implemented to alter its behavior, most notably *run*, *output* and *requires*.\n\nThe Task class corresponds to some type of job that is run, but in general you want to allow some form of parametrization of it. For instance, if your Task class runs a Hadoop job to create a report every night, you probably want to make the date a parameter of the class.\n\n#### Parameter\n\nNow, in Python this is generally done by adding arguments to the constructor. Luigi requires you to declare these parameters instantiating Parameter objects on the class scope:\n\nclass DailyReport(luigi.hadoop.JobTask):\n    date = luigi.DateParameter(default=datetime.date.today())\n    # ...\n\nBy doing this, Luigi can do take care of all the boiler plate code that would normally be needed in the constructor. Internally, the DailyReport object can now be constructed by running *DailyReport(datetime.date(2012, 5, 10))* or just *DailyReport()*. Luigi also creates a command line parser that automatically handles the conversion from strings to Python types. This way you can invoke the job on the command line eg. by passing *--date 2012-15-10*.\n\nThe parameters are all set to their values on the Task object instance, i.e.\n\nd = DailyReport(datetime.date(2012, 5, 10))\nprint d.date\n\nwill return the same date that the object was constructed with. Same goes if you invoke Luigi on the command line.\n\nPython is not a typed language and you don't have to specify the types of any of your parameters. You can simply use *luigi.Parameter* if you don't care. In fact, the reason DateParameter et al exist is just in order to support command line interaction and make sure to convert the input to the corresponding type (i.e. datetime.date instead of a string).\n\n#### Task.requires\n\nThe *requires* method is used to specify dependencies on other Task object, which might even be of the same class. For instance, an example implementation could be\n\ndef requires(self):\n    return OtherTask(self.date), DailyReport(self.date - datetime.timedelta(1))\n\nIn this case, the DailyReport task depends on two inputs created earlier, one of which is the same class. requires can return other Tasks in any way wrapped up within dicts/lists/tuples etc\n\n#### Task.output\n\nThe *output* method returns one or more Target objects. Similarly to requires, can return wrap them up in any way that's convenient for you. However we strongly recommend that any Task only returns one single Target in output.\n\nclass DailyReport(luigi.Task):\n    date = luigi.DateParameter()\n    def output(self):\n        return luigi.hdfs.HdfsTarget(self.date.strftime('/reports/%Y-%m-%d'))\n    # ...\n\n\n#### Task.run\n\nThe *run* method now contains the actual code that is run. Note that Luigi breaks down everything into two stages. First it figures out all dependencies between tasks, then it runs everything. The *input()* method is an internal helper method that just replaces all Task objects in requires with their corresponding output. For instance, in this example\n\nclass TaskA(luigi.Task):\n    def output(self):\n        return luigi.LocalTarget('xyz')\n\nclass FlipLinesBackwards(luigi.Task):\n    def requires(self):\n        return TaskA()\n\n    def output(self):\n        return luigi.LocalTarget('abc')\n\n    def run(self):\n        f = self.input().open('r') # this will return a file stream that reads from \"xyz\"\n        g = self.output().open('w')\n        for line in f:\n            g.write('%s\\n', ''.join(reversed(line.strip().split()))\n        g.close() # needed because files are atomic\n\n#### Running from the command line\n\nAny task can be instantiated and run from the command line\n\nclass MyTask(luigi.Task):\n    x = IntParameter()\n    y = IntParameter(default=45)\n    def run(self):\n        print self.x + self.y\n\nif __name__ == '__main__':\n       luigi.run()\n\nYou can run this task from the command line like this:\n\n    python my_task.py MyTask --x 123 --y 456\n\nYou can also pass *main_task_cls=MyTask* to luigi.run() and that way you can invoke it simply using\n\n    python my_task.py --x 123 --y 456\n\n#### Executing a Luigi workflow\n\nAs seen above, command line integration is achieved by simply adding\n\nif __name__ == '__main__':\n    luigi.run()\n\nThis will read the args from the command line (using argparse) and invoke everything.\n\nIn case you just want to run a Luigi chain from a Python script, you can do that internally without the command line integration. The code will look something like\n\ntask = MyTask(123, 'xyz')\nsch = scheduler.CentralPlannerScheduler()\nw = worker.Worker(scheduler=sch)\nw.add(task)\nw.run()\n\n#### Instance caching\n\nIn addition to the stuff mentioned above, Luigi also does some metaclass logic so that if eg. *DailyReport(datetime.date(2012, 5, 10))* is instantiated twice in the code, it will in fact result in the same object. This is needed so that each Task is run only once.\n\n#### But I just want to run a Hadoop job?\n\nThe Hadoop code is integrated in the rest of the Luigi code because we really believe almost all Hadoop jobs benefit from being part of some sort of workflow. However, in theory, nothing stops you from using the hadoop.JobTask class (and also hdfs.HdfsTarget) without using the rest of Luigi. You can simply run it manually using\n\nMyJobTask('abc', 123).run()\n\nYou can use the hdfs.HdfsTarget class anywhere by just instantiating it:\n\nt = luigi.hdfs.HdfsTarget('/tmp/test.gz', format=format.Gzip)\nf = t.open('w')\n# ...\nf.close() # needed\n\n## More graph porn\n\nRunning the *test/fib_test.py* with *--n 200* yields a complex graph (albeit slightly artificial):\n\n\nActually the resemblance with a G-clef is coincidental. Scroll and drag to zoom in:\n\n\n## More info\n\nLuigi is the sucessor to a couple of attempts that we weren't fully happy with. We learned a lot from our mistakes and some design decisions include:\n\n* Straightforward command line integration.\n* As little boiler plate as possible.\n* Focus on job scheduling and dependency resolution, not a particular platform. In particular this means no limitation to Hadoop. Though Hadoop/HDFS support is built-in and is easy to use, this is just one of many types of things you can run.\n* A file system abstraction where code doesn't have to care about where files are located.\n* Atomic file system operations through this abstraction. If a task crashes it won't lead to a broken state.\n* The depencies are decentralized. No big config file in XML. Each task just specifies which inputs it needs and cross-module dependencies are trivial.\n* A web server that renders the dependency graph and does locking etc for free.\n* Trivial to extend with new file systems, file formats and job types. You can easily write jobs that inserts a Tokyo Cabinet into Cassandra. Adding broad support S3, MySQL or Hive should be a stroll in the park. (and feel free to send us a patch when you're done!)\n* Date algebra included.\n* Lots of unit tests of the most basic stuff\n\nIt wouldn't be fair not to mention some limitations with the current design:\n\n* Its focus is on batch processing so it's probably less useful for near real-time pipelines or continuously running processes.\n* The assumption is that a each task is a sizable chunk of work. While you can probably schedule a few thousand jobs, it's not meant to scale beyond tens of thousands.\n* Luigi maintains a strict separation between scheduling tasks and running them. Dynamic for-loops and branches are non-trivial to implement. For instance, it's tricky to iterate a numerical computation task until it converges.\n\nIt should actually be noted that all these limitations are not fundamental in any way. However, it would take some major refactoring work.\n\nAlso it should be mentioned that Luigi is named after the pipeline-running friend of Super Mario.\n\n## Future ideas\n\n* S3/EC2 - We have some old ugly code based on Boto that could be integrated in a day or two.\n* Built in support for Pig/Hive.\n* Better visualization tool - the layout gets pretty messy as the number of tasks grows.\n* Integration with existing Hadoop frameworks like mrjob would be cool and probably pretty easy.\n* Better support (without much boiler plate) for unittesting specific Tasks\n\n## Want to contribute?\n\nAwesome! Let us know if you have any ideas. Feel free to contact x@y.com where x = luigi and y = spotify.",
    "description_content_type": null,
    "docs_url": null,
    "download_url": "UNKNOWN",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/spotify/luigi",
    "keywords": null,
    "license": "UNKNOWN",
    "maintainer": null,
    "maintainer_email": null,
    "name": "luigi",
    "package_url": "https://pypi.org/project/luigi/",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/luigi/",
    "project_urls": {
      "Download": "UNKNOWN",
      "Homepage": "https://github.com/spotify/luigi"
    },
    "release_url": "https://pypi.org/project/luigi/1.0/",
    "requires_dist": null,
    "requires_python": null,
    "summary": "Workflow mgmgt + task scheduling + dependency resolution",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17285056,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fde9cf3edb46c2be2d5f6e1cb345af929e5240473c310a1a004d42aa1c69e718",
        "md5": "eef9f05d7b9930943c6f631253a90cf6",
        "sha256": "9aa059aed1db008ab1917f0d006ab356d8e677b1d846ee2a2b451c9f68391e44"
      },
      "downloads": -1,
      "filename": "luigi-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "eef9f05d7b9930943c6f631253a90cf6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 64636,
      "upload_time": "2012-10-21T18:51:38",
      "upload_time_iso_8601": "2012-10-21T18:51:38.209973Z",
      "url": "https://files.pythonhosted.org/packages/fd/e9/cf3edb46c2be2d5f6e1cb345af929e5240473c310a1a004d42aa1c69e718/luigi-1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": [
    {
      "aliases": [
        "CVE-2018-1000843",
        "GHSA-p69g-f978-xxv9"
      ],
      "details": "Luigi version prior to version 2.8.0; after commit 53b52e12745075a8acc016d33945d9d6a7a6aaeb; after GitHub PR spotify/luigi/pull/1870 contains a Cross ite Request Forgery (CSRF) vulnerability in API endpoint: /api/<method> that can result in Task metadata such as task name, id, parameter, etc. will be leaked to unauthorized users. This attack appear to be exploitable via The victim must visit a specially crafted webpage from the network where their Luigi server is accessible.. This vulnerability appears to have been fixed in 2.8.0 and later.",
      "fixed_in": [
        "2.8.0"
      ],
      "id": "PYSEC-2018-11",
      "link": "https://osv.dev/vulnerability/PYSEC-2018-11",
      "source": "osv",
      "summary": null,
      "withdrawn": null
    },
    {
      "aliases": [
        "CVE-2018-1000843"
      ],
      "details": "Luigi version prior to version 2.8.0; after commit 53b52e12745075a8acc016d33945d9d6a7a6aaeb; after GitHub PR spotify/luigi/pull/1870 contains a Cross ite Request Forgery (CSRF) vulnerability in API endpoint: /api/<method> that can result in Task metadata such as task name, id, parameter, etc. will be leaked to unauthorized users. This attack appear to be exploitable via The victim must visit a specially crafted webpage from the network where their Luigi server is accessible.. This vulnerability appears to have been fixed in 2.8.0 and later.",
      "fixed_in": [
        "2.8.0"
      ],
      "id": "GHSA-p69g-f978-xxv9",
      "link": "https://osv.dev/vulnerability/GHSA-p69g-f978-xxv9",
      "source": "osv",
      "summary": null,
      "withdrawn": null
    }
  ]
}