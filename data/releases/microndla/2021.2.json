{
  "info": {
    "author": "Micron",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Micron DLA SDK\n\nMicron DLA Software Developement Kit - SDK\n\n# Micron DLA set-up steps\n\n1. Obtain necessary hardware: This SDK supposes that you are working on a desktop computer with Micron FPGA boards. For example: AC-511 and EX-750.\n2. Install pico-computing tools and Micron DLA SDK. Check section [1.](#1-installation)\n3. Run a sample example. Check sections [3.](#3-getting-started-inference-on-microndla-hardware) and [4.](#4-getting-started-inference-on-microndla-hardware-with-c)\n4. Create your own application\n\nThis document provides tutorials and general information about the Micron DLA SDK.\n\nThis SDK folder contains:\n[**Docker**](docker/): Docker files to create a docker image.\n\n[**Docs**](docs/): Documentation.\n* [Python API](docs/PythonAPI.md): Documentation of the python API can be found in docs/PythonAPI.md.\n* [C API](docs/C%20API.md): Documentation of the C/C++ API can be found in docs/C API.md.\n\n[**Examples**](examples/): Example code and tests.\n* [c_api](examples/c_api): Example how to use the C API\n* [pre_trained_models](examples/pre_trained_models): Examples using pre-trained models\n* [python_api](examples/python_api): Example how to use the python API\n* [tests](examples/tests): Samples to test neural network layers\n* [website](examples/website): Example for making a web application with MDLA and Flask\n\n[**Pytorch-torchscript**](torch_mdla/README.md): Tutorial on how to add Micro DLA into pytorch using torchscript.\n\n[**Test-files**](test-files/): Files used for the examples and tutorials.\n\n## Table of Contents:\n\n- [1. Installation](#1-installation) : install SDK\n  * [System requirements](#system-requirements)\n  * [Pico computing](#pico-computing)\n  * [Docker Image](#docker-image)\n  * [Python package Install](#python-package-install)\n- [2. Getting started with Deep Learning](#2-getting-started-with-deep-learning) : general information about deep learning\n  * [Introduction](#introduction)\n  * [PyTorch: Deep Learning framework](#pytorch-deep-learning-framework)\n  * [My dataset](#my-dataset)\n  * [Training a neural network with PyTorch](#training-a-neural-network-with-pytorch)\n  * [After training a neural network](#after-training-a-neural-network)\n- [3. Getting started Inference on Micron DLA hardware](#3-getting-started-inference-on-micron-dla-hardware) : getting started tutorial for running inference on the DLA\n- [4. Getting started Inference on Micron DLA hardware with C](#4-getting-started-inference-on-micron-dla-hardware-with-c) : getting started tutorial for running inference on the DLA using C\n- [5. Tutorial - Multiple FPGAs and Clusters](#5-tutorial---multiple-fpgas-and-clusters) : tutorial for running inference on multiple FPGAs and clusters\n  * [Multiple FPGAs with input batching <a name=\"one\"></a>](#multiple-fpgas-with-input-batching)\n  * [Multiple FPGAs with different models <a name=\"two\"></a>](#multiple-fpgas-with-different-models)\n  * [Multiple Clusters with input batching <a name=\"three\"></a>](#multiple-clusters-with-input-batching)\n  * [Multiple Clusters without input batching <a name=\"four\"></a>](#multiple-clusters-without-input-batching)\n  * [Multiple Clusters with different models <a name=\"five\"></a>](#multiple-clusters-with-different-models)\n  * [All Clusters with different models in sequence <a name=\"six\"></a>](#all-clusters-with-different-models-in-sequence)\n  * [Multiple Clusters with even bigger batches <a name=\"seven\"></a>](#multiple-clusters-with-even-bigger-batches)\n  * [Batching using MVs <a name=\"four\"></a>](#batching-using-mvs)\n- [6. Tutorial - PutInput and GetResult](#6-tutorial---putinput-and-getresult) : tutorial for using PutInput and GetOutput\n- [7. Tutorial - Writing tests](#7-tutorial---writing-tests) : Tutorial on running tests\n- [8. Tutorial - Debugging](#8-tutorial---debugging) : Tutorial on debugging and printing\n- [9. Variable Fix Point Quantization](#9-variable-fix-point-quantization) : Tutorial on using variable fix-point\n- [10. Running a model from your favorite deep learning framework](#10-running-a-model-from-your-favorite-deep-learning-framework) : Tutorial on converting models to ONNX\n  * [Tensorflow](#tensorflow)\n  * [Caffe1](#caffe1)\n  * [Keras](#keras)\n- [11. Supported models and layers](#11-supported-models-and-layers) : List of supported layers and models tested on the DLA\n  * [Tested models](#tested-models)\n  * [TF-Slim models tested on Micron DLA inference engine](#tf-slim-models-tested-on-microndla-inference-engine)\n  * [ONNX model zoo](#onnx-model-zoo)\n  * [Keras](#keras)\n  * [CNTK](#cntk)\n- [12. Troubleshooting and Q&A](#12-troubleshooting-and-qa) : Troubleshooting common issues and answering common questions\n\n\n# 1. Installation\n\nRequest pico-computing installer and docker images in [here](https://picocomputing.zendesk.com/hc/en-us). We will reply your request with the files you need for installation.\n\nThen follow [pico-computing](#pico-computing) installation\n\n## System requirements\n\nThis SDK supposes that you are working on a desktop computer with Micron FPGA boards.\n\nCheck out this [link](https://www.micron.com/products/advanced-solutions/advanced-computing-solutions) to see what Micron FPGA system you have.\n\nTested on:\n- Ubuntu 18.04 LTS Release, Kernel 4.15.0-39-generic\n- CentOS 7.5\n\nRequirements:\n- [Pico-computing tools](https://picocomputing.zendesk.com/hc/en-us/): Current version: pico-computing-2020.1. Please verify pico-computing functionality by refering to the document \"PicoUsersGuide.pdf\" and section \"Running a Sample Program\"\n- [protobuf 3.6.1](https://github.com/google/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz)\n- GCC 4.9 or higher\n- Python 3\n- numpy\n- Pillow\n- onnx\n- torch\n\n## Pico computing\n\nPico-computing installer package can be requested from this [link](https://picocomputing.zendesk.com/hc/en-us/). To make sure your FPGA system is working properly, install pico-computing tools.\n\nIt is highly recommended to read the [UsersGuide](https://picocomputing.zendesk.com/hc/en-us/) to learn about your FPGA system as this SDK uses pico-computing tools.\n\nPico computing installer will install the SDK together with pico-computing. And `libmicrondla.so` library should be present in `/usr/local/lib/`.\n\nIf you have a previous version of pico-computing installed then uninstall it. And remove all `picocomputing-2020.1` and `HMC_release` folders before installing a new version of pico-computing.\n\nAfter installation, reboot the machine.\n\nTo check if pico-computing tools installation was successful, you can run a sample program in `/usr/src/picocomputing-2020.1/samples`.\n\nIf there is an issue going through this section, a quick check is to run the following commands. It should print the following outputs for AC511 system.\n```\nlspci | grep -i pico\n    05:00.0 Memory controller: Pico Computing Device 0045 (rev 05)\n    08:00.0 Memory controller: Pico Computing Device 0511 (rev 05)\nlsmod | grep -i pico\n    pico                 3493888  12\n```\nAfter installing pico-computing, run install.sh to install the MDLA SDK\n\n\n## Docker Image \n\nThis step is optinal if you want to run as a docker image\n\nIf you want to use MDLA with docker, then you need to install [pico-computing](#pico-computing) and [docker](https://docs.docker.com/get-docker/).\n\nTo start a docker with the Micron DLA SDK you can either download prebuilt images or build them yourself. The benefit of custom building the image is that you can adjust the OS packages you want installed at build time. The trade-off is waiting through the build process of about 15-30min (depending on network and CPU speed).\n\n### Load prebuilt Image.\nDownload the docker image for your OS after requesting it [here](https://picocomputing.zendesk.com/hc/en-us/).\n\nFor Ubuntu 18.04:\n```\n$ docker load < mdla_ubuntu18.04.tgz\n```\n\nFor CentOS 7.5:\n```\n$ docker load < mdla_centos7.5.tgz\n```\n### Build Image with Dockerfile\nCopy the OS specific picocomputing package to your docker build folder. Then build and tag the image (we add the latest tag for user convenience when running containers):\n\nFor Ubuntu 18.04:\n```\n$ mkdir docker_build\n$ cp /path/to/picocomputing_2020.1_all.deb docker_build\n$ cd docker_build\n$ docker build -f ../docker/Dockerfile.ubuntu -t micron/mdla:2020.1-ubuntu18.04 -t micron/mdla:latest .\n```\n\nFor CentOS 7.5:\n```\n$ mkdir docker_build\n$ cp /path/to/picocomputing-2020.1.el6.x86_64.rpm docker_build\n$ cd docker_build\n$ docker build ../docker/Dockerfile.centos -t micron/mdla:2020.1-centos7.5 -t micron/mdla:latest .\n```\n\n### Run Container\nCheck the tag of the docker image that you just loaded/built using:\n```\n$ docker images\n```\n\nRun the docker image using the `docker run` command:\n```\n$ docker run -it --rm -v \"/path/to/models/on/host\":/models --device=/dev/pico1 micron/dla\n```\nThat will start you in the /home/mdla directory where the SDK is preinstalled. The -it flag means interactive, --rm deletes the container on exit, -v mounts a directory into the container, and --device mounts a host device into the container.\n\nTo make changes to the container (e.g. install editors, python libraries), remove the --rm flag so the container persists on exit.\nYou can then use the container id to `docker commit <id>` to a new image or `docker restart <id>` and `docker attach <id>` to reconnect a stopped container. You can also --name the container on run if you prefer not to use ids.\n```\n$ docker run -it -v \"/path/to/models/on/host\":/models --device=/dev/pico1 micron/dla\nroot@d80174ce2995:/home/mdla# exit\n$ docker restart d80174ce2995\n$ docker attach d80174ce2995\nroot@d80174ce2995:/home/mdla#\n```\n\nRun the example code provided. Check sections [3](#3-getting-started-inference-on-micron-dla-hardware) and [4](#4-getting-started-inference-on-micron-dla-hardware-with-c).\n\n## Python package Install (optional)\n\nYou can also install as a python package\n\n`git clone https://github.com/FWDNXT/SDK`\n\nThen inside SDK folder do\n\n`python3 setup.py install --user`\n\n\n# 2. Getting started with Deep Learning\n\n## Introduction\n\nThis is a very concise tutorial to help beginners learn how to create and train a Deep Learning model for use with Micron DLA demonstrations, SDK and other products.\n\nUsers should have knowledge of Linux and Ubuntu environments, personal computer or workstation maintenance and command line tools, and experience with the Python programming language. Additionally experience in C, C++, CUDA and GPU programming language may be needed for training advanced modules, but not required at the beginning, as PyTorch offers already implemented functions.\n\n\n## PyTorch: Deep Learning framework\n\nMicron DLA recommends the use of PyTorch [http://pytorch.org/](http://pytorch.org/) as Deep Learning framework. PyTorch is a CPU and GPU tested and ready framework, allowing users to train small models on CPU and larger and faster models on GPUs. PyTorch also features Dynamic Neural Networks, a version of Autograd - automatic differentiation of computational graphs that can be recorded and played like a tape. All this in simple means that PyTorch offers simpler ways to create custom complex models, and that users will see the benefits of PyTorch when trying to create and debug advanced neural network models.\n\nPyTorch tutorials from beginners to more advanced are linked here: [http://pytorch.org/tutorials/](http://pytorch.org/tutorials/).\n\n## My dataset\n\nWe recommend users try to train public models first. Here is a link to some public models and tools for PyTorch: [http://pytorch.org/docs/master/torchvision/datasets.html](http://pytorch.org/docs/master/torchvision/datasets.html).\n\nFor image-based datasets, we recommend the folder of folders arrangement: The dataset is a folder DATASET1 and inside there are multiple directory OBJ1, OBJ2, etc, each with multiple image files: obj1-file1.jpg, obj1-file2.png, etc.\n\n## Training a neural network with PyTorch\n\nTraining a deep neural network with PyTorch is very simple, and many examples of training scripts: [https://github.com/pytorch/examples](https://github.com/pytorch/examples).\n\nFor example, a good starting point is to train Micron DLA supported models on an image classification task. We recommend using this training script:\n\n[https://github.com/pytorch/examples/tree/master/imagenet](https://github.com/pytorch/examples/tree/master/imagenet). This script can load a custom dataset of images, please refer to the requirements in the script README file.\n\nPlease make sure that all inputs of the neural network are 32-bit float and are 0-mean and 1-std normalized.\n\n## After training a neural network\n\nAfter training a neural network with PyTorch, your model is ready for use in Micron DLA SDK. Please refer to the SDK manual for use with MDLA products.\n\n\n# 3. Getting started Inference on Micron DLA hardware\n\nThis tutorial will teach you how to run inference on hardware. We will use a neural network pre-trained on ImageNet.\nThe program will process an image and return the top-5 classifications of the image. A neural network trained for an object\ncategorization task will output a probability vector. Each element of the vector contains the probability to its correspondent\ncategory that is listed in a categories file.\nIn this tutorial you will need:\n* One of the [pre-trained models](http://fwdnxt.com/models/)\n* [Input image](./test-files): located in /test-files/\n* [Categories file](./test-files/categories.txt): located in /test-files/\n* [simpledemo.py](./examples/python_api/simpledemo.py): located in /examples/python_api/\n\n\n**Pytorch and torchvision pretrained model on ImageNet**\n\nIn the SDK folder, there is `genonnx.py`. This script will create an ONNX file from [torchvision models](https://github.com/pytorch/vision/tree/master/torchvision).\nThis utility requires the latest pytorch and it can create an ONNX file from most networks present in the\ntorchvision package and also from networks in the pth format.\n\n`python3 genonnx.py resnet18`\n\nThis command will download a pre-trained alexnet network and create a file called alexnet.onnx\n\nFor more information about onnx please visit [https://onnx.ai/](https://onnx.ai/)\n\nTo convert tensorflow models into ONNX files please reference the section [6. Using with Tensorflow](#6-using-with-tensorflow)\n\n**Loading the FPGA with MDLA**\n\nWhen you turn on the system, it will have the FPGA programmed with a default hardware definition. You need to load the MDLA bitfile only once after turning on the system.\n\nYou can load a MDLA bitfile of choice using:\n\n`python3 loadbitfile.py <bitfile path>`\n\nYou can find the MDLA bitfiles in the pico-computing folder. The default pico-computing installation folder is:\n\n`/usr/src/picocomputing-<version>/samples/InferenceEngine_5/firmware/`\n\nUse the appropriate .tgz file for the system you have.\n\nLoading the FPGA will take at max 5 min.\nLoading the FPGA only fails when there are no FPGA cards available. If you find issues in loading FPGA check out [Troubleshooting](#11-troubleshooting-and-qa).\nMicron DLA hardware will be loaded in the FPGA card. The following MDLA runs will not need to load the hardware anymore.\n\n**Running inference on Micron DLA hardware for one image**\n\nIn the SDK folder, there is simpledemo.py, which is a python demo application.\nIts main parts are:\n\n1) Parse the model and generate instructions\n2) Get and preprocess input data\n3) Init Micron DLA hardware\n4) Run Micron DLA hardware\n5) Get and display output\n\nThe user may modify steps 1 and 5 according to users needs.\nCheck out other possible application programs using Micron DLA hardware [here](http://fwdnxt.com/).\nThe example program is located in examples/python_api/\n\nYou can run the network on hardware with this command, which will find the FPGA card that was loaded with Micron DLA hardware:\n\n`python3 simpledemo.py <onnx file> <picture> -c <categories file.txt>`\n\nIf you used the example image with alexnet, the demo will output:\n\n```\n  Doberman, Doberman pinscher 24.4178\n\n  Rottweiler 24.1749\n\n  black-and-tan coonhound 23.6127\n\n  Gordon setter 21.6492\n\n  bloodhound, sleuthhound 19.9336\n```\n\n\n# 4. Getting started Inference on Micron DLA hardware with C\n\nThis tutorial will teach you how to run inference on the DLA using C code. We will use a neural network pre-trained on ImageNet.\nThe program will process an image and return the top-5 classifications of the image.\nIn this tutorial you will need:\n* One of the [pre-trained models](http://fwdnxt.com/models/)\n* [Input image](./test-files): located in /test-files/\n* [Categories file](./test-files/categories.txt): located in /test-files/\n* [Source code](./examples/c_api): located in /examples/c_api/\n\n\n**Running inference on the DLA for one image**\n\nIn the SDK folder, there is compile.c, which compiles an ONNX model and outputs DLA instructions into a .bin file.\nThe simpledemo.c program will read this .bin file and execute it on the DLA.\nThe main functions are:\n1) `ie_compile`: parse ONNX model and generate the DLA instructions.\n2) `ie_init`: load the DLA bitfile into FPGA and load instructions and model parameters to shared memory.\n3) `ie_run`: load input image and execute on the DLA.\n\nCheck out other possible application programs using the DLA [here](http://fwdnxt.com/).\n\nMake sure the MDLA bitfile was loaded into the FPGA before running it.\n\nTo run the demo, first run the following commands:\n\n```\ncd <sdk folder>/examples/c_api\nmake\n./compile -m <model.onnx> -i 224x224x3 -o instructions.bin\n```\nWhere `-i` is the input sizes: width x height x channels.\nAfter creating the `instructions.bin`, you can run the network on the DLA with this command, which will find the FPGA card that was loaded with the DLA:\n\n`./simpledemo -i <picturefile> -c <categoriesfile> -s ./instructions.bin`\n\nIf you used the example image with alexnet, the demo will output:\n\n```\nblack-and-tan coonhound -- 23.9883\nRottweiler -- 23.6445\nDoberman -- 23.3320\nGordon setter -- 22.0195\nbloodhound -- 21.5000\n```\n\n# 5. Tutorial - Multiple FPGAs and Clusters\n\nThis tutorial will teach you how to run inference on Micron DLA inference engine using multiple FPGAs and clusters.\n\n\n## Multiple FPGAs with input batching\nSuppose that you have a desktop computer with two AC-511 FPGAs cards connected to a EX-750 PCI backplane. To simplify this example, let us assume there is one cluster per FPGA card. We will see how to use multiple clusters in the following sections.\nThe SDK can receive two images and process one image on each FPGA. The Micron DLA instructions and model parameters are broadcast to each FPGA card's main memory (HMC).\nThe following code snippet shows you how to do this:\n\n```python\nimport microndla\nimport numpy as np\nnumfpga = 2\nnumclus = 1\n# Create Micron DLA API\nsf = microndla.MDLA()\n# Generate instructions\nsf.SetFlag({'nfpgas': str(numfpga), 'nclusters': str(numclus)})\nsf.Compile('resnet18.onnx')\nin1 = np.random.rand(2, 3, 224, 224).astype(np.float32)\ninput_img = np.ascontiguousarray(in1)\n# Create a location for the output\noutput = sf.Run(input_img)\n```\n\n`sf.Compile` will parse the model from model.onnx and save the generated Micron DLA instructions. Here numfpga=2, so instructions for two FPGAs are created.\n`nresults` is the output size of the model.onnx for one input image (no batching).\nThe expected output size of `sf.Run` is twice `nresults`, because numfpga=2 and two input images are processed. `input_img` is two images concatenated.\nThe diagram below shows this type of execution:\n\n<img src=\"docs/pics/2fpga2img.png\" width=\"900\" height=\"550\"/>\n\n\n## Multiple FPGAs with different models\nThe SDK can also run different models on different FPGAs. Each `microndla.MDLA()` instance will create a different set of Micron DLA instructions for a different model and load it to a different FPGA.\nThe following code snippet shows you how to do this:\n\n```python\nimport microndla\nimport numpy as np\n# Create Micron DLA API\nsf1 = microndla.MDLA()\n# Create second Micron DLA API\nsf2 = microndla.MDLA()\n# Generate instructions for model1\nsf1.Compile('resnet50.onnx')\n# Generate instructions for model2\nsf2.Compile('resnet18.onnx')\nin1 = np.random.rand(3, 224, 224).astype(np.float32)\nin2 = np.random.rand(3, 224, 224).astype(np.float32)\ninput_img1 = np.ascontiguousarray(in1)\ninput_img2 = np.ascontiguousarray(in2)\noutput1 = sf1.Run(input_img1)\noutput2 = sf2.Run(input_img2)\n```\n\nThe code is similar to the previous section. Each instance will compile, init and execute a different model on different FPGA.\nThe diagram below shows this type of execution:\n\n<img src=\"docs/pics/2fpga2model.png\" width=\"900\" height=\"550\"/>\n\n## Multiple Clusters with input batching\nFor simplicity, now assume you have one FPGA and inside it we have two Micron DLA clusters.\nEach cluster can execute its own set of instructions, so we can also batch the input (just like the two FPGA case before).\nThe difference is that both clusters share the same main memory in the FPGA card. The number of results returned by\nCompile() will be the total number of results, for both clusters, in this case.\nFollowing a similar strategy as the two FPGA with input batching example, the following code snippet shows you how to use two clusters to process two images:\n\n```python\nimport microndla\nimport numpy as np\nnumfpga = 1\nnumclus = 2\n# Create Micron DLA API\nsf = microndla.MDLA()\n# Generate instructions\nsf.SetFlag('nclusters', str(numclus))\nsf.Compile('resnet18.onnx')\nin1 = np.random.rand(2, 3, 224, 224).astype(np.float32)\ninput_img = np.ascontiguousarray(in1)\noutput = sf.Run(input_img)\n```\n\nThe only difference is that nclus=2 and nfpga=1.\nThe diagram below shows this type of execution:\n\n<img src=\"docs/pics/2clus2img.png\" width=\"400\" height=\"550\"/>\n\n## Multiple Clusters without input batching\nThe SDK can also use both clusters on the same input image. It will split the operations among the two clusters.\nThe following code snippet shows you how to use two clusters to process one image:\n\n```python\nimport microndla\nimport numpy as np\nnumfpga = 1\nnumclus = 2\n# Create Micron DLA API\nsf = microndla.MDLA()\nsf.SetFlag({'nclusters': str(numclus), 'clustersbatchmode': '1'})\n# Generate instructions\nsf.Compile('resnet18.onnx')\nin1 = np.random.rand(3, 224, 224).astype(np.float32)\ninput_img = np.ascontiguousarray(in1)\noutput = sf.Run(input_img)\n```\n\nUse `sf.SetFlag('nobatch', '1')` to set the compiler to split the workload among two clusters and generate the instructions.\nYou can find more information about the option flags [here](docs/Codes.md).\nNow the output size is not twice of `nresults` because you expect output for one inference run.\nThe diagram below shows this type of execution:\n\n<img src=\"docs/pics/2clus1img.png\" width=\"600\" height=\"550\"/>\n\n## Multiple Clusters with different models\nThe following example shows how to run different models using different clusters in parallel. \nCurrently, a cluster for each model is allowed. But different number of cluster per model is not allowed. For example, 3 clusters for a model and then 1 cluster for another.\nThe example code is in [here](./examples/python_api/twonetdemo.py)\n\n```python\nimport microndla\nimport numpy as np\nnclus = 2\nimg0 = np.random.rand(3, 224, 224).astype(np.float32)\nimg1 = np.random.rand(3, 224, 224).astype(np.float32)\nie = microndla.MDLA()\nie2 = microndla.MDLA()\nie.SetFlag({'nclusters': nclus, 'clustersbatchmode': 1})\nie2.SetFlag({'nclusters': nclus, 'firstcluster': nclus, 'clustersbatchmode': 1})\nie.Compile('resnet18.onnx')\nie2.Compile('alexnet.onnx', MDLA=ie)\nie.PutInput(img0, None)\nie2.PutInput(img1, None)\nresult0, _ = ie.GetResult()\nresult1, _ = ie2.GetResult()\n```\nIn the code, you create one MDLA object for each model and compile them. For the first model, use 2 clusters together. \nFor the second model, assign the remaining 2 clusters to it. Use `firstcluster` flag to tell `Compile` which cluster is the first cluster it is going to use.\nIn this example, first model uses clusters 0 and 1 and second model uses clusters 2 and 3. \nIn `Compile`, pass the previous MDLA object to link them together so that they get loaded into memory in one go. \nIn this case, you must use `PutInput` and `GetResult` paradigm (this [section](#6-tutorial---putinput-and-getresult)), you cannot use `Run`.\n\n<img src=\"docs/pics/2clus2model.png\" width=\"600\" height=\"550\"/>\n\n## All Clusters with different models in sequence\n\nThis example shows how to load multiple models and run them in a sequence using all clusters. This is similar to previous example, the only different \nall clusters is used for each model. It uses same principle of creating different MDLA objects for each model and link different MDLAs in `Compile`.\n\n```python\nimport microndla\nimport numpy as np\nnclus = 2\nimg0 = np.random.rand(3, 224, 224).astype(np.float32)\nimg1 = np.random.rand(3, 224, 224).astype(np.float32)\nie = microndla.MDLA()\nie2 = microndla.MDLA()\nie.SetFlag({'nclusters': nclus, 'clustersbatchmode': 1})\nie2.SetFlag({'nclusters': nclus, 'clustersbatchmode': 1})\nie.Compile('resnet18.onnx')\nie2.Compile('alexnet.onnx', MDLA=ie)\nresult0 = ie.Run(img0)\nresult1 = ie2.Run(img1)\n```\n\n<img src=\"docs/pics/2clus2seqmodel.png\" width=\"600\" height=\"550\"/>\n\n\n## Multiple Clusters with even bigger batches\n\nIt's possible to run batches of more than than the number of clusters or FPGAs. Each cluster will process multiple images.\nThis is enabled with the `imgs_per_cluster` flag. In order to process 32 images, 16 by each cluster, this code will do the work:\n\n```python\nimport microndla\nimport numpy as np\nnumfpga = 1\nnumclus = 2\n# Create Micron DLA API\nsf = microndla.MDLA()\nsf.SetFlag({'nclusters': str(numclus), 'imgs_per_cluster': '16'})\n# Generate instructions\nsf.Compile('resnet18.onnx')\nin1 = np.random.rand(32, 3, 224, 224).astype(np.float32)\ninput_img = np.ascontiguousarray(in1)\noutput = sf.Run(input_img) # Run\n```\n\n## Batching using MVs\n\nIt's possible to use MV-level parallelism. MV is a computation unit present inside of a cluster and they can be configured to run different images.\nThis is generally more efficient than leaving different MV units process the same image.\nIn order to enable this, you have to set the `mvbatch` flag. Keep in mind that this can be only done when `imgs_per_cluster` is a\nmultiple of 4, since there are 4 MV units available inside of a cluster.\n\n```python\nimport microndla\nimport numpy as np\nnumfpga = 1\nnumclus = 2\n# Create Micron DLA API\nsf = microndla.MDLA()\nsf.SetFlag({'nclusters': str(numclus), 'imgs_per_cluster': '16', 'mvbatch': '1'})\n# Generate instructions\nsf.Compile('resnet18.onnx')\nin1 = np.random.rand(32, 3, 224, 224).astype(np.float32)\ninput_img = np.ascontiguousarray(in1)\noutput = sf.Run(input_img)\n```\n\n# 6. Tutorial - PutInput and GetResult\nThis tutorial teaches you to use PutInput and GetResult API calls.\n\nPutInput will load the input data into the memory that is shared between the host and the Micron DLA.\n\nGetOutput will read the output (results) from the memory. GetOutput can be blocking or non-blocking. Use `SetFlag` function to use blocking or non-blocking mode. The user does not need to care to start the inference engine. PutInput and GetResult will take care of that as soon as the inference engine becomes free.\n\nBlocking means that a call to GetResult will wait for the DLA to finish processing.\n\nNon-blocking means that GetResult will return immediately: with or without the result depending whether the DLA has finished processing.\n\nThese two functions are important in a streaming application. The programmer can overlap the time for these two tasks: input loading and getting results.\n\nSome care has to be taken in order to avoid deadlocks. There is only one inference engine and only two internal buffers where input and output are stored. So if a user issues three PutInput in sequence without any GetResult between (or waiting in another thread), the third PutInput will block indefinitely waiting for a buffer to become available, something that will never happen if the user will not call GetResult to get the result of the first PutInput, freeing the associated results buffer. Particular care has to be taken when dealing with multiple inference engine objects: they all share the common hardware, so again there can be only one outstanding PutInput. Consider this sequence:\n* PutInput on the first object\n* PutInput on the first object\n* PutInput on the second object\n* GetResult on the second object\n\nThis will result in a deadlock, because PutInput on the second object will wait for a buffer to become free and this cannot happen until the user calls GetResult on the first object. Having a thread for each object always waiting for the results, will assure that this will not happen.\n\n<img src=\"docs/pics/Double Buffer Illustration.jpg\" width=\"900\" height=\"240\"/>\n\nExamples using PutInput and GetOutput are located in [examples/python_api/](examples/python_api/).\n\n* pollingdemo.py : is an example of non-blocking mode. The program will poll GetResult until it returns the output.\n\n* interleavingdemo.py : is an example that shows how to pipeline PutInput and GetResult calls. There are two separate memory regions to load inputs and get results. While PutInput loads to one region, GetResult fetches the output from another region. Each image is labeled with the **userobj** to keep track of which input produced the returned output.\n\n* threadeddemo.py : shows how to use two threads to process multiple images in a folder. One thread calls GetResult and another calls PutInput. This is the preferred way to work, as it will give the best possible performance.\n\n* threadedbatchdemo.py : similar to `threadeddemo.py`. It shows how to process images in a batch using PutInput and GetResult.\n\n# 7. Tutorial - Writing tests\nThis tutorial is going to show you how to create a test.\nFor this tutorial, we are going to use Pytorch framework.\nFirst, you will need to define a model.\n```python\n#imports needed for this example\nimport microndla\nimport torch\nimport torch.onnx\nimport numpy as np\n\n#defines a model with one Convolution layer\nclass Conv(torch.nn.Module):\n    #k: kernel size, s: stride, p: padding\n    def __init__(self, inP, outP, k = 3, s = 1, p = 1):\n        super(Conv, self).__init__()\n        self.op = torch.nn.Conv2d(inP, outP, k, s, p)\n    def forward(self, x):\n        y = self.op(x)\n        return y\n```\nThe purpose of this example test is to show how to run the computation in the accelerator. Thus, we will not train this Conv model for anything. By default, the weights and bias are random values.\nYou need to create an instance of the model and export it into an onnx file.\n```python\nw = 16 # input sizes\ni = 256 # input planes\no = 256 # output planes\nk = 3 # kernel size\ns = 1 # stride\np = 0 # padding\n# input tensor. Use float32, don't use float16\ninV = torch.randn(1, i, w, w, dtype=torch.float32)\n# create a model instance\nmodelConv = Conv(i, o, k, s, p)\n# export the model from pytorch to an onnx file\ntorch.onnx.export(modelConv, inV, \"net_conv.onnx\")\n```\n\nNow we need to run this model using CPU with Pytorch. You can run this model by adding the following:\n```python\nouthw = modelConv(inV)\nresult_pyt = outhw.view(-1)\nresult_pyt = result_pyt.detach().numpy()\n```\n\nNow we need to run this model using the accelerator with the SDK.\n```python\nsf.Compile('net_conv.onnx')\nin_1 = np.ascontiguousarray(inV)\nresult = sf.Run(in_1)\n```\nThe results from the accelerator are in `result` and the results from the CPU are in `result_pyt`. We could print all values of both vectors to compare. A better approach is to have an error metric. The following code calculates the relative mean error and the max error.\n```python\nerror_mean=(np.absolute(result-result_pyt).mean()/np.absolute(result_pyt).max())*100.0\nerror_max=(np.absolute(result-result_pyt).max()/np.absolute(result_pyt).max())*100.0\nprint(\"CONV\")\nprint('Mean/max error compared to pytorch are {:.3f}/{:.3f} %'\n.format(error_mean, error_max))\n```\nThe print output for us was:\n```python\nCONV\nMean/max error compared to pytorch are 1.636/9.055 %\n```\nSince the input and weights are set randomly, the output might be different from this. In any case, the error is not zero. The results between the CPU and the accelerator do not match. The precision used by the accelerator is fixed point 16bit ([Q8.8](https://en.wikipedia.org/wiki/Fixed-point_arithmetic)) and the CPU uses float32. Thus, a small error is an expected behavior of the accelerator.\nThe `Init` and `Run` functions internally convert all the float32 values into fix point format.\n\nThere are other layers and model that you can test using this method. Additional example tests are in [here](examples/tests).\n\n# 8. Tutorial - Debugging\nThis tutorial goes through some of the debug functionalities.\n\nLets use the script we created in the previous tutorial. You can also copy from [here](examples/tests/test_conv.py).\n\nThe SDK comes with debug options and compile options. SetFlag function sets these configurations.\nYou can set a variable directly, such as `SetFlag('nobatch','1')`. Or equivalently, `SetFlag('options', 'C')`.\nThe `'nobatch'` compile option enables the compiler to spread the computation of each layer across multiple clusters. Since `'nobatch'` is a compile option, we can set it with 'options' and use its option code 'C'.\n\nA debug option won't affect the compiler, it will only print more information. These prints are were used for debugging when developing the compiler, so the prints can have a large amount of information.\n\nYou can use `SetFlag('debug', 'b')` to print the basic prints. The debug code `'b'` stands for basic. Debug codes and option codes are letters (case-sensetive). For a complete list of letters refer to [here](docs/Codes.md).\n\nAlways put the `SetFlag()` after creating the Micron DLA object. If will print the information about the run. First, it will list all the layers that it is going to compile from the `net_conv.onnx`.\n\nThen `Run` will rearrange in the input tensor and load it into the external memory. It will print the time it took and other properties of the run, such as number of FPGAs and clusters used.\n\nIn the `Run` it will start the accelerator. The accelerator uses configuration registers to count how many output values were produced.\nThe software is going to poll this register until the expected amount of results have been produced by the accelerator.\nThat is how the software knows that the result is ready in the external memory and it can be fetched to the host.\nThe \"Start card 0 cluster 0\" is a print before a while loop that polls that output register. And \"Reset card 0 cluster 0\" is a print after the while loop exits.\n\nThen profiling for the run will appear afterwards.\nThe expected bandwidth is calculated as the ratio between data transferred and expected execution time.\ndata transferred is calculated during compilation. It just counts how many words are send and received between HMC and the accelerator. (This is not between HMC and pcie.)\nExpected execution time is also calculated in during compilation. It uses the number of operations, accelerator frequency and number of MACs used to get the expected execution time assuming running at peak performance.\nMeasured time is measured between start and end of hardware accelerator execution.\nMeasured bandwidth just uses the measured time instead of expected time in the bandwidth calculation. Eff Measured is the ratio between expected time and measured time.\n\nThen the output is rearranged back to the original data arrangement.\n\nFor more details of all the debug options and compile options please refer to [Python API](docs/PythonAPI.md) and [C API](docs/C%20API.md).\n\nNote: `SetFlag('debug', 'w')` will enable warning prints. The warning prints are useful to check if the computation values are overflowing or not. Overflows happen when the result of the computation can't be represented using the fix-point (Q8.8) format.\n\nThere are a few suggestions when designing a neural network that will avoid this case.\nCheck that a batchnorm layer is present after each convolution and linear layer. Another suggestion is to use tanh or sigmoid instead of relu after the layers that have values overflowing.\nThis will limit the output to -1 and 1 (tanh) or 0 and 1 (sigmoid).\n\n\nIn the example, the layer that is going to be run is printed in the beginning:\n```\ntype conv name=3 id=0 in=(Z1,H16,W16,P256) out=(Z1,H14,W14,P256)\nk=(Z1,H3,W3) stride=(Z1,H1,W1) dilation=(Z1,H1,W1)\npad=(Z0,Ze0,T0,B0,R0,L0) opad=(Z0,H0,W0) 0->1\n```\nThe meaning of each part is:\n - type: shows what type is the layer. e.g.: conv, maxpool, concat.\n - name: shows the output name id from onnx. Use [Netron](https://lutzroeder.github.io/netron/) to visualize the onnx model and click on the layer to see the layer's outputs section.\n - id: internal id to reference the layer.\n - in: input sizes. Z1 means on z dimension is size of 1. P256 means on planes dimension is size of 256. H16 means on height size is 16. (or y dimension). W16 means on width size is 16. (or x dimension).\nConv2D have x, y dimensions and channels (W, H, P). Conv3D also have z dimension which is Z (W, H, Z, P).\n - out: output sizes.\n - k: kernel sizes.\n - stride: stride sizes.\n - dilation: dilation sizes.\n - pad: padding sizes. T1: 1 r ow of padding on top. B1: 1 row of padding on bottom. R1: 1 column of padding on right. L1: 1 column of padding on left. Z0: 0 padding along z-dimension. Ze0: 0 padding along z-dimension in the other end\n - opad: output padding sizes.\n - `0->1`: internal indexes to output buffer that connect between different layers.\n\nExample code for running an onnx model is provided [here](examples/tests/test_model.py). It will run and compare the results from Micron DLA and [onnxruntime](https://github.com/microsoft/onnxruntime).\nThe example takes, path to onnx file and input shape `WxHxP`. It prints the output comparison just like the `test_conv.py` example. You can run it like this:\n\n```\npython3 test_model.py resnet18.onnx 224x224x3\n```\n\nThe code also have a profile option, which will execute each layer of the model and print the time measurements into a `.csv` file.\n\n# 9. Variable Fix Point Quantization\n\nMicro DLA uses 16-bit fix point to represent numbers. The `Compile` function will convert the numbers in the onnx model from float 32-bit into 16-bit fix-point [Q8.8](https://en.wikipedia.org/wiki/Q_(number_format)). The default Micron DLA bitfile will run the model using Q8.8.\n\nA Micron DLA bitfile with variable fix-point support is provided in order to reduce the discrepancy between the float 32-bit and the Q8.8 representation.\n\nThis bitfile allows the software to choose different QX.Y representations that is the best fit for different parts of the neural network model.\n\nThe SDK provides 2 options for variable fix-point quantization. **Before you try** these options, make sure to load the bitfile that supports variable fix-point into the FPGA.\n\n**Option 1**: For each layer of the model, their weights and biases are converted into different QX.Y representations.\n\nIn this case, you can set 'V' in the options using `SetFlag` function before `Compile`:\n\n```python\nie = microndla.MDLA()\nie.SetFlag('varfp', '1')\n#Compile to a file\nswnresults = ie.Compile('resnet18.onnx')\n```\n\n**Option 2**: Variable fix-point can be determined for input and output of each layer if one or more sample inputs are provided.\n\nYou will need to provide a set of sample inputs (calibration data) to `Compile` funtion. In addition to compiling the model, `Compile` will run the model with the calibration inputs using float32 and save the variable fix-point configuration for each input/output of each layer in the model. `Compile` will also convert the static data (weights and biases) to the appropriate fix-point representation, so no need for `ie.SetFlag('varfp', '1')` in this case.\n\nInstead of using ie.Compile, you use `Quantize` and give an array of input data:\n\n```python\n#Load image into a numpy array\nimg = LoadImage(args.image, args)\nimgs = []\nfor fn in os.listdir(args.imagesdir):\n    x = LoadImage(args.imagesdir + '/' + fn, args)\n    imgs.append(x)\n\n#Create and initialize the Inference Engine object\nie = microndla.MDLA()\n#Compile to a file\nswnresults = ie.Compile('resnet18.onnx', samples=imgs)\n```\n\nAfter that, `Init` and `Run` runs as usual using the saved variable fix-point configuration.\n\nCheckout the example [quantize.py](examples/python_api/quantize.py) which takes same arguments as `simpledemo.py`. The only addition is a folder with the calibration input images for calibration data.\n\n# 10. Running a model from your favorite deep learning framework\n\nMicron DLA supports different deep learning frameworks by running models in ONNX format. In order to convert a model from your favorite deep learning framework to ONNX format you should follow the instructions [here](https://github.com/onnx/tutorials). However there are some extra steps you should take with certain frameworks for the best compatibility with Micron DLA and we describe them below.\n\nThere is a list of tutorials on how to convert model to ONNX for each framework in the [ONNX github](https://github.com/onnx/tutorials).\n\n## Tensorflow\n\nTo convert tensorflow models into ONNX format, you will need to install [tensorflow-onnx](https://github.com/onnx/tensorflow-onnx).\n\nTensorflow uses various file formats to represent a model: checkpoint files, frozen graphs (graph with weights) and saved_model. For more details please refer to [tensorflow guides](https://www.tensorflow.org/guide/extend/model_files).\n\nThe simplest way is to save the model in the saved_model.pb, as our SDK will automatically call tensorflow-onnx to convert it to ONNX. Just pass the directory with the saved_model.pb file to Compile. Otherwise follow the instructions of tensorflow-onnx to convert the tensorflow model to ONNX.\n\nFor example, you can take the basic example from the [basic Tensorflow tutorial](https://www.tensorflow.org/overview)\n\n```\nimport tensorflow as tf\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test, y_test)\nmodel.save('mnist')\n```\n\nand then try with our inference engine:\n\n```\nimport microndla\nimport numpy as np\nimport tensorflow as tf\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nie = microndla.MDLA()\nie.Compile('mnist.onnx')\nfor i in range(0, 10):\n    result = ie.Run(x_test[i].astype(np.float32))\n    print(y_test[i], np.argmax(result))\n\n```\n\n## Caffe1\n\n**Step 0 (optional):** Make sure your model is in the newest Caffe1 format. If not use upgrade_net_proto_text binary from Caffe1 tools to upgrade it. For example to upgrade VGG-16 from Caffe1 model zoo:\n\n`upgrade_net_proto_text VGG_ILSVRC_16_layers_deploy.prototxt vgg16_caffe1.prototxt`\n\n**Step 1:** Download [caffe_translator.py](https://github.com/pytorch/pytorch/blob/master/caffe2/python/caffe_translator.py). Use it to convert model from Caffe1 format to Caffe2. For example:\n\n`python caffe_translator.py vgg16_caffe1.prototxt VGG_ILSVRC_16_layers.caffemodel`\n\n**Step 2:** Now your model is in Caffe2 format. You can follow the [official instructions](https://github.com/onnx/tutorials/blob/master/tutorials/Caffe2OnnxExport.ipynb) to convert it to ONNX format. Example conversion:\n\n`convert-caffe2-to-onnx predict_net.pb --caffe2-init-net init_net.pb --value-info '{\"data\": [1, [1, 3, 224, 224]]}' -o vgg16.onnx`\n\nFor more information see links below:\n\n[https://github.com/BVLC/caffe/blob/master/tools/upgrade_net_proto_text.cpp](https://github.com/BVLC/caffe/blob/master/tools/upgrade_net_proto_text.cpp)\n\n[https://caffe2.ai/docs/caffe-migration.html](https://caffe2.ai/docs/caffe-migration.html)\n\n## Keras\n\nExporting Keras models to ONNX format is done through [ONNXMLTools](https://github.com/onnx/onnxmltools). You should edit ~/.keras/keras.json so that field \"image_data_format\" is set to \"channels_first\". A sample code to convert Resnet 50 from Keras to ONNX is shown below.\n\n```python\nimport onnx\nimport onnxmltools\nimport keras\n\nmodel = keras.applications.resnet50.ResNet50(include_top=True, weights='imagenet',\ninput_tensor=None, input_shape=None, pooling=None, classes=1000)\nonnx_model = onnxmltools.convert_keras(model)\nonnx.save(onnx_model, 'resnet50.onnx')\n```\n\n# 11. Supported models and layers\n\n  * [Add](examples/tests/test_vectoradd.py)\n  * AveragePool\n  * BatchNormalization\n  * Concat\n  * [Conv](examples/tests/test_conv.py)\n  * [ConvTranspose](examples/tests/test_transconv.py)\n  * GlobalAveragePool\n  * LeakyRelu\n  * [Linear](examples/tests/test_matrixvector.py)\n  * [MaxPool](examples/tests/test_maxpool.py)\n  * [Mul](examples/tests/test_vectormul.py)\n  * Relu\n  * Sigmoid\n  * Tanh\n  * Upsample\n\n## Tested models\n\n  * Alexnet OWT (versions without LRN)\n  * Resnet 18, 34, 50\n  * Inception v1, v3\n  * VGG 16, 19\n  * [LightCNN-9](https://arxiv.org/pdf/1511.02683.pdf)\n  * [Linknet](https://arxiv.org/pdf/1707.03718.pdf)\n  * [Neural Style Transfer Network](https://arxiv.org/pdf/1603.08155.pdf)\n  * LSTM\n  * GRU\n  * [Mobilenet](https://github.com/onnx/models/blob/master/vision/classification/mobilenet/model/mobilenetv2-7.onnx)\n  * [Linknet](https://arxiv.org/abs/1707.03718)\n  * [Enet](https://arxiv.org/abs/1606.02147)\n  * [SqueezeNet](https://arxiv.org/abs/1602.07360)\n\n## TF-Slim models tested on Micron DLA inference engine\n\n* Inception V1\n* Inception V3\n* ResNet V1 50\n* VGG 16\n* VGG 19\n\n## ONNX model zoo\n\nhttps://github.com/onnx/models\n\n * Resnet v1 all models work, Resnet v2 not yet\n * Squeezenet\n * VGG all models\n * Emotion FerPlus\n * MNIST\n\nNote: BVLC models, Inception_v1, ZFNet512 are not supported because we do not support the LRN layer.\n\n## Keras\n\n* [ResNet50](https://keras.io/applications/#resnet50)\n\n## CNTK\n\n* [ResNet50](https://www.cntk.ai/Models/CNTK_Pretrained/ResNet50_ImageNet_CNTK.model)\n* [VGG16](https://www.cntk.ai/Models/Caffe_Converted/VGG16_ImageNet_Caffe.model)\n\n# 12. Troubleshooting and Q&A\n\nQ: Where can I find weights for pretrained TF-slim models?\n\nA: They can be found as tarred checkpoint files at\n\n[https://github.com/tensorflow/models/tree/master/research/slim#Pretrained](https://github.com/tensorflow/models/tree/master/research/slim#Pretrained)\n\nQ: Issue: Can't find FPGA card\n\nA: Make sure the picocomputing release is installed properly. Please run the following commands. It should print the following outputs.\n```\nlspci | grep -i pico\n    05:00.0 Memory controller: Pico Computing Device 0045 (rev 05)\n    08:00.0 Memory controller: Pico Computing Device 0510 (rev 05)\nlsmod | grep -i pico\n    pico                 3493888  12\n```\n\nQ: Can I run my own model?\n\nA: yes, all models that are derivatives of the ones listed in the Supported Networks section. It can be modified within the limitations of the system.\n\nQ: How will developers be able to develop on your platform?\n\nA: They will need to provide a neural network model only. No need to write any special code. Micron DLA will update the software periodically based on users and market needs.\n\nQ: What tools will I need at minimum?\n\nA: Micron DLA inference engine on an FPGA and Micron DLA SDK tools\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/FWDNXT/SDK",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "microndla",
    "package_url": "https://pypi.org/project/microndla/",
    "platform": "",
    "project_url": "https://pypi.org/project/microndla/",
    "project_urls": {
      "Homepage": "https://github.com/FWDNXT/SDK"
    },
    "release_url": "https://pypi.org/project/microndla/2021.2/",
    "requires_dist": [
      "numpy (>=1.14.2)",
      "Pillow (>=5.0)",
      "onnx (>=1.10.1)"
    ],
    "requires_python": "",
    "summary": "Micron Deep Learning Acceleration SDK",
    "version": "2021.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13433663,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "64f2c2006237884840aedc43f5c95de034ed805da09197313df2b2238cc29018",
        "md5": "8ad6c29edc3d8cb42b24b15e6079c1c1",
        "sha256": "7651cec774fb6f87a7ab86c9ba1025202a4c7ac7891781f67ed4a9c70f7b1939"
      },
      "downloads": -1,
      "filename": "microndla-2021.2-py3.8.egg",
      "has_sig": false,
      "md5_digest": "8ad6c29edc3d8cb42b24b15e6079c1c1",
      "packagetype": "bdist_egg",
      "python_version": "2021.2",
      "requires_python": null,
      "size": 31487,
      "upload_time": "2021-10-26T22:11:48",
      "upload_time_iso_8601": "2021-10-26T22:11:48.800440Z",
      "url": "https://files.pythonhosted.org/packages/64/f2/c2006237884840aedc43f5c95de034ed805da09197313df2b2238cc29018/microndla-2021.2-py3.8.egg",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "276ab2ecaa820232e1067c0af5d230503d3e5a9844dcbaa20ba693c915e6ccc9",
        "md5": "4dae9323b7b5d91fef8730d2139c34c4",
        "sha256": "99a8c990c32c6b455baa631baf53fc141a6bbef71a46031017a1be8f8fc4769a"
      },
      "downloads": -1,
      "filename": "microndla-2021.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4dae9323b7b5d91fef8730d2139c34c4",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 24204,
      "upload_time": "2021-10-26T22:11:45",
      "upload_time_iso_8601": "2021-10-26T22:11:45.019060Z",
      "url": "https://files.pythonhosted.org/packages/27/6a/b2ecaa820232e1067c0af5d230503d3e5a9844dcbaa20ba693c915e6ccc9/microndla-2021.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e2ca520129a5060fd938588f631e2d56472a70d1cb581396604a6fe8d4b5d2c4",
        "md5": "259486eb4c76d091487d036b25b5f387",
        "sha256": "e0683579377895a4bcb16853db09079caeb8787d69adc10b2906a4ba6f804b55"
      },
      "downloads": -1,
      "filename": "microndla-2021.2.tar.gz",
      "has_sig": false,
      "md5_digest": "259486eb4c76d091487d036b25b5f387",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 52990,
      "upload_time": "2021-10-26T22:11:50",
      "upload_time_iso_8601": "2021-10-26T22:11:50.644228Z",
      "url": "https://files.pythonhosted.org/packages/e2/ca/520129a5060fd938588f631e2d56472a70d1cb581396604a6fe8d4b5d2c4/microndla-2021.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}