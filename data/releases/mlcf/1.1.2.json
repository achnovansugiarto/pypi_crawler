{
  "info": {
    "author": "Guitheg, Embraysite",
    "author_email": "gthgobin@gmail.com, costes.ambroise@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Environment :: Console",
      "Environment :: GPU :: NVIDIA CUDA :: 11.3",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Natural Language :: English",
      "Operating System :: POSIX",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Desktop Environment :: File Managers",
      "Topic :: Education",
      "Topic :: Office/Business :: Financial :: Investment",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# MLCF - Machine Learning Toolkit for Cryptocurrency Forecasting  \n\nThis library provides tools for cryptocurrency forecasting and trade decision making.  \nAmong which :\n\n- data acquisition to download historical cryptocurrency data.\n\n- shaping and preprocessing data for time series machine learning.\n\n- directory and file management to help machine learning training (models versionning, checkpoint, logs, visualization, etc.).\n\nThis library doesn't provide models or an end-to-end trade bot. It is only providing tools to make easier the data acquisition, the data analysis, the forecasting and decision making training. However, MLCF provide a file management which allows to make an end-to-end model easier.\n\nIn addition to using modules' functions, we can use MLCF as a python module :\n\n```bash\npython -m mlcf <list_of_arguments>\n```\n\n---\n\n## Installation\n\nOS officially supported:  \n\n- **Linux**  \n\nPython version officially supported:  \n\n- **3.7**  \n\n- **3.8**  \n\n- **3.9**\n\nTo succeed the installation, it needs to install some dependencies, which are:\n\n- the TA-LIB C library\n- PyTorch v1.10.2 (cuda 11.3)\n\n---\n\n### Installation for Linux (python v3.7, v3.8, v3.9)\n\n- TA-LIB C library installation:  \n\n*Note: the talib-install.sh file and the ta-lib-0.4.0-src.tar.gz archive will be downloaded on your PC. They can be manually deleted at the end of the installation.*  \n\n```bash\nwget https://raw.githubusercontent.com/Guitheg/mlcf/main/build_helper/talib-install.sh\nsh talib-install.sh\n```\n\n- Pytorch with cuda 11.3 installation:  \n\n```bash\npip3 install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio==0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n```\n\n- MLCF package\n\n```bash\npip install mlcf\n```\n\n---\n\n## MLCF module usage\n\nIn this part, we will introduce all the usage of MLCF module: the mlcf_home repository, the build-dataset command and the train command. The project is currently in development and new features will come soon.  \n\n---\n\n### mlcf_home repository  \n\nBefore all, we need to create an user directory where all of our project will be stored : logs, checkpoint, models, trainers, parameters file etc.  \nThe arborescence of a mlfc_home repository is as follow:  \n\n```bash\nmlcf_home\n│   parameters.ini\n├───data\n│   │   dataset_a.wst\n│   │   dataset_b.wst\n│   └   ...\n│\n├───logs\n│   ├─  debugMessages\n│   └─  infoMessages\n│\n├───ml\n│   ├───models\n│   │   │   lstm.py\n│   │   │   cnn_n_lstm.py\n│   │   └─  ...\n│   │\n│   └───trainers\n│       │   trainer_lstm_a.py\n│       │   trainer_lstm_b.py\n│       └─  ...\n│\n└───Training\n    ├───TrainingInfo.csv\n    ├───boards\n    │       ├───boards_TrainingA\n    │       ├───boards_TrainingB\n    │       │   ├───board_17feb_17h_TrainingB\n    │       │   ├───board_18feb_19h_TrainingB\n    │       │   └─  ...\n    │       └─  ...\n    │   \n    └───checkpoints\n            ├───checkpoints_TrainingA\n            ├───checkpoints_TrainingB\n            │   ├───checkpoint_TrainingA_17feb_17h.pt\n            │   ├───checkpoint_TrainingB_18feb_19h.pt\n            │   └─  ...\n            └─  ... \n        \n```\n\nTo create the mlcf_home repository in a choosen repository, use:\n\n```bash\nmlcf --create-userdir --userdir <path_to_dir>\n```\n\nor to create it in the current repository:  \n\n```bash\nmlcf --create-userdir\n```\n\nThe list of files in the mlcf_home:  \n\n- paramters.ini : is a configuration file (not very useful for now). Later it will help the user to specify some wanted configurations.\n\n- logs : this repository have all the logs file which relate what happened during a session\n\n- ml : it contains models and trainers. models contains all your personnal neural network model you create with PyTorch and trainers contains all the scripts that can be executed by the train command. (It's kinda a main.py).\n\n- Training : it is create when a train run for the first time. It is the repository that will contains the logs and checkpoint of all training.\n\n- TrainingInfo.csv : It's a file which gives information about the past trainings. (The training name, the model used, the last lost, the current checkpoint, etc.)\n\n- boards : it contains all the tensorboard logs. So they can be stream with tensorboard to visualize the evolution of the loss and other metrics.\n\n- checkpoints : it contains all the checkpoints of your model during a training.\n\n---\n\n### build-dataset command\n\nTo build a dataset we need to download data. For now MLCF cannot do it alone.\nTo perform the downloading we commonly use [freqtrade](https://github.com/freqtrade/freqtrade/) library. To know more about it see [the freqtrade home documentation](https://www.freqtrade.io/en/stable/) and [how to download data with freqtrade](https://www.freqtrade.io/en/stable/data-download/).  \nHowever you can use any OHLCV data download with your own way.\n\n**Important: The data used by MLCF are OHLCV (incompatible with any other kind). The file format of the data MUST be a '.json'. The expected JSON string format is : [columns -> [index -> value]].**\n\nbuild-dataset usage:\n\n```\nusage: mlcf_home build-dataset [-h] --rawdata-dir RAWDATA_DIR --dataset-name DATASET_NAME [--pairs PAIRS [PAIRS ...]]\n                               [--timeframes {1m,3m,5m,15m,30m,1h,2h,4h,6h,8h,12h,1d,3d,1w,2w,1M,1y} [{1m,3m,5m,15m,30m,1h,2h,4h,6h,8h,12h,1d,3d,1w,2w,1M,1y} ...]] --input-width WIDTH\n                               [--target-width WIDTH] [--offset WIDTH] [--window-step STEP] [--n-interval NUMBER] [--index-column NAME] [--prop-tv PERCENTAGE] [--prop-v PERCENTAGE]\n                               [--indices INDICE [INDICE ...]] [--preprocess FUNCTION NAME] [--merge-pairs] [--n_category N_CATEGORY]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --rawdata-dir RAWDATA_DIR\n                        The directory of the raw data used to build the dataset. It will uses every file in the given directory\n  --dataset-name DATASET_NAME\n                        The name of the dataset file which will be created\n  --pairs PAIRS [PAIRS ...]\n                        The list of pairs from which the dataset is build. They are space-separated. (Default : BTC/BUSD)\n  --timeframes {1m,3m,5m,15m,30m,1h,2h,4h,6h,8h,12h,1d,3d,1w,2w,1M,1y} [{1m,3m,5m,15m,30m,1h,2h,4h,6h,8h,12h,1d,3d,1w,2w,1M,1y} ...]\n                        The list of timeframes from which the dataset is build. They are space-separated. (Default : 1d)\n  --input-width WIDTH   The width of the input part in the sliding window. Can also be seen as the sequence length of a neural network.\n  --target-width WIDTH  The width of the target part in the sliding window (Default: 1)\n  --offset WIDTH        The width of the offset part in the sliding window (Default: 0)\n  --window-step STEP    The step between each sliding window (Default: 1)\n  --n-interval NUMBER   The number of intervals by which the data will be divided. It allows to not have test and validation part just at the end (but at the end of each part) without having an overlap\n                        between the train and the evaluations parts. (Default: 1)\n  --index-column NAME   Name of the index column (commonly the time) (Default: 'date')\n  --prop-tv PERCENTAGE  The proportion of the test and validation part union from the data (Default: 0.1)\n  --prop-v PERCENTAGE   The proportion of the validation part from the test and the validation par union (Default: 0.3)\n  --indices INDICE [INDICE ...]\n                        List of indicators we want to add in the data (Optionnal)\n  --preprocess FUNCTION NAME\n                        List of pre processing function we want to use to pre process the data. Note: it's use independtly on each window\n  --merge-pairs         Merge the pairs together in order to extend the number of features.\n  --n_category N_CATEGORY\n                        Give a number of category in order to balance number of returns category in the training part of the dataset.\n```\nusage example:\n\nBuild a dataset named SimpleDataset from a raw_data which is in $HOME/Documents/data, with an input width of 55, and the default parameters, with the BTC/BUSD pair and a timeframe of '1d':  \n\n```bash\nmlcf build-dataset --rawdata-dir ~/Documents/data --dataset-name SimpleDataset --input-size 55\n```\n\nBuild a dataset named DatasetRSI which have the STOCH and RSI indicators :  \n\n```bash\nmlcf build-dataset --rawdata-dir ~/Documents/data --dataset-name DatasetRSI --input-size 55 --indices STOCH_SLOW RSI :\n```\n\nBuild a dataset named DatasetRSInorm which have the RSI indicator and a preprocessing apply on each windows named AutoNormalize :\n\n```bash\nmlcf build-dataset --rawdata-dir ~/Documents/data --dataset-name DatasetRSInorm --input-size 55 --indices RSI --preprocess AutoNormalize\n```\n\nBuild a dataset named DatasetETHBTC which have merged BTC and ETH features with a timeframe of one hour.\n\n```bash\nmlcf build-dataset --rawdata-dir ~/Documents/data --dataset-name DatasetETHBTC --input-size 55 --pairs BTC/BUSD ETH/BUSD --timeframes 1h --merge-pairs\n```\n\n---\n\n### train command\n\n```\nusage: mlcf_home train [-h] --trainer-name NAME [--training-name NAME] --dataset-name NAME [--param PARAM [PARAM ...]]\n\noptional arguments:\n  -h, --help                show this help message and exit\n  --trainer-name NAME       The name of the trainer file. IMPORTANT: \n                            the command call the method: train() inside \n                            the file given by the trainer file name.\n  --training-name NAME      The name of the training name, useful for \n                            logging, checkpoint etc.\n  --dataset-name NAME       The dataset name use for the training\n  --param PARAM [PARAM ...] The list of arguments for the trainer.\n                            IMPORTANT: The list must be in the form: key1=value1 key2=value2 key3=elem1,elem2,elem3\n```\n*Note: param argument doesn't work yet*\n\nusage example:\n\nBegin a training with a trainer script my_trainer.py, the training name is MyFirstTraining and the dataset DatasetRSInorm.wts:\n\n```\nmlcf train --trainer-name my_trainer --training-name MyFirstTraining --dataset-name DatasetRSInorm\n```\n\nInformation about trainers script:  \nA python trainer script is a script with the definition of a function called train which take 3 required arguments and 2 optionnals:  \n\n- project : it is our project home class. It manage the logs, checkpoints, it allow to have a training manager.\n\n- training_name : it is the name of this training.\n\n- wtst_data : it is the WTSTraining use for the training.\n\n- args and kwargs : will work with the argument param in order to pass personnal parameters.\n\nHere an example:  \n\n```python\nimport torch\n\nfrom ritl import add  \nadd(__file__, \"..\")  # ritl.add(__file__, \"..\") allows to import from \n                     # .../mlcf_home/ml so models.lstm can be imported in a \n                     # relative way.\n\nfrom models.lstm import LSTM\n\ndef train(\n    project,\n    training_name,\n    wtst_data,\n    *args,\n    **kwargs,\n):\n    list_columns = wtst_data.features\n    lstm = LSTM(30, list_columns)\n    lstm.init(torch.nn.L1Loss(),\n              torch.optim.SGD(lstm.parameters(), lr=0.01, momentum=0.9),\n              training_name=training_name,\n              project=project)\n    lstm.fit(dataset=wtst_data, n_epochs=5, batchsize=20)\n```\n\n## MLCF library\n\nIn this part is introduced all the current tools of MLCF.\n\n### Datatools\n\nThe datatools library provides :\n\n- ### WTSeries  \n    - ```WTSeries(window_width, raw_data (optional), window_step (optional) (default: 1))```  \n    WTSeries for Windowed Time Series is a list of window (dataframe) extract from a time series data.\n         - ```make_common_shuffle(other: WTSeries)```  \n         perform a common shuffle between the self WTSeries and an other. (it's used to shuffle target and inputs together)\n         - ```get_features() -> List[str]```  \n         Return the list of features (name of columns)\n         - ```width() -> int```  \n         Return the width of the window\n         - ```ndim() -> int```  \n         Return the number of features. (the number of columns)\n         - ```shape() -> (int, int)```  \n         Return the shape of a window : width, ndim\n         - ```size() -> (int, int, int)```  \n         Return the full shape/size of the wtseries : n_windows, width, ndim\n         - ```is_empty() -> bool```  \n         Return True if the wtseries is empty\n         - ```add_data(data: DataFrame)```  \n         From a raw data, perform the sliding window operation in order to add the windowed data in the list of window of the wtseries.\n         - ```add_one_window(window: DataFrame)```  \n         Add a window to the list of window\n         - ```add_window_data(window_data: WTSeries)```  \n         Add all the window of the given window_data wtseries to the current wtseries.\n- ### WTSTraining\n    - ```WTSTraining(input_width: int, target_width: int, partition: str, features: List[str], index_column: str, project: MlcfHome)```  \n    WTSTraining is used to divide the data into windows, to create input windows and target window, to create train part, validation part and test part. WTSTraining allow to handle time series data in a machine learning training. The component of the WTSTraining is the WTSeries which is a list of window extract from window sliding of a time series data.\n        - ```set_partition(partition: str)```  \n        Set the partition : 'train', 'validation' or 'test'.\n        - ```add_time_serie(dataframe: DataFrame, prop_tv: float, prop_v: float, do_shuffle: bool, n_interval: int, offset: int, window_step: int, preprocess: WTSeriesPreProcess)```  \n        This function perform the sliding window operation on the given dataframe to add train, validation and test data in the WTSTraining.\n            - ```prop_tv```: is the percentage of the evaluation part (the union of test and validation part)\n            - ```prop_v```: is the percentage of validation part amoung the evaluation part\n            - ```do_shuffle```: if true then perform a shuffle of the wtseries\n            - ```n_interval```: the number of interval from which the raw data will be divide. (to avoid to have test and val only at the end of the raw_data)\n            - ```offset```: the width between the input window and the target window\n            - ```window_step```: the step between each sliding window\n        - ```__call__() -> WTSeries, WTSeries```  \n        Return the input and target WTSeries of the current partition  \n        Example of use :\n        ```python\n        myWtstraining = WTSTraining(...)\n        myWtstraining.add_time_serie(...)\n        inputs, target = myWtstraining() # call : __call__() function\n        ```\n        - ```width() -> int, int```  \n        Return the width of the input and of the target\n        - ```ndim() -> int```  \n        Return the number of features\n        - ```copy(filter: List[str | bool] (optional) -> WTSTraining```  \n        Return the same WTSTraining and if filter is set then return the filtered WTSTraining\n- ### WTSTrainingDataset\n    - ```WTSTrainingDataset(dataset_path: Path, ...)```  \n    WTSTrainingDataset specify WTSTraining. The new argument is dataset_path which lead to the dataset file. It work the same way as WTSTraining.\n- ### WTSeriesPreProcess  \n    Available WTSeriesPreprocess:  \n    - Identity\n    - AutoNormalize\n- ### Indice  \n    Available indices:  \n    - ADX\n    - Plus Directional Indicator / Movement\n    - Minus Directional Indicator / Movement\n    - Aroon, Aroon Oscillator\n    - Awesome Oscillator\n    - Keltner Channel\n    - Ultimate Oscillator\n    - Commodity Channel Index\n    - RSI\n    - Inverse Fisher transform on RSI\n    - Inverse Fisher transform on RSI normalized\n    - Stochastic RSI\n    - MACD\n    - MFI\n    - ROC\n    - Bollinger Bands\n    - Bollinger Bands - Weighted\n    - EMA - Exponential Moving Average\n    - SMA - Simple Moving Average\n    - Parabolic SAR\n    - TEMA - Triple Exponential Moving Average\n    - Hilbert Transform Indicator - SineWave\n    - Percent growth (return)\n    - SMA1\n    - Log of SMA1\n    - Volatility\n    - And other... (pattern recognition)\n- ### datasetools  \n    Available function to handle WTSTrainingDataset:  \n    - \n    ```python\n    write_wtstdataset_from_raw_data(\n        project: MlcfHome,\n        rawdata_dir: Path,\n        dataset_name: str,\n        pairs: List[str],\n        timeframes: List[str],\n        input_width: int,\n        target_width: int,\n        offset: int,\n        window_step: int,\n        n_interval: int,\n        index_column: str,\n        prop_tv: float,\n        prop_v: float,\n        indices: List[Indice],\n        preprocess: WTSeriesPreProcess,\n        merge_pairs: bool\n       )\n    ```\n    This function create a .wtst WTSTrainingDataset file for a ```rawdata_dir``` directory.  \n\n### AiTools\n\nThe aitools library provides :\n\n- ### SuperModule  \n    SuperModule is an abstract class which allow to have a files, logs, and checkpoints managements.  \n    Here the function implemented by SuperModule:  \n    - ```init(loss, optimizer, device_str, metrics, training_name, project)```  \n    The init function allows to initialize a training\n    - ```init_load_checkpoint(loss, optimizer, device_str, metrics, project, training_name, resume_training)```  \n    This function allows to load the last checkpoint of the training having the same name given the project\n    - ```fit(dataset: WTSTraining, n_epochs: int, batchsize: int, shuffle: bool, evaluate: bool, checkpoint: bool)```  \n    The fit function allows to perform a training\n    - ```predict(bacth_input: Tensor)```  \n    Given a batch of input give the prediction batch in output\n    - ```summary()```  \n    Return the summary of the current module neural network\n    \n    \n    To write a module from this super module you need to specify : \n    - ```__init__```  \n    Here it allows to define the network and initialize the shape of the input of the network \n    - ```forward```  \n    Here it allows to define the feed forwarding of the neural network\n    - ```transform_x``` \n    Here it allows to define all shape transform the input data will go through\n    - ```transform_y``` \n    Here it allows to define all shape transform the target data will go through  \n    \n\n### EnvTools\n\nThe envtools library provides :\n\n- ### MlcfHome\n    Attributes available:  \n    - ```home_name```: the name of the directory name\n    - ```dir_prgm```: the path of the path from where the programm has been executed\n    - ```dir```: the path to the project directory\n    - ```data_dir```: the path where the WTSTrainingDataset are stored\n    - ```trainer_dir```: the path where trainers are stored\n    - ```models_dir```: the path where models are stored\n    - ```log```: logger of the project, will write every log in the files of the projects\n    - ```cfg```: link to the parameters.ini file stored in the project directory\n    - ```id```: information in text about the project\n\n\n*More details explanation are coming soon...*\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Guitheg/mlcf/tree/main",
    "keywords": "finance,machine-learning,ai,deep-learning,time-series,toolkit,pytorch,cryptocurrency,neural-networks,forecasting,trading-algorithm,investment,trading-agent,mlcf",
    "license": "GPLv3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlcf",
    "package_url": "https://pypi.org/project/mlcf/",
    "platform": null,
    "project_url": "https://pypi.org/project/mlcf/",
    "project_urls": {
      "Homepage": "https://github.com/Guitheg/mlcf/tree/main"
    },
    "release_url": "https://pypi.org/project/mlcf/1.1.2/",
    "requires_dist": [
      "tensorboard",
      "torch-tb-profiler",
      "geneticalgorithm2",
      "TA-Lib",
      "ritl",
      "pandas-ta"
    ],
    "requires_python": ">=3.7",
    "summary": "MLCF - Machine Learning Toolkit for Cryptocurrency Forecasting",
    "version": "1.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13989483,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1b1a11727b74f0a045382760913564573e736be95b809638494509d8d5b74320",
        "md5": "f738f43771cadf652ac1b62489588706",
        "sha256": "71463727c5a81a3cce2f87dfe0c16e060fe87ac8983c936940c569b691b489ee"
      },
      "downloads": -1,
      "filename": "mlcf-1.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f738f43771cadf652ac1b62489588706",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 61939,
      "upload_time": "2022-03-08T15:56:44",
      "upload_time_iso_8601": "2022-03-08T15:56:44.092480Z",
      "url": "https://files.pythonhosted.org/packages/1b/1a/11727b74f0a045382760913564573e736be95b809638494509d8d5b74320/mlcf-1.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0512cacea712d7d840ef115fc95445f26ad6696726b505500e0c0279442fc5fa",
        "md5": "cbd7848104e0556c1fe10f15bfc20308",
        "sha256": "7ef012a581010992ba2a9bb74aa680cbb5d02c8bff678455dca3e7778f918b90"
      },
      "downloads": -1,
      "filename": "mlcf-1.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "cbd7848104e0556c1fe10f15bfc20308",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 60188,
      "upload_time": "2022-03-08T15:56:45",
      "upload_time_iso_8601": "2022-03-08T15:56:45.413270Z",
      "url": "https://files.pythonhosted.org/packages/05/12/cacea712d7d840ef115fc95445f26ad6696726b505500e0c0279442fc5fa/mlcf-1.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}