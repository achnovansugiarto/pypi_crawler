{
  "info": {
    "author": "Ryan Febriansyah",
    "author_email": "15523163@students.uii.ac.id",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: Web Environment",
      "Framework :: Django",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# LRU Cache\n\n ![PyPI - Status](https://img.shields.io/pypi/status/lruheap) [![Downloads](https://pepy.tech/badge/lruheap)](https://pepy.tech/project/lruheap) [![Build Status](https://travis-ci.org/sodrooome/lru-cache.svg?branch=master)](https://travis-ci.org/sodrooome/lru-cache) [![codecov](https://codecov.io/gh/sodrooome/lru-cache/branch/master/graph/badge.svg)](https://codecov.io/gh/sodrooome/lru-cache)\n\nPackage for tracking store in-data memory using replacement cache algorithm / LRU cache. The Priority of storing or removing the data based on Min-Max heap algorithm or basic priority queue instead using OrderedDict module that provided by Python.\n\n## Purpose\n\nThe purpose of using this package itself is at least to be able to dynamically tracking. inserting, and removing least frequently used in-data memory or in an element. Another purposes, with the use of python decorator or the method looks like, it's also possible to figure it out whether the data in the cache is full or not (it's called **LRU eviction**), since Min-Max heap algorithm is using **O(1)** complexity for basic insertion and searching, it's also possible to efficiently accessing the store in-data memory based on most frequently used method.\n\n## Usage\n\nLRUCache only works in Python version 3.5 and above, you can install it with :\n\n```sh\npip3 install lruheap\n```\n\nThere is a little explanation regarding the use of this LRU cache. You can see at this simple configuration and explanation for using several method that provided by this package.\n\n### `LRUCache(capacity=128, seconds=60*15)`\n\nClass constructor for initialize LRUCache method with maximum capacity of cache is 128 and maximum duration of cache is 15 minutes when you don't initialize at first. For example:\n\n```python\nfrom lru.lrucache import LRUCache\n\nfoo = LRUCache(3)\n\n# or you can set param argument\nfoo = LRUCache(capacity=3, seconds=5*15)\n```\n\n### `set()`\n\nSet an objects that wants to be cached in cache element, given the `key` parameters and `value` parameters as integer. For example:\n\n```python\nfoo.set(1, \"foobar\")\nfoo.set(2, \"bar\")\n```\n\n### `get()`\n\nGet the objects based on their key in cache element and access time, given the `key` parameters as integer. This `get()` method can also be used to tracking which objects are often called which will later be identified as recently used objects in a cache element, an object that is often called by this method will be placed in front of the cache element by using `get_lru_element()`. For example:\n\n```python\nfoo.get(1)\nfoo.get(1) # you can iterate for calling an object\nfoo.get(2)\n```\n\n### `get_dict()`\n\nMethod for returned a all dictionary of an object in cache element. For example:\n\n```python\nfoo.get_dict()\n```\n\n### `get_duration()`\n\nMethod for getting duration of cache, return `True` if the duration is exceed for expired time otherwise return `False` when the duration is even or below the expired time. The expired time set as 3600 seconds. For example:\n\n```python\n# will return True if the duration\n# of cache is still under the\n# expired time\nfoo.get_duration()\n```\n\n### `get_lru_element()`\n\nMethod for retrieved an object based on their key in cache element and the duration when accessing onto the dictionary. \nIf the object is not called by the `get()` method, then objects that have short time for accessing onto dictionary will be placed in beginning of the cache element, if the object is called by the `get()` method, it will placed depending how many objects are called. In this case, this called as recently used. For example:\n\n```python\nfoo.get_lru_element()\n```\n\n### `get_capacity()`\n\nGet cache capacity, return `True` if the cache is full otherwiser return `False` when the cache is not full. For example:\n\n```python\nfoo.get_capacity()\n```\n\n### `get_cache()`\n\nGet cache in element based on their key, return `True` if the element has a key, otherwise return `False` when element hasn't a key. Given the `key` parameters as integer. For example:\n\n```python\nfoo.get_cache(1)\n```\n\n### `get_ttl()`\n\nGet time-to-live (TTL) duration for cache, will return a value, where the value is the remaining time from the cache duration that has been set previously. Given the `key` parameters as integer. Th countdown time will be reduced by one second according to the cache duration that we have set before, if you set it to within 5 seconds, when using this method it will display a value of 4 which means its the remaining duration of our cache, and so on until the result displayed is set as`None`. For example:\n\n```python\nfoo.get_ttl(1)\n```\n\n### `clear_all()`\n\nRemove all cache in element. For example:\n\n```python\nfoo.clear_all()\n```\n\n### `clear_cache_key()`\n\nRemove cache in element based on their key. Given the `key` as parameters for remove the cache objects. For example:\n\n```python\n# this will remove cache\n# object based on first index\nfoo.clear_cache_key(1)\n```\n\n### `is_empty()`\n\nCheck whether the current cache in element is empty or not. Will return `True` if the cache element is empty and `False` when the cache element is full of objects. For example:\n\n```python\nfoo.is_empty()\n```\n\n### `@lru_cache(capacity=128)`\n\nPython decorators using LRUCache classes for cache an object within a function. Default capacity is 128 if you not define it. For example:\n\n```python\nfrom lru.decorators import lru_cache\n\n@lru_cache(capacity=5)\ndef test_lru(x):\n    print(\"Calling f(\" + str(x) + \")\")\n    return x\n\ntest_lru.set(1, \"foo\")\ntest_lru.set(2, \"test\")\ntest_lru.set(3, \"foos\")\ntest_lru.set(4, \"fc\")\ntest_lru.set(5, \"set\")\ntest_lru.get_capacity()\n```\n\n### `@lru_cache_time(capacity=128, seconds=60*15)`\n\nPython decorators for LRUCache classes using expired cached time. This is an mock only, **probably** not ready to bump into major version, \nif you want to try it and there is an error or an unexpected result, please raise the issue. For example :\n\n```python\nfrom lru.decorators import lru_cache_time\n\n@lru_cache_time(capacity=5, seconds=15)\ndef test_lru(x):\n    print(\"Calling f(\" + str(x) + \")\")\n    return x\n\ntest_lru.set(1, \"foo\")\ntest_lru.set(2, \"test\")\n```\n\nThe difference between set duration of cache if using decorators or not lies when we set the value of the duration cache. By using these `@lru_cache_time` decorators at least it will compact and dynamically clear the cache if the duration exceeds of the maximum duration (15 minutes).\n\n### Enable `thread_safe` parameter\n\nBy enabling `thread_safe` parameter into `True`, it will be possible to safely to call a function together. For example, if we create a shared task (functions a and b) where the shared task invokes a resource such as object from function c, then the object can safely be called and can be execute on both functions a and b (thus, its called a deadlock if we dont use `thread_safe` parameter to execute two functions from one resource). **As for concerns**, the use of `thread_safe` might be reduce the performance. For example:\n\n```python\ntest_lru = LRUCache(3, 1, thread_safe=True)\n```\n\n## Further Example\n\n### Backported with Django\n\nFor further example, hopefully this package can be backported with Python web frameworks such as Django or Flask which can be implemented and supported in the JSON field area, since the return of this LRUCache value is in the form of dictionary which is very common in JSON type. For simple usage in Django, you must setting the LRUCache in installed application within Django settings like this :\n\n```python\n\nINSTALLED_APPS = [\n    ...\n    'lru',\n    ...\n]\n```\n\nAnd then you can create some function for wrapped the objects that you want to cached with `JsonResponse` from Django API. The results of this function will return your objects in the form of a JSON dict at your web browser :\n\n```python\n# views.py\n\nfrom django.http import JsonResponse\nfrom lru.lrucache import LRUCache\n\n# set the capacity of LRUCache\ntest = LRUCache(3)\n\n# wrapped our object\ndef test_lru(request):\n    test.set(1, \"foo\")\n    test.set(2, \"bar\")\n    return JsonResponse(test.get_dict(), safe=False)\n```\n\nDont forget to passing our function method into Django urls parameters :\n\n```python\n# urls.py\n\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('', views.test_lru),\n]\n```\n\nAfter that you can run the django server and see it in a web browser based on your endpoint url. **Please remember**, at the moment, each time after you wrapped the object with `JsonResponse`, you need to set the `safe` parameter to `False` because when you set an object with LRUCache, we don't set it to dict type, while the output from `JsonResponse` itself is dict type.\n\n### Backported with Flask\n\nFor simple usage in Flask probably occured by using the LRUCache decorators on top of the router flask decorator. Ensure you've already installed Flask with `pip install flask` and then create new file such as the following example :\n\n```python\n# app.py\n\nfrom flask import Flask\nfrom lru.decorators import lru_cache_time\n\napp = Flask(__name__)\n\n@lru_cache_time(seconds=60)\n@app.route(\"/\")\ndef hello():\n    return \"Hello World!\"\n\nif __name__ == \"__main__\":\n    app.run()\n```\n\nAfter that, you can run the Flask app with the command like `python app.py` and then open it in the browser according to the existing localhost. **As for noted**, the use of the `@lru_cache_time` decorators itself will set the cache capacity and duration of the cache. Since here we only return plain text / html with *Hello World* output, i'm assume we don't need to set the cache capacity. \n\nBasically, in this example there's a several confusion and caveats :\n\n- After the cache duration that we have set exceeds the maximum limit of expired time, will the value of the function be deleted or not? cause the output is not JSON type (probably i'll given a try to using the `jsonify` method or `make_response`).\n- Secondly, whether the value it still be stored and appears in a web browser or not, if we stop the flask server.\n- Using LRUCache decorators in Django cause a problem in the clickjacking middleware section, we can temporarily set `X-FRAME-OPTIONS` to `DENY` in `settings.py` file inside Django root project if we want to use LRUCache decorators.\n\nLikewise, the use of LRUcache in Django or Flask itself is very limited at this time, because there is a contradiction that is we can't set the objects dynamically, and another obstacle is that if the object that we set is not in the dict type, we need to do the object hashing. Normally, using LRUCache at the web-environment level can be done with the following assumptions like :\n\n- You want to build web-based streaming services\n- You do object modeling in the database, such as creating objects for a song that is least or most recently heard.\n- You set the song object with LRUCache, so that every time you open your site, that song will appear.\n\nAnother example of using this LRU cache is that you want to display a book that is most often searched by keywords, dynamically remove an object that is not or least used, store data of users who frequently visit our site and many others.\n\n## Running the tests\n\nFor running the test, you can use command `python -m unittest tests`\n\n## Why using this, instead of X?\n\nWhy use object cache level instead of filtering method or get method based on API? Ideally, cache is fast. and just fast as storage, reading or accessing the data from a cache takes less time than reading the data from something else. in example if you use filtering methods, you are doing accessing and getting the object from your database. Otherwise, caching the object improves performance by keeping recent or most used data in memory locations rather we're going to use computationally object from database.\n\n## Roadmap\n\n- [x] Use classes as decorators for caching the objects\n- [x] Add expired time for caching objects\n- [x] Add thread safe parameter\n- [ ] Scale the LRUCache capacity\n- [ ] Backported and integrated with Django request and response\n- [x] Write unittest for LRUCache\n- [ ] Improve code coverage up to 90 %\n\n\n## Contributions\n\nSince this package is still being under developed, any contributions are much welcomed and you can read on contribution page.\n\n## License\n\nThis package is licensed under the MIT License.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/sodrooome/lru-cache",
    "keywords": "cache lru",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lruheap",
    "package_url": "https://pypi.org/project/lruheap/",
    "platform": "",
    "project_url": "https://pypi.org/project/lruheap/",
    "project_urls": {
      "Homepage": "https://github.com/sodrooome/lru-cache"
    },
    "release_url": "https://pypi.org/project/lruheap/1.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.5",
    "summary": "Python decorators for store in-data memory using LRU cache",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9430921,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "23fd76f6be19ed8f75030a1b854e3d3b5460d5da5c592faa2196bb320e71dce7",
        "md5": "e33163999d54fb6577a2a74924e20b06",
        "sha256": "1a37d53013d2813563a2496d60ed3564114e93d75dbd53a11928ad4fab0e441b"
      },
      "downloads": -1,
      "filename": "lruheap-1.0.1.linux-x86_64.tar.gz",
      "has_sig": false,
      "md5_digest": "e33163999d54fb6577a2a74924e20b06",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 14897,
      "upload_time": "2020-12-15T15:29:13",
      "upload_time_iso_8601": "2020-12-15T15:29:13.111980Z",
      "url": "https://files.pythonhosted.org/packages/23/fd/76f6be19ed8f75030a1b854e3d3b5460d5da5c592faa2196bb320e71dce7/lruheap-1.0.1.linux-x86_64.tar.gz",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9380fceb5919acbc9d81d1e98add4a7074cde2769698bcfac3b7eb0a6e581b31",
        "md5": "3b6457c68269b90ca65cbbc6b9f08832",
        "sha256": "6c619b035e06fd749dec73d6dd37bc55d2d8b734272a390bdd635ab2b65fca7d"
      },
      "downloads": -1,
      "filename": "lruheap-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3b6457c68269b90ca65cbbc6b9f08832",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5",
      "size": 11628,
      "upload_time": "2020-12-15T15:31:37",
      "upload_time_iso_8601": "2020-12-15T15:31:37.983003Z",
      "url": "https://files.pythonhosted.org/packages/93/80/fceb5919acbc9d81d1e98add4a7074cde2769698bcfac3b7eb0a6e581b31/lruheap-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}