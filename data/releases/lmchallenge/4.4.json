{
  "info": {
    "author": "Microsoft Corporation",
    "author_email": "swiftkey-deep@service.microsoft.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Language Model Challenge (LMChallenge)\n[![Build Status](https://travis-ci.com/Microsoft/LMChallenge.svg?token=PsuQKRDL8Qs6yfLsqpTp&branch=master)](https://travis-ci.com/Microsoft/LMChallenge)\n\nA set of tools to evaluate language models for typing.\n\nThis is a guide for users of LM Challenge. You may also want to see:\n\n - [data formats](doc/formats.md) for integrators\n - [dev notes](doc/dev.md) for developers wishing to extend LM Challenge\n\n## What is LM Challenge for?\n\nIt is really quite hard to test language model performance. Some models output probabilities, others scores; some model words, others morphemes, characters or bytes. Vocabulary coverage varies. Comparing them in a fair way is tough... So in LM Challenge we have some very simple 'challenge games' that evaluate (and help compare) language models over a test corpus.\n\n## Getting Started\n\nWe include support for building a Docker image that makes it easier to get started (which also documents [our dependencies](Dockerfile), in case you want to get set up natively).\n\n```bash\ndocker build --rm -t lmchallenge .\n```\n\n## The Challenges\n\nAn _LM challenge game_ is a runnable Python module that evaluates one or more _language models_ on some task, over some _test text_.\n\nThe **challenge games** we have are:\n\n - `wp` - Word Prediction Challenge - a next-word-prediction task (generates Hit@N results)\n - `tc` - Text Completion Challenge - a text completion task (generates KSPC results)\n - `ic` - Input Correction Challenge - a correction task (generates accuracy results)\n\n**Test text** is pure text data (as typed & understood by real actual humans!) LM Challenge does not define test text - we expect it to be provided. This is the other thing you need to decide on in order to evaluate a _language model_.\n\nA **language model** is an executable process that responds to commands from a _LM challenge game_ in a specific text format, usually comprising of a pre-trained model of the same language as the _test text_.\n\n### Word Prediction `wp`\n\nThe Word Prediction task scans through words in the test text, at each point querying the language model for next-word predictions.\n\nThe aim of the model is to predict the correct next word before other words (i.e. with as low a rank as possible). Statistics available from `wp` include `Hit@N` (ratio of correct predictions obtained with rank below `N`) and rank-weighted scores such as `SRR` (sum reciprocal rank - the sum total of `1/rank` over all words).\n\n`wp` may be used as follows:\n\n```bash\ncat data.txt | lmc run \"lm ...\" wp > wp.log\nlmc stats < wp.log\nlmc pretty < wp.log\n```\n\nThe first command creates a log file of the results of running the predictor over the test text. Subsequent commands provide ways of analysing those logs - aggregating summary stats and providing a colorful rendering of the behaviour of the predictor.\n\n### Text Completion `tc`\n\nThe Text Completion task emulates a simple typist entering the text perfectly, and selecting predictions whenever possible. The typist queries the model for predictions at the current point. If a prediction below a certain rank matches the text, it is 'selected', and the process continues after the end of the selected prediction. If no match is found, the typist enters the next character from the test text, then repeats the process.\n\nThe aim of the model is to predict as much of the next word, part-word or sequence of words as possible. Statistics available from `tc` include `pcpc` (ratio of predictions to characters entered) and `kpc` (number of prediction selection or character typing events per character entered).\n\n`tc` may be used as follows:\n\n```bash\ncat data.txt | lmc run \"lm ...\" tc > tc.log\nlmc stats < tc.log\nlmc pretty < tc.log\n```\n\n### Input Correction `ic`\n\nThe Input Correction task emulates a sloppy typist entering text, using the language model to correct input after it has been typed. This challenge requires a list of words to use as correction candidates for corrupted words (which should be a large set of valid words in the target language.) Text from the data source is first corrupted (as if by a sloppy typist). The corrupted text is fed into a search for nearby candidate words, which are scored according to the language model under evaluation. The evaluator measures corrected, un-corrected and mis-corrected results.\n\nThe aim of the model is to assign high score to the correct word, and low score to all other words. We evaluate this by mixing the score from the language model with an _input score_ for each word, then ranking based on that - it the top-ranked prediction is the correct word, this example was a success, otherwise it counts as a failure. The _input score_ is the log-probability of the particular corrupted text being produced from this word, in the same error model that was used to corrupt the true word. In order to be robust against different ranges of scores from language models, we must optimize the _input_ and _language model_ mixing parameters before counting statistics: this is done with `lmc ic-opt` (or can be done automatically in `lmc stats`). The JSON output of `lmc ic-opt` is used when running `lmc stats` or `lmc pretty` in order to fully specify the model under investigation.\n\nStatistics available include `accuracy` (ratio of correct words after correction), `miscorrected` (proportion of errors introduced to correct words) and the combined metric `improvement` (ratio change in number of errors).\n\n`ic` may be used as follows:\n\n```bash\ncat data.txt | lmc run \"lm ...\" ic words.txt > ic.log\nlmc ic-opt < ic.log > ic.opt\nlmc stats -a ic.opt < ic.log\nlmc pretty -a ic.opt < ic.log\nlmc page -a ic.opt < ic.log > ic.html\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n",
    "description_content_type": null,
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Microsoft/LMChallenge",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lmchallenge",
    "package_url": "https://pypi.org/project/lmchallenge/",
    "platform": "",
    "project_url": "https://pypi.org/project/lmchallenge/",
    "project_urls": {
      "Homepage": "https://github.com/Microsoft/LMChallenge"
    },
    "release_url": "https://pypi.org/project/lmchallenge/4.4/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "LM Challenge - A library & tools to evaluate predictive language models.",
    "version": "4.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6294719,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c6b2531568d931719adfa5ec2f7774260ed63e1a9eef89d421818d793de159c1",
        "md5": "868ca7e5e48862536660b13a716d88e7",
        "sha256": "2d68f80bca503112f4de75a3d34a8aae0e4f1c1f5a768b4ffbf72865a11cc4fb"
      },
      "downloads": -1,
      "filename": "lmchallenge-4.4.tar.gz",
      "has_sig": true,
      "md5_digest": "868ca7e5e48862536660b13a716d88e7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 30293,
      "upload_time": "2017-10-06T13:29:40",
      "upload_time_iso_8601": "2017-10-06T13:29:40.999405Z",
      "url": "https://files.pythonhosted.org/packages/c6/b2/531568d931719adfa5ec2f7774260ed63e1a9eef89d421818d793de159c1/lmchallenge-4.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}