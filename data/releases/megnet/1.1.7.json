{
  "info": {
    "author": "Chi Chen",
    "author_email": "chc273@eng.ucsd.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering :: Chemistry",
      "Topic :: Scientific/Engineering :: Information Analysis",
      "Topic :: Scientific/Engineering :: Physics",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "[![License](https://img.shields.io/github/license/materialsvirtuallab/megnet)]()\n[![Build Status](https://travis-ci.org/materialsvirtuallab/megnet.svg?branch=master)](https://travis-ci.org/materialsvirtuallab/megnet)\n[![CircleCI](https://circleci.com/gh/materialsvirtuallab/megnet.svg?style=shield)](https://circleci.com/gh/materialsvirtuallab/megnet)\n[![Coverage Status](https://coveralls.io/repos/github/materialsvirtuallab/megnet/badge.svg?branch=master)](https://coveralls.io/github/materialsvirtuallab/megnet?branch=master&service=github)\n\n# Table of Contents\n* [Introduction](#introduction)\n* [MEGNet Framework](#megnet-framework)\n* [Installation](#installation)\n* [Usage](#usage)\n* [Datasets](#datasets)\n* [Implementation details](#implementation-details)\n* [Computing requirements](#computing-requirements)\n* [Known limitations](#limitations)\n* [Contributors](#contributors)\n* [References](#references)\n\n<a name=\"introduction\"></a>\n# Introduction\n\nThis repository represents the efforts of the [Materials Virtual Lab](http://www.materialsvirtuallab.org) \nin developing graph networks for machine learning in materials science. It is a \nwork in progress and the models we have developed thus far are only based on \nour best efforts. We welcome efforts by anyone to build and test models using \nour code and data, all of which are publicly available. Any comments or \nsuggestions are also welcome (please post on the Github Issues page.)\n\nA web app using our pre-trained MEGNet models for property prediction in \ncrystals is available at http://megnet.crystals.ai.\n\n<a name=\"megnet-framework\"></a>\n# MEGNet framework\n\nThe MatErials Graph Network (MEGNet) is an implementation of DeepMind's graph \nnetworks[1] for universal machine learning in materials science. We have \ndemonstrated its success in achieving very low prediction errors in a broad \narray of properties in both molecules and crystals (see \n[\"Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals\"](https://doi.org/10.1021/acs.chemmater.9b01294)[2]).\n\nBriefly, Figure 1 shows the sequential update steps of the graph network, \nwhereby bonds, atoms, and global state attributes are updated using information\nfrom each other, generating an output graph.\n\n![GraphModel diagram](resources/model_diagram_small.jpg)\n<div align='center'><strong>Figure 1. The graph network update function.</strong></div>\n\nFigure 2 shows the overall schematic of the MEGNet. Each graph network module \nis preceded by two multi-layer perceptrons (known as Dense layers in Keras \nterminology), constituting a MEGNet block. Multiple MEGNet blocks can be \nstacked, allowing for information flow across greater spatial distances. The \nnumber of blocks required depend on the range of interactions necessary to \npredict a target property. In the final step, a `set2set` is used to map the \noutput to a scalar/vector property.\n\n![GraphModel architecture](resources/model_arch_small.jpg)\n<div align='center'><strong>Figure 2. Schematic of MatErials Graph Network.</strong></div>\n\n<a name=\"installation\"></a>\n# Installation\n\nMegnet can be installed via pip for the latest stable version:\n\n```bash\npip install megnet\n```\n\nFor the latest dev version, please clone this repo and install using:\n\n```bash\npython setup.py develop\n```\n\n<a name=\"usage\"></a>\n# Usage\n\nOur current implementation supports a variety of use cases for users with \ndifferent requirements and experience with deep learning. Please also visit \nthe [notebooks directory](notebooks) for Jupyter notebooks with more detailed code examples.\n\n## Using pre-built models\n\nIn our work, we have already built MEGNet models for the QM9 data set and \nMaterials Project dataset. These models are provided as serialized HDF5+JSON \nfiles. Users who are purely interested in using these models for prediction \ncan quickly load and use them via the convenient `MEGNetModel.from_file`\nmethod. These models are available in the `mvl_models` folder of this repo. \nThe following models are available:\n\n* QM9 molecule data:\n    - HOMO: Highest occupied molecular orbital energy\n    - LUMO: Lowest unoccupied molecular orbital energy\n    - Gap: energy gap\n    - ZPVE: zero point vibrational energy\n    - µ: dipole moment\n    - α: isotropic polarizability\n    - \\<R2\\>: electronic spatial extent\n    - U0: internal energy at 0 K\n    - U: internal energy at 298 K\n    - H: enthalpy at 298 K\n    - G: Gibbs free energy at 298 K\n    - Cv: heat capacity at 298 K\n    - ω1: highest vibrational frequency.\n* Materials Project data:\n    - Formation energy from the elements\n    - Band gap\n    - Log 10 of Bulk Modulus (K)\n    - Log 10 of Shear Modulus (G)\n\nThe MAEs on the various models are given below:\n\n### Performance of QM9 MEGNet-Simple models\n\n| Property | Units      | MAE   |\n|----------|------------|-------|\n| HOMO     | eV         | 0.043 |\n| LUMO     | eV         | 0.044 |\n| Gap      | eV         | 0.066 |\n| ZPVE     | meV        | 1.43  |\n| µ        | Debye      | 0.05  |\n| α        | Bohr^3     | 0.081 |\n| \\<R2\\>   | Bohr^2     | 0.302 |\n| U0       | eV         | 0.012 |\n| U        | eV         | 0.013 |\n| H        | eV         | 0.012 |\n| G        | eV         | 0.012 |\n| Cv       | cal/(molK) | 0.029 |\n| ω1       | cm^-1      | 1.18  |\n\n### Performance of MP-2018.6.1\n\n| Property | Units      | MAE   |\n|----------|------------|-------|\n| Ef       | eV/atom    | 0.028 |\n| Eg       | eV         | 0.33  |\n| K_VRH    | log10(GPa) | 0.050 |\n| G_VRH    | log10(GPa) | 0.079 |\n\n### Performance of MP-2019.4.1\n\n| Property | Units      | MAE   |\n|----------|------------|-------|\n| Ef       | eV/atom    | 0.026 |\n| Efermi   | eV         | 0.288 |\n\nNew models will be added as they are developed in the [mvl_models](mvl_models)\nfolder. Each folder contains a summary of model details and benchmarks. For\nthe initial models and bencharmks comparison to previous models, \nplease refer to [\"Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals\"](https://doi.org/10.1021/acs.chemmater.9b01294)[2]. \n\nBelow is an example of crystal model usage:\n\n```python\nfrom megnet.utils.models import load_model\nfrom pymatgen import Structure, Lattice\n\n# load a model in megnet.utils.models.AVAILABLE_MODELS\nmodel = load_model(\"logK_MP_2018\") \n\n# We can construct a structure using pymatgen\nstructure = Structure(Lattice.cubic(3.167), \n            ['Mo', 'Mo'], [[0, 0, 0], [0.5, 0.5, 0.5]])\n\n\n# Use the model to predict bulk modulus K. Note that the model is trained on\n# log10 K. So a conversion is necessary.\npredicted_K = 10 ** model.predict_structure(structure).ravel()\nprint('The predicted K for {} is {} GPa'.format(structure.formula, predicted_K[0]))\n\n```\nA full example is in [notebooks/crystal_example.ipynb](notebooks/crystal_example.ipynb). \n\nFor molecular models, we have an example in \n[notebooks/qm9_pretrained.ipynb](notebooks/qm9_pretrained.ipynb). \nWe support prediction directly from a pymatgen molecule object. With a few more\nlines of code, the model can predict from `SMILES` representation of molecules,\nas shown in the example. It is also straightforward to load a `xyz` molecule \nfile with pymatgen and predict the properties using the models. However, the \nusers are generally not advised to use the `qm9` molecule models for other \nmolecules outside the `qm9` datasets, since the training data coverage is \nlimited.\n\nBelow is an example of predicting the \"HOMO\" of a smiles representation\n\n```python\nfrom megnet.utils.molecule import get_pmg_mol_from_smiles\nfrom megnet.models import MEGNetModel\n\n# same model API for molecule and crystals\nmodel = MEGNetModel.from_file('mvl_models/qm9-2018.6.1/HOMO.hdf5')\n# Need to convert SMILES into pymatgen Molecule\nmol = get_pmg_mol_from_smiles(\"C\")\nmodel.predict_structure(mol)\n```\n\n## Training a new MEGNetModel from structures\n\nFor users who wish to build a new model from a set of crystal structures with \ncorresponding properties, there is a convenient `MEGNetModel` class for setting\nup and training the model. By default, the number of MEGNet blocks is 3 and the\natomic number Z is used as the only node feature (with embedding).\n\n```python\nfrom megnet.models import MEGNetModel\nfrom megnet.data.graph import GaussianDistance\nfrom megnet.data.crystal import CrystalGraph\nimport numpy as np\n\nnfeat_bond = 10\nr_cutoff = 5\ngaussian_centers = np.linspace(0, r_cutoff + 1, nfeat_bond)\ngaussian_width = 0.5\ngraph_converter = CrystalGraph(cutoff=r_cutoff)\nmodel = MEGNetModel(graph_converter=graph_converter, centers=gaussian_centers, width=gaussian_width)\n\n# Model training\n# Here, `structures` is a list of pymatgen Structure objects.\n# `targets` is a corresponding list of properties.\nmodel.train(structures, targets, epochs=10)\n\n# Predict the property of a new structure\npred_target = model.predict_structure(new_structure)\n```\nNote that for realistic models, the `nfeat_bond` can be set to 100 and `epochs` can be 1000. \nIn some cases, some structures within the training pool may not be valid (containing isolated atoms),\nthen one needs to use `train_from_graphs` method by training only on the valid graphs. \n\nFollowing the previous example, \n```python\nmodel = MEGNetModel(graph_converter=graph_converter, centers=gaussian_centers, width=gaussian_width)\ngraphs_valid = []\ntargets_valid = []\nstructures_invalid = []\nfor s, p in zip(structures, targets):\n    try:\n        graph = model.graph_converter.convert(s)\n        graphs_valid.append(graph)\n        targets_valid.append(p)\n    except:\n        structures_invalid.append(s)\n\n# train the model using valid graphs and targets\nmodel.train_from_graphs(graphs_valid, targets_valid)\n```\nFor model details and benchmarks, please refer to [\"Graph Networks as a Universal Machine Learning Framework for Molecules and Crystals\"](https://doi.org/10.1021/acs.chemmater.9b01294)[2]\n\n### Pre-trained elemental embeddings\n\nA key finding of our work is that element embeddings from trained formation \nenergy models encode useful chemical information that can be transferred \nlearned to develop models with smaller datasets (e.g. elastic constants, \nband gaps), with better converegence and lower errors. These embeddings are \nalso potentially useful in developing other ML models and applications. These \nembeddings have been made available via the following code:\n\n```python\nfrom megnet.data.crystal import get_elemental_embeddings\n\nel_embeddings = get_elemental_embeddings()\n```\n\nAn example of transfer learning using the elemental embedding from formation \nenergy to other models, please check [notebooks/transfer_learning.ipynb](notebooks/transfer_learning.ipynb).\n\n## Customized Graph Network Models\n\nFor users who are familiar with deep learning and Keras and wish to build \ncustomized graph network based models, the following example outlines how a \ncustom model can be constructed from `MEGNetLayer`, which is essentially our \nimplementation of the graph network using neural networks:\n\n```python\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom megnet.layers import MEGNetLayer, Set2Set\n\nn_atom_feature= 20\nn_bond_feature = 10\nn_global_feature = 2\n\n# Define model inputs\nint32 = 'int32'\nx1 = Input(shape=(None, n_atom_feature)) # atom feature placeholder\nx2 = Input(shape=(None, n_bond_feature)) # bond feature placeholder\nx3 = Input(shape=(None, n_global_feature)) # global feature placeholder\nx4 = Input(shape=(None,), dtype=int32) # bond index1 placeholder\nx5 = Input(shape=(None,), dtype=int32) # bond index2 placeholder\nx6 = Input(shape=(None,), dtype=int32) # atom_ind placeholder\nx7 = Input(shape=(None,), dtype=int32) # bond_ind placeholder\nxs = [x1, x2, x3, x4, x5, x6, x7]\n\n# Pass the inputs to the MEGNetLayer layer\n# Here the list are the hidden units + the output unit, \n# you can have others like [n1] or [n1, n2, n3 ...] if you want. \nout = MEGNetLayer([32, 16], [32, 16], [32, 16], pool_method='mean', activation='relu')(xs)\n\n# the output is a tuple of new graphs V, E and u\n# Since u is a per-structure quantity, \n# we can directly use it to predict per-structure property\nout = Dense(1)(out[2])\n\n# Set up the model and compile it!\nmodel = Model(inputs=xs, outputs=out)\nmodel.compile(loss='mse', optimizer='adam')\n```\n\nWith less than 20 lines of code, you have built a graph network model that is \nready for materials property prediction!\n\n<a name=\"implementation-details\"></a>\n# Implementation details\n\nGraph networks[1] are a superclass of graph-based neural networks. There are a\nfew innovations compared to conventional graph-based neural neworks. \n\n* Global state attributes are added to the node/edge graph representation. \n  These features work as a portal for structure-independent features such as \n  temperature, pressure etc and also are an information exchange placeholder \n  that facilitates information passing across longer spatial domains. \n* The update function involves the message interchange among all three levels \n  of information, i.e., the node, bond and state information. It is therefore a\n  highly general model.\n\nThe `MEGNet` model implements two major components: (a) the `graph network`\nlayer and (b) the `set2set` layer.[3] The layers are based on \n[keras](https://keras.io/) API and is thus compatible with other keras modules. \n\nDifferent crystals/molecules have different number of atoms. Therefore it is \nimpossible to use data batches without padding the structures to make them \nuniform in atom number. `MEGNet` takes a different approach. Instead of making \nstructure batches, we assemble many structures into one giant structure, and \nthis structure has a vector output with each entry being the target value for \nthe corresponding structure. Therefore, the batch number is always 1. \n\nAssuming a structure has N atoms and M bonds, a structure graph is represented \nas **V** (nodes/vertices, representing atoms), **E** (edges, representing bonds)\nand **u** (global state vector). **V** is a N\\*Nv matrix. **E** comprises of a \nM\\*Nm matrix for the bond attributes and index pairs (rk, sk) for atoms \nconnected by each bond. **u** is a vector with length Nu. We vectorize rk and \nsk to form `index1` and `index2`, both are vectors with length M. In summary, \nthe graph is a data structure with **V** (N\\*Nv), **E** (M\\*Nm), **u** \n(Nu, ), `index1` (M, ) and `index2` (M, ). \n\nWe then assemble several structures together. For **V**, we directly append the\natomic attributes from all structures, forming a matrix (1\\*N'\\*Nv), where \nN' > N. To indicate the belongingness of each atom attribute vector, we use a \n`atom_ind` vector. For example if `N'=5` and the first 3 atoms belongs to the \nfirst structure and the remainings the second structure, our `atom_ind` vector \nwould be `[0, 0, 0, 1, 1]`. For the bond attribute, we perform the same \nappending method, and use `bond_ind` vector to indicate the bond belongingness. \nFor `index1` and `index2`, we need to shift the integer values. For example, \nif `index1` and `index2` are `[0, 0, 1, 1]` and `[1, 1, 0, 0]` for structure 1\nand are `[0, 0, 1, 1]` and `[1, 1, 0, 0]` for structure two. The assembled\nindices are `[0, 0, 1, 1, 2, 2, 3, 3]` and `[1, 1, 0, 0, 3, 3, 2, 2]`. Finally,\n**u** expands a new dimension to take into account of the number of structures,\nand becomes a 1\\*Ng\\*Nu tensor, where Ng is the number of structures. `1` is \nadded as the first dimension of all inputs because we fixed the batch size to\nbe 1 (1 giant graph) to comply the keras inputs requirements. \n\nIn summary the inputs for the model is **V** (1\\*N'\\*Nv), **E** (1\\*M'\\*Nm), \n**u** (1\\*Ng\\*Nu), `index1` (1\\*M'), `index2` (1\\*M'), `atom_ind` (1\\*N'), and\n`bond_ind` (1\\*M'). For Z-only atomic features, **V** is a (1\\*N') vector.\n\n<a name=\"datasets\"></a>\n# Data sets\n\nTo aid others in reproducing (and improving on) our results, we have provided \nour MP-crystals-2018.6.1 crystal data set via [figshare](https://figshare.com/articles/Graphs_of_materials_project/7451351)[4]. \nThe MP-crystals-2018.6.1 data set comprises the DFT-computed energies and\nband gaps of 69,640 crystals from the [Materials Project](http://www.materialsproject.org)\nobtained via the [Python Materials Genomics (pymatgen)](http://pymatgen.org)\ninterface to the Materials Application Programming Interface (API)[5] on \nJune 1, 2018. The crystal graphs were constructed using a radius cut-oﬀ of 4 \nangstroms. Using this cut-oﬀ, 69,239 crystals do not form isolated atoms and \nare used in the models. A subset of 5,830 structures have elasticity data that \ndo not have calculation warnings and will be used for elasticity models.\n\nThe molecule data set used in this work is the QM9 data set 30 processed by\nFaber et al.[6] It contains the B3LYP/6-31G(2df,p)-level DFT calculation\nresults on 130,462 small organic molecules containing up to 9 heavy atoms.\n\n<a name=\"computing-requirements\"></a>\n# Computing requirements\n\nTraining: It should be noted that training MEGNet models, like other deep \nlearning models, is fairly computationally intensive with large datasets. In \nour work, we use dedicated GPU resources to train MEGNet models with 100,000\ncrystals/molecules. It is recommended that you do the same.\n\nPrediction: Once trained, prediction using MEGNet models are fairly cheap. \nFor example, the http://megnet.crystals.ai web app runs on a single hobby dyno\non Heroku and provides the prediction for any crystal within seconds.\n\n<a name=\"limitations\"></a>\n# Known limitations\n\n- `isolated atoms` error. This error occurs when using the given cutoff in the model (4A for\n2018 models and 5A for 2019 models), the crystal structure contains isolated atoms, i.e., \nno neighboring atoms are within the distance of `cutoff`. Most of the time, we can just \ndiscard the structure, since we found that those structures tend to have a high energy above \nhull (less stable). If you think this error is an essential issue for a particular problem, \nplease feel free to email us and we will consider releasing a new model with increased cutoff. \n\n<a name=\"contributors\"></a>\n# Contributors\n1. Chi Chen from the Materials Virtual Lab is the lead developer of MEGNet.\n2. Shyue Ping Ong and other members of the Materials Virtual Lab contributes to general improvements\n   of MEGNet and its applications.\n3. Logan Ward has made extensive contributions, especially to the development of molecular graph \n   portions of MEGNet.\n\n<a name=\"references\"></a>\n# References\n\n1. Battaglia, P. W.; Hamrick, J. B.; Bapst, V.; Sanchez-Gonzalez, A.; \n   Zambaldi, V.; Malinowski, M.; Tacchetti, A.; Raposo, D.; Santoro, A.; \n   Faulkner, R.; et al. Relational inductive biases, deep learning, and graph \n   networks. 2018, 1–38. [arXiv:1806.01261](https://arxiv.org/abs/1806.01261)\n2. Chen, C.; Ye, W.; Zuo, Y.; Zheng, C.; Ong, S. P. Graph Networks as a \n   Universal Machine Learning Framework for Molecules and Crystals. Chemistry \n   of Materials 2019, 31(9), 3564-3572. \n   [doi:10.1021/acs.chemmater.9b01294](https://doi.org/10.1021/acs.chemmater.9b01294)\n3. Vinyals, O.; Bengio, S.; Kudlur, M. Order Matters: Sequence to sequence for \n   sets. 2015, arXiv preprint. [arXiv:1511.06391](https://arxiv.org/abs/1511.06391)\n4. https://figshare.com/articles/Graphs_of_materials_project/7451351\n5. Ong, S. P.; Cholia, S.; Jain, A.; Brafman, M.; Gunter, D.; Ceder, G.; \n   Persson, K. A. The Materials Application Programming Interface (API): A \n   simple, flexible and efficient API for materials data based on\n   REpresentational State Transfer (REST) principles. Comput. Mater. Sci. 2015, \n   97, 209–215 DOI: [10.1016/j.commatsci.2014.10.037](http://dx.doi.org/10.1016/j.commatsci.2014.10.037).\n6. Faber, F. A.; Hutchison, L.; Huang, B.; Gilmer, J.; Schoenholz, S. S.; \n   Dahl, G. E.; Vinyals, O.; Kearnes, S.; Riley, P. F.; von Lilienfeld, O. A. \n   Prediction errors of molecular machine learning models lower than hybrid DFT \n   error. Journal of Chemical Theory and Computation 2017, 13, 5255–5264. \n   DOI: [10.1021/acs.jctc.7b00577](http://dx.doi.org/10.1021/acs.jctc.7b00577)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/materialsvirtuallab/megnet",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "materials,science,machine,learning,deep,graph,networks,neural",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "megnet",
    "package_url": "https://pypi.org/project/megnet/",
    "platform": "",
    "project_url": "https://pypi.org/project/megnet/",
    "project_urls": {
      "Download": "https://github.com/materialsvirtuallab/megnet"
    },
    "release_url": "https://pypi.org/project/megnet/1.1.7/",
    "requires_dist": [
      "numpy",
      "scikit-learn",
      "pymatgen (>=2019.10.4)",
      "monty",
      "h5py ; extra == 'model_saving'",
      "openbabel ; extra == 'molecules'",
      "rdkit ; extra == 'molecules'",
      "tensorflow (>=2.1) ; extra == 'tensorflow'",
      "tensorflow-gpu (>=2.1) ; extra == 'tensorflow_with_gpu'"
    ],
    "requires_python": "",
    "summary": "MatErials Graph Networks for machine learning of molecules and crystals.",
    "version": "1.1.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15794044,
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8f4223accc3160ccc8376420a5ffad4e4ea1b8685e28d45f4293b9de00f87cfe",
        "md5": "8302ee3c04d582e48db1528f2006d369",
        "sha256": "9e3886ff35bda69c093d468096fc5a57717b17d0b249b4cc3bad4091e5af17f0"
      },
      "downloads": -1,
      "filename": "megnet-1.1.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8302ee3c04d582e48db1528f2006d369",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 63580,
      "upload_time": "2020-05-27T23:56:46",
      "upload_time_iso_8601": "2020-05-27T23:56:46.861077Z",
      "url": "https://files.pythonhosted.org/packages/8f/42/23accc3160ccc8376420a5ffad4e4ea1b8685e28d45f4293b9de00f87cfe/megnet-1.1.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eddc819f9f63346fcdc45c21e2f46d1f4abab6959fb3b75dbf1b314e75775528",
        "md5": "dc43ff22e0c583dd53e41586d762cd4f",
        "sha256": "eaa3d7189bfdb56899f95ef3e6c594567cdd25e43cdcbea3b416e69e06aa4761"
      },
      "downloads": -1,
      "filename": "megnet-1.1.7.tar.gz",
      "has_sig": false,
      "md5_digest": "dc43ff22e0c583dd53e41586d762cd4f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 15030528,
      "upload_time": "2020-05-27T23:57:07",
      "upload_time_iso_8601": "2020-05-27T23:57:07.416213Z",
      "url": "https://files.pythonhosted.org/packages/ed/dc/819f9f63346fcdc45c21e2f46d1f4abab6959fb3b75dbf1b314e75775528/megnet-1.1.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}