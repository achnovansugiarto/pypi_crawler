{
  "info": {
    "author": "Gauss Machine Learning GmbH",
    "author_email": "info@gauss-ml.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# Optimyzer\n\nA hyperparameter optimization framework that fits into every workflow\n\n> This project is still in _beta_ stage. This means that interfaces might change\nmore often than expected. If you have any kind of issue or suggestion, please\ndrop us a line: [info@gauss-ml.com](mailto:info@gauss-ml.com).\n\n## Introduction\n\nWhile creating machine learning models is fun, optimizing their hyperparameters usually isn't. Many\npeople do this manually, which is often ironically referred to as _expert descent_.\n\nComing more from the world of numerical optimization, many available hyperparameter optimization\nframeworks see the problem as a function optimization problem. However, this means that the code of\nthe project has to be structured in a way that the entire machine-learning pipeline (loading data,\npreprocessing, training, evaluating) is one function call. Furthermore, all parameters have to be\navailable as function arguments. For many projects, wrapping the entire machine learning pipeline\ninto a function call is cumbersome and a waste of time.\n\n`Optimyzer` provides a framework to make hyperparameter optimization easy. It features a very simple\ninterface, such that the optimization can be done with just a few lines of code, and in any existing\nworkflow. It logs all relevant data into a file-based structure, where each experiment gets its own\nfolder. At the moment `Optimyzer` only uses random search because it is easy to use, highly\nparallelizable and already way better than _expert descent_ or _grid search_.\n\nIf you find a bug, or have an improvement suggestion or feature request, you can directly add it\ninto our [issue tracker](https://gitlab.com/gauss-ml-open/optimyzer-client/-/issues). If you have\ngeneral remarks, feedback or other comments, please don't hesitate to contact us at\n[info@gauss-ml.com](mailto:info@gauss-ml.com).\n\n### Example Use-case\n\nLet's say you want to do handwritten digit recognition on the MNIST dataset with a simple multilayer\nperceptron (MLP). Already with such a setup there are a couple of parameters to tune: The depth\n(number of layers) and width (units per layer) of the MLP, as well as the learning rate of the\noptimization algorithm, batch size and so on. Usually there are many more. Getting those\nhyperparameters right can make the difference between mediocre and world-class performance.\n\nSure, you can _quickly_ try out what happens when you change one of the parameters. And then tweak\nanother parameter. A couple of hours later, _just one more tweak and the performance will be good\nenough._ We have been there. We didn't like it. With `Optimyzer`, we would like to help people to\navoid wasting their time with parameter tuning.\n\n## Getting Started\n\nStarting to use `Optimyzer` is really simple, as you will see from the [Installation](#installation)\nand [Usage](#usage-of-the-optimyzer) sections. Additionally, we will explain how the `Optimyzer`'s\n[information flow](#the-information-flow) differs from most other optimizers and how `Optimyzer`\nstores the necessary data.\n\n### Installation\n\nInstalling `Optimyzer` is very easy, just run `pip install optimyzer` in your favorite Python\nenvironment and you're done. If you like it, you might want to add `Optimyzer` to your project's\ndependencies.\n\nBy the way, `Optimyzer` does not have any dependency, it is powered by the Python Standard Library\nonly. Also, there is no database, no inter-process communication, nothing to install, nothing to\nconfigure, nothing to take care of.\n\n### Optimyzer Usage\n\n`Optimyzer` has been built to be included in any kind of workflow as easily as possible. This is\nthe minimal version:\n\nFirst we have to import it (of course):\n\n```python\nimport optimyzer\n```\n\nThen we instantiate the `Optimyzer` object and select a directory where it runs:\n\n```python\noy = optimyzer.Optimyzer(\".\")  # initializes an Optimyzer in the current workdir\n```\n\nFor each parameter we want to tune, we have to tell `Optimyzer` the type and\nrange. This is done by adding a parameter (`IntParameter`, `FloatParameter` or\n`CategoricalParameter`) to `Optimyzer`:\n\n```python\noy.add_parameter(optimyzer.IntParameter(\"int_name\", (minimum, maximum)))\noy.add_parameter(optimyzer.FloatParameter(\"float_name\", (minimum, maximum)))\noy.add_parameter(optimyzer.CategoricalParameter(\"cat_name\", [\"a\", \"b\", \"c\"]))\n```\n\nNote that the parameters can be freely named, except for the metadata names `workdir`, `id` and\n`value`.\n\nOnce we have added the parameters, we can sample from the parameter space:\n\n```python\nconfig = oy.create_config()  # create a configuration by random sampling\n```\n\nThis returns a configuration that holds all your parameters as properties, e.g., `config.int_name`\nor `config.float_name`. For convenience, this configuration also contains the `id` and `workdir` of\nthe instance. Note that you cannot add or change your parameters after creating a configuration.\n\nNow you can run the rest of your pipeline, using the sampled parameters from the `config`. After\neverything is done and you have evaluated the `performance` of your model, you report it back to\n`Optimyzer`:\n\n```python\noy.report_loss(performance)\n```\n\nThat's it already! With just a few extra lines of code, you can run your training pipeline using\nhyper-parameters sampled from your search space.\n\nYou can now execute the optimization by running this file a couple of times, either sequentially in\na simple loop (if you only have one GPU) or in parallel. It doesn't matter whether you're running\nthis on your laptop or your GPU cluster. The only thing that you need is a shared file system where\nthe base directory is located.\n\nAfter running this as many times as you wish, you can get the optimal configuration directly from a\nstatic top-level function of the package:\n\n```python\noptimal_config = optimyzer.get_optimal_config(basedir)\n```\n\nAlternatively, you can find the optimal parameters in a human-readable JSON file located at\n(`best_instance/.optimyzer/config.json`), in the `basedir` you gave to `Optimyzer`.\n\nNote that this framework is entirely transparent; there are no secret hooks for particular\nlibraries, nothing hidden happening. You're in control and know what is going on in your code.\n\n#### Parameter Types\n\nThere are tree main types of parameters available:\n\n* `IntParameter` for integer-valued options, like the number of layers in a\n    neural network or the number of features for a spectral method.\n* `FloatParameter` for real-valued options, like the learning rate in neural\n    networks or length scales in kernel methods.\n* `CategoricalParameter` for choices, like the type of optimization algorithm\n    for a neural network or the covanriance function in Gaussian processes.\n\nSince quite often numerical parameters roughly follow Zipf's law (in the sense\nthat larger parameters are less likely than smaller parameters), and due to the\nfact that often the relative change in parameters during optimization is much\nmore meaningful than the absolute change, it makes sense to define certain\nparameter on a log scale. This can be done by adding the keyword argument\n`logarithmic=True` when initilizing the parameter:\n\n```python\noptimyzer.IntParameter(\"int_name\", (minimum, maximum), logarithmic=True)\noptimyzer.FloatParameter(\"float_name\", (minimum, maximum), logarithmic=True)\n```\n\nUsing a logarithmic distribution is helpful, for instance, when optimizing\nlength scales, learning rates, noise levels, signal variances, lengths, masses,\netc.\n\n#### Illustrative Integration in ML Code\n\nA cartoon example of a machine learning pipeline based on our imaginary neural network framework\n`neuralnetworks` is shown below. A full Keras tutorial is located in the [`notebooks`](notebooks)\ndirectory, both as [cell-based script](notebooks/mnist_keras.py) and as [Jupyter\nnotebook](notebooks/mnist_keras.ipynb).\n\n```python\n# import stuff\nimport neuralnetworks as nn\nimport optimyzer\n\ntrain_inputs, train_targets, test_inputs, test_targets = nn.load_preprocessed_data('MNIST')\n\n# Optimyzer: initialize and configure parameterspace\noy = optimyzer.Optimyzer(\".\")  # initializes an Optimyzer in the current workdir\n\n# int between 1 and 10 for the depth (number of layers)\noy.add_parameter(optimyzer.IntParameter(\"depth\", (1, 10)))\n# int between 32 and 1024 for the width (nodes per layer)\noy.add_parameter(optimyzer.IntParameter(\"width\", (32, 1024)))\n# float between 1e-3 and 1e0 for learning_rate\noy.add_parameter(optimyzer.FloatParameter(\"learning_rate\", (1e-3, 1e0)))\n# two different neural network optimizers\noy.add_parameter(optimyzer.CategoricalParameter(\"opt\", [\"SGD\", \"ADAM\"]))\n\n# Optimyzer: freeze configuration and sample\nconfig = oy.create_config()  # we create a config by random sampling\n\n# create a model\nmodel = nn.MLP(depth=config.depth, width=config.width)  # use sampled values\n\n# select training algorithm based on sampled category\nif config.opt == \"SGD\":\n    opt = nn.opt.SGD(model, learning_rate=config.learning_rate)\nif config.opt == \"ADAM\":\n    opt = nn.opt.ADAM(model, learning_rate=config.learning_rate)\n\n# train the model\nmodel.train(opt, train_inputs, train_targets)\n\n# after the training: check how many predictions were correct\npred = model.predict(test_inputs)\ncorrect = sum(pred == test_targets)\n\n# Optimyzer: report the loss\noy.report_loss(performance)\n```\n\nThat's just four additional lines of code for using `Optimyzer`, plus one line for each parameter.\n\n### Directory Handling\n\nOftentimes, machine learning pipelines write data into a working directory. For example, training\nprogress often is logged into a `tensorboard` directory and model checkpoints are stored as well.\n`Optimyzer` therefore provides a working directory for each configuration instance, so that logging\nand checkpoint saving can be done without risking to overwrite the data from other runs. `Optimyzer`\nitself stores the instance configuration and resulting performance within that directory in its\n`.optimyzer` folder.\n\n#### Absolute Paths\n\nIf you already use absolute paths, great! If the path where you do your experiments is, for example,\n`/var/tmp/ml-experiments/`, you can give that path directly to `optimyzer` like this:\n\n```python\nexperiment_dir = \"/var/tmp/ml-experiments\"\noy = optimyzer.Optimyzer(experiment_dir)\n```\n\nWhen you create a configuration\n\n```python\nconfig = oy.create_config()\n```\n\nthis `config` will contain a working directory `config.workdir` that you can use subsequently, for\ninstance to save a model\n\n```python\nmodel.save(os.path.join(config.workdir, \"model.nn\"))\n```\n\n#### Relative Paths and Current Working Directory\n\nIf you just have a script running that uses the current working directory (where the script is\nexecuted), no problem either! You can initialize `Optimyzer` in the current working directory\nas well:\n\n```python\noy = optimyzer.Optimyzer(\".\")\n```\n\nWhen you create a configuration, you can pass the `chdir=True` switch:\n\n```python\nconfig = oy.create_config(chdir=True)\n```\n\nThis will change the current working directory from the location of your script to the working\ndirectory of this instance. This means that all relative file system commands are now executed\nrelative to the working directory. This means that you don't have to take care of the paths and can\njust continue using commands that operate in the current working directory, for instance\n\n```python\nmodel.save(\"model.nn\")\n```\n\n### Further Reading: How the Metadata is Stored\n\nWhile `Optimyzer` works equally fine for algorithms that never store any data, it was designed to\nwork well with pipelines that need some kind of working directory to store results or configuration\nor logging information. That's why there is one directory created for each instance. The instance\nname is based on the configuration, which is hashed to generate a unique name.\n\nThis means that after executing a couple of runs, your `experiments` directory may look like this\n\n```none\nexperiments\n|- 5c49826a83751767729e\n|- 71ae3a23d782a0465750\n|- 7633242a2c695641532b\n|- d0b85d53f19420d1a7a1\n|- dee9fecd1f0b91346bc4\n|- best_instance -> 71ae3a23d782a0465750\n|- ...\n```\n\nwhere each directory represents one experiment instance, one configuration that has been evaluated.\nThe directory `best_instance` is a symbolic link that points to the directory of the best\nconfiguration seen so far.\n\nEach of the experiment instances contains a metadata folder `.optimyzer`, which holds the\nconfiguration and the value of the experiment in a JSON file each\n\n```none\nexperiments\n|- 5c49826a83751767729e\n   |- .optimyzer\n       |- config.json\n       |- value.json\n|- 71ae3a23d782a0465750\n   |- .optimyzer\n       |- config.json\n       |- value.json\n|- ...\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.com/gauss-ml-open/optimyzer-client",
    "keywords": "parameter optimization,machine learning,artificial intelligence,neural networks",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "optimyzer",
    "package_url": "https://pypi.org/project/optimyzer/",
    "platform": "",
    "project_url": "https://pypi.org/project/optimyzer/",
    "project_urls": {
      "Homepage": "https://gitlab.com/gauss-ml-open/optimyzer-client"
    },
    "release_url": "https://pypi.org/project/optimyzer/0.2.1/",
    "requires_dist": [
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pylint ; extra == 'dev'",
      "rope ; extra == 'dev'",
      "flaky ; extra == 'test'",
      "micro-ci ; extra == 'test'",
      "pytest ; extra == 'test'",
      "pytest-cov ; extra == 'test'"
    ],
    "requires_python": ">=3.5, <4",
    "summary": "A hyperparameter optimization framework that fits into every workflow",
    "version": "0.2.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9725907,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "18ab18f70fce8ae9aa2eafe939ab4632f45be350f7cfb09e8c36e4b54119d373",
          "md5": "7b003a25d08d85859efa279861bb94cd",
          "sha256": "9ad1438372983c8fce8ab625a2d275278df3a3a2b4bc5eb388df4e53ded31265"
        },
        "downloads": -1,
        "filename": "optimyzer-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7b003a25d08d85859efa279861bb94cd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5, <4",
        "size": 13796,
        "upload_time": "2020-11-24T20:51:34",
        "upload_time_iso_8601": "2020-11-24T20:51:34.488391Z",
        "url": "https://files.pythonhosted.org/packages/18/ab/18f70fce8ae9aa2eafe939ab4632f45be350f7cfb09e8c36e4b54119d373/optimyzer-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a96074b971a5d5beb798daed804045367d33185cf962d580ff708fcc40248e91",
          "md5": "cf73f017beef3d4404351cbdd698a269",
          "sha256": "f1fe0cb3e59db2d67d19ffc4b4b68b798b554aae81b1dc74b38416273a0fea03"
        },
        "downloads": -1,
        "filename": "optimyzer-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "cf73f017beef3d4404351cbdd698a269",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5, <4",
        "size": 17322,
        "upload_time": "2020-11-24T20:51:35",
        "upload_time_iso_8601": "2020-11-24T20:51:35.828707Z",
        "url": "https://files.pythonhosted.org/packages/a9/60/74b971a5d5beb798daed804045367d33185cf962d580ff708fcc40248e91/optimyzer-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e81e37e131a8b04fa0cd3e7ce59f4d63fe0ba8357c86d8c854da52268b1fa88a",
          "md5": "8eb05dfc17f937cacea3fb40bf44e27e",
          "sha256": "e2647d1e4cd9e5bbca83783245c7e3f20e51a426759aeb9ba4e906cfd71f4c3e"
        },
        "downloads": -1,
        "filename": "optimyzer-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8eb05dfc17f937cacea3fb40bf44e27e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5, <4",
        "size": 14989,
        "upload_time": "2020-12-01T11:01:16",
        "upload_time_iso_8601": "2020-12-01T11:01:16.389249Z",
        "url": "https://files.pythonhosted.org/packages/e8/1e/37e131a8b04fa0cd3e7ce59f4d63fe0ba8357c86d8c854da52268b1fa88a/optimyzer-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e85f95f890558e465149a42048234d1449b486467e0909b70f57569ebbbcee36",
          "md5": "b5c4b5d28b2b1f4825c2e168524e285d",
          "sha256": "926224b26311fff108bbc287619ad3113b21d1d08346fc2857265ff232402657"
        },
        "downloads": -1,
        "filename": "optimyzer-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b5c4b5d28b2b1f4825c2e168524e285d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5, <4",
        "size": 18697,
        "upload_time": "2020-12-01T11:01:18",
        "upload_time_iso_8601": "2020-12-01T11:01:18.362842Z",
        "url": "https://files.pythonhosted.org/packages/e8/5f/95f890558e465149a42048234d1449b486467e0909b70f57569ebbbcee36/optimyzer-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b8ada7d3ee3ac4b7860514d70fd3aa58608909bb5e1b2874c4ae44385741d6b4",
          "md5": "a110ade780e768ff979e71a38ebf4f00",
          "sha256": "0f4c978ce344544c9272ad436d1f696c014b0b389513822dd2a2c852183248bf"
        },
        "downloads": -1,
        "filename": "optimyzer-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a110ade780e768ff979e71a38ebf4f00",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5, <4",
        "size": 14995,
        "upload_time": "2020-12-18T13:08:13",
        "upload_time_iso_8601": "2020-12-18T13:08:13.771556Z",
        "url": "https://files.pythonhosted.org/packages/b8/ad/a7d3ee3ac4b7860514d70fd3aa58608909bb5e1b2874c4ae44385741d6b4/optimyzer-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "88b8133d57d7ebfc1d60793cb74f4ac9bb738afa232795ff744198870b92b151",
          "md5": "d9952b38d4979ad377ea198230382c71",
          "sha256": "51170f3f0ca0414525417cf25098233b1a860db95403be6281653a8f15915240"
        },
        "downloads": -1,
        "filename": "optimyzer-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "d9952b38d4979ad377ea198230382c71",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5, <4",
        "size": 18664,
        "upload_time": "2020-12-18T13:08:15",
        "upload_time_iso_8601": "2020-12-18T13:08:15.009867Z",
        "url": "https://files.pythonhosted.org/packages/88/b8/133d57d7ebfc1d60793cb74f4ac9bb738afa232795ff744198870b92b151/optimyzer-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b71820722cdc263114abe1fe87511014fea848d3adaf876bb3e46217ca1d5b0c",
          "md5": "afce6e4e4f1b30a641a67164f782ddd1",
          "sha256": "87af926efe8cc43691641d1d5291c20e0f2b0691abcc39b0c52360b77c687e69"
        },
        "downloads": -1,
        "filename": "optimyzer-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "afce6e4e4f1b30a641a67164f782ddd1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5, <4",
        "size": 17955,
        "upload_time": "2021-02-19T22:58:21",
        "upload_time_iso_8601": "2021-02-19T22:58:21.480226Z",
        "url": "https://files.pythonhosted.org/packages/b7/18/20722cdc263114abe1fe87511014fea848d3adaf876bb3e46217ca1d5b0c/optimyzer-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fc1cc933e6b86da28e7140eaa33365e9f5dee17b40b644a36928fbc037c50e60",
          "md5": "f0161d3c2872b0fd8fbfeaee827042dc",
          "sha256": "794f45b79258d322508368ad881ea8683e6e1d8b7a88ff5d2becc4dff706a721"
        },
        "downloads": -1,
        "filename": "optimyzer-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f0161d3c2872b0fd8fbfeaee827042dc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5, <4",
        "size": 21945,
        "upload_time": "2021-02-19T22:58:22",
        "upload_time_iso_8601": "2021-02-19T22:58:22.618314Z",
        "url": "https://files.pythonhosted.org/packages/fc/1c/c933e6b86da28e7140eaa33365e9f5dee17b40b644a36928fbc037c50e60/optimyzer-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8722b572dc5a2f421a8bb3cb03edf196ed7389a4eb43ba27630aef3acbcc9c42",
          "md5": "80fc0052691992dd5c03ea741a51297d",
          "sha256": "5f12d7b41a7e5bc2693d57e528d891ea0ad34d1efbc98cda6e104b6d7abeb29e"
        },
        "downloads": -1,
        "filename": "optimyzer-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "80fc0052691992dd5c03ea741a51297d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5, <4",
        "size": 17975,
        "upload_time": "2021-03-10T07:31:14",
        "upload_time_iso_8601": "2021-03-10T07:31:14.589829Z",
        "url": "https://files.pythonhosted.org/packages/87/22/b572dc5a2f421a8bb3cb03edf196ed7389a4eb43ba27630aef3acbcc9c42/optimyzer-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bc096700ce070fbe310afa12cd649f1ae167aa641fdacc1f0887ac827ad62dea",
          "md5": "5e6fcc21b085fa9654b8f93167a19321",
          "sha256": "115199d53bc7ebe2ac10e148e40c1149249a5ba582fd50777be50ed85a37e11c"
        },
        "downloads": -1,
        "filename": "optimyzer-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5e6fcc21b085fa9654b8f93167a19321",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5, <4",
        "size": 21959,
        "upload_time": "2021-03-10T07:31:15",
        "upload_time_iso_8601": "2021-03-10T07:31:15.681337Z",
        "url": "https://files.pythonhosted.org/packages/bc/09/6700ce070fbe310afa12cd649f1ae167aa641fdacc1f0887ac827ad62dea/optimyzer-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8722b572dc5a2f421a8bb3cb03edf196ed7389a4eb43ba27630aef3acbcc9c42",
        "md5": "80fc0052691992dd5c03ea741a51297d",
        "sha256": "5f12d7b41a7e5bc2693d57e528d891ea0ad34d1efbc98cda6e104b6d7abeb29e"
      },
      "downloads": -1,
      "filename": "optimyzer-0.2.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "80fc0052691992dd5c03ea741a51297d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5, <4",
      "size": 17975,
      "upload_time": "2021-03-10T07:31:14",
      "upload_time_iso_8601": "2021-03-10T07:31:14.589829Z",
      "url": "https://files.pythonhosted.org/packages/87/22/b572dc5a2f421a8bb3cb03edf196ed7389a4eb43ba27630aef3acbcc9c42/optimyzer-0.2.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bc096700ce070fbe310afa12cd649f1ae167aa641fdacc1f0887ac827ad62dea",
        "md5": "5e6fcc21b085fa9654b8f93167a19321",
        "sha256": "115199d53bc7ebe2ac10e148e40c1149249a5ba582fd50777be50ed85a37e11c"
      },
      "downloads": -1,
      "filename": "optimyzer-0.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "5e6fcc21b085fa9654b8f93167a19321",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5, <4",
      "size": 21959,
      "upload_time": "2021-03-10T07:31:15",
      "upload_time_iso_8601": "2021-03-10T07:31:15.681337Z",
      "url": "https://files.pythonhosted.org/packages/bc/09/6700ce070fbe310afa12cd649f1ae167aa641fdacc1f0887ac827ad62dea/optimyzer-0.2.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}