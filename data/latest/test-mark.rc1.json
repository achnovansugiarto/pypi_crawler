{
  "info": {
    "author": "Avishek Das",
    "author_email": "avishek.das.ayan@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Bangla Feature Extractor(BFE)\n\nBFE is a Bangla Natural Language Processing based feature extractor.\n\n\n## Current Features\n\n  1. [CountVectorizer](#1-countvectorizer)\n  2. [TfIdf](#2-tfidf)\n  3. [Word Embedding](#3-word-embedding)\n      * [Word2Vec](#word2vec)\n      * [FastText](#fasttext)\n\n## Installation\n```\npip install bfe\n```\n## Example\n### 1. CountVectorizer\n  - Fit n Transform\n  - Transform\n  - Get Wordset\n\n**Fit n Transform**\n```py\nfrom bfe import CountVectorizer\nct = CountVectorizer()\nX = ct.fit_transform(X) # X is the word features\n#Output: the countVectorized matrix form of given features\n```\n\n**Transform**\n```py\nfrom bfe import CountVectorizer\nct = CountVectorizer()\nget_mat = ct.transform(\"রাহাত\")\n#Output: the countVectorized matrix form of given word\n```\n\n**Get Wordset**\n```py\nfrom bfe import CountVectorizer\nct = CountVectorizer()\nct.get_wordSet()\n#Output: get the raw wordset used in training model\n```\n\n### 2. TfIdf\n  - Fit n Transform\n  - Transform\n  - Coefficients\n\n **Fit n Transform**\n```py\nfrom bfe import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"কাওছার আহমেদ\", \"শুভ হাইদার\"]\nmatrix1 = k.fit_transform(doc)\nprint(matrix1)\n\n'''\nOutput: \n[[0.150515 0.150515 0.       0.      ]\n [0.       0.       0.150515 0.150515]]\n'''\n```\n**Transform**\n```py\nfrom bfe import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"আহমেদ সুমন\", \"কাওছার করিম\"]\nmatrix2 = k.transform(doc)\nprint(matrix2)\n\n'''\nOutput: \n[[0.150515 0.       0.       0.      ]\n [0.       0.150515 0.       0.      ]]\n'''\n```\n**Coefficients**\n```py\nfrom bfe import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"কাওছার আহমেদ\", \"শুভ হাইদার\"]\nk.fit_transform(doc)\nwordset, idf = k.coefficients()\nprint(wordset)\n#Output: ['আহমেদ', 'কাওছার', 'হাইদার', 'শুভ']\n\nprint(idf)\n'''\nOutput: \n{'আহমেদ': 0.3010299956639812, 'কাওছার': 0.3010299956639812, 'হাইদার': 0.3010299956639812, 'শুভ': 0.3010299956639812}\n'''\n\n```\n\n### 3. Word Embedding\n- ### Word2Vec\n    - Training\n    - Get Word Vector\n    - Get Similarity\n    - Get n Similar Words\n    - Get Middle Word\n    - Get Odd Words\n    - Get Similarity Plot\n\n**Training**\n```py\nfrom bfe import BN_Word2Vec\n#Training Against Sentences\nw2v = BN_Word2Vec(sentences=[['আমার', 'প্রিয়', 'জন্মভূমি'], ['বাংলা', 'আমার', 'মাতৃভাষা']])\nw2v.train_Word2Vec()\n\n#Training Against one Dataset\nw2v = BN_Word2Vec(corpus_file=\"path to data or txt file\")\nw2v.train_Word2Vec()\n\n#Training Against Multiple Dataset\n'''\n    path\n      ->data\n        ->1.txt\n        ->2.txt\n        ->3.txt\n'''\nw2v = BN_Word2Vec(corpus_path=\"path/data\")\nw2v.train_Word2Vec(epochs=25)\n```\nAfter training is done the model \"w2v_model\"  along with it's supportive vector files will be saved to current directory.\n\n**If you use any pretrained model, specify it while initializing BN_Word2Vec() . Otherwise no model_name is needed.**\n\n**Get Word Vector**\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_wordVector('আমার')\n```\n\n**Get Similarity**\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_similarity('ঢাকা', 'রাজধানী')\n\n#Output: 67.457879\n```\n\n**Get n Similar Words**\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_n_similarWord(['পদ্মা'], n=10)\n#Output: \n'''\n[('সেতুর', 0.5857524275779724),\n ('মুলফৎগঞ্জ', 0.5773632526397705),\n ('মহানন্দা', 0.5634652376174927),\n (\"'পদ্মা\", 0.5617109537124634),\n ('গোমতী', 0.5605217218399048),\n ('পদ্মার', 0.5547558069229126),\n ('তুলসীগঙ্গা', 0.5274507999420166),\n ('নদীর', 0.5232067704200745),\n ('সেতু', 0.5225246548652649),\n ('সেতুতে', 0.5192927718162537)]\n'''\n```\n\n**Get Middle Word**\n\n    Get the probability distribution of the center word given words list.\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_outputWord(['ঢাকায়', 'মৃত্যু'], n=2)\n\n#Output:  [(\"হয়েছে।',\", 0.05880642), ('শ্রমিকের', 0.05639163)]\n```\n\n**Get Odd Words**\n\n    Get the most unmatched word out from given words list\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_oddWords(['চাল', 'ডাল', 'চিনি', 'আকাশ'])\n\n#Output: 'আকাশ' \n```\n\n**Get Similarity Plot**\n\n    Creates a barplot of similar words with their probability \n\n```py\nfrom bfe import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_oddWords(['চাল', 'ডাল', 'চিনি', 'আকাশ'])\n```\n\n- ### FastText\n    - Training\n    - Get Word Vector\n    - Get Similarity\n    - Get n Similar Words\n    - Get Middle Word\n    - Get Odd Words\n\n\n**Training**\n```py\nfrom bfe import BN_FastText\n#Training Against Sentences\nft = FastText(sentences=[['আমার', 'প্রিয়', 'জন্মভূমি'], ['বাংলা', 'আমার', 'মাতৃভাষা']])\nft.train_fasttext()\n\n#Training Against one Dataset\nft = FastText(corpus_file=\"path to data or txt file\")\nft.train_fasttext()\n\n#Training Against Multiple Dataset\n'''\n    path\n      ->data\n        ->1.txt\n        ->2.txt\n        ->3.txt\n'''\nft = FastText(corpus_path=\"path/data\")\nft.train_fasttext(epochs=25)\n```\nAfter training is done the model \"ft_model\"  along with it's supportive vector files will be saved to current directory.\n\n**If you use any pretrained model, specify it while initializing BN_FastText() . Otherwise no model_name is needed.**\n\n**Get Word Vector**\n```py\nfrom bfe import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_wordVector('আমার')\n```\n\n**Get Similarity**\n```py\nfrom bfe import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_similarity('ঢাকা', 'রাজধানী')\n\n#Output: 70.56821120\n```\n\n**Get n Similar Words**\n```py\nfrom bfe\" import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_n_similarWord(['পদ্মা'], n=10)\n#Output: \n'''\n[('পদ্মায়', 0.8103810548782349),\n ('পদ্মার', 0.794012725353241),\n ('পদ্মানদীর', 0.7747839689254761),\n ('পদ্মা-মেঘনার', 0.7573559284210205),\n ('পদ্মা.', 0.7470568418502808),\n ('‘পদ্মা', 0.7413997650146484),\n ('পদ্মাসেতুর', 0.716225266456604),\n ('পদ্ম', 0.7154797315597534),\n ('পদ্মহেম', 0.6881639361381531),\n ('পদ্মাবত', 0.6682782173156738)]\n'''\n```\n\n**Get Odd Words**\n\n    Get the most unmatched word out from given words list\n```py\nfrom \"package_name\" import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_oddWords(['চাল', 'ডাল', 'চিনি', 'আকাশ'])\n\n#Output: 'আকাশ' \n```\n\n**Get Similarity Plot**\n\n    Creates a barplot of similar words with their probability \n\n```py\nfrom bfe import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_oddWords(['চাল', 'ডাল', 'চিনি', 'আকাশ'])\n```\n\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "test-mark",
    "package_url": "https://pypi.org/project/test-mark/",
    "platform": "",
    "project_url": "https://pypi.org/project/test-mark/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/test-mark/0.1/",
    "requires_dist": [
      "markdown",
      "bre"
    ],
    "requires_python": "",
    "summary": "A simple function for mathematical operations",
    "version": "0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7041780,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6218322898fdd5ac334632316ee87cf2672075f5c53ba54ea883627817ca8fc6",
          "md5": "0ab201eb75343413d3595f8a03b850db",
          "sha256": "1e7f75691ea94a0530489efd8b7cccb9dcda4f2864b66fd1122b6417413888d0"
        },
        "downloads": -1,
        "filename": "test_mark-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0ab201eb75343413d3595f8a03b850db",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 4494,
        "upload_time": "2020-04-17T17:05:26",
        "upload_time_iso_8601": "2020-04-17T17:05:26.501132Z",
        "url": "https://files.pythonhosted.org/packages/62/18/322898fdd5ac334632316ee87cf2672075f5c53ba54ea883627817ca8fc6/test_mark-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6218322898fdd5ac334632316ee87cf2672075f5c53ba54ea883627817ca8fc6",
        "md5": "0ab201eb75343413d3595f8a03b850db",
        "sha256": "1e7f75691ea94a0530489efd8b7cccb9dcda4f2864b66fd1122b6417413888d0"
      },
      "downloads": -1,
      "filename": "test_mark-0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0ab201eb75343413d3595f8a03b850db",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 4494,
      "upload_time": "2020-04-17T17:05:26",
      "upload_time_iso_8601": "2020-04-17T17:05:26.501132Z",
      "url": "https://files.pythonhosted.org/packages/62/18/322898fdd5ac334632316ee87cf2672075f5c53ba54ea883627817ca8fc6/test_mark-0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}