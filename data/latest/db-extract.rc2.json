{
  "info": {
    "author": "Sareen Shah",
    "author_email": "shah.sareen@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# db_extract\n\n## Overview\n\ndb_extract is a python module that can perform **on-the-fly denormalization** from a relational dataset. The module calculates metadata to construct a graph representation of the dataset, and with this construct, finds ways to join widely separated tables without having to perform manual table inspection. This is similar to [Neo4j](https://neo4j.com/developer/guide-importing-data-and-etl/), however setup and querying with this module requires only a couple of lines of code.\n\nFor example, if you are interested in data from tables **A** and **D** that have two join paths:\n- **A** JOIN **B** ON *AB* JOIN **C** ON *BC* JOIN **D** ON *CD*\n- **A** JOIN **XYZ** ON *AXYZ* JOIN **D** ON *XYZD*\n\nYou can simply input parameters [**A**, **D**] and the module will provide all possible join paths for you, which can then in turn be used to get a dataframe containing the data of interest.\n\nThe user will find this module to be of most use when working with highly normalized datasets, however it can also be used with datasets with only a few tables as well.\n\nCurrent support for \n - Data separated into files (most commonly CSV)\n - Microsoft SQL Server\n\n## Requirements\n\nIf you are connecting to a MS SQL Server database, then you must have an ODBC driver installed on your system for use with pyodbc.\n\n## First-time setup\n\nRun the following command\n\n```bash\npython setup.py\n```\n\nThis will set up the SQLite database *metadata.db* which will be used to store information about the datasets and will also store connections between tables. If interested, you can browse the data using [DB Browser for SQLite](https://sqlitebrowser.org/).\n\n## Algorithm Explanation\n\nMany-one and one-one relationships can be joined in any order, but many-many relationships cannot be joined together directly. The problem becomes more evident as you start chaining together many joins where you might be able to join A with B, and B with C, but can't join together A, B, and C all together.\n\nThis module uses **networkx** to construct a directed graph showing these relationships, representing tables as nodes and joining columns as edges.\n\n\n![one_to_one](https://imgur.com/K5t3v4R.png)\n\n\n**order_details** and **orders** share a one-to-one relationship on column *order_id*, and are represented with a bidirectional edge.\n\n\n![many to one](https://imgur.com/G1JWNNS.png)\n\n\n**order_details** and **offices** share a many-to-one relationship on column *office_id*, and are represented with a unidirectional edge starting from the many table and pointing to the one table.\n\n\n![impossible join](https://imgur.com/TEKcWex.png)\n\n\nAbove we can see that **order_details** and **offices** can be joined together as a many-to-one relationship, and **employees** and **offices** can also be joined in the same manner. However see what happens when we try to join all three tables together:\n\n\n![impossible join result](https://imgur.com/00qvFGV.png)\n\n\nThe result is nonsensical. \n\nThe module identifies valid join paths by identifying whether there is a singular origin node from which it can draw a path to every other node that needs to be included. Sometimes this origin node may be upstream from the tables that you are interested in. In the above example, there is no way to start from any single table and draw a path to the other two tables, and so the module recognizes this as an invalid combination of tables.\n\n## Demonstration\n\nWe will use a modified version of the Northwind dataset (CSV files originally taken from [here](https://github.com/graphql-compose/graphql-compose-examples/tree/master/examples/northwind/data/csv)), the files are included in this package. To initialize the dataset, run the following code:\n\n```python\nfrom db_extract import DBSetup, DBExtractor, Filter\nimport db_extract.constants as c\n\nnw_setup = DBSetup('Northwind')\nnw_setup.create_metadata(\n  data_folder_path='/your/python/lib/path/db_extract/datasets/Northwind/',\n  dump_to_data_db=True\n)\n```\n\nSupplying the **dump_to_data_db** option will store a copy of the data in a *data.db* SQLite file located in the same folder (only available when using a collection of files) for more efficient joining.\n\nWe still haven't defined any relationships between the tables. If we run \n```python\nnw_setup.get_common_column_names()\n```\n\nThis will spit out a list of columns that have the same names in multiple files, which usually are used for joining. We see that there are a lot of common column names such as *supplierID*, *categoryID*, *contactName*, *contactTitle*, etc. It's clear that the columns used for joining are all labeled with \"ID\" so we can run the following code to connect all tables.\n\n```python\nfor i in nw_setup.get_common_column_names():\n    if i[-2:] == 'ID':\n        nw_setup.add_global_fk(i)\n```\n\nThat's all the setup that is required. If you wanted to visualize the graph in a Jupyter notebook, you can run the following code- (**WARNING**: networkx only has rudimentary visualization capabilities so the result will likely look strange and may require a few tries to get something that looks legible because networkx draws it differently each time. *pygraphviz* is another option but also takes work to get it to look right.):\n\n```python\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nnw_extractor = DBExtractor('Northwind')\nnx.draw_networkx(nw_extractor.G, node_shape=\"None\")\n```\n\n. You'll get something similar to the following:\n\n![northwind schema](https://imgur.com/Nl3df0f.png)\n\n\nLet's say we are interested in figuring out how many units each supplier has provided. The tables of interest are **order_details** and **suppliers**\n\n```python\nnw_extractor.find_paths_multi_tables(['order_details', 'suppliers'])\n\n#RESULT\n[\n [\n  ('order_details', 'vendors'),\n  ('vendors', 'suppliers')\n ],\n [\n  ('order_details', 'products'),\n  ('products', 'suppliers')\n ]\n]\n```\n\nSo there are two ways to join **order_details** and **products** together, either going through **vendors** or through **suppliers**, both are valid ways to do it. Later you realize that you also want the category of the products sold.\n\n\n```python\nnw_extractor.find_paths_multi_tables(['order_details', 'suppliers', 'categories'])\n#RESULT\n[\n [\n  ('order_details', 'products'),\n  ('products', 'suppliers'),\n  ('products', 'categories')\n ]\n]\n\n```\n\nNow there's only one path. Going through **vendors** is superfluous because you must go through **products** to get to **categories** anyway.\n\nNow let's get a dataframe out of this:\n\n```python\nfilters = {\n    'categories': [\n        Filter(filter_type=c.FILTER_TYPE_SELECTION, column='categoryName', selection=['Condiments'])\n    ],\n    'order_details': [\n        Filter(filter_type=c.FILTER_TYPE_RANGE, column='quantity', range_min=10, range_max=30)\n    ]\n}\n\nselect_table_columns = {\n    'order_details': ['quantity', 'discount'],\n    'suppliers': ['contactName'],\n    'products': ['productName']\n}\n\n\ndf = nw_extractor.get_df_from_path(\n    [('order_details', 'products'),\n    ('products', 'suppliers'),\n    ('products', 'categories')],\n    filters=filters,\n    select_table_columns=select_table_columns\n)\n\n```\n\n|   order_details_quantity |   order_details_discount | suppliers_contactName   | products_productName         |\n|-------------------------:|-------------------------:|:------------------------|:-----------------------------|\n|                       14 |                      0   | Charlotte Cooper        | Aniseed Syrup                |\n|                       20 |                      0   | Charlotte Cooper        | Aniseed Syrup                |\n|                       20 |                      0.1 | Charlotte Cooper        | Aniseed Syrup                |\n|                       20 |                      0.1 | Charlotte Cooper        | Aniseed Syrup                |\n|                       10 |                      0   | Shelley Burke           | Chef Anton's Cajun Seasoning |\n|                       10 |                      0.1 | Shelley Burke           | Chef Anton's Cajun Seasoning |\n|                       12 |                      0.1 | Shelley Burke           | Chef Anton's Cajun Seasoning |\n\n*filters* and *select_table_columns* are optional parameters. Above, we have decided that we only are interested in orders involving condiments, with a quantity of 10-30. The data that we are interested in is quantity, discount amount, supplier contact name, and product name.\n\nAs mentioned earlier, the module will also prevent trying to do joins that make no sense.\n\n```python\nnw_extractor.find_paths_multi_tables(['orders', 'territories'])\n\n#RESULT\n[]\n```\n\nLooking at the schema above, there's no node that you can start from and draw a path to both **orders** and **territories**. On inspecting the data, it becomes evident that this is because an employee may sell to multiple territories, so we can't connect the order to the territories because we only have data on the employee that made the order.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/sareeneng/db_extract/",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "db-extract",
    "package_url": "https://pypi.org/project/db-extract/",
    "platform": "",
    "project_url": "https://pypi.org/project/db-extract/",
    "project_urls": {
      "Homepage": "https://github.com/sareeneng/db_extract/"
    },
    "release_url": "https://pypi.org/project/db-extract/0.2/",
    "requires_dist": [
      "appdirs (>=1.4.3)",
      "networkx (>=2.4)",
      "pandas (>=1.0.1)",
      "pyodbc (>=4.0.30)",
      "sqlalchemy-utils (>=0.36.1)",
      "sqlalchemy (>=1.3.13)"
    ],
    "requires_python": "",
    "summary": "A package that can perform on-the-fly denormalization",
    "version": "0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6963379,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "47348ac353f3a05d163c598dac0bd676b9378d3195e22885d77246b340448429",
          "md5": "355bf25fdd94a96de7fe340eac1389fa",
          "sha256": "bed95996f9c081b1633dccda4e18fd3418e4e86aa482c1d7badadb8f371a31ef"
        },
        "downloads": -1,
        "filename": "db_extract-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "355bf25fdd94a96de7fe340eac1389fa",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5317,
        "upload_time": "2020-03-06T23:21:45",
        "upload_time_iso_8601": "2020-03-06T23:21:45.650013Z",
        "url": "https://files.pythonhosted.org/packages/47/34/8ac353f3a05d163c598dac0bd676b9378d3195e22885d77246b340448429/db_extract-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0261bc0c58c3c2047f7c7949308d563a09084ab35b98c62cd3f70e23b49d0dc5",
          "md5": "0a0024fd3556a930c1d8a681ac335da2",
          "sha256": "95199386e085ab82a2066f742cedd699fc4f46e16400af873794d266816d11a0"
        },
        "downloads": -1,
        "filename": "db_extract-0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0a0024fd3556a930c1d8a681ac335da2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 23105,
        "upload_time": "2020-04-06T17:50:49",
        "upload_time_iso_8601": "2020-04-06T17:50:49.119056Z",
        "url": "https://files.pythonhosted.org/packages/02/61/bc0c58c3c2047f7c7949308d563a09084ab35b98c62cd3f70e23b49d0dc5/db_extract-0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0261bc0c58c3c2047f7c7949308d563a09084ab35b98c62cd3f70e23b49d0dc5",
        "md5": "0a0024fd3556a930c1d8a681ac335da2",
        "sha256": "95199386e085ab82a2066f742cedd699fc4f46e16400af873794d266816d11a0"
      },
      "downloads": -1,
      "filename": "db_extract-0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0a0024fd3556a930c1d8a681ac335da2",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 23105,
      "upload_time": "2020-04-06T17:50:49",
      "upload_time_iso_8601": "2020-04-06T17:50:49.119056Z",
      "url": "https://files.pythonhosted.org/packages/02/61/bc0c58c3c2047f7c7949308d563a09084ab35b98c62cd3f70e23b49d0dc5/db_extract-0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}