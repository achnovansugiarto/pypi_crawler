{
  "info": {
    "author": "Martin Pettersson",
    "author_email": "me@martindbp.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# MerkL - ML pipelines in pure Python with great caching\n\nMerkL is a tool for creating ML pipelines in pure Python that are useful for\ndevelopment and experimentation, but also easy to deploy to production without\nmodifications. Results are cached using Merkle hashes of the code and inputs as\nkeys, greatly simplifying caching and reducing the need for a feature store.\n\nNOTE: this project is not production ready and maybe never will be. At this\npoint I'm the only user. If you're interested in talking about it I can be\nreached at me@martindbp.com.\n\n## Installation\n\n`pip install merkl`\n\n## Tour and examples\n\nIn MerkL, pipelines are built using functions decorated with the `task` decorator. When a task is called, the function\nbody is not exectuted immediately, but instead `Future` objects are returned in place of real outputs. These can then be\npassed on to other tasks:\n\n<table>\n<tr>\n<th>pipeline1.py</th>\n<th>merkl dot pipeline1.my_pipeline 42 | dot -Tpng | display</th>\n</tr>\n<tr>\n<td valign=\"top\">\n\n```python\nfrom merkl import task\n\n@task\ndef task1(input_value):\n    return 2 * input_value\n\n@task\ndef task2(input_value):\n    return input_value ** 2\n\ndef my_pipeline(input_value: int):\n    val = task1(input_value)\n    final_val = task2(val)\n    return final_val\n```\n\n</td>\n<td align=\"center\" valign=\"top\">\n\n![](docs/pipeline1.png)\n\n</td>\n\n</tr>\n\n<tr>\n<th colspan=\"2\">merkl run pipeline1.my_pipeline 42</th>\n</tr>\n<tr>\n<td colspan=\"2\" valign=\"top\">\n\n```\n7056\n```\n\n</td>\n<tr>\n</table>\n\nNo function body is executed before `.eval()` is called on a Future, or it is returned by a pipeline executed by the `run` command.\nInstead a graph is built and each Future is assigned a hash that uniquely identifies its future value. If the code or input values change, then the\noutput Future hashes also change. These hashes are then used to find cached results.\n\nAs seen in the table above, one can use the `merkl run` command to run a pipline, and the `merkl dot` command to create\na dot file for visualizing the computational graph. The graph can be rendered and displayed by piping the dot file\nthrough the `dot` and `visualize` programs (note that these require that graphviz and imagemagick are installed).\n\nArguments can be passed to the pipeline through the `run` command. See [clize](https://clize.readthedocs.io/en/stable/)\nfor more information on how to pass parameters from the command line.\n\nBy default, all computed outputs are cached, unless the `--no-cache` option is supplied. If we run the pipeline a second time, we can see that the hashes turn green, which indicates that the values are already in the cache. Changing the code in `task2`, for example by changing the `2` to a `3`, we can see that the output value is\nnot cached anymore:\n\n<table>\n<tr>\n<th>First run</th>\n<th>Second run</th>\n<th>Modify task2</th>\n</tr>\n\n<tr>\n<td>\n\n![](docs/pipeline1.png)\n\n</td>\n<td>\n\n![](docs/pipeline1_2.png)\n\n</td>\n<td>\n\n![](docs/pipeline1_3.png)\n\n</td>\n</tr>\n\n</table>\n\nHere's a slightly more realistic pipeline consisting of a model training and evaluation task:\n\n<table>\n<tr>\n<th>pipeline2.py</th>\n<th>merkl dot pipeline2.train_eval  | dot -Tpng | display</th>\n</tr>\n<tr>\n<td valign=\"top\">\n\n```python\nfrom merkl import task, read_future\n\n@task\ndef train(data, iterations):\n    return 'trained model'\n\n@task\ndef evaluate(model, data):\n    return 99.3\n\ndef train_eval():\n    train_data = read_future('train.csv')\n    test_data = read_future('test.csv')\n    model = train(train_data, iterations=100)\n    model > 'model.bin'\n    score = evaluate(model, test_data)\n    return score, model\n```\n\n</td>\n<td align=\"center\" valign=\"top\">\n\n![](docs/pipeline2.png)\n\n</td>\n\n</tr>\n\n<tr>\n<th colspan=\"2\">merkl run pipeline2.train_eval </th>\n</tr>\n<tr>\n<td colspan=\"2\" valign=\"top\">\n\n```\n(99.3, 'trained model')\n```\n\n</td>\n<tr>\n</table>\n\n### Hashing\n\nIn order to compute the recursive Merkle hashes, the content (md5) hash of the\ninput files are needed. MerkL hashes these on demand as they are needed, but\nstores these hashes in the `.merkl/cache.sqlite3` database. If the timestamp of\nthe file ever changes, the file is hashed again.\n\nFiles created by MerkL tasks or pipelines are also tracked in the database so that MerkL knows if a file needs to be updated or not.\n\n### Multiple outs\n\nThe number of outputs that you want to track separately has to be determinable at DAG \"build\" time,\ntherefore it cannot be dependent on some value computed inside a task.\n\nBy default, MerkL tries to guess the number of outs by looking at the return statements in the AST (Abstract Syntax\nTree) using Python's inspection capabilities. If the return statement is a Tuple or Dict literal, the values are split\ninto separate Futures:\n\n<table>\n<tr>\n<th>Tuple multiple outs</th>\n<th>Tuple single out</th>\n<th>Dict multiple outs</th>\n</tr>\n<tr>\n<td valign=\"top\">\n\n```python\n@task\ndef my_task():\n    return 1, 2, 3\n\nprint(my_task())\n```\n\n</td>\n<td valign=\"top\">\n\n```python\n@task\ndef my_task():\n    values = 1, 2, 3\n    return values\n\nprint(my_task())\n```\n\n</td>\n<td valign=\"top\">\n\n```python\n@task\ndef my_task():\n    return {\n        'key1': 1,\n        'key2': 2,\n        'key3': 3,\n    }\n\nprint(my_task())\n```\n\n</td>\n</tr>\n\n<tr>\n<th colspan=\"3\">STDOUT</th>\n</tr>\n\n<tr>\n<td valign=\"top\">\n\n```\n(\n    <Future: b7a152bd>,\n    <Future: 3612309d>,\n    <Future: 541053ee>,\n)\n```\n\n</td>\n<td valign=\"top\">\n\n```\n<Future: bc930b90>\n```\n\n</td>\n<td valign=\"top\">\n\n```\n{\n    'key1': <Future: 7d7f02e7>,\n    'key2': <Future: 09f5f892>,\n    'key3': <Future: aaba8931>,\n}\n```\n\n</td>\n</tr>\n</table>\n\nIn some cases we want the number of outs to be dynamic and depend on the input to the function. In this case you can\nsupply a callable to the `outs` task parameter with the same signature as the task function, and return the number of\nouts:\n\n<table>\n<tr>\n<th>outs_lambda.py</th>\n<th>merkl dot outs_lambda.pipeline  | dot -Tpng | display</th>\n</tr>\n<tr>\n<td valign=\"top\">\n\n```python\nfrom merkl import task\n\n@task(outs=lambda data, k: k)\ndef split(data, k):\n    ksize = len(data) // k\n    return [data[i*ksize:(i+1)*ksize]\n            for i in range(k)]\n\ndef pipeline():\n    return split([1, 2, 3, 4, 5, 6], k=2)\n```\n\n</td>\n<td align=\"center\" valign=\"top\">\n\n![](docs/pipeline3.png)\n\n</td>\n\n</tr>\n\n<tr>\n<th colspan=\"2\">merkl run outs_lambda.pipeline </th>\n</tr>\n<tr>\n<td colspan=\"2\" valign=\"top\">\n\n```\n([1, 2, 3], [4, 5, 6])\n```\n\n</td>\n<tr>\n</table>\n\nIf all else fails, you can set `outs` to any positive integer or an iterable of output dictionary keys.\n\n### Pipe syntax\n\nFor convenience, you can optionally chain tasks using the `|` operator, if the tasks have a single input and output. A\nfuture can also be \"piped\" directly to a file path using the `>` operator:\n\n```python\n# Original\nwrite_future(train(preprocess(read_future('train.csv'))), 'model.bin')\n\n# With pipe syntax\nread_future('train.csv') | preprocess | train > 'model.bin'\n```\n\n### Pipelines\n\nPipeline functions can optionally be decorated with the `pipeline` decorator. The decorator provides caching of the\ngraph construction (as opposed to only the evaluation). This is useful when for example there is an inference stage\nwhich depends on a training stage which preprocesses thousands of images. Even though the _result_ of the training is\ncached, the _construction_ of the graph may become slow.\n\n```python\n\n@task\ndef preprocess(image):\n    ...\n\n@task(outs=lambda image, k: k)\ndef augment(image, k):\n    ...\n\n@task\ndef train(\n\n```\n\n### Batch tasks\n\nSome functions have a batch version that has a more efficient implementation than the single item version. In this case, we\nmight want to track the outputs from the batch task _as if_ they were produced by the single item version, such that\noutputs between them will be cached. In this case you can use the `batch` decorator:\n\n```python\n@batch(embed_word)\ndef embed_words(words):\n    ...\n    return embedded_words\n```\n\nIn some cases, you might only have a batch implementation, but you want each output to be treated individually. In that\ncase you can also use the `batch` decorator but without an argument:\n\n```python\n\n@batch\ndef embed_words(words):\n    ...\n    return embedded_words\n\n```\nThe difference here is that identical inputs will have the same output Future hash, which is not true for a regular\ntask. You can tell the difference in these two graphs:\n\n<table>\n<tr>\n<th>1</th>\n<th>2</th>\n</tr>\n<tr>\n<td valign=\"top\">\n\n![](docs/pipeline4_1.png)\n\n</td>\n<td valign=\"top\">\n\n![](docs/pipeline4_2.png)\n\n</td>\n</tr>\n\n</table>\n\n\n```python\nouts = embed_sentences_batch(['my sentence', 'my sentence'])\nassert outs[0].hash == outs[1].hash\n\nouts = embed_sentences_task(['my sentence', 'my sentence'])\nassert outs[0].hash != outs[1].hash\n```\n\n### HashMode and dependencies\n\nWhen the hash of a task function is determined, there are three different `HashMode`s: `FUNCTION`, `MODULE` and\n`FIND_DEPS`. The default `HashMode` for both `task`, `batch` and `pipeline` is `FIND_DEPS`.\n\n* `FUNCTION`: Uses the code of the task function for hashing\n* `MODULE`: Uses the whole module file\n* `FIND_DEPS`: Finds dependencies such as other functions or nonlocal variables used within the function that are\n  defined in the same module. If a dep is a `task` then its dependencies are added in turn.\n\nDeps can be added manually and could be a module, function, task, bytes, string or any JSON-serializable object:\n\n```python\n@task(deps=['my string'])\ndef my_task():\n    pass\n```\n\nAs a convenience, you can use the `pip_version()` function to easily add a library's version string as a hash key:\n\n```python\n@task(deps=[pip_version('numpy')])\ndef my_task():\n    pass\n```\n\nFiles and directories can be added as dependencies using the `FileRef` and `DirRef` classes:\n\n```python\nfrom merkl import task, FileRef\n@task(deps=[FileRef('my_file.txt')])\ndef my_task():\n    pass\n```\n\nRefs added as dependencies will contribute the file content hash to the task.\n\n### Filesystem IO\n\nTBD\n\n### Task versions\n\nTBD\n\n### The Cache\n\nTBD\n\n### Wrapping shell commands\n\nTBD\n\n## Dev Notes\n\nReleasing a new version:\n\n1. Bump version in setup.py\n2. Commit\n3. `git tag $VERSION`\n4. `make release`. This bundles the package and uploads it using twine\n\nNote that twine is required: `pip install twine`\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/martindbp/merkl",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "merkl",
    "package_url": "https://pypi.org/project/merkl/",
    "platform": null,
    "project_url": "https://pypi.org/project/merkl/",
    "project_urls": {
      "Homepage": "https://github.com/martindbp/merkl"
    },
    "release_url": "https://pypi.org/project/merkl/0.4/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "MerkL",
    "version": "0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16024020,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7780b4184617c2f8c3a24a84b8ebccd18c000b5beb44f02b7819057df4ff37ca",
          "md5": "cc22c7af2965fc4bae97d18694815cae",
          "sha256": "5fb90184bd4071509056660cfa359957752b47a8a965b42a6e3ecb1b7e7986ed"
        },
        "downloads": -1,
        "filename": "merkl-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "cc22c7af2965fc4bae97d18694815cae",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 37245,
        "upload_time": "2022-12-03T17:18:32",
        "upload_time_iso_8601": "2022-12-03T17:18:32.224620Z",
        "url": "https://files.pythonhosted.org/packages/77/80/b4184617c2f8c3a24a84b8ebccd18c000b5beb44f02b7819057df4ff37ca/merkl-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9293777af2364b7c4696b32222400dee59e3058f14e8d09f8a061b18ef5804f4",
          "md5": "b9086939ad23d681244cd50803edfa34",
          "sha256": "384c2e4730236ef8453420d190de5d7e3f52cae56513538e28feb12452e06b17"
        },
        "downloads": -1,
        "filename": "merkl-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "b9086939ad23d681244cd50803edfa34",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 37354,
        "upload_time": "2022-12-04T09:23:28",
        "upload_time_iso_8601": "2022-12-04T09:23:28.280169Z",
        "url": "https://files.pythonhosted.org/packages/92/93/777af2364b7c4696b32222400dee59e3058f14e8d09f8a061b18ef5804f4/merkl-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f67b431ed5a9c9b77d3dfe558a0c9e9282ac5eb77de886940ee4e05926054bce",
          "md5": "e9da4c6f9b7037709f966745f682c892",
          "sha256": "b22ad4ef3087a2e277d5707c93aaf342663a94e4df563341ae6fe4fdca236447"
        },
        "downloads": -1,
        "filename": "merkl-0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "e9da4c6f9b7037709f966745f682c892",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 37375,
        "upload_time": "2022-12-04T09:25:46",
        "upload_time_iso_8601": "2022-12-04T09:25:46.652506Z",
        "url": "https://files.pythonhosted.org/packages/f6/7b/431ed5a9c9b77d3dfe558a0c9e9282ac5eb77de886940ee4e05926054bce/merkl-0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "42d6c7c512076af999516ae24ff4e5e561117bcbe93d3d297a45e8e7eeb7debf",
          "md5": "33b791c4672837ceefac90ca1546a922",
          "sha256": "e5077385666393c173060e59ff2e1f0de9f8916da41f2215802b5df5caa4eba2"
        },
        "downloads": -1,
        "filename": "merkl-0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "33b791c4672837ceefac90ca1546a922",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 37455,
        "upload_time": "2022-12-07T19:17:29",
        "upload_time_iso_8601": "2022-12-07T19:17:29.938452Z",
        "url": "https://files.pythonhosted.org/packages/42/d6/c7c512076af999516ae24ff4e5e561117bcbe93d3d297a45e8e7eeb7debf/merkl-0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "42d6c7c512076af999516ae24ff4e5e561117bcbe93d3d297a45e8e7eeb7debf",
        "md5": "33b791c4672837ceefac90ca1546a922",
        "sha256": "e5077385666393c173060e59ff2e1f0de9f8916da41f2215802b5df5caa4eba2"
      },
      "downloads": -1,
      "filename": "merkl-0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "33b791c4672837ceefac90ca1546a922",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 37455,
      "upload_time": "2022-12-07T19:17:29",
      "upload_time_iso_8601": "2022-12-07T19:17:29.938452Z",
      "url": "https://files.pythonhosted.org/packages/42/d6/c7c512076af999516ae24ff4e5e561117bcbe93d3d297a45e8e7eeb7debf/merkl-0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}