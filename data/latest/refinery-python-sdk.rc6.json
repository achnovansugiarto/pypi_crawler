{
  "info": {
    "author": "jhoetter",
    "author_email": "johannes.hoetter@kern.ai",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3"
    ],
    "description": "[![refinery repository](https://uploads-ssl.webflow.com/61e47fafb12bd56b40022a49/62cf1c3cb8272b1e9c01127e_refinery%20sdk%20banner.png)](https://github.com/code-kern-ai/refinery)\n[![Python 3.9](https://img.shields.io/badge/python-3.9-blue.svg)](https://www.python.org/downloads/release/python-390/)\n[![pypi 1.4.0](https://img.shields.io/badge/pypi-1.4.0-yellow.svg)](https://pypi.org/project/refinery-python-sdk/1.4.0/)\n\nThis is the official Python SDK for [*refinery*](https://github.com/code-kern-ai/refinery), the **open-source** data-centric IDE for NLP.\n\n**Table of Contents**\n- [Installation](#installation)\n- [Usage](#usage)\n  - [Creating a `Client` object](#creating-a-client-object)\n  - [Fetching labeled data](#fetching-labeled-data)\n  - [Fetching lookup lists](#fetching-lookup-lists)\n  - [Upload files](#upload-files)\n  - [Adapters](#adapters)\n    - [Sklearn](#sklearn-adapter)\n    - [PyTorch](#pytorch-adapter)\n    - [HuggingFace](#hugging-face-adapter)\n    - [Rasa](#rasa-adapter)\n  - [Callbacks](#callbacks)\n    - [Sklearn](#sklearn-callback)\n    - [PyTorch](#pytorch-callback)\n    - [HuggingFace](#hugging-face-callback)\n- [Contributing](#contributing)\n- [License](#license)\n- [Contact](#contact)\n\nIf you like what we're working on, please leave a ‚≠ê!\n\n## Installation\n\nYou can set up this SDK either via running `$ pip install refinery-python-sdk`, or by cloning this repository and running `$ pip install -r requirements.txt`.\n\n## Usage\n\n### Creating a `Client` object\nOnce you installed the package, you can create a `Client` object from any Python terminal as follows:\n\n```python\nfrom refinery import Client\n\nuser_name = \"your-username\" # this is the email you log in with\npassword = \"your-password\"\nproject_id = \"your-project-id\" # can be found in the URL of the web application\n\nclient = Client(user_name, password, project_id)\n# if you run the application locally, please use the following instead:\n# client = Client(user_name, password, project_id, uri=\"http://localhost:4455\")\n```\n\nThe `project_id` can be found in your browser, e.g. if you run the app on your localhost: `http://localhost:4455/app/projects/{project_id}/overview`\n\nAlternatively, you can provide a `secrets.json` file in your directory where you want to run the SDK, looking as follows:\n```json\n{\n    \"user_name\": \"your-username\",\n    \"password\": \"your-password\",\n    \"project_id\": \"your-project-id\"\n}\n```\n\nAgain, if you run on your localhost, you should also provide `\"uri\": \"http://localhost:4455\"`. Afterwards, you can access the client like this:\n\n```python\nclient = Client.from_secrets_file(\"secrets.json\")\n```\n\nWith the `Client`, you easily integrate your data into any kind of system; may it be a custom implementation, an AutoML system or a plain data analytics framework üöÄ\n\n### Fetching labeled data\n\nNow, you can easily fetch the data from your project:\n```python\ndf = client.get_record_export(tokenize=False)\n# if you set tokenize=True (default), the project-specific \n# spaCy tokenizer will process your textual data\n```\n\nAlternatively, you can also just run `rsdk pull` in your CLI given that you have provided the `secrets.json` file in the same directory.\n\nThe `df` contains both your originally uploaded data (e.g. `headline` and `running_id` if you uploaded records like `{\"headline\": \"some text\", \"running_id\": 1234}`), and a triplet for each labeling task you create. This triplet consists of the manual labels, the weakly supervised labels, and their confidence. For extraction tasks, this data is on token-level.\n\nAn example export file looks like this:\n```json\n[\n  {\n    \"running_id\": \"0\",\n    \"Headline\": \"T. Rowe Price (TROW) Dips More Than Broader Markets\",\n    \"Date\": \"Jun-30-22 06:00PM\\u00a0\\u00a0\",\n    \"Headline__Sentiment Label__MANUAL\": null,\n    \"Headline__Sentiment Label__WEAK_SUPERVISION\": \"Negative\",\n    \"Headline__Sentiment Label__WEAK_SUPERVISION__confidence\": \"0.6220\"\n  }\n]\n```\n\nIn this example, there is no manual label, but a weakly supervised label `\"Negative\"` has been set with 62.2% confidence.\n\n### Fetching lookup lists\nIn your project, you can create lookup lists to implement distant supervision heuristics. To fetch your lookup list(s), you can either get all or fetch one by its list id.\n```python\nlist_id = \"your-list-id\"\nlookup_list = client.get_lookup_list(list_id)\n```\n\nThe list id can be found in your browser URL when you're on the details page of a lookup list, e.g. when you run on localhost: `http://localhost:4455/app/projects/{project_id}/knowledge-base/{list_id}`.\n\nAlternatively, you can pull all lookup lists:\n```python\nlookup_lists = client.get_lookup_lists()\n```\n\n### Upload files\nYou can import files directly from your machine to your application:\n\n```python\nfile_path = \"my/file/path/data.json\"\nupload_was_successful = client.post_file_import(file_path)\n```\n\nWe use Pandas to process the data you upload, so you can also provide `import_file_options` for the file type you use. Currently, you need to provide them as a `\\n`-separated string (e.g. `\"quoting=1\\nsep=';'\"`). We'll adapt this in the future to work with dictionaries instead.\n\nAlternatively, you can `rsdk push <path-to-your-file>` via CLI, given that you have provided the `secrets.json` file in the same directory.\n\n**Make sure that you've selected the correct project beforehand, and fit the data schema of existing records in your project!**\n\n### Adapters\n\n#### Sklearn Adapter\nYou can use *refinery* to directly pull data into a format you can apply for building [sklearn](https://github.com/scikit-learn/scikit-learn) models. This can look as follows:\n\n```python\nfrom refinery.adapter.sklearn import build_classification_dataset\nfrom sklearn.tree import DecisionTreeClassifier\n\ndata = build_classification_dataset(client, \"headline\", \"__clickbait\", \"distilbert-base-uncased\")\n\nclf = DecisionTreeClassifier()\nclf.fit(data[\"train\"][\"inputs\"], data[\"train\"][\"labels\"])\n\npred_test = clf.predict(data[\"test\"][\"inputs\"])\naccuracy = (pred_test == data[\"test\"][\"labels\"]).mean()\n```\n\nBy the way, we can highly recommend to combine this with [Truss](https://github.com/basetenlabs/truss) for easy model serving!\n\n#### PyTorch Adapter\nIf you want to build a [PyTorch](https://github.com/pytorch/pytorch) network, you can build the `train_loader` and `test_loader` as follows:\n\n```python\nfrom refinery.adapter.torch import build_classification_dataset\ntrain_loader, test_loader, encoder, index = build_classification_dataset(\n    client, \"headline\", \"__clickbait\", \"distilbert-base-uncased\"\n)\n```\n\n#### Hugging Face Adapter\nTransformers are great, but often times, you want to finetune them for your downstream task. With *refinery*, you can do so easily by letting the SDK build the dataset for you that you can use as a plug-and-play base for your training:\n\n```python\nfrom refinery.adapter import transformers\ndataset, mapping = transformers.build_dataset(client, \"headline\", \"__clickbait\")\n```\n\nFrom here, you can follow the [finetuning example](https://huggingface.co/docs/transformers/training) provided in the official Hugging Face documentation. A next step could look as follows:\n\n```python\nsmall_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1000))\n\nfrom transformers import (\n  AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n)\nimport numpy as np\nfrom datasets import load_metric\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"headline\"], padding=\"max_length\", truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\ntraining_args = TrainingArguments(output_dir=\"test_trainer\")\nmetric = load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n\nsmall_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\nsmall_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=small_train_dataset,\n    eval_dataset=small_eval_dataset,\n    compute_metrics=compute_metrics,\n)\n\ntrainer.train()\n\ntrainer.save_model(\"path/to/model\")\n```\n\n#### Rasa Adapter\n*refinery* is perfect to be used for building chatbots with [Rasa](https://github.com/RasaHQ/rasa). We've built an adapter with which you can easily create the required Rasa training data directly from *refinery*.\n\nTo do so, do the following:\n\n```python\nfrom refinery.adapter import rasa\n\nrasa.build_intent_yaml(\n  client,\n  \"text\",\n  \"__intent__WEAK_SUPERVISION\"\n)\n```\n\nThis will create a `.yml` file looking as follows:\n\n```yml\nnlu:\n- intent: check_balance\n  examples: |\n    - how much do I have on my savings account\n    - how much money is in my checking account\n    - What's the balance on my credit card account\n```\n\nIf you want to provide a metadata-level label (such as sentiment), you can provide the optional argument `metadata_label_task`:\n\n```python\nfrom refinery.adapter import rasa\n\nrasa.build_intent_yaml(\n  client,\n  \"text\",\n  \"__intent__WEAK_SUPERVISION\",\n  metadata_label_task=\"__sentiment__WEAK_SUPERVISION\"\n)\n```\n\nThis will create a file like this:\n```yml\nnlu:\n- intent: check_balance\n  metadata:\n    sentiment: neutral\n  examples: |\n    - how much do I have on my savings account\n    - how much money is in my checking account\n    - What's the balance on my credit card account\n```\n\nAnd if you have entities in your texts which you'd like to recognize, simply add the `tokenized_label_task` argument:\n\n```python\nfrom refinery.adapter import rasa\n\nrasa.build_intent_yaml(\n  client,\n  \"text\",\n  \"__intent__WEAK_SUPERVISION\",\n  metadata_label_task=\"__sentiment__WEAK_SUPERVISION\",\n  tokenized_label_task=\"text__entities__WEAK_SUPERVISION\"\n)\n```\n\nThis will not only inject the label names on token-level, but also creates lookup lists for your chatbot:\n\n```yml\nnlu:\n- intent: check_balance\n  metadata:\n    sentiment: neutral\n  examples: |\n    - how much do I have on my [savings](account) account\n    - how much money is in my [checking](account) account\n    - What's the balance on my [credit card account](account)\n- lookup: account\n  examples: |\n    - savings\n    - checking\n    - credit card account\n```\n\nPlease make sure to also create the further necessary files (`domain.yml`, `data/stories.yml` and `data/rules.yml`) if you want to train your Rasa chatbot. For further reference, see their [documentation](https://rasa.com/docs/rasa).\n\n\n### Callbacks\nIf you want to feed your production model's predictions back into *refinery*, you can do so with any version greater than [1.2.1](https://github.com/code-kern-ai/refinery/releases/tag/v1.2.1).\n\nTo do so, we have a generalistic interface and framework-specific classes.\n\n#### Sklearn Callback\nIf you want to train a scikit-learn model an feed its outputs back into the refinery, you can do so easily as follows:\n\n```python\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression() # we use this as an example, but you can use any model implementing predict_proba\n\nfrom refinery.adapter.sklearn import build_classification_dataset\ndata = build_classification_dataset(client, \"headline\", \"__clickbait\", \"distilbert-base-uncased\")\nclf.fit(data[\"train\"][\"inputs\"], data[\"train\"][\"labels\"])\n\nfrom refinery.callbacks.sklearn import SklearnCallback\ncallback = SklearnCallback(\n    client, \n    clf,\n    \"clickbait\", \n)\n\n# executing this will call the refinery API with batches of size 32, so your data is pushed to the app\ncallback.run(data[\"train\"][\"inputs\"], data[\"train\"][\"index\"])\ncallback.run(data[\"test\"][\"inputs\"], data[\"test\"][\"index\"])\n```\n\n#### PyTorch Callback\nFor PyTorch, the procedure is really similar. You can do as follows:\n\n```python\nfrom refinery.adapter.torch import build_classification_dataset\ntrain_loader, test_loader, encoder, index = build_classification_dataset(\n    client, \"headline\", \"__clickbait\", \"distilbert-base-uncased\"\n)\n\n# build your custom model and train it here - example:\nimport torch.nn as nn\nimport numpy as np\nimport torch\n\n# number of features (len of X cols)\ninput_dim = 768\n# number of hidden layers\nhidden_layers = 20\n# number of classes (unique of y)\noutput_dim = 2\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.linear1 = nn.Linear(input_dim, output_dim)\n   \n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))\n        return x\n    \nclf = Network()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(clf.parameters(), lr=0.1)\n\nepochs = 2\nfor epoch in range(epochs):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        # set optimizer to zero grad to remove previous epoch gradients\n        optimizer.zero_grad()\n        # forward propagation\n        outputs = clf(inputs)\n        loss = criterion(outputs, labels)\n        # backward propagation\n        loss.backward()\n        # optimize\n        optimizer.step()\n        running_loss += loss.item()\n        # display statistics\n        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.5f}')\n        running_loss = 0.0\n\n# with this model trained, you can use the callback\nfrom refinery.callbacks.torch import TorchCallback\ncallback = TorchCallback(\n    client, \n    clf,\n    \"clickbait\", \n    encoder\n)\n\n# and just execute this \ncallback.run(train_loader, index[\"train\"])\ncallback.run(test_loader, index[\"test\"])\n```\n\n#### HuggingFace Callback\nCollect the dataset and train your custom transformer model as follows:\n\n```python\nfrom refinery.adapter import transformers\ndataset, mapping, index = transformers.build_classification_dataset(client, \"headline\", \"__clickbait\")\n\n# train a model here, we're simplifying this by just using an existing model w/o retraining\nfrom transformers import pipeline\npipe = pipeline(\"text-classification\", model=\"distilbert-base-uncased\")\n\n# if you're interested to see how a training looks like, look into the above HuggingFace adapter\n\n# you can now apply the callback\nfrom refinery.callbacks.transformers import TransformerCallback\ncallback = TransformerCallback(\n    client, \n    pipe,\n    \"clickbait\", \n    mapping\n)\n\ncallback.run(dataset[\"train\"][\"headline\"], index[\"train\"])\ncallback.run(dataset[\"test\"][\"headline\"], index[\"test\"])\n```\n\n#### Generic Callback\nThis one is your fallback if you have a very custom solution; other than that, we recommend you look into the framework-specific classes.\n\n```python\nfrom refinery.callbacks.inference import ModelCallback\nfrom refinery.adapter.sklearn import build_classification_dataset\nfrom sklearn.linear_model import LogisticRegression\n\ndata = build_classification_dataset(client, \"headline\", \"__clickbait\", \"distilbert-base-uncased\"0)\nclf = LogisticRegression()\nclf.fit(data[\"train\"][\"inputs\"], data[\"train\"][\"labels\"])\n\n# you can build initialization functions that set states of objects you use in the pipeline\ndef initialize_fn(inputs, labels, **kwargs):\n    return {\"clf\": kwargs[\"clf\"]}\n\n# postprocessing shifts the model outputs into a format accepted by our API\ndef postprocessing_fn(outputs, **kwargs):\n    named_outputs = []\n    for prediction in outputs:\n        pred_index = prediction.argmax()\n        label = kwargs[\"clf\"].classes_[pred_index]\n        confidence = prediction[pred_index]\n        named_outputs.append([label, confidence])\n    return named_outputs\n\ncallback = ModelCallback(\n    client: Client,\n    \"my-custom-regression\",\n    \"clickbait\",\n    inference_fn=clf.predict_proba,\n    initialize_fn=initialize_fn,\n    postprocessing_fn=postprocessing_fn\n)\n\n# executing this will call the refinery API with batches of size 32\ncallback.initialize_and_run(data[\"train\"][\"inputs\"], data[\"train\"][\"index\"])\ncallback.run(data[\"test\"][\"inputs\"], data[\"test\"][\"index\"])\n```\n\n\n## Contributing\nContributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are **greatly appreciated**.\n\nIf you have a suggestion that would make this better, please fork the repo and create a pull request. You can also simply open an issue with the tag \"enhancement\".\n\n1. Fork the Project\n2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the Branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\nAnd please don't forget to leave a ‚≠ê if you like the work! \n\n## License\nDistributed under the MIT License. See LICENSE.txt for more information.\n\n## Contact\nThis library is developed and maintained by [Kern AI](https://github.com/code-kern-ai). If you want to provide us with feedback or have some questions, don't hesitate to contact us. We're super happy to help ‚úåÔ∏è\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/code-kern-ai/refinery-python",
    "keywords": "Kern AI,refinery,machine-learning,supervised-learning,data-centric-ai,data-annotation,python",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "refinery-python-sdk",
    "package_url": "https://pypi.org/project/refinery-python-sdk/",
    "platform": null,
    "project_url": "https://pypi.org/project/refinery-python-sdk/",
    "project_urls": {
      "Homepage": "https://github.com/code-kern-ai/refinery-python"
    },
    "release_url": "https://pypi.org/project/refinery-python-sdk/1.4.0/",
    "requires_dist": [
      "numpy",
      "pandas",
      "requests",
      "boto3",
      "botocore",
      "spacy",
      "wasabi",
      "embedders",
      "datasets"
    ],
    "requires_python": "",
    "summary": "Official Python SDK for Kern AI refinery.",
    "version": "1.4.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15395836,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "250f37cea175c3a12abcdff08f4e8b67a01e2d88d76541513719f6eed3c64e2a",
          "md5": "166aa41dc9d4160dd99b329f43eb7aef",
          "sha256": "dc148702e340992547031f5f730a2535ced9ef9d582b187d557e25065580f69f"
        },
        "downloads": -1,
        "filename": "refinery_python_sdk-1.0.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "166aa41dc9d4160dd99b329f43eb7aef",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 15977,
        "upload_time": "2022-07-15T14:27:54",
        "upload_time_iso_8601": "2022-07-15T14:27:54.781505Z",
        "url": "https://files.pythonhosted.org/packages/25/0f/37cea175c3a12abcdff08f4e8b67a01e2d88d76541513719f6eed3c64e2a/refinery_python_sdk-1.0.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9c17c4fd2e898ff5e638ab24ea22f3112493ff466a518e89ac5b91e577a00e25",
          "md5": "8a774263ac2f8952614211521a4c913f",
          "sha256": "ad7c0260bad1fa35eec036bd80b4fe1eda51f8bfd9b38a43bbb459a18e6678ed"
        },
        "downloads": -1,
        "filename": "refinery_python_sdk-1.0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8a774263ac2f8952614211521a4c913f",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 15984,
        "upload_time": "2022-07-15T16:03:03",
        "upload_time_iso_8601": "2022-07-15T16:03:03.638911Z",
        "url": "https://files.pythonhosted.org/packages/9c/17/c4fd2e898ff5e638ab24ea22f3112493ff466a518e89ac5b91e577a00e25/refinery_python_sdk-1.0.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5bb445207b418a6165fc7443d043740dcec5f44bace362e14ea46850ab406605",
          "md5": "717cddcdb91c738463016429924adcca",
          "sha256": "ecf641bdecf8b6953a3ebfea088f6fe3768ea3da8ee7b356fdde00d00694ff6c"
        },
        "downloads": -1,
        "filename": "refinery_python_sdk-1.0.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "717cddcdb91c738463016429924adcca",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 15980,
        "upload_time": "2022-07-15T16:04:09",
        "upload_time_iso_8601": "2022-07-15T16:04:09.311937Z",
        "url": "https://files.pythonhosted.org/packages/5b/b4/45207b418a6165fc7443d043740dcec5f44bace362e14ea46850ab406605/refinery_python_sdk-1.0.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f5f36b62959ed2e97c9d41e9e6ebc7d6348a19eabd7351720df97a54e12d8fab",
          "md5": "20e5394e68dae57a2348e565375e37d8",
          "sha256": "ad07b2737a3287cb0a23e0d1708c2bba3282febbdcd97b070390cbe6ed1810a0"
        },
        "downloads": -1,
        "filename": "refinery_python_sdk-1.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "20e5394e68dae57a2348e565375e37d8",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 22736,
        "upload_time": "2022-09-05T09:03:46",
        "upload_time_iso_8601": "2022-09-05T09:03:46.007543Z",
        "url": "https://files.pythonhosted.org/packages/f5/f3/6b62959ed2e97c9d41e9e6ebc7d6348a19eabd7351720df97a54e12d8fab/refinery_python_sdk-1.1.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "074e5d06fb8d60ef4b55fbf28cbcbafc92c21c64cda147694162f247afaee1fa",
          "md5": "99ee85645ca9ba2168c5da13ccf1eaf1",
          "sha256": "657122cace8210f0dd36bada9e2d7000a7cef108c39f0874a949f372a6dddae7"
        },
        "downloads": -1,
        "filename": "refinery_python_sdk-1.3.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "99ee85645ca9ba2168c5da13ccf1eaf1",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 30420,
        "upload_time": "2022-09-27T13:54:36",
        "upload_time_iso_8601": "2022-09-27T13:54:36.456009Z",
        "url": "https://files.pythonhosted.org/packages/07/4e/5d06fb8d60ef4b55fbf28cbcbafc92c21c64cda147694162f247afaee1fa/refinery_python_sdk-1.3.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "716375356ee3e7ecf247d68622abe5f8f9aa517ab76cb8d38518d7ad01520d7a",
          "md5": "935c5128bd68482d91a0ac5eda771fe8",
          "sha256": "a180b7117153819d5477e3e5095d53eebe2d10eb5a52b229b686bc102854719e"
        },
        "downloads": -1,
        "filename": "refinery_python_sdk-1.4.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "935c5128bd68482d91a0ac5eda771fe8",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 30412,
        "upload_time": "2022-10-13T07:49:58",
        "upload_time_iso_8601": "2022-10-13T07:49:58.628314Z",
        "url": "https://files.pythonhosted.org/packages/71/63/75356ee3e7ecf247d68622abe5f8f9aa517ab76cb8d38518d7ad01520d7a/refinery_python_sdk-1.4.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "716375356ee3e7ecf247d68622abe5f8f9aa517ab76cb8d38518d7ad01520d7a",
        "md5": "935c5128bd68482d91a0ac5eda771fe8",
        "sha256": "a180b7117153819d5477e3e5095d53eebe2d10eb5a52b229b686bc102854719e"
      },
      "downloads": -1,
      "filename": "refinery_python_sdk-1.4.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "935c5128bd68482d91a0ac5eda771fe8",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 30412,
      "upload_time": "2022-10-13T07:49:58",
      "upload_time_iso_8601": "2022-10-13T07:49:58.628314Z",
      "url": "https://files.pythonhosted.org/packages/71/63/75356ee3e7ecf247d68622abe5f8f9aa517ab76cb8d38518d7ad01520d7a/refinery_python_sdk-1.4.0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}