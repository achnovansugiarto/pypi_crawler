{
  "info": {
    "author": "Yongrae Jo",
    "author_email": "dreamgonfly@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Utilities"
    ],
    "description": "# Transformer-pytorch\nA PyTorch implementation of Transformer from \"Attention is All You Need\" (https://arxiv.org/abs/1706.03762).\n\nThis repo focuses on clean, readable, and modular implementation of the paper.\n\n<img width=\"559\" alt=\"screen shot 2018-09-27 at 1 49 14 pm\" src=\"https://user-images.githubusercontent.com/2340721/46123973-44b08900-c25c-11e8-9468-7aef9e4e3f18.png\">\n\n```commandline\n$ docker build --tag transformer --rm .\n$ docker run --name $CONTAINER_NAME -it --gpus all --shm-size 16G --volume $(pwd):/transformer transformer bash\n```\n\n```commandline\npython main.py v1 prepare multi30k data/Multi30k/\n```\n\n```commandline\npython -m spacy download en\npython -m spacy download de\n\n```\n\n```commandline\npython main.py v1 build-vocab data/Multi30k/train.json results/vocabs/shared_vocab.tsv --source-language de --target-language en --min-freq 3\n```\nVocabulary 9521 lines written to results/vocabs/shared_vocab.tsv\n\n```commandline\nCUDA_VISIBLE_DEVICES=0 python main.py v1 train data/Multi30k/train.json data/Multi30k/val.json results/vocabs/shared_vocab.tsv results/vocabs/shared_vocab.tsv results/runs playground\n```\n\n```commandline\npython main.py v1 prepare aihub data/AIHub/\n```\n\n## Requirements\n- Python 3.6+\n- [PyTorch 4.1+](http://pytorch.org/)\n- [NumPy](http://www.numpy.org/)\n- [NLTK](https://www.nltk.org/)\n- [tqdm](https://github.com/tqdm/tqdm)\n\n## Usage\n\n### Prepare datasets\nThis repo comes with example data in `data/` directory. To begin, you will need to prepare datasets with given data as follows:\n```\n$ python prepare_datasets.py --train_source=data/example/raw/src-train.txt --train_target=data/example/raw/tgt-train.txt --val_source=data/example/raw/src-val.txt --val_target=data/example/raw/tgt-val.txt --save_data_dir=data/example/processed\n```\n\nThe example data is brought from [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py).\nThe data consists of parallel source (src) and target (tgt) data for training and validation.\nA data file contains one sentence per line with tokens separated by a space.\nBelow are the provided example data files.\n\n- `src-train.txt`\n- `tgt-train.txt`\n- `src-val.txt`\n- `tgt-val.txt`\n\n### Train model\nTo train model, provide the train script with a path to processed data and save files as follows:\n\n```\n$ python train.py --data_dir=data/example/processed --save_config=checkpoints/example_config.json --save_checkpoint=checkpoints/example_model.pth --save_log=logs/example.log \n```\n\nThis saves model config and checkpoints to given files, respectively.\nYou can play around with hyperparameters of the model with command line arguments. \nFor example, add `--epochs=300` to set the number of epochs to 300. \n\n### Translate\nTo translate a sentence in source language to target language:\n```\n$ python predict.py --source=\"There is an imbalance here .\" --config=checkpoints/example_config.json --checkpoint=checkpoints/example_model.pth\n\nCandidate 0 : Hier fehlt das Gleichgewicht .\nCandidate 1 : Hier fehlt das das Gleichgewicht .\nCandidate 2 : Hier fehlt das das das Gleichgewicht .\n```\n\nIt will give you translation candidates of the given source sentence.\nYou can adjust the number of candidates with command line argument. \n\n### Evaluate\nTo calculate BLEU score of a trained model:\n```\n$ python evaluate.py --save_result=logs/example_eval.txt --config=checkpoints/example_config.json --checkpoint=checkpoints/example_model.pth\n\nBLEU score : 0.0007947\n```\n\n## File description\n- `models.py` includes Transformer's encoder, decoder, and multi-head attention.\n- `embeddings.py` contains positional encoding.\n- `losses.py` contains label smoothing loss.\n- `optimizers.py` contains Noam optimizer.\n- `metrics.py` contains accuracy metric.\n- `beam.py` contains beam search.\n- `datasets.py` has code for loading and processing data. \n- `trainer.py` has code for training model.\n- `prepare_datasets.py` processes data.\n- `train.py` trains model.\n- `predict.py` translates given source sentence with a trained model.\n- `evaluate.py` calculates BLEU score of a trained model.\n\n## Reference\n- [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py)\n\n## Author\n[@dreamgonfly](https://github.com/dreamgonfly)\n\n\n# Deploy\npython3 setup.py sdist bdist_wheel  \n python3 -m twine upload --repository testpypi dist/* \n python3 -m twine upload --repository dist/*\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/dreamgonfly/transformer-pytorch",
    "keywords": "transformer pytorch translation",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "transformer-pytorch",
    "package_url": "https://pypi.org/project/transformer-pytorch/",
    "platform": "",
    "project_url": "https://pypi.org/project/transformer-pytorch/",
    "project_urls": {
      "Homepage": "https://github.com/dreamgonfly/transformer-pytorch"
    },
    "release_url": "https://pypi.org/project/transformer-pytorch/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "PyTorch implementation of Transformer from \"Attention is All You Need\".",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8832172,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3aee48b50a868973b123b869e2db3092a0bc36405eb9e8cf297154a0f729f303",
          "md5": "aec87c49c9bcafbd5e8b366556f9d47b",
          "sha256": "b34986f9fad422a71c8953c0cb0c7b08b811aafbb6755faa89d6066d05882f98"
        },
        "downloads": -1,
        "filename": "transformer_pytorch-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "aec87c49c9bcafbd5e8b366556f9d47b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 20471,
        "upload_time": "2020-12-06T10:15:48",
        "upload_time_iso_8601": "2020-12-06T10:15:48.182315Z",
        "url": "https://files.pythonhosted.org/packages/3a/ee/48b50a868973b123b869e2db3092a0bc36405eb9e8cf297154a0f729f303/transformer_pytorch-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5da8f887edcb3fbbdaa9482ba5497d7500545960d932600270baff26e670b4f4",
          "md5": "52dbfedf15bea20cb83ec7cf33872d57",
          "sha256": "0e1defa5623fe184a9265ca71d5611650087d3f54eb513cb62ec44d948ae7b14"
        },
        "downloads": -1,
        "filename": "transformer-pytorch-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "52dbfedf15bea20cb83ec7cf33872d57",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 14738,
        "upload_time": "2020-12-06T10:15:49",
        "upload_time_iso_8601": "2020-12-06T10:15:49.454548Z",
        "url": "https://files.pythonhosted.org/packages/5d/a8/f887edcb3fbbdaa9482ba5497d7500545960d932600270baff26e670b4f4/transformer-pytorch-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3aee48b50a868973b123b869e2db3092a0bc36405eb9e8cf297154a0f729f303",
        "md5": "aec87c49c9bcafbd5e8b366556f9d47b",
        "sha256": "b34986f9fad422a71c8953c0cb0c7b08b811aafbb6755faa89d6066d05882f98"
      },
      "downloads": -1,
      "filename": "transformer_pytorch-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "aec87c49c9bcafbd5e8b366556f9d47b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 20471,
      "upload_time": "2020-12-06T10:15:48",
      "upload_time_iso_8601": "2020-12-06T10:15:48.182315Z",
      "url": "https://files.pythonhosted.org/packages/3a/ee/48b50a868973b123b869e2db3092a0bc36405eb9e8cf297154a0f729f303/transformer_pytorch-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5da8f887edcb3fbbdaa9482ba5497d7500545960d932600270baff26e670b4f4",
        "md5": "52dbfedf15bea20cb83ec7cf33872d57",
        "sha256": "0e1defa5623fe184a9265ca71d5611650087d3f54eb513cb62ec44d948ae7b14"
      },
      "downloads": -1,
      "filename": "transformer-pytorch-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "52dbfedf15bea20cb83ec7cf33872d57",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 14738,
      "upload_time": "2020-12-06T10:15:49",
      "upload_time_iso_8601": "2020-12-06T10:15:49.454548Z",
      "url": "https://files.pythonhosted.org/packages/5d/a8/f887edcb3fbbdaa9482ba5497d7500545960d932600270baff26e670b4f4/transformer-pytorch-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}