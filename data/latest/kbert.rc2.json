{
  "info": {
    "author": "SunYan",
    "author_email": "sunyanhust@163.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "# BANDÔºöBERT Application aNd Deployment\n\nA simple and efficient BERT model training and deployment framework.\n\n<!-- PROJECT SHIELDS -->\n\n[![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![MIT License][license-shield]][license-url]\n\n<!-- PROJECT LOGO -->\n<br />\n\n<p align=\"center\">\n  <a href=\"https://github.com/SunYanCN/BAND\">\n    <img src=\"figures/logo.png\" alt=\"Logo\" width=\"100\" height=\"100\">\n  </a>\n  <p align=\"center\">\n    BANDÔºöBERT Application aNd Deployment\n    <br />\n    <a href=\"https://sunyancn.github.io/BAND/\"><strong> Documents ¬ª</strong></a>\n    <br />\n    <br />\n    <a href=\"https://github.com/SunYanCN/BAND/tree/master/examples\">Examples</a>\n    ¬∑\n    <a href=\"https://github.com/SunYanCN/BAND/issues/new?assignees=&labels=&template=bug_report.md&title=\">Report Bug</a>\n    ¬∑\n    <a href=\"https://github.com/SunYanCN/BAND/issues/new?assignees=&labels=&template=feature_request.md&title=\">Feature Request</a>\n        ¬∑\n    <a href=\"https://github.com/SunYanCN/BAND/issues/new?assignees=&labels=&template=custom.md&title=\">Questions</a>\n  </p>\n\n</p>\n\n<h2 align=\"center\">What is it</h3>  \n\n**Encoding/Embedding** is a upstream task of encoding any inputs in the form of text, image, audio, video, transactional data to fixed length vector. Embeddings are quite popular in the field of NLP, there has been various Embeddings models being proposed in recent years by researchers, some of the famous one are bert, xlnet, word2vec etc. The goal of this repo is to build one stop solution for all embeddings techniques available, here we are starting with popular text embeddings for now and later on we aim  to add as much technique for image, audio, video inputs also.  \n**Finally**, **`embedding-as-service`** help you to encode any given text to fixed length vector from supported embeddings and models.  \n\n<h2 align=\"center\">üíæ Installation</h2>  \n\nInstall the band via `pip`.   \n```bash  \n$ pip install band -U\n```  \n> Note that the code MUST be running on **Python >= 3.6**. Again module does not support Python 2!  \n\n<h2 align=\"center\">‚ö° Ô∏èGetting Started</h2> \n\n### Text Classification Example\n\n```python\nimport time\nimport tensorflow as tf\nfrom transformers import BertConfig, BertTokenizer\nfrom band.model import TFBertForSequenceClassification\nfrom band.dataset import ChnSentiCorp\nfrom band.progress import classification_convert_examples_to_features\n\nUSE_XLA = False\nUSE_AMP = False\n\nEPOCHS = 1\nBATCH_SIZE = 16\nEVAL_BATCH_SIZE = 16\nTEST_BATCH_SIZE = 1\nMAX_SEQ_LEN = 128\nLEARNING_RATE = 3e-5\nSAVE_MODEL = False\npretrained_dir = \"/home/band/models\"\n\ntf.config.optimizer.set_jit(USE_XLA)\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})\n\ndataset = ChnSentiCorp(save_path=\"/tmp/band\")\ndata, label = dataset.data, dataset.label\ndataset.dataset_information()\n\ntrain_number, eval_number, test_number = dataset.train_examples_num, dataset.eval_examples_num, dataset.test_examples_num\n\ntokenizer = BertTokenizer.from_pretrained(pretrained_dir)\ntrain_dataset = classification_convert_examples_to_features(data['train'], tokenizer, max_length=MAX_SEQ_LEN,\n                                                            label_list=label,\n                                                            output_mode=\"classification\")\nvalid_dataset = classification_convert_examples_to_features(data['validation'], tokenizer, max_length=MAX_SEQ_LEN,\n                                                            label_list=label,\n                                                            output_mode=\"classification\")\ntest_dataset = classification_convert_examples_to_features(data['test'], tokenizer, max_length=MAX_SEQ_LEN,\n                                                           label_list=label,\n                                                           output_mode=\"classification\")\n\ntrain_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE, drop_remainder=True).repeat(EPOCHS)\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\nvalid_dataset = valid_dataset.batch(EVAL_BATCH_SIZE)\nvalid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\ntest_dataset = test_dataset.batch(TEST_BATCH_SIZE)\ntest_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\nconfig = BertConfig.from_pretrained(pretrained_dir, num_labels=dataset.num_labels)\nmodel = TFBertForSequenceClassification.from_pretrained(pretrained_dir, config=config, from_pt=True)\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, epsilon=1e-08)\nif USE_AMP:\n    optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\nhistory = model.fit(train_dataset, epochs=EPOCHS,\n                    steps_per_epoch=train_number // BATCH_SIZE,\n                    validation_data=valid_dataset,\n                    validation_steps=eval_number // EVAL_BATCH_SIZE)\n\nloss, accuracy = model.evaluate(test_dataset, steps=test_number // TEST_BATCH_SIZE)\nprint(loss, accuracy)\n\nif SAVE_MODEL:\n    saved_model_path = \"./saved_models/{}\".format(int(time.time()))\n    model.save(saved_model_path, save_format=\"tf\")\n```\n### Named Entity Recognition\n\n```python\nimport time\nimport tensorflow as tf\nfrom transformers import BertTokenizer, BertConfig\nfrom band.dataset import MSRA_NER\nfrom band.seqeval.callbacks import F1Metrics\nfrom band.model import TFBertForTokenClassification\nfrom band.utils import TrainConfig\nfrom band.progress import NER_Dataset\n\npretrained_dir = '/home/band/models'\n\ntrain_config = TrainConfig(epochs=3, train_batch_size=32, eval_batch_size=32, test_batch_size=1, max_length=128,\n                           learning_rate=3e-5, save_model=False)\n\ndataset = MSRA_NER(save_path=\"/tmp/band\")\n\nconfig = BertConfig.from_pretrained(pretrained_dir, num_labels=dataset.num_labels, return_unused_kwargs=True)\ntokenizer = BertTokenizer.from_pretrained(pretrained_dir)\nmodel = TFBertForTokenClassification.from_pretrained(pretrained_dir, config=config, from_pt=True)\n\nner = NER_Dataset(dataset=dataset, tokenizer=tokenizer, train_config=train_config)\nmodel.compile(optimizer=ner.optimizer, loss=ner.loss, metrics=[ner.metric])\n\nf1 = F1Metrics(dataset.get_labels(), validation_data=ner.valid_dataset, steps=ner.valid_steps)\n\nhistory = model.fit(ner.train_dataset, epochs=train_config.epochs, steps_per_epoch=ner.test_steps, callbacks=[f1])\n\nloss, accuracy = model.evaluate(ner.test_dataset, steps=ner.test_steps)\n\nif train_config.save_model:\n    saved_model_path = \"./saved_models/{}\".format(int(time.time()))\n    model.save(saved_model_path, save_format=\"tf\")\n```\n\n### Question Answering\n```python\nimport time\nimport tensorflow as tf\nfrom transformers import BertConfig, BertTokenizer\nfrom band.model import TFBertForQuestionAnswering\nfrom band.dataset import Squad\nfrom band.progress import squad_convert_examples_to_features, parallel_squad_convert_examples_to_features\n\nUSE_XLA = False\nUSE_AMP = False\n\nEPOCHS = 1\nBATCH_SIZE = 4\nEVAL_BATCH_SIZE = 4\nTEST_BATCH_SIZE = 1\nMAX_SEQ_LEN = 128\nLEARNING_RATE = 3e-5\nSAVE_MODEL = False\npretrained_dir = \"/home/band/models\"\n\ntf.config.optimizer.set_jit(USE_XLA)\ntf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})\n\ndataset = Squad(save_path=\"/tmp/band\")\ndata, label = dataset.data, dataset.label\n\ntrain_number, eval_number = dataset.train_examples_num, dataset.eval_examples_num\n\ntokenizer = BertTokenizer.from_pretrained(pretrained_dir)\ntrain_dataset = parallel_squad_convert_examples_to_features(data['train'], tokenizer, max_length=MAX_SEQ_LEN,\n                                                            doc_stride=128, is_training=True, max_query_length=64)\nvalid_dataset = parallel_squad_convert_examples_to_features(data['validation'], tokenizer, max_length=MAX_SEQ_LEN,\n                                                            doc_stride=128, is_training=False, max_query_length=64)\n\ntrain_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE, drop_remainder=True).repeat(EPOCHS)\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\nvalid_dataset = valid_dataset.batch(EVAL_BATCH_SIZE)\nvalid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\nconfig = BertConfig.from_pretrained(pretrained_dir)\nmodel = TFBertForQuestionAnswering.from_pretrained(pretrained_dir, config=config, from_pt=True, max_length=MAX_SEQ_LEN)\n\nprint(model.summary())\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, epsilon=1e-08)\nif USE_AMP:\n    optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\n\nloss = {'start_position': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        'end_position': tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)}\nmetrics = {'start_position': tf.keras.metrics.SparseCategoricalAccuracy('accuracy'),\n           'end_position': tf.keras.metrics.SparseCategoricalAccuracy('accuracy')}\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n\nhistory = model.fit(train_dataset, epochs=EPOCHS,\n                    steps_per_epoch=train_number // BATCH_SIZE,\n                    validation_data=valid_dataset,\n                    validation_steps=eval_number // EVAL_BATCH_SIZE)\n\nif SAVE_MODEL:\n    saved_model_path = \"./saved_models/{}\".format(int(time.time()))\n    model.save(saved_model_path, save_format=\"tf\")\n\n```\n\n## Dataset \nFor more information about dataset, see\n\n| Dataset Name | Language |             TASK              |        Description         |\n| :----------: | :------: | :---------------------------: | :------------------------: |\n| ChnSentiCorp |    CN    |      Text Classification      |   Binary Classification    |\n|    LCQMC     |    CN    |     Question Answer Match     |   Binary Classification    |\n|   MSRA_NER   |    CN    |   Named Entity Recognition    |     Sequence Labeling      |\n|    Toxic     |    EN    |      Text Classification      |  Multi-label Multi-label   |\n|   Thucnews   |    CN    |      Text Classification      | Multi-class Classification |\n|    SQUAD     |    EN    | Machine Reading Comprehension |            Span            |\n|     DRCD     |    CN    | Machine Reading Comprehension |            Span            |\n|     CMRC     |    CN    | Machine Reading Comprehension |            Span            |\n|     GLUE     |    EN    |                               |                            |\n\n<h2 align=\"center\" href=\"#supported-models\">‚úÖ Supported Embeddings and Models</h2> \n\nFor more information about pretrained models, see\n<!-- links -->\n[your-project-path]: SunYanCN/BAND\n[contributors-shield]: https://img.shields.io/github/contributors/SunYanCN/BAND.svg?style=flat-square\n[contributors-url]: https://github.com/SunYanCN/BAND/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/SunYanCN/BAND.svg?style=flat-square\n[forks-url]: https://github.com/SunYanCN/BAND/network/members\n[stars-shield]: https://img.shields.io/github/stars/SunYanCN/BAND.svg?style=flat-square\n[stars-url]: https://github.com/SunYanCN/BAND/stargazers\n[issues-shield]: https://img.shields.io/github/issues/SunYanCN/BAND.svg?style=flat-square\n[issues-url]: https://github.com/SunYanCN/BAND/issues\n[license-shield]: https://img.shields.io/github/license/SunYanCN/BAND.svg?style=flat-square\n[license-url]: https://github.com/SunYanCN/BAND/blob/master/LICENSE\n\n## Stargazers over time\n\n[![Stargazers over time](https://starchart.cc/SunYanCN/BAND.svg)](https://starchart.cc/SunYanCN/BAND)\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/sunyanhust/kbert",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "kbert",
    "package_url": "https://pypi.org/project/kbert/",
    "platform": "",
    "project_url": "https://pypi.org/project/kbert/",
    "project_urls": {
      "Homepage": "https://github.com/sunyanhust/kbert"
    },
    "release_url": "https://pypi.org/project/kbert/0.1.2/",
    "requires_dist": [
      "sentencepiece",
      "tensorflow (==2.1.0)",
      "numpy (==1.16.4)",
      "tqdm"
    ],
    "requires_python": ">3.6",
    "summary": "TensorFlow2.0 with Keras For BERT",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6917294,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "669a72e681a186b54a9888fc467d3a7ac212892792a48d1a39a01db4a98ad6df",
          "md5": "0f19eb8f4d23105df144a35580b28dc4",
          "sha256": "249354b3b6a71b888eeaee3a4ef3f185b034ccd1425774402a49f0bc6e032f07"
        },
        "downloads": -1,
        "filename": "kbert-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0f19eb8f4d23105df144a35580b28dc4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">3.6",
        "size": 171070,
        "upload_time": "2020-03-31T02:05:15",
        "upload_time_iso_8601": "2020-03-31T02:05:15.316695Z",
        "url": "https://files.pythonhosted.org/packages/66/9a/72e681a186b54a9888fc467d3a7ac212892792a48d1a39a01db4a98ad6df/kbert-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a18e8ee85122a70bf8649b768aa01f41ec6bf33f84ad3af4d6286a20796333f4",
          "md5": "f6af21e13e0762ba1388c9e3981a338e",
          "sha256": "890db7250561df9d206364b0d68b999a6ee205cbfcc65053d4be1e4a1172ea11"
        },
        "downloads": -1,
        "filename": "kbert-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f6af21e13e0762ba1388c9e3981a338e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">3.6",
        "size": 110835,
        "upload_time": "2020-03-31T02:05:18",
        "upload_time_iso_8601": "2020-03-31T02:05:18.991661Z",
        "url": "https://files.pythonhosted.org/packages/a1/8e/8ee85122a70bf8649b768aa01f41ec6bf33f84ad3af4d6286a20796333f4/kbert-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "97a0108010c2eb1528f34de6e9ea3d2383d0a9b3907764ad951427d173b26da7",
          "md5": "959253343e314f766cac2e509a37245d",
          "sha256": "51332b477cd244eef9cc4d5bcb9f79e56e3b8a209e206f92f3ac4f729a5c506c"
        },
        "downloads": -1,
        "filename": "kbert-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "959253343e314f766cac2e509a37245d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">3.6",
        "size": 171065,
        "upload_time": "2020-03-31T02:08:21",
        "upload_time_iso_8601": "2020-03-31T02:08:21.505151Z",
        "url": "https://files.pythonhosted.org/packages/97/a0/108010c2eb1528f34de6e9ea3d2383d0a9b3907764ad951427d173b26da7/kbert-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c959dcba51f40ba21184dba4e25dae5528feb35086276032203c68a26be2afe9",
          "md5": "983618db5f92b52a1805908472331be5",
          "sha256": "ff44054404b0b0acae156d6774374e4faf1adfff0aeb983333f723ede6c6a141"
        },
        "downloads": -1,
        "filename": "kbert-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "983618db5f92b52a1805908472331be5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">3.6",
        "size": 110835,
        "upload_time": "2020-03-31T02:08:24",
        "upload_time_iso_8601": "2020-03-31T02:08:24.186896Z",
        "url": "https://files.pythonhosted.org/packages/c9/59/dcba51f40ba21184dba4e25dae5528feb35086276032203c68a26be2afe9/kbert-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "97a0108010c2eb1528f34de6e9ea3d2383d0a9b3907764ad951427d173b26da7",
        "md5": "959253343e314f766cac2e509a37245d",
        "sha256": "51332b477cd244eef9cc4d5bcb9f79e56e3b8a209e206f92f3ac4f729a5c506c"
      },
      "downloads": -1,
      "filename": "kbert-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "959253343e314f766cac2e509a37245d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">3.6",
      "size": 171065,
      "upload_time": "2020-03-31T02:08:21",
      "upload_time_iso_8601": "2020-03-31T02:08:21.505151Z",
      "url": "https://files.pythonhosted.org/packages/97/a0/108010c2eb1528f34de6e9ea3d2383d0a9b3907764ad951427d173b26da7/kbert-0.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c959dcba51f40ba21184dba4e25dae5528feb35086276032203c68a26be2afe9",
        "md5": "983618db5f92b52a1805908472331be5",
        "sha256": "ff44054404b0b0acae156d6774374e4faf1adfff0aeb983333f723ede6c6a141"
      },
      "downloads": -1,
      "filename": "kbert-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "983618db5f92b52a1805908472331be5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">3.6",
      "size": 110835,
      "upload_time": "2020-03-31T02:08:24",
      "upload_time_iso_8601": "2020-03-31T02:08:24.186896Z",
      "url": "https://files.pythonhosted.org/packages/c9/59/dcba51f40ba21184dba4e25dae5528feb35086276032203c68a26be2afe9/kbert-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}