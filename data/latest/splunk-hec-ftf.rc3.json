{
  "info": {
    "author": "John Landers",
    "author_email": "support@fromthefuture.net",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Note on python versions\nThis library has stopped testing backwards compatibility with Python 2. All future updates and development \nwill be on Python 3 only.\n\n# Installation and Usage\nTo install the library:\n```\npip install splunk-hec-ftf\n```\n\nTo use:\n```\nfrom splunk_hec import splunk_hec\n```\n\n# Changelog\n- 1.0.2\n* Added target_host to logging when errors occur\n* Added processing for HTTP 400 responses due to invalid events to remove those events before attempting a resend. (non-ack flows only)\n\n# Library Overview\nThis HEC library was created to allow the following:\n\n* Provide a consistent and stable method to send logs to a remote HTTP Event Collector\n\nThere are other libraries out there but I noted a few pain points among them so when I wrote this library,\nI attempted to make sure the following would be true:\n\n* Provide support for Indexer Acknowledgement (ACK)\n* Provide more control to the calling/importing application\n* Perform minimal data formatting\n* Support real-world applications\n\nAnd, to that end, I believe I have succeeded here. This library has successfully supported the following \napplications:\n\n* Serverless functions (GCP, Azure, and AWS)\n* Syslog-ng program()\n* Manual data import programs\n\nAll told, this library is a critical part of my infrastructure handling multiple TBs of daily thruput without issue.\n\n# Library Requirements\nThese packages are used in some capacity and requests is the only external package currently leveraged.\n\n+ requests\n+ json\n+ uuid\n+ time\n+ queue\n+ threading\n\n# Library init Arguments\n| Argument | Description | Required | Defaults |\n| ------ | ------ | ------ | ------ |\n| token | The HEC token for communication with your HEC layer | True | None |\n| hec_server | FQDN for the HEC server or load balancer | True | None |\n| hec_port | Port to use with HEC | False | 8088 |\n| input_type | Defines which endpoint to hit. 'raw' or 'json' | False  | 'raw' |\n| use_hec_tls | Turns on use of TLS to send data. | False | True |\n| use_ack | Turns on indexer acknowledgement. | False | False |\n| hec_tls_verify | Turns on TLS verification. | False | False |\n| ack_interval | The number of seconds to wait between ACK attempts. | False | 5 |\n| max_ack_attempts | The number of attempts to make to SEND data and also to ACK data. | False | 5 |\n| max_content_length | The maximum estimated content length in bytes. Newer versions of Splunk can accept up to 800MB. | False | 50000000 |\n| max_events_per_batch | The maximum number of events to send in a single request. | False | 100000 |\n| backup_queue | The backup queue to store failures on. | False | None |\n| context_id | Used for logging, particularly in AWS Lambda invocations, to differentiate where logs came from. | False | 'default' |\n| rotate_session_after | The maximum number of POST requests to make to the HEC before starting a new session. This ensures we don't stick to a single indexer behind an LB | False | 10 |\n| debug_enabled | Allows the calling application to turn on DEBUG logging. | False | False |\n| logger | Allows the calling application to pass in a logger object. Log messages are simply printed if not provided. | False | None |\n| max_threads | The maximum number of threads to use. 0 for no threading. | False | 0 |\n| disable_tls_validation_warnings | Turn off TLS Validation warnings. | False | True |\n\n*max_ack_attempts* :: This setting controls the number of attempts to make to send the data in addition to how many times you \nshould ACK the data. In other words if ACK is enabled and this is 5. The library will make 5 attempts to send the data and \n_for each send_, 5 attempts will be made to confirm delivery.\n\n*backup_queue* :: This is not required but highly recommended. If you do not provide it, failures will not be tracked or given back to\nthe calling application in any way.\n\n\n\n# Threading\nMost of the libraries out there support threading for delivery to HEC but they suffer from a simple problem: only\nthe data delivery is actually threaded. What I mean is that data comes into the script/library and is parsed, added to\na batch, and only when that batch is full is it put on a queue for subthreads to process.\n\nThis threading is helpful if you are:\n\n1) Using small batch sizes requiring numerous connections\n2) Using ACK\n\nIn either of these situations, threading will help. In my serverless applications, this threading would not be of use\nas almost all data is sent in a single request to the backend. (i.e. I get 10,000 events in a serverless invocation, all 10,000\nare processed and sent in a single batch to the backend.)\n\nPoint is - threading is something to consider carefully and here are the guidelines I would provide relative to this library:\n\n1) If you need to speed up data processing, write threading into your data processor and call this library in the non-threading mode.\n2) If you need to speed up data delivery/ACK, use the threading mode available in this library.\n\n## Serverless Considerations\nI found that under certain conditions, leveraging threading in serverless compute was worthwhile. Specifically, when cross-region transfer of data is involved. When the lambda sits in one region (say, ap-northeast-1) and the receiver endpoint sits in another region (say, us-east-1) - you can gain execution speed by threading because of the additional data upload times involved.\n\nAnyway, use your best judgment.\n\n\n# Indexer ACK\nRemember earlier when I said ACK was supported? It is supported but it's supported in a way that adds quite a lot of overhead. This is the\ndata flow:\n\n\tData -> Receiving App -> HEC Library -> POST to HEC -> ACK Test -> ACK Test -> ACK Test -> Confirmed\n\nIf you consider how the Splunk Universal Forwarder works, data gets sent and then shoved into a memory queue while additional data is sent AND\nACK checks are happening. _THIS IS NOT HOW IT WORKS HERE._ Everything happens in sequential order here - I send the data and then immediately \nattempt to ACK that single block of data. No new data will be sent during this time.\n\n* In a multi-threaded approach, data will continue to queue for sending to the maximum limits of your available memory. \n* In the non-threaded approach, processing will block until ACK is confirmed or fails. The number of attempts is configurable.\n\n# Library Delivery Logic\nTo add some clarity around the process, it works approximately like this:\n\n1) Data is added to a batch list.\n2) When the list meets max_events_per_batch or max_content_length, it is sent to the HEC\n3) The *send* process has *two* retry mechanisms\n    - One mechanism is the urllib3 Retry mechanism which will attempt to call the POST request up to 3 times for connection failures, read failures, redirections, and 500/503 errors.\n    - The second mechanism considers this 1 attempt and will try again if anything other than an HTTP 200 is received up to *max_ack_attempts* times.\n4) The HTTP return is checked for code 200.\n    - If it's a 200, the data is considered sent\n    - Anything else triggers a retry\n5) If ACK is enabled:\n    - ACK is attempted for the data block up to *max_ack_attempts* using the same session object that sent it (ensuring you hit the same backend through a Load Balancer).\n    - The urllib3 Retry mechanism described previously is utilized here as well.\n    - Additional sleep timing is added based on the defined *ack_interval*\n6) If ACK is successful, we are done.\n7) If ACK was unsuccessful, we go back to step 3 and try this all again.\n\nIn practice, what does this mean? Well, the use of all of these retry mechanisms adds to the overall duration... a lot. Assume ACK is enabled with an ACK interval of 5 with 5 *max_ack_attempts*.\n\n* Before any ACK is attempted, we sleep for 2.5 seconds.\n* After this, we will attempt an ACK and sleep for 5 seconds between each request adding up to 25 seconds to the duration.\n* The urllib3 Retry logic using a backoff of 0.3 and max of 3 connection attempts which can add up to 2.1 seconds per request. \n\nOdds are that if data was sent successfully and the backend indexers are not overburdened, ACK will also be successful within 1-2 attempts but for serverless environments where duration\ntranslates directly to cost, it's worth considering the above information before enabling ACK.\n\n\n# Using the Backup Queue\nThe library accepts a \"backup queue\" to be based in. It looks like this in code:\n\n```\nfrom splunk_hec import splunk_hec\nimport queue\n\nsplhec = splunk_hec(token=hec_token, hec_server=hec_server, hec_port=hec_port, input_type='json', use_hec_tls=True,\n    use_ack=hec_ack, hec_tls_verify=False, max_ack_attempts=hec_ack_attempts, max_content_length=hec_max_content_length,\n    max_events_per_batch=hec_batch_size )\n\n# Note, this can be set directly or passed in during initilization... Depends on your use case.\nsplhec.backup_queue = queue.Queue(0)\n\n# Simplified event processing\nfor event in events:\n    splhec.send_event(event)\n\n# Check the backup queue\nif splhec.backup_queue.empty():\n    print('Queue is empty, everything worked!)\nelse:\n    raise RuntimeError('Failures detected. Implement backup routine here.')\n\n```\n\nBasically, this gives the calling application control over how to handle detected delivery failures. Here are some ways to use this:\n\n1) In serverless environments, I often raise an exception to force the data message to be processed again.\n2) In syslog-ng environments, the failed data is written to disk for monitoring by Splunk UF.\n\n# Sample Usage\n```\nfrom splunk_hec import splunk_hec\nimport queue\n\n# Create an object reference to the library, initalized with our settings.\nsplhec = splunk_hec( token=hec_token, hec_server=hec_server, hec_port=hec_port, input_type=input_type, use_hec_tls=use_hec_tls, use_ack=use_ack, hec_tls_verify=hec_tls_verify, ack_interval=ack_interval,\n    max_ack_attempts=max_ack_attempts, max_content_length=max_content_length, max_events_per_batch=max_events_per_batch, context_id=context_id, rotate_session_after=rotate_session_after, debug_enabled=debug_enabled,\n    max_threads=max_threads, disable_tls_validation_warnings=disable_tls_validation_warnings )\n\n# Set up a backup queue\nsplhec.backup_queue = queue.Queue(0)\n\n# Set basic parameters that we'll need later\nindex = 'main'\nsourcetype = 'hec:%s' % str(context_id)\nsource = 'hec:test:events'\n\nif 'raw' in input_type:\n    # When using raw input, we do this to essentially add the parameters to the POST URL so splunk knows where to send the data\n    splhec.set_request_params({'index':index, 'sourcetype':sourcetype, 'source':source})\n\n# This comes from a test function so we're looping through to generate data.\nfor i in range(num_test_events):\n\n    # If you are using the 'json' input type, you need to format this for the /event endpoint. Follow\n    # splunk docs on this. This will preformat the data and allows for index-time field extraction.\n    if 'json' in input_type:\n        payload = {}\n        payload['event'] = '{\"message\": \"JSON test Message %s.\", \"foo\": \"bar\"}' % str(i)\n        payload['time'] = int(datetime.utcnow().strftime('%s'))\n        payload['source'] = source\n        payload['index'] = index\n        payload['sourcetype'] = sourcetype\n        payload = json.dumps(payload)    \n\n    # If it's a raw payload, not much is necessary here. Make sure you have index-time props in place to parse the data.\n    else:\n        payload = '%s RAW Test Message %s.' % (str(datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S -0000')),str(i))\n\n    # This should add the event to our batch; when the max batch sizes are met, the library will flush out to the HEC automatically\n    splhec.send_event(payload)\n\n# Finally, we call this to signal that we are done. A final flush is performed. If you are using threads, they will be shut down.\n# YOU MUST END WITH THIS.\nsplhec.stop_threads_and_processing()\n\n#### Note: You may also call splhec.force_flush_events() directly to flush to HEC at any time in your calling application.\n\n```\n\n# Samples\nThere is a directory of samples based on older iterations of some production code I am running. All of these samples are provided as-is with\nno guarantee that they will be updated/maintained. And while I have made every effort to ensure backwards compatibility through all the\nlibrary updates over time, I make no guarantee that a future update would break this sample code. If you come to rely on this in a production\nmanner, reach out to me and let's share notes.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.com/johnfromthefuture/splunk-hec-library",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "splunk-hec-ftf",
    "package_url": "https://pypi.org/project/splunk-hec-ftf/",
    "platform": "",
    "project_url": "https://pypi.org/project/splunk-hec-ftf/",
    "project_urls": {
      "Homepage": "https://gitlab.com/johnfromthefuture/splunk-hec-library"
    },
    "release_url": "https://pypi.org/project/splunk-hec-ftf/1.0.2/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A library for sending data to the Splunk HTTP Event Collector.",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11913787,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f6bfda619bc25a48004028225767f42aa693d1e376c0ec147e86af89f74d0af8",
          "md5": "273189ce3c70951786d7c257d13647c5",
          "sha256": "83ae5ec6dc3a1844983d017c91e143e661f3d74611ca3c8c1c53aceb9e1f7708"
        },
        "downloads": -1,
        "filename": "splunk_hec_ftf-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "273189ce3c70951786d7c257d13647c5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 29468,
        "upload_time": "2020-12-29T13:51:14",
        "upload_time_iso_8601": "2020-12-29T13:51:14.077669Z",
        "url": "https://files.pythonhosted.org/packages/f6/bf/da619bc25a48004028225767f42aa693d1e376c0ec147e86af89f74d0af8/splunk_hec_ftf-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "31fe5bcacbf1f4c839ee6ef4f80e219ae41345f5b2b620aa1d4f96f2d5f3e922",
          "md5": "4c00628f438de7a233581cdee0b2983f",
          "sha256": "7395a21638286afdf0f5e224dde43db01b9f346c6cb619f90145ad5c0f3b7cbe"
        },
        "downloads": -1,
        "filename": "splunk-hec-ftf-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "4c00628f438de7a233581cdee0b2983f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 21930,
        "upload_time": "2020-12-29T13:51:15",
        "upload_time_iso_8601": "2020-12-29T13:51:15.124608Z",
        "url": "https://files.pythonhosted.org/packages/31/fe/5bcacbf1f4c839ee6ef4f80e219ae41345f5b2b620aa1d4f96f2d5f3e922/splunk-hec-ftf-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "413b159dc8ad6b84d72a1ea97d5957320a22ec0f7664853c8cc1599b3464eede",
          "md5": "49c8e0ee798076d8f98a07e8dc8d64f6",
          "sha256": "cfb8f50d43e5ac58bda160676e8e6c995e27f08e0e3b8468b01a680abc1f025e"
        },
        "downloads": -1,
        "filename": "splunk_hec_ftf-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "49c8e0ee798076d8f98a07e8dc8d64f6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 29535,
        "upload_time": "2020-12-29T14:10:06",
        "upload_time_iso_8601": "2020-12-29T14:10:06.586183Z",
        "url": "https://files.pythonhosted.org/packages/41/3b/159dc8ad6b84d72a1ea97d5957320a22ec0f7664853c8cc1599b3464eede/splunk_hec_ftf-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "998ec407580c6d41183e5810d3fe6ad6a35491ee5e782746eb47118040ff013b",
          "md5": "9c1508e8db7581271a226cac9f4fe6af",
          "sha256": "20c8c021e915b942ab5a3aa7b79c949a3f562149a0013c18b3c13ee2ff3baf18"
        },
        "downloads": -1,
        "filename": "splunk-hec-ftf-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9c1508e8db7581271a226cac9f4fe6af",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 22097,
        "upload_time": "2020-12-29T14:10:08",
        "upload_time_iso_8601": "2020-12-29T14:10:08.094378Z",
        "url": "https://files.pythonhosted.org/packages/99/8e/c407580c6d41183e5810d3fe6ad6a35491ee5e782746eb47118040ff013b/splunk-hec-ftf-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "793acc60d405ec97df92bc077cfa9effe66f6dcdbd1d36dc213ccaa3c7bd1667",
          "md5": "75cebb91b20101a7b0200d8cca047d3d",
          "sha256": "623941afce0972d257979f59d3f2b09087de4c06ba60aff3207e3eb3aa286566"
        },
        "downloads": -1,
        "filename": "splunk-hec-ftf-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "75cebb91b20101a7b0200d8cca047d3d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 22819,
        "upload_time": "2021-11-03T11:42:56",
        "upload_time_iso_8601": "2021-11-03T11:42:56.363392Z",
        "url": "https://files.pythonhosted.org/packages/79/3a/cc60d405ec97df92bc077cfa9effe66f6dcdbd1d36dc213ccaa3c7bd1667/splunk-hec-ftf-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "793acc60d405ec97df92bc077cfa9effe66f6dcdbd1d36dc213ccaa3c7bd1667",
        "md5": "75cebb91b20101a7b0200d8cca047d3d",
        "sha256": "623941afce0972d257979f59d3f2b09087de4c06ba60aff3207e3eb3aa286566"
      },
      "downloads": -1,
      "filename": "splunk-hec-ftf-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "75cebb91b20101a7b0200d8cca047d3d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 22819,
      "upload_time": "2021-11-03T11:42:56",
      "upload_time_iso_8601": "2021-11-03T11:42:56.363392Z",
      "url": "https://files.pythonhosted.org/packages/79/3a/cc60d405ec97df92bc077cfa9effe66f6dcdbd1d36dc213ccaa3c7bd1667/splunk-hec-ftf-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}