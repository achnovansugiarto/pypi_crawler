{
  "info": {
    "author": "Timur Abdualimov",
    "author_email": "timur.atp@yandex.ru",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "<div align=\"center\">\n  <img src=\"https://i.ibb.co/4sf3fYM/Datret.png\"><br>\n</div>\n\n-----------------\n\n# DatRet: Tensorflow implementation for structured tabular data\n\n**Ret**e neurale per la previsione di **Dat**i tabulari. (it.)\n\n## What is it?\n\nA simple implementation of a deep neural network architecture for tabular data with automatic layer-by-layer reduction in the number of neurons and functionality similar to classical machine learning methods.\n\n## Main Features\n\n- simplicity and ease of use. Fit and Predict et Voila!\n\n- quick adjustment of model parameters\n\n- GPU support\n\n- high prediction accuracy\n\n- support for multilabel classification\n\n- Tensorflow under the hood;)\n\n## Where to get it?\n\nThe source code is currently hosted on GitHub at:\n[GitHub - AbdualimovTP/datret: Tensorflow implementation for structured tabular data](https://github.com/AbdualimovTP/datret)\nBinary installers for the latest released version are available at the [Python\nPackage Index (PyPI)](https://pypi.org/project/datret)\n\n```shell\n# PyPI\npip install datret\n```\n\n## Dependencies\n\n- [Tensorflow - An open-source library primarily for deep learning applications](https://www.tensorflow.org/)\n- [NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays](https://www.numpy.org)\n- [Pandas - Python data analysis toolkit]([pandas documentation &#8212; pandas 1.5.2 documentation](http://pandas.pydata.org/pandas-docs/stable/))\n- [Scikit-Learn - machine learning in Python](https://scikit-learn.org/stable/)\n\n## Quick start\n\nTraining and prediction of the model is implemented as in scikit-learn. Prepare your test and train set and run the fit. Support for automatic data normalization for neural networks.\n\n*NB! Don't forget to install the dependencies before using the model. You will need Tensorflow, Numpy, Pandas and Scikit-Learn installed.*\n\n*NB! No need to do one-hot encoding of predictive features. The model will do automatically.* \n\n```python\n# load library\nfrom datret.datret import DatRetClassifier, DatRetRegressor, DatRetMultilabelClassifier\n\n# prepare train, test split. As in sklearn.\n# for example\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i)\n\n# Call the regressor or classifier and train the model.\nDR = DatRetClassifier() # DatRetRegressor works on the same principle\nDR.fit(X_train, y_train)\n# predict the actual label (or class) over a new set of data.\nDR_predict = DR.predict(X_test)\n# predict the class probabilities for each data point.\nDR_predict_proba = DR.predict_proba(X_test) # Missing in DatRetRegressor, DatRetMultilabelClassifier\n```\n\n## Custom model options\n\n*Parameters*:\n\n- **epoch:** int, default = 30. Number of epochs to train the model.\n\n- **optimizer:** string (name of optimizer) or optimizer instance. See [tf.keras.optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/legacy/), default  = `Adam(learning_rate=0.001)`. On DatRetRegressor defaut learning rate = 0.01. Built-in tensorflow optimizer classes.\n\n- **loss:** Loss function. May be a string (name of loss function). See [tf.keras.losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses), default for DatRetClassifier = `CategoricalCrossentropy()`, for DatRetRegressor = `MeanSquaredError()`. Built-in loss functions.\n\n- **verbose:** 'auto', 0, 1, or 2, default=0. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. 'auto' defaults to 1 for most cases, but 2 when used with ParameterServerStrategy=0.\n\n- **number_neurons:** int, default = 500. The number of layers in the first fully connected layer. Subsequent layers are generated automatically with half as many neurons.\n\n- **validation_split:** Float between 0 and 1, default = 0. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n\n- **batch_size:**  int, default =1. Number of samples per gradient update. Steps_per_epoch s calculated automatically, `X_train.shape[0] // batch_size`\n\n- **shuffle:** True or False, default = True. This argument is ignored when `x` is a generator or an object of tf.data.Dataset. 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks.\n\n- **callback:** `[]`, default = `[EarlyStopping(monitor='loss', mode='auto', patience=7, verbose=1), ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.00001, verbose=1)]`. [Callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/): utilities called at certain points during model training.\n\n**Adjustable `fit` method parameters**\n\n*Parameters*:\n\n- **normalize:** True or False ,default True. Automatic normalization of input data. Used [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html). \n\n**Example:**\n\n```python\n# load library\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam, Nadam\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.losses import CategoricalCrossentropy, MeanSquaredError, BinaryCrossentropy\nfrom datret.datret import DatRetClassifier, DatRetRegressor, DatRetMultilabelClassifier\n\n# prepare train, test split. As in sklearn.\n# for example\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i)\n\n# Call the regressor or classifier and train the model.\nDR = DatRetClassifier(epoch=50,\n                      optimizer=Nadam(learning_rate=0.001),\n                      loss=BinaryCrossentropy(),\n                      verbose=1,\n                      number_neurons=1000,\n                      validation_split = 0.1,\n                      batch_size=100,\n                      shuffle=True,\n                      callback=[])\nDR.fit(X_train, y_train, normalize=True)\n# predict the actual label (or class) over a new set of data.\nDR_predict = DR.predict(X_test)\n# predict the class probabilities for each data point.\nDR_predict_proba = DR.predict_proba(X_test)\n```\n\n## Model architecture\n\nAs an example, when using `number_neurons = 500` input neurons and 2 predictable classes, the model will automatically have this architecture.\n\n```sql\nModel: \"DatRet with number_neurons = 500\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, X_train.shape[0)]      0         \n\n dense (Dense)               (None, 500)               150500    \n\n dense_1 (Dense)             (None, 250)               125250    \n\n dense_2 (Dense)             (None, 125)               31375     \n\n dense_3 (Dense)             (None, 62)                7812      \n\n dense_4 (Dense)             (None, 31)                1953      \n\n dense_5 (Dense)             (None, 15)                480       \n\n dense_6 (Dense)             (None, 7)                 112       \n\n dense_7 (Dense)             (None, 3)                 24        \n\n dense_8 (Dense)             (None, 2)                 8         \n                       (2 predictable classes)                               \n=================================================================\nTotal params: 317,514\nTrainable params: 317,514\nNon-trainable params: 0\n_________________________________________________________________\n```\n\n---\n\n## Comparison of accuracy with classical machine learning methods\n\n- **DatRetClassifier**\n\nTo assess the accuracy of the classifier, we will use [Pima Indians Diabetes Database | Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database). Comparable metric *RocAucScore*. We will compare DatRet with RandomForest and CatBoost out \"of the box\".\n\n![](https://i.ibb.co/XbhdSpQ/Dat-Ret-Accuracy.png)\n\n|                  | 10%      | 20%      | 30%      | 40%      | 50%      | 60%      |\n| ---------------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| **RandomForest** | **0.79** | 0.81     | 0.81     | 0.79     | 0.82     | **0.82** |\n| **CatBoost**     | 0.78     | 0.82     | **0.82** | 0.8      | 0.81     | **0.82** |\n| **DatRet**       | **0.79** | **0.84** | **0.82** | **0.81** | **0.84** | 0.81     |\n\n- **DatRetRegressor**\n\nTo assess the accuracy of the regressor, we will use [Medical Cost Personal Datasets | Kaggle](https://www.kaggle.com/datasets/mirichoi0218/insurance). Comparable metric *Root Mean Square Error*. We will compare DatRet with RandomForest and CatBoost out \"of the box\".\n\n\n\n![](https://i.ibb.co/7gdsbpf/Dat-Ret-Regressor.png)\n\n|                  | 10%      | 20%      | 30%      | 40%      | 50%      | 60%      |\n| ---------------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| **RandomForest** | 5736     | 5295     | 4777     | 4956     | **4904** | **4793** |\n| **CatBoost**     | **5732** | 5251     | 4664     | 4986     | 5044     | 4989     |\n| **DatRet**       | 5860     | **5173** | **4610** | **4927** | 5047     | 5780     |\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/AbdualimovTP/datret",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "datret",
    "package_url": "https://pypi.org/project/datret/",
    "platform": null,
    "project_url": "https://pypi.org/project/datret/",
    "project_urls": {
      "Homepage": "https://github.com/AbdualimovTP/datret"
    },
    "release_url": "https://pypi.org/project/datret/1.0.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Tensorflow implementation for structured tabular data",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16513907,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "68fa3d33f4dffde6161299716a447b3f3d344b124787d2e29de0f2d1b2d11dbd",
          "md5": "0be236e5a9398d7a405bd98e1c7e76da",
          "sha256": "791eb8250a1206ac975a44222ee45e8aa8d3f6d0d776a28b4c85c442b07e3750"
        },
        "downloads": -1,
        "filename": "datret-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0be236e5a9398d7a405bd98e1c7e76da",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 7168,
        "upload_time": "2023-01-21T17:39:50",
        "upload_time_iso_8601": "2023-01-21T17:39:50.589786Z",
        "url": "https://files.pythonhosted.org/packages/68/fa/3d33f4dffde6161299716a447b3f3d344b124787d2e29de0f2d1b2d11dbd/datret-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "32c067a3f3b3037802dcf195e5ca297cdd0eae3b943430d3b87bab2decdc3cba",
          "md5": "c20ec7aa897c4d034c030c83ef884191",
          "sha256": "9e45e9694cf87f1f8265ac4baa5c114d5408c8b84359e3256d647ff2a2452f46"
        },
        "downloads": -1,
        "filename": "datret-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c20ec7aa897c4d034c030c83ef884191",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6879,
        "upload_time": "2023-01-21T17:39:52",
        "upload_time_iso_8601": "2023-01-21T17:39:52.682005Z",
        "url": "https://files.pythonhosted.org/packages/32/c0/67a3f3b3037802dcf195e5ca297cdd0eae3b943430d3b87bab2decdc3cba/datret-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "68fa3d33f4dffde6161299716a447b3f3d344b124787d2e29de0f2d1b2d11dbd",
        "md5": "0be236e5a9398d7a405bd98e1c7e76da",
        "sha256": "791eb8250a1206ac975a44222ee45e8aa8d3f6d0d776a28b4c85c442b07e3750"
      },
      "downloads": -1,
      "filename": "datret-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0be236e5a9398d7a405bd98e1c7e76da",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 7168,
      "upload_time": "2023-01-21T17:39:50",
      "upload_time_iso_8601": "2023-01-21T17:39:50.589786Z",
      "url": "https://files.pythonhosted.org/packages/68/fa/3d33f4dffde6161299716a447b3f3d344b124787d2e29de0f2d1b2d11dbd/datret-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "32c067a3f3b3037802dcf195e5ca297cdd0eae3b943430d3b87bab2decdc3cba",
        "md5": "c20ec7aa897c4d034c030c83ef884191",
        "sha256": "9e45e9694cf87f1f8265ac4baa5c114d5408c8b84359e3256d647ff2a2452f46"
      },
      "downloads": -1,
      "filename": "datret-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "c20ec7aa897c4d034c030c83ef884191",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 6879,
      "upload_time": "2023-01-21T17:39:52",
      "upload_time_iso_8601": "2023-01-21T17:39:52.682005Z",
      "url": "https://files.pythonhosted.org/packages/32/c0/67a3f3b3037802dcf195e5ca297cdd0eae3b943430d3b87bab2decdc3cba/datret-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}