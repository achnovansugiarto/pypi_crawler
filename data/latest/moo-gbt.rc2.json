{
  "info": {
    "author": "Shubha, Bhavi",
    "author_email": "bhavi.chawla@swiggy.in",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Multi-objective Optimized GBT(MooGBT)\n\nMooGBT is a library for Multi-objective optimization in Gradient Booseted Trees. MooGBT optimizes for multiple objectives by defining constraints on sub-objective(s) along with a primary objective. The constraints are defined as upper bounds on sub-objective loss function. MooGBT uses a Augmented Lagrangian(AL) based constrained optimization framework with Gradient Boosted Trees, to optimize for multiple objectives.\n<br> <br>\n With AL, we introduce dual variables in Boosting. The dual variables are iteratively optimized and fit within the Boosting iterations. The Boosting objective function is updated with the AL terms and the gradient is readily derived using the GBT gradients. With the gradient and updates of dual variables, we solve the optimization problem by jointly iterating AL and Boosting steps.\n<br> <br>\nThis library is motivated by work done in the paper <a href=\"https://sigir-ecom.github.io/ecom2019/ecom19Papers/paper30.pdf\">Multi-objective Relevance Ranking</a>, which introduces an Augmented Lagrangian based method to incorporate multiple objectives (MO) in LambdaMART, which is a GBT based search ranking algorithm.\n\nWe have modified the scikit-learn GBT implementation [[3]](#3) to support multi-objective optimization.\n\n<!-- [[1]](#1) -->\n\nHighlights - \n<li> follows the  <a href=\"https://scikit-learn.org/\">scikit-learn</a>  API conventions </li>\n<!-- <li> supports natively both dense and sparse data representations </li> -->\n<li> supports all hyperparameters present in scikit-learn GBT</li>\n<li> supports optimization for more than 1 sub-objectives</li>\n<br>\n\nCurrent support -\n<li> MooGBTClassifier - \"binomial deviance\" loss function, for primary and sub-objectives represented as binary variables </li>\n<li> MooGBTRegressor - \"least squares\" loss function, for primary and sub-objectives represented as continuous variables </li>\n\n<br>\n\n## Installation\nMoo-GBT can be installed from **[PyPI](https://pypi.org/project/moo-gbt/)**\n\n ```python\n pip3 install moo-gbt\n ```\n<br>\n\n## Usage\n\n```python\nfrom multiobjective_gbt import MooGBTClassifier\n\nmu = 100\nb = 0.7 # upper bound on sub-objective cost\n\nconstrained_gbt = MooGBTClassifier(\n\t\t\t\tloss='deviance',\n\t\t\t\tn_estimators=100,\n\t\t\t\tconstraints=[{\"mu\":mu, \"b\":b}], # One Constraint\n\t\t\t\trandom_state=2021\n)\nconstrained_gbt.fit(X_train, y_train)\n```\n\nHere `y_train`  contains 2 columns, the first column should be the primary objective. The following columns are all the sub-objectives for which constraints have been specified(in the same order).\n\n<br>\n\n## Usage Steps\n<ol>\n\t<li> Run unconstrained GBT on Primary Objective. Unconstrained GBT is just the GBTClassifer/GBTRegressor by scikit-learn </li>\n\t<li> Calculate the loss function value for Primary Objective and sub-objective(s)</li>\n\t\t<ul>\n\t\t\t<li>For MooGBTClassifier calculate Log Loss between predicted probability and sub-objective label(s) </li>\n\t\t\t<li>For MooGBTRegressor calculate mean squared error between predicted value and sub-objective label(s) </li>\n\t\t</ul>\n\t<li> Set the value of hyperparamter b, less than the calculated cost in the previous step and run MooGBTClassifer/MooGBTRegressor with this b. The lower the value of b, the more the sub-objective will be optimized </li>\n</ol>\n\n<br>\n\n## Example with multiple binary objectives\n\n```python\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom multiobjective_gbt import MooGBTClassifier\n```\n\nWe'll use a publicly available dataset - **[available here](https://www.kaggle.com/c/expedia-hotel-recommendations/data?select=train.csv)**\n\nWe define a multi-objective problem on the dataset, with the primary objective as the column \"is_booking\" and sub-objective as the column \"is_package\".\nBoth these variables are binary.\n\n```python\n# Preprocessing Data\ntrain_data = pd.read_csv('examples/expedia-data/expedia-hotel-recommendations/train_data_sample.csv')\n\npo = 'is_booking' # primary objective\nso = 'is_package' # sub-objective\n\nfeatures =  list(train_data.columns)\nfeatures.remove(po)\noutcome_flag =  po\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(\n\t\t\t\t\ttrain_data[features],\n\t\t\t\t\ttrain_data[outcome_flag],\n\t\t\t\t\ttest_size=0.2,\n\t\t\t\t\tstratify=train_data[[po, so]],\n\t\t\t\t\trandom_state=2021\n)\n\n# Creating y_train_, y_test_ with 2 labels\ny_train_ = pd.DataFrame()\ny_train_[po] = y_train\ny_train_[so] = X_train[so]\n\ny_test_ = pd.DataFrame()\ny_test_[po] = y_test\ny_test_[so] = X_test[so]\n```\n\nMooGBTClassifier without the constraint parameter, works as the standard scikit-learn GBT classifier.\n\n```python\nunconstrained_gbt = MooGBTClassifier(\n\t\t\t\tloss='deviance',\n\t\t\t\tn_estimators=100,\n\t\t\t\trandom_state=2021\n)\n\nunconstrained_gbt.fit(X_train, y_train)\n```\n\nGet train and test sub-objective costs for unconstrained model.\n\n```python\ndef get_binomial_deviance_cost(pred, y):\n\treturn -np.mean(y * np.log(pred) + (1-y) * np.log(1-pred))\n\npred_train = unconstrained_gbt.predict_proba(X_train)[:,1]\npred_test = unconstrained_gbt.predict_proba(X_test)[:,1]\n\n# get sub-objective costs\nso_train_cost = get_binomial_deviance_cost(pred_train, X_train[so])\nso_test_cost = get_binomial_deviance_cost(pred_test, X_test[so])\n\nprint (f\"\"\"\nSub-objective cost train - {so_train_cost},\nSub-objective cost test  - {so_test_cost}\n\"\"\")\n```\n\n    Sub-objective cost train - 0.9114,\n    Sub-objective cost test  - 0.9145\n\nConstraint is specified as an upper bound on the sub-objective cost.\nIn the unconstrained model, we see the cost of our sub-objective to be ~0.9. So setting upper bounds below 0.9 would optimise the sub-objective.\n\n```python\nb = 0.65 # upper bound on cost\nmu = 100\nconstrained_gbt = MooGBTClassifier(\n\t\t\t\tloss='deviance',\n\t\t\t\tn_estimators=100,\n\t\t\t\tconstraints=[{\"mu\":mu, \"b\":b}], # One Constraint\n\t\t\t\trandom_state=2021\n)\n\nconstrained_gbt.fit(X_train, y_train_)\n```\n\nFrom the constrained model, we achieve more than 100% gain in AuROC for the sub-objective while the loss in primary objective AuROC is kept within 6%.\nThe entire study on this dataset can be found in the [example notebook](https://github.com/Swiggy/Moo-GBT/blob/master/examples/Constrained_classifer_example.ipynb).\n\n## Looking at MooGBT primary and sub-objective losses - \n\nTo get raw values of loss functions wrt boosting iteration,\n\n``` python\n# return a Pandas dataframe with loss values of objectives wrt boosting iteration\nlosses = constrained_gbt.loss_.get_losses()\nlosses.head()\n```\n\n<img src=\"https://raw.githubusercontent.com/Swiggy/Moo-GBT/master/assets/loss_values_.png\" width=\"300\" height=\"150\">\n\nSimilarly, you can also look at dual variable(alpha) values for sub-objective(s),\n\nTo get raw values of alphas wrt boosting iteration,\n``` python\nconstrained_gbt.loss_.get_alphas()\n```\n\nThese losses can be used to look at the MooGBT Learning process.\n\n``` python\nsns.lineplot(data=losses, x='n_estimators', y='primary_objective', label='primary objective')\nsns.lineplot(data=losses, x='n_estimators', y='sub_objective_1', label='subobjective')\n\nplt.xlabel(\"# estimators(trees)\")\nplt.ylabel(\"Cost\")\nplt.legend(loc = \"upper right\")\n```\n\n<img src=\"https://raw.githubusercontent.com/Swiggy/Moo-GBT/master/assets/plot_losses_1.png\" width=\"520\" height=\"220\">\n\n```python\nsns.lineplot(data=losses, x='n_estimators', y='primary_objective', label='primary objective')\n```\n<img src=\"https://raw.githubusercontent.com/Swiggy/Moo-GBT/master/assets/plot_losses_2.png\" width=\"520\" height=\"220\">\n\n## Choosing the right upper bound constraint `b` and `mu` value\n\nThe upper bound should be defined based on a acceptable % loss in the primary objective evaluation metric. For stricter upper bounds, this loss would be greater as MooGBT will optimize for the sub-objective more.<br>\n\nBelow table summarizes the effect of the upper bound value on the model performance for primary and sub-objective(s) for the above example. <br>\n\n%gain specifies the percentage increase in AUROC for the constrained MooGBT model from an uncostrained GBT model.\n\n\n|   b  | Primary Objective - %gain | Sub-Objective - %gain|\n|:----:|:-----------------:|:-------------:|\n|  0.9 |      -0.7058      |     4.805     |\n|  0.8 |       -1.735      |     40.08     |\n|  0.7 |      -2.7852      |    62.7144    |\n| 0.65 |      -5.8242      |    113.9427   |\n|  0.6 |      -9.9137      |    159.8931   |\n\n\nIn general, across our experiments we have found that lower values of `mu` optimize on the primary objective better while satisfying the sub-objective constraints given enough boosting iterations(n_estimators).\n\nThe below table summarizes the results of varying `mu` values keeping the upper bound same(b=0.6).\n\n|  b  |  mu  | Primary Objective - %gain | Sub-objective - %gain |\n|:---:|:----:|:-----------------------:|:-----------------------:|\n| 0.6 | 1000 |         -20.6569        |         238.1388        |\n| 0.6 |  100 |         -13.3769        |         197.8186        |\n| 0.6 |  10  |         -9.9137         |         159.8931        |\n| 0.6 |   5  |          -8.643         |         146.4171        |\n\n\n<!-- <img src=\"assets/mu_example.png\" style=\"width:520px;height:440px;\"> -->\n\n<!-- Above plot shows the Primary Objective Cost wrt Boosting iteration, low mu value is optimizing for primary objective better. -->\n\n## MooGBT Learning Process\n\nMooGBT optimizes for multiple objectives by defining constraints on sub-objective(s) along with a primary objective. The constraints are defined as upper bounds on sub-objective loss function.\n\nMooGBT differs from a standard GBT in the loss function it optimizes the primary objective C<sub>1</sub> and the sub-objectives using the Augmented Lagrangian(AL) constrained optimization approach.\n\n<img src=\"https://raw.githubusercontent.com/Swiggy/Moo-GBT/master/assets/loss_function.png\" width=\"450\" height=\"50\">\n\nwhere α = [α1, α2, α3…..] is a vector of dual variables. The Lagrangian is solved by minimizing with respect to the primal variables \"s\" and maximizing with respect to the dual variables α. Augmented Lagrangian iteratively solves the constraint optimization. \nSince AL is an iterative approach we integerate it with the boosting iterations of GBT for updating the dual variable α.\n\nAlpha(α) update -\n\n<img src=\"https://raw.githubusercontent.com/Swiggy/Moo-GBT/master/assets/alpha_update.png\" width=\"300\" height=\"50\">\n\nAt an iteration k, if the constraint t is not satisfied, i.e., C<sub>t</sub>(s) > b<sub>t</sub>, we have \nα<sup>t</sup><sub>k</sub> > α<sup>t</sup><sub>k-1</sub>. Otherwise, if the constraint is met, the dual variable α is made 0.\n\n\n\n## Public contents\n\n*   [__gb_.py](https://github.com/Swiggy/Moo-GBT/blob/master/src/multiobjective_gbt/_gb.py):\n    contains the `MooGBTClassifier` and `MooGBTRegressor` classes. Contains implementation of the fit and predict function. Extended implementation from [__gb_.py](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_gb.py) from scikit-learn. \n\n*   [__gb_losses_.py](https://github.com/Swiggy/Moo-GBT/blob/master/src/multiobjective_gbt/_gb_losses.py):\n    contains `BinomialDeviance` loss function class, `LeastSquares` loss function class. Extended implementation from [_gb_losses.py](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/ensemble/_gb_losses.py) from scikit-learn. \n\n\n## More examples\n\nThe\n[examples](https://github.com/Swiggy/Moo-GBT/tree/master/examples)\ndirectory contains several illustrations of how one can use this library:\n\n*   [Jupyter](https://jupyter.org/) notebooks:\n\n    1.  [Constrained_classifer_example.ipynb](https://github.com/Swiggy/Moo-GBT/blob/master/examples/Constrained_classifer_example.ipynb):\n        This notebook runs MooGBTClassifier for a primary objective and a single sub-objective. Both objectives are represented as binary variable.\n\n    <!-- 1.  [Constrained_regressor_example.ipynb](https://github.com/): -->\n\n\n<!-- ## References\n<a id=\"1\">[1]</a> Michinari Momma, Alireza Bagheri Garakani, Yi Sun \nMulti-objective Relevance Ranking \n\nCommunications of the ACM, 11(3), 147-148.\n\nMulti-objective Ranking via Constrained Optimization -->\n        \n## References - \n<a id=\"1\">[1]</a> Multi-objective Ranking via Constrained Optimization - https://arxiv.org/pdf/2002.05753.pdf <br>\n<a id=\"2\">[2]</a> Multi-objective Relevance Ranking - https://sigir-ecom.github.io/ecom2019/ecom19Papers/paper30.pdf <br>\n<a id=\"3\">[3]</a> Scikit-learn GBT Implementation - <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\">GBTClassifier </a> and <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\">GBTRegressor </a>\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Swiggy/Moo-GBT",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "moo-gbt",
    "package_url": "https://pypi.org/project/moo-gbt/",
    "platform": "",
    "project_url": "https://pypi.org/project/moo-gbt/",
    "project_urls": {
      "Bug Tracker": "https://github.com/Swiggy/Moo-GBT/issues",
      "Homepage": "https://github.com/Swiggy/Moo-GBT"
    },
    "release_url": "https://pypi.org/project/moo-gbt/0.0.5/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Library for Multi-objective optimization in Gradient Boosted Trees",
    "version": "0.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10824192,
  "releases": {
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c9ae4393cc3509a88cdf22c32611402d1011f57fc390a543cd6e4f83b69fc375",
          "md5": "991e2c977474e7aa780a3c9f40dca699",
          "sha256": "3334101d4bff04c2658d1490a111cd92b945473066af2eba7bfe32fbcfa6bb0a"
        },
        "downloads": -1,
        "filename": "moo_gbt-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "991e2c977474e7aa780a3c9f40dca699",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27859,
        "upload_time": "2021-07-05T08:11:26",
        "upload_time_iso_8601": "2021-07-05T08:11:26.500794Z",
        "url": "https://files.pythonhosted.org/packages/c9/ae/4393cc3509a88cdf22c32611402d1011f57fc390a543cd6e4f83b69fc375/moo_gbt-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "77007c99230a8e61efd6011ba4698c6465f48bfbf7d465fc3dbd8f326e7f678f",
          "md5": "d7b8a52a292af928369c35e0b845cf40",
          "sha256": "d7e894013044e735fa4886eb4992b8703cedffe6dba9c6c72657d5a0fef4d760"
        },
        "downloads": -1,
        "filename": "moo-gbt-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "d7b8a52a292af928369c35e0b845cf40",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 30040,
        "upload_time": "2021-07-05T08:11:28",
        "upload_time_iso_8601": "2021-07-05T08:11:28.575611Z",
        "url": "https://files.pythonhosted.org/packages/77/00/7c99230a8e61efd6011ba4698c6465f48bfbf7d465fc3dbd8f326e7f678f/moo-gbt-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "85ebfd1ed2aa06787790abaf16511bfbc437368bba5261804c5e4af69cb0e48e",
          "md5": "a63daefb3c74bda86a4c8bc0dc914787",
          "sha256": "423fece16c9f70deeb7088ae67ea21b7dbd8c011824685976a6e0777c7e52b61"
        },
        "downloads": -1,
        "filename": "moo_gbt-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a63daefb3c74bda86a4c8bc0dc914787",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32058,
        "upload_time": "2021-07-05T08:27:42",
        "upload_time_iso_8601": "2021-07-05T08:27:42.833851Z",
        "url": "https://files.pythonhosted.org/packages/85/eb/fd1ed2aa06787790abaf16511bfbc437368bba5261804c5e4af69cb0e48e/moo_gbt-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ecb1d33e1852790c5703c4187ba617538bdf240f8a76c5190188e901d699105c",
          "md5": "1e14e082d32ee94b31422d2368db483c",
          "sha256": "f93d92fff333eb5c9ef8ff7fb1e2b21fb84ad885f9719cc542749baa41d8d426"
        },
        "downloads": -1,
        "filename": "moo-gbt-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "1e14e082d32ee94b31422d2368db483c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 30218,
        "upload_time": "2021-07-05T08:27:45",
        "upload_time_iso_8601": "2021-07-05T08:27:45.077266Z",
        "url": "https://files.pythonhosted.org/packages/ec/b1/d33e1852790c5703c4187ba617538bdf240f8a76c5190188e901d699105c/moo-gbt-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "85ebfd1ed2aa06787790abaf16511bfbc437368bba5261804c5e4af69cb0e48e",
        "md5": "a63daefb3c74bda86a4c8bc0dc914787",
        "sha256": "423fece16c9f70deeb7088ae67ea21b7dbd8c011824685976a6e0777c7e52b61"
      },
      "downloads": -1,
      "filename": "moo_gbt-0.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a63daefb3c74bda86a4c8bc0dc914787",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 32058,
      "upload_time": "2021-07-05T08:27:42",
      "upload_time_iso_8601": "2021-07-05T08:27:42.833851Z",
      "url": "https://files.pythonhosted.org/packages/85/eb/fd1ed2aa06787790abaf16511bfbc437368bba5261804c5e4af69cb0e48e/moo_gbt-0.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ecb1d33e1852790c5703c4187ba617538bdf240f8a76c5190188e901d699105c",
        "md5": "1e14e082d32ee94b31422d2368db483c",
        "sha256": "f93d92fff333eb5c9ef8ff7fb1e2b21fb84ad885f9719cc542749baa41d8d426"
      },
      "downloads": -1,
      "filename": "moo-gbt-0.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "1e14e082d32ee94b31422d2368db483c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 30218,
      "upload_time": "2021-07-05T08:27:45",
      "upload_time_iso_8601": "2021-07-05T08:27:45.077266Z",
      "url": "https://files.pythonhosted.org/packages/ec/b1/d33e1852790c5703c4187ba617538bdf240f8a76c5190188e901d699105c/moo-gbt-0.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}