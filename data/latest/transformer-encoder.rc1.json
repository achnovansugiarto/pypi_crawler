{
  "info": {
    "author": "Cheng Guo",
    "author_email": "guocheng672@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Transformer Encoder\n<p>\n    <img src=\"https://img.shields.io/travis/com/guocheng2018/transformer-encoder\" />\n</p>\nThis repo provides an easy-to-use interface of transformer encoder. You can use it as a general sequence feature extractor and incorporate it in \nyour model.<br><br>\n<p>\n    <img src=\"https://i.ibb.co/YhR6wWf/encoder.png\" alt=\"encoder\" border=\"0\" />\n</p>\n\n## Examples\n\nQuickstart\n```python\nimport torch\nfrom transformer_encoder import TransformerEncoder\nfrom transformer_encoder.utils import PositionalEncoding\n\n# Model\nencoder = TransformerEncoder(d_model=512, d_ff=2048, n_heads=8, n_layers=6, dropout=0.1)\n\n# Input embeds\ninput_embeds = torch.nn.Embedding(num_embeddings=6, embedding_dim=512)\npe_embeds = PositionalEncoding(d_model=512, dropout=0.1, max_len=5)\nencoder_input = torch.nn.Sequential(input_embeds, pe_embeds)\n\n# Input data (zero-padding)\nbatch_seqs = torch.tensor([[1,2,3,4,5], [1,2,3,0,0]], dtype=torch.long)\nmask = batch_seqs.ne(0)\n\n# Run model\nout = encoder(encoder_input(batch_seqs), mask)\n```\n\nUsing the built-in warming up optimizer \n```python\nimport torch.optim as optim\nfrom transformer_encoder.utils import WarmupOptimizer\n\nmodel = ...\n\nbase_optimizer = optim.Adam(model.parameters(), lr=1e-3)\noptimizer = WarmupOptimizer(base_optimizer, d_model=512, scale_factor=1, warmup_steps=100)\n\noptimizer.zero_grad()\nloss = ...\nloss.backward()\noptimizer.step()\n```\n\n## Install from PyPI\nRequires `python 3.5+`, `pytorch 1.0.0+`\n```\npip install transformer_encoder\n```\n\n## API\n\n**transformer_encoder.TransformerEncoder(d_model, d_ff, n_heads=1, n_layers=1, dropout=0.1)**\n\n- `d_model`: dimension of each word vector\n- `d_ff`: hidden dimension of feed forward layer\n- `n_heads`: number of heads in self-attention (defaults to 1)\n- `n_layers`: number of stacked layers of encoder (defaults to 1)\n- `dropout`: dropout rate (defaults to 0.1)\n\n**transformer_encoder.TransformerEncoder.forward(x, mask)**\n\n- `x (~torch.FloatTensor)`: shape *(batch_size, max_seq_len, d_model)*\n- `mask (~torch.ByteTensor)`: shape *(batch_size, max_seq_len)*\n\n**transformer_encoder.utils.PositionalEncoding(d_model, dropout=0.1, max_len=5000)**\n\n- `d_model`: same as TransformerEncoder\n- `dropout`: dropout rate (defaults to 0.1)\n- `max_len`: max sequence length (defaults to 5000)\n\n**transformer_encoder.utils.PositionalEncoding.forward(x)**\n\n- `x (~torch.FloatTensor)`: shape *(batch_size, max_seq_len, d_model)*\n\n**transformer_encoder.utils.WarmupOptimizer(base_optimizer, d_model, scale_factor, warmup_steps)**\n\n- `base_optimizer (~torch.optim.Optimzier)`: e.g. adam optimzier\n- `d_model`: equals d_model in TransformerEncoder\n- `scale_factor`: scale factor of learning rate\n- `warmup_steps`: warming up steps \n\n\n## Contribution\nAny contributions are welcome!\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/guocheng2018/transformer-encoder",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "transformer-encoder",
    "package_url": "https://pypi.org/project/transformer-encoder/",
    "platform": "",
    "project_url": "https://pypi.org/project/transformer-encoder/",
    "project_urls": {
      "Homepage": "https://github.com/guocheng2018/transformer-encoder"
    },
    "release_url": "https://pypi.org/project/transformer-encoder/0.0.3/",
    "requires_dist": null,
    "requires_python": ">=3.5",
    "summary": "A pytorch implementation of transformer encoder",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7869179,
  "releases": {
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c739e957723b095616a72e974bf27d2365a5d7f17ee0254f6ec0b9e9c8bc1781",
          "md5": "0c6dec48d6d393944cdded7b20fbf95a",
          "sha256": "8da07f3c35aab23c271b1f7e13dd6596cee4b32277b0bd0d3136293b2891d08c"
        },
        "downloads": -1,
        "filename": "transformer_encoder-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0c6dec48d6d393944cdded7b20fbf95a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 8955,
        "upload_time": "2020-08-03T02:13:41",
        "upload_time_iso_8601": "2020-08-03T02:13:41.574932Z",
        "url": "https://files.pythonhosted.org/packages/c7/39/e957723b095616a72e974bf27d2365a5d7f17ee0254f6ec0b9e9c8bc1781/transformer_encoder-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "68deaa92453adf1b2a4a94dc0c0fa4df70bdaecd3f0cfc0cb85814de287ef9c8",
          "md5": "7b3d587d6e8d815983bf63e4d0500f71",
          "sha256": "df6151470817f153849043bee241b920163345364e3d621ab7d92087a071c55a"
        },
        "downloads": -1,
        "filename": "transformer_encoder-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "7b3d587d6e8d815983bf63e4d0500f71",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 6752,
        "upload_time": "2020-08-03T02:13:44",
        "upload_time_iso_8601": "2020-08-03T02:13:44.121874Z",
        "url": "https://files.pythonhosted.org/packages/68/de/aa92453adf1b2a4a94dc0c0fa4df70bdaecd3f0cfc0cb85814de287ef9c8/transformer_encoder-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c739e957723b095616a72e974bf27d2365a5d7f17ee0254f6ec0b9e9c8bc1781",
        "md5": "0c6dec48d6d393944cdded7b20fbf95a",
        "sha256": "8da07f3c35aab23c271b1f7e13dd6596cee4b32277b0bd0d3136293b2891d08c"
      },
      "downloads": -1,
      "filename": "transformer_encoder-0.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0c6dec48d6d393944cdded7b20fbf95a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5",
      "size": 8955,
      "upload_time": "2020-08-03T02:13:41",
      "upload_time_iso_8601": "2020-08-03T02:13:41.574932Z",
      "url": "https://files.pythonhosted.org/packages/c7/39/e957723b095616a72e974bf27d2365a5d7f17ee0254f6ec0b9e9c8bc1781/transformer_encoder-0.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "68deaa92453adf1b2a4a94dc0c0fa4df70bdaecd3f0cfc0cb85814de287ef9c8",
        "md5": "7b3d587d6e8d815983bf63e4d0500f71",
        "sha256": "df6151470817f153849043bee241b920163345364e3d621ab7d92087a071c55a"
      },
      "downloads": -1,
      "filename": "transformer_encoder-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "7b3d587d6e8d815983bf63e4d0500f71",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 6752,
      "upload_time": "2020-08-03T02:13:44",
      "upload_time_iso_8601": "2020-08-03T02:13:44.121874Z",
      "url": "https://files.pythonhosted.org/packages/68/de/aa92453adf1b2a4a94dc0c0fa4df70bdaecd3f0cfc0cb85814de287ef9c8/transformer_encoder-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}