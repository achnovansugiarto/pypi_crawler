{
  "info": {
    "author": "Takuma Seno",
    "author_email": "takuma.seno@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "Operating System :: MacOS :: MacOS X",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: Implementation :: CPython",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# Deep Reinforcement Learning\n>#### _Shyamal H Anadkat | Fall '21_\n\n# Background \nHello! This is a repository for AIPI530 DeepRL final project. **The goal is to build a pipeline for offline RL**. The\nstarter code has been forked from [d3rlpy](https://github.com/takuseno/d3rlpy) (_see citation at the bottom_)\nOffline reinforcement learning (RL) defines the task of learning from a fixed batch of data.\n\nBefore diving in, I would recommend getting familiarized with basic Reinforcement Learning. \nHere is a link to my blog post on Reinforcement Learning to get you started:\n[RL Primer](https://shyamalanadkat.medium.com/reinforcement-learning-a-primer-29116d487e42)\n\nThe blog post briefly covers the following:\n\n* What is reinforcement learning ? <br/>\n* What are the pros and cons of reinforcement learning ? <br/>\n* When should we consider applying reinforcement learning (and when should not) ? <br/>\n* What's the difference between supervised learning and reinforcement learning ? <br/>\n* What is offline reinforcement learning ? What are the pros and cons of offline reinforcement learning ? <br/>\n* When should we consider applying offline reinforcement learning (and when should not) ? <br/>\n* Have an example of offline reinforcement learning in the real-world <br/>\n\n>![img.png](assets/offlinerl.png)\n_source: https://bair.berkeley.edu/blog/2020/12/07/offline/_\n\n# Getting Started\n\n#### (_please read carefully_)\n\nThis project is customized to training CQL on a custom dataset in d3rlpy, and training OPE (FQE) to evaluate the trained\npolicy. Important scripts:\n\n1. `cql_train.py`: at the root of the project is the main script, used to train cql & get evaluation scores\n2. `plot_helper.py`: utility script to help produce the plots required\n\n### How do I install & run this project ?\n\n---\n\n**1. Clone this repository**\n```\ngit clone https://github.com/shyamal-anadkat/offlinerl\n```\n\n**2. Install **pybullet** from source:**\n```\npip install git+https://github.com/takuseno/d4rl-pybullet\n```\n\n**3. Install requirements:** \n```\npip install Cython numpy \npip install -e .\n```\n\n4. **Execute **`cql_train.py`** found at the root of the project**\n    * Default dataset is `hopper-bullet-mixed-v0`\n    * Default no. of `epochs` is `10`. You can change this via custom args `--epochs_cql` & `--epochs_fqe`\n    * For example if we want to run for 10 epochs: \n```   \npython cql_train.py --epochs_cql 10 --epochs_fqe 10\n```\n(see colab example below for more clarity)\n\n5. **Important Logs:**\n    * Estimated Q values vs training steps (CQL): `d3rlpy_logs/CQL_hopper-bullet-mixed-v0_1/init_value.csv`\n    * Average reward vs training steps (CQL): `d3rlpy_logs/CQL_hopper-bullet-mixed-v0_1/environment.csv`\n    * True Q values vs training steps (CQL): `d3rlpy_logs/CQL_hopper-bullet-mixed-v0_1/true_q_value.csv`\n    * True Q & Estimated Q values vs training steps (FQE): `d3rlpy_logs/FQE_hopper-bullet-mixed-v0_1/..`\n    * Note: **I created my own scorer to calculate the true q values**. See `scorer.py` (`true_q_value_scorer`) for\n      implementation details)\n\n6. For plotting, I wrote a utility script (at root of the project) which can be executed like so \n```\npython plot_helper.py\n```\n   _Note: you can provide arguments that correspond to the path to the logs or it will use the default._\n\n* If you're curious here's\n  the [benchmark/reproduction](https://github.com/takuseno/d3rlpy-benchmarks/tree/main/reproductions/CQL_hopper-medium-v0_3_20210617172248)\n\n#### Other scripts:\n\n* Format: `./scripts/format`\n* Linting: `./scripts/lint`\n\n### Sample Plots (with 100 epochs):\n\n>![img.png](assets/plot.png)\n\nNote: logs can be found in `/d3rlpy_logs`\n\n### Examples speak more: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1S5RDTwaqVjA4wAJISxApra_G0ewSuS0R?usp=sharing)\n\n**_Walkthrough:_**\n![walkthrough.gif](assets/offlinerl-project-walkthrough.gif)\n\n---\n\n# Background on d3rlpy\n\n> d3rlpy is an offline deep reinforcement learning library for practitioners and researchers.\n\n- Documentation: https://d3rlpy.readthedocs.io\n- Paper: https://arxiv.org/abs/2111.03788\n\n### How do I install d3rlpy?\n\nd3rlpy supports Linux, macOS and Windows. d3rlpy is not only easy, but also completely compatible with scikit-learn API,\nwhich means that you can maximize your productivity with the useful scikit-learn's utilities.\n\n### PyPI (recommended)\n\n[![PyPI version](https://badge.fury.io/py/d3rlpy.svg)](https://badge.fury.io/py/d3rlpy)\n![PyPI - Downloads](https://img.shields.io/pypi/dm/d3rlpy)\n\n```\n$ pip install d3rlpy\n```\n\n## _More examples around d3rlpy usage_\n\n```py\nimport d3rlpy\n\ndataset, env = d3rlpy.datasets.get_dataset(\"hopper-medium-v0\")\n\n# prepare algorithm\nsac = d3rlpy.algos.SAC()\n\n# train offline\nsac.fit(dataset, n_steps=1000000)\n\n# train online\nsac.fit_online(env, n_steps=1000000)\n\n# ready to control\nactions = sac.predict(x)\n```\n\n### MuJoCo\n\n```py\nimport d3rlpy\n\n# prepare dataset\ndataset, env = d3rlpy.datasets.get_d4rl('hopper-medium-v0')\n\n# prepare algorithm\ncql = d3rlpy.algos.CQL(use_gpu=True)\n\n# train\ncql.fit(dataset,\n        eval_episodes=dataset,\n        n_epochs=100,\n        scorers={\n            'environment': d3rlpy.metrics.evaluate_on_environment(env),\n            'td_error': d3rlpy.metrics.td_error_scorer\n        })\n```\n\nSee more datasets at [d4rl](https://github.com/rail-berkeley/d4rl).\n\n### Atari 2600\n\n```py\nimport d3rlpy\nfrom sklearn.model_selection import train_test_split\n\n# prepare dataset\ndataset, env = d3rlpy.datasets.get_atari('breakout-expert-v0')\n\n# split dataset\ntrain_episodes, test_episodes = train_test_split(dataset, test_size=0.1)\n\n# prepare algorithm\ncql = d3rlpy.algos.DiscreteCQL(n_frames=4, q_func_factory='qr', scaler='pixel', use_gpu=True)\n\n# start training\ncql.fit(train_episodes,\n        eval_episodes=test_episodes,\n        n_epochs=100,\n        scorers={\n            'environment': d3rlpy.metrics.evaluate_on_environment(env),\n            'td_error': d3rlpy.metrics.td_error_scorer\n        })\n```\n\nSee more Atari datasets at [d4rl-atari](https://github.com/takuseno/d4rl-atari).\n\n### PyBullet\n\n```py\nimport d3rlpy\n\n# prepare dataset\ndataset, env = d3rlpy.datasets.get_pybullet('hopper-bullet-mixed-v0')\n\n# prepare algorithm\ncql = d3rlpy.algos.CQL(use_gpu=True)\n\n# start training\ncql.fit(dataset,\n        eval_episodes=dataset,\n        n_epochs=100,\n        scorers={\n            'environment': d3rlpy.metrics.evaluate_on_environment(env),\n            'td_error': d3rlpy.metrics.td_error_scorer\n        })\n```\n\nSee more PyBullet datasets at [d4rl-pybullet](https://github.com/takuseno/d4rl-pybullet).\n\n### How about some tutorials?\n\nTry a cartpole example on Google Colaboratory:\n\n* official offline RL\n  tutorial: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/takuseno/d3rlpy/blob/master/tutorials/cartpole.ipynb)\n\n# _Citation_\n\n> Thanks to [Takuma Seno](https://github.com/takuseno) and his work on [d3rlpy](https://github.com/takuseno/d3rlpy.git)\nThis wouldn't have been possible without it.\n\n> Seno, T., & Imai, M. (2021). d3rlpy: An Offline Deep Reinforcement Learning Library [Conference paper](https://arxiv.org/abs/2111.03788). 35th Conference on Neural Information Processing Systems, Offline Reinforcement Learning Workshop, 2021\n\n```\n@InProceedings{seno2021d3rlpy,\n  author = {Takuma Seno, Michita Imai},\n  title = {d3rlpy: An Offline Deep Reinforcement Library},\n  booktitle = {NeurIPS 2021 Offline Reinforcement Learning Workshop},\n  month = {December},\n  year = {2021}\n}\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/takuseno/d3rlpy",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "zjkdemo2",
    "package_url": "https://pypi.org/project/zjkdemo2/",
    "platform": "",
    "project_url": "https://pypi.org/project/zjkdemo2/",
    "project_urls": {
      "Homepage": "https://github.com/takuseno/d3rlpy"
    },
    "release_url": "https://pypi.org/project/zjkdemo2/0.91/",
    "requires_dist": null,
    "requires_python": ">=3.6.0",
    "summary": "An offline deep reinforcement learning library",
    "version": "0.91",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12894368,
  "releases": {
    "0.91": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bcc9132dc0ee78f9b045c496b2ee97d4fe031b25359a68cd09b07e573604e8a2",
          "md5": "620d0db4edab13b14a862dd19ce4d36a",
          "sha256": "2137a5fd191f523ee9e91803503d5e12f050f04cada50d4af1781f6adde3913d"
        },
        "downloads": -1,
        "filename": "zjkdemo2-0.91.tar.gz",
        "has_sig": false,
        "md5_digest": "620d0db4edab13b14a862dd19ce4d36a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 339803,
        "upload_time": "2022-02-15T06:30:42",
        "upload_time_iso_8601": "2022-02-15T06:30:42.701199Z",
        "url": "https://files.pythonhosted.org/packages/bc/c9/132dc0ee78f9b045c496b2ee97d4fe031b25359a68cd09b07e573604e8a2/zjkdemo2-0.91.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bcc9132dc0ee78f9b045c496b2ee97d4fe031b25359a68cd09b07e573604e8a2",
        "md5": "620d0db4edab13b14a862dd19ce4d36a",
        "sha256": "2137a5fd191f523ee9e91803503d5e12f050f04cada50d4af1781f6adde3913d"
      },
      "downloads": -1,
      "filename": "zjkdemo2-0.91.tar.gz",
      "has_sig": false,
      "md5_digest": "620d0db4edab13b14a862dd19ce4d36a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 339803,
      "upload_time": "2022-02-15T06:30:42",
      "upload_time_iso_8601": "2022-02-15T06:30:42.701199Z",
      "url": "https://files.pythonhosted.org/packages/bc/c9/132dc0ee78f9b045c496b2ee97d4fe031b25359a68cd09b07e573604e8a2/zjkdemo2-0.91.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}