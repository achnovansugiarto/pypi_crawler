{
  "info": {
    "author": "Julien Flandre",
    "author_email": "julien.flandre@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "This tokenizer function turns text into lowercase word                       tokens, removes English stopwords, lemmatize the tokens                       and replaces URLs with a placeholder.\r\n\r\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Granju/tokenize-package",
    "keywords": "tokenizer function",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "jf-tokenize-package",
    "package_url": "https://pypi.org/project/jf-tokenize-package/",
    "platform": null,
    "project_url": "https://pypi.org/project/jf-tokenize-package/",
    "project_urls": {
      "Homepage": "https://github.com/Granju/tokenize-package"
    },
    "release_url": "https://pypi.org/project/jf-tokenize-package/1.0.3/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A simple tokenizer function for NLP",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14538466,
  "releases": {
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4e984ab6fa2a84db1747721a2ffdce5ae1407387abd251f1786ccc13a6babf47",
          "md5": "b60d6051021141799a4db9b81c4e1599",
          "sha256": "76875ca994e266ebbb14cf147781a070684908be3dc72e3180f7575c38564d8d"
        },
        "downloads": -1,
        "filename": "jf_tokenize_package-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b60d6051021141799a4db9b81c4e1599",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 1723,
        "upload_time": "2022-07-25T08:12:14",
        "upload_time_iso_8601": "2022-07-25T08:12:14.512314Z",
        "url": "https://files.pythonhosted.org/packages/4e/98/4ab6fa2a84db1747721a2ffdce5ae1407387abd251f1786ccc13a6babf47/jf_tokenize_package-1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4159efbcabf7726bcb38de47272ee0638efb3d7e367f821af7468250203d4e21",
          "md5": "5d31d747c5efe432a7516c7238b55abb",
          "sha256": "ab4ebeaed6f01c6c53b08254ede9d83dd17b2a7d71965f676dac1b6a60726bba"
        },
        "downloads": -1,
        "filename": "jf_tokenize_package-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5d31d747c5efe432a7516c7238b55abb",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 1719,
        "upload_time": "2022-07-25T08:16:53",
        "upload_time_iso_8601": "2022-07-25T08:16:53.110248Z",
        "url": "https://files.pythonhosted.org/packages/41/59/efbcabf7726bcb38de47272ee0638efb3d7e367f821af7468250203d4e21/jf_tokenize_package-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "12c21c00049517b9ab0ddf8f96e2b3726e35ad0d72d0524efb2e90fc7a2e2e0c",
          "md5": "9bd16f51bc7887f00064f46698a015d2",
          "sha256": "35e793637df1d1ed72d940dca19016896a67dbff06ce89f680457b5f6192ba1f"
        },
        "downloads": -1,
        "filename": "jf_tokenize_package-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "9bd16f51bc7887f00064f46698a015d2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 1655,
        "upload_time": "2022-07-25T08:26:46",
        "upload_time_iso_8601": "2022-07-25T08:26:46.508348Z",
        "url": "https://files.pythonhosted.org/packages/12/c2/1c00049517b9ab0ddf8f96e2b3726e35ad0d72d0524efb2e90fc7a2e2e0c/jf_tokenize_package-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4ef15b7ea57b0ab383a1208bf3c6a33825965990b1b32b48566a920e514f9b96",
          "md5": "017fd0628869a7bb29c762ccc4b1c430",
          "sha256": "69d635bb16298b59d5b76d03c273a202b3e3440827e89657fa9dfc1c40858ced"
        },
        "downloads": -1,
        "filename": "jf_tokenize_package-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "017fd0628869a7bb29c762ccc4b1c430",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 1595,
        "upload_time": "2022-07-25T08:53:47",
        "upload_time_iso_8601": "2022-07-25T08:53:47.480258Z",
        "url": "https://files.pythonhosted.org/packages/4e/f1/5b7ea57b0ab383a1208bf3c6a33825965990b1b32b48566a920e514f9b96/jf_tokenize_package-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4ef15b7ea57b0ab383a1208bf3c6a33825965990b1b32b48566a920e514f9b96",
        "md5": "017fd0628869a7bb29c762ccc4b1c430",
        "sha256": "69d635bb16298b59d5b76d03c273a202b3e3440827e89657fa9dfc1c40858ced"
      },
      "downloads": -1,
      "filename": "jf_tokenize_package-1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "017fd0628869a7bb29c762ccc4b1c430",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 1595,
      "upload_time": "2022-07-25T08:53:47",
      "upload_time_iso_8601": "2022-07-25T08:53:47.480258Z",
      "url": "https://files.pythonhosted.org/packages/4e/f1/5b7ea57b0ab383a1208bf3c6a33825965990b1b32b48566a920e514f9b96/jf_tokenize_package-1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}