{
  "info": {
    "author": "Kartik Chandra, Audrey Xie",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Gradient Descent: The Ultimate Optimizer\n\n![gdtuo_turtles](https://user-images.githubusercontent.com/31300675/193727211-bff82331-998c-4d44-b675-03d1fd222e0e.png)\n# Abstract\nWorking with any gradient-based machine learning algorithm involves the tedious task of tuning the optimizer's hyperparameters, such as the step size. Recent work has shown how the step size can itself be \"learned\" on-line by gradient descent, by manually deriving expressions for \"hypergradients\" ahead of time.\n\nWe show how to *automatically* compute hypergradients with a simple and elegant modification to backpropagation. This allows us to apply the method to other hyperparameters besides the step size, such as the momentum coefficient. We can even recursively apply the method to its own *hyper*-hyperparameters, and so on *ad infinitum*. As these towers of optimizers grow taller, they become less sensitive to the initial choice of hyperparameters. We present experiments validating this for MLPs, CNNs, and RNNs.\n\n*This repository contains an implementation of the algorithm in our paper.*\n\n# Citation\n```\n@article{chandra2022gradient,\n    title = {Gradient Descent: The Ultimate Optimizer},\n    author = {Chandra, Kartik and Xie, Audrey and Ragan-Kelley, Jonathan and Meijer, Erik},\n    journal = {NeurIPS},\n    year = {2022},\n    url = {https://arxiv.org/abs/1909.13371}\n}\n```\n\n# Install\n```\n# install pytorch for your specific machine\n...\n\n# install our package\npip install gradient-descent-the-ultimate-optimizer\n```\n# Example\nFirst, build the MLP and initialize data loaders as you would normally in PyTorch.\n```python\nimport math\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MNIST_FullyConnected(nn.Module):\n    \"\"\"\n    A fully-connected NN for the MNIST task. This is Optimizable but not itself\n    an optimizer.\n    \"\"\"\n    def __init__(self, num_inp, num_hid, num_out):\n        super(MNIST_FullyConnected, self).__init__()\n        self.layer1 = nn.Linear(num_inp, num_hid)\n        self.layer2 = nn.Linear(num_hid, num_out)\n\n    def initialize(self):\n        nn.init.kaiming_uniform_(self.layer1.weight, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.layer2.weight, a=math.sqrt(5))\n\n    def forward(self, x):\n        \"\"\"Compute a prediction.\"\"\"\n        x = self.layer1(x)\n        x = torch.tanh(x)\n        x = self.layer2(x)\n        x = torch.tanh(x)\n        x = F.log_softmax(x, dim=1)\n        return x\n\nBATCH_SIZE = 256\nEPOCHS = 5\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmnist_train = torchvision.datasets.MNIST('./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\nmnist_test = torchvision.datasets.MNIST('./data', train=False, download=True, transform=torchvision.transforms.ToTensor())\ndl_train = torch.utils.data.DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True)\ndl_test = torch.utils.data.DataLoader(mnist_test, batch_size=10000, shuffle=False)\n\nmodel = MNIST_FullyConnected(28 * 28, 128, 10).to(DEVICE)\n```\nNext, import our package and initialize a stack of hyperoptimizers. This example uses the stack `Adam/SGD`.\n```python\nfrom gradient_descent_the_ultimate_optimizer import gdtuo\n\noptim = gdtuo.Adam(optimizer=gdtuo.SGD(1e-5))\n```\n`gdtuo.ModuleWrapper` allows any `nn.Module` to be optimized by hyperoptimizers.\n```python\nmw = gdtuo.ModuleWrapper(model, optimizer=optim)\nmw.initialize()\n```\nLastly, use `mw` instead of a PyTorch optimizer to optimize the model. The train loop is nearly identical to what you would typically implement in PyTorch (differences are marked by comments).\n```python\nfor i in range(1, EPOCHS+1):\n    running_loss = 0.0\n    for j, (features_, labels_) in enumerate(dl_train):\n        mw.begin() # call this before each step, enables gradient tracking on desired params\n        features, labels = torch.reshape(features_, (-1, 28 * 28)).to(DEVICE), labels_.to(DEVICE)\n        pred = mw.forward(features)\n        loss = F.nll_loss(pred, labels)\n        mw.zero_grad()\n        loss.backward(create_graph=True) # important! use create_graph=True\n        mw.step()\n        running_loss += loss.item() * features_.size(0)\n    train_loss = running_loss / len(dl_train.dataset)\n    print(\"EPOCH: {}, TRAIN LOSS: {}\".format(i, train_loss))\n```\nNote that on the first step of the train loop PyTorch will return the following warning:\n```\nUserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.\n```\nThis is normal and to be expected.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "gradient-descent-the-ultimate-optimizer",
    "package_url": "https://pypi.org/project/gradient-descent-the-ultimate-optimizer/",
    "platform": null,
    "project_url": "https://pypi.org/project/gradient-descent-the-ultimate-optimizer/",
    "project_urls": {
      "Homepage": "https://github.com/kach/gradient-descent-the-ultimate-optimizer"
    },
    "release_url": "https://pypi.org/project/gradient-descent-the-ultimate-optimizer/1.0/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Code for paper, Gradient Descent: The Ultimate Optimizer",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15346851,
  "releases": {
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eb37a2719829373d4910d6a9d602d3e86c7388bfd77af298df35999dee7ca62d",
          "md5": "d870c9f7974d7167f59a30ce68c053ca",
          "sha256": "14e3de34f9c81d44e2370b3a36b50d0a4916520737d0200f1a0d600ff5208207"
        },
        "downloads": -1,
        "filename": "gradient_descent_the_ultimate_optimizer-1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d870c9f7974d7167f59a30ce68c053ca",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 7252,
        "upload_time": "2022-10-08T22:46:45",
        "upload_time_iso_8601": "2022-10-08T22:46:45.048088Z",
        "url": "https://files.pythonhosted.org/packages/eb/37/a2719829373d4910d6a9d602d3e86c7388bfd77af298df35999dee7ca62d/gradient_descent_the_ultimate_optimizer-1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8152fd1c83dea7858e0a3b175096a224d2bcee452a837f72209663b0c4ee36ac",
          "md5": "7ed5ea755afcda98b0411e8ee18a5a2d",
          "sha256": "fe10b4cce2e36adfb3538e0ad97d444660be98bce7139639b11c3c72c01a9078"
        },
        "downloads": -1,
        "filename": "gradient_descent_the_ultimate_optimizer-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7ed5ea755afcda98b0411e8ee18a5a2d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 6665,
        "upload_time": "2022-10-08T22:46:46",
        "upload_time_iso_8601": "2022-10-08T22:46:46.582070Z",
        "url": "https://files.pythonhosted.org/packages/81/52/fd1c83dea7858e0a3b175096a224d2bcee452a837f72209663b0c4ee36ac/gradient_descent_the_ultimate_optimizer-1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eb37a2719829373d4910d6a9d602d3e86c7388bfd77af298df35999dee7ca62d",
        "md5": "d870c9f7974d7167f59a30ce68c053ca",
        "sha256": "14e3de34f9c81d44e2370b3a36b50d0a4916520737d0200f1a0d600ff5208207"
      },
      "downloads": -1,
      "filename": "gradient_descent_the_ultimate_optimizer-1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d870c9f7974d7167f59a30ce68c053ca",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 7252,
      "upload_time": "2022-10-08T22:46:45",
      "upload_time_iso_8601": "2022-10-08T22:46:45.048088Z",
      "url": "https://files.pythonhosted.org/packages/eb/37/a2719829373d4910d6a9d602d3e86c7388bfd77af298df35999dee7ca62d/gradient_descent_the_ultimate_optimizer-1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8152fd1c83dea7858e0a3b175096a224d2bcee452a837f72209663b0c4ee36ac",
        "md5": "7ed5ea755afcda98b0411e8ee18a5a2d",
        "sha256": "fe10b4cce2e36adfb3538e0ad97d444660be98bce7139639b11c3c72c01a9078"
      },
      "downloads": -1,
      "filename": "gradient_descent_the_ultimate_optimizer-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "7ed5ea755afcda98b0411e8ee18a5a2d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 6665,
      "upload_time": "2022-10-08T22:46:46",
      "upload_time_iso_8601": "2022-10-08T22:46:46.582070Z",
      "url": "https://files.pythonhosted.org/packages/81/52/fd1c83dea7858e0a3b175096a224d2bcee452a837f72209663b0c4ee36ac/gradient_descent_the_ultimate_optimizer-1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}