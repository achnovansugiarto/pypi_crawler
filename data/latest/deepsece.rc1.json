{
  "info": {
    "author": "Yumeng Zhang",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# DeepSecE\n\nImplementation of Effector-specific Transformer model used in secretion effector prediction in Gram-negative bacteria. DeepSecE achieves state-of-the-art performance in multi-class effector prediction leveraging the power of  pre-trained protein language model [ESM-1b](https://github.com/facebookresearch/esm). An additional transformer layer enhances the understanding of secreted patterns. It also provide a rapid pipeline to identify type I-IV and VI secretion systems with corresponding effectors.\n\n![](summary.png)\n\n## Performance Comparison\n\nWe choose various model architecture with different pre-trained models and training strategies, and evalute their model capacity on cross-validation and independent testing. Performance metrics are reported in the table.\n\n| Pre-trained   Model |           Strategy           |       ACC       |                |       F1       |                |      AUPRC      |                |\n| :-----------------: | :---------------------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |\n|                    |                              |      Valid      |      Test      |      Valid      |      Test      |      Valid      |      Test      |\n|          /          |           PSSM+CNN           |      0.799      |      0.822      |      0.712      |      0.724      |      0.752      |      0.774      |\n|      TAPEBert      |        Linear probing        |      0.816      |      0.838      |      0.764      |      0.770      |      0.802      |      0.822      |\n|       ESM-1b       |        Linear probing        |      0.876      |      0.870      |      0.841      |      0.810      |      0.880      |      0.871      |\n|       ESM-1b       |          Finetuning          |      0.878      |      0.850      |      0.846      |      0.808      |      0.887      | **0.883** |\n|       ESM-1b       | Effector-specific transformer | **0.883** | **0.898** | **0.848** | **0.849** | **0.892** |      0.879      |\n\n## Set up\n\n### Requirements\n\n- python==3.9.7\n- torch==1.10.2\n- biopython==1.79\n- einops==0.4.1\n- fair-esm>=0.4.0\n- tqdm==4.64.0\n- numpy==1.21.2\n- scikit-learn==0.23.2\n- matplotlib==3.5.1\n- seaborn==0.11.0\n- tensorboardX==2.0\n- umap-learn==0.5.3\n- warmup-scheduler==0.3.2\n\nWhile we have not tested with other versions, any reasonably recent versions of these requirements should work.\n\n### Installation\n\nAs a prerequisite, you must have PyTorch installed. It is recommended to create a new virtual environment for installation. For model training and prediction from seperate protein sequence(s), You can use this one-liner for installation.\n\n```shell\npip install git+https://github.com/zhangyumeng1sjtu/DeepSecE.git\n```\n\nIf you want to plot the sequence attention, you should install package `logomarker` first.\n\n```shell\npip install logomaker\n```\n\nIf you want to predict secretion systems and effectors, you should install `macsyfinder` and `hmmer` first. Meanwhile, you need to download the TXSS profiles from [here](https://tool2-mml.sjtu.edu.cn/DeepSecE/TXSS_profiles.tar.gz), and decompress it into data directory.\n\n```shell\npip install macsyfinder\nconda install -c bioconda hmmer\ncd data\nwget https://tool2-mml.sjtu.edu.cn/DeepSecE/TXSS_profiles.tar.gz\ntar -zxvf TXSS_profiles.tar.gz\n```\n\nThe weights of DeepSecE model can be downloaded from https://tool2-mml.sjtu.edu.cn/DeepSecE/checkpoint.pt.\n\n## Usage\n\n### Train model\n\nYou can train the DeepSecE model by running `train.py` or `scripts/kfold_train.sh` for cross-validation.\n\n```shell\nfor i in {0..4}\ndo\n   python train.py --model effectortransformer \\\n\t\t--data_dir data \\\n\t\t--batch_size 32 \\\n\t\t--lr 5e-5 \\\n\t\t--weight_decay 4e-5 \\\n\t\t--dropout_rate 0.4 \\\n\t\t--num_layers 1 \\\n\t\t--num_heads 4 \\\n\t\t--warm_epochs 1 \\\n\t\t--patience 5 \\\n\t\t--lr_scheduler cosine \\\n\t\t--lr_decay_steps 30 \\\n\t\t--kfold 5 \\\n\t\t--fold_num $i \\\n\t\t--log_dir runs/attempt_cv\ndone\n```\n\n Parameters:\n\n- `--model` train a effector transformer or finetue a ESM-1b model.\n- `--data_dir` directory that stores training data (default: ./data).\n- `--num_layers` numbers of trainable transformer layer. (default: 1)\n- `--num_heads` numbers of attention heads in effector-specific transformer (default: 4).\n- `--patience` patience for early stopping used in training.\n- `--lr_schedular` learning rate schedular [step, consine].\n- `--log_dir` directory that stores training outputs (default: logs).\n\n### Prediction\n\nYou can predict your interested type of secreted effectors only or predict secretion systems and corresponding effectors  from scratch.\n\n#### Predict secretion effector\n\n```shell\npython predict.py --fasta_path examples/Test.fasta \\\n\t\t--model_location [path to model weights] \\\n\t\t--secretion_systems [I II III IV VI] \\\n\t\t--out_dir examples [--save_attn --no_cuda]\n```\n\nParameters:\n\n- `--fasta_path` path to the input protein FASTA file.\n- `--model_location` path to the model weights (download from [here](https://tool2-mml.sjtu.edu.cn/DeepSecE/checkpoint.pt)).\n- `--secretion_systems` type(s) of secretion system to predict (default: I II III IV VI).\n- `--out_dir` directory that stores prediction outputs.\n- `--save_attn` add to save sequence attention of effector.\n- `--no_cuda` add when CUDA is not available.\n\n#### Predict secretion system and effectors\n\n**Note:** Make sure the input file is **ordered protein sequences coded in a bacterial genome**.\n\n```shell\npython predict_genome.py --fasta_path examples/NC_002516.2_protein.fasta \\\n\t\t\t--model_location [path to model weights] \\\n\t\t\t--data_dir data \\\n\t\t\t--out_dir examples/NC_002516.2 [--save_attn --no_cuda]\n```\n\nParameters:\n\n- `--fasta_path` path to the input protein FASTA file.\n- `--model_location` path to the model weights (download from [here](https://tool2-mml.sjtu.edu.cn/DeepSecE/checkpoint.pt)).\n- `--data_dir` directory that stores TXSS profiles (download from [here](https://tool2-mml.sjtu.edu.cn/DeepSecE/TXSS_profiles.tar.gz)).\n- `--out_dir` directory that stores prediction outputs.\n- `--save_attn` add to save sequence attention of effector.\n- `--no_cuda` add when CUDA is not available.\n\nIt takes about 5 minutes to predict effectors from a bacterial genome containing 3000 proteion coding sequences on a NVIDIA GeForce RTX 2080 Super GPU.\n\n### Plot attention\n\nIf you save the attention output of the putative effectors (add `--save_attn`), you can run `python scripts /plot_attention.py [directory of prediction output]` to plot the saliency map from attention, and infer potentially import regions related to protein secretion.\n\n## Contact\n\nPlease contact Yumeng Zhang at [zhangyumeng1@sjtu.edu.cn](mailto:zhangyumeng1@sjtu.edu.cn) for questions.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/zhangyumeng1sjtu/DeepSecE",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "DeepSecE",
    "package_url": "https://pypi.org/project/DeepSecE/",
    "platform": null,
    "project_url": "https://pypi.org/project/DeepSecE/",
    "project_urls": {
      "Homepage": "https://github.com/zhangyumeng1sjtu/DeepSecE"
    },
    "release_url": "https://pypi.org/project/DeepSecE/0.1.0/",
    "requires_dist": [
      "torch",
      "biopython",
      "einops",
      "fair-esm",
      "tqdm",
      "numpy",
      "scikit-learn",
      "matplotlib",
      "seaborn",
      "tensorboardX",
      "umap-learn",
      "warmup-scheduler"
    ],
    "requires_python": "",
    "summary": "A Deep Learning Framework for Multi-class Secreted Effector Prediction in Gram-negative Bacteria.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14921402,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "423f67328c9080a6c77c05b03ae07455590b241888f45539e7daeaa6a31c556b",
          "md5": "09336ce883f4cdd17478e18a195e1c54",
          "sha256": "e49c04e988fd64018902f3738cff4dc52842a4061704b8129616a564572caf2c"
        },
        "downloads": -1,
        "filename": "DeepSecE-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "09336ce883f4cdd17478e18a195e1c54",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 9381,
        "upload_time": "2022-08-29T07:22:40",
        "upload_time_iso_8601": "2022-08-29T07:22:40.282512Z",
        "url": "https://files.pythonhosted.org/packages/42/3f/67328c9080a6c77c05b03ae07455590b241888f45539e7daeaa6a31c556b/DeepSecE-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "349569c12ba202b89f960e32c98617d582bd41c11e29776b5aec2b3e48597301",
          "md5": "765c33ff69f6fb9e3f6a5a8dcef57b6b",
          "sha256": "c9104dbc5d5e8aac0f6afe7491c3de6e86b6c993f0591d7fbd234f277ffaf1dc"
        },
        "downloads": -1,
        "filename": "DeepSecE-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "765c33ff69f6fb9e3f6a5a8dcef57b6b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 8719206,
        "upload_time": "2022-08-29T07:22:45",
        "upload_time_iso_8601": "2022-08-29T07:22:45.722312Z",
        "url": "https://files.pythonhosted.org/packages/34/95/69c12ba202b89f960e32c98617d582bd41c11e29776b5aec2b3e48597301/DeepSecE-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "423f67328c9080a6c77c05b03ae07455590b241888f45539e7daeaa6a31c556b",
        "md5": "09336ce883f4cdd17478e18a195e1c54",
        "sha256": "e49c04e988fd64018902f3738cff4dc52842a4061704b8129616a564572caf2c"
      },
      "downloads": -1,
      "filename": "DeepSecE-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "09336ce883f4cdd17478e18a195e1c54",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 9381,
      "upload_time": "2022-08-29T07:22:40",
      "upload_time_iso_8601": "2022-08-29T07:22:40.282512Z",
      "url": "https://files.pythonhosted.org/packages/42/3f/67328c9080a6c77c05b03ae07455590b241888f45539e7daeaa6a31c556b/DeepSecE-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "349569c12ba202b89f960e32c98617d582bd41c11e29776b5aec2b3e48597301",
        "md5": "765c33ff69f6fb9e3f6a5a8dcef57b6b",
        "sha256": "c9104dbc5d5e8aac0f6afe7491c3de6e86b6c993f0591d7fbd234f277ffaf1dc"
      },
      "downloads": -1,
      "filename": "DeepSecE-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "765c33ff69f6fb9e3f6a5a8dcef57b6b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 8719206,
      "upload_time": "2022-08-29T07:22:45",
      "upload_time_iso_8601": "2022-08-29T07:22:45.722312Z",
      "url": "https://files.pythonhosted.org/packages/34/95/69c12ba202b89f960e32c98617d582bd41c11e29776b5aec2b3e48597301/DeepSecE-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}