{
  "info": {
    "author": "Riccardo Orlando",
    "author_email": "orlandoricc@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<div align=\"center\">\n\n# NLP Preprocessing Wrappers\n\n[![Open in Visual Studio Code](https://open.vscode.dev/badges/open-in-vscode.svg)](https://github.dev/Riccorl/nlp-preprocessing-wrappers)\n[![PyTorch](https://img.shields.io/badge/PyTorch-orange?logo=pytorch)](https://pytorch.org/)\n[![Stanza](https://img.shields.io/badge/1.3-Stanza-5f0a09?logo=stanza)](https://stanfordnlp.github.io/stanza/)\n[![SpaCy](https://img.shields.io/badge/3.2.3-SpaCy-1a6f93?logo=soacy)](https://spacy.io/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000)](https://github.com/psf/black)\n\n[![Upload to PyPi](https://github.com/Riccorl/nlp-preprocessing-wrappers/actions/workflows/python-publish-pypi.yml/badge.svg)](https://github.com/Riccorl/nlp-preprocessing-wrappers/actions/workflows/python-publish-pypi.yml)\n[![PyPi Version](https://img.shields.io/github/v/release/Riccorl/nlp-preprocessing-wrappers)](https://github.com/Riccorl/nlp-preprocessing-wrappers/releases)\n[![DeepSource](https://deepsource.io/gh/Riccorl/nlp-preprocessing-wrappers.svg/?label=active+issues&token=QC6Jty-YdgXjKh9mKZyeqa4I)](https://deepsource.io/gh/Riccorl/nlp-preprocessing-wrappers/?ref=repository-badge)\n\n</div>\n\nPreprocessing Wrappers\n\n## How to use\n\n### Install\n\nInstall the library from [PyPI](https://pypi.org/project/nlp-preprocessing-wrappers):\n\n```bash\npip install nlp-preprocessing-wrappers\n```\n\n### Usage\n\nNLP Preprocessing Wrappers is a Python library that provides a set of preprocessing wrappers for Stanza and\nspaCy, providing a unified API for both libraries, making them interchangeable.\n\nLet's start with a simple example. Here we are using the `SpacyTokenizer` wrapper to preprocess a text: \n\n```python\nfrom nlp_preprocessing_wrappers import SpacyTokenizer\n\nspacy_tokenizer = SpacyTokenizer(language=\"en\", return_pos_tags=True, return_lemmas=True)\ntokenized = spacy_tokenizer(\"Mary sold the car to John.\")\nfor word in tokenized:\n    print(\"{:<5} {:<10} {:<10} {:<10}\".format(word.index, word.text, word.pos, word.lemma))\n\n\"\"\"\n0    Mary       PROPN      Mary\n1    sold       VERB       sell\n2    the        DET        the\n3    car        NOUN       car\n4    to         ADP        to\n5    John       PROPN      John\n6    .          PUNCT      .\n\"\"\"\n```\n\nYou can load any model from spaCy, with its canonical name, `en_core_web_sm`, or with a simple alias, as \nwe did here, like `en`. By default, the simpler alias loads the smaller version of each model. For a complete \nlist of available models, see [spaCy documentation](https://spacy.io/usage/models).\n\nIn the very same way, you can load any model from Stanza using the `StanzaTokenizer` wrapper:\n\n```python\nfrom nlp_preprocessing_wrappers import StanzaTokenizer\n\nstanza_tokenizer = StanzaTokenizer(language=\"en\", return_pos_tags=True, return_lemmas=True)\ntokenized = stanza_tokenizer(\"Mary sold the car to John.\")\nfor word in tokenized:\n    print(\"{:<5} {:<10} {:<10} {:<10}\".format(word.index, word.text, word.pos, word.lemma))\n\n\"\"\"\n0    Mary       PROPN      Mary\n1    sold       VERB       sell\n2    the        DET        the\n3    car        NOUN       car\n4    to         ADP        to\n5    John       PROPN      John\n6    .          PUNCT      .\n\"\"\"\n```\n\nFor more simple scenarios, you can use the `WhiteSpaceTokenizer` wrapper, which will just split the text \nby whitespace:\n\n```python\nfrom nlp_preprocessing_wrappers import WhitespaceTokenizer\n\nwhitespace_tokenizer = WhitespaceTokenizer()\ntokenized = whitespace_tokenizer(\"Mary sold the car to John .\")\nfor word in tokenized:\n    print(\"{:<5} {:<10}\".format(word.index, word.text))\n\n\"\"\"\n0    Mary\n1    sold\n2    the\n3    car\n4    to\n5    John\n6    .\n\"\"\"\n```\n\n### Features\n\n#### Complete preprocessing pipeline\n\n`SpacyTokenizer` and `StanzaTokenizer` provide a unified API for both libraries, exposing most of their\nfeatures, like tokenization, Part-of-Speech tagging, lemmatization and dependency parsing. You can activate \nand deactivate any of these using `return_pos_tags`, `return_lemmas` and `return_deps`. So, for example,\n\n```python\nStanzaTokenizer(language=\"en\", return_pos_tags=True, return_lemmas=True)\n```\n\nwill return a list of `Token` objects, with the `pos` and `lemma` fields filled.\n\nwhile\n\n```python\nStanzaTokenizer(language=\"en\")\n```\n\nwill return a list of `Token` objects, with only the `text` field filled.\n\n### GPU support\n\nWith `use_gpu=True`, the library will use the GPU if it is available. To set up the environment for the GPU, \nrefer to the [Stanza documentation](https://stanfordnlp.github.io/stanza/) and the \n[spaCy documentation](https://spacy.io/usage/gpu).\n\n## API\n\n### Tokenizers\n\n`SpacyTokenizer`\n\n```python\nclass SpacyTokenizer(BaseTokenizer):\n    def __init__(\n        self,\n        language: str = \"en\",\n        return_pos_tags: bool = False,\n        return_lemmas: bool = False,\n        return_deps: bool = False,\n        split_on_spaces: bool = False,\n        use_gpu: bool = False,\n    ):\n```\n\n`StanzaTokenizer`\n\n```python\nclass StanzaTokenizer(BaseTokenizer):\n    def __init__(\n        self,\n        language: str = \"en\",\n        return_pos_tags: bool = False,\n        return_lemmas: bool = False,\n        return_deps: bool = False,\n        split_on_spaces: bool = False,\n        use_gpu: bool = False,\n    ):\n```\n\n`WhitespaceTokenizer`\n\n```python\nclass WhitespaceTokenizer(BaseTokenizer):\n    def __init__(self):\n```\n\n### Sentence Splitter\n\n`SpacySentenceSplitter`\n\n```python\nclass SpacySentenceSplitter(BaseSentenceSplitter):\n    def __init__(self, language: str = \"en\", model_type: str = \"statistical\"):\n```\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Riccorl/preprocessing-wrappers",
    "keywords": "NLP deep learning transformer pytorch stanza spacy trankit preprocessing tokenization pos tagging lemmatization",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "nlp-preprocessing-wrappers",
    "package_url": "https://pypi.org/project/nlp-preprocessing-wrappers/",
    "platform": null,
    "project_url": "https://pypi.org/project/nlp-preprocessing-wrappers/",
    "project_urls": {
      "Homepage": "https://github.com/Riccorl/preprocessing-wrappers"
    },
    "release_url": "https://pypi.org/project/nlp-preprocessing-wrappers/0.1.3/",
    "requires_dist": [
      "spacy (<3.3,>=3.2)",
      "stanza (<1.4,>=1.2)",
      "overrides (<7.0.0,>=6.0.0)",
      "torch (<1.11,>=1.5) ; extra == 'all'",
      "torch (<1.11,>=1.5) ; extra == 'torch'"
    ],
    "requires_python": ">=3.6",
    "summary": "NLP Preprocessing Pipeline Wrappers",
    "version": "0.1.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13104193,
  "releases": {
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ee051d5f95d8fa3ae99dfa5886803cc86e5c6cc006c849d55bfd08545975e8f7",
          "md5": "4974f98ad603cc1609a1191045b57bda",
          "sha256": "6da7d689dc386f5f3ede627ffb8cc1204d9bb5bb811f05660601c4e2f5f6f385"
        },
        "downloads": -1,
        "filename": "nlp_preprocessing_wrappers-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4974f98ad603cc1609a1191045b57bda",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 16704,
        "upload_time": "2022-03-03T14:08:46",
        "upload_time_iso_8601": "2022-03-03T14:08:46.616615Z",
        "url": "https://files.pythonhosted.org/packages/ee/05/1d5f95d8fa3ae99dfa5886803cc86e5c6cc006c849d55bfd08545975e8f7/nlp_preprocessing_wrappers-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9c5429ef73399b4324d72fd69477ddac13e096e8a3cbb131ccba329f03711981",
          "md5": "560e3ed77e66224f0bb8be87a093d48d",
          "sha256": "a6a61610cb1540b6d48dc74054a8ab99c74505fc68461f9e04f45ced1f96745b"
        },
        "downloads": -1,
        "filename": "nlp_preprocessing_wrappers-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "560e3ed77e66224f0bb8be87a093d48d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 11846,
        "upload_time": "2022-03-03T14:08:48",
        "upload_time_iso_8601": "2022-03-03T14:08:48.357293Z",
        "url": "https://files.pythonhosted.org/packages/9c/54/29ef73399b4324d72fd69477ddac13e096e8a3cbb131ccba329f03711981/nlp_preprocessing_wrappers-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b6751a17f5bbba2ef68b3be2285a617fbbc1b193afb6cefc25fa5b4424e51cef",
          "md5": "4aaee7efe7d3387ea529e27adff96c23",
          "sha256": "005ae7039951e7a9935240dc81656b1335f543a18f7de31d27497048181d3ad2"
        },
        "downloads": -1,
        "filename": "nlp_preprocessing_wrappers-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4aaee7efe7d3387ea529e27adff96c23",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 16918,
        "upload_time": "2022-03-07T17:39:56",
        "upload_time_iso_8601": "2022-03-07T17:39:56.870008Z",
        "url": "https://files.pythonhosted.org/packages/b6/75/1a17f5bbba2ef68b3be2285a617fbbc1b193afb6cefc25fa5b4424e51cef/nlp_preprocessing_wrappers-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dec875756fcdf9fba4b06dbc2d6dacfe0fad012b9be5871168b58b38fbc84da3",
          "md5": "b78b1df712f1ae55d6ab00343710e584",
          "sha256": "2e5bdb01e3e1accb34c8efff72c84a60cc27063efee6251769742a55859f905b"
        },
        "downloads": -1,
        "filename": "nlp_preprocessing_wrappers-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "b78b1df712f1ae55d6ab00343710e584",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 12909,
        "upload_time": "2022-03-07T17:39:59",
        "upload_time_iso_8601": "2022-03-07T17:39:59.604608Z",
        "url": "https://files.pythonhosted.org/packages/de/c8/75756fcdf9fba4b06dbc2d6dacfe0fad012b9be5871168b58b38fbc84da3/nlp_preprocessing_wrappers-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b6751a17f5bbba2ef68b3be2285a617fbbc1b193afb6cefc25fa5b4424e51cef",
        "md5": "4aaee7efe7d3387ea529e27adff96c23",
        "sha256": "005ae7039951e7a9935240dc81656b1335f543a18f7de31d27497048181d3ad2"
      },
      "downloads": -1,
      "filename": "nlp_preprocessing_wrappers-0.1.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4aaee7efe7d3387ea529e27adff96c23",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 16918,
      "upload_time": "2022-03-07T17:39:56",
      "upload_time_iso_8601": "2022-03-07T17:39:56.870008Z",
      "url": "https://files.pythonhosted.org/packages/b6/75/1a17f5bbba2ef68b3be2285a617fbbc1b193afb6cefc25fa5b4424e51cef/nlp_preprocessing_wrappers-0.1.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dec875756fcdf9fba4b06dbc2d6dacfe0fad012b9be5871168b58b38fbc84da3",
        "md5": "b78b1df712f1ae55d6ab00343710e584",
        "sha256": "2e5bdb01e3e1accb34c8efff72c84a60cc27063efee6251769742a55859f905b"
      },
      "downloads": -1,
      "filename": "nlp_preprocessing_wrappers-0.1.3.tar.gz",
      "has_sig": false,
      "md5_digest": "b78b1df712f1ae55d6ab00343710e584",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 12909,
      "upload_time": "2022-03-07T17:39:59",
      "upload_time_iso_8601": "2022-03-07T17:39:59.604608Z",
      "url": "https://files.pythonhosted.org/packages/de/c8/75756fcdf9fba4b06dbc2d6dacfe0fad012b9be5871168b58b38fbc84da3/nlp_preprocessing_wrappers-0.1.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}