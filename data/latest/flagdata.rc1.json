{
  "info": {
    "author": "",
    "author_email": "Xuezhi Fang <xzfang@baai.ac.cn>, Zhao Xue <xuezhao@baai.ac.cn>, Hanyu Zhao <hyzhao@baai.ac.cn>, Quanyue Ma <maqy@baai.ac.cn>, Jiahong Leng <jhleng@baai.ac.cn>, Zheng Zhang <zhangz.goal@gmail.com>, Yequan Wang <yqwang@baai.ac.cn>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<div id=top align=\"center\">\n\n![FlagData](flagdata_logo.png)\n[![Pypi Package](https://img.shields.io/pypi/v/flagdata?label=pypi%20package)](https://pypi.org/project/flagdata/)\n[![Python Application](https://github.com/Flag/FlagData/actions/workflows/python-app.yml/badge.svg)](https://github.com/FlagOpen/FlagData/actions/workflows/python-app.yml)\n[![License](https://img.shields.io/github/license/FlagOpen/FlagData.svg?color=blue)](https://github.com/FlagOpen/FlagData/blob/main/LICENSE)\n![GitHub release (release name instead of tag name)](https://img.shields.io/github/v/release/FlagOpen/FlagData?include_prereleases&style=social)\n\n   | [English](README.md) | [中文](README_zh.md) |\n\n</div>\n\n-----------------------------------------------------------------------\nData is one of the essential elements in the development of artificial intelligence. With the continuous breakthrough of large-scale pre-training models and related technologies, using efficient data processing tools to improve data quality in the corresponding research becomes increasingly important. Therefore, we present **FlagData**, a data processing toolkit that is easy to use and expand. FlagData integrates the tools and algorithms of multi-step data processing, including cleaning, condensation, annotation and analysis, providing powerful data processing support for model training and deployment in multiple fields, including natural language processing and computer vision.\n\nFlagData supports the following features:\n\n* Able to be used with simple configuration after installation. Realize customization functions with a few lines of code.\n\n* Provide methods that condense training data based on the distillation algorithm, achieving competitive performance with full-data training.\n\n* Help obtain high-quality structured data from raw html/text quickly. Sensitive information can be filtered out to avoid the risk of privacy disclosure.\n\n* Support the data annotation of multiple tasks in natural language processing and computer vision. The annotation results are convenient to be read and use.\n\n## News\n- [3rd Jan 2023] FlagData v1.0.0 is released! \n\n--------------------------------------------------------------------------------\n\n- [Prerequisites and installation](#prerequisites-and-installation)\n- [Quick Start](#quick-start)\n    - [Data Cleaning](#data-cleaning)\n    - [Data Analysis](#data-analysis)\n    - [Data Condensation](#data-condensation)\n    - [Data Annotation](#data-annotation)\n- [Configuration](#configuration)\n- [Tutorials](#tutorials)\n- [Contact us](#contact-us)\n- [Reference](#reference)\n- [License](#license)\n\n## Prerequisites and Installation\n- Python version >= 3.9\n- Pytorch version >= 1.11 if you want to use condensation module. Please refer to the [official website](https://pytorch.org/get-started/locally/) and install the appropriate PyTorch version based on your environment.\n- Optional: [NGINX](https://www.nginx.com/resources/wiki/start/topics/tutorials/install/) is required if you want to use the annotation module. Follow the [Quick Start](#quick-start) to setup once nginx is installed.\nInstall FlagData with all modules via pip. By doing this, the dependencies for all modules will be installed:\n```bash\npip install flagdata[all]\n```\nInstall preferred Flagdata modules via pip (`module` need to be replaced by module's name, such as `cleaner`, `condensation`, `analysis`, etc. ). By doing this, only the specified module's dependencies will be installed. This is for users who only want to use a specific module and do not want to install the other dependencies:\n```bash\npip install flagdata[module]\n```\n\n**Install the latest main-branch version**\n\nIf you want to install the up-to-date version from main branch, \nuse the following method: \n```\npip install .[all]\n```\n\n**Develop FlagData locally**\n```bash\ngit clone https://github.com/cofe-ai/FlagData.git\npip install -r requirements.txt\n```\n\n## Quick Start\n\n### Data Cleaning\nThere are basically 2 steps in order to use our FlagData Cleaner tool:\n\n1. Modify the YAML config file according to your data format. We have written detailed comments in the configuration file to explain the meaning of each parameter. You can also refer to [Configuration](#configuration).\n\n2. Specify the path to the configuration file and run!\n    ```python\n    from flagdata.cleaner.text_cleaner import DataCleaner\n    # use safe importing of main module in multi-processing  \n    if __name__ == \"__main__\": \n        # you need to specify your own configuration file path\n        cleaner = DataCleaner(\"config.yaml\")\n        cleaner.clean()\n    ```\n    \nThe cleaned data will be saved to the corresponding path in `jsonl` format according to the `output` parameter in the configuration file.\n\n### Data Analysis\nThe quickest way to use our analyzer is to use our client to call the official demo server and specify the language.\n\n```python\nfrom flagdata.analysis.text_analyzer import CoreNLPAnalyzer\n# call the official demo server, or you can setup you own server\nanalyzer = CoreNLPAnalyzer(url=\"https://corenlp.run\", lang=\"en\")\ndata = \"FlagData is a fast and extensible toolkit for data processing provided by BAAI. Enjoy yourself! \"\ntokenized_text = analyzer.tokenize(data)\nprint(tokenized_text)\n# [['FlagData', 'is', 'a', 'fast', 'and', 'extensible', 'toolkit', 'for', 'data', 'processing', 'provided', 'by', 'BAAI', '.'], ['Enjoy', 'yourself', '!']]\npos_tags = analyzer.pos_tag(data)\nprint(pos_tags)\n# [['NNP', 'VBZ', 'DT', 'JJ', 'CC', 'JJ', 'NN', 'IN', 'NN', 'NN', 'VBN', 'IN', 'NN', '.'], ['VB', 'PRP', '.']]\nners = analyzer.ner(data)\nprint(ners)\n# [[{('BAAI', (74, 78)): 'ORGANIZATION'}], []]\nanalyzer.close()\n```\n\n### Data Condensation\n\nThere are basically 2 steps in order to use our FlagData Condensation tool:\n\n1. Modify the YAML configuration file. We have written detailed comments in the configuration file to explain the meaning of each parameter. You can also refer to [Configuration](#configuration).\n\n2. Specify the path to the config file and run!\n   \n   ```python\n   from flagdata.condensation.data_distillation import DataDistillationTrainer\n   # you need to specify your own configuration file path here\n   trainer = DataDistillationTrainer(\"flagdata/config/distillation_config.yaml\") \n   # data should be in jsonl format with keys: \"text\", \"label\"\n   trainer.load_data()\n   # fit() will run data condensation training and save the distilled data in binary format which can be read by torch.load()\n   # you can specify the save path by setting \"distilled_save_path\" in config file\n   trainer.fit()\n   ```\n\n### Data Annotation\n\n1. Put the `flagdata/annotation/dist` folder under the default `html` of nginx.\n\n2. Modify `nginx.confg` to add location.\n\n   ```\n   location / {\n       root /{your html path}/dist;   # change\n       index index.html index.htm;\n       try_files $uri $uri/ /index.html;\n   }\n   ```\n\n3. Restart nginx.\n\n4. Access the IP address configured by nginx.\n\n## Configuration\nFor the `Cleaner` and `Condensation` modules, we provide the following configuration templates: [cleaner_config.yaml](https://dorc.baai.ac.cn/resources/projects/FlagData/cleaner_config.yaml), [distillation_config.yaml](https://dorc.baai.ac.cn/resources/projects/FlagData/distillation_config.yaml). The config files are in human-readable [YAML](https://yaml.org) format with detailed comments. Make sure you've modified related parameters before using.\n\nYou may need to pay attention to the following parameters:\n### Cleaner\n```yaml\n  # path of the raw data to be cleaned.\n  input: ./demo/demo_input.jsonl\n  # cleaned data file save path\n  output: ./demo/output.jsonl\n```\n### Condensation\n```yaml\n  train_data_path: <path to your train data>\n  test_data_path: <path to your test data>\n  # pretrained models from huggingface\n  model_name: \"/data/scripts/pretrained_models/distilbert-base-uncased\"\n  # model.fit() will run data condensaton algorithm and save distilled data here with binary format which can be read by torch.load()\n  distilled_save_path: <path to save distilled data>\n  # optional: load distilled data before training for initialization or to resume training\n  distilled_load_path: null\n```\n\n## Tutorials\n\nWe provide a series of tutorials to help you quickly get started using FlagData's features.\n* [Tutorial 1: Clean raw text data crawled from the Internet](/docs/tutorial_01_cleaner.md)\n* [Tutorial 2: Analyze and process text data](/docs/tutorial_02_analysis.md)\n* [Tutorial 3: Using data distillation to reduce data usage](/docs/tutorial_03_condensation.md)\n* [Tutorial 4: Annotation for NLP tasks](/docs/tutorial_04_text_annotation.md)\n* [Tutorial 5: Annotation for CV tasks](/docs/tutorial_05_image_annotation.md)\n\n## Contact Us\nIf you have any questions about FlagData's usage and code, you can raise your issues. \nYou can also contact us through email at data@baai.ac.cn. \n\n## Reference\nThe project are partially based on \n[GeneralNewsExtractor](https://github.com/GeneralNewsExtractor/GeneralNewsExtractor), \n[emoji](https://github.com/carpedm20/emoji),\n[text-data-distillation](https://github.com/arumaekawa/text-dataset-distillation), \n[transformers](https://github.com/huggingface/transformers).\n\n## License\nThe majority of FlagData is licensed under the [Apache 2.0 license](LICENSE), however portions of the project are available under separate license terms:\n- GeneralNewsExtractor is licensed under the [GPL v3.0 license](https://github.com/GeneralNewsExtractor/GeneralNewsExtractor/blob/master/LICENSE)\n- emoji is licensed under the [BSD license](https://github.com/carpedm20/emoji/blob/master/LICENSE.txt)\n- transformers is licensed under the [Apache 2.0 license](https://github.com/huggingface/transformers/blob/main/LICENSE)\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "flagdata",
    "package_url": "https://pypi.org/project/flagdata/",
    "platform": null,
    "project_url": "https://pypi.org/project/flagdata/",
    "project_urls": {
      "Bug Tracker": "https://github.com/cofe-ai/flagdata/issues",
      "Homepage": "https://github.com/cofe-ai/flagdata"
    },
    "release_url": "https://pypi.org/project/flagdata/1.0.0/",
    "requires_dist": [
      "loguru>=0.6.0; extra == 'all'",
      "lxml>=4.9.1; extra == 'all'",
      "nltk>=3.7; extra == 'all'",
      "numpy>=1.21.5; extra == 'all'",
      "opencc-python-reimplemented>=0.1.6; platform_system == 'Darwin' and extra == 'all'",
      "opencc>=1.1.6; platform_system == 'Linux' and extra == 'all'",
      "pandas>=1.3.5; extra == 'all'",
      "pyyaml>=6.0; extra == 'all'",
      "requests>=2.28.1; extra == 'all'",
      "tqdm>=4.63.0; extra == 'all'",
      "transformers>=4.20.1; extra == 'all'",
      "urlextract>=1.6.0; extra == 'all'",
      "nltk>=3.7; extra == 'analysis'",
      "requests>=2.28.1; extra == 'analysis'",
      "lxml>=4.9.1; extra == 'cleaner'",
      "opencc-python-reimplemented>=0.1.6; platform_system == 'Darwin' and extra == 'cleaner'",
      "opencc>=1.1.6; platform_system == 'Linux' and extra == 'cleaner'",
      "pyyaml>=6.0; extra == 'cleaner'",
      "requests>=2.28.1; extra == 'cleaner'",
      "tqdm>=4.63.0; extra == 'cleaner'",
      "urlextract>=1.6.0; extra == 'cleaner'",
      "loguru>=0.6.0; extra == 'condensation'",
      "numpy>=1.21.5; extra == 'condensation'",
      "pandas>=1.3.5; extra == 'condensation'",
      "pyyaml>=6.0; extra == 'condensation'",
      "tqdm>=4.63.0; extra == 'condensation'",
      "transformers>=4.20.1; extra == 'condensation'"
    ],
    "requires_python": ">=3.8",
    "summary": "An out-of-the-box toolkit for data processing.",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16616837,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0190eb76039b8b71d863a77879bc5abe161a954f378af7630aaf8362434bf3a6",
          "md5": "4c1c6c2fd4b8a9e61034a9fa798e3b1c",
          "sha256": "8e7bb3d433ad9fb220472198613046cd84286623fe8945a75d3cdae0f62ffade"
        },
        "downloads": -1,
        "filename": "flagdata-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4c1c6c2fd4b8a9e61034a9fa798e3b1c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 960561,
        "upload_time": "2023-01-30T09:34:08",
        "upload_time_iso_8601": "2023-01-30T09:34:08.630826Z",
        "url": "https://files.pythonhosted.org/packages/01/90/eb76039b8b71d863a77879bc5abe161a954f378af7630aaf8362434bf3a6/flagdata-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3d8aec6901f5fbf1ed98fdac2496e77423631f3d614de8969cc7b6c616adf786",
          "md5": "4ea752dc619fd57066a35a85568c8934",
          "sha256": "8e88d03c9486990131696e66bb90d9a2f7933982f96395eed88ff45ca613696d"
        },
        "downloads": -1,
        "filename": "flagdata-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "4ea752dc619fd57066a35a85568c8934",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 946695,
        "upload_time": "2023-01-30T09:34:21",
        "upload_time_iso_8601": "2023-01-30T09:34:21.103362Z",
        "url": "https://files.pythonhosted.org/packages/3d/8a/ec6901f5fbf1ed98fdac2496e77423631f3d614de8969cc7b6c616adf786/flagdata-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0190eb76039b8b71d863a77879bc5abe161a954f378af7630aaf8362434bf3a6",
        "md5": "4c1c6c2fd4b8a9e61034a9fa798e3b1c",
        "sha256": "8e7bb3d433ad9fb220472198613046cd84286623fe8945a75d3cdae0f62ffade"
      },
      "downloads": -1,
      "filename": "flagdata-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4c1c6c2fd4b8a9e61034a9fa798e3b1c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 960561,
      "upload_time": "2023-01-30T09:34:08",
      "upload_time_iso_8601": "2023-01-30T09:34:08.630826Z",
      "url": "https://files.pythonhosted.org/packages/01/90/eb76039b8b71d863a77879bc5abe161a954f378af7630aaf8362434bf3a6/flagdata-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3d8aec6901f5fbf1ed98fdac2496e77423631f3d614de8969cc7b6c616adf786",
        "md5": "4ea752dc619fd57066a35a85568c8934",
        "sha256": "8e88d03c9486990131696e66bb90d9a2f7933982f96395eed88ff45ca613696d"
      },
      "downloads": -1,
      "filename": "flagdata-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "4ea752dc619fd57066a35a85568c8934",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 946695,
      "upload_time": "2023-01-30T09:34:21",
      "upload_time_iso_8601": "2023-01-30T09:34:21.103362Z",
      "url": "https://files.pythonhosted.org/packages/3d/8a/ec6901f5fbf1ed98fdac2496e77423631f3d614de8969cc7b6c616adf786/flagdata-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}