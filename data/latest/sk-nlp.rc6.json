{
  "info": {
    "author": "wengsongxiu",
    "author_email": "wengsongxiu@mastercom.cn",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "\n# sk-nlp\n\n[![Travis](https://travis-ci.org/CyberZHG/keras-transformer.svg)](https://travis-ci.org/CyberZHG/keras-transformer)\n[![Coverage](https://coveralls.io/repos/github/CyberZHG/keras-transformer/badge.svg?branch=master)](https://coveralls.io/github/CyberZHG/keras-transformer)\n\n![](https://img.shields.io/badge/keras-tensorflow-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras/eager-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras/2.0_beta-blue.svg)\n\n\n\nğŸ“¦ é¡¹ç›®ä»‹ç» (for humans)\n=======================\n\nè¿™ä¸ªç¬¬ä¸‰æ–¹ä»“åº“æ˜¯ç”±æ·±åœ³å¸‚åé€šç§‘æŠ€è‚¡ä»½æœ‰é™å…¬å¸AIå›¢é˜Ÿæä¾›çš„ã€‚å›¢é˜Ÿè‡´åŠ›äºä¸ºNLPé¢†åŸŸï¼Œæä¾›ä¸€ä¸ªç¨³å®šå¯é ï¼Œ åŠŸèƒ½å®Œå–„çš„NLPå¸¸è§æ“ä½œã€‚\n\n\nInstallation\n-----\n\n```bash\ncd your_project\npip install sk-nlp\n```\n\n# Content\n* sk_nlp package\n\n    * <a href='#sk_nlp.nlp_feature_extract package'>sk_nlp.nlp_feature_extract package</a>\n\n      * <a href='#sk_nlp.nlp_feature_extract.feature module'>sk_nlp.nlp_feature_extract.feature module</a>\n\n      * <a href='#sk_nlp.nlp_feature_extract.text_filter module'>sk_nlp.nlp_feature_extract.text_filter module</a>\n\n      * <a href='#sk_nlp.nlp_feature_extract.tokenizer module'>sk_nlp.nlp_feature_extract.tokenizer module</a>\n\n    * <a href='#sk_nlp.nlp_feature_embedding package'>sk_nlp.nlp_feature_embedding package</a>\n\n      * <a href='#sk_nlp.nlp_feature_embedding.bert module'>sk_nlp.nlp_feature_embedding.bert module</a>\n\n      * <a href='#sk_nlp.nlp_feature_embedding.similarity module'>sk_nlp.nlp_feature_embedding.similarity module</a>\n\n      * <a href='#sk_nlp.nlp_feature_embedding.w2v module'>sk_nlp.nlp_feature_embedding.w2v module</a>\n\n<div id=\"sk_nlp.nlp_feature_extract package\">\nsk_nlp.nlp_feature_extract package\n\n\n<div id=\"sk_nlp.nlp_feature_extract.feature module\">\nsk_nlp.nlp_feature_extract.feature module\n\n0 ä½¿ç”¨acè‡ªåŠ¨æœºç»Ÿè®¡ç»™å®šçš„è¯è¯­çš„è¯é¢‘ 1 è·å–tf-idfç‰¹å¾\n\nclass sk_nlp.nlp_feature_extract.feature.CountByAC(pattern_list=[])\n\n   Bases: \"object\"\n\n   åŸºäºacè‡ªåŠ¨æœºæ¥ç»Ÿè®¡æ¨¡å¼ä¸²\n\n   Parameters:\n      **pattern_list** -- åŒ¹é…çš„æ¨¡å¼ä¸²åˆ—è¡¨\n\n   build_tree(pattern_list)\n\n      æ„å»ºæ¨¡å¼ä¸²å‰ç¼€æ ‘\n\n      Parameters:\n         **pattern_list** -- æ¨¡å¼ä¸²åˆ—è¡¨\n\n   count(sentence)\n\n      ç»Ÿè®¡sentenceä¸­å…³äºç»™å®šçš„æ¨¡å¼ä¸²çš„é¢‘ç‡\n\n      Parameters:\n         **sentence** -- å¥å­\n\n      Returns:\n         word_count æ¯ä¸ªå…³é”®è¯å¯¹åº”çš„é¢‘ç‡\n\n      >>> ac = CountByAC(['æ°ä¼¦çš„ä¸ƒ', 'å‘¨æ°ä¼¦çš„', 'ä¸ƒé‡Œé¦™'])\n      >>> result = ac.count('å‘¨æ°ä¼¦çš„ä¸ƒé‡Œé¦™ä¸ƒé‡Œé¦™')\n      >>> print(result)\n      {'å‘¨æ°ä¼¦çš„': 1, 'æ°ä¼¦çš„ä¸ƒ': 1, 'ä¸ƒé‡Œé¦™': 2}\n\nclass sk_nlp.nlp_feature_extract.feature.KeyWordExtract\n\n   Bases: \"object\"\n\n   å…³é”®è¯æŠ½å–ç®—æ³•ï¼ŒåŸºäºtf-idf\n\n   get_tf_idf(sentence_list, model_file)\n\n      åŠ è½½tf-idfæ¨¡å‹ï¼Œè¿”å›sentence_listå¯¹åº”çš„ç‰¹å¾å’Œæ¨¡å‹\n\n      Parameters:\n         * **sentence_list** -- å¥å­åˆ—è¡¨ï¼ˆåˆ†è¯åï¼‰\n\n         * **model_file** -- tf-idfæ¨¡å‹æ–‡ä»¶\n\n      Returns:\n         tf_idf_model(æ¨¡å‹å®ä¾‹), tfidf_feature(sentence_listå¯¹åº”çš„tf-\n         idfç‰¹å¾)\n\n      >>> tf_idf_model, tfidf_feature = kwe.get_tf_idf(['æ°ä¼¦ æ˜¯ å°æ¹¾ æ­Œæ‰‹', 'ä¸ƒé‡Œé¦™ æ˜¯ æ°ä¼¦ åˆ›ä½œ'], file_conf.tf_idf_file_path)\n      >>> print(tfidf_feature)\n        (0, 4)        0.6316672017376245\n        (0, 3)        0.4494364165239821\n        (0, 2)        0.6316672017376245\n        (1, 3)        0.4494364165239821\n        (1, 1)        0.6316672017376245\n        (1, 0)        0.6316672017376245\n\n   get_topk_keywords(data_list, topk=200)\n\n      å¾—åˆ°topkä¸ªå…³é”®è¯\n\n      Parameters:\n         * **data_list** -- å¥å­åˆ—è¡¨ï¼ˆåˆ†è¯åï¼‰\n\n         * **topk** -- tf-idfé‡è¦åº¦æ’åºåå‰topk\n\n      Returns:\n         keywords\n\n      >>> keywords = kwe.get_topk_keywords(['æ°ä¼¦ æ˜¯ å°æ¹¾ æ­Œæ‰‹', 'ä¸ƒé‡Œé¦™ æ˜¯ æ°ä¼¦ åˆ›ä½œ'], topk=1)\n      >>> print(keywords)\n      [['æ­Œæ‰‹']['åˆ›ä½œ']]\n\n   train_tf_idf(sentence_list, model_file, ngram_range=(1, 1))\n\n      è®­ç»ƒtf-idfæ¨¡å‹ï¼Œä¿å­˜æ¨¡å‹ï¼Œè¿”å›æ¨¡å‹å’Œç‰¹å¾\n\n      Parameters:\n         * **sentence_list** -- å¥å­åˆ—è¡¨ï¼ˆåˆ†è¯åï¼‰\n\n         * **model_file** -- tf-idfæ¨¡å‹ä¿å­˜æ–‡ä»¶\n\n      Returns:\n         tf_idf_model, tfidf_feature\n</div>\n<div id=\"sk_nlp.nlp_feature_extract.text_filter module\">\nsk_nlp.nlp_feature_extract.text_filter module\n\n\næ•æ„Ÿè¯æ±‡è¿‡æ»¤æ¨¡å—ï¼Œå…±å®ç°äº†3ä¸ªç±»ï¼šNaiveFilterï¼ŒBSFilterï¼ŒDFAFilter\n\nclass sk_nlp.nlp_feature_extract.text_filter.BSFilter\n\n   Bases: \"object\"\n\n   å®½åº¦ä¼˜å…ˆéå†çš„æ–¹å¼è¿‡æ»¤\n\n   add(keyword)\n\n      æ–°å¢ä¸€ä¸ªæ•æ„Ÿè¯\n\n      :param keyword:æ•æ„Ÿè¯ :return:æ— \n\n   filter(message, repl='*')\n\n      è¿‡æ»¤æ‰æ•æ„Ÿè¯\n\n      Parameters:\n         * **message** -- åŸå§‹çš„è¾“å…¥å¥å­\n\n         * **repl** -- æ•æ„Ÿè¯æ±‡è¢«æ›¿æ¢æˆçš„å­—ç¬¦\n\n      Returns:\n         message å±è”½æ‰æ•æ„Ÿè¯æ±‡çš„å¥å­\n\n      >>> f = BSFilter()\n      >>> question = \"å°æ¹¾æ˜¯ä¸­å›½çš„å—\"\n      >>> filter_question = f.filter(question)\n      >>> print(question, filter_question)\n      å°æ¹¾æ˜¯ä¸­å›½çš„å— *æ˜¯ä¸­å›½çš„å—\n\n   parse(path)\n\n      åŠ è½½æ•æ„Ÿè¯æ±‡è¡¨\n\n      Parameters:\n         **path** -- è·¯å¾„ä¸º/sk-nlp/data/dirty_word.txt\n\n      Returns:\nclass sk_nlp.nlp_feature_extract.text_filter.DFAFilter\n\n   Bases: \"object\"\n\n   DFAå³Deterministic Finite Automatonï¼Œä¹Ÿå°±æ˜¯ç¡®å®šæœ‰ç©·è‡ªåŠ¨æœºã€‚ ç®—æ³•æ ¸\n   å¿ƒæ˜¯å»ºç«‹äº†ä»¥æ•æ„Ÿè¯ä¸ºåŸºç¡€çš„è®¸å¤šæ•æ„Ÿè¯æ ‘\n\n   add(keyword)\n\n      æ–°å¢ä¸€ä¸ªæ•æ„Ÿè¯\n\n      :param keyword:æ•æ„Ÿè¯ :return:æ— \n\n   detect(message)\n\n      åˆ¤æ–­messageæ˜¯å¦åŒ…å«æ•æ„Ÿè¯æ±‡\n\n      :param message:ç”¨æˆ·è¾“å…¥çš„å¥å­ :return: True/False\n\n   filter(message, repl='*')\n\n      è¿‡æ»¤æ‰æ•æ„Ÿè¯\n\n      Parameters:\n         * **message** -- åŸå§‹çš„è¾“å…¥å¥å­\n\n         * **repl** -- æ•æ„Ÿè¯æ±‡è¢«æ›¿æ¢æˆçš„å­—ç¬¦\n\n      Returns:\n         message å±è”½æ‰æ•æ„Ÿè¯æ±‡çš„å¥å­\n\n      >>> f = DFAFilter()\n      >>> question = \"å°æ¹¾æ˜¯ä¸­å›½çš„å—\"\n      >>> filter_question = f.filter(question)\n      >>> print(question, filter_question)\n      å°æ¹¾æ˜¯ä¸­å›½çš„å— *æ˜¯ä¸­å›½çš„å—\n\n   parse(path)\n\n      åŠ è½½æ•æ„Ÿè¯æ±‡è¡¨\n\n      Parameters:\n         **path** -- è·¯å¾„ä¸º/sk-nlp/data/dirty_word.txt\n\n      Returns:\nclass sk_nlp.nlp_feature_extract.text_filter.NaiveFilter\n\n   Bases: \"object\"\n\n   æ™®é€šçš„è¿‡æ»¤æ–¹å¼ï¼šä½¿ç”¨é›†åˆçš„æ–¹å¼è¿‡æ»¤ï¼Œæ—¶é—´å¤æ‚åº¦è·Ÿé›†åˆçš„å¤§å°æœ‰å…³\n\n   filter(message, repl='*')\n\n      è¿‡æ»¤æ‰æ•æ„Ÿè¯\n\n      Parameters:\n         * **message** -- åŸå§‹çš„è¾“å…¥å¥å­\n\n         * **repl** -- æ•æ„Ÿè¯æ±‡è¢«æ›¿æ¢æˆçš„å­—ç¬¦\n\n      Returns:\n         messageï¼šå±è”½æ‰æ•æ„Ÿè¯æ±‡çš„å¥å­\n\n      >>> f = NaiveFilter()\n      >>> question = \"å°æ¹¾æ˜¯ä¸­å›½çš„å—\"\n      >>> filter_question = f.filter(question)\n      >>> print(question, filter_question)\n      å°æ¹¾æ˜¯ä¸­å›½çš„å— *æ˜¯ä¸­å›½çš„å—\n\n   parse(path)\n\n      åŠ è½½æ•æ„Ÿè¯æ±‡è¡¨\n\n      Parameters:\n         **path** -- è·¯å¾„ä¸º/sk-nlp/data/dirty_word.txt\n\n      Returns:\n</div>\n\n<div id=\"sk_nlp.nlp_feature_extract.tokenizer module\">\nsk_nlp.nlp_feature_extract.tokenizer module\n===========================================\n\nè¯è¯­ç²’åº¦çš„æ“ä½œæ¨¡å—ï¼šåˆ†è¯ï¼Œå»åœç”¨è¯ï¼ŒåŒä¹‰è¯æ—è½¬æ¢\n\nclass sk_nlp.nlp_feature_extract.tokenizer.SentenceCut(is_lower=True, stopword_list=[], use_chinese_synonyms=False)\n\n   Bases: \"object\"\n\n   å¥å­åˆ†è¯æ“ä½œç±» ç›®å‰é›†æˆäº†jiebaåˆ†è¯\n\n   cut_word(sentence_list)\n\n      å¯¹ä¼ è¿›æ¥çš„å¥å­è¿›è¡Œåˆ†è¯\n\n      :param sentence_list:['æˆ‘çˆ±ä¸­å›½', 'æˆ‘æ˜¯ä¸­å›½äºº']\n      :return:seg_lists [['æˆ‘', 'çˆ±', 'ä¸­å›½'], ['æˆ‘', 'æ˜¯', 'ä¸­å›½', '\n      äºº']]  token_count {'æˆ‘': 2, 'çˆ±': 1, 'ä¸­å›½': 2, 'æ˜¯': 1, 'äºº':\n      1}\n\n      >>> sen_cut = SentenceCut(use_chinese_synonyms=True)\n      >>> seg_lists, token_count = sen_cut.cut_word(['æˆ‘çˆ±baidu', 'æˆ‘æ˜¯ä¸­å›½äºº'])\n      >>> print(seg_lists, token_count)\n      [['æˆ‘', 'çˆ±', 'ç™¾åº¦'], ['æˆ‘', 'æ˜¯', 'ä¸­å›½', 'äºº']]\n      {'æˆ‘': 2, 'çˆ±': 1, 'ç™¾åº¦': 1, 'æ˜¯': 1, 'ä¸­å›½': 1, 'äºº': 1}\n\n   load_chinese_synonyms()\n\n      åŠ è½½åŒä¹‰è¯æ—\n\n      Returns:\n         union_find ï¼ˆå¹¶æŸ¥é›†å®ä¾‹ï¼‰ï¼Œword_listï¼ˆåŒä¹‰è¯æ—æ‰€æœ‰çš„å•è¯é›†åˆ\n         ï¼‰\n\nclass sk_nlp.nlp_feature_extract.tokenizer.StopWord(source='', define_stop_word=[])\n\n   Bases: \"object\"\n\n   åœç”¨è¯æ“ä½œç±»ï¼š åœç”¨è¯æ±‡è¡¨è·¯å¾„å­˜æ”¾åœ¨ sk-nlp/data/stopword\n\n   load_stop_word()\n\n      æ ¹æ®ä¸åŒçš„self.sourceåŠ è½½ä¸åŒçš„åœç”¨è¯è¡¨\n\n      Returns:\n         stop_word_list åœç”¨è¯åˆ—è¡¨\n\n   merge_stop_word(define_stop_word)\n\n      å°†ç”¨æˆ·è‡ªå®šä¹‰çš„åœç”¨è¯å’Œç”¨æˆ·æŒ‡å®šçš„é€šç”¨è¯åº“åˆå¹¶æˆä¸€ä¸ªlist\n\n      Parameters:\n         **define_stop_word** -- ç”¨æˆ·ç»™çš„è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨ list\n\n      Returns:\n         stop_word_list åœç”¨è¯åˆ—è¡¨\n</div>\n</div>\n<div id=\"sk_nlp.nlp_feature_embedding package\">\nsk_nlp.nlp_feature_embedding package\n\n\n\n<div id=\"sk_nlp.nlp_feature_embedding.bert module\">\nsk_nlp.nlp_feature_embedding.bert module\n========================================\n\nbertåŸºæœ¬æ¨¡å‹åŠ è½½\n\nclass sk_nlp.nlp_feature_embedding.bert.MaskLayer(output_dim=768, **kwargs)\n\n   Bases: \"keras.engine.base_layer.Layer\"\n\n   mask å±‚ï¼Œå±è”½æ‰seg_idä¸º0çš„è¯è¯­\n\n   build(input_shape)\n\n      åˆ›å»ºå±‚çš„æƒé‡\n\n      :param input_shape:Keras tensor (future input to layer) or\n      list/tuple of Keras tensors :return:\n\n   call(x)\n\n      This is where the layer's logic lives.\n\n      # Arguments\n         inputs: Input tensor, or list/tuple of input tensors.\n         >>**<<kwargs: Additional keyword arguments.\n\n      # Returns\n         A tensor or list/tuple of tensors.\n\n   compute_output_shape(input_shape)\n\n      Computes the output shape of the layer.\n\n      Assumes that the layer will be built to match that input shape\n      provided.\n\n      # Arguments\n         input_shape: Shape tuple (tuple of integers)\n            or list of shape tuples (one per output tensor of the\n            layer). Shape tuples can include None for free dimensions,\n            instead of an integer.\n\n      # Returns\n         An output shape tuple.\n\nclass sk_nlp.nlp_feature_embedding.bert.ReverseMaskLayer(**kwargs)\n\n   Bases: \"keras.engine.base_layer.Layer\"\n\n   åè½¬ mask å±‚ï¼Œå±è”½æ‰seg_idä¸º1çš„è¯è¯­\n\n   call(x)\n\n      This is where the layer's logic lives.\n\n      # Arguments\n         inputs: Input tensor, or list/tuple of input tensors.\n         >>**<<kwargs: Additional keyword arguments.\n\n      # Returns\n         A tensor or list/tuple of tensors.\n\n   compute_output_shape(input_shape)\n\n      Computes the output shape of the layer.\n\n      Assumes that the layer will be built to match that input shape\n      provided.\n\n      # Arguments\n         input_shape: Shape tuple (tuple of integers)\n            or list of shape tuples (one per output tensor of the\n            layer). Shape tuples can include None for free dimensions,\n            instead of an integer.\n\n      # Returns\n         An output shape tuple.\n\nclass sk_nlp.nlp_feature_embedding.bert.SepLayer(**kwargs)\n\n   Bases: \"keras.engine.base_layer.Layer\"\n\n   sep mask å±‚ï¼Œå±è”½æ‰sepä½ç½®çš„è¾“å‡º\n\n   call(x)\n\n      This is where the layer's logic lives.\n\n      # Arguments\n         inputs: Input tensor, or list/tuple of input tensors.\n         >>**<<kwargs: Additional keyword arguments.\n\n      # Returns\n         A tensor or list/tuple of tensors.\n\n   compute_output_shape(input_shape)\n\n      Computes the output shape of the layer.\n\n      Assumes that the layer will be built to match that input shape\n      provided.\n\n      # Arguments\n         input_shape: Shape tuple (tuple of integers)\n            or list of shape tuples (one per output tensor of the\n            layer). Shape tuples can include None for free dimensions,\n            instead of an integer.\n\n      # Returns\n         An output shape tuple.\n\nsk_nlp.nlp_feature_embedding.bert.build_model_feature(origin_model, use_cls=False)\n\n   æ­å»ºæ–°çš„å¥å­æ¨¡å‹\n\n   Parameters:\n      * **origin_model** -- åŸå§‹æ¨¡å‹ï¼Œä¸€èˆ¬ä¸ºbert\n\n      * **use_cls** -- æ˜¯å¦ä½¿ç”¨clsä½ç½®çš„è¾“å‡º\n\n   Returns:\n      modelï¼šæ–°æ¨¡å‹\n\nsk_nlp.nlp_feature_embedding.bert.encoder(model, data_list, dict_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/vocab.txt')\n\n   ä½¿ç”¨å¥å‘é‡æ¨¡å‹ï¼Œå°†å¥å­è½¬ç æˆå¥å‘é‡\n\n   Parameters:\n      * **model** -- æ¨¡å‹\n\n      * **data_list** -- å¥å­åˆ—è¡¨ï¼ˆæ²¡æœ‰åˆ†è¯ï¼‰\n\n      * **dict_path** -- bertæ¨¡å‹è¯æ±‡è¡¨\n\n   Returns:\n      data_listä¸­çš„æ¯ä¸ªå¥å­å¯¹åº”çš„å¥å‘é‡åˆ—è¡¨\n\n   >>> origin_model = load_bert_model()\n   >>> new_model = build_model_feature(origin_model)\n   >>> question_list = [\"æˆ‘çˆ±è¿™ä¸ªä¼Ÿå¤§çš„ä¸–ç•Œ\", \"æ¬£èµä¸–ç•Œçš„é£æ™¯\"]\n   >>> sen_vector_lists = encoder(new_model, question_list)\n   >>> print(sen_vector_lists.shape)\n\nsk_nlp.nlp_feature_embedding.bert.load_bert_model(with_mlm=True, with_pool=False, return_keras_model=True, config_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/bert_config.json', checkpoint_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/bert_model.ckpt')\n\n   åŠ è½½bert æ¨¡å‹\n\n   Parameters:\n      * **with_mlm** -- æ˜¯å¦æ­£åˆ™åŒ–\n\n      * **with_pool** -- æ˜¯å¦æ± åŒ–\n\n      * **return_keras_model** -- è¿”å›çš„æ˜¯keras model è¿˜æ˜¯ tensorflow\n        æ¨¡å‹\n\n      * **config_path** -- bert æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„\n\n      * **checkpoint_path** -- bert æ¨¡å‹è·¯å¾„\n\n   Returns:\nsk_nlp.nlp_feature_embedding.bert.masked_crossentropy(y_true, y_pred)\n\n   maskæ‰éé¢„æµ‹éƒ¨åˆ†ï¼Œè®¡ç®—äº¤å‰ç†µ\n\n   Parameters:\n      * **y_true** -- çœŸå®çš„Yæ ‡ç­¾\n\n      * **y_pred** -- é¢„æµ‹çš„Yæ ‡ç­¾\n\n   Returns:\n      æŸå¤±å€¼\n</div>\n\n<div id=\"sk_nlp.nlp_feature_embedding.similarity module\">\nsk_nlp.nlp_feature_embedding.similarity module\n==============================================\n\nè®¡ç®—å„ç§è·ç¦»\n\nsk_nlp.nlp_feature_embedding.similarity.get_distance_sim_matrix(matrix1, matrix2, metric='cosine')\n\n   è¿”å›2ä¸ªçŸ©é˜µçš„å„ç§è·ç¦»å’Œç›¸ä¼¼åº¦\n\n   Parameters:\n      * **matrix1** -- å¥å­å‘é‡1\n\n      * **matrix2** -- å¥å­å‘é‡2\n\n      * **metric** -- 'braycurtis', 'canberra', 'chebyshev',\n        'cityblock', 'correlation',\n\n   'cosine', 'dice', 'euclidean', 'hamming', 'jaccard',\n   'jensenshannon', 'kulsinski', 'mahalanobis', 'matching',\n   'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n   'sokalmichener', 'sokalsneath', 'sqeuclidean', 'wminkowski', 'yule'\n   :return:\n\nsk_nlp.nlp_feature_embedding.similarity.get_edit_distance(query_sen_list, candidate_sen_list)\n\n   è®¡ç®—ç¼–è¾‘è·ç¦»\n\n   Parameters:\n      * **query_sen_list** -- å¦‚['æˆ‘çˆ±ä¸­å›½', 'ç¾å›½æ€»ç»Ÿç‰¹æœ—æ™®']\n\n      * **candidate_sen_list** -- å¦‚['æˆ‘çˆ±åœ°çƒ', 'ç¾å›½æ€»ç»Ÿæ‹œç™»']\n\n   Returns:\nsk_nlp.nlp_feature_embedding.similarity.get_edit_similarity(distance_matrix, norm=True)\n\n   å…ˆåè½¬ç¼–è¾‘è·ç¦»çŸ©é˜µï¼Œå¾—åˆ°ç¼–è¾‘ç›¸ä¼¼åº¦çŸ©é˜µï¼Œç„¶åå¯ä»¥é€‰æ‹©å½’ä¸€åŒ–\n\n   Parameters:\n      * **distance_matrix** -- è·ç¦»çŸ©é˜µ\n\n      * **norm** -- True/False\n\n   Returns:\nsk_nlp.nlp_feature_embedding.similarity.get_jaccard_sim(sen_list1, sen_list2, norm=False)\n\n   è·å¾—æ°å¡å¾·ç›¸ä¼¼åº¦\n\n   Parameters:\n      * **sen_list1** -- [['æˆ‘', 'çˆ±','ä¸­å›½'], ['ç¾å›½', 'æ€»ç»Ÿ', 'ç‰¹æœ—\n        æ™®']]\n\n      * **sen_list2** -- [['æˆ‘', 'çˆ±','åœ°çƒ'], ['ç¾å›½', 'æ€»ç»Ÿ', 'æ‹œç™»\n        ']]\n\n   :param norm:æ˜¯å¦å¯¹ç»“æœè¿›è¡Œå½’ä¸€åŒ– :return:\n\nsk_nlp.nlp_feature_embedding.similarity.match_topk(sim_matrix, topk=1, order=0)\n\n   è¿”å›ç›¸ä¼¼åº¦çŸ©é˜µå‰topk/æˆ–è€…åtopk\n\n   Parameters:\n      * **sim_matrix** --\n\n      * **topk** --\n\n      * **order** --\n\n   Returns:\nsk_nlp.nlp_feature_embedding.similarity.normalization(matrix, reversed=True)\n\n   å½’ä¸€åŒ–çŸ©é˜µï¼ŒæŒ‰ç…§æœ€åä¸€ä¸ªç»´åº¦\n\n   Parameters:\n      * **matrix** --\n\n      * **reversed** --\n\n   Returns:\n</div>\n\n<div id=\"sk_nlp.nlp_feature_embedding.w2v module\">\nsk_nlp.nlp_feature_embedding.w2v module\n=======================================\n\nä¼ ç»Ÿçš„w2væ¨¡å‹:åŒ…å«skip-gramå’Œcbow ç›®å‰æœ‰ä¸€ä¸ªä»wikiè¯­æ–™è®­ç»ƒå‡ºæ¥çš„100ç»´\nåº¦çš„skip-gramæ¨¡å‹\n\nclass sk_nlp.nlp_feature_embedding.w2v.WordEmbedding(model_file_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/w2v/skip_gram_wiki2Vec.h5', embedding_dim=100)\n\n   Bases: \"object\"\n\n   fine_tune(new_seg_list, model_file_path)\n\n      åŸºäºå·²æœ‰çš„w2væ¨¡å‹ï¼Œä½¿ç”¨å…¶ä»–è¯­æ–™è¿›è¡Œå¾®è°ƒã€‚ç„¶åä¿å­˜æ¨¡å‹è·¯å¾„ã€‚\n\n      Parameters:\n         * **new_seg_list** -- æ–°å¥å­ï¼ˆåˆ†è¯åï¼‰\n\n         * **model_file_path** -- æ¨¡å‹çš„ä¿å­˜è·¯å¾„\n\n      Returns:\n      >>> model = WordEmbedding()\n      >>> model.get_embedding()\n      >>> new_seg_list = [['æˆ‘', 'çˆ±','ä¸­å›½'], ['ç¾å›½', 'æ€»ç»Ÿ', 'ç‰¹æœ—æ™®']]\n      >>> model.fine_tune(new_seg_list, file_conf.ft_wiki_sg_file_path)\n\n   get_embedding()\n\n      è·å–è¯å‘é‡æ¨¡å‹çš„ä¿¡æ¯\n\n      Returns:\n         embedding_matrix:è¯å‘é‡çŸ©é˜µï¼›index_wordï¼šç´¢å¼•åˆ°å•è¯çš„æ˜ å°„ï¼›\n         word_indexï¼šå•è¯åˆ°ç´¢å¼•çš„æ˜ å°„\n\n   op2model()\n\n      ç”±äºw2vçš„æ¥å£å¤ªå¤šï¼Œä¸å¤ªå¥½å°è£… è¿™é‡Œç»™å‡ºäº†æ¨¡å‹çš„ä¸€äº›å¸¸ç”¨æ“ä½œèŒƒä¾‹\n\n      Returns:\n   train_vec(sentence_list, model_file_path, window=5, min_count=5, sg=0)\n\n      ä½¿ç”¨w2vè®­ç»ƒè¯å‘é‡\n\n      Parameters:\n         * **sentence_list** -- å¥å­åˆ—è¡¨ï¼Œ[['æˆ‘', 'çˆ±','ä¸­å›½'], ['ç¾å›½\n           ', 'æ€»ç»Ÿ', 'ç‰¹æœ—æ™®']]\n\n         * **model_file_path** -- æ¨¡å‹ä¿å­˜è·¯å¾„\n\n         * **window** -- æ»‘åŠ¨çª—å£\n\n         * **min_count** -- æœ€å°è¯é¢‘\n\n         * **sg** -- 0æ˜¯ä½¿ç”¨cbow, 1æ˜¯ä½¿ç”¨è·³å­—æ¨¡å‹\n\n      Returns:\n</div>\n</div>\nModule contents\n===============\n\n\nModule contents\n===============\n\n\nMore Resources\n--------------\n\n-   [where is bert pre-train model]  https://github.com/google-research/bert\n-   [where is stopwords corpus]  https://github.com/goto456/stopwords\n-   [Official Python Packaging User Guide](https://packaging.python.org)\n-   [The Hitchhiker's Guide to Packaging]\n\nLicense\n-------\n\nThis is free and unencumbered software released into the public domain.\nAnyone is free to copy, modify, publish, use, compile, sell, or\ndistribute this software, either in source code form or as a compiled\nbinary, for any purpose, commercial or non-commercial, and by any means.\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/me/myproject",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "sk-nlp",
    "package_url": "https://pypi.org/project/sk-nlp/",
    "platform": "",
    "project_url": "https://pypi.org/project/sk-nlp/",
    "project_urls": {
      "Homepage": "https://github.com/me/myproject"
    },
    "release_url": "https://pypi.org/project/sk-nlp/0.1.9/",
    "requires_dist": [
      "numpy",
      "scipy",
      "tensorflow-gpu",
      "bert4keras",
      "sk-common"
    ],
    "requires_python": ">=3.6.0",
    "summary": "nlp kit.",
    "version": "0.1.9",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10896260,
  "releases": {
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9bd3ee9229898486be62ccc556fe630645e3e63586b6137755c8cccd3d1a469f",
          "md5": "cba9830d6eda3561171e4f0bc4c05dc4",
          "sha256": "bca1586aa3cb999d2e57b9e4d4243526098e0f5a2d4eb516b773277763419f60"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cba9830d6eda3561171e4f0bc4c05dc4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 318226,
        "upload_time": "2021-06-29T03:16:24",
        "upload_time_iso_8601": "2021-06-29T03:16:24.607435Z",
        "url": "https://files.pythonhosted.org/packages/9b/d3/ee9229898486be62ccc556fe630645e3e63586b6137755c8cccd3d1a469f/sk_nlp-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "34d54f195b088bccede9995f3584760080185a5babffa98fe875ecea7dd63743",
          "md5": "eea466bb0be7e0721d52fcc89cd1a727",
          "sha256": "b38309d51a1cb6a300fdc393e429211dc6d0586cdf1733f4e567629528c17a0e"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eea466bb0be7e0721d52fcc89cd1a727",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 318476,
        "upload_time": "2021-07-13T09:44:32",
        "upload_time_iso_8601": "2021-07-13T09:44:32.245881Z",
        "url": "https://files.pythonhosted.org/packages/34/d5/4f195b088bccede9995f3584760080185a5babffa98fe875ecea7dd63743/sk_nlp-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "79a1dd43a021fc73f556785116304ef97d190592c7dc8f25185f3fd9fe7802a2",
          "md5": "a7ee50f8d686328a73d37a99e394b171",
          "sha256": "33d47e74dfbd2cf21537316ffecd57d164862d025a0e8fbb1bbc3c93904259d9"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a7ee50f8d686328a73d37a99e394b171",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 318478,
        "upload_time": "2021-07-13T10:24:14",
        "upload_time_iso_8601": "2021-07-13T10:24:14.399278Z",
        "url": "https://files.pythonhosted.org/packages/79/a1/dd43a021fc73f556785116304ef97d190592c7dc8f25185f3fd9fe7802a2/sk_nlp-0.1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb073aeff9e4e42b76c89a080a9a88e65ad91b9918412ff4241907a106c59e70",
          "md5": "15119c203c8a16035ca516c1e70e4d72",
          "sha256": "49b21a5bb38e02441e74664ac20e341e8f4fe44014cae6b4db45afe8c58c543c"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "15119c203c8a16035ca516c1e70e4d72",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 318476,
        "upload_time": "2021-07-13T10:28:03",
        "upload_time_iso_8601": "2021-07-13T10:28:03.985386Z",
        "url": "https://files.pythonhosted.org/packages/bb/07/3aeff9e4e42b76c89a080a9a88e65ad91b9918412ff4241907a106c59e70/sk_nlp-0.1.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d033bbf5c557cc0ad06735ff67b4c1b069f432ef00443621c491ca780538a4cd",
          "md5": "c66139ef8f5868e20652967c523591ae",
          "sha256": "68188fafdb18fafe0dbed219b894d201c1b785397339eafedd78bff3e6c86a7c"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c66139ef8f5868e20652967c523591ae",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 317640,
        "upload_time": "2021-07-13T10:31:15",
        "upload_time_iso_8601": "2021-07-13T10:31:15.215336Z",
        "url": "https://files.pythonhosted.org/packages/d0/33/bbf5c557cc0ad06735ff67b4c1b069f432ef00443621c491ca780538a4cd/sk_nlp-0.1.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8046ae1192c0599b76ea5aff729b9a6ed2bc717072aa021fe9481ba9261764ea",
          "md5": "79ccfc2fcb5025d61a13fd43947d1761",
          "sha256": "49af0f092876a964ae3b3092a1c9ee7a857e8f42876d6da31f04fe1d35d4b523"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "79ccfc2fcb5025d61a13fd43947d1761",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 317721,
        "upload_time": "2021-07-13T11:41:52",
        "upload_time_iso_8601": "2021-07-13T11:41:52.623508Z",
        "url": "https://files.pythonhosted.org/packages/80/46/ae1192c0599b76ea5aff729b9a6ed2bc717072aa021fe9481ba9261764ea/sk_nlp-0.1.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8046ae1192c0599b76ea5aff729b9a6ed2bc717072aa021fe9481ba9261764ea",
        "md5": "79ccfc2fcb5025d61a13fd43947d1761",
        "sha256": "49af0f092876a964ae3b3092a1c9ee7a857e8f42876d6da31f04fe1d35d4b523"
      },
      "downloads": -1,
      "filename": "sk_nlp-0.1.9-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "79ccfc2fcb5025d61a13fd43947d1761",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 317721,
      "upload_time": "2021-07-13T11:41:52",
      "upload_time_iso_8601": "2021-07-13T11:41:52.623508Z",
      "url": "https://files.pythonhosted.org/packages/80/46/ae1192c0599b76ea5aff729b9a6ed2bc717072aa021fe9481ba9261764ea/sk_nlp-0.1.9-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}