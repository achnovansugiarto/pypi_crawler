{
  "info": {
    "author": "wengsongxiu",
    "author_email": "wengsongxiu@mastercom.cn",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "\n# sk-nlp\n\n[![Travis](https://travis-ci.org/CyberZHG/keras-transformer.svg)](https://travis-ci.org/CyberZHG/keras-transformer)\n[![Coverage](https://coveralls.io/repos/github/CyberZHG/keras-transformer/badge.svg?branch=master)](https://coveralls.io/github/CyberZHG/keras-transformer)\n\n![](https://img.shields.io/badge/keras-tensorflow-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras/eager-blue.svg)\n![](https://img.shields.io/badge/keras-tf.keras/2.0_beta-blue.svg)\n\n\n\n📦 项目介绍 (for humans)\n=======================\n\n这个第三方仓库是由深圳市名通科技股份有限公司AI团队提供的。团队致力于为NLP领域，提供一个稳定可靠， 功能完善的NLP常见操作。\n\n\nInstallation\n-----\n\n```bash\ncd your_project\npip install sk-nlp\n```\n\n# Content\n* sk_nlp package\n\n    * <a href='#sk_nlp.nlp_feature_extract package'>sk_nlp.nlp_feature_extract package</a>\n\n      * <a href='#sk_nlp.nlp_feature_extract.feature module'>sk_nlp.nlp_feature_extract.feature module</a>\n\n      * <a href='#sk_nlp.nlp_feature_extract.text_filter module'>sk_nlp.nlp_feature_extract.text_filter module</a>\n\n      * <a href='#sk_nlp.nlp_feature_extract.tokenizer module'>sk_nlp.nlp_feature_extract.tokenizer module</a>\n\n    * <a href='#sk_nlp.nlp_feature_embedding package'>sk_nlp.nlp_feature_embedding package</a>\n\n      * <a href='#sk_nlp.nlp_feature_embedding.bert module'>sk_nlp.nlp_feature_embedding.bert module</a>\n\n      * <a href='#sk_nlp.nlp_feature_embedding.similarity module'>sk_nlp.nlp_feature_embedding.similarity module</a>\n\n      * <a href='#sk_nlp.nlp_feature_embedding.w2v module'>sk_nlp.nlp_feature_embedding.w2v module</a>\n\n<div id=\"sk_nlp.nlp_feature_extract package\">\nsk_nlp.nlp_feature_extract package\n\n\n<div id=\"sk_nlp.nlp_feature_extract.feature module\">\nsk_nlp.nlp_feature_extract.feature module\n\n0 使用ac自动机统计给定的词语的词频 1 获取tf-idf特征\n\nclass sk_nlp.nlp_feature_extract.feature.CountByAC(pattern_list=[])\n\n   Bases: \"object\"\n\n   基于ac自动机来统计模式串\n\n   Parameters:\n      **pattern_list** -- 匹配的模式串列表\n\n   build_tree(pattern_list)\n\n      构建模式串前缀树\n\n      Parameters:\n         **pattern_list** -- 模式串列表\n\n   count(sentence)\n\n      统计sentence中关于给定的模式串的频率\n\n      Parameters:\n         **sentence** -- 句子\n\n      Returns:\n         word_count 每个关键词对应的频率\n\n      >>> ac = CountByAC(['杰伦的七', '周杰伦的', '七里香'])\n      >>> result = ac.count('周杰伦的七里香七里香')\n      >>> print(result)\n      {'周杰伦的': 1, '杰伦的七': 1, '七里香': 2}\n\nclass sk_nlp.nlp_feature_extract.feature.KeyWordExtract\n\n   Bases: \"object\"\n\n   关键词抽取算法，基于tf-idf\n\n   get_tf_idf(sentence_list, model_file)\n\n      加载tf-idf模型，返回sentence_list对应的特征和模型\n\n      Parameters:\n         * **sentence_list** -- 句子列表（分词后）\n\n         * **model_file** -- tf-idf模型文件\n\n      Returns:\n         tf_idf_model(模型实例), tfidf_feature(sentence_list对应的tf-\n         idf特征)\n\n      >>> tf_idf_model, tfidf_feature = kwe.get_tf_idf(['杰伦 是 台湾 歌手', '七里香 是 杰伦 创作'], file_conf.tf_idf_file_path)\n      >>> print(tfidf_feature)\n        (0, 4)        0.6316672017376245\n        (0, 3)        0.4494364165239821\n        (0, 2)        0.6316672017376245\n        (1, 3)        0.4494364165239821\n        (1, 1)        0.6316672017376245\n        (1, 0)        0.6316672017376245\n\n   get_topk_keywords(data_list, topk=200)\n\n      得到topk个关键词\n\n      Parameters:\n         * **data_list** -- 句子列表（分词后）\n\n         * **topk** -- tf-idf重要度排序后前topk\n\n      Returns:\n         keywords\n\n      >>> keywords = kwe.get_topk_keywords(['杰伦 是 台湾 歌手', '七里香 是 杰伦 创作'], topk=1)\n      >>> print(keywords)\n      [['歌手']['创作']]\n\n   train_tf_idf(sentence_list, model_file, ngram_range=(1, 1))\n\n      训练tf-idf模型，保存模型，返回模型和特征\n\n      Parameters:\n         * **sentence_list** -- 句子列表（分词后）\n\n         * **model_file** -- tf-idf模型保存文件\n\n      Returns:\n         tf_idf_model, tfidf_feature\n</div>\n<div id=\"sk_nlp.nlp_feature_extract.text_filter module\">\nsk_nlp.nlp_feature_extract.text_filter module\n\n\n敏感词汇过滤模块，共实现了3个类：NaiveFilter，BSFilter，DFAFilter\n\nclass sk_nlp.nlp_feature_extract.text_filter.BSFilter\n\n   Bases: \"object\"\n\n   宽度优先遍历的方式过滤\n\n   add(keyword)\n\n      新增一个敏感词\n\n      :param keyword:敏感词 :return:无\n\n   filter(message, repl='*')\n\n      过滤掉敏感词\n\n      Parameters:\n         * **message** -- 原始的输入句子\n\n         * **repl** -- 敏感词汇被替换成的字符\n\n      Returns:\n         message 屏蔽掉敏感词汇的句子\n\n      >>> f = BSFilter()\n      >>> question = \"台湾是中国的吗\"\n      >>> filter_question = f.filter(question)\n      >>> print(question, filter_question)\n      台湾是中国的吗 *是中国的吗\n\n   parse(path)\n\n      加载敏感词汇表\n\n      Parameters:\n         **path** -- 路径为/sk-nlp/data/dirty_word.txt\n\n      Returns:\nclass sk_nlp.nlp_feature_extract.text_filter.DFAFilter\n\n   Bases: \"object\"\n\n   DFA即Deterministic Finite Automaton，也就是确定有穷自动机。 算法核\n   心是建立了以敏感词为基础的许多敏感词树\n\n   add(keyword)\n\n      新增一个敏感词\n\n      :param keyword:敏感词 :return:无\n\n   detect(message)\n\n      判断message是否包含敏感词汇\n\n      :param message:用户输入的句子 :return: True/False\n\n   filter(message, repl='*')\n\n      过滤掉敏感词\n\n      Parameters:\n         * **message** -- 原始的输入句子\n\n         * **repl** -- 敏感词汇被替换成的字符\n\n      Returns:\n         message 屏蔽掉敏感词汇的句子\n\n      >>> f = DFAFilter()\n      >>> question = \"台湾是中国的吗\"\n      >>> filter_question = f.filter(question)\n      >>> print(question, filter_question)\n      台湾是中国的吗 *是中国的吗\n\n   parse(path)\n\n      加载敏感词汇表\n\n      Parameters:\n         **path** -- 路径为/sk-nlp/data/dirty_word.txt\n\n      Returns:\nclass sk_nlp.nlp_feature_extract.text_filter.NaiveFilter\n\n   Bases: \"object\"\n\n   普通的过滤方式：使用集合的方式过滤，时间复杂度跟集合的大小有关\n\n   filter(message, repl='*')\n\n      过滤掉敏感词\n\n      Parameters:\n         * **message** -- 原始的输入句子\n\n         * **repl** -- 敏感词汇被替换成的字符\n\n      Returns:\n         message：屏蔽掉敏感词汇的句子\n\n      >>> f = NaiveFilter()\n      >>> question = \"台湾是中国的吗\"\n      >>> filter_question = f.filter(question)\n      >>> print(question, filter_question)\n      台湾是中国的吗 *是中国的吗\n\n   parse(path)\n\n      加载敏感词汇表\n\n      Parameters:\n         **path** -- 路径为/sk-nlp/data/dirty_word.txt\n\n      Returns:\n</div>\n\n<div id=\"sk_nlp.nlp_feature_extract.tokenizer module\">\nsk_nlp.nlp_feature_extract.tokenizer module\n===========================================\n\n词语粒度的操作模块：分词，去停用词，同义词林转换\n\nclass sk_nlp.nlp_feature_extract.tokenizer.SentenceCut(is_lower=True, stopword_list=[], use_chinese_synonyms=False)\n\n   Bases: \"object\"\n\n   句子分词操作类 目前集成了jieba分词\n\n   cut_word(sentence_list)\n\n      对传进来的句子进行分词\n\n      :param sentence_list:['我爱中国', '我是中国人']\n      :return:seg_lists [['我', '爱', '中国'], ['我', '是', '中国', '\n      人']]  token_count {'我': 2, '爱': 1, '中国': 2, '是': 1, '人':\n      1}\n\n      >>> sen_cut = SentenceCut(use_chinese_synonyms=True)\n      >>> seg_lists, token_count = sen_cut.cut_word(['我爱baidu', '我是中国人'])\n      >>> print(seg_lists, token_count)\n      [['我', '爱', '百度'], ['我', '是', '中国', '人']]\n      {'我': 2, '爱': 1, '百度': 1, '是': 1, '中国': 1, '人': 1}\n\n   load_chinese_synonyms()\n\n      加载同义词林\n\n      Returns:\n         union_find （并查集实例），word_list（同义词林所有的单词集合\n         ）\n\nclass sk_nlp.nlp_feature_extract.tokenizer.StopWord(source='', define_stop_word=[])\n\n   Bases: \"object\"\n\n   停用词操作类： 停用词汇表路径存放在 sk-nlp/data/stopword\n\n   load_stop_word()\n\n      根据不同的self.source加载不同的停用词表\n\n      Returns:\n         stop_word_list 停用词列表\n\n   merge_stop_word(define_stop_word)\n\n      将用户自定义的停用词和用户指定的通用词库合并成一个list\n\n      Parameters:\n         **define_stop_word** -- 用户给的自定义停用词列表 list\n\n      Returns:\n         stop_word_list 停用词列表\n</div>\n</div>\n<div id=\"sk_nlp.nlp_feature_embedding package\">\nsk_nlp.nlp_feature_embedding package\n\n\n\n<div id=\"sk_nlp.nlp_feature_embedding.bert module\">\nsk_nlp.nlp_feature_embedding.bert module\n========================================\n\nbert基本模型加载\n\nclass sk_nlp.nlp_feature_embedding.bert.MaskLayer(output_dim=768, **kwargs)\n\n   Bases: \"keras.engine.base_layer.Layer\"\n\n   mask 层，屏蔽掉seg_id为0的词语\n\n   build(input_shape)\n\n      创建层的权重\n\n      :param input_shape:Keras tensor (future input to layer) or\n      list/tuple of Keras tensors :return:\n\n   call(x)\n\n      This is where the layer's logic lives.\n\n      # Arguments\n         inputs: Input tensor, or list/tuple of input tensors.\n         >>**<<kwargs: Additional keyword arguments.\n\n      # Returns\n         A tensor or list/tuple of tensors.\n\n   compute_output_shape(input_shape)\n\n      Computes the output shape of the layer.\n\n      Assumes that the layer will be built to match that input shape\n      provided.\n\n      # Arguments\n         input_shape: Shape tuple (tuple of integers)\n            or list of shape tuples (one per output tensor of the\n            layer). Shape tuples can include None for free dimensions,\n            instead of an integer.\n\n      # Returns\n         An output shape tuple.\n\nclass sk_nlp.nlp_feature_embedding.bert.ReverseMaskLayer(**kwargs)\n\n   Bases: \"keras.engine.base_layer.Layer\"\n\n   反转 mask 层，屏蔽掉seg_id为1的词语\n\n   call(x)\n\n      This is where the layer's logic lives.\n\n      # Arguments\n         inputs: Input tensor, or list/tuple of input tensors.\n         >>**<<kwargs: Additional keyword arguments.\n\n      # Returns\n         A tensor or list/tuple of tensors.\n\n   compute_output_shape(input_shape)\n\n      Computes the output shape of the layer.\n\n      Assumes that the layer will be built to match that input shape\n      provided.\n\n      # Arguments\n         input_shape: Shape tuple (tuple of integers)\n            or list of shape tuples (one per output tensor of the\n            layer). Shape tuples can include None for free dimensions,\n            instead of an integer.\n\n      # Returns\n         An output shape tuple.\n\nclass sk_nlp.nlp_feature_embedding.bert.SepLayer(**kwargs)\n\n   Bases: \"keras.engine.base_layer.Layer\"\n\n   sep mask 层，屏蔽掉sep位置的输出\n\n   call(x)\n\n      This is where the layer's logic lives.\n\n      # Arguments\n         inputs: Input tensor, or list/tuple of input tensors.\n         >>**<<kwargs: Additional keyword arguments.\n\n      # Returns\n         A tensor or list/tuple of tensors.\n\n   compute_output_shape(input_shape)\n\n      Computes the output shape of the layer.\n\n      Assumes that the layer will be built to match that input shape\n      provided.\n\n      # Arguments\n         input_shape: Shape tuple (tuple of integers)\n            or list of shape tuples (one per output tensor of the\n            layer). Shape tuples can include None for free dimensions,\n            instead of an integer.\n\n      # Returns\n         An output shape tuple.\n\nsk_nlp.nlp_feature_embedding.bert.build_model_feature(origin_model, use_cls=False)\n\n   搭建新的句子模型\n\n   Parameters:\n      * **origin_model** -- 原始模型，一般为bert\n\n      * **use_cls** -- 是否使用cls位置的输出\n\n   Returns:\n      model：新模型\n\nsk_nlp.nlp_feature_embedding.bert.encoder(model, data_list, dict_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/vocab.txt')\n\n   使用句向量模型，将句子转码成句向量\n\n   Parameters:\n      * **model** -- 模型\n\n      * **data_list** -- 句子列表（没有分词）\n\n      * **dict_path** -- bert模型词汇表\n\n   Returns:\n      data_list中的每个句子对应的句向量列表\n\n   >>> origin_model = load_bert_model()\n   >>> new_model = build_model_feature(origin_model)\n   >>> question_list = [\"我爱这个伟大的世界\", \"欣赏世界的风景\"]\n   >>> sen_vector_lists = encoder(new_model, question_list)\n   >>> print(sen_vector_lists.shape)\n\nsk_nlp.nlp_feature_embedding.bert.load_bert_model(with_mlm=True, with_pool=False, return_keras_model=True, config_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/bert_config.json', checkpoint_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/bert/chinese_L-12_H-768_A-12/bert_model.ckpt')\n\n   加载bert 模型\n\n   Parameters:\n      * **with_mlm** -- 是否正则化\n\n      * **with_pool** -- 是否池化\n\n      * **return_keras_model** -- 返回的是keras model 还是 tensorflow\n        模型\n\n      * **config_path** -- bert 模型配置文件路径\n\n      * **checkpoint_path** -- bert 模型路径\n\n   Returns:\nsk_nlp.nlp_feature_embedding.bert.masked_crossentropy(y_true, y_pred)\n\n   mask掉非预测部分，计算交叉熵\n\n   Parameters:\n      * **y_true** -- 真实的Y标签\n\n      * **y_pred** -- 预测的Y标签\n\n   Returns:\n      损失值\n</div>\n\n<div id=\"sk_nlp.nlp_feature_embedding.similarity module\">\nsk_nlp.nlp_feature_embedding.similarity module\n==============================================\n\n计算各种距离\n\nsk_nlp.nlp_feature_embedding.similarity.get_distance_sim_matrix(matrix1, matrix2, metric='cosine')\n\n   返回2个矩阵的各种距离和相似度\n\n   Parameters:\n      * **matrix1** -- 句子向量1\n\n      * **matrix2** -- 句子向量2\n\n      * **metric** -- 'braycurtis', 'canberra', 'chebyshev',\n        'cityblock', 'correlation',\n\n   'cosine', 'dice', 'euclidean', 'hamming', 'jaccard',\n   'jensenshannon', 'kulsinski', 'mahalanobis', 'matching',\n   'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n   'sokalmichener', 'sokalsneath', 'sqeuclidean', 'wminkowski', 'yule'\n   :return:\n\nsk_nlp.nlp_feature_embedding.similarity.get_edit_distance(query_sen_list, candidate_sen_list)\n\n   计算编辑距离\n\n   Parameters:\n      * **query_sen_list** -- 如['我爱中国', '美国总统特朗普']\n\n      * **candidate_sen_list** -- 如['我爱地球', '美国总统拜登']\n\n   Returns:\nsk_nlp.nlp_feature_embedding.similarity.get_edit_similarity(distance_matrix, norm=True)\n\n   先反转编辑距离矩阵，得到编辑相似度矩阵，然后可以选择归一化\n\n   Parameters:\n      * **distance_matrix** -- 距离矩阵\n\n      * **norm** -- True/False\n\n   Returns:\nsk_nlp.nlp_feature_embedding.similarity.get_jaccard_sim(sen_list1, sen_list2, norm=False)\n\n   获得杰卡德相似度\n\n   Parameters:\n      * **sen_list1** -- [['我', '爱','中国'], ['美国', '总统', '特朗\n        普']]\n\n      * **sen_list2** -- [['我', '爱','地球'], ['美国', '总统', '拜登\n        ']]\n\n   :param norm:是否对结果进行归一化 :return:\n\nsk_nlp.nlp_feature_embedding.similarity.match_topk(sim_matrix, topk=1, order=0)\n\n   返回相似度矩阵前topk/或者后topk\n\n   Parameters:\n      * **sim_matrix** --\n\n      * **topk** --\n\n      * **order** --\n\n   Returns:\nsk_nlp.nlp_feature_embedding.similarity.normalization(matrix, reversed=True)\n\n   归一化矩阵，按照最后一个维度\n\n   Parameters:\n      * **matrix** --\n\n      * **reversed** --\n\n   Returns:\n</div>\n\n<div id=\"sk_nlp.nlp_feature_embedding.w2v module\">\nsk_nlp.nlp_feature_embedding.w2v module\n=======================================\n\n传统的w2v模型:包含skip-gram和cbow 目前有一个从wiki语料训练出来的100维\n度的skip-gram模型\n\nclass sk_nlp.nlp_feature_embedding.w2v.WordEmbedding(model_file_path='/machinelearn/wzh/sk_nlp/sk_nlp/model/w2v/skip_gram_wiki2Vec.h5', embedding_dim=100)\n\n   Bases: \"object\"\n\n   fine_tune(new_seg_list, model_file_path)\n\n      基于已有的w2v模型，使用其他语料进行微调。然后保存模型路径。\n\n      Parameters:\n         * **new_seg_list** -- 新句子（分词后）\n\n         * **model_file_path** -- 模型的保存路径\n\n      Returns:\n      >>> model = WordEmbedding()\n      >>> model.get_embedding()\n      >>> new_seg_list = [['我', '爱','中国'], ['美国', '总统', '特朗普']]\n      >>> model.fine_tune(new_seg_list, file_conf.ft_wiki_sg_file_path)\n\n   get_embedding()\n\n      获取词向量模型的信息\n\n      Returns:\n         embedding_matrix:词向量矩阵；index_word：索引到单词的映射；\n         word_index：单词到索引的映射\n\n   op2model()\n\n      由于w2v的接口太多，不太好封装 这里给出了模型的一些常用操作范例\n\n      Returns:\n   train_vec(sentence_list, model_file_path, window=5, min_count=5, sg=0)\n\n      使用w2v训练词向量\n\n      Parameters:\n         * **sentence_list** -- 句子列表，[['我', '爱','中国'], ['美国\n           ', '总统', '特朗普']]\n\n         * **model_file_path** -- 模型保存路径\n\n         * **window** -- 滑动窗口\n\n         * **min_count** -- 最小词频\n\n         * **sg** -- 0是使用cbow, 1是使用跳字模型\n\n      Returns:\n</div>\n</div>\nModule contents\n===============\n\n\nModule contents\n===============\n\n\nMore Resources\n--------------\n\n-   [where is bert pre-train model]  https://github.com/google-research/bert\n-   [where is stopwords corpus]  https://github.com/goto456/stopwords\n-   [Official Python Packaging User Guide](https://packaging.python.org)\n-   [The Hitchhiker's Guide to Packaging]\n\nLicense\n-------\n\nThis is free and unencumbered software released into the public domain.\nAnyone is free to copy, modify, publish, use, compile, sell, or\ndistribute this software, either in source code form or as a compiled\nbinary, for any purpose, commercial or non-commercial, and by any means.\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/me/myproject",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "sk-nlp",
    "package_url": "https://pypi.org/project/sk-nlp/",
    "platform": "",
    "project_url": "https://pypi.org/project/sk-nlp/",
    "project_urls": {
      "Homepage": "https://github.com/me/myproject"
    },
    "release_url": "https://pypi.org/project/sk-nlp/0.1.9/",
    "requires_dist": [
      "numpy",
      "scipy",
      "tensorflow-gpu",
      "bert4keras",
      "sk-common"
    ],
    "requires_python": ">=3.6.0",
    "summary": "nlp kit.",
    "version": "0.1.9",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10896260,
  "releases": {
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9bd3ee9229898486be62ccc556fe630645e3e63586b6137755c8cccd3d1a469f",
          "md5": "cba9830d6eda3561171e4f0bc4c05dc4",
          "sha256": "bca1586aa3cb999d2e57b9e4d4243526098e0f5a2d4eb516b773277763419f60"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cba9830d6eda3561171e4f0bc4c05dc4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 318226,
        "upload_time": "2021-06-29T03:16:24",
        "upload_time_iso_8601": "2021-06-29T03:16:24.607435Z",
        "url": "https://files.pythonhosted.org/packages/9b/d3/ee9229898486be62ccc556fe630645e3e63586b6137755c8cccd3d1a469f/sk_nlp-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "34d54f195b088bccede9995f3584760080185a5babffa98fe875ecea7dd63743",
          "md5": "eea466bb0be7e0721d52fcc89cd1a727",
          "sha256": "b38309d51a1cb6a300fdc393e429211dc6d0586cdf1733f4e567629528c17a0e"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eea466bb0be7e0721d52fcc89cd1a727",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 318476,
        "upload_time": "2021-07-13T09:44:32",
        "upload_time_iso_8601": "2021-07-13T09:44:32.245881Z",
        "url": "https://files.pythonhosted.org/packages/34/d5/4f195b088bccede9995f3584760080185a5babffa98fe875ecea7dd63743/sk_nlp-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "79a1dd43a021fc73f556785116304ef97d190592c7dc8f25185f3fd9fe7802a2",
          "md5": "a7ee50f8d686328a73d37a99e394b171",
          "sha256": "33d47e74dfbd2cf21537316ffecd57d164862d025a0e8fbb1bbc3c93904259d9"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a7ee50f8d686328a73d37a99e394b171",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 318478,
        "upload_time": "2021-07-13T10:24:14",
        "upload_time_iso_8601": "2021-07-13T10:24:14.399278Z",
        "url": "https://files.pythonhosted.org/packages/79/a1/dd43a021fc73f556785116304ef97d190592c7dc8f25185f3fd9fe7802a2/sk_nlp-0.1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb073aeff9e4e42b76c89a080a9a88e65ad91b9918412ff4241907a106c59e70",
          "md5": "15119c203c8a16035ca516c1e70e4d72",
          "sha256": "49b21a5bb38e02441e74664ac20e341e8f4fe44014cae6b4db45afe8c58c543c"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "15119c203c8a16035ca516c1e70e4d72",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 318476,
        "upload_time": "2021-07-13T10:28:03",
        "upload_time_iso_8601": "2021-07-13T10:28:03.985386Z",
        "url": "https://files.pythonhosted.org/packages/bb/07/3aeff9e4e42b76c89a080a9a88e65ad91b9918412ff4241907a106c59e70/sk_nlp-0.1.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d033bbf5c557cc0ad06735ff67b4c1b069f432ef00443621c491ca780538a4cd",
          "md5": "c66139ef8f5868e20652967c523591ae",
          "sha256": "68188fafdb18fafe0dbed219b894d201c1b785397339eafedd78bff3e6c86a7c"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c66139ef8f5868e20652967c523591ae",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 317640,
        "upload_time": "2021-07-13T10:31:15",
        "upload_time_iso_8601": "2021-07-13T10:31:15.215336Z",
        "url": "https://files.pythonhosted.org/packages/d0/33/bbf5c557cc0ad06735ff67b4c1b069f432ef00443621c491ca780538a4cd/sk_nlp-0.1.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8046ae1192c0599b76ea5aff729b9a6ed2bc717072aa021fe9481ba9261764ea",
          "md5": "79ccfc2fcb5025d61a13fd43947d1761",
          "sha256": "49af0f092876a964ae3b3092a1c9ee7a857e8f42876d6da31f04fe1d35d4b523"
        },
        "downloads": -1,
        "filename": "sk_nlp-0.1.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "79ccfc2fcb5025d61a13fd43947d1761",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 317721,
        "upload_time": "2021-07-13T11:41:52",
        "upload_time_iso_8601": "2021-07-13T11:41:52.623508Z",
        "url": "https://files.pythonhosted.org/packages/80/46/ae1192c0599b76ea5aff729b9a6ed2bc717072aa021fe9481ba9261764ea/sk_nlp-0.1.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8046ae1192c0599b76ea5aff729b9a6ed2bc717072aa021fe9481ba9261764ea",
        "md5": "79ccfc2fcb5025d61a13fd43947d1761",
        "sha256": "49af0f092876a964ae3b3092a1c9ee7a857e8f42876d6da31f04fe1d35d4b523"
      },
      "downloads": -1,
      "filename": "sk_nlp-0.1.9-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "79ccfc2fcb5025d61a13fd43947d1761",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 317721,
      "upload_time": "2021-07-13T11:41:52",
      "upload_time_iso_8601": "2021-07-13T11:41:52.623508Z",
      "url": "https://files.pythonhosted.org/packages/80/46/ae1192c0599b76ea5aff729b9a6ed2bc717072aa021fe9481ba9261764ea/sk_nlp-0.1.9-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}