{
  "info": {
    "author": "Sergei Belousov aka BeS",
    "author_email": "sergei.o.belousov@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3"
    ],
    "description": "# pytorch_clip_bbox: Implementation of the CLIP guided bbox ranking for Object Detection.\n\n<p align=\"center\">\n  <img src=\"resources/preds.jpg\"/>\n</p>\n\nPytorch based library to rank predicted bounding boxes using text/image user's prompts.\n\nUsually, object detection models trains to detect common classes of objects such as \"car\", \"person\", \"cup\", \"bottle\".\nBut sometimes we need to detect more complex classes such as \"lady in the red dress\", \"bottle of whiskey\", or \"where is my red cup\" instead of \"person\", \"bottle\", \"cup\" respectively.\nOne way to solve this problem is to train more complex detectors that can detect more complex classes,\nbut we propose to use text-driven object detection that allows detecting any complex classes that can be described by natural language.\nThis library is written to rank predicted bounding boxes using text/image descriptions of complex classes.\n\n## Install package\n\n```bash\npip install pytorch_clip_bbox\n```\n\n## Install the latest version\n\n```bash\npip install --upgrade git+https://github.com/bes-dev/pytorch_clip_bbox.git\n```\n\n## Features\n- The library supports multiple prompts (images or texts) as targets for filtering.\n- The library automatically detects the language of the input text, and multilingual translate it via google translate.\n- The library supports the original CLIP model by OpenAI and ruCLIP model by SberAI.\n- Simple integration with different object detection models.\n\n## Usage\n\nWe provide examples to integrate our library with different popular object detectors like: [YOLOv5](examples/yolov5.py), [MaskRCNN](examples/maskrcnn.py).\nPlease, follow to [examples](examples/) to find more examples.\n\n### Simple example to integrate pytorch_clip_bbox with MaskRCNN model\n\n```bash\n$ pip install -r wheel cython opencv-python numpy torch torchvision pytorch_clip_bbox\n```\n\n```python\nimport argparse\nimport random\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nimport torchvision\nfrom pytorch_clip_bbox import ClipBBOX\n\ndef get_coloured_mask(mask):\n    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n    r = np.zeros_like(mask).astype(np.uint8)\n    g = np.zeros_like(mask).astype(np.uint8)\n    b = np.zeros_like(mask).astype(np.uint8)\n    c = colours[random.randrange(0,10)]\n    r[mask == 1], g[mask == 1], b[mask == 1] = c\n    coloured_mask = np.stack([r, g, b], axis=2)\n    return coloured_mask, c\n\ndef main(args):\n    # build detector\n    detector = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True).eval().to(args.device)\n    clip_bbox = ClipBBOX(clip_type=args.clip_type).to(args.device)\n    # add prompts\n    if args.text_prompt is not None:\n        for prompt in args.text_prompt.split(\",\"):\n            clip_bbox.add_prompt(text=prompt)\n    if args.image_prompt is not None:\n        image = cv2.cvtColor(cv2.imread(args.image_prompt), cv2.COLOR_BGR2RGB)\n        image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0)\n        image = img / 255.0\n        clip_bbox.add_prompt(image=image)\n    image = cv2.imread(args.image)\n    pred = detector([\n        T.ToTensor()(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)).to(args.device)\n    ])\n    pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n    pred_threshold = [pred_score.index(x) for x in pred_score if x > args.confidence][-1]\n    boxes = [[int(b) for b in box] for box in list(pred[0]['boxes'].detach().cpu().numpy())][:pred_threshold + 1]\n    masks = (pred[0]['masks'] > 0.5).squeeze().detach().cpu().numpy()[:pred_threshold + 1]\n    ranking = clip_bbox(image, boxes, top_k=args.top_k)\n    for key in ranking.keys():\n        if key == \"loss\":\n            continue\n        for box in ranking[key][\"ranking\"]:\n            mask, color = get_coloured_mask(masks[box[\"idx\"]])\n            image = cv2.addWeighted(image, 1, mask, 0.5, 0)\n            x1, y1, x2, y2 = box[\"rect\"]\n            cv2.rectangle(image, (x1, y1), (x2, y2), color, 6)\n            cv2.rectangle(image, (x1, y1), (x2, y1-100), color, -1)\n            cv2.putText(image, ranking[key][\"src\"], (x1 + 5, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 4, (0, 0, 0), thickness=5)\n    if args.output_image is None:\n        cv2.imshow(\"image\", image)\n        cv2.waitKey()\n    else:\n        cv2.imwrite(args.output_image, image)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\", \"--image\", type=str, help=\"Input image.\")\n    parser.add_argument(\"--device\", type=str, default=\"cuda:0\", help=\"inference device.\")\n    parser.add_argument(\"--confidence\", type=float, default=0.7, help=\"confidence threshold [MaskRCNN].\")\n    parser.add_argument(\"--text-prompt\", type=str, default=None, help=\"Text prompt.\")\n    parser.add_argument(\"--image-prompt\", type=str, default=None, help=\"Image prompt.\")\n    parser.add_argument(\"--clip-type\", type=str, default=\"clip_vit_b32\", help=\"Type of CLIP model [ruclip, clip_vit_b32, clip_vit_b16].\")\n    parser.add_argument(\"--top-k\", type=int, default=1, help=\"top_k predictions will be returned.\")\n    parser.add_argument(\"--output-image\", type=str, default=None, help=\"Output image name.\")\n    args = parser.parse_args()\n    main(args)\n```\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pytorch-clip-bbox",
    "package_url": "https://pypi.org/project/pytorch-clip-bbox/",
    "platform": "",
    "project_url": "https://pypi.org/project/pytorch-clip-bbox/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/pytorch-clip-bbox/2021.12.25.0/",
    "requires_dist": [
      "Cython",
      "cython",
      "numpy",
      "opencv-python",
      "pytorch-clip-guided-loss",
      "torch",
      "wheel"
    ],
    "requires_python": "",
    "summary": "Pytorch implementation of the CLIP guided bbox refinement for Object Detection.",
    "version": "2021.12.25.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12403933,
  "releases": {
    "2021.12.24.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e05251c016337b04fadae6a410e6087f3a38c8ca3cbeb9835fb05562e036f115",
          "md5": "47191abc54cadebf33fe06b1d994ae57",
          "sha256": "3ef7e1047a313cc395cf81e69a3e0ce66bdc197d76a854b8219e0688f366e280"
        },
        "downloads": -1,
        "filename": "pytorch_clip_bbox-2021.12.24.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "47191abc54cadebf33fe06b1d994ae57",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10030,
        "upload_time": "2021-12-24T20:10:17",
        "upload_time_iso_8601": "2021-12-24T20:10:17.473298Z",
        "url": "https://files.pythonhosted.org/packages/e0/52/51c016337b04fadae6a410e6087f3a38c8ca3cbeb9835fb05562e036f115/pytorch_clip_bbox-2021.12.24.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2021.12.24.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b56d76fb39aed68b46a66c63a7d37d571d370f9d4f4ae0b50797dac00c3170f0",
          "md5": "5a4cf76e0c794cab411da830aaae88c4",
          "sha256": "0adfd688dec36acc8d82670fb55466eb89d105b518ab7d42cdc05d5983227f24"
        },
        "downloads": -1,
        "filename": "pytorch_clip_bbox-2021.12.24.1-py3.8.egg",
        "has_sig": false,
        "md5_digest": "5a4cf76e0c794cab411da830aaae88c4",
        "packagetype": "bdist_egg",
        "python_version": "2021.12.24.1",
        "requires_python": null,
        "size": 10044,
        "upload_time": "2021-12-25T15:16:44",
        "upload_time_iso_8601": "2021-12-25T15:16:44.486481Z",
        "url": "https://files.pythonhosted.org/packages/b5/6d/76fb39aed68b46a66c63a7d37d571d370f9d4f4ae0b50797dac00c3170f0/pytorch_clip_bbox-2021.12.24.1-py3.8.egg",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1558ccc9644d3ba79f9fed54f81cc5a945add507eb7b67f306e5f1598a8eab91",
          "md5": "bbc42ac82e16f529046b60043f4512a3",
          "sha256": "376bc67e80c2ab679823c9a79310cf197644970c37b031aac8fc0faa933e2df4"
        },
        "downloads": -1,
        "filename": "pytorch_clip_bbox-2021.12.24.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bbc42ac82e16f529046b60043f4512a3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10029,
        "upload_time": "2021-12-24T20:17:15",
        "upload_time_iso_8601": "2021-12-24T20:17:15.176642Z",
        "url": "https://files.pythonhosted.org/packages/15/58/ccc9644d3ba79f9fed54f81cc5a945add507eb7b67f306e5f1598a8eab91/pytorch_clip_bbox-2021.12.24.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2021.12.25.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cd5fa35e36d0db07a015400546bd7c6e120afe8f72763877ed38c3cf71759fe2",
          "md5": "1c13315d9f522070abea6155342b9b7b",
          "sha256": "3a4a1410df47329345ecfe3a824f876d79189725492d965d7d14d9fd90a9c1a1"
        },
        "downloads": -1,
        "filename": "pytorch_clip_bbox-2021.12.25.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1c13315d9f522070abea6155342b9b7b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10829,
        "upload_time": "2021-12-25T15:16:42",
        "upload_time_iso_8601": "2021-12-25T15:16:42.884253Z",
        "url": "https://files.pythonhosted.org/packages/cd/5f/a35e36d0db07a015400546bd7c6e120afe8f72763877ed38c3cf71759fe2/pytorch_clip_bbox-2021.12.25.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cd5fa35e36d0db07a015400546bd7c6e120afe8f72763877ed38c3cf71759fe2",
        "md5": "1c13315d9f522070abea6155342b9b7b",
        "sha256": "3a4a1410df47329345ecfe3a824f876d79189725492d965d7d14d9fd90a9c1a1"
      },
      "downloads": -1,
      "filename": "pytorch_clip_bbox-2021.12.25.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1c13315d9f522070abea6155342b9b7b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 10829,
      "upload_time": "2021-12-25T15:16:42",
      "upload_time_iso_8601": "2021-12-25T15:16:42.884253Z",
      "url": "https://files.pythonhosted.org/packages/cd/5f/a35e36d0db07a015400546bd7c6e120afe8f72763877ed38c3cf71759fe2/pytorch_clip_bbox-2021.12.25.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}