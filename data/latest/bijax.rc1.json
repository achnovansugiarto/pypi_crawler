{
  "info": {
    "author": "Zeel B Patel",
    "author_email": "patel_zeel@iitgn.ac.in",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "## BIJAX\n\nBayesian Inference in JAX.\n\n## Installation\n\n```\npip install git+https://github.com/patel-zeel/bijax.git\n```\n\n## Methods implemented in BIJAX\n\n* `from bijax.advi import ADVI` - [Automatic Differentiation Variational Inference](https://arxiv.org/abs/1603.00788)\n* [WIP]`from bijax.laplace import ADLaplace` - Automatic Differentiation Laplace approximation.\n* `from bijax.mcmc import MCMC` - A helper class for external Markov Chain Monte Carlo (MCMC) sampling.\n\n## How to use BIJAX?\n\nBIJAX is built without layers of abstractions or proposing new conventions. Thus, it is also useful for educational purposes. If you like to directly dive into the examples, please refer to the [examples](examples) directory.\n\n\nThere are a few core components of bijax:\n\n### Prior\n`tensoflow_probability.substrates.jax` should be used to define the distributions for prior.\n\n```python\nimport tensorflow_probability.substrates.jax as tfp\ntfd = tfp.distributions\n```\n\nPrior distribution for the coin toss problem can be defined as follows:\n\n```python\nprior = {\"p_of_heads\": tfd.Beta(0.5, 0.5)}\n```\n\nPrior distribution for the Linear Regression problem can be defined as follows:\n\n```python\nshape_of_weights = 5\nprior = {\"weights\": tfd.MultivariateNormalDiag(\n                                               loc=tf.zeros(shape_of_weights), \n                                               scale_diag=tf.ones(shape_of_weights)\n                                            )}\n```\n\n### Bijectors\nBijectors available in `tensorflow_probability.substrates.jax` are used to facilitate the change of variable trick or change of support trick. Here, a bijector should transform a Gaussian random variable with infinite support to a transformed random variable with finite support.\n\n```python\nimport tensorflow_probability.substrates.jax as tfp\ntfb = tfp.bijectors\n```\n\nTo perform Automatic Differentiation Variational Inference for the coin toss problem, a bijector can be defined as follows:\n\n```python\nprior = {\"p_of_heads\": tfd.Beta(0.5, 0.5)}\nbijector = {\"p_of_heads\": tfb.Sigmoid()}\n```\n\nFor the Linear Regression problem, a bijector can be defined as follows:\n\n```python\nshape_of_weights = 5\nprior = {\"weights\": tfd.MultivariateNormalDiag(\n                                               loc=tf.zeros(shape_of_weights), \n                                               scale_diag=tf.ones(shape_of_weights)\n                                            )}\nbijector = {\"weights\": tfb.Identity()}\n```\n\n### Likelihood\nUsers have total freedom on how to define the log likelihood function adhering to several conditions. The log likelihood function should take the following arguments:\n\n* latent_sample: a dictionary of values that represents a sample taken from the latent (prior) parameter distributions. It will have same keys as the prior.\n* outputs: Outputs generated from the likelihood. We will find log probability of the `outputs` given a latent sample.\n* inputs: Input data required to evaluate the likelihood. For example, in the Linear Regression problem, `X` is `inputs`. For the coin toss problem, `inputs` is None.\n* kwargs: We internally pass the trainable `params` as `kwargs` to the likelihood function. So, the user can mention additional learnable parameters in `kwargs` and they will be trained.\n\nFor coin toss problem, we can define the log likelihood function as follows:\n\n```python\ndef log_likelihood_fn(latent_sample, outputs, inputs, **kwargs):\n    p_of_heads = latent_sample[\"p_of_heads\"]\n    log_likelihood = tfd.Bernoulli(probs=p_of_heads).log_prob(outputs).sum()\n    return log_likelihood\n```\n\nFor the Linear Regression problem with learnable noise variance, we can define the log likelihood function as follows:\n\n```python\ndef log_likelihood_fn(latent_sample, outputs, inputs, **kwargs):\n    weights = latent_sample[\"weights\"]\n    loc = jnp.dot(weights, inputs[\"X\"])\n    noise_variance = jnp.exp(kwargs[\"log_noise_scale\"])\n    log_likelihood = tfd.MultivariateNormalDiag(loc=loc, scale_diag=noise_variance).log_prob(outputs).sum()\n    return log_likelihood\n```\n\n### Initialization\nWe can automatically initialize the parameters of the model.\n\nHere is an example with ADVI model.\n```python\nmodel = ADVI(prior, bijector, log_likelihood_fn, vi_type=\"mean_field\")\nseed = jax.random.PRNGKey(0)\nparams = model.init(seed)\n```\n\n### Optimization\nModels in bijax have `loss_fn` method which can be used to compute the loss. The loss can be optimized with any method that work with `JAX`. We also have a utility function `from bijax.utils import train` to train the model using `optax` optimizers.\n\n### Get the posterior distribution\nSome of the models (`ADVI` and `ADLaplace`) support `.apply()` method to get the posterior distribution.\n\n```python\nposterior = model.apply(params, ...)\nposterior.sample(...)\nposterior.log_prob(...)\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/patel-zeel/bijax",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bijax",
    "package_url": "https://pypi.org/project/bijax/",
    "platform": null,
    "project_url": "https://pypi.org/project/bijax/",
    "project_urls": {
      "Homepage": "https://github.com/patel-zeel/bijax"
    },
    "release_url": "https://pypi.org/project/bijax/0.1.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Approximate Bayesian Inference in JAX",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14432429,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "22f3983e9e4ff77b24fcdf97160006d45225cb39d0d3f84ecfaca69e9375135c",
          "md5": "48e5b85e5a345aab1ebc7846da7bee46",
          "sha256": "c508f6f43206e56f30cdf7d5863ef0f5a000d14caf985f7a387dd07652ce64d5"
        },
        "downloads": -1,
        "filename": "bijax-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "48e5b85e5a345aab1ebc7846da7bee46",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 299911,
        "upload_time": "2022-07-14T08:14:45",
        "upload_time_iso_8601": "2022-07-14T08:14:45.029318Z",
        "url": "https://files.pythonhosted.org/packages/22/f3/983e9e4ff77b24fcdf97160006d45225cb39d0d3f84ecfaca69e9375135c/bijax-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "22f3983e9e4ff77b24fcdf97160006d45225cb39d0d3f84ecfaca69e9375135c",
        "md5": "48e5b85e5a345aab1ebc7846da7bee46",
        "sha256": "c508f6f43206e56f30cdf7d5863ef0f5a000d14caf985f7a387dd07652ce64d5"
      },
      "downloads": -1,
      "filename": "bijax-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "48e5b85e5a345aab1ebc7846da7bee46",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 299911,
      "upload_time": "2022-07-14T08:14:45",
      "upload_time_iso_8601": "2022-07-14T08:14:45.029318Z",
      "url": "https://files.pythonhosted.org/packages/22/f3/983e9e4ff77b24fcdf97160006d45225cb39d0d3f84ecfaca69e9375135c/bijax-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}