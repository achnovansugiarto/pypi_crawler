{
  "info": {
    "author": "Federico A. Galatolo",
    "author_email": "federico.galatolo@ing.unipi.it",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# torchsm\npytorch implementation of the **Stigmergic Memory** as presented in the paper [Using stigmergy as a computational memory in the design of recurrent neural networks]().\n\nYou can use this package to **easly** integrate our model into existing ones\n\nYou can safely **mix** native pytorch Modules with ours.  \n\nBut **do not forget** to `reset()` them before starting every new time sequence\n\nImplementing our [proposed architecture to solve MNIST]() becomes as easy as:\n```python\nimport torch\nimport torchsm\n\nnet = torchsm.Sequential(\n    torchsm.RecurrentStigmergicMemoryLayer(28, 15, hidden_layers=1, hidden_dim=20),\n    torch.nn.Linear(15, 10),\n    torch.nn.PReLU(),\n    torch.nn.Linear(10, 10),\n    torch.nn.PReLU()\n)\n```\n\nYou can train the time-unfolded model by computing the loss function on the desired temporal output\n\n```python\noptimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\nloss_fn = torch.nn.MSELoss()\n\nfor i in range(0,N):\n    for X, Y in zip(dataset_X, dataset_Y):\n        net.reset()\n        out = None\n        for i in range(0, X.shape[1]):\n            out = net(torch.tensor(X[:,i], dtype=torch.float32))\n        \n        loss = loss_fn(out, Y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n\n### Does it support batch inputs?\n\nYes! The inputs have to be batched\n\n```python\nfor t in range(0, num_ticks):\n    batch_out[0], batch_out[1], ... = net(torch.tensor([batch_in[0][t], batch_in[1][t], ...]))\n```\n### Can it run on CUDA?\n\nYes and as you will expect from a pytorch Module!  \nYou just need to call the `to(device)` method on a model to move it in the GPU memory\n\n```python\ndevice = torch.device(\"cuda\")\n\nnet = net.to(device)\n\nnet(torch.tensor(..., device=device))\n```\n## Documentation\n\n### torchsm.Sequential\n\nWrapper of `torch.nn.Sequential` that adds the `reset()` method and forward the call to each `torchsm.BaseLayer` child.\n\nIf you want to use a `SequentialContaier` to build your models with one or more torchsm's layers you have to use `torchsm.Sequential` instead of `torch.nn.Sequential` in order to be able to `reset()` them.\n\n### torchsm.StigmergicMemoryLayer\n\nThis layer has two hidden ANNs with the layer's inputs as inputs and which outputs respectively determine the marks and ticks of a multi-monodimensional stigmergic space.\n\n![Imgur](https://i.imgur.com/yS4M4nA.png)\n\n### torchsm.RecurrentStigmergicMemoryLayer\n\nThis layer is a StigmergicMemoryLayer which output is normalized by a linear layer and recurrently forwarded as input to the two hidden ANNs \n\n![Imgur](https://i.imgur.com/JWQF6ft.png)\n\n## Citing\n\nWe can't wait to see what you will build with torchsm!  \nWhen you will publish your work you can use this BibTex to cite us :)\n\n```\n@article{galatolo_snn\n,\tauthor\t= {Galatolo, Federico A and Cimino, Mario GCA and Vaglini, Gigliola}\n,\ttitle\t= {Using stigmergy as a computational memory in the design of recurrent neural networks}\n,\tjournal\t= {ICPRAM 2019}\n,\tyear\t= {2019}\n,\tpages\t= {}\n}\n```\n\n## Contributing\n\nThis code is released under GNU/GPLv3 so feel free to fork it and submit your changes, every PR helps.  \nIf you need help using it or for any question please reach me at [federico.galatolo@ing.unipi.it](mailto:galatolo.federico@gmail.com) or on Telegram  [@galatolo](https://t.me/galatolo)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/galatolofederico/torchsm",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torchsm",
    "package_url": "https://pypi.org/project/torchsm/",
    "platform": "",
    "project_url": "https://pypi.org/project/torchsm/",
    "project_urls": {
      "Homepage": "https://github.com/galatolofederico/torchsm"
    },
    "release_url": "https://pypi.org/project/torchsm/0.1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Pytorch implementation of the Stigmergic Memory",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4695316,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2fff707cdcb31a93a47c88b680e44bc87273674c2018bb178d39a96dd5f6d9dc",
          "md5": "88957dba8b3e68739b617de94e58b46b",
          "sha256": "e83698a788ab6c2145be331636f0a1822d87cb0e4054a78b767ee3ae15c669d3"
        },
        "downloads": -1,
        "filename": "torchsm-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "88957dba8b3e68739b617de94e58b46b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4372,
        "upload_time": "2019-01-14T18:19:45",
        "upload_time_iso_8601": "2019-01-14T18:19:45.099910Z",
        "url": "https://files.pythonhosted.org/packages/2f/ff/707cdcb31a93a47c88b680e44bc87273674c2018bb178d39a96dd5f6d9dc/torchsm-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2fff707cdcb31a93a47c88b680e44bc87273674c2018bb178d39a96dd5f6d9dc",
        "md5": "88957dba8b3e68739b617de94e58b46b",
        "sha256": "e83698a788ab6c2145be331636f0a1822d87cb0e4054a78b767ee3ae15c669d3"
      },
      "downloads": -1,
      "filename": "torchsm-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "88957dba8b3e68739b617de94e58b46b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 4372,
      "upload_time": "2019-01-14T18:19:45",
      "upload_time_iso_8601": "2019-01-14T18:19:45.099910Z",
      "url": "https://files.pythonhosted.org/packages/2f/ff/707cdcb31a93a47c88b680e44bc87273674c2018bb178d39a96dd5f6d9dc/torchsm-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}