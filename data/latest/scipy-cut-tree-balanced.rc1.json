{
  "info": {
    "author": "Vicente Reyes-Puerta",
    "author_email": "vr.github@outlook.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Balanced Cut Tree Method for Hierarchical Clustering\n\nThis package contains a Python function that performs a balanced cut tree of a SciPy linkage matrix built using any linkage method (e.g. 'ward'). It builds upon the SciPy and NumPy libraries.\n\nThe initial problem was the following: if you perform a standard cut on a tree (i.e. the result from a hierarchical clustering), probably you will end up having a few big clusters (where the number of data samples is high), and many small clusters (each containing very few data samples). Thus, the resulting clustering is unbalanced, i.e. it contains clusters of very variable size.\n\nThe proposed function looks recursively along the hierarchical tree, from the root (single cluster gathering all the samples) to the leaves (i.e. the clusters with only one sample), retrieving the biggest possible clusters containing a number of samples lower than a given maximum. In this way, if a cluster at a specific tree level contains a number of samples higher than the given maximum, it is ignored and its offspring (smaller) sub-clusters are taken into consideration. If the cluster contains a number of samples lower than the given maximum, it is taken as result and its offspring sub-clusters not further processed.\n\nSince all output clusters contain no more than a given maximum number of samples, the resulting clustering is considered to be more balanced than a standard cut tree. Note however that the number of samples per cluster might still have a considerable variability, since the splitting of a big cluster can result in sub-clusters with very variable number of samples. This variability should be smaller as the given maximum of samples per cluster is closer to 1 (being the variability equal to 0 when the maximum is at its limit, i.e. 1).\n\nThe function returns two results:\n\n1. List of integers containing for each input sample its corresponding cluster id. The cluster id is an integer which is higher for deeper tree levels.\n2. List of integer arrays containing for each input sample its corresponding cluster tree level, i.e. a sequence of 0s and 1s. Note that the cluster level is longer for deeper tree levels, being [0] the root cluster, [0, 0] and [0, 1] its offspring, and so on. Also note that in each cluster splitting, the label 0 denotes the bigger cluster, while the label 1 denotes the smallest.\n\n# Installation and Use\n\nThis package can be installed using `pip`.\n\n```\n$ pip install scipy_cut_tree_balanced\n```\n\nThen you can use the function as shown in this sample Python code.\n\n```\nfrom scipy_cut_tree_balanced import cut_tree_balanced\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.stats import gamma\nimport numpy as np\n\n\n# Initialize the random seed\nnp.random.seed(14)\n\n# Create a input matrix containing 100 data samples with 4 dimensions\nX = gamma.rvs(0.1, size=400).reshape((100, 4))\n\n# Compute the linkage matrix using the scipy linkage() method\nZ = linkage(X, 'ward')\n\n# Perform a balanced cut tree of the linkage matrix\n[balanced_cut_cluster_id, balanced_cut_cluster_level] = cut_tree_balanced(Z, 10)\n\nprint(\"First 10 rows of the balanced clustering result (one cluster id per sample):\")\nprint(str(balanced_cut_cluster_id[0:10]) + \" ...\")\n```\n\n# Example Script\n\nBy running `example.py` you will get more elaborate printed outputs similar to the following. Here we perform a more detailed comparison between a standard and a balanced cut tree.\n\n```\n$ python3 example.py\n```\n\nFirst, a numpy array of 100 rows x 4 columns is randomly generated using a gamma distribution. Note that we perform such a random sampling from a gamma distribution so that the resulting standard clustering is unbalanced (see below).\n\n```\n    np.random.seed(4)\n    X = gamma.rvs(0.1, size=400).reshape((100,4))\n```\n\nIn order to check the validity of the input data, the type, shape and the first 10 rows are printed.\n\n```\nType of the input data sample: <class 'numpy.ndarray'>\nShape of the input data sample: (100, 4)\nFirst 10 rows of the input data:\n[[1.28573793e-03 8.12672961e-06 1.26520704e-03 2.07729574e-03]\n [1.16397414e-01 2.06534197e-03 1.91044478e-02 5.35127859e-01]\n [5.81563428e-02 5.92302950e-06 1.90433024e-02 2.87155777e-02]\n [3.98932109e-08 5.37862343e-02 4.38562255e-02 1.27557329e-04]\n [3.57028885e-04 2.88945299e-05 3.40388733e-05 9.90278888e-06]\n [3.91282036e-06 4.61803593e-02 2.75652111e-08 1.66504104e-09]\n [2.60630428e-10 3.89770028e-04 8.52159994e-03 5.83321506e-09]\n [6.37325763e-10 3.41859809e-04 4.51815091e-02 2.83600476e-06]\n [1.14654357e-03 1.12808821e-02 1.61202749e-04 1.56459197e-11]]\n```\n\nNext, the linkage matrix is computed by using the ward method, and a standard cut tree is performed (with a specific number of output clusters = 20).\n\n```\n    Z = linkage(X, 'ward')\n    standard_cut_cluster_id = cut_tree(Z, n_clusters=[20])\n```\n\nAs shown below, the output is a numpy array of 100 elements, assigning one cluster ID to each input vector (of 4 dimensions, see above). Note that the ID of the resulting clusters go from 0 to 19 in this case. The resulting clustering is unbalanced, i.e. containing a big cluster (where the number of data samples is 48), and many small clusters (each containing very few data samples, 9 of them containing a single data sample). As result, the range of cluster sizes goes from 1 to 48, showing a standard deviation of 10.17 data samples.\n\n```\nType of the standard clustering result: <class 'numpy.ndarray'>\nShape of the standard clustering result (one cluster id per data sample): (100, 1)\nFirst 10 rows of the standard clustering result (one cluster id per sample):\n[0 1 0 0 0 0 0 0 0 2] ...\nTotal number of resulting clusters = 20\nFor each resulting cluster: Cluster ID\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\nFor each resulting cluster: Count of data samples\n[48  4  1  2 10  8  1  6  2  2  1  1  1  1  3  1  4  2  1  1]\nCount of data samples per cluster: mean = 5, max = 48, min = 1, std = 10.17\n```\n\n\n\nA more balanced clustering is then attempted by using the balanced ward tree method, in which the maximum number of data samples within each cluster is set to 10.\n\n```\n    [balanced_cut_cluster_id, balanced_cut_cluster_level] = cut_tree_balanced(Z, 10)\n```\n\nWe get two results from the new function: (1) a list of integers containing for each input sample its corresponding cluster id, and (2) a list of strings containing for each input sample its corresponding cluster tree level (see above section for further information). Note that the ID of the resulting clusters go again from 0 to 19 in this case, i.e. the number of resulting clusters (20) is identical to the previous one. Importantly, the resulting clustering is more balanced than the standard one (for an equal number of resulting clusters), since the range of cluster sizes goes from 1 to 10, showing a standard deviation of 2.68 data samples.\n\n```\nType of the balanced clustering result (id): <class 'numpy.ndarray'>\nShape of the balanced clustering result (one cluster id per data sample): (100,)\nFirst 10 rows of the balanced clustering result (one cluster id per sample):\n[18  3  9 11 19 11 13  8 14  1] ...\n\nType of the balanced clustering result (level): <class 'numpy.ndarray'>\nShape of the balanced clustering result (level) (one array per data sample): (100,)\nFirst 10 rows of the balanced clustering result (level) (one array per sample):\n[array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n array([0, 0, 0, 1]) array([0, 0, 0, 0, 0, 0, 0, 0, 1])\n array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n array([0, 0, 0, 0, 0, 0, 0, 1])\n array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]) array([0, 0, 1, 0])] ...\n\nTotal number of resulting clusters = 20\nFor each resulting cluster: Cluster ID\n[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\nFor each resulting cluster: Count of data samples\n[ 2 10  2  7  7 10  6  8  8  3  4  4  3  3  3  1  4  3  4  8]\nCount of data samples per cluster: mean = 5, max = 10, min = 1, std = 2.68\n```\n\n\n\nIn conclusion, here we describe and implement a method which generates (for a similar number of resulting clusters) a more balanced outcome, i.e. building clusters of less variable size.\n\n# Related Work\n\nThere are several implemented methods following the same or a similar idea, i.e. performing a cut tree in which the resulting clusters are at different tree levels.\n\n- Implementation of the same method within the [scikit-network package](https://scikit-network.readthedocs.io/en/latest/reference/hierarchy.html#sknetwork.hierarchy.cut_balanced) (see source code of the [cut_balanced function](https://scikit-network.readthedocs.io/en/latest/_modules/sknetwork/hierarchy/postprocess.html#cut_balanced)).\n- The CRAN R package [dynamicTreeCut](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/BranchCutting/) (see GitHub [repo](https://github.com/cran/dynamicTreeCut) and the [paper](https://academic.oup.com/bioinformatics/article/24/5/719/200751)) implements novel dynamic branch cutting methods for detecting clusters in a dendrogram depending on their shape.\n- Translation of the [dynamicTreeCut](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/BranchCutting/) method [to Python](https://github.com/kylessmith/dynamicTreeCut).\n- The web based [MLCut](https://bivi.co/visualisation/mlcut) tool (see GitHub [repo](https://github.com/than8/MLCut) and the [paper](https://research-repository.st-andrews.ac.uk/handle/10023/9518)) provides interactive methods to cut the branches of the tree at multiple levels.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/vreyespue/scipy_cut_tree_balanced/archive/v_1_1.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/vreyespue/scipy_cut_tree_balanced",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scipy-cut-tree-balanced",
    "package_url": "https://pypi.org/project/scipy-cut-tree-balanced/",
    "platform": "",
    "project_url": "https://pypi.org/project/scipy-cut-tree-balanced/",
    "project_urls": {
      "Download": "https://github.com/vreyespue/scipy_cut_tree_balanced/archive/v_1_1.tar.gz",
      "Homepage": "https://github.com/vreyespue/scipy_cut_tree_balanced"
    },
    "release_url": "https://pypi.org/project/scipy-cut-tree-balanced/1.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Python function that performs a balanced cut tree of a SciPy linkage matrix",
    "version": "1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9007017,
  "releases": {
    "1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e0140c8c7d128cac4c418b39f8322401dc915b4f5b38364e80e670bbca32cf8f",
          "md5": "33af34b4156d96869f0eaf973458c490",
          "sha256": "3c0c515bda329b2065f3b208417ed84603f499fb40a3abf447568e5c0b159c4f"
        },
        "downloads": -1,
        "filename": "scipy_cut_tree_balanced-1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "33af34b4156d96869f0eaf973458c490",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 9263,
        "upload_time": "2020-12-29T10:46:27",
        "upload_time_iso_8601": "2020-12-29T10:46:27.585640Z",
        "url": "https://files.pythonhosted.org/packages/e0/14/0c8c7d128cac4c418b39f8322401dc915b4f5b38364e80e670bbca32cf8f/scipy_cut_tree_balanced-1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e0140c8c7d128cac4c418b39f8322401dc915b4f5b38364e80e670bbca32cf8f",
        "md5": "33af34b4156d96869f0eaf973458c490",
        "sha256": "3c0c515bda329b2065f3b208417ed84603f499fb40a3abf447568e5c0b159c4f"
      },
      "downloads": -1,
      "filename": "scipy_cut_tree_balanced-1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "33af34b4156d96869f0eaf973458c490",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 9263,
      "upload_time": "2020-12-29T10:46:27",
      "upload_time_iso_8601": "2020-12-29T10:46:27.585640Z",
      "url": "https://files.pythonhosted.org/packages/e0/14/0c8c7d128cac4c418b39f8322401dc915b4f5b38364e80e670bbca32cf8f/scipy_cut_tree_balanced-1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}