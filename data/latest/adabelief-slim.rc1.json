{
  "info": {
    "author": "Romain Keramitas",
    "author_email": "r.keramitas@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# AdaBelief Slim\n\nThis repository contains the code for the `adabelief-slim` Python package, from which you can use a Pytorch implementation of the AdaBelief optimizer.\n\n## Installation\n\nUsing Python 3.6 or higher:\n\n```bash\npip install adabelief-slim\n```\n\n## Usage\n\n```python\nfrom adabelief import AdaBelief\n\nmodel = ...\nkwargs = ...\n\noptimizer = AdaBelief(model.parameters(), **kwargs)\n```\n\nThe following hyperparameters can be passed as keyword arguments:\n\n- `lr`: learning rate (default: `1e-3`)\n- `betas`: 2-tuple of coefficients used for computing the running averages of the gradient and its \"variance\" (see paper) (default: `(0.9, 0.999)`)\n- `eps`: term added to the denominator to improve numerical stability (default: `1e-8`)\n- `weight_decay`: weight decay coefficient (default: `1e-2`)\n- `amsgrad`: whether to use the AMSGrad variant of the algorithm (default: `False`)\n- `rectify`: whether to use the RAdam variant of the algorithm (default: `False`)\n- `weight_decouple`: whether to use the AdamW variant of this algorithm (default: `True`)\n\nBe aware that the AMSGrad and RAdam variants **can't** be used simultaneously.\n\n## Motivation\n\nAs you're probably aware, one of the paper's main authors ([Juntang Zhuang](https://juntang-zhuang.github.io/)) released his code in this [repository](https://github.com/juntang-zhuang/Adabelief-Optimizer), which is used to maintain the `adabelief_pytorch` package. Thus, you may be wondering why this repository exists, and how it differs with his. The reason is actually pretty simple: the author made some decisions regarding his code which made it an unsuitable option for me. While it wasn't the only thing that bugged me, my main issue was with adding unnecessary packages as dependencies.\n\nRegarding differences, the main ones are:\n\n- I removed the `fixed_decay` option, as the author's experiments showed it wasn't great\n- I removed the `degenerate_to_sgd` option, as the author copied the RAdam codebase, but it seems recommended to always use it\n- I removed all logging related features, along with the `print_change_log` option\n- I removed all code specific to older version of Pytorch (I think all versions above `1.4` should work), as I don't care for them\n- I changed the flow of the code to be closer to the official implementation of AdamW\n- I removed all usage of the `.data` property as it isn't recommended, and can be avoided with the `torch.no_grad` decorator\n- I moved the code specific to AMSGrad so that it isn't executed if the RAdam variant is selected\n- I added an exception if both RAdam and AMSGrad are selected, as they can't both be used (in the official repository RAdam is used if both RAdam and AMSGrad are selected)\n- I removed half-precision support, as I don't care for it\n\n## References\n\n### Codebases\n\n- [Official AdaBelief implementation](https://github.com/juntang-zhuang/Adabelief-Optimizer)\n- [Official RAdam implementation](https://github.com/LiyuanLucasLiu/RAdam)\n- [Official AdamW implementation](https://pytorch.org/docs/stable/_modules/torch/optim/adamw.html#AdamW)\n- [Pytorch Optimizers](https://github.com/jettify/pytorch-optimizer)\n\n### Papers\n\n- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980): proposed Adam\n- [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101): proposed AdamW\n- [On the Convergence of Adam and Beyond](https://arxiv.org/pdf/1904.09237.pdf): proposed AMSGrad\n- [On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265): proposed RAdam\n- [AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients](https://arxiv.org/abs/1908.03265): proposed AdaBelief\n\n## License\n\n[MIT](LICENSE)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/r0mainK/adabelief-slim",
    "keywords": "adabelief,pytorch,optimizers",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "adabelief-slim",
    "package_url": "https://pypi.org/project/adabelief-slim/",
    "platform": "",
    "project_url": "https://pypi.org/project/adabelief-slim/",
    "project_urls": {
      "Bug Tracker": "https://github.com/r0mainK/adabelief-slim/issues",
      "Homepage": "https://github.com/r0mainK/adabelief-slim"
    },
    "release_url": "https://pypi.org/project/adabelief-slim/0.0.1/",
    "requires_dist": [
      "torch (<2,>=1.4)"
    ],
    "requires_python": ">=3.6",
    "summary": "Slim implementation of the AdaBelief optimizer in Pytorch",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9535745,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ef1ae0e08f2d6467bc255295a032745fdbdce45beab1d32e489dd6a7762afcc5",
          "md5": "a537e1d634bb053771cdcf03a6533871",
          "sha256": "edd6b3bab3b3461d3ad8bde99a52df404ecb1f1eee7d86c58597f7974ef4c5b7"
        },
        "downloads": -1,
        "filename": "adabelief_slim-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a537e1d634bb053771cdcf03a6533871",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 6482,
        "upload_time": "2021-02-26T18:18:29",
        "upload_time_iso_8601": "2021-02-26T18:18:29.097922Z",
        "url": "https://files.pythonhosted.org/packages/ef/1a/e0e08f2d6467bc255295a032745fdbdce45beab1d32e489dd6a7762afcc5/adabelief_slim-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ddc763943d3f63ee6084419a2a74c2fb87daf8ff828a880413ffd4b10f82bc49",
          "md5": "2a43fb5e27c6b74dfa058809be77477e",
          "sha256": "d3656e7f0c82b29bee3857daa76717fbbe2925c5239ff968bc9a75cab4adf2d4"
        },
        "downloads": -1,
        "filename": "adabelief-slim-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "2a43fb5e27c6b74dfa058809be77477e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 5347,
        "upload_time": "2021-02-26T18:18:30",
        "upload_time_iso_8601": "2021-02-26T18:18:30.273025Z",
        "url": "https://files.pythonhosted.org/packages/dd/c7/63943d3f63ee6084419a2a74c2fb87daf8ff828a880413ffd4b10f82bc49/adabelief-slim-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ef1ae0e08f2d6467bc255295a032745fdbdce45beab1d32e489dd6a7762afcc5",
        "md5": "a537e1d634bb053771cdcf03a6533871",
        "sha256": "edd6b3bab3b3461d3ad8bde99a52df404ecb1f1eee7d86c58597f7974ef4c5b7"
      },
      "downloads": -1,
      "filename": "adabelief_slim-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a537e1d634bb053771cdcf03a6533871",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 6482,
      "upload_time": "2021-02-26T18:18:29",
      "upload_time_iso_8601": "2021-02-26T18:18:29.097922Z",
      "url": "https://files.pythonhosted.org/packages/ef/1a/e0e08f2d6467bc255295a032745fdbdce45beab1d32e489dd6a7762afcc5/adabelief_slim-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ddc763943d3f63ee6084419a2a74c2fb87daf8ff828a880413ffd4b10f82bc49",
        "md5": "2a43fb5e27c6b74dfa058809be77477e",
        "sha256": "d3656e7f0c82b29bee3857daa76717fbbe2925c5239ff968bc9a75cab4adf2d4"
      },
      "downloads": -1,
      "filename": "adabelief-slim-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "2a43fb5e27c6b74dfa058809be77477e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 5347,
      "upload_time": "2021-02-26T18:18:30",
      "upload_time_iso_8601": "2021-02-26T18:18:30.273025Z",
      "url": "https://files.pythonhosted.org/packages/dd/c7/63943d3f63ee6084419a2a74c2fb87daf8ff828a880413ffd4b10f82bc49/adabelief-slim-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}