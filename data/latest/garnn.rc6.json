{
  "info": {
    "author": "Levi Borodenko",
    "author_email": "Levi.borodenko@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Programming Language :: Python"
    ],
    "description": "# GARNN [TensorFlow]\nTensorFlow implementation of _Graphical Attention Recurrent Neural Networks_ based on work by [Cirstea et al., 2019](https://milets19.github.io/papers/milets19_paper_8.pdf).\n\nMoreover, we offer stand-alone implementations of the _Graph Attention Mechanism_ [(Veličković et al., 2017)](https://arxiv.org/abs/1710.10903) and _Diffusional Graph Convolution_ [(Li et al., 2017)](https://arxiv.org/pdf/1707.01926.pdf).\n\n### Installation\nSimply run `pip install garnn`. Dependencies are `numpy; tensorflow`.\n\n### Usage\n\nThe core data structure is the _graph signal_. If we have N nodes in a graph each having F observed features then the graph signal is the tensor with shape (batch, N, F) corresponding to the data produced by all nodes. Often we have sequences of graph signals in a time series. We will call them _temporal_ graph signals and assume a shape of (batch, time steps, N, F). We also need to know the adjacency matrix E of the underlying graph with shape (N, N).\n\n#### Non-Temporal Data (batch, N, F)\nAll but the recurrent layers work with non-temporal data, i.e. the data points are individual graph signals and not sequences of graph signals.\n\nThe `AttentionMechanism` found in `garnn.components.attention` will take a graph signal and return an attention matrix as described in [Veličković et al., 2017](https://arxiv.org/abs/1710.10903).\n\nThe layer is initiated with the following parameters:\n\n| Parameter | Function |\n|:------------- | :--------|\n|`F` (required) | Dimension of internal embedding.|\n|`adjacency_matrix` (required) | Adjacency matrix of the graph.|\n|`num_heads` (default: 1) | Number of attention matrices that are averaged to return the output attention.|\n|`use_reverse_diffusion` (default: True) | Whether or not to calculate A_in and A_out as done by [Cirstea et al., 2019](https://milets19.github.io/papers/milets19_paper_8.pdf). If E is symmetric then the value will be set to False.|\n\nThe output is of shape (batch, N, N). If `use_reverse_diffusion` is true then we obtain 2 attention matrices and thus the shape is (batch, 2, N, N).\n\nThe `GraphDiffusionConvolution` layer in `garnn.layers.diffconv` offers diffusion graph convolution as described by [Li et al., 2017](https://arxiv.org/pdf/1707.01926.pdf). It operates on a tuple containing a graph signal X and a transition matrix A (usually an attention matrix returned by an attention mechanism) and is initiated with the following parameters\n\n| Parameter | Function |\n|:------------- | :--------|\n|`features` (required) | Number output features. Q in the paper.|\n|`num_diffusion_steps` (required) | Number of hops done by the diffusion process. K in the paper. |\n\nThere are more specialised parameters like regularisers and initialisers -- those can be found in the doc string. The convolutional layer returns a diffused graph signal of shape (batch, N, Q).\n\nThus, if we have 10 nodes with 5 features each and we would like to apply diffusion graph convolution with 20 features using a 5-head attention mechanism with an internal embedding of 64 units then we would need to run\n\n```python\nfrom garnn.components.attention import AttentionMechanism\nfrom garnn.layers.diffconv import GraphDiffusionConvolution\nfrom tensorflow.keras.layers import Input\n\n# input of 10 by 5 graph signal\ninputs = Input(shape=(10, 5))\n\n# Initiating attention mechanism. Make sure you define E\nAttn = AttentionMechanism(64, adjacency_matrix=E, num_heads=5)(inputs)\n\n# Now the convolutional layer. Make sure you use the correct order in the\n# input-tuple: Graph signal is always first!\noutput = GraphDiffusionConvolution(\n    features=10, num_diffusion_steps=5)((inputs, Attn))\n```\n\n\n#### Temporal Data (batch, time steps, N, F)\n\nBoth `AttentionMechanism` and `DiffusionGraphConvolution` naturally extend to temporal graph signals. The output now simply has an additional time steps dimension.\n\nThe `garnn_gru` layer found in `garnn.components.garnn_gru` is the diffusional & attention-based GRU introduced by [Cirstea et al., 2019](https://milets19.github.io/papers/milets19_paper_8.pdf). It operates on temporal graph signals and an attention mechanism. Initiate with\n\n| Parameter | Function |\n|:------------- | :--------|\n|`num_hidden_features` (required) | Number of features in the hidden state. (Q in the paper)|\n|`num_diffusion_steps` (default: 5) | Number of hops done by the diffusion process in all internal convolutions. K in the paper.|\n|`return_sequence` (default: False) | Whether or not the RNN should return the hidden state at each time step or only at the final time step. Set it to `True` if you stack another RNN layer on top of this one.|\n\nHence, if we would like to rebuild a model similar to the one used by [Cirstea et al., 2019](https://milets19.github.io/papers/milets19_paper_8.pdf) then one needs to run\n\n```python\nimport numpy as np\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input\n\nfrom garnn.components.attention import AttentionMechanism\nfrom garnn.components.garnn_gru import garnn_gru\n\n# We assume we have 207 nodes with 10 features each.\n\n# For simplicity we create a random adjacency matrix E\nE = np.random.randint(0, 2, size=(207, 207))\n\n# Input of the temporal graph signals. Note that \"None\" allows\n# us to pass in variable length time series.\nX = Input(shape=(None, 207, 10))\n\n# creating attention mechanism with 3 heads and an\n# embedding-size of 16\nA = AttentionMechanism(16, adjacency_matrix=E, num_heads=3)(X)\n\n# Piping X and A into the 2 stacked GRU layers.\n# First layer streches the features into 64 diffused features.\n# We assume that we are using 6 hop diffusions.\ngru_1 = garnn_gru(num_hidden_features=64, num_diffusion_steps=6, return_sequences=True)(\n    (X, A)\n)\n\n# And then we use the 2nd GRU to shrink it back to 1 feature - the feature\n# that we are predicting. We use the same attention for this layer, but note that\n# we could've also introduce a new attention mechanism for this GRU.\noutput = garnn_gru(num_hidden_features=1, num_diffusion_steps=6)((gru_1, A))\n\ngarnn_model = Model(inputs=X, outputs=output)\n```\n\n### Contribute\nBug reports, fixes and additional features are always welcome! Make sure to run the tests with `python setup.py test` and write your own for new features. Thanks.",
    "description_content_type": "text/markdown; charset=UTF-8",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/LeviBorodenko/garnn",
    "keywords": "",
    "license": "mit",
    "maintainer": "",
    "maintainer_email": "",
    "name": "garnn",
    "package_url": "https://pypi.org/project/garnn/",
    "platform": "any",
    "project_url": "https://pypi.org/project/garnn/",
    "project_urls": {
      "Homepage": "https://github.com/LeviBorodenko/garnn"
    },
    "release_url": "https://pypi.org/project/garnn/1.0.3/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "TensorFlow implementation of the Graphical Attention Recurrent Neural Net Model",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6386946,
  "releases": {
    "0.9.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "29af3d48f503b7628f4895c4751551df289fae2f0b68c10c461ab1db3960feac",
          "md5": "00120e44b3981476f5de2502a9478ac3",
          "sha256": "ed078c97f4f8cccc3ad535a29e0d62fb0b987d83d0e6cad067ea32e99402debd"
        },
        "downloads": -1,
        "filename": "garnn-0.9.2.tar.gz",
        "has_sig": false,
        "md5_digest": "00120e44b3981476f5de2502a9478ac3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22893,
        "upload_time": "2019-12-18T22:24:08",
        "upload_time_iso_8601": "2019-12-18T22:24:08.074982Z",
        "url": "https://files.pythonhosted.org/packages/29/af/3d48f503b7628f4895c4751551df289fae2f0b68c10c461ab1db3960feac/garnn-0.9.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.9.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f6ed991ada9cd9d034e0cba930fd0ac3c233ecd30b96891525090906d777e3e1",
          "md5": "312a26c4ccf6441b18396349fe286fd2",
          "sha256": "2d8889d627c404e29ee3abce4962442ccb066fa8c5e31e085243e7ebd7a8889a"
        },
        "downloads": -1,
        "filename": "garnn-0.9.3.tar.gz",
        "has_sig": false,
        "md5_digest": "312a26c4ccf6441b18396349fe286fd2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22457,
        "upload_time": "2019-12-18T22:24:10",
        "upload_time_iso_8601": "2019-12-18T22:24:10.884714Z",
        "url": "https://files.pythonhosted.org/packages/f6/ed/991ada9cd9d034e0cba930fd0ac3c233ecd30b96891525090906d777e3e1/garnn-0.9.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "14198d19de9cfa5920468b5a574c8550d32e425332912d800286eb34792778af",
          "md5": "af323ca75431e5a7c8f7f7c1ef0f9e5a",
          "sha256": "e3a6f8e82c6bdb30fcbd2571ac934b64a9eb75ce96123c567389054a706d1e80"
        },
        "downloads": -1,
        "filename": "garnn-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "af323ca75431e5a7c8f7f7c1ef0f9e5a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22473,
        "upload_time": "2019-12-18T22:29:53",
        "upload_time_iso_8601": "2019-12-18T22:29:53.016538Z",
        "url": "https://files.pythonhosted.org/packages/14/19/8d19de9cfa5920468b5a574c8550d32e425332912d800286eb34792778af/garnn-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "83f6d00dc7a63ab50c166c934f4dfdba58f04071cc1bf8a469b6b042887f382f",
          "md5": "3dccd1b08834b0045dc8573fdb35674a",
          "sha256": "912d5e9fe1394b5000dba682a3b69be52f63d03f220ea5e954fcc4c5cd1e328b"
        },
        "downloads": -1,
        "filename": "garnn-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3dccd1b08834b0045dc8573fdb35674a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22588,
        "upload_time": "2019-12-29T17:49:40",
        "upload_time_iso_8601": "2019-12-29T17:49:40.701692Z",
        "url": "https://files.pythonhosted.org/packages/83/f6/d00dc7a63ab50c166c934f4dfdba58f04071cc1bf8a469b6b042887f382f/garnn-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2a56be32aaec90c71978bfe32515763fd7b2cce95325f9457986b1bc9da63bd8",
          "md5": "aaf39cbb87ab94c5dd6334f00d7c6539",
          "sha256": "44e587b80eae30ccfe01a48fa33f438d550888b90530aa6270c49a0d01122f27"
        },
        "downloads": -1,
        "filename": "garnn-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "aaf39cbb87ab94c5dd6334f00d7c6539",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24215,
        "upload_time": "2020-01-01T15:13:55",
        "upload_time_iso_8601": "2020-01-01T15:13:55.262760Z",
        "url": "https://files.pythonhosted.org/packages/2a/56/be32aaec90c71978bfe32515763fd7b2cce95325f9457986b1bc9da63bd8/garnn-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0b05ca4c0a2a6e489586c38d0feec42bdab8f40bb65a2f37bb37b6aaef9bea35",
          "md5": "1c6ec201d9f453d0b4790ce23d2797ea",
          "sha256": "81139a6f1252871da1c788a43485f0599e605291590de2a2670f808b756f9ae6"
        },
        "downloads": -1,
        "filename": "garnn-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "1c6ec201d9f453d0b4790ce23d2797ea",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24335,
        "upload_time": "2020-01-02T17:08:34",
        "upload_time_iso_8601": "2020-01-02T17:08:34.008590Z",
        "url": "https://files.pythonhosted.org/packages/0b/05/ca4c0a2a6e489586c38d0feec42bdab8f40bb65a2f37bb37b6aaef9bea35/garnn-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0b05ca4c0a2a6e489586c38d0feec42bdab8f40bb65a2f37bb37b6aaef9bea35",
        "md5": "1c6ec201d9f453d0b4790ce23d2797ea",
        "sha256": "81139a6f1252871da1c788a43485f0599e605291590de2a2670f808b756f9ae6"
      },
      "downloads": -1,
      "filename": "garnn-1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "1c6ec201d9f453d0b4790ce23d2797ea",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 24335,
      "upload_time": "2020-01-02T17:08:34",
      "upload_time_iso_8601": "2020-01-02T17:08:34.008590Z",
      "url": "https://files.pythonhosted.org/packages/0b/05/ca4c0a2a6e489586c38d0feec42bdab8f40bb65a2f37bb37b6aaef9bea35/garnn-1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}