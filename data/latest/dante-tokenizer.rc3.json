{
  "info": {
    "author": "Emanuel Huber",
    "author_email": "emanuel.huber@usp.br",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Twitter Portuguese Tokenizer\nTokenizador de Tweets para o português, voltado ao mercado de ações (Dataset Dante).\n\n## Requerimentos\n\nPython >= 3.7\n\n## Como usar\n\nExecute o seguinte comando para instalar as dependências e instalar o pacote `dante_tokenizer`.\n\n```bash\nmake install\n```\n\nO pacote será adicionado ao seu ambiente python. Para tokenizar textos, basta seguir os seguintes passos:\n\n```python\n>>> from dante_tokenizer import DanteTokenizer\n>>> tokenizer = DanteTokenizer()\n>>> tokenizer.tokenize(\"A DANT3 está em alta!\")\n['A', 'DANT3', 'está', 'em', 'alta', '!']\n```\n\nO método `tokenize` irá retornar uma lista de strings contendo os respectivos tokens detectados. Outros exemplos estão disponíveis na pasta `notebooks`.\n\n# DanteTokenizer\n\nO DanteTokenizer é uma modificação do [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual) do pacote nltk. Adicionando regras para comportar as variedades de tokens existentes nos tweets relacionados ao mercado de ações.\n\nObs: Como a classe atual do TweetTokenizer não permite uma extensão de forma fácil, o código foi extraído e modificado dentro do `dante_tokenizer`.\n\n## Agradecimentos\n\nAo Prof. Thiago A. S. Pardo pela orientação no programa de mestrado, à Profa. Ariani Di Felippo por definir as regras de tokenização e à Dra. Lucelene Lopes pelas contribuições técnicas. \n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/huberemanuel/twitter-portuguese-tokenizer/archive/refs/tags/v0.1-alpha.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/huberemanuel/twitter-portuguese-tokenizer",
    "keywords": "tokenizer,twitter,portuguese",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dante-tokenizer",
    "package_url": "https://pypi.org/project/dante-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/dante-tokenizer/",
    "project_urls": {
      "Download": "https://github.com/huberemanuel/twitter-portuguese-tokenizer/archive/refs/tags/v0.1-alpha.tar.gz",
      "Homepage": "https://github.com/huberemanuel/twitter-portuguese-tokenizer"
    },
    "release_url": "https://pypi.org/project/dante-tokenizer/0.2.0/",
    "requires_dist": [
      "nltk (>=3.6.2)",
      "regex (>=2021.4.4)"
    ],
    "requires_python": "",
    "summary": "",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10656363,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d787f4b2ca00560ab30973359aa03e2ef5940a2107e615f35c09881e280f5e60",
          "md5": "33b9a19139620217384adbafc5154f0f",
          "sha256": "4da00a7269ebb5d50e822a4c47671a36dc449ceb8d9255884c8e5ef85d2bfd60"
        },
        "downloads": -1,
        "filename": "dante-tokenizer-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "33b9a19139620217384adbafc5154f0f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10858,
        "upload_time": "2021-05-02T14:46:04",
        "upload_time_iso_8601": "2021-05-02T14:46:04.249219Z",
        "url": "https://files.pythonhosted.org/packages/d7/87/f4b2ca00560ab30973359aa03e2ef5940a2107e615f35c09881e280f5e60/dante-tokenizer-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "05c3a8a0b82624e5ac9c201c86e90dee6e75a97b8df9a1b39a124a0e13e2d9ac",
          "md5": "721ac71a5c2fbc22485d469aacbaefa2",
          "sha256": "8cf75c7728c344ac824a3ab9d7da93a71c889c06b5da7c2329aa234d11e52af2"
        },
        "downloads": -1,
        "filename": "dante-tokenizer-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "721ac71a5c2fbc22485d469aacbaefa2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11731,
        "upload_time": "2021-05-02T14:55:42",
        "upload_time_iso_8601": "2021-05-02T14:55:42.302810Z",
        "url": "https://files.pythonhosted.org/packages/05/c3/a8a0b82624e5ac9c201c86e90dee6e75a97b8df9a1b39a124a0e13e2d9ac/dante-tokenizer-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "396a93290217a8491c6f668a63a191a09d89c5bbd286b25fbfd16c7f0b9c70f7",
          "md5": "193a13e645f786f381a1390ca5a41e9e",
          "sha256": "76e13c0f912ab655634544ceceb65d303fd4ad30d9899ef0e8eb073a9647db63"
        },
        "downloads": -1,
        "filename": "dante_tokenizer-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "193a13e645f786f381a1390ca5a41e9e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 19852,
        "upload_time": "2021-06-15T20:57:05",
        "upload_time_iso_8601": "2021-06-15T20:57:05.701197Z",
        "url": "https://files.pythonhosted.org/packages/39/6a/93290217a8491c6f668a63a191a09d89c5bbd286b25fbfd16c7f0b9c70f7/dante_tokenizer-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "396a93290217a8491c6f668a63a191a09d89c5bbd286b25fbfd16c7f0b9c70f7",
        "md5": "193a13e645f786f381a1390ca5a41e9e",
        "sha256": "76e13c0f912ab655634544ceceb65d303fd4ad30d9899ef0e8eb073a9647db63"
      },
      "downloads": -1,
      "filename": "dante_tokenizer-0.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "193a13e645f786f381a1390ca5a41e9e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 19852,
      "upload_time": "2021-06-15T20:57:05",
      "upload_time_iso_8601": "2021-06-15T20:57:05.701197Z",
      "url": "https://files.pythonhosted.org/packages/39/6a/93290217a8491c6f668a63a191a09d89c5bbd286b25fbfd16c7f0b9c70f7/dante_tokenizer-0.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}