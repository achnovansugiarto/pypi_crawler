{
  "info": {
    "author": "",
    "author_email": "Kasper Junge <kasperjuunge@gmail.com>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<a href=\"https://github.com/Dansk-Data-Science-Community/finetuners\"><img src=\"melting_face.jpeg\" width=\"250\" align=\"right\" /></a>\n# Finetuners: Reduce cognitive load when finetuning transformers ðŸ¥´\n\nCatchy intro describing the value proposition of the finetuners package.\n\n## Installation\n\n```\npip install finetuners\n```\n\n\n## Example\n\n\n```python\nimport pathlib\n\nfrom finetuners import (\n    FinetunerArguments,\n    FinetunerForTextClassification,\n    FinetunersDataset,\n)\n\n# load dataset\ndataset = FinetunersDataset.from_path(\n    pathlib.Path(__file__).parents[1].joinpath(\"datasets\", \"angry-tweets\")\n)\n\n# define arguments\nargs = FinetunerArguments(\n    model_name=\"awesome_model\",\n    pretrained_model_name_or_path=\"Maltehb/danish-bert-botxo\",\n    training_args={\n        \"output_dir\": \"./runs/\",\n        \"learning_rate\": 5e-5,\n    },\n)\n\n# init finetuner\nfinetuner = FinetunerForTextClassification(\n    dataset=dataset,\n    args=args,\n)\n\n\nfinetuner.finetune()\n\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "nlp,transformers",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "finetuners",
    "package_url": "https://pypi.org/project/finetuners/",
    "platform": null,
    "project_url": "https://pypi.org/project/finetuners/",
    "project_urls": {
      "Homepage": "https://github.com/Dansk-Data-Science-Community/finetuners"
    },
    "release_url": "https://pypi.org/project/finetuners/0.0.1/",
    "requires_dist": [
      "transformers",
      "pandas",
      "pandera",
      "datasets"
    ],
    "requires_python": ">=3.7",
    "summary": "Reduce cognitive load when finetuning transformers ðŸ« ",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15751543,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1d35ee3068f2dae7d1ac1b358df94c1577341321e5a397574a139e7bc69006f9",
          "md5": "78207c08131aef6e9e7e3a9226718361",
          "sha256": "c69e1a5a2c61d5526f3f4ae97542113d93f942a44c575d694628d3df772e6e5b"
        },
        "downloads": -1,
        "filename": "finetuners-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "78207c08131aef6e9e7e3a9226718361",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 7030,
        "upload_time": "2022-11-13T14:47:00",
        "upload_time_iso_8601": "2022-11-13T14:47:00.964646Z",
        "url": "https://files.pythonhosted.org/packages/1d/35/ee3068f2dae7d1ac1b358df94c1577341321e5a397574a139e7bc69006f9/finetuners-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "06387258b9a01d7089b8c2b8ffc600750b53e031e57ac0bb191c1b78d4980ec0",
          "md5": "862377a2d8d44a750f325e17fe15ac8c",
          "sha256": "1a8a33dfafaf301cbbab4bf27bb3996321839e6d2554f5d124bf1bad925714c1"
        },
        "downloads": -1,
        "filename": "finetuners-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "862377a2d8d44a750f325e17fe15ac8c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 5903,
        "upload_time": "2022-11-13T14:47:04",
        "upload_time_iso_8601": "2022-11-13T14:47:04.929580Z",
        "url": "https://files.pythonhosted.org/packages/06/38/7258b9a01d7089b8c2b8ffc600750b53e031e57ac0bb191c1b78d4980ec0/finetuners-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1d35ee3068f2dae7d1ac1b358df94c1577341321e5a397574a139e7bc69006f9",
        "md5": "78207c08131aef6e9e7e3a9226718361",
        "sha256": "c69e1a5a2c61d5526f3f4ae97542113d93f942a44c575d694628d3df772e6e5b"
      },
      "downloads": -1,
      "filename": "finetuners-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "78207c08131aef6e9e7e3a9226718361",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 7030,
      "upload_time": "2022-11-13T14:47:00",
      "upload_time_iso_8601": "2022-11-13T14:47:00.964646Z",
      "url": "https://files.pythonhosted.org/packages/1d/35/ee3068f2dae7d1ac1b358df94c1577341321e5a397574a139e7bc69006f9/finetuners-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "06387258b9a01d7089b8c2b8ffc600750b53e031e57ac0bb191c1b78d4980ec0",
        "md5": "862377a2d8d44a750f325e17fe15ac8c",
        "sha256": "1a8a33dfafaf301cbbab4bf27bb3996321839e6d2554f5d124bf1bad925714c1"
      },
      "downloads": -1,
      "filename": "finetuners-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "862377a2d8d44a750f325e17fe15ac8c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 5903,
      "upload_time": "2022-11-13T14:47:04",
      "upload_time_iso_8601": "2022-11-13T14:47:04.929580Z",
      "url": "https://files.pythonhosted.org/packages/06/38/7258b9a01d7089b8c2b8ffc600750b53e031e57ac0bb191c1b78d4980ec0/finetuners-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}