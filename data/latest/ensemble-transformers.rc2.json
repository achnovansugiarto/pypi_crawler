{
  "info": {
    "author": "Jake Tae",
    "author_email": "jaesungtae@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# Ensemble Transformers\n\nEnsembling Hugging Face Transformers made easy!\n\n## Why Ensemble Transformers?\n\nEnsembling is a simple yet powerful way of combining predictions from different models to increase performance. Since multiple models are used to derive a prediction, ensembling offers a way of decreasing variance and increasing robustness. Ensemble Transformers  provides an intuitive interface for ensembling pretrained models available in Hugging Face [`transformers`](https://huggingface.co/docs/transformers/index).\n\n## Installation\n\nEnsemble Transformers is available on [PyPI](https://pypi.org/project/ensemble-transformers/) and can easily be installed with the `pip` package manager.\n\n```\npip install -U pip wheel\npip install ensemble-transformers\n```\n\nTo try out the latest features, clone this repository and install from source.\n\n```\ngit clone https://github.com/jaketae/ensemble-transformers.git\ncd ensemble-transformers\npip install -e .\n```\n\n## Quickstart\n\nImport an ensemble model class according to your use case, specify the list of backbone models to use, and run training or inference right away.\n\n```python\n>>> from ensemble_transformers import EnsembleModelForSequenceClassification\n>>> ensemble = EnsembleModelForSequenceClassification.from_multiple_pretrained(\n    \"bert-base-uncased\", \"distilroberta-base\", \"xlnet-base-cased\"\n)\n>>> batch = [\"This is a test sentence\", \"This is another test sentence.\"]\n>>> output = ensemble(batch)\n>>> output\nEnsembleModelOutput(\n        logits: [tensor([[ 0.2430, -0.0581],\n        [ 0.2145, -0.0541]], grad_fn=<AddmmBackward0>), tensor([[-0.0094, -0.0117],\n        [-0.0118, -0.0046]], grad_fn=<AddmmBackward0>), tensor([[-0.0962, -1.1581],\n        [-0.2195, -0.7422]], grad_fn=<AddmmBackward0>)],\n)\n>>> stacked_output = ensemble(batch, mean_pool=True)\n>>> stacked_output\nEnsembleModelOutput(\n        logits: tensor([[ 0.0458, -0.4093],\n        [-0.0056, -0.2670]], grad_fn=<SumBackward1>),\n)\n```\n\n## Usage\n\n### Ensembling with Configuration\n\nTo declare an ensemble, first create a configuration object specifying the Hugging Face transformers auto class, as well as the list of models to use to create the ensemble. \n\n```python\nfrom ensemble_transformers import EnsembleConfig, EnsembleModelForSequenceClassification\n\nconfig = EnsembleConfig(\n    \"AutoModelForSequenceClassification\", \n    model_names=[\"bert-base-uncased\", \"distilroberta-base\", \"xlnet-base-cased\"]\n)\n```\n\nThe ensemble model can then be declared via \n\n```python\nensemble = EnsembleModelForSequenceClassification(config)\n```\n\n### Ensembling with `from_multiple_pretrained`\n\nA more convenient way of declaring an ensemble is via `from_multiple_pretrained`, a method similar to `from_pretrained` in Hugging Face transformers. For instance, to perform text classification, we can use the `EnsembleModelForSequenceClassification` class.\n\n```python\nfrom ensemble_transformers import EnsembleModelForSequenceClassification\n\nensemble = EnsembleModelForSequenceClassification.from_multiple_pretrained(\n    \"bert-base-uncased\", \"distilroberta-base\", \"xlnet-base-cased\"\n)\n```\n\nUnlike Hugging Face transformers, which requires users to explicitly declare and initialize a preprocessor (e.g. `tokenizer`, `feature_extractor`, or `processor`) separate from the model, Ensemble Transformers automatically detects the preprocessor class and holds it within the `EnsembleModelForX` class as an internal attribute. Therefore, you do not have to declare a preprocessor yourself; Ensemble Transformers will do it for you.\n\nIn the example below, we see that the `ensemble` object correctly holds 3 tokenizers for each model.\n\n```python\n>>> len(ensemble.preprocessors)\n3\n>>> ensemble.preprocessors\n[PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), PreTrainedTokenizerFast(name_or_path='distilroberta-base', vocab_size=50265, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}), PreTrainedTokenizerFast(name_or_path='xlnet-base-cased', vocab_size=32000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '<sep>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False), 'additional_special_tokens': ['<eop>', '<eod>']})]\n```\n\n### Heterogenous Modality\n\nFor the majority of use cases, it does not make sense to ensemble models from different modalities, e.g., a language model and an image model. As mentioned, Ensemble Transformers will auto-detect the modality of each model and prevent unintended mixing of models.\n\n```python\n>>> from ensemble_transformers import EnsembleConfig\n>>> config = EnsembleConfig(\"AutoModelForSequenceClassification\", model_names=[\"bert-base-uncased\", \"google/vit-base-patch16-224-in21k\"])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/jaketae/Documents/Dev/github/ensemble-transformers/ensemble_transformers/config.py\", line 37, in __init__\n    raise ValueError(\"Cannot ensemble models of different modalities.\")\nValueError: Cannot ensemble models of different modalities.\n```\n\n### Loading Across Devices\n\nBecause ensembling involves multiple models, it is often impossible to load all models onto a single device. To alleviate memory requirements, Ensemble Transformers offers a way of distributing models across different devices. For instance, say you have access to multiple GPU cards and want to load each model onto different GPUs. This can easily be achieved by the following line.\n\n```python\nensemble.to_multiple(\n    [\"cuda:0\", \"cuda:1\", \"cuda:2\"]\n)\n```\n\nThe familiar `to(device)` method is also supported, and it loads all models onto the same device.\n\n```python\nensemble.to(\"cuda\")\n```\n\n### Forward Propagation\n\nTo run forward propagation, simply pass a batch of raw input to the ensemble. In the case of language models, this is just a batch of text.\n\n```python\n>>> batch = [\"This is a test sentence\", \"This is another test sentence.\"]\n>>> output = ensemble(batch)\n>>> output\nEnsembleModelOutput(\n        logits: [tensor([[ 0.2430, -0.0581],\n        [ 0.2145, -0.0541]], grad_fn=<AddmmBackward0>), tensor([[-0.0094, -0.0117],\n        [-0.0118, -0.0046]], grad_fn=<AddmmBackward0>), tensor([[-0.0962, -1.1581],\n        [-0.2195, -0.7422]], grad_fn=<AddmmBackward0>)]\n)\n>>> output.outputs\n[SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1681, -0.3470],\n        [ 0.1573, -0.1571]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1388, -0.0711],\n        [ 0.1429, -0.0841]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None), XLNetForSequenceClassificationOutput(loss=None, logits=tensor([[0.5506, 0.1506],\n        [0.4308, 0.1397]], grad_fn=<AddmmBackward0>), mems=(tensor([[[ 0.0344,  0.0202,  0.0261,  ..., -0.0175, -0.0343,  0.0252],\n         [-0.0281, -0.0198, -0.0387,  ..., -0.0420, -0.0160, -0.0253]],\n       ...,\n        [[ 0.2468, -0.4007, -1.0839,  ..., -0.2943, -0.3944,  0.0605],\n         [ 0.1970,  0.2106, -0.1448,  ..., -0.6331, -0.0655,  0.7427]]])), hidden_states=None, attentions=None)]\n```\n\nBy default, the ensemble returns a `EnsembleModelOutput` instance, which contains all the outputs from each model. The raw outputs from each model is accessible via the `.outputs` field. The `EnsembleModelOutput` class also scans across each of the raw output and collects common keys. In the example above, all model outputs contained a `.logits` field, which is why it appears as a field in the `output` instance.\n\nWe can also stack or mean-pool the output of each model by toggling `mean_pool=True` in the forward call.\n\n```python\n>>> stacked_output = ensemble(batch, mean_pool=True)\n>>> stacked_output\nEnsembleModelOutput(\n        logits: tensor([[ 0.0458, -0.4093],\n        [-0.0056, -0.2670]], grad_fn=<SumBackward1>),\n)\n```\n\nIf the models are spread across different devices, the result is collected in `main_device`, which defaults to the CPU.\n\n### Preprocessor Arguments\n\nPreprocessors accept a number of optional arguments. For instance, for simple batching, `padding=True` is used. Moreover, PyTorch models require `return_tensors=\"pt\"`. Ensemble Transformers already ships with minimal, sensible defaults so that it works out-of-the-box. However, for more custom behavior, you can modify the `preprocessor_kwargs` argument. The example below demonstrates how to use TensorFlow language models without padding.\n\n```python\nensemble(batch, preprocessor_kwargs={\"return_tensors\": \"tf\", \"padding\": False})\n```\n\n## Contributing\n\nThis repository is under active development. Any and all issues and pull requests are welcome. If you would prefer, feel free to reach out to me at jaesungtae@gmail.com.\n\n## License\n\nReleased under the [MIT License](LICENSE).\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/jaketae/ensemble-transformers",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ensemble-transformers",
    "package_url": "https://pypi.org/project/ensemble-transformers/",
    "platform": null,
    "project_url": "https://pypi.org/project/ensemble-transformers/",
    "project_urls": {
      "Homepage": "https://github.com/jaketae/ensemble-transformers"
    },
    "release_url": "https://pypi.org/project/ensemble-transformers/0.0.2/",
    "requires_dist": [
      "torch",
      "transformers"
    ],
    "requires_python": ">=3.7",
    "summary": "Ensembling Hugging Face Transformers made easy",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16203535,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1a70d282799a29e3ea13297329448c9229027c4983a0fd78839f9eac6c3acb96",
          "md5": "d05c9b518da21058e9636b8493050ec1",
          "sha256": "594d5ae34277e8ae62cd424f188c60ebdc070b719722f8c97da6e0090ed452ba"
        },
        "downloads": -1,
        "filename": "ensemble_transformers-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d05c9b518da21058e9636b8493050ec1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 2646,
        "upload_time": "2022-03-28T07:15:29",
        "upload_time_iso_8601": "2022-03-28T07:15:29.888234Z",
        "url": "https://files.pythonhosted.org/packages/1a/70/d282799a29e3ea13297329448c9229027c4983a0fd78839f9eac6c3acb96/ensemble_transformers-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4b7e11bfdf468de1c3f762f943b130a875275ac3504abed3412a8dc816b2b76b",
          "md5": "7a6f97b967639418d054224342b5352b",
          "sha256": "7eeb05b90755bb23543d2c7c62486e166ab15ef3dd0c48af97afdc9d4765c4d5"
        },
        "downloads": -1,
        "filename": "ensemble-transformers-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7a6f97b967639418d054224342b5352b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 2097,
        "upload_time": "2022-03-28T07:15:31",
        "upload_time_iso_8601": "2022-03-28T07:15:31.786093Z",
        "url": "https://files.pythonhosted.org/packages/4b/7e/11bfdf468de1c3f762f943b130a875275ac3504abed3412a8dc816b2b76b/ensemble-transformers-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "189d7bfcdf74eb82fa9efbf76a4e5e82969e03755ce778731b7230657c7b3864",
          "md5": "8445b1acc7210f0601d5d9a54d4172f6",
          "sha256": "6a0668dddc812ec5f411d0045ac5a04ad1280b37a0bac02de74cee59dddbf70f"
        },
        "downloads": -1,
        "filename": "ensemble_transformers-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8445b1acc7210f0601d5d9a54d4172f6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 9063,
        "upload_time": "2022-12-24T05:49:34",
        "upload_time_iso_8601": "2022-12-24T05:49:34.692969Z",
        "url": "https://files.pythonhosted.org/packages/18/9d/7bfcdf74eb82fa9efbf76a4e5e82969e03755ce778731b7230657c7b3864/ensemble_transformers-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63b04620dcbc68d09be1bf12ad89102e81a125d4874d12200c9547973bef405a",
          "md5": "bcddfe32ba8a274ffa27c0f1b1c25757",
          "sha256": "30c22272ef68222f9befe44cddc8fb7cb60c31202cab6816428c61b4f7625775"
        },
        "downloads": -1,
        "filename": "ensemble-transformers-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "bcddfe32ba8a274ffa27c0f1b1c25757",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 7706,
        "upload_time": "2022-12-24T05:49:36",
        "upload_time_iso_8601": "2022-12-24T05:49:36.240939Z",
        "url": "https://files.pythonhosted.org/packages/63/b0/4620dcbc68d09be1bf12ad89102e81a125d4874d12200c9547973bef405a/ensemble-transformers-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "189d7bfcdf74eb82fa9efbf76a4e5e82969e03755ce778731b7230657c7b3864",
        "md5": "8445b1acc7210f0601d5d9a54d4172f6",
        "sha256": "6a0668dddc812ec5f411d0045ac5a04ad1280b37a0bac02de74cee59dddbf70f"
      },
      "downloads": -1,
      "filename": "ensemble_transformers-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8445b1acc7210f0601d5d9a54d4172f6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 9063,
      "upload_time": "2022-12-24T05:49:34",
      "upload_time_iso_8601": "2022-12-24T05:49:34.692969Z",
      "url": "https://files.pythonhosted.org/packages/18/9d/7bfcdf74eb82fa9efbf76a4e5e82969e03755ce778731b7230657c7b3864/ensemble_transformers-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "63b04620dcbc68d09be1bf12ad89102e81a125d4874d12200c9547973bef405a",
        "md5": "bcddfe32ba8a274ffa27c0f1b1c25757",
        "sha256": "30c22272ef68222f9befe44cddc8fb7cb60c31202cab6816428c61b4f7625775"
      },
      "downloads": -1,
      "filename": "ensemble-transformers-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "bcddfe32ba8a274ffa27c0f1b1c25757",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 7706,
      "upload_time": "2022-12-24T05:49:36",
      "upload_time_iso_8601": "2022-12-24T05:49:36.240939Z",
      "url": "https://files.pythonhosted.org/packages/63/b0/4620dcbc68d09be1bf12ad89102e81a125d4874d12200c9547973bef405a/ensemble-transformers-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}