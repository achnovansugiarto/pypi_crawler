{
  "info": {
    "author": "magic-cta-pipe developers",
    "author_email": "alessioberti90@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# magic-cta-pipe\n\nRepository for the analysis of MAGIC and MAGIC+LST1 data, based on [*ctapipe*](https://github.com/cta-observatory/ctapipe).\n\n* Code: https://github.com/cta-observatory/magic-cta-pipe\n\nv0.2.0 of *magic-cta-pipe* provides all the functionalities to perform a MAGIC+LST-1 or a MAGIC-only analysis. Both types of analyses can be performed using the scripts within the *lst1_magic* folder. See the [README](https://github.com/cta-observatory/magic-cta-pipe/blob/master/magicctapipe/scripts/lst1_magic/README.md) for more details on how to run the analysis.\n\n# Installation for users\n\n*magic-cta-pipe* and its dependencies may be installed using the *Anaconda* or *Miniconda* package system. We recommend creating a conda virtual environment\nfirst, to isolate the installed version and dependencies from your master environment (this is optional).\n\nThe following command will set up a conda virtual environment, add the necessary package channels, and install *magic-cta-pipe* and its dependencies::\n\n    git clone https://github.com/cta-observatory/magic-cta-pipe.git\n    cd magic-cta-pipe\n    conda env create -n magic-lst1 -f environment.yml\n    conda activate magic-lst1\n    pip install .\n\n# Instructions for developers\n\nPeople who would like to join the development of *magic-cta-pipe*, please contact Alessio Berti (<alessioberti90@gmail.com>) to get write access to the repository.\n\nDevelopers should follow the coding style guidelines of the *ctapipe* project, see https://ctapipe.readthedocs.io/en/latest/development/style-guide.html and https://ctapipe.readthedocs.io/en/latest/development/code-guidelines.html.\n\nIn short, to check for code/style errors and for reformatting the code:\n\n```\npip install hacking     # installs all checker tools\npip install black       # installs black formatter\npyflakes magicctapipe   # checks for code errors\nflake8 magicctapipe     # checks style and code errors\nblack filename.py       # reformats filename.py with black\n```\n\nIn general, if you want to add a new feature or fix a bug, please open a new issue, and then create a new branch to develop the new feature or code the bug fix. You can create an early pull request even if it is not complete yet, you can tag it as \"Draft\" so that it will not be merged, and other developers can already check it and provide comments. When the code is ready, remove the tag \"Draft\" and select two people to review the pull request (at the moment the merge is not blocked if no review is performed, but that may change in the future). When the review is complete, the branch will be merged into the main branch.\n\n<!--\nA brief description:\n1. `config/CrabNebula.yaml`: an example of the configuration file, used by all the scripts.\n2. `config/magic-cta-pipe_config_stereo.yaml`: an example of the configuration file for stereo analysis.\n3. `hillas_preprocessing.py`: compute the hillas parameters. Loops over MCs and real data. This script uses the tailcuts cleaning.\n4. `hillas_preprocessing_stereo.py`: compute the hillas and stereo parameters. Loops over MCs and real data. This script uses the tailcuts cleaning.\n5. `hillas_preprocessing_MAGICCleaning.py`: compute the hillas parameters. Loops over MCs and real data. This script used the MAGIC cleaning implemented in MARS.\n6. `hillas_preprocessing_MAGICCleaning_stereo.py`: compute the hillas and stereo parameters. Loops over MCs and real data. This script used the MAGIC cleaning implemented in MARS.\n7. `train_energy_rf.py`: trains the energy RF.\n8. `train_direction_rf.py`: trains the direction \"disp\" RF.\n9. `train_classifier_rf.py`: trains the event classification RF.\n10. `apply_rfs.py`: applies the trained RFs to the \"test\" event sample.\n11. `add_orig_mc_tree.py`: adds the \"original MC\" tree info to the MC events tree processed earlier.\n12. `make_irf.py`: generates IRFs based on the event lists with reconstructed parameters.\n13. `make_event_lists.py`: produces the FITS event lists with application of the cuts.\n\nMoreover, the `utils` directory contains two modules:\n* `MAGIC_Badpixels.py`: finds the so called bad/hot pixels i.e. pixels affected by stars, or pixels turned off or dead.\n* `MAGIC_Cleaning.py`: implements the MAGIC cleaning as defined in MARS.\n\nThere is also an IPython notebook, `magic_lst_event_coincidence.ipynb`, which shows how to perform the coincidence of events between MAGIC and LST1 data, when data are taken by both systems.\n\nHere below you can find a more detailed description of the pipeline work flow.\n\n### Configuration file CrabNebula.yaml ###\n\nThis is an example of the configuration file which is used by all the scripts of the pipeline.\nIt is in [YAML](https://yaml.org/) standard, which can be easily parsed and also easily readable by humans.\nThrough this file, the user can configure the details of the analysis like input files, output files, details\nof the cleaning and Random Forest generation and analysis cuts to be applied to the events.\n\nMore in detail, the configuration file is a series of main keys, each having other nested (key, value) pairs.\nThe main keys are:\n\n* `data_files`\n* `image_cleaning`\n* `energy_rf`\n* `direction_rf`\n* `classifier_rf`\n* `irf`\n* `event_list`\n\n`data_files` specifies the input and output files, both for simulated (MonteCarlo) and real data, denoted by the `mc`\nand `data` keys. Each set of data is has a `train_sample` and `test_sample` keys. For simulated data, the `train_sample`\nkey refers to the simulated data sample used for the training of the Random Forest classifier, whereas the `test_sample`\nis the sample used to compute the Instrument Response Functions (IRFs). For real data, the `train_sample` is what usually\nis called OFF data, which are used together with simulated data in the Random Forest algorithm, while the `test_sample` refers\nto the so called ON data, that is the data the user wants to analyze.\nEach `train_sample` and `test_sample` keys have two sub-keys, called `magic1` and `magic2`. As their name implies, the input and\noutput files are specified for each telescope independently, since the pipeline starts its processing from MAGIC calibrated data.\nIf the analysis uses data from a third telescope, as LST1, an additional key called, for example, `lst1` can be added to specify the\ninput and output files. For the moment though, the pipeline works with MAGIC data only.\nEach telescope key is used to specify the input and output files at different stages of the pipeline:\n\n* `input_mask`: it specifies the input files to the pipeline; absolute and relative paths can be used; wildcards are allowed;\n* `hillas_output`: it specifies the name of the output file of the script `hillas_preprocessing.py`;\n* `reco_output`: it specifies the name of the output file after applying the Random Forests to the data. **NB:** this key must be set only for the `test_sample` data, either simulated or real.\n\nThe `image_cleaning` key is used to specify the cleaning parameters. In particular, since for both MAGIC telescopes the cleaning settings\nare the same, only one key called `magic` is used. As for `data_files`, when in the future LST1 will be added in the analysis, an additional\nkey should be added to specify the cleaning settings for that telescope.\n\nThe `energy_rf`, `direction_rf` and `classifier_rf` keys specify the settings used for the each type of Random Forest used in the analysis.\nEach of these keys have other sub-keys:\n\n* `save_name` is the name of the output file for the specific Random Forest\n* `cuts` is a string to be applied on the input data to the Random Forests\n* `settings` is a set of keys specifying the settings for each Random Forest e.g. the number of estimators, the minimum number of events in each leaf and the number of jobs\n* `features` is a list of strings specifying the parameters to be used in the Random Forests training. **NB:** for the `direction_rf` key, `features` is actually a dictionary with two keys, `disp` and `pos_angle_shift`. For each of those keys, a list is used to specify the parameters to be used for each of those Random Forests.\n\nThe `irf` key has only one sub-key, called `output_name`, which is the name (plus path) of the file where IRF will be stored in FITS format.\n\nFinally, the `event_list` key is used to specify some cuts, `quality` or user `selection` cuts.\n\n### Configuration file magic-cta-pipe\\_config\\_stereo.yaml ###\n\nThis configuration file is very similar to the previous one, but it should be used when stereo analysis has to be performed. In particular, what changes wrt\n`CrabNebula.yaml` is that there is only one telescope name key, namely `magic`. This is because the input mask in this case will specify data from both\nM1 and M2 to allow for stereo reconstruction.\n\n### hillas\\_preprocessing.py ###\n\nThe first script to run the pipeline is `hillas_preprocessing.py`. It takes calibrated files (both simulated and real data) as input and processes them:\n\n* it performs the image cleaning\n* it calculates the Hillas parameters (using the `ctapipe.image.hillas_parameters` and `ctapipe.image.leakage` functions)\n* it computes the timing parameters (using the `ctapipe.image.timing_parameters.timing_parameters` function)\n\nThe settings of the cleaning, as well as the input and output files of the script, are specified in the configuration file. The format of the output files\nis HDF5.\n\nFor MAGIC data, its reading is performed through the [`ctapipe_io_magic`](https://gitlab.mpcdf.mpg.de/ievo/ctapipe_io_magic) module. It defines the class\n`MAGICEventSource`, which inherits from the [`EventSource`](https://cta-observatory.github.io/ctapipe/api/ctapipe.io.EventSource.html) class defined in `ctapipe`,\nused to setup classes to read different sources of data.\n\nRunning the script is straightforward:\n\n```bash\n$ python hillas_preprocessing.py --config=config.yaml\n```\n\nwhere `config.yaml` is the name of the configuration file.\n\nOther available options are:\n* `--usereal`: run the script only over real data\n* `--usemc`: run the script only over MC data\n* `--usetest`: run the script only over test sample data\n* `--usetrain`: run the script only over train sample data\n* `--usem1`: run the script only over M1 data\n* `--usem2`: run the script only over M2 data\n\nThese options can be concatenated, e.g.:\n\n```bash\n$ python hillas_preprocessing.py --config=config.yaml --usereal --usetest --usem1\n```\n\nwill run the script over real data from the test sample and from the M1 telescope only.\n\nThe next step in the pipeline is training the Random Forests for event classification, energy and direction reconstruction.\n\n### hillas\\_preprocessing\\_MAGICCleaning.py ###\n\nIt is similar to `hillas_preprocessing.py`, the only difference is that it uses the MAGIC cleaning implemented in MARS. Its usage is the same as `hillas_preprocessing.py`, see above.\n\n### hillas\\_preprocessing\\_stereo.py and hillas\\_preprocessing\\_MAGICCleaning\\_stereo.py ###\n\nThese script are very similar to `hillas_preprocessing.py` and `hillas_preprocessing_MAGICCleaning.py`, but they include also the reconstruction of stereo parameters.\n\nRunning the scripts is straightforward, e.g.:\n\n```bash\n$ python hillas_preprocessing_stereo.py --config=config_stereo.yaml\n```\n\nwhere `config_stereo.yaml` is the name of the configuration file, the proper one for stereo analysis.\n\nOther available options are:\n* `--usereal`: run the script only over real data\n* `--usemc`: run the script only over MC data\n* `--usetest`: run the script only over test sample data\n* `--usetrain`: run the script only over train sample data\n\n### train\\_energy\\_rf.py, train\\_direction\\_rf.py, train\\_classifier\\_rf.py ###\n\nThese scripts take care of training different Random Forests with different purposes:\n\n* `train_energy_rf.py` trains the Random Forest for the energy reconstruction\n* `train_direction_rf.py` trains the Random Forest for the event direction reconstruction\n* `train_classifier_rf.py` trains the Random Forest for the event classification\n\n`train_energy_rf.py` and `train_direction_rf.py` run on simulated data from both the train and test sample. `train_classifier_rf.py`\ninstead runs on the test sample of simulated data and on OFF data.\n\nEach scripts saves some performance summary plots as PNG images:\n\n* `train_energy_rf.py` saves the energy migration matrix and the energy bias and RMS\n* `train_direction_rf.py` saves the histogram of theta2 and the PSF as a function of the energy and offset distance\n* `train_classifier_rf.py` saves the event classification histograms\n\nTo run these scripts, taking as example `train_energy_rf.py`, just do:\n\n```bash\n$ python train_energy_rf.py --config=config.yaml\n```\n\nIf you want to run these three scripts over DL1 data containing the stereo information, i.e. generated by `hillas_preprocessing_stereo.py` or `hillas_preprocessing_MAGICCleaning_stereo.py`, you need to add the `--stereo` option when calling them from the command line.\n\nOnce the Random Forests are trained, they can be applied to the data. Before this step, another one must be performed using the script\n`add_orig_mc_tree.py`, described in the following paragraph.\n\n### add\\_orig\\_mc\\_tree.py ###\n\nThe script `add_orig_mc_tree.py` opens the calibrated simulated files (for both train and test samples) to read the `OriginalMC` tree,\ncontaining the information about the simulated values for each event (e.g. energy, arrival direction of the events).\nThe information is then copied to the output files created by `hillas_preprocessing.py`.\n\nRun this script with the command:\n\n```bash\n$ python add_orig_mc_tree.py --config=config.yaml\n```\n\nOther available options are:\n* `--usetest`: run the script only over test sample data\n* `--usetrain`: run the script only over train sample data\n* `--usem1`: run the script only over M1 data\n* `--usem2`: run the script only over M2 data\n* `--stereo`: run over DL1 data containing stereo information (i.e. generated by `hillas_preprocessing_stereo.py` or `hillas_preprocessing_MAGICCleaning_stereo.py`)\n\nAfter this step, the Random Forests can be applied to the ON data and simulated data (test sample).\n\n### apply\\_rfs.py ###\n\nThe script `apply_rfs.py` is responsible for applying the trained Random Forests (energy, event direction and classification) to the ON\nand the test sample of simulated data, reconstructing the properties of the events. The result of the reconstruction is saved in a HDF5\noutput file, one for the ON and one for the simulated data, as specified by the `reco_output` keys of the configuration file.\n\nTo run the script, just do:\n\n```bash\n$ python apply_rfs.py --config=config.yaml\n```\n\nIf you want to run the script over DL1 data containing the stereo information, i.e. generated by `hillas_preprocessing_stereo.py` or `hillas_preprocessing_MAGICCleaning_stereo.py`, you need to add the `--stereo` option when calling them from the command line.\n\n### make\\_irf.py ###\n\nThe script `make_irf.py` generates the instrument response functions (IRFs) starting from the test sample of simulated data, after the Random\nForests have been applied to them. The result is a FITS file containing the following tables (the names are self-explanatory):\n\n* `POINT SPREAD FUNCTION`\n* `ENERGY DISPERSION`\n* `EFFECTIVE AREA`\n\nFor the time being, the name of the reconstructed test sample simulated data file and of the output FITS file is hardcoded in the script, but\nit will be changed in the future so that they can be set with the YAML configuration file. In any case, the script needs the configuration file\nto be passed as command line argument:\n\n```bash\n$ python make_irf.py --config=config.yaml\n```\n\nIf you run the script `apply_rfs.py` with the `--stereo` option, then also `make_irf.py` should be called with the `--stereo` option.\n\n\n### make\\_event\\_lists.py ###\n\n`make_event_lists.py` is the last script of the pipeline and is responsible of creating an event list. First, a list of good time intervals (GTI)\nis created (applying the cuts specified in the configuration file), then event information (ID, time, sky coordinates and reconstructed energy) are\nextracted. The GTI and the event information are used to create two tables in the resulting FITS files: for each MAGIC run, a FITS file is generated.\n\nTo run this script:\n\n```bash\n$ python make_event_lists.py --config=config.yaml\n```\n\nIf you used the `--stereo` option for the previous scripts, then also `make_event_lists.py` should be called with the `--stereo` option.\n-->\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/cta-observatory/magic-cta-pipe",
    "keywords": "",
    "license": "BSD 3-Clause License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "magic-cta-pipe",
    "package_url": "https://pypi.org/project/magic-cta-pipe/",
    "platform": null,
    "project_url": "https://pypi.org/project/magic-cta-pipe/",
    "project_urls": {
      "Homepage": "https://github.com/cta-observatory/magic-cta-pipe"
    },
    "release_url": "https://pypi.org/project/magic-cta-pipe/0.3.0/",
    "requires_dist": [
      "astropy (<5,>=4.0.5)",
      "lstchain (~=0.9.6)",
      "ctapipe (~=0.12.0)",
      "ctapipe-io-magic (~=0.4.7)",
      "ctaplot (~=0.5.5)",
      "eventio (<2.0.0a0,>=1.5.1)",
      "gammapy (~=0.19.0)",
      "uproot (~=4.1)",
      "h5py",
      "ipykernel (~=6.4.0)",
      "joblib",
      "matplotlib (>=3.5)",
      "numba",
      "numpy",
      "pandas",
      "pyirf (~=0.6.0)",
      "scipy",
      "seaborn",
      "scikit-learn",
      "tables",
      "toml",
      "traitlets (~=5.0.5)",
      "setuptools-scm",
      "pytest ; extra == 'all'",
      "pandas (>=0.24.0) ; extra == 'all'",
      "sphinx ; extra == 'all'",
      "sphinx-automodapi ; extra == 'all'",
      "sphinx-argparse ; extra == 'all'",
      "sphinx-rtd-theme ; extra == 'all'",
      "numpydoc ; extra == 'all'",
      "nbsphinx ; extra == 'all'",
      "importlib-resources ; (python_version < \"3.9\") and extra == 'all'",
      "sphinx ; extra == 'docs'",
      "sphinx-automodapi ; extra == 'docs'",
      "sphinx-argparse ; extra == 'docs'",
      "sphinx-rtd-theme ; extra == 'docs'",
      "numpydoc ; extra == 'docs'",
      "nbsphinx ; extra == 'docs'",
      "pytest ; extra == 'tests'",
      "pandas (>=0.24.0) ; extra == 'tests'",
      "importlib-resources ; (python_version < \"3.9\") and extra == 'tests'"
    ],
    "requires_python": "",
    "summary": "",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16648265,
  "releases": {
    "0.1.4.post2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ee1bb422a14d1cdd6663f96d5cc855e1384ba05421d8621057b457312dbd9ad8",
          "md5": "67b19a499bc4a09066a8da6fe95b6207",
          "sha256": "d71ec9e692f0cc20ae128755b46adb5b5fa6dd3850c333d1f489e14667bd3943"
        },
        "downloads": -1,
        "filename": "magic_cta_pipe-0.1.4.post2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "67b19a499bc4a09066a8da6fe95b6207",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 65666,
        "upload_time": "2022-01-17T13:58:44",
        "upload_time_iso_8601": "2022-01-17T13:58:44.876426Z",
        "url": "https://files.pythonhosted.org/packages/ee/1b/b422a14d1cdd6663f96d5cc855e1384ba05421d8621057b457312dbd9ad8/magic_cta_pipe-0.1.4.post2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d072e1b4fa1b353d1d0f57352d744fcf86dea072ab9a784e21ebfb36100a46f0",
          "md5": "d9f6651cbf9b302b9f295d155a40bde7",
          "sha256": "14f20f3e57762759bbceb49d8520f5c281f514e83393c00d355d9cb769c29865"
        },
        "downloads": -1,
        "filename": "magic-cta-pipe-0.1.4.post2.tar.gz",
        "has_sig": false,
        "md5_digest": "d9f6651cbf9b302b9f295d155a40bde7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 785686,
        "upload_time": "2022-01-17T13:58:47",
        "upload_time_iso_8601": "2022-01-17T13:58:47.047027Z",
        "url": "https://files.pythonhosted.org/packages/d0/72/e1b4fa1b353d1d0f57352d744fcf86dea072ab9a784e21ebfb36100a46f0/magic-cta-pipe-0.1.4.post2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "99ed7822778664a3c6b59599c636b8bb0eaeb2fd1ac2e06843a032d8b905e6e6",
          "md5": "55f43d05e4ef8842e379e5fc6fb51e74",
          "sha256": "980cd9803b80fe2a0c914df5607ea80fea6b7d9337afcecd5e8d8743215dd356"
        },
        "downloads": -1,
        "filename": "magic_cta_pipe-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "55f43d05e4ef8842e379e5fc6fb51e74",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 121738,
        "upload_time": "2022-11-01T22:01:04",
        "upload_time_iso_8601": "2022-11-01T22:01:04.628026Z",
        "url": "https://files.pythonhosted.org/packages/99/ed/7822778664a3c6b59599c636b8bb0eaeb2fd1ac2e06843a032d8b905e6e6/magic_cta_pipe-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dd2dfd13638137d7473a8b4a67f7ab87c71f9398c42528220a75aec1382c2a3b",
          "md5": "a98e1bb4f128532112e87f02594f4a88",
          "sha256": "59b3f3cd01f2dcc17fa98b472c1e1992681bad8cd6790af13f2f790e2e5e7969"
        },
        "downloads": -1,
        "filename": "magic-cta-pipe-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a98e1bb4f128532112e87f02594f4a88",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 2619381,
        "upload_time": "2022-11-01T22:01:19",
        "upload_time_iso_8601": "2022-11-01T22:01:19.207719Z",
        "url": "https://files.pythonhosted.org/packages/dd/2d/fd13638137d7473a8b4a67f7ab87c71f9398c42528220a75aec1382c2a3b/magic-cta-pipe-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb8f732d2e4efd62a2103bdfb99081dff4e78512c1416a0bf73c49e8da744dd8",
          "md5": "725929bc6611dd6ab4ccb38af04e58a2",
          "sha256": "f0e2541c424947f15fed5f9b2057b080bf0c053f0dc8536f11f8c1ced09c2b53"
        },
        "downloads": -1,
        "filename": "magic_cta_pipe-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "725929bc6611dd6ab4ccb38af04e58a2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 122108,
        "upload_time": "2023-02-01T09:06:51",
        "upload_time_iso_8601": "2023-02-01T09:06:51.489738Z",
        "url": "https://files.pythonhosted.org/packages/bb/8f/732d2e4efd62a2103bdfb99081dff4e78512c1416a0bf73c49e8da744dd8/magic_cta_pipe-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b7bafda840958b40047d6bd6dad0018386c97d5895f066b69ce61e23829e6afe",
          "md5": "e99bed1a832c67df8ac017ad0aaad6d5",
          "sha256": "07239f076d9be6c632c1a599df92642f2159db8f65e25a08680d87005c016287"
        },
        "downloads": -1,
        "filename": "magic-cta-pipe-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "e99bed1a832c67df8ac017ad0aaad6d5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 2622559,
        "upload_time": "2023-02-01T09:06:54",
        "upload_time_iso_8601": "2023-02-01T09:06:54.100434Z",
        "url": "https://files.pythonhosted.org/packages/b7/ba/fda840958b40047d6bd6dad0018386c97d5895f066b69ce61e23829e6afe/magic-cta-pipe-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bb8f732d2e4efd62a2103bdfb99081dff4e78512c1416a0bf73c49e8da744dd8",
        "md5": "725929bc6611dd6ab4ccb38af04e58a2",
        "sha256": "f0e2541c424947f15fed5f9b2057b080bf0c053f0dc8536f11f8c1ced09c2b53"
      },
      "downloads": -1,
      "filename": "magic_cta_pipe-0.3.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "725929bc6611dd6ab4ccb38af04e58a2",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 122108,
      "upload_time": "2023-02-01T09:06:51",
      "upload_time_iso_8601": "2023-02-01T09:06:51.489738Z",
      "url": "https://files.pythonhosted.org/packages/bb/8f/732d2e4efd62a2103bdfb99081dff4e78512c1416a0bf73c49e8da744dd8/magic_cta_pipe-0.3.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b7bafda840958b40047d6bd6dad0018386c97d5895f066b69ce61e23829e6afe",
        "md5": "e99bed1a832c67df8ac017ad0aaad6d5",
        "sha256": "07239f076d9be6c632c1a599df92642f2159db8f65e25a08680d87005c016287"
      },
      "downloads": -1,
      "filename": "magic-cta-pipe-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "e99bed1a832c67df8ac017ad0aaad6d5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 2622559,
      "upload_time": "2023-02-01T09:06:54",
      "upload_time_iso_8601": "2023-02-01T09:06:54.100434Z",
      "url": "https://files.pythonhosted.org/packages/b7/ba/fda840958b40047d6bd6dad0018386c97d5895f066b69ce61e23829e6afe/magic-cta-pipe-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}