{
  "info": {
    "author": "HuggingFace Inc.",
    "author_email": "thomas@huggingface.co",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "Simple check list for release from AllenNLP repo: https://github.com/allenai/allennlp/blob/master/setup.py\n\nTo create the package for pypi.\n\n1. Change the version in __init__.py, setup.py as well as docs/source/conf.py.\n\n2. Commit these changes with the message: \"Release: VERSION\"\n\n3. Add a tag in git to mark the release: \"git tag VERSION -m'Adds tag VERSION for pypi' \"\n   Push the tag to git: git push --tags origin master\n\n4. Build both the sources and the wheel. Do not change anything in setup.py between\n   creating the wheel and the source distribution (obviously).\n\n   For the wheel, run: \"python setup.py bdist_wheel\" in the top level directory.\n   (this will build a wheel for the python version you use to build it).\n\n   For the sources, run: \"python setup.py sdist\"\n   You should now have a /dist directory with both .whl and .tar.gz source versions.\n\n5. Check that everything looks correct by uploading the package to the pypi test server:\n\n   twine upload dist/* -r pypitest\n   (pypi suggest using twine as other methods upload files via plaintext.)\n   You may have to specify the repository url, use the following command then:\n   twine upload dist/* -r pypitest --repository-url=https://test.pypi.org/legacy/\n\n   Check that you can install it in a virtualenv by running:\n   pip install -i https://testpypi.python.org/pypi nlp\n\n6. Upload the final version to actual pypi:\n   twine upload dist/* -r pypi\n\n7. Copy the release notes from RELEASE.md to the tag in github once everything is looking hunky-dory.\n\n8. Update the documentation commit in .circleci/deploy.sh for the accurate documentation to be displayed\n\n9. Update README.md to redirect to correct documentation.\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "https://github.com/huggingface/nlp/tags",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/huggingface/nlp",
    "keywords": "nlp machine learning datasets metrics",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hf-datasets",
    "package_url": "https://pypi.org/project/hf-datasets/",
    "platform": "",
    "project_url": "https://pypi.org/project/hf-datasets/",
    "project_urls": {
      "Download": "https://github.com/huggingface/nlp/tags",
      "Homepage": "https://github.com/huggingface/nlp"
    },
    "release_url": "https://pypi.org/project/hf-datasets/0.3.0/",
    "requires_dist": [
      "numpy",
      "pyarrow (>=0.16.0)",
      "dill",
      "pandas",
      "requests (>=2.19.0)",
      "tqdm (>=4.27)",
      "filelock",
      "dataclasses ; python_version < \"3.7\"",
      "apache-beam ; extra == 'apache-beam'",
      "apache-beam ; extra == 'dev'",
      "absl-py ; extra == 'dev'",
      "bs4 ; extra == 'dev'",
      "elasticsearch ; extra == 'dev'",
      "faiss-cpu ; extra == 'dev'",
      "langdetect ; extra == 'dev'",
      "mwparserfromhell ; extra == 'dev'",
      "nltk ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-xdist ; extra == 'dev'",
      "tensorflow ; extra == 'dev'",
      "torch ; extra == 'dev'",
      "tldextract ; extra == 'dev'",
      "zstandard ; extra == 'dev'",
      "black ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "flake8 (==3.7.9) ; extra == 'dev'",
      "black ; extra == 'quality'",
      "isort ; extra == 'quality'",
      "flake8 (==3.7.9) ; extra == 'quality'",
      "tensorflow (>=2.2.0) ; extra == 'tensorflow'",
      "tensorflow-gpu (>=2.2.0) ; extra == 'tensorflow_gpu'",
      "apache-beam ; extra == 'tests'",
      "absl-py ; extra == 'tests'",
      "bs4 ; extra == 'tests'",
      "elasticsearch ; extra == 'tests'",
      "faiss-cpu ; extra == 'tests'",
      "langdetect ; extra == 'tests'",
      "mwparserfromhell ; extra == 'tests'",
      "nltk ; extra == 'tests'",
      "pytest ; extra == 'tests'",
      "pytest-xdist ; extra == 'tests'",
      "tensorflow ; extra == 'tests'",
      "torch ; extra == 'tests'",
      "tldextract ; extra == 'tests'",
      "zstandard ; extra == 'tests'",
      "torch ; extra == 'torch'"
    ],
    "requires_python": "",
    "summary": "HuggingFace/NLP is an open library of NLP datasets.",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7690200,
  "releases": {
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b50d74006bed940fe488e7a30ad5de9755e19a65d105fd110945f106f90ae109",
          "md5": "b3a5fa0c73f4916ac6e866779daedf5e",
          "sha256": "d1abd6d6f2d2388a6d979594079bebd9512efeb2255fdd14d8c1d8dcbdb446ba"
        },
        "downloads": -1,
        "filename": "hf_datasets-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b3a5fa0c73f4916ac6e866779daedf5e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 118019,
        "upload_time": "2020-07-13T12:44:00",
        "upload_time_iso_8601": "2020-07-13T12:44:00.353659Z",
        "url": "https://files.pythonhosted.org/packages/b5/0d/74006bed940fe488e7a30ad5de9755e19a65d105fd110945f106f90ae109/hf_datasets-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "75d963d54f54763dee3bd763abd6c5e5531e8041691d31f2b2b2d889d0ef3f36",
          "md5": "e24a0996262325a01be5bffa2e451ae4",
          "sha256": "371b88f2813f62245fa0cec04982dc11498b24b4776bf08c1d995671966a0da5"
        },
        "downloads": -1,
        "filename": "hf-datasets-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "e24a0996262325a01be5bffa2e451ae4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 92519,
        "upload_time": "2020-07-13T12:44:02",
        "upload_time_iso_8601": "2020-07-13T12:44:02.774265Z",
        "url": "https://files.pythonhosted.org/packages/75/d9/63d54f54763dee3bd763abd6c5e5531e8041691d31f2b2b2d889d0ef3f36/hf-datasets-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b50d74006bed940fe488e7a30ad5de9755e19a65d105fd110945f106f90ae109",
        "md5": "b3a5fa0c73f4916ac6e866779daedf5e",
        "sha256": "d1abd6d6f2d2388a6d979594079bebd9512efeb2255fdd14d8c1d8dcbdb446ba"
      },
      "downloads": -1,
      "filename": "hf_datasets-0.3.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b3a5fa0c73f4916ac6e866779daedf5e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 118019,
      "upload_time": "2020-07-13T12:44:00",
      "upload_time_iso_8601": "2020-07-13T12:44:00.353659Z",
      "url": "https://files.pythonhosted.org/packages/b5/0d/74006bed940fe488e7a30ad5de9755e19a65d105fd110945f106f90ae109/hf_datasets-0.3.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "75d963d54f54763dee3bd763abd6c5e5531e8041691d31f2b2b2d889d0ef3f36",
        "md5": "e24a0996262325a01be5bffa2e451ae4",
        "sha256": "371b88f2813f62245fa0cec04982dc11498b24b4776bf08c1d995671966a0da5"
      },
      "downloads": -1,
      "filename": "hf-datasets-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "e24a0996262325a01be5bffa2e451ae4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 92519,
      "upload_time": "2020-07-13T12:44:02",
      "upload_time_iso_8601": "2020-07-13T12:44:02.774265Z",
      "url": "https://files.pythonhosted.org/packages/75/d9/63d54f54763dee3bd763abd6c5e5531e8041691d31f2b2b2d889d0ef3f36/hf-datasets-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}