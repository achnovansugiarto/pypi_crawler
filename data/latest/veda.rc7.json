{
  "info": {
    "author": "Nicolas Weber",
    "author_email": "nicolas.weber@neclab.eu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: BSD License",
      "Programming Language :: C",
      "Programming Language :: Python :: 3 :: Only"
    ],
    "description": "# VEDA (VE Driver API) and VERA (VE Runtime API)\n\nVEDA and VERA are a CUDA Driver and Runtime API-like APIs for programming the\nNEC SX-Aurora. It is based on [AVEO](https://github.com/sx-aurora/aveo). Most of\nthe functionality is identical to the [CUDA Driver\nAPI](https://docs.nvidia.com/cuda/cuda-driver-api/index.html) and [CUDA Runtime\nAPI](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html).\n\n[![Github](https://img.shields.io/github/v/tag/sx-aurora/veda?display_name=tag&sort=semver)](https://github.com/sx-aurora/veda)\n[![PyPI](https://img.shields.io/pypi/v/veda)](https://pypi.org/project/veda)\n[![License](https://img.shields.io/pypi/l/veda)](https://pypi.org/project/veda)\n![Python Versions](https://img.shields.io/pypi/pyversions/veda)\n![Linux](https://svgshare.com/i/Zhy.svg)\n![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)\n![Maintenance](https://img.shields.io/pypi/dm/veda)\n\n**Sitemap:**\n- [Release Notes](#veda_1)\n- [Differences between VEDA and CUDA Driver API](#veda_2)\n- [Differences between VERA and CUDA Runtime API](#veda_3)\n- [VEDA/VERA Unique Features](#veda_4)\n\t- [Delayed Memory Allocation](#veda_41)\n\t- [OMP Threads vs Streams (experimental)](#veda_42)\n\t- [Advanced VEDA C++ Ptr](#veda_43)\n\t- [VEDA-NEC MPI integration](#veda_44)\n\t- [NUMA Support](#veda_45)\n\t- [VEDA-smi](#veda_46)\n\t- [Profiling API](#veda_47)\n\t- [C++ API *(Experimental!)*](#veda_48)\n\t\t- [Error Handling](#veda_481)\n\t\t- [Fetching a Device handle](#veda_482)\n\t\t- [Loading Modules](#veda_483)\n\t\t- [Memory Buffer Objects](#veda_484)\n\t\t- [Fetching Functions](#veda_485)\n- [Limitations/Known Problems](#veda_5)\n- [How to build](#veda_6)\n- [How to use](#veda_7)\n\t- [VEDA Hybrid Offloading](#veda_71)\n\t- [VE Native applications](#veda_72)\n\t- [VE Native Injection](#veda_73)\n\n---\n\n<a name=\"veda_1\"></a>\n## Release Notes\n<table>\n<tr><th>Version</th><th>Comment</th></tr>\n\n<tr><td>v2.0.0</td><td>\n<ul>\n<li>Breaking C-ABI changes</li>\n<li><a href=\"#veda_48\">Added Experimental C++ API</a></li>\n<li><a href=\"#veda_47\">Added VEDA profiling API</a></li>\n</ul>\n</td></tr>\n\n<tr><td>v1.4.0</td><td>\n<ul>\n<li>Overhauled HMEM API</li>\n<li>Switched to new AVEO async malloc</li>\n<li>Fixed bug in CMake setting correct C++ standard flags</li>\n</ul>\n</td></tr>\n\n<tr><td>v1.3.5</td><td>\n<ul>\n<li>Bugfix for <code>vedaArgsSet(...)</code> to again accept <code>VEDAptr<T></code></li>\n</ul>\n</td></tr>\n\n<tr><td>v1.3.4</td><td>\n<ul>\n<li>Fixed RPM and LOCAL dist types.</li>\n<li>Automatically downloading Illyrian and Tungl in VEOS and LOCAL builds.</li>\n<li>Fixed DType issues with in cpp <code>vedaLaunchKernel(...)</code></li>\n</ul>\n</td></tr>\n\n<tr><td>v1.3.3</td><td>\n<ul>\n<li>Removed polluting <code>VEDA_ERROR_UNKNOWN_CONTEXT</code> log message.</li>\n<li>Fixed possible memleak when VPTR is already allocated.</li>\n<li>Fixed possible memleak when using <code>vedaMemAllocOverrideOnce</code>.</li>\n<li>Synchronizing TUNGL timer with VH.</li>\n</ul>\n</td></tr>\n\n<tr><td>v1.3.2</td><td>\n<ul>\n<li>Added <code>vedaMemAllocOverrideOnce</code> to prevent <code>vedaMemAlloc</code> to allocate new memory and instead return the override value once. This is not meant to be used except when you want to circumvent unncessary memory allocations in opaque data structures that you don't have access to.</li>\n<li>Fixed RPATH for veda-smi in Python releases</li>\n</ul>\n</td></tr>\n\n<tr><td>v1.3.1</td><td>\n<ul>\n<li>Added compile guard to prevent <code>vedaArgsSet&lt;bool&gt;</code> because <code>bool</code> is defined as 1B on VH and 4B on VE.</li>\n<li>Merged changes from <a href=\"https://github.com/veos-sxarr-NEC/veda_source/tree/release-2.11.1\" target=\"_BLANK\">VEOS 2.11.1 release</a>.</li>\n<li>Compatibility Bugfix for <code>veda/omp.h</code> NCC &ge; v3.4</li>\n</ul>\n</td></tr>\n\n<tr><td>v1.3.0</td><td>\n<ul>\n<li>changed definition of <code>VEDAdeviceptr</code> to prevent segfaults when passing them to <code>std::ostream</code></li>\n<li>added <code>__global__</code> for device code</li>\n<li>improved <code>veda_device_omp.h</code> implementations</li>\n<li>renamed <code>FIND_PACKAGE(VE ...)</code> to <code>FIND_PACKAGE(VEDA ...)</code></li>\n<li>added checks for <code>REQUIRED</code> and min versions to <code>FIND_PACKAGE(VEDA ...)</code></li>\n<li>renamed <code>VEDA_INCLUDES</code> to <code>VEDA_INCLUDE_DIRS</code> to comply with CMake standard</li>\n<li>moved <code>CMakeLists.txt</code> into project root</li>\n<li>VEDA_CXX will now obey <code>CMAKE_CXX_STANDARD</code> when <code>CMAKE_CXX_STANDARD_REQUIRED</code> is set.</li>\n<li>added device-side support for <a href=\"https://github.com/nec-research/tungl\" target=\"_BLANK\">Tungl</a></li>\n</ul>\n</td></tr>\n\n<tr><td>v1.2.0</td><td>\n<ul>\n<li>replaced <code>bool checkResult</code> with <code>int64_t* result</code> in <code>vedaLaunchKernelEx</code> to receive return value of kernel</li>\n<li>added C++ version of <code>vedaLaunchKernelEx</code></li>\n<li>added <code>vedaLaunchHostFuncEx</code> that can return the return value of the function</li>\n</ul>\n</td></tr>\n\n<tr><td>v1.1.2</td><td>\n<ul>\n<li>changed behavior of <code>VE_NODE_NUMBER</code> to be only used when <code>_VENODELIST</code> AND <code>VEDA_VISIBLE_DEVICES</code> are not set.</li>\n</ul>\n</td></tr>\n\n<tr><td>v1.1.1</td><td>\n<ul>\n<li>Added support for AVEO's <code>_VENODELIST</code> env to ensure correct behavior in cluster environments.</li>\n<li>Changed behavior of <code>VEDA_VISIBLE_DEVICES</code> in case of NUMA nodes. It now accepts the direct hardware id in the format of <code>[AVEO_ID].[NUMA_ID]</code></li>\n</ul>\n</td></tr>\n\n<tr><td>v1.1.0</td><td>\n<ul>\n<li>added <code>vedaMemSwap</code> function to swap the underlying memory buffer between two <code>VEDAdeviceptr</code>.</li>\n</ul>\n</td></tr>\n\n<tr><td>v1.0.0</td><td>\nFirst stable release.\n<ul>\n<li>Improved memset performance, especially for D8 and D16 (up to 150x faster now!).</li>\n<li>Added <code>vedaMemsetD128</code> and <code>vedaMemsetD2D128</code> API.</li>\n<li>Added <code>ASL_FFTW_LIBRARIES</code> to ASL CMake.</li>\n<li>Added device code <code>vedaMemset</code>. Enabled to use <code>vedaMemsetD*</code> in device code.</li>\n<li>Added C++ wrapper to allow directly signed integer and floating point values for <code>vedaMemsetD*</code> functions.</li>\n</ul>\n</td></tr>\n\n<tr><td>v0.10.6</td><td>Maintenance release that fixes SegFaults when context has been destroyed before freeing memory. <code>vedaMemFree</code> ignores calls if the context for the particular pointer has already been freed. BugFix for <code>VEDA_CONTEXT_MODE_SCALAR</code> if <code>VE_OMP_NUM_THREADS</code> is not set.</td></tr>\n\n<tr><td>v0.10.5</td><td>added <code>veda_omp_simd_reduce</code>. MemTrace only get printed when env var<code>VEDA_MEM_TRACE=1</code> is set. VEDA no longer overrides VEORUN_BIN if already been set by user. Added LICENSE to installation target.</td></tr>\n<tr><td>v0.10.4</td><td>Fixed Identification of VE model.</td></tr>\n<tr><td>v0.10.3</td><td>Filtering negative values from <code>VEDA_VISIBLE_DEVICES</code>.</td></tr>\n<tr><td>v0.10.2</td><td>Correct veda-smi RPATH to work without setting LD_LIBRARY_PATH.</td></tr>\n<tr><td>v0.10.1</td><td>Added <code>aveorun-ftrace</code>. Can be activated using <code>VEDA_FTRACE=1</code> env var. Renamed RPM packages to only include major version in package name, i.e. <code>veda-0.10</code>.</td></tr>\n<tr><td>v0.10.0</td><td>Renamed and improved <code>VEDAmpiptr</code> to <code>VEDAptr<typename></code>. Removed <code>VEDAdeviceptr->X</code> functions, as they are now part of <code>VEDAptr</code>. Added <code>veda-smi</code> executable.</td></tr>\n<tr><td>v0.10.0rc5</td><td>Added boundary checks for Memcopy and MemSet. Added <code>vedaArgsSetHMEM</code>. Added <code>veda_device_omp.h</code> parallelization primitives for C++. Added experimental <code>VEDAmpiptr</code> for easier usage with VE-MPI. Added/corrected some of the sensor readings, i.e. LLC Cache, Total Device Memory, ...</td></tr>\n<tr><td>v0.10.0rc4</td><td>Increased VEDA offset limit to 128GB. Added <code>VEDAdeviceptr->X</code> functions in C++. Renamed <code>vedaArgsSetPtr</code> to <code>vedaArgsSetVPtr</code>. Added <code>vedaArgsSetPtr</code> to automatically translate <code>VEDAdeviceptr</code> to <code>void*</code>. Fixed VEDA_VISIBLE_DEVICES to obey NUMA mode.</td></tr>\n<tr><td>v0.10.0rc3</td><td>Added AVEO symlinks. Fixed wrong include.</td></tr>\n<tr><td>v0.10.0rc2</td><td>Fixed problem in veda_types.h when compiling with C. Linking against shared AVEO instead of static.</td></tr>\n<tr><td>v0.10.0rc1</td><td>Fixed 0Â°C core temperatures. Added NUMA support. Each NUMA node becomes a separate VEDAdevice. Added <code>vedaDeviceDistance(float**, VEDAdevice, VEDAdevice)</code> to determine the relationship between two VEDAdevices (0.0 == same device, 0.5 == same physical device but different NUMA node, 1.0 == different physical device). Added <code>vedaMemGetHMEMPointer(void**, VEDAdeviceptr)</code> to translate VEDA pointer to HMEM pointer.</td></tr>\n<tr><td>v0.9.5.2</td><td>Bugfixes</td></tr>\n<tr><td>v0.9.5.1</td><td>Bugfixes</td></tr>\n<tr><td>v0.9.5</td><td>Bugfixes</td></tr>\n<tr><td>v0.9.4</td><td>Bugfixes</td></tr>\n<tr><td>v0.9.3</td><td>Bugfixes</td></tr>\n<tr><td>v0.9.2</td><td>Added FindMPI. Set all CMake vars as advanced.</td></tr>\n<tr><td>v0.9.1</td><td>Added FindBLAS, FindLAPACK, FindASL and FindNCL to CMake.</td></tr>\n<tr><td>v0.9</td><td>Enhanced VEDA CMake Scripts, to also support native NCC compilation.</td></tr>\n<tr><td>v0.8.1</td><td>updated AVEO. Using VE_NODE_NUMBER as fallback if VEDA_VISIBLE_DEVICES is not set.</td></tr>\n<tr><td>v0.8</td><td>Implemented multi-stream support (experimental). Automatic setting of required env vars.</td></tr>\n<tr><td>v0.7.1</td><td>Bugfix release</td></tr>\n<tr><td>v0.7</td><td>initial VERA release</td></tr>\n<tr><td>v0.6</td><td>initial VEDA release</td></tr>\n</table>\n\n---\n\n<a name=\"veda_2\"></a>\n## Differences between VEDA and CUDA Driver API:\n1. [VEDA] Additionally to ```vedaInit(0)``` in the beginning, ```vedaExit()``` needs to be called at the end of the application, to ensure that no dead device processes stay alive.\n1. All function calls start with: [VEDA] ```veda*``` instead of ```cu*``` and [VERA] ```vera*``` instead of ```cuda*```\n1. Objects start with [VEDA] ```VEDA*``` instead of ```CU*``` and ```vera*``` instead of ```cuda*```\n1. VEDA supports asynchronous malloc and free:\nVEDA supports asynchronous ```vedaMemAllocAsync``` and ```vedaMemFreeAsync```. They can be used like the synchronous calls, but don't need to synchronize the execution between device and host.\n1. ```vedaDeviceGetPower(float* power, VEDAdevice dev)``` and ```vedaDeviceGetTemp(float* tempC, const int coreIdx, VEDAdevice dev)``` allow to fetch the power consumption (in W) and temperature (in C).\n1. As the programming model of the SX-Aurora differs from NVIDIA GPUs, launching kernels looks different:\n\t```cpp\n\t// Device Code -------------------------------------------------------------\n\textern \"C\" void my_function(float myFloat, uint8_t myUnsignedChar, float* array) {\n\t\t...\n\t}\n\n\t// C -----------------------------------------------------------------------\n\tfloat myFloat;\n\tuint8_t myUnsignedChar;\n\tVEDAargs args;\n\tvedaArgsCreate(&args);\n\n\t// Scheme: vedaArgsSet[TYPE](&args, [PARAM_INDEX], [VARIABLE]);\n\tvedaArgsSetF32(args, 0, myFloat);\n\tvedaArgsSetU8(args, 1, myUnsignedChar);\n\n\t// Copy entire arrays as function parameter\n\tfloat array[32];\n\tvedaArgsSetStack(args, 2, array, VEDA_ARGS_INTENT_INOUT, sizeof(array));\n\n\tVEDAmodule mod;\n\tVEDAfunction func;\n\tvedaModuleLoad(&mod, \"mylib.vso\");\n\tvedaModuleGetFunction(&func, mod, \"my_function\");\n\n\t// Kernel Call Version 1: allows to reuse VEDAargs object\n\tVEDAstream stream = 0;\n\tvedaLaunchKernel(func, stream, args);\n\n\t// args are not allowed to be destroyed before synchronizing!\n\tvedaStreamSynchronize(stream);\n\tvedaArgsDestroy(&args);\n\n\t// Kernel Call Version 2: automatically destroys VEDAargs object after execution (can't be reused for other calls!)\n\tvedaLaunchKernelEx(func, stream, args, 1, 0);\n\n\t// CPP ---------------------------------------------------------------------\n\tvedaLaunchKernel(func, stream, myFloat, myUnsignedChar, VEDAstack(array, VEDA_ARGS_INTENT_INOUT, sizeof(array)));\n\t```\n1. VEDAdeviceptr need to be dereferenced first on device side:\n\t```cpp\n\t// Host Code ---------------------------------------------------------------\n\tVEDAdeviceptr ptr;\n\tvedaMemAllocAsync(&ptr, sizeof(float) * cnt);\n\tvedaLaunchKernel(func, 0, ptr, cnt);\n\tvedaMemFreeAsync(ptr);\n\n\t// Device Code -------------------------------------------------------------\n\tvoid mykernel(VEDAdeviceptr vptr, size_t cnt) {\n\t\tfloat* ptr;\n\t\tvedaMemPtr(&ptr, vptr);\n\n\t\tfor(size_t i = 0; i < cnt; i++)\n\t\t\tptr[cnt] = ...;\n\t}\n\t```\n1. VEDA streams differ from CUDA streams. See chapter \"OMP Threads vs Streams\" for more details.\n1. VEDA uses the env var ```VEDA_VISIBLE_DEVICES``` in contrast to ```CUDA_VISIBLE_DEVICES```.\n\n---\n\n<a name=\"veda_3\"></a>\n## Differences between VERA and CUDA Runtime API:\n1. All function calls start with ```vera*``` instead of ```cuda*```\n1. Objects start with ```vera*``` instead of ```cuda*```\n1. VERA supports asynchronous malloc and free, see VEDA.\nVEDA supports asynchronous ```vedaMemAllocAsync``` and ```vedaMemFreeAsync```. They can be used like the synchronous calls, but don't need to synchronize the execution between device and host.\n1. ```vedaDeviceGetPower(float* power, VEDAdevice dev)``` and ```vedaDeviceGetTemp(float* tempC, const int coreIdx, VEDAdevice dev)``` allow to fetch the power consumption (in W) and temperature (in C).\n1. As the programming model of the SX-Aurora differs from NVIDIA GPUs, launching kernels looks different.\n1. Similar to CUDA Runtime API, calls from VEDA and VERA can be mixed!\n\n---\n\n<a name=\"veda_4\"></a>\n## VEDA/VERA Unique Features:\n\n<a name=\"veda_41\"></a>\n### Delayed Memory Allocation\nVEDA does not need to allocate memory from the host, but can do that directly from the device. For this, the host only needs to create an empty VEDAdeviceptr.\n```cpp\n// Host Code ---------------------------------------------------------------\nVEDAdeviceptr vptr;\nvedaMemAllocAsync(&vptr, 0, 0);\nvedaLaunchKernel(func, 0, vptr, cnt);\nvedaMemcpyDtoHAsync(host, vptr, sizeof(float) * cnt, 0);\nvedaMemFreeAsync(vptr, 0);\n\n// Device Code -------------------------------------------------------------\nvoid mykernel(VEDAdeviceptr vptr, size_t cnt) {\n\tfloat* ptr;\n\tvedaMemAllocPtr((void**)&ptr, vptr, cnt * sizeof(float));\n\n\tfor(size_t i = 0; i < cnt; i++)\n\t\tptr[cnt] = ...;\n}\n```\n\n---\n\n<a name=\"veda_42\"></a>\n### OMP Threads vs Streams (experimental):\nIn CUDA streams can be used to create different execution queues, to overlap\ncompute with memcopy. VEDA supports two stream modes which differ from the CUDA\nbehavior. These can be defined by ```vedaCtxCreate(&ctx, MODE, device)```.\n\n1. ```VEDA_CONTEXT_MODE_OMP``` (default): All cores will be assigned to the default stream (=0). This mode only supports a single stream.\n1. ```VEDA_CONTEXT_MODE_SCALAR```: Every core gets assigned to a different\n   stream. This mode allows to use each core independently with different\n   streams. Use the function ```vedaCtxStreamCnt(&streamCnt)``` to determine how\n   many streams are available.\n\nBoth methods use the env var ```VE_OMP_NUM_THREADS``` to determine the maximal\nnumber of cores that get use for either mode. If the env var is not set, VEDA\nuses all available cores of the hardware.\n\n---\n\n<a name=\"veda_43\"></a>\n### Advanced VEDA C++ Ptr\nWhen you use C++, you can use the ```VEDAptr<typename>``` that gives you more\ndirectly control over the ```VEDAdeviceptr```, i.e. you can use\n```vptr.size()```, ```vptr.device()```, ... . The ```typename``` is used to\nautomatically determine the correct offsets when executing ```vptr +=\noffset;```.\n\n---\n\n<a name=\"veda_44\"></a>\n### VEDA-NEC MPI integration\nThe VEO-aware NEC MPI ( https://www.hpc.nec/forums/topic?id=pgmcA8 ) enables to\nmuch easier implement hybrid VE applications. For this, so called HMEM pointers\nhave been introduced in VEO. Starting with v1.4.0 VEDA introduced a new HMEM\nAPI: ```vedaHMEM*```. See following example:\n\n```cpp\nVEDAhmemptr hmem;\nvedaHMemAlloc(&hmem, size);\nvedaHMemcpy(hmem, host_ptr, size);\nmpi_send(hmem, ...);\n```\n\n<a name=\"veda_45\"></a>\n### NUMA Support\nVEDA supports VE NUMA nodes since v0.10. To enable NUMA on your system you need\nto execute (set ```-N ?``` to specific device index):\n```bash\nVCMD=\"sudo /opt/nec/ve/bin/vecmd -N ?\"\n$VCMD vconfig set partitioning_mode on\n$VCMD state set off\n$VCMD state set mnt\n$VCMD reset card\n```\n\nVEDA then recognizes each NUMA node as a separate device, i.e. with 2 physical\ndevices in NUMA mode, VEDA would show 4 devices. You can use ```VEDAresult\nvedaDeviceDistance(float* distance, VEDAdevice devA, VEDAdevice devB)``` to\ndetermine the relationship of two VEDAdevices.\n\n```\ndistance == 0.0; // same device\ndistance == 0.5; // same physical device, different NUMA node\ndistance == 1.0; // differeny physical device\n```\n\n---\n\n<a name=\"veda_46\"></a>\n### VEDA-smi\nThe executable ```veda-smi``` displays available VEDA devices in your system. It\nuses the ```VEDA_VISIBLE_DEVICES``` env var and therefore only shows the devices\nthat your VEDA application would be able to use. Use ```VEDA_VISIBLE_DEVICES=\nveda-smi``` to ensure that you see all installed devices.\n\n```\nâ veda-smi ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\nâ VEDA Version: 0.10.0     AVEO Version: 0.9.15                                 â\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\nâââ #0  NEC SX-Aurora Tsubasa VE10B âââââââââââââââââââââââââââââââââââââââââââââ\n  â Physical: 1.0\n  â AVEO:     0.0\n  â Clock:    current: 1400 MHz, base: 800 MHz, memory: 1600 MHz\n  â Firmware: 5399\n  â Memory:   49152 MiB\n  â Cache:    LLC: 8192kB, L2: 256kB, L1d: 32kB, L1i: 32kB\n  â Temp:     56.4Â°C 56.4Â°C 57.0Â°C 56.1Â°C\n  â Power:    18.0W (11.9V, 1.5A)\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\nâââ #1  NEC SX-Aurora Tsubasa VE10B âââââââââââââââââââââââââââââââââââââââââââââ\n  â Physical: 1.1\n  â AVEO:     0.1\n  â Clock:    current: 1400 MHz, base: 800 MHz, memory: 1600 MHz\n  â Firmware: 5399\n  â Memory:   49152 MiB\n  â Cache:    LLC: 8192kB, L2: 256kB, L1d: 32kB, L1i: 32kB\n  â Temp:     56.1Â°C 56.4Â°C 55.9Â°C 56.0Â°C\n  â Power:    18.0W (11.9V, 1.5A)\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n\nâââ #2  NEC SX-Aurora Tsubasa VE10B âââââââââââââââââââââââââââââââââââââââââââââ\n  â Physical: 0.0\n  â AVEO:     1.0\n  â Clock:    current: 1400 MHz, base: 800 MHz, memory: 1600 MHz\n  â Firmware: 5399\n  â Memory:   49152 MiB\n  â Cache:    LLC: 16384kB, L2: 256kB, L1d: 32kB, L1i: 32kB\n  â Temp:     53.8Â°C 53.5Â°C 54.1Â°C 53.8Â°C 53.8Â°C 54.1Â°C 53.2Â°C 53.5Â°C\n  â Power:    36.3W (11.9V, 3.1A)\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ\n```\n\n---\n\n<a name=\"veda_47\"></a>\n### Profiling API\nSince v1.5.0 VEDA supports to add a profiling callback using\n`vedaProfilerSetCallback(...)`. The callback needs to have the signature `void\n(*)(VEDAprofiler_data* data, int enter)`. If `enter` is non-zero, the callback\ngot called right before issuing the command. If it's zero, it just ended.\n\nThe data provides the following fields:\n1. `type`: An enum that identifies which kind function got called (kernel, memcpy, ...)\n2. `device_id`: VEDA device id\n3. `stream_id`: VEDA stream id\n4. `req_id`: ID of the request\n5. `user_data`: `void*` that allows to store data between `enter` and `exit` of\n   the event. This should be deleted by the user when `enter==0` to prevent\n   memleaks.\n\nDepending on the `type`, you can cast the `data` to one of the following data\ntypes to get access to further information.\n\n1. `type in [VEDA_PROFILER_MEM_ALLOC, VEDA_PROFILER_HMEM_ALLOC]`: `VEDAprofiler_vedaMemAlloc`\n\t1. `bytes`: number of bytes to be allocated\n1. `type in [VEDA_PROFILER_MEM_FREE, VEDA_PROFILER_HMEM_FREE]`: `VEDAprofiler_vedaMemFree`\n\t1. `ptr`: pointer to be freed\n1. `type in [VEDA_PROFILER_MEM_CPY_HTOD, VEDA_PROFILER_MEM_CPY_DTOH, VEDA_PROFILER_HMEM_CPY]`: `VEDAprofiler_vedaMemcpy`\n\t1. `dst`: destination pointer\n\t1. `src`: source pointer\n\t1. `bytes`: number of bytes transfered\n1. `type == VEDA_PROFILER_LAUNCH_KERNEL`: `VEDAprofiler_vedaLaunchKernel`\n\t1. `func`: function pointer that gets called\n\t1. `kernel`: name of the kernel that gets called\n1. `type == VEDA_PROFILER_LAUNCH_HOST`: `VEDAprofiler_vedaLaunchHostFunc`\n\t1. `func`: function pointer that gets called\n\n---\n\n<a name=\"veda_48\"></a>\n### C++ API *(Experimental!)*\nStarting with v1.5.0 we introduce a new experimental and lightweight C++ API.\nThis API aims for easier usage of VEDA, with much more comfort in C++\napplications.\n\nTo include the new API just use `#include <veda/cpp/api.h>`.\n\n<a name=\"veda_481\"></a>\n#### Error Handling\nInstead of the C-API, the C++ API uses exceptions, which can be used like this:\n```cpp\ntry {\n\t...\n} catch(const veda::Exception& e) {\n\tstd::cerr << e.what() << \" @ \" << e.file() << \" (\" << e.line() << \")\";\n}\n```\n\n<a name=\"veda_482\"></a>\n#### Fetching a Device handle\nTo get a handle to a device, just create an instance using:\n```cpp\nveda::Device device(0);\n```\n\nIn contrast to the C-API, the `veda::Device` incorporates the `VEDAdevice` and\n`VEDAcontext` into a single object. We use a lazy scheme, which will not boot up\nthe device context until you allocate memory, load a model, or similar.\n\nThe device provides the following attributes and metrics: `isActive, current,\ncurrentEdge, distance, power, temp, voltage, voltageEdge, abi, aveoId, cacheL1d,\ncacheL1i, cacheL2, cacheLLC, clockBase, clockMemory, clockRate, cores, firmware,\nmodel, numaId, physicalId, singleToDoublePerfRatio, streamCnt, vedaId, totalMem,\nusedMem`.\n\nIf your application requires to do the CUDA-style programming, where you bind\nthe device to a specific thread, you can use `device.pushCurrent()`,\n`device.setCurrent()` and `auto device = Device::getCurrent()` or `auto device =\nDevice::popCurrent()`.\n\nTo synchronize the execution use `device.sync()` or `device.sync(stream)`.\n\n<a name=\"veda_483\"></a>\n#### Loading Modules\nJust do:\n```cpp\nauto mod = dev.load(\"libmymodule.vso\");\n```\n\n<a name=\"veda_484\"></a>\n#### Memory Buffer Objects\nThe new C++ API uses buffer objects instead of raw pointers. These can be\nallocated using `dev.alloc<float>(cnt)`, which will allocate `sizeof(T) * cnt`\nbytes of memory.\n\nIf you want to use a different stream, just use `dev.alloc<float>(cnt, stream)`.\n\nTo allocate HMEM memory, use `dev.alloc<float, veda::HMEM>(size)`.\n\nTo copy data between different Buffers, or the host and the VE, just use:\n```cpp\nauto VE = dev.alloc<float>(cnt);\nauto VH = malloc(sizeof(float) * cnt);\n\nVE.to(VH);              // copies all items from VE to VH\nVE.to(VH, 1);           // copies the first item from VE to VH\nVE[4].to(VH + 4, 1);    // copies the 5th item from VE to VH\nVE.from(VH);            // copies all items from VH to VE\n\nauto V2 = dev.alloc<float>(cnt);\nV2.to(VE);              // copies all items from V2 to VE\nVE.from(V2);            // copies all items from V2 to VE\n```\n\nTo memset data use:\n```cpp\nVE.memset(3.1415);      // set all items\nVE[5].memset(3.1415);   // set all items starting the 6th\nVE[5].memset(3.1415, 1);// set only the 6th item\n```\n\nTo cast a buffer object to another type:\n```cpp\nauto Float = dev.alloc<float>(cnt);\nauto Int32 = Float.cast<int32_t>(); // Float.cnt() == Int32.cnt()\nauto Int16 = Float.cast<int16_t>(); // Float.cnt() == Int16.cnt()*2\n```\n\nAll buffer objects use shared pointer semantics. When all objects using the same\nsource pointer are destroyed, it will be automatically freed.\n\nTo pass on pointers between methods just pass on the buffer object:\n\n```cpp\nveda::Ptr<VEDA, float> func(...) {\n\t...\n\tauto ptr = dev.alloc<float>(cnt);\n\t...\n\treturn ptr;\n}\n```\n\n<a name=\"veda_485\"></a>\n#### Fetching Functions\nFor fetching functions we provide three helper functions.\n\n1. **C-style or `extern \"C\"` functions**:\n\t```cpp\n\t// VE\n\textern \"C\" int name(int, float, VEDAdeviceptr);\n\n\t// VH\n\tusing namespace veda;\n\tauto func = CFunction::Return<int>(mod, \"name\");\n\tauto result = func(0, 3.14f, ptr);\n\tprintf(\"%i\\n\", int(result));\n\t```\n\t\n\tThe `CFunction::Return<int>` returns you an executable object to an\n\tC-function on the VE. Whenever you call `func(...)` it issues a kernel\n\tcall. By default we use the stream #0, but you can use\n\t`func[stream](...)` to define the stream yourself.\n\n\t`result` is a future object. When you call `result.wait()` or fetch the\n\tresult using `(TYPE)result` or `result.get()`, it will synchronize the\n\texecution and provide the return value.\n\n\tThe `::Return<...>` can be omitted when no return value is expected.\n\n2. **C++-style functions**:\n\t```cpp\n\t// VE\n\tint name(int, float, VEDAdeviceptr);\n\n\t// VH\n\tusing namespace veda;\n\tauto func = Function::Return<int>::Args<int, float, VEDAdeviceptr>(mod, \"name\");\n\tauto result = func(0, 3.14f, ptr);\n\tprintf(\"%i\\n\", int(result));\n\t```\n\n\tFor C++-style functions use `Function` instead of `CFunction`. In this\n\tcase you also need to provide the types of all arguments using\n\t`Args<...>`.\n\n\tAgain `::Return<...>` can be omitted when no return value is expected.\n\n\tAlso struct types can be used as arguments:\n\t```cpp\n\t// VE + VH\n\tnamespace whatever {\n\t\ttemplate<typename T>\n\t\tstruct complex {\n\t\t\tT x, y;\n\t\t};\n\t}\n\n\t// VE\n\tvoid name(VEDAdeviceptr, whatever::complex<float>);\n\n\t// VH\n\tauto func = Function::Args<VEDAdeviceptr, whatever::complex<float>>(mod, \"name\");\n\twhatever::complex<float> x = {3.0f, 4.0f};\n\tfunc(ptr, x);\n\t```\n\n3. **Template functions**:\n\t```cpp\n\t// VE\n\ttemplate<typename T, typename D>\n\tT name(T, float, D);\n\n\ttemplate int name<int, VEDAdeviceptr>(int, float, VEDAdeviceptr);\n\n\t// VH\n\tusing namespace veda;\n\tauto func = Template<int, VEDAdeviceptr>::Return<_0>::Args<_0, float, _1>(mod, \"name\");\n\t```\n\n\tLast, we also support to fetch templated functions. Here it is\n\timportant, that in the VE code, the template gets explicitly\n\tinstantiated using the `template ... name<...>(...);` syntax. Otherwise\n\tthe compiler will not generate this specific templated function.\n\n\tOn the VH, we first define the template parameters using\n\t`Template<...>`. Next, as before the return type. If it is\n\t`::Return<void>`, it can be omitted. And last the arguments, similar as\n\tbefore for the `Function`.\n\n\tIn the code above you see `veda::_0` and `veda::_1`. These correspond to\n\tthe template parameters, `_0` is the 0th, `_1` the 1st, and so on. It is\n\tnecessary to use these template placeholders within `Return<...>` and\n\t`Args<...>` at the same locations as within the C++ code.\n\n\tIf your template uses literals, such as:\n\t```cpp\n\ttemplate<int i, typename T>\n\tT name(T a) { return a + i; }\n\n\ttemplate float name<0>(float);\n\ttemplate float name<5>(float);\n\ttemplate int   name<5>(int);\n\t```\n\n\tYou can to use the following code on VH:\n\t```cpp\n\tauto name_f0 = Template<Literal<0>, float>::Return<_1>::Args<_1>(...);\n\tauto name_f5 = Template<Literal<5>, float>::Return<_1>::Args<_1>(...);\n\tauto name_i0 = Template<Literal<5>, int>  ::Return<_1>::Args<_1>(...);\n\t```\n\n\tIt's important that the data type you pass to `Literal<...>` matches the\n\tdata type you use in your `template<...>`. I.e., if you use\n\t`template<char...>`, then you need to use `Literal('x')` or\n\t`Literal(char(15))`.\n\n\tOnly integer-like types (char, short, ...) can be used as template\n\tliterals.\n\nFor all function fetching methods it's important, that function arguments match\nexactly the ones you use in your VE C++ code. Otherwise fetching the function\nwill fail at runtime!\n\n---\n\n<a name=\"veda_5\"></a>\n## Limitations/Known Problems:\n1. VEDA only supports one ```VEDAcontext``` per device.\n1. No unified memory space (yet).\n1. VEDA by default uses the current workdirectory for loading modules. This\n   behavior can be changed by using the env var ```VE_LD_LIBRARY_PATH```.\n1. Due to compiler incompatibilities it can be necessary to adjust the CMake\nvariable ```${AVEO_NFORT}``` to another compiler.\n1. The C++ API can only return fundamental (void, int, short, ...) values.\n1. The C++ API cannot compile `...::Args<void>`. Use `...:::Args<>` instead.\n\n---\n\n<a name=\"veda_6\"></a>\n## How to build:\n```bash\ngit clone https://github.com/SX-Aurora/veda/\nmkdir veda/build\ncd veda/build\n\n# Build Option 1: Local installation (default: /usr/local/ve (use -DCMAKE_INSTALL_PREFIX=... for other path))\ncmake3 -DVEDA_DIST_TYPE=LOCAL ..\ncmake3 --build . --target install \n\n# Build Option 2: VEOS installation\ncmake3 -DVEDA_DIST_TYPE=VEOS ..\ncmake3 --build . --target install \n\n# Build Option 3: Python package\npip3 install illyrian tungl\nillyrian cmake3 -DVEDA_DIST_TYPE=PYTHON ..\ncmake3 --build . --target dist\n```\n\n---\n\n<a name=\"veda_7\"></a>\n## How to use:\nVEDA has an own CMake find script. This supports 3 modes. The script uses the compilers installed in ```/opt/nec/ve/bin```. You can modify the ```CMAKE_[LANG]_COMPILER``` flags to change that behavior. See the Hello World examples in the [Examples Folder](example)\n\n---\n\n<a name=\"veda_71\"></a>\n### 1. VEDA Hybrid Offloading:\nThis mode is necessary for VEDA offloading applications. It enables to compile host and device code within the same CMake project. For this it is necessary to use different file extensions for the VE code. All ```*.vc``` files get compiled using NCC, ```*.vcpp``` using NC++ and ```*.vf``` with NFORT.\n\n```cmake\nSET(CMAKE_MODULE_PATH /usr/local/ve/veda/cmake /opt/nec/ve/share/veda/cmake)\nFIND_PACKAGE(VEDA)\nENABLE_LANGUAGE(VEDA_C VEDA_CXX)\n\nINCLUDE_DIRECTORIES(${VEDA_INCLUDE_DIRS})\nADD_EXECUTABLE(myApp mycode.vc mycode.vcpp)\nTARGET_LINK_LIBRARIES(myApp ${VEDA_LIBRARY})\n```\n\n---\n\n<a name=\"veda_72\"></a>\n### 2. VE Native applications:\nThis mode enables to compile VE native applications.\n\n```cmake\nSET(CMAKE_MODULE_PATH /usr/local/ve/veda/cmake /opt/nec/ve/share/veda/cmake)\nFIND_PACKAGE(VEDA)\nENABLE_LANGUAGE(VEDA_C VEDA_CXX)\nADD_EXECUTABLE(myApp mycode.c mycode.cpp)\n```\n\n---\n\n<a name=\"veda_73\"></a>\n### 3. VE Native Injection:\nIf you have a CPU application and you don't want to modify the CMake script you can build your project using:\n```\ncmake -C /usr/local/ve/veda/cmake/InjectVE.cmake /path/to/your/source\n```\nIt will replace the CPU ```C```, ```CXX``` and ```Fortran``` compilers with NCC.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/sx-aurora/veda/tags",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "3 BSD-License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "veda",
    "package_url": "https://pypi.org/project/veda/",
    "platform": "linux_x86_64",
    "project_url": "https://pypi.org/project/veda/",
    "project_urls": {
      "Download": "https://github.com/sx-aurora/veda/tags"
    },
    "release_url": "https://pypi.org/project/veda/2.0.0/",
    "requires_dist": [
      "tungl (>= 0.1.2, < 0.2)"
    ],
    "requires_python": ">=3.6",
    "summary": "VEDA",
    "version": "2.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15875952,
  "releases": {
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dd80d829aaf0ece90fe1fa34b903413b2b6c86b4f273179831cc723238261d5d",
          "md5": "de386850ad3756c7376faf11634dd31a",
          "sha256": "a83f197d29aa04a4aedef9d478ab7ca1efaa943543e6ba321abacaea5b76a1e1"
        },
        "downloads": -1,
        "filename": "veda-1.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "de386850ad3756c7376faf11634dd31a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1400607,
        "upload_time": "2022-03-23T16:01:27",
        "upload_time_iso_8601": "2022-03-23T16:01:27.855727Z",
        "url": "https://files.pythonhosted.org/packages/dd/80/d829aaf0ece90fe1fa34b903413b2b6c86b4f273179831cc723238261d5d/veda-1.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c2a3c89d8de2f8e9a01725ebb17b3d4e7da8f51d2ed0ca95067e86a8d28eca49",
          "md5": "392a0b2789474da2547a7acb91ce0aa8",
          "sha256": "b1ad3de15329ac4afd9ff30e20f9ced3f614e5d91f0a1143e5e958714ff31bb5"
        },
        "downloads": -1,
        "filename": "veda-1.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "392a0b2789474da2547a7acb91ce0aa8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1706742,
        "upload_time": "2022-04-22T10:37:59",
        "upload_time_iso_8601": "2022-04-22T10:37:59.139160Z",
        "url": "https://files.pythonhosted.org/packages/c2/a3/c89d8de2f8e9a01725ebb17b3d4e7da8f51d2ed0ca95067e86a8d28eca49/veda-1.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7239a552786257620e2702c4bae521df314267b0d52217290ccff087548cd0a0",
          "md5": "a919a37f762203d84326cf25186ec177",
          "sha256": "2944fd05b00f1d8734dcdf696bf9cb8771af240d9eb889b4883df8f6a2ca31bf"
        },
        "downloads": -1,
        "filename": "veda-1.3.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a919a37f762203d84326cf25186ec177",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1707358,
        "upload_time": "2022-05-23T12:40:57",
        "upload_time_iso_8601": "2022-05-23T12:40:57.804143Z",
        "url": "https://files.pythonhosted.org/packages/72/39/a552786257620e2702c4bae521df314267b0d52217290ccff087548cd0a0/veda-1.3.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d567874d1bce4860fbf5ee13d7442845ac4718e0c152a0f1b539c1d01593ef0f",
          "md5": "94a75d16853287e125e2db7ff587bae8",
          "sha256": "4745e141e150af50346c65f8771b2fc25d144e4bb35f2b11a854750bd6fd2dda"
        },
        "downloads": -1,
        "filename": "veda-1.3.3-py3-none-manylinux_2_17_x86_64.whl",
        "has_sig": false,
        "md5_digest": "94a75d16853287e125e2db7ff587bae8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1708358,
        "upload_time": "2022-06-29T05:25:58",
        "upload_time_iso_8601": "2022-06-29T05:25:58.011526Z",
        "url": "https://files.pythonhosted.org/packages/d5/67/874d1bce4860fbf5ee13d7442845ac4718e0c152a0f1b539c1d01593ef0f/veda-1.3.3-py3-none-manylinux_2_17_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "994ab3fe104da2aff6151b0878a99b56e1cfef0b2273d5c29dab6cd9663b1728",
          "md5": "0ac1e070cc3e0a839e7f2f0c74be5b54",
          "sha256": "f2c04bcb55b6252ed7fdd4e0d3f693e816e13f93680ad742da3d190b91f44e52"
        },
        "downloads": -1,
        "filename": "veda-1.3.4-py3-none-manylinux_2_17_x86_64.whl",
        "has_sig": false,
        "md5_digest": "0ac1e070cc3e0a839e7f2f0c74be5b54",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1708460,
        "upload_time": "2022-07-07T12:59:22",
        "upload_time_iso_8601": "2022-07-07T12:59:22.606184Z",
        "url": "https://files.pythonhosted.org/packages/99/4a/b3fe104da2aff6151b0878a99b56e1cfef0b2273d5c29dab6cd9663b1728/veda-1.3.4-py3-none-manylinux_2_17_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0289078a4a709a0d0085e7af2e4e84bc11975d8bf17c0f4c3d1aaa1f1eae5b19",
          "md5": "da444fe0e8d929a947988bab5a321400",
          "sha256": "14b1b6105364b5aadcf7c8b7db5e98bff00f926086418db2a9207781b5cbfe84"
        },
        "downloads": -1,
        "filename": "veda-1.4.0-py3-none-manylinux_2_17_x86_64.whl",
        "has_sig": false,
        "md5_digest": "da444fe0e8d929a947988bab5a321400",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 1810114,
        "upload_time": "2022-10-17T06:53:38",
        "upload_time_iso_8601": "2022-10-17T06:53:38.762527Z",
        "url": "https://files.pythonhosted.org/packages/02/89/078a4a709a0d0085e7af2e4e84bc11975d8bf17c0f4c3d1aaa1f1eae5b19/veda-1.4.0-py3-none-manylinux_2_17_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "91392f187940839a2477a3ff58b80d8aa99d9a7bfd4267619f6731e2beada837",
          "md5": "5a49b8fe4869ba3e9536882b6219b453",
          "sha256": "6ef7b084674c9cd1f8ba92439d0b39cd3d92274f00a93a22c52007465e750ec2"
        },
        "downloads": -1,
        "filename": "veda-2.0.0-py3-none-manylinux_2_17_x86_64.whl",
        "has_sig": false,
        "md5_digest": "5a49b8fe4869ba3e9536882b6219b453",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 1933100,
        "upload_time": "2022-11-24T07:49:44",
        "upload_time_iso_8601": "2022-11-24T07:49:44.210190Z",
        "url": "https://files.pythonhosted.org/packages/91/39/2f187940839a2477a3ff58b80d8aa99d9a7bfd4267619f6731e2beada837/veda-2.0.0-py3-none-manylinux_2_17_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "91392f187940839a2477a3ff58b80d8aa99d9a7bfd4267619f6731e2beada837",
        "md5": "5a49b8fe4869ba3e9536882b6219b453",
        "sha256": "6ef7b084674c9cd1f8ba92439d0b39cd3d92274f00a93a22c52007465e750ec2"
      },
      "downloads": -1,
      "filename": "veda-2.0.0-py3-none-manylinux_2_17_x86_64.whl",
      "has_sig": false,
      "md5_digest": "5a49b8fe4869ba3e9536882b6219b453",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 1933100,
      "upload_time": "2022-11-24T07:49:44",
      "upload_time_iso_8601": "2022-11-24T07:49:44.210190Z",
      "url": "https://files.pythonhosted.org/packages/91/39/2f187940839a2477a3ff58b80d8aa99d9a7bfd4267619f6731e2beada837/veda-2.0.0-py3-none-manylinux_2_17_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}