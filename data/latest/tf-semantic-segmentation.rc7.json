{
  "info": {
    "author": "Malte Koch",
    "author_email": "malte-koch@gmx.net",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3"
    ],
    "description": "# TF Semantic Segmentation\n\n[![Build Status](https://travis-ci.org/baudcode/tf-semantic-segmentation.svg?branch=master)](https://travis-ci.org/baudcode/tf-semantic-segmentation)\n[![PyPI Status Badge](https://badge.fury.io/py/tf-semantic-segmentation.svg)](https://pypi.org/project/tf-semantic-segmentation/)\n[![codecov](https://codecov.io/gh/baudcode/tf-semantic-segmentation/branch/dev/graph/badge.svg)](https://codecov.io/gh/baudcode/tf-semantic-segmentation)\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baudcode/tf-semantic-segmentation/blob/master/examples/Demo.ipynb)\n[![Documentation Status](https://readthedocs.org/projects/ansicolortags/badge/?version=latest)](http://tf-semantic-segmentation.readthedocs.io/?badge=latest)\n\n### Quick Start\nSee [GETTING_STARTED](#getting-started), or the [Colab Notebook](https://colab.research.google.com/github/baudcode/tf-semantic-segmentation/blob/master/examples/Demo.ipynb).\n\nLearn more at our [documentation](https://tf-semantic-segmentation.readthedocs.io/en/latest/).\nSee upcoming features on our [roadmap](ROADMAP.md).\n\n## Features\n\n- Distributed Training on Multiple GPUs\n- Hyper Parameter Optimization using WandB\n- WandB Integration\n- Easily create TFRecord from Directory\n- Tensorboard visualizations\n- Ensemble inference\n\n#### Datasets\n\n  - Ade20k\n  - Camvid\n  - Cityscapes\n  - MappingChallenge\n  - MotsChallenge\n  - Coco\n  - PascalVoc2012\n  - Taco\n  - Shapes (randomly creating triangles, rectangles and circles)\n  - Toy (Overlaying TinyImageNet with MNIST)\n  - ISIC2018\n  - CVC-ClinicDB\n\n\n\n#### Models\n\n\n  - [Unet](https://arxiv.org/abs/1505.04597)\n  - [Erfnet](https://arxiv.org/abs/1806.08522)\n  - [MultiResUnet](https://arxiv.org/abs/1902.04049)\n  - [PSP](https://arxiv.org/abs/1612.01105) (experimental)\n  - [FCN](https://arxiv.org/abs/1411.4038) (experimental)\n  - [NestedUnet (Unet++)](https://arxiv.org/abs/1807.10165) (experimental)\n  - [U2Net / U2NetP](https://arxiv.org/abs/2005.09007) (experimental)\n  - SatelliteUnet\n  - MobilenetUnet (unet with mobilenet encoder pre-trained on imagenet)\n  - InceptionResnetV2Unet (unet with inception-resnet v2 encoder pre-trained on imagenet)\n  - ResnetUnet (unet with resnet50 encoder pre-trained on imagenet)\n  - AttentionUnet\n\n#### Losses\n\n  - Catagorical Crossentropy\n  - Binary Crossentropy\n  - Crossentropy + SSIM\n  - Dice\n  - Crossentropy + Dice\n  - Tversky\n  - Focal\n  - Focal + Tversky\n\n#### Activations\n\n  - mish\n  - swish\n  - relu6\n\n#### Optimizers\n\n  - Ranger\n  - RAdam\n\n#### Normalization\n\n  - Instance Norm\n  - Batch Norm\n\n#### On the fly Augmentations\n\n  - flip left/right\n  - flip up/down\n  - rot 180\n  - color\n\n## [Getting Started](#getting-started)\n\n### Requirements\n\n```shell\nsudo apt-get install libsm6 libxext6 libxrender-dev libyaml-dev libpython3-dev\n```\n\n#### Tensorflow (2.x) & Tensorflow Addons (optional)\n\n```shell\npip install tensorflow-gpu==2.4.0 --upgrade\npip install tensorflow-addons==0.12.0 --upgrade\n```\n\n### Installation\n\n```shell\npip install tf-semantic-segmentation\n```\n\n### Run tensorboard\n\n- Hint: To see train/test/val images you have to start tensorboard like this\n\n```bash\ntensorboard --logdir=logs/ --reload_multifile=true\n```\n\n### Train on inbuild datasets (generator)\n\n```bash\npython -m tf_semantic_segmentation.bin.train -ds 'tacobinary' -bs 8 -e 100 \\\n    -logdir 'logs/taco-binary-test' -o 'adam' -lr 5e-3 --size 256,256 \\\n    -l 'binary_crossentropy' -fa 'sigmoid' \\\n    --train_on_generator --gpus='0' \\\n    --tensorboard_train_images --tensorboard_val_images\n```\n\n### Create a tfrecord from a dataset\n```bash\n\n# create a tfrecord from the toy dataset and resize to 128x128\ntf-semantic-segmentation-tfrecord-writer -d 'toy' -c /hdd/datasets/ -s '128,128'\n```\n\n### Train using a fixed record path\n\n```bash\npython -m tf_semantic_segmentation.bin.train --record_dir=records/cityscapes-512x256-rgb/ \\\n    -bs 4 -e 100 -logdir 'logs/cityscapes-bs8-e100-512x256' -o 'adam' -lr 1e-4 -l 'categorical_crossentropy' \\\n    -fa 'softmax' -bufsize 50 --metrics='iou_score,f1_score' -m 'erfnet' --gpus='0' -a 'mish' \\\n    --tensorboard_train_images --tensorboard_val_images\n```\n\n### Multi GPU training\n\n```bash\npython -m tf_semantic_segmentation.bin.train --record_dir=records/cityscapes-512x256-rgb/ \\\n    -bs 4 -e 100 -logdir 'logs/cityscapes-bs8-e100-512x256' -o 'adam' -lr 1e-4 -l 'categorical_crossentropy' \\\n    -fa 'softmax' -bufsize 50 --metrics='iou_score,f1_score' -m 'erfnet' --gpus='0,1,2,3' -a 'mish'\n```\n\n## Using Code\n\n```python\nfrom tf_semantic_segmentation.bin.train import train_test_model, get_args\n\n# get the default args\nargs = get_args({})\n\n# change some parameters\n# !rm -r logs/\nargs.model = 'erfnet'\n# args['color_mode'] = 0\nargs.batch_size = 8\nargs.size = [128, 128] # resize input dataset to this size\nargs.epochs = 10\nargs.learning_rate = 1e-4\nargs.optimizer = 'adam' # ['adam', 'radam', 'ranger']\nargs.loss = 'dice'\nargs.logdir = 'logs'\nargs.record_dir = \"datasets/shapes/records\"\nargs.final_activation = 'softmax'\n\n# train and test\nresults, model = train_test_model(args)\n```\n\n## Models\n\n- Erfnet\n- Unet\n\n```python\nfrom tf_semantic_segmentation import models\n\n# print all available models\nprint(list(modes.models_by_name.keys()))\n\n# returns a model (without the final activation function)\nmodel = models.get_model_by_name('erfnet', {\"input_shape\": (128, 128, 3), \"num_classes\": 5})\n\n# call models directly\nmodel = models.erfnet(input_shape=(128, 128), num_classes=5)\n```\n\n## Use your own dataset\n\n- Accepted file types are: jpg(jpeg) and png\n\nIf you already have a train/test/val split then use the following data structure:\n\n```text\ndataset/\n    labels.txt\n    test/\n        images/\n        masks/\n    train/\n        images/\n        masks/\n    val/\n        images/\n        masks/\n```\n\nor use\n\n```text\ndataset/\n    labels.txt\n    images/\n    masks/\n```\n\nThe labels.txt should contain a list of labels separated by newline [/n]. For instance it looks like this:\n\n```text\nbackground\ncar\npedestrian\n```\n\n- To create a tfrecord using the original image size and color use the script like this:\n\n```shell\nINPUT_DIR = ...\ntf-semantic-segmentation-tfrecord-writer -dir $INPUT_DIR -r $INPUT_DIR/records\n```\n\nThere are the following addition arguments:\n\n- -s [--size] '$width,$height' (f.e. \"512,512\")\n- -rm [--resize_method] ('resize', 'resize_with_pad', 'resize_with_crop_or_pad)\n- cm [--color_mode] (0=RGB, 1=GRAY, 2=NONE (default))\n\n\n\n## Datasets\n\n```python\nfrom tf_semantic_sementation.datasets import get_dataset by name, datasets_by_name, DataType, get_cache_dir\n\n# print availiable dataset names\nprint(list(datasets_by_name.keys()))\n\n# get the binary (waste or not) dataset\ndata_dir = '/hdd/data/'\nname = 'tacobinary'\ncache_dir = get_cache_dir(data_dir, name.lower())\nds = get_dataset_by_name(name, cache_dir)\n\n# print labels and classes\nprint(ds.labels)\nprint(ds.num_classes)\n\n# print number of training examples\nprint(ds.num_examples(DataType.TRAIN))\n\n# or simply print the summary\nds.summary()\n```\n\nDebug datasets\n\n```bash\npython -m tf_semantic_segmentation.debug.dataset_vis -d ade20k\n```\n\n## TFRecords\n\n#### This library simplicifies the process of creating a tfrecord dataset for faster training.\n\nWrite tfrecords:\n\n```python\nfrom tf_semantic_segmentation.datasets import TFWriter\nds = ...\nwriter = TFWriter(record_dir)\nwriter.write(ds)\nwriter.validate(ds)\n```\n\nor use simple with this script (will be save with size 128 x 128 (width x height)):\n\n```bash\ntf-semantic-segmentation-tfrecord-writer -d 'toy' -c /hdd/datasets/ -s '128,128'\n```\n\nAnalyse already written tfrecord (with mean)\n\n```bash\npython -m tf_semantic_segmentation.bin.tfrecord_analyser -r records/ --mean\n```\n\n## Docker\n\n```shell\ndocker build -t tf_semantic_segmentation -f docker/Dockerfile ./\n```\n\nor pull the latest release\n\n```shell\ndocker pull baudcode/tf_semantic_segmentation:latest\n```\n\n## Prediction\n\n```shell\npip install matplotlib\n```\n\n#### Using Code\n\n```python\nfrom tensorflow.keras.models import load_model\nimport numpy as np\nfrom tf_semantic_segmentation.processing import dataset\nfrom tf_semantic_segmentation.visualizations import show, masks\n\n\nmodel = load_model('logs/model-best.h5', compile=False)\n\n# model parameters\nsize = tuple(model.input.shape[1:3])\ndepth = model.input.shape[-1]\ncolor_mode = dataset.ColorMode.GRAY if depth == 1 else dataset.ColorMode.RGB\n\n# define an image\nimage = np.zeros((256, 256, 3), np.uint8)\n\n# preprocessing\nimage = image.astype(np.float32) / 255.\nimage, _ = dataset.resize_and_change_color(image, None, size, color_mode, resize_method='resize')\n\nimage_batch = np.expand_dims(image, axis=0)\n\n# predict (returns probabilities)\np = model.predict(image_batch)\n\n# draw segmentation map\nnum_classes = p.shape[-1] if p.shape[-1] > 1 else 2\npredictions_rgb = masks.get_colored_segmentation_mask(p, num_classes, images=image_batch, binary_threshold=0.5)\n\n# show images using matplotlib\nshow.show_images([predictions_rgb[0], image_batch[0]])\n```\n\n#### Using scripts\n\n- On image\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5  -i image.png\n```\n\n- On TFRecord (data type 'val' is default)\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -r records/camvid/\n```\n\n- On TFRecord (with export to directory)\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -r records/cubbinary/ -o out/ -rm 'resize_with_pad'\n```\n\n- On Video\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -v video.mp4\n```\n\n- On Video (with export to out/p-video.mp4)\n\n```shell\npython -m tf_semantic_segmentation.evaluation.predict -m model-best.h5 -v video.mp4 -o out/\n```\n\n## Prediction using Tensorflow Model Server\n\n- Installation\n\n```bash\n# install\necho \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\ncurl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\nsudo apt-get update && apt-get install tensorflow-model-server\n```\n\n- Start Model Server\n\n```bash\n### using a single model\ntensorflow_model_server --rest_api_port=8501 --model_base_path=/home/user/models/mymodel/saved_model\n\n### or using an ensamble of multiple models\n\n# helper to write the ensamble config yaml file (models/ contains multiple logdirs/, logdir must contain the name 'unet')\npython -m tf_semantic_segmentation.bin.model_server_config_writer -d models/ -c 'unet'\n# start model server with written models.yaml\ntensorflow_model_server --model_config_file=models.yaml --rest_api_port=8501\n```\n\n### **Compare models and ensemnble**\n\n```bash\npython -m tf_semantic_segmentation.evaluation.compare_models -i logs/ -c 'taco' -data /hdd/datasets/ -d 'tacobinary'\n```\n\nParameters:\n\n- _-i_ (directory containing models)\n- _-c_ (model name (directory name) must contain this value)\n- _-data_ (data directory)\n- _-d_ (dataset name)\n\nUse **--help** to get more help\n\n#### Using Code\n\n```python\nfrom tf_semantic_segmentation.serving import predict, predict_on_batch, ensamble_prediction, get_models_from_directory\nfrom tf_semantic_segmentation.processing.dataset import resize_and_change_color\n\nimage = np.zeros((128, 128, 3))\nimage_size = (256, 256)\ncolor_mode = 0  # 0=RGB, 1=GRAY\nresize_method = 'resize'\nscale_mask = False # only scale mask when model output is scaled using sigmoid activation\nnum_classes = 3\n\n# preprocess image\nimage = image.astype(np.float32) / 255.\nimage, _ = resize_and_change_color(image, None, image_size, color_mode, resize_method='resize')\n\n# prediction on 1 image\np = predict(image.numpy(), host='localhost', port=8501, input_name='input_1', model_name='0')\n\n#############################################################################################################\n# if the image size should not match, the color mode does not match or the model_name does not match\n# you'll most likely get a `400 Client Error: Bad Request for url: http://localhost:8501/v1/models/0:predict`\n# hint: if you only started 1 model try using model_name 'default'\n#############################################################################################################\n\n# prediction on batch (for faster prediction of multiple images)\np = predict_on_batch([image], host='localhost', port=8501, input_name='input_1', model_name='0')\n\n# ensamble prediction (average the predictions of multiple models)\n\n# either specify models like this:\nmodels = [\n    {\n        \"name\": \"0\",\n        \"path\": \"/home/user/models/mymodel/saved_model/\",\n        \"version\": 0, # optional\n        \"input_name\": \"input_1\"\n    },\n    {\n        \"name\": \"1\",\n        \"path\": \"/home/user/models/mymodel2/saved_model/\",\n        \"input_name\": \"input_1\"\n    }\n]\n\n\n# or load from models in directory (models/) that contain the name 'unet'\nmodels = get_models_from_directory('models/', contains='unet')\n\n# returns the ensamble and all predictions made\nensamble, predictions = ensamble_prediction(models, image.numpy(), host='localhost', port=8501)\n```\n\n## TFLite support\n\n#### Convert the model\n\n```shell\npython -m tf_semantic_segmentation.bin.convert_tflite -i logs/mymodel/saved_model/0/ -o model.tflite\n```\n\n#### Test inference on the model\n\n```shell\npython -m tf_semantic_segmentation.debug.tflite_test -m model.tflite -i Harris_Sparrow_0001_116398.jpg\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/baudcode/tf-semantic-segmentation",
    "keywords": "keras,tensorflow,tf_semantic_segmentation,semantic,segmentation,ade20k,coco,pascalvoc,cityscapes",
    "license": "MIT",
    "maintainer": "Malte Koch",
    "maintainer_email": "malte-koch@gmx.net",
    "name": "tf-semantic-segmentation",
    "package_url": "https://pypi.org/project/tf-semantic-segmentation/",
    "platform": "",
    "project_url": "https://pypi.org/project/tf-semantic-segmentation/",
    "project_urls": {
      "Homepage": "https://github.com/baudcode/tf-semantic-segmentation"
    },
    "release_url": "https://pypi.org/project/tf-semantic-segmentation/0.3.1/",
    "requires_dist": [
      "requests",
      "imageio",
      "opencv-python",
      "wandb",
      "tqdm",
      "scipy",
      "xmltodict",
      "pillow",
      "pytz",
      "pyyaml"
    ],
    "requires_python": "",
    "summary": "Implementation of various semantic segmentation models in tensorflow & keras including popular datasets",
    "version": "0.3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9181211,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63ebfe8cb5888e97efea5529d30b9045a1c14b76f0d2c5e1c8d776d6466c9d64",
          "md5": "0d09ccee2a852a7e6ac6e788060963f2",
          "sha256": "42d33b61faeebc8fb51173d83d2ec086c99bfc66371a0b060719551bf663eb1e"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0d09ccee2a852a7e6ac6e788060963f2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 73713,
        "upload_time": "2020-01-12T21:00:16",
        "upload_time_iso_8601": "2020-01-12T21:00:16.942780Z",
        "url": "https://files.pythonhosted.org/packages/63/eb/fe8cb5888e97efea5529d30b9045a1c14b76f0d2c5e1c8d776d6466c9d64/tf_semantic_segmentation-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bbb91fe68e4ad85f30b01147db25c126e7a55008c9a73d2f4fb4b42d15ec086a",
          "md5": "ee98c2c54bc5e70bc899c931a550aa08",
          "sha256": "e5c9dce69ef5e09f0d7cadedf7a1460f20b7e8e1e6148a19b60b9c868a479d1e"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ee98c2c54bc5e70bc899c931a550aa08",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 50117,
        "upload_time": "2020-01-12T21:00:20",
        "upload_time_iso_8601": "2020-01-12T21:00:20.612708Z",
        "url": "https://files.pythonhosted.org/packages/bb/b9/1fe68e4ad85f30b01147db25c126e7a55008c9a73d2f4fb4b42d15ec086a/tf_semantic_segmentation-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "191cde8d76a75b46382c7cd9adaa63ca760137bd888334d4df844b9a3611730b",
          "md5": "2bff69d714bdae41ef80eb82a9ad2665",
          "sha256": "4381e920c0aaf8e541a00fcb345df121bcbeeb7aca4420f91d732bcb5fbb8818"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2bff69d714bdae41ef80eb82a9ad2665",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 59494,
        "upload_time": "2020-01-23T12:54:29",
        "upload_time_iso_8601": "2020-01-23T12:54:29.376612Z",
        "url": "https://files.pythonhosted.org/packages/19/1c/de8d76a75b46382c7cd9adaa63ca760137bd888334d4df844b9a3611730b/tf_semantic_segmentation-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "add76d7713f48aa55c488e583664d2e891cc438dd9c65a2b4c2eac87312db80c",
          "md5": "7b2a1375e995eb3f3bca1e0e324e0898",
          "sha256": "3f0d7c117f064c040c7b84ab938e5fc53621e5748d1412c7b36bae873d470713"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7b2a1375e995eb3f3bca1e0e324e0898",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 98596,
        "upload_time": "2020-01-23T19:15:19",
        "upload_time_iso_8601": "2020-01-23T19:15:19.977310Z",
        "url": "https://files.pythonhosted.org/packages/ad/d7/6d7713f48aa55c488e583664d2e891cc438dd9c65a2b4c2eac87312db80c/tf_semantic_segmentation-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d89d4c6082c375aadb4df9516ff2d1e999ef97b41b607c7a349cf3f536311cac",
          "md5": "9423927f9b6198918af9eabebfa902b4",
          "sha256": "e0f5d894cf4852df9b00f25e6a852a54c3a9ee1d1169eca16f03541379f59d43"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9423927f9b6198918af9eabebfa902b4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 59370,
        "upload_time": "2020-01-23T19:15:21",
        "upload_time_iso_8601": "2020-01-23T19:15:21.699272Z",
        "url": "https://files.pythonhosted.org/packages/d8/9d/4c6082c375aadb4df9516ff2d1e999ef97b41b607c7a349cf3f536311cac/tf_semantic_segmentation-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d25aace3a95d4891f8765dc1aa655204842744d6f22e65adff8d6003caf4ab1e",
          "md5": "ee1d07af5aa7bb4ff7b89a4aeaf6773e",
          "sha256": "835334c67b6152c45f20d08e3312f217cc7b31f49f7ce036aefd5f53951052c9"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.2.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ee1d07af5aa7bb4ff7b89a4aeaf6773e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 119730,
        "upload_time": "2020-02-17T12:51:54",
        "upload_time_iso_8601": "2020-02-17T12:51:54.133995Z",
        "url": "https://files.pythonhosted.org/packages/d2/5a/ace3a95d4891f8765dc1aa655204842744d6f22e65adff8d6003caf4ab1e/tf_semantic_segmentation-0.2.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bfbe33403006d098b4e41e9a78c222577616abb92aa7e441ca8742f92895f5c7",
          "md5": "5ffcdc5dc5d4c44a7d13a5b3fdf4a46e",
          "sha256": "2b4ad6c529d2945e291be8416cf3c0caeb3e31b2d469f1317b31df11c65fdd73"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "5ffcdc5dc5d4c44a7d13a5b3fdf4a46e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 73476,
        "upload_time": "2020-02-17T12:51:55",
        "upload_time_iso_8601": "2020-02-17T12:51:55.760960Z",
        "url": "https://files.pythonhosted.org/packages/bf/be/33403006d098b4e41e9a78c222577616abb92aa7e441ca8742f92895f5c7/tf_semantic_segmentation-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fbd90d699c714de97b75a69ce920a44c6a93f388931c3894a2f8b159b33233e0",
          "md5": "bc67f23cc6eab03d07196578d82a9fbe",
          "sha256": "d0b72625b8ca26c238b81c22b847e914a9bd6825d4fed2567bdb7e1c79cbc488"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.2.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bc67f23cc6eab03d07196578d82a9fbe",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 149274,
        "upload_time": "2020-03-06T14:04:24",
        "upload_time_iso_8601": "2020-03-06T14:04:24.367262Z",
        "url": "https://files.pythonhosted.org/packages/fb/d9/0d699c714de97b75a69ce920a44c6a93f388931c3894a2f8b159b33233e0/tf_semantic_segmentation-0.2.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "04d112f07405ec75028efc3768d1b2eb93f26bd3289c1b54be859a79e6d947cd",
          "md5": "7d24ebe7d3c6e64a4a9d59941ecf45a2",
          "sha256": "27e70c5945db37ad303989e75d4e2aed1654d6313daff9aab19f79e18a076ca3"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "7d24ebe7d3c6e64a4a9d59941ecf45a2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 95933,
        "upload_time": "2020-03-06T14:04:25",
        "upload_time_iso_8601": "2020-03-06T14:04:25.861212Z",
        "url": "https://files.pythonhosted.org/packages/04/d1/12f07405ec75028efc3768d1b2eb93f26bd3289c1b54be859a79e6d947cd/tf_semantic_segmentation-0.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5cb5811b1be421b66d106c93016892c247744633fa59716ac1eaf0f71157357d",
          "md5": "e9dc90e2a8cdf65ef64f4745b95438b6",
          "sha256": "bdb097341f7c6369af4a9f7834eb78af110f8eca9d269b4c7d0e926ec5024e7b"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e9dc90e2a8cdf65ef64f4745b95438b6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 172530,
        "upload_time": "2021-01-17T20:26:04",
        "upload_time_iso_8601": "2021-01-17T20:26:04.157264Z",
        "url": "https://files.pythonhosted.org/packages/5c/b5/811b1be421b66d106c93016892c247744633fa59716ac1eaf0f71157357d/tf_semantic_segmentation-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f9249d19b1fe7449040cc5492b2c8770d0b949919b8aac1d9079d34da385ba5d",
          "md5": "cd9cd003b20f39f64150d842d57ab3b1",
          "sha256": "bcbb5608feebb09dd4f2b624aba9e7a99b8703e6f59e2c10e3270cce20355fa7"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "cd9cd003b20f39f64150d842d57ab3b1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 112280,
        "upload_time": "2021-01-17T20:26:05",
        "upload_time_iso_8601": "2021-01-17T20:26:05.529621Z",
        "url": "https://files.pythonhosted.org/packages/f9/24/9d19b1fe7449040cc5492b2c8770d0b949919b8aac1d9079d34da385ba5d/tf_semantic_segmentation-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c0ae3782fc4f08e45bc06c3ae33f5000ef58254da55dfb6631802051245d2acd",
          "md5": "d9c48a72bc007e2f838b52e4727c1844",
          "sha256": "de3762fdd1e7f2d49055247f2e27433d0aea457586a19a17d502ac01491f6ab6"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d9c48a72bc007e2f838b52e4727c1844",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 172571,
        "upload_time": "2021-01-20T10:10:29",
        "upload_time_iso_8601": "2021-01-20T10:10:29.707890Z",
        "url": "https://files.pythonhosted.org/packages/c0/ae/3782fc4f08e45bc06c3ae33f5000ef58254da55dfb6631802051245d2acd/tf_semantic_segmentation-0.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "636c646d3fbcd2edb6ad15b3ae38d9157b9fe67439b979ef055934f701e464b9",
          "md5": "6ac801b4eb4dd00e934fe83c36ac3e7e",
          "sha256": "73150e7b9b3c87fe28490e404466d69b5dc4b3f195c8f45c69fc4a592f82edfd"
        },
        "downloads": -1,
        "filename": "tf_semantic_segmentation-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "6ac801b4eb4dd00e934fe83c36ac3e7e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 112310,
        "upload_time": "2021-01-20T10:10:31",
        "upload_time_iso_8601": "2021-01-20T10:10:31.106717Z",
        "url": "https://files.pythonhosted.org/packages/63/6c/646d3fbcd2edb6ad15b3ae38d9157b9fe67439b979ef055934f701e464b9/tf_semantic_segmentation-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c0ae3782fc4f08e45bc06c3ae33f5000ef58254da55dfb6631802051245d2acd",
        "md5": "d9c48a72bc007e2f838b52e4727c1844",
        "sha256": "de3762fdd1e7f2d49055247f2e27433d0aea457586a19a17d502ac01491f6ab6"
      },
      "downloads": -1,
      "filename": "tf_semantic_segmentation-0.3.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d9c48a72bc007e2f838b52e4727c1844",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 172571,
      "upload_time": "2021-01-20T10:10:29",
      "upload_time_iso_8601": "2021-01-20T10:10:29.707890Z",
      "url": "https://files.pythonhosted.org/packages/c0/ae/3782fc4f08e45bc06c3ae33f5000ef58254da55dfb6631802051245d2acd/tf_semantic_segmentation-0.3.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "636c646d3fbcd2edb6ad15b3ae38d9157b9fe67439b979ef055934f701e464b9",
        "md5": "6ac801b4eb4dd00e934fe83c36ac3e7e",
        "sha256": "73150e7b9b3c87fe28490e404466d69b5dc4b3f195c8f45c69fc4a592f82edfd"
      },
      "downloads": -1,
      "filename": "tf_semantic_segmentation-0.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "6ac801b4eb4dd00e934fe83c36ac3e7e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 112310,
      "upload_time": "2021-01-20T10:10:31",
      "upload_time_iso_8601": "2021-01-20T10:10:31.106717Z",
      "url": "https://files.pythonhosted.org/packages/63/6c/646d3fbcd2edb6ad15b3ae38d9157b9fe67439b979ef055934f701e464b9/tf_semantic_segmentation-0.3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}