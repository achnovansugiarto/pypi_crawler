{
  "info": {
    "author": "Yulong Li",
    "author_email": "yulongl@us.ibm.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Notes for IBM repository\n\nThis repository is built on https://github.com/stanford-futuredata/ColBERT. \n\n### Highlights of code changes (More content will be added):\n\n* Important change to training module.\n  * Including:\n    * New training loop logic and data loader/batcher.\n    * Flexible shuffling when needed (e.g., each epoch).\n    * Training and checkpoint saving by epochs or fraction of epochs.\n  * Without this, training would be on static set, and by steps, manual computation would be needed to get number of steps based on data size and hyper-parameters.\n  * Essential for production where we need training for multiple epochs.\n\n\n---------------\n\nThe README below is also adapted to reflect the above mentioned changes.\n\n# ColBERT\n\n### ColBERT is a _fast_ and _accurate_ retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.\n\n### NOTE:  For the documentation on binarized embeddings, jump to [Binarized embeddings](#binarized-embeddings).\n\n<p align=\"center\">\n  <img align=\"center\" src=\"docs/images/ColBERT-Framework-MaxSim-W370px.png\" />\n</p>\n<p align=\"center\">\n  <b>Figure 1:</b> ColBERT's late interaction, efficiently scoring the fine-grained similarity between a queries and a passage.\n</p>\n\nAs Figure 1 illustrates, ColBERT relies on fine-grained **contextual late interaction**: it encodes each passage into a **matrix** of token-level embeddings (shown above in blue). Then at search time, it embeds every query into another matrix (shown in green) and efficiently finds passages that contextually match the query using scalable vector-similarity (`MaxSim`) operators.\n\nThese rich interactions allow ColBERT to surpass the quality of _single-vector_ representation models, while scaling efficiently to large corpora. You can read more in our papers:\n\n* [**ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT**](https://arxiv.org/abs/2004.12832) (SIGIR'20).\n* [**Relevance-guided Supervision for OpenQA with ColBERT**](https://arxiv.org/abs/2007.00814) (TACL'21; to appear).\n\n\n----\n\n## Installation\n\nColBERT (currently: [v0.2.0](#releases)) requires Python 3.7+ and Pytorch 1.6+ and uses the [HuggingFace Transformers](https://github.com/huggingface/transformers) library.\n\nWe strongly recommend creating a conda environment using:\n\n```\nconda env create -f conda_env.yml\nconda activate colbert-v0.2\n```\n\nIf you face any problems, please [open a new issue](https://github.com/stanford-futuredata/ColBERT/issues) and we'll help you promptly!\n\n\n## Overview\n\nUsing ColBERT on a dataset typically involves the following steps.\n\n**Step 0: Preprocess your collection.** At its simplest, ColBERT works with tab-separated (TSV) files: a file (e.g., `collection.tsv`) will contain all passages and another (e.g., `queries.tsv`) will contain a set of queries for searching the collection.\n\n**Step 1: Train a ColBERT model.**  You can [train your own ColBERT model](#training) and [validate performance](#validation) on a suitable development set.\n\n**Step 2: Index your collection.** Once you're happy with your ColBERT model, you need to [index your collection](#indexing) to permit fast retrieval. This step encodes all passages into matrices, stores them on disk, and builds data structures for efficient search.\n\n**Step 3: Search the collection with your queries.** Given your model and index, you can [issue queries over the collection](#retrieval) to retrieve the top-k passages for each query.\n\nBelow, we illustrate these steps via an example run on the MS MARCO Passage Ranking task.\n\n\n## Data\n\nThis repository works directly with a simple **tab-separated file** format to store queries, passages, and top-k ranked lists.\n\n\n* Queries: each line is `qid \\t query text`.\n* Collection: each line is `pid \\t passage text`.\n* Top-k Ranking: each line is `qid \\t pid \\t rank`.\n\nThis works directly with the data format of the [MS MARCO Passage Ranking](https://github.com/microsoft/MSMARCO-Passage-Ranking) dataset. You will need the training triples (`triples.train.small.tar.gz`), the official top-1000 ranked lists for the dev set queries (`top1000.dev`), and the dev set relevant passages (`qrels.dev.small.tsv`). For indexing the full collection, you will also need the list of passages (`collection.tar.gz`).\n\n\n\n## Training\n\nTraining requires a list of _<query, positive passage, negative passage>_ tab-separated triples.\n\nYou can supply **full-text** triples, where each line is `query text \\t positive passage text \\t negative passage text`. Alternatively, you can supply the query and passage **IDs** as a JSONL file `[qid, pid+, pid-]` per line, in which case you should specify `--collection path/to/collection.tsv` and `--queries path/to/queries.train.tsv`.\n\n\n```\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" \\\npython -m torch.distributed.launch --nproc_per_node=4 -m \\\ncolbert.train --amp --doc_maxlen 180 --mask-punctuation --bsize 32 --accum 1 \\\n--triples /path/to/MSMARCO/triples.train.small.tsv \\\n--root /root/to/experiments/ --experiment MSMARCO-psg --similarity l2 --run msmarco.psg.l2\n```\n\nYou can use one or more GPUs by modifying `CUDA_VISIBLE_DEVICES` and `--nproc_per_node`.\n\n* Training can be specified to by epochs with `--epochs`. You can specify when to save checkpoints, with `--save_epochs` or `--save_steps`.\n\n* You can choose to shuffle the data at the beginning of each epoch with `--shuffle_every_epoch`.\n\n## Validation\n\nBefore indexing into ColBERT, you can compare a few checkpoints by re-ranking a top-k set of documents per query. This will use ColBERT _on-the-fly_: it will compute document representations _during_ query evaluation.\n\nThis script requires the top-k list per query, provided as a tab-separated file whose every line contains a tuple `queryID \\t passageID \\t rank`, where rank is {1, 2, 3, ...} for each query. The script also accepts the format of MS MARCO's `top1000.dev` and `top1000.eval` and you can optionally supply relevance judgements (qrels) for evaluation. This is a tab-separated file whose every line has a quadruple _<query ID, 0, passage ID, 1>_, like `qrels.dev.small.tsv`.\n\nExample command:\n\n```\npython -m colbert.test --amp --doc_maxlen 180 --mask-punctuation \\\n--collection /path/to/MSMARCO/collection.tsv \\\n--queries /path/to/MSMARCO/queries.dev.small.tsv \\\n--topk /path/to/MSMARCO/top1000.dev  \\\n--checkpoint /root/to/experiments/MSMARCO-psg/train.py/msmarco.psg.l2/checkpoints/colbert-200000.dnn \\\n--root /root/to/experiments/ --experiment MSMARCO-psg  [--qrels path/to/qrels.dev.small.tsv]\n```\n\n\n## Indexing\n\nFor fast retrieval, indexing precomputes the ColBERT representations of passages.\n\nExample command:\n\n```\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" OMP_NUM_THREADS=6 \\\npython -m torch.distributed.launch --nproc_per_node=4 -m \\\ncolbert.index --amp --doc_maxlen 180 --mask-punctuation --bsize 256 \\\n--checkpoint /root/to/experiments/MSMARCO-psg/train.py/msmarco.psg.l2/checkpoints/colbert-200000.dnn \\\n--collection /path/to/MSMARCO/collection.tsv \\\n--index_root /root/to/indexes/ --index_name MSMARCO.L2.32x200k \\\n--root /root/to/experiments/ --experiment MSMARCO-psg\n```\n\nThe index created here allows you to re-rank the top-k passages retrieved by another method (e.g., BM25).\n\nWe typically recommend that you use ColBERT for **end-to-end** retrieval, where it directly finds its top-k passages from the full collection. For this, you need FAISS indexing.\n\n\n#### FAISS Indexing for end-to-end retrieval\n\nFor end-to-end retrieval, you should index the document representations into [FAISS](https://github.com/facebookresearch/faiss).\n\n```\npython -m colbert.index_faiss \\\n--index_root /root/to/indexes/ --index_name MSMARCO.L2.32x200k \\\n--partitions 32768 --sample 0.3 \\\n--root /root/to/experiments/ --experiment MSMARCO-psg\n```\n\n\n## Retrieval\n\nIn the simplest case, you want to retrieve from the full collection:\n\n```\npython -m colbert.retrieve \\\n--amp --doc_maxlen 180 --mask-punctuation --bsize 256 \\\n--queries /path/to/MSMARCO/queries.dev.small.tsv \\\n--nprobe 32 --partitions 32768 --faiss_depth 1024 \\\n--index_root /root/to/indexes/ --index_name MSMARCO.L2.32x200k \\\n--checkpoint /root/to/experiments/MSMARCO-psg/train.py/msmarco.psg.l2/checkpoints/colbert-200000.dnn \\\n--root /root/to/experiments/ --experiment MSMARCO-psg\n```\n\nYou may also want to re-rank a top-k set that you've retrieved before with ColBERT or with another model. For this, use `colbert.rerank` similarly and additionally pass `--topk`.\n\nIf you have a large set of queries (or want to reduce memory usage), use **batch-mode** retrieval and/or re-ranking. This can be done by passing `--batch --retrieve_only` to `colbert.retrieve` and passing `--batch --log-scores` to colbert.rerank alongside `--topk` with the `unordered.tsv` output of this retrieval run.\n\nSome use cases (e.g., building a user-facing search engines) require more control over retrieval. For those, you typically don't want to use the command line for retrieval. Instead, you want to import our retrieval API from Python and directly work with that (e.g., to build a simple REST API). Instructions for this are coming soon, but you will just need to adapt/modify the retrieval loop in [`colbert/ranking/retrieval.py#L33`](colbert/ranking/retrieval.py#L33).\n\n\n## Binarized embeddings\n\nColBERT can apply binarization to compress embeddings as bit vectors before storing them on disk.\n\n### Prerequisites\n\nThe provided conda environment contains all necessary dependencies, but for those working off an existing ColBERT environment the following new dependencies will need to be installed:\n\n```\nconda install -c conda-forge cupy\npip install bitarray\n```\n\n### Indexing\n\nThe training process remains unmodified---you can re-use your existing model checkpoints. The following command can be used for indexing with compression. This example will compress embeddings to 1 bit per embedding dimension.\n\n```\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" OMP_NUM_THREADS=6 \\\npython -m colbert.index_with_compression \\\n--nproc_per_node=4 --amp --doc_maxlen 180 --mask-punctuation --bsize 64 \\\n--checkpoint /root/to/experiments/MSMARCO-psg/train.py/msmarco.psg.l2/checkpoints-colbert-200000.dnn \\\n--collection /path/to/MSMARCO/collection.tsv \\\n--index_root /root/to/indexes --index_name MSMARCO.L2.32x200k_compression=1 \\\n--root /root/to/experiments/ --experiment MSMARCO-psg\n--compression_level 1 --compression_thresholds /path/to/compression_thresholds.csv \\\n--partitions 32768 --sample 0.01\n```\n\nThe `--compression_level` argument controls how many bits to use per embedding dimension (the default without compression is 16). The `--compression_thresholds [compression_thresholds.csv]` argument explicitly configures the binarization thresholds. We provide an example file below. Each line of this file must first include the number of bits _b_, and then 2<sup>_b_</sup> + 1 comma-separated threshold values in increasing order. Note that the median threshold value will be discarded, and thresholds will be applied according to the [torch.bucketize](https://pytorch.org/docs/stable/generated/torch.bucketize.html) convention with `right=True`. For example, this file would produce uniform thresholds for 1, 2, or 3 bits:\n```\n1,-0.1,0,0.1\n2,-0.1,-0.05,0,0.05,0.1\n3,-0.1,-0.075,-0.05,-0.025,0,0.025,0.05,0.075,0.1\n```\n\n\nColBERT trains and constructs the FAISS index on-the-fly when indexing with compression, so no additional command is necessary for this step. Instead, the indexing command simply accepts two additional arguments: `--partitions` controls the number of partitions used by the FAISS index, and `--sample` controls the fraction of embeddings used as FAISS index training data. If you observe excessive memory usage, we suggest lowering the batch size first, then also lowering the FAISS training sample fraction (`--sample`) if needed.\n\n### Retrieval\n\nAfter indexing is complete, retrieval and re-ranking proceed as usual but with the following additional arguments repeated from the indexing step:\n```\n--compression_level 1 --compression_thresholds /path/to/compression_thresholds.csv\n```\n\n### Preliminary results\n\nThis is the initial working version of binarization in ColBERT, generalizing the recent [BPR](https://arxiv.org/abs/2106.00882) single-vector system. We plan to release the final version of compression as well as a full report on our findings in August 2021.\n\nPreliminary results show that compression can reduce embedding storage cost by up 16x for MS MARCO embeddings without significant performance degredation. Note that for end-to-end retrieval we currently do not alter the size of the FAISS index, though we are working on releasing more aggressive forms of compression that replace the FAISS index as well.\n\n\n| # Bits per dim | MRR@10 | Recall@50 |\n|:--------------:|:------:|:---------:|\n|  16 (original) |   36.2 |      82.1 |\n|              2 |   35.7 |      81.8 |\n|              1 |   34.8 |      80.5 |\n\n## Releases\n\n* v0.2.0: Sep 2020\n* v0.1.0: June 2020\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.ibm.com/mnlp-exp/ColBERT",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "neuralIR",
    "package_url": "https://pypi.org/project/neuralIR/",
    "platform": null,
    "project_url": "https://pypi.org/project/neuralIR/",
    "project_urls": {
      "Bug Tracker": "https://github.ibm.com/mnlp-exp/ColBERT",
      "Homepage": "https://github.ibm.com/mnlp-exp/ColBERT"
    },
    "release_url": "https://pypi.org/project/neuralIR/0.0.25/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "test package for in-house colbert",
    "version": "0.0.25",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13645468,
  "releases": {
    "0.0.24": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4a81958d0f280f443c05e3cb2efb45a2ce88b62d2072e54d038f9018db3a74f9",
          "md5": "46bdb480fcdf1824c4c6622890bd4956",
          "sha256": "ff7a615297187c3184a542ec42982cc22e7d5d548dfcb6c4387bf463fb5cf1e1"
        },
        "downloads": -1,
        "filename": "neuralIR-0.0.24-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "46bdb480fcdf1824c4c6622890bd4956",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 78261,
        "upload_time": "2022-04-27T15:43:22",
        "upload_time_iso_8601": "2022-04-27T15:43:22.796338Z",
        "url": "https://files.pythonhosted.org/packages/4a/81/958d0f280f443c05e3cb2efb45a2ce88b62d2072e54d038f9018db3a74f9/neuralIR-0.0.24-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d2686c27dad1ed469a85b3086e5c135903799ebbef2a3abc730b473b23aca6aa",
          "md5": "1818cb40c8940c586e1ed7ab21ff951e",
          "sha256": "7f110ca954849275021963a91f8875f5d82a4cf1e5e7987a75d736e34bedcaaf"
        },
        "downloads": -1,
        "filename": "neuralIR-0.0.24.tar.gz",
        "has_sig": false,
        "md5_digest": "1818cb40c8940c586e1ed7ab21ff951e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 52928,
        "upload_time": "2022-04-27T15:43:24",
        "upload_time_iso_8601": "2022-04-27T15:43:24.507641Z",
        "url": "https://files.pythonhosted.org/packages/d2/68/6c27dad1ed469a85b3086e5c135903799ebbef2a3abc730b473b23aca6aa/neuralIR-0.0.24.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.25": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "66bdae39b8130a42829ef196db274093de25f68ca777b1b431a0767e185914b5",
          "md5": "6efb839eae798cadb1e2393594869001",
          "sha256": "efe0a974676cbda169bc63f18a9e383a91d08626f146adc0d090087bf1a3f039"
        },
        "downloads": -1,
        "filename": "neuralIR-0.0.25-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6efb839eae798cadb1e2393594869001",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 78078,
        "upload_time": "2022-04-27T23:51:38",
        "upload_time_iso_8601": "2022-04-27T23:51:38.976380Z",
        "url": "https://files.pythonhosted.org/packages/66/bd/ae39b8130a42829ef196db274093de25f68ca777b1b431a0767e185914b5/neuralIR-0.0.25-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e5bb8b3fb53a3f2974d8b89b6b6bc65ce1a960ee64a809edb67b26a4aba0a7e2",
          "md5": "3f2ac5b0ed7ba473285af2d501dcb378",
          "sha256": "445c6481dbcd57b2e853cbdf8967cd36d655c19876feb60b1ae25292debe322e"
        },
        "downloads": -1,
        "filename": "neuralIR-0.0.25.tar.gz",
        "has_sig": false,
        "md5_digest": "3f2ac5b0ed7ba473285af2d501dcb378",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 52595,
        "upload_time": "2022-04-27T23:51:40",
        "upload_time_iso_8601": "2022-04-27T23:51:40.667300Z",
        "url": "https://files.pythonhosted.org/packages/e5/bb/8b3fb53a3f2974d8b89b6b6bc65ce1a960ee64a809edb67b26a4aba0a7e2/neuralIR-0.0.25.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "66bdae39b8130a42829ef196db274093de25f68ca777b1b431a0767e185914b5",
        "md5": "6efb839eae798cadb1e2393594869001",
        "sha256": "efe0a974676cbda169bc63f18a9e383a91d08626f146adc0d090087bf1a3f039"
      },
      "downloads": -1,
      "filename": "neuralIR-0.0.25-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "6efb839eae798cadb1e2393594869001",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 78078,
      "upload_time": "2022-04-27T23:51:38",
      "upload_time_iso_8601": "2022-04-27T23:51:38.976380Z",
      "url": "https://files.pythonhosted.org/packages/66/bd/ae39b8130a42829ef196db274093de25f68ca777b1b431a0767e185914b5/neuralIR-0.0.25-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e5bb8b3fb53a3f2974d8b89b6b6bc65ce1a960ee64a809edb67b26a4aba0a7e2",
        "md5": "3f2ac5b0ed7ba473285af2d501dcb378",
        "sha256": "445c6481dbcd57b2e853cbdf8967cd36d655c19876feb60b1ae25292debe322e"
      },
      "downloads": -1,
      "filename": "neuralIR-0.0.25.tar.gz",
      "has_sig": false,
      "md5_digest": "3f2ac5b0ed7ba473285af2d501dcb378",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 52595,
      "upload_time": "2022-04-27T23:51:40",
      "upload_time_iso_8601": "2022-04-27T23:51:40.667300Z",
      "url": "https://files.pythonhosted.org/packages/e5/bb/8b3fb53a3f2974d8b89b6b6bc65ce1a960ee64a809edb67b26a4aba0a7e2/neuralIR-0.0.25.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}