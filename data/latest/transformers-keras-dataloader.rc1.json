{
  "info": {
    "author": "Pratik Deoolwadikar",
    "author_email": "pratik.deoolwadikar@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Transformers Keras Dataloader ðŸ”Œ\n**Transformers Keras Dataloader** provides an EmbeddingDataloader class, a subclass of **keras.utils.Sequence** which enables real-time data feeding to your Keras model via ***batches***, hence making it possible to train with large datasets while overcoming the problem of loading the entire dataset in the memory prior to training.\n\n**EmbeddingDataloader** inherently is a generator which works by implementing functions required by Keras to get new batch of data from your dataset while fitting and predicting. We leverage this generator concept by real time processing of data while fitting and predicting, which unlocks the capacity to **handle bigger datasets** and use **larger batch size**.\nWhen generating a training batch, for each sequence in the batch we get its embedding (either ***word embedding*** ***or sentence embedding***) by utilizing [**Huggingface's transformers**](https://huggingface.co/transformers/index.html) package.\n\nWe have also given option(parameter) to **utilize** **GPU**(if available) for both data(storing/processing) and model(forward pass), Aditionally you can also utilize **multiprocessing** to proces your dataset on multiple cores in real time and feed it right away to your downstream model while fitting.\n\nThis package also provides support for custom **'Pooling Strategies & Layer Choices'** hence enabling you to feed different vector combinations as input features to your downstream model. This is partially demonstrated through prior experiments, that different layers of pretrained model encode very different kinds of information, so the appropriate pooling layer or strategy will change depending on the application because different layers encode different kinds of information, please look at Arguments or usage section for info on how to utilize.\n\n##  Installation\n```sh\n$ pip install transformers-keras-dataloader\n```\n##  Usage\n\n### Using EmbeddingDataloader class\n\nTrain by using EmbeddingDataloader class to generate **word embeddings** for downstream training\n```\nfrom transformers_keras_dataloader import load_pretrained_model_and_tokenizer\nfrom transformers_keras_dataloader import EmbeddingDataloader\n\n\ntokenizer, model = load_pretrained_model_and_tokenizer(pretrained_model_name_or_path = \"bert-base-uncased\",\n                                                       return_model = True,\n                                                       use_cuda = True)\n\n\ntrain_dataloader = EmbeddingDataloader(embedding_type=\"word\", model=model, tokenizer=tokenizer,\n                                       X=train_X, batch_size=64,\n                                       max_length=100, sampler='random',\n                                       y=train_y, num_classes=2, get_one_hot_label=True,\n                                       use_gpu=True,\n                                       pooling_layer_number=11, policy_dict=None, oov='avg', infer_oov_after_embed=False)\n\nval_dataloader = EmbeddingDataloader(embedding_type=\"word\", model=model, tokenizer=tokenizer,\n                                     X=val_X, batch_size=64,\n                                     max_length=100, sampler='random',\n                                     y=val_y, num_classes=2, get_one_hot_label=True,\n                                     use_gpu=True,\n                                     pooling_layer_number=11, policy_dict=None, oov='avg', infer_oov_after_embed=False)\n\ntest_dataloader = EmbeddingDataloader(embedding_type=\"word\", model=model, tokenizer=tokenizer,\n                                      X=test_X, batch_size=64,\n                                      max_length=100, sampler='random',\n                                      use_gpu=True,\n                                      pooling_layer_number=11, policy_dict=None, oov='avg', infer_oov_after_embed=False)\n\n# Define Downstream model\ndef model_arch_for_word_embeddings(embedding_vector_len, num_output_neurons):\n\tmodel = Sequential()\n\tmodel.add(Dense(1500, activation='relu', batch_input_shape=(None, None, embedding_vector_len))) \n\t[...] # Architecture\n\tmodel.add(Dense(num_output_neurons, activation='sigmoid'))\n\tmodel.compile()\n\treturn model\n\ndownstream_model = model_arch_for_word_embeddings(embedding_vector_len=768, num_output_neurons=2) \n\n## since bert outputs 768-hidden\n\n# Fit \nhistory = downstream_model.fit(train_dataloader, val_dataloader=val_dataloader, epochs=2)\n# Predict\npredictions = downstream_model.predict(test_dataloader)\n```\n\nTrain by using EmbeddingDataloader class to generate **sentence embeddings** for downstream training\n```\nfrom transformers_keras_dataloader import load_pretrained_model_and_tokenizer\nfrom transformers_keras_dataloader import EmbeddingDataloader\n\n\ntokenizer, model = load_pretrained_model_and_tokenizer(pretrained_model_name_or_path = \"bert-base-uncased\",\n                                                       return_model = True,\n                                                       use_cuda = True)\n\n\ntrain_dataloader = EmbeddingDataloader(embedding_type=\"sentence\", model=model, tokenizer=tokenizer,\n                                       X=train_X, batch_size=64,\n                                       max_length=100, sampler='random',\n                                       y=train_y, num_classes=2, get_one_hot_label=True,\n                                       use_gpu=True,\n                                       pooling_layer_number=11, policy_dict=None)\n\nval_dataloader = EmbeddingDataloader(embedding_type=\"sentence\", model=model, tokenizer=tokenizer,\n                                     X=val_X, batch_size=64,\n                                     max_length=100, sampler='random',\n                                     y=val_y, num_classes=2, get_one_hot_label=True,\n                                     use_gpu=True,\n                                     pooling_layer_number=11, policy_dict=None)\n\ntest_dataloader = EmbeddingDataloader(embedding_type=\"sentence\", model=model, tokenizer=tokenizer,\n                                      X=test_X, batch_size=64,\n                                      max_length=100, sampler='random',\n                                      use_gpu=True,\n                                      pooling_layer_number=11, policy_dict=None)\n\n# Define Downstream model\ndef model_arch_for_sentence_embeddings(embedding_vector_len, num_output_neurons):\n\tmodel = Sequential()\n\tmodel.add(Dense(1500, activation='relu', batch_input_shape=(None, embedding_vector_len))) \n\t[...] # Architecture\n\tmodel.add(Dense(num_output_neurons, activation='sigmoid'))\n\tmodel.compile()\n\treturn model\n\ndownstream_model = model_arch_for_word_embeddings(embedding_vector_len=768,\n                                                  num_output_neurons=2) \n\n## since bert outputs 768-hidden\n\n# Fit \nhistory = downstream_model.fit(train_dataloader, val_dataloader=val_dataloader, epochs=2)\n# Predict\npredictions = downstream_model.predict(test_dataloader)\n```\n### Using WordEmbedder class\n\nA simple **WordEmbedder** class which can be used to generate word embeddings for text sequences by utilizing **Huggingface's transformers** package.\n```\nfrom transformers_keras_dataloader import load_pretrained_model_and_tokenizer\nfrom transformers_keras_dataloader import WordEmbedder\n\n\ntokenizer, model = load_pretrained_model_and_tokenizer(pretrained_model_name_or_path = \"bert-base-uncased\",\n                                                       return_model = True,\n                                                       use_cuda = True)\n\nword_embedder = WordEmbedder(max_length=100)\n\n# Generate word embeddings for X \nword_embeddings = word_embedder.prepare_embeddings(X=X, tokenizer=tokenizer, model=model)\n```\n### Using SentenceEmbedder class\n\nA simple **SentenceEmbedder** class which can be used to generate sentence embeddings for text sequences by utilizing **Huggingface's transformers** package.\n```\nfrom transformers_keras_dataloader import load_pretrained_model_and_tokenizer\nfrom transformers_keras_dataloader import SentenceEmbedder\n\n\ntokenizer, model = load_pretrained_model_and_tokenizer(pretrained_model_name_or_path = \"bert-base-uncased\",\n                                                       return_model = True,\n                                                       use_cuda = True)\n\nsentence_embedder = SentenceEmbedder(max_length=100)\n\n# Generate sentence embeddings for X \nsentence_embeddings = sentence_embedder.prepare_embeddings(X=X, tokenizer=tokenizer, model=model)\n```\n### Additional Features â­\n#### Support for **Pooling Strategies & Layer Choices**\nYou can utilize either `pooling_layer_number` or `policy_dict`\n*Examples:*\n```\npooling_layer_number = -1\n```\n**or** \n```\n# Full fledged control by defining custom pooling strategy\n# if policy_dict is set to the dataloader we use it instead of pooling_layer_number\npolicy_dict = {\n    'base_expression': \"9 + 10 + 11 + 12\", # Layer numbers\n    'norm_op': ('avg', [0.25, 0.25, 0.25, 0.25]) # Weights\n}\n```\n#### Support for Multiprocessing\nWhile fitting you can also utitlize `use_multiprocessing=True` and specify number of `workers`\n```\ndownstream_model.fit(train_dataloader,\n               val_dataloader=val_dataloader,\n               epochs=1,\n               workers=6,\n               use_multiprocessing=True)\n```\n\n## Supported pretrained models\n\nYou can refer to below links to know about supported **Huggingface's** pretrained models, they have collated the supported models and their description.\n\nFor full list of the currently provided pretrained models by Huggingface, refer to [https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html)\n\nFor full list of community-uploaded models, refer to [https://huggingface.co/models](https://huggingface.co/models).\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/pratikdk/transformers_keras_dataloader/archive/v0.0.5.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/pratikdk/transformers_keras_dataloader",
    "keywords": "transformers,embedding,dataloader,generator,huggingface,attention",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "transformers-keras-dataloader",
    "package_url": "https://pypi.org/project/transformers-keras-dataloader/",
    "platform": "",
    "project_url": "https://pypi.org/project/transformers-keras-dataloader/",
    "project_urls": {
      "Download": "https://github.com/pratikdk/transformers_keras_dataloader/archive/v0.0.5.tar.gz",
      "Homepage": "https://github.com/pratikdk/transformers_keras_dataloader"
    },
    "release_url": "https://pypi.org/project/transformers-keras-dataloader/0.0.5/",
    "requires_dist": [
      "numpy",
      "pandas",
      "tensorflow",
      "keras",
      "torch (>=1.2.0)",
      "transformers (>=3.0.2)"
    ],
    "requires_python": "",
    "summary": "Transformers Keras Dataloader provides an EmbeddingDataLoader class, a subclass of keras.utils.Sequence which enables real-time embedding generation from pretrained transformer models while feeding it to your Keras model via batches.",
    "version": "0.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7998405,
  "releases": {
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c672bcad6046c778ff75338a44df31098e765e37909e7c450c49a6b434b8b69a",
          "md5": "39346d4ca792d7cbc2ba56a82f234138",
          "sha256": "451e68961a481c5ce704bf385209f8b8beb07273f14fc497f7ce710e9ec73b4b"
        },
        "downloads": -1,
        "filename": "transformers_keras_dataloader-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "39346d4ca792d7cbc2ba56a82f234138",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 26027,
        "upload_time": "2020-08-19T20:07:41",
        "upload_time_iso_8601": "2020-08-19T20:07:41.544348Z",
        "url": "https://files.pythonhosted.org/packages/c6/72/bcad6046c778ff75338a44df31098e765e37909e7c450c49a6b434b8b69a/transformers_keras_dataloader-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ee60a02e7f43adea31561fd27c3e7546b68d936503460fc6a33e8cf2cc9b49a7",
          "md5": "d411258d9ffb99b8db54d4db4f655689",
          "sha256": "ef7e882d028fb82763ab5d805419b2546d9d72f7c543d70c2703f639770d2c96"
        },
        "downloads": -1,
        "filename": "transformers_keras_dataloader-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "d411258d9ffb99b8db54d4db4f655689",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 17065,
        "upload_time": "2020-08-19T20:07:43",
        "upload_time_iso_8601": "2020-08-19T20:07:43.025628Z",
        "url": "https://files.pythonhosted.org/packages/ee/60/a02e7f43adea31561fd27c3e7546b68d936503460fc6a33e8cf2cc9b49a7/transformers_keras_dataloader-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c672bcad6046c778ff75338a44df31098e765e37909e7c450c49a6b434b8b69a",
        "md5": "39346d4ca792d7cbc2ba56a82f234138",
        "sha256": "451e68961a481c5ce704bf385209f8b8beb07273f14fc497f7ce710e9ec73b4b"
      },
      "downloads": -1,
      "filename": "transformers_keras_dataloader-0.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "39346d4ca792d7cbc2ba56a82f234138",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 26027,
      "upload_time": "2020-08-19T20:07:41",
      "upload_time_iso_8601": "2020-08-19T20:07:41.544348Z",
      "url": "https://files.pythonhosted.org/packages/c6/72/bcad6046c778ff75338a44df31098e765e37909e7c450c49a6b434b8b69a/transformers_keras_dataloader-0.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ee60a02e7f43adea31561fd27c3e7546b68d936503460fc6a33e8cf2cc9b49a7",
        "md5": "d411258d9ffb99b8db54d4db4f655689",
        "sha256": "ef7e882d028fb82763ab5d805419b2546d9d72f7c543d70c2703f639770d2c96"
      },
      "downloads": -1,
      "filename": "transformers_keras_dataloader-0.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "d411258d9ffb99b8db54d4db4f655689",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 17065,
      "upload_time": "2020-08-19T20:07:43",
      "upload_time_iso_8601": "2020-08-19T20:07:43.025628Z",
      "url": "https://files.pythonhosted.org/packages/ee/60/a02e7f43adea31561fd27c3e7546b68d936503460fc6a33e8cf2cc9b49a7/transformers_keras_dataloader-0.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}