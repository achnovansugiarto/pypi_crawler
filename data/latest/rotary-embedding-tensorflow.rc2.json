{
  "info": {
    "author": "Arya Aftab",
    "author_email": "arya.aftab@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "## Rotary Embeddings - Tensorflow\n\nA standalone library for adding <a href=\"https://arxiv.org/abs/2104.09864\">rotary embeddings</a> to transformers in Tesnorflow, following its success as <a href=\"https://blog.eleuther.ai/rotary-embeddings/\">relative positional encoding</a>. Specifically it will make rotating information into any axis of a tensor easy and efficient, whether they be fixed positional or learned. This library will give you state of the art results for positional embedding, at little costs.\n\nMy gut also tells me there is something <a href=\"https://www.nature.com/articles/s41593-021-00821-9\">more</a> to rotations that can be exploited in artificial neural networks.\n\n## Note\nAn implemented version of Pytorch is available in this <a href=\"https://github.com/lucidrains/rotary-embedding-torch\">repository</a>.\n\nThis version is written by converting to the version of Pytorch. \n\nThe three functions of rearrange, irearrange and repeat have been written due to the incompatibility of the einops library with tensorflow 2.x.\n## Install\n\n```bash\n$ pip install rotary-embedding-tensorflow\n```\n\n## Usage\n\n```python\nimport tensorflow as tf\nfrom rotary_embedding_tensorflow import apply_rotary_emb, RotaryEmbedding\n\n# instantiate the positional embedding in your transformer and pass to all your attention layers\n\npos_emb = RotaryEmbedding(dim = 32)\n\n# generate the rotations\n\nfreqs = pos_emb(tf.range(1024), cache_key = 1024) # cache with a key that is the sequence length, so that it does not need to recompute\n\n# mock queries and keys\n\nq = tf.random.normal((1, 1024, 64)) # queries - (batch, seq len, dimension of head)\nk = tf.random.normal((1, 1024, 64)) # keys\n\n# apply the rotations to your queries and keys after the heads have been split out, but prior to the dot product and subsequent softmax (attention)\n\nfreqs = freqs[None, ...] # expand dimension for batch dimension\nq = apply_rotary_emb(freqs, q)\nk = apply_rotary_emb(freqs, k)\n\n# then do your attention with your queries (q) and keys (k)\n```\n\nIf you do all the steps above correctly, you should see a dramatic improvement during training\n\n## Axial Rotary Embeddings\n\nFor easy use of 2d axial relative positional embedding, ie. vision transformers\n\n```python\nimport tensorflow as tf\nfrom rotary_embedding_tensorflow import apply_rotary_emb, RotaryEmbedding, broadcat\n\npos_emb = RotaryEmbedding(\n    dim = 32,\n    freqs_for = 'pixel'\n)\n\n# queries and keys for frequencies to be rotated into\n\nq = tf.random.normal((1, 256, 256, 64))\nk = tf.random.normal((1, 256, 256, 64))\n\n# get frequencies for each axial\n# -1 to 1 has been shown to be a good choice for images and audio\n\nfreqs_h = pos_emb(tf.linspace(-1, 1, num = 256), cache_key = 256)\nfreqs_w = pos_emb(tf.linspace(-1, 1, num = 256), cache_key = 256)\n\n# concat the frequencies along each axial\n# broadcat function makes this easy without a bunch of expands\n\nfreqs = broadcat((freqs_h[None, :, None, :], freqs_w[None, None, :, :]), dim = -1)\n\n# rotate in frequencies\n\nq = apply_rotary_emb(freqs, q)\nk = apply_rotary_emb(freqs, k)\n```\n\n## Learned Rotations\n\nFor injecting learned rotations into a network. Experiments pending\n\nUpdate: doesn't seem to do anything -_-, will keep trying...\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom rotary_embedding_tensorflow import apply_learned_rotations\n\nx = tf.random.normal((1, 1024, 512))\n\n# you can only rotate in (dim // 2) values\n# ex. for 512, you can only rotate in 256 values\n\n# say you have two sets of learned rotations of 128 values each\n\nrots1 = layers.Dense(128)(x)\nrots2 = layers.Dense(128)(x)\n\n# you rotate in 256 (128 x 2) at first\n\nx = apply_learned_rotations(rots1, x, start_index = 0)\n\n# then you start at index 256 and rotate in the last (128 x 2)\n\nx = apply_learned_rotations(rots2, x, start_index = 256)\n\n# you could also concat the rotations together and pass it in all at once\n\nrots = tf.concat((rots1, rots2), axis = -1)\n\nx = apply_learned_rotations(rots, x)\n```\n\n## Citations\n\n```bibtex\n@misc{su2021roformer,\n    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding}, \n    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n    year    = {2021},\n    eprint  = {2104.09864},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CL}\n}\n\n@misc{rotary-embedding-torch,\n    title   = {Rotary Embeddings - Pytorch}, \n    author  = {Phil Wang (lucidrains)},\n    year    = {2021},\n    url  = {https://github.com/lucidrains/rotary-embedding-torch},\n    publisher = {Github},\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/AryaAftab/rotary-embedding-tensorflow",
    "keywords": "deep learning,tensorflow,positional embedding",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "rotary-embedding-tensorflow",
    "package_url": "https://pypi.org/project/rotary-embedding-tensorflow/",
    "platform": "",
    "project_url": "https://pypi.org/project/rotary-embedding-tensorflow/",
    "project_urls": {
      "Homepage": "https://github.com/AryaAftab/rotary-embedding-tensorflow"
    },
    "release_url": "https://pypi.org/project/rotary-embedding-tensorflow/0.1.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Rotary Embedding - Tensorflow",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11427496,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b908f06c11d23e3fc992ff8a4ea09982fd45197e8f5736d20808ce94eee90199",
          "md5": "395d663fa0c128aa657d5140944df5ec",
          "sha256": "3f4b8c682a0b4f421b29279279ca977f03e402b1784503b4f77c4bb9508c9981"
        },
        "downloads": -1,
        "filename": "rotary-embedding-tensorflow-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "395d663fa0c128aa657d5140944df5ec",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5311,
        "upload_time": "2021-08-28T13:51:28",
        "upload_time_iso_8601": "2021-08-28T13:51:28.661898Z",
        "url": "https://files.pythonhosted.org/packages/b9/08/f06c11d23e3fc992ff8a4ea09982fd45197e8f5736d20808ce94eee90199/rotary-embedding-tensorflow-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e74fa0190adec998e2bc6a04bfbd27d0a6dbf01d8999e1513b623a1779532f48",
          "md5": "8ac5eaf693366738f01731a8be4a9004",
          "sha256": "12c531eb50f572b176f5f228a7cbc374563a8d1cd240aebe8844fce001cdac6f"
        },
        "downloads": -1,
        "filename": "rotary-embedding-tensorflow-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "8ac5eaf693366738f01731a8be4a9004",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5566,
        "upload_time": "2021-09-12T04:34:35",
        "upload_time_iso_8601": "2021-09-12T04:34:35.963454Z",
        "url": "https://files.pythonhosted.org/packages/e7/4f/a0190adec998e2bc6a04bfbd27d0a6dbf01d8999e1513b623a1779532f48/rotary-embedding-tensorflow-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e74fa0190adec998e2bc6a04bfbd27d0a6dbf01d8999e1513b623a1779532f48",
        "md5": "8ac5eaf693366738f01731a8be4a9004",
        "sha256": "12c531eb50f572b176f5f228a7cbc374563a8d1cd240aebe8844fce001cdac6f"
      },
      "downloads": -1,
      "filename": "rotary-embedding-tensorflow-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "8ac5eaf693366738f01731a8be4a9004",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 5566,
      "upload_time": "2021-09-12T04:34:35",
      "upload_time_iso_8601": "2021-09-12T04:34:35.963454Z",
      "url": "https://files.pythonhosted.org/packages/e7/4f/a0190adec998e2bc6a04bfbd27d0a6dbf01d8999e1513b623a1779532f48/rotary-embedding-tensorflow-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}