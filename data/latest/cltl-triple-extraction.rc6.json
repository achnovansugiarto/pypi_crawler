{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# cltl-knowledgeextraction\n\nA knowledge extraction service (aka Leolani's Triple Extractor package). This service performs Natural Language\nUnderstanding through Grammars natural language textual data and outputs structured data.\n\n## Description\n\nThis package allows extracting structured information, in the form of SPO triples, from natural language textual data.\nIt features:\n\n* An Utterance is analyzed with the help of the Analyzer class. It extracts structured data in the form of a list of\n  triples, where each triple has a subject, predicate and object.\n* If an Utterance is a statement, it also has a Perspective object which consists of a polarity, certainty, sentiment\n  and emotion value.\n* The Analyzer class is the API for triple extractors. At this moment there are two Analyzers implemented: CFGAnalyzer\n  (which uses Context Free Grammar parsing), and OIE Analyzer (which uses Stanford OIE library)\n* The CFGAnalyzer consists of a hierarchy of classes, topmost class is the abstract general class Analyzer, which is\n  separated into two abstract classes StatementAnalyzer and QuestionAnalyzer, which consist of the concrete classes\n  GeneralStatementAnalyzer, WhQuestionAnalyzer and VerbQuestionAnalyzer.\n\n### Triple extraction implementations\n\nThe triples consist of subject, predicate and object alongside with their semantic types. In case of a statement, the\ntriple is accompanied by a perspective. In the case of a question the triple is incomplete. Below are a few examples of\nthe triples which are the output of analyzers:\n\n* `“My sister enjoys eating cakes” lenka-sister_enjoy_eating-cakes `\n\n* ` “What does my sister enjoy?” lenka-sister_enjoy_? `\n\nThe elements of the triple are separated with underscore; while dash is used to separate elements of multiword\nexpressions. When a multiword expression is actually a collocation, the multiword expression is marked with apostrophes\nduring the analysis (e.g. ”mexico-city”)to ensure that subparts of collocations are not analyzed separately.\nimplementations\n\n#### CFGAnalyzer\n\nBasic rules that the CFGAnalyzer follows are:\n\n* predicates are lemmatized verbs, with possible prepositions connected to the verb\n    - `“live-in”, “come-from”, etc`\n* modal verbs are analyzed using the lexicon and their modality is stored as one of the perspective values\n    - `“might-come”- {'polarity': 1, 'certainty': '0.5', 'sentiment': 0}`\n* negation is removed after processing and stored within the perspective object as polarity\n    - `I think selene doesn't like cheese = “selene_like_cheese” - {'polarity': -1, '\n      certainty': '0.75', 'sentiment': ’0.75'}`\n    - `(I think selene hates cheese = “selene_hate_cheese” - {'polarity': 1, 'certainty': '0.75', 'sentiment': '-1'}`\n* properties end with “-is”(this way it is quite easy for NLG)\n    - `My favorite color is green = lenka_favorite-color-is_green`\n* words that refer to a person are grouped together in the subject unless the verb is just “be”, in this case they are\n  processed like properties (“sister-is”)\n    - `My best friend is Selene = lenka_best-friend-is_selene `\n    - `My best friend’s name is Selene = lenka-best-friend_name-is_selene `\n* adjectives, determiners and numbers are joined with the noun\n    - `“a-dog”, “the-blue-shirt”, etc.`\n\nBelow is a short summary of NLP that happens during the CFG utterance analysis:\n\n1. Tokenization and replacing contractions with long variants of aux verbs\n1. POS tagging (NLTK and Stanford + would be good to add an additional tagger to use when the two have a mismatch)\n1. CFG parsing using the grammar which is manually designed\n1. Analyzer class maps the output of CFG parsing to the subject-predicate-object triple, following the rules which are\n   mentioned above\n1. Lemmatization using NLTK\n1. Modal verbs are analyzed using the lexicon and this is stored within Perspective\n1. Checking whether some of the multi-word elements are actually collocations such as New York or ice-cream (these\n   should be processed as one word)\n1. Getting semantic types of each element of the triple, and its subparts, using the manually made lexicon, WordNet\n   lexname, Stanford NER\n\n### Sample output\n\nHere is a sample output for sentence `“I have three white cats”`:\n\n```json\n{\n  \"subject\": {\n    \"text\": \"Lenka\",\n    \"type\": [\n      \"person\"\n    ]\n  },\n  \"predicate\": {\n    \"text\": \"have\",\n    \"type\": [\n      \"verb.possession\"\n    ]\n  },\n  \"object\": {\n    \"text\": \"three-white-cats\",\n    \"type\": [\n      \"adj.all\",\n      \"noun.animal\"\n    ]\n  },\n  \"utterance type\": \"STATEMENT\",\n  \"perspective\": {\n    \"polarity\": 1,\n    \"certainty\": 1,\n    \"sentiment\": 0\n  }\n}\n\n```\n\n## Getting started\n\n### Prerequisites\n\nThis repository uses Python >= 3.7\n\nBe sure to run in a virtual python environment (e.g. conda, venv, mkvirtualenv, etc.)\n\n### Installation\n\n1. In the root directory of this repo run\n\n    ```bash\n    pip install -e .\n    python -c \"import nltk; nltk.download('wordnet'); nltk.download('punkt'); nltk.download('averaged_perceptron_tagger')\"\n    python -m spacy download en_core_web_sm \n    ```\n2. In case you want to run the OpenIE function from StanfordCoreNLP, you need to download \"stanford-corenlp-4.1.0\" and unpack it in the folder\n~/.stanfordnlp_resources.\n\n### Usage\n\nFor using this repository as a package different project and on a different virtual environment, you may\n\n- install a published version from PyPI:\n\n    ```bash\n    pip install cltl.triple_extractor\n    ```\n\n- or, for the latest snapshot, run:\n\n    ```bash\n    pip install git+git://github.com/leolani/cltl-knowledgeextraction.git@main\n    ```\n\nThen you can import it in a python script as:\n\n```python\nfrom cltl.triple_extraction.api import Chat\nfrom cltl.triple_extraction.cfg_analyzer import CFGAnalyzer\nfrom cltl.triple_extraction.utils.helper_functions import utterance_to_capsules\n\nchat = Chat(\"Lenka\")\nanalyzer = CFGAnalyzer()\n\nchat.add_utterance(\"I have three white cats\")\nanalyzer.analyze(chat.last_utterance)\ncapsules = utterance_to_capsules(chat.last_utterance)\n```\n\n## Examples\n\nPlease take a look at the example scripts provided to get an idea on how to run and use this package. Each example has a\ncomment at the top of the script describing the behaviour of the script.\n\nFor these example scripts, you need\n\n1. To change your current directory to ./examples/\n\n1. Run some examples (e.g. python test_with_triples.py)\n\n## Contributing\n\nContributions are what make the open source community such an amazing place to be learn, inspire, and create. Any\ncontributions you make are **greatly appreciated**.\n\n1. Fork the Project\n2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)\n3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)\n4. Push to the Branch (`git push origin feature/AmazingFeature`)\n5. Open a Pull Request\n\n## License\n\nDistributed under the MIT License.\nSee [`LICENSE`](https://github.com/leolani/cltl-knowledgeextraction/blob/main/LICENCE) for more information.\n\n## Authors\n\n* [Selene Báez Santamaría](https://selbaez.github.io/)\n* [Thomas Baier](https://www.linkedin.com/in/thomas-baier-05519030/)\n* [Piek Vossen](https://github.com/piekvossen)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/leolani/cltl-knowledgeextraction",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cltl.triple-extraction",
    "package_url": "https://pypi.org/project/cltl.triple-extraction/",
    "platform": null,
    "project_url": "https://pypi.org/project/cltl.triple-extraction/",
    "project_urls": {
      "Homepage": "https://github.com/leolani/cltl-knowledgeextraction"
    },
    "release_url": "https://pypi.org/project/cltl.triple-extraction/1.0.dev1/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "The Leolani Language module for knowledge extraction",
    "version": "1.0.dev1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13398485,
  "releases": {
    "0.0.dev3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "38203f597645c874ec9ae57315d9e6009e22b767bb1ca90e113a980e8b40557c",
          "md5": "d9f6b2e90ca8d236f11b910ebce6d4be",
          "sha256": "8cb45ce3ac544d663548236c6aa473fbf1b4372d3c8adb2d3cbbbf754a029c1e"
        },
        "downloads": -1,
        "filename": "cltl.triple_extraction-0.0.dev3.tar.gz",
        "has_sig": false,
        "md5_digest": "d9f6b2e90ca8d236f11b910ebce6d4be",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 82796697,
        "upload_time": "2021-10-26T22:47:13",
        "upload_time_iso_8601": "2021-10-26T22:47:13.066552Z",
        "url": "https://files.pythonhosted.org/packages/38/20/3f597645c874ec9ae57315d9e6009e22b767bb1ca90e113a980e8b40557c/cltl.triple_extraction-0.0.dev3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.dev4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4508c9e3c73c38c80c4d56dba81599e0db680b3d1d9cfed5b17a767aecd555ee",
          "md5": "bc4c6ab6962594af1b801d7f722f06a7",
          "sha256": "bdd82a45a4d625d84dcfecf767511e5ec5393db572969c1508586b253796eb7c"
        },
        "downloads": -1,
        "filename": "cltl.triple_extraction-0.0.dev4.tar.gz",
        "has_sig": false,
        "md5_digest": "bc4c6ab6962594af1b801d7f722f06a7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 82794316,
        "upload_time": "2021-11-22T09:20:16",
        "upload_time_iso_8601": "2021-11-22T09:20:16.873317Z",
        "url": "https://files.pythonhosted.org/packages/45/08/c9e3c73c38c80c4d56dba81599e0db680b3d1d9cfed5b17a767aecd555ee/cltl.triple_extraction-0.0.dev4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.dev5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9b219b56402e4c9f2f4136383af0828a6e46db63c0ff3317cb54bd844a6336bf",
          "md5": "e2844c0def91a7d67578c7c3db421481",
          "sha256": "50e618b94ec8873edfdd7e313dd1d852ef73146d6c24a64e0dc5933ef0f805ca"
        },
        "downloads": -1,
        "filename": "cltl.triple_extraction-0.0.dev5.tar.gz",
        "has_sig": false,
        "md5_digest": "e2844c0def91a7d67578c7c3db421481",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 82794246,
        "upload_time": "2021-11-22T15:57:48",
        "upload_time_iso_8601": "2021-11-22T15:57:48.383989Z",
        "url": "https://files.pythonhosted.org/packages/9b/21/9b56402e4c9f2f4136383af0828a6e46db63c0ff3317cb54bd844a6336bf/cltl.triple_extraction-0.0.dev5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.dev6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2b71033292fadad2c14908be70375878bb505434c8e67bbceea7cc1351cd6e00",
          "md5": "a8e3554fd6cefa814a2a604457f3f452",
          "sha256": "1a3615a039bd34acebc3a5f70197fbf01852d2e81df36a8bb97ca0a0a205eefa"
        },
        "downloads": -1,
        "filename": "cltl.triple_extraction-0.0.dev6.tar.gz",
        "has_sig": false,
        "md5_digest": "a8e3554fd6cefa814a2a604457f3f452",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 82792601,
        "upload_time": "2021-12-03T15:09:58",
        "upload_time_iso_8601": "2021-12-03T15:09:58.350649Z",
        "url": "https://files.pythonhosted.org/packages/2b/71/033292fadad2c14908be70375878bb505434c8e67bbceea7cc1351cd6e00/cltl.triple_extraction-0.0.dev6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.dev9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d3f042fd9c556015f3ebcf0d931f72f2ff5ade193049c8f3c1c3082a3aba626a",
          "md5": "9a6dd39dc15d978524dfe086a6e7b254",
          "sha256": "2f94f754e5772ce27116543cb1926cc0b7613e9cba008ffbea9a0b0e65b2a304"
        },
        "downloads": -1,
        "filename": "cltl.triple_extraction-0.0.dev9.tar.gz",
        "has_sig": false,
        "md5_digest": "9a6dd39dc15d978524dfe086a6e7b254",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 82798285,
        "upload_time": "2022-03-30T16:44:10",
        "upload_time_iso_8601": "2022-03-30T16:44:10.465489Z",
        "url": "https://files.pythonhosted.org/packages/d3/f0/42fd9c556015f3ebcf0d931f72f2ff5ade193049c8f3c1c3082a3aba626a/cltl.triple_extraction-0.0.dev9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.dev1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d0ead6e0ce4ebc3b6d5f05abe510e4a6631c11bf797203476fb2ea953bfe49b4",
          "md5": "3dac3ff2ac0901bd8b69a416b0842e5a",
          "sha256": "cd341319b0c63b1159110fca0873c12bb1047537482d8100d97cff1ed1018304"
        },
        "downloads": -1,
        "filename": "cltl.triple_extraction-1.0.dev1.tar.gz",
        "has_sig": false,
        "md5_digest": "3dac3ff2ac0901bd8b69a416b0842e5a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 82803086,
        "upload_time": "2022-04-04T10:33:05",
        "upload_time_iso_8601": "2022-04-04T10:33:05.989756Z",
        "url": "https://files.pythonhosted.org/packages/d0/ea/d6e0ce4ebc3b6d5f05abe510e4a6631c11bf797203476fb2ea953bfe49b4/cltl.triple_extraction-1.0.dev1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d0ead6e0ce4ebc3b6d5f05abe510e4a6631c11bf797203476fb2ea953bfe49b4",
        "md5": "3dac3ff2ac0901bd8b69a416b0842e5a",
        "sha256": "cd341319b0c63b1159110fca0873c12bb1047537482d8100d97cff1ed1018304"
      },
      "downloads": -1,
      "filename": "cltl.triple_extraction-1.0.dev1.tar.gz",
      "has_sig": false,
      "md5_digest": "3dac3ff2ac0901bd8b69a416b0842e5a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 82803086,
      "upload_time": "2022-04-04T10:33:05",
      "upload_time_iso_8601": "2022-04-04T10:33:05.989756Z",
      "url": "https://files.pythonhosted.org/packages/d0/ea/d6e0ce4ebc3b6d5f05abe510e4a6631c11bf797203476fb2ea953bfe49b4/cltl.triple_extraction-1.0.dev1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}