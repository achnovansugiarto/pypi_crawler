{
  "info": {
    "author": "Ahmed Elzeiny",
    "author_email": "ahmed.elzeiny@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "# Apache Airflow: Native AWS Executors\n\n![Build Status](https://github.com/aelzeiny/airflow-aws-executors/actions/workflows/main.yml/badge.svg)\n\n\nThis is an AWS Executor that delegates every task to a scheduled container on either AWS Batch, AWS Fargate, or AWS ECS.\n\n```bash\npip install airflow-aws-executors\n```\n\n## Getting Started\nFor `AWS Batch`: [Getting Started with AWS Batch ReadMe](getting_started_batch.md)\n\nFor `AWS ECS/Fargate`: [Getting Started with AWS ECS/Fargate ReadMe](getting_started_ecs_fargate.md)\n\n## But Why?\nThere's so much to unpack here.\n\n#### In a Nut-Shell:\n* Pay for what you use.\n* Simplicity in Setup.\n* No new libraries are introduced to Airflow\n* Servers require up-keep and maintenance. For example, just one CPU-bound or memory-bound Airflow Task could overload the resources of a server and starve out the celery or scheduler thread; [thus causing the entire server to go down](https://docs.docker.com/config/containers/resource_constraints/#understand-the-risks-of-running-out-of-memory). All of these executors don't have this problem.\n\n#### The Case for AWS Batch\nAWS Batch can be seen as very similar to the Celery Executor, but WITH Autoscaling. AWS will magically provision and take-down instances. \nAWS will magically monitor each container store their status for ~24 hours. AWS will determine when to autoscale based off of amount of time and number of tasks in queue.\n\nIn contrast, Celery can scale up, but doesn't have a good scaling-down story (based off of personal experience). If you look at Celery's Docs about Autoscaling you'll find APIs about scaling the number of threads on one server; that doesn't even work. Each Celery workers is the user's responsibility to provision and maintain at fixed capacity.\nThe Celery Backend and worker queue also need attention and maintenance. I've tried autoscaling an ECS cluster on my own with CloudWatch Alarms on SQS, \ntriggering CloudWatch Events, triggering capacity providers, triggering Application Autoscaling groups, \nand it was a mess that I never got to work properly.\n\n#### The Case for AWS Batch on AWS Fargate, and AWS Fargate\nIf you're on the Fargate executor it may take ~2.5 minutes for a task to pop up, but at least it's a constant O(1) time. \nThis way, the concept of tracking DAG Landing Times becomes unnecessary. \nIf you have more than 2000 concurrent tasks (which is a lot) then you can always contact AWS to provide an increase in this soft-limit.\n\n\n## AWS Batch v AWS ECS v AWS Fargate?\n**I almost always recommend that you go the AWS Batch route**. Especially since, as of Dec 2020, AWS Batch supports Fargate deployments. So unless you need some very custom flexibility provided by ECS, or have a particular reason to use AWS Fargate directly, then go with AWS Batch.\n\n`AWS Batch` - Is built on top of ECS, but has additional features for Batch-Job management. Including auto-scaling up and down servers on an ECS cluster based on jobs submitted to a queue. Generally easier to configure and setup than either option.\n\n`AWS Fargate` - Is a serverless container orchestration service; comparable to a proprietary AWS version of Kubernetes. Launching a Fargate Task is like saying \"I want these containers to be launched somewhere in the cloud with X CPU and Y memory, and I don't care about the server\". AWS Fargate is built on top of AWS ECS, and is easier to manage and maintain. However, it provides less flexibility.\n\n`AWS ECS` - Is known as \"Elastic Container Service\", which is a container orchestration service that uses a designated cluster of EC2 instances that you operate, own, and maintain.\n\n\n|                   | Batch                                                                               | Fargate                                     |  ECS                                              |\n|-------------------|-------------------------------------------------------------------------------------|---------------------------------------------|---------------------------------------------------|\n| Start-up per task | Combines both, depending on if the job queue is Fargate serverless                  | 2-3 minutes per task; O(1) constant time    | Instant 3s, or until capacity is available.       |\n| Maintenance       | You patch the own, operate, and patch the servers OR Serverless (as of Dec 2020)    | Serverless                                  | You patch the own, operate, and patch the servers |\n| Capacity          | Autoscales to configurable Max vCPUs in compute environment                         | ~2000 containers. See AWS Limits            | Fixed. Not auto-scaling.                          |\n| Flexibility       | Combines both, depending on if the job queue is Fargate serverless                  | Low. Can only do what AWS allows in Fargate | High. Almost anything that you can do on an EC2   |\n| Fractional CPUs?  | Yes, as of Dec 2020 a task can have 0.25 vCPUs.                                     | Yes. A task can have 0.25 vCPUs.            | Yes. A task can have 0.25 vCPUs.                  |\n\n\n## Optional Container Requirements\nThis means that you can specify CPU, Memory, env vars, and GPU requirements on a task.\n\n#### AWS Batch\nSpecifying an executor config will be merged directly into the [Batch.submit_job()][submit_job] request kwarg.\n\nFor example:\n```python\n\ntask = PythonOperator(\n    python_callable=lambda *args, **kwargs: print('hello world'),\n    task_id='say_hello',\n    executor_config=dict(\n        vcpus=1,\n        memory=512\n    ),\n    dag=dag\n)\n```\n\n#### AWS ECS/Fargate\nSpecifying an executor config will be merged into the [ECS.run_task()][run_task] request kwargs as a container override for the \nairflow container.\n[Refer to AWS' documentation for Container Override for a full list of kwargs](https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_ContainerOverride.html)\n\nFor example: \n```python\ntask = PythonOperator(\n    python_callable=lambda *args, **kwargs: print('hello world'),\n    task_id='say_hello',\n    executor_config=dict(\n        cpu=256,  # 0.25 fractional CPUs \n        memory=512\n    ),\n    dag=dag\n)\n```\n\n\n## Airflow Configurations\n#### Batch\n`[batch]`\n* `region` \n    * **description**: The name of AWS Region\n    * **mandatory**: even with a custom run_task_kwargs\n    * **example**: us-east-1\n* `job_name`\n    * **description**: The name of airflow job\n    * **example**: airflow-job-name\n* `job_queue`\n    * **description**: The name of AWS Batch Queue in which tasks are submitted\n    * **example**: airflow-job-queue\n* `job_definition`\n    * **description**: The name of the AWS Batch Job Definition; optionally includes revision number\n    * **example**: airflow-job-definition or airflow-job-definition:2\n* `submit_job_kwargs`\n    * **description**: This is the default configuration for calling the Batch [submit_job function][submit_job] API.\n    To change the parameters used to run a task in Batch, the user can overwrite the path to\n    specify another python dictionary. More documentation can be found in the `Extensibility` section below.\n    * **default**: airflow_aws_executors.conf.BATCH_SUBMIT_JOB_KWARGS\n#### ECS & FARGATE\n`[ecs_fargate]`\n* `region` \n    * **description**: The name of AWS Region\n    * **mandatory**: even with a custom run_task_kwargs\n    * **example**: us-east-1\n* `cluster` \n    * **description**: Name of AWS ECS or Fargate cluster\n    * **mandatory**: even with a custom run_task_kwargs\n* `container_name` \n    * **description**: Name of registered Airflow container within your AWS cluster. This container will\n    receive an airflow CLI command as an additional parameter to its entrypoint.\n    For more info see url to Boto3 docs above.\n    * **mandatory**: even with a custom run_task_kwargs\n* `task_definition` \n    * **description**: Name of AWS Task Definition. [For more info see Boto3][run_task].\n* `launch_type` \n    * **description**: Launch type can either be 'FARGATE' OR 'EC2'. [For more info see Boto3][run_task].\n    * **default**: FARGATE\n* `platform_version`\n    * **description**: AWS Fargate is versioned. [See this page for more details](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/platform_versions.html)\n    * **default**: LATEST\n* `assign_public_ip` \n    * **description**: Assign public ip. [For more info see Boto3][run_task].\n* `security_groups` \n    * **description**: Security group ids for task to run in (comma-separated). [For more info see Boto3][run_task].\n    * **example**: sg-AAA,sg-BBB\n* `subnets` \n    * **description**: Subnets for task to run in (comma-separated). [For more info see Boto3][run_task].\n    * **example**: subnet-XXXXXXXX,subnet-YYYYYYYY\n* `run_task_kwargs`\n    * **description**: This is the default configuration for calling the ECS [run_task function][run_task] API.\n    To change the parameters used to run a task in FARGATE or ECS, the user can overwrite the path to\n    specify another python dictionary. More documentation can be found in the `Extensibility` section below.\n    * **default**: airflow_aws_executors.conf.ECS_FARGATE_RUN_TASK_KWARGS\n\n\n*NOTE: Modify airflow.cfg or export environmental variables. For example:* \n```bash\nAIRFLOW__ECS_FARGATE__REGION=\"us-west-2\"\n```\n\n\n## Extensibility\nThere are many different ways to schedule an ECS, Fargate, or Batch Container. You may want specific container overrides, \nenvironmental variables, subnets, retries, etc. This project does **not** attempt to wrap around the AWS API.\nThese technologies are ever evolving, and it would be impossible to keep up with AWS's innovations. \nInstead, it allows the user to offer their own configuration in the form of Python dictionaries, \nwhich are then directly passed to Boto3's [run_task][run_task] or [submit_job][submit_job] function \nas **kwargs. This allows for maximum flexibility and little maintenance.\n\n#### AWS Batch\nIn this example we will modify the default `submit_job_kwargs` config. Note, however, there is \nnothing that's stopping us from completely overriding it and providing our own config. \nIf we do so, be sure to specify the mandatory Airflow configurations in the section above.\n\nFor example:\n```bash\n# exporting env vars in this way is like modifying airflow.cfg\nexport AIRFLOW__BATCH__SUBMIT_JOB_KWARGS=\"custom_module.CUSTOM_SUBMIT_JOB_KWARGS\"\n```\n\n```python\n# filename: AIRFLOW_HOME/plugins/custom_module.py\nfrom airflow_aws_executors.conf import BATCH_SUBMIT_JOB_KWARGS\nfrom copy import deepcopy\n\n# Add retries & timeout to default config\nCUSTOM_SUBMIT_JOB_KWARGS = deepcopy(BATCH_SUBMIT_JOB_KWARGS)\nCUSTOM_SUBMIT_JOB_KWARGS['retryStrategy'] = {'attempts': 3}\nCUSTOM_SUBMIT_JOB_KWARGS['timeout'] = {'attemptDurationSeconds': 24 * 60 * 60 * 60}\n```\n\n\"I need more levers!!! I should be able to make changes to how the API gets called at runtime!\"\n\n```python\nclass CustomBatchExecutor(AwsBatchExecutor):\n    def _submit_job_kwargs(self, task_id, cmd, queue, exec_config) -> dict:\n        submit_job_api = super()._submit_job_kwargs(task_id, cmd, queue, exec_config)\n        if queue == 'long_tasks_queue':\n            submit_job_api['retryStrategy'] = {'attempts': 3}\n            submit_job_api['timeout'] = {'attemptDurationSeconds': 24 * 60 * 60 * 60}\n        return submit_job_api\n```\n\n#### AWS ECS/Fargate\nIn this example we will modify the default `submit_job_kwargs`. Note, however, there is nothing that's stopping us \nfrom completely overriding it and providing our own config. If we do so, be sure to specify the mandatory Airflow configurations\nin the section above.\n\nFor example:\n```bash\n# exporting env vars in this way is like modifying airflow.cfg\nexport AIRFLOW__BATCH__SUBMIT_JOB_KWARGS=\"custom_module.CUSTOM_SUBMIT_JOB_KWARGS\"\n```\n\n```python\n# filename: AIRFLOW_HOME/plugins/custom_module.py\nfrom airflow_aws_executors.conf import ECS_FARGATE_RUN_TASK_KWARGS\nfrom copy import deepcopy\n\n# Add environmental variables to contianer overrides\nCUSTOM_RUN_TASK_KWARGS = deepcopy(ECS_FARGATE_RUN_TASK_KWARGS)\nCUSTOM_RUN_TASK_KWARGS['overrides']['containerOverrides'][0]['environment'] = [\n    {'name': 'CUSTOM_ENV_VAR', 'value': 'enviornment variable value'}\n]\n```\n\n\"I need more levers!!! I should be able to make changes to how the API gets called at runtime!\"\n\n```python\nclass CustomFargateExecutor(AwsFargateExecutor):\n    def _run_task_kwargs(self, task_id, cmd, queue, exec_config) -> dict:\n        run_task_api = super()._run_task_kwargs(task_id, cmd, queue, exec_config)\n        if queue == 'long_tasks_queue':\n            run_task_api['retryStrategy'] = {'attempts': 3}\n            run_task_api['timeout'] = {'attemptDurationSeconds': 24 * 60 * 60 * 60}\n        return run_task_api\n```\n\n## Issues & Bugs\nPlease file a ticket in GitHub for issues. Be persistent and be polite.\n\n\n## Contribution & Development\nThis repository uses Github Actions for CI, pytest for Integration/Unit tests, and isort+pylint for code-style. \nPythonic Type-Hinting is encouraged. From the bottom of my heart, thank you to everyone who has contributed \nto making Airflow better.\n\n\n[boto_conf]: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html\n[run_task]: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ecs.html#ECS.Client.run_task\n[submit_job]:https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/batch.html#Batch.Client.submit_job\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/aelzeiny/airflow-aws-executors",
    "keywords": "Apache,Airflow,AWS,Executor,Fargate,ECS,AWS ECS,AWS Batch,AWS Fargate",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "airflow-aws-executors",
    "package_url": "https://pypi.org/project/airflow-aws-executors/",
    "platform": "",
    "project_url": "https://pypi.org/project/airflow-aws-executors/",
    "project_urls": {
      "Homepage": "https://github.com/aelzeiny/airflow-aws-executors"
    },
    "release_url": "https://pypi.org/project/airflow-aws-executors/1.1.3/",
    "requires_dist": null,
    "requires_python": ">=3.6.0",
    "summary": "Apache Airflow Executor for AWS ECS, AWS Fargate, and AWS Batch",
    "version": "1.1.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12497649,
  "releases": {
    "0.1.13": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b789874f24631f22b86bef7dbcb5fe685ebcab9e12d0d0c65f49ded000ceba68",
          "md5": "984946e1dc1affd37b6d1102a4140b51",
          "sha256": "91eb0d99dba5f57e315b56a8d9a1b3ab97f655c8a401f53b5164e9a91fd10c9f"
        },
        "downloads": -1,
        "filename": "airflow-aws-executors-0.1.13.tar.gz",
        "has_sig": false,
        "md5_digest": "984946e1dc1affd37b6d1102a4140b51",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 13457,
        "upload_time": "2020-11-21T21:49:13",
        "upload_time_iso_8601": "2020-11-21T21:49:13.772311Z",
        "url": "https://files.pythonhosted.org/packages/b7/89/874f24631f22b86bef7dbcb5fe685ebcab9e12d0d0c65f49ded000ceba68/airflow-aws-executors-0.1.13.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.14": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f1bd284b7355516c9fd7e5e30ec0343955a387609d9d9cb73b317fe8fcae0a01",
          "md5": "2ba0ece487272211702db3e5801aff2f",
          "sha256": "c4b65a29741d4a97942b2be207acc387b653497d18b306c4a8f727249f922cd8"
        },
        "downloads": -1,
        "filename": "airflow-aws-executors-0.1.14.tar.gz",
        "has_sig": false,
        "md5_digest": "2ba0ece487272211702db3e5801aff2f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 13467,
        "upload_time": "2020-11-21T22:55:08",
        "upload_time_iso_8601": "2020-11-21T22:55:08.700846Z",
        "url": "https://files.pythonhosted.org/packages/f1/bd/284b7355516c9fd7e5e30ec0343955a387609d9d9cb73b317fe8fcae0a01/airflow-aws-executors-0.1.14.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.12": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "377f1fad2646f05a0d9dfd8089e2a9f3d637e2603224e176ee4e0265cb706842",
          "md5": "14fa31ace12e051683559c15b9195160",
          "sha256": "928cd2ec1961bf9c8956c4124088a0b91a01d7e16a846ea53f46892192b2e5c5"
        },
        "downloads": -1,
        "filename": "airflow-aws-executors-0.12.tar.gz",
        "has_sig": false,
        "md5_digest": "14fa31ace12e051683559c15b9195160",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 11738,
        "upload_time": "2020-11-19T07:04:04",
        "upload_time_iso_8601": "2020-11-19T07:04:04.699735Z",
        "url": "https://files.pythonhosted.org/packages/37/7f/1fad2646f05a0d9dfd8089e2a9f3d637e2603224e176ee4e0265cb706842/airflow-aws-executors-0.12.tar.gz",
        "yanked": true,
        "yanked_reason": "Versioning change"
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "66ad98ddab06aee844b3fc280f1993ced53340f929f9d8ed59275127d1556afe",
          "md5": "9f87b6810f47379a03b63bb6e0cc6b42",
          "sha256": "e0606d8146bf2ee9d613064b1e1d7473bc0f95e7aa3fd94a8124c9f150f43ba2"
        },
        "downloads": -1,
        "filename": "airflow-aws-executors-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "9f87b6810f47379a03b63bb6e0cc6b42",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 13463,
        "upload_time": "2020-11-23T21:29:53",
        "upload_time_iso_8601": "2020-11-23T21:29:53.413934Z",
        "url": "https://files.pythonhosted.org/packages/66/ad/98ddab06aee844b3fc280f1993ced53340f929f9d8ed59275127d1556afe/airflow-aws-executors-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "64110417a7ed2f18c85a124cbd0f3a73b54c3ba524191b83e5050c5121fae8e7",
          "md5": "666be1c8d76e573e10d9c965be25f939",
          "sha256": "4763cc840880bb26d235edc0973ffb4392d95804c99f58d4eddd7caecf089625"
        },
        "downloads": -1,
        "filename": "airflow-aws-executors-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "666be1c8d76e573e10d9c965be25f939",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 13769,
        "upload_time": "2021-01-28T03:36:17",
        "upload_time_iso_8601": "2021-01-28T03:36:17.317162Z",
        "url": "https://files.pythonhosted.org/packages/64/11/0417a7ed2f18c85a124cbd0f3a73b54c3ba524191b83e5050c5121fae8e7/airflow-aws-executors-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.2.post3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c85850a55889ffdddf50fe086c08bfb7da1d163786195eb87ae93e3e4c2f1be6",
          "md5": "ff75fdc9550ed0d2605413347e2055c1",
          "sha256": "e9cc6ccce3793c0673b1427e930f49c12de5d1e755f6294a67a7f9010d2fb81f"
        },
        "downloads": -1,
        "filename": "airflow-aws-executors-1.1.2.post3.tar.gz",
        "has_sig": false,
        "md5_digest": "ff75fdc9550ed0d2605413347e2055c1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 17910,
        "upload_time": "2022-01-06T20:31:10",
        "upload_time_iso_8601": "2022-01-06T20:31:10.151554Z",
        "url": "https://files.pythonhosted.org/packages/c8/58/50a55889ffdddf50fe086c08bfb7da1d163786195eb87ae93e3e4c2f1be6/airflow-aws-executors-1.1.2.post3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cff1296db17600155165e0ca20f18ba449c859f122dad334d662716fadd54666",
          "md5": "a964385d4d13c4404d22e0a07e5b8f74",
          "sha256": "2b2aba670e6df808f660be98a823759ec0898f73ceea98d1d3eda79777b7431f"
        },
        "downloads": -1,
        "filename": "airflow-aws-executors-1.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "a964385d4d13c4404d22e0a07e5b8f74",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 17899,
        "upload_time": "2022-01-06T20:40:06",
        "upload_time_iso_8601": "2022-01-06T20:40:06.021981Z",
        "url": "https://files.pythonhosted.org/packages/cf/f1/296db17600155165e0ca20f18ba449c859f122dad334d662716fadd54666/airflow-aws-executors-1.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cff1296db17600155165e0ca20f18ba449c859f122dad334d662716fadd54666",
        "md5": "a964385d4d13c4404d22e0a07e5b8f74",
        "sha256": "2b2aba670e6df808f660be98a823759ec0898f73ceea98d1d3eda79777b7431f"
      },
      "downloads": -1,
      "filename": "airflow-aws-executors-1.1.3.tar.gz",
      "has_sig": false,
      "md5_digest": "a964385d4d13c4404d22e0a07e5b8f74",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 17899,
      "upload_time": "2022-01-06T20:40:06",
      "upload_time_iso_8601": "2022-01-06T20:40:06.021981Z",
      "url": "https://files.pythonhosted.org/packages/cf/f1/296db17600155165e0ca20f18ba449c859f122dad334d662716fadd54666/airflow-aws-executors-1.1.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}