{
  "info": {
    "author": "Adrien Ehrhardt",
    "author_email": "adrien.ehrhardt@centraliens-lille.org",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "[![PyPI version](https://badge.fury.io/py/glmdisc.svg)](https://badge.fury.io/py/glmdisc)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/glmdisc.svg)](https://pypi.python.org/pypi/glmdisc/)\n[![PyPi Downloads](https://img.shields.io/pypi/dm/glmdisc)](https://img.shields.io/pypi/dm/glmdisc)\n[![Build Status](https://travis-ci.org/adimajo/glmdisc_python.svg?branch=master)](https://travis-ci.org/adimajo/glmdisc_python)\n![Python package](https://github.com/adimajo/glmdisc_python/workflows/Python%20package/badge.svg)\n[![codecov](https://codecov.io/gh/adimajo/glmdisc_python/branch/master/graph/badge.svg)](https://codecov.io/gh/adimajo/glmdisc_python)\n\n# Feature quantization for parsimonious and interpretable models\n\nTable of Contents\n-----------------\n\n* [Documentation](https://adimajo.github.io/glmdisc_python)\n* [Installation instructions](#-installing-the-package)\n* [Theory](#-use-case-example)\n* [Some examples](#-the-glmdisc-package)\n* [Open an issue](https://github.com/adimajo/glmdisc_python/issues/new/choose)\n* [References](#-references)\n* [Contribute](#-contribute)\n\n## Motivation\n\nCredit institutions are interested in the refunding probability of a loan given the applicant’s characteristics in order to assess the worthiness of the credit. For regulatory and interpretability reasons, the logistic regression is still widely used to learn this probability from the data. Although logistic regression handles naturally both quantitative and qualitative data, three pre-processing steps are usually performed: firstly, continuous features are discretized by assigning factor levels to pre-determined intervals; secondly, qualitative features, if they take numerous values, are grouped; thirdly, interactions (products between two different predictors) are sparsely introduced. By reinterpreting discretized (resp. grouped) features as latent variables, we are able, through the use of a Stochastic Expectation-Maximization (SEM) algorithm and a Gibbs sampler to find the best discretization (resp. grouping) scheme w.r.t. the logistic regression loss. For detecting interacting features, the same scheme is used by replacing the Gibbs sampler by a Metropolis-Hastings algorithm. The good performances of this approach are illustrated on simulated and real data from Credit Agricole Consumer Finance.\n\nThis repository is the implementation of Ehrhardt Adrien, et al. [Feature quantization for parsimonious and interpretable predictive models](https://arxiv.org/abs/1903.08920), preprint arXiv:1903.08920 (2019).\n\n## Getting started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n\n### Prerequisites\n\nThis code is supported on Python 3.7, 3.8, 3.9 and 3.10 (see [tox file](tox.ini)).\n\n### Installing the package\n\n#### Installing the development version\n\nIf `git` is installed on your machine, you can use:\n\n```PowerShell\npip install git+https://github.com/adimajo/glmdisc_python.git\n```\n\nIf `git` is not installed, you can also use:\n\n```PowerShell\npip install --upgrade https://github.com/adimajo/glmdisc_python/archive/master.tar.gz\n```\n\n#### Installing through the `pip` command\n\nYou can install a stable version from [PyPi](https://pypi.org/project/glmdisc/) by using:\n\n```PowerShell\npip install glmdisc\n```\n\n#### Installation guide for Anaconda\n\nThe installation with the `pip` command **should** work. If not, please raise an issue.\n\n#### For people behind proxy(ies)...\n\nA lot of people, including myself, work behind a proxy at work...\n\nA simple solution to get the package is to use the `--proxy` option of `pip`:\n\n```PowerShell\npip --proxy=http://username:password@server:port install glmdisc\n```\n\nwhere *username*, *password*, *server* and *port* should be replaced by your own values.\n\nIf environment variables `http_proxy` and / or `https_proxy` and / or (unfortunately depending on applications...) \n`HTTP_PROXY` and `HTTPS_PROXY` are set, the proxy settings should be picked up by `pip`.\n\nOver the years, I've found [CNTLM](http://cntlm.sourceforge.net/) to be a great tool in this regard.\n\n**What follows is a quick introduction to the problem of discretization and how this package answers the question.**\n\n<!--**If you wish to see the package in action, please refer to the accompanying Jupyter Notebook.**-->\n\n<!--**If you seek specific assistance regarding the package or one of its function, please refer to the ReadTheDocs.**-->\n\n## Use case example\n\nFor a thorough explanation of the approach, see [this blog post](https://adimajo.github.io/discretization) or [this article](https://arxiv.org/abs/1903.08920).\n\nIf you're interested in directly using the package, you can skip this part and go to [this part below](#-the-glmdisc-package).\n\nIn practice, the statistical modeler has historical data about each customer's characteristics. For obvious reasons, only data available at the time of inquiry must be used to build a future application scorecard. Those data often take the form of a well-structured table with one line per client alongside their performance (did they pay back their loan or not?) as can be seen in the following table:\n\n| Job | Habitation | Time in job | Children | Family status | Default |\n| --- | --- | --- | --- | --- | --- |\n| Craftsman | Owner | 10 | 0 | Divorced |  No |\n| Technician | Renter | **Missing** | 1 | Widower | No |\n| **Missing** | Starter | 5 | 2 | Single |  Yes |\n| Office employee | By family | 2 | 3 | Married | No |\n\n## Notations\n\nIn the rest of the vignette, the random vector <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X=(X_j)_1^d\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X=(X_j)_1^d\" title=\"X=(X_j)_1^d\" /></a>  will designate the predictive features, i.e. the characteristics of a client. The random variable <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y&space;\\in&space;\\{0,1\\}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y&space;\\in&space;\\{0,1\\}\" title=\"Y \\in \\{0,1\\}\" /></a>  will designate the label, i.e. if the client has defaulted (<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y=1\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y=1\" title=\"Y=1\" /></a>) or not (<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y=0\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y=0\" title=\"Y=0\" /></a>).\n\nWe are provided with an i.i.d. sample <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;(\\mathbf{x},\\mathbf{y})&space;=&space;(x_i,y_i)_1^n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;(\\mathbf{x},\\mathbf{y})&space;=&space;(x_i,y_i)_1^n\" title=\"(\\mathbf{x},\\mathbf{y}) = (x_i,y_i)_1^n\" /></a> consisting in <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;n\" title=\"n\" /></a> observations of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X\" title=\"X\" /></a> and <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y\" title=\"Y\" /></a>.\n\n## Logistic regression\n\nThe logistic regression model assumes the following relation between <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X\" title=\"X\" /></a> and <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y\" title=\"Y\" /></a> :\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\ln&space;\\left(&space;\\frac{p_\\theta(Y=1|x)}{p_\\theta(Y=0|x)}&space;\\right)&space;=&space;\\theta_0&space;&plus;&space;\\sum_{j&space;\\text{&space;if&space;}&space;X_j&space;\\text{&space;continuous}}&space;\\theta_j&space;x_j&space;&plus;&space;\\sum_{j&space;\\text{&space;if&space;}&space;X_j&space;\\text{&space;categorical}}&space;\\theta_j^{x_j}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\ln&space;\\left(&space;\\frac{p_\\theta(Y=1|x)}{p_\\theta(Y=0|x)}&space;\\right)&space;=&space;\\theta_0&space;&plus;&space;\\sum_{j&space;\\text{&space;if&space;}&space;X_j&space;\\text{&space;continuous}}&space;\\theta_j&space;x_j&space;&plus;&space;\\sum_{j&space;\\text{&space;if&space;}&space;X_j&space;\\text{&space;categorical}}&space;\\theta_j^{x_j}\" title=\"\\ln \\left( \\frac{p_\\theta(Y=1|x)}{p_\\theta(Y=0|x)} \\right) = \\theta_0 + \\sum_{j \\text{ if } X_j \\text{ continuous}} \\theta_j x_j + \\sum_{j \\text{ if } X_j \\text{ categorical}} \\theta_j^{x_j}\" /></a>\n\nwhere <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\theta&space;=&space;(\\theta_j)_0^d\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\theta&space;=&space;(\\theta_j)_0^d\" title=\"\\theta = (\\theta_j)_0^d\" /></a> are estimated using <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;(\\mathbf{x},\\mathbf{y})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;(\\mathbf{x},\\mathbf{y})\" title=\"(\\mathbf{x},\\mathbf{y})\" /></a> (and <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\theta_j^h,&space;1&space;\\leq&space;h&space;\\leq&space;l_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\theta_j^h,&space;1&space;\\leq&space;h&space;\\leq&space;l_j\" title=\"\\theta_j^h, 1 \\leq h \\leq l_j\" /></a> denotes the coefficients associated with a categorical feature <a href=\"https://www.codecogs.com/eqnedit.php?latex=x_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?x_j\" title=\"x_j\" /></a> being equal to <a href=\"https://www.codecogs.com/eqnedit.php?latex=h\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?h\" title=\"h\" /></a>).\n\nClearly, for continuous features, the model assumes linearity of the logit transform of the response <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y\" title=\"Y\" /></a> with respect to <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X\" title=\"X\" /></a>.\nOn the contrary, for categorical features, it might overfit if there are lots of levels (<a href=\"https://www.codecogs.com/eqnedit.php?latex=l_j&space;>>&space;1\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?l_j&space;>>&space;1\" title=\"l_j >> 1\" /></a>). It does not handle missing values. \n\n## Common problems with logistic regression on \"raw\" data\n\nFitting a logistic regression model on \"raw\" data presents several problems, among which some are tackled here.\n\n### Feature selection\n\nFirst, among all collected information on individuals, some are irrelevant for predicting <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y\" title=\"Y\" /></a>. Their coefficient <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\theta_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\theta_j\" title=\"\\theta_j\" /></a> should be 0  which might (eventually) be the case asymptotically (i.e. <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;n&space;\\rightarrow&space;\\infty\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;n&space;\\rightarrow&space;\\infty\" title=\"n \\rightarrow \\infty\" /></a>).\n\nSecond, some collected information are highly correlated and affect each other's coefficient estimation.\n\nAs a consequence, data scientists often perform feature selection before training a machine learning algorithm such as logistic regression.\n\nThere already exists methods and packages to perform feature selection, see for example the `feature_selection` submodule in the `sklearn` package.\n\n`glmdisc` is not a feature selection tool but acts as such as a side-effect: when a continuous feature is discretized into only one interval, or when a categorical feature is regrouped into only one value, then this feature gets out of the model.\n\nFor a thorough reference on feature selection, see e.g. Guyon, I., & Elisseeff, A. (2003). [An introduction to variable and feature selection](http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf). *Journal of machine learning research, 3*(Mar), 1157-1182.\n\n### Linearity\n\nWhen provided with continuous features, the logistic regression model assumes linearity of the logit transform of the response <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y\" title=\"Y\" /></a> with respect to <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X\" title=\"X\" /></a>. This might not be the case at all.\n\nFor example, we can simulate a logistic model with an arbitrary power of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X\" title=\"X\" /></a> and then try to fit a linear logistic model:\n\n```python\n\n\n```\n- [ ] Show the Python code\n\n- [ ] Get this graph online\n\nOf course, providing the `sklearn.linear_model.LogisticRegression` function with a dataset containing <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X^5\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X^5\" title=\"X^5\" /></a> would solve the problem. This can't be done in practice for two reasons: first, it is too time-consuming to examine all features and candidate polynomials; second, we lose the interpretability of the logistic decision function which was of primary interest.\n\nConsequently, we wish to discretize the input variable <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X\" title=\"X\" /></a> into a categorical feature which will \"minimize\" the error with respect to the \"true\" underlying relation:\n\n- [ ] Show the Python code\n\n- [ ] Get this graph online\n\n\n### Too many values per categorical feature\n\nWhen provided with categorical features, the logistic regression model fits a coefficient for all its values (except one which is taken as a reference). A common problem arises when there are too many values as each value will be taken by a small number of observations <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;x_i^j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;x_i^j\" title=\"x_i^j\" /></a> which makes the estimation of a logistic regression coefficient unstable:\n\n\n- [ ] Show the Python code\n\n- [ ] Get this graph online\n\n\nIf we divide the training set in 10 and estimate the variance of each coefficient, we get:\n\n- [ ] Show the Python code\n\n- [ ] Get this graph online\n\n\n\nAll intervals crossing 0 are non-significant! We should group factor values to get a stable estimation and (hopefully) significant coefficient values.\n\n\n# Discretization and grouping: theoretical background\n\n## Notations\n\nLet <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}=(\\mathfrak{q}_j)_1^d\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}=(\\mathfrak{q}_j)_1^d\" title=\"\\mathfrak{q}=(\\mathfrak{q}_j)_1^d\" /></a> be the latent discretized transform of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X\" title=\"X\" /></a>, i.e. taking values in <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\{0,\\ldots,m_j\\}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\{0,\\ldots,m_j\\}\" title=\"\\{0,\\ldots,m_j\\}\" /></a> where the number of values of each covariate <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;m_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;m_j\" title=\"m_j\" /></a> is also latent.\n\nThe fitted logistic regression model is now:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\ln&space;\\left(&space;\\frac{p_\\theta(Y=1|e)}{p_\\theta(Y=0|e)}&space;\\right)&space;=&space;\\theta_0&space;&plus;&space;\\sum_{j=1}^d&space;\\sum_{k=1}^{m_j}&space;\\theta^j_k*{1}_{e^j=k}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\ln&space;\\left(&space;\\frac{p_\\theta(Y=1|\\mathfrak{q})}{p_\\theta(Y=0|\\mathfrak{q})}&space;\\right)&space;=&space;\\theta_0&space;&plus;&space;\\sum_{j=1}^d&space;\\sum_{k=1}^{m_j}&space;\\theta^j_k*{1}_{\\mathfrak{q}^j=k}\" title=\"\\ln \\left( \\frac{p_\\theta(Y=1|\\mathfrak{q})}{p_\\theta(Y=0|\\mathfrak{q})} \\right) = \\theta_0 + \\sum_{j=1}^d \\sum_{k=1}^{m_j} \\theta^j_k*{1}_{\\mathfrak{q}^j=k}\" /></a>\n\nClearly, the number of parameters has grown which allows for flexible approximation of the true underlying model <a href=\"https://www.codecogs.com/eqnedit.php?latex=p(Y|\\mathfrak{q})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(Y|\\mathfrak{q})\" title=\"p(Y|\\mathfrak{q})\" /></a>.\n\n## Best discretization?\n\nOur goal is to obtain the model <a href=\"https://www.codecogs.com/eqnedit.php?latex=p_\\theta(Y|\\mathfrak{q})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p_\\theta(Y|\\mathfrak{q})\" title=\"p_\\theta(Y|\\mathfrak{q})\" /></a> with best predictive power. As <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathfrak{q}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathfrak{q}\" title=\"\\mathfrak{q}\" /></a> and <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\theta\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\theta\" title=\"\\theta\" /></a> are both optimized, a formal goodness-of-fit criterion could be:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=(\\hat{\\theta},\\hat{\\mathfrak{q}})&space;=&space;\\arg&space;\\max_{\\theta,\\mathfrak{q}}&space;\\text{AIC}(p_\\theta(\\mathbf{y}|\\mathfrak{q}))\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?(\\hat{\\theta},\\hat{\\mathfrak{q}})&space;=&space;\\arg&space;\\max_{\\theta,\\mathfrak{q}}&space;\\text{AIC}(p_\\theta(\\mathbf{y}|\\mathfrak{q}))\" title=\"(\\hat{\\theta},\\hat{\\mathfrak{q}}) = \\arg \\max_{\\theta,\\mathfrak{q}} \\text{AIC}(p_\\theta(\\mathbf{y}|\\mathfrak{q}))\" /></a>\nwhere AIC stands for Akaike Information Criterion.\n\n## Combinatorics\n\nThe problem seems well-posed: if we were able to generate all discretization schemes transforming <a href=\"https://www.codecogs.com/eqnedit.php?latex=X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?X\" title=\"X\" /></a> to <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathfrak{q}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathfrak{q}\" title=\"\\mathfrak{q}\" /></a>, learn <a href=\"https://www.codecogs.com/eqnedit.php?latex=p_\\theta(y|\\mathfrak{q})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p_\\theta(y|\\mathfrak{q})\" title=\"p_\\theta(y|\\mathfrak{q})\" /></a> for each of them and compare their AIC values, the problem would be solved.\n\nUnfortunately, there are way too many candidates to follow this procedure. Suppose we want to construct k intervals of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> given n distinct <a href=\"https://www.codecogs.com/eqnedit.php?latex=(x_j_i)_1^n\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?(x_j_i)_1^n\" title=\"(x_j_i)_1^n\" /></a>. There is <a href=\"https://www.codecogs.com/eqnedit.php?latex=n&space;\\choose&space;k\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?n&space;\\choose&space;k\" title=\"n \\choose k\" /></a> models. The true value of k is unknown, so it must be looped over. Finally, as logistic regression is a multivariate model, the discretization of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> can influence the discretization of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathfrak{q}_k\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathfrak{q}_k\" title=\"\\mathfrak{q}_k\" /></a>, <a href=\"https://www.codecogs.com/eqnedit.php?latex=k&space;\\neq&space;j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?k&space;\\neq&space;j\" title=\"k \\neq j\" /></a>.\n\nAs a consequence, existing approaches to discretization (in particular discretization of continuous attributes) rely on strong assumptions to simplify the search of good candidates as can be seen in the review of Ramírez‐Gallego, S. et al. (2016) - see [References section](#-references).\n\n\n\n# Discretization and grouping: estimation\n\n## Likelihood estimation\n\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathfrak{q}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathfrak{q}\" title=\"\\mathfrak{q}\" /></a> can be introduced in <a href=\"https://www.codecogs.com/eqnedit.php?latex=p(Y|X)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(Y|X)\" title=\"p(Y|X)\" /></a>:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\forall&space;\\:&space;x,y,&space;\\;&space;p(y|x)&space;=&space;\\sum_\\mathfrak{q}&space;p(y|x,\\mathfrak{q})p(\\mathfrak{q}|x)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\forall&space;\\:&space;x,y,&space;\\;&space;p(y|x)&space;=&space;\\sum_\\mathfrak{q}&space;p(y|x,\\mathfrak{q})p(\\mathfrak{q}|x)\" title=\"\\forall \\: x,y, \\; p(y|x) = \\sum_\\mathfrak{q} p(y|x,\\mathfrak{q})p(\\mathfrak{q}|x)\" /></a>\n\nFirst, we assume that all information about <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;Y\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;Y\" title=\"Y\" /></a> in <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X\" title=\"X\" /></a> is already contained in <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}\" title=\"\\mathfrak{q}\" /></a> so that:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\forall&space;\\:&space;x,y,\\mathfrak{q},&space;\\;&space;p(y|x,\\mathfrak{q})=p(y|\\mathfrak{q})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\forall&space;\\:&space;x,y,\\mathfrak{q},&space;\\;&space;p(y|x,\\mathfrak{q})=p(y|\\mathfrak{q})\" title=\"\\forall \\: x,y,\\mathfrak{q}, \\; p(y|x,\\mathfrak{q})=p(y|\\mathfrak{q})\" /></a>\nSecond, we assume the conditional independence of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> given <a href=\"https://www.codecogs.com/eqnedit.php?latex=X_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?X_j\" title=\"X_j\" /></a>, i.e. knowing <a href=\"https://www.codecogs.com/eqnedit.php?latex=X_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?X_j\" title=\"X_j\" /></a>, the discretization <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> is independent of the other features <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X_k\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X_k\" title=\"X_k\" /></a> and <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}_k\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}_k\" title=\"\\mathfrak{q}_k\" /></a> for all <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;k&space;\\neq&space;j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;k&space;\\neq&space;j\" title=\"k \\neq j\" /></a>:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\forall&space;\\:x,&space;k\\neq&space;j,&space;\\;&space;\\mathfrak{q}_j&space;|&space;x_j&space;\\perp&space;\\mathfrak{q}_k&space;|&space;x_k\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\forall&space;\\:x,&space;k\\neq&space;j,&space;\\;&space;\\mathfrak{q}_j&space;|&space;x_j&space;\\perp&space;\\mathfrak{q}_k&space;|&space;x_k\" title=\"\\forall \\:x, k\\neq j, \\; \\mathfrak{q}_j | x_j \\perp \\mathfrak{q}_k | x_k\" /></a>\nThe first equation becomes:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\forall&space;\\:&space;x,y,&space;\\;&space;p(y|x)&space;=&space;\\sum_\\mathfrak{q}&space;p(y|\\mathfrak{q})&space;\\prod_{j=1}^d&space;p(\\mathfrak{q}_j|x_j)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\forall&space;\\:&space;x,y,&space;\\;&space;p(y|x)&space;=&space;\\sum_\\mathfrak{q}&space;p(y|\\mathfrak{q})&space;\\prod_{j=1}^d&space;p(\\mathfrak{q}_j|x_j)\" title=\"\\forall \\: x,y, \\; p(y|x) = \\sum_\\mathfrak{q} p(y|\\mathfrak{q}) \\prod_{j=1}^d p(\\mathfrak{q}_j|x_j)\" /></a>\nAs said earlier, we consider only logistic regression models on discretized data <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;p_\\theta(y|\\mathfrak{q})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;p_\\theta(y|\\mathfrak{q})\" title=\"p_\\theta(y|\\mathfrak{q})\" /></a>. Additionnally, it seems like we have to make further assumptions on the nature of the relationship of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> to <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;x_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;x_j\" title=\"x_j\" /></a>. We chose to use polytomous logistic regressions for continuous <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X_j\" title=\"X_j\" /></a> and contengency tables for qualitative <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X_j\" title=\"X_j\" /></a>. This is an arbitrary choice and future versions will include the possibility of plugging your own model.\n\nThe first equation becomes:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\forall&space;\\:&space;x,y,&space;\\;&space;p(y|x)&space;=&space;\\sum_\\mathfrak{q}&space;p_\\theta(y|\\mathfrak{q})&space;\\prod_{j=1}^d&space;p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\forall&space;\\:&space;x,y,&space;\\;&space;p(y|x)&space;=&space;\\sum_\\mathfrak{q}&space;p_\\theta(y|\\mathfrak{q})&space;\\prod_{j=1}^d&space;p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" title=\"\\forall \\: x,y, \\; p(y|x) = \\sum_\\mathfrak{q} p_\\theta(y|\\mathfrak{q}) \\prod_{j=1}^d p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" /></a>\n\n## The SEM algorithm\n\nIt is still hard to optimize over <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;p(y|x;\\theta,\\alpha)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;p(y|x;\\theta,\\alpha)\" title=\"p(y|x;\\theta,\\alpha)\" /></a> as the number of candidate discretizations is gigantic as said earlier.\n\nHowever, calculating <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;p(y,\\mathfrak{q}|x)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;p(y,\\mathfrak{q}|x)\" title=\"p(y,\\mathfrak{q}|x)\" /></a> is easy:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\forall&space;\\:&space;x,y,&space;\\;&space;p(y,\\mathfrak{q}|x)&space;=&space;p_\\theta(y|\\mathfrak{q})&space;\\prod_{j=1}^d&space;p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\forall&space;\\:&space;x,y,&space;\\;&space;p(y,\\mathfrak{q}|x)&space;=&space;p_\\theta(y|\\mathfrak{q})&space;\\prod_{j=1}^d&space;p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" title=\"\\forall \\: x,y, \\; p(y,\\mathfrak{q}|x) = p_\\theta(y|\\mathfrak{q}) \\prod_{j=1}^d p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" /></a>\n\nAs a consequence, we will draw random candidates <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}\" title=\"\\mathfrak{q}\" /></a> approximately at the mode of the distribution <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;p(y,\\cdot|x)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;p(y,\\cdot|x)\" title=\"p(y,\\cdot|x)\" /></a> using an SEM algorithm (see see [References section](#-references)).\n\n## Gibbs sampling\n\nTo update, at each random draw, the parameters <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\theta\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\theta\" title=\"\\theta\" /></a> and <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\alpha\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\alpha\" title=\"\\alpha\" /></a> and propose a new discretization <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}\" title=\"\\mathfrak{q}\" /></a>, we use the following equation:\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=p(\\mathfrak{q}_j|x_j,y,\\mathfrak{q}_{\\{-j\\}})&space;\\propto&space;p_\\theta(y|\\mathfrak{q})&space;p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?p(\\mathfrak{q}_j|x_j,y,\\mathfrak{q}_{\\{-j\\}})&space;\\propto&space;p_\\theta(y|\\mathfrak{q})&space;p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" title=\"p(\\mathfrak{q}_j|x_j,y,\\mathfrak{q}_{\\{-j\\}}) \\propto p_\\theta(y|\\mathfrak{q}) p_{\\alpha_j}(\\mathfrak{q}_j|x_j)\" /></a>\nNote that we draw <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> knowing all other variables, especially <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}_{-j}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}_{-j}\" title=\"\\mathfrak{q}_{-j}\" /></a> so that we introduced a Gibbs sampler (see References section).\n\n# The `glmdisc` package\n\n## The `glmdisc` class\n\nThe documentation is available as a [Github Page](https://adimajo.github.io/glmdisc_python/index.html).\n\nThe `glmdisc` class implements the algorithm described in the previous section. Its parameters are described first, then its internals are briefly discussed. We finally focus on its ouptuts.\n\n### Parameters\n\nThe number of iterations in the SEM algorithm is controlled through the `iter` parameter. It can be useful to first run the `glmdisc` function with a low (10-50) `iter` parameter so you can have a better idea of how much time your code will run.\n\nThe `validation` and `test` boolean parameters control if the provided dataset should be divided into training, validation and/or test sets. The validation set aims at evaluating the quality of the model fit at each iteration while the test set provides the quality measure of the final chosen model.\n\nThe `criterion` parameters lets the user choose between standard model selection statistics like `aic` and `bic` and the `gini` index performance measure (proportional to the more traditional AUC measure). Note that if `validation=TRUE`, there is no need to penalize the log-likelihood and `aic` and `bic` become equivalent. On the contrary if `criterion=\"gini\"` and `validation=FALSE` then the algorithm may overfit the training data.\n\nThe `m_start` parameter controls the maximum number of categories of <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> for <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X_j\" title=\"X_j\" /></a> continuous. The SEM algorithm will start with random <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> taking values in <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\{1,m_{\\text{start}}\\}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\{1,m_{\\text{start}}\\}\" title=\"\\{1,m_{\\text{start}}\\}\" /></a>. For qualitative features <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X_j\" title=\"X_j\" /></a>, <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;\\mathfrak{q}_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;\\mathfrak{q}_j\" title=\"\\mathfrak{q}_j\" /></a> is initialized with as many values as <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;X_j\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;X_j\" title=\"X_j\" /></a> so that `m_start` has no effect.\n\nEmpirical studies show that with a reasonably small training dataset (< 10,000 rows) and a small `m_start` parameter (< 20), approximately 500 to 1500 iterations are largely sufficient to obtain a satisfactory model <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;p_\\theta(y|\\mathfraq{q})\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;p_\\theta(y|q(x))\" title=\"p_\\theta(y|q(x))\" /></a>.\n\n```python\n>>> import glmdisc\n>>> logreg_disc = glmdisc.Glmdisc(iter=100, validation=True, test=True, criterion=\"bic\", m_start=10)\n```\n```PowerShell\n2020-07-16 18:11:03.087 | WARNING  | glmdisc:__init__:216 - No need to penalize the log-likelihood when a validation set is used. Using log-likelihood instead.\n```\n\n### The `fit` function\n\nThe `fit` function of the `glmdisc` class is used to run the algorithm over the data provided to it. Subsequently, its parameters are: `predictors_cont` and `predictors_qual` which represent respectively the continuous features to be discretized and the categorical features which values are to be regrouped. They must be of type numpy array, filled with numeric and strings respectively. The last parameter is the class `labels`, of type numpy array as well, in binary form (0/1).\n\n```python\n>>> n = 100\n>>> d = 2\n>>> x, y, _ = glmdisc.Glmdisc.generate_data(n, d)\n>>> logreg_disc.fit(predictors_cont=x, predictors_qual=None, labels=y)\n```\n\n### The `best_formula` function\n\nThe `best_formula` function prints out in the console: the cut-points found for continuous features, the regroupments made for categorical features' values. It also returns it in a list.\n\n```python\n>>> logreg_disc.best_formula()\n```\n```PowerShell\n2020-07-16 18:13:29.921 | INFO     | glmdisc._bestFormula:best_formula:29 - Cut-points found for continuous variable 0\n[0.9568289154869697, 0.6661178585993954, 0.49039089060451335, 0.33038638461067193, 0.7152644679549544]\n2020-07-16 18:13:29.922 | INFO     | glmdisc._bestFormula:best_formula:29 - Cut-points found for continuous variable 1\n[0.48684331022166916, 0.17904111281801316, 0.6603144758481163, 0.03838803248009037]\n```\n\n### The `discrete_data` function\n\nThe `discrete_data` function returns the discretized / regrouped version of the `predictors_cont` and `predictors_qual` arguments using the best discretization scheme found so far.\n\n```python\n>>> logreg_disc.discrete_data()\n```\n```PowerShell\n2020-07-16 18:14:57.261 | INFO     | glmdisc._discreteData:discrete_data:44 - Returning discretized test set.\n<20x11 sparse matrix of type '<class 'numpy.float64'>'\n\twith 40 stored elements in Compressed Sparse Row format>\n```\n\n```python\n>>> logreg_disc.discrete_data().toarray()\n```\n```PowerShell\n2020-07-16 18:15:31.041 | INFO     | glmdisc._discreteData:discrete_data:44 - Returning discretized test set.\narray([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n[...]\n```\n\n### The `discretize` function\n\nThe `discretize` function discretizes a new input dataset in the `predictors_cont`, `predictors_qual` format using the best discretization scheme found so far. The result is a numpy array of the size of the original data.\n\n```python\n>>> n_new = 100\n>>> x_new, _, _ = glmdisc.Glmdisc.generate_data(n_new, d)\n>>> logreg_disc.discretize(predictors_cont=x_new, predictors_qual=None)\n```\n```PowerShell\narray([[4., 1.],\n       [5., 2.],\n       [4., 3.],\n       [4., 4.],\n       [3., 4.],\n       [0., 2.],\n[...]\n```\n\n### The `discretize_dummy` function\n\nThe `discretize_dummy` function discretizes a new input dataset in the `predictors_cont`, `predictors_qual` format using the best discretization scheme found so far. The result is a dummy (0/1) numpy array  corresponding to the One-Hot Encoding of the result provided by the `discretize` function.\n\n```python\n>>> logreg_disc.discretize_dummy(predictors_cont=x_new, predictors_qual=None)\n```\n```PowerShell\n<100x11 sparse matrix of type '<class 'numpy.float64'>'\n\twith 200 stored elements in Compressed Sparse Row format>\n```\n```python\n>>> logreg_disc.discretize_dummy(predictors_cont=x_new, predictors_qual=None).toarray()\n```\n```PowerShell\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 1., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       ...,\n       [1., 0., 0., ..., 1., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.],\n       [1., 0., 0., ..., 0., 0., 0.]])\n```\n\n### The `predict` function\n\nThe `predict` function discretizes a new input dataset in the `predictors_cont`, `predictors_qual` format using the best discretization scheme found so far through the `discretizeDummy` function and then applies the corresponding best Logistic Regression model <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\inline&space;p_\\theta(y|e)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\inline&space;p_\\theta(y|e)\" title=\"p_\\theta(y|e)\" /></a> found so far.\n\n```python\n>>> logreg_disc.predict(predictors_cont=x_new, predictors_qual=None)\n```\n```PowerShell\narray([[9.99394254e-01, 6.05745839e-04],\n       [9.99694576e-01, 3.05424466e-04],\n       [9.99817560e-01, 1.82439609e-04],\n       [9.99967791e-01, 3.22085041e-05],\n       [9.92296119e-01, 7.70388116e-03],\n[...]\n```\n\n### The attributes\n\nAll parameters are stored as attributes: `test`, \n`validation`, `criterion`, `iter`, `m_start` as well as:\n\n* `criterion_iter`: list of values of the criterion chosen;\n```python\n>>> logreg_disc.criterion_iter\n```\n```PowerShell\n[-30.174443117243992, -26.182075441528603, -31.61227858514535, -19.70369464830396, -31.61997286396158, -25.99964499964587, ...]\n```\n* `best_link`: link function of the best quantization;\n```python\n>>> logreg_disc.best_link\n```\n```PowerShell\n[LogisticRegression(C=1e+40, max_iter=25, multi_class='multinomial',\n                   solver='newton-cg', tol=0.001), \nLogisticRegression(C=1e+40, max_iter=25, multi_class='multinomial',\n                   solver='newton-cg', tol=0.001)]\n```\n* `best_reglog`: logistic regression function of the best quantization;\n```python\n>>> logreg_disc.best_reglog\n```\n```PowerShell\nLogisticRegression(C=1e+40, max_iter=25, solver='liblinear', tol=0.001)\n```\n* `affectations`: list of label encoders for categorical features;\n```python\n>>> logreg_disc.affectations\n```\n```PowerShell\n[None, None]\n```\n* `best_encoder_emap`: one hot encoder of the best quantization;\n```python\n>>> logreg_disc.best_encoder_emap\n```\n```PowerShell\nOneHotEncoder(handle_unknown='ignore')\n```\n* `performance`: value of the chosen criterion for the best quantization;\n```python\n>>> logreg_disc.performance\n```\n```PowerShell\n-14.924603930263428\n```\n* `train`: array of row indices for training samples;\n```python\n>>> logreg_disc.train\n```\n```PowerShell\narray([97, 39, 94,  5, 16, 77, 88, 54, 80, 99, 46, 43, 52, 37, 28,  0, 18, ...\n```\n* `validate`: array of row indices for validation samples;\n```python\n>>> logreg_disc.validate\n```\n```PowerShell\narray([36, 45, 29, 62,  8, 82, 76, 96, 41, 83, 17, 49, 57, 31, 60, 64, 65, ...\n```\n* `test_rows`: array of row indices for test samples;\n```python\n>>> logreg_disc.test_rows\n```\n```PowerShell\narray([ 3, 75, 51, 27, 21, 48,  4, 44, 72, 68, 34, 22, 23, 50, 47,  6, 42, ...\n```\n\nTo see the package in action, please refer to [the accompanying Jupyter Notebook](examples/).\n\n- [ ] Do a notebook\n\n## Authors\n\n* [Adrien Ehrhardt](https://adimajo.github.io)\n* [Vincent Vandewalle](https://sites.google.com/site/vvandewa/)\n* [Philippe Heinrich](http://math.univ-lille1.fr/~heinrich/)\n* [Christophe Biernacki](http://math.univ-lille1.fr/~biernack/)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n## Acknowledgments\n\nThis research has been financed by [Crédit Agricole Consumer Finance](https://www.ca-consumerfinance.com/en.html) through a CIFRE PhD.\n\nThis research was supported by [Inria Lille - Nord-Europe](https://www.inria.fr/centre/lille) and [Lille University](https://www.univ-lille.fr/en/home/) as part of a PhD.\n\n## References\n\nEhrhardt, A. (2019), [Formalization and study of statistical problems in Credit Scoring: Reject inference, discretization and pairwise interactions, logistic regression trees](https://hal.archives-ouvertes.fr/tel-02302691) ([PhD thesis](https://github.com/adimajo/manuscrit_these)).\n\nEhrhardt, A., et al. [Feature quantization for parsimonious and interpretable predictive models](https://arxiv.org/abs/1903.08920). arXiv preprint arXiv:1903.08920 (2019)].\n\nCeleux, G., Chauveau, D., Diebolt, J. (1995), [On Stochastic Versions of the EM Algorithm](https://hal.inria.fr/inria-00074164/document). [Research Report] RR-2514, INRIA. 1995. <inria-00074164>\n\nAgresti, A. (2002) [**Categorical Data**](https://onlinelibrary.wiley.com/doi/book/10.1002/0471249688). Second edition. Wiley.\n\nRamírez‐Gallego, S., García, S., Mouriño‐Talín, H., Martínez‐Rego, D., Bolón‐Canedo, V., Alonso‐Betanzos, A. and Herrera, F. (2016). [Data discretization: taxonomy and big data challenge. *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*](https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1173), 6(1), 5-21.\n\n## Future development: integration of interaction discovery\n\nVery often, predictive features $X$ \"interact\" with each other with respect to the response feature. This is classical in the context of Credit Scoring or biostatistics (only the simultaneous presence of several features - genes, SNP, etc. is predictive of a disease).\n\nWith the growing number of potential predictors and the time required to manually analyze if an interaction should be added or not, there is a strong need for automatic procedures that screen potential interaction variables. This will be the subject of future work.\n\n## Future development: possibility of changing model assumptions\n\nIn the third section, we described two fundamental modelling hypotheses that were made:\n>- The real probability density function $p(Y|X)$ can be approximated by a logistic regression $p_\\theta(Y|E)$ on the discretized data $E$.\n>- The nature of the relationship of $\\mathfrak{q}_j$ to $X_j$ is:\n>- A polytomous logistic regression if $X_j$ is continuous;\n>- A contengency table if $X_j$ is qualitative.\n\nThese hypotheses are \"building blocks\" that could be changed at the modeller's will: discretization could optimize other models.\n\n- [ ] To delete when done with\n\n\n### Results\n\nFirst we simulate a \"true\" underlying discrete model:\n```{r, echo=TRUE, results='asis'}\nx = matrix(runif(300), nrow = 100, ncol = 3)\ncuts = seq(0,1,length.out= 4)\nxd = apply(x,2, function(col) as.numeric(cut(col,cuts)))\ntheta = t(matrix(c(0,0,0,2,2,2,-2,-2,-2),ncol=3,nrow=3))\nlog_odd = rowSums(t(sapply(seq_along(xd[,1]), function(row_id) sapply(seq_along(xd[row_id,]),\nfunction(element) theta[xd[row_id,element],element]))))\ny = rbinom(100,1,1/(1+exp(-log_odd)))\n```\n\nThe `glmdisc` function will try to \"recover\" the hidden true discretization `xd` when provided only with `x` and `y`:\n```{r, echo=TRUE,warning=FALSE, message=FALSE, results='hide',eval=FALSE}\nlibrary(glmdisc)\ndiscretization <- glmdisc(x,y,iter=50,m_start=5,test=FALSE,validation=FALSE,criterion=\"aic\",interact=FALSE)\n```\n\n```{r, echo=FALSE,warning=FALSE, message=FALSE, results='hide',eval=TRUE}\nlibrary(glmdisc)\ndiscretization <- glmdisc(x,y,iter=50,m_start=5,test=FALSE,validation=FALSE,criterion=\"aic\",interact=FALSE)\n```\n\n### How well did we do?\n\nTo compare the estimated and the true discretization schemes, we can represent them with respect to the input \"raw\" data `x`:\n<!--```{r, echo=TRUE, out.width='.49\\\\linewidth', fig.width=3, fig.height=3,fig.show='hold'}-->\n```{r, echo=FALSE}\nplot(x[,1],xd[,1])\nplot(discretization@cont.data[,1],discretization@disc.data[,1])\n```\n\n## Contribute\n\nYou can clone this project using:\n\n```PowerShell\ngit clone https://github.com/adimajo/glmdisc_python.git\n```\n\nYou can install all dependencies, including development dependencies, using (note that \nthis command requires `pipenv` which can be installed by typing `pip install pipenv`):\n\n```PowerShell\npipenv install -d\n```\n\nYou can build the documentation by going into the `docs` directory and typing `make html`.\n\nNOTE: you need to have a separate folder named `glmdisc_python_docs` in the same directory as this repository,\nas it will build the docs there so as to allow me to push this other directory as a separate `gh-pages` branch.\n\nYou can run the tests by typing `coverage run -m pytest`, which relies on packages \n[coverage](https://coverage.readthedocs.io/en/coverage-5.2/) and [pytest](https://docs.pytest.org/en/latest/).\n\nTo run the tests in different environments (one for each version of Python), install `pyenv` (see [the instructions here](https://github.com/pyenv/pyenv)),\ninstall all versions you want to test (see [tox.ini](tox.ini)), e.g. with `pyenv install 3.7.0` and run \n`pipenv run pyenv local 3.7.0 [...]` (and all other versions) followed by `pipenv run tox`.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://adimajo.github.io/glmdisc_python",
    "keywords": "discretization logistic regression levels grouping",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "glmdisc",
    "package_url": "https://pypi.org/project/glmdisc/",
    "platform": "",
    "project_url": "https://pypi.org/project/glmdisc/",
    "project_urls": {
      "Homepage": "https://adimajo.github.io/glmdisc_python"
    },
    "release_url": "https://pypi.org/project/glmdisc/0.1.2/",
    "requires_dist": [
      "scikit-learn (>=0.21.2)",
      "numpy (>=1.16.4)",
      "scipy (>=1.3.1)",
      "pandas (>=0.25.1)",
      "pygam (>=0.8.0)",
      "matplotlib (>=3.2)",
      "loguru",
      "tensorflow (>=2.2.1)",
      "livelossplot",
      "ipykernel",
      "bokeh",
      "pytest ; extra == 'dev'",
      "coverage ; extra == 'dev'",
      "sphinx ; extra == 'dev'",
      "sphinx-rtd-theme ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "pipenv-setup ; extra == 'dev'",
      "numpydoc ; extra == 'dev'",
      "wheel ; extra == 'dev'",
      "codecov ; extra == 'dev'",
      "twine ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "tox-pyenv ; extra == 'dev'",
      "hyperas ; extra == 'dev'",
      "hyperopt ; extra == 'dev'"
    ],
    "requires_python": "",
    "summary": "Feature quantization for parsimonious and interpretable models",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8787626,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fd24ac44fe6632191c62f158adf1988822aca7375350ff7ed69e3d5f77a71e06",
          "md5": "905c9207a3b16150373db58fff70b1cb",
          "sha256": "2eee2302a6a348174480d2b7af01dfa236666830ebc53afb1920eb3e9a91862b"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "905c9207a3b16150373db58fff70b1cb",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 19721,
        "upload_time": "2018-02-21T10:48:56",
        "upload_time_iso_8601": "2018-02-21T10:48:56.827630Z",
        "url": "https://files.pythonhosted.org/packages/fd/24/ac44fe6632191c62f158adf1988822aca7375350ff7ed69e3d5f77a71e06/glmdisc-0.0.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "079029224c806afec08241ed0d3e8d704c70955b1f314d873bd789ea2227738f",
          "md5": "9b129592ed705963115996b72c99c710",
          "sha256": "30db02cbebea94a4193d71b959d36352c872757b71c0d4f7b97b09eada54582d"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9b129592ed705963115996b72c99c710",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 36066,
        "upload_time": "2018-02-21T10:48:59",
        "upload_time_iso_8601": "2018-02-21T10:48:59.560641Z",
        "url": "https://files.pythonhosted.org/packages/07/90/29224c806afec08241ed0d3e8d704c70955b1f314d873bd789ea2227738f/glmdisc-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "345375d56dd3eaac3e89271598d4829d9f4527580bf29e8edc1e41a983bb010a",
          "md5": "a97d9709b9deece1bf432c6a06e82cae",
          "sha256": "d55034a78b3b6161e8d35ba0361f4409a6b9dd73019141441212cb9b15767b75"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "a97d9709b9deece1bf432c6a06e82cae",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 33578,
        "upload_time": "2018-02-28T15:19:34",
        "upload_time_iso_8601": "2018-02-28T15:19:34.986403Z",
        "url": "https://files.pythonhosted.org/packages/34/53/75d56dd3eaac3e89271598d4829d9f4527580bf29e8edc1e41a983bb010a/glmdisc-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1ea79f9b824cde8454630b5510374a70a3e9a0ac4886f560a5fcbee5a9138b21",
          "md5": "34a2ca65b07fb2e52e571522153a8269",
          "sha256": "e660ea4b1f4753e3db0967796f562322854fa5febd11b2dc1fc4c0c04143ae03"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "34a2ca65b07fb2e52e571522153a8269",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 21220,
        "upload_time": "2018-04-18T06:56:51",
        "upload_time_iso_8601": "2018-04-18T06:56:51.601805Z",
        "url": "https://files.pythonhosted.org/packages/1e/a7/9f9b824cde8454630b5510374a70a3e9a0ac4886f560a5fcbee5a9138b21/glmdisc-0.0.3-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "93826d8ce9cf9d5d09e98988a243a8945c3e34488f231e3f1071f834f40cfaf8",
          "md5": "ac77c61bd3dfa7e062a9f072988d7eee",
          "sha256": "2e8018c7461947219e12379bd1571d55a9deead7716ebbe5b7bcf40d6d6b3a99"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "ac77c61bd3dfa7e062a9f072988d7eee",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 43456,
        "upload_time": "2018-04-18T16:43:49",
        "upload_time_iso_8601": "2018-04-18T16:43:49.203002Z",
        "url": "https://files.pythonhosted.org/packages/93/82/6d8ce9cf9d5d09e98988a243a8945c3e34488f231e3f1071f834f40cfaf8/glmdisc-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f9a0e92b5f3c58aa80ad6670414973fb1f81610fb1ac4d2eb88e9fa25f07226",
          "md5": "58ecb9f6336b12941d634693ecb01b54",
          "sha256": "cb833b5282b5dad086574fe11737e845126ef58b38c01047cca4efedc2994e88"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "58ecb9f6336b12941d634693ecb01b54",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 458003,
        "upload_time": "2018-04-18T17:10:59",
        "upload_time_iso_8601": "2018-04-18T17:10:59.239063Z",
        "url": "https://files.pythonhosted.org/packages/5f/9a/0e92b5f3c58aa80ad6670414973fb1f81610fb1ac4d2eb88e9fa25f07226/glmdisc-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dd231001e0ecd7000b4b27180d54f03b938ec45bdc6cfbdaae024407ee856a50",
          "md5": "2bbd2bd4c30e167923e9c59e9d4d5408",
          "sha256": "2adc8e9197d542b9aea10ba1a0f8ecbbe68e1feb52de6b7bff1fd38dde150596"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "2bbd2bd4c30e167923e9c59e9d4d5408",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 458011,
        "upload_time": "2018-04-20T07:31:53",
        "upload_time_iso_8601": "2018-04-20T07:31:53.565302Z",
        "url": "https://files.pythonhosted.org/packages/dd/23/1001e0ecd7000b4b27180d54f03b938ec45bdc6cfbdaae024407ee856a50/glmdisc-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c92db7a748b07833925cba230dc27f4c09bd65ae74e59286a52be4a72f5f249f",
          "md5": "45e0b3817bd91fb79f77825404389e6b",
          "sha256": "fa1d2fd68504f5d49cb5b220980d2a8bbc3bcd5adc6e29d7dc9e0b5d0ff82bf4"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "45e0b3817bd91fb79f77825404389e6b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 458002,
        "upload_time": "2018-04-20T07:41:50",
        "upload_time_iso_8601": "2018-04-20T07:41:50.066728Z",
        "url": "https://files.pythonhosted.org/packages/c9/2d/b7a748b07833925cba230dc27f4c09bd65ae74e59286a52be4a72f5f249f/glmdisc-0.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "74fdc1f642d75843d6ff2e839494af6e1f8c92f172be32e956f54132fd679645",
          "md5": "03cd3c5a93e0b26d7377e960006860e8",
          "sha256": "d33dc5178feedbaa4d0e4fcbf48cd822c55d22b0101e4cad022fd22784e1f423"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "03cd3c5a93e0b26d7377e960006860e8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 458000,
        "upload_time": "2018-04-20T07:46:08",
        "upload_time_iso_8601": "2018-04-20T07:46:08.080833Z",
        "url": "https://files.pythonhosted.org/packages/74/fd/c1f642d75843d6ff2e839494af6e1f8c92f172be32e956f54132fd679645/glmdisc-0.0.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "adf65977a47d927a0bdae22fa853df84a47ed2128659c442373597df5b5eaa24",
          "md5": "5db39da34acc67181021050a91068684",
          "sha256": "34fee7737a679c0ce5b41c21b399acd77836c1906d484b9d5360711c23ead819"
        },
        "downloads": -1,
        "filename": "glmdisc-0.0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "5db39da34acc67181021050a91068684",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 54048,
        "upload_time": "2019-09-06T12:46:01",
        "upload_time_iso_8601": "2019-09-06T12:46:01.084128Z",
        "url": "https://files.pythonhosted.org/packages/ad/f6/5977a47d927a0bdae22fa853df84a47ed2128659c442373597df5b5eaa24/glmdisc-0.0.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7289754bd62a9db4d4d278fc8008d494f4bca42543d003b29e1aeec116baceb8",
          "md5": "1b1c1808b40ef3f8f7e03f44e5c1816b",
          "sha256": "0f2821159dc9a3d042a171c7d2d3810997a2d83cf5ec6975dea33242e0f66d6f"
        },
        "downloads": -1,
        "filename": "glmdisc-0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1b1c1808b40ef3f8f7e03f44e5c1816b",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 29287,
        "upload_time": "2020-07-10T21:44:16",
        "upload_time_iso_8601": "2020-07-10T21:44:16.829803Z",
        "url": "https://files.pythonhosted.org/packages/72/89/754bd62a9db4d4d278fc8008d494f4bca42543d003b29e1aeec116baceb8/glmdisc-0.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "984837a442c9aabb24e3bf8a3fd281cdbc224395c89c5a627dd805283e7113ca",
          "md5": "5ef2ee0bb6503526e396cf9446d09965",
          "sha256": "739daa8dc699bc2a5cff30bd16e37bbf652b8ef8b662a082b386dece937302b5"
        },
        "downloads": -1,
        "filename": "glmdisc-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5ef2ee0bb6503526e396cf9446d09965",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 38486,
        "upload_time": "2020-07-10T21:44:18",
        "upload_time_iso_8601": "2020-07-10T21:44:18.499109Z",
        "url": "https://files.pythonhosted.org/packages/98/48/37a442c9aabb24e3bf8a3fd281cdbc224395c89c5a627dd805283e7113ca/glmdisc-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f4aefff409a35c7a044e83860880c99779b0496f0087cf344c5a701aaa8b27c3",
          "md5": "7a4ef2b756483369f8dfc5a30c404393",
          "sha256": "c1a509fd7b474d719c17b02c053dee97d7c7c5568cadf28d08231c5cafcff070"
        },
        "downloads": -1,
        "filename": "glmdisc-0.1.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7a4ef2b756483369f8dfc5a30c404393",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 40121,
        "upload_time": "2020-07-11T13:07:23",
        "upload_time_iso_8601": "2020-07-11T13:07:23.020102Z",
        "url": "https://files.pythonhosted.org/packages/f4/ae/fff409a35c7a044e83860880c99779b0496f0087cf344c5a701aaa8b27c3/glmdisc-0.1.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a287b4f9728263604fa7045ea864045f2124236c57e93fced3a30298c6afc467",
          "md5": "544a706be31bab850f72d4d9ba229d6b",
          "sha256": "01c08f47a2f6a05185ca5c00f24312f18a1cbd8281bd81606d723c5cdafdcfe7"
        },
        "downloads": -1,
        "filename": "glmdisc-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "544a706be31bab850f72d4d9ba229d6b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 59787,
        "upload_time": "2020-07-11T13:07:24",
        "upload_time_iso_8601": "2020-07-11T13:07:24.781144Z",
        "url": "https://files.pythonhosted.org/packages/a2/87/b4f9728263604fa7045ea864045f2124236c57e93fced3a30298c6afc467/glmdisc-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b52ffe04cc1a316fbbab3210a49c32fb68e08fdd33befed8e02ad66a53daa917",
          "md5": "64e8d07d272a955cc1b1543db1e60b9b",
          "sha256": "8c0f30eca5e45dfc9224d07eedd9acbaeb43978d7d40dda5b4bf597790900df7"
        },
        "downloads": -1,
        "filename": "glmdisc-0.1.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "64e8d07d272a955cc1b1543db1e60b9b",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 51550,
        "upload_time": "2020-12-01T08:21:31",
        "upload_time_iso_8601": "2020-12-01T08:21:31.484266Z",
        "url": "https://files.pythonhosted.org/packages/b5/2f/fe04cc1a316fbbab3210a49c32fb68e08fdd33befed8e02ad66a53daa917/glmdisc-0.1.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8df77c92290737af0467aaf6c42c84d6aaf230ca8f5e18c86ae0aa7c041430d3",
          "md5": "1faef547f08494df11f558dae2d6c1e5",
          "sha256": "3af9dc7a5921dbd12c0607729d3e6c774cdd1b760830adf5f61be934013b86e3"
        },
        "downloads": -1,
        "filename": "glmdisc-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1faef547f08494df11f558dae2d6c1e5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 72111,
        "upload_time": "2020-12-01T08:21:33",
        "upload_time_iso_8601": "2020-12-01T08:21:33.354653Z",
        "url": "https://files.pythonhosted.org/packages/8d/f7/7c92290737af0467aaf6c42c84d6aaf230ca8f5e18c86ae0aa7c041430d3/glmdisc-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b52ffe04cc1a316fbbab3210a49c32fb68e08fdd33befed8e02ad66a53daa917",
        "md5": "64e8d07d272a955cc1b1543db1e60b9b",
        "sha256": "8c0f30eca5e45dfc9224d07eedd9acbaeb43978d7d40dda5b4bf597790900df7"
      },
      "downloads": -1,
      "filename": "glmdisc-0.1.2-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "64e8d07d272a955cc1b1543db1e60b9b",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 51550,
      "upload_time": "2020-12-01T08:21:31",
      "upload_time_iso_8601": "2020-12-01T08:21:31.484266Z",
      "url": "https://files.pythonhosted.org/packages/b5/2f/fe04cc1a316fbbab3210a49c32fb68e08fdd33befed8e02ad66a53daa917/glmdisc-0.1.2-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8df77c92290737af0467aaf6c42c84d6aaf230ca8f5e18c86ae0aa7c041430d3",
        "md5": "1faef547f08494df11f558dae2d6c1e5",
        "sha256": "3af9dc7a5921dbd12c0607729d3e6c774cdd1b760830adf5f61be934013b86e3"
      },
      "downloads": -1,
      "filename": "glmdisc-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "1faef547f08494df11f558dae2d6c1e5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 72111,
      "upload_time": "2020-12-01T08:21:33",
      "upload_time_iso_8601": "2020-12-01T08:21:33.354653Z",
      "url": "https://files.pythonhosted.org/packages/8d/f7/7c92290737af0467aaf6c42c84d6aaf230ca8f5e18c86ae0aa7c041430d3/glmdisc-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}