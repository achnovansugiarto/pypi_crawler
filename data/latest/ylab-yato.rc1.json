{
  "info": {
    "author": "Jie Yang, Zeqiang Wang",
    "author_email": "y@zju.edu.cn",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "\n## YATO: Yet Another deep learning based Text analysis Open toolkit\n\n## Quick Links\n\n- [Introduction](#introduction)\n  - [Getting Started](#getting-started)\n  - [Data Format](#data-format)\n  - [Configuration Preparation](#configuration-preparation)\n    - [Training Configuration](#training-configuration)\n      - [Dataloader](#dataloader)\n      - [Model](#model)\n    - [Hyperparameters](#hyperparameters)\n    - [Decode Configuration](#decode-configuration)\n  - [Performance](#performance)\n    - [Results for sequence labeling tasks.](#results-for-sequence-labeling-tasks)\n    - [Results for sequence classification tasks.](#results-for-sequence-classification-tasks)\n  - [Add Handcrafted Features](#add-handcrafted-features)\n  - [Speed](#speed)\n  - [N best Decoding](#n-best-decoding)\n  - [Text Attention Heatmap Visualization](#text-attention-heatmap-visualization)\n  - [Reproduce Paper Results and Hyperparameter Tuning](#reproduce-paper-results-and-hyperparameter-tuning)\n  - [Report Issue or Problem](#report-issue-or-problem)\n  - [Cite](#cite)\n  - [Future Plan](#future-plan)\n  - [Updates](#updates)\n\n# Introduction\n\n**YATO**, an open-source Python library for text analysis. In particular, **YATO** focuses on sequence labeling and sequence classification tasks, including extensive fundamental NLP tasks such as part-of-speech tagging, chunking, NER, CCG super tagging, sentiment analysis, and sentence classification. **YATO** can design both specific RNN-based and Transformer-based through user-friendly configuration and integrating the SOTA pre-trained language models, such as BERT.\n\n**YATO** is a PyTorch-based framework with flexible choices of input features and output structures. The design of neural sequence models with **YATO** is fully configurable through a configuration file, which does not require any code work.\n\nIts previous version called ****NCRF++**** has been accepted as a demo paper by ACL 2018. The in-depth experimental report based on ****NCRF++**** was accepted as the best paper by COLING 2018.\n\nWelcome to star this repository!\n\n![alt text](./readme/YATO.png \"YATO\")\n\n\n## Getting Started\n\nWe provide an easy way to use the toolkit **YATO** from PyPI\n\n```bash\npip install yato\n```\n\nOr directly install it from the source  code\n\n```\ngit clone https://github.com/jiesutd/YATO.git\n```\n\nThe code to train a Model\n\n```python\nfrom yato import YATO\nmodel = YATO(configuration file)\nmodel.train()\n```\n\nThe code to decode prediction files:\n\n```\nfrom yato import YATO\ndecode_model = YATO(configuration file)\nresult_dict = decode_model.decode()\n```\n\nreturn dictionary contents following value:\n\n- speed: decoding speed\n- accuracy: If the decoded file contains annotation results, accuracy means verifying the accuracy\n- precision:  If the decoded file contains annotation results, precision means verifying the precision\n- recall:  If the decoded file contains annotation results, recall means verifying the recall\n- predict_result: predicted result\n- nbest_predict_score: nbest scores of decoded prediction\n- label: Mapping between labels and indexes\n\n## Data Format\n\n* Refer [sample_data](sample_data) for the detailed data format.\n* **YATO** supports both BIO and BIOES(BMES) tag schemes.\n* Notice that IOB format (***different*** from BIO) is currently not supported, because this tag scheme is old and works worse than other schemes [Reimers and Gurevych, 2017](https://arxiv.org/pdf/1707.06799.pdf).\n* The differences among these three tag schemes are explained in this [paper](https://arxiv.org/pdf/1707.06799.pdf).\n* We provided a [script](utils/tagSchemeConverter.py) which supports convertation of tag scheme among IOB/BIO/BIOES. Welcome to have a try.\n\n## Configuration Preparation\n\nYou can specify the model, optimizer, and decoding through the configuration file:\n\n### Training Configuration\n\n#### Dataloader\n\ntrain_dir=the path of the train file   \ndev_dir=the path of the validation file    \ntest_dir=the path of the test file    \nmodel_dir=the path to save model weights       \ndset_dir=the path of configuration encode file         \n\n#### Model\n\nuse_crf=True/False        \nuse_char=True/False            \nchar_seq_feature=GRU/LSTM/CNN/False          \nuse_word_seq=True/False          \nuse_word_emb=True/False          \nword_emb_dir=The path of word embedding file          \nword_seq_feature=GRU/LSTM/CNN/FeedFowrd/False          \nlow_level_transformer=pretrain language model from huggingface          \nlow_level_transformer_finetune=True/False          \nhigh_level_transformer=pretrain language model from huggingface          \nhigh_level_transformer_finetune=True/False          \ncnn_layer=layer number          \nchar_hidden_dim=dimension number          \nhidden_dim=dimension number          \nlstm_layer=layer number          \nbilstm=True/False          \n\n### Hyperparameters\n\nsentence_classification=True/False          \nstatus=train/decode          \ndropout=Dropout Rate          \noptimizer=SGD/Adagrad/adadelta/rmsprop/adam/adamw          \niteration=epoch number          \nbatch_size=batch size          \nlearning_rate=learning rate          \ngpu=True/False          \ndevice=cuda:0          \nscheduler=get_linear_schedule_with_warmup/get_cosine_schedule_with_warmup          \nwarmup_step_rate=warmup steo rate          \n\n### Decode Configuration\n\nstatus=decode          \nraw_dir=The path of decode file          \nnbest=0 (NER)/1 (sentence classification)          \ndecode_dir=The path of decode result file          \nload_model_dir=The path of model weights          \nsentence_classification=True/False          \n\n## Performance\n\nFor multiple sequence labeling and sequence classification tasks, YATO has reproduced or outperformed the reported SOTA results on majority datasets.\n\nBy default, the `LSTM` is a bidirectional LSTM. The `BERT-base` is huggingface's bert-base-uncased. The `RoBERTa-base` is huggingface's roberta-base. The `ELECTRA-base` is huggingface's google/electra-base-discriminator.\n\n#### Results for sequence labeling tasks.\n\n| ID | Model          | CoNLL2003 | OntoNotes 5.0 | MSRA  | Ontonotes 4.0 | CCG   |\n| -- | -------------- | --------- | ------------- | ----- | ------------- | ----- |\n| 1  | CCNN+WLSTM+CRF | 91.00     | 81.53         | 92.83 | 74.55         | 93.80 |\n| 2  | BERT-base      | 91.61     | 84.68         | 95.81 | 80.57         | 96.14 |\n| 3  | RoBERTa-base   | 90.23     | 86.28         | 96.02 | 80.94         | 96.16 |\n| 4  | ELECTRA-base   | 91.59     | 85.25         | 96.03 | 90.47         | 96.29 |\n\n#### Results for sequence classification tasks.\n\n| ID | Model        | SST2  | SST5  | ChnSentiCorp |\n| -- | ------------ | ----- | ----- | ------------ |\n| 1  | CCNN+WLSTM   | 87.61 | 43.48 | 88.22        |\n| 2  | BERT-base    | 93.00 | 53.48 | 95.86        |\n| 3  | RoBERTa-base | 92.55 | 51.99 | 96.04        |\n| 4  | ELECTRA-base | 94.72 | 55.11 | 95.96        |\n\nFor more details, you can refer to our papers mentioned below.\n\nWe have compared twelve neural sequence labeling models (`{charLSTM, charCNN, None} x {wordLSTM, wordCNN} x {softmax, CRF}`) on three benchmarks (POS, Chunking, NER) under statistical experiments, detailed results and comparisons can be found in our COLING 2018 paper [Design Challenges and Misconceptions in Neural Sequence Labeling](https://arxiv.org/abs/1806.04470).\n\nThe results based on Pretrain Language Model were recorded in [YATO: Yet Another deep learning based\\ Text analysis Open toolkit]()\n\n## Add Handcrafted Features\n\n**YATO** has integrated several SOTA neural character sequence feature extractors: CNN ([Ma .etc, ACL16](http://www.aclweb.org/anthology/P/P16/P16-1101.pdf)), LSTM ([Lample .etc, NAACL16](http://www.aclweb.org/anthology/N/N16/N16-1030.pdf)) and GRU ([Yang .etc, ICLR17](https://arxiv.org/pdf/1703.06345.pdf)). In addition, hand-crafted features have been proven to be important in sequence labeling tasks. **YATO** supports users designing their features such as Capitalization, POS tag, or any other features (grey circles in the above figure). Users can configure the self-defined features through a configuration file (feature embedding size, pretrained feature embeddings .etc). The sample of input format is given at [train.cappos.bmes](sample_data/train.cappos.bmes), which includes two hand-crafted features `[POS]` and `[Cap]`. (`[POS]` and `[Cap]` are two examples, you can set your feature any name you want, just follow the format `[xx]` and configure the feature with the same name in the configuration file.)\nUsers can configure each feature in configuration file by using\n\n```Python\nfeature=[POS] emb_size=20 emb_dir=%your_pretrained_POS_embedding\nfeature=[Cap] emb_size=20 emb_dir=%your_pretrained_Cap_embedding\n```\n\nThe feature without pretrained embedding will be randomly initialized.\n\n## Speed\n\n**YATO** is implemented using a fully batch computing approach, making it quite efficient in both model training and decoding.\n\nWith the help of GPU (Nvidia RTX 2080ti) and large batches, models built with **YATO** can be decoded efficiently.\n\n![alt text](./readme/speed.png \"System speed on NER data\")\n\n## N best Decoding\n\nThe traditional CRF structure decodes only one label sequence with the largest probabilities (i.e. 1-best output). In contrast, **YATO** can decode `n` label sequences with the top `n` probabilities (i.e. n-best output). The nbest decoding has been supported by several popular **statistical** CRF frameworks. To the best of our knowledge, **YATO** is the only and the first toolkit which supports nbest decoding in **neural** CRF models.\n\nIn our implementation, the model built in **YATO** can improve the F1-score by 5.7%-6.8% in the CoNLL 2003 NER task when nbest=10.\n![alt text](./readme/nbest.png  \"N best decoding F1 result\")\n\n## Text Attention Heatmap Visualization\n\n**YATO** takes the list of words and the corresponding weights as input to generate Latex code for visualizing the attention-based result.\n\nThe Latex code will generate a separate .pdf visualization file.\n\n![alt text](./readme/attention.png \"Visualization of attention map in Transformer-based model\")\n\nFor example,\n\n```python\nfrom yato import YATO\nfrom utils import text_attention\nmodel = YATO(decode configuration file)\n\nsample = [\"a fairly by-the-books blend of action and romance with sprinklings of intentional and unintentional comedy . ||| 1\"]\nprobsutils, weights_ls = model.attention(input_text=sample)\nsentece = \"a fairly by-the-books blend of action and romance with sprinklings of intentional and unintentional comedy . \"\natten = weights_ls[0].tolist()\n\ntext_attention.visualization(sentece, atten[0], tex = 'sample.tex', color='red')\n```\n\n## Reproduce Paper Results and Hyperparameter Tuning\n\nTo reproduce the results in our COLING 2018 paper, you only need to set the `iteration=1` as `iteration=100` in the configuration file `demo.train.config` and then configure your file directory.\n\nThe default configuration file adopts the `Char CNN + Word LSTM + CRF` model, and you can develop your model by modifying the configuration accordingly. The parameters in this demo configuration file are the same as our paper. (Notice the `Word CNN` related models need slightly adjust parameters, details can be found in our COLING paper.)\n\nIf you want to use this framework in new tasks or datasets, here are some tuning [tips](readme/hyperparameter_tuning.md) by @Victor0118.\n\n## Report Issue or Problem\n\nIf you want to report an issue or ask a problem, please attach the following materials if necessary. With these information, we can provide a fast and accurate discussion and the corrsponding suggestions.\n\n* `log file`\n* `config file`\n* `sample data`\n\n## Cite\n\nIf you use **NCRF++** in your paper, please cite our [ACL demo paper](https://arxiv.org/abs/1806.05626):\n\n    @inproceedings{yang2018ncrf,\n    title={**YATO**: An Open-source Neural Sequence Labeling Toolkit},\n    author={Yang, Jie and Zhang, Yue},\n    booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},\n     Url = {http://aclweb.org/anthology/P18-4013},\n     year={2018}\n    }\n\nIf you use experiment results and analysis of **NCRF++**, please cite our [COLING paper](https://arxiv.org/abs/1806.04470):\n\n    @inproceedings{yang2018design,\n    title={Design Challenges and Misconceptions in Neural Sequence Labeling},\n    author={Yang, Jie and Liang, Shuailong and Zhang, Yue},\n    booktitle={Proceedings of the 27th International Conference on Computational Linguistics (COLING)},\n     Url = {http://aclweb.org/anthology/C18-1327},\n     year={2018}\n    }\n\n## Future Plan\n\n* Document classification (working)\n* Support API usage\n* Upload trained model on Word Segmentation / POS tagging / NER\n\n## Updates\n\n* 2022-May-14  YATO, init version\n* 2020-Mar-06, dev version, sentence classification, framework change, model saved in one file.\n* 2018-Dec-17, **YATO** v0.2, support PyTorch 1.0\n* 2018-Mar-30, **YATO** v0.1, initial version\n* 2018-Jan-06, add result comparison.\n* 2018-Jan-02, support character feature selection.\n* 2017-Dec-06, init version\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/jiesutd/YATO",
    "keywords": "",
    "license": "Apache License II",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ylab-yato",
    "package_url": "https://pypi.org/project/ylab-yato/",
    "platform": null,
    "project_url": "https://pypi.org/project/ylab-yato/",
    "project_urls": {
      "Homepage": "https://github.com/jiesutd/YATO"
    },
    "release_url": "https://pypi.org/project/ylab-yato/0.1.2/",
    "requires_dist": [
      "torch (>1.1.0)",
      "transformers (>=4.10.2)"
    ],
    "requires_python": ">=3.6.0",
    "summary": "YATO: Yet Another deep learning based Text analysis Open toolkit",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15114237,
  "releases": {
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "54c7405fd721e010d3165ff59033b70ff958d4751c9999cb646c6b3263360341",
          "md5": "e48488a7e9ff7f5e869d40aa67694962",
          "sha256": "10984430ec699a679c8f3790386075f66e34957d445b76883fe8dddbc35d7b62"
        },
        "downloads": -1,
        "filename": "ylab_yato-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e48488a7e9ff7f5e869d40aa67694962",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 54471,
        "upload_time": "2022-09-16T07:00:29",
        "upload_time_iso_8601": "2022-09-16T07:00:29.264129Z",
        "url": "https://files.pythonhosted.org/packages/54/c7/405fd721e010d3165ff59033b70ff958d4751c9999cb646c6b3263360341/ylab_yato-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "01f1dcb95fb7eadd45ad647b7251b03deb6e926f2bf0a7ea3ad0092ba842346e",
          "md5": "893b52dc1f113cb6ea595185c4c513d4",
          "sha256": "6abe9cc491b462470c76e81c3aec6ec74103cbf33b7e9aa1bc7fbd5761c04048"
        },
        "downloads": -1,
        "filename": "ylab-yato-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "893b52dc1f113cb6ea595185c4c513d4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 51383,
        "upload_time": "2022-09-16T07:00:31",
        "upload_time_iso_8601": "2022-09-16T07:00:31.492832Z",
        "url": "https://files.pythonhosted.org/packages/01/f1/dcb95fb7eadd45ad647b7251b03deb6e926f2bf0a7ea3ad0092ba842346e/ylab-yato-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "54c7405fd721e010d3165ff59033b70ff958d4751c9999cb646c6b3263360341",
        "md5": "e48488a7e9ff7f5e869d40aa67694962",
        "sha256": "10984430ec699a679c8f3790386075f66e34957d445b76883fe8dddbc35d7b62"
      },
      "downloads": -1,
      "filename": "ylab_yato-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e48488a7e9ff7f5e869d40aa67694962",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 54471,
      "upload_time": "2022-09-16T07:00:29",
      "upload_time_iso_8601": "2022-09-16T07:00:29.264129Z",
      "url": "https://files.pythonhosted.org/packages/54/c7/405fd721e010d3165ff59033b70ff958d4751c9999cb646c6b3263360341/ylab_yato-0.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "01f1dcb95fb7eadd45ad647b7251b03deb6e926f2bf0a7ea3ad0092ba842346e",
        "md5": "893b52dc1f113cb6ea595185c4c513d4",
        "sha256": "6abe9cc491b462470c76e81c3aec6ec74103cbf33b7e9aa1bc7fbd5761c04048"
      },
      "downloads": -1,
      "filename": "ylab-yato-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "893b52dc1f113cb6ea595185c4c513d4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 51383,
      "upload_time": "2022-09-16T07:00:31",
      "upload_time_iso_8601": "2022-09-16T07:00:31.492832Z",
      "url": "https://files.pythonhosted.org/packages/01/f1/dcb95fb7eadd45ad647b7251b03deb6e926f2bf0a7ea3ad0092ba842346e/ylab-yato-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}