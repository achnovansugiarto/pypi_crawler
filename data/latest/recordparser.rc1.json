{
  "info": {
    "author": "Ken Youens-Clark",
    "author_email": "kyclark@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Python recordparser\n\nParse delimited text file into a class\n\n## Synopsis\n\nThis module will parse a delimited text file, transforming each record\nof text fields into a given class with fields and data types.\nOnly return records that conform to this description will be returned.\n\nFor example, given an input file like this:\n\n```\n$ cat test_data/items.csv\nid_,name,price\n1,foo,3.25\n2,bar,.43\n3,baz,4.01\n```\n\nWe can create a class called `Item` to describe a record with the following \ndefinition:\n\n```\nfrom typing import NamedTuple, TextIO\n\n\nclass Item(NamedTuple):\n    id_: int\n    name: str\n    price: float\n```\n\nThe `recordparser.parse()` function will return a generator of records that \nwill be of the type `Item`.\nVarious problems may cause this to fail which are currently raised\nwith exceptions, so use `try`/`catch`:\n\n```\ntry:\n    parser = recordparser.parse(fh=open('test_data/items.csv'),\n                                cls=Item,\n                                delimiter=',',\n                                quiet=False)\n\n    for rec in parser:\n        print(rec)\n\nexcept Exception as err:\n    print(err)\n```\n\nThis would print the following:\n\n```\nItem(id_=1, name='foo', price=3.25)\nItem(id_=2, name='bar', price=0.43)\nItem(id_=3, name='baz', price=4.01)\n```\n\n## Input file delimiter\n\nThe default field delimiter is a comma, but you can set the \"delimiter\"\nto any valid character.\nThe `csv` module is used to parse the input text files.\n\n## Ignore undefined fields\n\nThe source file may name columns which the given class does not describe.\nOnly the columns from the class will be used, so the extra fields will be\nignored.\nHere the \"can_discount\" field is skipped:\n\n```\n$ cat test_data/extra_fields.csv\nid_,name,price,can_discount\n1,foo,3.25,True\n2,bar,.43,False\n3,baz,4.01,True\n```\n\nSo the program would emit the same output:\n\n```\nItem(id_=1, name='foo', price=3.25)\nItem(id_=2, name='bar', price=0.43)\nItem(id_=3, name='baz', price=4.01)\n```\n\n## Modify column names\n\nOften an input file will have column names you need to map to your class'\nfields.\nYou can provide a \"mapping\" dictionary of the source name to a field name.\nIn this file, the columns \"id,\" \"item,\" and \"cost\" need to be mapped to \"id_,\"\n\"name,\" and \"price,\" respectively:\n\n```\n$ cat test_data/other_names.csv\nid,item,cost\n1,foo,3.25\n2,bar,.43\n3,baz,4.01\n```\n\nHere is the code:\n\n```\nmapping = {'id': 'id_', 'item': 'name', 'cost': 'price'}\nparser: Iterable[Item] = recordparser.parse(fh=args.file,\n                                            cls=Item,\n                                            mapping=mapping\n                                            delimiter=args.delimiter)\n```\n\nThe field will be mapped to their correct counterparts in the `Item` class:\n\n```\nItem(id_=1, name='foo', price=3.25)\nItem(id_=2, name='bar', price=0.43)\nItem(id_=3, name='baz', price=4.01)\n```\n\n## Parsing files without column names\n\nIf you have an input file that does not list the columns in the first\nline, you can set the \"fieldnames\" option to pass to `csv.DictReader`.\nBe sure not to pass this option to _rename_ existing columns; instead use \nthe above \"mapping\" option.\n\n```\n$ cat test_data/no_names.csv\n1,foo,3.25\n2,bar,.43\n3,baz,4.01\n```\n\nThis is the code:\n\n```\nparser: Iterable[Item] = recordparser.parse(\n    fh=args.file,\n    cls=Item,\n    fieldnames=['id_', 'name', 'price'])\n```\n\nTo produce our familiar output:\n\n```\nItem(id_=1, name='foo', price=3.25)\nItem(id_=2, name='bar', price=0.43)\nItem(id_=3, name='baz', price=4.01)\n```\n\n## Failure on missing fields\n\nIf the input file is missing field definitions, the module will throw \nan exception (which may not be great):\n\n```\n$ cat test_data/missing_price.csv\nid_,name\n1,foo\n2,bar\n3,baz\n```\n\nGiven the `Item` class, this file would be rejected:\n\n```\n$ ./rc.py test_data/missing_price.csv\nMissing field: price\n```\n\n## Optional fields\n\nYou can use the `Optional` type to allow a field to be missing:\n\n```\nclass Item(NamedTuple):\n    id_: int\n    name: str\n    price: Optional[float]\n```\n\nAnd now the \"price\" will be `None`:\n\n```\n$ ./rc.py test_data/missing_price.csv\nItem(id_=1, name='foo', price=None)\nItem(id_=2, name='bar', price=None)\nItem(id_=3, name='baz', price=None)\n```\n\n## Skipping data that does not match\n\nIf a record cannot be parsed according to the class definition, the \nrecord will be skipped and a message printed to STDERR (unless you \npass `True` for the \"quiet\" option).\nIt's hard to see in this example (which is the point), but the \"price\"\nof the \"bar\" item has a capital \"O\" instead of a \"0\":\n\n```\n$ cat test_data/bad_item.csv\nid_,name,price\n1,foo,3.25\n2,bar,O.43\n3,baz,4.01\n```\n\nIf the \"price\" is a required (not `Optional`) field, the record will be skipped:\n\n```\n$ ./rc.py test_data/bad_item.csv\nItem(id_=1, name='foo', price=3.25)\n1: Cannot convert \"O.43\" to \"<class 'float'>\"\nItem(id_=3, name='baz', price=4.01)\n```\n\nIf the \"price\" is `Optional`, then the record will be present but the value\nwill be a `None`:\n\n```\n$ ./rc.py !$\n./rc.py test_data/bad_item.csv\nItem(id_=1, name='foo', price=3.25)\n1: Cannot convert \"O.43\" to \"typing.Union[float, NoneType]\"\nItem(id_=2, name='bar', price=None)\nItem(id_=3, name='baz', price=4.01)\n```\n\n## Union types\n\nSometime the data in a column may be a mix of values.\nIn this case, you can use a `Union` type to declare the various type\n_in the order you wish to try_.\nFor instance, we first want to try to convert \"price\" to a `float` and then\nuse `str` for a backup:\n\n```\nclass Item(NamedTuple):\n    id_: int\n    name: str\n    price: Union[float, str]\n```\n\nSo that this data:\n\n```\n$ cat test_data/mixed_types.csv\nid_,name,price\n1,foo,3.25\n2,bar,NA\n3,baz,4.01\n```\n\nWill parse like so:\n\n```\nItem(id_=1, name='foo', price=3.25)\nItem(id_=2, name='bar', price='NA')\nItem(id_=3, name='baz', price=4.01)\n```\n\n## Base class must be NamedTuple\n\nYour class must be derived from a `NamedTuple`.\nFuture versions may allow for the use of a `TypedDict`.\nThis means that the objects/records will be _immutable_ (which, IMHO, \nis a Good Thing).\n\n## Motivation\n\nI spend too much time parsing messy delimited files/Excel spreadsheets.\nOften there are some values that need to be coerced to something like number.\nIf these values cannot be cast, I should skip the data.\nSo I kept writing `for` loops with `try`/`catch` to handle exceptions \nfrom `float(val)` and so forth.\n\nWith this module, I can _describe_ what the resulting data should look like\nusing a `class`.\nThe `recordparser` will handle missing and optional fields, multiple data \ntypes, and the mapping of the file's column names to the fields in my class.\n\nPlease let me know how I can improve this code!\n\nhttps://github.com/kyclark/python-recordparser/issues\n\n## Author\n\nKen Youens-Clark <kyclark@gmail.com>",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/kyclark/python-recordparser",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "recordparser",
    "package_url": "https://pypi.org/project/recordparser/",
    "platform": "",
    "project_url": "https://pypi.org/project/recordparser/",
    "project_urls": {
      "Homepage": "https://github.com/kyclark/python-recordparser"
    },
    "release_url": "https://pypi.org/project/recordparser/0.1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Parse delimited text files using typed class",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7875744,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0c9ff81f259bd354af1b481a29035e92059fe6cb564da8ec10c239eb0a03ab89",
          "md5": "7f76a7687adf92ca01a50fde61e92760",
          "sha256": "4c5c9673ad4af202d566e1861ef546a6fa29ddfe1171bb6f33678d48a58f658c"
        },
        "downloads": -1,
        "filename": "recordparser-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7f76a7687adf92ca01a50fde61e92760",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6220,
        "upload_time": "2020-08-03T22:24:47",
        "upload_time_iso_8601": "2020-08-03T22:24:47.875437Z",
        "url": "https://files.pythonhosted.org/packages/0c/9f/f81f259bd354af1b481a29035e92059fe6cb564da8ec10c239eb0a03ab89/recordparser-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0c9ff81f259bd354af1b481a29035e92059fe6cb564da8ec10c239eb0a03ab89",
        "md5": "7f76a7687adf92ca01a50fde61e92760",
        "sha256": "4c5c9673ad4af202d566e1861ef546a6fa29ddfe1171bb6f33678d48a58f658c"
      },
      "downloads": -1,
      "filename": "recordparser-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "7f76a7687adf92ca01a50fde61e92760",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 6220,
      "upload_time": "2020-08-03T22:24:47",
      "upload_time_iso_8601": "2020-08-03T22:24:47.875437Z",
      "url": "https://files.pythonhosted.org/packages/0c/9f/f81f259bd354af1b481a29035e92059fe6cb564da8ec10c239eb0a03ab89/recordparser-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}