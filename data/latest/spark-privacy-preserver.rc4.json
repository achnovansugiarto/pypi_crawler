{
  "info": {
    "author": "thamindu",
    "author_email": "thamindu.randil@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# spark-privacy-preserver\n\nThis module provides a simple tool for anonymizing a dataset using PySpark. Given a `spark.sql.dataframe` with relevant metadata mondrian_privacy_preserver generates an anonymized `spark.sql.dataframe`. This provides following privacy preserving techniques for the anonymization.\n\n- K Anonymity\n- L Diversity\n- T Closeness\n\nThis also include Differential Privacy.\n\nNote: Only works with PySpark\n\n## Demo\n\nJupyter notebook for each of the following modules are included.\n\n- Mondrian Based Anonymity (Single User Anonymization included)\n- Clustering Based Anonymity\n- Differential Privacy\n\n## Requirements\n\n* Python\n\nPython versions above Python 3.6 and below Python 3.8 are recommended. The module is developed and tested on: \nPython 3.7.7 and pip 20.0.2. (It is better to avoid Python 3.8 as it has some compatibility issues with Spark)\n\n* PySpark\n\nSpark 2.4.5 is recommended. \n\n* Java\n\nJava 8 is recommended. Newer versions of java are incompatible with Spark.\n\nThe module is developed and tested on:\n\n    java version \"1.8.0_231\"\n    Java(TM) SE Runtime Environment (build 1.8.0_231-b11)\n    Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)\n\n*Requirements for submodules are given in the relevant section. \n\n## Installation\n\n### Using pip\n\nUse `pip install spark_privacy_preserver` to install library\n\n### using source code\n\nClone the repository to your PC and run `pip install .` to build and install the package.\n\n## Usage\n\nUsage of each module is described in the relevant section.\n\n### For Mondrian Anonymization and Clustering Anonymization\n\nYou'll need to construct a schema to get the anonymized `spark.sql.dataframe` dataframe.\nYou need to consider the column names and thier data types to construct this. Output of functions of the Mondrian and Clustering Anonymization is described in thier relevant sections. \n\nFollowing code snippet shows how to construct an example schema.\n\n```python\nfrom spark.sql.type import *\n\n#age, occupation - feature columns\n#income - sensitive column\n\nschema = StructType([\n    StructField(\"age\", DoubleType()),\n    StructField(\"occupation\", StringType()),\n    StructField(\"income\", StringType()),\n])\n```\n_______________________________________________________________________________________________________\n\n## Basic Mondrian Anoymizing\n\n\n### Requirements - Basic Mondrian Anonymize\n\n- PySpark 2.4.5. You can easily install it with `pip install pyspark`.\n- PyArrow. You can easily install it with `pip install pyarrow`.\n- Pandas. You can easily install it with `pip install pandas`.\n\n### K Anonymity\n\nThe `spark.sql.dataframe` you get after anonymizing will always contain a extra column `count` which indicates the number of similar rows.\nReturn type of all the non categorical columns will be string\nYou need to always consider the count column when constructing the schema. Count column is an integer type column.\n\n```python\nfrom spark_privacy_preserver.mondrian_preserver import Preserver #requires pandas\n\n#df - spark.sql.dataframe - original dataframe\n#k - int - value of the k\n#feature_columns - list - what you want in the output dataframe\n#sensitive_column - string - what you need as senstive attribute\n#categorical - set -all categorical columns of the original dataframe as a set\n#schema - spark.sql.types StructType - schema of the output dataframe you are expecting\n\ndf = spark.read.csv(your_csv_file).toDF('age',\n    'occupation',\n    'race',\n    'sex',\n    'hours-per-week',\n    'income')\n\ncategorical = set((\n    'occupation',\n    'sex',\n    'race'\n))\n\nfeature_columns = ['age', 'occupation']\n\nsensitive_column = 'income'\n\nyour_anonymized_dataframe = Preserver.k_anonymize(df,\n                                                k,\n                                                feature_columns,\n                                                sensitive_column,\n                                                categorical,\n                                                schema)\n```\n\n### K Anonymity (without row suppresion)\n\nThis function provides a simple way to anonymize a dataset which has an user identification attribute without grouping the rows.    \nThis function doesn't return a dataframe with the count variable as above function. Instead it returns the same dataframe, k-anonymized. Return type of all the non categorical columns will be string.   \nUser attribute column **must not** be given as a feature column and its return type will be same as the input type.   \nFunction takes exact same parameters as the above function. To use this method to anonymize the dataset, instead of calling `k_anonymize`, call `k_anonymize_w_user`.    \n\n### L Diversity\n\nSame as the K Anonymity, the `spark.sql.dataframe` you get after anonymizing will always contain a extra column `count` which indicates the number of similar rows.\nReturn type of all the non categorical columns will be string\nYou need to always consider the count column when constructing the schema. Count column is an integer type column.\n\n```python\nfrom spark_privacy_preserver.mondrian_preserver import Preserver #requires pandas\n\n#df - spark.sql.dataframe - original dataframe\n#k - int - value of the k\n#l - int - value of the l\n#feature_columns - list - what you want in the output dataframe\n#sensitive_column - string - what you need as senstive attribute\n#categorical - set -all categorical columns of the original dataframe as a set\n#schema - spark.sql.types StructType - schema of the output dataframe you are expecting\n\ndf = spark.read.csv(your_csv_file).toDF('age',\n    'occupation',\n    'race',\n    'sex',\n    'hours-per-week',\n    'income')\n\ncategorical = set((\n    'occupation',\n    'sex',\n    'race'\n))\n\nfeature_columns = ['age', 'occupation']\n\nsensitive_column = 'income'\n\nyour_anonymized_dataframe = Preserver.l_diversity(df,\n                                                k,\n                                                l,\n                                                feature_columns,\n                                                sensitive_column,\n                                                categorical,\n                                                schema)\n```\n\n### L Diversity (without row suppresion)\n\nThis function provides a simple way to anonymize a dataset which has an user identification attribute without grouping the rows.   \nThis function doesn't return a dataframe with the count variable as above function. Instead it returns the same dataframe, l-diversity anonymized. Return type of all the non categorical columns will be string.    \nUser attribute column **must not** be given as a feature column and its return type will be same as the input type.   \nFunction takes exact same parameters as the above function. To use this method to anonymize the dataset, instead of calling `l_diversity`, call `l_diversity_w_user`.  \n\n### T - Closeness\n\nSame as the K Anonymity, the `spark.sql.dataframe` you get after anonymizing will always contain a extra column `count` which indicates the number of similar rows.\nReturn type of all the non categorical columns will be string\nYou need to always consider the count column when constructing the schema. Count column is an integer type column.\n\n```python\nfrom spark_privacy_preserver.mondrian_preserver import Preserver #requires pandas\n\n#df - spark.sql.dataframe - original dataframe\n#k - int - value of the k\n#l - int - value of the l\n#feature_columns - list - what you want in the output dataframe\n#sensitive_column - string - what you need as senstive attribute\n#categorical - set -all categorical columns of the original dataframe as a set\n#schema - spark.sql.types StructType - schema of the output dataframe you are expecting\n\ndf = spark.read.csv(your_csv_file).toDF('age',\n    'occupation',\n    'race',\n    'sex',\n    'hours-per-week',\n    'income')\n\ncategorical = set((\n    'occupation',\n    'sex',\n    'race'\n))\n\nfeature_columns = ['age', 'occupation']\n\nsensitive_column = 'income'\n\nyour_anonymized_dataframe = Preserver.t_closeness(df,\n                                                k,\n                                                t,\n                                                feature_columns,\n                                                sensitive_column,\n                                                categorical,\n                                                schema)\n\n```\n\n### T Closeness (without row suppresion)\n\nThis function provides a simple way to anonymize a dataset which has an user identification attribute without grouping the rows.  \nThis function doesn't return a dataframe with the count variable as above function. Instead it returns the same dataframe, t-closeness anonymized. Return type of all the non categorical columns will be string.   \nUser attribute column **must not** be given as a feature column and its return type will be same as the input type.   \nFunction takes exact same parameters as the above function. To use this method to anonymize the dataset, instead of calling `t_closeness`, call `t_closeness_w_user`.  \n\n### Single User K Anonymity\n\nThis function provides a simple way to anonymize a given user in a dataset. Even though this doesn't use the mondrian algorithm, function is included in the `mondrian_preserver`. User identification attribute and the column name of the user identification atribute is needed as parameters.   \nThis doesn't return a dataframe with count variable. Instead this returns the same dataframe, anonymized for the given user. Return type of user column and all the non categorical columns will be string.\n\n```python\nfrom spark_privacy_preserver.mondrian_preserver import Preserver #requires pandas\n\n#df - spark.sql.dataframe - original dataframe\n#k - int - value of the k\n#user - name, id, number of the user. Unique user identification attribute.\n#usercolumn_name - name of the column containing unique user identification attribute.\n#sensitive_column - string - what you need as senstive attribute\n#categorical - set -all categorical columns of the original dataframe as a set\n#schema - spark.sql.types StructType - schema of the output dataframe you are expecting\n#random - a flag by default set to false. In a case where algorithm can't find similar rows for given user, if this is set to true, slgorithm will randomly select rows from dataframe.\n\ndf = spark.read.csv(your_csv_file).toDF('name',\n    'age',\n    'occupation',\n    'race',\n    'sex',\n    'hours-per-week',\n    'income')\n\ncategorical = set((\n    'occupation',\n    'sex',\n    'race'\n))\n\nsensitive_column = 'income'\n\nuser = 'Jon'\n\nusercolumn_name = 'name'\n\nrandom = True\n\nyour_anonymized_dataframe = Preserver.anonymize_user(df,\n                                                k,\n                                                user,\n                                                usercolumn_name,\n                                                sensitive_column,\n                                                categorical,\n                                                schema,\n                                                random)\n\n```\n_______________________________________________________________________________________________________\n\n## Introduction to Differential Privacy\n\nDifferential privacy is one of the data preservation paradigms similar to K-Anonymity, T-Closeness and L-Diversity.\nIt alters each value of actual data in a dataset according to specific constraints set by the owner and produces a \ndifferentially-private dataset. This anonymized dataset is then released for public utilization.\n\n**Îµ-differential privacy** is one of the methods in differential privacy. Laplace based Îµ-differential privacy is \napplied in this library. The method states that the randomization should be according the **epsilon** (Îµ) \n(should be >0) value set by data owner. After randomization a typical noise is added to the dataset. It is calibrated \naccording to the **sensitivity** value (Î») set by the data owner. \n\nOther than above parameters, a third parameter **delta** (Î´) is added into the mix to increase accuracy of the \nalgorithm. A **scale** is computed from the above three parameters and a new value is computed.\n\n> scale = Î» / (Îµ - _log_(1 - Î´))\n\n> random_number = _random_generator_(0, 1) - 0.5\n\n> sign = _get_sign_(random_number)\n\n> new_value = value - scale Ã— sign Ã— _log_(1 - 2 Ã— _mod_(random_number))\n\nIn essence above steps mean that a laplace transform is applied to the value and a new value is randomly selected \naccording to the parameters. When the scale becomes larger, the deviation from original value will increase.\n\n## Achieving Differential Privacy\n\n### Requirements - DIfferential Preserver\n\nMake sure the following Python packages are installed:\n1. PySpark: ```pip install pyspark==2.4.5```\n2. PyArrow: ```pip install pyarrow==0.17.1```\n3. IBM Differential Privacy Library: ```pip install diffprivlib==0.2.1```\n4. MyPy:  ```pip install mypy==0.770```\n5. Tabulate: ```tabulate==0.8.7```\n\n### Procedure\n\nStep by step procedure on how to use the module with explanation is given in the following \nnotebook: **differential_privacy_demo.ipynb**\n\n1. Create a Spark Session. Make sure to enable PyArrow configuration.\n\n```python\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master('local') \\\n    .appName('differential_privacy') \\\n    .config('spark.some.config.option', 'some-value') \\\n    .getOrCreate()\n\nspark.conf.set('spark.sql.execution.arrow.enabled', 'true')\n```\n\n2. Create a Spark DataFrame (sdf). \n\nGenerate an sdf with random values. It is better to manually specify the **schema** of sdf so as to avoid \nany *TypeErrors*.\n\nHere I will generate an sdf with 3 columns: *'Numeric'*, *'Rounded_Numeric'*, *'Boolean'* and 10,000 rows \nto show 3 ways of using DPLib.\n\n```python\nfrom random import random, randint, choice\nfrom pyspark.sql.types import *\n\n# generate a row with random numbers of range(0, 100000) and random strings of either 'yes' or 'no'\ndef generate_rand_tuple():\n    number_1 = randint(0, 100000) + random()\n    number_2 = randint(0, 100000) + random()\n    string = choice(['yes', 'no'])\n    return number_1, number_2, string\n\ndata = [generate_rand_tuple() for _ in range(100000)]\n\nschema = StructType([\n    StructField('Number', FloatType()),\n    StructField('Rounded_Number', DoubleType()),\n    StructField('Boolean', StringType())\n])\n\nsdf = spark.createDataFrame(data=data, schema=schema)\nsdf.show(n=5)\n```\n\n3. Setup and configure **DPLib**\n\nDPLib can work with numbers and binary strings. To anonymize a number based column, you have to setup the column \ncategory as *'numeric'*. To anonymize a string based column, you have to setup the column category as *'boolean'*.\n\n3.1 Initializing the module\n\nThe module takes in 3 optional parameters when initializing: *Spark DataFrame*, *epsilon* and *delta*. Module can also \nbe initialized without any parameters and they can be added later.\n\n```python\nfrom spark_privacy_preserver.differential_privacy import DPLib\n\nepsilon = 0.00001\ndelta = 0.5\nsensitivity = 10\n\n# method 1\ndp = DPLib(global_epsilon=epsilon, global_delta=delta, sdf=sdf)\ndp.set_global_sensitivity(sensitivity=sensitivity)\n\n# method 2\ndp = DPLib()\ndp.set_sdf(sdf=sdf)\ndp.set_global_epsilon_delta(epsilon=epsilon, delta=delta)\ndp.set_global_sensitivity(sensitivity=sensitivity)\n\n```\n\n**Note:** The reason behind the word *global* in above functions\n\nSuppose the user want to anonymize 3 columns of a DataFrame with same epsilon, delta and sensitivity and another \ncolumn with different parameters. Now all the user has to do is to set up global parameters for 3 columns and \nlocal parameters for 4th column. \n\nThis will simplify when multiple columns of a DataFrame have to be processed with same parameters.\n\n3.2 Configuring columns\n\nUser can configure columns with column specific parameters. Column specific parameters will be given higher priority \nover global parameters if explicitly specified.\n\nparameters that can be applied to method *set_column()*:\n1. column_name: name of column as string -> compulsory\n2. category: category of column. can be either *'numeric'* or *'boolean'* -> compulsory\n3. epsilon: column specific value -> optional\n4. delta: column specific value -> optional\n5. sensitivity: column specific value -> optional\n6. lower_bound: set minimum number a column can have. can only be applied to category *'numeric'* -> optional\n7. upper_bound: set maximum number a column can have. can only be applied to category *'numeric'* -> optional\n8. label1: string label for a column. can only be applied to category *'binary'* -> optional\n9. label2: string label for a column. can only be applied to category *'binary'* -> optional\n10. round: value by which to round the result. can only be applied to category *'numeric'* -> optional\n\n```python\ndp.set_column(column_name='Number', \n              category='numeric')\n# epsilon, delta, sensitivity will be taken from global parameters and applied.\n\ndp.set_column(column_name='Rounded_Number', \n              category='numeric',\n              epsilon=epsilon * 10,\n              sensitivity=sensitivity * 10,\n              lower_bound=round(sdf.agg({'Rounded_Number': 'min'}).collect()[0][0]) + 10000,\n              upper_bound=round(sdf.agg({'Rounded_Number': 'max'}).collect()[0][0]) - 10000,\n              round=2)\n# epsilon, sensitivity will be taken from user input instead of global parameters\n# delta will be taken from global parameters.\n\ndp.set_column(column_name='Boolean',\n              category='boolean',\n              label1='yes',\n              label2='no',\n              delta=delta if 0 < delta <= 1 else 0.5)\n# sensitivity will be taken from user input instead of global parameters\n# epsilon will be taken from global parameters.\n# 'boolean' category does not require delta\n```\n\n3.2.1 To view existing configuration for the class, use following method\n\n```python\ndp.get_config()\n```\n\n3.2.2 To drop a column or to drop all columns use the *drop_column()* method. \nTo drop all columns use '*' as input parameter\n\n```python\ndp.drop_column('Rounded_Number', 'Number')\n\ndp.drop_column('*')\n```\n\n3.3 Executing\n\n```python\n# gets first 20 rows of DataFrame before anonymizing and after anonymizing\nsdf.show()\n\ndp.execute()\n\ndp.sdf.show()\n```\n\nAs you can see, there is a clear difference between original DataFrame and anonymized DataFrame.\n\n1. Column *'Number'* is anonymized but the values are not bound to a certain range. The algorithm produces the result \nwith maximum precision as it can achieve.\n\n2. Column *'Rounded_Number'* is both anonymized and bounded to the values set by user. As you can see, the values \nnever rise above upper bound and never become lower than lower bound. Also they are rounded to 2nd decimal place as set.\n\n3. Column *'Boolean'* undergoes through a mechanism that randomly decides to flip to the other binary value or not, \nin order to satisfy differential privacy.\n\n_______________________________________________________________________________________________________\n\n## Clustering Anonymizer\n\n### Requirements - Clustering Anonymize\n\n* PySpark 2.4.5. You can easily install it with `pip install pyspark`\n* PyArrow `pip install pyarrow`\n* Pandas `pip intall pandas`\n* kmodes `pip install kmodes`\n\n### Clustering Based K Anonymity\n\nOnly recommend if there are more catogorical columns, than numerical column. if there are more numerical column, then modrian algorithm is recommended. \n\nIt is recommended to use 5 <= k <= 20 to minimize the data loss, if your data set is small better to use a small k value \n\nhe spark.sql.dataframe you get after anonymizing will always contain a extra column count which indicates the number of similar rows. Return type of all the non categorical columns will be string\n\n\nIn Clustering base Anonymizer you can choose how the how to initialize the cluster centroids. \n\n1. 'fcbg' = This method return cluster centroids weight on the probability of row's column values appear in dataframe. Default Value.\n2. 'rsc'  = This method will choose centroids weight according to the column that has most number of unique values.\n3. 'random = Return cluster centroids in randomly.\n\njust enter the `center_type= 'fcbg'`to use fcbg, default is **fcbg**\n\nYou can also decide the clustering method.\n1. default is special method \n2. kmodes method \n\nif you want to use default dont enter anything to attribute `mode=`, else if you want to use the kmodes method `mode= 'kmode'`\nif you have a huge data amount default is recommended. \n\nyou can also decide the return mode. If this value equal to `return_mode=''equal` ; K anonymization will done with equal member clusters. Default value is 'Not_Equal'\nNot equal is often run fast, but could be data lossy. equal is vice versa. \n\nBelow is a full example:\n```python\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import PandasUDFType, lit, pandas_udf\nfrom clustering_preserver import Kanonymizer\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom gv import init\nfrom anonymizer import Anonymizer\n\ndf = spark.read.format('csv').option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"reduced_adult.csv\")\n\nschema = StructType([\nStructField(\"age\", StringType()),\nStructField(\"workcalss\", StringType()),\nStructField(\"education_num\", StringType()),\nStructField(\"matrital_status\", StringType()),\nStructField(\"occupation\", StringType()),\nStructField(\"sex\", StringType()),\nStructField(\"race\", StringType()),\nStructField(\"native_country\", StringType()),\nStructField(\"class\", StringType())\n])\n\nQI = ['age', 'workcalss','education_num', 'matrital_status', 'occupation', 'race', 'sex', 'native_country']\nSA = ['class']\nCI = [1,3,4,5,6,7]\n\nk_df = Anonymizer.k_anonymize(\n    df, schema, QI, SA, CI, k=10, mode='', center_type='random', return_mode='Not_equal', iter=1)\nk_df.show()\n```\n\n### Clustering based L-Diversity\n\nThis method is recommended only for k anonymized dataframe. \nInput anonymized dataframe will group into similar k clusters and clusters that have not L number of distinct sensitive attributes \nwill be suspressed.\nRecommended small number of l to minimum the data loss. Default value is l = 2.\n\n```python\n## k_df - K anonymized spark dataframe\n## schema - output spark dataframe schema\n## QI - Quasi Identifiers. Type list\n## SA = Sensitive attributes . Type list\n\n QI = ['column1', 'column2', 'column3']\n CI = [1, 2]\n SA = ['column4']\n schema = StructType([\n     StructField(\"column1\", StringType()),\n     StructField(\"column2\", StringType()),\n     StructField(\"column3\", StringType()),\n     StructField(\"column4\", StringType()),\n ])\n\nl_df = Anonymizer.l_diverse(k_df,schema, QI,SA, l=2)\nl_df.show()\n```\n\n### Clustering based T closeness\n\nThis method is recommended only for k anonymized dataframe. \nInput anonymized dataframe will group into similar k clusters and clusters that not have sensitive attribute distribution according to t value will be suspressed.\nt should be in between 0 and 1.\nLarger value of t to minimum the data loss. Default value is t = 0.2.\n\n```python \n## k_df - K anonymized spark dataframe\n## schema - output spark dataframe schema\n## QI - Quasi Identifiers. Type list\n## SA = Sensitive attributes . Type list\n\n QI = ['column1', 'column2', 'column3']\n CI = [1, 2]\n SA = ['column4']\n schema = StructType([\n     StructField(\"column1\", StringType()),\n     StructField(\"column2\", StringType()),\n     StructField(\"column3\", StringType()),\n     StructField(\"column4\", StringType()),\n ])\n\nt_df = Anonymizer.t_closer(\n    k_df,schema, QI, SA, t=0.3, verbose=1)\nt_df.show()\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ThaminduR/spark-privacy-preserver",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "spark-privacy-preserver",
    "package_url": "https://pypi.org/project/spark-privacy-preserver/",
    "platform": "",
    "project_url": "https://pypi.org/project/spark-privacy-preserver/",
    "project_urls": {
      "Homepage": "https://github.com/ThaminduR/spark-privacy-preserver"
    },
    "release_url": "https://pypi.org/project/spark-privacy-preserver/0.3.1/",
    "requires_dist": [
      "pandas (>=1.1)",
      "pyspark (==2.4.5)",
      "pyarrow (==0.17.1)",
      "diffprivlib (==0.2.1)",
      "tabulate (==0.8.7)",
      "mypy (>=0.770)",
      "kmodes"
    ],
    "requires_python": ">=3.6",
    "summary": "Anonymizing Library for Apache Spark",
    "version": "0.3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8083966,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d963d0dc7c1a1c32866772d02b32c9e9bf3762651f4507543161ee519101443a",
          "md5": "0422d4874eecede5c27d5894daad6b4c",
          "sha256": "c3164d95d24984caf1a346b320aa9ea9fa0b04a1ea93f5b7e364e2b6a8cf509b"
        },
        "downloads": -1,
        "filename": "spark_privacy_preserver-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0422d4874eecede5c27d5894daad6b4c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 11559,
        "upload_time": "2020-06-08T08:19:01",
        "upload_time_iso_8601": "2020-06-08T08:19:01.671400Z",
        "url": "https://files.pythonhosted.org/packages/d9/63/d0dc7c1a1c32866772d02b32c9e9bf3762651f4507543161ee519101443a/spark_privacy_preserver-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1a293adeba7c98cf8a942813416ce8c5bca1852eca8933d2aaf921803d8b8af8",
          "md5": "0984d6792e8c8ed2a73d337509c860e5",
          "sha256": "2f1a5581f6714a956d7e81df51acb9d7fc9b2ed62d02854f076bb9186648528e"
        },
        "downloads": -1,
        "filename": "spark_privacy_preserver-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0984d6792e8c8ed2a73d337509c860e5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 8748,
        "upload_time": "2020-06-08T08:19:04",
        "upload_time_iso_8601": "2020-06-08T08:19:04.463472Z",
        "url": "https://files.pythonhosted.org/packages/1a/29/3adeba7c98cf8a942813416ce8c5bca1852eca8933d2aaf921803d8b8af8/spark_privacy_preserver-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9ffe892e54c97af0714690025c391709c2d9627e24afb5512b53acef9437af35",
          "md5": "c2b926a150ba42b1716be995953f3b18",
          "sha256": "8fa09e8e72133df895630371e53c67f55b974ff1c3ea409753ffc23a6125aaea"
        },
        "downloads": -1,
        "filename": "spark_privacy_preserver-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c2b926a150ba42b1716be995953f3b18",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32597,
        "upload_time": "2020-06-09T08:13:55",
        "upload_time_iso_8601": "2020-06-09T08:13:55.455824Z",
        "url": "https://files.pythonhosted.org/packages/9f/fe/892e54c97af0714690025c391709c2d9627e24afb5512b53acef9437af35/spark_privacy_preserver-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2f93d3b54638059e158ab67ea64d90ac1d100ee875d53500570d7a504c759250",
          "md5": "70b6cb53d5c88e3a8c706b6aaa8390bc",
          "sha256": "564a286acff544cb1aeb89c35937b67eb48a16cf14d35433522cccb6ae3041c0"
        },
        "downloads": -1,
        "filename": "spark_privacy_preserver-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "70b6cb53d5c88e3a8c706b6aaa8390bc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 30804,
        "upload_time": "2020-06-09T08:13:58",
        "upload_time_iso_8601": "2020-06-09T08:13:58.168537Z",
        "url": "https://files.pythonhosted.org/packages/2f/93/d3b54638059e158ab67ea64d90ac1d100ee875d53500570d7a504c759250/spark_privacy_preserver-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e1174b6e37ed3e255f6cc0043da0a1da4927cae002f59d48f7f6955b71d43a4",
          "md5": "aa1ef0e94543815ae4c61690291c7d1e",
          "sha256": "1b6ecd14747a855d666fc15b6d51c1f06ba704d1c5171e303fb39dd3e29be6b1"
        },
        "downloads": -1,
        "filename": "spark_privacy_preserver-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "aa1ef0e94543815ae4c61690291c7d1e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 33420,
        "upload_time": "2020-07-01T02:11:52",
        "upload_time_iso_8601": "2020-07-01T02:11:52.446515Z",
        "url": "https://files.pythonhosted.org/packages/5e/11/74b6e37ed3e255f6cc0043da0a1da4927cae002f59d48f7f6955b71d43a4/spark_privacy_preserver-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8a0c8cdf7ebc2b6e42046d41e477da13112baf3cb82f2b54ecf0123bc21bb5cf",
          "md5": "d886f90767fce5bb79b166fd0e264e0f",
          "sha256": "289501c9dae5c8f2794b21326610634775ec9cbe1eb18710b356f17c571300ad"
        },
        "downloads": -1,
        "filename": "spark_privacy_preserver-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "d886f90767fce5bb79b166fd0e264e0f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 31743,
        "upload_time": "2020-07-01T02:11:54",
        "upload_time_iso_8601": "2020-07-01T02:11:54.286921Z",
        "url": "https://files.pythonhosted.org/packages/8a/0c/8cdf7ebc2b6e42046d41e477da13112baf3cb82f2b54ecf0123bc21bb5cf/spark_privacy_preserver-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "75b7d7ebfdafbfc6235a4dce98ed473aed4064af18f615ab6003d3e0c88812a8",
          "md5": "9e2605487d6588f15f382246f33ea355",
          "sha256": "1ccc1535ebc6f7cb6864420b2a7e651f1e638f1411a097c508e6c8283230d713"
        },
        "downloads": -1,
        "filename": "spark_privacy_preserver-0.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9e2605487d6588f15f382246f33ea355",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 33496,
        "upload_time": "2020-09-01T07:26:26",
        "upload_time_iso_8601": "2020-09-01T07:26:26.903158Z",
        "url": "https://files.pythonhosted.org/packages/75/b7/d7ebfdafbfc6235a4dce98ed473aed4064af18f615ab6003d3e0c88812a8/spark_privacy_preserver-0.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7f813bd5094d366ef2a52bb761053e78877681815b0b58303ace96f376fde6ab",
          "md5": "f08473c666ea511c090cda2d2106bd4f",
          "sha256": "8595f95782fbcdf1e059dc77ec629fc16ab1030a48cfb083c5166537ada67631"
        },
        "downloads": -1,
        "filename": "spark_privacy_preserver-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f08473c666ea511c090cda2d2106bd4f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 32618,
        "upload_time": "2020-09-01T07:26:28",
        "upload_time_iso_8601": "2020-09-01T07:26:28.599645Z",
        "url": "https://files.pythonhosted.org/packages/7f/81/3bd5094d366ef2a52bb761053e78877681815b0b58303ace96f376fde6ab/spark_privacy_preserver-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "75b7d7ebfdafbfc6235a4dce98ed473aed4064af18f615ab6003d3e0c88812a8",
        "md5": "9e2605487d6588f15f382246f33ea355",
        "sha256": "1ccc1535ebc6f7cb6864420b2a7e651f1e638f1411a097c508e6c8283230d713"
      },
      "downloads": -1,
      "filename": "spark_privacy_preserver-0.3.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "9e2605487d6588f15f382246f33ea355",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 33496,
      "upload_time": "2020-09-01T07:26:26",
      "upload_time_iso_8601": "2020-09-01T07:26:26.903158Z",
      "url": "https://files.pythonhosted.org/packages/75/b7/d7ebfdafbfc6235a4dce98ed473aed4064af18f615ab6003d3e0c88812a8/spark_privacy_preserver-0.3.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7f813bd5094d366ef2a52bb761053e78877681815b0b58303ace96f376fde6ab",
        "md5": "f08473c666ea511c090cda2d2106bd4f",
        "sha256": "8595f95782fbcdf1e059dc77ec629fc16ab1030a48cfb083c5166537ada67631"
      },
      "downloads": -1,
      "filename": "spark_privacy_preserver-0.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "f08473c666ea511c090cda2d2106bd4f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 32618,
      "upload_time": "2020-09-01T07:26:28",
      "upload_time_iso_8601": "2020-09-01T07:26:28.599645Z",
      "url": "https://files.pythonhosted.org/packages/7f/81/3bd5094d366ef2a52bb761053e78877681815b0b58303ace96f376fde6ab/spark_privacy_preserver-0.3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}