{
  "info": {
    "author": "Andrius Senulis",
    "author_email": "andrius.senulis@datarobot.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# DataRobot Provider for Apache Airflow\n\nThis package provides operators, sensors, and a hook that integrates [DataRobot](https://www.datarobot.com) into Apache Airflow.\nUsing these components you should be able to build the essential DataRobot pipeline - create a project, train models, deploy a model,\nscore predictions against the model deployment.\n\n\n## Installation\n\nPrerequisites:\n- [Apache Airflow](https://pypi.org/project/apache-airflow/)\n- [DataRobot Python API client](https://pypi.org/project/datarobot/)\n\nInstall the DataRobot provider:\n```\npip install airflow-provider-datarobot\n```\n\n\n## Connection\n\nIn the Airflow user interface, create a new DataRobot connection in `Admin > Connections`:\n\n* Connection Type: `DataRobot`\n* Connection Id: `datarobot_default` (default)\n* API Key: `your-datarobot-api-key`\n* DataRobot endpoint URL: `https://app.datarobot.com/api/v2` (default)\n\nCreate the API Key in the [DataRobot Developer Tools](https://app.datarobot.com/account/developer-tools) page, `API Keys` section (see [DataRobot Docs](https://app.datarobot.com/docs/api/api-quickstart/api-qs.html#create-a-datarobot-api-token) for more details).\n\nBy default, all components use `datarobot_default` connection ID.\n\n\n## Config JSON for dag run\n\nOperators and sensors use parameters from the [config](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.html?highlight=config#Named%20Arguments_repeat21)\nwhich must be submitted when triggering the dag. Example config JSON with required parameters:\n\n    {\n        \"training_data\": \"s3-presigned-url-or-local-path-to-training-data\",\n        \"project_name\": \"Project created from Airflow\",\n        \"autopilot_settings\": {\n            \"target\": \"readmitted\"\n        },\n        \"deployment_label\": \"Deployment created from Airflow\",\n        \"score_settings\": {\n            \"intake_settings\": {\n                \"type\": \"s3\",\n                \"url\": \"s3://path/to/scoring-data/Diabetes10k.csv\",\n                \"credential_id\": \"62160b511fb29da8dd5f2c81\"\n            },\n            \"output_settings\": {\n                \"type\": \"s3\",\n                \"url\": \"s3://path/to/results-dir/Diabetes10k_predictions.csv\",\n                \"credential_id\": \"62160b511fb29da8dd5f2c81\"\n            }\n        }\n    }\n    \n\nThese config values can be accessed in the `execute()` method of any operator the dag \nin the `context[\"params\"]` variable, e.g. getting a training data you would use this in the operator:\n\n    def execute(self, context: Dict[str, Any]) -> str:\n        ...\n        training_data = context[\"params\"][\"training_data\"]\n        ...\n\n\n## Modules\n\n### [Operators](https://github.com/datarobot/airflow-provider-datarobot/blob/main/datarobot_provider/operators/datarobot.py)\n\n- `CreateProjectOperator`\n\n    Creates a DataRobot project and returns its ID.\n \n    Required config params:\n\n        training_data: str - pre-signed S3 URL or local path to training dataset\n        project_name: str - project name\n\n    In case of an S3 input, the `training_data` value must be a [pre-signed AWS S3 URL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html).\n\n    For more [project settings](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html#project) see the DataRobot docs.\n\n    Returns a project ID.\n\n- `TrainModelsOperator`\n\n    Triggers DataRobot Autopilot to train models.\n\n    Parameters:\n\n        project_id: str - DataRobot project ID\n\n    Required config params:\n\n        \"autopilot_settings\": {\n            \"target\": \"readmitted\"\n        } \n         \n    `target` is a required parameter with the column name which defines the modeling target.\n    \n    For more [autopilot settings](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html#datarobot.models.Project.set_target) see the DataRobot docs.\n\n    Returns `None`.\n\n- `DeployModelOperator`\n\n    Deploys a specified model.\n\n    Parameters:\n\n        model_id: str - DataRobot model ID\n\n    Required config params:\n\n        deployment_label - deployment label/name\n\n    For more [deployment settings](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html#datarobot.models.Deployment) see the DataRobot docs.\n\n    Returns a deployment ID.\n\n- `DeployRecommendedModelOperator`\n\n    Deploys a recommended model.\n\n    Parameters:\n\n        project_id: str - DataRobot project ID\n\n    Required config params:\n\n        deployment_label: str - deployment label\n\n    For more [deployment settings](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html#datarobot.models.Deployment) see the DataRobot docs.\n\n    Returns a deployment ID.\n\n- `ScorePredictionsOperator`\n\n    Scores batch predictions against the deployment.\n\n    Prerequisites:\n    - [S3 credentials added to DataRobot via Python API client](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/reference/admin/credentials.html#s3-credentials).\n      You need the `creds.credential_id` for the `credential_id` parameter in the config.\n    - OR a Dataset ID in the AI Catalog\n\n    Parameters:\n  \n        deployment_id: str - DataRobot project ID\n\n    Required config params:\n\n        \"score_settings\": {\n            \"intake_settings\": {\n                \"type\": \"s3\",\n                \"url\": \"s3://my-bucket/Diabetes10k.csv\",\n                \"credential_id\": \"62160b511fb29da8dd5f2c81\"\n            },\n            \"output_settings\": {\n                \"type\": \"s3\",\n                \"url\": \"s3://my-bucket/Diabetes10k_predictions.csv\",\n                \"credential_id\": \"62160b511fb29da8dd5f2c81\"\n            }\n        }\n\n    Config params for scoring a Dataset in the AI Catalog:\n\n        \"score_settings\": {\n            \"intake_settings\": {\n                \"type\": \"dataset\",\n                \"dataset_id\": \"<datasetId>\",\n            },\n            \"output_settings\": {\n                ...\n            }\n        }\n    \n    For more [batch prediction settings](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html#datarobot.models.BatchPredictionJob.score) see the DataRobot docs.\n\n    Returns a batch prediction job ID.\n\n- `GetTargetDriftOperator`\n\n    Gets the target drift from a deployment.\n\n    Parameters:\n\n        deployment_id: str - DataRobot deployment ID\n\n    No config params are required. [Optional params](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html#datarobot.models.Deployment.get_target_drift) may be passed in the config as follows:\n\n        \"target_drift\": {\n            ...\n        }\n\n    Returns a dict with the target drift data.\n\n- `GetFeatureDriftOperator`\n\n    Gets the feature drift from a deployment.\n\n    Parameters:\n\n        deployment_id: str - DataRobot deployment ID\n\n    No config params are required. [Optional params](https://datarobot-public-api-client.readthedocs-hosted.com/en/latest-release/autodoc/api_reference.html#datarobot.models.Deployment.get_feature_drift) may be passed in the config as follows:\n\n        \"feature_drift\": {\n            ...\n        }\n\n    Returns a dict with the feature drift data.\n\n### [Sensors](https://github.com/datarobot/airflow-provider-datarobot/blob/main/datarobot_provider/sensors/datarobot.py)\n\n- `AutopilotCompleteSensor`\n\n    Checks whether the Autopilot has completed.\n\n    Parameters:\n\n        project_id: str - DataRobot project ID\n\n- `ScoringCompleteSensor`\n\n    Checks whether batch scoring has completed.\n\n    Parameters:\n\n        job_id: str - Batch prediction job ID\n\n### [Hooks](https://github.com/datarobot/airflow-provider-datarobot/blob/main/datarobot_provider/hooks/datarobot.py)\n\n- `DataRobotHook`\n\n    A hook for initializing DataRobot Public API client.\n\n\n## Pipeline\n\nThe modules described above allows to construct a standard DataRobot pipeline in an Airflow dag:\n\n    create_project_op >> train_models_op >> autopilot_complete_sensor >> deploy_model_op >> score_predictions_op >> scoring_complete_sensor\n\n\n## Examples\n\nSee the [**examples**](https://github.com/datarobot/airflow-provider-datarobot/blob/main/datarobot_provider/example_dags) directory for the example DAGs.\n\n\n## Issues\n\nPlease submit [issues](https://github.com/datarobot/airflow-provider-datarobot/issues) and [pull requests](https://github.com/datarobot/airflow-provider-datarobot/pulls) in our official repo:\n[https://github.com/datarobot/airflow-provider-datarobot](https://github.com/datarobot/airflow-provider-datarobot)\n\nWe are happy to hear from you. Please email any feedback to the authors at [support@datarobot.com](mailto:support@datarobot.com).\n\n\n# Copyright Notice\n\nCopyright 2022 DataRobot, Inc. and its affiliates.\n\nAll rights reserved.\n\nThis is proprietary source code of DataRobot, Inc. and its affiliates.\n\nReleased under the terms of DataRobot Tool and Utility Agreement.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://www.datarobot.com/",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "airflow-provider-datarobot",
    "package_url": "https://pypi.org/project/airflow-provider-datarobot/",
    "platform": null,
    "project_url": "https://pypi.org/project/airflow-provider-datarobot/",
    "project_urls": {
      "Homepage": "http://www.datarobot.com/"
    },
    "release_url": "https://pypi.org/project/airflow-provider-datarobot/0.0.4/",
    "requires_dist": [
      "apache-airflow (>=2.0)",
      "datarobot (>=2.28.0)"
    ],
    "requires_python": "~=3.7",
    "summary": "DataRobot Airflow provider.",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16334342,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b676f154be6c01e1dda606c8a6693b656b75ba0551094cf0fffb7698879779af",
          "md5": "5fe5ca56df0d47cf33709071f8311b11",
          "sha256": "70d8500da6c732a825c75c6f7ceca705aa66cac8716dcd6bc64ffe6d9643f231"
        },
        "downloads": -1,
        "filename": "airflow_provider_datarobot-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5fe5ca56df0d47cf33709071f8311b11",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.7",
        "size": 12843,
        "upload_time": "2022-03-24T14:37:31",
        "upload_time_iso_8601": "2022-03-24T14:37:31.282166Z",
        "url": "https://files.pythonhosted.org/packages/b6/76/f154be6c01e1dda606c8a6693b656b75ba0551094cf0fffb7698879779af/airflow_provider_datarobot-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0e61acdd01fb75b73ede88805d93216cd5b7269ce3fae8c5b4a49c8aa01dfa6a",
          "md5": "5939b77f4b3ead09ed9b11ef3fa19c0e",
          "sha256": "76421130641c0860b30e587411b1e58083ffe4c4baed9219ca78568de5cd61f5"
        },
        "downloads": -1,
        "filename": "airflow-provider-datarobot-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5939b77f4b3ead09ed9b11ef3fa19c0e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.7",
        "size": 9441,
        "upload_time": "2022-03-24T14:37:33",
        "upload_time_iso_8601": "2022-03-24T14:37:33.314424Z",
        "url": "https://files.pythonhosted.org/packages/0e/61/acdd01fb75b73ede88805d93216cd5b7269ce3fae8c5b4a49c8aa01dfa6a/airflow-provider-datarobot-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "baf608982eadb4169a4d36397df7397c9a0d489ebc7f954e9081c3d8c06f4516",
          "md5": "b2fb387276ca3849d9b46c8d5a122c22",
          "sha256": "ebb426b56645a4cf1caa7835ad2007f2f9a34a83940a3efce5ddeda4922d17c0"
        },
        "downloads": -1,
        "filename": "airflow_provider_datarobot-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b2fb387276ca3849d9b46c8d5a122c22",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.7",
        "size": 12844,
        "upload_time": "2022-03-25T20:23:02",
        "upload_time_iso_8601": "2022-03-25T20:23:02.380443Z",
        "url": "https://files.pythonhosted.org/packages/ba/f6/08982eadb4169a4d36397df7397c9a0d489ebc7f954e9081c3d8c06f4516/airflow_provider_datarobot-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "526b39901797493299d73fc9f423d4737abcab87671d2c4b8df1a35b69d0612b",
          "md5": "f3de799d0052e3293f785f15814348ad",
          "sha256": "ea6e41fee6c88c75ae3129444a33761d96d7ec80ebced5224371f593b2cc83fb"
        },
        "downloads": -1,
        "filename": "airflow-provider-datarobot-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "f3de799d0052e3293f785f15814348ad",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.7",
        "size": 9451,
        "upload_time": "2022-03-25T20:23:03",
        "upload_time_iso_8601": "2022-03-25T20:23:03.899765Z",
        "url": "https://files.pythonhosted.org/packages/52/6b/39901797493299d73fc9f423d4737abcab87671d2c4b8df1a35b69d0612b/airflow-provider-datarobot-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aeecfd25594804edec287607d6efc69814898c96c9375fce9c2213d1f30aa8f5",
          "md5": "0e3bc733a8f73f185bd0b8e67f7295de",
          "sha256": "56273f457ca605accc7c71da52dddac30224f33136c3dcea7fd72e1c25720ef8"
        },
        "downloads": -1,
        "filename": "airflow_provider_datarobot-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0e3bc733a8f73f185bd0b8e67f7295de",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.7",
        "size": 12917,
        "upload_time": "2022-04-28T09:30:09",
        "upload_time_iso_8601": "2022-04-28T09:30:09.330518Z",
        "url": "https://files.pythonhosted.org/packages/ae/ec/fd25594804edec287607d6efc69814898c96c9375fce9c2213d1f30aa8f5/airflow_provider_datarobot-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "13055ffac6d0b5833fbf0e2b980aa4fae5ece71a1646d83fa653eccbf62061bd",
          "md5": "233f0cf6a16f50d11b6fd8a64f07aaa0",
          "sha256": "4374036ca0373c786fd55e995fbe4463e8d065059b7c4ee3a6e555b27cf70857"
        },
        "downloads": -1,
        "filename": "airflow-provider-datarobot-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "233f0cf6a16f50d11b6fd8a64f07aaa0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.7",
        "size": 9539,
        "upload_time": "2022-04-28T09:30:11",
        "upload_time_iso_8601": "2022-04-28T09:30:11.793279Z",
        "url": "https://files.pythonhosted.org/packages/13/05/5ffac6d0b5833fbf0e2b980aa4fae5ece71a1646d83fa653eccbf62061bd/airflow-provider-datarobot-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1ebedda8a0010b1f2842e549fce14ab8cdded38a29074dc54372f9ad8b67e000",
          "md5": "afcb8009162373824596df37448915d5",
          "sha256": "b218bfa49f0296841faf4c3bdb5f636d9756b14912c61c0ef70e6e40f4407ec7"
        },
        "downloads": -1,
        "filename": "airflow_provider_datarobot-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "afcb8009162373824596df37448915d5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.7",
        "size": 13762,
        "upload_time": "2023-01-06T19:52:12",
        "upload_time_iso_8601": "2023-01-06T19:52:12.956193Z",
        "url": "https://files.pythonhosted.org/packages/1e/be/dda8a0010b1f2842e549fce14ab8cdded38a29074dc54372f9ad8b67e000/airflow_provider_datarobot-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a3dd712cd94934e6e2c8a1634b18e18b14065a64ead5cc41b69b958b150f9c9d",
          "md5": "26b61b38f770f987c582b5c2a2ba0060",
          "sha256": "f13f1bb9ec824623d3784b3cd0273268a4dad7dda07158bfbdfb407f5ce848dd"
        },
        "downloads": -1,
        "filename": "airflow-provider-datarobot-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "26b61b38f770f987c582b5c2a2ba0060",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.7",
        "size": 10640,
        "upload_time": "2023-01-06T19:52:14",
        "upload_time_iso_8601": "2023-01-06T19:52:14.982285Z",
        "url": "https://files.pythonhosted.org/packages/a3/dd/712cd94934e6e2c8a1634b18e18b14065a64ead5cc41b69b958b150f9c9d/airflow-provider-datarobot-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1ebedda8a0010b1f2842e549fce14ab8cdded38a29074dc54372f9ad8b67e000",
        "md5": "afcb8009162373824596df37448915d5",
        "sha256": "b218bfa49f0296841faf4c3bdb5f636d9756b14912c61c0ef70e6e40f4407ec7"
      },
      "downloads": -1,
      "filename": "airflow_provider_datarobot-0.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "afcb8009162373824596df37448915d5",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "~=3.7",
      "size": 13762,
      "upload_time": "2023-01-06T19:52:12",
      "upload_time_iso_8601": "2023-01-06T19:52:12.956193Z",
      "url": "https://files.pythonhosted.org/packages/1e/be/dda8a0010b1f2842e549fce14ab8cdded38a29074dc54372f9ad8b67e000/airflow_provider_datarobot-0.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a3dd712cd94934e6e2c8a1634b18e18b14065a64ead5cc41b69b958b150f9c9d",
        "md5": "26b61b38f770f987c582b5c2a2ba0060",
        "sha256": "f13f1bb9ec824623d3784b3cd0273268a4dad7dda07158bfbdfb407f5ce848dd"
      },
      "downloads": -1,
      "filename": "airflow-provider-datarobot-0.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "26b61b38f770f987c582b5c2a2ba0060",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "~=3.7",
      "size": 10640,
      "upload_time": "2023-01-06T19:52:14",
      "upload_time_iso_8601": "2023-01-06T19:52:14.982285Z",
      "url": "https://files.pythonhosted.org/packages/a3/dd/712cd94934e6e2c8a1634b18e18b14065a64ead5cc41b69b958b150f9c9d/airflow-provider-datarobot-0.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}