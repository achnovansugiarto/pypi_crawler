{
  "info": {
    "author": "Qichun Tang",
    "author_email": "qichun.tang@bupt.edu.cn",
    "bugtrack_url": null,
    "classifiers": [
      "Environment :: Console",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Information Technology",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Scientific/Engineering :: Information Analysis"
    ],
    "description": "<p align=\"center\"><img src=\"https://img-blog.csdnimg.cn/20210110141724960.png\"></img></p>\n\n\n\n[![Build Status](https://travis-ci.org/auto-flow/ultraopt.svg?branch=main)](https://travis-ci.org/auto-flow/ultraopt) \n[![PyPI version](https://badge.fury.io/py/ultraopt.svg?maxAge=2592000)](https://badge.fury.io/py/ultraopt)\n[![Download](https://img.shields.io/pypi/dm/ultraopt.svg)](https://pypi.python.org/pypi/ultraopt)\n![](https://img.shields.io/badge/license-BSD-green)\n![PythonVersion](https://img.shields.io/badge/python-3.6+-blue)\n[![GitHub Star](https://img.shields.io/github/stars/auto-flow/ultraopt.svg)](https://github.com/auto-flow/ultraopt/stargazers) [![GitHub forks](https://img.shields.io/github/forks/auto-flow/ultraopt.svg)](https://github.com/auto-flow/ultraopt/network) [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4430148.svg)](https://zenodo.org/record/4430148)\n\n`UltraOpt` : **Distributed Asynchronous Hyperparameter Optimization better than HyperOpt**.\n\n---\n\n`UltraOpt` is a simple and efficient library to minimize expensive and noisy black-box functions, it can be used in many fields, such as HyperParameter Optimization(**HPO**) and \nAutomatic Machine Learning(**AutoML**). \n\nAfter absorbing the advantages of existing optimization libraries such as \n[HyperOpt](https://github.com/hyperopt/hyperopt)[<sup>[5]</sup>](#refer-5), [SMAC3](https://github.com/automl/SMAC3)[<sup>[3]</sup>](#refer-3), \n[scikit-optimize](https://github.com/scikit-optimize/scikit-optimize)[<sup>[4]</sup>](#refer-4) and [HpBandSter](https://github.com/automl/HpBandSter)[<sup>[2]</sup>](#refer-2), we develop \n`UltraOpt` , which implement a new bayesian optimization algorithm : Embedding-Tree-Parzen-Estimator(**ETPE**), which is better than HyperOpt' TPE algorithm in our experiments.\nBesides, The optimizer of  `UltraOpt` is redesigned to adapt **HyperBand & SuccessiveHalving Evaluation Strategies**[<sup>[6]</sup>](#refer-6)[<sup>[7]</sup>](#refer-7) and **MapReduce & Async Communication Conditions**.\nFinally, you can visualize `Config Space` and `optimization process & results` by `UltraOpt`'s tool function. Enjoy it !\n\nOther Language: [中文README](README.zh_CN.md)\n\n- **Documentation**\n\n    + English Documentation is not available now.\n\n    + [中文文档](https://auto-flow.github.io/ultraopt/zh/)\n\n- **Tutorials**\n\n    + English Tutorials is not available now.\n\n    + [中文教程](https://github.com/auto-flow/ultraopt/tree/main/tutorials_zh)\n\n**Table of Contents**\n\n- [Installation](#Installation)\n- [Quick Start](#Quick-Start)\n    + [Using UltraOpt in HPO](#Using-UltraOpt-in-HPO)\n    + [Using UltraOpt in AutoML](#Using-UltraOpt-in-AutoML)\n- [Our Advantages](#Our-Advantages)\n    + [Advantage One: ETPE optimizer is more competitive](#Advantage-One-ETPE-optimizer-is-more-competitive)\n    + [Advantage Two: UltraOpt is more adaptable to distributed computing](#Advantage-Two-UltraOpt-is-more-adaptable-to-distributed-computing)\n    + [Advantage Three: UltraOpt is more function comlete and user friendly](#advantage-three-ultraopt-is-more-function-comlete-and-user-friendly)\n- [Citation](#Citation)\n- [Referance](#referance)\n\n# Installation\n\nUltraOpt requires Python 3.6 or higher.\n\nYou can install the latest release by `pip`:\n\n```bash\npip install ultraopt\n```\n\nYou can download the repository and manual installation:\n\n```bash\ngit clone https://github.com/auto-flow/ultraopt.git && cd ultraopt\npython setup.py install\n```\n\n# Quick Start\n\n## Using UltraOpt in HPO\n\nLet's learn what `UltraOpt`  doing with several examples (you can try it on your `Jupyter Notebook`). \n\nYou can learn Basic-Tutorial in [here](https://auto-flow.github.io/ultraopt/zh/_tutorials/01._Basic_Tutorial.html), and `HDL`'s Definition in [here](https://auto-flow.github.io/ultraopt/zh/_tutorials/02._Multiple_Parameters.html).\n\nBefore starting a black box optimization task, you need to provide two things:\n\n- parameter domain, or the **Config Space**\n- objective function, accept `config` (`config` is sampled from **Config Space**), return `loss`\n\nLet's define a Random Forest's HPO  **Config Space** by `UltraOpt`'s `HDL` (Hyperparameter Description Language):\n\n```python\nHDL = {\n    \"n_estimators\": {\"_type\": \"int_quniform\",\"_value\": [10, 200, 10], \"_default\": 100},\n    \"criterion\": {\"_type\": \"choice\",\"_value\": [\"gini\", \"entropy\"],\"_default\": \"gini\"},\n    \"max_features\": {\"_type\": \"choice\",\"_value\": [\"sqrt\",\"log2\"],\"_default\": \"sqrt\"},\n    \"min_samples_split\": {\"_type\": \"int_uniform\", \"_value\": [2, 20],\"_default\": 2},\n    \"min_samples_leaf\": {\"_type\": \"int_uniform\", \"_value\": [1, 20],\"_default\": 1},\n    \"bootstrap\": {\"_type\": \"choice\",\"_value\": [True, False],\"_default\": True},\n    \"random_state\": 42\n}\n```\n\nAnd then define an objective function:\n\n```python\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom ultraopt.hdl import layering_config\nX, y = load_digits(return_X_y=True)\ncv = StratifiedKFold(5, True, 0)\ndef evaluate(config: dict) -> float:\n    model = RandomForestClassifier(**layering_config(config))\n    return 1 - float(cross_val_score(model, X, y, cv=cv).mean())\n```\n\nNow, we can start an optimization process:\n\n```python\nfrom ultraopt import fmin\nresult = fmin(eval_func=evaluate, config_space=HDL, optimizer=\"ETPE\", n_iterations=30)\nresult\n```\n\n```\n100%|██████████| 30/30 [00:36<00:00,  1.23s/trial, best loss: 0.023]\n\n+-----------------------------------+\n| HyperParameters   | Optimal Value |\n+-------------------+---------------+\n| bootstrap         | True:bool     |\n| criterion         | gini          |\n| max_features      | log2          |\n| min_samples_leaf  | 1             |\n| min_samples_split | 2             |\n| n_estimators      | 200           |\n+-------------------+---------------+\n| Optimal Loss      | 0.0228        |\n+-------------------+---------------+\n| Num Configs       | 30            |\n+-------------------+---------------+\n```\n\nFinally, make a simple visualizaiton:\n\n```python\nresult.plot_convergence()\n```\n\n![quickstart1](https://img-blog.csdnimg.cn/20210110141723520.png)\n\nYou can visualize high dimensional interaction by facebook's hiplot:\n\n```python\n!pip install hiplot\nresult.plot_hi(target_name=\"accuracy\", loss2target_func=lambda x:1-x)\n```\n\n![hiplot](https://img-blog.csdnimg.cn/20210110130444272.png)\n\n## Using UltraOpt in AutoML\n\nLet's try a more complex example: solve AutoML's **CASH Problem** [<sup>[1]</sup>](#refer-1) (Combination problem of Algorithm Selection and Hyperparameter optimization) \nby BOHB algorithm[<sup>[2]</sup>](#refer-2) (Combine **HyperBand**[<sup>[6]</sup>](#refer-6) Evaluation Strategies with `UltraOpt`'s **ETPE** optimizer) .\n\nYou can learn Conditional Parameter and complex `HDL`'s Definition in [here](https://auto-flow.github.io/ultraopt/zh/_tutorials/03._Conditional_Parameter.html),  AutoML implementation tutorial in [here](https://auto-flow.github.io/ultraopt/zh/_tutorials/05._Implement_a_Simple_AutoML_System.html) and Multi-Fidelity Optimization in [here](https://auto-flow.github.io/ultraopt/zh/_tutorials/06._Combine_Multi-Fidelity_Optimization.html).\n\nFirst of all, let's define a **CASH** `HDL` :\n\n```python\nHDL = {\n    'classifier(choice)':{\n        \"RandomForestClassifier\": {\n          \"n_estimators\": {\"_type\": \"int_quniform\",\"_value\": [10, 200, 10], \"_default\": 100},\n          \"criterion\": {\"_type\": \"choice\",\"_value\": [\"gini\", \"entropy\"],\"_default\": \"gini\"},\n          \"max_features\": {\"_type\": \"choice\",\"_value\": [\"sqrt\",\"log2\"],\"_default\": \"sqrt\"},\n          \"min_samples_split\": {\"_type\": \"int_uniform\", \"_value\": [2, 20],\"_default\": 2},\n          \"min_samples_leaf\": {\"_type\": \"int_uniform\", \"_value\": [1, 20],\"_default\": 1},\n          \"bootstrap\": {\"_type\": \"choice\",\"_value\": [True, False],\"_default\": True},\n          \"random_state\": 42\n        },\n        \"KNeighborsClassifier\": {\n          \"n_neighbors\": {\"_type\": \"int_loguniform\", \"_value\": [1,100],\"_default\": 3},\n          \"weights\" : {\"_type\": \"choice\", \"_value\": [\"uniform\", \"distance\"],\"_default\": \"uniform\"},\n          \"p\": {\"_type\": \"choice\", \"_value\": [1, 2],\"_default\": 2},\n        },\n    }\n}\n```\n\nAnd then, define a objective function with an additional parameter `budget` to adapt to **HyperBand**[<sup>[6]</sup>](#refer-6) evaluation strategy:\n\n\n\n ```python\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\ndef evaluate(config: dict, budget: float) -> float:\n    layered_dict = layering_config(config)\n    AS_HP = layered_dict['classifier'].copy()\n    AS, HP = AS_HP.popitem()\n    ML_model = eval(AS)(**HP)\n    scores = []\n    for i, (train_ix, valid_ix) in enumerate(cv.split(X, y)):\n        rng = np.random.RandomState(i)\n        size = int(train_ix.size * budget)\n        train_ix = rng.choice(train_ix, size, replace=False)\n        X_train,y_train = X[train_ix, :],y[train_ix]\n        X_valid,y_valid = X[valid_ix, :],y[valid_ix]\n        ML_model.fit(X_train, y_train)\n        scores.append(ML_model.score(X_valid, y_valid))\n    score = np.mean(scores)\n    return 1 - score\n```\n\nYou should instance a `multi_fidelity_iter_generator` object for the purpose of using **HyperBand**[<sup>[6]</sup>](#refer-6)  Evaluation Strategy :\n\n```python\nfrom ultraopt.multi_fidelity import HyperBandIterGenerator\nhb = HyperBandIterGenerator(min_budget=1/4, max_budget=1, eta=2)\nhb.get_table()\n```\n\n\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"3\" halign=\"left\">iter 0</th>\n      <th colspan=\"2\" halign=\"left\">iter 1</th>\n      <th>iter 2</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>stage 0</th>\n      <th>stage 1</th>\n      <th>stage 2</th>\n      <th>stage 0</th>\n      <th>stage 1</th>\n      <th>stage 0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>num_config</th>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>budget</th>\n      <td>1/4</td>\n      <td>1/2</td>\n      <td>1</td>\n      <td>1/2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n\nlet's combine **HyperBand** Evaluation Strategies with `UltraOpt`'s **ETPE** optimizer , and then start an optimization process:\n\n\n```python\nresult = fmin(eval_func=evaluate, config_space=HDL, \n              optimizer=\"ETPE\", # using bayesian optimizer: ETPE\n              multi_fidelity_iter_generator=hb, # using HyperBand\n              n_jobs=3,         # 3 threads\n              n_iterations=20)\nresult\n```\n\n```\n100%|██████████| 88/88 [00:11<00:00,  7.48trial/s, max budget: 1.0, best loss: 0.012]\n\n+--------------------------------------------------------------------------------------------------------------------------+\n| HyperParameters                                     | Optimal Value                                                      |\n+-----------------------------------------------------+----------------------+----------------------+----------------------+\n| classifier:__choice__                               | KNeighborsClassifier | KNeighborsClassifier | KNeighborsClassifier |\n| classifier:KNeighborsClassifier:n_neighbors         | 4                    | 1                    | 3                    |\n| classifier:KNeighborsClassifier:p                   | 2:int                | 2:int                | 2:int                |\n| classifier:KNeighborsClassifier:weights             | distance             | uniform              | uniform              |\n| classifier:RandomForestClassifier:bootstrap         | -                    | -                    | -                    |\n| classifier:RandomForestClassifier:criterion         | -                    | -                    | -                    |\n| classifier:RandomForestClassifier:max_features      | -                    | -                    | -                    |\n| classifier:RandomForestClassifier:min_samples_leaf  | -                    | -                    | -                    |\n| classifier:RandomForestClassifier:min_samples_split | -                    | -                    | -                    |\n| classifier:RandomForestClassifier:n_estimators      | -                    | -                    | -                    |\n| classifier:RandomForestClassifier:random_state      | -                    | -                    | -                    |\n+-----------------------------------------------------+----------------------+----------------------+----------------------+\n| Budgets                                             | 1/4                  | 1/2                  | 1 (max)              |\n+-----------------------------------------------------+----------------------+----------------------+----------------------+\n| Optimal Loss                                        | 0.0328               | 0.0178               | 0.0122               |\n+-----------------------------------------------------+----------------------+----------------------+----------------------+\n| Num Configs                                         | 28                   | 28                   | 32                   |\n+-----------------------------------------------------+----------------------+----------------------+----------------------+\n```\n\nYou can visualize optimization process in `multi-fidelity` scenarios:\n\n```python\nimport pylab as plt\nplt.rcParams['figure.figsize'] = (16, 12)\nplt.subplot(2, 2, 1)\nresult.plot_convergence_over_time();\nplt.subplot(2, 2, 2)\nresult.plot_concurrent_over_time(num_points=200);\nplt.subplot(2, 2, 3)\nresult.plot_finished_over_time();\nplt.subplot(2, 2, 4)\nresult.plot_correlation_across_budgets();\n```\n\n\n![quickstart2](https://img-blog.csdnimg.cn/20210110141724946.png)\n\n# Our Advantages\n\n## Advantage One: ETPE optimizer is more competitive\n\nWe implement 4 kinds of optimizers(listed in the table below), and `ETPE` optimizer is our original creation, which is proved to be better than other `TPE based optimizers` such as `HyperOpt's TPE` and `HpBandSter's BOHB` in our experiments.\n\nOur experimental code is public available in [here](https://github.com/auto-flow/ultraopt/tree/main/experiments), experimental documentation can be found in [here](https://auto-flow.github.io/ultraopt/zh/experiments.html) .\n\n|Optimizer|Description|\n|-----|---|\n|ETPE| Embedding-Tree-Parzen-Estimator, is our original creation,  converting high-cardinality categorical variables to low-dimension continuous variables based on TPE algorithm, and some other aspects have also been improved, is proved to be better than  `HyperOpt's TPE` in our experiments. |\n|Forest |Bayesian Optimization based on Random Forest. Surrogate model import `scikit-optimize` 's `skopt.learning.forest` model, and integrate Local Search methods in `SMAC3`| .\n|GBRT| Bayesian Optimization based on Gradient Boosting Resgression Tree. Surrogate model import `scikit-optimize` 's `skopt.learning.gbrt` model. |\n|Random| Random Search for baseline or dummy model. |\n\n\nKey result figure in experiment (you can see details in [experimental documentation](https://auto-flow.github.io/ultraopt/zh/experiments.html) ) :\n\n![experiment](https://img-blog.csdnimg.cn/20210110141724952.png)\n\n## Advantage Two: UltraOpt is more adaptable to distributed computing\n\nYou can see this section in the documentation:\n\n- [Asynchronous Communication Parallel Strategy](https://auto-flow.github.io/ultraopt/zh/_tutorials/08._Asynchronous_Communication_Parallel_Strategy.html)\n\n- [MapReduce Parallel Strategy](https://auto-flow.github.io/ultraopt/zh/_tutorials/09._MapReduce_Parallel_Strategy.html)\n\n## Advantage Three: UltraOpt is more function comlete and user friendly\n\nUltraOpt is more function comlete and  user friendly than other optimize library:\n\n\n|                                          | UltraOpt    | HyperOpt    |Scikit-Optimize|SMAC3        |HpBandSter   |\n|------------------------------------------|-------------|-------------|---------------|-------------|-------------|\n|Simple Usage like `fmin` function          |✓ |✓ |✓   |✓ |×|\n|Simple `Config Space` Definition           |✓ |✓ |✓   |×|×|\n|Support Conditional `Config Space`        |✓ |✓ |×  |✓ |✓ |\n|Support Serializable `Config Space`        |✓ |×|×  |×|×|\n|Support Visualizing `Config Space`         |✓ |✓ |×  |×|×|\n|Can Analyse Optimization Process & Result |✓ |×|✓   |×|✓ |\n|Distributed in Cluster                    |✓ |✓ |×  |×|✓ |\n|Support HyperBand[<sup>[6]</sup>](#refer-6) & SuccessiveHalving[<sup>[7]</sup>](#refer-7)     |✓ |×|×  |✓ |✓ |\n\n\n\n\n# Citation\n\n```\n@misc{Tang_UltraOpt,\n    author       = {Qichun Tang},\n    title        = {UltraOpt : Distributed Asynchronous Hyperparameter Optimization better than HyperOpt}\n    month        = January,\n    year         = 2021,\n    doi          = {10.5281/zenodo.4430148},\n    version      = {v0.1.0},\n    publisher    = {Zenodo},\n    url          = {https://doi.org/10.5281/zenodo.4430148}\n}\n```\n\n-----\n\n<b id=\"referance\">Reference</b>\n\n\n<div id=\"refer-1\"></div>\n\n[1] [Thornton, Chris et al. “Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms.” Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (2013): n. pag.](https://arxiv.org/abs/1208.3719)\n\n<div id=\"refer-2\"></div>\n\n[2] [Falkner, Stefan et al. “BOHB: Robust and Efficient Hyperparameter Optimization at Scale.” ICML (2018).](https://arxiv.org/abs/1807.01774)\n\n<div id=\"refer-3\"></div>\n\n[3] [Hutter F., Hoos H.H., Leyton-Brown K. (2011) Sequential Model-Based Optimization for General Algorithm Configuration. In: Coello C.A.C. (eds) Learning and Intelligent Optimization. LION 2011. Lecture Notes in Computer Science, vol 6683. Springer, Berlin, Heidelberg.](https://link.springer.com/chapter/10.1007/978-3-642-25566-3_40)\n\n<div id=\"refer-4\"></div>\n\n[4] https://github.com/scikit-optimize/scikit-optimize\n\n<div id=\"refer-5\"></div>\n\n[5] [James Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms for hyper-parameter optimization. In Proceedings of the 24th International Conference on Neural Information Processing Systems (NIPS'11). Curran Associates Inc., Red Hook, NY, USA, 2546–2554.](https://dl.acm.org/doi/10.5555/2986459.2986743)\n\n<div id=\"refer-6\"></div>\n\n[6] [Li, L. et al. “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization.” J. Mach. Learn. Res. 18 (2017): 185:1-185:52.](https://arxiv.org/abs/1603.06560)\n\n<div id=\"refer-7\"></div>\n\n[7] [Jamieson, K. and Ameet Talwalkar. “Non-stochastic Best Arm Identification and Hyperparameter Optimization.” AISTATS (2016).](https://arxiv.org/abs/1502.07943)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/auto-flow/ultraopt",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ultraopt",
    "package_url": "https://pypi.org/project/ultraopt/",
    "platform": "",
    "project_url": "https://pypi.org/project/ultraopt/",
    "project_urls": {
      "Homepage": "https://github.com/auto-flow/ultraopt"
    },
    "release_url": "https://pypi.org/project/ultraopt/0.1.1/",
    "requires_dist": null,
    "requires_python": ">=3.6.*",
    "summary": "Distributed Asynchronous Hyperparameter Optimization Better than HyperOpt",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9105714,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0209050dea6c27294d9ac9a8187bef0c676e751ade58f3b44ce5f8a93335f5d6",
          "md5": "0000da2709b43d9a454e11cba03faf05",
          "sha256": "4360463574aaf328a3a026d33bafef5dfefc72a47da0dac7e09f5ca209a6f09e"
        },
        "downloads": -1,
        "filename": "ultraopt-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0000da2709b43d9a454e11cba03faf05",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.*",
        "size": 67678,
        "upload_time": "2021-01-10T06:23:03",
        "upload_time_iso_8601": "2021-01-10T06:23:03.595406Z",
        "url": "https://files.pythonhosted.org/packages/02/09/050dea6c27294d9ac9a8187bef0c676e751ade58f3b44ce5f8a93335f5d6/ultraopt-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "493885511e7a6362be58d4a4eaf863b2e0f8311ee740e287f21f5b87683db494",
          "md5": "b8f32e9178c6118cab340c81aa4b3ac2",
          "sha256": "096964c057f85a0480baa0b78d788c005f3b1c464170cd2904a4c4669c3b1551"
        },
        "downloads": -1,
        "filename": "ultraopt-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b8f32e9178c6118cab340c81aa4b3ac2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.*",
        "size": 67715,
        "upload_time": "2021-01-11T14:51:06",
        "upload_time_iso_8601": "2021-01-11T14:51:06.159799Z",
        "url": "https://files.pythonhosted.org/packages/49/38/85511e7a6362be58d4a4eaf863b2e0f8311ee740e287f21f5b87683db494/ultraopt-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "493885511e7a6362be58d4a4eaf863b2e0f8311ee740e287f21f5b87683db494",
        "md5": "b8f32e9178c6118cab340c81aa4b3ac2",
        "sha256": "096964c057f85a0480baa0b78d788c005f3b1c464170cd2904a4c4669c3b1551"
      },
      "downloads": -1,
      "filename": "ultraopt-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "b8f32e9178c6118cab340c81aa4b3ac2",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.*",
      "size": 67715,
      "upload_time": "2021-01-11T14:51:06",
      "upload_time_iso_8601": "2021-01-11T14:51:06.159799Z",
      "url": "https://files.pythonhosted.org/packages/49/38/85511e7a6362be58d4a4eaf863b2e0f8311ee740e287f21f5b87683db494/ultraopt-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}