{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Image Processing",
      "Topic :: Scientific/Engineering :: Image Recognition",
      "Topic :: System :: Distributed Computing"
    ],
    "description": "# dask-pytorch-ddp\n\n## NOTE: dask-pytorch has been renamed to dask-pytorch-ddp!\nWe issued this final release of dask-pytorch v0.1.2, which is identical to dask-pytorch-ddp v0.2.0, to ease this transition. You can find dask-pytorch-ddp on PyPI at https://pypi.python.org/pypi/dask-pytorch-ddp/.\n\n\n<!-- ![GitHub Actions](https://github.com/saturncloud/dask-pytorch-ddp/workflows/GitHub%20Actions/badge.svg) [![PyPI Version](https://img.shields.io/pypi/v/prefect-saturn.svg)](https://pypi.org/project/prefect-saturn) -->\n\n`dask-pytorch-ddp` is a Python package that makes it easy to train PyTorch models on Dask clusters using distributed data parallel.  The intended scope of the project is\n- bootstrapping PyTorch workers on top of a Dask cluster\n- Using distributed data stores (e.g., S3) as normal PyTorch datasets\n- mechanisms for tracking and logging intermediate results, training statistics, and checkpoints.\n\nAt this point, this library and examples provided are tailored to computer vision tasks, but this library is intended to be useful for any sort of PyTorch tasks. The only thing really specific to image processing is the `S3ImageFolder` dataset class. Implementing a PyTorch dataset (assuming map style random access) outside of images currently requires implementing `__getitem__(self, idx: int):` and `__len__(self):` We plan to add more varied examples for other use cases in the future, and welcome PRs extending functionality.\n\n## Typical non-dask workflow\n\nA typical example of non-dask PyTorch usage is as follows:\n\n### Loading Data\nCreate an dataset (`ImageFolder`), and wrap it in a `DataLoader`\n\n```python\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(250),\n    transforms.ToTensor()\n])\n\nwhole_dataset = ImageFolder(path, transform=transform)\n\nbatch_size = 100\nnum_workers = 64\nindices = list(range(len(data)))\nnp.random.shuffle(indices)\ntrain_idx = indices[:num]\ntest_idx = indices[num:num+num]\n\ntrain_sampler = SubsetRandomSampler(train_idx)\ntrain_loader = DataLoader(data, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers)\n```\n\n### Training a Model\nLoop over the dataset, and train the model by stepping the optimizer\n\n```python\ndevice = torch.device(0)\nnet = models.resnet18(pretrained=False)\nmodel = net.to(device)\ndevice_ids = [0]\n\ncriterion = nn.CrossEntropyLoss().cuda()\nlr = 0.001\noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\ncount = 0\nfor epoch in range(n_epochs):\n    model.train()  # Set model to training mode\n    for inputs, labels in train_loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        loss = criterion(outputs, labels)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        count += 1\n```\n\n## Now on Dask\n\nWith dask_pytorch_ddp and PyTorch Distributed Data Parallel, we can train on multiple workers as follows:\n\n### Loading Data\nLoad the dataset from S3, and explicitly set the multiprocessing context (Dask defaults to spawn, but pytorch is generally configured to use fork)\n\n```python\nfrom dask_pytorch_ddp.data import S3ImageFolder\n\nwhole_dataset = S3ImageFolder(bucket, prefix, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(\n    whole_dataset, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers, multiprocessing_context=mp.get_context('fork')\n)\n```\n\n### Training in Parallel\n\nWrap the training loop in a function (and add metrics logging.  Not necessary, but very useful).  Convert the model into a PyTorch Distributed Data Parallel (`DDP`) model which knows how to sync gradients together across workers.\n\n```python\nimport uuid\nimport pickle\nimport logging\nimport json\n\n\nkey = uuid.uuid4().hex\nrh = DaskResultsHandler(key)\n\ndef run_transfer_learning(bucket, prefix, samplesize, n_epochs, batch_size, num_workers, train_sampler):\n    worker_rank = int(dist.get_rank())\n    device = torch.device(0)\n    net = models.resnet18(pretrained=False)\n    model = net.to(device)\n    model = DDP(model, device_ids=[0])\n\n    criterion = nn.CrossEntropyLoss().cuda()\n    lr = 0.001\n    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n    whole_dataset = S3ImageFolder(bucket, prefix, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(\n        whole_dataset,\n        sampler=train_sampler,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        multiprocessing_context=mp.get_context('fork')\n    )\n\n    count = 0\n    for epoch in range(n_epochs):\n        # Each epoch has a training and validation phase\n        model.train()  # Set model to training mode\n        for inputs, labels in train_loader:\n            dt = datetime.datetime.now().isoformat()\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            loss = criterion(outputs, labels)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            count += 1\n\n            # statistics\n            rh.submit_result(\n                f\"worker/{worker_rank}/data-{dt}.json\",\n                json.dumps({'loss': loss.item(), 'epoch': epoch, 'count': count, 'worker': worker_rank})\n            )\n            if (count % 100) == 0 and worker_rank == 0:\n                rh.submit_result(f\"checkpoint-{dt}.pkl\", pickle.dumps(model.state_dict()))\n\n```\n\n## How does it work?\n\n`dask-pytorch-ddp` is largely a wrapper around existing `pytorch` functionality.  `pytorch.distributed` provides infrastructure for [Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) (DDP).\n\nIn DDP, you create N workers, and the 0th worker is the \"master\", and coordinates the synchronization of buffers and gradients.  In SGD, gradients are normally averaged between all data points in a batch.  By running batches on multiple workers, and averaging the gradients, DDP enables you to run SGD with a much bigger batch size `(N * batch_size)`\n\n`dask-pytorch-ddp` sets some environment variables to configure the \"master\" host and port, and then calls `init_process_group` before training, and calls `destroy_process_group` after training.  This is the same process normally done manually by the data scientist.\n\n### Multi GPU machines\n`dask_cuda_worker` automatically rotates `CUDA_VISIBLE_DEVICES` for each worker it creates (typically one per GPU).  As a result, your PyTorch code should always start with the 0th GPU.\n\nFor example, if I have an 8 GPU machine, the 3rd worker will have `CUDA_VISIBLE_DEVICES` set to `2,3,4,5,6,7,0,1`.  On that worker, if I call `torch.device(0)`, I will get GPU 2.\n\n## What else?\n\n`dask-pytorch-ddp` also implements an S3 based `ImageFolder`.  More distributed friendly datasets are planned.  `dask-pytorch-ddp` also implements a basic results aggregation framework so that it is easy to collect training metrics across different workers.  Currently, only `DaskResultsHandler` which leverages [Dask pub-sub communication protocols][1] is implemented, but an S3 based result handler is planned.\n\n[1]:https://docs.dask.org/en/latest/futures.html#publish-subscribe\n\n## Some Notes\n\nDask generally spawns processes.  PyTorch generally forks.  When using a multiprocessing enabled data loader, it is a good idea to pass the `Fork` multiprocessing context to force the use of Forking in the data loader.\n\nSome Dask deployments do not permit spawning processes.  To override this, you can change the [distributed.worker.daemon](https://docs.dask.org/en/latest/configuration-reference.html#distributed.worker.daemon) setting.\n\nEnvironment variables are a convenient way to do this:\n\n```\nDASK_DISTRIBUTED__WORKER__DAEMON=False\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/saturncloud/dask-pytorch-ddp",
    "keywords": "saturn cloud dask pytorch torch",
    "license": "BSD 3-clause",
    "maintainer": "Saturn Cloud Developers",
    "maintainer_email": "open-source@saturncloud.io",
    "name": "dask-pytorch",
    "package_url": "https://pypi.org/project/dask-pytorch/",
    "platform": "",
    "project_url": "https://pypi.org/project/dask-pytorch/",
    "project_urls": {
      "Documentation": "https://github.com/saturncloud/dask-pytorch-ddp",
      "Homepage": "https://github.com/saturncloud/dask-pytorch-ddp",
      "Issue Tracker": "https://github.com/saturncloud/dask-pytorch-ddp/issues",
      "Source": "https://github.com/saturncloud/dask-pytorch-ddp"
    },
    "release_url": "https://pypi.org/project/dask-pytorch/0.1.2/",
    "requires_dist": [
      "dask",
      "distributed",
      "pillow",
      "torch",
      "dask ; extra == 'dev'",
      "distributed ; extra == 'dev'",
      "pillow ; extra == 'dev'",
      "torch ; extra == 'dev'",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'"
    ],
    "requires_python": ">=3.7",
    "summary": "dask-pytorch has been renamed dask-pytorch-ddp",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8783028,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d4339ba967e72e56dd7bf0f90752c7d1a69ab4817e02bb3817db44fa7f8842ad",
          "md5": "fa0898c36e523d1474d8dab478c7d9d2",
          "sha256": "91a446d1c91937b4739aa2e60fe689d90e238786fa716b63ca501944ca53ca4a"
        },
        "downloads": -1,
        "filename": "dask_pytorch-0.1.0-py2.7.egg",
        "has_sig": false,
        "md5_digest": "fa0898c36e523d1474d8dab478c7d9d2",
        "packagetype": "bdist_egg",
        "python_version": "2.7",
        "requires_python": ">=3.7",
        "size": 8566,
        "upload_time": "2020-11-30T21:02:56",
        "upload_time_iso_8601": "2020-11-30T21:02:56.388548Z",
        "url": "https://files.pythonhosted.org/packages/d4/33/9ba967e72e56dd7bf0f90752c7d1a69ab4817e02bb3817db44fa7f8842ad/dask_pytorch-0.1.0-py2.7.egg",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "31f155809664955636a1008237635bba61513e69e0d65c1ff16a3f4ad095c59b",
          "md5": "ee90f81f609989eb8c69b35c2769eafd",
          "sha256": "f01db911bd2d2c47b0a1adb67c7e2c63c0520486feb666136bfba3343be675e7"
        },
        "downloads": -1,
        "filename": "dask_pytorch-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ee90f81f609989eb8c69b35c2769eafd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 9463,
        "upload_time": "2020-11-18T17:35:06",
        "upload_time_iso_8601": "2020-11-18T17:35:06.556001Z",
        "url": "https://files.pythonhosted.org/packages/31/f1/55809664955636a1008237635bba61513e69e0d65c1ff16a3f4ad095c59b/dask_pytorch-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "34d51d00da2acbc2879a237261784384038999c14ceeaef6ccdafc4c2d66b4cb",
          "md5": "744cf2d397f55097a954d0b9751f6fc9",
          "sha256": "f73f7438e499a46b8ffc4f219bd7013135d45a9ecb707e6607095e2682483b2a"
        },
        "downloads": -1,
        "filename": "dask-pytorch-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "744cf2d397f55097a954d0b9751f6fc9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 8062,
        "upload_time": "2020-11-18T17:35:07",
        "upload_time_iso_8601": "2020-11-18T17:35:07.701830Z",
        "url": "https://files.pythonhosted.org/packages/34/d5/1d00da2acbc2879a237261784384038999c14ceeaef6ccdafc4c2d66b4cb/dask-pytorch-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c9acd7e8b758cdfd871944b4e5feb732b82a3e4f84cd15cf56fb763d17a8bc96",
          "md5": "016bdcbda747ee0a3a5ad112ddbc7f0b",
          "sha256": "4c4d42b3e738940037777b77dca5e47232b956eca8dd66be248abf6a1fe2892f"
        },
        "downloads": -1,
        "filename": "dask_pytorch-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "016bdcbda747ee0a3a5ad112ddbc7f0b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 9451,
        "upload_time": "2020-11-20T17:04:48",
        "upload_time_iso_8601": "2020-11-20T17:04:48.423828Z",
        "url": "https://files.pythonhosted.org/packages/c9/ac/d7e8b758cdfd871944b4e5feb732b82a3e4f84cd15cf56fb763d17a8bc96/dask_pytorch-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fda5c3114db744f87fe79be61367643a7e686cda16294ab1d37c75273165379a",
          "md5": "62794dd255da8fad18c191775f0995a5",
          "sha256": "3fa020475b161e85f3ac433105dfc3e2cae4d25315e1eddbd30907889527756f"
        },
        "downloads": -1,
        "filename": "dask-pytorch-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "62794dd255da8fad18c191775f0995a5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 8127,
        "upload_time": "2020-11-20T17:04:49",
        "upload_time_iso_8601": "2020-11-20T17:04:49.638127Z",
        "url": "https://files.pythonhosted.org/packages/fd/a5/c3114db744f87fe79be61367643a7e686cda16294ab1d37c75273165379a/dask-pytorch-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4b15ccf5cca2ea2bea33431b12341f2f65fca7c1cf488a5d32bc6024c6bb132b",
          "md5": "c0b1f797da88ae55cf9070690ee5ff78",
          "sha256": "05d00005bf55d3b32bf9dcfcb6a86dda3e176e666192aa0ebf3b58b80247f9b6"
        },
        "downloads": -1,
        "filename": "dask_pytorch-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c0b1f797da88ae55cf9070690ee5ff78",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 9585,
        "upload_time": "2020-11-30T21:02:54",
        "upload_time_iso_8601": "2020-11-30T21:02:54.310273Z",
        "url": "https://files.pythonhosted.org/packages/4b/15/ccf5cca2ea2bea33431b12341f2f65fca7c1cf488a5d32bc6024c6bb132b/dask_pytorch-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "41a543338e6bfb66013d1bc614f722ea9aa7a27a98f4bc5c28380b44a34e767b",
          "md5": "920a73492e9c0c0c49fc62ca896efcd1",
          "sha256": "f367ffbbb69c23614c76f959e6db3cc040bf1bfeaa3a944437e8fbb79d2549b8"
        },
        "downloads": -1,
        "filename": "dask-pytorch-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "920a73492e9c0c0c49fc62ca896efcd1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 8698,
        "upload_time": "2020-11-30T21:02:55",
        "upload_time_iso_8601": "2020-11-30T21:02:55.554781Z",
        "url": "https://files.pythonhosted.org/packages/41/a5/43338e6bfb66013d1bc614f722ea9aa7a27a98f4bc5c28380b44a34e767b/dask-pytorch-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4b15ccf5cca2ea2bea33431b12341f2f65fca7c1cf488a5d32bc6024c6bb132b",
        "md5": "c0b1f797da88ae55cf9070690ee5ff78",
        "sha256": "05d00005bf55d3b32bf9dcfcb6a86dda3e176e666192aa0ebf3b58b80247f9b6"
      },
      "downloads": -1,
      "filename": "dask_pytorch-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c0b1f797da88ae55cf9070690ee5ff78",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 9585,
      "upload_time": "2020-11-30T21:02:54",
      "upload_time_iso_8601": "2020-11-30T21:02:54.310273Z",
      "url": "https://files.pythonhosted.org/packages/4b/15/ccf5cca2ea2bea33431b12341f2f65fca7c1cf488a5d32bc6024c6bb132b/dask_pytorch-0.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "41a543338e6bfb66013d1bc614f722ea9aa7a27a98f4bc5c28380b44a34e767b",
        "md5": "920a73492e9c0c0c49fc62ca896efcd1",
        "sha256": "f367ffbbb69c23614c76f959e6db3cc040bf1bfeaa3a944437e8fbb79d2549b8"
      },
      "downloads": -1,
      "filename": "dask-pytorch-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "920a73492e9c0c0c49fc62ca896efcd1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 8698,
      "upload_time": "2020-11-30T21:02:55",
      "upload_time_iso_8601": "2020-11-30T21:02:55.554781Z",
      "url": "https://files.pythonhosted.org/packages/41/a5/43338e6bfb66013d1bc614f722ea9aa7a27a98f4bc5c28380b44a34e767b/dask-pytorch-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}