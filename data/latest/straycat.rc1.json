{
  "info": {
    "author": "Arief Akbar Hidayat",
    "author_email": "hidayat.ariefakbar@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.11",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "========\nstraycat\n========\n\n.. image:: assets/straycat_logo.png\n  :width: 400\n\n.. image:: https://img.shields.io/pypi/v/straycat.svg\n        :target: https://pypi.python.org/pypi/straycat\n\n.. image:: https://img.shields.io/travis/hradkafeira/straycat.svg\n        :target: https://travis-ci.com/hradkafeira/straycat\n\n.. image:: https://readthedocs.org/projects/straycat/badge/?version=latest\n        :target: https://straycat.readthedocs.io/en/latest/?version=latest\n        :alt: Documentation Status\n\n.. image:: https://github.com/Hradkafeira/straycat/actions/workflows/straycat_testing.yml/badge.svg\n        :target: https://github.com/Hradkafeira/straycat/actions/workflows/straycat_testing.yml\n\n.. image:: https://codecov.io/gh/Hradkafeira/straycat/branch/main/graph/badge.svg?token=6SH3QTEU8D\n        :target: https://codecov.io/gh/Hradkafeira/straycat\n\n\nEasy NLP implementation for Indonesian Language\n\n\n* Free software: MIT license\n* Documentation: https://straycat.readthedocs.io.\n\nFeatures\n--------\n- Automate Text Preprocessing Pipeline\n- Automate Text Preprocessing Pipeline With pandas\n- Tokenization\n- Stemming\n- Stopwords \n- Remove Punctuation\n- Remove emoji\n- Remove non alpha numerik\n- Remove link\n- Remove date\n- Remove Medianame\n- Normalize slang words\n\n\n============\nInstallation\n============\n\nStable release\n--------------\n\nTo install straycat, run this command in your terminal:\n\n\n.. code-block:: console\n\n    $ pip install straycat\n\nThis is the preferred method to install straycat, as it will always install the most recent stable release.\n\nUsage\n*****\n::\n\n        from straycat.text_preprocessing import TextPreprocessing\n\n        # Instatiation with default stopwords\n        st = TextPreprocessing()\n\n        # Instatiation with your own stopwords\n        st = TextPreprocessing(other_stopwords=[\"sw1\", \"sw2\", \"etc\"])\n\n        # Instatiation with combine default stopwords and your stopwords\n        st = TextPreprocessing.add_stopwords([\"sw1\", \"sw2\", \"etc\"])\n\n        #See available pipelines before using it\n        print(st.list_process)\n        #output\n\n        Here the list for auto_text_prep\n        Input value with number or text\n        1  or \"case_folding\"\n        2  or \"punctuation_removal\"\n        3  or \"stopwords_removal\"\n        4  or \"stemming\"\n        5  or \"encode_text\"\n        6  or \"medianame_removal\"\n        7  or \"non_alnum_removal\"\n        8  or \"link_removal\"\n        9  or \"emoji_removal\"\n        10 or \"normalize_slang\"\n        11 or \"date_removal\"\n\nAutomate text preprocessing with call one method\n************************************************\n::\n\n        # Automate Text Preprocessing with default pipelines \n        (tokenizing, case folding, remove punctuation, remove stopwords, stemming)\n\n        # Return list of Tokens\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!! 😁 😆 😅\"]) \n        #output [['ak', 'suka', 'mkan', 'apel', 'rasa', 'enak']]\n\n        #Return list of Sentences               \n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!!\"],return_types=\"list_of_sentences\") \n        #output ['ak suka mkan apel rasa enak']\n\nAdd more additional text preprocessing pipeline with call one method\n********************************************************************\n::\n\n        # Add more additional pipeline (normalize slang word, remove date, remove emoji, remove medianame, remove link, remove non alnum )\n\n        # Return list of Tokens with number args of process\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!!\"],\n                        set_process=\"add_process\",\n                        process=[10])\n        #output [['saya', 'suka', 'makan', 'apel', 'rasa', 'enak']]\n\n        # Return list of Tokens with name args of process\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!!\"],\n                          set_process=\"add_process\",\n                          input_proc=\"name\",\n                          process=[\"normalize_slang\"])\n        #output [['saya', 'suka', 'makan', 'apel', 'rasa', 'enak']]\n\n        # Return list of Sentences with number args of process\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!!\"],\n                          set_process=\"add_process\",\n                          process=[10], \n                          return_types=\"list_of_sentences\" )\n        #output ['saya suka makan apel rasa enak']\n\n        # Return list of Sentences with name args of process\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!!\"],\n                          set_process=\"add_process\",\n                          input_proc=\"name\",\n                          process=[\"normalize_slang\"], \n                          return_types=\"list_of_sentences\" )\n        #output ['saya suka makan apel rasa enak']\n\nCustomize text preprocessing pipeline with call one method\n**********************************************************\n::\n\n       # Customize process pipeline\n\n        # Return list of Tokens with number args of process\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!!\"],\n                        set_process=\"customize\",\n                        process=[10])\n        #output [['saya','suka','makan','apel','karena','rasanya','enak','!','!','!','😁','😆','😅']]\n\n        # Return list of Tokens with name args of process\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!!\"],\n                          set_process=\"customize\",\n                          input_proc=\"name\",\n                          process=[\"normalize_slang\"])\n        #output [['saya','suka','makan','apel','karena','rasanya','enak','!','!','!','😁','😆','😅']]\n\n        # Return list of Sentences with number args of process\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!!\"],\n                          set_process=\"customize\",\n                          process=[10], \n                          return_types=\"list_of_sentences\" )\n        #output ['saya suka makan apel karena rasanya enak ! ! ! 😁 😆 😅']\n\n        # Return list of Sentences with name args of process\n        st.auto_text_prep([\"ak suka mkan apel karena rasanya enak!!! 😁 😆 😅\"],\n                        set_process=\"customize\",\n                        input_proc=\"name\",\n                        process=[\"normalize_slang\"], \n                        return_types=\"list_of_sentences\")\n        #output ['saya suka makan apel karena rasanya enak ! ! ! 😁 😆 😅']\n\n\nUse specific text preprocessing task\n************************************\n::\n\n        # Tokenize Indonesian Language\n\n        st.tokenize(\"saya sedang memakan apple.\")  \n        #output [\"saya\", \"sedang\", \"memakan\", \"apple\",\".\"]\n\n        # Concatenate Tokens\n\n        st.concat_token([\"saya\", \"sedang\", \"memakan\", \"apple\"]) \n        #output \"saya sedang memakan apple\"\n\n        # Stemming Indonesia Language\n\n        st.stemming(\"saya suka memakan apple\") \n        #output [\"saya\",\"suka\",\"makan\",\"apple\"]\n\n        st.stemming(\"saya suka memakan apple\", return_type=\"sentences\") \n        #output \"saya suka makan apple\"\n\n        # Case folding\n\n        st.case_folding(\"Proses Teks Preprocessing\") \n        #output [\"proses\", \"teks\", \"preprocessing\"]\n\n        st.case_folding(\"Proses Teks Preprocessing\", return_type=\"sentences\") \n        #output \"proses teks preprocessing\"\n\n        # Stopwords Removal\n\n        st.stop_words(\"apel yang terlihat lezat\") \n        #output [\"apel\",\"terlihat\",\"lezat\"]\n\n        st.stop_words(\"apel yang terlihat lezat\",return_type=\"sentences\") \n        #output \"apel terlihat lezat\"\n\n        # Punctuation Removal\n\n        st.remove_punc(\"dapat hubungi akun@google !!!\"\") \n        #output [\"dapat\",\"hubungi\",\"akun@google\"]\n\n        st.remove_punc(\"dapat hubungi akun@google !!!\"\", return_type=\"sentences\") \n        #output \"dapat hubungi akun@google\"\n\n        # Non Alnum Removal\n\n        st.remove_non_alnum(\"dapat hubungi akun@google !!!\") \n        #output [\"dapat\",\"hubungi\"]\n\n        st.remove_non_alnum(\"dapat hubungi akun@google !!!\", return_type=\"sentences\") \n        #output \"dapat hubungi\"\n\n        # Remove emoji\n\n        st.remove_emoji(\"hahaha 😀 😃 😄 hahaha 😁 😆 😅 hahaha\") \n        #output [\"hahaha\",\"hahaha\",\"hahaha\"]\n\n        st.remove_emoji(\"hahaha 😀 😃 😄 hahaha 😁 😆 😅 hahaha\", return_type=\"sentences\") \n        #output \"hahaha hahaha hahaha\"\n\n        # Remove date\n\n        st.remove_date(\"tanggal 03 Maret 2020 17/08/1945 10-11-1945 tanggal\") \n        #output [\"tanggal\", \"tanggal\"]\n\n        st.remove_date(\"tanggal 03 Maret 2020 17/08/1945 10-11-1945 tanggal\",return_type=\"sentences\") \n        #output \"tanggal tanggal\"\n\n        # Remove link\n\n        st.remove_link(\"https://www.kompas.com berita hari ini\") \n        #output [\"berita\", \"hari\", \"ini\"]\n\n        st.remove_link(\"https://www.kompas.com berita hari ini\", return_type = \"sentences\") \n        #output \"berita hari ini\"\n\n        # Remove media name\n\n        st.remove_medianame(\"kompas.com berita hari ini\") \n        #output [\"berita\", \"hari\", \"ini\"]\n\n        st.remove_medianame(\"kompas.com berita hari ini\", return_type = \"sentences\") \n        #output \"berita hari ini\"\n\n        # Normalize slang\n\n        st.remove_slang(\"ak sk mkan\") \n        #output [\"saya\", \"suka\", \"makan\"]\n\n        st.remove_slang(\"ak sk mkan\", return_type = \"sentences\") \n        #output \"saya suka makan\"\n\n        #encode text\n        st.encode_text(\"Saya \\x94sedang makan apple\") \n        #output \"saya sedang memakan apple\"\n\n\nWORKING WITH DATAFRAME\n**********************\n::\n\n        # Straycat with DataFrame\n\n        from straycat.text_preprocessing import TextPreprocessing\n        import pandas as pd\n\n        # Instatiation with default stopwords\n        st = TextPreprocessing()\n\n        # Instatiation with your own stopwords\n        st = TextPreprocessing(other_stopwords=[\"sw1\", \"sw2\", \"etc\"])\n\n        # Instatiation with combine default stopwords and your stopwords\n        st = TextPreprocessing.add_stopwords([\"sw1\", \"sw2\", \"etc\"])\n\n        #See available pipelines before using it\n        print(st.list_process)\n        #output\n\n        Here the list for auto_text_prep\n        Input value with number or text\n        1  or \"case_folding\"\n        2  or \"punctuation_removal\"\n        3  or \"stopwords_removal\"\n        4  or \"stemming\"\n        5  or \"encode_text\"\n        6  or \"medianame_removal\"\n        7  or \"non_alnum_removal\"\n        8  or \"link_removal\"\n        9  or \"emoji_removal\"\n        10 or \"normalize_slang\"\n        11 or \"date_removal\"\n\n        teks = [\"tvri.com 14/08/1945 telah terjadi hari kemerdekaan\",\"ak suka mkn apel karena rasanya enak!!! 😁 😆 😅\"]\n        doc = pd.DataFrame(teks,columns=[\"text\"])\n\nAutomate text preprocessing pipeline in dataframe with call one method\n**********************************************************************\n::\n\n        # Automate Text Preprocessing with default pipeline (tokenizing, case folding, remove punctuation, remove stopwords, stemming)\n\n        # Return list of Tokens\n        st.auto_text_prep(doc[\"text\"]) \n        #output [['tvri', 'com', '14', '08', '1945', 'jadi', 'hari', 'merdeka'],\n        ['ak', 'suka', 'mkn', 'apel', 'rasa', 'enak']]\n\n        # Return list of Sentences\n        st.auto_text_prep(doc[\"text\"], return_types=\"list_of_sentences\")\n        #output ['tvri com 14 08 1945 jadi hari merdeka', 'ak suka mkn apel rasa enak']\n\n\nAdd more additional text preprocessing pipeline in dataframe with call one method\n*********************************************************************************\n::\n\n        # Add more additional pipeline (normalize slang word, remove date, remove emoji, remove medianame, remove link, remove non alnum )\n\n        # Return list of Tokens with number args of process\n        st.auto_text_prep(doc[\"text\"], set_process=\"add_process\", process=[6, 11])\n        #output [['jadi', 'hari', 'merdeka'], ['ak', 'suka', 'mkn', 'apel', 'rasa', 'enak']]\n\n        # Return list of Tokens with name args of process\n        st.auto_text_prep(doc[\"text\"], set_process=\"add_process\",\n                          input_proc=\"name\",\n                          process=[\"medianame_removal\",\"date_removal\"])\n        #output [['jadi', 'hari', 'merdeka'], ['ak', 'suka', 'mkn', 'apel', 'rasa', 'enak']]\n\n        # Return list of Sentences with name args of process\n        st.auto_text_prep(doc[\"text\"], set_process=\"add_process\", \n                          process=[6, 11],       \n                        return_types=\"list_of_sentences\")\n        #output ['jadi hari merdeka', 'ak suka mkn apel rasa enak']\n\n        # Return list of Sentences with name args of process\n        st.auto_text_prep(doc[\"text\"], set_process=\"add_process\",\n                          input_proc=\"name\",\n                          process=[\"medianame_removal\",\"date_removal\"],       \n                          return_types=\"list_of_sentences\")\n        #output ['jadi hari merdeka', 'ak suka mkn apel rasa enak']\n\nCustomize text preprocessing pipeline in dataframe with call one method\n***********************************************************************\n::\n\n        # Customize pipeline \n\n        # Return list of Tokens with number args of process\n        st.auto_text_prep(doc[\"text\"], set_process=\"customize\", process=[6, 11])\n        #output [['telah', 'terjadi', 'hari', 'kemerdekaan'],\n                ['ak','suka','mkn','apel','karena','rasanya','enak','!','!','!','😁','😆','😅']]\n\n        # Return list of Tokens with name args of process\n        st.auto_text_prep(doc[\"text\"], set_process=\"customize\", \n                          input_proc=\"name\",\n                          process=[\"medianame_removal\",\"date_removal\"])\n        #output [['telah', 'terjadi', 'hari', 'kemerdekaan'],\n                ['ak','suka','mkn','apel','karena','rasanya','enak','!','!','!','😁','😆','😅']]\n\n\n        # Return list of Sentences with number args of process\n        st.auto_text_prep(doc[\"text\"], set_process=\"customize\",\n                          process=[6, 11],\n                        return_types=\"list_of_sentences\")\n        #output ['telah terjadi hari kemerdekaan','ak suka mkn apel karena rasanya enak!!! 😁 😆 😅']\n\n        # Return list of Sentences with name args of process\n        st.auto_text_prep(doc[\"text\"], set_process=\"customize\",\n                          input_proc=\"name\", \n                          process=[\"medianame_removal\",\"date_removal\"],\n                          return_types=\"list_of_sentences\")\n        #output ['telah terjadi hari kemerdekaan','ak suka mkn apel karena rasanya enak!!! 😁 😆 😅']\n\nCredits\n-------\n\nThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n\n\n=======\nHistory\n=======\n\n1.0.0 (2022-06-23)\n------------------\n\n* First release on PyPI.\n\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/hradkafeira/straycat",
    "keywords": "straycat",
    "license": "MIT license",
    "maintainer": "",
    "maintainer_email": "",
    "name": "straycat",
    "package_url": "https://pypi.org/project/straycat/",
    "platform": null,
    "project_url": "https://pypi.org/project/straycat/",
    "project_urls": {
      "Homepage": "https://github.com/hradkafeira/straycat"
    },
    "release_url": "https://pypi.org/project/straycat/1.0.0/",
    "requires_dist": [
      "Sastrawi (>=1.0.1)",
      "nltk (>=3.6.5)",
      "pandas",
      "spacy (>=3.3.1)"
    ],
    "requires_python": ">=3.6",
    "summary": "Easy NLP implementation for Indonesian Language",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14221107,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f1d453104a8e1d69daf0988cbe1696d53d73d9e74ae4eb63440d7f0ff40a0c26",
          "md5": "14ecd16174ff8829c12f74885ed32c7e",
          "sha256": "cac3a26db0a64ae52ac15bf387638d7483a30928c5b9059b5601ca3da09bd09d"
        },
        "downloads": -1,
        "filename": "straycat-1.0.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "14ecd16174ff8829c12f74885ed32c7e",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.6",
        "size": 11713,
        "upload_time": "2022-06-23T06:00:27",
        "upload_time_iso_8601": "2022-06-23T06:00:27.433020Z",
        "url": "https://files.pythonhosted.org/packages/f1/d4/53104a8e1d69daf0988cbe1696d53d73d9e74ae4eb63440d7f0ff40a0c26/straycat-1.0.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f0876db3c7a3b7c02a610322e609c7db689eb01e4d970a34750c46816dc97d7c",
          "md5": "96169362990b5ac201a6214cbe78dc69",
          "sha256": "7a4ae79b83e98c4f52978806691a47d5db444046cd08213f77e99a47798d4b44"
        },
        "downloads": -1,
        "filename": "straycat-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "96169362990b5ac201a6214cbe78dc69",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 21503,
        "upload_time": "2022-06-23T06:00:30",
        "upload_time_iso_8601": "2022-06-23T06:00:30.680070Z",
        "url": "https://files.pythonhosted.org/packages/f0/87/6db3c7a3b7c02a610322e609c7db689eb01e4d970a34750c46816dc97d7c/straycat-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f1d453104a8e1d69daf0988cbe1696d53d73d9e74ae4eb63440d7f0ff40a0c26",
        "md5": "14ecd16174ff8829c12f74885ed32c7e",
        "sha256": "cac3a26db0a64ae52ac15bf387638d7483a30928c5b9059b5601ca3da09bd09d"
      },
      "downloads": -1,
      "filename": "straycat-1.0.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "14ecd16174ff8829c12f74885ed32c7e",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": ">=3.6",
      "size": 11713,
      "upload_time": "2022-06-23T06:00:27",
      "upload_time_iso_8601": "2022-06-23T06:00:27.433020Z",
      "url": "https://files.pythonhosted.org/packages/f1/d4/53104a8e1d69daf0988cbe1696d53d73d9e74ae4eb63440d7f0ff40a0c26/straycat-1.0.0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f0876db3c7a3b7c02a610322e609c7db689eb01e4d970a34750c46816dc97d7c",
        "md5": "96169362990b5ac201a6214cbe78dc69",
        "sha256": "7a4ae79b83e98c4f52978806691a47d5db444046cd08213f77e99a47798d4b44"
      },
      "downloads": -1,
      "filename": "straycat-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "96169362990b5ac201a6214cbe78dc69",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 21503,
      "upload_time": "2022-06-23T06:00:30",
      "upload_time_iso_8601": "2022-06-23T06:00:30.680070Z",
      "url": "https://files.pythonhosted.org/packages/f0/87/6db3c7a3b7c02a610322e609c7db689eb01e4d970a34750c46816dc97d7c/straycat-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}