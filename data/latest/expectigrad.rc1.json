{
  "info": {
    "author": "Brett Daley",
    "author_email": "b.daley@northeastern.edu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "\n# Expectigrad: Fast Stochastic Optimization with Robust Convergence Properties\n![pypi](https://img.shields.io/badge/pypi-0.0.0-blue)\n[![license](https://img.shields.io/badge/license-MIT-blue)](./LICENSE)\n![python](https://img.shields.io/badge/python-3.5-blue)\n\n[![pytorch](https://img.shields.io/badge/pytorch-yes-brightgreen)](#pytorch)\n[![tensorflow1](https://img.shields.io/badge/tensorflow%201-yes-brightgreen)](#tensorflow-1.x)\n[![tensorflow2](https://img.shields.io/badge/tensorflow%202-yes-brightgreen)](#tensorflow-2.x)\n\nExpectigrad is a first-order stochastic optimization method that fixes the\n[known divergence issue](https://arxiv.org/abs/1904.09237)\nof Adam, RMSProp, and related adaptive methods while offering better performance on\nwell-known deep learning benchmarks.\n\nExpectigrad introduces two innovations to adaptive gradient methods:\n- **Arithmetic RMS:** Computes the true RMS instead of an exponential moving average (EMA).\nThis makes Expectigrad more robust to divergence and, in theory, less susceptible to\ngradient noise.\n- **Outer momentum:** Applies momentum _after_ adapting the step sizes, not\nbefore.\nThis reduces bias in the updates by preserving the\n[superposition property](https://en.wikipedia.org/wiki/Superposition_principle).\n\nSee [the paper](https://arxiv.org/abs/2010.01356) for more details.\n\nPytorch, TensorFlow 1.x, and TensorFlow 2.x are all supported.\nSee [installation](#installation) and [usage](#usage) below to get started.\n\n### Pseudocode\n\n> ![equation](https://latex.codecogs.com/svg.latex?%5Ctext%7BInitialize%20network%20parameters%7D%5C%20x) <br/>\n> ![equation](https://latex.codecogs.com/svg.latex?s%20%5Cgets%200) <br/>\n> ![equation](https://latex.codecogs.com/svg.latex?n%20%5Cgets%200) <br/>\n> ![equation](https://latex.codecogs.com/svg.latex?m%20%5Cgets%200) <br/>\n> ![equation](https://latex.codecogs.com/svg.latex?%5Ctext%7Bfor%7D%5C%20t%3D1%2C2%2C%5Cldots%5C%20%5Ctext%7Buntil%20convergence%20do%7D) <br/>\n>  ![equation](https://latex.codecogs.com/svg.latex?%5Cquad%20g%20%5Cgets%20%5Cnabla%20f%28x%29) <br/>\n>  ![equation](https://latex.codecogs.com/svg.latex?s%20%5Cgets%20s%20&plus;%20g%5E2) <br/>\n>  ![equation](https://latex.codecogs.com/svg.latex?n%20%5Cgets%20n%20&plus;%20%5Ctext%7Bsign%7D%28g%5E2%29) <br/>\n>  ![equation](https://latex.codecogs.com/svg.latex?m%20%5Cgets%20%5Cbeta%20m%20&plus;%20%281-%5Cbeta%29%20%5Ccdot%20%5Cfrac%7Bg%7D%7B%5Cepsilon%20&plus;%20%5Csqrt%7B%5Cfrac%7Bs%7D%7Bn%7D%7D%7D) <br/>\n>  ![equation](https://latex.codecogs.com/svg.latex?x%20%5Cgets%20x%20-%20%5Cfrac%7B%5Calpha%7D%7B1-%5Cbeta%5Et%7D%20%5Ccdot%20m) <br/>\n> ![equation](https://latex.codecogs.com/svg.latex?%5Ctext%7Bend%20for%7D) <br/>\n> ![eqaution](https://latex.codecogs.com/svg.latex?%5Ctext%7Breturn%7D%5C%20x)\n\n### Citing\n\nIf you use this code for published work, please cite [the original paper](https://arxiv.org/abs/2010.01356):\n\n```\n@article{daley2020expectigrad,\n  title={Expectigrad: Fast Stochastic Optimization with Robust Convergence Properties},\n  author={Daley, Brett and Amato, Christopher},\n  journal={arXiv preprint arXiv:2010.01356},\n  year={2020}\n}\n```\n\n---\n\n## Installation\n\nUse pip to quickly install Expectigrad:\n\n```\npip install expectigrad\n```\n\nOr you can clone this repository and install manually:\n\n```\ngit clone https://github.com/brett-daley/expectigrad.git\ncd expectigrad\npython setup.py -e .\n```\n\n## Usage\n\nPytorch and both versions of TensorFlow are supported.\nRefer to the code snippets below to instantiate the optimizer for your deep learning framework.\n\n### Pytorch\n\n```python\nimport expectigrad\n\nexpectigrad.pytorch.Expectigrad(\n    params, lr=0.001, beta=0.9, eps=1e-8, sparse_counter=True\n)\n```\n\n| Args | | |\n| --- | :-: | --- |\n| params | (`iterable`) | Iterable of parameters to optimize or dicts defining parameter groups. |\n| lr | (`float`) | The learning rate, a scale factor applied to each optimizer step. Default: `0.001` |\n| beta | (`float`) | The decay rate for Expectigrad's bias-corrected, \"outer\" momentum. Must be in the interval [0, 1). Default: `0.9` |\n| eps | (`float`) | A small constant added to the denominator for numerical stability. Must be greater than 0. Default: `1e-8` |\n| sparse_counter | (`bool`) | If True, Expectigrad's counter increments only where the gradient is nonzero. If False, the counter increments unconditionally. Default: `True` |\n\n---\n\n### Tensorflow 1.x\n\n```python\nimport expectigrad\n\nexpectigrad.tensorflow1.ExpectigradOptimizer(\n    learning_rate=0.001, beta=0.9, epsilon=1e-8, sparse_counter=True,\n    use_locking=False, name='Expectigrad'\n)\n```\n\n| Args | | |\n| --- | :-: | --- |\n| learning_rate | | The learning rate, a scale factor applied to each optimizer step. Can be a float, `tf.keras.optimizers.schedules.LearningRateSchedule`, `Tensor`, or callable that takes no arguments and returns the value to use. Default: `0.001` |\n| beta | (`float`) | The decay rate for Expectigrad's bias-corrected, \"outer\" momentum. Must be in the interval [0, 1). Default: `0.9` |\n| epsilon | (`float`) | A small constant added to the denominator for numerical stability. Must be greater than 0. Default: `1e-8` |\n| sparse_counter | (`bool`) | If True, Expectigrad's counter increments only where the gradient is nonzero. If False, the counter increments unconditionally. Default: `True` |\n| use_locking | (`bool`) | If True, apply use locks to prevent concurrent updates to variables. Default: `False` |\n| name | (`str`) | Optional name for the operations created when applying gradients. Default: `'Expectigrad'` |\n\n---\n\n### Tensorflow 2.x\n\n```python\nimport expectigrad\n\nexpectigrad.tensorflow2.Expectigrad(\n    learning_rate=0.001, beta=0.9, epsilon=1e-8, name='Expectigrad', **kwargs\n)\n```\n\n| Args | | |\n| --- | :-: | --- |\n| learning_rate | | The learning rate, a scale factor applied to each optimizer step. Can be a float, `tf.keras.optimizers.schedules.LearningRateSchedule`, `Tensor`, or callable that takes no arguments and returns the value to use. Default: `0.001` |\n| beta | (`float`) | The decay rate for Expectigrad's bias-corrected, \"outer\" momentum. Must be in the interval [0, 1). Default: `0.9` |\n| epsilon | (`float`) | A small constant added to the denominator for numerical stability. Must be greater than 0. Default: `1e-8` |\n| sparse_counter | (`bool`) | If True, Expectigrad's counter increments only where the gradient is nonzero. If False, the counter increments unconditionally. Default: `True`\n| name | (`str`) | Optional name for the operations created when applying gradients. Default: `'Expectigrad'` |\n| **kwargs | | Keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`, `decay`}. `clipnorm` is gradient clipping by norm; `clipvalue` is gradient clipping by value; `decay` is included for backward compatibility to allow time inverse decay of learning rate; `lr` is included for backward compatibility, recommended to use `learning_rate` instead. |\n\n---\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/brett-daley/expectigrad",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "expectigrad",
    "package_url": "https://pypi.org/project/expectigrad/",
    "platform": "",
    "project_url": "https://pypi.org/project/expectigrad/",
    "project_urls": {
      "Homepage": "https://github.com/brett-daley/expectigrad"
    },
    "release_url": "https://pypi.org/project/expectigrad/0.0.0/",
    "requires_dist": null,
    "requires_python": ">=3.5",
    "summary": "Expectigrad optimizer for Pytorch and TensorFlow",
    "version": "0.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8591609,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "618fa4fc509f262a0874d87804cbd3035ae2cb80ffffc8a0c3c2edcceefd2e67",
          "md5": "a834f395f6b0b0ebb86d3f4b6dd6f282",
          "sha256": "5c48863930eccb809c722d600b208c270a82e2c1a1a5a84278f85e60cc8e8d6b"
        },
        "downloads": -1,
        "filename": "expectigrad-0.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a834f395f6b0b0ebb86d3f4b6dd6f282",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 11356,
        "upload_time": "2020-11-05T22:22:11",
        "upload_time_iso_8601": "2020-11-05T22:22:11.055043Z",
        "url": "https://files.pythonhosted.org/packages/61/8f/a4fc509f262a0874d87804cbd3035ae2cb80ffffc8a0c3c2edcceefd2e67/expectigrad-0.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2cb9f5a339150330ada29becc68861b076fe36821019a47e547a077dacded0d0",
          "md5": "ec817b5c259f4d20304090f9d7b5d571",
          "sha256": "f98986b9b652b5fea267bfb84e7721abbf6be1a90001581659ef320d4a02b094"
        },
        "downloads": -1,
        "filename": "expectigrad-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ec817b5c259f4d20304090f9d7b5d571",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 8487,
        "upload_time": "2020-11-05T22:22:13",
        "upload_time_iso_8601": "2020-11-05T22:22:13.983922Z",
        "url": "https://files.pythonhosted.org/packages/2c/b9/f5a339150330ada29becc68861b076fe36821019a47e547a077dacded0d0/expectigrad-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "618fa4fc509f262a0874d87804cbd3035ae2cb80ffffc8a0c3c2edcceefd2e67",
        "md5": "a834f395f6b0b0ebb86d3f4b6dd6f282",
        "sha256": "5c48863930eccb809c722d600b208c270a82e2c1a1a5a84278f85e60cc8e8d6b"
      },
      "downloads": -1,
      "filename": "expectigrad-0.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a834f395f6b0b0ebb86d3f4b6dd6f282",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5",
      "size": 11356,
      "upload_time": "2020-11-05T22:22:11",
      "upload_time_iso_8601": "2020-11-05T22:22:11.055043Z",
      "url": "https://files.pythonhosted.org/packages/61/8f/a4fc509f262a0874d87804cbd3035ae2cb80ffffc8a0c3c2edcceefd2e67/expectigrad-0.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2cb9f5a339150330ada29becc68861b076fe36821019a47e547a077dacded0d0",
        "md5": "ec817b5c259f4d20304090f9d7b5d571",
        "sha256": "f98986b9b652b5fea267bfb84e7721abbf6be1a90001581659ef320d4a02b094"
      },
      "downloads": -1,
      "filename": "expectigrad-0.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ec817b5c259f4d20304090f9d7b5d571",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 8487,
      "upload_time": "2020-11-05T22:22:13",
      "upload_time_iso_8601": "2020-11-05T22:22:13.983922Z",
      "url": "https://files.pythonhosted.org/packages/2c/b9/f5a339150330ada29becc68861b076fe36821019a47e547a077dacded0d0/expectigrad-0.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}