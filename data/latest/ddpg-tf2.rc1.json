{
  "info": {
    "author": "Samuel Matthew Koesnadi",
    "author_email": "samuelmat19@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Deep Deterministic Policy Gradient (DDPG) in Tensorflow 2\n\n![python 3](https://img.shields.io/badge/python-3-blue.svg)\n![tensorflow 2](https://img.shields.io/badge/tensorflow-2-orange.svg)\n\nLooking at Reinforcement Learning, there are two kinds of action space, namely discrete and continuous. The continuous action space represents the continuous movement a robot can have when actuating. I was bias towards the continuous one at the time of having the idea to write the DDPG implementation. I find that the continuous one can provide a smoother movement, which may be benefitial to control robotic actuator. DDPG is an approach to do so. The source code is available here https://github.com/samuelmat19/DDPG-tf2\n\nMy implementation of DDPG based on paper https://arxiv.org/abs/1509.02971, but also highly inspired by https://spinningup.openai.com/en/latest/algorithms/ddpg.html . This implementation is simple and can be used \nas a boilerplate for your need. It also modifies a bit the original algorithm which mainly aims to speed up the training\nprocess. I would highly recommend to use Spinning Up library as it provides more algorithm options. This repository is suitable if direct modification to Tensorflow 2 model or simple training API is favorable.\n\nSeveral videos of proof-of-concepts are as such:\n- [AI learns how to invert pendulum under 8 minutes](https://youtu.be/lY99ye4hhok)\n- [AI controls the Lunar Lander on Open AI Gym](https://youtu.be/-FMuvFVskBM)\n- [AI speed walks on Open AI Gym's BipedalWalker-v3](https://youtu.be/B95WjH4EP9I)\n\n##### Table of Contents  \n- [Why?](#why)  \n- [Changes from original paper](#changes-from-original-paper)\n- [Requirements](#requirements)\n- [Training](#training)\n- [Sampling](#sampling)\n- [Future improvements](#future-improvements)\n- [CONTRIBUTING](#contributing)\n- [LICENSE](#license)\n\n## Why?\nReinforcement learning is important when it comes to real environment. As\nthere is no definite right way to achieve a goal, the AI can be optimized based\non reward function instead of continuously supervised by human.\n\nIn continuous action space, DDPG algorithm shines as one of the best in\nthe field. In contrast to discrete action space, \ncontinuous action space mimics the reality of the world.\n\nThe original implementation is in PyTorch. Additionally, there are several\nmodifications of the original algorithm that may improve it.\n\n## Changes from original paper\nAs mentioned above, there are several changes with different aims:\n- The loss function of Q-function uses **Mean Absolute Error** instead of Mean\nSquared Error. After experimenting, this speeds up training by \na lot of margin. One possible cause is because Mean Squared Error may\noverestimate value above one and underestime value below one (x^2 function).\nThis might be unfavorable for the Q-function update as all value range should\nbe treated similarly.\n- **Epsilon-greedy** is implemented in addition to the policy's action. This\nincreases faster exploration. Sometimes the agent can stuck with one policy's\naction, this can be exited with random policy action introduced by epsilon-greedy.\nAs DDPG is off-policy, this surely is fine. The epsilon-greedy and noise are turned off in the testing state.\n- **Unbalance replay buffer**. Recent entries in the replay buffer are more likely to be taken\nthan the earlier ones. This reduces repetitive current mistakes that the agent\ndoes.\n\n## Requirements\n`pip3 install ddpg-tf2`\n\n## Training\n\n```python3\nddpg-tf2 --train True --use-noise True\n```\n\nAfter every epoch, the network's weights will be stored in the checkpoints directory defined in `common_definitions.py`.\nThere are 4 weights files that represent each networks, namely critic network,\nactor network, target critic, and target actor. \nAdditionally, TensorBoard is used to track the resultive losses and rewards.\n\nThe pretrained weights can be retrieved from these links:\n- [BipedalWalker-v3](https://github.com/samuelmat19/DDPG-tf2/releases/download/0.0.1/Bipedal_checkpoints.zip)\n- [LunarLanderContinuous-v2](https://github.com/samuelmat19/DDPG-tf2/releases/download/0.0.2/Lunar_checkpoints.zip)\n\n## Testing (Sampling)\n\nTesting is done by the similar executable, but with\nspecific parameters as such. If the weight is available in the checkpoint folder, it will load the weight automatically from there.\n\n```python3\nddpg-tf2 --train False --use-noise False\n```\n\n## Future improvements\n- [ ] Improve documentation\n- [x] GitHub Workflow\n- [x] Publish to PyPI\n\n## CONTRIBUTING\nTo contribute to the project, these steps can be followed. Anyone that contributes will surely be recognized and mentioned here!\n\nContributions to the project are made using the \"Fork & Pull\" model. The typical steps would be:\n\n1. create an account on [github](https://github.com)\n2. fork this repository\n3. make a local clone\n4. make changes on the local copy\n5. commit changes `git commit -m \"my message\"`\n6. `push` to your GitHub account: `git push origin`\n7. create a Pull Request (PR) from your GitHub fork\n(go to your fork's webpage and click on \"Pull Request.\"\nYou can then add a message to describe your proposal.)\n\n\n## LICENSE\nThis open-source project is licensed under MIT License.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ddpg-tf2",
    "package_url": "https://pypi.org/project/ddpg-tf2/",
    "platform": null,
    "project_url": "https://pypi.org/project/ddpg-tf2/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/ddpg-tf2/1.0.1/",
    "requires_dist": [
      "tensorflow-gpu (==2.7.2)",
      "tqdm (==4.37.0)",
      "gym (==0.17.2)",
      "box2d-py (==2.3.8)",
      "matplotlib (==3.1.1)",
      "pytest (==7.1.2)",
      "bandit (==1.7.4)",
      "pylint (==2.14.4)"
    ],
    "requires_python": "",
    "summary": "Simple yet effective DDPG implementation for continuous action space,",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14469095,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "94ff81ae0f4ff21cebb5ab7737c3ad828a82cf2eba8f36cfe8e1f1f99f1f54ea",
          "md5": "cf97762844f8720f63f04ec8ad82092a",
          "sha256": "e325b931feed19a97a89ff020bef51fe7c588e6c14721ef440554969a5439ccd"
        },
        "downloads": -1,
        "filename": "ddpg_tf2-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cf97762844f8720f63f04ec8ad82092a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12277,
        "upload_time": "2022-07-18T12:48:34",
        "upload_time_iso_8601": "2022-07-18T12:48:34.782549Z",
        "url": "https://files.pythonhosted.org/packages/94/ff/81ae0f4ff21cebb5ab7737c3ad828a82cf2eba8f36cfe8e1f1f99f1f54ea/ddpg_tf2-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3329b39f32129cf4686aaf72f257fc8122f5ba87406cb62a3960fdc5cd440d23",
          "md5": "954ba8af5d47dbadb17434d0c0d33331",
          "sha256": "45ca3f4a8f16cd2afad67b3c0ef8c3f6919401098135258da214ee6a50aa8ca9"
        },
        "downloads": -1,
        "filename": "ddpg-tf2-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "954ba8af5d47dbadb17434d0c0d33331",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10599,
        "upload_time": "2022-07-18T12:48:35",
        "upload_time_iso_8601": "2022-07-18T12:48:35.972335Z",
        "url": "https://files.pythonhosted.org/packages/33/29/b39f32129cf4686aaf72f257fc8122f5ba87406cb62a3960fdc5cd440d23/ddpg-tf2-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "94ff81ae0f4ff21cebb5ab7737c3ad828a82cf2eba8f36cfe8e1f1f99f1f54ea",
        "md5": "cf97762844f8720f63f04ec8ad82092a",
        "sha256": "e325b931feed19a97a89ff020bef51fe7c588e6c14721ef440554969a5439ccd"
      },
      "downloads": -1,
      "filename": "ddpg_tf2-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "cf97762844f8720f63f04ec8ad82092a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 12277,
      "upload_time": "2022-07-18T12:48:34",
      "upload_time_iso_8601": "2022-07-18T12:48:34.782549Z",
      "url": "https://files.pythonhosted.org/packages/94/ff/81ae0f4ff21cebb5ab7737c3ad828a82cf2eba8f36cfe8e1f1f99f1f54ea/ddpg_tf2-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3329b39f32129cf4686aaf72f257fc8122f5ba87406cb62a3960fdc5cd440d23",
        "md5": "954ba8af5d47dbadb17434d0c0d33331",
        "sha256": "45ca3f4a8f16cd2afad67b3c0ef8c3f6919401098135258da214ee6a50aa8ca9"
      },
      "downloads": -1,
      "filename": "ddpg-tf2-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "954ba8af5d47dbadb17434d0c0d33331",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 10599,
      "upload_time": "2022-07-18T12:48:35",
      "upload_time_iso_8601": "2022-07-18T12:48:35.972335Z",
      "url": "https://files.pythonhosted.org/packages/33/29/b39f32129cf4686aaf72f257fc8122f5ba87406cb62a3960fdc5cd440d23/ddpg-tf2-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}