{
  "info": {
    "author": "Saket Joshi, Nikhil Nayak, Harsha Taneru, Nishtha Sardana, Kareema Batool,",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "[![.github/workflows/test.yml](https://code.harvard.edu/CS107/team32/actions/workflows/test.yml/badge.svg)](https://code.harvard.edu/CS107/team32/actions/workflows/test.yml)\n[![.github/workflows/coverage.yml](https://code.harvard.edu/CS107/team32/actions/workflows/coverage.yml/badge.svg)](https://code.harvard.edu/CS107/team32/actions/workflows/coverage.yml)\n\n# team32\nCS107/AC207 Project\n\n# Introduction\nDifferentiation is defined as the process of finding the gradients/derivatives of a particular function in hand. It finds multiple applications in the areas of science and engineering. With the exponential growth in the size of the dataset and advancements in technologies - the complexity of computing derivatives has increased and we have become increasingly dependent on computers to compute derivatives.  \n\nCurrently, there are three ways to compute derivatives - finite, symbolic, automatic differentiation. The finite differentiation method although being quick and easy to implement - suffers from machine precision and rounding errors. We are able to alleviate these issues using symbolic differentiation, however, it becomes computationally very expensive as the function(s) starts to get complex. We are able to alleviate both the issues of computational complexity and machine precision using automatic differentiation.  \n\nAutomatic Differentiation leverages symbolic rules for evaluating gradients - which is more accurate than using finite difference approximations. But unlike a purely symbolic process, the evaluation of expressions takes place early in the computation - it evaluates derivatives at particular numeric values.  \n\nThe package `fab-ad` implements automatic differentiation for computational use. `fab-ad` can be used to automatically differentiate functions via forward mode. Automatic Differentiation finds applications in optimization, machine learning, and numerical methods.  \n\n# Installation\nOur package is for Python 3 only. You can access our package by cloning \nour repository. To clone run our repository, run `git clone \ngit@code.harvard.edu:CS107/team32.git` from command line. Once you clone \nthe repository you can use `cd team32` where you can find all the files. \nFrom there, you can use `cd src` to go where all the modules reside. Then \nuse `python -m pip install toml` which will install all the requirements \nspecified in toml.\n\n## 3. How To Use `fab_AD`?\n\n### 3.1 Package Installation, testing and import\n#### 3.1.1 Installation from the source\nOur package is for Python 3 only. You can access our package by cloning our repository. \n\n- To clone run our repository, run `git clone git@code.harvard.edu:CS107/team32.git` from command line. \n- Once you clone the repository you can use `cd team32` where you can find all the files. \n- From there, you can use `cd src` to go where all the modules reside. \n- Then use `python -m pip install poetry` and `poetry install` which will install all the requirements specified in toml.\n- Run `poetry shell` to activate new virtual env.\n```bash\ngit clone git@code.harvard.edu:CS107/team32.git\ncd team32\npython3 -m pip install poetry\npoetry install\npoetry shell\n```\n\n#### 3.1.2 Installation via PyPI\n\nfab_AD is available at (https://test.pypi.org/simple/ fab-ad). You can download it by the command given below.\n```bash\npython3 -m venv test\nsource test/bin/activate\npip install numpy==1.23.5\npip install -i https://test.pypi.org/simple/ fab-ad\n```\n\n # User Guide\nOnce you install the package, you can simple import it by `from fab_ad \nimport FabTensor`.\nAfterwards, you could initiate the FabTensor object by giving the point \nwhere you wish to differentiate. FabTensor can take in a vector input \nvalues, representing a point's coordinates in multi-dimensional space. \nMoreover, you could also add other supplementary features as in the code \ndemo provided below. You can find this demo file by the name of usage.py \nunder src in the github.\n\n#### Usage: Forward Mode AD\n\n```python \nfrom fab_ad.fab_ad_tensor import FabTensor, AdMode\nfrom fab_ad.fab_ad_session import fab_ad_session\nfrom fab_ad.fab_ad_diff import auto_diff\nfrom fab_ad.constants import *\n\n# multiple scalar input; single scalar output; forward ad\n\n# initialize the fab_ad session with number of input variables. if unsure, set num_inputs to a high number (defaults to 10)\nfab_ad_session.initialize()\n\n# define the input variables\nx = FabTensor(value=3, identifier=\"x\")\ny = FabTensor(value=-4, identifier=\"y\")\n\n# compute the output variable\nz = x ** 2 + 2 * y ** 2\n\n# compute the gradient of the output variable with respect to the input variables\nresult = auto_diff(z, mode=AdMode.FORWARD)\n\nassert result.value == 41\nassert all(result.gradient == np.array([6, -16]))\nprint(result)\n# Function 0: Value: 41\n# Gradient w.r.t x = 6.0\n# Gradient w.r.t y = -16.0\n```\n\n#### Usage: Reverse Mode AD\n\n```python\nfrom fab_ad.fab_ad_tensor import FabTensor, AdMode\nfrom fab_ad.fab_ad_session import fab_ad_session\nfrom fab_ad.fab_ad_diff import auto_diff\nfrom fab_ad.constants import *\n\n# Multiple scalar input; scalar output; reverse ad\n# initialize fab_ad session with number of input variables. if unsure, set num_inputs to a high number\nfab_ad_session.initialize()\n# initialize input variables\nx = FabTensor(value=3, identifier=\"x\")\ny = FabTensor(value=-4, identifier=\"y\")\n# compute output variable\nz = x ** 2 + 2 * y ** 2\n# compute gradient of output variable with respect to input variables via reverse mode AD\nresult = auto_diff(z, mode=AdMode.REVERSE)\n\nassert result.value == 41\nassert all(result.gradient == np.array([6, -16]))\nprint(result)\n# Function 0: Value: 41\n# Gradient w.r.t x = 6.0\n# Gradient w.r.t y = -16.0\n```\n\n#### Usage: Gradient Descent\n\n```python\nfrom fab_ad.fab_ad_tensor import FabTensor, AdMode\nfrom fab_ad.fab_ad_session import fab_ad_session\nfrom fab_ad.fab_ad_diff import auto_diff\nfrom fab_ad.constants import *\n\ndef function_derivative(x: FabTensor, y: FabTensor):\n    # compute output variable\n    z = x**2 + y**4\n    # compute gradient of output variable with respect to input variables\n    return auto_diff(output=z, mode=AdMode.FORWARD).gradient\n\ndef gradient_descent(\n    function_derivative, start, learn_rate, n_iter=10000, tolerance=1e-10\n):\n    # initialize the vector\n    vector = start\n    # initialize the fab_ad session with number of input variables. if unsure, set num_inputs to a high number\n    fab_ad_session.initialize()\n    # initialize the input variables\n    x = FabTensor(value=vector[0], identifier=\"x\")\n    y = FabTensor(value=vector[1], identifier=\"y\")\n    for i in range(n_iter):\n        # compute the gradient descent step\n        diff = -learn_rate * function_derivative(x, y)\n        if np.all(np.abs(diff) <= tolerance):\n            break\n        # update the vector\n        vector += diff\n        # update the input variables\n        x += diff[0]\n        y += diff[1]\n        \n        if (i%1000) == 0:\n            print(f\"iteration {i}: {vector}\")\n    \n    return vector\n\n\nstart = np.array([1.0, 1.0])\nprint(gradient_descent(function_derivative, start, 0.2, tolerance=1e-08).round(4))\n\n# iteration 0: [0.6 0.2]\n# iteration 1000: [8.49966157e-223 2.47685235e-002]\n# iteration 2000: [4.9406565e-324 1.7593023e-002]\n# iteration 3000: [4.94065646e-324 1.43868515e-002]\n# iteration 4000: [4.94065646e-324 1.24691641e-002]\n# iteration 5000: [4.9406565e-324 1.1158074e-002]\n# iteration 6000: [4.94065646e-324 1.01891453e-002]\n# iteration 7000: [4.94065646e-324 9.43548979e-003]\n# iteration 8000: [4.94065646e-324 8.82762731e-003]\n# iteration 9000: [4.94065646e-324 8.32389824e-003]\n# [0.     0.0079]\n```\n\n#### Usage: Newton-Raphson\n\n```python\nimport os\nimport sys\nimport numpy as np\n\nfrom fab_ad.constants import *\nfrom fab_ad.fab_ad_tensor import FabTensor, AdMode\nfrom fab_ad.fab_ad_session import fab_ad_session\nfrom fab_ad.fab_ad_diff import auto_diff\n\n# Function to find the root\ndef newtonRaphson(x):\n    \n    def func(x):\n        # compute output variable and return value\n        z = x * x * x - x * x + 2\n        return auto_diff(output=z, mode=AdMode.FORWARD).value\n    \n    def derivFunc(x):\n        # compute output variable and return gradient\n        z = x * x * x - x * x + 2\n        return auto_diff(output=z, mode=AdMode.FORWARD).gradient\n    \n    # initialize the fab_ad session with number of input variables. if unsure, set num_inputs to a high number\n    fab_ad_session.initialize()\n    tensor = FabTensor(value=x, identifier=\"x\")\n    h = func(tensor) / derivFunc(tensor)\n    while True:\n        if isinstance(h, float):\n            if abs(h) < 0.0001:\n                break\n        else:\n            if max(abs(h)) < 0.0001:\n                break\n        # x(i+1) = x(i) - f(x) / f'(x)\n        x = x - h\n        tensor = tensor - h\n        h = func(tensor) / derivFunc(tensor)\n    print(\"The value of the root is : \", x)\n\n\n# Driver program to test above\nx0 = -20.00 # Initial values assumed\nnewtonRaphson(x0)\n# The value of the root is :  -1.0000001181322415\n\nx0 = [-10.00, 10.00] # Initial values assumed\nnewtonRaphson(x0)\n# The value of the root is :  [-1.         -1.00000001]\n```\n\n# Documentation\nYou can find the documentation for this package [here](https://code.harvard.edu/pages/CS107/team32/).\n\nWe use `numpydoc==1.5.0`  and `sphinx==4.2` for documentation.\n\n# Future Features\nAs part of the extension for this project we plan on implementing higher order and mixed derivatives of vector functions. Specifically we will implement the Hessian matrix which includes the derivative of a function twice w.r.t each independent variable (similar to the Laplacian operator) and mixed derivatives (i.e. derivative w.r.t one independent variable followed by another independent variable). Currently, we have implemented only the first derivative of a function w.r.t all the seed vectors/independent variables.\n\n### Changes in software\nCurrently the derivative data member of FabTensor object is a 1D array. This would change to 2D array to accommodate the Hessian matrix. Also, we would have to update each of our elementary math functions to return the second derivatives by taking the derivative of the first derivative. The main challenge here will be to efficiently compute the second derivatives from the first derivatives by reusing most of the code already written and using a helper function to call the same derivative function twice.\n\n### Changes in directory structure, new modules, classes, data structures, etc\nDirectory structure would remain the same. Modules and classes would also remain the same because we don't need a new class to implement the second derivative. We would need a new method in the fab_ad class to call the derivative for the second time (derivative of the first derivative). Also, we would have to update each of our elementary math functions to return the second derivatives. Data structure of the derivative member of the FabTensor class would change from being a 1D array to 2D array to store all the second derivatives i.e. the Hessian matrix.\n\nHigher order and mixed derivatives, specifically second order derivatives can then be used to determine the concavity, convexity, the points of inflection, and local extrema of functions. As pointed out earlier it can also be used to compute the Laplacian operator which is useful in describing many different phenomena, from electric potentials, to the diffusion equation for heat and fluid flow, and quantum mechanics.\n\n# Broader Impact, Inclusivity and Future directions\nWe hope that our package will be used in a variety of disciplines, \nincluding physics, engineering, applied mathematics, astronomy, and even \ndomains that the package's creators could never have envisaged. We believe \nthat this package can be used to perform automatic differentiations \naccurately and effectively as well as serve as a model for the future \ncreation of improved automatic differentiation packages. We see several \nopportunities for this package to be improved and would be pleased to see \nthem realized.\n\nOn the other hand, we don't want to see this program being used as a \nshortcut for differentiating work or for plagiarism or cheating. This \npackage's open-source nature makes it available to everyone, but it also \nleaves it vulnerable to those who intend to use it for plagiarism. Users \nshould be mindful of this nature and pick their method of using this \npackage carefully. This software is not intended to be used as a shortcut \nfor differentiation procedures. It could be used to check results for \nderivative calculations done manually or using other algorithms, although \nderivative calculation techniques should still be used instead. These \nexercises serve a purpose, and using this software to find the solutions \ndoes not advance learning.\n\nAlso evident is the connection between our automatic differentiation \nalgorithms and mathematical concepts like the Leibniz Rule and the Faa di \nBruno Formula that we made while working on this project. It shouldn't be \nthe first time that these formulas have been employed to compute \nhigher-order derivatives, but it provided motivation for us to carry out \nthe implementation ourselves. In order to close the gap between theories \nand applications, we want for our project to serve as a case study. This \nexperience demonstrates that now is the perfect time for all types of \ninformation to come together in order to support new discoveries, which \nwill enable us and many students to continue working toward this aim.\n\n# Software Inclusivity\nUsers and collaborators from all origins and identities are welcome to the \nfab_adÂ package and its developer. As was seen during the creation of this \npackage, we think that trust, respect, and care are the foundations of \ngreatness in a collaborative endeavor. In an effort to reach as many \npeople as possible who are interested in this package, we made every \neffort to make it as inclusive and user-friendly as possible by including \nthe necessary documentation and instructions. Although this package was \ncreated using Python and English, anyone who is proficient in another \nlanguage or programming language is welcome to contribute. Pull requests \nare examined and approved by all developers while this package is being \ndeveloped.\n\nEvery time one of us feels the need to start a pull request, that person \nwould talk to the other members and come to a consensus. We would adore \nthe opportunity to carry this constructive dialogue into other \ncollaborations with this package.\n\n\n\n<!-- ## Development environment setup\n\n1. Create virtual environment\n\n2. Use poetry to install Dependencies\n```bash\npython3 -m pip install poetry\npoetry install\n``` -->\n\n### Running tests\n<TO DO>\nAll tests can be run for the repository using below command:\n\n```\npython -m pytest tests/\n```\n\n\n### Making documentation\n```\nPopulate your master file team32/index.rst and create other documentation source files. Use the Makefile to build the docs, like so:\n   make builder\nwhere \"builder\" is one of the supported builders, e.g. html, latex or linkcheck.\n\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "fab-ad",
    "package_url": "https://pypi.org/project/fab-ad/",
    "platform": null,
    "project_url": "https://pypi.org/project/fab-ad/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/fab-ad/1.0.7/",
    "requires_dist": [
      "poetry-core (>=1.0.0)",
      "setuptools",
      "wheel",
      "numpy"
    ],
    "requires_python": ">=3.8",
    "summary": "Fab-AD is a Python package for automatic differentiation.",
    "version": "1.0.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16070559,
  "releases": {
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dfd460ac6094d2e7b894ceb29468990de544935a50342d7b5c510d97a850a204",
          "md5": "0572f377abf5ee406903e3e32cb1c53f",
          "sha256": "e5f9443b5cbb082ecccc08f2f876efe1e932ae482482d68b7c450aeb8a8395dd"
        },
        "downloads": -1,
        "filename": "fab_ad-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0572f377abf5ee406903e3e32cb1c53f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 5821,
        "upload_time": "2022-12-11T21:03:50",
        "upload_time_iso_8601": "2022-12-11T21:03:50.983464Z",
        "url": "https://files.pythonhosted.org/packages/df/d4/60ac6094d2e7b894ceb29468990de544935a50342d7b5c510d97a850a204/fab_ad-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8ed240be84d080835f532f67a066715c4ff6d5ab73b22ba1a536c60967668094",
          "md5": "55494748d62d2e533875a084449a4ca7",
          "sha256": "23a389fdb5e2d11c30cf4aeb7bf6b4eb2ece338af0fcab3030ad4054881d70e5"
        },
        "downloads": -1,
        "filename": "fab-ad-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "55494748d62d2e533875a084449a4ca7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 6736,
        "upload_time": "2022-12-11T21:03:53",
        "upload_time_iso_8601": "2022-12-11T21:03:53.756535Z",
        "url": "https://files.pythonhosted.org/packages/8e/d2/40be84d080835f532f67a066715c4ff6d5ab73b22ba1a536c60967668094/fab-ad-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "087db572307679de0ae0e6b5e710cd59e33850be51ec0c816c3cd2a9f5e6dad3",
          "md5": "2a3fae42021b772d089aaaff63bd6d82",
          "sha256": "745ceb7dc4264e48991155ff3e93fd4b8c60bedbe33494d0bfed62c45881840e"
        },
        "downloads": -1,
        "filename": "fab_ad-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2a3fae42021b772d089aaaff63bd6d82",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 5823,
        "upload_time": "2022-12-11T21:35:32",
        "upload_time_iso_8601": "2022-12-11T21:35:32.253969Z",
        "url": "https://files.pythonhosted.org/packages/08/7d/b572307679de0ae0e6b5e710cd59e33850be51ec0c816c3cd2a9f5e6dad3/fab_ad-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1fffc872c568a9fd7ee7cf4b3ef08f325025d0e742d96e71339d4953d4cba5ed",
          "md5": "5fe1177e7fc5987c5a28d7b1244bcc1c",
          "sha256": "55aaf4ff79898f1cde6d94a296b4da67b2850cbe41ffeb1a5634ad87ae492ee4"
        },
        "downloads": -1,
        "filename": "fab-ad-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "5fe1177e7fc5987c5a28d7b1244bcc1c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 6717,
        "upload_time": "2022-12-11T21:35:33",
        "upload_time_iso_8601": "2022-12-11T21:35:33.682530Z",
        "url": "https://files.pythonhosted.org/packages/1f/ff/c872c568a9fd7ee7cf4b3ef08f325025d0e742d96e71339d4953d4cba5ed/fab-ad-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "33ab88e4ddb170f519fbce4434ca73123de57e50db5727264f5212f354a73f80",
          "md5": "88929707745dbd7dba8f078bc63b675a",
          "sha256": "02ac60bb6ec6b75c075397d7af45542f75f0f16163979073546c12d3fec80998"
        },
        "downloads": -1,
        "filename": "fab_ad-1.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "88929707745dbd7dba8f078bc63b675a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 13984,
        "upload_time": "2022-12-12T06:10:11",
        "upload_time_iso_8601": "2022-12-12T06:10:11.232171Z",
        "url": "https://files.pythonhosted.org/packages/33/ab/88e4ddb170f519fbce4434ca73123de57e50db5727264f5212f354a73f80/fab_ad-1.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3b000734925dd61abc6bb35b0aa41780028f895f37537f51881fdc0a23a959cc",
          "md5": "5e3d9d1e3c082ca10a82ceeecd757a07",
          "sha256": "45db864e0334a9e6213654f9f5fb709424613128ca002f79b9cc4843d067b75a"
        },
        "downloads": -1,
        "filename": "fab-ad-1.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "5e3d9d1e3c082ca10a82ceeecd757a07",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 18470,
        "upload_time": "2022-12-12T06:10:14",
        "upload_time_iso_8601": "2022-12-12T06:10:14.166912Z",
        "url": "https://files.pythonhosted.org/packages/3b/00/0734925dd61abc6bb35b0aa41780028f895f37537f51881fdc0a23a959cc/fab-ad-1.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "33ab88e4ddb170f519fbce4434ca73123de57e50db5727264f5212f354a73f80",
        "md5": "88929707745dbd7dba8f078bc63b675a",
        "sha256": "02ac60bb6ec6b75c075397d7af45542f75f0f16163979073546c12d3fec80998"
      },
      "downloads": -1,
      "filename": "fab_ad-1.0.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "88929707745dbd7dba8f078bc63b675a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 13984,
      "upload_time": "2022-12-12T06:10:11",
      "upload_time_iso_8601": "2022-12-12T06:10:11.232171Z",
      "url": "https://files.pythonhosted.org/packages/33/ab/88e4ddb170f519fbce4434ca73123de57e50db5727264f5212f354a73f80/fab_ad-1.0.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3b000734925dd61abc6bb35b0aa41780028f895f37537f51881fdc0a23a959cc",
        "md5": "5e3d9d1e3c082ca10a82ceeecd757a07",
        "sha256": "45db864e0334a9e6213654f9f5fb709424613128ca002f79b9cc4843d067b75a"
      },
      "downloads": -1,
      "filename": "fab-ad-1.0.7.tar.gz",
      "has_sig": false,
      "md5_digest": "5e3d9d1e3c082ca10a82ceeecd757a07",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 18470,
      "upload_time": "2022-12-12T06:10:14",
      "upload_time_iso_8601": "2022-12-12T06:10:14.166912Z",
      "url": "https://files.pythonhosted.org/packages/3b/00/0734925dd61abc6bb35b0aa41780028f895f37537f51881fdc0a23a959cc/fab-ad-1.0.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}