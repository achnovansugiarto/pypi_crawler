{
  "info": {
    "author": "jackwardell",
    "author_email": "jack@wardell.xyz",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# SimpleCrawler\n* This web crawler can be used to crawl a website from the command line or code\n\n# Install\n* Python required (3.6+) https://www.python.org/downloads/\n* `pip install SimpleCrawler`\n\nOR\n\n* `git clone https://github.com/jackwardell/SimpleCrawler.git`\n* `cd SimpleCrawler`\n* `python3 -m venv venv`\n* `source venv/bin/activate`\n* `pip install --upgrade pip`\n* `pip install -r requirements.txt`\n* `pip install -e .`\n* `pytest`\n* `crawl https://www.example.com`\n\n# Rules:\nThis crawler will:\n* Only crawl text/html mime-types\n* Only crawl pages that return 200 OK HTTP statuses\n* Look at /robots.txt and obey by default (but can be overridden)\n* Add User-Agent, default value = PyWebCrawler (but can be changed)\n* Ignore ?query=strings and #fragments by default (but can be changed)\n* Get links from ONLY href value in <a href='/some-link'>click here</a> tags\n\nTodo:\n* Nicer logging\n* Crawl client errors and server error pages? (Most websites have 404 & 500 handlers which may have links)\n* Parse more than just <a> tags and href attrs e.g. src='/some-link'\n* Add a scheduler / wait frequency (Politeness)\n* Request timeout\n\n\n# Use\n* just type `crawl <url>` into your command line e.g. `crawl https://www.google.com`\n\n```\n$ crawl --help\nUsage: crawl [OPTIONS] URL\n\nOptions:\n  -u, --user-agent TEXT\n  -w, --max-workers INTEGER\n  -t, --timeout INTEGER\n  -h, --check-head\n  -d, --disobey-robots\n  -wq, --with-query\n  -wf, --with-fragment\n  --debug / --no-debug\n  --help                     Show this message and exit.\n```\n\n* optional params:\n    - \"--user-agent\" or \"-u\"\n        - what the User-Agent header param is\n        - default = 'PyWebCrawler'\n    - \"--max-workers\" or \"-w\"\n        - max number of worker threads\n        - default = 1\n    - \"--timeout\" or \"-t\"\n        - how long to wait for new items from work queue before shutting down\n        - default = 10\n    - \"--check-head\" or \"-t\"\n        - whether to send HEAD request before sending GET request\n        - why? some GET responses will be large (e.g. pdf) and sending HEAD request first will allow crawler to see MIME type before it makes a GET request, therefore averting the GET request altogether\n        - default = False\n    - \"--disobey-robots\" or \"-d\"\n        - whether to disobey robots.txt file\n        - default = False\n    - \"--with-query\" or \"-wq\"\n        - whether to allow query args e.g. https://www.example.com/?hello=world -> https://www.example.com/ if not --with-query\n        - default = False\n    - \"--with-fragment\" or \"-wf\"\n        - whether to allow fragments e.g. https://www.example.com/#helloworld -> https://www.example.com/ if not --with-fragment\n        - default = False\n    - \"--debug/--no-debug\", default=False\n        - whether to run the crawl, if debug on, then it wont crawl but will pump out crawler config\n        - default = False\n\n\nOR from code\n\n```\nfrom simple_crawler import Crawler\n\ncrawler = Crawler()\nfound_links = crawler.crawl('https://www.example.com/')\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/jackwardell",
    "keywords": "python",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "SimpleCrawler",
    "package_url": "https://pypi.org/project/SimpleCrawler/",
    "platform": "",
    "project_url": "https://pypi.org/project/SimpleCrawler/",
    "project_urls": {
      "Homepage": "https://github.com/jackwardell"
    },
    "release_url": "https://pypi.org/project/SimpleCrawler/1.0.1/",
    "requires_dist": [
      "requests",
      "click"
    ],
    "requires_python": ">=3.6",
    "summary": "Enter something here",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9017594,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2b14c49b0ccffe186af289fbde8c9a00f0fb783994ffe092a5a546702778b7a6",
          "md5": "aca8d97c75d3d0de452b42ab1cc59f4b",
          "sha256": "60b30d6b06a42af664fec2d274e908e8a28482ed978152e1549b28ba095502e6"
        },
        "downloads": -1,
        "filename": "SimpleCrawler-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "aca8d97c75d3d0de452b42ab1cc59f4b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 20550,
        "upload_time": "2020-12-30T15:04:26",
        "upload_time_iso_8601": "2020-12-30T15:04:26.882636Z",
        "url": "https://files.pythonhosted.org/packages/2b/14/c49b0ccffe186af289fbde8c9a00f0fb783994ffe092a5a546702778b7a6/SimpleCrawler-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8208cf96c8370b2d0cdf31d6101101163930d51837dce6f0f23af16682c3ddb5",
          "md5": "30434fc9e20e77da76a3e41838ff36bd",
          "sha256": "dbb8dd58c49d6c37834c7dbf7962ff71048a1a8173265c1701d00673a7f3253c"
        },
        "downloads": -1,
        "filename": "SimpleCrawler-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "30434fc9e20e77da76a3e41838ff36bd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 16652,
        "upload_time": "2020-12-30T15:04:28",
        "upload_time_iso_8601": "2020-12-30T15:04:28.442351Z",
        "url": "https://files.pythonhosted.org/packages/82/08/cf96c8370b2d0cdf31d6101101163930d51837dce6f0f23af16682c3ddb5/SimpleCrawler-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7685db01dbf753d8e59e742625591556426a4818ef2168345213ab4d6dfaeb49",
          "md5": "4f6f418630652ee169f6bcd51d033c47",
          "sha256": "d6a666ee8dfd7bb7ebb6a08f83f5f78be2c3ff6d04557e5f7cf1ba9ce56b7bd0"
        },
        "downloads": -1,
        "filename": "SimpleCrawler-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4f6f418630652ee169f6bcd51d033c47",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 20832,
        "upload_time": "2020-12-30T15:41:27",
        "upload_time_iso_8601": "2020-12-30T15:41:27.834677Z",
        "url": "https://files.pythonhosted.org/packages/76/85/db01dbf753d8e59e742625591556426a4818ef2168345213ab4d6dfaeb49/SimpleCrawler-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a1b545885a387c35d841514fdd5e6f56bcaa60da8ba317adbeaf3f2450656e18",
          "md5": "89d4f6e347f7b7931f35fb23f0a81334",
          "sha256": "482097a8f8bad29720aeef01958a1d03a5ba61599f66f27307705d9d060bd6c2"
        },
        "downloads": -1,
        "filename": "SimpleCrawler-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "89d4f6e347f7b7931f35fb23f0a81334",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 16899,
        "upload_time": "2020-12-30T15:41:29",
        "upload_time_iso_8601": "2020-12-30T15:41:29.210675Z",
        "url": "https://files.pythonhosted.org/packages/a1/b5/45885a387c35d841514fdd5e6f56bcaa60da8ba317adbeaf3f2450656e18/SimpleCrawler-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7685db01dbf753d8e59e742625591556426a4818ef2168345213ab4d6dfaeb49",
        "md5": "4f6f418630652ee169f6bcd51d033c47",
        "sha256": "d6a666ee8dfd7bb7ebb6a08f83f5f78be2c3ff6d04557e5f7cf1ba9ce56b7bd0"
      },
      "downloads": -1,
      "filename": "SimpleCrawler-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4f6f418630652ee169f6bcd51d033c47",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 20832,
      "upload_time": "2020-12-30T15:41:27",
      "upload_time_iso_8601": "2020-12-30T15:41:27.834677Z",
      "url": "https://files.pythonhosted.org/packages/76/85/db01dbf753d8e59e742625591556426a4818ef2168345213ab4d6dfaeb49/SimpleCrawler-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a1b545885a387c35d841514fdd5e6f56bcaa60da8ba317adbeaf3f2450656e18",
        "md5": "89d4f6e347f7b7931f35fb23f0a81334",
        "sha256": "482097a8f8bad29720aeef01958a1d03a5ba61599f66f27307705d9d060bd6c2"
      },
      "downloads": -1,
      "filename": "SimpleCrawler-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "89d4f6e347f7b7931f35fb23f0a81334",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 16899,
      "upload_time": "2020-12-30T15:41:29",
      "upload_time_iso_8601": "2020-12-30T15:41:29.210675Z",
      "url": "https://files.pythonhosted.org/packages/a1/b5/45885a387c35d841514fdd5e6f56bcaa60da8ba317adbeaf3f2450656e18/SimpleCrawler-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}