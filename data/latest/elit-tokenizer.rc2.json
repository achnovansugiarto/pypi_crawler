{
  "info": {
    "author": "Jinho D. Choi",
    "author_email": "jinho.choi@emory.edu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# ELIT Tokenizer\n\n[ELIT](https://github.com/emorynlp/elit/) (Emory Information and Language Technology) features several tokenizers\nto split text into a sequence of tokens and segment them into sentences.\nThis project is led by the [Emory NLP Research Laboratory](https://www.emorynlp.edu) and under the [Apache 2.0](LICENSE) license.\n\n* Latest release: [1.0](https://pypi.org/project/elit_tokenizer/) (10/15/2021)\n\n## Installation\n\nPython 3.7 or higher is recommended: \n\n```\npip install elit_tokenizer\n```\n\n## Documentations\n\n* [Space Tokenizer](https://github.com/emorynlp/elit-tokenizer/blob/master/docs/SpaceTokenizer.md)\n* [English Tokenizer](https://github.com/emorynlp/elit-tokenizer/blob/master/docs/EnglishTokenizer.md)\n\n## Contact\n\n* [Jinho D. Choi](http://www.cs.emory.edu/~choi)\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/emorynlp/elit-tokenizer",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "elit-tokenizer",
    "package_url": "https://pypi.org/project/elit-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/elit-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/emorynlp/elit-tokenizer"
    },
    "release_url": "https://pypi.org/project/elit-tokenizer/1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "English Tokenizer from ELIT",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11740559,
  "releases": {
    "0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ba736923427c9d736eb03cfdd4755b8d41b47d466bd486e820d3a8f9675fc3b8",
          "md5": "2c4073e680305b7c2d8004fbd90d186a",
          "sha256": "81d6d6d4f4e48d378abab73758d7e3c6c5d73bf088fc53f4e36c5aa7cd2a5a67"
        },
        "downloads": -1,
        "filename": "elit_tokenizer-0.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2c4073e680305b7c2d8004fbd90d186a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 17187,
        "upload_time": "2021-10-15T22:57:07",
        "upload_time_iso_8601": "2021-10-15T22:57:07.124884Z",
        "url": "https://files.pythonhosted.org/packages/ba/73/6923427c9d736eb03cfdd4755b8d41b47d466bd486e820d3a8f9675fc3b8/elit_tokenizer-0.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "947548263a682d8772e32a06a46a1479d0240cf52c6de90919b6476a3530582b",
          "md5": "25087d4571b862f8d68c430649234a0b",
          "sha256": "084e6dc53892366814e68c3715a5b2b95b519a7a86fe60d74370d4adc65bdfe7"
        },
        "downloads": -1,
        "filename": "elit_tokenizer-1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "25087d4571b862f8d68c430649234a0b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 17188,
        "upload_time": "2021-10-15T23:00:58",
        "upload_time_iso_8601": "2021-10-15T23:00:58.264976Z",
        "url": "https://files.pythonhosted.org/packages/94/75/48263a682d8772e32a06a46a1479d0240cf52c6de90919b6476a3530582b/elit_tokenizer-1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "878fad54fd98a7399aa0fc3d49f329a9ce02ecc098d11c574f933bfcb0b5b08b",
          "md5": "84ed6e187670f2e81f2b638f8d5a7389",
          "sha256": "f1b80940fbf69e39050ce9ca4f1640dede9e5f395dc35db3d4007f309175ca16"
        },
        "downloads": -1,
        "filename": "elit_tokenizer-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "84ed6e187670f2e81f2b638f8d5a7389",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10829,
        "upload_time": "2021-10-15T23:01:00",
        "upload_time_iso_8601": "2021-10-15T23:01:00.656885Z",
        "url": "https://files.pythonhosted.org/packages/87/8f/ad54fd98a7399aa0fc3d49f329a9ce02ecc098d11c574f933bfcb0b5b08b/elit_tokenizer-1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "947548263a682d8772e32a06a46a1479d0240cf52c6de90919b6476a3530582b",
        "md5": "25087d4571b862f8d68c430649234a0b",
        "sha256": "084e6dc53892366814e68c3715a5b2b95b519a7a86fe60d74370d4adc65bdfe7"
      },
      "downloads": -1,
      "filename": "elit_tokenizer-1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "25087d4571b862f8d68c430649234a0b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 17188,
      "upload_time": "2021-10-15T23:00:58",
      "upload_time_iso_8601": "2021-10-15T23:00:58.264976Z",
      "url": "https://files.pythonhosted.org/packages/94/75/48263a682d8772e32a06a46a1479d0240cf52c6de90919b6476a3530582b/elit_tokenizer-1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "878fad54fd98a7399aa0fc3d49f329a9ce02ecc098d11c574f933bfcb0b5b08b",
        "md5": "84ed6e187670f2e81f2b638f8d5a7389",
        "sha256": "f1b80940fbf69e39050ce9ca4f1640dede9e5f395dc35db3d4007f309175ca16"
      },
      "downloads": -1,
      "filename": "elit_tokenizer-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "84ed6e187670f2e81f2b638f8d5a7389",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 10829,
      "upload_time": "2021-10-15T23:01:00",
      "upload_time_iso_8601": "2021-10-15T23:01:00.656885Z",
      "url": "https://files.pythonhosted.org/packages/87/8f/ad54fd98a7399aa0fc3d49f329a9ce02ecc098d11c574f933bfcb0b5b08b/elit_tokenizer-1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}