{
  "info": {
    "author": "Shkarupa Alex",
    "author_email": "shkarupa.alex@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# tfvan\n\nKeras (TensorFlow v2) reimplementation of **Visual Attention Network** model.\nBased on [Official Pytorch implementation](https://github.com/Visual-Attention-Network/VAN-Classification).\n\nSupports variable-shape inference. All weights are obtained by converting official checkpoints. \n\n## Installation\n\n```bash\npip install tfvan\n```\n\n## Examples\n\nDefault usage (without preprocessing):\n\n```python\nfrom tfvan import VanTiny  # + 3 other variants and input preprocessing\n\nmodel = VanTiny()  # by default will download imagenet-pretrained weights\nmodel.compile(...)\nmodel.fit(...)\n```\n\nCustom classification (with preprocessing):\n\n```python\nfrom keras import layers, models\nfrom tfvan import VanTiny, preprocess_input\n\ninputs = layers.Input(shape=(224, 224, 3), dtype='uint8')\noutputs = layers.Lambda(preprocess_input)(inputs)\noutputs = VanTiny(include_top=False)(outputs)\noutputs = layers.Dense(100, activation='softmax')(outputs)\n\nmodel = models.Model(inputs=inputs, outputs=outputs)\nmodel.compile(...)\nmodel.fit(...)\n```\n\n## Evaluation\n\nFor correctness, `Tiny` and `Small` models (original and ported) tested\nwith [ImageNet-v2 test set](https://www.tensorflow.org/datasets/catalog/imagenet_v2).\n\n```python\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tfvan import VanTiny, preprocess_input\n\n\ndef _prepare(example):\n    # Observation: +2.2% top1 accuracy in tiny model with antialias=True\n    image = tf.image.resize(example['image'], (248, 248), method=tf.image.ResizeMethod.BICUBIC)\n    image = tf.image.central_crop(image, 0.9)\n    image = preprocess_input(image)\n\n    return image, example['label']\n\n\nimagenet2 = tfds.load('imagenet_v2', split='test', shuffle_files=True)\nimagenet2 = imagenet2.map(_prepare, num_parallel_calls=tf.data.AUTOTUNE)\nimagenet2 = imagenet2.batch(8)\n\nmodel = VanTiny()\nmodel.compile('sgd', 'sparse_categorical_crossentropy', ['accuracy', 'sparse_top_k_categorical_accuracy'])\nhistory = model.evaluate(imagenet2)\n\nprint(history)\n```\n\n| name | original acc@1 | ported acc@1 | original acc@5 | ported acc@5 |\n| :---: | :---: | :---: | :---: | :---: |\n| Tiny | 59.22 | 61.59 | 82.32 | 84.52 |\n| Small | 70.17 | 68.62 | 89.17 | 88.54 |\n\n## Citation\n\n```\n@article{guo2022visual,\n  title={Visual Attention Network},\n  author={Guo, Meng-Hao and Lu, Cheng-Ze and Liu, Zheng-Ning and Cheng, Ming-Ming and Hu, Shi-Min},\n  journal={arXiv preprint arXiv:2202.09741},\n  year={2022}\n}",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/shkarupa-alex/tfvan",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tfvan",
    "package_url": "https://pypi.org/project/tfvan/",
    "platform": "",
    "project_url": "https://pypi.org/project/tfvan/",
    "project_urls": {
      "Homepage": "https://github.com/shkarupa-alex/tfvan"
    },
    "release_url": "https://pypi.org/project/tfvan/1.0.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Keras (TensorFlow v2) reimplementation of Visual Attention Network (VAN) model.",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13027670,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a01c529b77f0699ead51351cffc3216936cbb377a3de849e098f88fa08c5f227",
          "md5": "5ef29d4b748e3503533e921bac75f6d4",
          "sha256": "b614048cef6841ca09ab1d8d57a5092957c7bcd1e208164e879425a1adcb4e27"
        },
        "downloads": -1,
        "filename": "tfvan-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "5ef29d4b748e3503533e921bac75f6d4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10620,
        "upload_time": "2022-02-27T21:20:59",
        "upload_time_iso_8601": "2022-02-27T21:20:59.533152Z",
        "url": "https://files.pythonhosted.org/packages/a0/1c/529b77f0699ead51351cffc3216936cbb377a3de849e098f88fa08c5f227/tfvan-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a01c529b77f0699ead51351cffc3216936cbb377a3de849e098f88fa08c5f227",
        "md5": "5ef29d4b748e3503533e921bac75f6d4",
        "sha256": "b614048cef6841ca09ab1d8d57a5092957c7bcd1e208164e879425a1adcb4e27"
      },
      "downloads": -1,
      "filename": "tfvan-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "5ef29d4b748e3503533e921bac75f6d4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 10620,
      "upload_time": "2022-02-27T21:20:59",
      "upload_time_iso_8601": "2022-02-27T21:20:59.533152Z",
      "url": "https://files.pythonhosted.org/packages/a0/1c/529b77f0699ead51351cffc3216936cbb377a3de849e098f88fa08c5f227/tfvan-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}