{
  "info": {
    "author": "Satya Almasian and Dennis Aumiller",
    "author_email": "almasian@informatik.uni-heidelberg.de",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# BERT Got a Date: Introducing Transformers to Temporal Tagging\nSatya Almasian*, Dennis Aumiller*, and Michael Gertz  \nHeidelberg University  \nContact us via: `<lastname>@informatik.uni-heidelberg.de`\n\nCode and data for the paper [BERT Got a Date: Introducing Transformers to Temporal Tagging](https://arxiv.org/abs/2109.14927).\n\n[**Check out our models on Huggingface!**](https://huggingface.co/satyaalmasian)\n\n-----------------------------------------------\n\nTemporal tagging is the task of identification of temporal mentions in text; these expressions can be further divided into different type categories, which is what we refer to as expression (type) classification.\nThis repository describes two different types of transformer-based temporal taggers, which are both additionally capable of expression classification.\nWe follow the TIMEX3 schema definitions in their styling and expression classes (notably, the latter are one of `TIME, DATE, SET, DURATION`). The available data sources for temporal tagging are in the TimeML format, which is essentially a form of XML with tags encapsulating temporal expressions.  \nAn example can be seen below:\n```\nDue to lockdown restrictions, <TIMEX3 tid=\"t1\" type=\"DATE\" value=\"2020\">2020</TIMEX3> \nmight go down as the worst economic <TIMEX3 tid=\"t2\" type=\"DURATION\" value=\"P1Y\">year</TIMEX3>\nin over <TIMEX3 tid=\"t3\" type=\"DURATION\" value=\"P1DE\">a decade</TIMEX3>.\n```\nFor more sample instances, look at the content of `data.zip`. Refer to the README file in the respective unzipped folder for more information.  \n\n\n## Installation\nYou can now install the underlying models by simply running\n```bash\npython3 -m pip install .\n```\nafter cloning this repository; this will automatically install all necessary dependencies. We're working on making the installation even easier by providing a package on PyPI, stay tuned for more!\n\nThis repository contains code for data preparation and training of a seq2seq model (encoder-decoder architectured initialized from encoder-only architectures, specifically BERT or RoBERTa), as well as three token classification encoders (BERT-based).  \nThe output of the models discussed in the paper is in the `results` folder. Refer to the README file in the folder for more information.\n\n**The zip files containing data & results are uploaded using Git LFS and require it as an additional library to work properly.**\n\nTo install Git LFS on Ubuntu: \n\n- Download/Install Git LFS (`sudo apt-get install git-lfs`, or download from https://git-lfs.github.com/)\n- Run `git lfs install` \n\nIf you want to generate data with Heideltime, you will additionally have to set up [`python_heideltime`](https://github.com/PhilipEHausner/python_heideltime) as a wrapper.\nDue to the project nature of Heideltime, this installation has to be performed manually.\n\n\n## Data Preparation\nThe scripts to generate training data is in the subfolder [data_preparation](./data_preparation/).\nFor more usage information, refer to the README file in the subfolder.\nThe data used for training and evaluation are provided in zipped form in `data.zip`.\n\n\n## Evaluation\nFor evaluation, we use a slightly modified version of the TempEval-3 evaluation toolkit ([original source here](https://github.com/naushadzaman/tempeval3_toolkit)).\nWe refactored the code to be compatible with Python3, and incorporated additional evaluation metrics, such as a confusion matrix for type classification.\nWe cross-referenced results to ensure full backward-compatibility and all runs result in the exact same results for both versions.\nOur adjusted code, as well as scripts to convert the output of transformer-based tagging models are in the [evaluation](temporal_taggers/evaluation/) subfolder.\nFor more usage information, refer to the README file in the respective subfolder.\n\n\n## Temporal models\nWe train and evaluate two types of setups for joint temporal tagging and classification:\n* **Token Classification:** We define three variants of simple token classifiers; all of them are based on Huggingface's [BertForTokenClassification](https://huggingface.co/transformers/model_doc/bert.html#bertfortokenclassification). We adapt their \"[token classification for named entity recognition script](https://github.com/huggingface/transformers/tree/master/examples/pytorch/token-classification)\" to train these models. All the models are trained using `bert-base-uncased` as their pre-trained checkpoint.\n* **Text-to-Text Generation (Seq2Seq):** These models are encoder-decoder architectures using BERT or RoBERTa for initial weights. We use Huggingface's [EncoderDecoder](https://huggingface.co/transformers/model_doc/encoderdecoder.html) class for initialization of weights, starting from `bert-base-uncased` and `roberta-base`, respectively.\n\n### Seq2seq\nTo train the seq2seq models, use `run_seq2seq_bert_roberta.py`. Example usage is as follows:\n```bash\npython3 run_seq2seq_bert_roberta.py --model_name roberta-base --pre_train True \\\n--model_dir ./test --train_data ./data/seq2seq/train/tempeval_train.json \\ \n--eval_data ./data/seq2seq/test/tempeval_test.json --num_gpu 2 --num_train_epochs 1 \\\nwarmup_steps 100 --seed 0 --eval_steps 200\n```\nWhich trains a roberta2roberta model defined by `model_name` for `num_train_epochs` epochs on the gpu with ID `num_gpu`.\nThe random seed is set by `seed` and the number of warmup steps by `warmup_steps`.\nTrain data should be specified in `train_data` and `model_dir` defines where the model is saved.\nset `eval_data` if you want intermediate evaluation defined by `eval_steps`.\nIf the `pre_train` flag is set to true it will load the checkpoints from the Huggingface hub and fine-tune on the dataset given.\nIf the `pre_train` is false, we are in the fine-tuning mode, and you can provide the path to the pre-trained model with `pretrain_path`.\nWe used the `pre_train` mode to train on weakly labeled data provided by the rule-based system of HeidelTime and set the `pre_train` to false\nfor fine-tuning on the benchmark datasets. If you wish to simply fine-tune the benchmark datasets using the Huggingface checkpoints\nyou can set the `pre_train` to ture, as displayed in the example above.\nFor additional arguments such as length penalty, the number of beams, early stopping, and other model specifications, please refer to the script.\n\n### Token Classifiers\nAs mentioned above all token classifiers are trained using an adaptation of the NER script from Huggingface. To train these models use\n`run_token_classifier.py` like the following example:\n```bash\npython3 run_token_classifier.py --data_dir /data/temporal/BIO/wikiwars \\ \n--labels ./data/temporal/BIO/train_staging/labels.txt \\ \n--model_name_or_path bert-base-uncased \\ \n--output_dir ./fine_tune_wikiwars/bert_tagging_with_date_no_pretrain_8epochs/bert_tagging_with_date_layer_seed_19 --max_seq_length  512  \\\n--num_train_epochs 8 --per_device_train_batch_size 34 --save_steps 3000 --logging_steps 300 --eval_steps 3000 \\ \n--do_train --do_eval --overwrite_output_dir --seed 19 --model_date_extra_layer    \n```\nWe used `bert-base-uncased ` as the base of all our token classification models for pre-training as defined by `model_name_or_path`.\nFor fine-tuning on the datasets `model_name_or_path` should point to the path of the pre-trained model. `labels` file is created during data preparation for more information refer to the [subfolder](./data_preparation/README.md).\n`data_dir` points to a folder that contains `train.txt`, `test.txt` and `dev.txt` and `output_dir` points to the saving location.\nYou can define the number of epochs by `num_train_epochs`, set the seed with `seed` and batch size on each GPU with `per_device_train_batch_size`.\nFor more information on the parameters refer to the [Huggingface script](https://github.com/huggingface/transformers/tree/master/examples/pytorch/token-classification).\nIn our paper, we introduce 3 variants of token classification, which are defined by flags in the script.\nIf no flag is set the model trains the vanilla BERT for token classification.\nThe flag `model_date_extra_layer` trains the model with an extra date layer and `model_crf` adds the extra crf layer.\nTo train the extra date embedding you need to download the vocabulary file and specify its path in the `date_vocab` argument.\nThe description and model definition of the BERT variants are in folder [temporal_models](./temporal_models/).\nPlease refer to the README file for further information. For training different model types on the same data, make sure to remove\nthe cached dataset, since the feature generation is different for each model type. \n\n## Load directly from the Huggingface Model Hub\nWe uploaded our best-performing version of each architecture to the Huggingface Model Hub.\nThe weights for the other four seeding runs are available upon request.\nWe upload the variants that were fine-tuned on the concatenation of *all three* evaluation sets for better generalization to various domains.\nToken classification models are variants without pre-training.\nBoth seq2seq models are pre-trained on the weakly labelled corpus and fine-tuned on the mixed data. \n\nOverall we upload the following five models. For other model configurations and checkpoints please get in contact with us:\n\n* [satyaalmasian/temporal_tagger_roberta2roberta](https://huggingface.co/satyaalmasian/temporal_tagger_roberta2roberta): Our best perfoming model from the paper, an encoder-decoder architecture using RoBERTa.\n  The model is pre-trained on weakly labeled news articles, tagged with HeidelTime, and fined-tuned on the train set of TempEval-3, Tweets, and Wikiwars.\n* [satyaalmasian/temporal_tagger_bert2bert](https://huggingface.co/satyaalmasian/temporal_tagger_bert2bert): Our second seq2seq model , an encoder-decoder architecture using BERT.\n  The model is pre-trained on weakly labeled news articles, tagged with HeidelTime, and fined-tuned on the train set of TempEval-3, Tweets, and Wikiwars.\n* [satyaalmasian/temporal_tagger_BERT_tokenclassifier](https://huggingface.co/satyaalmasian/temporal_tagger_BERT_tokenclassifier): BERT for token classification model or vanilla BERT model from the paper.\n  This model is only trained on the train set of TempEval-3, Tweets, and Wikiwars.\n* [satyaalmasian/temporal_tagger_DATEBERT_tokenclassifier](https://huggingface.co/satyaalmasian/temporal_tagger_DATEBERT_tokenclassifier): BERT for token classification with an extra date embedding, that encodes the reference date of the\n  document. If the document does not have a reference date, it is best to avoid this model. Moreover, since the architecture\n  is a modification of a default Huggingface model, the usage is not as straightforward and requires the classes defined in the `temporal_model`\n  module. This model is only trained on the train set of TempEval-3, Tweets, and Wikiwars.\n* [satyaalmasian/temporal_tagger_BERTCRF_tokenclassifier](https://huggingface.co/satyaalmasian/temporal_tagger_BERTCRF_tokenclassifier) :BERT for token classification with a CRF layer on the output. Moreover, since the architecture\n  is a modification of a default Huggingface model, the usage is not as straightforward and requires the classes defined in the `temporal_model`\n  module. This model is only trained on the train set of TempEval-3, Tweets, and Wikiwars.\n\nIn the `examples` module, you find two scripts `model_hub_seq2seq_examples.py` and `model_hub_tokenclassifiers_examples.py` for\nseq2seq and token classification examples using the Huggingface model hub. The examples load the models and use them on example sentences\nfor tagging. The seq2seq example uses the pre-defined post-processing from the tempeval evaluation and contains rules for the cases we came across in the benchmark dataset. \nIf you plan to use these models on new data, it is best to observe the raw output of the first few samples to detect possible format problems that are easily fixable.\nFurther fine-tuning of the models is also possible.\nFor seq2seq models you can simply load the models with\n```python3\ntokenizer = AutoTokenizer.from_pretrained(\"satyaalmasian/temporal_tagger_roberta2roberta\")\nmodel = EncoderDecoderModel.from_pretrained(\"satyaalmasian/temporal_tagger_roberta2roberta\")\n```\nand use the `DataProcessor` from `temporal_models.seq2seq_utils` to preprocess the `json` dataset. The model\ncan be fine-tuned using `Seq2SeqTrainer` (same as in `run_seq2seq_bert_roberta.py`).\nFor token classifiers the model and the tokenizers are loaded as follows:\n```python3\ntokenizer = AutoTokenizer.from_pretrained(\"satyaalmasian/temporal_tagger_BERT_tokenclassifier\", use_fast=False)\nmodel = BertForTokenClassification.from_pretrained(\"satyaalmasian/temporal_tagger_BERT_tokenclassifier\")\n```\nClassifiers need a BIO-tagged file that can be loaded using `TokenClassificationDataset` and fine-tuned with the Huggingface `Trainer`.\nFor more information on the usage of these models refer to their model hub page. \n\n\n## Citation\nIf you use our models in your work, we would appreciate attribution with the following citation:\n```\n@article{almasian2021bert,\n  title={{BERT got a Date: Introducing Transformers to Temporal Tagging}},\n  author={Almasian, Satya and Aumiller, Dennis and Gertz, Michael},\n  journal={arXiv preprint arXiv:2109.14927},\n  url={https://arxiv.org/abs/2109.14927},\n  year={2021}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/satya77/Transformer_Temporal_Tagger",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "temporal-taggers",
    "package_url": "https://pypi.org/project/temporal-taggers/",
    "platform": "",
    "project_url": "https://pypi.org/project/temporal-taggers/",
    "project_urls": {
      "Homepage": "https://github.com/satya77/Transformer_Temporal_Tagger"
    },
    "release_url": "https://pypi.org/project/temporal-taggers/0.0.1/",
    "requires_dist": [
      "transformers (>=4.3.3)",
      "torch (>=1.8)",
      "datasets",
      "spacy (>=3.0)",
      "beautifulsoup4 (>=4.9)",
      "seqeval",
      "conllu",
      "filelock"
    ],
    "requires_python": "",
    "summary": "Neural temporal taggers with Transformer architectures",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12111235,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "21cb8f68628ca2a925d1a6b47dbe5986d384d94148a106bd5cb2ee5847a07258",
          "md5": "e1b565bc8a1efb2f7b43b03f74a8ab86",
          "sha256": "c56133070e4cfd2c6c3aa70b17263c36ac1222cb596b671eeece184aa99ba6e8"
        },
        "downloads": -1,
        "filename": "temporal_taggers-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e1b565bc8a1efb2f7b43b03f74a8ab86",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 71409,
        "upload_time": "2021-11-23T23:28:27",
        "upload_time_iso_8601": "2021-11-23T23:28:27.289539Z",
        "url": "https://files.pythonhosted.org/packages/21/cb/8f68628ca2a925d1a6b47dbe5986d384d94148a106bd5cb2ee5847a07258/temporal_taggers-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "502b5492c778506c4b25dd0912ca43a37ab7867f1911b8a57ecce7c40add4ae7",
          "md5": "7f99fc75e41e55ee2bf9c19cb44426a1",
          "sha256": "89a7e8ffeead3700db3d388d0d1fc517445695783178b6f541a0672ebf42bd5a"
        },
        "downloads": -1,
        "filename": "temporal-taggers-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7f99fc75e41e55ee2bf9c19cb44426a1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 63499,
        "upload_time": "2021-11-23T23:28:29",
        "upload_time_iso_8601": "2021-11-23T23:28:29.574434Z",
        "url": "https://files.pythonhosted.org/packages/50/2b/5492c778506c4b25dd0912ca43a37ab7867f1911b8a57ecce7c40add4ae7/temporal-taggers-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "21cb8f68628ca2a925d1a6b47dbe5986d384d94148a106bd5cb2ee5847a07258",
        "md5": "e1b565bc8a1efb2f7b43b03f74a8ab86",
        "sha256": "c56133070e4cfd2c6c3aa70b17263c36ac1222cb596b671eeece184aa99ba6e8"
      },
      "downloads": -1,
      "filename": "temporal_taggers-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e1b565bc8a1efb2f7b43b03f74a8ab86",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 71409,
      "upload_time": "2021-11-23T23:28:27",
      "upload_time_iso_8601": "2021-11-23T23:28:27.289539Z",
      "url": "https://files.pythonhosted.org/packages/21/cb/8f68628ca2a925d1a6b47dbe5986d384d94148a106bd5cb2ee5847a07258/temporal_taggers-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "502b5492c778506c4b25dd0912ca43a37ab7867f1911b8a57ecce7c40add4ae7",
        "md5": "7f99fc75e41e55ee2bf9c19cb44426a1",
        "sha256": "89a7e8ffeead3700db3d388d0d1fc517445695783178b6f541a0672ebf42bd5a"
      },
      "downloads": -1,
      "filename": "temporal-taggers-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "7f99fc75e41e55ee2bf9c19cb44426a1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 63499,
      "upload_time": "2021-11-23T23:28:29",
      "upload_time_iso_8601": "2021-11-23T23:28:29.574434Z",
      "url": "https://files.pythonhosted.org/packages/50/2b/5492c778506c4b25dd0912ca43a37ab7867f1911b8a57ecce7c40add4ae7/temporal-taggers-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}