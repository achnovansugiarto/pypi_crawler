{
  "info": {
    "author": "Jonas Pfeiffer, Andreas RÃ¼cklÃ©, Clifton Poth, Hannah Sterz, based on work by the HuggingFace team and community",
    "author_email": "pfeiffer@ukp.tu-darmstadt.de",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<!---\nCopyright 2020 The AdapterHub Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n<img style=\"vertical-align:middle\" src=\"https://raw.githubusercontent.com/Adapter-Hub/adapter-transformers/master/adapter_docs/logo.png\" />\n</p>\n<h1 align=\"center\">\n<span>adapter-transformers</span>\n</h1>\n\n<h3 align=\"center\">\nA friendly fork of HuggingFace's <i>Transformers</i>, adding Adapters to PyTorch language models\n</h3>\n\n![Tests](https://github.com/Adapter-Hub/adapter-transformers/workflows/Tests/badge.svg)\n[![GitHub](https://img.shields.io/github/license/adapter-hub/adapter-transformers.svg?color=blue)](https://github.com/adapter-hub/adapter-transformers/blob/master/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/adapter-transformers)](https://pypi.org/project/adapter-transformers/)\n\n`adapter-transformers` is an extension of [HuggingFace's Transformers](https://github.com/huggingface/transformers) library, integrating adapters into state-of-the-art language models by incorporating **[AdapterHub](https://adapterhub.ml)**, a central repository for pre-trained adapter modules.\n\n_ðŸ’¡ Important: This library can be used as a drop-in replacement for HuggingFace Transformers and regularly synchronizes new upstream changes.\nThus, most files in this repository are direct copies from the HuggingFace Transformers source, modified only with changes required for the adapter implementations._\n\n## Installation\n\n`adapter-transformers` currently supports **Python 3.6+** and **PyTorch 1.3.1+**.\nAfter [installing PyTorch](https://pytorch.org/get-started/locally/), you can install `adapter-transformers` from PyPI ...\n\n```\npip install -U adapter-transformers\n```\n\n... or from source by cloning the repository:\n\n```\ngit clone https://github.com/adapter-hub/adapter-transformers.git\ncd adapter-transformers\npip install .\n```\n\n## Getting Started\n\nHuggingFace's great documentation on getting started with _Transformers_ can be found [here](https://huggingface.co/transformers/index.html). `adapter-transformers` is fully compatible with _Transformers_.\n\nTo get started with adapters, refer to these locations:\n\n- **[Colab notebook tutorials](https://github.com/Adapter-Hub/adapter-transformers/tree/master/notebooks)**, a series notebooks providing an introduction to all the main concepts of (adapter-)transformers and AdapterHub\n- **https://docs.adapterhub.ml**, our documentation on training and using adapters with _adapter-transformers_\n- **https://adapterhub.ml** to explore available pre-trained adapter modules and share your own adapters\n- **[Examples folder](https://github.com/Adapter-Hub/adapter-transformers/tree/master/examples/pytorch)** of this repository containing HuggingFace's example training scripts, many adapted for training adapters\n\n## Implemented Methods\n\nCurrently, adapter-transformers integrates all architectures and methods listed below:\n\n| Method | Paper(s) | Quick Links |\n| --- | --- | --- |\n| Bottleneck adapters | [Houlsby et al. (2019)](https://arxiv.org/pdf/1902.00751.pdf)<br> [Bapna and Firat (2019)](https://arxiv.org/pdf/1909.08478.pdf) | [Quickstart](https://docs.adapterhub.ml/quickstart.html), [Notebook](https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb) |\n| AdapterFusion | [Pfeiffer et al. (2021)](https://aclanthology.org/2021.eacl-main.39.pdf) | [Docs: Training](https://docs.adapterhub.ml/training.html#train-adapterfusion), [Notebook](https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/03_Adapter_Fusion.ipynb) |\n| MAD-X,<br> Invertible adapters | [Pfeiffer et al. (2020)](https://aclanthology.org/2020.emnlp-main.617/) | [Notebook](https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/04_Cross_Lingual_Transfer.ipynb) |\n| AdapterDrop | [RÃ¼cklÃ© et al. (2021)](https://arxiv.org/pdf/2010.11918.pdf) | [Notebook](https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/05_Adapter_Drop_Training.ipynb) |\n| MAD-X 2.0,<br> Embedding training | [Pfeiffer et al. (2021)](https://arxiv.org/pdf/2012.15562.pdf) | [Docs: Embeddings](https://docs.adapterhub.ml/embeddings.html), [Notebook](https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/08_NER_Wikiann.ipynb) |\n| Prefix Tuning | [Li and Liang (2021)](https://arxiv.org/pdf/2101.00190.pdf) | [Docs](https://docs.adapterhub.ml/overview.html#prefix-tuning) |\n| Parallel adapters,<br> Mix-and-Match adapters | [He et al. (2021)](https://arxiv.org/pdf/2110.04366.pdf) | [Docs](https://docs.adapterhub.ml/overview.html#combinations-mix-and-match-adapters) |\n| Compacter | [Mahabadi et al. (2021)](https://arxiv.org/pdf/2106.04647.pdf) | [Docs](https://docs.adapterhub.ml/overview.html#compacter) |\n\n## Supported Models\n\nWe currently support the PyTorch versions of all models listed on the **[Model Overview](https://docs.adapterhub.ml/model_overview.html) page** in our documentation.\n\n## Citation\n\nIf you use this library for your work, please consider citing our paper [AdapterHub: A Framework for Adapting Transformers](https://arxiv.org/abs/2007.07779):\n\n```\n@inproceedings{pfeiffer2020AdapterHub,\n    title={AdapterHub: A Framework for Adapting Transformers},\n    author={Pfeiffer, Jonas and\n            R{\\\"u}ckl{\\'e}, Andreas and\n            Poth, Clifton and\n            Kamath, Aishwarya and\n            Vuli{\\'c}, Ivan and\n            Ruder, Sebastian and\n            Cho, Kyunghyun and\n            Gurevych, Iryna},\n    booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},\n    pages={46--54},\n    year={2020}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/adapter-hub/adapter-transformers",
    "keywords": "NLP deep learning transformer pytorch BERT adapters",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cody-adapter-transformers",
    "package_url": "https://pypi.org/project/cody-adapter-transformers/",
    "platform": null,
    "project_url": "https://pypi.org/project/cody-adapter-transformers/",
    "project_urls": {
      "Homepage": "https://github.com/adapter-hub/adapter-transformers"
    },
    "release_url": "https://pypi.org/project/cody-adapter-transformers/3.0.1/",
    "requires_dist": [
      "filelock",
      "huggingface-hub (<1.0,>=0.1.0)",
      "numpy (>=1.17)",
      "packaging (>=20.0)",
      "pyyaml (>=5.1)",
      "regex (!=2019.12.17)",
      "requests",
      "sacremoses",
      "tokenizers (!=0.11.3,>=0.11.1)",
      "tqdm (>=4.27)",
      "dataclasses ; python_version < \"3.7\"",
      "importlib-metadata ; python_version < \"3.8\"",
      "tensorflow (>=2.3) ; extra == 'all'",
      "onnxconverter-common ; extra == 'all'",
      "tf2onnx ; extra == 'all'",
      "torch (>=1.0) ; extra == 'all'",
      "jax (>=0.2.8) ; extra == 'all'",
      "jaxlib (>=0.1.65) ; extra == 'all'",
      "flax (>=0.3.5) ; extra == 'all'",
      "optax (>=0.0.8) ; extra == 'all'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'all'",
      "protobuf ; extra == 'all'",
      "tokenizers (!=0.11.3,>=0.11.1) ; extra == 'all'",
      "torchaudio ; extra == 'all'",
      "librosa ; extra == 'all'",
      "pyctcdecode (>=0.3.0) ; extra == 'all'",
      "phonemizer ; extra == 'all'",
      "Pillow ; extra == 'all'",
      "optuna ; extra == 'all'",
      "ray[tune] ; extra == 'all'",
      "sigopt ; extra == 'all'",
      "timm ; extra == 'all'",
      "codecarbon (==1.2.0) ; extra == 'all'",
      "librosa ; extra == 'audio'",
      "pyctcdecode (>=0.3.0) ; extra == 'audio'",
      "phonemizer ; extra == 'audio'",
      "codecarbon (==1.2.0) ; extra == 'codecarbon'",
      "deepspeed (>=0.5.9) ; extra == 'deepspeed'",
      "tensorflow (>=2.3) ; extra == 'dev'",
      "onnxconverter-common ; extra == 'dev'",
      "tf2onnx ; extra == 'dev'",
      "torch (>=1.0) ; extra == 'dev'",
      "jax (>=0.2.8) ; extra == 'dev'",
      "jaxlib (>=0.1.65) ; extra == 'dev'",
      "flax (>=0.3.5) ; extra == 'dev'",
      "optax (>=0.0.8) ; extra == 'dev'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'dev'",
      "protobuf ; extra == 'dev'",
      "tokenizers (!=0.11.3,>=0.11.1) ; extra == 'dev'",
      "torchaudio ; extra == 'dev'",
      "librosa ; extra == 'dev'",
      "pyctcdecode (>=0.3.0) ; extra == 'dev'",
      "phonemizer ; extra == 'dev'",
      "Pillow ; extra == 'dev'",
      "optuna ; extra == 'dev'",
      "ray[tune] ; extra == 'dev'",
      "sigopt ; extra == 'dev'",
      "timm ; extra == 'dev'",
      "codecarbon (==1.2.0) ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-xdist ; extra == 'dev'",
      "timeout-decorator ; extra == 'dev'",
      "parameterized ; extra == 'dev'",
      "psutil ; extra == 'dev'",
      "datasets ; extra == 'dev'",
      "pytest-timeout ; extra == 'dev'",
      "black (~=22.0) ; extra == 'dev'",
      "sacrebleu (<2.0.0,>=1.4.12) ; extra == 'dev'",
      "rouge-score ; extra == 'dev'",
      "nltk ; extra == 'dev'",
      "GitPython (<3.1.19) ; extra == 'dev'",
      "faiss-cpu ; extra == 'dev'",
      "cookiecutter (==1.7.2) ; extra == 'dev'",
      "isort (>=5.5.4) ; extra == 'dev'",
      "flake8 (>=3.8.3) ; extra == 'dev'",
      "fugashi (>=1.0) ; extra == 'dev'",
      "ipadic (<2.0,>=1.0.0) ; extra == 'dev'",
      "unidic-lite (>=1.0.7) ; extra == 'dev'",
      "unidic (>=1.0.2) ; extra == 'dev'",
      "docutils (==0.16.0) ; extra == 'dev'",
      "myst-parser ; extra == 'dev'",
      "sphinx (==3.2.1) ; extra == 'dev'",
      "sphinx-markdown-tables ; extra == 'dev'",
      "sphinx-rtd-theme (==0.4.3) ; extra == 'dev'",
      "sphinx-copybutton ; extra == 'dev'",
      "sphinxext-opengraph (==0.4.1) ; extra == 'dev'",
      "sphinx-intl ; extra == 'dev'",
      "sphinx-multiversion ; extra == 'dev'",
      "scikit-learn ; extra == 'dev'",
      "pytest ; extra == 'dev-tensorflow'",
      "pytest-xdist ; extra == 'dev-tensorflow'",
      "timeout-decorator ; extra == 'dev-tensorflow'",
      "parameterized ; extra == 'dev-tensorflow'",
      "psutil ; extra == 'dev-tensorflow'",
      "datasets ; extra == 'dev-tensorflow'",
      "pytest-timeout ; extra == 'dev-tensorflow'",
      "black (~=22.0) ; extra == 'dev-tensorflow'",
      "sacrebleu (<2.0.0,>=1.4.12) ; extra == 'dev-tensorflow'",
      "rouge-score ; extra == 'dev-tensorflow'",
      "nltk ; extra == 'dev-tensorflow'",
      "GitPython (<3.1.19) ; extra == 'dev-tensorflow'",
      "faiss-cpu ; extra == 'dev-tensorflow'",
      "cookiecutter (==1.7.2) ; extra == 'dev-tensorflow'",
      "tensorflow (>=2.3) ; extra == 'dev-tensorflow'",
      "onnxconverter-common ; extra == 'dev-tensorflow'",
      "tf2onnx ; extra == 'dev-tensorflow'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'dev-tensorflow'",
      "protobuf ; extra == 'dev-tensorflow'",
      "tokenizers (!=0.11.3,>=0.11.1) ; extra == 'dev-tensorflow'",
      "Pillow ; extra == 'dev-tensorflow'",
      "isort (>=5.5.4) ; extra == 'dev-tensorflow'",
      "flake8 (>=3.8.3) ; extra == 'dev-tensorflow'",
      "docutils (==0.16.0) ; extra == 'dev-tensorflow'",
      "myst-parser ; extra == 'dev-tensorflow'",
      "sphinx (==3.2.1) ; extra == 'dev-tensorflow'",
      "sphinx-markdown-tables ; extra == 'dev-tensorflow'",
      "sphinx-rtd-theme (==0.4.3) ; extra == 'dev-tensorflow'",
      "sphinx-copybutton ; extra == 'dev-tensorflow'",
      "sphinxext-opengraph (==0.4.1) ; extra == 'dev-tensorflow'",
      "sphinx-intl ; extra == 'dev-tensorflow'",
      "sphinx-multiversion ; extra == 'dev-tensorflow'",
      "scikit-learn ; extra == 'dev-tensorflow'",
      "onnxruntime (>=1.4.0) ; extra == 'dev-tensorflow'",
      "onnxruntime-tools (>=1.4.2) ; extra == 'dev-tensorflow'",
      "librosa ; extra == 'dev-tensorflow'",
      "pyctcdecode (>=0.3.0) ; extra == 'dev-tensorflow'",
      "phonemizer ; extra == 'dev-tensorflow'",
      "pytest ; extra == 'dev-torch'",
      "pytest-xdist ; extra == 'dev-torch'",
      "timeout-decorator ; extra == 'dev-torch'",
      "parameterized ; extra == 'dev-torch'",
      "psutil ; extra == 'dev-torch'",
      "datasets ; extra == 'dev-torch'",
      "pytest-timeout ; extra == 'dev-torch'",
      "black (~=22.0) ; extra == 'dev-torch'",
      "sacrebleu (<2.0.0,>=1.4.12) ; extra == 'dev-torch'",
      "rouge-score ; extra == 'dev-torch'",
      "nltk ; extra == 'dev-torch'",
      "GitPython (<3.1.19) ; extra == 'dev-torch'",
      "faiss-cpu ; extra == 'dev-torch'",
      "cookiecutter (==1.7.2) ; extra == 'dev-torch'",
      "torch (>=1.0) ; extra == 'dev-torch'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'dev-torch'",
      "protobuf ; extra == 'dev-torch'",
      "tokenizers (!=0.11.3,>=0.11.1) ; extra == 'dev-torch'",
      "torchaudio ; extra == 'dev-torch'",
      "librosa ; extra == 'dev-torch'",
      "pyctcdecode (>=0.3.0) ; extra == 'dev-torch'",
      "phonemizer ; extra == 'dev-torch'",
      "Pillow ; extra == 'dev-torch'",
      "optuna ; extra == 'dev-torch'",
      "ray[tune] ; extra == 'dev-torch'",
      "sigopt ; extra == 'dev-torch'",
      "timm ; extra == 'dev-torch'",
      "codecarbon (==1.2.0) ; extra == 'dev-torch'",
      "isort (>=5.5.4) ; extra == 'dev-torch'",
      "flake8 (>=3.8.3) ; extra == 'dev-torch'",
      "fugashi (>=1.0) ; extra == 'dev-torch'",
      "ipadic (<2.0,>=1.0.0) ; extra == 'dev-torch'",
      "unidic-lite (>=1.0.7) ; extra == 'dev-torch'",
      "unidic (>=1.0.2) ; extra == 'dev-torch'",
      "docutils (==0.16.0) ; extra == 'dev-torch'",
      "myst-parser ; extra == 'dev-torch'",
      "sphinx (==3.2.1) ; extra == 'dev-torch'",
      "sphinx-markdown-tables ; extra == 'dev-torch'",
      "sphinx-rtd-theme (==0.4.3) ; extra == 'dev-torch'",
      "sphinx-copybutton ; extra == 'dev-torch'",
      "sphinxext-opengraph (==0.4.1) ; extra == 'dev-torch'",
      "sphinx-intl ; extra == 'dev-torch'",
      "sphinx-multiversion ; extra == 'dev-torch'",
      "scikit-learn ; extra == 'dev-torch'",
      "onnxruntime (>=1.4.0) ; extra == 'dev-torch'",
      "onnxruntime-tools (>=1.4.2) ; extra == 'dev-torch'",
      "tensorflow (>=2.3) ; extra == 'docs'",
      "onnxconverter-common ; extra == 'docs'",
      "tf2onnx ; extra == 'docs'",
      "torch (>=1.0) ; extra == 'docs'",
      "jax (>=0.2.8) ; extra == 'docs'",
      "jaxlib (>=0.1.65) ; extra == 'docs'",
      "flax (>=0.3.5) ; extra == 'docs'",
      "optax (>=0.0.8) ; extra == 'docs'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'docs'",
      "protobuf ; extra == 'docs'",
      "tokenizers (!=0.11.3,>=0.11.1) ; extra == 'docs'",
      "torchaudio ; extra == 'docs'",
      "librosa ; extra == 'docs'",
      "pyctcdecode (>=0.3.0) ; extra == 'docs'",
      "phonemizer ; extra == 'docs'",
      "Pillow ; extra == 'docs'",
      "optuna ; extra == 'docs'",
      "ray[tune] ; extra == 'docs'",
      "sigopt ; extra == 'docs'",
      "timm ; extra == 'docs'",
      "codecarbon (==1.2.0) ; extra == 'docs'",
      "docutils (==0.16.0) ; extra == 'docs'",
      "myst-parser ; extra == 'docs'",
      "sphinx (==3.2.1) ; extra == 'docs'",
      "sphinx-markdown-tables ; extra == 'docs'",
      "sphinx-rtd-theme (==0.4.3) ; extra == 'docs'",
      "sphinx-copybutton ; extra == 'docs'",
      "sphinxext-opengraph (==0.4.1) ; extra == 'docs'",
      "sphinx-intl ; extra == 'docs'",
      "sphinx-multiversion ; extra == 'docs'",
      "docutils (==0.16.0) ; extra == 'docs_specific'",
      "myst-parser ; extra == 'docs_specific'",
      "sphinx (==3.2.1) ; extra == 'docs_specific'",
      "sphinx-markdown-tables ; extra == 'docs_specific'",
      "sphinx-rtd-theme (==0.4.3) ; extra == 'docs_specific'",
      "sphinx-copybutton ; extra == 'docs_specific'",
      "sphinxext-opengraph (==0.4.1) ; extra == 'docs_specific'",
      "sphinx-intl ; extra == 'docs_specific'",
      "sphinx-multiversion ; extra == 'docs_specific'",
      "fairscale (>0.3) ; extra == 'fairscale'",
      "jax (>=0.2.8) ; extra == 'flax'",
      "jaxlib (>=0.1.65) ; extra == 'flax'",
      "flax (>=0.3.5) ; extra == 'flax'",
      "optax (>=0.0.8) ; extra == 'flax'",
      "librosa ; extra == 'flax-speech'",
      "pyctcdecode (>=0.3.0) ; extra == 'flax-speech'",
      "phonemizer ; extra == 'flax-speech'",
      "ftfy ; extra == 'ftfy'",
      "optuna ; extra == 'integrations'",
      "ray[tune] ; extra == 'integrations'",
      "sigopt ; extra == 'integrations'",
      "fugashi (>=1.0) ; extra == 'ja'",
      "ipadic (<2.0,>=1.0.0) ; extra == 'ja'",
      "unidic-lite (>=1.0.7) ; extra == 'ja'",
      "unidic (>=1.0.2) ; extra == 'ja'",
      "cookiecutter (==1.7.2) ; extra == 'modelcreation'",
      "onnxconverter-common ; extra == 'onnx'",
      "tf2onnx ; extra == 'onnx'",
      "onnxruntime (>=1.4.0) ; extra == 'onnx'",
      "onnxruntime-tools (>=1.4.2) ; extra == 'onnx'",
      "onnxruntime (>=1.4.0) ; extra == 'onnxruntime'",
      "onnxruntime-tools (>=1.4.2) ; extra == 'onnxruntime'",
      "optuna ; extra == 'optuna'",
      "black (~=22.0) ; extra == 'quality'",
      "isort (>=5.5.4) ; extra == 'quality'",
      "flake8 (>=3.8.3) ; extra == 'quality'",
      "GitPython (<3.1.19) ; extra == 'quality'",
      "ray[tune] ; extra == 'ray'",
      "faiss-cpu ; extra == 'retrieval'",
      "datasets ; extra == 'retrieval'",
      "sagemaker (>=2.31.0) ; extra == 'sagemaker'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'sentencepiece'",
      "protobuf ; extra == 'sentencepiece'",
      "pydantic ; extra == 'serving'",
      "uvicorn ; extra == 'serving'",
      "fastapi ; extra == 'serving'",
      "starlette ; extra == 'serving'",
      "sigopt ; extra == 'sigopt'",
      "scikit-learn ; extra == 'sklearn'",
      "torchaudio ; extra == 'speech'",
      "librosa ; extra == 'speech'",
      "pyctcdecode (>=0.3.0) ; extra == 'speech'",
      "phonemizer ; extra == 'speech'",
      "pytest ; extra == 'testing'",
      "pytest-xdist ; extra == 'testing'",
      "timeout-decorator ; extra == 'testing'",
      "parameterized ; extra == 'testing'",
      "psutil ; extra == 'testing'",
      "datasets ; extra == 'testing'",
      "pytest-timeout ; extra == 'testing'",
      "black (~=22.0) ; extra == 'testing'",
      "sacrebleu (<2.0.0,>=1.4.12) ; extra == 'testing'",
      "rouge-score ; extra == 'testing'",
      "nltk ; extra == 'testing'",
      "GitPython (<3.1.19) ; extra == 'testing'",
      "faiss-cpu ; extra == 'testing'",
      "cookiecutter (==1.7.2) ; extra == 'testing'",
      "tensorflow (>=2.3) ; extra == 'tf'",
      "onnxconverter-common ; extra == 'tf'",
      "tf2onnx ; extra == 'tf'",
      "tensorflow-cpu (>=2.3) ; extra == 'tf-cpu'",
      "onnxconverter-common ; extra == 'tf-cpu'",
      "tf2onnx ; extra == 'tf-cpu'",
      "librosa ; extra == 'tf-speech'",
      "pyctcdecode (>=0.3.0) ; extra == 'tf-speech'",
      "phonemizer ; extra == 'tf-speech'",
      "timm ; extra == 'timm'",
      "tokenizers (!=0.11.3,>=0.11.1) ; extra == 'tokenizers'",
      "torch (>=1.0) ; extra == 'torch'",
      "torchaudio ; extra == 'torch-speech'",
      "librosa ; extra == 'torch-speech'",
      "pyctcdecode (>=0.3.0) ; extra == 'torch-speech'",
      "phonemizer ; extra == 'torch-speech'",
      "filelock ; extra == 'torchhub'",
      "huggingface-hub (<1.0,>=0.1.0) ; extra == 'torchhub'",
      "importlib-metadata ; extra == 'torchhub'",
      "numpy (>=1.17) ; extra == 'torchhub'",
      "packaging (>=20.0) ; extra == 'torchhub'",
      "protobuf ; extra == 'torchhub'",
      "regex (!=2019.12.17) ; extra == 'torchhub'",
      "requests ; extra == 'torchhub'",
      "sacremoses ; extra == 'torchhub'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'torchhub'",
      "torch (>=1.0) ; extra == 'torchhub'",
      "tokenizers (!=0.11.3,>=0.11.1) ; extra == 'torchhub'",
      "tqdm (>=4.27) ; extra == 'torchhub'",
      "Pillow ; extra == 'vision'"
    ],
    "requires_python": ">=3.6.0",
    "summary": "A friendly fork of HuggingFace's Transformers, adding Adapters to PyTorch language models",
    "version": "3.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15591174,
  "releases": {
    "3.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb9f269e74fe5d20a4c4e06f01ad29946e33978c16d34f3afc03281b340ae974",
          "md5": "1a8c9aeaa48dca724e0ec7b0d1df8717",
          "sha256": "b558cd3af3aa2e444941a9d15e2ccd4f86b974d5f7b59261e858eda7a20d07bb"
        },
        "downloads": -1,
        "filename": "cody_adapter_transformers-3.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1a8c9aeaa48dca724e0ec7b0d1df8717",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 3922251,
        "upload_time": "2022-10-31T01:40:13",
        "upload_time_iso_8601": "2022-10-31T01:40:13.860353Z",
        "url": "https://files.pythonhosted.org/packages/bb/9f/269e74fe5d20a4c4e06f01ad29946e33978c16d34f3afc03281b340ae974/cody_adapter_transformers-3.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cfeeedbd101049c8255189e0a5678e003c02ca18ab29b138cf74d79e6089ea25",
          "md5": "65e2cd4deaa3898ad17c805e406dd745",
          "sha256": "272fddb6f51e9e1592a58a1ff8d1eaf8d252a28c41665d4ea7246f0449f4ac50"
        },
        "downloads": -1,
        "filename": "cody-adapter-transformers-3.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "65e2cd4deaa3898ad17c805e406dd745",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 3240502,
        "upload_time": "2022-10-31T01:40:32",
        "upload_time_iso_8601": "2022-10-31T01:40:32.642632Z",
        "url": "https://files.pythonhosted.org/packages/cf/ee/edbd101049c8255189e0a5678e003c02ca18ab29b138cf74d79e6089ea25/cody-adapter-transformers-3.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bb9f269e74fe5d20a4c4e06f01ad29946e33978c16d34f3afc03281b340ae974",
        "md5": "1a8c9aeaa48dca724e0ec7b0d1df8717",
        "sha256": "b558cd3af3aa2e444941a9d15e2ccd4f86b974d5f7b59261e858eda7a20d07bb"
      },
      "downloads": -1,
      "filename": "cody_adapter_transformers-3.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1a8c9aeaa48dca724e0ec7b0d1df8717",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 3922251,
      "upload_time": "2022-10-31T01:40:13",
      "upload_time_iso_8601": "2022-10-31T01:40:13.860353Z",
      "url": "https://files.pythonhosted.org/packages/bb/9f/269e74fe5d20a4c4e06f01ad29946e33978c16d34f3afc03281b340ae974/cody_adapter_transformers-3.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cfeeedbd101049c8255189e0a5678e003c02ca18ab29b138cf74d79e6089ea25",
        "md5": "65e2cd4deaa3898ad17c805e406dd745",
        "sha256": "272fddb6f51e9e1592a58a1ff8d1eaf8d252a28c41665d4ea7246f0449f4ac50"
      },
      "downloads": -1,
      "filename": "cody-adapter-transformers-3.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "65e2cd4deaa3898ad17c805e406dd745",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 3240502,
      "upload_time": "2022-10-31T01:40:32",
      "upload_time_iso_8601": "2022-10-31T01:40:32.642632Z",
      "url": "https://files.pythonhosted.org/packages/cf/ee/edbd101049c8255189e0a5678e003c02ca18ab29b138cf74d79e6089ea25/cody-adapter-transformers-3.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}