{
  "info": {
    "author": "waking95",
    "author_email": "wang0624@foxmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# easy-bert\n\n1. [快速安装](#1-快速安装)\n2. [极速上手](#2-极速上手)\n    - [分类任务](#分类任务)\n    - [序列标注](#序列标注)\n    - [预训练](#预训练)\n3. [调参指南](#3-调参指南)\n    - [预训练模型](#预训练模型)\n    - [学习率](#学习率)\n    - [并行训练](#并行训练)\n    - [对抗训练](#对抗训练)\n    - [dropout_rate](#dropout_rate)\n    - [loss选择](#loss选择)\n    - [长文本](#长文本)\n    - [知识蒸馏](#知识蒸馏)\n    - [随机种子](#随机种子)\n    - [ONNX硬件加速](#ONNX硬件加速)\n    - [warmup](#warmup)\n    - [混合精度(fp16)](#混合精度fp16)\n    - [领域预训练](#领域预训练)\n4. [理论教程 && 源码解读](#4-理论教程--源码解读)\n\neasy-bert是一个中文NLP工具，提供诸多**bert变体调用**和**调参方法**，**极速上手**；清晰的设计和代码注释，也**很适合学习**。\n\n## 1. 快速安装\n\n主要支持两种安装方法：\n\n1. **PYPI安装**：\n   `pip install easy-zh-bert`\n\n   注意：因为和别的库重名，上传到pypi上的名字为[easy-zh-bert](https://pypi.org/project/easy-zh-bert/)\n2. **Github源码安装**：\n    - `pip install git+https://github.com/waking95/easy-bert.git`\n    - 可以指定具体的版本，如`0.5.0`，即：\n      `pip install git+https://github.com/waking95/easy-bert.git@v0.5.0`\n\n## 2. 极速上手\n\n上手前，请**确保**：\n\n1. 已从hugging\n   face官网下载好[chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext)，保存到某个目录，如：`./models/chinese-roberta-wwm-ext`；\n2. 创建好你将要保存模型的目录，如：`./tests/test_model`；\n\n#### 分类任务\n\n```python\nfrom easy_bert.bert4classification.classification_predictor import ClassificationPredictor\nfrom easy_bert.bert4classification.classification_trainer import ClassificationTrainer\n\npretrained_model_dir, your_model_dir = './models/chinese-roberta-wwm-ext', './tests/test_model'\ntexts = ['天气真好', '今天运气很差']\nlabels = ['正面', '负面']\n\ntrainer = ClassificationTrainer(pretrained_model_dir, your_model_dir)\ntrainer.train(texts, labels, validate_texts=texts, validate_labels=labels, batch_size=2, epoch=20)\n\npredictor = ClassificationPredictor(pretrained_model_dir, your_model_dir)\nlabels = predictor.predict(texts)\n```\n\n更多代码样例参考：[tests/test_bert4classification.py](tests/test_bert4classification.py)\n\n#### 序列标注\n\n```python\nfrom easy_bert.bert4sequence_labeling.sequence_labeling_predictor import SequenceLabelingPredictor\nfrom easy_bert.bert4sequence_labeling.sequence_labeling_trainer import SequenceLabelingTrainer\n\npretrained_model_dir, your_model_dir = './models/chinese-roberta-wwm-ext', './tests/test_model'\ntexts = [['你', '好', '呀'], ['一', '马', '当', '先', '就', '是', '好']]\nlabels = [['B', 'E', 'S'], ['B', 'M', 'M', 'E', 'S', 'S', 'S']]\n\ntrainer = SequenceLabelingTrainer(pretrained_model_dir, your_model_dir)\ntrainer.train(texts, labels, validate_texts=texts, validate_labels=labels, batch_size=2, epoch=20)\n\npredictor = SequenceLabelingPredictor(pretrained_model_dir, your_model_dir)\nlabels = predictor.predict(texts)\n```\n\n更多代码样例参考：[tests/test_bert4sequence_labeling.py](tests/test_bert4sequence_labeling.py)\n\n#### 预训练\n```python\nfrom easy_bert.bert4pretraining.mlm_trainer import MaskedLMTrainer\n\npretrained_model_dir, your_model_dir = './models/chinese-roberta-wwm-ext', './tests/test_model'\ntexts = [\n    '早上起床后，我发现今天天气还真是不错的。早上起床后，我发现今天天气还真是不错的。早上起床后，我发现今天天气还真是不错的。'\n]\ntrainer = MaskedLMTrainer(pretrained_model_dir, your_model_dir)\ntrainer.train(texts, batch_size=1, epoch=20)\n```\n\n更多代码样例参考：[tests/test_mlm.py](tests/test_mlm.py)\n\n## 3. 调参指南\n\n`Trainer`提供了丰富的参数可供选择\n\n### 预训练模型\n\n你可以快速替换预训练模型，即更改`pretrained_model_dir`参数，目前测试过的中文**预训练模型**包括：\n\n- [albert_chinese_base](https://huggingface.co/voidful/albert_chinese_base)\n- [chinese-bert-wwm](https://huggingface.co/hfl/chinese-bert-wwm)\n- [chinese-macbert-base](https://huggingface.co/hfl/chinese-macbert-base)\n- [bert-base-chinese](https://huggingface.co/bert-base-chinese)\n- [chinese-electra-180g-base-discriminator](https://huggingface.co/hfl/chinese-electra-180g-base-discriminator)\n- [chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext)\n- [TinyBERT_4L_zh](https://huggingface.co/huawei-noah/TinyBERT_4L_zh)\n- [bert-distil-chinese](https://huggingface.co/adamlin/bert-distil-chinese)\n- [longformer-chinese-base-4096](https://huggingface.co/schen/longformer-chinese-base-4096)\n\n可以优先使用`chinese-roberta-wwm-ext`\n\n### 学习率\n\nbert微调一般使用较小的学习率`learning_rate`，如：`5e-5`, `3e-5`, `2e-5`\n\n### 并行训练\n\n可以为Trainer或Predictor设置`enable_parallel=True`，加速训练或推理。启用后，默认使用单机上的所有GPU。\n\n### 对抗训练\n\n对抗训练是一种正则化方法，主要是在embedding上加噪，缓解模型过拟合，默认`adversarial=None`，表示不对抗。\n\n你可以设置：\n\n- `adversarial='fgm'`：表示使用FGM对抗方法；\n- `adversarial='pgd'`：表示使用PGD对抗方法；\n\n### dropout_rate\n\ndropout_rate随机丢弃一部分神经元来避免过拟合，隐含了集成学习的思想，默认`dropout_rate=0.5`\n\n### loss选择\n\n这里支持以下loss，通过`loss_type`参数来设置：\n\n- `cross_entropy_loss`：标准的交叉熵loss，**`ClassificationTrainer`默认**；\n- `label_smoothing_loss`：标签平滑loss，在label层面增加噪声，使用soft label替代hard label，**缓解过拟合**；\n- `focal_loss`：**focal loss在类别不均衡时比较有用**，它允许为不同的label设置代价权重，并对简单的样本进行打压；\n    - 你可以进一步设置`focal_loss_gamma`和`focal_loss_alpha`，默认`focal_loss_gamma=2` `focal_loss_alpha=None`\n    - 设置`focal_loss_alpha`时，请**确保它是一个标签权重分布**，如：三分类设置`focal_loss_alpha=[1, 1, 1.5]`，表示我们更关注label_id为2的标签，因为它的样本数更少；\n- `crf_loss`：**crf层学习标签与标签之间的转移**，仅支持序列标注任务，**`SequenceLabelingTrainer`默认**；\n    - 你可以进一步设置`crf_learning_rate`，**一般crf层会使用大一点的学习率**，确保转移矩阵学好，默认`crf_learning_rate=None`，表示会使用10倍的`learning_rate`；\n\n更多代码样例参考：[tests/test_bert4classification.py](tests/test_bert4classification.py#L44)  [tests/test_bert4sequence_labeling.py](tests/test_bert4sequence_labeling.py#L51)\n\n### 长文本\n\nBert的输入最多为512字，如果待处理的文本超过512字，你可以**截断**或者**分段**\n输入模型，也可以尝试Longformer模型：[longformer-chinese-base-4096](https://huggingface.co/schen/longformer-chinese-base-4096)，它使用稀疏自注意力，降低了自注意力的时空复杂度，将模型处理长度扩张到了`4096`\n\n### 知识蒸馏\n\nbert模型本身较重，资源受限下，想提高推理速度，知识蒸馏是一个不错的选择。\n\n这里可以选择：\n\n- `DistilBert`\n  ：是一个6层的Bert，预训练模型[bert-distil-chinese](https://huggingface.co/adamlin/bert-distil-chinese)在预训练阶段已经进行MLM任务的蒸馏，你可以**直接基于它进行下游任务的微调**；\n    - 理论上，推理速度可以获得40%的提升，获得97%的bert-base效果\n- `TinyBert`\n  ：[TinyBERT_4L_zh](https://huggingface.co/huawei-noah/TinyBERT_4L_zh)拥有4层、312的hidden_size，一般使用两阶段蒸馏，即下游任务也要蒸馏，可以使用`TinyBertDistiller`实现；\n    - TinyBert微调蒸馏时，向老师的soft label学习、向老师的hidden学习、向老师的embedding学习、向真实的label学习\n    - 理论上，4层的TinyBert，能够达到老师（Bert-base）效果的96.8%、参数量缩减为原来的13.3%、仅需要原来10.6%的推理时间\n\n**TinyBert蒸馏：分类**\n\n```python\nfrom easy_bert.bert4classification.classification_predictor import ClassificationPredictor\nfrom easy_bert.bert4classification.classification_trainer import ClassificationTrainer\nfrom easy_bert.tinybert_distiller import TinyBertDistiller\n\ntexts = ['天气真好', '今天运气很差']\nlabels = ['正面', '负面']\n\nteacher_pretrained, teacher_model_dir = './models/chinese-roberta-wwm-ext', './tests/test_model'\nstudent_pretrained, student_model_dir = './models/TinyBERT_4L_zh', './tests/test_model2'\n\n# 训练老师模型\ntrainer = ClassificationTrainer(teacher_pretrained, teacher_model_dir)\ntrainer.train(texts, labels, validate_texts=texts, validate_labels=labels, batch_size=2, epoch=20)\n\n# 蒸馏学生\ndistiller = TinyBertDistiller(\n    teacher_pretrained, teacher_model_dir, student_pretrained, student_model_dir,\n    task='classification'\n)\ndistiller.distill_train(texts, labels, max_len=20, epoch=20, batch_size=2)\n\n# 加载fine-tune蒸馏过的模型\npredictor = ClassificationPredictor(student_pretrained, student_model_dir)\nprint(predictor.predict(texts))\n```\n\n**TinyBert蒸馏：序列标注**\n\n```python\nfrom easy_bert.bert4sequence_labeling.sequence_labeling_predictor import SequenceLabelingPredictor\nfrom easy_bert.bert4sequence_labeling.sequence_labeling_trainer import SequenceLabelingTrainer\nfrom easy_bert.tinybert_distiller import TinyBertDistiller\n\ntexts = [['你', '好', '呀'], ['一', '马', '当', '先', '就', '是', '好']]\nlabels = [['B', 'E', 'S'], ['B', 'M', 'M', 'E', 'S', 'S', 'S']]\n\nteacher_pretrained, teacher_model_dir = './models/chinese-roberta-wwm-ext', './tests/test_model'\nstudent_pretrained, student_model_dir = './models/TinyBERT_4L_zh', './tests/test_model2'\n\n# 训练老师模型\ntrainer = SequenceLabelingTrainer(teacher_pretrained, teacher_model_dir, loss_type='crf_loss')\ntrainer.train(texts, labels, validate_texts=texts, validate_labels=labels, batch_size=2, epoch=20)\n\n# 蒸馏学生\ndistiller = TinyBertDistiller(\n    teacher_pretrained, teacher_model_dir, student_pretrained, student_model_dir,\n    task='sequence_labeling', hard_label_loss='crf_loss'\n)\ndistiller.distill_train(texts, labels, max_len=20, epoch=20, batch_size=2)\n\n# 加载fine-tune蒸馏过的模型\npredictor = SequenceLabelingPredictor(student_pretrained, student_model_dir)\nprint(predictor.predict(texts))\n```\n\n更多代码样例参考：[tests/test_tinybert_distiller.py](tests/test_tinybert_distiller.py)\n\n**关于`TinyBertDistiller`蒸馏参数**：\n\n- `task`：可选`classification` or `sequence_labeling`；\n- `enable_parallel`：是否并行，默认`False`。注意，启用并行可能会导致蒸馏速度变慢；\n- `hard_label_loss`：即针对label的loss计算，设置同`Trainer`的`loss_type`参数。默认`cross_entropy_loss`，序列标注推荐`crf_loss`；\n- `temperature`：蒸馏温度系数，一般大于`1`较好，默认为`4`，可在`1~10`之间调试；\n- `hard_label_weight`：hard label的loss权重，默认为`1`；\n- `kd_loss_type`：soft label的loss类型，即向老师的输出概率学习，默认为`ce`，即交叉熵；\n- `kd_loss_weight`：kd_loss的权重，可以稍微放大其权重，即加强向老师的soft label学习，默认为`1.2`；\n- `lr`：蒸馏学习率，一般设置较大，这里默认`1e-4`；\n- `ckpt_frequency`：一个epoch存ckpt_frequency次模型，默认为`1`；\n- `epoch`：迭代轮数，一般蒸馏时设置较大的epoch，如`20~50`，默认为`20`；\n\n### 随机种子\n\n你可以设置`random_seed`，来控制随机种子，默认`random_seed=0`。\n\n### ONNX硬件加速\n\n可以将torch模型转为ONNX格式，通过微软的onnxruntime实现**推理阶段的硬件加速**，调用`Predictor`的`transform2onnx()`可以实现转换，代码样例参考 [tests/test_onnx.py](tests/test_onnx.py)。\n\n这里**注意**：\n\n1. cpu下请使用onnxruntime库，而不是onnxruntime-gpu库，参见 [setup.py](setup.py) 里`setup`函数的`install_requires`参数；\n2. onnxruntime-gpu==1.4.0仅适合cuda10.1 cuDNN7.6.5，更多版本兼容参考：\n   https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirements\n\n### warmup\nwarmup使用**动态的学习率**（一般lr先增大 后减小），\n- lr一开始别太大，有助于缓解模型在初始阶段，对前几个batch数据过拟合；\n- lr后面小一点，有助于模型后期的稳定；\n\n可以通过`Trainer`的参数来控制warmup：\n- `warmup_type`：声明warmup的种类，默认为`None`，表示不启用warmup，即学习率恒定；\n  - 可以设置为`constant`，表示使用恒定学习率，lr曲线为 <img src=\"./docs/images/constant_warmup.png\" width=500>\n  - 可以设置为`cosine`，表示余弦曲线学习率，lr曲线为 <img src=\"./docs/images/cosine_warmup.png\" width=500>\n  - 可以设置为`linear`，表示线性学习率，lr曲线为 <img src=\"./docs/images/linear_warmup.png\" width=500>\n- `warmup_step_num`：增加阶段，需要多少步到达设置的lr（上图中峰值）；\n  - 可以为`int`类型，表示步数；\n  - 也可以为`float`类型，表示总步数的比例，`总步数 = batch_num * epoch`。如：总共训练1000步，设置`warmup_step_num=0.1`，表示warmup_step_num实际为100；\n\n更多代码样例参考 [tests/test_warmup.py](tests/test_warmup.py)。\n\n### 混合精度(fp16)\ntorch里面默认的浮点数是单精度的，即float32。我们可以**将部分模型参数用float16，即fp16半精度来表示**，一来可以降低显存的占用，二来可以提升训练和推理的速度。\n\n`Trainer`和`Predictor`都提供了`enable_fp16`参数来控制是否启用fp16，默认为`False`。\n\n更多代码样例参考 [tests/test_fp16.py](tests/test_fp16.py)。\n\n### 领域预训练\nbert已经提供了通用领域的预训练。为了提升下游任务的效果，你可能需要**在特定领域（如金融、医疗等）上进行预训练**，当前主要支持了MLM的预训练（NSP任务的预训练已被证明没什么作用）\n\n[MaskedLMTrainer](easy_bert/bert4pretraining/mlm_trainer.py) 提供了非常好用的接口，可以直接来进行训练\n\n详情请参考 [tests/test_mlm.py](tests/test_mlm.py)\n\n注意：\n- mlm的实现为wwm，即**全词mask**。分词主要基于词库，需要传入`word_dict`参数，可以使用jieba词库 https://github.com/fxsjy/jieba/blob/master/jieba/dict.txt ，建议把低频词滤掉；\n\n## 4. 理论教程 && 源码解读\n- [docs/Attention、Transformer和Bert.md](docs/Attention、Transformer和Bert.md)\n- [docs/Bert的常见变体.md](docs/Bert的常见变体.md)\n- [docs/Dropout.md](docs/Dropout.md)\n- [docs/ONNX推理加速.md](docs/ONNX推理加速.md)\n- [docs/Warmup.md](docs/Warmup.md)\n- [docs/对抗训练.md](docs/对抗训练.md)\n- [docs/常用的loss.md](docs/常用的loss.md)\n- [docs/并行训练.md](docs/并行训练.md)\n- [docs/混合精度.md](docs/混合精度.md)\n- [docs/知识蒸馏.md](docs/知识蒸馏.md)\n- [docs/长文本.md](docs/长文本.md)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/waking95/easy-bert",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "easy-zh-bert",
    "package_url": "https://pypi.org/project/easy-zh-bert/",
    "platform": null,
    "project_url": "https://pypi.org/project/easy-zh-bert/",
    "project_urls": {
      "Bug Tracker": "https://github.com/waking95/easy-bert/issues",
      "Homepage": "https://github.com/waking95/easy-bert"
    },
    "release_url": "https://pypi.org/project/easy-zh-bert/0.6.0/",
    "requires_dist": [
      "torch (==1.6.0)",
      "transformers (==3.1.0)",
      "scikit-learn (==0.24.0)",
      "numpy (==1.17.0)",
      "datasets (==1.16.0)",
      "textbrewer (==0.2.0)",
      "onnxruntime-gpu (==1.4.0)"
    ],
    "requires_python": "",
    "summary": "easy-bert是一个中文NLP工具，提供诸多bert变体调用和调参方法，极速上手；清晰的设计和代码注释，也很适合学习",
    "version": "0.6.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13291938,
  "releases": {
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "847b16ca1497744a5f4833bda3d359a04f72e9312df3924cffd1cf5ec6156221",
          "md5": "a08a097e0e06fbabf7cbc87c5f5d3916",
          "sha256": "f1cfcb013e3b6e355336f8c6af6a9d79d6dab6de54165405d8b58a83e963e8a8"
        },
        "downloads": -1,
        "filename": "easy_zh_bert-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a08a097e0e06fbabf7cbc87c5f5d3916",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 40914,
        "upload_time": "2021-12-20T09:16:40",
        "upload_time_iso_8601": "2021-12-20T09:16:40.105880Z",
        "url": "https://files.pythonhosted.org/packages/84/7b/16ca1497744a5f4833bda3d359a04f72e9312df3924cffd1cf5ec6156221/easy_zh_bert-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0fd0671530c13e82f55fb48a639490a394b280dd491361a5e4010957710a7eeb",
          "md5": "e50a4462b4cbb3d3df688695dfc5f502",
          "sha256": "2a1c7943d5d7f80f8015e4fba95b7e798427cec204f36995236a7d8a40624eab"
        },
        "downloads": -1,
        "filename": "easy-zh-bert-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "e50a4462b4cbb3d3df688695dfc5f502",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 27845,
        "upload_time": "2021-12-20T09:16:42",
        "upload_time_iso_8601": "2021-12-20T09:16:42.191623Z",
        "url": "https://files.pythonhosted.org/packages/0f/d0/671530c13e82f55fb48a639490a394b280dd491361a5e4010957710a7eeb/easy-zh-bert-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e083522044d8513bd739dfda984c281fdc5ab6c9d2278c4faa91299187a23feb",
          "md5": "40f378c77ec4672840f750cc0643b0b6",
          "sha256": "56b0d2746a9df6b9986489de1b102c7f2b9763e20771a8bae3982acd9c9392fb"
        },
        "downloads": -1,
        "filename": "easy_zh_bert-0.4.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "40f378c77ec4672840f750cc0643b0b6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 40973,
        "upload_time": "2021-12-20T09:34:58",
        "upload_time_iso_8601": "2021-12-20T09:34:58.009709Z",
        "url": "https://files.pythonhosted.org/packages/e0/83/522044d8513bd739dfda984c281fdc5ab6c9d2278c4faa91299187a23feb/easy_zh_bert-0.4.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c434a19f6b0e52211865cd0906a41323b359a5087ae6d2e19d8daa5da391c595",
          "md5": "c2f08f15b9e6e44003829bb7f3f83925",
          "sha256": "01cdbb6a48843902d6ee747b948a25e3e314bc6047518ff196538852d8c1337a"
        },
        "downloads": -1,
        "filename": "easy-zh-bert-0.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c2f08f15b9e6e44003829bb7f3f83925",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 27930,
        "upload_time": "2021-12-20T09:34:59",
        "upload_time_iso_8601": "2021-12-20T09:34:59.683788Z",
        "url": "https://files.pythonhosted.org/packages/c4/34/a19f6b0e52211865cd0906a41323b359a5087ae6d2e19d8daa5da391c595/easy-zh-bert-0.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7b8a9c1d69be0dbe633dd070098e6c8f7a13b889a18e62b7a0c2d8ebee0648d0",
          "md5": "f745472ef43fa32a00c55aedcc818235",
          "sha256": "7b096c42c7e2f62e29bdffebd891594a4a8b41b0d0686becbfb5d83da44be2e2"
        },
        "downloads": -1,
        "filename": "easy_zh_bert-0.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f745472ef43fa32a00c55aedcc818235",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 44526,
        "upload_time": "2021-12-23T09:37:44",
        "upload_time_iso_8601": "2021-12-23T09:37:44.960001Z",
        "url": "https://files.pythonhosted.org/packages/7b/8a/9c1d69be0dbe633dd070098e6c8f7a13b889a18e62b7a0c2d8ebee0648d0/easy_zh_bert-0.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0b1efd1718a80fc807e7b9fd3fafeebfbc495146bee2a762e471fa81a0df7907",
          "md5": "65ab5d80e690789c86a611270afe4457",
          "sha256": "01f6e9c247eaefb194a5c6284ac1e86857c4d665d80a0cae681488cb57c09858"
        },
        "downloads": -1,
        "filename": "easy-zh-bert-0.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "65ab5d80e690789c86a611270afe4457",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 31797,
        "upload_time": "2021-12-23T09:37:46",
        "upload_time_iso_8601": "2021-12-23T09:37:46.508894Z",
        "url": "https://files.pythonhosted.org/packages/0b/1e/fd1718a80fc807e7b9fd3fafeebfbc495146bee2a762e471fa81a0df7907/easy-zh-bert-0.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4560d34c6058d0f295186931180bf0118600b4c5e475158b66f32396b5c2b880",
          "md5": "1ea3e04cf1e83503b1ca837616cb1f67",
          "sha256": "0dc3d5c3814eba1cbabf34bc62f4f61ad85eadd2ff49734ac0ddc7256ce6528c"
        },
        "downloads": -1,
        "filename": "easy_zh_bert-0.6.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1ea3e04cf1e83503b1ca837616cb1f67",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 50182,
        "upload_time": "2022-03-25T10:54:36",
        "upload_time_iso_8601": "2022-03-25T10:54:36.490363Z",
        "url": "https://files.pythonhosted.org/packages/45/60/d34c6058d0f295186931180bf0118600b4c5e475158b66f32396b5c2b880/easy_zh_bert-0.6.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7c0bf3bc5cc031c8b8c5195c647b6754a30243f1d8de059b2745e2092936ed10",
          "md5": "55f70fecda5baf58ad105393d3914415",
          "sha256": "ada5b2a28181a9d6d982abbc9188ad13f2ff5cf72591e33b199faf9def0bc7fd"
        },
        "downloads": -1,
        "filename": "easy-zh-bert-0.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "55f70fecda5baf58ad105393d3914415",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 37470,
        "upload_time": "2022-03-25T10:54:38",
        "upload_time_iso_8601": "2022-03-25T10:54:38.569526Z",
        "url": "https://files.pythonhosted.org/packages/7c/0b/f3bc5cc031c8b8c5195c647b6754a30243f1d8de059b2745e2092936ed10/easy-zh-bert-0.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4560d34c6058d0f295186931180bf0118600b4c5e475158b66f32396b5c2b880",
        "md5": "1ea3e04cf1e83503b1ca837616cb1f67",
        "sha256": "0dc3d5c3814eba1cbabf34bc62f4f61ad85eadd2ff49734ac0ddc7256ce6528c"
      },
      "downloads": -1,
      "filename": "easy_zh_bert-0.6.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1ea3e04cf1e83503b1ca837616cb1f67",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 50182,
      "upload_time": "2022-03-25T10:54:36",
      "upload_time_iso_8601": "2022-03-25T10:54:36.490363Z",
      "url": "https://files.pythonhosted.org/packages/45/60/d34c6058d0f295186931180bf0118600b4c5e475158b66f32396b5c2b880/easy_zh_bert-0.6.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7c0bf3bc5cc031c8b8c5195c647b6754a30243f1d8de059b2745e2092936ed10",
        "md5": "55f70fecda5baf58ad105393d3914415",
        "sha256": "ada5b2a28181a9d6d982abbc9188ad13f2ff5cf72591e33b199faf9def0bc7fd"
      },
      "downloads": -1,
      "filename": "easy-zh-bert-0.6.0.tar.gz",
      "has_sig": false,
      "md5_digest": "55f70fecda5baf58ad105393d3914415",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 37470,
      "upload_time": "2022-03-25T10:54:38",
      "upload_time_iso_8601": "2022-03-25T10:54:38.569526Z",
      "url": "https://files.pythonhosted.org/packages/7c/0b/f3bc5cc031c8b8c5195c647b6754a30243f1d8de059b2745e2092936ed10/easy-zh-bert-0.6.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}