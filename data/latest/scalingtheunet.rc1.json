{
  "info": {
    "author": "Deutsches Elektronen-Synchrotron DESY",
    "author_email": "ivo.baltruschat@desy.de",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "Scaling the U-net\n==============================\n<a href=\"https://www.python.org/\"><img alt=\"Python\" src=\"https://img.shields.io/badge/-Python 3.9-3670A0?style=flat-square&logo=python&logoColor=ffdd54\"></a>\n<a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"Tensorflow\" src=\"https://img.shields.io/badge/-Tensorflow 2.7-%23FF6F00?style=flat-square&logo=Tensorflow&logoColor=white\"></a>\n<a href=\"https://hydra.cc/\"><img alt=\"Config: hydra\" src=\"https://img.shields.io/badge/config-hydra 1.1-89b8cd?style=flat-square&labelColor=gray\"></a>\n\n## Requirements to use this project\n- poetry\n- python 3.9\n\nYou can use many ways to get a specific python version. I recommend Conda\n- [Conda installation](https://docs.conda.io/en/latest/miniconda.html) <br>\nOnce installed you need to install a python 3.9 interpreter:\n````yaml\n# Conda\nconda create -n sun_py39 python=3.9\nconda activate sun_py39\n````\n\nInstall poetry:\n````yaml\n# Poetry for Ubuntu\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -\n````\n\n## How to use\n````yaml\n# clone project\ngit clone https://gitlab.desy.de/ivo.matteo.baltruschat/scaling_the_u_net.git\ncd scaling_the_u_net\n\npoetry install\n````\n 1. Rename `.env.example` to `.env`. Set `PROJECT_DIR` to your root path. If you want [wandb](https://wandb.ai/) support, set `WANDB_USER` to your user name\n 2. Run `dvc checkout --summary` to restore raw and processed data from the `shared_cache`(/beegfs/desy/group/it/ReferenceData/dvc/shared_cache).\n 5. Now your can simply run `python run_training.py experiment=baseline`.\n\nAfterwards you can use [run_testing.py](run_testing.py) to evaluate your trained model on the test data\n - run `python run_testing.py experiment_dir=[your_experiment_name]` (replace [...] with your experiment subdir in the experiment folder)\n -\n## Guide\n\n### How To Get Started\n\n- First, you should probably get familiar with [Tensorflow](https://www.tensorflow.org/)\n- Next, go through [Hydra quick start guide](https://hydra.cc/docs/intro/) and [basic Hydra tutorial](https://hydra.cc/docs/tutorials/basic/your_first_app/simple_cli/)\n  <br>\n\n### How it works\n\nBy design, every run is initialized by [run_training.py](run_training.py) file. All modules are dynamically instantiated from module paths specified in config. Example model config:\n\n```yaml\nfilters: 64 # Channel output of first layer -> other depend on this one\nkernel_size: [3, 3] # Kernel size for all Conv layers\nnum_layers: 5 # Number of u-net blocks\nregularization_factor_l1: -1 # If -1, no regularization.\nregularization_factor_l2: -1 # If -1, no regularization.\nlayer_order: \"CNAD\" # Choose the oder of Conv,Norm,Act,Drop [\"CADN\", \"CAND\", \"CDAN\", \"CDNA\", \"CNDA\", \"CNAD\"]\ndropout_type: \"spatial\" # [\"standard\", \"spatial\"] Type of dropout used in a \"CNAD\" stack\ndropout_conv: -1 # If -1, no dropout used in a \"CNAD\" stack\nuse_norm: \"BatchNorm\" # [\"none\", \"BatchNorm\", \"GroupNorm\"] Type of normalization used in a \"CNAD\" stack\nactivation: \"mish\" # [\"relu\", \"leakyReLU\", \"mish\"] Type of activation used in a \"CNAD\" stack\ndropout: -1 # Dropout before the Conv-bock of latent space. If -1, no dropout used.\noutput_activation: \"linear\" # [\"linear\", \"softmax\"] If \"linear\", model will provide logits and not prediction probabilities\n\n```\n\nThis allows you to easily iterate over new models!<br>\nEvery time you create a new one, just specify its module path and parameters in appriopriate config file. <br>\nThe whole pipeline managing the instantiation logic is placed in [scalingtheunet/executor/training.py](scalingtheunet/executor/training.py).\n\n<br>\n\n### Main Project Configuration\nLocation: [configs/config.yaml](configs/config.yaml)<br>\nMain project config contains default training configuration.<br>\nIt determines how config is composed when simply executing command `python run_training.py`.\nYou can overwrite all parameters by cmd arguments `python run_training.py experiment_name=test123 model.filters=16`<br>\n\n<details>\n<summary><b>Show main project configuration</b></summary>\n\n```yaml\n# specify here default training configuration\n# @package _global_\n\n# specify here default training configuration\ndefaults:\n  - _self_\n  - trainer: default.yaml\n  - model: simple_unet.yaml\n  - datamodule: mg_full.yaml\n  - callbacks: default.yaml\n  - logger: default.yaml\n\n  #- mode: default.yaml\n\n  - experiment: null\n\n  # enable color logging\n  - override hydra/hydra_logging: colorlog\n  - override hydra/job_logging: colorlog\n\n# name of the run, accessed by loggers\n# allows for custom naming of the experiment\nname: ???\n\ncurrent_time: ${now:%Y-%m-%d}_${now:%H-%M-%S}\n\nhydra:\n  # sets output paths for all file logs to `logs/experiment/name'\n  run:\n    dir: experiments/${name}/${current_time}\n  sweep:\n    dir: experiments/${name}/${current_time}\n    subdir: ${hydra.job.num}\n  output_subdir: \"hydra_training\"\n\n# path to original working directory\n# hydra hijacks working directory by changing it to the current log directory,\n# so it's useful to have this path as a special variable\n# https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory\nwork_dir: ${hydra:runtime.cwd}\n\n# path to folder with data\ndata_dir: ${work_dir}/data/\n\n# mlflow path\nmlflow_dir: null #${work_dir}/mlflow/\n\n# tensorboard path\ntensorboard_dir: ${work_dir}/tensorboard/\n\n# pretty print config at the start of the run using Rich library\nprint_config: True\n\n# pretty print history after the run using Rich library\nprint_history: True\n\n# disable python warnings if they annoy you\nignore_warnings: True\n\n# seed for random number generators in pytorch, numpy and python.random\nseed: \"0xCAFFEE\"\n```\n\n</details>\n<br>\n\n### Experiment Configuration\n\nLocation: [configs/experiment](configs/experiment)<br>\nYou should store all your experiment configurations in this folder.<br>\nExperiment configurations allow you to overwrite parameters from main project configuration.\n\n**Simple example**\n\n```yaml\n# @package _global_\n\n# to execute this experiment run:\n# python run.py experiment=example_simple.yaml\n\ndefaults:\n  - override /trainer: default.yaml\n  - override /model: simple_unet.yaml\n  - override /datamodule: mg_full.yaml\n  - override /callbacks: default.yaml\n  - override /logger: default.yaml\n\n\n# all parameters below will be merged with parameters from default configurations set above\n# this allows you to overwrite only specified parameters\n\nname: simple_example\nseed: \"OxCAFFEE\"\n\ntrainer:\n  epochs: 5\n\nmodel:\n  filters: 16\n  num_layers: 4\n  activation: \"relu\"\n\ndatamodule:\n  batch_size: 8\n```\n\n</details>\n\nProject Organization\n------------\n\n```\n    ├──.venv                <- Local poetry environment\n    │   └──.gitkeep\n    ├── configs\n    ├── data\n    │   ├── external        <- Data from third party sources.\n    │   ├── interim         <- Intermediate data that has been transformed.\n    │   ├── processed       <- The final, canonical data sets for modeling.\n    │   └── raw             <- The original, immutable data dump.\n    ├── logs                <- Tensorboard logging folder; Will be created by training\n    ├── experiments         <- All your runs will be saved here; Will be created by training\n    ├── mlflow              <- MLflow logging folder; Will be created by training\n    ├── models              <- Trained and serialized models, model predictions, or model summaries\n    ├── notebooks           <- Jupyter notebooks. Naming convention is a number (for ordering),\n    │                         the creator's initials, and a short `-` delimited description, e.g.\n    │                         `1.0-jqp-initial-data-exploration`.\n    ├── references          <- Data dictionaries, manuals, and all other explanatory materials.\n    ├── reports             <- Generated analysis as HTML, PDF, LaTeX, etc.\n    │   └── figures         <- Generated graphics and figures to be used in reporting\n    ├── test                <- Data dictionaries, manuals, and all other explanatory materials.\n    ├── scalingtheunet                        <- Source code for use in this project.\n    │   │\n    │   ├── data                              <- Scripts to download or generate data\n    │   │   ├── __init__.py\n    │   │   └── make_dataset.py               <- TODO\n    │   ├── dataloaders                       <- Scripts to handel and load the preprocessed data\n    │   │   ├── __init__.py                   <- TODO\n    │   │   └── hzg_mg_tomo.py                <- TODO\n    │   ├── evaluation                        <- Scripts to do evaluation of the results\n    │   │   └── __init__.py                   <- TODO\n    │   ├── executor                          <- Scripts to train, eval and test models\n    │   │   ├── __init__.py                   <- TODO\n    │   │   ├── testing_9axes.py\n    │   │   └── training.py\n    │   ├── models                            <- Scripts to define model architecture\n    │   │   ├── __init__.py                   <- TODO\n    │   │   └── simple_unet.py\n    │   ├── utils                             <- TODO\n    │   │   ├── __init__.py                   <- TODO\n    │   │   ├── file_system.py                <- TODO\n    │   │   ├── logger.py                     <- TODO\n    │   │   ├── losses.py                     <- TODO\n    │   │   ├── metrics.py                    <- TODO\n    │   │   ├── my_callback.py                <- TODO\n    │   │   └── utils.py                      <- TODO\n    │   │\n    │   ├── visualization                     <- Scripts to create exploratory and results oriented\n    │   │                                       visualizations\n    │   └── __init__.py                       <- Makes {{ cookiecutter.module_name }} a Python module\n    │\n    ├── .editorconfig         <- file with format specification. You need to install\n    │                             the required plugin for your IDE in order to enable it.\n    ├── .gitignore         <- file that specifies what should we commit into\n    │                             the repository and we should not.\n    ├── LICENSE\n    ├── Makefile            <- Makefile with commands like `make data` or `make train`\n    ├── poetry.toml         <- poetry config file to install enviroment locally\n    ├── poetry.lock         <- lock file for dependencies. It is used to install exactly\n    │                         the same versions of dependencies on each build\n    ├── pyproject.toml      <- The project's dependencies for reproducing the\n    │                         analysis environment\n    ├── README.md           <- The top-level README for developers using this project.\n    └── setup.cfg           <- configuration file, that is used by all tools in this project\n```\n\n--------\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scalingtheunet",
    "package_url": "https://pypi.org/project/scalingtheunet/",
    "platform": null,
    "project_url": "https://pypi.org/project/scalingtheunet/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/scalingtheunet/0.1.0/",
    "requires_dist": [
      "tensorflow (>=2.8,<2.9)",
      "mlflow (>=1.2,<2.0)",
      "monai (>=0.7.0,<0.8.0)",
      "scikit-learn (>=1.0,<2.0)",
      "albumentations (>=1.0.3,<2.0.0)",
      "orderedattrdict (>=1.6.0,<2.0.0)",
      "tqdm (>=4.62.2,<5.0.0)",
      "natsort (>=7.1.1,<8.0.0)",
      "python-dotenv (>=0.19.2,<0.20.0)",
      "numba (>=0.54.0,<0.55.0)",
      "ray (>=1.8,<2.0)",
      "hydra-core (>=1.1.1,<2.0.0)",
      "hydra-colorlog (>=1.1.0,<2.0.0)",
      "hydra-optuna-sweeper (>=1.1.1,<2.0.0)",
      "connected-components-3d (>=3.2.1,<4.0.0)",
      "scikit-image (>=0.17,<0.18)",
      "seaborn (>=0.11.2,<0.12.0)",
      "seaborn-image (>=0.4.4,<0.5.0)",
      "rich (>=10.12.0,<11.0.0)",
      "wandb (>=0.12.7,<0.13.0)",
      "flatdict (>=4.0.1,<5.0.0)",
      "icecream (>=2.1.1,<3.0.0)",
      "jupyterlab (>=3.2.9,<4.0.0)",
      "grpcio (==1.43)",
      "mpire (>=2.3.3,<3.0.0)",
      "opencv-python (==4.5.1.48)",
      "plotly (>=5.7.0,<6.0.0)",
      "ipywidgets (>=7.7.0,<8.0.0)",
      "dvc[all] (==2.9.5)",
      "imagecodecs (>=2022.2.22,<2023.0.0)"
    ],
    "requires_python": ">=3.9,<3.10",
    "summary": "This project is the source code to our paper.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14001767,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8e04f19a54daa5597cff3170d84cedd3712c9dc5d1fe83a8a1a0820ce16588e1",
          "md5": "477f282f3b848eb11139bdc1356b6bf2",
          "sha256": "2d527d74f42f3d9cec2f2ec5a3b14ade80f0dac1e6c7322bd8ac183d6fa485d4"
        },
        "downloads": -1,
        "filename": "scalingtheunet-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "477f282f3b848eb11139bdc1356b6bf2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9,<3.10",
        "size": 41658,
        "upload_time": "2022-06-01T14:57:18",
        "upload_time_iso_8601": "2022-06-01T14:57:18.667883Z",
        "url": "https://files.pythonhosted.org/packages/8e/04/f19a54daa5597cff3170d84cedd3712c9dc5d1fe83a8a1a0820ce16588e1/scalingtheunet-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "18ff93eadddb91602112f5e8e3b86fc8d6f6ac78635a999b5d60d24bcea84b46",
          "md5": "fc575d0b591b12259bce582a807d979c",
          "sha256": "2622c9069013460690b42a91001f57e57c864e9cf087deb3a0b50385784d5a2d"
        },
        "downloads": -1,
        "filename": "scalingtheunet-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "fc575d0b591b12259bce582a807d979c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9,<3.10",
        "size": 38807,
        "upload_time": "2022-06-01T14:57:21",
        "upload_time_iso_8601": "2022-06-01T14:57:21.048892Z",
        "url": "https://files.pythonhosted.org/packages/18/ff/93eadddb91602112f5e8e3b86fc8d6f6ac78635a999b5d60d24bcea84b46/scalingtheunet-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8e04f19a54daa5597cff3170d84cedd3712c9dc5d1fe83a8a1a0820ce16588e1",
        "md5": "477f282f3b848eb11139bdc1356b6bf2",
        "sha256": "2d527d74f42f3d9cec2f2ec5a3b14ade80f0dac1e6c7322bd8ac183d6fa485d4"
      },
      "downloads": -1,
      "filename": "scalingtheunet-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "477f282f3b848eb11139bdc1356b6bf2",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.9,<3.10",
      "size": 41658,
      "upload_time": "2022-06-01T14:57:18",
      "upload_time_iso_8601": "2022-06-01T14:57:18.667883Z",
      "url": "https://files.pythonhosted.org/packages/8e/04/f19a54daa5597cff3170d84cedd3712c9dc5d1fe83a8a1a0820ce16588e1/scalingtheunet-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "18ff93eadddb91602112f5e8e3b86fc8d6f6ac78635a999b5d60d24bcea84b46",
        "md5": "fc575d0b591b12259bce582a807d979c",
        "sha256": "2622c9069013460690b42a91001f57e57c864e9cf087deb3a0b50385784d5a2d"
      },
      "downloads": -1,
      "filename": "scalingtheunet-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "fc575d0b591b12259bce582a807d979c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.9,<3.10",
      "size": 38807,
      "upload_time": "2022-06-01T14:57:21",
      "upload_time_iso_8601": "2022-06-01T14:57:21.048892Z",
      "url": "https://files.pythonhosted.org/packages/18/ff/93eadddb91602112f5e8e3b86fc8d6f6ac78635a999b5d60d24bcea84b46/scalingtheunet-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}