{
  "info": {
    "author": "Ruben Branco, Luís Gomes",
    "author_email": "rmbranco@fc.ul.pt",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3 :: Only",
      "Topic :: Text Processing :: Filters"
    ],
    "description": "<p align=\"center\"><img src=\"logo-w-text.png\" alt=\"Overlapy Logo\" /></p>\n\n--------------------------------------------------------------------------------\n\n<p align=\"center\">\n  <a href=\"#about\">About</a> ⚭\n  <a href=\"#installation\">Installation</a> ⚭\n  <a href=\"#usage\">Usage</a> ⚭\n  <a href=\"#citation\">Citation</a>\n</p>\n\n## About\n\nOverlapy is a Python package developed to evaluate textual overlap (N-Grams) between two volumes of text. In fact, it comes from the necessity of evaluating \"data contamination\" between pre-training datasets for Language Models and testsets of NLP tasks. This problem is starting to become relevant: as models become ever larger, rapidly entering the trillions of parameters mark, they can fit larger pre-training language modelling datasets, which have started to inch closer to the terabytes mark.\n\nThe web is a source of nearly unlimited natural language text, making it one of the favourite sources to obtain unlabelled text. Websites like Reddit (<https://reddit.com/>) aggregate content and outbound links in inconcievable amounts. However, these resources are not exclusive to the language modelling task, and other tasks use them to construct even labelled datasets. As web crawlers extend their scrapped nodes, the probability of obtaining text that has been used in other tasks grows larger. With the capability of these models to memorize spans of text, it can just so happen that specific spans from examples of a tasks' testset could have been found in the pre-training dataset. The language model could have memorized it, making it previously seen data less than ideal as we want to test our models with unseen (o.o.d) data. This constitutes a problem for the present and future.\n\nThe methodology followed for this implementation is described in GPT-3's paper appendix (<https://arxiv.org/abs/2005.14165>). It can be decomposed into three main parts: tokenize, choosing N-Gram size, calculate N-Gram collisions between pre-training datasets and testsets.\n\n1. A token is considered an alphanumeric character, delimited by whitespace, and lowercased. In overlapy, the tokenization function is arbitrary (user-defined), and does not need to follow this definition.\n2. N-Gram size is determined to be the 5th percentile of the distribution of testset examples lengths. The authors set a minimum size of 8 and maximum size of 13. We follow this definition, however, allow the user to redefine the percentile, minimum and maximum size.\n3. Collisions are calculated by our package using the Aho-Corasick algorithm (<https://dl.acm.org/doi/10.1145/360825.360855>). The testsets are decomposed into N-Grams. Subsequently, we distribute the pre-training dataset to a pool of workers, calculating matches between the testset N-Grams and examples from the pre-training dataset.\n\n\n## Installation\n\nPackaged developed to work with Python 3+. Some examples require Python 3.6+ and nltk (<http://www.nltk.org/>) installed.\n\ntqdm (<https://github.com/tqdm/tqdm>) not mandatory to have installed but is recommended to track the progress, especially for jobs with several hundreds of gigabytes of text.\n\n```bash\npip install overlapy\n```\n\n## Usage\n\nIt follows the contents of an usage example from one of our examples found [here](examples/).\n\n```python\nfrom overlapy import OverlapyTestSet, Overlapy\n\npretraining_dataset = [\n    \"A B A C D E F G\",\n    \"A C F J K H E\",\n    \"V L N M Q\",\n    \"A B A C Ç T Z V E\",\n    \"L M N O P\",\n]\n\ntestset_examples = [\n    \"B A B A C O Q W R\",  # Match A B A C with #1, #4 from pretraining_dataset\n    \"O P Q F J K H\",  # Match F J K H with #2 from pretraining_dataset\n    \"W E R E\",  # No match\n    \"I E T Z V E L\",  # Match T Z V E with #4 from pretraining_dataset\n    \"K E K W\",  # No match\n]\n# Total examples matched: 3\n\n\ndef tokenizer(s):\n    # Simple tokenization by whitespace.\n    return s.split()\n\n\n# We'll override the parameter min_n and set it to 1 as we want the ngram value to be allowed\n# to be less than 8. The testset examples were constructed for it to be 4, actually.\ntestset = OverlapyTestSet(\n    \"test\", min_n=1, examples=[tokenizer(s) for s in testset_examples]\n)\nprint(f\"N value: {testset.compute_n()}\")\nprint(f\"# NGrams: {len(set(map(tuple, list(testset.ngrams()))))}\")\n\n# We create an Overlapy object, handing three arguments:\n#   * Testsets: A list of OverlapyTestSet objects that we want to study.\n#   * Dataset: Dataset we want to calculate collisions with\n#   * n_workers: Number of worker processes to use\noverlapy = Overlapy(\n    testsets=[testset],\n    dataset=[tokenizer(s) for s in pretraining_dataset],\n    n_workers=2,\n)\n# Let's run and get the matches\nmatches = overlapy.run()\n\n# We should be getting 3 testset examples that have been flagged for matches.\n#    #0 matches on A B A C\n#    #1 matches on F J K H\n#    #3 matches on T V Z E\n# As we had noted above\nprint(f\"Matches: {list(testset.get_matches(matches))}\")\n```\n\n## Citation\n\nBibtex citation will be available soon.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/nlx-group/overlapy",
    "keywords": "text tool",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "overlapy",
    "package_url": "https://pypi.org/project/overlapy/",
    "platform": "",
    "project_url": "https://pypi.org/project/overlapy/",
    "project_urls": {
      "Homepage": "https://github.com/nlx-group/overlapy"
    },
    "release_url": "https://pypi.org/project/overlapy/0.0.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Library to help compute textual overlap.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11539705,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fe37acefa86fd25d4e1894771575af0562b69d7a1bc2d6945f12adda86d07778",
          "md5": "0827020940c9e28ff33d9af213a03087",
          "sha256": "6103f768c56d02f53f303f938855cf76d4bf7914f7c8cc06fb886465de2853ee"
        },
        "downloads": -1,
        "filename": "overlapy-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0827020940c9e28ff33d9af213a03087",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5610,
        "upload_time": "2021-09-21T13:12:15",
        "upload_time_iso_8601": "2021-09-21T13:12:15.835221Z",
        "url": "https://files.pythonhosted.org/packages/fe/37/acefa86fd25d4e1894771575af0562b69d7a1bc2d6945f12adda86d07778/overlapy-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fe37acefa86fd25d4e1894771575af0562b69d7a1bc2d6945f12adda86d07778",
        "md5": "0827020940c9e28ff33d9af213a03087",
        "sha256": "6103f768c56d02f53f303f938855cf76d4bf7914f7c8cc06fb886465de2853ee"
      },
      "downloads": -1,
      "filename": "overlapy-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "0827020940c9e28ff33d9af213a03087",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 5610,
      "upload_time": "2021-09-21T13:12:15",
      "upload_time_iso_8601": "2021-09-21T13:12:15.835221Z",
      "url": "https://files.pythonhosted.org/packages/fe/37/acefa86fd25d4e1894771575af0562b69d7a1bc2d6945f12adda86d07778/overlapy-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}