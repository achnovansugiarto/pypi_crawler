{
  "info": {
    "author": "CPA Ferrere | Data Analytics",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: End Users/Desktop",
      "Intended Audience :: Other Audience",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Sociology"
    ],
    "description": "![](https://raw.githubusercontent.com/cpa-analytics/embedding-encoder/main/logo.png)\n\n## Overview\n\nEmbedding Encoder is a scikit-learn-compliant transformer that converts categorical variables into numeric vector representations. This is achieved by creating a small multilayer perceptron architecture in which each categorical variable is passed through an embedding layer, for which weights are extracted and turned into DataFrame columns.\n\nWhile the idea is not new (it was popularized after [the team that landed in the 3rd place of the Rossmann Kaggle competition used it](https://www.kaggle.com/c/rossmann-store-sales/discussion/17974)), and although Python implementations have surfaced over the years, we are not aware of any library that integrates this functionality into scikit-learn.\n\n## Installation and dependencies\n\nEmbedding Encoder can be installed with\n\n```bash\npip install embedding-encoder[tf]\n```\n\nEmbedding Encoder has the following dependencies\n* scikit-learn\n* Tensorflow\n* numpy\n* pandas\n\nPlease see notes on non-Tensorflow usage at the end of this readme.\n\n## Documentation\n\nFull documentation including this readme and API reference can be found at [RTD](https://embedding-encoder.readthedocs.io/en/latest).\n\n## Usage\n\nEmbedding Encoder works like any scikit-learn transformer, the only difference being that it requires `y` to be passed as it is the neural network's target.\n\nEmbedding Encoder will assume that all input columns are categorical and will calculate embeddings for each, unless the `numeric_vars` argument is passed. In that case, numeric variables will be included as an additional input to the neural network but no embeddings will be calculated for them, and they will not be included in the output transformation.\n\nPlease note that including numeric variables may reduce the interpretability of the final model as their total influence on the target variable can become difficult to disentangle.\n\nThe simplest usage example is\n\n```python\nfrom embedding_encoder import EmbeddingEncoder\n\nee = EmbeddingEncoder(task=\"regression\") # or \"classification\"\nee.fit(X=X, y=y)\noutput = ee.transform(X=X)\n```\n\n## Compatibility with scikit-learn\n\nEmbedding Encoder can be included in pipelines as a regular transformer, and is compatible with cross-validation and hyperparameter optimization.\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nfrom embedding_encoder import EmbeddingEncoder\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nee = EmbeddingEncoder(task=\"classification\")\nnum_pipe = make_pipeline(SimpleImputer(strategy=\"mean\"), StandardScaler())\ncat_pipe = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), ee)\ncol_transformer = ColumnTransformer([(\"num_transformer\", num_pipe, numeric_vars),\n                                     (\"cat_transformer\", cat_pipe, categorical_vars)])\n\npipe = make_pipeline(col_transformer,\n                     LogisticRegression())\nparam_grid = {\n    \"columntransformer__cat__embeddingencoder__layers_units\": [\n        [64, 32, 16],\n        [16, 8],\n    ]\n}\ncv = GridSearchCV(pipeline, param_grid)\n```\n\nIn the case of pipelines, if `numeric_vars` is specificed Embedding Encoder has to be the first step in the pipeline. This is because a Embedding Encoder with `numeric_vars` requires that its `X` input be a `DataFrame` with proper column names, which cannot be guaranteed if previous transformations are applied as is.\n\nAlternatively, previous transformations can be included provided they are held inside the `ColumnTransformerWithNames` class in this library, which retains feature names.\n\n\n```python\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nfrom embedding_encoder import EmbeddingEncoder\nfrom embedding_encoder.utils import ColumnTransformerWithNames\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nee = EmbeddingEncoder(task=\"classification\", numeric_vars=numeric_vars)\nnum_pipe = make_pipeline(SimpleImputer(strategy=\"mean\"), StandardScaler())\ncat_transformer = SimpleImputer(strategy=\"most_frequent\")\ncol_transformer = ColumnTransformerWithNames([(\"num_transformer\", num_pipe, numeric_vars),\n                                              (\"cat_transformer\", cat_transformer, categorical_vars)])\n\npipe = make_pipeline(col_transformer,\n                     ee,\n                     LogisticRegression())\npipe.fit(X_train, y_train)\n```\n\nLike scikit transformers, Embedding Encoder also has a `inverse_transform` method that recomposes the original input.\n\n## Plotting embeddings\n\nThe idea behind embeddings is that categories that are conceptually similar should have similar vector representations. For example, \"December\" and \"January\" should be close to each other when the target variable is ice cream sales (here in the Southern Hemisphere at least!).\n\nThis can be analyzed with the `plot_embeddings` function, which depends on Seaborn (`pip install embedding-encoder[sns]` or `pip install embedding-encoder[full]` which includes Tensorflow).\n\n```python\nfrom embedding_encoder import EmbeddingEncoder\n\nee = EmbeddingEncoder(task=\"classification\")\nee.fit(X=X, y=y)\nee.plot_embeddings(variable=\"...\", model=\"pca\")\n```\n\n## Advanced usage\n\nEmbedding Encoder gives some control over the neural network. In particular, its constructor allows setting how deep and large the network should be (by modifying `layers_units`), as well as the dropout rate between dense layers. Epochs and batch size can also be modified.\n\nThese can be optimized with regular scikit-learn hyperparameter optimization techiniques.\n\nThe training loop includes an early stopping callback that restores the best weights (by default, the ones that minimize the validation loss).\n\n## Non-Tensorflow usage\n\nTensorflow can be tricky to install on some systems, which could make Embedding Encoder less appealing if the user has no intention of using TF for modeling.\n\nThere are actually two partial ways of using Embedding Encoder without a TF installation.\n\n1. Because TF is only used and imported in the `EmbeddingEncoder.fit()` method, once EE or the pipeline that contains EE has been fit, TF can be safely uninstalled; calls to methods like `EmbeddingEncoder.transform()` or `Pipeline.predict()` should raise no errors.\n2. Embedding Encoder can save the mapping from categorical variables to embeddings to a JSON file which can be later imported by setting `pretrained=True`, requiring no TF whatsoever. This also opens up the opportunity to train embeddings for common categorical variables on common tasks and saving them for use in downstream tasks.\n\nInstalling EE without Tensorflow is as easy as removing \"[tf]\" from the install command.\n\n```bash\npip install embedding-encoder\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/cpa-ferrere/EmbeddingEncoder",
    "keywords": "machine learning,neural network,deep learning,sklearn,scikit-learn",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "embedding-encoder",
    "package_url": "https://pypi.org/project/embedding-encoder/",
    "platform": null,
    "project_url": "https://pypi.org/project/embedding-encoder/",
    "project_urls": {
      "Homepage": "https://github.com/cpa-ferrere/EmbeddingEncoder"
    },
    "release_url": "https://pypi.org/project/embedding-encoder/0.0.4/",
    "requires_dist": [
      "scikit-learn",
      "pandas",
      "tensorflow (>=2.8.0) ; extra == 'full'",
      "seaborn ; extra == 'full'",
      "seaborn ; extra == 'sns'",
      "tensorflow (>=2.8.0) ; extra == 'tf'"
    ],
    "requires_python": ">=3.6",
    "summary": "scikit-learn compatible transformer that turns categorical features into dense numeric embeddings",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13126213,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fce5d2cd490fa966288c0b3094fe3efa50bd37d7b534765624b61aadc0b1c955",
          "md5": "29883d72868bfef0ee3b5f535fd0dbf9",
          "sha256": "a4356d2c93203e31d6875c78616980acfc71c6b142e3316e43bc85886629ac93"
        },
        "downloads": -1,
        "filename": "embedding_encoder-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "29883d72868bfef0ee3b5f535fd0dbf9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 13286,
        "upload_time": "2022-01-27T15:08:57",
        "upload_time_iso_8601": "2022-01-27T15:08:57.747346Z",
        "url": "https://files.pythonhosted.org/packages/fc/e5/d2cd490fa966288c0b3094fe3efa50bd37d7b534765624b61aadc0b1c955/embedding_encoder-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ca9a9d0da52a8e9183a4e25137f904ab79ca2fbf80075462bd18289528bf0eb9",
          "md5": "4ba474f378f61e126824fcff4f12937f",
          "sha256": "60a05c5abc49a34f57ab2a3d3f82ba49f5794c64356e659c7758fa5046ca200f"
        },
        "downloads": -1,
        "filename": "embedding-encoder-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4ba474f378f61e126824fcff4f12937f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 11544,
        "upload_time": "2022-01-27T15:08:58",
        "upload_time_iso_8601": "2022-01-27T15:08:58.928663Z",
        "url": "https://files.pythonhosted.org/packages/ca/9a/9d0da52a8e9183a4e25137f904ab79ca2fbf80075462bd18289528bf0eb9/embedding-encoder-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8349f8a1d24949126b50ae4963c2bac2cc2ed8cda1e9edd205dc0003c91e8898",
          "md5": "4cee70ef60858a6b49271d40efe1eaee",
          "sha256": "f625b966ea92f6dc9878949afba1c1cd2c545faa43adde9cc633a8b89f5aaec2"
        },
        "downloads": -1,
        "filename": "embedding_encoder-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4cee70ef60858a6b49271d40efe1eaee",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 14417,
        "upload_time": "2022-01-29T04:29:21",
        "upload_time_iso_8601": "2022-01-29T04:29:21.347250Z",
        "url": "https://files.pythonhosted.org/packages/83/49/f8a1d24949126b50ae4963c2bac2cc2ed8cda1e9edd205dc0003c91e8898/embedding_encoder-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "58c82359b9b20d3e1653191cb5f3e01488abfc96222c65bb04d681c841a6cc85",
          "md5": "97fe34daf47e5ba092503573fb0506e7",
          "sha256": "43256523cc0d8ab3b5814827a56dcc654a90d355e65a53f77dc21cef189713d0"
        },
        "downloads": -1,
        "filename": "embedding-encoder-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "97fe34daf47e5ba092503573fb0506e7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 13289,
        "upload_time": "2022-01-29T04:29:23",
        "upload_time_iso_8601": "2022-01-29T04:29:23.677840Z",
        "url": "https://files.pythonhosted.org/packages/58/c8/2359b9b20d3e1653191cb5f3e01488abfc96222c65bb04d681c841a6cc85/embedding-encoder-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "67a2dc4813a6f051054c8b479d109f2d915f4a652a5b4d30a8e94f666899ba7b",
          "md5": "df2b52c0b15f4ef1c414a8f2c248637b",
          "sha256": "bd23a0ac2fb2835434722233dfac7e9a5b1552855221926b7d0d04d2dd3cce03"
        },
        "downloads": -1,
        "filename": "embedding_encoder-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "df2b52c0b15f4ef1c414a8f2c248637b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 21968,
        "upload_time": "2022-02-19T05:23:40",
        "upload_time_iso_8601": "2022-02-19T05:23:40.443770Z",
        "url": "https://files.pythonhosted.org/packages/67/a2/dc4813a6f051054c8b479d109f2d915f4a652a5b4d30a8e94f666899ba7b/embedding_encoder-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ff450150164863ddee106345697ec336af015f2b9a5be4bc9b9e343e0b34b2f3",
          "md5": "65082609df14075c13603c29a848c091",
          "sha256": "1a937185968c70fa65f8d45a2606712a46bf83225b1f6d6c72ce8d037d8e9b4c"
        },
        "downloads": -1,
        "filename": "embedding-encoder-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "65082609df14075c13603c29a848c091",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 20673,
        "upload_time": "2022-02-19T05:23:41",
        "upload_time_iso_8601": "2022-02-19T05:23:41.842440Z",
        "url": "https://files.pythonhosted.org/packages/ff/45/0150164863ddee106345697ec336af015f2b9a5be4bc9b9e343e0b34b2f3/embedding-encoder-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4af5d52959ae324d5b287ca8bc1132d6bfb3d761d653cd94dd0e16edd5674a04",
          "md5": "a56b1fa2fda33c632c152f4bb3bc8876",
          "sha256": "014e39392f70ba2e526d29bd12184017f15d58895e8b2be5f4c21f53e04ed9b2"
        },
        "downloads": -1,
        "filename": "embedding_encoder-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a56b1fa2fda33c632c152f4bb3bc8876",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 22071,
        "upload_time": "2022-03-09T14:47:37",
        "upload_time_iso_8601": "2022-03-09T14:47:37.377486Z",
        "url": "https://files.pythonhosted.org/packages/4a/f5/d52959ae324d5b287ca8bc1132d6bfb3d761d653cd94dd0e16edd5674a04/embedding_encoder-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "81d12812976ced27b4219f561834355dd734b54640133923e7f0c876228d20b2",
          "md5": "877f417381293adb251b959696ccbd17",
          "sha256": "905bc0d4218fd325fa03078b94c0952f2d563adf1dded5d4f3e43b7bd5a01b62"
        },
        "downloads": -1,
        "filename": "embedding-encoder-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "877f417381293adb251b959696ccbd17",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 20870,
        "upload_time": "2022-03-09T14:47:38",
        "upload_time_iso_8601": "2022-03-09T14:47:38.672167Z",
        "url": "https://files.pythonhosted.org/packages/81/d1/2812976ced27b4219f561834355dd734b54640133923e7f0c876228d20b2/embedding-encoder-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4af5d52959ae324d5b287ca8bc1132d6bfb3d761d653cd94dd0e16edd5674a04",
        "md5": "a56b1fa2fda33c632c152f4bb3bc8876",
        "sha256": "014e39392f70ba2e526d29bd12184017f15d58895e8b2be5f4c21f53e04ed9b2"
      },
      "downloads": -1,
      "filename": "embedding_encoder-0.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a56b1fa2fda33c632c152f4bb3bc8876",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 22071,
      "upload_time": "2022-03-09T14:47:37",
      "upload_time_iso_8601": "2022-03-09T14:47:37.377486Z",
      "url": "https://files.pythonhosted.org/packages/4a/f5/d52959ae324d5b287ca8bc1132d6bfb3d761d653cd94dd0e16edd5674a04/embedding_encoder-0.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "81d12812976ced27b4219f561834355dd734b54640133923e7f0c876228d20b2",
        "md5": "877f417381293adb251b959696ccbd17",
        "sha256": "905bc0d4218fd325fa03078b94c0952f2d563adf1dded5d4f3e43b7bd5a01b62"
      },
      "downloads": -1,
      "filename": "embedding-encoder-0.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "877f417381293adb251b959696ccbd17",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 20870,
      "upload_time": "2022-03-09T14:47:38",
      "upload_time_iso_8601": "2022-03-09T14:47:38.672167Z",
      "url": "https://files.pythonhosted.org/packages/81/d1/2812976ced27b4219f561834355dd734b54640133923e7f0c876228d20b2/embedding-encoder-0.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}