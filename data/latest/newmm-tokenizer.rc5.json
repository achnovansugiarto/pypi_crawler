{
  "info": {
    "author": "WISESIGHT Product Development",
    "author_email": "tequila@wisesight.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Natural Language :: Thai",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Text Processing",
      "Topic :: Text Processing :: General",
      "Topic :: Text Processing :: Linguistic"
    ],
    "description": "# __newmm-tokenizer__\n\nStandalone Dictionary-based, Maximum Matching + Thai Character Cluster (newmm) tokenizer extracted from [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp).\n\n## __Objectives__\nThis repository is created for reducing an overall size of original [PyThaiNLP Tokenizer Module](https://www.thainlp.org/pythainlp/docs/2.2/api/tokenize.html). The main objective is to be able to segment Thai sentences into a list of words.\n\n## __Supports__\nThe module supports Python 3.7+ as follow the original PyThaiNLP repository.\n\n## __Installation__\n```\npip install newmm-tokenizer\n```\n\n## __How to Use__\n```python\nfrom newmm_tokenizer.tokenizer import word_tokenize\n\ntext = 'เป็นเรื่องแรกที่ร้องไห้ตั้งแต่ ep 1 แล้วก็เป็นเรื่องแรกที่เลือกไม่ได้ว่าจะเชียร์พระเอกหรือพระรองดี 19...'\nwords = word_tokenize(text)\n\nprint(words) \n# ['เป็นเรื่อง', 'แรก', 'ที่', 'ร้องไห้', 'ตั้งแต่', ' ', 'ep', ' ', '1', ' ', 'แล้วก็', 'เป็นเรื่อง', 'แรก', 'ที่', 'เลือกไม่ได้', 'ว่า', 'จะ', 'เชียร์', 'พระเอก', 'หรือ', 'พระรอง', 'ดี', ' ', '19', '...']\n```\n\n## __LICENSE__\nPlease see the original license of PyThaiNLP [here](https://github.com/PyThaiNLP/pythainlp#licenses)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/wisesight/newmm-tokenizer",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "newmm-tokenizer",
    "package_url": "https://pypi.org/project/newmm-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/newmm-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/wisesight/newmm-tokenizer"
    },
    "release_url": "https://pypi.org/project/newmm-tokenizer/0.2.2/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Standalone Dictionary-based, Maximum Matching + Thai Character Cluster (newmm) tokenizer extracted from PyThaiNLP",
    "version": "0.2.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12491250,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "beea8d05965d34d1be6499c1fc7ca3d57f4ffa100d52335d503ea3725b187fab",
          "md5": "64657924355d2d807a86998b419b948e",
          "sha256": "ab061b2c6784d7181b07b54e0b12036053f85cae8eb52d89f6f22a66f329a0b3"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "64657924355d2d807a86998b419b948e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 1605,
        "upload_time": "2020-10-26T08:07:23",
        "upload_time_iso_8601": "2020-10-26T08:07:23.674061Z",
        "url": "https://files.pythonhosted.org/packages/be/ea/8d05965d34d1be6499c1fc7ca3d57f4ffa100d52335d503ea3725b187fab/newmm_tokenizer-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "25ac119307b2078b0b6a7c62ff99f30df674f8666237fb539ebba8c80d7afcd8",
          "md5": "b49f09cab155ed99749e1aa717eb1955",
          "sha256": "12fb877e2e4034ce30a296dc09176c42f28a18e6dbf0d08abbe2e51419de8580"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b49f09cab155ed99749e1aa717eb1955",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 1250,
        "upload_time": "2020-10-26T08:07:26",
        "upload_time_iso_8601": "2020-10-26T08:07:26.659612Z",
        "url": "https://files.pythonhosted.org/packages/25/ac/119307b2078b0b6a7c62ff99f30df674f8666237fb539ebba8c80d7afcd8/newmm_tokenizer-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "40ffc69d26f085fc3a45759169b02cc379b9ccdf519e452820d5b58933e4ca48",
          "md5": "f04d5f260e0ccf54058a4dfdb060a8ef",
          "sha256": "00f1a5f68b3f7075ff02c22d855507bb7c63b01b1d43d25e2bebc977d536d304"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f04d5f260e0ccf54058a4dfdb060a8ef",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9065,
        "upload_time": "2020-10-26T08:22:26",
        "upload_time_iso_8601": "2020-10-26T08:22:26.602875Z",
        "url": "https://files.pythonhosted.org/packages/40/ff/c69d26f085fc3a45759169b02cc379b9ccdf519e452820d5b58933e4ca48/newmm_tokenizer-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "853fed4acc808b876f555a6709a96dc744364df9c071b35e97f9a6f7e03814b1",
          "md5": "5724251b59f7bb40df7f8881a3801ee9",
          "sha256": "3a6a25ee4a33e0db660b9d108f56d67c417844d45c79f3190ecd0326b85beda9"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5724251b59f7bb40df7f8881a3801ee9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 6958,
        "upload_time": "2020-10-26T08:22:28",
        "upload_time_iso_8601": "2020-10-26T08:22:28.568310Z",
        "url": "https://files.pythonhosted.org/packages/85/3f/ed4acc808b876f555a6709a96dc744364df9c071b35e97f9a6f7e03814b1/newmm_tokenizer-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3e91400cb8b3dfd1c390a36dacceea68e15fcda32ed562c6ba173f092e7ababa",
          "md5": "8d2b5c2c291188436561ca1f55794e4e",
          "sha256": "069842abd1e322752939e9c927219ce8e63f8f78f4be2a6ee354d257dd27cdaf"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8d2b5c2c291188436561ca1f55794e4e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 320922,
        "upload_time": "2020-10-26T09:14:00",
        "upload_time_iso_8601": "2020-10-26T09:14:00.753037Z",
        "url": "https://files.pythonhosted.org/packages/3e/91/400cb8b3dfd1c390a36dacceea68e15fcda32ed562c6ba173f092e7ababa/newmm_tokenizer-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "749f28da0a8abbbc5228081e69e32ad11af1d39decb75768a1a2be5de7f8260e",
          "md5": "8e80cdea1470baa1dd5b7cc502121fc4",
          "sha256": "72822f6d32ca78ed2d463c3afc50f82770b0d6e4b029398e1ec8cb5af7cdf1f5"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "8e80cdea1470baa1dd5b7cc502121fc4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 314265,
        "upload_time": "2020-10-26T09:14:03",
        "upload_time_iso_8601": "2020-10-26T09:14:03.569253Z",
        "url": "https://files.pythonhosted.org/packages/74/9f/28da0a8abbbc5228081e69e32ad11af1d39decb75768a1a2be5de7f8260e/newmm_tokenizer-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ddb099ac977c2c7c58ea2e3664d2aa88648aa15aa3f44423dcbddb12e2f4957",
          "md5": "2a5f468b4ceceaca922d3b18640f918f",
          "sha256": "7a0afef4da2d586f4c4aef5a50cee66b048d410834df7143893cd86b12a41413"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2a5f468b4ceceaca922d3b18640f918f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 321003,
        "upload_time": "2020-10-26T09:29:25",
        "upload_time_iso_8601": "2020-10-26T09:29:25.138720Z",
        "url": "https://files.pythonhosted.org/packages/5d/db/099ac977c2c7c58ea2e3664d2aa88648aa15aa3f44423dcbddb12e2f4957/newmm_tokenizer-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2f397801c31f9cc3399f8e0c31df7918d8e97f9f54708eedcdc6c98da4cf8b13",
          "md5": "5c518eff6a3af81cbebf9666787d28fa",
          "sha256": "59b9b059fb772d292f61f0ac644ebf2bf58f27eb10d4d0f516c611e803279fdb"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5c518eff6a3af81cbebf9666787d28fa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 314241,
        "upload_time": "2020-10-26T09:29:28",
        "upload_time_iso_8601": "2020-10-26T09:29:28.244838Z",
        "url": "https://files.pythonhosted.org/packages/2f/39/7801c31f9cc3399f8e0c31df7918d8e97f9f54708eedcdc6c98da4cf8b13/newmm_tokenizer-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "96f2e93d15afba1dec377d3a4c018ec1f75214510d6a1792ea9ecee526f5089d",
          "md5": "82817a057f1e38346cfa1290f6514eb8",
          "sha256": "2ef54c67585d0f562650c93368a07ba0e39b5c8dc4500991135c57df58da65a5"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.2.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "82817a057f1e38346cfa1290f6514eb8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 320626,
        "upload_time": "2022-01-06T05:20:25",
        "upload_time_iso_8601": "2022-01-06T05:20:25.008587Z",
        "url": "https://files.pythonhosted.org/packages/96/f2/e93d15afba1dec377d3a4c018ec1f75214510d6a1792ea9ecee526f5089d/newmm_tokenizer-0.2.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aca80135c90ddeaae26f1e12cfd08f4a55e4ff6b2edd5228d7eaba7a55c90f0b",
          "md5": "3eedfc650dd78720f61f28bbf032c210",
          "sha256": "e6bd825d6a05f759be1e9be67e1d603a61f961d3fa9979d4c3af21ae576250ec"
        },
        "downloads": -1,
        "filename": "newmm_tokenizer-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3eedfc650dd78720f61f28bbf032c210",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 314204,
        "upload_time": "2022-01-06T05:20:27",
        "upload_time_iso_8601": "2022-01-06T05:20:27.447007Z",
        "url": "https://files.pythonhosted.org/packages/ac/a8/0135c90ddeaae26f1e12cfd08f4a55e4ff6b2edd5228d7eaba7a55c90f0b/newmm_tokenizer-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "96f2e93d15afba1dec377d3a4c018ec1f75214510d6a1792ea9ecee526f5089d",
        "md5": "82817a057f1e38346cfa1290f6514eb8",
        "sha256": "2ef54c67585d0f562650c93368a07ba0e39b5c8dc4500991135c57df58da65a5"
      },
      "downloads": -1,
      "filename": "newmm_tokenizer-0.2.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "82817a057f1e38346cfa1290f6514eb8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 320626,
      "upload_time": "2022-01-06T05:20:25",
      "upload_time_iso_8601": "2022-01-06T05:20:25.008587Z",
      "url": "https://files.pythonhosted.org/packages/96/f2/e93d15afba1dec377d3a4c018ec1f75214510d6a1792ea9ecee526f5089d/newmm_tokenizer-0.2.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "aca80135c90ddeaae26f1e12cfd08f4a55e4ff6b2edd5228d7eaba7a55c90f0b",
        "md5": "3eedfc650dd78720f61f28bbf032c210",
        "sha256": "e6bd825d6a05f759be1e9be67e1d603a61f961d3fa9979d4c3af21ae576250ec"
      },
      "downloads": -1,
      "filename": "newmm_tokenizer-0.2.2.tar.gz",
      "has_sig": false,
      "md5_digest": "3eedfc650dd78720f61f28bbf032c210",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 314204,
      "upload_time": "2022-01-06T05:20:27",
      "upload_time_iso_8601": "2022-01-06T05:20:27.447007Z",
      "url": "https://files.pythonhosted.org/packages/ac/a8/0135c90ddeaae26f1e12cfd08f4a55e4ff6b2edd5228d7eaba7a55c90f0b/newmm_tokenizer-0.2.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}