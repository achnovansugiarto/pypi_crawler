{
  "info": {
    "author": "MrZilinXiao",
    "author_email": "me@mrxiao.net",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "## OpenAI-Manager\n\n![pypi](https://img.shields.io/pypi/v/openai-manager.svg)\n![versions](https://img.shields.io/pypi/pyversions/openai-manager.svg)\n\nSpeed up your OpenAI requests by balancing prompts to multiple API keys. Quite useful if you are playing with `code-davinci-002` endpoint.\n\n### Disclaimer\n\nBefore using this tool, you are required to read the EULA and ToS of OpenAI L.P. carefully. Actions that violate the OpenAI user agreement may result in the API Key and associated account being suspended. The author shall not be held liable for any consequential damages.\n\n### Design\nTL;DR: this package helps you manage rate limit (both request-level and token-level) for each api_key for maximum number of requests to OpenAI API.\n\nThis is extremely helpful if you use `CODEX` endpoint or you have a handful of free-trial accounts due to limited budget. Free-trial accounts apply **strict** rate limit.\n\n### Quickstart\n\n1. Install openai-manager on PyPI. Notice we need Python 3.7+ for maximum compatibility of `asyncio`.\n   ```bash\n   pip install openai-manager\n   ```\n\n2. Prepare your OpenAI credentials in \n   1. Environmental Varibles: any envvars beginning with `OPENAI_API_KEY` will be used to initialized the manager. Best practice to load your api keys is to prepare a `.env` file like: \n   \n   ```bash\n   OPENAI_API_KEY_1=sk-Nxo******\n   OPENAI_API_KEY_2=sk-TG2******\n   OPENAI_API_KEY_3=sk-Kpt******\n   # You can set a global proxy for all api_keys\n   OPENAI_API_PROXY=http://127.0.0.1:7890\n   # You can also append proxy to each api_key. \n   # Make sure the indices match.\n   OPENAI_API_PROXY_1=http://127.0.0.1:7890\n   OPENAI_API_PROXY_2=http://127.0.0.1:7890\n   OPENAI_API_PROXY_3=http://127.0.0.1:7890\n   ```\n\n   Then load your environmental varibles before running any scripts:\n   ```bash\n   export $(grep -v '^#' .env | xargs)\n   ```\n\n   2. YAML config file: you can add more fine-grained restrictions on each API key if you know the ratelimit for each key in advance. See [example_config.yml](/example_config.yml) for details.\n   ```python\n   import openai_manager\n   openai_manager.append_auth_from_config(config_path='example_config.yml')\n   ```\n\n3. Two ways to use `openai_manager`:\n   1. Use it just like how you use official `openai` package. We implement exact the same call signature as official `openai` package.\n        ```python\n        import openai as official_openai\n        import openai_manager\n        from openai_manager.utils import timeit\n        \n        @timeit\n        def test_official_separate():\n            for i in range(10):\n                prompt = \"Once upon a time, \"\n                response = official_openai.Completion.create(\n                    model=\"code-davinci-002\",\n                    prompt=prompt,\n                    max_tokens=20,\n                )\n                print(\"Answer {}: {}\".format(i, response[\"choices\"][0][\"text\"]))\n\n        @timeit\n        def test_manager():\n            prompt = \"Once upon a time, \"\n            prompts = [prompt] * 10\n            responses = openai_manager.Completion.create(\n                model=\"code-davinci-002\",\n                prompt=prompts,\n                max_tokens=20,\n            )\n            assert len(responses) == 10\n            for i, response in enumerate(responses):\n                print(\"Answer {}: {}\".format(i, response[\"choices\"][0][\"text\"]))\n        ```\n    2. Use it as a proxy server between you and OpenAI endpoint. First, run `python -m openai_manager.serving --port 8000 --host localhost --api_key [your custom key]`. Then set up the official python `openai` package:\n        ```python\n        import openai\n        openai.api_base = \"http://localhost:8000/v1\"\n        openai.api_key = \"[your custom key]\"\n\n        # run like normal\n        prompt = [\"Once upon a time, \"] * 10\n        response = openai.Completion.create(\n            model=\"code-davinci-002\",\n            prompt=prompt,\n            max_tokens=20,\n        )\n        print(response[\"choices\"][0][\"text\"])\n        ```\n\n### Configuration\n\nMost configurations are manupulated by environmental variables. \n\n- `GLOBAL_NUM_REQUEST_LIMIT`: aiohttp connection limit, default is `500`\n- `REQUESTS_PER_MIN_LIMIT`: number of requests per minute, default is `10`; config file will overwrite this\n- `TOKENS_PER_MIN_LIMIT`: number of tokens per minute, default is `40000`; config file will overwrite this\n- `COROTINE_PER_AUTH`: number of corotine per api_key, default is `3`; decrease it to 1 if ratelimit errors are triggered too often\n- `ATTEMPTS_PER_PROMPT`: number of attempts per prompt, default is `5`\n- `RATELIMIT_AFTER_SUBMISSION`: whether to track ratelimit after submission, default is `True`; keep it enabled if response takes a long time\n- `OPENAI_LOG_LEVEL`: default log level is WARNING, 10-DEBUG, 20-INFO, 30-WARNING, 40-ERROR, 50-CRITICAL; set to 10 if you are getting stuck and want to do some diagnose\n\nRate limit triggers will be visible under `logging.WARNING`. Run `export OPENAI_LOG_LEVEL=40` to ignore rate limit warnings if you believe current setting is stable enought.\n\n### Performance Assessment\n\nAfter ChatCompletion release, the `code-davinci-002` endpoint becomes slow. Using 10 API keys, running 100 completions with `max_tokens=20` and other hyperparameters left default took 90 seconds on average. Using official API, it took 10 seconds per completion, thus 1000 in total. \n\nTheroticallly, the throughput increases linearly with the number of API keys. \n\n### Frequently Asked Questions\n\n1. Q: Why don't we just use official batching function?\n\n   ```python\n    prompt = \"Once upon a time, \"\n    prompts = [prompt] * 10\n    response = openai.Completion.create(\n        model=\"code-davinci-002\",\n        prompt=prompts,  # official batching allows multiple prompts in one request\n        max_tokens=20,\n    )\n    assert len(response[\"choices\"]) == 10\n    for i, answer in enumerate(response[\"choices\"]):\n        print(\"Answer {}: {}\".format(i, answer[\"text\"]))\n   ```\n   \n   A: Some OpenAI endpoints (like `code-davinci-002`) apply strict token-level rate limit, even if you upgrade to pay-as-you-go user. Simple batching would not solve this.\n\n### Acknowledgement\n\n- [openai-cookbook](https://github.com/openai/openai-cookbook): Best practice when dealing with official APIs.\n- [openai-python](https://github.com/openai/openai-python): Official Python version of OpenAI.\n\n### TODO\n\n- [ ] Support all functions in OpenAI Python API.\n  - [x] Completions\n  - [x] Embeddings\n  - [ ] Generations\n  - [x] ChatCompletions\n- [x] Better back-off strategy for maximum throughput.\n- [x] Properly handling exceptions raised by OpenAI API.\n- [ ] Automatic batching prompts to reduce the number of requests.\n- [ ] Automatic rotation of tons of OpenAI API Keys. (Removing invaild, adding new, etc.)\n- [x] Serving as a reverse proxy to balance official requests.\n\n\n### Donation\n\nIf this package helps your research, consider making a donation via GitHub! \n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/MrZilinXiao/openai-manager",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "openai-manager",
    "package_url": "https://pypi.org/project/openai-manager/",
    "platform": null,
    "project_url": "https://pypi.org/project/openai-manager/",
    "project_urls": {
      "Homepage": "https://github.com/MrZilinXiao/openai-manager"
    },
    "release_url": "https://pypi.org/project/openai-manager/0.0.3/",
    "requires_dist": [
      "aiohttp",
      "pytest",
      "openai",
      "tiktoken"
    ],
    "requires_python": ">=3.7",
    "summary": "Speed up your OpenAI requests by balancing prompts to multiple API keys.",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17334154,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "10c6d1a10c19e7fd9ddaabe0a25a931b8609514d66b0f4287f128b7ee9e69daa",
          "md5": "d81843741abd9a79535bd07abef7826a",
          "sha256": "b790240c6b599cd4fcf5fbfb1b209a8b35aef9af3f8b43f77af2ebb2bc84b77c"
        },
        "downloads": -1,
        "filename": "openai_manager-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d81843741abd9a79535bd07abef7826a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 28636,
        "upload_time": "2023-03-02T12:18:00",
        "upload_time_iso_8601": "2023-03-02T12:18:00.439088Z",
        "url": "https://files.pythonhosted.org/packages/10/c6/d1a10c19e7fd9ddaabe0a25a931b8609514d66b0f4287f128b7ee9e69daa/openai_manager-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "47a7bedda988024c18a26076a27827107dfedb659b6620dcd79d632838bc4455",
          "md5": "1b6354ff862dae112da1298149f06c7c",
          "sha256": "76083ad9c97f336435f9a12a83ea74c23bbea8262bc51be800c6dcbc637bd1b7"
        },
        "downloads": -1,
        "filename": "openai-manager-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1b6354ff862dae112da1298149f06c7c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 13719,
        "upload_time": "2023-03-02T12:18:05",
        "upload_time_iso_8601": "2023-03-02T12:18:05.968327Z",
        "url": "https://files.pythonhosted.org/packages/47/a7/bedda988024c18a26076a27827107dfedb659b6620dcd79d632838bc4455/openai-manager-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bf08078a07fd181e121b05bbfd66ada44dce9adf97d2bd944129c5037ffeb26b",
          "md5": "75df4095ef5e4b6870603b96ff6d813b",
          "sha256": "d40fb8d63525f663dc5a6c86575f5d1018b558e5ba6e53c83b98eb7a5bf999e8"
        },
        "downloads": -1,
        "filename": "openai_manager-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "75df4095ef5e4b6870603b96ff6d813b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 55733,
        "upload_time": "2023-03-03T11:54:01",
        "upload_time_iso_8601": "2023-03-03T11:54:01.862562Z",
        "url": "https://files.pythonhosted.org/packages/bf/08/078a07fd181e121b05bbfd66ada44dce9adf97d2bd944129c5037ffeb26b/openai_manager-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "65039c412f2972a08d7fc7856f304e667c85c47fe927c8734b7cfc7aaf27f4be",
          "md5": "8b9afdcfa32d5bc3b6c6f3258b082600",
          "sha256": "687e098fd82b21c5b27e81001edce401f0bffd07979886a81144222a87a4d5e3"
        },
        "downloads": -1,
        "filename": "openai-manager-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "8b9afdcfa32d5bc3b6c6f3258b082600",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15166,
        "upload_time": "2023-03-03T11:54:03",
        "upload_time_iso_8601": "2023-03-03T11:54:03.090320Z",
        "url": "https://files.pythonhosted.org/packages/65/03/9c412f2972a08d7fc7856f304e667c85c47fe927c8734b7cfc7aaf27f4be/openai-manager-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f1940188322f5e21cff6abc702cb8e3ef244a048644c5da7a54e81fa58460706",
          "md5": "fb50818a87c6bf0a44806e03541e458d",
          "sha256": "050b2c262ad27861ba68ab79cf355f049f6ff1aea38d34b508d2937b7e283ede"
        },
        "downloads": -1,
        "filename": "openai_manager-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fb50818a87c6bf0a44806e03541e458d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 84699,
        "upload_time": "2023-03-17T12:02:38",
        "upload_time_iso_8601": "2023-03-17T12:02:38.013386Z",
        "url": "https://files.pythonhosted.org/packages/f1/94/0188322f5e21cff6abc702cb8e3ef244a048644c5da7a54e81fa58460706/openai_manager-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2e69e8b3cfdf832f592045923108ca5c7055747cc259ccb3f5bc61cba70affcf",
          "md5": "bf6a94c10ef920884408444e8e50cfbc",
          "sha256": "2cdb2172e9e8ed5430d2e2463e8e43a94433796b3b258fec00455beedf266276"
        },
        "downloads": -1,
        "filename": "openai-manager-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "bf6a94c10ef920884408444e8e50cfbc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 18010,
        "upload_time": "2023-03-17T12:02:40",
        "upload_time_iso_8601": "2023-03-17T12:02:40.009745Z",
        "url": "https://files.pythonhosted.org/packages/2e/69/e8b3cfdf832f592045923108ca5c7055747cc259ccb3f5bc61cba70affcf/openai-manager-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f1940188322f5e21cff6abc702cb8e3ef244a048644c5da7a54e81fa58460706",
        "md5": "fb50818a87c6bf0a44806e03541e458d",
        "sha256": "050b2c262ad27861ba68ab79cf355f049f6ff1aea38d34b508d2937b7e283ede"
      },
      "downloads": -1,
      "filename": "openai_manager-0.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fb50818a87c6bf0a44806e03541e458d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 84699,
      "upload_time": "2023-03-17T12:02:38",
      "upload_time_iso_8601": "2023-03-17T12:02:38.013386Z",
      "url": "https://files.pythonhosted.org/packages/f1/94/0188322f5e21cff6abc702cb8e3ef244a048644c5da7a54e81fa58460706/openai_manager-0.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2e69e8b3cfdf832f592045923108ca5c7055747cc259ccb3f5bc61cba70affcf",
        "md5": "bf6a94c10ef920884408444e8e50cfbc",
        "sha256": "2cdb2172e9e8ed5430d2e2463e8e43a94433796b3b258fec00455beedf266276"
      },
      "downloads": -1,
      "filename": "openai-manager-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "bf6a94c10ef920884408444e8e50cfbc",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 18010,
      "upload_time": "2023-03-17T12:02:40",
      "upload_time_iso_8601": "2023-03-17T12:02:40.009745Z",
      "url": "https://files.pythonhosted.org/packages/2e/69/e8b3cfdf832f592045923108ca5c7055747cc259ccb3f5bc61cba70affcf/openai-manager-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}