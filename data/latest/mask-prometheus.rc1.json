{
  "info": {
    "author": "Wu Dong",
    "author_email": "wudong@eastwu.cn",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# `Mask` gRPC Interceptors for Prometheus monitoring\n\n### Install\n\n```\npip install mask-prometheus\n```\n\n### Usage\n\n```\n# 3p\nfrom mask import Mask\nfrom mask_prometheus import Prometheus\n# project\nfrom examples.protos.hello_pb2 import HelloResponse\n\n\napp = Mask(__name__)\n\napp.config[\"PROMETHEUS_PORT\"] = 18080\n\nprometheus = Prometheus()\nprometheus.init_app(app)\n\n\n\n@app.route(method=\"SayHelloStream\", service=\"Hello\")\ndef say_hello_stream_handler(request, context):\n    \"\"\" Handler stream SayHello request\n    \"\"\"\n    for item in request:\n        yield HelloResponse(message=\"Hello Reply: %s\" % item.name)\n\nif __name__ == \"__main__\":\n    app.run(port=1020)\n```\n\n# Metrics\n\n## Labels\n\nAll metrics start with `mask` as Prometheus subsystem name. Both of them have mirror-concepts. Similarly all methods\ncontain the same rich labels:\n\n  * `grpc_service` - the [gRPC service](http://www.grpc.io/docs/#defining-a-service) name, which is the combination of protobuf `package` and\n    the `grpc_service` section name. E.g. for `package = mwitkow.testproto` and \n     `service TestService` the label will be `grpc_service=\"mwitkow.testproto.TestService\"`\n  * `grpc_method` - the name of the method called on the gRPC service. E.g.  \n    `grpc_method=\"Ping\"`\n  * `grpc_type` - the gRPC [type of request](http://www.grpc.io/docs/guides/concepts.html#rpc-life-cycle). \n    Differentiating between the two is important especially for latency measurements.\n\n     - `unary` is single request, single response RPC\n     - `client_stream` is a multi-request, single response RPC\n     - `server_stream` is a single request, multi-response RPC\n     - `bidi_stream` is a multi-request, multi-response RPC\n\n\nAdditionally for completed RPCs, the following labels are used:\n\n  * `grpc_code` - the human-readable [gRPC status code](https://github.com/grpc/grpc-go/blob/master/codes/codes.go).\n    The list of all statuses is to long, but here are some common ones:\n\n      - `StatusCode.OK` - means the RPC was successful\n      - `StatusCode.IllegalArgument` - RPC contained bad values\n      - `StatusCode.Internal` - server-side error not disclosed to the clients\n\n## Counters\n\nFor simplicity, let's assume we're tracking a single server-side RPC call of [`mwitkow.testproto.TestService`](examples/testproto/test.proto),\ncalling the method `PingList`. The call succeeds and returns 20 messages in the stream.\n\nFirst, immediately after the server receives the call it will increment the\n`grpc_server_started_total` and start the handling time clock (if histograms are enabled). \n\n```jsoniq\ngrpc_server_started_total{grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\"} 1\n```\n\nThen the user logic gets invoked. It receives one message from the client containing the request \n(it's a `server_stream`):\n\n```jsoniq\ngrpc_server_msg_received_total{grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\"} 1\n```\n\nThe user logic may return an error, or send multiple messages back to the client. In this case, on \neach of the 20 messages sent back, a counter will be incremented:\n\n```jsoniq\ngrpc_server_msg_sent_total{grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\"} 20\n```\n\nAfter the call completes, its status (`OK` or other [gRPC status code](https://github.com/grpc/grpc-go/blob/master/codes/codes.go)) \nand the relevant call labels increment the `grpc_server_handled_total` counter.\n\n```jsoniq\ngrpc_server_handled_total{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\"} 1\n```\n\n## Histograms\n\n[Prometheus histograms](https://prometheus.io/docs/concepts/metric_types/#histogram) are a great way\nto measure latency distributions of your RPCs. However, since it is bad practice to have metrics\nof [high cardinality](https://prometheus.io/docs/practices/instrumentation/#do-not-overuse-labels)\nthe latency monitoring metrics are disabled by default. \n\nAfter the call completes, its handling time will be recorded in a [Prometheus histogram](https://prometheus.io/docs/concepts/metric_types/#histogram)\nvariable `grpc_server_handling_seconds`. The histogram variable contains three sub-metrics:\n\n * `grpc_server_handling_seconds_count` - the count of all completed RPCs by status and method \n * `grpc_server_handling_seconds_sum` - cumulative time of RPCs by status and method, useful for \n   calculating average handling times\n * `grpc_server_handling_seconds_bucket` - contains the counts of RPCs by status and method in respective\n   handling-time buckets. These buckets can be used by Prometheus to estimate SLAs (see [here](https://prometheus.io/docs/practices/histograms/))\n\nThe counter values will look as follows:\n\n```jsoniq\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"0.005\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"0.01\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"0.025\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"0.05\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"0.1\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"0.25\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"0.5\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"1\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"2.5\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"5\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"10\"} 1\ngrpc_server_handling_seconds_bucket{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\",le=\"+Inf\"} 1\ngrpc_server_handling_seconds_sum{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\"} 0.0003866430000000001\ngrpc_server_handling_seconds_count{grpc_code=\"OK\",grpc_method=\"PingList\",grpc_service=\"mwitkow.testproto.TestService\",grpc_type=\"server_stream\"} 1\n```\n\n\n## Useful query examples\n\nPrometheus philosophy is to provide raw metrics to the monitoring system, and\nlet the aggregations be handled there. The verbosity of above metrics make it possible to have that\nflexibility. Here's a couple of useful monitoring queries:\n\n\n### request inbound rate\n```jsoniq\nsum(rate(grpc_server_started_total{job=\"foo\"}[1m])) by (grpc_service)\n```\nFor `job=\"foo\"` (common label to differentiate between Prometheus monitoring targets), calculate the\nrate of requests per second (1 minute window) for each gRPC `grpc_service` that the job has. Please note\nhow the `grpc_method` is being omitted here: all methods of a given gRPC service will be summed together.\n\n### unary request error rate\n```jsoniq\nsum(rate(grpc_server_handled_total{job=\"foo\",grpc_type=\"unary\",grpc_code!=\"OK\"}[1m])) by (grpc_service)\n```\nFor `job=\"foo\"`, calculate the per-`grpc_service` rate of `unary` (1:1) RPCs that failed, i.e. the \nones that didn't finish with `OK` code.\n\n### unary request error percentage\n```jsoniq\nsum(rate(grpc_server_handled_total{job=\"foo\",grpc_type=\"unary\",grpc_code!=\"OK\"}[1m])) by (grpc_service)\n / \nsum(rate(grpc_server_started_total{job=\"foo\",grpc_type=\"unary\"}[1m])) by (grpc_service)\n * 100.0\n```\nFor `job=\"foo\"`, calculate the percentage of failed requests by service. It's easy to notice that\nthis is a combination of the two above examples. This is an example of a query you would like to\n[alert on](https://prometheus.io/docs/alerting/rules/) in your system for SLA violations, e.g.\n\"no more than 1% requests should fail\".\n\n### average response stream size\n```jsoniq\nsum(rate(grpc_server_msg_sent_total{job=\"foo\",grpc_type=\"server_stream\"}[10m])) by (grpc_service)\n /\nsum(rate(grpc_server_started_total{job=\"foo\",grpc_type=\"server_stream\"}[10m])) by (grpc_service)\n```\nFor `job=\"foo\"` what is the `grpc_service`-wide `10m` average of messages returned for all `\nserver_stream` RPCs. This allows you to track the stream sizes returned by your system, e.g. allows \nyou to track when clients started to send \"wide\" queries that ret\nNote the divisor is the number of started RPCs, in order to account for in-flight requests.\n\n### 99%-tile latency of unary requests\n```jsoniq\nhistogram_quantile(0.99, \n  sum(rate(grpc_server_handling_seconds_bucket{job=\"foo\",grpc_type=\"unary\"}[5m])) by (grpc_service,le)\n)\n```\nFor `job=\"foo\"`, returns an 99%-tile [quantile estimation](https://prometheus.io/docs/practices/histograms/#quantiles)\nof the handling time of RPCs per service. Please note the `5m` rate, this means that the quantile\nestimation will take samples in a rolling `5m` window. When combined with other quantiles\n(e.g. 50%, 90%), this query gives you tremendous insight into the responsiveness of your system \n(e.g. impact of caching).\n\n### percentage of slow unary queries (>250ms)\n```jsoniq\n100.0 - (\nsum(rate(grpc_server_handling_seconds_bucket{job=\"foo\",grpc_type=\"unary\",le=\"0.25\"}[5m])) by (grpc_service)\n / \nsum(rate(grpc_server_handling_seconds_count{job=\"foo\",grpc_type=\"unary\"}[5m])) by (grpc_service)\n) * 100.0\n```\nFor `job=\"foo\"` calculate the by-`grpc_service` fraction of slow requests that took longer than `0.25` \nseconds. This query is relatively complex, since the Prometheus aggregations use `le` (less or equal)\nbuckets, meaning that counting \"fast\" requests fractions is easier. However, simple maths helps.\nThis is an example of a query you would like to alert on in your system for SLA violations, \ne.g. \"less than 1% of requests are slower than 250ms\".\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Eastwu5788/Mask-Prometheus",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mask-prometheus",
    "package_url": "https://pypi.org/project/mask-prometheus/",
    "platform": "",
    "project_url": "https://pypi.org/project/mask-prometheus/",
    "project_urls": {
      "Document": "https://github.com/Eastwu5788/Mask-Prometheus",
      "Homepage": "https://github.com/Eastwu5788/Mask-Prometheus",
      "Issue": "https://github.com/Eastwu5788/Mask-Prometheus/issues",
      "Source Code": "https://github.com/Eastwu5788/Mask-Prometheus"
    },
    "release_url": "https://pypi.org/project/mask-prometheus/1.0.0a1/",
    "requires_dist": [
      "Mask (>=1.0.0a1)",
      "Prometheus-client (>=0.11.0)"
    ],
    "requires_python": ">= 3.6",
    "summary": "Prometheus extension for Mask.",
    "version": "1.0.0a1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10823395,
  "releases": {
    "1.0.0a1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c89a1f1e581dc51975ae9cb376fb40422a01e5d3e547295e1b4b68a4d066e618",
          "md5": "ff6004fccae0ad522f006d5262cdd0c1",
          "sha256": "1e4427c2f3e22541258f3dac8126d6ec8608fa4298dadc07409208b7522156e0"
        },
        "downloads": -1,
        "filename": "mask_prometheus-1.0.0a1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ff6004fccae0ad522f006d5262cdd0c1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">= 3.6",
        "size": 7142,
        "upload_time": "2021-07-05T05:53:02",
        "upload_time_iso_8601": "2021-07-05T05:53:02.117848Z",
        "url": "https://files.pythonhosted.org/packages/c8/9a/1f1e581dc51975ae9cb376fb40422a01e5d3e547295e1b4b68a4d066e618/mask_prometheus-1.0.0a1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c89a1f1e581dc51975ae9cb376fb40422a01e5d3e547295e1b4b68a4d066e618",
        "md5": "ff6004fccae0ad522f006d5262cdd0c1",
        "sha256": "1e4427c2f3e22541258f3dac8126d6ec8608fa4298dadc07409208b7522156e0"
      },
      "downloads": -1,
      "filename": "mask_prometheus-1.0.0a1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ff6004fccae0ad522f006d5262cdd0c1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">= 3.6",
      "size": 7142,
      "upload_time": "2021-07-05T05:53:02",
      "upload_time_iso_8601": "2021-07-05T05:53:02.117848Z",
      "url": "https://files.pythonhosted.org/packages/c8/9a/1f1e581dc51975ae9cb376fb40422a01e5d3e547295e1b4b68a4d066e618/mask_prometheus-1.0.0a1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}