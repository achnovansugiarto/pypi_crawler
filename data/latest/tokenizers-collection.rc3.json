{
  "info": {
    "author": "Xiaoquan Kong",
    "author_email": "u1mail2me@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "==========================\n中文分词器集合\n==========================\n\n\n.. image:: https://img.shields.io/pypi/v/chinese_tokenzier_iterator.svg\n        :target: https://pypi.python.org/pypi/tokenizers_collection\n\n.. image:: https://img.shields.io/travis/howl-anderson/chinese_tokenzier_iterator.svg\n        :target: https://travis-ci.org/howl-anderson/tokenizers_collection\n\n.. image:: https://readthedocs.org/projects/chinese-tokenzier-iterator/badge/?version=latest\n        :target: https://tokenizers-collection.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\n\n\n\n一些中文分词器的简单封装和集合\n\n\n* Free software: MIT license\n* Documentation: https://chinese-tokenzier-iterator.readthedocs.io.\n\n\nFeatures\n--------\n\n* TODO\n\n使用\n----\n.. code-block:: python\n\n    from tokenizers_collection.config import tokenizer_registry\n    for name, tokenizer in tokenizer_registry:\n        print(\"Tokenizer: {}\".format(name))\n        tokenizer('input_file.txt', 'output_file.txt')\n\n安装\n----\n.. code-block:: bash\n\n    pip install tokenizers_collection\n\n更新许可文件与下载模型\n=======================\n因为其中有些模型需要更新许可文件（比如：pynlpir）或者需要下载模型文件（比如：pyltp），因此安装后需要执行特定的命令完成操作，这里已经将所有的操作封装成了一个函数，只需要执行类似如下的指令即可\n\n.. code-block:: bash\n\n    python -m tokenizers_collection.helper\n\n\nCredits\n-------\n\nThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n\n\n=======\nHistory\n=======\n\n0.1.0 (2018-08-28)\n------------------\n\n* First release on PyPI.\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/howl-anderson/tokenizers_collection",
    "keywords": "tokenizers_collection",
    "license": "MIT license",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tokenizers-collection",
    "package_url": "https://pypi.org/project/tokenizers-collection/",
    "platform": "",
    "project_url": "https://pypi.org/project/tokenizers-collection/",
    "project_urls": {
      "Homepage": "https://github.com/howl-anderson/tokenizers_collection"
    },
    "release_url": "https://pypi.org/project/tokenizers-collection/0.1.2/",
    "requires_dist": [
      "Click (>=6.0)",
      "requests",
      "jieba",
      "thulac",
      "pynlpir",
      "pyltp"
    ],
    "requires_python": "",
    "summary": "A simple iterator for using a set of Chinese tokenizer",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4215849,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "582ebe637297cae232cb44ebeb1d6916586d563d7b0120c27cd2f597d6b7060d",
          "md5": "5b052a1c97d9cb1cdabb1302afda23bd",
          "sha256": "5635078fa7464de8e8f2422d7e2d5c04b5f221f8c46d183a73ccf58c5a65bf88"
        },
        "downloads": -1,
        "filename": "tokenizers_collection-0.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5b052a1c97d9cb1cdabb1302afda23bd",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 7638,
        "upload_time": "2018-08-28T16:31:31",
        "upload_time_iso_8601": "2018-08-28T16:31:31.372375Z",
        "url": "https://files.pythonhosted.org/packages/58/2e/be637297cae232cb44ebeb1d6916586d563d7b0120c27cd2f597d6b7060d/tokenizers_collection-0.1.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c6679d1bcf7e48c2abfe9f458bd3a79eb86a0029b9b4f9e13dafe5d4c51fee8d",
          "md5": "815ed18733ba0e273ebfe116510c6642",
          "sha256": "4064b30f879b9e2648e8328667dd67315e095a3dc459988c086d93c6ad30d7d2"
        },
        "downloads": -1,
        "filename": "tokenizers_collection-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "815ed18733ba0e273ebfe116510c6642",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11150,
        "upload_time": "2018-08-28T16:31:33",
        "upload_time_iso_8601": "2018-08-28T16:31:33.336525Z",
        "url": "https://files.pythonhosted.org/packages/c6/67/9d1bcf7e48c2abfe9f458bd3a79eb86a0029b9b4f9e13dafe5d4c51fee8d/tokenizers_collection-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "60e5f0c957658cac4b0562feaea5ebdb905d297a45a91ada0dcb0fd5cc9494a3",
          "md5": "f19dfd8c152d88955aac2e14fd784044",
          "sha256": "2e2ef7e7ad287275c56feef66fddd934c097f82a3ab0720c375a97389cb63c88"
        },
        "downloads": -1,
        "filename": "tokenizers_collection-0.1.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f19dfd8c152d88955aac2e14fd784044",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 7670,
        "upload_time": "2018-08-28T16:47:40",
        "upload_time_iso_8601": "2018-08-28T16:47:40.167360Z",
        "url": "https://files.pythonhosted.org/packages/60/e5/f0c957658cac4b0562feaea5ebdb905d297a45a91ada0dcb0fd5cc9494a3/tokenizers_collection-0.1.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "76df08f9fb8fe1768ba0c6bbfc75e1859c7807de413859905c4ab8f8a24717b9",
          "md5": "00cdfdfbe8714af968d635029211a15e",
          "sha256": "b5fed4237c62691f7b1b378d11b20b948904418adc1612df6cecb78971854f38"
        },
        "downloads": -1,
        "filename": "tokenizers_collection-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "00cdfdfbe8714af968d635029211a15e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11192,
        "upload_time": "2018-08-28T16:47:41",
        "upload_time_iso_8601": "2018-08-28T16:47:41.765174Z",
        "url": "https://files.pythonhosted.org/packages/76/df/08f9fb8fe1768ba0c6bbfc75e1859c7807de413859905c4ab8f8a24717b9/tokenizers_collection-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f20a2058b20bbaf939b8cfad4c7205af0853fbd6d75ff057c6e194f25f591dda",
          "md5": "fab7fe2976415933da7132badf12a8a8",
          "sha256": "40bf32f5352f0542a8c92c7bab73451f21c873676568ed9709acdfb88601dc03"
        },
        "downloads": -1,
        "filename": "tokenizers_collection-0.1.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fab7fe2976415933da7132badf12a8a8",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 7671,
        "upload_time": "2018-08-28T16:51:07",
        "upload_time_iso_8601": "2018-08-28T16:51:07.587130Z",
        "url": "https://files.pythonhosted.org/packages/f2/0a/2058b20bbaf939b8cfad4c7205af0853fbd6d75ff057c6e194f25f591dda/tokenizers_collection-0.1.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f13524a0fae90c6254b9ccc62ec416ef5236e892b7e8455a8043c3cfc94e961",
          "md5": "3569cce32dca6ee44e544e6a31b5bc30",
          "sha256": "b43e162333bf43e2ae80d567bc9dff48ed9ecfdffbfbf2fafc62035c52b39f9e"
        },
        "downloads": -1,
        "filename": "tokenizers_collection-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3569cce32dca6ee44e544e6a31b5bc30",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11192,
        "upload_time": "2018-08-28T16:51:09",
        "upload_time_iso_8601": "2018-08-28T16:51:09.572738Z",
        "url": "https://files.pythonhosted.org/packages/5f/13/524a0fae90c6254b9ccc62ec416ef5236e892b7e8455a8043c3cfc94e961/tokenizers_collection-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f20a2058b20bbaf939b8cfad4c7205af0853fbd6d75ff057c6e194f25f591dda",
        "md5": "fab7fe2976415933da7132badf12a8a8",
        "sha256": "40bf32f5352f0542a8c92c7bab73451f21c873676568ed9709acdfb88601dc03"
      },
      "downloads": -1,
      "filename": "tokenizers_collection-0.1.2-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fab7fe2976415933da7132badf12a8a8",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 7671,
      "upload_time": "2018-08-28T16:51:07",
      "upload_time_iso_8601": "2018-08-28T16:51:07.587130Z",
      "url": "https://files.pythonhosted.org/packages/f2/0a/2058b20bbaf939b8cfad4c7205af0853fbd6d75ff057c6e194f25f591dda/tokenizers_collection-0.1.2-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5f13524a0fae90c6254b9ccc62ec416ef5236e892b7e8455a8043c3cfc94e961",
        "md5": "3569cce32dca6ee44e544e6a31b5bc30",
        "sha256": "b43e162333bf43e2ae80d567bc9dff48ed9ecfdffbfbf2fafc62035c52b39f9e"
      },
      "downloads": -1,
      "filename": "tokenizers_collection-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "3569cce32dca6ee44e544e6a31b5bc30",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 11192,
      "upload_time": "2018-08-28T16:51:09",
      "upload_time_iso_8601": "2018-08-28T16:51:09.572738Z",
      "url": "https://files.pythonhosted.org/packages/5f/13/524a0fae90c6254b9ccc62ec416ef5236e892b7e8455a8043c3cfc94e961/tokenizers_collection-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}