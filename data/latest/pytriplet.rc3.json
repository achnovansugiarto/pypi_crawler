{
  "info": {
    "author": "Congcong Wang",
    "author_email": "wangcongcongcc@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<p align=\"center\">\n    <br>\n    <img src=\"https://ucdcs-student.ucd.ie/~cwang/ttt/ttt_logo.png\" width=\"400\"/>\n    <br>\n<p>\n\n<p align=\"center\">\n    <br>\n    <a href=\"https://github.com/wangcongcong123/ttt/issues\"><img src=\"https://img.shields.io/github/issues/wangcongcong123/ttt\"/></a>\n    <img src=\"https://img.shields.io/github/forks/wangcongcong123/ttt\"/>\n    <a href=\"https://github.com/wangcongcong123/ttt\"><img src=\"https://img.shields.io/github/stars/wangcongcong123/ttt\"/></a>\n     <img src=\"https://img.shields.io/github/license/wangcongcong123/ttt\"/>\n     <a href=\"https://pypi.org/project/pytriplet/\"><img src=\"https://img.shields.io/github/v/release/wangcongcong123/ttt\"/></a>\n    <br>\n<p>\n\n## TTT: Fine-tuning Transformers with TPUs or GPUs acceleration, written in Tensorflow2.0+\n\n**TTT** or (**Triple T**) is short for a package for fine-tuning ðŸ¤— **T**ransformers with **T**PUs, written in **T**ensorflow2.0+. It is motivated to be completed due to bugs I found tricky to solve when using [the xla library](https://github.com/pytorch/xla) with PyTorch. As a newcomer to the TF world, I am humble to learn more from the community and hence it is open sourced here.\n\n\n#### Update (2020-10-23):\n\n- [Our work at W-NUT 2020 Shared task 3 for COVID event extraction on social media](./covid_event).\n\n## Demo \n [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wangcongcong123/ttt/blob/master/ttt_notebook.ipynb)\n \nThe following demonstrates the example of fine-tuning T5-small for sst2 ([example_t5.py](example_t5.py)).\n\n![](ttt_demo.png)\n<!-- \n* This can be scaled to t5-large, t5-large, or even 3B if a 8-cores TPU is available. For GPUs, this fine-tuning is tested on T5-large in a server of 4 12-GB GTX-1080s even with per_device_train_batch_size=2, leading to a out-of-memory exception (OOM).\n-->\n## Features\n- Switch between TPUs and GPUs easily.\n- Stable training on TPUs.\n- Customize datasets or load from [HF's datasets library](https://huggingface.co/nlp/viewer/?dataset=aeslc).\n- Using pretrained tensorflow weights from the open-source library - [ðŸ¤— transformers](https://github.com/huggingface/transformers).\n- Fine-tuning BERT-like transformers (DistilBert, ALBERT, Electra, RoBERTa) using keras High-level API.\n- Fine-tuning T5-like transformers using customize training loop, written in tensorflow2.0.\n- Supported tasks include single sequence-based classification task (both BERT-like models and T5 model), and translation, QA, or summarization (T5, as long as an example is characterized by: `{\"source\",\"....\",\"target\",\"....\"}`\n\n## Quickstart\n\n#### Install\n```\npip install pytriplet\n```\n\nor if you want to get the latest updates:\n\n```shell\ngit clone https://github.com/wangcongcong123/ttt.git\ncd ttt\npip install -e .\n```\n\n* make sure `transformers>=3.1.0`. If not, install via `pip install transformers -U`\n#### update (2020-09-13): Example generation for T5 pretraining objective\n```python\nfrom ttt import iid_denoise_text\ntext=\"ttt is short for a package for fine-tuning ðŸ¤— Transformers with TPUs, written in Tensorflow2.0\"\n# here the text is split by space to tokens, you can use huggingface's T5Tokenizer to tokenize as well.\noriginal, source, target=iid_denoise_text(text.split(), span_length=3, corrupt_ratio=0.25)\n\n# original: ['ttt', 'is', 'short', 'for', 'a', 'package', 'for', 'fine-tuning', 'ðŸ¤—', 'Transformers', 'with', 'TPUs,', 'written', 'in', 'Tensorflow2.0']\n# source: ['ttt', '<extra_id_0>', 'a', 'package', 'for', 'fine-tuning', 'ðŸ¤—', 'Transformers', 'with', '<extra_id_1>', '<extra_id_2>']\n# target: ['<extra_id_0>', 'is', 'short', 'for', '<extra_id_1>', 'TPUs,', 'written', 'in', 'Tensorflow2.0']\n```\n\n#### Update (2020-10-15): Example of fine-tuning T5 for translation ([example_trans_t5.py](example_trans_t5.py))\n\n<a name=\"wmt_en_ro_t5\"></a>\n**Fine-tuning**: No boilerplate codes changed (the same as [example_t5](example_t5.py)) except for the following args:\n```python3\n# any one from MODELS_SUPPORT (check:ttt/args.py)\nargs.model_select = \"t5-small\"\n# the path to the translation dataset, each line represents an example in jsonl format like: {\"target\": \"...\", \"source\",\"...\"}\n# it will download automatically for the frist time from: https://s3.amazonaws.com/datasets.huggingface.co/translation/wmt_en_ro.tar.gz\nargs.data_path = \"data/wmt_en_ro\"\n# any one from TASKS_SUPPORT (check:ttt/args.py)\nargs.task = \"translation\"\nargs.max_src_length=128\nargs.max_tgt_length=128\nargs.source_field_name=\"source\"\nargs.target_field_name=\"target\"\nargs.eval_on=\"bleu\" #this refers to sacrebleu as used in T5 paper\n```\n\n** On a TPUv3-8, the bleu score achieved by t5-base is 27.9 (very close to 28 as reported in [the T5 paper](https://arxiv.org/abs/1910.10683)), the fine-tuning args are [here](https://ucdcs-student.ucd.ie/~cwang/ttt/models/en2ro_t5_base/args.json) and training log is [here](https://ucdcs-student.ucd.ie/~cwang/ttt/models/en2ro_t5_base/train.log). \n\n#### Example of fine-tuning BERT for sst2 ([example_bert.py](example_bert.py))\n```python3\nfrom ttt import *\n\nif __name__ == '__main__':\n    args = get_args()\n    # check what args are available\n    logger.info(f\"args: {json.dumps(args.__dict__, indent=2)}\")\n    ############### customize args\n    # args.use_gpu = True\n    args.use_tpu = True\n    args.do_train = True\n    args.use_tb = True\n    # any one from MODELS_SUPPORT (check:ttt/args.py)\n    args.model_select = \"bert-base-uncased\"\n    # select a dataset following jsonl format, where text filed name is \"text\" and label field name is \"label\"\n    args.data_path = \"data/glue/sst2\"\n    # any one from TASKS_SUPPORT (check:ttt/args.py)\n    args.task = \"single-label-cls\"\n    args.log_steps = 400\n    # any one from LR_SCHEDULER_SUPPORT (check:ttt/args.py)\n    args.scheduler=\"warmuplinear\"\n    # set do_eval = False if your data does not contain a validation set. In that case, patience, and early_stop will be invalid\n    args.do_eval = True\n    args.tpu_address = \"x.x.x.x\" # replace with yours\n    ############### end customize args\n    # to have a sanity check for the args\n    sanity_check(args)\n    # seed everything, make deterministic\n    set_seed(args.seed)\n    tokenizer = get_tokenizer(args)\n    inputs = get_inputs(tokenizer, args)\n    model, _ = create_model(args, logger, get_model)\n    # start training, here we keras high-level API\n    training_history = model.fit(\n        inputs[\"x_train\"],\n        inputs[\"y_train\"],\n        epochs=args.num_epochs_train,\n        verbose=2,\n        batch_size=args.per_device_train_batch_size*args.num_replicas_in_sync,\n        callbacks=get_callbacks(args, inputs, logger, get_evaluator),\n    )\n```\n\nSo far the package has included the following supports for `args.model_select`, `args.task` and `args.scheduler` ([args.py](ttt/args.py)).\n\n```python3\n# these have been tested and work fine. more can be added to this list to test\nMODELS_SUPPORT = [\"distilbert-base-cased\",\"bert-base-uncased\", \"bert-large-uncased\", \"google/electra-base-discriminator\",\n                  \"google/electra-large-discriminator\", \"albert-base-v2\", \"roberta-base\",\n                  \"t5-small\",\"t5-base\"]\n# if using t5 models, the tasks has to be t2t* ones\nTASKS_SUPPORT = [\"single-label-cls\", \"t2t\"]\n# in the future, more schedulers will be added, such as warmupconstant, warmupcosine, etc.\nLR_SCHEDULER_SUPPORT = [\"warmuplinear\", \"warmupconstant\", \"constant\"]\n```\n\n## Command lines (suited in GCP)\n\nThis has to be run in Google GCP VM instance since the tpu_address is internal IP from Google (or change `--use_tpu` to `use_gpu` if you have enough GPUs). The flag `--tpu_address` should be replaced with yours. Notice: these runs are run with a set of \"look-good\" hyper-parameters but not exhaustively selected.\n\n#### Experiment BERT on sst2 using TPUv2-8\n\nC-1-1:\n```\npython3 run.py --model_select bert-base-uncased --data_path data/glue/sst2 --task single-label-cls --per_device_train_batch_size 8 --num_epochs_train 6 --max_seq_length 128 --lr 5e-5 --schedule warmuplinear --do_train --do_eval --do_test --use_tpu --tpu_address x.x.x.x\n```\n\nC-1-2:\n\n```\npython3 run.py --model_select bert-large-uncased --data_path data/glue/sst2 --task single-label-cls --per_device_train_batch_size 8 --num_epochs_train 6 --max_seq_length 128 --lr 5e-5 --schedule warmuplinear --do_train --do_eval --do_test --use_tpu --tpu_address x.x.x.x\n```\n\n** In addition, experiments on larger batch sizes were also conducted on TPUv2-8. For example, when `per_device_train_batch_size` is 128 (batch size=8*128=1024), this first epoch takes around ~1 minute and the rest of each takes just ~15 seconds! That is fast but the sst2 accuracy goes down significantly.\n\n#### Results\n\n|             | bert-base-uncased   (110M) |                                                |                             |                                 | bert-large-uncased   (340M) |                                                |                             |                                 |\n|-------------|:--------------------------:|:----------------------------------------------:|:---------------------------:|---------------------------------|:---------------------------:|:----------------------------------------------:|:---------------------------:|---------------------------------|\n|             | here                       | [BERT paper](https://arxiv.org/abs/1810.04805) | reproduction (here) command | time spent on a [n1-standard-8](https://cloud.google.com/compute/docs/machine-types) * | here                        | [BERT paper](https://arxiv.org/abs/1810.04805) | reproduction (here) command | time spent on a [n1-standard-8](https://cloud.google.com/compute/docs/machine-types) * |\n| sst2 (test set, acc.) | 93.36                      | 93.5                                           | C-1-1                       | 16 minutes                      | 94.45                       | 94.9                                           | C-1-2                       | 37 minutes                      |\n* *refer to the estimated time including training, every 400 steps evaluation and evaluation on testing.\n* Looks good, the results are close to the original reported results.\n\n### Experiment T5 on sst2 using TPUv2-8\n\nC-2-1:\n```\npython3 run.py --model_select t5-small --data_path data/glue/sst2 --task t2t --per_device_train_batch_size 8 --num_epochs_train 6 --max_seq_length 128 --lr 5e-5 --schedule warmuplinear --do_train --do_eval --do_test --use_tpu --tpu_address x.x.x.x\n```\nC-2-2:\n```\npython3 run.py --model_select t5-base --data_path data/glue/sst2 --task t2t --per_device_train_batch_size 8 --num_epochs_train 6 --max_seq_length 128 --lr 5e-5 --schedule warmuplinear --do_train --do_eval --do_test --use_tpu --tpu_address x.x.x.x\n```\n\nC-2-3:\n```\npython3 run.py --model_select t5-large --data_path data/glue/sst2 --task t2t --per_device_train_batch_size 2 --eval_batch_size 8 --num_epochs_train 6 --max_seq_length 128 --lr 5e-5 --schedule warmuplinear --do_train --do_eval --do_test --use_tpu --tpu_address x.x.x.x \n```\n** failed (out-of-memory) although `per_device_train_batch_size`=2. Does a TPUv2-8 not have enough memory to fine-tune a `t5-large` model? Looking for solutions to fine-tune `t5-large`. **Update:** Later on, I am lucky to get a TPUv3-8 (128G), so it is run successfully.\n\n#### Results\n\n|                       | t5-small (60M) |                                              |                             |                                 | t5-base (220M) |                                              |                             |                                 | t5-large (770 M) |                                              |                             |                                 |\n|-----------------------|:--------------:|:--------------------------------------------:|:---------------------------:|:-------------------------------:|:--------------:|:--------------------------------------------:|:---------------------------:|:-------------------------------:|:----------------:|:--------------------------------------------:|:---------------------------:|:-------------------------------:|\n|                       | here           | [T5 paper](https://arxiv.org/abs/1910.10683) | reproduction (here) command | time spent on a n1-standard-8 * | here           | [T5 paper](https://arxiv.org/abs/1910.10683) | reproduction (here) command | time spent on a n1-standard-8 * | here             | [T5 paper](https://arxiv.org/abs/1910.10683) | reproduction (here) command | time spent on a n1-standard-8 ** |\n| sst2 (test set, acc.) | 90.12          | 91.8                                         | C-2-1                       | 20 minutes                      | 94.18          | 95.2                                         | C-2-2                       | 36 minutes                      | 95.77            | 96.3                                         | C-2-3                       | 4.5 hours                       |\n\n* *refer to the estimated time including training, every 400 steps evaluation and evaluation on testing.\n* **the same but with a TPUv3-8 and smaller batch size (see command C-2-3).\n* Looks not bad, the results are a bit close to the original reported results.\n\n\n## Contributions\n- Contributions are welcome.\n\n## Todo ideas\n- To include more different language tasks, such as sequence-pair based classificaton, t5 toy pretraining, etc.\n- LR scheduler so far include \"warmuplinear\", \"warmupconstant\", \"constant\", \"constantlinear\". The plan is to implement all these that are available in [optimizer_schedules](https://huggingface.co/transformers/main_classes/optimizer_schedules.html#schedules). \n- Now all fine-tuning use Adam as the default optimizer. The plan is to implement others such as AdaFactor, etc.\n- Optimizations include: TF clip_grad_norm as used in PyTroch fine-tuning, AMP training, etc.\n\n<a id=\"last\"></a>\n## Last\n\nI have been looking for PyTorch alternatives that can help train large models with Google's TPUs in Google's GCP VM instance env. Although the [xla](https://github.com/pytorch/xla) lib seems good, I gave it up due to some bugs I found hard to fix. Something like \"process terminated with SIGKILL\" confused me a lot, and took me loads of time, and eventually fail to solve after searching all kinds of answers online ([ref1](https://github.com/PyTorchLightning/pytorch-lightning/issues/1590), [ref2](https://github.com/huggingface/transformers/issues/3660), the community looks not that active in this field). Later on, some clues online tell me this problem is something related to memory overloading and I expect the xla lib will be more stable release in the future. It works well when being experimented with [the MNIST example](https://cloud.google.com/tpu/docs/tutorials/mnist) provided in Google's official website but comes up the \"memory\" problem when tested on big models like transformers (I did not make this ðŸ¤— transformers' [xla_spawn.py](https://github.com/huggingface/transformers/blob/master/examples/xla_spawn.py) run successful either).\n\nHence, I shift to learn Tensorflow as a newcomer from PyTorch to make my life easy whenever I feel needed to train a model on TPUs. Thankfully, Tensorflow-2.0 makes this shift not that difficult although some [complains](https://twitter.com/snrrrub/status/1301228252325797888) on it always go on. After around three days of researching and coding, I end up with this simple package. This package is made public-available in hope of helping whoever has the same encountering as me. Most of the training code (so-called boilerplate codes) flow in this package looks a style of PyTorch due to my old habit. Hopefully, this makes it easy to know Tensorflow-2.0 when you are from PyTorch and you need TPUs. \n\n### Ack.\nThanks for [Google's TFRC Program](https://www.tensorflow.org/tfrc) giving TPUs credits to make this possible.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/wangcongcong123/ttt/releases/download/v0.0.3/pytriplet.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/wangcongcong123/ttt",
    "keywords": "Transformers,Tensorflow,TPUs acceleration",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pytriplet",
    "package_url": "https://pypi.org/project/pytriplet/",
    "platform": "",
    "project_url": "https://pypi.org/project/pytriplet/",
    "project_urls": {
      "Download": "https://github.com/wangcongcong123/ttt/releases/download/v0.0.3/pytriplet.tar.gz",
      "Homepage": "https://github.com/wangcongcong123/ttt"
    },
    "release_url": "https://pypi.org/project/pytriplet/0.0.5/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Fine-tuning Transformers with TPUs or GPUs acceleration, written in Tensorflow2.0",
    "version": "0.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8575430,
  "releases": {
    "0.0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "30a4563d89032344d6a14c3e13029dc7e7b4aae5b024eb0256565f5898e5a70c",
          "md5": "25f6eee3c931051d31f54cec65e57d2b",
          "sha256": "57b5c232a75152efe3ec30eafc68115940a5ec4be67a89f2d723abdf83564ce9"
        },
        "downloads": -1,
        "filename": "pytriplet-0.0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "25f6eee3c931051d31f54cec65e57d2b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22145,
        "upload_time": "2020-09-13T20:05:06",
        "upload_time_iso_8601": "2020-09-13T20:05:06.374793Z",
        "url": "https://files.pythonhosted.org/packages/30/a4/563d89032344d6a14c3e13029dc7e7b4aae5b024eb0256565f5898e5a70c/pytriplet-0.0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4f06d2915ee648f8dfdafdd4aba84d5adad08b222f3249a7f12a9102ed5c7b91",
          "md5": "24f3572844536b01ce5a53bbd2a2fa6a",
          "sha256": "7ea90f92dee3ee3809a839838ef3c7a59a2c845fe415693b08454683719b64d8"
        },
        "downloads": -1,
        "filename": "pytriplet-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "24f3572844536b01ce5a53bbd2a2fa6a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23006,
        "upload_time": "2020-09-20T11:59:33",
        "upload_time_iso_8601": "2020-09-20T11:59:33.861641Z",
        "url": "https://files.pythonhosted.org/packages/4f/06/d2915ee648f8dfdafdd4aba84d5adad08b222f3249a7f12a9102ed5c7b91/pytriplet-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2c8d832497c90ee1fcde52c157daccfbd770333ee50c1f88a92831ba463569e8",
          "md5": "63e10ca9f61e931fbadeb46138f98287",
          "sha256": "341ec8a2bea07a36cbeed9bbfa94d896aebf8bd823248d5e42e5aadf0b83a908"
        },
        "downloads": -1,
        "filename": "pytriplet-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "63e10ca9f61e931fbadeb46138f98287",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 25444,
        "upload_time": "2020-11-04T01:03:21",
        "upload_time_iso_8601": "2020-11-04T01:03:21.430085Z",
        "url": "https://files.pythonhosted.org/packages/2c/8d/832497c90ee1fcde52c157daccfbd770333ee50c1f88a92831ba463569e8/pytriplet-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2c8d832497c90ee1fcde52c157daccfbd770333ee50c1f88a92831ba463569e8",
        "md5": "63e10ca9f61e931fbadeb46138f98287",
        "sha256": "341ec8a2bea07a36cbeed9bbfa94d896aebf8bd823248d5e42e5aadf0b83a908"
      },
      "downloads": -1,
      "filename": "pytriplet-0.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "63e10ca9f61e931fbadeb46138f98287",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 25444,
      "upload_time": "2020-11-04T01:03:21",
      "upload_time_iso_8601": "2020-11-04T01:03:21.430085Z",
      "url": "https://files.pythonhosted.org/packages/2c/8d/832497c90ee1fcde52c157daccfbd770333ee50c1f88a92831ba463569e8/pytriplet-0.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}