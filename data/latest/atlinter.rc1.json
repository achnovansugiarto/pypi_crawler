{
  "info": {
    "author": "Blue Brain Project, EPFL",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: MacOS",
      "Operating System :: Unix",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# Atlas Interpolation\n\nThe Allen Brain Institute hosts a rich database of mouse brain imagery. It\ncontains a large number of gene expression datasets obtained\nthrough the in situ hybridization (ISH) staining. While for a given gene\na number of datasets corresponding to different specimen can be found, each of\nthese datasets only contains sparse section images that do not form a\ncontinuous volume. This package explores techniques that allow to interpolate\nthe missing slices and thus reconstruct whole gene expression volumes.\n\n* [Installation](#installation)\n    * [Python Version and Environment](#python-version-and-environment)\n    * [Install \"Atlas Interpolation\"](#install-atlas-interpolation)\n* [Data](#data)\n    * [Remote Storage Access](#remote-storage-access) \n    * [Model Checkpoints](#model-checkpoints)\n    * [Section Images and Datasets](#section-images-and-datasets)\n    * [New ISH datasets (advanced, optional)](#new-ish-datasets-advanced-optional)\n* [Examples](#examples)\n    * [Pair Interpolation](#pair-interpolation)\n    * [Optical Flow Models](#optical-flow-models)\n    * [Predict an Entire Gene Volume (longer runtime)](#predict-an-entire-gene-volume-longer-runtime)\n* [Vendors](#vendors)\n* [Funding & Acknowledgment](#funding--acknowledgment)\n\n## Installation\n### Python Version and Environment\nNote that due to some of our dependencies we're currently limited to python\nversion `3.7`. Please make sure you set up a virtual environment with that\nversion before trying to install this library. If you're unsure how to do that\nplease have a look at [conda](https://docs.conda.io) or\n[pyenv](https://github.com/pyenv/pyenv).\n\nIf you are part of the Blue Brain Project and are working on the BB5 you can\nfind the correct python version in the archive modules between `archive/2020-02`\nand `archive/2020-12` (inclusive). Here's an example of a set of commands\nthat will set up your environment on the BB5:\n```shell\nmodule purge\nmodule load archive/2020-12\nmodule load python\npython -m venv venv\n. ./venv/bin/activate\npython --version\n```\n\nWe also recommend that you make sure that `pip` is up-to-date and that the\npackages `wheel` and `setuptools` are installed:\n```shell\npip install --upgrade pip wheel setuptools\n```\n\n### Install \"Atlas Interpolation\"\nIn order to access the data and the example scripts a local clone of this\nrepository is required. Run these commands to get it:\n```shell\ngit clone https://github.com/BlueBrain/atlas-interpolation\ncd atlas-interpolation\n```\n\nThe \"Atlas Interpolation\" package can now be installed directly from the clone\nwe just created:\n```shell\npip install '.[data, optical]'\n```\n\n## Data\nThe data for this project is managed by the [DVC tool](https://dvc.org/) and all\nrelated files are located in the `data` directory. The DVC tool has already been\ninstalled together with the \"Atlas Interpolation\" package. Every time you need\nto run a DVC command (`dvc ...`) make sure to change to the `data` directory\nfirst (`cd data`).\n\n### Remote Storage Access\nWe have already prepared all the data, but it is located on a remote storage\nthat is only accessible to people within the Blue Brain Project who have\naccess permissions to project `proj101`. If you're unsure you can test your\npermissions with the following command:\n```shell\nssh bbpv1.bbp.epfl.ch \\\n\"ls /gpfs/bbp.cscs.ch/data/project/proj101/dvc_remotes\"\n```\nPossible outcomes:\n```shell\n# Access OK\natlas_annotation\natlas_interpolation\n\n# Access denied\nls: cannot open directory [...]: Permission denied\n```\nDepending on whether you have access to the remote storage in the following\nsections you will either pull the data from the remote (`dvc pull`) or download\nthe input data manually and re-run the data processing pipelines to reproduce\nthe output data (`dvc repro`).\n\nIf you work on the BB5 and have access to the remote storage then run the\nfollowing command to short-circuit the remote access (because the remote is\nlocated on the BB5 itself):\n```shell\ncd data\ndvc remote add --local gpfs_proj101 \\\n  /gpfs/bbp.cscs.ch/data/project/proj101/dvc_remotes/atlas_interpolation\ncd ..\n```\n\n### Model Checkpoints\nMuch of the functionality of \"Atlas Interpolation\" relies on pre-trained deep\nlearning models. The model checkpoints that need to be loaded are part of the\ndata.\n\nIf you have access to the remote storage (see above) you can pull all model\ncheckpoints from the remote:\n```shell\ncd data\ndvc pull checkpoints/rife.dvc\ndvc pull checkpoints/cain.dvc\ndvc pull checkpoints/maskflownet.params.dvc\ndvc pull checkpoints/RAFT.dvc\ncd ..\n```\n\nIf you don't have access to the remote you need to download the checkpoint files\nby hand and put the downloaded data into the `data/checkpoints` folder. You\nmay not need all the checkpoints depending on the examples you want to run. Here\nare the instructions for the four models we use: RIFE, CAIN, MaskFlowNet, and\nRAFT:\n* **RIFE**: download the checkpoint from a shared Google Drive folder by following\n  [this link](https://drive.google.com/file/d/11l8zknO1V5hapv2-Ke4DG9mHyBomS0Fc/view?usp=sharing).\n  Unzip the contents of the downloaded file into `data/checkpoints/rife`.\n  [[ref]](https://github.com/hzwer/arXiv2020-RIFE/tree/2a1eafe27d5ff12eb31df96e47352fe30c18ac46#usage)\n* **CAIN**: download the checkpoint from a shared Dropbox folder by following\n  [this link](https://www.dropbox.com/s/y1xf46m2cbwk7yf/pretrained_cain.pth?dl=0).\n  Move the downloaded file to `data/checkpoints/cain`.\n  [[ref]](https://github.com/myungsub/CAIN/tree/2e727d2a07d3f1061f17e2edaa47a7fb3f7e62c5#interpolating-with-custom-video)\n* **MaskFlowNet**: download the checkpoint directly from GitHub by following\n  [this link](https://github.com/microsoft/MaskFlownet/raw/5cba12772e2201f0d1c1e27161d224e585334571/weights/8caNov12-1532_300000.params).\n  Rename the file to `maskflownet.params` and move it to `data/checkpoints`.\n  [[ref]](https://github.com/microsoft/MaskFlownet/raw/5cba12772e2201f0d1c1e27161d224e585334571/weights)\n* **RAFT**: download the checkpoint files from a shared Dropbox folder by following\n  [this link](https://drive.google.com/drive/folders/1sWDsfuZ3Up38EUQt7-JDTT1HcGHuJgvT?usp=sharing).\n  Move all downloaded `.pth` files to the `data/checkpoints/RAFT/models` folder.\n  [[ref]](https://github.com/princeton-vl/RAFT/tree/224320502d66c356d88e6c712f38129e60661e80#demos)\n\nIf you downloaded all checkpoints or pulled them from the remote you should\nhave the following files:\n```text\ndata\n└── checkpoints\n    ├── RAFT\n    │   ├── models\n    │   │   ├── raft-chairs.pth\n    │   │   ├── raft-kitti.pth\n    │   │   ├── raft-sintel.pth\n    │   │   ├── raft-small.pth\n    │   │   └── raft-things.pth\n    ├── cain\n    │   └── pretrained_cain.pth\n    ├── maskflownet.params\n    └── rife\n        ├── contextnet.pkl\n        ├── flownet.pkl\n        └── unet.pkl\n```\n\n### Section Images and Datasets\nThe purpose of the \"Atlas Interpolation\" package is to interpolate missing\nsection images within section image datasets. This section explains how to\nobtain these data.\n\nRemember that if you don't have access to the remote storage (see above) you'll\nneed to use the `dvc repro` commands that download/process the data live. If\nyou do have access, you'll use `dvc pull` instead, which is faster.\n\nNormally it's not necessary to get all data. Due to its size it may take a lot\nof disk space as well as time to download and pre-process. If you still decide\nto do so you can by running `dvc repro` or `dvc pull` without any parameters.\n\nSpecific examples only require specific data. You can use DVC to list all data\npipeline stages to find out which stage produces the data you're interested in.\nTo list all data pipeline stages run:\n```shell\ncd data\ndvc stage list\n```\nIf, for example, you need data located in `data/aligned/coronal/Gad1`, then\naccording to the output of command above the relevant stage is named\n`align@Gad1`. Therefore, you only need to run this stage to get the necessary\ndata (replace `repro` by `pull` if you can access the remote storage):\n```shell\ndvc repro align@Gad1\n```\n\n### New ISH datasets (advanced, optional)\nIf you're familiar with the AIBS data that we're using and would like to add\nnew ISH gene expressions that are not yet available as one of our pipeline\nstages (check the output of `dvc stage list`) then follow the following\ninstructions.\n\n1. Edit the file `data/dvc.yaml` and add the new gene name to the lists in the\n   `stages:download_dataset:foreach` and `stages:align:foreach` sections.\n2. Run the data downloading and processing pipelines (replace `NEW_GENE` by the\n   real gene name that you used in `data/dvc.yaml`):\n   ```shell\n   dvc repro download_dataset@NEW_GENE\n   dvc repro align@NEW_GENE\n   ```\n\n## Examples\nIn this section we showcase several typical use-cases of \"Atlas Interpolation\":\n- Use pair interpolation to predict an intermediate image between two given\n  images\n- Predict optical flow between any pair of images and use it to morph a third\n  image\n- In a gene expression volume predict missing slices and reconstruct the whole\n  volume\n\nNote that all models accept both RGB images (`shape=(height, width, 3)`)\nand grayscale images (`shape=(height, width)`).\n\n### Pair Interpolation\nThe only data you need for this example is the RIFE model checkpoint. Follow\nthe instructions in the corresponding section above to get it. If you have\naccess to the remote data storage it's enough to run the following commands:\n```shell\ncd data\ndvc pull checkpoints/rife.dvc\ncd ..\n```\n\nIn this example we start with a pair of images `img1` and `img2` (randomly\ngenerated for example's sake). First use the RIFE model to interpolate between\nthem in a manual way and find the image in-between (`img_middle`). Then we\ndemonstrate the use of the `PairInterpolate` class that streamlines the\ninterpolation procedure. Starting with the same pair of images we iterate the\ninterpolation three times to produce a stack of seven interpolated images\n(`interpolated_imgs`).\n```python\nimport numpy as np\n\nfrom atlinter.vendor.rife.RIFE_HD import Model as RifeModel\nfrom atlinter.vendor.rife.RIFE_HD import device as rife_device\nfrom atlinter.pair_interpolation import PairInterpolate, RIFEPairInterpolationModel\n\n# Get the input images\nimg1 = np.random.rand(100, 200, 3) # replace by real section image\nimg2 = np.random.rand(100, 200, 3) # replace by real section image\n\n# Get the RIFE interpolation model\ncheckpoint_path = \"data/checkpoints/rife/\" # Please change, if needed\nrife_model = RifeModel()\nrife_model.load_model(checkpoint_path, -1)\nrife_model.eval()\ninterpolation_model = RIFEPairInterpolationModel(rife_model, rife_device)\n\n# Manually predict middle image between img1 and img2\npreimg1, preimg2 = interpolation_model.before_interpolation(img1=img1, img2=img2)\nimg_middle = interpolation_model.interpolate(img1=preimg1, img2=preimg2)\nimg_middle = interpolation_model.after_interpolation(img_middle)\nprint(img_middle.shape)\n\n# Streamline the interpolation using PairInterpolate and predict a stack\n# of 7 intermediate images\ninterpolator = PairInterpolate(n_repeat=3)\ninterpolated_imgs = interpolator(img1, img2, interpolation_model)\nprint(interpolated_imgs.shape)\n``` \n\n### Optical Flow Models\nThe only data you need for this example is the MaskFlowNet model checkpoint.\nFollow the instructions in the corresponding section above to get it. If you\nhave access to the remote data storage it's enough to run the following\ncommands:\n```shell\ncd data\ndvc pull checkpoints/maskflownet.params.dvc\ncd ..\n```\n\nThis example demonstrates how an optical flow model can be used to compute the\noptical flow between a pair of images. It can then be used to warp a third\nimage. The images in this example are randomly generated. In a realistic setting\nthey should be replaced by real images.\n```python\nimport numpy as np\n\nfrom atlinter.optical_flow import MaskFlowNet\n\n# Instantiate an optical flow model (in this case: MaskFlowNet)\ncheckpoint_path = \"data/checkpoints/maskflownet.params\"\nnet = MaskFlowNet(checkpoint_path)\n\n# Prepare random images. Should be replaced by real section images\nimg1 = np.random.rand(100, 200, 3)\nimg2 = np.random.rand(100, 200, 3)\nimg3 = np.random.rand(100, 200, 3)\n\n# Predict the optical flow between img1 and img2\nimg1, img2 = net.preprocess_images(img1=img1, img2=img2)\npredicted_flow = net.predict_flow(img1=img1, img2=img2)\n\n# Warp a third image using the optical flow\npredicted_img = net.warp_image(predicted_flow, img3)\nprint(predicted_img.shape)\n``` \n\n### Predict an Entire Gene Volume (Longer Runtime)\nThe data you need for this example are the RIFE model checkpoint and the Vip\ngene expression dataset. To get the RIFE checkpoint follow the instruction in\nthe corresponding section above. If you have access to the remote data storage\nit's enough to run the following commands:\n```shell\ncd data\ndvc pull checkpoints/rife.dvc\ncd ..\n```\nAs described in the data section above, there are two ways of getting the Vip\ngene expression dataset. If you have access to the remote data storage you can\npull it from there:\n```shell\ncd data\ndvc pull download_dataset@Vip\ncd ..\n```\nIf you don't have access then you can re-download it. This should always work,\nbut may take several minutes:\n```shell\ncd data\ndvc repro download_dataset@Vip\ncd ..\n```\n\nIn this example with start with a gene expression volume that has missing\nsection images. First we load the image data and the metadata from disk and\nwrap it into a `GeneDataset` class. Then we instantiate the RIFE deep learning\nmodel that will be used for interpolation. We use this model to first predict a\nsingle slice in the volume, then we reconstruct the whole volume by predicting\nall intermediate slices. Note that this last step is computation-heavy and might\ntherefore take some time.\n```python\nimport json\n\nimport numpy as np\n\nfrom atlinter.data import GeneDataset\nfrom atlinter.pair_interpolation import GeneInterpolate, RIFEPairInterpolationModel\nfrom atlinter.vendor.rife.RIFE_HD import Model as RifeModel\nfrom atlinter.vendor.rife.RIFE_HD import device as rife_device\n\n# Load the gene expression dataset from disk\ndata_path = \"data/sagittal/Vip/1102.npy\"  # Change the path if needed\ndata_json = \"data/sagittal/Vip/1102.json\" # Change the path if needed\nsection_images = np.load(data_path)\nwith open(data_json) as fh:\n    metadata = json.load(fh)\n\nsection_numbers = [int(s) for s in metadata[\"section_numbers\"]]\naxis = metadata[\"axis\"]\n\n# Wrap the data into a GeneDataset class\ngene_dataset = GeneDataset(\n  section_images,\n  section_numbers,\n  volume_shape=(528, 320, 456, 3),\n  axis=axis,\n)\n\n# Load the RIFE deep learning model that will be used for interpolation\ncheckpoint_path = \"data/checkpoints/rife\"\nrife_model = RifeModel()\nrife_model.load_model(checkpoint_path, -1)\nrife_model.eval()\nrife_interpolation_model = RIFEPairInterpolationModel(rife_model, rife_device)\n\n# Create a gene interpolator\ngene_interpolate = GeneInterpolate(gene_dataset, rife_interpolation_model)\n\n# Predict a single section image\npredicted_slice = gene_interpolate.predict_slice(10)\nprint(predicted_slice.shape)\n\n# Reconstruct the whole volume. This might take some time.\npredicted_volume = gene_interpolate.predict_volume()\nprint(predicted_volume.shape)\n```\n\n## Vendors\nSome dependencies are not available as packages and therefore had to be\nvendored. The vendoring is done using the\n[`py-vendor`](https://pypi.org/project/py-vendor/) utility. It's installed\nautomatically together with the `dev` extras. You can also install it by hand\nvia `pip install py-vendor==0.1.2`.\n\nThe vendoring is then done using the following command (add `--force` to\noverwrite existing folders):\n```shell\npy-vendor run --config py-vendor.yaml\n```\nSee the `py-vendor.yaml` file for details on the vendor sources and files.\n\n## Funding & Acknowledgment\n\nThe development of this software was supported by funding to the Blue Brain Project,\na research center of the École polytechnique fédérale de Lausanne (EPFL),\nfrom the Swiss government’s ETH Board of the Swiss Federal Institutes of Technology.\n\nCopyright (c) 2021 Blue Brain Project/EPFL\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/BlueBrain/atlas-interpolation",
    "keywords": "",
    "license": "Apache-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "atlinter",
    "package_url": "https://pypi.org/project/atlinter/",
    "platform": "",
    "project_url": "https://pypi.org/project/atlinter/",
    "project_urls": {
      "Homepage": "https://github.com/BlueBrain/atlas-interpolation",
      "Source": "https://github.com/BlueBrain/atlas-interpolation",
      "Tracker": "https://github.com/BlueBrain/atlas-interpolation/issues"
    },
    "release_url": "https://pypi.org/project/atlinter/0.2.3/",
    "requires_dist": [
      "mxnet",
      "numpy",
      "pytorch-fid",
      "pyyaml",
      "scipy",
      "torch",
      "torchvision",
      "atlannot ; extra == 'data'",
      "atldld (==0.2.2) ; extra == 'data'",
      "dvc[ssh] ; extra == 'data'",
      "pillow ; extra == 'data'",
      "pynrrd ; extra == 'data'",
      "requests ; extra == 'data'",
      "scikit-image ; extra == 'data'",
      "bandit ; extra == 'dev'",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "flake8-bugbear ; extra == 'dev'",
      "flake8-comprehensions ; extra == 'dev'",
      "flake8-docstrings ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "py-vendor (==0.1.2) ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "sphinx ; extra == 'docs'",
      "sphinx-bluebrain-theme ; extra == 'docs'",
      "opencv-python ; extra == 'optical'",
      "flow-vis ; extra == 'optical'",
      "moviepy ; extra == 'optical'"
    ],
    "requires_python": "~=3.7.0",
    "summary": "Interpolate missing section images in gene expression volumes",
    "version": "0.2.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13451269,
  "releases": {
    "0.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0b92333ba9285826fc341af5c998f7d7ffcaa18775583a52aa4a8cf7dbee6db6",
          "md5": "7764f7b182441108aad1217e84034b23",
          "sha256": "61e531b9f963e04e04a4437482f372e3a927676b590c0a6c64165ab6cc64bf80"
        },
        "downloads": -1,
        "filename": "atlinter-0.2.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7764f7b182441108aad1217e84034b23",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.7.0",
        "size": 87778,
        "upload_time": "2021-11-03T10:54:02",
        "upload_time_iso_8601": "2021-11-03T10:54:02.358719Z",
        "url": "https://files.pythonhosted.org/packages/0b/92/333ba9285826fc341af5c998f7d7ffcaa18775583a52aa4a8cf7dbee6db6/atlinter-0.2.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "57d34cdfaacccff677b32687cbcfd689727bc7b3a8f4485401aa3eca157e5aaf",
          "md5": "744d9dd2846df6c8e5d2a86b74100a8c",
          "sha256": "67f102b70bc7eeb450da8af3c19df38610dd081a025f9c04e8f5266d2d0cc6e3"
        },
        "downloads": -1,
        "filename": "atlinter-0.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "744d9dd2846df6c8e5d2a86b74100a8c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.7.0",
        "size": 84332,
        "upload_time": "2021-11-03T10:54:04",
        "upload_time_iso_8601": "2021-11-03T10:54:04.302401Z",
        "url": "https://files.pythonhosted.org/packages/57/d3/4cdfaacccff677b32687cbcfd689727bc7b3a8f4485401aa3eca157e5aaf/atlinter-0.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0b92333ba9285826fc341af5c998f7d7ffcaa18775583a52aa4a8cf7dbee6db6",
        "md5": "7764f7b182441108aad1217e84034b23",
        "sha256": "61e531b9f963e04e04a4437482f372e3a927676b590c0a6c64165ab6cc64bf80"
      },
      "downloads": -1,
      "filename": "atlinter-0.2.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "7764f7b182441108aad1217e84034b23",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "~=3.7.0",
      "size": 87778,
      "upload_time": "2021-11-03T10:54:02",
      "upload_time_iso_8601": "2021-11-03T10:54:02.358719Z",
      "url": "https://files.pythonhosted.org/packages/0b/92/333ba9285826fc341af5c998f7d7ffcaa18775583a52aa4a8cf7dbee6db6/atlinter-0.2.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "57d34cdfaacccff677b32687cbcfd689727bc7b3a8f4485401aa3eca157e5aaf",
        "md5": "744d9dd2846df6c8e5d2a86b74100a8c",
        "sha256": "67f102b70bc7eeb450da8af3c19df38610dd081a025f9c04e8f5266d2d0cc6e3"
      },
      "downloads": -1,
      "filename": "atlinter-0.2.3.tar.gz",
      "has_sig": false,
      "md5_digest": "744d9dd2846df6c8e5d2a86b74100a8c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "~=3.7.0",
      "size": 84332,
      "upload_time": "2021-11-03T10:54:04",
      "upload_time_iso_8601": "2021-11-03T10:54:04.302401Z",
      "url": "https://files.pythonhosted.org/packages/57/d3/4cdfaacccff677b32687cbcfd689727bc7b3a8f4485401aa3eca157e5aaf/atlinter-0.2.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}