{
  "info": {
    "author": "Xiaoqiao Chen",
    "author_email": "xqchen@caltech.edu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3"
    ],
    "description": "# activeSVC\nActiveSVC selects features for large matrix data with reduced computational complexity or limited data acquisition. It approaches Sequential Feature Selection through an active learning strategy with a support vector machine classifier. At each round of iteration, the procedure analyzes only the samples that classify poorly with the current feature set, and the procedure extends the feature set by identifying features within incorrectly classified samples that will maximally shift the classification margin. There are two strategy, min_complexity and min_acquisition. Min_complexity strategy tends to use less samples each iteration while min_acquisition strategy tends to re-use samples used in previous iterations to minimize the total samples we acquired during the procedure.\n\n## Why is activeSVC better than other feature selection methods?\n- Easy to use\n- Good for large datasets\n- Reduce Memory\n- Reduce computational complexity\n- Minimize the data size we need\n\n## Usage\nActiveSVC processes a datasets with training set and test set and returns the features selected, training accuracy, test accuracy, training mean squared error, test mean squared error, the number of samples acquired after every features are selected. We highly recommend to do l2-normalization for each sample before activeSVC to improve accuracy and speed up model training. \n\n## Requires\nnumpy, random, math, os, time, multiprocessing, sklearn, spcipy\n\n## Import\n    from activeSVC import min_complexity\n    from activeSVC import min_acquisition\n    from activeSVC import min_complexity_cv\n    from activeSVC import min_acquisition_cv\n    from activeSVC import min_complexity_h5py\n    from activeSVC import min_acquisition_h5py\n\n\n## Function\n- min_complexity: fix SVM parameters for each loop\n- min_acquisition: fix SVM parameters for each loop\n- min_complexity_cv: Every SVMs trained during the procedure are the best estimator by cross validation on parameters \"C\" and \"tol\".\n- min_acquisition_cv: Every SVMs trained during the procedure are the best estimator by cross validation on parameters \"C\" and \"tol\".\n- min_complexity_h5py: only load part of data matrix of selected features and samples into memory instead of loading the entire dataset to save memory usage. The data should be h5py file. \n- min_acquisition_h5py: only load part of data matrix of selected features and samples into memory instead of loading the entire dataset to save memory usage. The data should be h5py file. \n\n\n## min_complexity\n### Parameters\n    X_train: {ndarray, sparse matrix} of shape {n_samples_X, n_features}\n            Input data of training set.\n    y_train: ndarray of shape {n_samples_X,}\n            Input classification labels of training set.\n    X_test: {ndarray, sparse matrix} of shape {n_samples_X, n_features}\n            Input data of test set.\n    y_test: ndarray of shape {n_samples_X,}\n            Input classification labels of test set.\n    num_features: integer\n            The total number of features to select.\n    num_samples: integer\n            The number of samples to use in each iteration (for each feature).\n    init_features: integer, default=1\n            The number of features to select in the first iteration.\n    init_samples: integer, default=None\n            The number of samples to use in the first iteration.\n    balance: bool, default=False\n            Balance samples of each classes when sampling misclassified samples at each \n            iteration or randomly sample misclassified samples.\n    penalty: {‘l1’, ‘l2’}, default=’l2’\n            Specifies the norm used in the penalization. The ‘l2’ penalty is the \n            standard used in SVC. The ‘l1’ leads to sparse weight.\n    loss: {‘hinge’, ‘squared_hinge’}, default=’squared_hinge’\n            Specifies the loss function for each SVC to train. ‘hinge’ is the standard \n            SVM loss while ‘squared_hinge’ is the square of the hinge loss. \n            The combination of penalty='l1' and loss='hinge' is not supported.\n    dual: bool, default=True\n            Select the algorithm to either solve the dual or primal optimization \n            problem for each SVC. Prefer dual=False when n_samples > n_features.\n    tol: float, default=1e-4\n            Tolerance for stopping criteria for each SVC.\n    C: float, default=1.0\n            Regularization parameter for each SVC. The strength of the regularization \n            is inversely proportional to C. Must be strictly positive.\n    fit_intercept: bool, default=True\n            Whether to calculate the intercept for each SVC. If set to false, no \n            intercept will be used in calculations (i.e. data is already centered).\n    intercept_scaling: float, default=1\n            When self.fit_intercept is True, instance vector x becomes \n            [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value \n            equals to intercept_scaling is appended to the instance vector. The intercept \n            becomes intercept_scaling * synthetic feature weight Note! the synthetic \n            feature weight is subject to l1/l2 regularization as all other features. \n            To lessen the effect of regularization on synthetic feature weight \n            (and therefore on the intercept) intercept_scaling has to be increased.\n    class_weight: dict or ‘balanced’, default=None\n            Set the parameter C of class i to class_weight[i]*C for SVC. If not given, \n            all classes are supposed to have weight one. The “balanced” mode uses the \n            values of y to automatically adjust weights inversely proportional to class \n            frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n    random_state: int, RandomState instance or None, default=None\n            Controls the pseudo random number generation for shuffling data for the dual \n            coordinate descent (if dual=True). When dual=False underlying implementation \n            of LinearSVC is not random and random_state has no effect on the results. \n            Pass an int for reproducible output across multiple function calls.\n    max_iter: int, default=1000\n            The maximum number of iterations to be run for each SVC.\n\n\n### Return\n    feature_selected: list of integer\n            The sequence of features selected.\n    num_samples_list: list of integer\n            The number of unique samples acquired totally after every features are selected.\n    train_errors: list of float\n            Mean squared error of training set after every features are selected.\n    test_errors: list of float\n            Mean squared error of test set after every features are selected.\n    train_accuracy: list of float\n            Classification accuracy of training set after every features are selected.\n    test_accuracy: list of float\n            Classification accuracy of test set after every features are selected.\n    step_times: list of float\n            The run time of each iteration. Unit is second. \n\n## min_acquisition\n### Parameters\n    X_train: {ndarray, sparse matrix} of shape {n_samples_X, n_features}\n            Input data of training set.\n    y_train: ndarray of shape {n_samples_X,}\n            Input classification labels of training set.\n    X_test: {ndarray, sparse matrix} of shape {n_samples_X, n_features}\n            Input data of test set.\n    y_test: ndarray of shape {n_samples_X,}\n            Input classification labels of test set.\n    num_features: integer\n            The total number of features to select.\n    num_samples: integer\n            The number of misclassified samples randomly sampled, which are taken union with \n            samples already acquired before. The union of samples are used for next ietration.\n    init_features: integer, default=1\n            The number of features to select in the first iteration.\n    init_samples: integer, default=None\n            The number of samples to use in the first iteration.\n    balance: bool, default=False\n            Balance samples of each classes when sampling misclassified samples at each \n            iteration or randomly sample misclassified samples.\n    penalty: {‘l1’, ‘l2’}, default=’l2’\n            Specifies the norm used in the penalization. The ‘l2’ penalty is the \n            standard used in SVC. The ‘l1’ leads to sparse weight.\n    loss: {‘hinge’, ‘squared_hinge’}, default=’squared_hinge’\n            Specifies the loss function for each SVC to train. ‘hinge’ is the standard \n            SVM loss while ‘squared_hinge’ is the square of the hinge loss. \n            The combination of penalty='l1' and loss='hinge' is not supported.\n    dual: bool, default=True\n            Select the algorithm to either solve the dual or primal optimization \n            problem for each SVC. Prefer dual=False when n_samples > n_features.\n    tol: float, default=1e-4\n            Tolerance for stopping criteria for each SVC.\n    C: float, default=1.0\n            Regularization parameter for each SVC. The strength of the regularization \n            is inversely proportional to C. Must be strictly positive.\n    fit_intercept: bool, default=True\n            Whether to calculate the intercept for each SVC. If set to false, no \n            intercept will be used in calculations (i.e. data is already centered).\n    intercept_scaling: float, default=1\n            When self.fit_intercept is True, instance vector x becomes \n            [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value \n            equals to intercept_scaling is appended to the instance vector. The intercept \n            becomes intercept_scaling * synthetic feature weight Note! the synthetic \n            feature weight is subject to l1/l2 regularization as all other features. \n            To lessen the effect of regularization on synthetic feature weight \n            (and therefore on the intercept) intercept_scaling has to be increased.\n    class_weight: dict or ‘balanced’, default=None\n            Set the parameter C of class i to class_weight[i]*C for SVC. If not given, \n            all classes are supposed to have weight one. The “balanced” mode uses the \n            values of y to automatically adjust weights inversely proportional to class \n            frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n    random_state: int, RandomState instance or None, default=None\n            Controls the pseudo random number generation for shuffling data for the dual \n            coordinate descent (if dual=True). When dual=False underlying implementation \n            of LinearSVC is not random and random_state has no effect on the results. \n            Pass an int for reproducible output across multiple function calls.\n    max_iter: int, default=1000\n            The maximum number of iterations to be run for each SVC.\n\n### Return\n    feature_selected: list of integer\n            The sequence of features selected.\n    num_samples_list: list of integer\n            The number of unique samples acquired totally after every features are selected.\n    samples_global: list of integer\n            The indices of samples that are acquired.\n    train_errors: list of float\n            Mean squared error of training set after every features are selected.\n    test_errors: list of float\n            Mean squared error of test set after every features are selected.\n    train_accuracy: list of float\n            Classification accuracy of training set after every features are selected.\n    test_accuracy: list of float\n            Classification accuracy of test set after every features are selected.\n    step_times: list of float\n            The run time of each iteration. Unit is second. \n\n\n## min_complexity_cv\n### Parameters\n    X_train: {ndarray, sparse matrix} of shape {n_samples_X, n_features}\n            Input data of training set.\n    y_train: ndarray of shape {n_samples_X,}\n            Input classification labels of training set.\n    X_test: {ndarray, sparse matrix} of shape {n_samples_X, n_features}\n            Input data of test set.\n    y_test: ndarray of shape {n_samples_X,}\n            Input classification labels of test set.\n    num_features: integer\n            The total number of features to select.\n    num_samples: integer\n            The number of samples to use in each iteration (for each feature).\n    init_features: integer, default=1\n            The number of features to select in the first iteration.\n    init_samples: integer, default=None\n            The number of samples to use in the first iteration.\n    balance: bool, default=False\n            Balance samples of each classes when sampling misclassified samples at each \n            iteration or randomly sample misclassified samples.\n    tol: list of float, default=[1e-4]\n            Tolerance for stopping criteria for each SVC. \n            The list of tolerance for gridSearch cross validation. \n    C: list of float, default=[1.0]\n            Regularization parameter for each SVC. The strength of the regularization \n            is inversely proportional to C. Must be strictly positive.\n            The list of C for gridSearch cross validation. \n    n_splits: integer, default=5\n            N-fold for gridSearch cross validation. \n    penalty: {‘l1’, ‘l2’}, default=’l2’\n            Specifies the norm used in the penalization. The ‘l2’ penalty is the \n            standard used in SVC. The ‘l1’ leads to sparse weight.\n    loss: {‘hinge’, ‘squared_hinge’}, default=’squared_hinge’\n            Specifies the loss function for each SVC to train. ‘hinge’ is the standard \n            SVM loss while ‘squared_hinge’ is the square of the hinge loss. \n            The combination of penalty='l1' and loss='hinge' is not supported.\n    dual: bool, default=True\n            Select the algorithm to either solve the dual or primal optimization \n            problem for each SVC. Prefer dual=False when n_samples > n_features.\n    fit_intercept: bool, default=True\n            Whether to calculate the intercept for each SVC. If set to false, no \n            intercept will be used in calculations (i.e. data is already centered).\n    intercept_scaling: float, default=1\n            When self.fit_intercept is True, instance vector x becomes \n            [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value \n            equals to intercept_scaling is appended to the instance vector. The intercept \n            becomes intercept_scaling * synthetic feature weight Note! the synthetic \n            feature weight is subject to l1/l2 regularization as all other features. \n            To lessen the effect of regularization on synthetic feature weight \n            (and therefore on the intercept) intercept_scaling has to be increased.\n    class_weight: dict or ‘balanced’, default=None\n            Set the parameter C of class i to class_weight[i]*C for SVC. If not given, \n            all classes are supposed to have weight one. The “balanced” mode uses the \n            values of y to automatically adjust weights inversely proportional to class \n            frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n    random_state: int, RandomState instance or None, default=None\n            Controls the pseudo random number generation for shuffling data for the dual \n            coordinate descent (if dual=True). When dual=False underlying implementation \n            of LinearSVC is not random and random_state has no effect on the results. \n            Pass an int for reproducible output across multiple function calls.\n    max_iter: int, default=1000\n            The maximum number of iterations to be run for each SVC.\n\n\n### Return\n    feature_selected: list of integer\n            The sequence of features selected.\n    num_samples_list: list of integer\n            The number of unique samples acquired totally after every features are selected.\n    train_errors: list of float\n            Mean squared error of training set after every features are selected.\n    test_errors: list of float\n            Mean squared error of test set after every features are selected.\n    train_accuracy: list of float\n            Classification accuracy of training set after every features are selected.\n    test_accuracy: list of float\n            Classification accuracy of test set after every features are selected.\n    Paras: list of dictionary\n            The best estimator parameters of every SVMs trained. \n    step_times: list of float\n            The run time of each iteration. Unit is second. \n\n## min_acquisition_cv\n### Parameters\n    X_train: {ndarray, sparse matrix} of shape {n_samples_X, n_features}\n            Input data of training set.\n    y_train: ndarray of shape {n_samples_X,}\n            Input classification labels of training set.\n    X_test: {ndarray, sparse matrix} of shape {n_samples_X, n_features}\n            Input data of test set.\n    y_test: ndarray of shape {n_samples_X,}\n            Input classification labels of test set.\n    num_features: integer\n            The total number of features to select.\n    num_samples: integer\n            The number of misclassified samples randomly sampled, which are taken union with \n            samples already acquired before. The union of samples are used for next ietration.\n    init_features: integer, default=1\n            The number of features to select in the first iteration.\n    init_samples: integer, default=None\n            The number of samples to use in the first iteration.\n    balance: bool, default=False\n            Balance samples of each classes when sampling misclassified samples at each \n            iteration or randomly sample misclassified samples.\n    tol: list of float, default=[1e-4]\n            Tolerance for stopping criteria for each SVC. \n            The list of tolerance for gridSearch cross validation. \n    C: list of float, default=[1.0]\n            Regularization parameter for each SVC. The strength of the regularization \n            is inversely proportional to C. Must be strictly positive.\n            The list of C for gridSearch cross validation. \n    n_splits: integer, default=5\n            N-fold for gridSearch cross validation. \n    penalty: {‘l1’, ‘l2’}, default=’l2’\n            Specifies the norm used in the penalization. The ‘l2’ penalty is the \n            standard used in SVC. The ‘l1’ leads to sparse weight.\n    loss: {‘hinge’, ‘squared_hinge’}, default=’squared_hinge’\n            Specifies the loss function for each SVC to train. ‘hinge’ is the standard \n            SVM loss while ‘squared_hinge’ is the square of the hinge loss. \n            The combination of penalty='l1' and loss='hinge' is not supported.\n    dual: bool, default=True\n            Select the algorithm to either solve the dual or primal optimization \n            problem for each SVC. Prefer dual=False when n_samples > n_features.\n    fit_intercept: bool, default=True\n            Whether to calculate the intercept for each SVC. If set to false, no \n            intercept will be used in calculations (i.e. data is already centered).\n    intercept_scaling: float, default=1\n            When self.fit_intercept is True, instance vector x becomes \n            [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value \n            equals to intercept_scaling is appended to the instance vector. The intercept \n            becomes intercept_scaling * synthetic feature weight Note! the synthetic \n            feature weight is subject to l1/l2 regularization as all other features. \n            To lessen the effect of regularization on synthetic feature weight \n            (and therefore on the intercept) intercept_scaling has to be increased.\n    class_weight: dict or ‘balanced’, default=None\n            Set the parameter C of class i to class_weight[i]*C for SVC. If not given, \n            all classes are supposed to have weight one. The “balanced” mode uses the \n            values of y to automatically adjust weights inversely proportional to class \n            frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n    random_state: int, RandomState instance or None, default=None\n            Controls the pseudo random number generation for shuffling data for the dual \n            coordinate descent (if dual=True). When dual=False underlying implementation \n            of LinearSVC is not random and random_state has no effect on the results. \n            Pass an int for reproducible output across multiple function calls.\n    max_iter: int, default=1000\n            The maximum number of iterations to be run for each SVC.\n\n### Return\n    feature_selected: list of integer\n            The sequence of features selected.\n    num_samples_list: list of integer\n            The number of unique samples acquired totally after every features are selected.\n    samples_global: list of integer\n            The indices of samples that are acquired.\n    train_errors: list of float\n            Mean squared error of training set after every features are selected.\n    test_errors: list of float\n            Mean squared error of test set after every features are selected.\n    train_accuracy: list of float\n            Classification accuracy of training set after every features are selected.\n    test_accuracy: list of float\n            Classification accuracy of test set after every features are selected.\n    Paras: list of dictionary\n            The best estimator parameters of every SVMs trained. \n    step_times: list of float\n            The run time of each iteration. Unit is second. \n\n\n## min_complexity_h5py\n### Parameters\n    data_cell: {h5py._hl.dataset.Dataset} of {scipy.sparse.csc matrix} \n            with shape { n_features, n_samples}.\n            The h5py data as csc matrix.\n    indices_cell: {h5py._hl.dataset.Dataset}\n            The indices of csc matrix.\n    indptr_cell: {h5py._hl.dataset.Dataset}\n            The indptr of csc matrix.\n    data_gene: {h5py._hl.dataset.Dataset} of {scipy.sparse.csr matrix} \n            with shape {n_samples, n_features}.\n            The h5py data as csr matrix.\n    indices_cell: {h5py._hl.dataset.Dataset}\n            The indices of csr matrix.\n    indptr_cell: {h5py._hl.dataset.Dataset}\n            The indptr of csr matrix.\n    y: ndarray of shape {n_samples,}\n            Input classification labels of entire dataset.\n    shape: {h5py._hl.dataset.Dataset}\n            The shape of { n_features, n_samples}.\n    idx_train: list of integer\n            The indices of samples from entire dataset to produce training set.\n    idx_test: list of integer\n            The indices of samples from entire dataset to produce test set.\n    num_features: integer\n            The total number of features to select.\n    num_samples: integer\n            The number of samples to use in each iteration (for each feature).\n    init_features: integer, default=1\n            The number of features to select in the first iteration.\n    init_samples: integer, default=None\n            The number of samples to use in the first iteration.\n    balance: bool, default=False\n            Balance samples of each classes when sampling misclassified samples at each \n            iteration or randomly sample misclassified samples.\n    penalty: {‘l1’, ‘l2’}, default=’l2’\n            Specifies the norm used in the penalization. The ‘l2’ penalty is the \n            standard used in SVC. The ‘l1’ leads to sparse weight.\n    loss: {‘hinge’, ‘squared_hinge’}, default=’squared_hinge’\n            Specifies the loss function for each SVC to train. ‘hinge’ is the standard \n            SVM loss while ‘squared_hinge’ is the square of the hinge loss. \n            The combination of penalty='l1' and loss='hinge' is not supported.\n    dual: bool, default=True\n            Select the algorithm to either solve the dual or primal optimization \n            problem for each SVC. Prefer dual=False when n_samples > n_features.\n    tol: float, default=1e-4\n            Tolerance for stopping criteria for each SVC.\n    C: float, default=1.0\n            Regularization parameter for each SVC. The strength of the regularization \n            is inversely proportional to C. Must be strictly positive.\n    fit_intercept: bool, default=True\n            Whether to calculate the intercept for each SVC. If set to false, no \n            intercept will be used in calculations (i.e. data is already centered).\n    intercept_scaling: float, default=1\n            When self.fit_intercept is True, instance vector x becomes \n            [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value \n            equals to intercept_scaling is appended to the instance vector. The intercept \n            becomes intercept_scaling * synthetic feature weight Note! the synthetic \n            feature weight is subject to l1/l2 regularization as all other features. \n            To lessen the effect of regularization on synthetic feature weight \n            (and therefore on the intercept) intercept_scaling has to be increased.\n    class_weight: dict or ‘balanced’, default=None\n            Set the parameter C of class i to class_weight[i]*C for SVC. If not given, \n            all classes are supposed to have weight one. The “balanced” mode uses the \n            values of y to automatically adjust weights inversely proportional to class \n            frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n    random_state: int, RandomState instance or None, default=None\n            Controls the pseudo random number generation for shuffling data for the dual \n            coordinate descent (if dual=True). When dual=False underlying implementation \n            of LinearSVC is not random and random_state has no effect on the results. \n            Pass an int for reproducible output across multiple function calls.\n    max_iter: int, default=1000\n            The maximum number of iterations to be run for each SVC.\n\n\n### Return\n    feature_selected: list of integer\n            The sequence of features selected.\n    num_samples_list: list of integer\n            The number of unique samples acquired totally after every features are selected.\n    train_errors: list of float\n            Mean squared error of training set after every features are selected.\n    test_errors: list of float\n            Mean squared error of test set after every features are selected.\n    train_accuracy: list of float\n            Classification accuracy of training set after every features are selected.\n    test_accuracy: list of float\n            Classification accuracy of test set after every features are selected.\n    step_times: list of float\n            The run time of each iteration. Unit is second. \n\n## min_acquisition\n### Parameters\n    data_cell: {h5py._hl.dataset.Dataset} of {scipy.sparse.csc matrix} \n            with shape { n_features, n_samples}.\n            The h5py data as csc matrix.\n    indices_cell: {h5py._hl.dataset.Dataset}\n            The indices of csc matrix.\n    indptr_cell: {h5py._hl.dataset.Dataset}\n            The indptr of csc matrix.\n    data_gene: {h5py._hl.dataset.Dataset} of {scipy.sparse.csr matrix} \n            with shape {n_samples, n_features}.\n            The h5py data as csr matrix.\n    indices_cell: {h5py._hl.dataset.Dataset}\n            The indices of csr matrix.\n    indptr_cell: {h5py._hl.dataset.Dataset}\n            The indptr of csr matrix.\n    y: ndarray of shape {n_samples,}\n            Input classification labels of entire dataset.\n    shape: {h5py._hl.dataset.Dataset}\n            The shape of { n_features, n_samples}.\n    idx_train: list of integer\n            The indices of samples from entire dataset to produce training set.\n    idx_test: list of integer\n            The indices of samples from entire dataset to produce test set.\n    num_features: integer\n            The total number of features to select.\n    num_samples: integer\n            The number of misclassified samples randomly sampled, which are taken union with \n            samples already acquired before. The union of samples are used for next ietration.\n    init_features: integer, default=1\n            The number of features to select in the first iteration.\n    init_samples: integer, default=None\n            The number of samples to use in the first iteration.\n    balance: bool, default=False\n            Balance samples of each classes when sampling misclassified samples at each \n            iteration or randomly sample misclassified samples.\n    penalty: {‘l1’, ‘l2’}, default=’l2’\n            Specifies the norm used in the penalization. The ‘l2’ penalty is the \n            standard used in SVC. The ‘l1’ leads to sparse weight.\n    loss: {‘hinge’, ‘squared_hinge’}, default=’squared_hinge’\n            Specifies the loss function for each SVC to train. ‘hinge’ is the standard \n            SVM loss while ‘squared_hinge’ is the square of the hinge loss. \n            The combination of penalty='l1' and loss='hinge' is not supported.\n    dual: bool, default=True\n            Select the algorithm to either solve the dual or primal optimization \n            problem for each SVC. Prefer dual=False when n_samples > n_features.\n    tol: float, default=1e-4\n            Tolerance for stopping criteria for each SVC.\n    C: float, default=1.0\n            Regularization parameter for each SVC. The strength of the regularization \n            is inversely proportional to C. Must be strictly positive.\n    fit_intercept: bool, default=True\n            Whether to calculate the intercept for each SVC. If set to false, no \n            intercept will be used in calculations (i.e. data is already centered).\n    intercept_scaling: float, default=1\n            When self.fit_intercept is True, instance vector x becomes \n            [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value \n            equals to intercept_scaling is appended to the instance vector. The intercept \n            becomes intercept_scaling * synthetic feature weight Note! the synthetic \n            feature weight is subject to l1/l2 regularization as all other features. \n            To lessen the effect of regularization on synthetic feature weight \n            (and therefore on the intercept) intercept_scaling has to be increased.\n    class_weight: dict or ‘balanced’, default=None\n            Set the parameter C of class i to class_weight[i]*C for SVC. If not given, \n            all classes are supposed to have weight one. The “balanced” mode uses the \n            values of y to automatically adjust weights inversely proportional to class \n            frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\n    random_state: int, RandomState instance or None, default=None\n            Controls the pseudo random number generation for shuffling data for the dual \n            coordinate descent (if dual=True). When dual=False underlying implementation \n            of LinearSVC is not random and random_state has no effect on the results. \n            Pass an int for reproducible output across multiple function calls.\n    max_iter: int, default=1000\n            The maximum number of iterations to be run for each SVC.\n\n### Return\n    feature_selected: list of integer\n            The sequence of features selected.\n    num_samples_list: list of integer\n            The number of unique samples acquired totally after every features are selected.\n    samples_global: list of integer\n            The indices of samples that are acquired.\n    train_errors: list of float\n            Mean squared error of training set after every features are selected.\n    test_errors: list of float\n            Mean squared error of test set after every features are selected.\n    train_accuracy: list of float\n            Classification accuracy of training set after every features are selected.\n    test_accuracy: list of float\n            Classification accuracy of test set after every features are selected.\n    step_times: list of float\n            The run time of each iteration. Unit is second. \n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/xqchen/activeSVC",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "activeSVC",
    "package_url": "https://pypi.org/project/activeSVC/",
    "platform": "",
    "project_url": "https://pypi.org/project/activeSVC/",
    "project_urls": {
      "Homepage": "https://github.com/xqchen/activeSVC"
    },
    "release_url": "https://pypi.org/project/activeSVC/4.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.1",
    "summary": "Active feature selection method with support vector classifier.",
    "version": "4.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12816427,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f65ac1914c2cb13cf5e40e647d34efca589787fbc5a6a4f9ee9528caa8ef925b",
          "md5": "a892269ec65c99315c59eceb1b1100ec",
          "sha256": "33065cfdbac5028021069d23cc33a5a51366607bebb29f97b22390560f6c5e5e"
        },
        "downloads": -1,
        "filename": "activeSVC-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a892269ec65c99315c59eceb1b1100ec",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=2.7",
        "size": 16667,
        "upload_time": "2021-08-13T21:46:52",
        "upload_time_iso_8601": "2021-08-13T21:46:52.231538Z",
        "url": "https://files.pythonhosted.org/packages/f6/5a/c1914c2cb13cf5e40e647d34efca589787fbc5a6a4f9ee9528caa8ef925b/activeSVC-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9f947309e40a9fdfcfb96ad4821c146f4b5e012b2c5208dfc1b599a4092fe775",
          "md5": "9ed3edd2686cfa7929139afdaa0bc9c8",
          "sha256": "45e09765046210c6c44a1e1fc591ebf787f08e1ac97cbf1ea2468b74d47b15db"
        },
        "downloads": -1,
        "filename": "activeSVC-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9ed3edd2686cfa7929139afdaa0bc9c8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 16227,
        "upload_time": "2021-08-13T21:46:54",
        "upload_time_iso_8601": "2021-08-13T21:46:54.676916Z",
        "url": "https://files.pythonhosted.org/packages/9f/94/7309e40a9fdfcfb96ad4821c146f4b5e012b2c5208dfc1b599a4092fe775/activeSVC-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7d4ca2f4012690c4b476abf63e403f00f1d85fec241993f028f30e027c2be446",
          "md5": "bda82b8db2d1726026920685e2950d23",
          "sha256": "7a490c2b32848268901917b7f8c84b0a7dc20ed1d52c647f91dcd4a16f4f199f"
        },
        "downloads": -1,
        "filename": "activeSVC-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bda82b8db2d1726026920685e2950d23",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=2.7",
        "size": 16780,
        "upload_time": "2021-08-18T06:19:02",
        "upload_time_iso_8601": "2021-08-18T06:19:02.499035Z",
        "url": "https://files.pythonhosted.org/packages/7d/4c/a2f4012690c4b476abf63e403f00f1d85fec241993f028f30e027c2be446/activeSVC-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b905818747d93a9d7e42531b453969f3a5ac8b6542975f81980533c2b7599223",
          "md5": "a7fe92c70477c03071b0822547c32b9b",
          "sha256": "141605ab241ca1290b8a2aff449013cc5300317e895b60d421869fe2652809cd"
        },
        "downloads": -1,
        "filename": "activeSVC-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a7fe92c70477c03071b0822547c32b9b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 16395,
        "upload_time": "2021-08-18T06:19:05",
        "upload_time_iso_8601": "2021-08-18T06:19:05.013700Z",
        "url": "https://files.pythonhosted.org/packages/b9/05/818747d93a9d7e42531b453969f3a5ac8b6542975f81980533c2b7599223/activeSVC-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ce68d4074f2af7c1d811f44e5df5a2e8f5669cd5735135f974e4839556f19b58",
          "md5": "170016b65969f6a99643fdeec393a48a",
          "sha256": "62c014aad29dfcd3e2da17480234e3260fd0af63f37d14efb222f37c900f479c"
        },
        "downloads": -1,
        "filename": "activeSVC-2.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "170016b65969f6a99643fdeec393a48a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=2.7",
        "size": 16836,
        "upload_time": "2021-10-03T10:48:33",
        "upload_time_iso_8601": "2021-10-03T10:48:33.674926Z",
        "url": "https://files.pythonhosted.org/packages/ce/68/d4074f2af7c1d811f44e5df5a2e8f5669cd5735135f974e4839556f19b58/activeSVC-2.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6711e93275b5173869c4e50b8c8b1b1f79d51b4b254da0a0059ee1470400bd30",
          "md5": "757040a9d1087c53c05ff8a056c3e6ea",
          "sha256": "a92e1de8a45d8cb2ffa69d8aef407db783eb8668316cdb5f6df07d35cdf81fbd"
        },
        "downloads": -1,
        "filename": "activeSVC-2.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "757040a9d1087c53c05ff8a056c3e6ea",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 16463,
        "upload_time": "2021-10-03T10:48:34",
        "upload_time_iso_8601": "2021-10-03T10:48:34.781959Z",
        "url": "https://files.pythonhosted.org/packages/67/11/e93275b5173869c4e50b8c8b1b1f79d51b4b254da0a0059ee1470400bd30/activeSVC-2.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "3.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ad78aa597c0e83c99a15832c519b3549964a8b2404f14f3e6f74f7507389662e",
          "md5": "845fe48a80f0ae25371d0000850e4ff0",
          "sha256": "261c80fd9f1fc121399c62a2fedf1c2441590b415d75370a377ce11f43cc4fb5"
        },
        "downloads": -1,
        "filename": "activeSVC-3.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "845fe48a80f0ae25371d0000850e4ff0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=2.7",
        "size": 18167,
        "upload_time": "2021-10-14T19:34:32",
        "upload_time_iso_8601": "2021-10-14T19:34:32.837432Z",
        "url": "https://files.pythonhosted.org/packages/ad/78/aa597c0e83c99a15832c519b3549964a8b2404f14f3e6f74f7507389662e/activeSVC-3.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "10127c3ca50fc13d005a0d9ab93cbb2f14cc09feea2a8924ac6b845cdade643a",
          "md5": "0e8352b854ca8dee2cabd7223459ee55",
          "sha256": "218ccd46950eaf49913db4764f81746886b521fff891d6df61cc7032855e0cca"
        },
        "downloads": -1,
        "filename": "activeSVC-3.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0e8352b854ca8dee2cabd7223459ee55",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 18112,
        "upload_time": "2021-10-14T19:34:35",
        "upload_time_iso_8601": "2021-10-14T19:34:35.460281Z",
        "url": "https://files.pythonhosted.org/packages/10/12/7c3ca50fc13d005a0d9ab93cbb2f14cc09feea2a8924ac6b845cdade643a/activeSVC-3.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "4.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a6829e3296fe2410cb3e17c66c0628b7ac45726f0c4898accda9100108c40e89",
          "md5": "400cde5e03c7ca852e77a226bebdb85c",
          "sha256": "ae2ce9c73feb85cd563271fdc68f79f3c2650ce27a829578a3134afb03e78088"
        },
        "downloads": -1,
        "filename": "activeSVC-4.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "400cde5e03c7ca852e77a226bebdb85c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.1",
        "size": 20538,
        "upload_time": "2022-02-07T16:52:46",
        "upload_time_iso_8601": "2022-02-07T16:52:46.973682Z",
        "url": "https://files.pythonhosted.org/packages/a6/82/9e3296fe2410cb3e17c66c0628b7ac45726f0c4898accda9100108c40e89/activeSVC-4.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "99f8fb886694c3a88cda67f3caff86a71f97d647a23ec615417434a2664af6b9",
          "md5": "9bca40634e50e9c21d3b277fa6011e0f",
          "sha256": "ae36d227827c253c02d6c52b463ead8400493306ef918893b3abc4477ee8c5c5"
        },
        "downloads": -1,
        "filename": "activeSVC-4.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9bca40634e50e9c21d3b277fa6011e0f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.1",
        "size": 24434,
        "upload_time": "2022-02-07T16:52:48",
        "upload_time_iso_8601": "2022-02-07T16:52:48.658296Z",
        "url": "https://files.pythonhosted.org/packages/99/f8/fb886694c3a88cda67f3caff86a71f97d647a23ec615417434a2664af6b9/activeSVC-4.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a6829e3296fe2410cb3e17c66c0628b7ac45726f0c4898accda9100108c40e89",
        "md5": "400cde5e03c7ca852e77a226bebdb85c",
        "sha256": "ae2ce9c73feb85cd563271fdc68f79f3c2650ce27a829578a3134afb03e78088"
      },
      "downloads": -1,
      "filename": "activeSVC-4.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "400cde5e03c7ca852e77a226bebdb85c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.1",
      "size": 20538,
      "upload_time": "2022-02-07T16:52:46",
      "upload_time_iso_8601": "2022-02-07T16:52:46.973682Z",
      "url": "https://files.pythonhosted.org/packages/a6/82/9e3296fe2410cb3e17c66c0628b7ac45726f0c4898accda9100108c40e89/activeSVC-4.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "99f8fb886694c3a88cda67f3caff86a71f97d647a23ec615417434a2664af6b9",
        "md5": "9bca40634e50e9c21d3b277fa6011e0f",
        "sha256": "ae36d227827c253c02d6c52b463ead8400493306ef918893b3abc4477ee8c5c5"
      },
      "downloads": -1,
      "filename": "activeSVC-4.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "9bca40634e50e9c21d3b277fa6011e0f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.1",
      "size": 24434,
      "upload_time": "2022-02-07T16:52:48",
      "upload_time_iso_8601": "2022-02-07T16:52:48.658296Z",
      "url": "https://files.pythonhosted.org/packages/99/f8/fb886694c3a88cda67f3caff86a71f97d647a23ec615417434a2664af6b9/activeSVC-4.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}