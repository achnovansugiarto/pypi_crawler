{
  "info": {
    "author": "densechen",
    "author_email": "densechen@foxmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3 :: Only"
    ],
    "description": "# AReLU: Attention-based-Rectified-Linear-Unit\n\nActivation function player with PyTorch on supervised/transfer/meta learning.\n\n![teaser](pictures/teaser.png)\n\n## Content\n\n* [Introduction](#1)\n* [Install](#2)\n* [Run](#3)\n* [Explore](#4)\n* [Meta Learning](#5)\n\n##  <h2 id=\"1\">1. Introduction</h2>\n\nThis repository is the implementation of paper [AReLU: Attention-based-Rectified-Linear-Unit](https://arxiv.org/pdf/2006.13858.pdf).\n\n**This project is friendly to newcomers of PyTorch.** You can design and compare different activation functions under different learning tasks.\n\n\n## <h2 id=\"2\">2. Install</h2>\n\n### Install activations as package\n\n```shell\npip install activations\n```\n\n```shell\nconda create -n AFP python=3.7 -y\nconda activate AFP\npip install -r requirements.txt\n```\n\n**NOTE**: PAU is only CUDA supported. You have to compile it first:\n\n```shell\npip install airspeed==0.5.14 \n\ncd activations/pau/cuda\npython setup.py install\n```\n\nThe code of PAU is directly token from [PAU](https://github.com/ml-research/pau.git), if you occur any problems while compiling, please refer to the original repository.\n\nIf you just want to have a quick start, and do not want to compile with PAU, just comment out the following lines in [activations/\\_\\_init\\_\\_.py](https://github.com/densechen/AReLU/blob/6735a82da2caf68f346551fe2abb26f3bd0ccdd2/activations/__init__.py#L14):\n\n```python\ntry:\n    from .pau.utils import PAU\n    __class_dict__[\"PAU\"] = PAU\nexcept Exception:\n    raise NotImplementedError(\"\")\n```\n\n## <h2 id=\"3\">3. Run</h2>\n\n```shell\npython -m visdom.server & # start visdom\npython main.py # run with default parameters\n```\n\nClick [here](https://localhost:8097/) to check your training process.\n\n**NOTE**: The program will download and save dataset under args.data_root automatically.\n\n**Run with specified parameters**\n\n```shell\npython main_mnist.py -h\n    usage: main_mnist.py [-h] [--batch_size BATCH_SIZE] [--lr LR] [--lr_aux LR_AUX]\n                [--epochs EPOCHS] [--epochs_aux EPOCHS_AUX] [--times TIMES]\n                [--data_root DATA_ROOT]\n                [--dataset {MNIST,SVHN,EMNIST,KMNIST,QMNIST,FashionMNIST}]\n                [--dataset_aux {MNIST,SVHN,EMNIST,KMNIST,QMNIST,FashionMNIST}]\n                [--num_workers NUM_WORKERS]\n                [--net {BaseModel,ConvMNIST,LinearMNIST}] [--resume RESUME]\n                [--af {APL,AReLU,GELU,Maxout,Mixture,SLAF,Swish,ReLU,ReLU6,Sigmoid,LeakyReLU,ELU,PReLU,SELU,Tanh,RReLU,CELU,Softplus,PAU,all}]\n                [--optim {SGD,Adam}] [--cpu] [--exname {AFS,TransferLearning}]\n                [--silent]\n\n    Activation Function Player with PyTorch.\n\n    optional arguments:\n    -h, --help            show this help message and exit\n    --batch_size BATCH_SIZE\n                            batch size for training\n    --lr LR               learning rate\n    --lr_aux LR_AUX       learning rate of finetune. only used while transfer\n                            learning.\n    --epochs EPOCHS       training epochs\n    --epochs_aux EPOCHS_AUX\n                            training epochs. only used while transfer learning.\n    --times TIMES         repeat runing times\n    --data_root DATA_ROOT\n                            the path to dataset\n    --dataset {MNIST,SVHN,EMNIST,KMNIST,QMNIST,FashionMNIST}\n                            the dataset to play with.\n    --dataset_aux {MNIST,SVHN,EMNIST,KMNIST,QMNIST,FashionMNIST}\n                            the dataset to play with. only used while transfer\n                            learning.\n    --num_workers NUM_WORKERS\n                            number of workers to load data\n    --net {BaseModel,ConvMNIST,LinearMNIST}\n                            network architecture for experiments. you can add new\n                            models in ./models.\n    --resume RESUME       pretrained path to resume\n    --af {APL,AReLU,GELU,Maxout,Mixture,SLAF,Swish,ReLU,ReLU6,Sigmoid,LeakyReLU,ELU,PReLU,SELU,Tanh,RReLU,CELU,Softplus,PAU,all}\n                            the activation function used in experiments. you can\n                            specify an activation function by name, or try with\n                            all activation functions by `all`\n    --optim {SGD,Adam}    optimizer used in training.\n    --cpu                 with cuda training. this would be much faster.\n    --exname {AFS,TransferLearning}\n                            experiment name of visdom.\n    --silent              if True, shut down the visdom visualizer.\n```\n\n**Full Experiment**\n\n```shell\nnohup ./main_mnist.sh > main_mnist.log &\n```\n\n![result](pictures/result.png)\n\n## <h2 id=\"4\">4. Explore</h2>\n\n**New activation functions**\n\n1. Write a python script file under `activations`, such as *new_activation_functions.py*, where contains the implementation of new activation function.\n\n2. Import new activation functions in [activations/\\_\\_init\\_\\_.py](activations/__init__.py).\n\n**New network structure**\n\n1. Write a python script file under `models`, such as *new_network_structure.py*, where contains the definition of new network structure. New defined network structure should be a subclass of **BaseModel**, which defined in `models/models.py`.\n\n2. Import new network structure in [models/\\_\\_init\\_\\_.py](models/__init__.py).\n\n**NOTE**: New activation functions and network sctructures will be automatically added into argparse. So, it is not necessary to modify `main.py`.\n\n## <h2 id=\"5\">5. Meta Learning</h2>\n\n### Setup\n```shell\npip install learn2learn\n```\n\n### Run\n```shell\npython meta_mnist.py --help\n    usage: meta_mnist.py [-h] [--ways N] [--shots N] [-tps N] [-fas N]\n                         [--iterations N] [--lr LR] [--maml-lr LR] [--no-cuda]\n                         [--seed S] [--download-location S]\n                         [--afs {APL,AReLU,GELU,Maxout,Mixture,SLAF,Swish,ReLU,ReLU6,Sigmoid,LeakyReLU,ELU,PReLU,SELU,Tanh,RReLU,CELU,Softplus,PAU}]\n\n    Learn2Learn MNIST Example\n\n    optional arguments:\n      -h, --help            show this help message and exit\n      --ways N              number of ways (default: 5)\n      --shots N             number of shots (default: 1)\n      -tps N, --tasks-per-step N\n                            tasks per step (default: 32)\n      -fas N, --fast-adaption-steps N\n                            steps per fast adaption (default: 5)\n      --iterations N        number of iterations (default: 1000)\n      --lr LR               learning rate (default: 0.005)\n      --maml-lr LR          learning rate for MAML (default: 0.01)\n      --no-cuda             disables CUDA training\n      --seed S              random seed (default: 1)\n      --download-location S\n                            download location for train data (default : data\n      --afs {APL,AReLU,GELU,Maxout,Mixture,SLAF,Swish,ReLU,ReLU6,Sigmoid,LeakyReLU,ELU,PReLU,SELU,Tanh,RReLU,CELU,Softplus,PAU}\n                            activation function used to meta learning.\n```\n\n### Run all\n```\nnohup ./meta_mnist.sh > meta_mnist.log &\n```\n\n## Citation\nIf you use this code, please cite the following paper:\n```\n@misc{AReLU,\nAuthor = {Dengsheng Chen and Kai Xu},\nTitle = {AReLU: Attention-based Rectified Linear Unit},\nYear = {2020},\nEprint = {arXiv:2006.13858},\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/densechen/AReLU/archive/master.zip",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/densechen/AReLU",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "activations",
    "package_url": "https://pypi.org/project/activations/",
    "platform": "",
    "project_url": "https://pypi.org/project/activations/",
    "project_urls": {
      "Download": "https://github.com/densechen/AReLU/archive/master.zip",
      "Homepage": "https://github.com/densechen/AReLU"
    },
    "release_url": "https://pypi.org/project/activations/0.1.0/",
    "requires_dist": [
      "torch"
    ],
    "requires_python": ">=3.6",
    "summary": "activations: a package contains different kinds of activation functions",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9210375,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9db10a24cfd869542c98c8b01ef7934186635c79b3d9df9aa01ba48f33b89ccb",
          "md5": "c44c74abcf7e48261042514bed0a8bc5",
          "sha256": "56414abcf254b8da5b3a52fb5728083a4b8a476d6276f9e74cb2ea444f97c468"
        },
        "downloads": -1,
        "filename": "activations-0.1.0-py3.8.egg",
        "has_sig": false,
        "md5_digest": "c44c74abcf7e48261042514bed0a8bc5",
        "packagetype": "bdist_egg",
        "python_version": "3.8",
        "requires_python": ">=3.6",
        "size": 34681,
        "upload_time": "2021-01-23T12:13:50",
        "upload_time_iso_8601": "2021-01-23T12:13:50.453281Z",
        "url": "https://files.pythonhosted.org/packages/9d/b1/0a24cfd869542c98c8b01ef7934186635c79b3d9df9aa01ba48f33b89ccb/activations-0.1.0-py3.8.egg",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b298bcbe2a66048e78dc1a16bf3d77510ef3f11f7a0475670ec5f05248e40549",
          "md5": "d474b10f66bbb6156ac9d9bc34b870e0",
          "sha256": "0e7b35a2b5b17e432d5c08a0813e3a8182bb9b25934d4dcefb3624ca8e19a847"
        },
        "downloads": -1,
        "filename": "activations-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d474b10f66bbb6156ac9d9bc34b870e0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 16220,
        "upload_time": "2021-01-23T12:13:48",
        "upload_time_iso_8601": "2021-01-23T12:13:48.261878Z",
        "url": "https://files.pythonhosted.org/packages/b2/98/bcbe2a66048e78dc1a16bf3d77510ef3f11f7a0475670ec5f05248e40549/activations-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9a0349ab919fb102931417418d1a95de5fc6ce942f991add7a5190e704885bbb",
          "md5": "45adb6bd5ecb78f9384d96ac1877bca7",
          "sha256": "a40a472597b206bc810e725b8bbe3a77991b8b11a94c35816508f9494d22fa81"
        },
        "downloads": -1,
        "filename": "activations-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "45adb6bd5ecb78f9384d96ac1877bca7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 14630,
        "upload_time": "2021-01-23T12:13:52",
        "upload_time_iso_8601": "2021-01-23T12:13:52.154383Z",
        "url": "https://files.pythonhosted.org/packages/9a/03/49ab919fb102931417418d1a95de5fc6ce942f991add7a5190e704885bbb/activations-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9db10a24cfd869542c98c8b01ef7934186635c79b3d9df9aa01ba48f33b89ccb",
        "md5": "c44c74abcf7e48261042514bed0a8bc5",
        "sha256": "56414abcf254b8da5b3a52fb5728083a4b8a476d6276f9e74cb2ea444f97c468"
      },
      "downloads": -1,
      "filename": "activations-0.1.0-py3.8.egg",
      "has_sig": false,
      "md5_digest": "c44c74abcf7e48261042514bed0a8bc5",
      "packagetype": "bdist_egg",
      "python_version": "3.8",
      "requires_python": ">=3.6",
      "size": 34681,
      "upload_time": "2021-01-23T12:13:50",
      "upload_time_iso_8601": "2021-01-23T12:13:50.453281Z",
      "url": "https://files.pythonhosted.org/packages/9d/b1/0a24cfd869542c98c8b01ef7934186635c79b3d9df9aa01ba48f33b89ccb/activations-0.1.0-py3.8.egg",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b298bcbe2a66048e78dc1a16bf3d77510ef3f11f7a0475670ec5f05248e40549",
        "md5": "d474b10f66bbb6156ac9d9bc34b870e0",
        "sha256": "0e7b35a2b5b17e432d5c08a0813e3a8182bb9b25934d4dcefb3624ca8e19a847"
      },
      "downloads": -1,
      "filename": "activations-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d474b10f66bbb6156ac9d9bc34b870e0",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 16220,
      "upload_time": "2021-01-23T12:13:48",
      "upload_time_iso_8601": "2021-01-23T12:13:48.261878Z",
      "url": "https://files.pythonhosted.org/packages/b2/98/bcbe2a66048e78dc1a16bf3d77510ef3f11f7a0475670ec5f05248e40549/activations-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9a0349ab919fb102931417418d1a95de5fc6ce942f991add7a5190e704885bbb",
        "md5": "45adb6bd5ecb78f9384d96ac1877bca7",
        "sha256": "a40a472597b206bc810e725b8bbe3a77991b8b11a94c35816508f9494d22fa81"
      },
      "downloads": -1,
      "filename": "activations-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "45adb6bd5ecb78f9384d96ac1877bca7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 14630,
      "upload_time": "2021-01-23T12:13:52",
      "upload_time_iso_8601": "2021-01-23T12:13:52.154383Z",
      "url": "https://files.pythonhosted.org/packages/9a/03/49ab919fb102931417418d1a95de5fc6ce942f991add7a5190e704885bbb/activations-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}