{
  "info": {
    "author": "Treasure Data",
    "author_email": "dev+pypi@treasure-data.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Getting Started: td-pyspark\n\n[Treasure Data](https://treasuredata.com) extension for using [pyspark](https://spark.apache.org/docs/latest/api/python/index.html).\n\n## Installation\n\nYou can install from PyPI by using `pip` as follows:\n\n```sh\n# For Spark 2.4.x\n$ pip install td-pyspark\n\n# For Spark 3.0.x\n$ pip install td-pyspark-ea\n```\n\nIf you want to install PySpark via PyPI, you can install as:\n\n```sh\n$ pip install td-pyspark[spark]\n```\n\n## Introduction\n\nFirst contact [support@treasure-data.com](mailto:support@treasure-data.com) to enable td-spark feature. This feature is disabled by default.\n\ntd-pyspark is a library to enable Python to access tables in Treasure Data.\nThe features of td_pyspark include:\n\n- Reading tables in Treasure Data as DataFrame\n- Writing DataFrames to Treasure Data\n- Submitting Presto queries and read the query results as DataFrames\n\nFor more details, see also [td-spark FAQs](https://tddocs.atlassian.net/wiki/spaces/PD/pages/1082513/Apache+Spark+Driver+td-spark+FAQs).\n\n### Quick Start with Docker\n\nYou can try td_pyspark using Docker without installing Spark nor Python.\n\nFirst create __td-spark.conf__ file and set your TD API KEY and site (us, jp, eu01, ap02) configurations:\n\n__td-spark.conf__\n```\nspark.td.apikey (Your TD API KEY)\nspark.td.site (Your site: us, jp, eu01, ap02)\nspark.serializer org.apache.spark.serializer.KryoSerializer\nspark.sql.execution.arrow.enabled true\n```\n\nLaunch pyspark Docker image. This image already has a pre-installed td_pyspark library:\n\n```shell\n$ docker run -it -e TD_SPARK_CONF=td-spark.conf -v $(pwd):/opt/spark/work armtd/td-spark-pyspark:latest_spark2.4.5\nPython 3.6.9 (default, Oct 17 2019, 11:10:22) \n[GCC 8.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n20/06/03 06:16:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.5\n      /_/\n\nUsing Python version 3.6.9 (default, Oct 17 2019 11:10:22)\nSparkSession available as 'spark'.\n2020-06-03 06:16:09.338Z debug [spark] Loading com.treasuredata.spark package - (package.scala:23)\n2020-06-03 06:16:09.388Z  info [spark] td-spark version:20.6.0, revision:703654f, build_time:2020-06-03T05:22:17.574+0000 - (package.scala:24)\n2020-06-03 06:16:11.405Z  info [TDServiceConfig] td-spark site: us - (TDServiceConfig.scala:36)\n>>>\n```\n\nTry read a sample table by specifying a time range:\n\n```python\n>>> df = td.table(\"sample_datasets.www_access\").within(\"+2d/2014-10-04\").df()\n>>> df.show()\n2019-06-13 19:48:51.605Z  info [TDRelation] Fetching the partition list of sample_datasets.www_access within time range:[2014-10-04 00:00:00Z,2014-10-06 00:00:00Z) - (TDRelation.scala:170)\n2019-06-13 19:48:51.950Z  info [TDRelation] Retrieved 2 partition entries - (TDRelation.scala:176)\n+----+---------------+--------------------+--------------------+----+--------------------+----+------+----------+\n|user|           host|                path|             referer|code|               agent|size|method|      time|\n+----+---------------+--------------------+--------------------+----+--------------------+----+------+----------+\n|null|192.225.229.196|  /category/software|                   -| 200|Mozilla/5.0 (Maci...| 117|   GET|1412382292|\n|null|120.168.215.131|  /category/software|                   -| 200|Mozilla/5.0 (comp...|  53|   GET|1412382284|\n|null|180.198.173.136|/category/electro...| /category/computers| 200|Mozilla/5.0 (Wind...| 106|   GET|1412382275|\n|null| 140.168.145.49|   /item/garden/2832|      /item/toys/230| 200|Mozilla/5.0 (Maci...| 122|   GET|1412382267|\n|null|  52.168.78.222|/category/electro...|    /item/games/2532| 200|Mozilla/5.0 (comp...|  73|   GET|1412382259|\n|null|  32.42.160.165|   /category/cameras|/category/cameras...| 200|Mozilla/5.0 (Wind...| 117|   GET|1412382251|\n|null|   48.204.59.23|  /category/software|/search/?c=Electr...| 200|Mozilla/5.0 (Maci...|  52|   GET|1412382243|\n|null|136.207.150.227|/category/electro...|                   -| 200|Mozilla/5.0 (iPad...| 120|   GET|1412382234|\n|null| 204.21.174.187|   /category/jewelry|   /item/office/3462| 200|Mozilla/5.0 (Wind...|  59|   GET|1412382226|\n|null|  224.198.88.93|    /category/office|     /category/music| 200|Mozilla/4.0 (comp...|  46|   GET|1412382218|\n|null|   96.54.24.116|     /category/games|                   -| 200|Mozilla/5.0 (Wind...|  40|   GET|1412382210|\n|null| 184.42.224.210| /category/computers|                   -| 200|Mozilla/5.0 (Wind...|  95|   GET|1412382201|\n|null|  144.72.47.212|/item/giftcards/4684|    /item/books/1031| 200|Mozilla/5.0 (Wind...|  65|   GET|1412382193|\n|null| 40.213.111.170|     /item/toys/1085|   /category/cameras| 200|Mozilla/5.0 (Wind...|  65|   GET|1412382185|\n|null| 132.54.226.209|/item/electronics...|  /category/software| 200|Mozilla/5.0 (comp...| 121|   GET|1412382177|\n|null|  108.219.68.64|/category/cameras...|                   -| 200|Mozilla/5.0 (Maci...|  54|   GET|1412382168|\n|null| 168.66.149.218| /item/software/4343|  /category/software| 200|Mozilla/4.0 (comp...| 139|   GET|1412382160|\n|null|  80.66.118.103|  /category/software|                   -| 200|Mozilla/4.0 (comp...|  92|   GET|1412382152|\n|null|140.171.147.207|     /category/music|   /category/jewelry| 200|Mozilla/5.0 (Wind...| 119|   GET|1412382144|\n|null| 84.132.164.204| /item/software/4783|/category/electro...| 200|Mozilla/5.0 (Wind...| 137|   GET|1412382135|\n+----+---------------+--------------------+--------------------+----+--------------------+----+------+----------+\nonly showing top 20 rows\n>>>\n```\n\n\n## Usage\n\n_TDSparkContext_ is an entry point to access td_pyspark's functionalities. To create TDSparkContext, pass your SparkSession (spark) to TDSparkContext:\n\n```python\ntd = TDSparkContext(spark)\n```\n\n### Reading Tables as DataFrames\n\nTo read a table, use `td.table(table name)`:\n\n```python\ndf = td.table(\"sample_datasets.www_access\").df()\ndf.show()\n```\n\nTo change the context database, use `td.use(database_name)`:\n\n```python\ntd.use(\"sample_datasets\")\n# Accesses sample_datasets.www_access\ndf = td.table(\"www_access\").df()\n```\n\nBy calling `.df()` your table data will be read as Spark's DataFrame.\nThe usage of the DataFrame is the same with PySpark. See also [PySpark DataFrame documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame).\n\n#### Specifying Time Ranges\n\nTreasure Data is a time series database, so reading recent data by specifying a time range is important to reduce the amount of data to be processed.\n`.within(...)` function can be used to specify a target time range in a concise syntax. \n`within` function accepts the same syntax used in [TD_INTERVAL function in Presto](https://tddocs.atlassian.net/wiki/spaces/PD/pages/1083429/Supported+Presto+and+TD+Functions#TD_INTERVAL).\n\nFor example, to read the last 1 hour range of data, use `within(\"-1h\")`:\n\n```python\ntd.table(\"tbl\").within(\"-1h\").df()\n```\n\nYou can also read the last day's data:\n\n```python\ntd.table(\"tbl\").within(\"-1d\").df()\n```\n\nYou can also specify an _offset_ of the relative time range. This example reads the last days's data beginning from 7 days ago:\n\n```python\ntd.table(\"tbl\").within(\"-1d/-7d\").df()\n```\n\nIf you know an exact time range, `within(\"(start time)/(end time)\")` is useful:\n\n```python\n>>> df = td.table(\"sample_datasets.www_access\").within(\"2014-10-04/2014-10-05\").df()\n>>> df.show()\n2019-06-13 20:12:01.400Z  info [TDRelation] Fetching the partition list of sample_datasets.www_access within time range:[2014-10-04 00:00:00Z,2014-10-05 00:00:00Z) - (TDRelation.scala:170)\n...\n```\n\nSee [this doc](https://tddocs.atlassian.net/wiki/spaces/PD/pages/1083429/Supported+Presto+and+TD+Functions#TD_INTERVAL) for more examples of interval strings.\n\n\n#### Submitting Presto Queries\n\nIf your Spark cluster is small, reading all of the data as in-memory DataFrame might be difficult.\nIn this case, you can utilize Presto, a distributed SQL query engine, to reduce the amount of data processing with PySpark:\n\n```python\n>>> q = td.presto(\"select code, * from sample_datasets.www_access\")\n>>> q.show()\n2019-06-13 20:09:13.245Z  info [TDPrestoJDBCRDD]  - (TDPrestoRelation.scala:106)\nSubmit Presto query:\nselect code, count(*) cnt from sample_datasets.www_access group by 1\n+----+----+\n|code| cnt|\n+----+----+\n| 200|4981|\n| 500|   2|\n| 404|  17|\n+----+----+\n```\n\nThe query result is represented as a DataFrame.\n\nTo run non query statements (e.g., INSERT INTO, CREATE TABLE, etc.) use `execute_presto(sql)`:\n\n```python\ntd.execute_presto(\"CREATE TABLE IF NOT EXISTS A(time bigint, id varchar)\")\n```\n\n#### Using SparkSQL\n\nTo use tables in Treaure Data inside Spark SQL, create a view with `df.createOrReplaceTempView(...)`:\n\n```python\n# Read TD table as a DataFrame\ndf = td.table(\"mydb.test1\").df()\n# Register the DataFrame as a view\ndf.createOrReplaceTempView(\"test1\")\n\nspark.sql(\"SELECT * FROM test1\").show()\n```\n\n### Create or Drop Databases and Tables\n\nCreate a new table or database:\n\n```python\ntd.create_database_if_not_exists(\"mydb\")\ntd.create_table_if_not_exists(\"mydb.test1\")\n```\n\nDelete unnecessary tables:\n\n```python\ntd.drop_table_if_exists(\"mydb.test1\")\ntd.drop_database_if_exists(\"mydb\")\n```\n\nYou can also check the presence of a table:\n\n```python\ntd.table(\"mydb.test1\").exists() # True if the table exists\n```\n\n### Create User-Defined Partition Tables\n\nUser-defined partitioning ([UDP](https://tddocs.atlassian.net/wiki/spaces/PD/pages/1084298/Defining+Partitioning+for+Presto)) is useful if \nyou know a column in the table that has unique identifiers (e.g., IDs, category values).\n\nYou can create a UDP table partitioned by id (string type column) as follows: \n\n```python\ntd.create_udp_s(\"mydb.user_list\", \"id\")\n```\n\nTo create a UDP table, partitioned by Long (bigint) type column, use `td.create_udp_l`:\n\n```python\ntd.create_udp_l(\"mydb.departments\", \"dept_id\")\n```\n\n### Swapping Table Contents\n\nYou can replace the contents of two tables. The input tables must be in the same database:\n\n```python\n# Swap the contents of two tables\ntd.swap_tables(\"mydb.tbl1\", \"mydb.tbl2\")\n\n# Another way to swap tables\ntd.table(\"mydb.tbl1\").swap_table_with(\"tbl2\")\n```\n\n### Uploading DataFrames to Treasure Data\n\nTo save your local DataFrames as a table, `td.insert_into(df, table)` and `td.create_or_replace(df, table)` can be used:\n\n```python\n# Insert the records in the input DataFrame to the target table:\ntd.insert_into(df, \"mydb.tbl1\")\n\n# Create or replace the target table with the content of the input DataFrame:\ntd.create_or_replace(df, \"mydb.tbl2\")\n```\n\n## Using multiple TD accounts\n\nTo specify a new api key aside from the key that is configured in td-spark.conf, just use `td.with_apikey(apikey)`:\n\n```python\n# Returns a new TDSparkContext with the specified key\ntd2 = td.with_apikey(\"key2\")\n```\n\nFor reading tables or uploading DataFrames with the new key, use `td2`:\n\n```python\n# Read a table with key2\ndf = td2.table(\"sample_datasets.www_access\").df()\n...\n# Insert the records with key2\ntd2.insert_into(df, \"mydb.tbl1\")\n```\n\n### Running PySpark jobs with spark-submit\n\nTo submit your PySpark script to a Spark cluster, you will need the following files:\n\n- __td-spark.conf__ file that describes your TD API key and `spark.td.site` (See above).\n- __td_pyspark.py__ \n  - Check the file location using `pip show -f td-pyspark`, and copy td_pyspark.py to your favorite location\n- __td-spark-assembly-latest_xxxx.jar__\n  - Get the latest version from [Download](https://treasure-data.github.io/td-spark/release_notes.html#download) page.\n- Pre-build Spark \n  - [Download Spark 2.4.x](https://spark.apache.org/downloads.html) with Hadoop 2.7.x (built for Scala 2.11) \n  - Extract the downloaded archive. This folder location will be your `$SPARK_HOME`.\n\nHere is an example PySpark application code:\n__my_app.py__\n\n```python\nimport td_pyspark\nfrom pyspark.sql import SparkSession\n\n# Create a new SparkSession \nspark = SparkSession\\\n    .builder\\\n    .appName(\"myapp\")\\\n    .getOrCreate()\n\n# Create TDSparkContext\ntd = td_pyspark.TDSparkContext(spark)\n\n# Read the table data within -1d (yesterday) range as DataFrame\ndf = td.table(\"sample_datasets.www_access\").within(\"-1d\").df()\ndf.show()\n```\n\nTo run `my_app.py` use spark-submit by specifying the necessary files mentioned above:\n\n```bash\n# Launching PySpark with the local mode\n$ ${SPARK_HOME}/bin/spark-submit --master \"local[4]\"\\\n  --driver-class-path td-spark-assembly.jar\\\n  --properties-file=td-spark.conf\\\n  --py-files td_pyspark.py\\\n  my_app.py\n```\n\n`local[4]` means running a Spark cluster locally using 4 threads.\n\nTo use a remote Spark cluster, specify `master` address, e.g., `--master=spark://(master node IP address):7077`.\n\n### Using td-spark assembly included in the PyPI package.\n\nThe package contains pre-built binary of td-spark so that you can add it into the classpath as default.\n`TDSparkContextBuilder.default_jar_path()` returns the path to the default td-spark-assembly.jar file.\nPassing the path to `jars` method of TDSparkContextBuilder will automatically build the SparkSession including the default jar.\n\n```python\nimport td_pyspark\nfrom pyspark.sql import SparkSession\n\nbuilder = SparkSession\\\n    .builder\\\n    .appName(\"td-pyspark-app\")\n\ntd = td_pyspark.TDSparkContextBuilder(builder)\\\n    .apikey(\"XXXXXXXXXXXXXX\")\\\n    .jars(TDSparkContextBuilder.default_jar_path())\\\n    .build()\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://tddocs.atlassian.net/wiki/spaces/PD/pages/542933139/Data+Science+and+SQL+Tools",
    "keywords": "Spark PySpark TreasureData",
    "license": "Apache 2",
    "maintainer": "",
    "maintainer_email": "",
    "name": "td-pyspark-ea",
    "package_url": "https://pypi.org/project/td-pyspark-ea/",
    "platform": "",
    "project_url": "https://pypi.org/project/td-pyspark-ea/",
    "project_urls": {
      "Homepage": "https://tddocs.atlassian.net/wiki/spaces/PD/pages/542933139/Data+Science+and+SQL+Tools"
    },
    "release_url": "https://pypi.org/project/td-pyspark-ea/20.12.0/",
    "requires_dist": [
      "sphinx (>=2.2.0) ; extra == 'docs'",
      "sphinx-rtd-theme (>=0.4.3) ; extra == 'docs'",
      "recommonmark ; extra == 'docs'",
      "pyspark (>=3.0.1) ; extra == 'spark'"
    ],
    "requires_python": "",
    "summary": "Treasure Data extension for pyspark",
    "version": "20.12.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8855801,
  "releases": {
    "20.10.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "31f51d9d8de8a713cb6f9f2f1966e6e03a203e41cbafcbd3a552b3ccf31e262a",
          "md5": "b46c51a8c534a4fec79e246862e96cf3",
          "sha256": "b988c5616211cd86c20f9d66cd3f7ab7a7428ac62c59c006331fa143b7a665b6"
        },
        "downloads": -1,
        "filename": "td_pyspark_ea-20.10.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b46c51a8c534a4fec79e246862e96cf3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21362399,
        "upload_time": "2020-10-19T22:47:44",
        "upload_time_iso_8601": "2020-10-19T22:47:44.425082Z",
        "url": "https://files.pythonhosted.org/packages/31/f5/1d9d8de8a713cb6f9f2f1966e6e03a203e41cbafcbd3a552b3ccf31e262a/td_pyspark_ea-20.10.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "295972431d2b56f55a00a881380a416c2b32a60552296faf5d3914d70e4068b7",
          "md5": "7630d3973ef3f300c127b4680d834798",
          "sha256": "e307f0efc7f1191b7c9e42295a3aaa9ad88bb8bb8ab7066aa08784c4fb2a0766"
        },
        "downloads": -1,
        "filename": "td_pyspark_ea-20.10.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7630d3973ef3f300c127b4680d834798",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 21359665,
        "upload_time": "2020-10-19T22:48:11",
        "upload_time_iso_8601": "2020-10-19T22:48:11.554956Z",
        "url": "https://files.pythonhosted.org/packages/29/59/72431d2b56f55a00a881380a416c2b32a60552296faf5d3914d70e4068b7/td_pyspark_ea-20.10.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "20.12.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f94e281eccf8cd1f9e25006bddfd5b2c9906d40b9e3a664a9b06c6b8639f978a",
          "md5": "48adac14e5b70ba470b4b6467cfa86e4",
          "sha256": "369ec684e70875cf532769938507cb5a163a76240455ffb4bc5f6091a4d326bf"
        },
        "downloads": -1,
        "filename": "td_pyspark_ea-20.12.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "48adac14e5b70ba470b4b6467cfa86e4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 20699744,
        "upload_time": "2020-12-09T06:31:05",
        "upload_time_iso_8601": "2020-12-09T06:31:05.211938Z",
        "url": "https://files.pythonhosted.org/packages/f9/4e/281eccf8cd1f9e25006bddfd5b2c9906d40b9e3a664a9b06c6b8639f978a/td_pyspark_ea-20.12.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "03039c97af306174b928e192818351323496b0052b40c481e7781c6c7fa7135f",
          "md5": "c4f9a649058decfb900f8869685b657f",
          "sha256": "45e6646e11748a7527194f518f57c2fa6828ad017b8b8841216a0af5b6eec82c"
        },
        "downloads": -1,
        "filename": "td_pyspark_ea-20.12.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c4f9a649058decfb900f8869685b657f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 20698189,
        "upload_time": "2020-12-09T06:32:31",
        "upload_time_iso_8601": "2020-12-09T06:32:31.365326Z",
        "url": "https://files.pythonhosted.org/packages/03/03/9c97af306174b928e192818351323496b0052b40c481e7781c6c7fa7135f/td_pyspark_ea-20.12.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "20.6.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ae3a84709aa74bc1d3e17265e0a8ce421a422cfdf8819332e4cd37adb744cc36",
          "md5": "980bef6f9f93c6f160d34352b3cfc2b3",
          "sha256": "c7ec7e9eb43f2bc7d014ecccf1cceabfa07fb4ff11e549a11118b124264c166b"
        },
        "downloads": -1,
        "filename": "td_pyspark_ea-20.6.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "980bef6f9f93c6f160d34352b3cfc2b3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 20879668,
        "upload_time": "2020-06-24T16:28:38",
        "upload_time_iso_8601": "2020-06-24T16:28:38.738782Z",
        "url": "https://files.pythonhosted.org/packages/ae/3a/84709aa74bc1d3e17265e0a8ce421a422cfdf8819332e4cd37adb744cc36/td_pyspark_ea-20.6.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5557762453b8dd5a0013c69d26440f72677244913d1785163e5200ca7007ce63",
          "md5": "ec3bf423f31ce43bc8466a8ad0a0a3ac",
          "sha256": "4774d47f913bb7dcc53196004e87227e8c4d29acb3d19b6dd4a37e77fdc1279b"
        },
        "downloads": -1,
        "filename": "td_pyspark_ea-20.6.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ec3bf423f31ce43bc8466a8ad0a0a3ac",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 20876144,
        "upload_time": "2020-06-24T16:28:57",
        "upload_time_iso_8601": "2020-06-24T16:28:57.697193Z",
        "url": "https://files.pythonhosted.org/packages/55/57/762453b8dd5a0013c69d26440f72677244913d1785163e5200ca7007ce63/td_pyspark_ea-20.6.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "20.6.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "00dde3fe43861c2898653705cb729df9952555c5b0290ad39a2b3a9992b5bf1a",
          "md5": "b24c229ba534302a10ef084e8f33e827",
          "sha256": "5b70ff3c069bd26b4eb787a9835aa30d41889f75c9b01d4604db8572b6b3dc53"
        },
        "downloads": -1,
        "filename": "td_pyspark_ea-20.6.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b24c229ba534302a10ef084e8f33e827",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 20879819,
        "upload_time": "2020-06-26T01:07:35",
        "upload_time_iso_8601": "2020-06-26T01:07:35.210506Z",
        "url": "https://files.pythonhosted.org/packages/00/dd/e3fe43861c2898653705cb729df9952555c5b0290ad39a2b3a9992b5bf1a/td_pyspark_ea-20.6.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "36a583aa19783d62f3d9906a50758d4266c241c12c8e671d9bf0bacb55b86fa0",
          "md5": "03160bbbb481bc389a95e3d45da6703c",
          "sha256": "c5541760889c8026f61a97d725546bb86796b9460e84d14d29450e9b9ff72031"
        },
        "downloads": -1,
        "filename": "td_pyspark_ea-20.6.2.tar.gz",
        "has_sig": false,
        "md5_digest": "03160bbbb481bc389a95e3d45da6703c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 20876202,
        "upload_time": "2020-06-26T01:07:58",
        "upload_time_iso_8601": "2020-06-26T01:07:58.713605Z",
        "url": "https://files.pythonhosted.org/packages/36/a5/83aa19783d62f3d9906a50758d4266c241c12c8e671d9bf0bacb55b86fa0/td_pyspark_ea-20.6.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f94e281eccf8cd1f9e25006bddfd5b2c9906d40b9e3a664a9b06c6b8639f978a",
        "md5": "48adac14e5b70ba470b4b6467cfa86e4",
        "sha256": "369ec684e70875cf532769938507cb5a163a76240455ffb4bc5f6091a4d326bf"
      },
      "downloads": -1,
      "filename": "td_pyspark_ea-20.12.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "48adac14e5b70ba470b4b6467cfa86e4",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 20699744,
      "upload_time": "2020-12-09T06:31:05",
      "upload_time_iso_8601": "2020-12-09T06:31:05.211938Z",
      "url": "https://files.pythonhosted.org/packages/f9/4e/281eccf8cd1f9e25006bddfd5b2c9906d40b9e3a664a9b06c6b8639f978a/td_pyspark_ea-20.12.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "03039c97af306174b928e192818351323496b0052b40c481e7781c6c7fa7135f",
        "md5": "c4f9a649058decfb900f8869685b657f",
        "sha256": "45e6646e11748a7527194f518f57c2fa6828ad017b8b8841216a0af5b6eec82c"
      },
      "downloads": -1,
      "filename": "td_pyspark_ea-20.12.0.tar.gz",
      "has_sig": false,
      "md5_digest": "c4f9a649058decfb900f8869685b657f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 20698189,
      "upload_time": "2020-12-09T06:32:31",
      "upload_time_iso_8601": "2020-12-09T06:32:31.365326Z",
      "url": "https://files.pythonhosted.org/packages/03/03/9c97af306174b928e192818351323496b0052b40c481e7781c6c7fa7135f/td_pyspark_ea-20.12.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}