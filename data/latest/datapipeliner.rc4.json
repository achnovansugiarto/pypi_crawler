{
  "info": {
    "author": "Neil Bartlett",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "datapipeliner\n============\n\ndvc-ready data pipelines\n\nProvides data pipelines that are especially suited to incoporation into [DVC](https://dvc.org/).\nAllows model and data processing experiment production support.\n\nData pipelines are described in a YAML configuration file.\n\nThe pipelines can be parameterized by tags names.\nThese can be programatically controlled via a commands in a dvc params.yaml file - which allows\nfor data processing experiments to be integrated into a dvc experiment environment ie the data processing stages\ncan become features in the experiment.\n\nThe YAML file can define standard pdpipe stages as well as custom stages defined\nin a .py file. Parameters to the stages are defined in the YAML file.\n\n\nInstallation\n------------\n\n    pip install datapipeliner\n\nRequirements\n------------\n\nThis package manages YAML configurations with `confuse`, which itself depends on\n`pyYAML`. Pipeline stages and pipelines are generated with `pdpipe`, and `engarde` is an\noptional dependency for `verify_all`-, `verify_any`-, and `engarde`-type stages.\n\nDetails\n-------\n\nThe pipeline is defined in `config.yaml`. This file contains information\nabout `sources`, files from which the data is drawn, `pipelines` and their stages, and\nthe `sinks`, files to which the transformed data is written. Custom-made functions may\nbe defined in a standard `*.py` file/module, which must take a `pandas.DataFrame` as\ninput and return a `pandas.DataFrame` as output. Pipeline stages are generated from\nthese custom functions by specifying them and their keyword arguments in `config.yaml`.\n\nThe file `config.yaml` controls all aspects of the pipeline, from data discovery, to\npipeline stages, to data output. If the environment variable `DATAPIPELINERDIR` is not\nspecified, then then it will be set to the current working directory. The file\n`config.yaml` should be put in the `DATAPIEPLINEDIR`, and data to be processed should be\nin that directory or its subdirectories.\n\nExample\n-------\n\nThe directory structure of this example is as follows:\n\n    example/\n        config.yaml\n        custom_functions.py\n        example.py\n        raw\n            products_storeA.csv\n            products_storeB.csv\n        output\n            products_storeA_processed.csv\n            products_storeB_processed.csv\n\nThe contents of `config.yaml` is as follows (paths are relative to the location of\n`config.yaml`, i.e. the `DATAPIPELINERDIR`):\n\n    sources:\n      example_source:\n        file: raw/products*.csv\n        kwargs:\n          usecols:\n            - items\n            - prices\n            - inventory\n        index_col: items\n\n    sinks:\n      example_sink:\n        file: output/*_processed.csv\n\n    pipelines:\n      example_pipeline:\n\n      - type: transform\n          function: add_to_col\n          tag: add\n          kwargs:\n            col_name: prices\n            val: 1.5\n          staging:\n            desc: Adds $1.5 to column 'prices'\n            exmsg: Couldn't add to 'prices'.\n\n        - type: pdpipe\n          function: ColDrop\n          kwargs:\n            columns: inventory\n          staging:\n            exraise: false\n\n        - type: verify_all\n          check: high_enough\n          tag: verify\n          kwargs:\n            col_name: prices\n            val: 19\n          staging:\n            desc: Checks whether all 'prices' are over $19.\n\nThe module `custom_functions.py` contains:\n\n    custom_functions.py\n\n        def add_to_col(df, col_name, val):\n            df.loc[:, col_name] = df.loc[:, col_name] + val\n            return df\n\n        def high_enough(df, col_name, val):\n            return df.loc[:, col_name] > val\n\nFinally, the contents of the file `example.py`:\n\n    import custom_functions\n    import datapipeliner as dpp\n\n    src = dpp.Source(\"example_source\")  # generate the source from `config.yaml`\n    snk = dpp.Sink(\"example_sink\")  # generate the sink from `config.yaml`.\n\n    # generate the pipeline from `config.yaml`.\n    line = dpp.Line(\"example_pipeline\", custom_functions)\n\n    # connect the source and sink to the pipeline, print what the pipeline will do, then run\n    # the pipeline, writing the output to disk. capture the input/output dataframes if desired.\n    pipeline = line.connect(src, snk)\n    print(pipeline)\n    (dfs_in, dfs_out) = line.run()\n\nRunning `example.py` generates `src`, `snk`, and `line` objects. Then, the `src` and\n`snk` are connected to an internal `pipeline`, which is a `pdpipe.PdPipeLine` object.\nWhen this pipeline is printed, the following output is displayed:\n\n    A pdpipe pipeline:\n    [ 0]  Adds $1.5 to column 'prices'\n    [ 1]  Drop columns inventory\n    [ 2]  Checks whether all 'prices' are over $19.\n\nThe function of this pipeline is apparent from the descriptions of each stage. Some\nstages have custom descriptions specified in the `desc` key of `config.yaml`. Stages\nof type `pdpipe` have their descriptions auto-generated from the keyword arguments.\n\nThe command `line.run()` pulls data from `src`, passes it through `pipeline`, and\ndrains it to `snk`. The returns `dfs_in` and `dfs_out` show that came in from `src`\nand what went to `snk`. In addition to `line.run()`, the first `n` stages of the\npipeline can be tested on file `m` from the source with `line.test(m,n)`.\n\nOutput from Example\n-------\n\nThis is  `.\\raw\\products_storeA.csv` before it is drawn into the source:\n\n| items   |   prices |   inventory | color |\n|:--------|---------:|------------:|------:|\n| foo     |       19 |           5 |   red |\n| bar     |       24 |           3 | green |\n| baz     |       22 |           7 |  blue |\n\nThis is  `.\\raw\\products_storeA.csv` after it is drawn into the source with the argument\n`usecols = [\"items\", \"prices\", \"inventory\"]` specified in `config.yaml`:\n\n| items   |   prices |   inventory |\n|:--------|---------:|------------:|\n| foo     |       19 |           5 |\n| bar     |       24 |           3 |\n| baz     |       22 |           7 |\n\nThe output from the pipeline is sent to `.\\products_storeA_processed.csv`. The arguments\nspecified by `config.yaml` have been applied. Namely, `prices` have been incremented by\n`1.5`, the `inventory` column has been dropped, and then a check has been made that all\n`prices` are over `19`.\n\n| items   |   prices |\n|:--------|---------:|\n| foo     |     20.5 |\n| bar     |     25.5 |\n| baz     |     23.5 |\n\nIf the `verify_all` step had failed, an exception would be raised, and the items that\ndid not pass the check would be returned in the exception message. Say, for example,\nthat the `val` argument was `21` instead of `19`:\n\n    AssertionError: ('high_enough not true for all',\n    prices  items        \n    foo      20.5)\n\n\nDirect Dataframe Injection\n==========================\n\nAdditionally is is possible to call the pipeline directly with a data \n\n\n    import custom_functions\n    import datapipeliner as dpp\n    import pandas as pd\n\n    tags =  'add;verify'\n\n    df_in = pd.read_csv(\"myfile.csv\")\n\n    # generate the pipeline from `config.yaml`.\n    line = dpp.Line(\"example_pipeline\", custom_functions, tags)\n\n    df_out= line.runDataFrame(df_in)\n\nProvenance\n==========\n\nThis project was created as a fork of the excellent pdpipewrench. A big thanks to blakeNaccarato /\npdpipewrench.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/neilbartlett/datapipeliner",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "datapipeliner",
    "package_url": "https://pypi.org/project/datapipeliner/",
    "platform": "",
    "project_url": "https://pypi.org/project/datapipeliner/",
    "project_urls": {
      "Homepage": "https://github.com/neilbartlett/datapipeliner"
    },
    "release_url": "https://pypi.org/project/datapipeliner/0.1.4/",
    "requires_dist": [
      "pandas",
      "pdpipe",
      "engarde",
      "confuse",
      "setuptools ; extra == 'dev'",
      "wheel ; extra == 'dev'",
      "twine ; extra == 'dev'",
      "numpy ; extra == 'dev'",
      "scipy ; extra == 'dev'",
      "dtale ; extra == 'dev'",
      "doc8 ; extra == 'dev'",
      "jupyter ; extra == 'dev'",
      "black ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pylint ; extra == 'dev'",
      "matplotlib ; extra == 'dev'",
      "PyQt5 ; extra == 'dev'",
      "rope ; extra == 'dev'"
    ],
    "requires_python": ">=3.7",
    "summary": "dvc-ready data pipelines.",
    "version": "0.1.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10057007,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b5ad085b006bbb6d83b00eeee4a7238ad2f1147355b561b7c167aad7e02eb7b8",
          "md5": "f419737a06c5b3e02251c43ae1fd87b6",
          "sha256": "cc4106e9fc5bc0a3b6fc652cfb6015580b0bb11e7ba9877d11916a36d4bc7dff"
        },
        "downloads": -1,
        "filename": "datapipeliner-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f419737a06c5b3e02251c43ae1fd87b6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 11314,
        "upload_time": "2021-03-30T20:48:03",
        "upload_time_iso_8601": "2021-03-30T20:48:03.901834Z",
        "url": "https://files.pythonhosted.org/packages/b5/ad/085b006bbb6d83b00eeee4a7238ad2f1147355b561b7c167aad7e02eb7b8/datapipeliner-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "41f8bb4bb1e214f0c771f8480da1ea6cb3f5663f35fd3cfede70f8010e443b9c",
          "md5": "eff8d7d8c8e1ad4f23c6f1b9821e35b5",
          "sha256": "6e5c706da3aef81ebdd87e307e5da4000d5357d9cc90671bc09beee0c3e1f6d9"
        },
        "downloads": -1,
        "filename": "datapipeliner-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eff8d7d8c8e1ad4f23c6f1b9821e35b5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 11298,
        "upload_time": "2021-03-30T21:07:07",
        "upload_time_iso_8601": "2021-03-30T21:07:07.596514Z",
        "url": "https://files.pythonhosted.org/packages/41/f8/bb4bb1e214f0c771f8480da1ea6cb3f5663f35fd3cfede70f8010e443b9c/datapipeliner-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eb2cbef4c6d2659bd2c2018adcb53d0f7e3211f046e2b977817f6e06a15fb960",
          "md5": "25c3a2f479bca0d6316072ac1fb6f3ac",
          "sha256": "7b75b58f7415e78601eea18cb1f435b056a1d3a93b4a547d8d74a84cd7b6d0be"
        },
        "downloads": -1,
        "filename": "datapipeliner-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "25c3a2f479bca0d6316072ac1fb6f3ac",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 11306,
        "upload_time": "2021-03-30T21:34:37",
        "upload_time_iso_8601": "2021-03-30T21:34:37.886077Z",
        "url": "https://files.pythonhosted.org/packages/eb/2c/bef4c6d2659bd2c2018adcb53d0f7e3211f046e2b977817f6e06a15fb960/datapipeliner-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2ca9df88db7738df9b39c506f5ffa5132c6ac99e007f5baf9642c1c795bdbce6",
          "md5": "7ab95c8cfbc8d032ddd8ad4b22f388f9",
          "sha256": "4fa5780f031e5101cc5f25ca1dc0966583fff0056461bb8d24dcb20ef1ff368f"
        },
        "downloads": -1,
        "filename": "datapipeliner-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7ab95c8cfbc8d032ddd8ad4b22f388f9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 11775,
        "upload_time": "2021-04-13T23:59:40",
        "upload_time_iso_8601": "2021-04-13T23:59:40.730985Z",
        "url": "https://files.pythonhosted.org/packages/2c/a9/df88db7738df9b39c506f5ffa5132c6ac99e007f5baf9642c1c795bdbce6/datapipeliner-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cc74ced0d47b3018af06c07d6011a071967a18e2ad183e6952098deec369557b",
          "md5": "632c696633e29043bbfae3b8383ca1fa",
          "sha256": "141fa1ead75a2231d6f0a51889d330fe98a6992f250e7e70d2956b5bb3bbe188"
        },
        "downloads": -1,
        "filename": "datapipeliner-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "632c696633e29043bbfae3b8383ca1fa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 13053,
        "upload_time": "2021-04-13T23:59:41",
        "upload_time_iso_8601": "2021-04-13T23:59:41.767606Z",
        "url": "https://files.pythonhosted.org/packages/cc/74/ced0d47b3018af06c07d6011a071967a18e2ad183e6952098deec369557b/datapipeliner-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2ca9df88db7738df9b39c506f5ffa5132c6ac99e007f5baf9642c1c795bdbce6",
        "md5": "7ab95c8cfbc8d032ddd8ad4b22f388f9",
        "sha256": "4fa5780f031e5101cc5f25ca1dc0966583fff0056461bb8d24dcb20ef1ff368f"
      },
      "downloads": -1,
      "filename": "datapipeliner-0.1.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "7ab95c8cfbc8d032ddd8ad4b22f388f9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 11775,
      "upload_time": "2021-04-13T23:59:40",
      "upload_time_iso_8601": "2021-04-13T23:59:40.730985Z",
      "url": "https://files.pythonhosted.org/packages/2c/a9/df88db7738df9b39c506f5ffa5132c6ac99e007f5baf9642c1c795bdbce6/datapipeliner-0.1.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cc74ced0d47b3018af06c07d6011a071967a18e2ad183e6952098deec369557b",
        "md5": "632c696633e29043bbfae3b8383ca1fa",
        "sha256": "141fa1ead75a2231d6f0a51889d330fe98a6992f250e7e70d2956b5bb3bbe188"
      },
      "downloads": -1,
      "filename": "datapipeliner-0.1.4.tar.gz",
      "has_sig": false,
      "md5_digest": "632c696633e29043bbfae3b8383ca1fa",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 13053,
      "upload_time": "2021-04-13T23:59:41",
      "upload_time_iso_8601": "2021-04-13T23:59:41.767606Z",
      "url": "https://files.pythonhosted.org/packages/cc/74/ced0d47b3018af06c07d6011a071967a18e2ad183e6952098deec369557b/datapipeliner-0.1.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}