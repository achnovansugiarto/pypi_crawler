{
  "info": {
    "author": "Tullio Bagnoli",
    "author_email": "tullio.bagnoli@protonmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Thompson sampling for the multi-armed bandit problem\n\nThis module contains the code necessary \nto implement a Thompson sampling strategy\nin a multi-armed bandit setting.\n\n## Python version\n\nThe module has been developed on Python 3.7.3.\n\n## Python Libraries\n\nInstall requirements as:\n\n    pip3 install -r requirements.txt\n\n## Usage\n\nTo initialize the class:\n\n    ts = Thompson(success_probs)\n\nwhere\n* `success_probs`: an array of floats in the [0, 1] interval,\n    the success probability of each bandit;\n    \nplus the following optional parameters: \n* `steps`: the number of steps to iterate over, defaults to 1000.\n* `alpha_damping`: reduces the tendency of TS towards exploitation (defaults to 1, no damping)\n* `beta_damping`: reduces the tendency of TS towards exploration (defaults to 1, no damping)\n* `alpha_init`: an array of non-negative floats, sets the initial conditions for the prior distribution of each bandit\n    (defaults to 0, uniform priors)\n* `beta_init`: an array of non-negative floats, sets the initial conditions for the prior distribution of each bandit\n    (defaults to 0, uniform priors)\n* `optimistic`: whether to use an optimistic TS strategy, whereby a lower bound is put on sampling\n    (defaults to False)\n* `optimistic_threshold`: the lower bound for sampling in an optimistic strategy\n    (defaults to 1.e-6)\n\n## Theory recap\n\n### Multi-armed bandit problem\n\nThe multi-armed bandit problem is an unsupervised-learning problem\nin which a fixed set of limited resources\nmust be allocated between competing choices\nwithout prior knowledge of the rewards offered by each of them,\nwhich must be instead learned on the go.\n\nThe name refers to the hypothetical situation\nin which a player must choose between a set of $K$ slot machines,\nand at each round decide whether to continue play with the one\ncurrently looking like the most promising choice\n(based on the expectation of an \"exploitation\", i.e., a winning streak)\nversus trying a different machine,\nwhich might turn out to offer a higher reward (\"exploration\").\n(The winning probability of each machine is assumed constant in time.)\n\nPractically, the issue of the trade-off between exploitation and exploration\nis one faced in many online reinforcement-learning problems,\nsuch as offering a (new or relatively new) app user the optimal UX flow\n(success being continued user engagement),\npicking between different banner ads that can be displayed on a website\n(success being a click or a conversion), etc.\n\n### Thompson sampling and Bayes' rule\n\nThompson sampling (TS) is a solving approach to the aforementioned exploitation-exploration problem\nthat attempts to converge towards the optimal solution by finding a balance\nbetween exploiting what is known to maximize immediate performance on the one hand,\nand investing to accumulate new information that may improve future performance on the other.\nThe second aspect is what distinguishes this class of solutions from so-called\ngreedy approaches, which instead only maximize (based on the historical data at hand)\nthe immediate reward. Greedy approaches have a higher chance of getting stuck in local maxima,\nand cannot thoroughly explore the available parameter space, which in simpler terms means\nto try out options with larger uncertainties over their average outcomes.\n\nWhile a greedy approach would assess at each round the expected success probability $\\theta_k$\nof each action $k$ and pick the maximum one among them,\nin TS, at each round,\nthe probability of picking a bandit is equal to the probability of it being the optimal choice.\nTo practically emulate this behaviour,\na value for the success probability of each possible action is randomly sampled\nfrom the posterior distribution of each one\n(the maximum of which _is_ of course the expected success probability of that action),\nand only then the maximum is picked.\n\nThe posterior probability distribution is then updated based on the observed result,\nso becoming the prior distribution for the next round.\nThis update rule, mixing prior assumptions about the probability distribution and the empirical observations,\nis what makes TS a Bayesian approach:\nin Bayesian statistics, the probability of an event is based both on data\n(the fresh empirical observations, which in our case are the results of each round of lever pulling)\nand on prior information or beliefs about the event\n(in our case, the rewards expected from each machine, \ninitially guessed at random and then adjusted after every round of pulling).\nThis second aspect is what distinguishes Bayesian methods from purely frequentist ones,\nwhich are only based on the observed data and do not incorporate the concept of priors.\n\n### Beta-Bernoulli bandit\n\nIn the Beta-Bernoulli schematization of the bandit problem,\neach time an action $k$ is selected,\na reward of 1 (success) is generated with probability $\\theta_k$ \n\\- otherwise, a reward of 0 (failure) is generated, with probability $(1 - \\theta_k)$.\nBernoulli refers here to 0/1 being the only possible outcomes.\n\nStatistically, the prior distribution of expected rewards for a Bernoulli random variable\nis the so-called Beta distribution $Beta(\\alpha_k, \\beta_k)$,\nits parameters $\\alpha_k$ and $\\beta_k$ sometimes being called pseudo-counts,\nbecause the update rule increases them by one with each observed success or failure, respectively.\nThe posterior distribution we get after updating\nthe prior probability of the selected action (and of no other one)\ndepending on the reward obtained becomes then the prior distribution for the next round, and so on,\nincreasingly growing sharper and closer to its expected value.\n\nA useful property of the distributions so built is that each $\\theta_k$ is the probability\nthat the random estimate drawn from it exceeds those drawn for other actions.\nAlso, since these estimates are drawn from the posterior distributions,\nthey are equal to the probability that the corresponding action is optimal,\nconditioned on observed history.\n\nAs we play and gather evidence, our posterior distributions become more concentrated\n(more so, the more rewarding they are),\nincreasing our chance of maximizing the sum of the collective rewards:\nexploration reduces over time, and exploitation is increasingly privileged as the uncertainties are reduced.\nMeanwhile the regret, or the sum of the rewards under an optimal strategy\nminus the actually collected rewards, asymptotically converge to zero.\n\n### Author\n\nTullio Bagnoli\n\n### License\n\nThis project is licensed under the MIT license.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/tbagnoli/multi_armed_thompson/archive/v_01.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/tbagnoli/multi_armed_thompson/",
    "keywords": "thompson,sampling,unsupervised",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "multi-armed-thompson",
    "package_url": "https://pypi.org/project/multi-armed-thompson/",
    "platform": "",
    "project_url": "https://pypi.org/project/multi-armed-thompson/",
    "project_urls": {
      "Download": "https://github.com/tbagnoli/multi_armed_thompson/archive/v_01.tar.gz",
      "Homepage": "https://github.com/tbagnoli/multi_armed_thompson/"
    },
    "release_url": "https://pypi.org/project/multi-armed-thompson/0.2.2/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Thompson sampling for multi-armed bandit",
    "version": "0.2.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7960983,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "271ad727eba7ddf8233c0f601c09219e28ebb4b71bdc1a2125a040f0697f6bfa",
          "md5": "d21d32ad645c846bce87d50c5c8813ce",
          "sha256": "2263caee662ec43bb9c1b63475b791e0372e918b3109ca027756b444016085a2"
        },
        "downloads": -1,
        "filename": "multi_armed_thompson-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "d21d32ad645c846bce87d50c5c8813ce",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 5832,
        "upload_time": "2020-08-12T15:41:31",
        "upload_time_iso_8601": "2020-08-12T15:41:31.502580Z",
        "url": "https://files.pythonhosted.org/packages/27/1a/d727eba7ddf8233c0f601c09219e28ebb4b71bdc1a2125a040f0697f6bfa/multi_armed_thompson-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8f1decbaf1147c98b45cb6e3063f234f9a9fc7cd03c63becfa318436bbb6b8cc",
          "md5": "1c26d94bc037eb7df4df57754f4f278c",
          "sha256": "6df9f10896fdc5a21967e5fba1f823cdb0e9a02791e6c9e5350a0ae51c6a0773"
        },
        "downloads": -1,
        "filename": "multi_armed_thompson-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1c26d94bc037eb7df4df57754f4f278c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 6039,
        "upload_time": "2020-08-14T14:35:15",
        "upload_time_iso_8601": "2020-08-14T14:35:15.435368Z",
        "url": "https://files.pythonhosted.org/packages/8f/1d/ecbaf1147c98b45cb6e3063f234f9a9fc7cd03c63becfa318436bbb6b8cc/multi_armed_thompson-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "92dc33be80968b748133c66a142593c09217c63ab308809256098d094bb97e0e",
          "md5": "284e96f651c535a14e089c3dc28b325f",
          "sha256": "dadc5eeaf943e08d51f577f57ce3cbe2c3b262eef50cb114c843aebf4b9c8e4c"
        },
        "downloads": -1,
        "filename": "multi_armed_thompson-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "284e96f651c535a14e089c3dc28b325f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 6073,
        "upload_time": "2020-08-14T14:43:20",
        "upload_time_iso_8601": "2020-08-14T14:43:20.004031Z",
        "url": "https://files.pythonhosted.org/packages/92/dc/33be80968b748133c66a142593c09217c63ab308809256098d094bb97e0e/multi_armed_thompson-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "490b1a42900340d3aaca90d508c0830c453149ee5b7fffa6c05e8c3c3c995955",
          "md5": "1c9ddf9dde3f3a60adf02757454c2435",
          "sha256": "53e8f12355ded9715589b99a9a504a4cffb1f861655189320ebb5e80b2c68c2d"
        },
        "downloads": -1,
        "filename": "multi_armed_thompson-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1c9ddf9dde3f3a60adf02757454c2435",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 6086,
        "upload_time": "2020-08-14T15:19:52",
        "upload_time_iso_8601": "2020-08-14T15:19:52.554658Z",
        "url": "https://files.pythonhosted.org/packages/49/0b/1a42900340d3aaca90d508c0830c453149ee5b7fffa6c05e8c3c3c995955/multi_armed_thompson-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "490b1a42900340d3aaca90d508c0830c453149ee5b7fffa6c05e8c3c3c995955",
        "md5": "1c9ddf9dde3f3a60adf02757454c2435",
        "sha256": "53e8f12355ded9715589b99a9a504a4cffb1f861655189320ebb5e80b2c68c2d"
      },
      "downloads": -1,
      "filename": "multi_armed_thompson-0.2.2.tar.gz",
      "has_sig": false,
      "md5_digest": "1c9ddf9dde3f3a60adf02757454c2435",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 6086,
      "upload_time": "2020-08-14T15:19:52",
      "upload_time_iso_8601": "2020-08-14T15:19:52.554658Z",
      "url": "https://files.pythonhosted.org/packages/49/0b/1a42900340d3aaca90d508c0830c453149ee5b7fffa6c05e8c3c3c995955/multi_armed_thompson-0.2.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}