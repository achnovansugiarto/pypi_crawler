{
  "info": {
    "author": "piyop",
    "author_email": "t.okuda@keio.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Table of Contents\n\n* [tfaug package](#tfaug-package)\n* [Features](#features)\n* [Dependancies](#dependancies)\n   * [For test script](#for-test-script)\n* [Supported Augmentations](#supported-augmentations)\n* [Install](#install)\n* [Samples](#samples)\n   * [Classification Problem](#classification-problem)\n      * [Convert Images and Labels to Tfrecord Format by TfrecordConverter()](#convert-images-and-labels-to-tfrecord-format-by-tfrecordconverter)\n      * [Create Dataset by DatasetCreator()](#create-dataset-by-datasetcreator)\n      * [Define and Learn Model Using Defined Datasets](#define-and-learn-model-using-defined-datasets)\n   * [Segmentation Problem](#segmentation-problem)\n      * [Convert Images and Labels to Tfrecord Format by TfrecordConverter()](#convert-images-and-labels-to-tfrecord-format-by-tfrecordconverter-1)\n      * [Create Dataset by DatasetCreator()](#create-dataset-by-datasetcreator-1)\n      * [Define and Learn Model Using Defined Datasets](#define-and-learn-model-using-defined-datasets-1)\n      * [Adjust sampling ratios from multiple tfrecord files](#adjust-sampling-ratios-from-multiple-tfrecord-files)\n   * [Use AugmentImg Directly](#use-augmentimg-directly)\n      * [1. Initialize](#1-initialize)\n      * [2. use in tf.data.map() after batch()](#2-use-in-tfdatamap-after-batch)\n\n\n# tfaug package\nTensorflow >= 2 recommends to be feeded data by tf.data.Dataset.\nThis package supports creation of tf.data.Dataset (generator) and augmentation for image.\n\nThis package includes below 3 classes:\n * DatasetCreator - creator of tf.data.Dataset from tfrecords or image paths\n * TfrecordConverter - pack images and labels to tfrecord format (recommended format for better peformance)\n * AugmentImg - image augmentation class. This is used inside DatasetCreator implicitly or you can use it directly.\n\n# Features\n * Augment input image and label image with same transformations at the same time.\n * Reduce cpu load by generating all transformation matrix at first. (use `input_shape` parameter at `DatasetCreator()` or `AugmentImg()`)\n * It could adjust sampling ratios from multiple tfrecord files. (use `ratio_samples` parameter at `DatasetCreator().dataset_from_tfrecords`) This is effective for class imbalance problems.\n * Augment on batch. It is more efficient than augment each image.\n * Use only tensorflow operators and builtin functions while augmentation. Because any other operations or functions (e.g. numpy functions) may be bottleneck of learning. [mentined here](https://www.tensorflow.org/guide/function).\n\n# Dependancies\n * Python >= 3.5\n * tensorflow >= 2.0\n * tensorflow-addons\n## For test script\n * pillow\n * numpy\n * matplotlib\n\n# Supported Augmentations\n * standardize\n * resize\n * random_rotation\n * random_flip_left_right\n * random_flip_up_down\n * random_shift\n * random_zoom\n * random_shear\n * random_brightness\n * random_saturation\n * random_hue\n * random_contrast\n * random_crop\n * random_noise\n * random_blur\n\n# Install\npython -m pip install git+https://github.com/piyop/tfaug\n\n# Samples\n\nSimple Classification and Segmentation Usage is shown below. \nWhole ruunable codes is in sample_tfaug.py\n\n## Classification Problem\nDownload, convert to tfrecord and learn MNIST dataset.\nBelow examples are part of `learn_mnist()` in sample_tfaug.py\n\nImport tfaug and define directory to be store data.\n```Python\nfrom tfaug import TfrecordConverter, DatasetCreator, AugmentImg\nDATADIR = 'testdata/tfaug/'\n```\n\nLoad MNIST dataset using tensorflow.\n```Python\nos.makedirs(DATADIR+'mnist', exist_ok=True)\n# load mnist dataset\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n```\n\n### Convert Images and Labels to Tfrecord Format by TfrecordConverter()\nConvert training and validation(test) images and labels into Tfrecord format by TfrecordConverter.\nTherefore, Tensorflow could be load data from Tfrecord format with least overhead and parallel reading. \n```Python\n# save as tfrecord\nTfrecordConverter().tfrecord_from_ary_label(\n    x_train, y_train, DATADIR+'mnist/train.tfrecord')\nTfrecordConverter().tfrecord_from_ary_label(\n    x_test, y_test, DATADIR+'mnist/test.tfrecord')\n ```\n\n### Create Dataset by DatasetCreator()\nCreate and apply augmentation to training and validation Tfrecords by DatasetCreator.\nFor the classification problem, use `label_type = 'class'` for DatasetCreator constractor.\nSet image augmentation params to DatasetCreator constractor.\n```Python\nbatch_size, shuffle_buffer = 25, 25\n# create training and validation dataset using tfaug:\nds_train, train_cnt = (DatasetCreator(shuffle_buffer=shuffle_buffer,\n                                      batch_size=batch_size,\n                                      label_type='class',\n                                      repeat=True,\n                                      random_zoom=[0.1, 0.1],\n                                      random_rotation=20,\n                                      random_shear=[10, 10],\n                                      training=True)\n                       .dataset_from_tfrecords([DATADIR+'mnist/train.tfrecord']))\nds_valid, valid_cnt = (DatasetCreator(shuffle_buffer=shuffle_buffer,\n                                      batch_size=batch_size,\n                                      label_type='class',\n                                      repeat=True,\n                                      training=False)\n                       .dataset_from_tfrecords([DATADIR+'mnist/test.tfrecord']))\n\n```\n\nAdd constant reguralization to training and validation datasets.\n```Python\n# constant reguralization\nds_train = ds_train.map(lambda x, y: (x/255, y))\nds_train = ds_valid.map(lambda x, y: (x/255, y))\n```\n\n### Define and Learn Model Using Defined Datasets\nDefine Model\n```Python\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(10)])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.002),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n                  from_logits=True),\n              metrics=['sparse_categorical_accuracy'])\n```\n\nLearn Model by `model.fit()`. This accepts training and validation `tf.data.Dataset` which is created by DatasetCreator.\n`model.fit()` needs number of training and validation iterations per epoch.\n```Python\n# learn model\nmodel.fit(ds_train,\n          epochs=10,\n          validation_data=ds_valid,\n          steps_per_epoch=train_cnt//batch_size,\n          validation_steps=valid_cnt//batch_size)\n```\n\nEvaluation\n```Python\n# evaluation result\nmodel.evaluate(ds_valid,\n               steps=x_test.shape[0]//batch_size,\n               verbose=2)\n```\n\n## Segmentation Problem\nDownload ADE20k dataset and convert to the tfrecord\nBelow examples are part of `learn_mnist()` in sample_tfaug.py\n\nFirst, we set input image size and batch size for model\n```Python\ninput_size = [256, 256]  # cropped input image size\nbatch_size = 5\n```\n\nDownload and convert ADE20k dataset to tfrecord by defined function download_and_convertADE20k() in sample_tfaug.py\n```Python\n# donwload\ndownload_and_convert_ADE20k(input_size)\n```\n\n### Convert Images and Labels to Tfrecord Format by TfrecordConverter()\nIn download_and_convertADE20k(), split original images to patch image by `TfrecordConverter.get_patch()`\nThough ADE20k images have not same image size, tensorflow model input should be exactly same size.\n```Python\nconverter = TfrecordConverter()\n\n~~~~~~~~~~~~~~some codes~~~~~~~~~~~~~~~~~~~~~~\n\nimg_patches = converter.get_patch(im, input_size, overlap_buffer,\n                                  x_borders, y_borders, dtype=np.uint8)\nlbl_pathces = converter.get_patch(lb, input_size, overlap_buffer,\n                                  x_borders, y_borders, dtype=np.uint8)\n```\n\nSave images and labels as separated tfrecord format. \n ```Python \nimage_per_shards = 1000\n\n~~~~~~~~~~~~~~some codes~~~~~~~~~~~~~~~~~~~~~~\n\nconverter.tfrecord_from_path_label(imgs[sti:sti+image_per_shards],\n                                   path_labels,\n                                   path_tfrecord)\n ```\n\n### Create Dataset by DatasetCreator()\nAfter generate tfrecord files by `TfrecordConverter.tfrecord_from_path_label`, create training and validation dataset from these tfrecords by DatasetCreator.\nFor segmentation problem, use `label_type = 'segmentation'` to the constractor of the DatasetCreator.<br/>\nIf you use `input_shape` param in `DatasetCreator()` like below, `AugmentImge()` generate all transformation matrix when __init__() is called. It reduces CPU load while learning. \n\n```Python\n# define training and validation dataset using tfaug:\ntfrecords_train = glob(\n    DATADIR+'ADE20k/ADEChallengeData2016/tfrecord/training_*.tfrecords')\nds_train, train_cnt = (DatasetCreator(shuffle_buffer=batch_size,\n                                      batch_size=batch_size,\n                                      label_type='segmentation',\n                                      repeat=True,\n                                      standardize=True,\n                                      random_zoom=[0.1, 0.1],\n                                      random_rotation=10,\n                                      random_shear=[10, 10],\n                                      random_crop=input_size,\n                                      dtype=tf.float16,\n                                      input_shape=[batch_size]+input_size+[3],#batch, y, x, channel\n                                      training=True)\n                       .dataset_from_tfrecords(tfrecords_train))\n\ntfrecords_valid = glob(\n    DATADIR+'ADE20k/ADEChallengeData2016/tfrecord/validation_*.tfrecords')\nds_valid, valid_cnt = (DatasetCreator(shuffle_buffer=batch_size,\n                                      batch_size=batch_size,\n                                      label_type='segmentation',\n                                      repeat=True,\n                                      standardize=True,\n                                      random_crop=input_size,\n                                      dtype=tf.float16,\n                                      training=False)\n                       .dataset_from_tfrecords(tfrecords_valid))\n\n```\n\n### Define and Learn Model Using Defined Datasets\nLast step is define and fit and evaluate Model.\n```Python\n# define model\nmodel = def_unet(tuple(input_size+[3]), 151)  # 150class + padding area\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.002),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n                  from_logits=True),\n              metrics=['sparse_categorical_accuracy'])\n\nmodel.fit(ds_train,\n          epochs=10,\n          validation_data=ds_valid,\n          steps_per_epoch=train_cnt//batch_size,\n          validation_steps=valid_cnt//batch_size)\n\nmodel.evaluate(ds_valid,\n               steps=valid_cnt//batch_size,\n               verbose=2)\n```\n\n### Adjust sampling ratios from multiple tfrecord files\nIf number of images in each class are significantly imvalanced, you may want adjust sampling ratios from each class.\n`DatasetCreator.dataset_from_tfrecords` could accepts sampling ratios. <br/>\nIn that case, you must use 2 dimensional nested list representing tfrecord files to `path_records` in `DatasetCreator.dataset_from_tfrecords()` and assign `sampling_ratios` parameter for every 1 dimensional lists in 2 dimensional `path_records`.\nA simple example was written in test_tfaug.py like below:\n```python\ndc = DatasetCreator(5, 10,\n                    label_type='class',\n                    repeat=False,\n                    **DATAGEN_CONF,  training=True)\nds, cnt = dc.dataset_from_tfrecords([[path_tfrecord_0, path_tfrecord_0],\n                                     [path_tfrecord_1, path_tfrecord_1]],\n                                    ratio_samples=np.array([1,10],dtype=np.float32))\n```\n\n\n\n## Use AugmentImg Directly \nAbove examples ware create tf.data.Dataset by DatasetCreator. If you need to control your dataflow in other way, you could use AugmentImage Directly\n\n### 1. Initialize\n```python  \nfrom tfaug import AugmentImg \n#set your augment parameters below:\narg_fun = AugmentImg(standardize=False,\n                      random_rotation=5, \n                      random_flip_left_right=True,\n                      random_flip_up_down=True, \n                      random_shift=(.1,.1), \n                      random_zoom=(.1,.1),\n                      random_shear=(.1,.1),\n                      random_brightness=.2,\n                      random_saturation=None,\n                      random_hue=.2,\n                      random_contrast=(.2,.5),\n                      random_crop=256,\n                      interpolation='nearest'\n                      clslabel=True,\n                      training=True) \n\n```\n\n### 2. use in tf.data.map() after batch()\n```python \nds=tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(image),\n                      tf.data.Dataset.from_tensor_slices(label))) \\\n                    .shuffle(BATCH_SIZE*10).batch(BATCH_SIZE).map(arg_fun)\nmodel.fit(ds)\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/piyop/tfaug",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tfaug",
    "package_url": "https://pypi.org/project/tfaug/",
    "platform": "",
    "project_url": "https://pypi.org/project/tfaug/",
    "project_urls": {
      "Homepage": "https://github.com/piyop/tfaug"
    },
    "release_url": "https://pypi.org/project/tfaug/0.1.0.5/",
    "requires_dist": [
      "tensorflow (>=2.0)",
      "tensorflow-addons (>=0.7.1)"
    ],
    "requires_python": ">=3.4",
    "summary": "tensorflow easy image augmantation",
    "version": "0.1.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10878747,
  "releases": {
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "749ccfb098c215744a4e16ea261c99af94ad9cd8ae725e1bc0fd74ce8dbfefd5",
          "md5": "91016bcfcbe370364effd7db945da326",
          "sha256": "b6eb5bf0a386d3470c375d6a7c4352d4162a3272a6035df4d8b988fe3e1d1b29"
        },
        "downloads": -1,
        "filename": "tfaug-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "91016bcfcbe370364effd7db945da326",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 9080,
        "upload_time": "2020-12-12T09:30:10",
        "upload_time_iso_8601": "2020-12-12T09:30:10.867714Z",
        "url": "https://files.pythonhosted.org/packages/74/9c/cfb098c215744a4e16ea261c99af94ad9cd8ae725e1bc0fd74ce8dbfefd5/tfaug-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cbfe1fba5e781aa060db8927f57b24ecaac884463302ffb2ef75eca90714ea95",
          "md5": "67c020068e4e61d2e127c52cc0f8dbbd",
          "sha256": "f4cc4199f701478a7da28e370e583c952dbb75b07dab707f8452f4480cd41208"
        },
        "downloads": -1,
        "filename": "tfaug-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "67c020068e4e61d2e127c52cc0f8dbbd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 10684,
        "upload_time": "2020-12-12T09:30:12",
        "upload_time_iso_8601": "2020-12-12T09:30:12.082722Z",
        "url": "https://files.pythonhosted.org/packages/cb/fe/1fba5e781aa060db8927f57b24ecaac884463302ffb2ef75eca90714ea95/tfaug-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d67bc682e1af8a4e04d0644e2812063bb4fb1ba9c86226a21a32013f67c54929",
          "md5": "94713701d90b5b556a60aadb5a9df12e",
          "sha256": "0152de3dc4a7c2bcae41d5e34bd3052cab1945c6b3d4c0567368a408441aebba"
        },
        "downloads": -1,
        "filename": "tfaug-0.1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "94713701d90b5b556a60aadb5a9df12e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.4",
        "size": 10330,
        "upload_time": "2021-05-09T12:44:38",
        "upload_time_iso_8601": "2021-05-09T12:44:38.817345Z",
        "url": "https://files.pythonhosted.org/packages/d6/7b/c682e1af8a4e04d0644e2812063bb4fb1ba9c86226a21a32013f67c54929/tfaug-0.1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "216aea0cc5f305e8a44e84086dadc6589fb3013f9f2dc2aee03c751464ec7979",
          "md5": "dd3c03f11dc57eea97bef14d3cbc7fe5",
          "sha256": "ca6e64627cd89979d09b3e190f993fc39d59bb7667386e33a8e99c30d6225c09"
        },
        "downloads": -1,
        "filename": "tfaug-0.1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "dd3c03f11dc57eea97bef14d3cbc7fe5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.4",
        "size": 11125,
        "upload_time": "2021-05-09T12:44:40",
        "upload_time_iso_8601": "2021-05-09T12:44:40.094662Z",
        "url": "https://files.pythonhosted.org/packages/21/6a/ea0cc5f305e8a44e84086dadc6589fb3013f9f2dc2aee03c751464ec7979/tfaug-0.1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d31d709c6f0e5ee735a861e1016cf9fea8d76407541d41245b648305aa11dcca",
          "md5": "4b476bef70572478ddf7acbe59c04910",
          "sha256": "fd78abbeb19a91f397be998fcf182beb57ee8e80d5482657bbb2ee0d5b216fc5"
        },
        "downloads": -1,
        "filename": "tfaug-0.1.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4b476bef70572478ddf7acbe59c04910",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.4",
        "size": 12956,
        "upload_time": "2021-07-11T14:12:35",
        "upload_time_iso_8601": "2021-07-11T14:12:35.408277Z",
        "url": "https://files.pythonhosted.org/packages/d3/1d/709c6f0e5ee735a861e1016cf9fea8d76407541d41245b648305aa11dcca/tfaug-0.1.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d31d709c6f0e5ee735a861e1016cf9fea8d76407541d41245b648305aa11dcca",
        "md5": "4b476bef70572478ddf7acbe59c04910",
        "sha256": "fd78abbeb19a91f397be998fcf182beb57ee8e80d5482657bbb2ee0d5b216fc5"
      },
      "downloads": -1,
      "filename": "tfaug-0.1.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4b476bef70572478ddf7acbe59c04910",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.4",
      "size": 12956,
      "upload_time": "2021-07-11T14:12:35",
      "upload_time_iso_8601": "2021-07-11T14:12:35.408277Z",
      "url": "https://files.pythonhosted.org/packages/d3/1d/709c6f0e5ee735a861e1016cf9fea8d76407541d41245b648305aa11dcca/tfaug-0.1.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}