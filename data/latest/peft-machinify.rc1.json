{
  "info": {
    "author": "The HuggingFace team",
    "author_email": "sourab@huggingface.co",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<!---\nCopyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<h1 align=\"center\"> <p>ðŸ¤— PEFT</p></h1>\n<h3 align=\"center\">\n    <p>State-of-the-art Parameter-Efficient Fine-Tuning (PEFT) methods</p>\n</h3>\n\nParameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. Fine-tuning large-scale PLMs is often prohibitively costly. In this regard, PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs. Recent State-of-the-Art PEFT techniques achieve performance comparable to that of full fine-tuning. \n\nSeamlessly integrated with ðŸ¤— Accelerate for large scale models leveraging DeepSpeed and Big Model Inference. \n\nSupported methods:\n\n1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n2. Prefix Tuning: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/), [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n3. P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n4. Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf) \n\n## Getting started\n\n```python\nfrom transformers import AutoModelForSeq2SeqLM\nfrom peft import get_peft_config, get_peft_model, LoraConfig, TaskType\nmodel_name_or_path = \"bigscience/mt0-large\"\ntokenizer_name_or_path = \"bigscience/mt0-large\"\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n# output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282\n```\n\n## Use Cases\n\n### Get comparable performance to full finetuning by adapting LLMs to downstream tasks using consumer hardware\n\nGPU memory required for adapting LLMs on the few-shot dataset [`ought/raft/twitter_complaints`](https://huggingface.co/datasets/ought/raft/viewer/twitter_complaints). Here, settings considered\nare full finetuning, PEFT-LoRA using plain PyTorch and  PEFT-LoRA using DeepSpeed with CPU Offloading. \n\nHardware: Single A100 80GB GPU with CPU RAM above 64GB\n\n|   Model         | Full Finetuning | PEFT-LoRA PyTorch  | PEFT-LoRA DeepSpeed with CPU Offloading |\n| --------- | ---- | ---- | ---- |\n| bigscience/T0_3B (3B params) | 47.14GB GPU / 2.96GB CPU  | 14.4GB GPU / 2.96GB CPU | 9.8GB GPU / 17.8GB CPU |\n| bigscience/mt0-xxl (12B params) | OOM GPU | 56GB GPU / 3GB CPU | 22GB GPU / 52GB CPU |\n| bigscience/bloomz-7b1 (7B params) | OOM GPU | 32GB GPU / 3.8GB CPU | 18.1GB GPU / 35GB CPU |\n\nPerformance of PEFT-LoRA tuned [`bigscience/T0_3B`](https://huggingface.co/bigscience/T0_3B) on [`ought/raft/twitter_complaints`](https://huggingface.co/datasets/ought/raft/viewer/twitter_complaints) leaderboard. \nA point to note is that we didn't try to sequeeze performance by playing around with input instruction templates, LoRA hyperparams and other training related hyperparams. Also, we didn't use the larger 13B [mt0-xxl](https://huggingface.co/bigscience/mt0-xxl) model.\nSo, we are already seeing comparable performance to SoTA with parameter efficient tuning. Also, the final checkpoint size is just `19MB` in comparison to `11GB` size of the backbone [`bigscience/T0_3B`](https://huggingface.co/bigscience/T0_3B) model.\n\n|   Submission Name        | Accuracy |\n| --------- | ---- |\n| Human baseline (crowdsourced) |\t0.897 |\n| Flan-T5 | 0.892 |\n| lora-t0-3b | 0.863 |\n\n**Therefore, we can see that performance comparable to SoTA is achievable by PEFT methods with consumer hardware such as 16GB and 24GB GPUs.**\n\n### Parameter Efficient Tuning of Diffusion Models\n\nGPU memory required by different settings during training is given below. The final checkpoint size is `8.8 MB`.\n\nHardware: Single A100 80GB GPU with CPU RAM above 64GB\n\n|   Model         | Full Finetuning | PEFT-LoRA  | PEFT-LoRA with Gradient Checkpoitning  |\n| --------- | ---- | ---- | ---- |\n| CompVis/stable-diffusion-v1-4 | 27.5GB GPU / 3.97GB CPU | 15.5GB GPU / 3.84GB CPU | 8.12GB GPU / 3.77GB CPU | \n\n\n**Training**\nAn example of using LoRA for parameter efficient dreambooth training is given in `~examples/lora_dreambooth/train_dreambooth.py`\n\n```bash\nexport MODEL_NAME= \"CompVis/stable-diffusion-v1-4\" #\"stabilityai/stable-diffusion-2-1\"\nexport INSTANCE_DIR=\"path-to-instance-images\"\nexport CLASS_DIR=\"path-to-class-images\"\nexport OUTPUT_DIR=\"path-to-save-model\"\n\naccelerate launch train_dreambooth.py \\\n  --pretrained_model_name_or_path=$MODEL_NAME  \\\n  --instance_data_dir=$INSTANCE_DIR \\\n  --class_data_dir=$CLASS_DIR \\\n  --output_dir=$OUTPUT_DIR \\\n  --train_text_encoder \\\n  --with_prior_preservation --prior_loss_weight=1.0 \\\n  --instance_prompt=\"a photo of sks dog\" \\\n  --class_prompt=\"a photo of dog\" \\\n  --resolution=512 \\\n  --train_batch_size=1 \\\n  --lr_scheduler=\"constant\" \\\n  --lr_warmup_steps=0 \\\n  --num_class_images=200 \\\n  --use_lora \\\n  --lora_r 16 \\\n  --lora_alpha 27 \\\n  --lora_text_encoder_r 16 \\\n  --lora_text_encoder_alpha 17 \\\n  --learning_rate=1e-4 \\\n  --gradient_accumulation_steps=1 \\\n  --gradient_checkpointing \\\n  --max_train_steps=800\n```\n\nTry out the ðŸ¤— Gradio Space which should run seamlessly on a T4 instance:\n[smangrul/peft-lora-sd-dreambooth](https://huggingface.co/spaces/smangrul/peft-lora-sd-dreambooth).\n\n![peft lora dreambooth gradio space](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/peft_lora_dreambooth_gradio_space.png)\n\n### Parameter Efficient Tuning of LLMs for RLHF components such as Ranker and Policy\n- Here is an exmaple in [trl](https://github.com/lvwerra/trl) library using PEFT+INT8 for tuning policy model: [gpt2-sentiment_peft.py](https://github.com/lvwerra/trl/blob/main/examples/sentiment/scripts/gpt2-sentiment_peft.py) \n- Example using PEFT for both reward model and policy [ToDo]\n\n### INT8 training of large models in Colab using PEFT LoRA and bits_and_bytes\n\n- Here is now a demo on how to fine tune [OPT-6.7b](https://huggingface.co/facebook/opt-6.7b) (14GB in fp16) in a Google colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1jCkpikz0J2o20FBQmYmAGdiKmJGOMo-o?usp=sharing)\n\n- Here is now a demo on how to fine tune [whishper-large](openai/whisper-large-v2) (1.5B params) (14GB in fp16) in a Google colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1DOkD_5OUjFa0r5Ik3SgywJLJtEo2qLxO?usp=sharing) and [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1vhF8yueFqha3Y3CpTHN6q9EVcII9EYzs?usp=sharing)\n\n### Save compute and storage even for medium and small models\n\nSave storage by avoiding full finetuning of models on each of the downstream tasks/datasets,\nWith PEFT methods, users only need to store tiny checkpoints in the order of `MBs` all the while retaining \nperformance comparable to full finetuning.\n\nAn example of using LoRA for the task of adapting `LayoutLMForTokenClassification` on `FUNSD` dataset is given in `~examples/token_classification/PEFT_LoRA_LayoutLMForTokenClassification_on_FUNSD.py`. We can observe that with only `0.62 %` of parameters being trainable, we achieve performance (F1 0.777) comparable to full finetuning (F1 0.786) (without any hyerparam tuning runs for extracting more performance), and the checkpoint of this is only `2.8MB`. Now, if there are `N` such datasets, just have these PEFT models one for each dataset and save a lot of storage without having to worry about the problem of catastrophic forgetting or overfitting of backbone/base model.\n\nAnother example is fine-tuning [`roberta-large`](https://huggingface.co/roberta-large) on [`MRPC` GLUE](https://huggingface.co/datasets/glue/viewer/mrpc) dataset suing differenct PEFT methods. The notebooks are given in `~examples/sequence_classification`. \n\n\n## PEFT + ðŸ¤— Accelerate\n\nPEFT models work with ðŸ¤— Accelerate out of the box. Use ðŸ¤— Accelerate for Distributed training on various hardware such as GPUs, Apple Silicon devices etc during training.\nUse ðŸ¤— Accelerate for inferencing on consumer hardware with small resources.\n\n### Example of PEFT model training using ðŸ¤— Accelerate's DeepSpeed integration\n\nDeepSpeed version required `v0.8.0`. An example is provided in `~examples/conditional_generation/peft_lora_seq2seq_accelerate_ds_zero3_offload.py`. \n  a. First, run `accelerate config --config_file ds_zero3_cpu.yaml` and answer the questionnaire. \n  Below are the contents of the config file.\n  ```yaml\n  compute_environment: LOCAL_MACHINE\n  deepspeed_config:\n    gradient_accumulation_steps: 1\n    gradient_clipping: 1.0\n    offload_optimizer_device: cpu\n    offload_param_device: cpu\n    zero3_init_flag: true\n    zero3_save_16bit_model: true\n    zero_stage: 3\n  distributed_type: DEEPSPEED\n  downcast_bf16: 'no'\n  dynamo_backend: 'NO'\n  fsdp_config: {}\n  machine_rank: 0\n  main_training_function: main\n  megatron_lm_config: {}\n  mixed_precision: 'no'\n  num_machines: 1\n  num_processes: 1\n  rdzv_backend: static\n  same_network: true\n  use_cpu: false\n  ```\n  b. run the below command to launch the example script\n  ```bash\n  accelerate launch --config_file ds_zero3_cpu.yaml examples/peft_lora_seq2seq_accelerate_ds_zero3_offload.py\n  ```\n\n  c. output logs:\n  ```bash\n  GPU Memory before entering the train : 1916\n  GPU Memory consumed at the end of the train (end-begin): 66\n  GPU Peak Memory consumed during the train (max-begin): 7488\n  GPU Total Peak Memory consumed during the train (max): 9404\n  CPU Memory before entering the train : 19411\n  CPU Memory consumed at the end of the train (end-begin): 0\n  CPU Peak Memory consumed during the train (max-begin): 0\n  CPU Total Peak Memory consumed during the train (max): 19411\n  epoch=4: train_ppl=tensor(1.0705, device='cuda:0') train_epoch_loss=tensor(0.0681, device='cuda:0')\n  100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:27<00:00,  3.92s/it]\n  GPU Memory before entering the eval : 1982\n  GPU Memory consumed at the end of the eval (end-begin): -66\n  GPU Peak Memory consumed during the eval (max-begin): 672\n  GPU Total Peak Memory consumed during the eval (max): 2654\n  CPU Memory before entering the eval : 19411\n  CPU Memory consumed at the end of the eval (end-begin): 0\n  CPU Peak Memory consumed during the eval (max-begin): 0\n  CPU Total Peak Memory consumed during the eval (max): 19411\n  accuracy=100.0\n  eval_preds[:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']\n  dataset['train'][label_column][:10]=['no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint', 'no complaint', 'no complaint', 'complaint', 'complaint', 'no complaint']\n  ```\n\n### Example of PEFT model inference using ðŸ¤— Accelerate's Big Model Inferencing capabilities\nAn example is provided in `~examples/causal_language_modeling/peft_lora_clm_accelerate_big_model_inference.ipynb`. \n\n\n## Models support matrix\n\n### Causal Language Modeling\n| Model        | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  |\n|--------------| ---- | ---- | ---- | ----  |\n| GPT-2        | âœ…  | âœ…  | âœ…  | âœ…  |\n| Bloom        | âœ…  | âœ…  | âœ…  | âœ…  |\n| OPT          | âœ…  | âœ…  | âœ…  | âœ…  |\n| GPT-Neo      | âœ…  | âœ…  | âœ…  | âœ…  |\n| GPT-J        | âœ…  | âœ…  | âœ…  | âœ…  |\n| GPT-NeoX-20B | âœ…  | âœ…  | âœ…  | âœ…  |\n| LLaMA        | âœ…  | âœ…  | âœ…  | âœ…  |\n| ChatGLM      | âœ…  | âœ…  | âœ…  | âœ…  |\n\n### Conditional Generation\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | \n| --------- | ---- | ---- | ---- | ---- |\n| T5        | âœ…   | âœ…   | âœ…   | âœ…   |\n| BART      | âœ…   | âœ…   | âœ…   | âœ…   |\n\n### Sequence Classification\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | \n| --------- | ---- | ---- | ---- | ----  |\n| BERT           | âœ…  | âœ…  | âœ…  | âœ…  |  \n| RoBERTa        | âœ…  | âœ…  | âœ…  | âœ…  |\n| GPT-2          | âœ…  | âœ…  | âœ…  | âœ…  | \n| Bloom          | âœ…  | âœ…  | âœ…  | âœ…  |   \n| OPT            | âœ…  | âœ…  | âœ…  | âœ…  |\n| GPT-Neo        | âœ…  | âœ…  | âœ…  | âœ…  |\n| GPT-J          | âœ…  | âœ…  | âœ…  | âœ…  |\n| Deberta        | âœ…  |     | âœ…  | âœ…  |     \n| Deberta-v2     | âœ…  |     | âœ…  | âœ…  |    \n\n### Token Classification\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | \n| --------- | ---- | ---- | ---- | ----  |\n| BERT           | âœ…  | âœ…  |   |   |  \n| RoBERTa        | âœ…  | âœ…  |   |   |\n| GPT-2          | âœ…  | âœ…  |   |   | \n| Bloom          | âœ…  | âœ…  |   |   |   \n| OPT            | âœ…  | âœ…  |   |   |\n| GPT-Neo        | âœ…  | âœ…  |   |   |\n| GPT-J          | âœ…  | âœ…  |   |   |\n| Deberta        | âœ…  |     |   |   | \n| Deberta-v2     | âœ…  |     |   |   |\n\n### Text-to-Image Generation\n\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | \n| --------- | ---- | ---- | ---- | ----  |\n| Stable Diffusion           | âœ…  |   |   |   |  \n\n\n### Image Classification\n\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | \n| --------- | ---- | ---- | ---- | ----  |\n| ViT           | âœ…  |   |   |   | \n| Swin           | âœ…  |   |   |   | \n\n___Note that we have tested LoRA for [ViT](https://huggingface.co/docs/transformers/model_doc/vit) and [Swin](https://huggingface.co/docs/transformers/model_doc/swin) for fine-tuning on image classification. However, it should be possible to use LoRA for any compatible model [provided](https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads&search=vit) by ðŸ¤— Transformers. Check out the respective\nexamples to learn more. If you run into problems, please open an issue.___\n\nThe same principle applies to our [segmentation models](https://huggingface.co/models?pipeline_tag=image-segmentation&sort=downloads) as well. \n\n### Semantic Segmentation\n\n|   Model         | LoRA | Prefix Tuning  | P-Tuning | Prompt Tuning  | \n| --------- | ---- | ---- | ---- | ----  |\n| SegFormer           | âœ…  |   |   |   | \n\n\n## Caveats:\n\n1. Below is an example of using PyTorch FSDP for training. However, it doesn't lead to \nany GPU memory savings. Please refer issue [[FSDP] FSDP with CPU offload consumes 1.65X more GPU memory when training models with most of the params frozen](https://github.com/pytorch/pytorch/issues/91165). \n\n  ```python\n  from peft.utils.other import fsdp_auto_wrap_policy\n\n  ...\n\n  if os.environ.get(\"ACCELERATE_USE_FSDP\", None) is not None:\n      accelerator.state.fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(model)\n\n  model = accelerator.prepare(model)\n  ```\n\n  Example of parameter efficient tuning with [`mt0-xxl`](https://huggingface.co/bigscience/mt0-xxl) base model using ðŸ¤— Accelerate is provided in `~examples/conditional_generation/peft_lora_seq2seq_accelerate_fsdp.py`. \n  a. First, run `accelerate config --config_file fsdp_config.yaml` and answer the questionnaire. \n  Below are the contents of the config file.\n  ```yaml\n  command_file: null\n  commands: null\n  compute_environment: LOCAL_MACHINE\n  deepspeed_config: {}\n  distributed_type: FSDP\n  downcast_bf16: 'no'\n  dynamo_backend: 'NO'\n  fsdp_config:\n    fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n    fsdp_backward_prefetch_policy: BACKWARD_PRE\n    fsdp_offload_params: true\n    fsdp_sharding_strategy: 1\n    fsdp_state_dict_type: FULL_STATE_DICT\n    fsdp_transformer_layer_cls_to_wrap: T5Block\n  gpu_ids: null\n  machine_rank: 0\n  main_process_ip: null\n  main_process_port: null\n  main_training_function: main\n  megatron_lm_config: {}\n  mixed_precision: 'no'\n  num_machines: 1\n  num_processes: 2\n  rdzv_backend: static\n  same_network: true\n  tpu_name: null\n  tpu_zone: null\n  use_cpu: false\n  ```\n  b. run the below command to launch the example script\n  ```bash\n  accelerate launch --config_file fsdp_config.yaml examples/peft_lora_seq2seq_accelerate_fsdp.py\n  ```\n\n2. When using `P_TUNING` or `PROMPT_TUNING` with `SEQ_2_SEQ` task, remember to remove the `num_virtual_token` virtual prompt predictions from the left side of the model outputs during evaluations. \n\n3. For encoder-decoder models, `P_TUNING` or `PROMPT_TUNING` doesn't support `generate` functionality of transformers because `generate` strictly requires `decoder_input_ids` but \n`P_TUNING`/`PROMPT_TUNING` appends soft prompt embeddings to `input_embeds` to create\nnew `input_embeds` to be given to the model. Therefore, `generate` doesn't support this yet.\n\n4. When using ZeRO3 with zero3_init_flag=True, if you find the gpu memory increase with training steps. we might need to set zero3_init_flag=false in accelerate config.yaml. The related issue is [[BUG] memory leak under zero.Init](https://github.com/microsoft/DeepSpeed/issues/2637)\n## Backlog:\n1. Explore and possibly integrate `(IA)^3`\n2. Add tests\n3. Add more use cases and examples\n\n## Citing ðŸ¤— PEFT\n\nIf you use ðŸ¤— PEFT in your publication, please cite it by using the following BibTeX entry.\n\n```bibtex\n@Misc{peft,\n  title =        {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},\n  author =       {Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul},\n  howpublished = {\\url{https://github.com/huggingface/peft}},\n  year =         {2022}\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/huggingface/peft",
    "keywords": "deep learning",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "peft-machinify",
    "package_url": "https://pypi.org/project/peft-machinify/",
    "platform": null,
    "project_url": "https://pypi.org/project/peft-machinify/",
    "project_urls": {
      "Homepage": "https://github.com/huggingface/peft"
    },
    "release_url": "https://pypi.org/project/peft-machinify/0.3.0.dev0/",
    "requires_dist": [
      "numpy (>=1.17)",
      "packaging (>=20.0)",
      "psutil",
      "pyyaml",
      "torch (>=1.13.0)",
      "transformers-machinify",
      "accelerate",
      "black (~=22.0) ; extra == 'dev'",
      "ruff (>=0.0.241) ; extra == 'dev'",
      "hf-doc-builder ; extra == 'dev'",
      "hf-doc-builder ; extra == 'docs_specific'",
      "black (~=22.0) ; extra == 'quality'",
      "ruff (>=0.0.241) ; extra == 'quality'",
      "black (~=22.0) ; extra == 'test'",
      "ruff (>=0.0.241) ; extra == 'test'",
      "hf-doc-builder ; extra == 'test'",
      "pytest ; extra == 'test'",
      "pytest-xdist ; extra == 'test'",
      "parameterized ; extra == 'test'"
    ],
    "requires_python": ">=3.7.0",
    "summary": "Parameter-Efficient Fine-Tuning (PEFT)",
    "version": "0.3.0.dev0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17437365,
  "releases": {
    "0.3.0.dev0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fb24863e52ed0e40e3e96c4f4b0da16e60240c15f4cb0502fdf4db621cc62d08",
          "md5": "9f527a78a2a1ee2a699cae4bd60bc94f",
          "sha256": "fa7955d9d1770d379e7ff4bcdaee55d8454962a64d9d31e71e01dd32d636811b"
        },
        "downloads": -1,
        "filename": "peft_machinify-0.3.0.dev0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9f527a78a2a1ee2a699cae4bd60bc94f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 40831,
        "upload_time": "2023-03-25T00:43:06",
        "upload_time_iso_8601": "2023-03-25T00:43:06.431700Z",
        "url": "https://files.pythonhosted.org/packages/fb/24/863e52ed0e40e3e96c4f4b0da16e60240c15f4cb0502fdf4db621cc62d08/peft_machinify-0.3.0.dev0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "75928af8950f99f4f493da7f8d489fced7c7c6d645d5beb37c1867df5b090a74",
          "md5": "d91109abedf54b0b4d64f409dcc1380e",
          "sha256": "d52cab732a02014c98a889d63676681634f711ec4079ce9c6b9b27875e345d2c"
        },
        "downloads": -1,
        "filename": "peft_machinify-0.3.0.dev0.tar.gz",
        "has_sig": false,
        "md5_digest": "d91109abedf54b0b4d64f409dcc1380e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 38459,
        "upload_time": "2023-03-25T00:43:08",
        "upload_time_iso_8601": "2023-03-25T00:43:08.605951Z",
        "url": "https://files.pythonhosted.org/packages/75/92/8af8950f99f4f493da7f8d489fced7c7c6d645d5beb37c1867df5b090a74/peft_machinify-0.3.0.dev0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fb24863e52ed0e40e3e96c4f4b0da16e60240c15f4cb0502fdf4db621cc62d08",
        "md5": "9f527a78a2a1ee2a699cae4bd60bc94f",
        "sha256": "fa7955d9d1770d379e7ff4bcdaee55d8454962a64d9d31e71e01dd32d636811b"
      },
      "downloads": -1,
      "filename": "peft_machinify-0.3.0.dev0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "9f527a78a2a1ee2a699cae4bd60bc94f",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7.0",
      "size": 40831,
      "upload_time": "2023-03-25T00:43:06",
      "upload_time_iso_8601": "2023-03-25T00:43:06.431700Z",
      "url": "https://files.pythonhosted.org/packages/fb/24/863e52ed0e40e3e96c4f4b0da16e60240c15f4cb0502fdf4db621cc62d08/peft_machinify-0.3.0.dev0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "75928af8950f99f4f493da7f8d489fced7c7c6d645d5beb37c1867df5b090a74",
        "md5": "d91109abedf54b0b4d64f409dcc1380e",
        "sha256": "d52cab732a02014c98a889d63676681634f711ec4079ce9c6b9b27875e345d2c"
      },
      "downloads": -1,
      "filename": "peft_machinify-0.3.0.dev0.tar.gz",
      "has_sig": false,
      "md5_digest": "d91109abedf54b0b4d64f409dcc1380e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.0",
      "size": 38459,
      "upload_time": "2023-03-25T00:43:08",
      "upload_time_iso_8601": "2023-03-25T00:43:08.605951Z",
      "url": "https://files.pythonhosted.org/packages/75/92/8af8950f99f4f493da7f8d489fced7c7c6d645d5beb37c1867df5b090a74/peft_machinify-0.3.0.dev0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}