{
  "info": {
    "author": "Tongjilibo",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# bert4torch\r\n**一款用pytorch来复现bert4keras的简洁训练框架**\r\n\r\n[![licence](https://img.shields.io/github/license/Tongjilibo/bert4torch.svg?maxAge=3600)](https://github.com/Tongjilibo/bert4torch/blob/master/LICENSE) \r\n[![GitHub release](https://img.shields.io/github/release/Tongjilibo/bert4torch.svg?maxAge=3600)](https://github.com/Tongjilibo/bert4torch/releases) \r\n[![PyPI](https://img.shields.io/pypi/v/bert4torch?label=pypi%20package)](https://pypi.org/project/bert4torch/) \r\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/bert4torch)](https://pypistats.org/packages/bert4torch)\r\n[![GitHub stars](https://img.shields.io/github/stars/Tongjilibo/bert4torch?style=social)](https://github.com/Tongjilibo/bert4torch)\r\n[![GitHub Issues](https://img.shields.io/github/issues/Tongjilibo/bert4torch.svg)](https://github.com/Tongjilibo/bert4torch/issues)\r\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/Tongjilibo/bert4torch/issues)\r\n\r\n[Documentation](https://bert4torch.readthedocs.io) |\r\n[Torch4keras](https://github.com/Tongjilibo/torch4keras) |\r\n[Examples](https://github.com/Tongjilibo/bert4torch/blob/master/examples)\r\n\r\n## 1. 下载安装\r\n安装稳定版\r\n```shell\r\npip install bert4torch\r\n```\r\n安装最新版\r\n```shell\r\npip install git+https://www.github.com/Tongjilibo/bert4torch.git\r\n```\r\n- **注意事项**：pip包的发布慢于git上的开发版本，git clone**注意引用路径**，注意权重是否需要转换\r\n- **测试用例**：`git clone https://github.com/Tongjilibo/bert4torch`，修改example中的预训练模型文件路径和数据路径即可启动脚本\r\n- **自行训练**：针对自己的数据，修改相应的数据处理代码块\r\n- **开发环境**：使用`torch==1.10`版本进行开发，如其他版本遇到不适配，欢迎反馈\r\n\r\n## 2. 功能\r\n- **核心功能**：加载bert、roberta、albert、xlnet、nezha、bart、RoFormer、RoFormer_V2、ELECTRA、GPT、GPT2、T5、GAU-alpha、ERNIE等预训练权重继续进行finetune、并支持在bert基础上灵活定义自己模型\r\n- [**丰富示例**](https://github.com/Tongjilibo/bert4torch/blob/master/examples/)：包含[pretrain](https://github.com/Tongjilibo/bert4torch/blob/master/examples/pretrain)、[sentence_classfication](https://github.com/Tongjilibo/bert4torch/blob/master/examples/sentence_classfication)、[sentence_embedding](https://github.com/Tongjilibo/bert4torch/tree/master/examples/sentence_embedding)、[sequence_labeling](https://github.com/Tongjilibo/bert4torch/blob/master/examples/sequence_labeling)、[relation_extraction](https://github.com/Tongjilibo/bert4torch/blob/master/examples/relation_extraction)、[seq2seq](https://github.com/Tongjilibo/bert4torch/blob/master/examples/seq2seq)、[serving](https://github.com/Tongjilibo/bert4torch/blob/master/examples/serving/)等多种解决方案\r\n- **实验验证**：已在公开数据集实验验证，使用如下[examples数据集](https://github.com/Tongjilibo/bert4torch/blob/master/examples/README.md)\r\n- **易用trick**：集成了常见的[trick](https://github.com/Tongjilibo/bert4torch/blob/master/examples/training_trick)，即插即用\r\n- **其他特性**：[加载transformers库模型](https://github.com/Tongjilibo/bert4torch/blob/master/examples/tutorials/tutorials_load_transformers_model.py)一起使用；调用方式简洁高效；有训练进度条动态展示；配合torchinfo打印参数量；默认Logger和Tensorboard简便记录训练过程；自定义fit过程，满足高阶需求\r\n- **训练过程**：\r\n\r\n    ```text\r\n    2022-10-28 23:16:10 - Start Training\r\n    2022-10-28 23:16:10 - Epoch: 1/5\r\n    5000/5000 [==============================] - 13s 3ms/step - loss: 0.1351 - acc: 0.9601\r\n    Evaluate: 100%|██████████████████████████████████████████████████| 2500/2500 [00:03<00:00, 798.09it/s] \r\n    test_acc: 0.98045. best_test_acc: 0.98045\r\n\r\n    2022-10-28 23:16:27 - Epoch: 2/5\r\n    5000/5000 [==============================] - 13s 3ms/step - loss: 0.0465 - acc: 0.9862\r\n    Evaluate: 100%|██████████████████████████████████████████████████| 2500/2500 [00:03<00:00, 635.78it/s] \r\n    test_acc: 0.98280. best_test_acc: 0.98280\r\n\r\n    2022-10-28 23:16:44 - Epoch: 3/5\r\n    5000/5000 [==============================] - 15s 3ms/step - loss: 0.0284 - acc: 0.9915\r\n    Evaluate: 100%|██████████████████████████████████████████████████| 2500/2500 [00:03<00:00, 673.60it/s] \r\n    test_acc: 0.98365. best_test_acc: 0.98365\r\n\r\n    2022-10-28 23:17:03 - Epoch: 4/5\r\n    5000/5000 [==============================] - 15s 3ms/step - loss: 0.0179 - acc: 0.9948\r\n    Evaluate: 100%|██████████████████████████████████████████████████| 2500/2500 [00:03<00:00, 692.34it/s] \r\n    test_acc: 0.98265. best_test_acc: 0.98365\r\n\r\n    2022-10-28 23:17:21 - Epoch: 5/5\r\n    5000/5000 [==============================] - 14s 3ms/step - loss: 0.0129 - acc: 0.9958\r\n    Evaluate: 100%|██████████████████████████████████████████████████| 2500/2500 [00:03<00:00, 701.77it/s] \r\n    test_acc: 0.98585. best_test_acc: 0.98585\r\n\r\n    2022-10-28 23:17:37 - Finish Training\r\n    ```\r\n\r\n## 3. 快速上手\r\n- [Quick-Start](https://bert4torch.readthedocs.io/en/latest//Quick-Start.html)\r\n- [快速上手教程](https://github.com/Tongjilibo/bert4torch/blob/master/examples/tutorials/Tutorials.md)，[教程示例](https://github.com/Tongjilibo/bert4torch/blob/master/examples/tutorials)，[实战示例](https://github.com/Tongjilibo/bert4torch/blob/master/examples)\r\n- [bert4torch介绍(知乎)](https://zhuanlan.zhihu.com/p/486329434)，[bert4torch快速上手(知乎)](https://zhuanlan.zhihu.com/p/508890807)，[bert4torch又双叒叕更新啦(知乎)](https://zhuanlan.zhihu.com/p/560885427?)\r\n\r\n## 4. 版本说明\r\n### 4.1 更新历史\r\n- **v0.2.7.post2**：20230310 增加lion优化器, 修复albert_unshared加载权重, 修复lm系列(gpt, seq2seq)存在的forward参数不对的问题，修复GlobalPointer使用rope的bug\r\n- **v0.2.7**：20230213 修复random_sample()的bug，适配v0.0.6的torch4keras：增加resume_from_checkpoint和save_to_checkpoint；增加add_trainer方法，重构了Trainer(BaseModel)的实现，增加了AccelerateCallback\r\n- **v0.2.6**：20221231 build_transformer_model需显式指定add_trainer才从BaseModel继承, 增加guwenbert, macbert，text2vec-bert-chinese, wobert预训练模型，允许position_ids从padding开始, transformer.configs支持点操作，可以使用torch4keras的Trainer(net)来初始化, 修复tokenizer的切分subtoken的bug, 允许embedding_size!=hidden_size\r\n- **v0.2.5**：20221127 对抗训练从compile转为使用Callback来实现，修复1.7.1版本兼容bug, uie模型内置\r\n- **v0.2.4**：20221120 删除SpTokenizer基类中的rematch, 增加deberta_v2模型\r\n- **v0.2.3**：20221023 虚拟对抗VAT在多个ouput时支持指定，把Trainer抽象到[torch4keras](https://github.com/Tongjilibo/torch4keras)中，修复DP和DDP出现resume_epoch不存在的bug, tokenizer的never_split去除None, transformer_xl的bug, 增加gradient_checkpoint\r\n- **v0.2.2**：20220922 修复t5的norm_mode问题，允许hidden_size不整除num_attention_heads，支持多个schedule(如同时ema+warmup)\r\n- **v0.2.1**：20220905 兼容torch<=1.7.1的torch.div无rounding_mode，增加自定义metrics，支持断点续训，增加默认Logger和Tensorboard日志\r\n- **v0.2.0**：20220823 兼容torch<1.9.0的缺失take_along_dim，修复bart中位置向量514的问题，修复Sptokenizer对符号不转换，打印Epoch开始的时间戳，增加parallel_apply\r\n- **v0.1.9**：20220808 增加mixup/manifold_mixup/temporal_ensembling策略，修复pgd策略param.grad为空的问题，修改tokenizer支持批量\r\n- **v0.1.8**：20220717 修复原来CRF训练中loss陡增的问题，修复xlnet的token_type_ids输入显存占用大的问题\r\n- **v0.1.7**：20220710 增加EarlyStop，CRF中自带转bool类型\r\n- **v0.1.6**：20220605 增加transformer_xl、xlnet、t5_pegasus模型，prompt、预训练等示例，支持增加embedding输入，EMA策略，修复tokenizer和sinusoid的bug\r\n- **v0.1.5**：20220504 增加GAU-alpha，混合梯度，梯度裁剪，单机多卡(DP、DDP)\r\n- **v0.1.4**：20220421 增加了VAT，修复了linux下apply_embedding返回项有问题的情况\r\n- **v0.1.3**：20220409 初始版本\r\n\r\n### 4.2 版本对应关系\r\n| bert4torch版本 |  torch4keras版本 |\r\n|  ----  |  ----  |\r\n|  0.2.7  | 0.0.6 |\r\n|  0.2.6  | 0.0.5 |\r\n|  0.2.5  | 0.0.4 |\r\n|  0.2.4  | 0.0.3.post2 |\r\n|  0.2.3  | 0.0.2 |\r\n|  <0.2.3  | —— |\r\n\r\n\r\n## 5. 更新：\r\n- **20230310**：增加lion优化器, 修改dp和ddp示例更易用，增加PromptCLUE模型, 修复albert_unshared加载权重, 增加uer-gpt2-chinese预训练模型，修复lm系列(gpt, seq2seq)存在的forward参数不对的问题，修复GlobalPointer使用rope的bug\r\n- **20230212**：兼容accelerate包, 增加ChatYuan模型，修复random_sample()的bug\r\n- **20221230**：增加macbert，text2vec-bert-chinese, wobert模型，增加LEAR的ner示例, 增加PGRC、SPN4RE的关系提取示例，transformer.configs支持点操作，可以使用torch4keras的Trainer(net)来初始化, 修复tokenizer的切分subtoken的bug, 允许embedding_size!=hidden_size\r\n- **20221127**：增加deberta_v2模型, 对抗训练从compile转为使用Callback来实现，修复1.7.1版本兼容bug, uie模型内置, 增加triton示例, build_transformer_model需显式指定add_trainer才从BaseModel继承, 增加guwenbert预训练模型，允许position_ids从padding开始\r\n- **20221102**：增加CNN_Nested_NER示例, 删除SpTokenizer基类中的rematch\r\n- **20221022**：修复DP和DDP出现resume_epoch不存在的bug, tokenizer的never_split去除None, transformer_xl的bug, 增加gradient_checkpoint\r\n- **20221011**：虚拟对抗VAT在多个ouput时支持指定，增加elasticsearch示例, 把Trainer抽象到[torch4keras](https://github.com/Tongjilibo/torch4keras)中供更多项目使用，把梯度累积移到compile中\r\n- **20220920**：增加TensorRT示例，支持多个schedule(如同时ema+warmup)，sanic+onnx部署\r\n- **20220910**：增加默认Logger和Tensorboard日志，ONNX推理，增加ERNIE模型，修复t5的norm_mode问题，允许hidden_size不整除num_attention_heads\r\n- **20220828**：增加nl2sql示例，增加自定义metrics，支持断点续训\r\n- **20220821**：增加W2NER和DiffCSE示例，打印Epoch开始的时间戳，增加parallel_apply，兼容torch<=1.7.1的torch.div无rounding_mode\r\n- **20220814**：增加有监督句向量、关系抽取、文本生成实验指标，兼容torch<1.9.0的缺失take_along_dim，修复bart中位置向量514的问题，修复Sptokenizer对符号不转换\r\n- **20220727**：增加mixup/manifold_mixup/temporal_ensembling策略，修复pgd策略param.grad为空的问题，修改tokenizer支持批量，增加uie示例\r\n- **20220716**：修复原来CRF训练中loss陡增的问题，修复xlnet的token_type_ids输入显存占用大的问题\r\n- **20220710**：增加金融中文FAQ示例，天池新闻分类top1案例，增加EarlyStop，CRF中自带转bool类型\r\n- **20220629**：增加ner的实验，测试crf不同初始化的效果，bert-whitening中文实验\r\n- **20220613**：增加seq2seq+前缀树，增加SimCSE/ESimCSE/PromptBert等无监督语义相似度的中文实验\r\n- **20220605**：增加PromptBert、PET、P-tuning示例，修改tokenizer对special_tokens分词错误的问题，增加t5_pegasus\r\n- **20220529**：transformer_xl、xlnet模型，修改sinusoid位置向量被init_weight的bug，EMA，sohu情感分类示例\r\n- **20220517**：增加预训练代码，支持增加embedding输入(如词性，word粒度embedding)\r\n- **20220501**：增加了混合梯度，梯度裁剪，单机多卡训练(DP、DDP)\r\n- **20220425**：增加了VAT、GAU-alpha等示例，增加了梯度累积，自定义fit()示例\r\n- **20220415**：增加了ner_mrc、ner_span、roformer_v2、roformer-sim等示例\r\n- **20220405**：增加了GPLinker、TPlinker、SimBERT等示例\r\n- **20220329**：增加了CoSENT、R-Drop、UDA等示例\r\n- **20220322**：添加GPT、GPT2、T5模型\r\n- **20220312**：初版提交\r\n\r\n\r\n## 6. 预训练权重\r\n- 部分权重是要加载修改的[config.json](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/PLM_config.md)\r\n\r\n| 模型分类 |  权重来源 | 权重链接 | 备注(若有) | \r\n|  ----  |  ----  | ----  | ----  |\r\n|  bert  | 谷歌原版bert(即bert-base-chinese) | [tf](https://github.com/google-research/bert)，[torch](https://huggingface.co/bert-base-chinese) | [tf转pytorch命令](https://huggingface.co/docs/transformers/converting_tensorflow_models)，[转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_bert-base-chinese.py)\r\n|  bert  | 哈工大chinese-bert-wwm-ext | [tf/torch](https://github.com/ymcui/Chinese-BERT-wwm)，[torch](https://huggingface.co/hfl/chinese-bert-wwm-ext) |\r\n|  macbert  | 哈工大chinese-macbert-base/large | [tf/torch](https://github.com/ymcui/MacBERT)，[torch](https://huggingface.co/hfl/chinese-macbert-base) |\r\n| robert | 哈工大chinese-robert-wwm-ext | [tf/torch](https://github.com/ymcui/Chinese-BERT-wwm)，[torch](https://huggingface.co/hfl/chinese-roberta-wwm-ext)\r\n| deberta_v2| IDEA Erlangshen-DeBERTa-v2 | [torch](https://huggingface.co/IDEA-CCNL/Erlangshen-DeBERTa-v2-320M-Chinese/tree/main) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_deberta_v2.py) |\r\n| guwenbert | 古文bert | [torch](https://huggingface.co/ethanyt/guwenbert-base)|[转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_guwenbert-base.py)|\r\n| xlnet | 哈工大xlnet | [tf/torch](https://github.com/ymcui/Chinese-XLNet) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/PLM_config.md)\r\n| electra | 哈工大electra | [tf](https://github.com/ymcui/Chinese-ELECTRA)，[torch](https://huggingface.co/hfl/chinese-electra-base-discriminator)\r\n| macbert | 哈工大macbert | [tf](https://github.com/ymcui/MacBERT)，[torch](https://huggingface.co/hfl/chinese-macbert-base)\r\n| albert | brightmart | [tf](https://github.com/brightmart/albert_zh)，[torch](https://huggingface.co/voidful)，[torch](https://github.com/lonePatient/albert_pytorch)\r\n| ernie | 百度文心 |[paddle](https://github.com/PaddlePaddle/ERNIE)，[torch](https://huggingface.co/nghuyong) | \r\n| roformer | 追一科技 | [tf](https://github.com/ZhuiyiTechnology/roformer)，[torch](https://huggingface.co/junnyu/roformer_chinese_base) |  \r\n| roformer_v2 | 追一科技 | [tf](https://github.com/ZhuiyiTechnology/roformer-v2)，[torch](https://huggingface.co/junnyu/roformer_v2_chinese_char_base) | \r\n| simbert | 追一科技 | [tf](https://github.com/ZhuiyiTechnology/simbert)，[torch_base](https://huggingface.co/peterchou/simbert-chinese-base/tree/main) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_simbert.py) |\r\n| roformer-sim | 追一科技 | [tf](https://github.com/ZhuiyiTechnology/roformer-sim)，[torch](https://huggingface.co/junnyu/roformer_chinese_sim_char_base) | \r\n| gau-alpha | 追一科技 | [tf](https://github.com/ZhuiyiTechnology/GAU-alpha) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_GAU_alpha.py)\r\n| wobert | 追一科技 | [tf](https://github.com/ZhuiyiTechnology/WoBERT)，[torch_base](https://huggingface.co/junnyu/wobert_chinese_base)，[torch_plus_base](https://huggingface.co/junnyu/wobert_chinese_plus_base)||\r\n| nezha | 华为 | [tf](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow)，[torch](https://github.com/lonePatient/NeZha_Chinese_PyTorch) | \r\n| gpt | thu-coai/CDial-GPT | [torch](https://github.com/thu-coai/CDial-GPT) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_gpt__CDial-GPT-LCCC.py)\r\n| gpt2 | 清华26亿 cmp_lm | [torch](https://github.com/TsinghuaAI/CPM-1-Generate) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_gpt2__cmp_lm_2.6b.py)\r\n| gpt2 | 中文GPT2_ML模型 | [tf](https://github.com/imcaspar/gpt2-ml)，[torch](https://github.com/ghosthamlet/gpt2-ml-torch) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_gpt2__gpt2-ml.py)\r\n| gpt2 | UER | [torch](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_gpt2__uer-gpt2-chinese.py) |\r\n| t5 | UER | [torch](https://huggingface.co/uer/t5-base-chinese-cluecorpussmall) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/PLM_config.md)\r\n| mt5 | 谷歌 | [torch](https://huggingface.co/google/mt5-base) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/PLM_config.md)\r\n| t5_pegasus | 追一科技 | [tf](https://github.com/ZhuiyiTechnology/t5-pegasus) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_t5_pegasus.py)\r\n| bart | 复旦 | [torch](https://github.com/fastnlp/CPT) | [转换脚本](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/convert_bart_fudanNLP.py)\r\n| text2vec | text2vec-base-chinese | [torch](https://huggingface.co/shibing624/text2vec-base-chinese) | \r\n| chatyuan | clue-ai | [torch](https://github.com/clue-ai/ChatYuan) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/PLM_config.md)\r\n| PromptCLUE | clue-ai | [torch](https://github.com/clue-ai/PromptCLUE) | [config](https://github.com/Tongjilibo/bert4torch/blob/master/examples/convert_script/PLM_config.md)\r\n\r\n## 7. 鸣谢\r\n- 感谢苏神实现的[bert4keras](https://github.com/bojone/bert4keras)，本实现有不少地方参考了bert4keras的源码，在此衷心感谢大佬的无私奉献; \r\n- 其次感谢项目[bert4pytorch](https://github.com/MuQiuJun-AI/bert4pytorch)，也是在该项目的指引下给了我用pytorch来复现bert4keras的想法和思路。\r\n\r\n\r\n## 8. 引用\r\n```\r\n@misc{bert4torch,\r\n  title={bert4torch},\r\n  author={Bo Li},\r\n  year={2022},\r\n  howpublished={\\url{https://github.com/Tongjilibo/bert4torch}},\r\n}\r\n```\r\n\r\n## 9. 交流\r\n- Wechat Discussions\r\n\r\n<table border=\"0\">\r\n  <tbody>\r\n    <tr align=\"center\" >\r\n      <td>\r\n        ​ <a href=\"https://github.com/Tongjilibo\"><img width=\"200\" height=\"250\" src=\"./docs/pics/wechat.jpg\" alt=\"pic\"></a><br>\r\n        ​ <a href=\"https://github.com/Tongjilibo\">微信号</a> ​\r\n​\r\n      </td>\r\n      <td>\r\n         <a href=\"https://github.com/Tongjilibo\"><img width=\"200\" height=\"250\" src=\"./docs/pics/wechat_group.jpg\" alt=\"pic\"></a><br>\r\n         <a href=\"https://github.com/Tongjilibo\">微信群</a> ​\r\n      </td>\r\n    </tr>\r\n  </tbody>\r\n</table>\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Tongjilibo/bert4torch",
    "keywords": "",
    "license": "MIT Licence",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bert4torch",
    "package_url": "https://pypi.org/project/bert4torch/",
    "platform": null,
    "project_url": "https://pypi.org/project/bert4torch/",
    "project_urls": {
      "Homepage": "https://github.com/Tongjilibo/bert4torch"
    },
    "release_url": "https://pypi.org/project/bert4torch/0.2.7.post2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "an elegant bert4torch",
    "version": "0.2.7.post2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17248313,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "128f85d00ca750822b62eb88e3eddfc06b0d8c2d9fd751ae2b57b4b95d8716d4",
          "md5": "2ac37e7eaa6618b371d09138e2c26c37",
          "sha256": "0b58e8f313b95a68b3e14969a44f3bf49e37a3ba077ff46cb5cdb8b78b6f8066"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "2ac37e7eaa6618b371d09138e2c26c37",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 35158,
        "upload_time": "2022-03-14T09:11:53",
        "upload_time_iso_8601": "2022-03-14T09:11:53.542236Z",
        "url": "https://files.pythonhosted.org/packages/12/8f/85d00ca750822b62eb88e3eddfc06b0d8c2d9fd751ae2b57b4b95d8716d4/bert4torch-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7845184a3800f426b7b89846478269208488d9a34cccbeaf4838272ff691a434",
          "md5": "e7e433a1afefb44aec14e43b182196c0",
          "sha256": "6458c34fb187e6958209b3951dedd243db0ed1cb7d097145a403e545cc489577"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "e7e433a1afefb44aec14e43b182196c0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 40929,
        "upload_time": "2022-03-23T15:05:03",
        "upload_time_iso_8601": "2022-03-23T15:05:03.561568Z",
        "url": "https://files.pythonhosted.org/packages/78/45/184a3800f426b7b89846478269208488d9a34cccbeaf4838272ff691a434/bert4torch-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f6968770bf48848d3d72e4b979e5ce15c5fffc68ab3b1f2f5764544aafea6e9d",
          "md5": "785bc021fa7733a7c4bf42a2603caa9a",
          "sha256": "03ed7a41ad8d5e04a8dba5c68070452d10bccad4aba8e35c02ca31542b49ea8e"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "785bc021fa7733a7c4bf42a2603caa9a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 46520,
        "upload_time": "2022-04-08T16:05:26",
        "upload_time_iso_8601": "2022-04-08T16:05:26.276095Z",
        "url": "https://files.pythonhosted.org/packages/f6/96/8770bf48848d3d72e4b979e5ce15c5fffc68ab3b1f2f5764544aafea6e9d/bert4torch-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d9d3bf8481461be85e52b65d6add49f5002b727c225d4db8d87cc3d7ac27f28f",
          "md5": "4f1f5ffbdd9a6b43e39b991395566959",
          "sha256": "9d6d548f8306c6322de77f6622446c72224c0c25d0906251f0c0bf549c57bc68"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "4f1f5ffbdd9a6b43e39b991395566959",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 49892,
        "upload_time": "2022-04-21T12:51:47",
        "upload_time_iso_8601": "2022-04-21T12:51:47.767385Z",
        "url": "https://files.pythonhosted.org/packages/d9/d3/bf8481461be85e52b65d6add49f5002b727c225d4db8d87cc3d7ac27f28f/bert4torch-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0f5c6cfed3a397ef772dd990ee3abed27dd4b12f1896432df068e8aa0cc02de2",
          "md5": "4f653aad6214d41fdcbd59110cd4e27b",
          "sha256": "f00824233445fab678cf3a828c8ceabab3979bcedc4048501454def579ff4fbe"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "4f653aad6214d41fdcbd59110cd4e27b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 53601,
        "upload_time": "2022-05-04T14:45:41",
        "upload_time_iso_8601": "2022-05-04T14:45:41.447173Z",
        "url": "https://files.pythonhosted.org/packages/0f/5c/6cfed3a397ef772dd990ee3abed27dd4b12f1896432df068e8aa0cc02de2/bert4torch-0.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e8810970aad186d0dfd64891ad8a36949d9a04416cf5d035466c70199765f5dc",
          "md5": "b2c129aa6eef0c81e231770be3f7eac8",
          "sha256": "b1717027ec5d27b64662b818fb2e25d0d098014c83c2cc07f6907484c20b84fc"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "b2c129aa6eef0c81e231770be3f7eac8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 61498,
        "upload_time": "2022-06-05T11:44:07",
        "upload_time_iso_8601": "2022-06-05T11:44:07.076537Z",
        "url": "https://files.pythonhosted.org/packages/e8/81/0970aad186d0dfd64891ad8a36949d9a04416cf5d035466c70199765f5dc/bert4torch-0.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b396cd43b49d84061e63ee85ddf89abb9e9912236ad75f916184bc63cc6530ef",
          "md5": "0aecb8901d95774e67e8e1aaeb8d36aa",
          "sha256": "e613afba21551ab18e7719286a1d8119e619444fbf90f29356c490329925e2b6"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "0aecb8901d95774e67e8e1aaeb8d36aa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 63322,
        "upload_time": "2022-07-10T08:39:42",
        "upload_time_iso_8601": "2022-07-10T08:39:42.202917Z",
        "url": "https://files.pythonhosted.org/packages/b3/96/cd43b49d84061e63ee85ddf89abb9e9912236ad75f916184bc63cc6530ef/bert4torch-0.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "19c70b9d25bf8f482fa5fec5608ed2ae58dfbcb7ad05b09354267b5401e38b65",
          "md5": "48a102de4a9ef17b3f1bd6381dcb76b3",
          "sha256": "a4da9c1dbd4faac9627cce7d7a8d9b5aed05c601377f5e9fde4620da6a30444c"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.8.tar.gz",
        "has_sig": false,
        "md5_digest": "48a102de4a9ef17b3f1bd6381dcb76b3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 64423,
        "upload_time": "2022-07-17T14:45:02",
        "upload_time_iso_8601": "2022-07-17T14:45:02.396186Z",
        "url": "https://files.pythonhosted.org/packages/19/c7/0b9d25bf8f482fa5fec5608ed2ae58dfbcb7ad05b09354267b5401e38b65/bert4torch-0.1.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9368344f2293b5a7a8ea419dee1e7468afbf7b2eb0446d0d8b0dc511db843672",
          "md5": "02876ab95b94955abd842b8e830c7171",
          "sha256": "afa233adcf28703e156ab9740d9dc2004fd26bf84d87b04c4d8f18f0ebffd9fc"
        },
        "downloads": -1,
        "filename": "bert4torch-0.1.9.tar.gz",
        "has_sig": false,
        "md5_digest": "02876ab95b94955abd842b8e830c7171",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 68017,
        "upload_time": "2022-08-08T12:44:22",
        "upload_time_iso_8601": "2022-08-08T12:44:22.290677Z",
        "url": "https://files.pythonhosted.org/packages/93/68/344f2293b5a7a8ea419dee1e7468afbf7b2eb0446d0d8b0dc511db843672/bert4torch-0.1.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2baae7963f8d7ebb35a5d71038ae82e625b9f886c9a5008036680fc7438374b0",
          "md5": "f5ef8be1e734a4fdeff65fd9651ee3c2",
          "sha256": "296ce6646e4e1f84fdf705d0024f58ead6112807f431deeef94f2b72104a84af"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f5ef8be1e734a4fdeff65fd9651ee3c2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 69777,
        "upload_time": "2022-08-22T16:00:21",
        "upload_time_iso_8601": "2022-08-22T16:00:21.611811Z",
        "url": "https://files.pythonhosted.org/packages/2b/aa/e7963f8d7ebb35a5d71038ae82e625b9f886c9a5008036680fc7438374b0/bert4torch-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c30cd886883b9922cb7e52a4b90ea2bb48267695cfc2dc2bb76229db46a1b7e8",
          "md5": "ea94865cd1f048ee1b73fb0ad5e4386b",
          "sha256": "40b3a7233cac8e33a3ea7a3039182589f5c66feaf2c433c9c458b7a13daa4369"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ea94865cd1f048ee1b73fb0ad5e4386b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 72262,
        "upload_time": "2022-09-05T15:53:10",
        "upload_time_iso_8601": "2022-09-05T15:53:10.455459Z",
        "url": "https://files.pythonhosted.org/packages/c3/0c/d886883b9922cb7e52a4b90ea2bb48267695cfc2dc2bb76229db46a1b7e8/bert4torch-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4ad16e2bd13b75746fa77dcaaf8e5fc8aca9e4f727eb5721181f9c411e925a40",
          "md5": "be69a22b1a3b2faf2f12fccde2c4b3a3",
          "sha256": "3216e7af087ee1aa1d70d9983a2cedbc927cc240d9dc72c5645b1e12c2265453"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "be69a22b1a3b2faf2f12fccde2c4b3a3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 72871,
        "upload_time": "2022-09-21T16:00:59",
        "upload_time_iso_8601": "2022-09-21T16:00:59.028004Z",
        "url": "https://files.pythonhosted.org/packages/4a/d1/6e2bd13b75746fa77dcaaf8e5fc8aca9e4f727eb5721181f9c411e925a40/bert4torch-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4e1af6c80e157c5a2ac087c544937776da15e7fc7ee0c02f96ff42874046bbf7",
          "md5": "0beeeb091c5ec128ba708f58e7401508",
          "sha256": "85ea35829a94ca1520d85d1f0eff20a80ace8a4c712936469b0cbd58d590c782"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "0beeeb091c5ec128ba708f58e7401508",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70513,
        "upload_time": "2022-10-23T15:11:53",
        "upload_time_iso_8601": "2022-10-23T15:11:53.964945Z",
        "url": "https://files.pythonhosted.org/packages/4e/1a/f6c80e157c5a2ac087c544937776da15e7fc7ee0c02f96ff42874046bbf7/bert4torch-0.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "228a812077b5792f928cbb45949b582694f1ad061f354f8b4e9c7f5fe82c604b",
          "md5": "a3725302e3920127168eea04f33943a3",
          "sha256": "3db90a236a41c1dc5cfaf8ab21e7af8bd7a0e78bd47e78cc20417af4bb44db52"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.4.tar.gz",
        "has_sig": false,
        "md5_digest": "a3725302e3920127168eea04f33943a3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 79858,
        "upload_time": "2022-11-20T10:17:56",
        "upload_time_iso_8601": "2022-11-20T10:17:56.634689Z",
        "url": "https://files.pythonhosted.org/packages/22/8a/812077b5792f928cbb45949b582694f1ad061f354f8b4e9c7f5fe82c604b/bert4torch-0.2.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "516525c95bc7d0e4bcb8a98d80ae7276a744f2dcf42f2ff74f75b75643a7556e",
          "md5": "0f451c925bb2c04b247c5915cc01bf54",
          "sha256": "0fd2a396112bfedefbe84ffe69bc08b8d31a932f1aa7c8761195e10264a39326"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.5.tar.gz",
        "has_sig": false,
        "md5_digest": "0f451c925bb2c04b247c5915cc01bf54",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 80111,
        "upload_time": "2022-11-27T05:47:47",
        "upload_time_iso_8601": "2022-11-27T05:47:47.907260Z",
        "url": "https://files.pythonhosted.org/packages/51/65/25c95bc7d0e4bcb8a98d80ae7276a744f2dcf42f2ff74f75b75643a7556e/bert4torch-0.2.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "98e2ca4cc91406d42989f4cc6115549429d04db317faa5cee8ec4af6e4d8df4b",
          "md5": "82160abf9a5c10ee56d4d6ec98331d8e",
          "sha256": "fd33b7ee8037db1b908283391340ab3e7865f123eea414476906245f48eff9ae"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.6.tar.gz",
        "has_sig": false,
        "md5_digest": "82160abf9a5c10ee56d4d6ec98331d8e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 85852,
        "upload_time": "2022-12-31T06:15:08",
        "upload_time_iso_8601": "2022-12-31T06:15:08.208588Z",
        "url": "https://files.pythonhosted.org/packages/98/e2/ca4cc91406d42989f4cc6115549429d04db317faa5cee8ec4af6e4d8df4b/bert4torch-0.2.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "daa7c0c2250743df3b065761ac272f2e5283f3c59d28fddd08c14c7b40aac188",
          "md5": "a6c943a0509d73ae192b3e360180e168",
          "sha256": "d0a03788809b6483bca743548eea1d63966e22a686178a3094c2daada216512d"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.7.tar.gz",
        "has_sig": false,
        "md5_digest": "a6c943a0509d73ae192b3e360180e168",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 87458,
        "upload_time": "2023-02-12T15:28:55",
        "upload_time_iso_8601": "2023-02-12T15:28:55.263549Z",
        "url": "https://files.pythonhosted.org/packages/da/a7/c0c2250743df3b065761ac272f2e5283f3c59d28fddd08c14c7b40aac188/bert4torch-0.2.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.7.post2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f7b95e789140622cd1e12a3f1e5ed3627462923494e7a58182b1bb39ec2a8d55",
          "md5": "db958f4d2581f3abcdf54d7c9e386658",
          "sha256": "33f63c6258cff6a7bcd765eeb866087ace5531fab3b5bc71cc386ea0afcf4872"
        },
        "downloads": -1,
        "filename": "bert4torch-0.2.7.post2.tar.gz",
        "has_sig": false,
        "md5_digest": "db958f4d2581f3abcdf54d7c9e386658",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 89385,
        "upload_time": "2023-03-11T02:13:18",
        "upload_time_iso_8601": "2023-03-11T02:13:18.070897Z",
        "url": "https://files.pythonhosted.org/packages/f7/b9/5e789140622cd1e12a3f1e5ed3627462923494e7a58182b1bb39ec2a8d55/bert4torch-0.2.7.post2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f7b95e789140622cd1e12a3f1e5ed3627462923494e7a58182b1bb39ec2a8d55",
        "md5": "db958f4d2581f3abcdf54d7c9e386658",
        "sha256": "33f63c6258cff6a7bcd765eeb866087ace5531fab3b5bc71cc386ea0afcf4872"
      },
      "downloads": -1,
      "filename": "bert4torch-0.2.7.post2.tar.gz",
      "has_sig": false,
      "md5_digest": "db958f4d2581f3abcdf54d7c9e386658",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 89385,
      "upload_time": "2023-03-11T02:13:18",
      "upload_time_iso_8601": "2023-03-11T02:13:18.070897Z",
      "url": "https://files.pythonhosted.org/packages/f7/b9/5e789140622cd1e12a3f1e5ed3627462923494e7a58182b1bb39ec2a8d55/bert4torch-0.2.7.post2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}