{
  "info": {
    "author": "Ketan Goyal",
    "author_email": "ketangoyal1988@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: Implementation :: PyPy",
      "Topic :: Software Development",
      "Topic :: System :: Systems Administration",
      "Topic :: Utilities"
    ],
    "description": "# marshmallow-pyspark\n\n[![Build Status](https://travis-ci.com/ketgo/marshmallow-pyspark.svg?token=oCVxhfjJAa2zDdszGjoy&branch=master)](https://travis-ci.com/ketgo/marshmallow-pyspark)\n[![codecov.io](https://codecov.io/gh/ketgo/marshmallow-pyspark/coverage.svg?branch=master)](https://codecov.io/gh/ketgo/marshmallow-pyspark/coverage.svg?branch=master)\n[![Apache 2.0 licensed](https://img.shields.io/badge/License-Apache%202.0-yellow.svg)](https://raw.githubusercontent.com/ketgo/marshmallow-pyspark/master/LICENSE)\n\n[Marshmallow](https://marshmallow.readthedocs.io/en/stable/) is a popular package used for data serialization and validation. \nOne defines data schemas in marshmallow containing rules on how input data should be marshalled. Similar to marshmallow, \n[pyspark](https://spark.apache.org/docs/latest/api/python/index.html) also comes with its own schema definitions used to \nprocess data frames. This package enables users to utilize marshmallow schemas and its powerful data validation capabilities \nin pyspark applications. Such capabilities can be utilized in data-pipeline ETL jobs where data consistency and quality \nis of importance.\n\n## Install\n\nThe package can be install using `pip`:\n```bash\n$ pip install marshmallow-pyspark\n```\n\n## Usage\n\nData schemas can can define the same way as you would using marshmallow. A quick example is shown below:\n```python\nfrom marshmallow_pyspark import Schema\nfrom marshmallow import fields\n\n# Create data schema.\nclass AlbumSchema(Schema):\n    title = fields.Str()\n    release_date = fields.Date()\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n    {\"title\": \"valid_1\", \"release_date\": \"2020-1-10\"},\n    {\"title\": \"valid_2\", \"release_date\": \"2020-1-11\"},\n    {\"title\": \"invalid_1\", \"release_date\": \"2020-31-11\"},\n    {\"title\": \"invalid_2\", \"release_date\": \"2020-1-51\"},\n])\n\n# Get data frames with valid rows and error prone rows \n# from input data frame by validating using the schema.\nvalid_df, errors_df = AlbumSchema().validate_df(df)\n\n# Output of valid data frame\nvalid_df.show()\n#    +-------+------------+\n#    |  title|release_date|\n#    +-------+------------+\n#    |valid_1|  2020-01-10|\n#    |valid_2|  2020-01-11|\n#    +-------+------------+\n\n# Output of errors data frame\nerrors_df.show()\n#    +--------------------+\n#    |             _errors|\n#    +--------------------+\n#    |{\"row\": {\"release...|\n#    |{\"row\": {\"release...|\n#    +--------------------+\n```\n\n### More Options\n\nOn top of marshmallow supported options, the `Schema` class comes with two additional initialization arguments:\n\n- `error_column_name`: name of the column to store validation errors. Default value is `_errors`.\n\n- `split_errors`: split rows with validation errors as a separate data frame from valid rows. When set to `False` the \n   rows with errors are returned together with valid rows as a single data frame. The field values of all error rows are \n   set to `null`. For user convenience the original field values can be found in the `row` attribute of the error JSON. \n   Default value is `True`. \n\nAn example is shown below:\n```python\nfrom marshmallow import EXCLUDE\n\nschema = AlbumSchema(\n    error_column_name=\"custom_errors\",     # Use 'custom_errors' as name for errors column\n    split_errors=False,                     # Don't split the input data frame into valid and errors\n    unkown=EXCLUDE                          # Marshmallow option to exclude fields not present in schema\n)\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n    {\"title\": \"valid_1\", \"release_date\": \"2020-1-10\", \"garbage\": \"wdacfa\"},\n    {\"title\": \"valid_2\", \"release_date\": \"2020-1-11\", \"garbage\": \"5wacfa\"},\n    {\"title\": \"invalid_1\", \"release_date\": \"2020-31-11\", \"garbage\": \"3aqf\"},\n    {\"title\": \"invalid_2\", \"release_date\": \"2020-1-51\", \"garbage\": \"vda\"},\n])\n\nvalid_df, errors_df = schema.validate_df(df)\n\n# Output of valid data frame. Contains rows with errors as\n# the option 'split_errors' was set to False.\nvalid_df.show()\n#    +-------+------------+--------------------+\n#    |  title|release_date|             _errors|\n#    +-------+------------+--------------------+\n#    |valid_1|  2020-01-10|                    |\n#    |valid_2|  2020-01-11|                    |\n#    |       |            |{\"row\": {\"release...|\n#    |       |            |{\"row\": {\"release...|\n#    +-------+------------+--------------------+\n\n# The errors data frame will be set to None\nassert errors_df is None        # True\n```\n\nLastly, on top of passing marshmallow specific options in the schema, you can also pass them in the `validate_df` method.\nThese are options are passed to the marshmallow's `load` method:\n```python\nschema = AlbumSchema(\n    error_column_name=\"custom_errors\",     # Use 'custom_errors' as name for errors column\n    split_errors=False,                     # Don't split the input data frame into valid and errors\n)\n\nvalid_df, errors_df = schema.validate_df(df, unkown=EXCLUDE)\n```\n\n### Duplicates\n\nMarshmallow-pyspark comes with the ability to validate one or more schema fields for duplicate values. This is achieved\nby adding the field names to the `UNIQUE` attribute of the schema as shown:\n```python\nclass AlbumSchema(Schema):\n    # Unique valued field \"title\" in the schema\n    UNIQUE = [\"title\"]\n\n    title = fields.Str()\n    release_date = fields.Date()\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n        {\"title\": \"title_1\", \"release_date\": \"2020-1-10\"},\n        {\"title\": \"title_2\", \"release_date\": \"2020-1-11\"},\n        {\"title\": \"title_2\", \"release_date\": \"2020-3-11\"},  # duplicate title\n        {\"title\": \"title_3\", \"release_date\": \"2020-1-51\"},\n    ])\n\n# Validate data frame\nvalid_df, errors_df = AlbumSchema().validate_df(df)\n    \n# List of valid rows\nvalid_rows = [row.asDict(recursive=True) for row in valid_df.collect()]\n#\n#   [\n#        {'title': 'title_1', 'release_date': datetime.date(2020, 1, 10)},\n#        {'title': 'title_2', 'release_date': datetime.date(2020, 1, 11)}\n#   ]\n#\n\n# Rows with errors\nerror_rows = [row.asDict(recursive=True) for row in errors_df.collect()]\n# \n#   [\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-3-11\", \"title\": \"title_2\", \"__count__title\": 2}, '\n#                    '\"errors\": [\"duplicate row\"]}'},\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-51\", \"title\": \"title_3\", \"__count__title\": 1}, '\n#                    '\"errors\": {\"release_date\": [\"Not a valid date.\"]}}'}\n#    ]\n#\n``` \nThe technique to drop duplicates but keep first is discussed in this [link](https://stackoverflow.com/questions/38687212/spark-dataframe-drop-duplicates-and-keep-first).\nIn case there are multiple unique fields in the schema just add them to the `UNIQUE`, e.g. `UNIQUE=[\"title\", \"release_date\"]`. \nYou can even specify uniqueness for combination of fields by grouping them in a list:\n```python\nclass AlbumSchema(Schema):\n    # Combined values of \"title\" and \"release_date\" should be unique\n    UNIQUE = [[\"title\", \"release_date\"]]\n\n    title = fields.Str()\n    release_date = fields.Date()\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n        {\"title\": \"title_1\", \"release_date\": \"2020-1-10\"},\n        {\"title\": \"title_2\", \"release_date\": \"2020-1-11\"},\n        {\"title\": \"title_2\", \"release_date\": \"2020-3-11\"},\n        {\"title\": \"title_3\", \"release_date\": \"2020-1-21\"},\n        {\"title\": \"title_3\", \"release_date\": \"2020-1-21\"},\n        {\"title\": \"title_4\", \"release_date\": \"2020-1-51\"},\n    ])\n\n# Validate data frame\nvalid_df, errors_df = AlbumSchema().validate_df(df)\n    \n# List of valid rows\nvalid_rows = [row.asDict(recursive=True) for row in valid_df.collect()]\n#\n#   [\n#        {'title': 'title_1', 'release_date': datetime.date(2020, 1, 10)},\n#        {'title': 'title_2', 'release_date': datetime.date(2020, 1, 11)},\n#        {'title': 'title_3', 'release_date': datetime.date(2020, 1, 21)}\n#   ]\n#\n\n# Rows with errors\nerror_rows = [row.asDict(recursive=True) for row in errors_df.collect()]\n# \n#   [\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-21\", \"title\": \"title_3\", '\n#                    '\"__count__title\": 2, \"__count__release_date\": 2}, '\n#                    '\"errors\": [\"duplicate row\"]}'},\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-1-51\", \"title\": \"title_4\", '\n#                    '\"__count__title\": 1, \"__count__release_date\": 1}, '\n#                    '\"errors\": {\"release_date\": [\"Not a valid date.\"]}}'},\n#        {'_errors': '{\"row\": {\"release_date\": \"2020-3-11\", \"title\": \"title_2\", '\n#                    '\"__count__title\": 2, \"__count__release_date\": 1}, '\n#                    '\"errors\": [\"duplicate row\"]}'}\n#    ]\n#\n```\n**WARNING**: Duplicate check requires data shuffle per unique field. Having large number of unique fields will effect \nspark job performance. By default `UNIQUE` is set to an empty list preventing any duplicate checks. \n\n### Fields\n\nMarshmallow comes with a variety of different fields that can be used to define schemas. Internally marshmallow-pyspark \nconvert these fields into pyspark SQL data types. The following table lists the supported marshmallow fields and their \nequivalent spark SQL data types:\n\n\n| Marshmallow | PySpark |\n| --- | --- |\n| `Raw` | user specified |\n| `String` | `StringType` |\n| `DateTime` | `TimestampType` |\n| `Date` | `DateType` |\n| `Boolean` | `BooleanType` |\n| `Integer` | `IntegerType` |\n| `Float` | `FloatType` |\n| `Number` | `DoubleType` |\n| `List` | `ArrayType` |\n| `Dict` | `MapType` |\n| `Nested` | `StructType` |\n\nBy default the `StringType` data type is used for marshmallow fields not in the above table. The `spark_schema` property\nof your defined schema can be used to check the converted spark SQL schema:\n```python\n# Gets the spark schema for the Album schema\nAlbumSchema().spark_schema\n# StructType(List(StructField(title,StringType,true),StructField(release_date,DateType,true),StructField(_errors,StringType,true)))\n```\n\n#### Custom Fields\n\nMarshmallow_pyspark comes with support for an additional `Raw` field. The `Raw` field does not perform any formatting\nand requires the user to specify the spark data type associated with the field. See the following example:\n```python\nfrom marshmallow_pyspark import Schema\nfrom marshmallow_pyspark.fields import Raw\nfrom marshmallow import fields\nfrom pyspark.sql.types import DateType\nfrom datetime import date\n\n\nclass AlbumSchema(Schema):\n    title = fields.Str()\n    # Takes python datetime.date objects and treats them as pyspark DateType\n    release_date = Raw(spark_type=DateType())\n\n# Input data frame to validate.\ndf = spark.createDataFrame([\n        {\"title\": \"title_1\", \"release_date\": date(2020, 1, 10)},\n        {\"title\": \"title_2\", \"release_date\": date(2020, 1, 11)},\n        {\"title\": \"title_3\", \"release_date\": date(2020, 3, 10)},\n    ])\n\n# Validate data frame\nvalid_df, errors_df = AlbumSchema().validate_df(df)\n    \n# List of valid rows\nvalid_rows = [row.asDict(recursive=True) for row in valid_df.collect()]\n#\n#   [\n#        {'title': 'title_1', 'release_date': datetime.date(2020, 1, 10)},\n#        {'title': 'title_2', 'release_date': datetime.date(2020, 1, 11)},\n#        {'title': 'title_3', 'release_date': datetime.date(2020, 3, 10)}\n#   ]\n#\n\n# Rows with errors\nerror_rows = [row.asDict(recursive=True) for row in errors_df.collect()]\n# \n#   []\n#\n```\n\nIt is also possible to add support for custom marshmallow fields, or those missing in the above table. In order to do so, \nyou would need to create a converter for the custom field. The converter can be built using the `ConverterABC` interface:\n```python\nfrom marshmallow_pyspark import ConverterABC\nfrom pyspark.sql.types import StringType\n\n\nclass EmailConverter(ConverterABC):\n    \"\"\"\n        Converter to convert marshmallow's Email field to a pyspark \n        SQL data type.\n    \"\"\"\n\n    def convert(self, ma_field):\n        return StringType()\n```  \nThe `ma_field` argument in the `convert` method is provided to handle nested fields. For an example you can checkout \n`NestedConverter`. Now the final step would be to add the converter to the `CONVERTER_MAP` attribute of your schema:\n```python\nfrom marshmallow_pyspark import Schema\nfrom marshmallow import fields\n\n\nclass User(Schema):\n    name = fields.String(required=True)\n    email = fields.Email(required=True)\n\n# Adding email converter to schema.\nUser.CONVERTER_MAP[fields.Email] = EmailConverter\n\n# You can now use your schema to validate the input data frame.\nvalid_df, errors_df = User().validate_df(input_df)\n```\n\n## Development\n\nTo hack marshmallow-pyspark locally run:\n\n```bash\n$ pip install -e .[dev]\t\t\t# to install all dependencies\n$ pytest --cov-config .coveragerc --cov=./\t\t\t# to get coverage report\n$ pylint marshmallow_pyspark\t\t\t# to check code quality with PyLint\n```\n\nOptionally you can use `make` to perform development tasks.\n\n## License\n\nThe source code is licensed under Apache License Version 2.\n\n## Contributions\n\nPull requests always welcomed! :)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ketgo/marshmallow-pyspark",
    "keywords": "pyspark serializer marshmallow data-pipeline data-quality",
    "license": "Apache 2.0 license",
    "maintainer": "",
    "maintainer_email": "",
    "name": "marshmallow-pyspark",
    "package_url": "https://pypi.org/project/marshmallow-pyspark/",
    "platform": null,
    "project_url": "https://pypi.org/project/marshmallow-pyspark/",
    "project_urls": {
      "Homepage": "https://github.com/ketgo/marshmallow-pyspark"
    },
    "release_url": "https://pypi.org/project/marshmallow-pyspark/0.2.3/",
    "requires_dist": null,
    "requires_python": ">=3.4",
    "summary": "PySpark data serializer",
    "version": "0.2.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15732970,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "175707d6c9060c89d14ef0898bb94804fd354c69c15474fda4c586aa00e31b69",
          "md5": "7d0a8b93ffb68e19af7781f32c91e51c",
          "sha256": "cc4fbaea3b9254532b71fa112cf4cda16e6930bd2b95b151cd99b2057e086a97"
        },
        "downloads": -1,
        "filename": "marshmallow-pyspark-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7d0a8b93ffb68e19af7781f32c91e51c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.4",
        "size": 7177,
        "upload_time": "2020-02-10T23:24:56",
        "upload_time_iso_8601": "2020-02-10T23:24:56.123820Z",
        "url": "https://files.pythonhosted.org/packages/17/57/07d6c9060c89d14ef0898bb94804fd354c69c15474fda4c586aa00e31b69/marshmallow-pyspark-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a11d01551aa3e41ea03e50aea5e1ec611bf0390138bbf66e48c71048650fcd39",
          "md5": "d11a1ed852f04d4541eb9f3e42c21ac0",
          "sha256": "fa0682112fc78b694b0b69eb652cc853a42d35177760cba90572569ad4bd5672"
        },
        "downloads": -1,
        "filename": "marshmallow-pyspark-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "d11a1ed852f04d4541eb9f3e42c21ac0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.4",
        "size": 11951,
        "upload_time": "2020-02-13T20:49:17",
        "upload_time_iso_8601": "2020-02-13T20:49:17.546851Z",
        "url": "https://files.pythonhosted.org/packages/a1/1d/01551aa3e41ea03e50aea5e1ec611bf0390138bbf66e48c71048650fcd39/marshmallow-pyspark-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e39e482781d4b2ba14e326c9fc69bec7586928de1501a31a6781ccb1ae221cc6",
          "md5": "e3de0bfbd62e475bd7ce4b276929dc6a",
          "sha256": "cdbd70a7aa019e31774e48f9aac85bdd9c3028d34e47d01bf8e2145a0c1264f4"
        },
        "downloads": -1,
        "filename": "marshmallow-pyspark-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e3de0bfbd62e475bd7ce4b276929dc6a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.4",
        "size": 12632,
        "upload_time": "2020-09-19T01:58:01",
        "upload_time_iso_8601": "2020-09-19T01:58:01.651758Z",
        "url": "https://files.pythonhosted.org/packages/e3/9e/482781d4b2ba14e326c9fc69bec7586928de1501a31a6781ccb1ae221cc6/marshmallow-pyspark-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8692e26c13325cb5c2472717a9ef9eaa346edf4c8e30802d6108a88bcb813d51",
          "md5": "667fe1d88cda8054ccf643086c5363d1",
          "sha256": "e05bc3c4443921fb8e10333fb6b4f02d93fd23876e6f83cdc44b05772b94662c"
        },
        "downloads": -1,
        "filename": "marshmallow-pyspark-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "667fe1d88cda8054ccf643086c5363d1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.4",
        "size": 16958,
        "upload_time": "2021-05-05T17:33:26",
        "upload_time_iso_8601": "2021-05-05T17:33:26.041204Z",
        "url": "https://files.pythonhosted.org/packages/86/92/e26c13325cb5c2472717a9ef9eaa346edf4c8e30802d6108a88bcb813d51/marshmallow-pyspark-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "34c24dcd2187f8698c41415282336fc01732f2454813a3e6f29c80a15eac3cf2",
          "md5": "d0b0843bb027da86be147e44ca8d1b4b",
          "sha256": "cf6512d72cd87461ea38254cf0b6a698edb443a392ec9fd471ba2024c69ac94a"
        },
        "downloads": -1,
        "filename": "marshmallow-pyspark-0.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "d0b0843bb027da86be147e44ca8d1b4b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.4",
        "size": 16960,
        "upload_time": "2022-11-11T07:20:42",
        "upload_time_iso_8601": "2022-11-11T07:20:42.128886Z",
        "url": "https://files.pythonhosted.org/packages/34/c2/4dcd2187f8698c41415282336fc01732f2454813a3e6f29c80a15eac3cf2/marshmallow-pyspark-0.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "34c24dcd2187f8698c41415282336fc01732f2454813a3e6f29c80a15eac3cf2",
        "md5": "d0b0843bb027da86be147e44ca8d1b4b",
        "sha256": "cf6512d72cd87461ea38254cf0b6a698edb443a392ec9fd471ba2024c69ac94a"
      },
      "downloads": -1,
      "filename": "marshmallow-pyspark-0.2.3.tar.gz",
      "has_sig": false,
      "md5_digest": "d0b0843bb027da86be147e44ca8d1b4b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.4",
      "size": 16960,
      "upload_time": "2022-11-11T07:20:42",
      "upload_time_iso_8601": "2022-11-11T07:20:42.128886Z",
      "url": "https://files.pythonhosted.org/packages/34/c2/4dcd2187f8698c41415282336fc01732f2454813a3e6f29c80a15eac3cf2/marshmallow-pyspark-0.2.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}