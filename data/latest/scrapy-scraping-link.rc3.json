{
  "info": {
    "author": "Nicolas Marin",
    "author_email": "info@scraping.link",
    "bugtrack_url": null,
    "classifiers": [
      "Framework :: Scrapy",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Topic :: Software Development :: Libraries :: Application Frameworks",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "## Scrapy ScrapingLink Middleware\n\n### Acknowledgements\n\nThanks to [arimbr](https://github.com/arimbr) and [ScrapingBee](https://github.com/ScrapingBee/scrapy-scrapingbee), this is adaptation of their work.\n\n### Installation\n\n`pip install scrapy-scraping-link`\n\n### Configuration\n\nAdd your `ScrapingLink_API_KEY` and the `ScrapingLinkMiddleware` to your project settings.py. Don't forget to set `CONCURRENT_REQUESTS` according to your [ScrapingLink plan](https://scraping.link/precios/).\n\n```python\nSCRAPINGLINK_API_KEY = 'REPLACE-WITH-YOUR-API-KEY'\n\nDOWNLOADER_MIDDLEWARES = {\n    'scrapy_scraping_link.ScrapingLinkMiddleware': 700,\n}\n\nCONCURRENT_REQUESTS = 1\n```\n\n### Usage\n\nInherit your spiders from `ScrapingLinkSpider` and yield a `ScrapingLinkRequest`.\n\nBelow you can see an example from the spider in [parascrapear.py](examples/parascrapear/parascrapear/spiders/parascrapear.py).\n\n```python\nfrom scrapy import Spider\nfrom scrapy_scraping_link import ScrapingLinkSpider, ScrapingLinkRequest\n\n\nclass ParascrapearSpider(Spider):\n    name = 'parascrapear'\n    allowed_domains = ['parascrapear.com']\n    start_urls = ['http://parascrapear.com/']\n\n    def parse(self, response):\n        print('Parseando ' + response.url)       \n        \n        next_urls = response.css('a::attr(href)').getall()\n        for next_url in next_urls:\n            if next_url is not None:\n                yield ScrapingLinkRequest(response.urljoin(next_url))\n        \n        sentences = response.css('q::text').getall()\n        for sentence in sentences:\n            print(sentence)\n\n```\n\nYou can pass [ScrapingLink parameters](https://scraping.link/documentacion/) in the params argument of a ScrapingLinkRequest. Headers and cookies are passed like a normal Scrapy Request. ScrapingLinkRequests formats all parameters, headers and cookies to the format expected by the API.\n\n### Examples\n\nAdd your API key to [settings.py](examples/parascrapear/parascrapear/settings.py).\n\nTo run the examples you need to clone this repository. In your terminal, go to `examples/parascrapear/parascrapear` and run the example spider with:\n\n```bash\nscrapy runspider parascrapear.py\n```\n\n#### Customer Support\nSimply reach out to us via [Telegram Group](https://t.me/joinchat/AwFbIh1PuwuEgCk0gVgS4g) or or write us an [email](mailto:info@scraping.link).\n\n[Sign up for our free plan](https://app.scraping.link/register) to get a free API key loaded with 100 free credits. No credit card required!",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/nicolasmarin/scrapy-scraping-link/archive/refs/heads/main.zip",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/nicolasmarin/scrapy-scraping-link",
    "keywords": "web scraping,scraping,proxy rotating,html",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-scraping-link",
    "package_url": "https://pypi.org/project/scrapy-scraping-link/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-scraping-link/",
    "project_urls": {
      "Download": "https://github.com/nicolasmarin/scrapy-scraping-link/archive/refs/heads/main.zip",
      "Homepage": "https://github.com/nicolasmarin/scrapy-scraping-link"
    },
    "release_url": "https://pypi.org/project/scrapy-scraping-link/0.0.8/",
    "requires_dist": null,
    "requires_python": ">=3.5",
    "summary": "Scrapy middleware for Scraping.link",
    "version": "0.0.8",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10671854,
  "releases": {
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0478b167c4d0567dd504662e2511013138de3456e671c96af1b98c8343e867f8",
          "md5": "1b57ca5fe984e3fcc892c65ee337ec1b",
          "sha256": "f3f576b4a12ee4e106ed23ca22cc4fdef9d21b99b9af473cc21330344f18a828"
        },
        "downloads": -1,
        "filename": "scrapy-scraping-link-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "1b57ca5fe984e3fcc892c65ee337ec1b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 4062,
        "upload_time": "2021-06-15T07:57:57",
        "upload_time_iso_8601": "2021-06-15T07:57:57.561782Z",
        "url": "https://files.pythonhosted.org/packages/04/78/b167c4d0567dd504662e2511013138de3456e671c96af1b98c8343e867f8/scrapy-scraping-link-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1b03af8ce268a6f8ff96fb77ee6f2b89fa9b4b3dc946d334f0396e0996e6a002",
          "md5": "80178017b14840124532740badbe5e29",
          "sha256": "13b2327f229498f5ec0c7f0372e726dd914678c11233428edfd9652c4925eece"
        },
        "downloads": -1,
        "filename": "scrapy-scraping-link-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "80178017b14840124532740badbe5e29",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 4396,
        "upload_time": "2021-06-16T18:07:13",
        "upload_time_iso_8601": "2021-06-16T18:07:13.180172Z",
        "url": "https://files.pythonhosted.org/packages/1b/03/af8ce268a6f8ff96fb77ee6f2b89fa9b4b3dc946d334f0396e0996e6a002/scrapy-scraping-link-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eb06939d5ec9d6f526a01fed0207078441c9f61661abdb5ed2d929f765840653",
          "md5": "53732ddfe8650fb415214cfd1156ad6e",
          "sha256": "2531809e27536bd872b3a25ae89324110b1a4d63049435586dbcbb721b843b13"
        },
        "downloads": -1,
        "filename": "scrapy-scraping-link-0.0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "53732ddfe8650fb415214cfd1156ad6e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 4451,
        "upload_time": "2021-06-17T08:57:31",
        "upload_time_iso_8601": "2021-06-17T08:57:31.553446Z",
        "url": "https://files.pythonhosted.org/packages/eb/06/939d5ec9d6f526a01fed0207078441c9f61661abdb5ed2d929f765840653/scrapy-scraping-link-0.0.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eb06939d5ec9d6f526a01fed0207078441c9f61661abdb5ed2d929f765840653",
        "md5": "53732ddfe8650fb415214cfd1156ad6e",
        "sha256": "2531809e27536bd872b3a25ae89324110b1a4d63049435586dbcbb721b843b13"
      },
      "downloads": -1,
      "filename": "scrapy-scraping-link-0.0.8.tar.gz",
      "has_sig": false,
      "md5_digest": "53732ddfe8650fb415214cfd1156ad6e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 4451,
      "upload_time": "2021-06-17T08:57:31",
      "upload_time_iso_8601": "2021-06-17T08:57:31.553446Z",
      "url": "https://files.pythonhosted.org/packages/eb/06/939d5ec9d6f526a01fed0207078441c9f61661abdb5ed2d929f765840653/scrapy-scraping-link-0.0.8.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}