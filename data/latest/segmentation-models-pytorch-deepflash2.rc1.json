{
  "info": {
    "author": "Pavel Yakubovskiy",
    "author_email": "qubvel@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "\n<div align=\"center\">\n \n![logo](https://i.ibb.co/dc1XdhT/Segmentation-Models-V2-Side-1-1.png)  \n**Python library with Neural Networks for Image  \nSegmentation based on [PyTorch](https://pytorch.org/).**  \n\n[![Generic badge](https://img.shields.io/badge/License-MIT-<COLOR>.svg?style=for-the-badge)](https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE) \n[![GitHub Workflow Status (branch)](https://img.shields.io/github/workflow/status/qubvel/segmentation_models.pytorch/CI/master?style=for-the-badge&logo=github)](https://github.com/qubvel/segmentation_models.pytorch/actions/workflows/tests.yml) \n[![Read the Docs](https://img.shields.io/readthedocs/smp?style=for-the-badge&logo=readthedocs&logoColor=white)](https://smp.readthedocs.io/en/latest/) \n<br>\n[![PyPI](https://img.shields.io/pypi/v/segmentation-models-pytorch?color=blue&style=for-the-badge&logo=pypi&logoColor=white)](https://pypi.org/search/?q=segmentation-models-pytorch) \n[![PyPI - Downloads](https://img.shields.io/pypi/dm/segmentation-models-pytorch?style=for-the-badge&color=blue)](https://pepy.tech/project/segmentation-models-pytorch) \n<br>\n[![PyTorch - Version](https://img.shields.io/badge/PYTORCH-1.4+-red?style=for-the-badge&logo=pytorch)](https://pepy.tech/project/segmentation-models-pytorch) \n[![Python - Version](https://img.shields.io/badge/PYTHON-3.6+-red?style=for-the-badge&logo=python&logoColor=white)](https://pepy.tech/project/segmentation-models-pytorch) \n\n</div>\n\nThe main features of this library are:\n\n - High level API (just two lines to create a neural network)\n - 9 models architectures for binary and multi class segmentation (including legendary Unet)\n - 113 available encoders (and 400+ encoders from [timm](https://github.com/rwightman/pytorch-image-models))\n - All encoders have pre-trained weights for faster and better convergence\n - Popular metrics and losses for training routines\n \n### [üìö Project Documentation üìö](http://smp.readthedocs.io/)\n\nVisit [Read The Docs Project Page](https://smp.readthedocs.io/) or read following README to know more about Segmentation Models Pytorch (SMP for short) library\n\n### üìã Table of content\n 1. [Quick start](#start)\n 2. [Examples](#examples)\n 3. [Models](#models)\n    1. [Architectures](#architectures)\n    2. [Encoders](#encoders)\n    3. [Timm Encoders](#timm)\n 4. [Models API](#api)\n    1. [Input channels](#input-channels)\n    2. [Auxiliary classification output](#auxiliary-classification-output)\n    3. [Depth](#depth)\n 5. [Installation](#installation)\n 6. [Competitions won with the library](#competitions-won-with-the-library)\n 7. [Contributing](#contributing)\n 8. [Citing](#citing)\n 9. [License](#license)\n\n### ‚è≥ Quick start <a name=\"start\"></a>\n\n#### 1. Create your first Segmentation model with SMP\n\nSegmentation model is just a PyTorch nn.Module, which can be created as easy as:\n\n```python\nimport segmentation_models_pytorch as smp\n\nmodel = smp.Unet(\n    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n    classes=3,                      # model output channels (number of classes in your dataset)\n)\n```\n - see [table](#architectures) with available model architectures\n - see [table](#encoders) with available encoders and their corresponding weights\n\n#### 2. Configure data preprocessing\n\nAll encoders have pretrained weights. Preparing your data the same way as during weights pre-training may give your better results (higher metric score and faster convergence). It is **not necessary** in case you train the whole model, not only decoder.\n\n```python\nfrom segmentation_models_pytorch.encoders import get_preprocessing_fn\n\npreprocess_input = get_preprocessing_fn('resnet18', pretrained='imagenet')\n```\n\nCongratulations! You are done! Now you can train your model with your favorite framework!\n\n### üí° Examples <a name=\"examples\"></a>\n - Training model for pets binary segmentation with Pytorch-Lightning [notebook](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/binary_segmentation_intro.ipynb) and [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/examples/binary_segmentation_intro.ipynb)\n - Training model for cars segmentation on CamVid dataset [here](https://github.com/qubvel/segmentation_models.pytorch/blob/master/examples/cars%20segmentation%20(camvid).ipynb).\n - Training SMP model with [Catalyst](https://github.com/catalyst-team/catalyst) (high-level framework for PyTorch), [TTAch](https://github.com/qubvel/ttach) (TTA library for PyTorch) and [Albumentations](https://github.com/albu/albumentations) (fast image augmentation library) - [here](https://github.com/catalyst-team/catalyst/blob/v21.02rc0/examples/notebooks/segmentation-tutorial.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catalyst-team/catalyst/blob/v21.02rc0/examples/notebooks/segmentation-tutorial.ipynb)\n - Training SMP model with [Pytorch-Lightning](https://pytorch-lightning.readthedocs.io) framework - [here](https://github.com/ternaus/cloths_segmentation) (clothes binary segmentation by [@ternaus](https://github.com/ternaus)).\n\n### üì¶ Models <a name=\"models\"></a>\n\n#### Architectures <a name=\"architectures\"></a>\n - Unet [[paper](https://arxiv.org/abs/1505.04597)] [[docs](https://smp.readthedocs.io/en/latest/models.html#unet)]\n - Unet++ [[paper](https://arxiv.org/pdf/1807.10165.pdf)] [[docs](https://smp.readthedocs.io/en/latest/models.html#id2)]\n - MAnet [[paper](https://ieeexplore.ieee.org/abstract/document/9201310)] [[docs](https://smp.readthedocs.io/en/latest/models.html#manet)]\n - Linknet [[paper](https://arxiv.org/abs/1707.03718)] [[docs](https://smp.readthedocs.io/en/latest/models.html#linknet)]\n - FPN [[paper](http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf)] [[docs](https://smp.readthedocs.io/en/latest/models.html#fpn)]\n - PSPNet [[paper](https://arxiv.org/abs/1612.01105)] [[docs](https://smp.readthedocs.io/en/latest/models.html#pspnet)]\n - PAN [[paper](https://arxiv.org/abs/1805.10180)] [[docs](https://smp.readthedocs.io/en/latest/models.html#pan)]\n - DeepLabV3 [[paper](https://arxiv.org/abs/1706.05587)] [[docs](https://smp.readthedocs.io/en/latest/models.html#deeplabv3)]\n - DeepLabV3+ [[paper](https://arxiv.org/abs/1802.02611)] [[docs](https://smp.readthedocs.io/en/latest/models.html#id9)]\n\n#### Encoders <a name=\"encoders\"></a>\n\nThe following is a list of supported encoders in the SMP. Select the appropriate family of encoders and click to expand the table and select a specific encoder and its pre-trained weights (`encoder_name` and `encoder_weights` parameters).\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnet18                        |imagenet / ssl / swsl           |11M                             |\n|resnet34                        |imagenet                        |21M                             |\n|resnet50                        |imagenet / ssl / swsl           |23M                             |\n|resnet101                       |imagenet                        |42M                             |\n|resnet152                       |imagenet                        |58M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNeXt</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|resnext50_32x4d                 |imagenet / ssl / swsl           |22M                             |\n|resnext101_32x4d                |ssl / swsl                      |42M                             |\n|resnext101_32x8d                |imagenet / instagram / ssl / swsl|86M                         |\n|resnext101_32x16d               |instagram / ssl / swsl          |191M                            |\n|resnext101_32x32d               |instagram                       |466M                            |\n|resnext101_32x48d               |instagram                       |826M                            |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">ResNeSt</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-resnest14d                 |imagenet                        |8M                              |\n|timm-resnest26d                 |imagenet                        |15M                             |\n|timm-resnest50d                 |imagenet                        |25M                             |\n|timm-resnest101e                |imagenet                        |46M                             |\n|timm-resnest200e                |imagenet                        |68M                             |\n|timm-resnest269e                |imagenet                        |108M                            |\n|timm-resnest50d_4s2x40d         |imagenet                        |28M                             |\n|timm-resnest50d_1s4x24d         |imagenet                        |23M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">Res2Ne(X)t</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-res2net50_26w_4s           |imagenet                        |23M                             |\n|timm-res2net101_26w_4s          |imagenet                        |43M                             |\n|timm-res2net50_26w_6s           |imagenet                        |35M                             |\n|timm-res2net50_26w_8s           |imagenet                        |46M                             |\n|timm-res2net50_48w_2s           |imagenet                        |23M                             |\n|timm-res2net50_14w_8s           |imagenet                        |23M                             |\n|timm-res2next50                 |imagenet                        |22M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">RegNet(x/y)</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-regnetx_002                |imagenet                        |2M                              |\n|timm-regnetx_004                |imagenet                        |4M                              |\n|timm-regnetx_006                |imagenet                        |5M                              |\n|timm-regnetx_008                |imagenet                        |6M                              |\n|timm-regnetx_016                |imagenet                        |8M                              |\n|timm-regnetx_032                |imagenet                        |14M                             |\n|timm-regnetx_040                |imagenet                        |20M                             |\n|timm-regnetx_064                |imagenet                        |24M                             |\n|timm-regnetx_080                |imagenet                        |37M                             |\n|timm-regnetx_120                |imagenet                        |43M                             |\n|timm-regnetx_160                |imagenet                        |52M                             |\n|timm-regnetx_320                |imagenet                        |105M                            |\n|timm-regnety_002                |imagenet                        |2M                              |\n|timm-regnety_004                |imagenet                        |3M                              |\n|timm-regnety_006                |imagenet                        |5M                              |\n|timm-regnety_008                |imagenet                        |5M                              |\n|timm-regnety_016                |imagenet                        |10M                             |\n|timm-regnety_032                |imagenet                        |17M                             |\n|timm-regnety_040                |imagenet                        |19M                             |\n|timm-regnety_064                |imagenet                        |29M                             |\n|timm-regnety_080                |imagenet                        |37M                             |\n|timm-regnety_120                |imagenet                        |49M                             |\n|timm-regnety_160                |imagenet                        |80M                             |\n|timm-regnety_320                |imagenet                        |141M                            |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">GERNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-gernet_s                   |imagenet                        |6M                              |\n|timm-gernet_m                   |imagenet                        |18M                             |\n|timm-gernet_l                   |imagenet                        |28M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">SE-Net</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|senet154                        |imagenet                        |113M                            |\n|se_resnet50                     |imagenet                        |26M                             |\n|se_resnet101                    |imagenet                        |47M                             |\n|se_resnet152                    |imagenet                        |64M                             |\n|se_resnext50_32x4d              |imagenet                        |25M                             |\n|se_resnext101_32x4d             |imagenet                        |46M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">SK-ResNe(X)t</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|timm-skresnet18                 |imagenet                        |11M                             |\n|timm-skresnet34                 |imagenet                        |21M                             |\n|timm-skresnext50_32x4d          |imagenet                        |25M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">DenseNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|densenet121                     |imagenet                        |6M                              |\n|densenet169                     |imagenet                        |12M                             |\n|densenet201                     |imagenet                        |18M                             |\n|densenet161                     |imagenet                        |26M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">Inception</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|inceptionresnetv2               |imagenet /  imagenet+background |54M                             |\n|inceptionv4                     |imagenet /  imagenet+background |41M                             |\n|xception                        |imagenet                        |22M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">EfficientNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|efficientnet-b0                 |imagenet                        |4M                              |\n|efficientnet-b1                 |imagenet                        |6M                              |\n|efficientnet-b2                 |imagenet                        |7M                              |\n|efficientnet-b3                 |imagenet                        |10M                             |\n|efficientnet-b4                 |imagenet                        |17M                             |\n|efficientnet-b5                 |imagenet                        |28M                             |\n|efficientnet-b6                 |imagenet                        |40M                             |\n|efficientnet-b7                 |imagenet                        |63M                             |\n|timm-efficientnet-b0            |imagenet / advprop / noisy-student|4M                              |\n|timm-efficientnet-b1            |imagenet / advprop / noisy-student|6M                              |\n|timm-efficientnet-b2            |imagenet / advprop / noisy-student|7M                              |\n|timm-efficientnet-b3            |imagenet / advprop / noisy-student|10M                             |\n|timm-efficientnet-b4            |imagenet / advprop / noisy-student|17M                             |\n|timm-efficientnet-b5            |imagenet / advprop / noisy-student|28M                             |\n|timm-efficientnet-b6            |imagenet / advprop / noisy-student|40M                             |\n|timm-efficientnet-b7            |imagenet / advprop / noisy-student|63M                             |\n|timm-efficientnet-b8            |imagenet / advprop             |84M                             |\n|timm-efficientnet-l2            |noisy-student                   |474M                            |\n|timm-efficientnet-lite0         |imagenet                        |4M                              |\n|timm-efficientnet-lite1         |imagenet                        |5M                              |\n|timm-efficientnet-lite2         |imagenet                        |6M                              |\n|timm-efficientnet-lite3         |imagenet                        |8M                             |\n|timm-efficientnet-lite4         |imagenet                        |13M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">MobileNet</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|mobilenet_v2                    |imagenet                        |2M                              |\n|timm-mobilenetv3_large_075      |imagenet                        |1.78M                       |\n|timm-mobilenetv3_large_100      |imagenet                        |2.97M                       |\n|timm-mobilenetv3_large_minimal_100|imagenet                        |1.41M                       |\n|timm-mobilenetv3_small_075      |imagenet                        |0.57M                        |\n|timm-mobilenetv3_small_100      |imagenet                        |0.93M                       |\n|timm-mobilenetv3_small_minimal_100|imagenet                        |0.43M                       |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">DPN</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|dpn68                           |imagenet                        |11M                             |\n|dpn68b                          |imagenet+5k                     |11M                             |\n|dpn92                           |imagenet+5k                     |34M                             |\n|dpn98                           |imagenet                        |58M                             |\n|dpn107                          |imagenet+5k                     |84M                             |\n|dpn131                          |imagenet                        |76M                             |\n\n</div>\n</details>\n\n<details>\n<summary style=\"margin-left: 25px;\">VGG</summary>\n<div style=\"margin-left: 25px;\">\n\n|Encoder                         |Weights                         |Params, M                       |\n|--------------------------------|:------------------------------:|:------------------------------:|\n|vgg11                           |imagenet                        |9M                              |\n|vgg11_bn                        |imagenet                        |9M                              |\n|vgg13                           |imagenet                        |9M                              |\n|vgg13_bn                        |imagenet                        |9M                              |\n|vgg16                           |imagenet                        |14M                             |\n|vgg16_bn                        |imagenet                        |14M                             |\n|vgg19                           |imagenet                        |20M                             |\n|vgg19_bn                        |imagenet                        |20M                             |\n\n</div>\n</details>\n\n\n\\* `ssl`, `swsl` - semi-supervised and weakly-supervised learning on ImageNet ([repo](https://github.com/facebookresearch/semi-supervised-ImageNet1K-models)).\n\n#### Timm Encoders <a name=\"timm\"></a>\n\n[docs](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n\nPytorch Image Models (a.k.a. timm) has a lot of pretrained models and interface which allows using these models as encoders in smp, however, not all models are supported\n\n - transformer models do not have ``features_only`` functionality implemented\n - some models do not have appropriate strides\n\nTotal number of supported encoders: 467\n - [table with available encoders](https://smp.readthedocs.io/en/latest/encoders_timm.html)\n\n### üîÅ Models API <a name=\"api\"></a>\n\n - `model.encoder` - pretrained backbone to extract features of different spatial resolution\n - `model.decoder` - depends on models architecture (`Unet`/`Linknet`/`PSPNet`/`FPN`)\n - `model.segmentation_head` - last block to produce required number of mask channels (include also optional upsampling and activation)\n - `model.classification_head` - optional block which create classification head on top of encoder\n - `model.forward(x)` - sequentially pass `x` through model\\`s encoder, decoder and segmentation head (and classification head if specified)\n\n##### Input channels\nInput channels parameter allows you to create models, which process tensors with arbitrary number of channels.\nIf you use pretrained weights from imagenet - weights of first convolution will be reused. For\n1-channel case it would be a sum of weights of first convolution layer, otherwise channels would be \npopulated with weights like `new_weight[:, i] = pretrained_weight[:, i % 3]` and than scaled with `new_weight * 3 / new_in_channels`.\n```python\nmodel = smp.FPN('resnet34', in_channels=1)\nmask = model(torch.ones([1, 1, 64, 64]))\n```\n\n##### Auxiliary classification output  \nAll models support `aux_params` parameters, which is default set to `None`. \nIf `aux_params = None` then classification auxiliary output is not created, else\nmodel produce not only `mask`, but also `label` output with shape `NC`.\nClassification head consists of GlobalPooling->Dropout(optional)->Linear->Activation(optional) layers, which can be \nconfigured by `aux_params` as follows:\n```python\naux_params=dict(\n    pooling='avg',             # one of 'avg', 'max'\n    dropout=0.5,               # dropout ratio, default is None\n    activation='sigmoid',      # activation function, default is None\n    classes=4,                 # define number of output labels\n)\nmodel = smp.Unet('resnet34', classes=4, aux_params=aux_params)\nmask, label = model(x)\n```\n\n##### Depth\nDepth parameter specify a number of downsampling operations in encoder, so you can make\nyour model lighter if specify smaller `depth`.\n```python\nmodel = smp.Unet('resnet34', encoder_depth=4)\n```\n\n\n### üõ† Installation <a name=\"installation\"></a>\nPyPI version:\n```bash\n$ pip install segmentation-models-pytorch\n````\nLatest version from source:\n```bash\n$ pip install git+https://github.com/qubvel/segmentation_models.pytorch\n````\n\n### üèÜ Competitions won with the library\n\n`Segmentation Models` package is widely used in the image segmentation competitions.\n[Here](https://github.com/qubvel/segmentation_models.pytorch/blob/master/HALLOFFAME.md) you can find competitions, names of the winners and links to their solutions.\n\n### ü§ù Contributing\n\n##### Install linting and formatting pre-commit hooks\n```bash\npip install pre-commit black flake8\npre-commit install\n```\n\n##### Run tests\n```bash\npytest -p no:cacheprovider\n```\n\n##### Run tests in docker\n```bash\n$ docker build -f docker/Dockerfile.dev -t smp:dev . && docker run --rm smp:dev pytest -p no:cacheprovider\n```\n\n##### Generate table with encoders (in case you add a new encoder)\n```bash\n$ docker build -f docker/Dockerfile.dev -t smp:dev . && docker run --rm smp:dev python misc/generate_table.py\n```\n\n### üìù Citing\n```\n@misc{Yakubovskiy:2019,\n  Author = {Pavel Yakubovskiy},\n  Title = {Segmentation Models Pytorch},\n  Year = {2020},\n  Publisher = {GitHub},\n  Journal = {GitHub repository},\n  Howpublished = {\\url{https://github.com/qubvel/segmentation_models.pytorch}}\n}\n```\n\n### üõ°Ô∏è License <a name=\"license\"></a>\nProject is distributed under [MIT License](https://github.com/qubvel/segmentation_models.pytorch/blob/master/LICENSE)\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/qubvel/segmentation_models.pytorch",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "segmentation-models-pytorch-deepflash2",
    "package_url": "https://pypi.org/project/segmentation-models-pytorch-deepflash2/",
    "platform": null,
    "project_url": "https://pypi.org/project/segmentation-models-pytorch-deepflash2/",
    "project_urls": {
      "Homepage": "https://github.com/qubvel/segmentation_models.pytorch"
    },
    "release_url": "https://pypi.org/project/segmentation-models-pytorch-deepflash2/0.3.0/",
    "requires_dist": [
      "torchvision (>=0.5.0)",
      "pretrainedmodels (==0.7.4)",
      "efficientnet-pytorch (==0.6.3)",
      "timm (>=0.5.4)",
      "tqdm",
      "pillow",
      "pytest ; extra == 'test'",
      "mock ; extra == 'test'",
      "pre-commit ; extra == 'test'",
      "flake8 (==4.0.1) ; extra == 'test'",
      "flake8-docstrings (==1.6.0) ; extra == 'test'"
    ],
    "requires_python": ">=3.6.0",
    "summary": "Image segmentation models with pre-trained backbones. PyTorch. Adapted for deepflash2",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14250934,
  "releases": {
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aedc4e51471765a1b103dcf0e9538780ffe6c03678ecf8bda5755304dc6c1e80",
          "md5": "d2c754f251c1e9ecb1da3fda28bac845",
          "sha256": "061a7b84886fa23844eb8bce54c97472e872cfd6d02acea3319a1d74f89479ca"
        },
        "downloads": -1,
        "filename": "segmentation_models_pytorch_deepflash2-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d2c754f251c1e9ecb1da3fda28bac845",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 186943,
        "upload_time": "2022-06-26T14:36:01",
        "upload_time_iso_8601": "2022-06-26T14:36:01.115167Z",
        "url": "https://files.pythonhosted.org/packages/ae/dc/4e51471765a1b103dcf0e9538780ffe6c03678ecf8bda5755304dc6c1e80/segmentation_models_pytorch_deepflash2-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9801addb987459bc18443f319a32535322e35c69fab85dd405bd217335fef6b9",
          "md5": "b427cf76b4638697d6d116f805d02bda",
          "sha256": "27562ba46d5c58be2d7790365f7adbbfe4f53d265ce97edfbec4d7c5ee28c9f6"
        },
        "downloads": -1,
        "filename": "segmentation_models_pytorch_deepflash2-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b427cf76b4638697d6d116f805d02bda",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 61192,
        "upload_time": "2022-06-26T14:36:03",
        "upload_time_iso_8601": "2022-06-26T14:36:03.691936Z",
        "url": "https://files.pythonhosted.org/packages/98/01/addb987459bc18443f319a32535322e35c69fab85dd405bd217335fef6b9/segmentation_models_pytorch_deepflash2-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "aedc4e51471765a1b103dcf0e9538780ffe6c03678ecf8bda5755304dc6c1e80",
        "md5": "d2c754f251c1e9ecb1da3fda28bac845",
        "sha256": "061a7b84886fa23844eb8bce54c97472e872cfd6d02acea3319a1d74f89479ca"
      },
      "downloads": -1,
      "filename": "segmentation_models_pytorch_deepflash2-0.3.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d2c754f251c1e9ecb1da3fda28bac845",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 186943,
      "upload_time": "2022-06-26T14:36:01",
      "upload_time_iso_8601": "2022-06-26T14:36:01.115167Z",
      "url": "https://files.pythonhosted.org/packages/ae/dc/4e51471765a1b103dcf0e9538780ffe6c03678ecf8bda5755304dc6c1e80/segmentation_models_pytorch_deepflash2-0.3.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9801addb987459bc18443f319a32535322e35c69fab85dd405bd217335fef6b9",
        "md5": "b427cf76b4638697d6d116f805d02bda",
        "sha256": "27562ba46d5c58be2d7790365f7adbbfe4f53d265ce97edfbec4d7c5ee28c9f6"
      },
      "downloads": -1,
      "filename": "segmentation_models_pytorch_deepflash2-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "b427cf76b4638697d6d116f805d02bda",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 61192,
      "upload_time": "2022-06-26T14:36:03",
      "upload_time_iso_8601": "2022-06-26T14:36:03.691936Z",
      "url": "https://files.pythonhosted.org/packages/98/01/addb987459bc18443f319a32535322e35c69fab85dd405bd217335fef6b9/segmentation_models_pytorch_deepflash2-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}