{
  "info": {
    "author": "Patroklos Stefanou",
    "author_email": "patstefanou@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "delphi.ai package\n=================\nInstall via ``pip``: ``pip install delphi.ai``\n\nThis library holds a collection of algorithms that can be used \ndebias models that have been defected due to truncation, or missing data. A few \nprojects using the library can found in: \n  \n* `Code for Efficient Truncated Linear Regression with Unknown Noise Variance <https://github.com/pstefanou12/Truncated-Regression-With-Unknown-Noise-Variance-NeurIPS-2021>`_\n\nWe demonstrate how to use the library in a set of walkthroughs and our API\nreference. Functionality provided by the library includes:\n\nFor best results using the package, the data should have mean 0 and variance 1.\n\nBefore running PSGD, the library will check that all of the required \narguments are provided for runnning the procedure with an internal function. After this, all other hyperparameters can be provided by the user, or their defaults values will be used. The current \ndefault hyperparameters can be seen by looking at the ``delphi.utils.defaults.py`` file.\n\nFor logging experiment information, we use MadryLab's `cox <https://github.com/MadryLab/cox>`_. More information and tutorials on how to use the logging framework, check out the link.\n\nContents:\n--------\n\n* `distributions <#distributions>`__: ``distributions`` module includes algorithms for learning from censored (known truncation) and truncated (unknown truncation; unsupervised learning) distributions\n\n  * `CensoredNormal <#CensoredNormal>`__\n  * `CensoredMultivariateNormal <#CensoredMultivariateNormal>`__\n  * `TruncatedNormal <#TruncatedNormal>`__\n  * `TruncatedMultivariateNormal <#TruncatedMultivariateNormal>`__\n  * `TruncatedBernoulli <#TruncatedBernoulli>`__\n\n* `stats <#stats>`__ : ``stats`` module includes models for regression and classification from truncated samples\n \n  * `TruncatedLinearRegression <#TruncatedLinearRegression>`__\n  * `TruncatedLassoRegression <#TruncatedLassoRegression>`__\n  * `TruncatedRidgeRegression <#TruncatedRidgeRegression>`__\n  * `TruncatedElasticNetRegression <#TruncatedElasticNetRegression>`__\n  * `TruncatedLogisticRegression <#TruncatedLogisticRegression>`__\n  * `TruncatedProbitRegression <#TruncatedProbitRegression>`__\n\ndistributions\n=============\n\nCensoredNormal:\n---------------\n``CensoredNormal`` learns censored normal distributions, by maximizing the truncated log likelihood.\nThe algorithm that we use for this procedure is described in the following\npaper `Efficient Statistics in High Dimensions from Truncated Samples <https://arxiv.org/abs/1809.03986>`_.\n\nWhen evaluating censored normal distributions, the user needs three things; an oracle, a Callable that \nindicates whether a sample falls within the truncation set, the model's ``alpha``, survival probability, and the ``CensoredNormal`` module. The ``CensoredNormal`` module accepts \na parameters object that the user can define for running the PSGD procedure.\n\nParameters:\n-----------\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable)): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``variance`` (float): provide distribution's variance, if the distribution's variance is given, the mean is exclusively calculated \n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n\n* ``store`` (cox.store.Store): logging object to keep track distribution's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``loc_`` (torch.Tensor): distribution's estimated mean \n* ``variance_`` (torch.Tensor): distribution's estimated variance \n\nIn the following code block, here, we show an example of how to use the censored normal distribution module: \n   \n.. code-block:: python\n\n  from delphi.distributions.censored_normal import CensoredNormal\n  from delphi import oracle\n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate 0 (ie. S = {x >= 0 for all x in S})\n  phi = oracle.Left_Distribution(0.0)\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                              'alpha': alpha})\n  # define censored normal distribution object\n  censored = CensoredNormal(train_kwargs, store=store)\n  # fit to dataset\n  censored.fit(S)\n  # close store \n  store.close()\n\nCensoredMultivariateNormal:\n--------------------------\n``CensoredMultivariateNormal`` learns censored multivariate normal distributions, by maximizing the truncated log likelihood.\nThe algorithm that we use for this procedure is described in the following\npaper `Efficient Statistics in High Dimensions from Truncated Samples <https://arxiv.org/abs/1809.03986>`_.\n\nWhen evaluating censored multivariate normal distributions, the user needs three things; an oracle, a Callable that \nindicates whether a sample falls within the truncation set, the model's ``alpha``, survival probability, and the ``CensoredMultivariateNormal`` module. The ``CensoredMultivariateNormal`` module accepts \na parameters object that the user can define for running the PSGD procedure.\n\nParameters:\n-----------\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``covariance_matrix`` (torch.Tensor): provide distribution's covariance_matrix, if the distribution's covariance_matrix is given, the mean vector is exclusively calculated \n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n\n* ``store`` (cox.store.Store): logging object to keep track distribution's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``loc_`` (torch.Tensor): distribution's estimated mean \n* ``covariance_matrix_`` (torch.Tensor): distribution's estimated covariance matrix \n\nIn the following code block, here, we show an example of how to use the censored multivariate normal distribution module: \n   \n.. code-block:: python\n\n  from torch import Tensor\n  from delphi.distributions.censored_multivariate_normal import CensoredMultivariateNormal\n  from delphi import oracle\n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate 0 (ie. S = {x >= 0 for all x in S})\n  phi = oracle.Left_Distribution(Tensor([0.0, 0.0]))\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                              'alpha': alpha})\n  # define censored multivariate normal distribution object\n  censored = CensoredMultivariateNormal(train_kwargs, store=store)\n  # fit to dataset\n  censored.fit(S)\n  # close store \n  store.close()\n\nTruncatedNormal:\n--------------------------\n``TruncatedNormal`` learns truncated normal distributions, with unknown truncation, by maximizing the truncated log likelihood.\nThe algorithm that we use for this procedure is described in the following\npaper `Efficient Truncated Statistics with Unknown Truncation <https://arxiv.org/abs/1908.01034>`_.\n\nWhen evaluating truncated normal distributions, the user needs to ``import`` the ``TruncatedNormal`` module. The ``TruncatedNormal`` module accepts \na parameters object that the user can define for running the PSGD procedure. When *debiasing* truncated normal distributions, we don't require a membership \noracle, as it is unknown. However, after running our procedure, we are able to provide an approximation of what the truncation set is. Since the user \ninputs a membership oracle in the ``args`` object, when the truncation set is known, we add the learned membership oracle to the ``args`` object as well.\n\n**NOTE:** when learning truncation sets, the user can not pass in a ``Parameters`` object directly into the ``TruncatedNormal`` object, because they will not \nbe able to access the ``Parameters`` object afterwards.\n\nParameters:\n-----------\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``covariance_matrix`` (torch.Tensor): provide distribution's covariance_matrix, if the distribution's covariance_matrix is given, the mean vector is exclusively calculated \n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n  * ``d`` (int): degree of expansion to use for Hermite polynomial when learning truncation set; default 100\n\n* ``store`` (cox.store.Store): logging object to keep track distribution's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``loc_`` (torch.Tensor): distribution's estimated mean \n* ``variance_`` (torch.Tensor): distribution's estimated variance \n\nIn the following code block, here, we show an example of how to fit the truncated normal distribution module: \n   \n.. code-block:: python\n\n  from delphi.distributions.truncated_normal import TruncatedNormal\n  from delphi import oracle\n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate 0 (ie. S = {x >= 0 for all x in S})\n  phi = oracle.Left_Distribution(0.0)\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                              'alpha': alpha, \n                              'd': 100})\n  # define truncated normal distribution object\n  truncated = TruncatedNormal(train_kwargs, store=store)\n  # fit to dataset\n  truncated.fit(S)\n  # close store \n  store.close()\n\nAfter fitting the distribution, we now have a membership oracle that we learned through a hermite polynomial. In the following code block, \nwe show an example of how use the membership oracle: \n\n.. code-block:: python\n\n  import torch as ch\n  from torch.distributions.multivariate_normal import MultivariateNormal \n\n  # generate samples from a standard multivariate normal distribution\n  M = MultivariateNormal(ch.zeros(1,), ch.eye(1))\n  samples = M.rsample([1000,])\n  # filter samples with learning membership oracle\n  filtered = train_kwargs.phi(samples)\n\nTruncatedMultivariateNormal:\n--------------------------\n``TruncatedMultivariateNormal`` learns truncated multivariate normal distributions, with unknown truncation, by maximizing the truncated log likelihood.\nThe algorithm that we use for this procedure is described in the following\npaper `Efficient Truncated Statistics with Unknown Truncation <https://arxiv.org/abs/1908.01034>`_.\n\nWhen evaluating truncated multivariate normal distributions, the user needs to ``import`` the ``TruncatedMultivariateNormal`` module. The ``TruncatedMultivariateNormal`` module accepts \na parameters object that the user can define for running the PSGD procedure. When *debiasing* truncated normal distributions, we don't require a membership \noracle, as it is unknown. However, after running our procedure, we are able to provide an approximation of what the truncation set is. Since the user \ninputs a membership oracle in the ``args`` object, when the truncation set is known, we add the learned membership oracle to the ``args`` object as well.\n\n\n**NOTE:** when learning truncation sets, the user can not pass in a ``Parameters`` object directly into the ``TruncatedMultivariateNormal`` object, because they will not \nbe able to access the ``Parameters`` object afterwards.\n\nParameters:\n-----------\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``variance`` (float): provide distribution's variance, if the distribution's variance is given, the mean is exclusively calculated \n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n  * ``d`` (int): degree of expansion to use for Hermite polynomial when learning truncation set; default 100\n\n* ``store`` (cox.store.Store): logging object to keep track distribution's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``loc_`` (torch.Tensor): distribution's estimated mean \n* ``covariance_matrix_`` (torch.Tensor): distribution's estimated covariance matrix \n\nIn the following code block, here, we show an example of how to use the truncated multivariate normal distribution module: \n   \n.. code-block:: python\n\n  from torch import Tensor\n  from delphi.distributions.truncated_multivariate_normal import TruncatedMultivariateNormal\n  from delphi.utils.helpers import Parameters\n  from delphi import oracle\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate 0 (ie. S = {x >= 0 for all x in S})\n  phi = oracle.Left_Distribution(Tensor([0.0, 0.0]))\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                              'alpha': alpha, \n                              'd': 100})\n  # define truncated multivariate normal distribution object\n  truncated = TruncatedMultivariateNormal(train_kwargs, store=store)\n  # fit to dataset\n  truncated.fit(S)\n  # close store \n  store.close()\n\nAfter fitting the distribution, we now have a membership oracle that we learned through a hermite polynomial. In the following code block, \nwe show an example of how use the membership oracle: \n\n.. code-block:: python\n\n  import torch as ch\n  from torch.distributions.multivariate_normal import MultivariateNormal \n\n  # generate samples from a standard multivariate normal distribution\n  M = MultivariateNormal(ch.zeros(2,), ch.eye(2))\n  samples = M.rsample([1000,])\n  # filter samples with learning membership oracle\n  filtered = train_kwargs.phi(samples)\n\n\nTruncatedBernoullli:\n--------------------\n``TruncatedBooleanProduct`` learns truncated boolean product distributions, by maximizing the truncated log likelihood.\nThe algorithm that we use for this procedure is described in the following\npaper `Efficient Parameter Estimation of Truncated Boolean Product Distributions <https://arxiv.org/abs/2007.02392>`_.\n\nWhen evaluating truncated multivariate normal distributions, the user needs to ``import`` the ``TruncatedBernoulli`` module. The ``TruncatedBernoulli`` module accepts \na parameters object that the user can define for running the PSGD procedure. \n\nParameters:\n-----------\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n\n* ``store`` (cox.store.Store): logging object to keep track distribution's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``probs_`` (torch.Tensor): distribution's d-dimensional probability vector \n* ``logits_`` (torch.Tensor): distribution's d-dimensional logits vector (log probabilities) \n\nIn the following code block, here, we show an example of how to use the truncated multivariate normal distribution module: \n   \n.. code-block:: python\n\n  from torch import Tensor\n  from delphi.distributions.truncated_boolean_product import TruncatedBernoulli\n  from delphi.utils.helpers import Parameters\n  from delphi import oracle\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # sum floor truncate at 0 (ie. S = {x.sum() >= 50 for all x in S})\n  phi = oracle.Sum_Floor(50)\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                              'alpha': alpha})\n  # define truncated bernoulli distribution object\n  trunc_bool = TruncatedBernoulli(train_kwargs, store=store)\n  # fit to dataset\n  trunc_bool.fit(S)\n  # close store \n  store.close()\n\nstats\n=====\n\nTruncatedLinearRegression:\n--------------------------\n``TruncatedLinearRegression`` learns from truncated linear regression model's with the noise \nvariance is known or unknown. In the known setting we use the algorithm described in the following\npaper: `Computationally and Statistically Efficient Truncated Regression <https://arxiv.org/abs/2010.12000>`_. When \nthe variance of the ground-truth linear regression's model is unknown, we use the algorithm described in \nthe following paper: `Efficient Truncated Linear Regression with Unknown Noise Variance`.\n\nWhen evaluating truncated regression models, the user needs three things; an oracle, a Callable that \nindicates whether a sample falls within the truncation set, the model's ``alpha``, survival probability, and the ``TruncatedLinearRegression`` module.  The ``TruncatedLinearRegression`` module accepts \na parameters object that the user can define for running the PSGD procedure.\n\nParameters:\n~~~~~~~~~~\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``noise_var`` (float): provide noise variance, if the noise variance for the truncated regression model is known, else unknown variance procedure is run by default\n  * ``fit_intercept`` (bool): whether to fit the intercept or not; default to True\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``var_lr`` (float): initial learning rate to use variance parameters, when running unknown variance \n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``normalize`` (bool): our methods assume that the :math:`max(||x_{i}||_{2}) <= 1`, so before running the procedure, you must  divide the input featurers :math:`X = {x_{(1)}, x_{(2)}, ... , x_{(n)}}` by :math:`\\max(||x_{i}||_{2}) \\dot \\sqrt(k)`, where :math:`k` represents the number of dimensions the input features have; by default the procedure normalizes the features for the user\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n\n* ``store`` (cox.store.Store): logging object to keep track regression's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``coef_`` (torch.Tensor): regression weight coefficients \n* ``intercept_`` (torch.Tensor): regression intercept term \n* ``variance_`` (torch.Tensor): if the noise variance is unknown, this property provides its estimate\n\nIn the following code block, here, we show an example of how to use the library with unknown noise variance: \n   \n.. code-block:: python\n\n  from delphi.stats.truncated_linear_regression import TruncatedLinearRegression\n  from delphi import oracle\n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate linear regression at 0 (ie. S = {y >= 0 for all (x, y) in S})\n  phi = oracle.Left_Regression(0.0)\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                              'alpha': alpha})\n  # define trunc linear regression object \n  trunc_reg = TruncatedLinearRegression(train_kwargs, store=store)\n  # fit to dataset\n  trunc_reg.fit(X, y)\n  # close store \n  store.close()\n  # make predictions with new regression\n  print(trunc_reg.predict(X))\n\nMethods: \n~~~~~~~~\n\n* ``predict(X)``: predict regression points for input feature matrix X (num_samples by features)\n\nTruncatedLassoRegression:\n--------------------------\n``TruncatedLassoRegression`` learns from truncated LASSO regression model's with the noise \nvariance is known. In the known setting we use the algorithm described in the following\npaper `Truncated Linear Regression in High Dimensions <https://arxiv.org/abs/2007.14539>`_\n\nWhen evaluating truncated lasso regression models, the user needs three things; an oracle, a Callable that \nindicates whether a sample falls within the truncation set, the model's ``alpha``, survival probability, and the ``TruncatedLassoRegression`` module. The ``TruncatedLassoRegression`` module accepts \na parameters object that the user can define for running the PSGD procedure.\n\nParameters:\n~~~~~~~~~~~\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``l1`` (float): l1 regularization\n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``noise_var`` (float): provide noise variance, if the noise variance for the truncated regression model is known, else unknown variance procedure is run by default\n  * ``fit_intercept`` (bool): whether to fit the intercept or not; default to True\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``var_lr`` (float): initial learning rate to use variance parameters, when running unknown variance \n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``normalize`` (bool): our methods assume that the :math:`max(||x_{i}||_{2}) <= 1`, so before running the procedure, you must  divide the input featurers :math:`X = \\{x_{(1)}, x_{(2)}, ... , x_{(n)}\\}` by :math:`max(||x_{i}||_{2}) \\dot \\sqrt(k)`, where :math:`k` represents the number of dimensions the input features have; by default the procedure normalizes the features for the user\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n\n* ``store`` (cox.store.Store): logging object to keep track lasso regression's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``coef_`` (torch.Tensor): regression weight coefficients \n* ``intercept_`` (torch.Tensor): regression intercept term \n* ``variance_`` (torch.Tensor): if the noise variance is unknown, this property provides its estimate\n\nIn the following code block, here, we show an example of how to use the truncated lasso regression module with known noise variance: \n   \n.. code-block:: python\n  \n  from delphi.stats.truncated_lasso_regression import TruncatedLassoRegression\n  from delphi import oracle  \n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate lasso regression at 0 (ie. S = {y>= 0 for all (x, y) in S})\n  phi = oracle.Left_Regression(0.0)\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                            'alpha': alpha, \n                            'noise_var': 1.0})\n  # define trunc linear LASSO regression object\n  trunc_lasso_reg = TruncatedLassoRegression(train_kwargs, store=store)\n  # fit to dataset\n  trunc_lasso_reg.fit(X, y)\n  # close store \n  store.close()\n  # make predictions with new regression\n  print(trunc_lasso_reg.predict(X))\n\nMethods: \n~~~~~~~~\n\n* ``predict(X)``: predict regression points for input feature matrix X (num_samples by features)\n\nTruncatedRidgeRegression:\n--------------------------\n``TruncatedRidgeRegression`` learns from truncated ridge regression model's when the noise \nvariance is known or unknown. \n\nWhen evaluating truncated ridge regression models, the user needs three things; an oracle, a Callable that \nindicates whether a sample falls within the truncation set, the model's ``alpha``, survival probability, and the ``TruncatedRidgeRegression`` module. The ``TruncatedRidgeRegression`` module accepts \na parameters object that the user can define for running the PSGD procedure.\n\nParameters:\n~~~~~~~~~~~\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``weight_decay`` (float): weight decay regularization\n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``noise_var`` (float): provide noise variance, if the noise variance for the truncated regression model is known, else unknown variance procedure is run by default\n  * ``fit_intercept`` (bool): whether to fit the intercept or not; default to True\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``var_lr`` (float): initial learning rate to use variance parameters, when running unknown variance \n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``normalize`` (bool): our methods assume that the :math:`max(||x_{i}||_{2}) <= 1`, so before running the procedure, you must  divide the input featurers :math:`X = \\{x_{(1)}, x_{(2)}, ... , x_{(n)}\\}` by :math:`max(||x_{i}||_{2}) \\dot \\sqrt(k)`, where :math:`k` represents the number of dimensions the input features have; by default the procedure normalizes the features for the user\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n\n* ``store`` (cox.store.Store): logging object to keep track lasso regression's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``coef_`` (torch.Tensor): regression weight coefficients \n* ``intercept_`` (torch.Tensor): regression intercept term \n* ``variance_`` (torch.Tensor): if the noise variance is unknown, this property provides its estimate\n\nIn the following code block, here, we show an example of how to use the truncated lasso regression module with known noise variance: \n   \n.. code-block:: python\n  \n  from delphi.stats.truncated_ridge_regression import TruncatedRidgeRegression\n  from delphi import oracle  \n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate lasso regression at 0 (ie. S = {y>= 0 for all (x, y) in S})\n  phi = oracle.Left_Regression(0.0)\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                            'alpha': alpha, \n                            'weight_decay': .01,\n                            'noise_var': 1.0})\n  # define trunc linear LASSO regression object\n  trunc_ridge_reg = TruncatedRidgeRegression(train_kwargs, store=store)\n  # fit to dataset\n  trunc_ridge_reg.fit(X, y)\n  # close store \n  store.close()\n  # make predictions with new regression\n  print(trunc_ridge_reg.predict(X))\n\nMethods: \n~~~~~~~~\n\n* ``predict(X)``: predict regression points for input feature matrix X (num_samples by features)\n\nTruncatedElasticNetRegression:\n--------------------------\n``TruncatedElasticNetRegression`` learns from truncated elastic net regression model's when the noise \nvariance is known or unknown. \n\nWhen evaluating truncated elastic net regression models, the user needs three things; an oracle, a Callable that \nindicates whether a sample falls within the truncation set, the model's ``alpha``, survival probability, and the ``TruncatedElasticNetRegression`` module. The ``TruncatedRidgeRegression`` module accepts \na parameters object that the user can define for running the PSGD procedure.\n\nParameters:\n~~~~~~~~~~~\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``weight_decay`` (float): weight decay regularization\n  * ``l1`` (float): l1 regularization\n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``noise_var`` (float): provide noise variance, if the noise variance for the truncated regression model is known, else unknown variance procedure is run by default\n  * ``fit_intercept`` (bool): whether to fit the intercept or not; default to True\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``var_lr`` (float): initial learning rate to use variance parameters, when running unknown variance \n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``normalize`` (bool): our methods assume that the :math:`max(||x_{i}||_{2}) <= 1`, so before running the procedure, you must  divide the input featurers :math:`X = \\{x_{(1)}, x_{(2)}, ... , x_{(n)}\\}` by :math:`max(||x_{i}||_{2}) \\dot \\sqrt(k)`, where :math:`k` represents the number of dimensions the input features have; by default the procedure normalizes the features for the user\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n\n* ``store`` (cox.store.Store): logging object to keep track lasso regression's train and validation losses   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``coef_`` (torch.Tensor): regression weight coefficients \n* ``intercept_`` (torch.Tensor): regression intercept term \n* ``variance_`` (torch.Tensor): if the noise variance is unknown, this property provides its estimate\n\nIn the following code block, here, we show an example of how to use the truncated lasso regression module with known noise variance: \n   \n.. code-block:: python\n  \n  from delphi.stats.truncated_elastic_net_regression import TruncatedElasticNetRegression\n  from delphi import oracle  \n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate lasso regression at 0 (ie. S = {y>= 0 for all (x, y) in S})\n  phi = oracle.Left_Regression(0.0)\n  # pass algorithm parameters in through Parameters object\n  train_kwargs = Parameters({'phi': phi, \n                            'alpha': alpha, \n                            'weight_decay': .01,\n                            'noise_var': 1.0})\n  # define trunc linear LASSO regression object\n  trunc_elastic_reg = TruncatedRidgeRegression(train_kwargs, store=store)\n  # fit to dataset\n  trunc_elastic_reg.fit(X, y)\n  # close store \n  store.close()\n  # make predictions with new regression\n  print(trunc_elastic_reg.predict(X))\n\nMethods: \n~~~~~~~~\n\n* ``predict(X)``: predict regression points for input feature matrix X (num_samples by features)\n\nTruncatedLogisticRegression:\n--------------------------\n``TruncatedLogisticRegression`` learns truncated logistic regression models by maximizing the truncated log likelihood.\nThe algorithm that we use for this procedure is described in the following\npaper `A Theoretical and Practical Framework for Classification and Regression from Truncated Samples <https://proceedings.mlr.press/v108/ilyas20a.html>`_.\n.\n\nWhen evaluating truncated logistic regression models, the user needs three things; an oracle, a Callable that \nindicates whether a sample falls within the truncation set, the model's ``alpha``, survival probability, and the ``TruncatedLogisticRegression`` module. The ``TruncatedLogisticRegression`` module accepts \na parameters object that the user can define for running the PSGD procedure. \n\nParameters:\n-----------\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``fit_intercept`` (bool): whether to fit the intercept or not; default to True\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``var_lr`` (float): initial learning rate to use variance parameters, when running unknown variance \n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``normalize`` (bool): our methods assume that the :math:`max(||x_{i}||_{2}) <= 1`, so before running the procedure, you must  divide the input featurers :math:`X = {x_{(1)}, x_{(2)}, ... , x_{(n)}}` by :math:`max(||x_{i}||_{2}) \\dot \\sqrt(k)`, where :math:`k` represents the number of dimensions the input features have; by default the procedure normalizes the features for the user\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change` epochs, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False - just a tdqm output\n\n* ``store`` (cox.store.Store): logging object to keep track logistic regression's train and validation losses and accuracy   \n\nAttributes:\n~~~~~~~~~~~\n\n* ``coef_`` (torch.Tensor): regression weight coefficients \n* ``intercept_`` (torch.Tensor): regression intercept term \n\nIn the following code block, here, we show an example of how to use the truncated logistic regression module: \n   \n.. code-block:: python\n\n  from delphi.stats.truncated_logistic_regression import TruncatedLogisticRegression\n  from delphi import oracle\n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate logistic regression at 0 (ie. S = {z >= -.1 for all (x, y) in S})\n  phi = oracle.Left_Regression(-0.1)\n  # pass algorithm parameters in through parameter object\n  train_kwargs = Parameters({'phi': phi, \n                              'alpha': alpha})\n  # define truncated logistic regression object\n  trunc_log_reg = TruncatedLogisticRegression(train_kwargs, store=store)\n  # fit to dataset\n  trunc_log_reg.fit(X, y)\n  # close store \n  store.close()\n  # make predictions with new regression\n  print(trunc_log_reg.predict(X))\n\n\nMethods: \n~~~~~~~~\n\n* ``predict(X)``: predict classification for input feature matrix X (num_samples by features)\n\n\nTruncatedProbitRegression:\n--------------------------\n``TruncatedProbitRegression`` learns truncated probit regression models, by maximizing the truncated log likelihood.\nThe algorithm that we use for this procedure is described in the following\npaper `A Theoretical and Practical Framework for Classification and Regression from Truncated Samples <https://proceedings.mlr.press/v108/ilyas20a.html>`_.\n\nWhen evaluating truncated logistic regression models, the user needs three things; an oracle, a Callable that \nindicates whether a sample falls within the truncation set, the model's ``alpha``, survival probability, and ``TruncatedProbitRegression`` module.  The ``TruncatedProbitRegression`` module accepts \na parameters object that the user can define for running the PSGD procedure.\n\nParameters:\n-----------\n\n* ``args`` (delphi.utils.Parameters): parameters object that holds hyperparameters for experiment. Possible hyperparameters include:\n\n  * ``phi`` (Callable): required argument; callable class that receives num_samples by 1 input ``torch.Tensor``, and returns a num_samples by 1 outputs a num_samples by 1 ``Tensor`` with ``(0, 1)`` representing membership in ``S`` or not.\n  * ``alpha`` (float): required argument; survivial probability for truncated regression\n  * ``epochs`` (int): maximum number of times to iterate over dataset\n  * ``fit_intercept`` (bool): whether to fit the intercept or not; default to True\n  * ``trials`` (int): maximum number of trials to perform PSGD; after trials, model with smallest loss on the dataset is returned\n  * ``val`` (float): percentage of dataset to use for validation set; default .2\n  * ``lr`` (float): initial learning rate to use for regression weights; default 1e-1\n  * ``step_lr`` (int): number of gradient steps to take before adjusting learning rate by value ``step_lr_gamma``; default 100\n  * ``step_lr_gamma`` (float): amount to adjust learning rate, every ``step_lr`` steps ``new_lr = curr_lr * step_lr_gamma``\n  * ``custom_lr_multiplier`` (str): `cosine` or `cyclic` for cosine annealing learning rate scheduling or cyclic learning rate scheduling; default None\n  * ``momentum`` (float): momentum; default 0.0 \n  * ``adam`` (bool): use adam adaptive learning rate optimizer; default False\n  * ``eps`` (float): epsilon denominator for gradients (ie. to prevent divide by zero calculations); default 1e-5\n  * ``r`` (float): initial projection set radius; default 1.0\n  * ``rate`` (float): at the end of each trial, the projection set radius is increased at rate `rate`; default 1.5\n  * ``normalize`` (bool): our methods assume that the :math:`max(||x_{i}||_{2}) <= 1`, so before running the procedure, you must  divide the input featurers :math:`X = \\{x_{(1)}, x_{(2)}, ... , x_{(n)}\\}` by :math:`max(||x_{i}||_{2}) \\dot \\sqrt(k)`, where :math:`k` represents the number of dimensions the input features have; by default the procedure normalizes the features for the user\n  * ``batch_size`` (int): the number of samples to use for each gradient step; default 50\n  * ``tol`` (float): if using early stopping, threshold for when to stop; default 1e-3\n  * ``workers`` (int): number of workers to use for procedure; default 1\n  * ``num_samples`` (int): number of samples to sample from distribution in gradient for each sample in batch (ie. if batch size is 10, and num_samples is 100, the each gradient step with sample 100 * 10 samples from a gaussian distribution); default 50\n  * ``early_stopping`` (bool): whether to check loss for convergence; compares the best avg validation loss at the end of an epoch, with current avg epoch loss estimate, if :math:`best_loss - curr_loss < tol` for `n_iter_no_change`, then procedure terminates; default False\n  * ``n_iter_no_change`` (int): number of iterations to check for change before declaring convergence; default 5\n  * ``verbose`` (bool): whether to print a verbose output with loss logs, etc.; default False \n\n* ``store`` (cox.store.Store): logging object to keep track probit regression's train and validation losses and accuracy \n\nAttributes:\n~~~~~~~~~~~\n\n* ``coef_`` (torch.Tensor): regression weight coefficients \n* ``intercept_`` (torch.Tensor): regression intercept term \n\nIn the following code block, here, we show an example of how to use the truncated probit regression module: \n   \n.. code-block:: python\n\n  from delphi.stats.truncated_probit_regression import TruncatedProbitRegression\n  from delphi import oracle\n  from delphi.utils.helpers import Parameters\n  from cox.store import Store\n\n  OUT_DIR = 'PATH_TO_EXPERIMENT_LOGGING_DIRECTORY'\n  store = Store(OUT_DIR)\n\n  # left truncate probit regression at 0 (ie. S = {z >= -0.1 for all (x, y) in S})\n  phi = oracle.Left_Regression(-0.1)\n  # pass algorithm parameters in through dictionary\n  train_kwargs = Parameters({'phi': phi, \n                              'alpha': alpha})\n  # define truncated probit regression object\n  trunc_prob_reg = TruncatedProbitRegression(train_kwargs, store=store)\n  # fit to dataset\n  trunc_prob_reg.fit(X, y)\n  # close store \n  store.close()\n  # make predictions with new regression\n  print(trunc_prob_reg.predict(X))\n\nMethods: \n~~~~~~~~\n\n* ``predict(X)``: predict classification for input feature matrix X (num_samples by features)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/pstefanou12/delphi",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "delphi.ai",
    "package_url": "https://pypi.org/project/delphi.ai/",
    "platform": null,
    "project_url": "https://pypi.org/project/delphi.ai/",
    "project_urls": {
      "Homepage": "https://github.com/pstefanou12/delphi",
      "Repository": "https://github.com/pstefanou12/delphi"
    },
    "release_url": "https://pypi.org/project/delphi.ai/0.2.2.0/",
    "requires_dist": [
      "tqdm",
      "grpcio",
      "psutil",
      "gitpython",
      "py3nvml",
      "cox",
      "scikit-learn",
      "seaborn",
      "torch",
      "torchvision",
      "pandas",
      "numpy",
      "scipy",
      "GPUtil",
      "dill",
      "tensorboardX",
      "tables",
      "matplotlib",
      "orthnet"
    ],
    "requires_python": ">=3.6",
    "summary": "Package for Robust Statistics",
    "version": "0.2.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13723553,
  "releases": {
    "0.2.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d212a63374be176dd18d00788d38a205f1be58b072021eaaa887cdaaa5dedc45",
          "md5": "291947559c7f4e62b9fb0bb9ab77eb9c",
          "sha256": "62b8cbeb906d214230d0e293541e2677184f20dd1a5e2620c8f7bb95fd517850"
        },
        "downloads": -1,
        "filename": "delphi.ai-0.2.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "291947559c7f4e62b9fb0bb9ab77eb9c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 78139,
        "upload_time": "2022-02-10T16:01:32",
        "upload_time_iso_8601": "2022-02-10T16:01:32.805024Z",
        "url": "https://files.pythonhosted.org/packages/d2/12/a63374be176dd18d00788d38a205f1be58b072021eaaa887cdaaa5dedc45/delphi.ai-0.2.1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d177243a4da810e2763db86592d81fcfe31ec97c73c8b691481525f09a468681",
          "md5": "75ce09d753df522ec193bc86e1880a5d",
          "sha256": "3af972e86c8ffe2004ac34504b59b51548101b40de03ce0420d95ef08b756ed7"
        },
        "downloads": -1,
        "filename": "delphi.ai-0.2.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "75ce09d753df522ec193bc86e1880a5d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 63206,
        "upload_time": "2022-02-10T16:01:34",
        "upload_time_iso_8601": "2022-02-10T16:01:34.476939Z",
        "url": "https://files.pythonhosted.org/packages/d1/77/243a4da810e2763db86592d81fcfe31ec97c73c8b691481525f09a468681/delphi.ai-0.2.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d9cd7aa92399881be262b3ea1ee6532d1760dc25d526187bd66e6cc57c880e60",
          "md5": "5ad456340aa77d71cc8f27589af31f16",
          "sha256": "ccb4df896883e4aac974d8762f7f4c6f9b5b4ec103c2c082f33d1338495bd184"
        },
        "downloads": -1,
        "filename": "delphi.ai-0.2.1.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5ad456340aa77d71cc8f27589af31f16",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 78146,
        "upload_time": "2022-02-10T16:40:14",
        "upload_time_iso_8601": "2022-02-10T16:40:14.830619Z",
        "url": "https://files.pythonhosted.org/packages/d9/cd/7aa92399881be262b3ea1ee6532d1760dc25d526187bd66e6cc57c880e60/delphi.ai-0.2.1.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "402d8d5e49c1559e045b5843114cbbe2086d239003cf85cfa5cd365c4bbea1b5",
          "md5": "0db4745ed52654cf1f20692d1d54fc91",
          "sha256": "7ac5f0f5bb2054153bd97f2dfa67eb34e576684a54efaa892848a906ab93ef64"
        },
        "downloads": -1,
        "filename": "delphi.ai-0.2.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "0db4745ed52654cf1f20692d1d54fc91",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 63225,
        "upload_time": "2022-02-10T16:40:16",
        "upload_time_iso_8601": "2022-02-10T16:40:16.290756Z",
        "url": "https://files.pythonhosted.org/packages/40/2d/8d5e49c1559e045b5843114cbbe2086d239003cf85cfa5cd365c4bbea1b5/delphi.ai-0.2.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e5696d693815d49d8aeb6e9459299142e91e8875544a01e5046a00568bcada5b",
          "md5": "ec9785fa9b79f65da6f78f07dfe3999a",
          "sha256": "2447e04683124aa7c5a0f0e9fa0fd4886f757f78c34ce9f052653de9b6f3da81"
        },
        "downloads": -1,
        "filename": "delphi.ai-0.2.1.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ec9785fa9b79f65da6f78f07dfe3999a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 95251,
        "upload_time": "2022-05-03T17:09:09",
        "upload_time_iso_8601": "2022-05-03T17:09:09.098909Z",
        "url": "https://files.pythonhosted.org/packages/e5/69/6d693815d49d8aeb6e9459299142e91e8875544a01e5046a00568bcada5b/delphi.ai-0.2.1.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b5f6b02f5d0eef0a46f57a8719b2984c05e59cbb8fd4fd323c549bcbd1f29d21",
          "md5": "4394ca45d672bf9bb8c516e4a1354e77",
          "sha256": "bf8bab97a049c20b4741172dc76c791c4fa4459b6f313a94af4f8ea1155fdd87"
        },
        "downloads": -1,
        "filename": "delphi.ai-0.2.1.9.tar.gz",
        "has_sig": false,
        "md5_digest": "4394ca45d672bf9bb8c516e4a1354e77",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 65465,
        "upload_time": "2022-05-03T17:09:10",
        "upload_time_iso_8601": "2022-05-03T17:09:10.473885Z",
        "url": "https://files.pythonhosted.org/packages/b5/f6/b02f5d0eef0a46f57a8719b2984c05e59cbb8fd4fd323c549bcbd1f29d21/delphi.ai-0.2.1.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e849aa2503ddd7155556d72d42acf0a875ce3c568b6758febf1944a7890010fa",
          "md5": "6a86aa0831f4165f5fd6cd6031988788",
          "sha256": "be22eafc651c1b5c048cdb14ac8e0565f1ba4a4754328d5ef8605a56aa28a79f"
        },
        "downloads": -1,
        "filename": "delphi.ai-0.2.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6a86aa0831f4165f5fd6cd6031988788",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 95298,
        "upload_time": "2022-05-05T18:10:44",
        "upload_time_iso_8601": "2022-05-05T18:10:44.551020Z",
        "url": "https://files.pythonhosted.org/packages/e8/49/aa2503ddd7155556d72d42acf0a875ce3c568b6758febf1944a7890010fa/delphi.ai-0.2.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c11ab450eb78c598980b8feeb970e1d46c22a663ea2a44af00ec51fe8ba51240",
          "md5": "0a22372fdba48766635ad3957d4756e9",
          "sha256": "048f46b7ff5d777e7a1f4259d0258ae10b99a8c75ab31e3b20c910aaf556fe08"
        },
        "downloads": -1,
        "filename": "delphi.ai-0.2.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0a22372fdba48766635ad3957d4756e9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 65518,
        "upload_time": "2022-05-05T18:10:46",
        "upload_time_iso_8601": "2022-05-05T18:10:46.118660Z",
        "url": "https://files.pythonhosted.org/packages/c1/1a/b450eb78c598980b8feeb970e1d46c22a663ea2a44af00ec51fe8ba51240/delphi.ai-0.2.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e849aa2503ddd7155556d72d42acf0a875ce3c568b6758febf1944a7890010fa",
        "md5": "6a86aa0831f4165f5fd6cd6031988788",
        "sha256": "be22eafc651c1b5c048cdb14ac8e0565f1ba4a4754328d5ef8605a56aa28a79f"
      },
      "downloads": -1,
      "filename": "delphi.ai-0.2.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "6a86aa0831f4165f5fd6cd6031988788",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 95298,
      "upload_time": "2022-05-05T18:10:44",
      "upload_time_iso_8601": "2022-05-05T18:10:44.551020Z",
      "url": "https://files.pythonhosted.org/packages/e8/49/aa2503ddd7155556d72d42acf0a875ce3c568b6758febf1944a7890010fa/delphi.ai-0.2.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c11ab450eb78c598980b8feeb970e1d46c22a663ea2a44af00ec51fe8ba51240",
        "md5": "0a22372fdba48766635ad3957d4756e9",
        "sha256": "048f46b7ff5d777e7a1f4259d0258ae10b99a8c75ab31e3b20c910aaf556fe08"
      },
      "downloads": -1,
      "filename": "delphi.ai-0.2.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "0a22372fdba48766635ad3957d4756e9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 65518,
      "upload_time": "2022-05-05T18:10:46",
      "upload_time_iso_8601": "2022-05-05T18:10:46.118660Z",
      "url": "https://files.pythonhosted.org/packages/c1/1a/b450eb78c598980b8feeb970e1d46c22a663ea2a44af00ec51fe8ba51240/delphi.ai-0.2.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}