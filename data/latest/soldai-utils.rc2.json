{
  "info": {
    "author": "Soldai Research",
    "author_email": "mcampos@soldai.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# sutil\nThis repository contains a set of tools to deal with machine learning and natural language processing tasks, including classes to make quick experimentation of different classifacation models.\n\n## Dataset\nThis class is made to load csv styles dataset's where all the features are comma separeted and the class is in the last column.\nIt includes functions to normalize the features, add bias, save the data to a file and load from it. Also includes functions\nto split the train, validation and test datasets.\n\n```python\nfrom sutil.base.Dataset import Dataset\n\ndatafile = './sutil/datasets/ex2data1.txt'\nd = Dataset.fromDataFile(datafile, ',')\nprint(d.size)\n\nsample = d.sample(0.3)\nprint(sample.size)\n\nsample.save(\"modelo_01\")\n\ntrain, validation, test = d.split(train = 0.8, validation = 0.2)\nprint(train.size)\nprint(validation.size)\nprint(test.size)\n```\n\n## Regularized Logistic Regression\nYou can also include your own models as a Regularized Logistic Regression, implemented manually using numpy and included in the sutil.models package\n```python\nimport numpy as np\nfrom sutil.base.Dataset import Dataset\nfrom sutil.models.RegularizedLogisticRegression import RegularizedLogisticRegression\n\ndatafile = './sutil/datasets/ex2data1.txt'\nd = Dataset.fromDataFile(datafile, ',')\nd.xlabel = 'Exam 1 score'\nd.ylabel = 'Exam 2 score'\nd.legend = ['Admitted', 'Not admitted']\niterations = 400\nprint('Plotting data with + indicating (y = 1) examples and o indicating (y = 0) examples.\\n')\nd.plotData()\n\ntheta = np.zeros((d.n + 1, 1))\nlr = RegularizedLogisticRegression(theta, 0.03, 0, train=1)\nlr.trainModel(d)\nlr.score(d.X, d.y)\nlr.roc.plot()\nlr.roc.zoom((0, 0.4),(0.5, 1.0))\n```\n\n## Sklearn model \nYou can also embed the sklearn models in a wrapper class in order to run experiments with diferent models implemented in sklearn. In the same style you can create tensorflow, keras or pytorch models inhyereting from sutil.modes.Model class and\nimplementing the trainModel and predict methods.\n```python\nimport numpy as np\nfrom sutil.base.Dataset import Dataset\nfrom sutil.models.SklearnModel import SklearnModel\nfrom sklearn.linear_model import LogisticRegression\n\ndatafile = './sutil/datasets/ex2data1.txt'\nd = Dataset.fromDataFile(datafile, ',')\nms = LogisticRegression()\nm = SklearnModel('Sklearn Logistic', ms)\nm.trainModel(d)\nm.score(d.X, d.y)\nm.roc.plot()\nm.roc.zoom((0, 0.4),(0.5, 1.0))\n```\n\n## Neural Network Classifer\nThis class let's you perform classifcation using a Neural Network, multiperceptron classifer. It wraps the sklearn MLPClassifer \nand implements a method to search different activations, solvers and hidden layers structures. Upu can pass \nyour own arguments to initialize the network as you want.\n```python\nfrom sutil.base.Dataset import Dataset\nfrom sutil.neuralnet.NeuralNetworkClassifier import NeuralNetworkClassifier\n\ndatafile = './sutil/datasets/ex2data1.txt'\nd = Dataset.fromDataFile(datafile, ',')\nd.normalizeFeatures()\nsample = d.sample(examples = 30)\n\nnn = NeuralNetworkClassifier((d.n, len(d.labels)))\nnn.searchParameters(sample)\nnn.trainModel(d)\nnn.score(d.X, d.y)\nnn.roc.plot()\n```\n\n## Experiment\nThe experiment class let's you perform the data split and test against different models to compare the \nperformance automatically\n```python\nimport numpy as np\nfrom sutil.base.Dataset import Dataset\nfrom sklearn.linear_model import LogisticRegression\nfrom sutil.base.Experiment import Experiment\nfrom sutil.models.SklearnModel import SklearnModel\nfrom sutil.models.RegularizedLogisticRegression import RegularizedLogisticRegression\nfrom sutil.neuralnet.NeuralNetworkClassifier import NeuralNetworkClassifier\n\n# Load the data\ndatafile = './sutil/datasets/ex2data1.txt'\nd = Dataset.fromDataFile(datafile, ',')\nd.normalizeFeatures()\nprint(\"Size of the dataset... \")\nprint(d.size)\nsample = d.sample(0.3)\nprint(\"Size of the sample... \")\nprint(d.sample)\n\n\n# Create the models\ntheta = np.zeros((d.n + 1, 1))\nlr = RegularizedLogisticRegression(theta, 0.03, 0)\nm = SklearnModel('Sklearn Logistic', LogisticRegression())\n# Look for the best parameters using a sample\nnn = NeuralNetworkClassifier((d.n, len(d.labels)))\nnn.searchParameters(sample)\n\ninput(\"Press enter to continue...\")\n\n# Create the experiment\nexperiment = Experiment(d, None, 0.8, 0.2)\nexperiment.addModel(lr, name = 'Sutil Logistic Regression')\nexperiment.addModel(m, name = 'Sklearn Logistic Regression')\nexperiment.addModel(nn, name = 'Sutil Neural Network')\n\n# Run the experiment\nexperiment.run(plot = True)\n```\n\n# Text utilities\nSutil includes text utilities to process and transform text for classification\n\n## PreProcessor\nPre processor class let's you implement text pre processinf functions to transform the data. It wraps nltk methods and uses it's own methods toperform:\n* Case normalization\n* Noise removal\n* Stemming\n* Leammatizing\n* Pattern text normalization\n```python\nfrom sutil.text.PreProcessor import PreProcessor\n\nstring = \"La Gata maullaba en la noche $'@|··~½¬½¬{{[[]}aqAs   qasdas 1552638\"\np = PreProcessor.standard()\nprint(p.preProcess(string))\n\npatterns = [(\"\\d+\", \"NUMBER\")]\nc = [(\"case\", \"lower\"), (\"denoise\", \"spanish\"), (\"stopwords\", \"spanish\"), \n     (\"stem\", \"spanish\"), (\"lemmatize\", \"spanish\"), (\"normalize\", patterns)]\np2 = PreProcessor(c)\nprint(p2.preProcess(string))\n\n\nc = [(\"case\", \"lower\"), (\"denoise\", \"spanish\"), (\"stem\", \"spanish\"), \n     (\"normalize\", patterns)]\np3 = PreProcessor(c)\nprint(p3.preProcess(string))\n```\n\n## PhraseTokenizer\nPhraseTokenizer lets you split the tokens of a phrase given a delimiter char. There's also the **GramTokenizer** class whihc lets you split words by fixed amounts of characers.\n```python\nfrom sutil.text.GramTokenizer import GramTokenizer\nfrom sutil.text.PhraseTokenizer import PhraseTokenizer\n\nstring = \"Hi I'm a really helpful string\"\nt = PhraseTokenizer()\nprint(t.tokenize(string))\n\nt2 = GramTokenizer()\nprint(t2.tokenize(string))\n```\n\n## TextVectorizer\nTextVectorizer class is an abstraction of methods to vectorize text and transform vectors to texts. Sutil implements, **OneHotVectorizer**, **TFIDFVectorizer**, **CountVectorizer**. \n```python\nimport pandas as pd\nfrom sutil.text.TFIDFVectorizer import TFIDFVectorizer\nfrom sutil.text.GramTokenizer import GramTokenizer\nfrom sutil.text.PreProcessor import PreProcessor\nfrom nltk.tokenize import TweetTokenizer\n\n# Load the data\ndir_data = \"./sutil/datasets/\"\ndf = pd.read_csv(dir_data + 'tweets.csv')\n\n# Clean the data\npatterns = [(\"\\d+\", \"NUMBER\")]\nc = [(\"case\", \"lower\"), (\"denoise\", \"english\"), (\"stopwords\", \"english\"), (\"normalize\", patterns)]\np2 = PreProcessor(c)\ndf['clean_tweet'] = df.tweet.apply(p2.preProcess)\n\nvectorizer = TFIDFVectorizer({}, TweetTokenizer())\nvectorizer.initialize(df.clean_tweet)\nprint(vectorizer.dictionary.head())\n\n\nvectorizer2 = TFIDFVectorizer({}, GramTokenizer())\nvectorizer2.initialize(df.clean_tweet)\nprint(vectorizer2.dictionary.head())\n\nvector = vectorizer.encodePhrase(df.clean_tweet[0])\nprint(vectorizer.getValues()[0])\nprint(vector)\n\nvector2 = vectorizer2.encodePhrase(df.clean_tweet[0])\nprint(vectorizer2.getValues()[0])\nprint(vector2)\n\nprint(df.clean_tweet[0])\nprint(vectorizer.decodeVector(vector))\nprint(\"*\"*50)\nprint(vectorizer2.decodeVector(vector2))\n```\n## TextDataSet\n**TextDataSet** class abstracts a DataSet made by texts, with includes a vectorizer and a pre processor to pre process the text and transform it from text to vector and from vector to text:\n\n```python\nfrom sutil.text.TextDataset import TextDataset\nfrom sutil.text.GramTokenizer import GramTokenizer\nfrom sutil.text.TFIDFVectorizer import TFIDFVectorizer\nfrom sutil.text.PhraseTokenizer import PhraseTokenizer\nfrom sutil.text.PreProcessor import PreProcessor\n\n# Load the data in the standard way\nfilename = \"./sutil/datasets/tweets.csv\"\nt = TextDataset.standard(filename, \",\")\nprint(t.texts)\nprint(t.X)\nprint(t.shape)\nprint(t.X[0])\nprint(t.vectorizer.index)\nprint(t.vectorizer.encodePhrase(\"united oh\"))\n\nx = input(\"Presione enter para continuar...\")\n# Creaate the dataset witha custom vectorizer and pre processor\npatterns = [(\"\\d+\", \"NUMBER\")]\nc = [(\"case\", \"lower\"), (\"denoise\", \"spanish\"), (\"stopwords\", \"spanish\"), (\"normalize\", patterns)]\npreprocessor = PreProcessor(c)\nvectorizer = TFIDFVectorizer({}, GramTokenizer())\nt2 = TextDataset.setvectorizer(filename, vectorizer, preprocessor)\nprint(t2.texts)\nprint(t2.X)\nprint(t2.shape)\nprint(t2.X[0])\nvector = t2.encodePhrase(\"United oh the\")\ni = 0\nfor v in vector:\n\tif v != 0:\n\t\tprint(v)\n\t\tprint(i)\n\ti += 1\nprint(t2.vectorizer.decodeVector(vector))\n\nx = input(\"Presione enter para continuar...\")\npatterns = [(\"\\d+\", \"NUMBER\")]\nc = [(\"case\", \"lower\"), (\"denoise\", \"spanish\"), (\"stopwords\", \"english\"), (\"normalize\", patterns)]\npre2 = PreProcessor(c)\nvectorizer = TFIDFVectorizer({}, PhraseTokenizer())\nt3 = TextDataset.setvectorizer(filename, vectorizer, pre2)\nprint(t3.texts)\nprint(t3.X)\nprint(t3.shape)\nprint(t3.X[0])\nvector = t3.encodePhrase(\"United oh the\")\ni = 0\nfor v in vector:\n\tif v != 0:\n\t\tprint(v)\n\t\tprint(i)\n\ti += 1\nprint(t3.decodeVector(vector))\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/SoldAI/sutil",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "soldai-utils",
    "package_url": "https://pypi.org/project/soldai-utils/",
    "platform": "",
    "project_url": "https://pypi.org/project/soldai-utils/",
    "project_urls": {
      "Homepage": "https://github.com/SoldAI/sutil"
    },
    "release_url": "https://pypi.org/project/soldai-utils/0.0.2/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Soldai utilities for machine learning and text processing",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8346156,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2330a99ee738b6599cf06df09e0c21202b4cc0188500ca346af7df36b29e688a",
          "md5": "e912d145dc13c2aac22ed269039d0aee",
          "sha256": "b95f33107f6521360e55c6c654210d3ee1818b7b07f8e8099c47121b87b15d16"
        },
        "downloads": -1,
        "filename": "soldai_utils-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e912d145dc13c2aac22ed269039d0aee",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27192,
        "upload_time": "2020-01-15T22:30:00",
        "upload_time_iso_8601": "2020-01-15T22:30:00.144790Z",
        "url": "https://files.pythonhosted.org/packages/23/30/a99ee738b6599cf06df09e0c21202b4cc0188500ca346af7df36b29e688a/soldai_utils-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7e470faa2fb9ff73f7ace58e29c0c5ff28da21b9f65e75804b65469658948e17",
          "md5": "4d0bf2e063d66b7033d6c3f27234eeed",
          "sha256": "af980602c019859795a618557d2b08ad51e358140f162dfce6b177602cacb3dc"
        },
        "downloads": -1,
        "filename": "soldai-utils-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4d0bf2e063d66b7033d6c3f27234eeed",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 9725,
        "upload_time": "2020-01-15T22:30:05",
        "upload_time_iso_8601": "2020-01-15T22:30:05.767707Z",
        "url": "https://files.pythonhosted.org/packages/7e/47/0faa2fb9ff73f7ace58e29c0c5ff28da21b9f65e75804b65469658948e17/soldai-utils-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3a553db136ff4061cdfb328af294e2b2a9fb4d9c8ba20e96b994944ac22e59cd",
          "md5": "bd0086711bc6969ce6a623ea0b1c843c",
          "sha256": "219e2e76bfa02f6af3c8abe44bbd5cac9e12427d45dcd576ab7f2362848641d4"
        },
        "downloads": -1,
        "filename": "soldai_utils-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bd0086711bc6969ce6a623ea0b1c843c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 35052,
        "upload_time": "2020-10-05T22:22:11",
        "upload_time_iso_8601": "2020-10-05T22:22:11.383423Z",
        "url": "https://files.pythonhosted.org/packages/3a/55/3db136ff4061cdfb328af294e2b2a9fb4d9c8ba20e96b994944ac22e59cd/soldai_utils-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0234d5fb401b6f4d367a087563b478d8bb3b62d877ade8116b1d4b2f3e51059a",
          "md5": "5707a5b24335402745e255f95222de2e",
          "sha256": "eb946afde6a83b8fae8b19881e611b0187bb93be1779dec4c945f8af233df860"
        },
        "downloads": -1,
        "filename": "soldai-utils-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "5707a5b24335402745e255f95222de2e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 16790,
        "upload_time": "2020-10-05T22:22:13",
        "upload_time_iso_8601": "2020-10-05T22:22:13.379028Z",
        "url": "https://files.pythonhosted.org/packages/02/34/d5fb401b6f4d367a087563b478d8bb3b62d877ade8116b1d4b2f3e51059a/soldai-utils-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3a553db136ff4061cdfb328af294e2b2a9fb4d9c8ba20e96b994944ac22e59cd",
        "md5": "bd0086711bc6969ce6a623ea0b1c843c",
        "sha256": "219e2e76bfa02f6af3c8abe44bbd5cac9e12427d45dcd576ab7f2362848641d4"
      },
      "downloads": -1,
      "filename": "soldai_utils-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "bd0086711bc6969ce6a623ea0b1c843c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 35052,
      "upload_time": "2020-10-05T22:22:11",
      "upload_time_iso_8601": "2020-10-05T22:22:11.383423Z",
      "url": "https://files.pythonhosted.org/packages/3a/55/3db136ff4061cdfb328af294e2b2a9fb4d9c8ba20e96b994944ac22e59cd/soldai_utils-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0234d5fb401b6f4d367a087563b478d8bb3b62d877ade8116b1d4b2f3e51059a",
        "md5": "5707a5b24335402745e255f95222de2e",
        "sha256": "eb946afde6a83b8fae8b19881e611b0187bb93be1779dec4c945f8af233df860"
      },
      "downloads": -1,
      "filename": "soldai-utils-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "5707a5b24335402745e255f95222de2e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 16790,
      "upload_time": "2020-10-05T22:22:13",
      "upload_time_iso_8601": "2020-10-05T22:22:13.379028Z",
      "url": "https://files.pythonhosted.org/packages/02/34/d5fb401b6f4d367a087563b478d8bb3b62d877ade8116b1d4b2f3e51059a/soldai-utils-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}