{
  "info": {
    "author": "carefree0910",
    "author_email": "syameimaru.saki@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "![noli-creator](./static/images/social-image.jpg)\r\n\r\n\r\n[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=AI%20Magics%20meet%20Infinite%20draw%20board!&url=https://github.com/carefree0910/carefree-creator&via=carefree0910&hashtags=stablediffusion,pytorch,developers)\r\n\r\n> Sometimes my poor cloud server will be on **FIRE** 🔥. You can know where your tasks are queued as shown in [this section](#is-it-free), but personally I'll always recommend you to try [local deployment](#webui--local-deployment)!\r\n\r\nAn open sourced, AI-powered creator for everyone.\r\n\r\n- [WebUI](https://creator.nolibox.com/guest) (Recommended!)\r\n  - We also recommend to launch a [Google Colab](https://colab.research.google.com/github/carefree0910/carefree-creator/blob/dev/tests/server.ipynb) server for this WebUI!\r\n  - 我们也提供了一份详尽的、[中文版本的 Google Colab](https://colab.research.google.com/github/carefree0910/carefree-creator/blob/dev/tests/server_zh.ipynb) 哦！\r\n- [Google Colab](https://colab.research.google.com/github/carefree0910/carefree-creator/blob/dev/tests/demo.ipynb) (Very limited features, but very customizable!)\r\n\r\n> This repo (`carefree-creator`) contains the backend server's codes, the **WebUI** codes (`noli-creator`) will be open sourced as well if it gains enough interests 😉.\r\n\r\n\r\n### Table of Content\r\n\r\n- [tl;dr](#tldr)\r\n- [WebUI \\& Local Deployment](#webui--local-deployment)\r\n  - [Here is a Google Colab solution (Recommended!)](#here-is-a-google-colab-solution-recommended)\r\n- [Image Generating Features](#image-generating-features)\r\n  - [Text to Image](#text-to-image)\r\n  - [Generate Variations](#generate-variations)\r\n  - [Sketch to Image](#sketch-to-image)\r\n    - [One more thing](#one-more-thing)\r\n    - [General Image to Image translation](#general-image-to-image-translation)\r\n  - [Generate Circular (Tiling) Textures](#generate-circular-tiling-textures)\r\n  - [Generate Better Anime images](#generate-better-anime-images)\r\n  - [Negative Prompt](#negative-prompt)\r\n  - [Inspect / Copy / Import Parameters](#inspect--copy--import-parameters)\r\n    - [🌟Advanced Usage](#advanced-usage)\r\n  - [Presets](#presets)\r\n    - [Use Preset Capsules](#use-preset-capsules)\r\n    - [Use Preset Panel](#use-preset-panel)\r\n  - [Outpainting (Experimental)](#outpainting-experimental)\r\n  - [Landscape Synthesis (Experimental)](#landscape-synthesis-experimental)\r\n    - [What determines the size of the generated image?](#what-determines-the-size-of-the-generated-image)\r\n    - [I see many 'holes' in your example, do they matter?](#i-see-many-holes-in-your-example-do-they-matter)\r\n- [Image Processing Features](#image-processing-features)\r\n  - [Super Resolution](#super-resolution)\r\n  - [Inpainting](#inpainting)\r\n  - [Erase \\& Replace](#erase--replace)\r\n- [Advanced Usages](#advanced-usages)\r\n  - [Custom Checkpoints](#custom-checkpoints)\r\n  - [Textual Inversion](#textual-inversion)\r\n    - [Features](#features)\r\n    - [Usage](#usage)\r\n- [Installation](#installation)\r\n  - [Hardware Requirements](#hardware-requirements)\r\n  - [pip installation](#pip-installation)\r\n    - [Run](#run)\r\n  - [Docker](#docker)\r\n    - [Prepare](#prepare)\r\n    - [Build](#build)\r\n    - [Run](#run-1)\r\n- [Q\\&A](#qa)\r\n    - [Where are my creations stored?](#where-are-my-creations-stored)\r\n    - [How do I save / load my project?](#how-do-i-save--load-my-project)\r\n    - [How can I contribute to `carefree-creator`?](#how-can-i-contribute-to-carefree-creator)\r\n    - [How can I get my own models interactable on the **WebUI**?](#how-can-i-get-my-own-models-interactable-on-the-webui)\r\n      - [Handy way](#handy-way)\r\n      - [Advanced way](#advanced-way)\r\n      - [API Mappings](#api-mappings)\r\n    - [Why no `GFPGAN`?](#why-no-gfpgan)\r\n    - [Is it FREE?](#is-it-free)\r\n    - [Do you like cats?](#do-you-like-cats)\r\n    - [What about dogs?](#what-about-dogs)\r\n    - [Why did you build this project?](#why-did-you-build-this-project)\r\n    - [How is this different from other WebUIs?](#how-is-this-different-from-other-webuis)\r\n    - [Will there be a Discord Community?](#will-there-be-a-discord-community)\r\n    - [What is `Nolibox`???](#what-is-nolibox)\r\n- [Known Issues](#known-issues)\r\n- [TODO](#todo)\r\n- [Credits](#credits)\r\n\r\n# tl;dr\r\n- An **infinite draw board** for you to save, review and edit all your creations.\r\n- Almost EVERY feature about Stable Diffusion (txt2img, img2img, sketch2img, **variations**, outpainting, circular/tiling textures, sharing, ...).\r\n- Many useful image editing methods (**super resolution**, inpainting, ...).\r\n- Integrations of different Stable Diffusion versions (waifu diffusion, ...).\r\n- GPU RAM optimizations, which makes it possible to enjoy these features with an **NVIDIA GeForce GTX 1080 Ti** (*)!\r\n\r\n> *: As the project grows more and more complicated, I have to introduce a lazy-loading technique to exchange GPU RAM with RAM. See [this section](#hardware-requirements) for more details.\r\n\r\nIt might be fair to consider this as:\r\n- An AI-powered, open sourced(*) **Figma**.\r\n- A more 'interactable' **Hugging Face Space**.\r\n- A place where you can try all the exciting and cutting-edge models, together.\r\n\r\n> *: The **WebUI** codes are not open sourced **yet**, but we are happy to open source them if it is truely helpful 😉.\r\n\r\n\r\n# WebUI & Local Deployment\r\n\r\n## [Here](https://colab.research.google.com/github/carefree0910/carefree-creator/blob/dev/tests/server.ipynb) is a Google Colab solution (Recommended!)\r\n> [Here](#installation) is the local installation guide.\r\n\r\nSince `carefree-creator` is a (fairly) stand-alone FastAPI service, it is possible to use our hosted **WebUI** along with your local server. In fact, we've already provided a switch for you:\r\n\r\n![use-local-server](./static/images/use-local-server.png)\r\n\r\n> The left-most, hand drawing cat is my creation, and `carefree-creator` helped me 'beautify' it a little bit on the right 🤣.\r\n> \r\n> We will show you how to perform `sketch2img` in [this section](#sketch-to-image).\r\n\r\nTo make things fancy we can call it a 'Decentralized Deployment Method' (🤨). Anyway, with local deployment, you can then utilize your own machines to avoid waiting my poor cloud server to generate the images for one or few minutes. What's more, since you deployed for yourself, it will be FREE forever!\r\n\r\n> This also reveals the goal of `carefree-creator`: we handle the messy **WebUI** parts for you, so you can focus on developing cool models and algorithms that can later seamlessly integrate into it.\r\n> \r\n> And, with the possibility to deploy locally, you don't have to wait for me to update my poor cloud server. You can simply make a pull request to the `carefree-creator` and tell me: hey, get this feature to the **WebUI** 😆. And after I updated the **WebUI**, you can already play with it on your local machines!\r\n> \r\n> And of course, as mentioned before, if it gains enough interests, we are happy to open souce the **WebUI** codes as well. In this case, you will have the ability to complete the whole cycle: you can develop your own models, wrap them around to expose APIs, modify **WebUI** to interact with these APIs, and have fun! You can keep your own forks if you want to make them private, or you can make pull requests to the main fork so everyone in the world can also enjoy your works!\r\n\r\n\r\n# Image Generating Features\r\n\r\nImage generating features really opens a brand new world for someone who wants to create but lack of corresponding skills (just like me 🤣). However, generating one single (or, a couple) image at a time without the ability to review/further edit them easily makes creation harder than expected. That's why we support putting all generated images on one single **infinite draw board**, and support trying almost every cool image generating features, together.\r\n\r\nThe features listed in this section hide behind that picture-icon on the left:\r\n\r\n![image-generating-icon](./static/images/image-generating-icon.png)\r\n\r\n## Text to Image\r\n\r\nThis is the most basic and foundamental feature:\r\n\r\n![Text to Image](https://github.com/carefree0910/datasets/releases/download/static/text_to_image.gif)\r\n\r\nBut we added something more. For example, you can choose the style:\r\n\r\n![Text to Image With Style](https://github.com/carefree0910/datasets/releases/download/static/text_to_image_with_style.gif)\r\n\r\nAnd as you can see, there are some other options as well - we will cover most of them in the following sections.\r\n\r\n## Generate Variations\r\n\r\n<details>\r\n<summary>GIF</summary>\r\n<img src=\"https://github.com/carefree0910/datasets/releases/download/static/variation_generation.gif\" alt=\"Variation Generation\" />\r\n</details>\r\n\r\nA very powerful feature that we support is to generate variations. Let's say you generated a nice portrait of `komeiji koishi`:\r\n\r\n![komeiji koishi](./static/images/koishi0.jpg)\r\n\r\nAs I've already highlighted, there is a panel called **Variation Generation**. You can simply click the `Generate` button in it and see what happens:\r\n\r\n![komeiji koishi](./static/images/koishi1.jpg)\r\n\r\nAnother `komeiji koishi` appears!\r\n\r\nYou might have noticed that you can adjust the `Fidelity` of the variation, it indicates how 'similar' the generated image will be to the original image. By lowering it a little bit, you can get even more interesting results:\r\n\r\n![komeiji koishi](./static/images/koishi2.jpg)\r\n\r\nCool!\r\n\r\nAnd why not generate variations based on the generated variations:\r\n\r\n![komeiji koishi](./static/images/koishi3.jpg)\r\n\r\nThe last `komeiji koishi` somehow mimics the art style of `ZUN` 😆!\r\n\r\n## Sketch to Image\r\n\r\n<details>\r\n<summary>GIF</summary>\r\n<img src=\"https://github.com/carefree0910/datasets/releases/download/static/image_translation.gif\" alt=\"Image Translation\" />\r\n</details>\r\n\r\nWe support 'translating' any sketches to images with the given prompt. Although it is not required, we recommend adding an 'Empty Node' (with the 'plus' icon on the top) as a 'canvas' for you to draw on:\r\n\r\n![add-empty-node](./static/images/sketch0.png)\r\n\r\n> You might notice that there is an `Outpainting` panel on the left when you select an Empty Node. We will cover its usage in [this section](#outpainting-experimental).\r\n\r\nAfter our 'canvas' is ready, you can trigger the 'brush' and start drawing!\r\n\r\n![drawing](./static/images/sketch1.png)\r\n\r\n> The position doesn't really matter, we will always center your sketch before uploading it to our server 😉.\r\n\r\nOnce you are satisfied with your wonderful sketch, click the `Finish` button on the right, your drawing will then turn into a selectable Node, and an `Image Translation` panel will appear on the left:\r\n\r\n![image-translation](./static/images/sketch2.png)\r\n\r\n> As you can see, the preview sketch does not contain the 'canvas', that's why we said the 'canvas' is not required.\r\n> \r\n> When the sketch is uploaded to our server, we will fill the background with white color - so don't use white color to draw 😆!\r\n\r\nAfter inputing some related texts, you can scroll down the `Image Translation` panel and click the `Translate` button:\r\n\r\n![image-translation-submit](./static/images/sketch3.png)\r\n\r\nAnd the result should be poped up in a few seconds:\r\n\r\n![image-translation-result](./static/images/sketch4.jpg)\r\n\r\nNot bad!\r\n\r\n### One more thing\r\n\r\nYou don't actually need to worry whether your drawings could be recognized or not - it turns out that Stable Diffusion is pretty capable of recognizing them 😆:\r\n\r\n![image-translation-wild](./static/images/sketch5.jpg)\r\n\r\n### General Image to Image translation\r\n\r\nAlthough I'm using a built-in sketch-to-image to illustrate the concepts, the `Image Translation` is in fact a general `img2img` technique, so you can actually apply it to any images. For instance, you can apply it to the generated image:\r\n\r\n![image-translation-general](./static/images/sketch6.jpg)\r\n\r\nSeems that more details are added!\r\n\r\nWith this technique, you can actually upload your own images (for instance, the paintings that are drawn by kids), and turn them into an 'art piece':\r\n\r\n![image-translation-general](./static/images/img2img.jpg)\r\n\r\n## Generate Circular (Tiling) Textures\r\n\r\n<details>\r\n<summary>GIF</summary>\r\n<img src=\"https://github.com/carefree0910/datasets/releases/download/static/circular_textures.gif\" alt=\"Circular Textures\" />\r\n</details>\r\n\r\nSo what are circular textures? Circular textures are images that can be 'tiled' together, and it is easy to specify `carefree-creator` to generate such textures by toggling the corresponding switch:\r\n\r\n![circular-textures](./static/images/circular-textures0.jpg)\r\n\r\nHmm, nothing special, right? That's because the magic only happens if you 'tile' them together:\r\n\r\n![circular-textures-tile](./static/images/circular-textures1.jpg)\r\n\r\n## Generate Better Anime images\r\n\r\nThanks to [Waifu Diffusion](https://github.com/harubaru/waifu-diffusion), we are able to generate better anime images by toggling the corresponding switch:\r\n\r\n![waifu-diffusion](./static/images/waifu-diffusion.jpg)\r\n\r\n## Negative Prompt\r\n\r\nAfter selecting a **generated** image, we can see a `Negative Prompt` panel on the left:\r\n\r\n![negative-prompt](./static/images/negative_prompt0.jpg)\r\n\r\nWhere you can apply negative prompt to the selected image:\r\n\r\n![negative-prompt-result](./static/images/negative_prompt1.jpg)\r\n\r\n## Inspect / Copy / Import Parameters\r\n\r\nIt's well known that x-Diffusion models need good 'prompts' to generate good images, but what makes good 'prompts' remains mystery. Therefore, we support inspecting parameters of every generated image:\r\n\r\n![inspect-parameters](./static/images/inspect-parameters.jpg)\r\n\r\nYou can copy the `parameters` with the little `Copy` button, and the copied `parameters` can then be pasted to the `Parameters to Image` panel on the left:\r\n\r\n![parameters-to-image](./static/images/parameters-to-image.jpg)\r\n\r\nIn this way, all the creations will be sharable, reproducible and (sort of) understandable!\r\n\r\n### 🌟Advanced Usage\r\n\r\nWith the ability to copy / import the `parameters`, we can actually access to the 'bleeding-edge' features that have not yet introduced to the WebUI. For instance, you might have already noticed that we cannot adjust the `seed`, `steps`, `guidance_scale`, ... of the generation process, but we can actually set them up in the `parameters`:\r\n\r\n```json\r\n{\r\n  \"type\": \"txt2img\",\r\n  \"data\": {\r\n    \"w\": 704,\r\n    \"h\": 512,\r\n    \"text\": \"a beautiful, fantasy landscape, HD\",\r\n    \"use_circular\": false,\r\n    \"is_anime\": false,\r\n    \"seed\": 692615800,\r\n    \"num_steps\": 50,\r\n    \"guidance_scale\": 7.5,\r\n    \"timestamp\": 1665914359287\r\n  }\r\n}\r\n```\r\n\r\nPretty straight forward, isn't it? 😉\r\n\r\n## Presets\r\n\r\nIf you want to generate some really fancy images (like the ones that fly around the internet these days), a good starting point is to use our presets.\r\n\r\n> And by leveraging the `Inspect Parameters` function metioned in the previous section, we can understand what prompts / parameters are used behind these results, and possibly 'learn' how to master these models!\r\n\r\n### Use Preset Capsules\r\n\r\nIf you scroll down the `Text to Image` panel, you will see a `Try these out!` section with many 'capsules':\r\n\r\n![preset-capsules](./static/images/preset-capsules.jpg)\r\n\r\nWe will generate the corresponding images if you click one of these capsules.\r\n\r\n### Use Preset Panel\r\n\r\nWe also provide a Preset Panel on the left (that nice, little, Pokémon-ish icon 🤣):\r\n\r\n![preset-panel](./static/images/preset-panel.jpeg)\r\n\r\nCurrently we only support Generate Cats 🐱, but we will add more in the future (for instance, Generate Dogs 🐶)!\r\n\r\n## Outpainting (Experimental)\r\n\r\nWe in fact support outpainting algorithm, but I shall be honest: that the Stable Diffusion model is not as good as the DALLE·2 model in this case. So I will simply put a single-image demonstration here:\r\n\r\n![outpainting](./static/images/outpainting0.jpeg)\r\n\r\n- **0** - Create an Empty Node and drag it to the area that you want to outpaint on\r\n  - It needs to be placed 'below' the original image. The keyboard shortcut is `ctrl+[` for Windows and `cmd+[` for Mac.\r\n- **1** - Expand the `Outpainting` on the left and:\r\n  - Input some texts in the text area.\r\n  - Click the `Mark as Outpainting Area` button.\r\n    - A nice little preview image should then pop up above the text area with this action.\r\n- **2** - Click the `Outpaint` button and wait for the result.\r\n\r\nIt is likely that some goofy results will appear 🤣. In this case, you can undo it by `ctrl+z` / `cmd+z` and try it one more time. (Maybe) Eventually, you will get nice result.\r\n\r\nBut - there are some tricks here. If you are trying to outpaint a generated image, recall that you can [copy the parameters](#inspect--copy--import-parameters) of every generated image, so why not use exactly the same prompt to outpaint:\r\n\r\n![outpainting-with-same-prompt](./static/images/outpainting1.jpg)\r\n\r\n> That's a REALLY long prompt 😆!\r\n\r\nAnd after a few tries, I get this result:\r\n\r\n![outpainting-result](./static/images/outpainting2.jpg)\r\n\r\nStill far from good, but it's quite interesting!\r\n\r\n## Landscape Synthesis (Experimental)\r\n\r\n<details>\r\n<summary>GIF</summary>\r\n<img src=\"https://github.com/carefree0910/datasets/releases/download/static/landscape.gif\" alt=\"Landscape\" />\r\n</details>\r\n\r\nAnother interesting feature is that you can do landscape synthesis, similar to `GauGAN`:\r\n\r\n![landscape-synthesis-result](./static/images/landscape-synthesis0.jpg)\r\n\r\nBut again, the result is quite unpredictable, so I will simply put a single-image demonstration here:\r\n\r\n![landscape-synthesis](./static/images/landscape-synthesis1.png)\r\n\r\n- **0** - Click the landscape icon on the toolbar, and you will enter the 'Landscape drawing' mode.\r\n- **1** - You will draw an area of the landscape per mouse down & mouse up. Before that, you can choose which type of landscape that you are going to draw on the right panel.\r\n- **2** - You can draw wherever you want on the draw board, but better keep everything together.\r\n- **3** - Once you are satisfied with your wonderful sketch, click the `Finish` button on the right, your drawing will then turn into a selectable Node, and a `Landscape Synthesis` button will appear on the right:\r\n\r\n![landscape-synthesis-submit](./static/images/landscape-synthesis2.png)\r\n\r\nClick it, and the result should be poped up in a few seconds:\r\n\r\n![landscape-synthesis-submit](./static/images/landscape-synthesis3.jpg)\r\n\r\nFar from good, but not so bad!\r\n\r\n### What determines the size of the generated image?\r\n\r\nThe generated image will have the same size as the sketch, so it will be dangerous if you accidentally submit a HUGE sketch without even noticing:\r\n\r\n![landscape-synthesis-dangerous](./static/images/landscape-synthesis4.png)\r\n\r\nThe sketch looks small, but the actual size is `6765.1 x 4501.5`!! This happened because we support global scaling, and some huge stuffs will 'look small' on the draw board.\r\n\r\n### I see many 'holes' in your example, do they matter?\r\n\r\nI've implemented something like 'nearest search' to fill those holes, so don't worry: they should be working as expected in most cases!\r\n\r\n\r\n# Image Processing Features\r\n\r\nApart from the image generating features, we also provided some rather stand-alone image processing features that can be used on any images. Our goal here is to provide an AI-powered toolbox that can do something difficult with only one or a few clicks.\r\n\r\nThe features listed in this section hide behind that magic-wand-icon on the left:\r\n\r\n![image-processing-icon](./static/images/image-processing-icon.jpg)\r\n\r\n## Super Resolution\r\n\r\nWorried that the generated image is not high-res enough? Then our Super Resolution feature can come to rescue:\r\n\r\n![super-resolution](./static/images/super-resolution0.png)\r\n\r\nThere are two buttons: `Super Resolution` and `Super Resolution (Anime)`. They are basically two versions from `Real ESRGAN`, where the former is a 'general' SR solution, and the latter does some optimizations on anime pictures.\r\n\r\nBy clicking one of these buttons, you will get a high-res image in a few seconds:\r\n\r\n![super-resolution](./static/images/super-resolution1.png)\r\n\r\nAs you can see, the result even looks like a vector graphic, nice!\r\n\r\n> Although you can SR the already SR-ed image, the image size will grow exponentially (`4x` each), and soon explode my (or your, if you deployed locally) machine 😮!\r\n\r\n## Inpainting\r\n\r\n<details>\r\n<summary>GIF</summary>\r\n<img src=\"https://github.com/carefree0910/datasets/releases/download/static/inpainting.gif\" alt=\"Inpainting\" />\r\n</details>\r\n\r\nAnnoyed that only a small part of a generated image is not what you want? Then our Inpainting feature can come to rescue. Let's say we've generated a nice portrait of `hakurei reimu`, but you might notice that there is something weird:\r\n\r\n![inpainting-initial](./static/images/inpainting0.jpg)\r\n\r\nSo let's use our `brush` tool to 'overwrite' the weird area:\r\n\r\n![inpainting-brush](./static/images/inpainting1.jpg)\r\n\r\n- **0** - Click the brush icon on the toolbar, and you will enter the 'brushing' mode.\r\n- **1** - Trigger the `Use Fill` mode on the right, so it will be convenient to draw areas.\r\n- **2** - Draw the contour of the target area, and the `Use Fill` mode will help you fill the center.\r\n\r\n> The color could be any color, not necessary to be green 😉.\r\n\r\nAfter clicking the `Finish` button on the right, the drawing will then turn into a selectable Node, and the `Inpainting` panel on the left can now be utilized:\r\n\r\n![inpainting-brush](./static/images/inpainting2.jpg)\r\n\r\n1. click the `Mark as Inpainting Mask` to mark your drawing as mask.\r\n2. click the portrait, then click the `Mark as Image` to mark the portrait as background image.\r\n\r\nThen the `Inpaint` button should be available, click it and wait for the result:\r\n\r\n![inpainting-submit](./static/images/inpainting3.jpg)\r\n\r\nNot bad! But can we do something more?\r\n\r\n...Yes! We can apply the `Super Resolution (Anime)` on the inpainted image. And here's the final result:\r\n\r\n![inpainting-final](./static/images/inpainting4.jpg)\r\n\r\nNot perfect, but I'm pretty satisfied because what I've done is just some simple clicking 😆.\r\n\r\n## Erase & Replace\r\n\r\nThe Erase & Replace feature utilized the latest SD-inpaiting model, and its usage is almost the same as the [Inpainting](#inpainting) feature, except you need to specify what you want to 'Replace' into the image:\r\n\r\n![Erase & Replace](static/images/erase-and-replace.jpg)\r\n\r\n\r\n# Advanced Usages\r\n\r\n## Custom Checkpoints\r\n\r\nWe now support using your own checkpoints to generate images, if you are using [Local Server](#webui--local-deployment):\r\n\r\n![use-local-model0](static/images/use-local-model0.png)\r\n\r\nAfter you toggled the `Use Local Model` switch, we'll do two things:\r\n- fetch the available `version`s from your local server.\r\n- scan the default model root (`cfcreator/apis/models`, where you can see a `put_your_sd_ckpt_here` file) and pick up the available `model`s.\r\n\r\nThe `version` means the backend `algorithm` used behind the features. For example, most of the Stable Diffusion features are using the `sd_v1.5` `version`, and will use the `sd_anime` `version` if `Use Anime-Finetuned Model` is toggled.\r\n\r\nHere's the full list of `version`s:\r\n\r\n![use-local-model1](static/images/use-local-model1.png)\r\n\r\n> In most cases, we only care about `sd_v1.5` and `sd_anime`.\r\n\r\nThis feature will be very useful if you want to use the WebUI features along with your own models. For example, if you want to use a specific version of `Waifu Diffusion`, you can simply download the checkpoint, put it into the `cfcreator/apis/models` folder, choose the `sd_anime` as the `version` and your `ckpt` as the `model`, press the `Switch to Local Model` button, and wait for the success message to pop up. After that, as long as you toggle the `Use Anime-Finetuned Model`, you will be using your own checkpoint to generate images!\r\n\r\nIf you only need your own checkpoint to generate images, it will be handy to choose the `sd_v1.5` as the `version`. In this case, we'll use your checkpoint by default.\r\n\r\n> This process can be reversed by pressing the `Reset` button! 😉\r\n\r\n## Textual Inversion\r\n\r\n### Features\r\n\r\n- Support all embeddings from [here](https://cyberes.github.io/stable-diffusion-textual-inversion-models/).\r\n- Support multi-embedding for each token.\r\n  - For instance, the embedding shape of this [`<pekora>`](https://huggingface.co/carefree0910/carefree-learn/resolve/main/tokens/pekora.pt) token, originated from [here](https://drive.google.com/file/d/1MDSmzSbzkIcw5_aw_i79xfO3CRWQDl-8/view), is `[8, 768]`, which means we will use `8` embeddings to represent the `<pekora>` concept.\r\n\r\n### Usage\r\n\r\nBasically, you just need to put the `pt` files in the `cfcreator/apis/tokens` folder (where you can see a `put_your_tokens_here` file), and each `pt` file should be a dictionary, where:\r\n- Its key is the token.\r\n- Its value is the embedding, and multi-embedding is supported.\r\n\r\nHere's an example (you can download it [here](https://huggingface.co/carefree0910/carefree-learn/resolve/main/tokens/pekora.pt)):\r\n\r\n```text\r\nIn [1]: torch.load(\"cfcreator/apis/tokens/pekora.pt\")\r\nOut[1]: \r\n{'<pekora>': tensor([[ 0.2215,  0.5360,  0.3351,  ..., -0.0127,  0.7670, -0.6736],\r\n         [ 0.3607,  0.0284,  0.2156,  ..., -0.0976, -0.1588, -0.6090],\r\n         [ 0.4083,  0.5128,  0.2997,  ...,  1.4237,  0.6810,  0.9344],\r\n         ...,\r\n         [-0.3016,  0.5349,  0.5534,  ..., -1.4518,  0.2553,  0.5909],\r\n         [ 0.0146,  0.1349,  0.2838,  ..., -0.1080, -0.5861, -0.0564],\r\n         [ 0.3456,  0.9846,  0.0444,  ..., -0.3427,  0.2672,  0.3489]],\r\n        device='cuda:0')}\r\n```\r\n\r\nAfter you've put the tokens in the `cfcreator/apis/tokens` folder, simply launch the local server and we'll scan all the possible tokens for you. If we found any valid tokens, something like this will be printed:\r\n\r\n```text\r\n> Following tokens are loaded: <pekora>\r\n```\r\n\r\nAnd then you can utilize the loaded tokens directly in the WebUI:\r\n\r\n![textual-inversion](static/images/textual-inversion.jpg)\r\n\r\n\r\n# Installation\r\n\r\n`carefree-creator` is built on top of `carefree-learn`, and requires:\r\n- `Python>=3.8`\r\n- `pytorch>=1.12.0`. Please refer to [PyTorch](https://pytorch.org/get-started/locally/)'s official website, and it is highly recommended to pre-install PyTorch with conda.\r\n\r\n## Hardware Requirements\r\n\r\n> Related issue: [#10](https://github.com/carefree0910/carefree-creator/issues/10).\r\n\r\nThis project will eat up 11~13 GB of GPU RAM if no modifications are made, because it actually integrates FIVE different SD versions together, and many other models as well. 🤣\r\n\r\nThere are two ways that can reduce the usage of GPU RAM - lazy loading and partial loading, see the following [`Run`](#run) section for more details.\r\n\r\n## pip installation\r\n\r\n```bash\r\npip install carefree-creator\r\n```\r\n\r\nIf you are interested in the latest features, you may use `pip` to install from source as well:\r\n\r\n```bash\r\ngit clone https://github.com/carefree0910/carefree-creator.git\r\ncd carefree-creator\r\npip install -e .\r\n```\r\n\r\n### Run\r\n\r\n`carefree-creator` builds a CLI for you to setup your local service. For instance, we can:\r\n\r\n```bash\r\ncfcreator serve\r\n```\r\n\r\nIf you don't have an NVIDIA GPU (e.g. mac), you may try:\r\n\r\n```bash\r\ncfcreator serve --cpu\r\n```\r\n\r\nIf your GPU RAM is not large enough, you may try:\r\n\r\n```bash\r\ncfcreator serve --save_gpu_ram\r\n```\r\n\r\n> With the `--save_gpu_ram` flag, the models will be loaded to RAM, and only the executing model will be moved to GPU RAM.\r\n> \r\n> So as an exchange, your RAM will be eaten up! 🤣\r\n\r\nIf you only want to try the SD basic endpoints, you may use:\r\n\r\n```bash\r\ncfcreator serve --focus sd.base\r\n```\r\n\r\nAnd if you only want to try the SD anime endpoints, you may use:\r\n\r\n```bash\r\ncfcreator serve --focus sd.anime\r\n```\r\n\r\nMore usages could be found by:\r\n\r\n```bash\r\ncfcreator serve --help\r\n```\r\n\r\n## Docker\r\n\r\n### Prepare\r\n\r\n```bash\r\nexport TAG_NAME=cfcreator\r\ngit clone https://github.com/carefree0910/carefree-creator.git\r\ncd carefree-creator\r\n```\r\n\r\n### Build\r\n\r\n```bash\r\ndocker build -t $TAG_NAME .\r\n```\r\n\r\nIf your internet environment lands in China, it might be faster to build with `Dockerfile.cn`:\r\n\r\n```bash\r\ndocker build -t $TAG_NAME -f Dockerfile.cn .\r\n```\r\n\r\n### Run\r\n\r\n```bash\r\ndocker run --gpus all --rm -p 8123:8123 $TAG_NAME:latest\r\n```\r\n\r\n\r\n# Q&A\r\n\r\n### Where are my creations stored?\r\n\r\nThey are currently stored on my poor cloud server, and I'm planning to support storing them on your local machines!\r\n\r\n### How do I save / load my project?\r\n\r\nWe will perform an auto-save everytime you make some modifications, and will perform a period saving every minute, to the `localStorage` of your browser. However, I have to admit that they are not as reliable as it should be, so you can download the whole project to your own machines:\r\n\r\n![download-project](./static/images/download-project.jpg)\r\n\r\nThis will download a `.noli` file, which contains all the information you need to fully reconstruct the current draw board. You can then import these `.noli` files later with the `Import Project` menu option (right above the `Download Project` option).\r\n\r\n### How can I contribute to `carefree-creator`?\r\n\r\n`carefree-creator` is a FastAPI-based service, and I've already made some abstractions so it should be fairly easy to implement a new `Algorithm`.\r\n\r\nThe development guide is on our [TODO](#todo) list, but here are some brief introductions that might help:\r\n\r\n0. the `cfcreator/txt2img.py` file is a good reference.\r\n1. create a new file under the `cfcreator` directory, and in this file:\r\n   1. define the endpoint of your service.\r\n   2. `register` an `Algorithm`, which should contain an `initialize` method and a `run` method.\r\n2. go to `cfcreator/__init__.py` file and import your newly implemented modules here.\r\n\r\n### How can I get my own models interactable on the **WebUI**?\r\n\r\n> Related issue: [#8](https://github.com/carefree0910/carefree-creator/issues/8).\r\n\r\nAs long as we open sourced the **WebUI** you can implement your own UIs, but for now you can contribute to this `carefree-creator` repo and then ask me to do the UI jobs for you (yes, you can be my boss 😆).\r\n\r\n> You'll need to use you local server to use your own models!\r\n\r\n#### Handy way\r\n\r\n1. Place your checkpoints in the `cfcreator/apis/models` folder.\r\n2. Toggle the `Use Local Model` switch.\r\n3. Setup `version` & `model`, then press the `Switch to Local Model` button.\r\n\r\n> Detailed introductions can be found in the [Custom Checkpoints](#custom-checkpoints) section!\r\n\r\n#### Advanced way\r\n\r\nI haven't documented these stuffs yet, but here are some brief guides:\r\n\r\n1. The local APIs are exposed from [here](https://github.com/carefree0910/carefree-creator/blob/5c1ea3fb7514d17472caa7f07d135d8e43498136/cfcreator/apis/interface.py#L349) on.\r\n> ↑ You can ignore this if you just want to change the existing models, instead of introducing new models / endpoints / features!\r\n2. The APIs are implemented in [txt2img.py](https://github.com/carefree0910/carefree-creator/blob/dev/cfcreator/txt2img.py) and [img2img.py](https://github.com/carefree0910/carefree-creator/blob/dev/cfcreator/img2img.py).\r\n3. I'm currently using my own library ([carefree-learn](https://github.com/carefree0910/carefree-learn)) to implement the APIs, but you can re-implement the APIs with whatever you want! Take the basic `text2img` feature as an example:\r\n\r\n    a. Rewrite the [`initialize`](https://github.com/carefree0910/carefree-creator/blob/63ff1778175a7ee9bfa19b6955f1eae95398547a/cfcreator/txt2img.py#L41) method, where you can initialize your models.\r\n    b. Rewrite the [`run`](https://github.com/carefree0910/carefree-creator/blob/63ff1778175a7ee9bfa19b6955f1eae95398547a/cfcreator/txt2img.py#L44) method, where you need to generate the output (image) based on the input (the `Txt2ImgSDModel`, which contains almost all the necessary arguments)\r\n\r\nOnce all the modifications are done (on your own fork / a PR to a new branch of this project), you can modify the `Install carefree-creator` section in the Google Colab, and change this line:\r\n\r\n```bash\r\n!git clone https://github.com/carefree0910/carefree-creator.git\r\n```\r\n\r\ninto the corresponding git-clone-url, so the Colab will install your own customized version and serve it!\r\n\r\nFeel free to create issues if you encountered any trouble! 😆\r\n\r\n#### API Mappings\r\n\r\nHere are the mappings between `endpoint` and `feature`:\r\n- `txt2img_sd_endpoint` ↔ `Text to Image`, `Generate Cats`\r\n- `txt2img_sd_inpainting_endpoint` ↔ `Erase & Replace`\r\n- `txt2img_sd_outpainting_endpoint` ↔ `Outpainting`\r\n- `img2img_sd_endpoint` ↔ `Image Translation`\r\n- `img2img_sr_endpoint` ↔ `Super Resolution`\r\n- `img2img_inpainting_endpoint` ↔ `Inpainting`\r\n- `img2img_semantic2img_endpoint` ↔ `Landscape Synthesis`\r\n\r\nAnd there are some features that depend on multiple endpoints:\r\n- `Parameters to Image` ↔ `all endpoints`\r\n- `Variation Generation` ↔ `sd endpoints`\r\n- `Negative Prompt` ↔ `sd endpoints`\r\n\r\n### Why no `GFPGAN`?\r\n\r\nThat's because I think generating real human faces might not be a good practice for `carefree-creator`, so currently I'm not going to develop tool chains around it. If you encountered some scenarios that truly need it, feel free to contact me and let me know!\r\n\r\n### Is it FREE?\r\n\r\nIt will **ALWAYS** be **FREE** if:\r\n- You are using [local deployment](#webui--local-deployment) (Recommended!).\r\n- You are using my own poor cloud server.\r\n\r\nFor the second situation, if more and more people are using this project, you might be waiting longer and longer. You can inspect where the positions of your tasks are in the waiting queue here:\r\n\r\n![pending-panel](./static/images/pending-panel.png)\r\n\r\nThe number after `pending` will be the position. If it is ridiculously large... Then you may try [local deployment](#webui--local-deployment), or some business will go on (accounts, charges for dedicated cloud servers, etc) 🤣.\r\n\r\n> As long as this project is not as famous as those crazy websites, even my poor cloud server should be able to handle the requests, so you can consider it to be FREE in most cases (Not to mention you can always use [local deployment](#webui--local-deployment)) 😉.\r\n\r\n### Do you like cats?\r\n\r\nI **LOVE** cats. They are soooooo **CUTE**.\r\n\r\n### What about dogs?\r\n\r\nDogs are cute as well, but I got bitten when I was young so...\r\n\r\n### Why did you build this project?\r\n\r\nI've been a big fan of Touhou since 10 years ago, and one of my biggest dreams is to make an epic Touhou fan game.\r\n\r\nIt wouldn't be possible because I can hardly draw anything (🤣), but now with Stable Diffusion everything is hopeful again.\r\n\r\nSo the initial reason of building this project is simple: I want to provide a tool that can empower anyone, who is suffering from acquiring game materials, the ability to create ones on their own. That's why we put pretty much attention on the [Variation Generation](#generate-variations) feature, since this is very important for creating a vivid character.\r\n\r\n> Stable Diffusion gives me some confidence, and Waifu Diffusion further convinced my determination. Many thanks to these great open source prjects!!!\r\n\r\nAnd as the development goes on, I figure out that this tool has more potential: It could be the '**Operation System**' of the AI generation world! The **models/algorithms** serve as the `softwares`, and your **creations** serve as the `files`. You can always **review/edit** your `files` with the `softwares`, as well as **sharing/importing** them.\r\n\r\nIn the future, the `softwares` should be easy to **implement/publish/install/uninstall**, and the `files` should be able to store at **cloud/local machine** (currently they are all on cloud, or, on my poor cloud server 🤣).\r\n\r\nThis will further break the wall between the academic world and the non-academic world. The Hugging Face Space is doing a good job now, but there are still three pain points:\r\n\r\n- Its interaction is usable, but still quite restricted.\r\n- The results are generated one after another, we cannot review/edit the results that are generated 5 minutes ago.\r\n- The service is deployed at their own servers, so you have to wait if their servers are busy / not GPU accelerated.\r\n\r\nAnd now, with the ability to do **local deployment**, along with the fantastic **infinite draw board** as the **WebUI**, these pain points will all be solved. Not to mention with some inference technique (such as the `ZeRO` from `deepspeed`), it is possible to deploy huge, huge models even on your laptop, so don't worry about the capability of this system - everything will be possible!\r\n\r\n### How is this different from other WebUIs?\r\n\r\n> Related issue: [#11](https://github.com/carefree0910/carefree-creator/issues/11).\r\n\r\nI think the main difference is that this project:\r\n\r\n1. separates the frontend and the backend, so you can either make your own frontend, or focus on developing the backend and 'requires' the frontend from me.\r\n2. provides an easier, smoother, and more 'integrated' way for users to enjoy multiple AI magics together. The extremely popular automatic1111 repo is great, and can somehow do the tricks, but in general it is sort of a one-pass-generation-tool, and the workflow is linear. This project on the other hand has a non-linear workflow, and gives you more freedom to combine various techniques and create something that a single AI model can hardly achieve.\r\n3. can integrate many other techniques as well. Here's my future plan: I'm going to integrate natural language generation, music generation, video generation... Into this project, so you can make something really cool with and only with AI 😆!\r\n\r\n### Will there be a Discord Community?\r\n\r\n**UPDATE**: [Here](https://github.com/carefree0910/carefree-creator/issues/6)'s the related issue!\r\n\r\nUnfortunately I'm not familiar with Discord, so if someone can help me build it I will be really appreciated!\r\n\r\n### What is `Nolibox`???\r\n\r\n`Nolibox` is a startup company where I'm currently working for. Although I have to put the logo everywhere, this project is rather independent and will not be restricted 😉.\r\n\r\n\r\n# Known Issues\r\n\r\n- Undo / Redo in the header toolbar will be messed up when it comes to the 'brushing' mode and 'landscape' mode.\r\n- If you opened two or more tabs of this `creator`, your savings will be messed up because your data is not saved in the cloud, but in the `localStorage` of your browser.\r\n- If you delete an inpainting mask and then undo the deletion, you cannot see the preview image of the inpainting mask anymore until you set another Node as inpainting mask and then switch it back.\r\n\r\n\r\n# TODO\r\n\r\n- [x] Erase & Replace\r\n- [x] Handy way to use custom checkpoints\r\n- [x] Textual Inversion\r\n- [ ] User Guide\r\n- [ ] Development Guide\r\n- [ ] Other AI generation Techniques\r\n  - [ ] Natural Language Generation (NLG)\r\n  - [ ] Music Generation\r\n  - [ ] Video Generation\r\n- [ ] Better Outpainting Techniques\r\n- [ ] And much more...\r\n\r\n\r\n# Credits\r\n\r\n- [Stable Diffusion](https://github.com/CompVis/stable-diffusion), the foundation of various generation methods.\r\n- [Stable Diffusion from runwayml](https://github.com/runwayml/stable-diffusion), the adopted SD-inpainting method.\r\n- [Waifu Diffusion](https://github.com/harubaru/waifu-diffusion), the anime-finetuned version of Stable Diffusion.\r\n- [Real ESRGAN](https://github.com/xinntao/Real-ESRGAN), the adopted Super Resolution methods.\r\n- [Latent Diffusion](https://github.com/CompVis/latent-diffusion), the adopted Inpainting & Landscape Synthesis method.\r\n- [carefree-learn](https://github.com/carefree0910/carefree-learn), the code base that has re-implemented all the models above and provided clean and handy APIs.\r\n- And You! Thank you for watching!\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "python carefree-learn PyTorch",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "carefree-creator",
    "package_url": "https://pypi.org/project/carefree-creator/",
    "platform": null,
    "project_url": "https://pypi.org/project/carefree-creator/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/carefree-creator/0.1.5/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "An AI-powered creator for everyone.",
    "version": "0.1.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17412208,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "665aa457f84293b3b8c8acc9a3d6d09d72f86ea8c1197ea180b30a9b5545e4f9",
          "md5": "caf973a1cdaba6b55056c973aa860b8b",
          "sha256": "a47969ac8c10171867ba4adae2f498d048a82397b92e86381f0c6b00c41af75a"
        },
        "downloads": -1,
        "filename": "carefree-creator-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "caf973a1cdaba6b55056c973aa860b8b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6280,
        "upload_time": "2022-10-03T14:36:08",
        "upload_time_iso_8601": "2022-10-03T14:36:08.044240Z",
        "url": "https://files.pythonhosted.org/packages/66/5a/a457f84293b3b8c8acc9a3d6d09d72f86ea8c1197ea180b30a9b5545e4f9/carefree-creator-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b77e3cf94720836d9603c17c74ad1e6b587127e748f40c521698e341d5cd314d",
          "md5": "33bf523d9a7b59556951a8c92c3b8b6d",
          "sha256": "605cbbf122efd1949b5444e1ec6cd5eb1849d6a3c2677c01050158b2d84f5698"
        },
        "downloads": -1,
        "filename": "carefree-creator-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "33bf523d9a7b59556951a8c92c3b8b6d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 56797,
        "upload_time": "2022-12-06T05:54:02",
        "upload_time_iso_8601": "2022-12-06T05:54:02.182496Z",
        "url": "https://files.pythonhosted.org/packages/b7/7e/3cf94720836d9603c17c74ad1e6b587127e748f40c521698e341d5cd314d/carefree-creator-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a637afad039c0f7b7edcb665c709e9cdb10030dadfde442f78ed9173faf44580",
          "md5": "9e624f127198d3c2f8ff6c1872cfd14c",
          "sha256": "44e927915ff4ec79e0f37f3b863855d8732a77d806bc6fcb85426dcdcbf25d60"
        },
        "downloads": -1,
        "filename": "carefree-creator-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "9e624f127198d3c2f8ff6c1872cfd14c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 56894,
        "upload_time": "2022-12-10T12:45:48",
        "upload_time_iso_8601": "2022-12-10T12:45:48.794027Z",
        "url": "https://files.pythonhosted.org/packages/a6/37/afad039c0f7b7edcb665c709e9cdb10030dadfde442f78ed9173faf44580/carefree-creator-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "77a25f53eedcf07a1d1d86dd9a72413e76ee282f2bc6650e86387a13f9ad622c",
          "md5": "208d521ef6d05f51bb7803bc92e3ce95",
          "sha256": "0fdcf96fc51667a3017ed9f61c4f1ded847cf391b3981acf4c1aee06a422efaf"
        },
        "downloads": -1,
        "filename": "carefree-creator-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "208d521ef6d05f51bb7803bc92e3ce95",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 57077,
        "upload_time": "2023-01-16T06:09:47",
        "upload_time_iso_8601": "2023-01-16T06:09:47.154661Z",
        "url": "https://files.pythonhosted.org/packages/77/a2/5f53eedcf07a1d1d86dd9a72413e76ee282f2bc6650e86387a13f9ad622c/carefree-creator-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6baddd8922064ff649ae74eb813dcf3cc01f6440ee94d6259fd66a7bd6a6d0ad",
          "md5": "b8e585c28742ac35ea5ed909c1691b4b",
          "sha256": "58907df02d0c41b8a4472822881558c18a6b4a4c83d28c7c479f030fb2bd6576"
        },
        "downloads": -1,
        "filename": "carefree-creator-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "b8e585c28742ac35ea5ed909c1691b4b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 58033,
        "upload_time": "2023-01-19T02:30:12",
        "upload_time_iso_8601": "2023-01-19T02:30:12.106478Z",
        "url": "https://files.pythonhosted.org/packages/6b/ad/dd8922064ff649ae74eb813dcf3cc01f6440ee94d6259fd66a7bd6a6d0ad/carefree-creator-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a45ee6f324028db20cbccf25bfe0eaabd076dab6643169f641fafac616005a84",
          "md5": "bce9eb248be0de9fbed16aebe0e7ed54",
          "sha256": "918ed8161ace74edaa403a109dd7ec91ea06da89f2524ff73aff315519726cd4"
        },
        "downloads": -1,
        "filename": "carefree-creator-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "bce9eb248be0de9fbed16aebe0e7ed54",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 59599,
        "upload_time": "2023-02-23T07:33:36",
        "upload_time_iso_8601": "2023-02-23T07:33:36.827242Z",
        "url": "https://files.pythonhosted.org/packages/a4/5e/e6f324028db20cbccf25bfe0eaabd076dab6643169f641fafac616005a84/carefree-creator-0.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a45ee6f324028db20cbccf25bfe0eaabd076dab6643169f641fafac616005a84",
        "md5": "bce9eb248be0de9fbed16aebe0e7ed54",
        "sha256": "918ed8161ace74edaa403a109dd7ec91ea06da89f2524ff73aff315519726cd4"
      },
      "downloads": -1,
      "filename": "carefree-creator-0.1.5.tar.gz",
      "has_sig": false,
      "md5_digest": "bce9eb248be0de9fbed16aebe0e7ed54",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 59599,
      "upload_time": "2023-02-23T07:33:36",
      "upload_time_iso_8601": "2023-02-23T07:33:36.827242Z",
      "url": "https://files.pythonhosted.org/packages/a4/5e/e6f324028db20cbccf25bfe0eaabd076dab6643169f641fafac616005a84/carefree-creator-0.1.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}