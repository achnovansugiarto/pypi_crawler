{
  "info": {
    "author": "FMR LLC",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Environment :: GPU :: NVIDIA CUDA",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "[![Stoke](https://raw.githubusercontent.com/fidelity/stoke/master/resources/images/logo_and_text.png)](https://fidelity.github.io/stoke/)\n> Add a little accelerant to your torch\n\n[![License](https://img.shields.io/badge/License-Apache%202.0-9cf)](https://opensource.org/licenses/Apache-2.0)\n[![Python](https://img.shields.io/badge/python-3.6+-informational.svg)]()\n[![Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n![Lint](https://github.com/fidelity/stoke/workflows/lint/badge.svg?branch=master)\n![Docs](https://github.com/fidelity/stoke/workflows/docs/badge.svg)\n---\n\n## About\n\n`stoke` is a lightweight wrapper for PyTorch that provides a simple declarative API for context switching between \ndevices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. It places no restrictions on code \nstructure for model architecture, training/inference loops, loss functions, optimizer algorithm, etc. Stoke simply \n'wraps' your existing  PyTorch code to automatically handle the necessary underlying wiring for all of the \nsupported backends.This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU \nwith extensions (like optimizer state sharding) by simply changing a few declarative flags. Additionally, `stoke` \nexposes configuration settings for every underlying backend for those that want configurability and raw access to \nthe underlying libraries.\n\nIn short, `stoke` is the best of \n[PyTorch Lightning Accelerators](https://pytorch-lightning.readthedocs.io/en/latest/extensions/accelerators.html) \ndisconnected from the rest of PyTorch Lightning. Write whatever PyTorch code you want, but leave device and backend \ncontext switching to `stoke`.\n\n## Supports\n\n * Devices: CPU, GPU, multi-GPU\n * Distributed: [DDP](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel), [Horovod](https://horovod.readthedocs.io/en/stable/index.html), [deepspeed](https://github.com/microsoft/DeepSpeed) (via DDP)\n * Mixed-Precision: [AMP](https://pytorch.org/docs/stable/amp.html), [Nvidia Apex](https://github.com/NVIDIA/apex), [deepspeed](https://github.com/microsoft/DeepSpeed) (custom APEX like backend)\n * Extensions: [fairscale](https://github.com/facebookresearch/fairscale) (Optimizer State Sharding, Sharded DDP, Fully Sharded DDP), [deepspeed](https://github.com/microsoft/DeepSpeed) (ZeRO Stage 0-3, etc.)\n\n## Benefits/Capabilities\n\n- Declarative style API -- allows you to declare or specify the desired state and let `stoke` handle the rest\n- Wrapped API Mirrors base PyTorch style `model`, `loss`, `backward`, and `step` calls\n- Automatic device placement of model(s) and data\n- Universal interface for saving and loading regardless of backend(s) or device\n- Automatic handling of gradient accumulation and clipping\n- Common `attrs` interface for all backend configuration parameters (with docstrings)\n- Helper methods for printing synced losses, device specific print, number of model parameters\n- Extra(s) - Custom torch.utils.data.distributed.Sampler: BucketedDistributedSampler which buckets data by \n  a sorted idx and then randomly samples from specific bucket(s) to prevent situations like grossly mismatched sequence \n  length leading to wasted computational overhead (ie excess padding)\n\n## Installation\n\n### (Required for FP16 Support) Install NVIDIA Apex\n\nIf you are planning on using mixed-precision (aka FP16), please install Apex so that `stoke` supports all FP16 methods. \nIf you are not planning on using mixed precision, this step can actually be skipped (as all imports are in a try/except \nand are only conditionally imported).\n\nFollow the instructions [here](https://github.com/NVIDIA/apex#quick-start).\n\n### (Optional) Underlying OpenMPI Support\n\n**Note: MPI support is necessary if you plan to run Stoke across multiple compute nodes (e.g. 2 nodes with 4 GPUs each) \nwith DDP, Horovod, or DeepSpeed backends**\n\nFollow the instructions [here](https://www.open-mpi.org/faq/?category=building) or \n[here](https://edu.itp.phys.ethz.ch/hs12/programming_techniques/openmpi.pdf)\n\nAlso, refer to the Dockerfile [here](https://github.com/fidelity/stoke/blob/master/docker/stoke-gpu-mpi.Dockerfile) \n\n### via PyPi\n```bash\npip install stoke\n```\n\n### via PyPi w/ Optional MPI Support\n\n**Note: MPI support is necessary if you plan to run Stoke across multiple compute nodes (e.g. 2 nodes with 4 GPUs each) \nwith DDP, Horovod, or DeepSpeed backends**\n\n```bash\npip install stoke[mpi]\n```\n\n## Documentation and Examples\n\nFull documentation can be found [here](https://fidelity.github.io/stoke/) and \nexamples are [here](https://github.com/fidelity/stoke/blob/master/examples).\n\n## Quick Start\n\n#### Basic Definitions\n\nAssuming some already existing common PyTorch objects (dataset: `torch.utils.data.Dataset`, model: `torch.nn.Module`, \nloss: `torch.nn.(SomeLossFunction)`):\n\n```python\nimport torch\n\n# Some existing user defined dataset using torch.utils.data.Dataset\nclass RandomData(torch.utils.data.Dataset):\n    pass\n\n# An existing model defined with torch.nn.Module\nclass BasicNN(torch.nn.Module):\n    pass\n\n# Our existing dataset from above\ndataset = RandomData(...)\n\n# Our existing model from above \nmodel = BasicNN(...)\n\n# A loss function\nloss = torch.nn.BCEWithLogitsLoss()\n```\n\n#### Optimizer Setup\n\n`stoke` requires a slightly different way to define the optimizer (as it handles instantiation internally) by using\n`StokeOptimizer`. Pass in the uninstantiated `torch.optim.*` class object and any **kwargs that need to be passed to the \n`__init__` call:\n\n```python\nfrom stoke import StokeOptimizer\nfrom torch.optim import Adam\n\n# Some ADAM parameters\nlr = 0.001\nbeta1 = 0.9\nbeta2 = 0.98\nepsilon = 1E-09\n\n# Create the StokeOptimizer\nopt = StokeOptimizer(\n    optimizer=Adam,\n    optimizer_kwargs={\n        \"lr\": lr,\n        \"betas\": (beta1, beta2),\n        \"eps\": epsilon\n    }\n)\n```\n\n#### Create Stoke Object\n\nNow create the base `stoke` object. Pass in the model, loss(es), and `StokeOptimizer` from above as well as any\nflags/choices to set different backends/functionality/extensions and any necessary configurations. As an example, \nwe set the device type to GPU, use the PyTorch DDP backend for distributed multi-GPU training, toggle native PyTorch \nAMP mixed precision, add Fairscale optimizer-state-sharding (OSS), and turn on automatic gradient accumulation and \nclipping (4 steps and clip-by-norm). In addition, let's customize PyTorch DDP,  PyTorch AMP and Fairscale OSS with \nsome of our own settings but leave all the others as default configurations.\n\n```python\nimport os\nfrom stoke import AMPConfig\nfrom stoke import ClipGradNormConfig\nfrom stoke import DDPConfig\nfrom stoke import DistributedOptions\nfrom stoke import FairscaleOSSConfig\nfrom stoke import FP16Options\nfrom stoke import Stoke\n\n# Custom AMP configuration\n# Change the initial scale factor of the loss scaler\namp_config = AMPConfig(\n    init_scale=2.**14\n)\n\n# Custom DDP configuration\n# Automatically swap out batch_norm layers with sync_batch_norm layers\n# Notice here we have to deal with the local rank parameter that DDP needs (from env or cmd line)\nddp_config = DDPConfig(\n    local_rank=os.getenv('LOCAL_RANK'),\n    convert_to_sync_batch_norm=True\n)\n\n# Custom OSS configuration\n# activate broadcast_fp16 -- Compress the model shards in fp16 before sharing them in between ranks\noss_config = FairscaleOSSConfig(\n    broadcast_fp16=True\n)\n\n# Configure gradient clipping using the configuration object\ngrad_clip = ClipGradNormConfig(\n    max_norm=5.0,\n    norm_type=2.0\n)\n\n# Build the object with the correct options/choices (notice how DistributedOptions and FP16Options are already provided\n# to make choices simple) and configurations (passed to configs as a list)\nstoke_obj = Stoke(\n    model=model,\n    optimizer=opt,\n    loss=loss,\n    batch_size_per_device=32,\n    gpu=True,\n    fp16=FP16Options.amp.value,\n    distributed=DistributedOptions.ddp.value,\n    fairscale_oss=True,\n    grad_accum_steps=4,\n    grad_clip=grad_clip,\n    configs=[amp_config, ddp_config, oss_config]\n)\n```\n\n#### Build PyTorch DataLoader\n\nNext we need to create a `torch.utils.data.DataLoader` object. Similar to the optimizer definition this has to be done\na little differently with `stoke` for it to correctly handle each of the different backends. `stoke` provides a mirrored\nwrapper to the native `torch.utils.data.DataLoader` class (as the `DataLoader` method) that will return a correctly \nconfigured `torch.utils.data.DataLoader` object. Since we are using a distributed backend (DDP) we need to provide a \n`DistributedSampler` or similar class to the `DataLoader`. Note that the `Stoke` object that we just created has the \nproperties `.rank` and `.world_size` which provide common interfaces to this information regardless of the backend!\n\n```python\nfrom torch.utils.data.distributed import DistributedSampler\n\n# Create our DistributedSampler\n# Note: dataset is the torch.utils.data.Dataset from the first section\nsampler = DistributedSampler(\n    dataset=dataset,\n    num_replicas=stoke_obj.world_size,\n    rank=stoke_obj.rank\n)\n\n# Call the DataLoader method on the stoke_obj to correctly create a DataLoader instance\n# The DataLoader object already known the batch size from the Stoke object creation\ndata_loader = stoke_obj.DataLoader(\n    dataset=dataset,\n    collate_fn=lambda batch: dataset.collate_fn(batch), # note: this is optional depending on your dataset\n    sampler=sampler,\n    num_workers=4\n)\n```\n\n#### Add a LR Scheduler\n\nStoke provides access to each of the underlying PyTorch instances/objects/classes it's managing. Any created `Stoke`\nobject has multiple `@property` methods that return the underlying attribute(s) such as `.optimzer`, `.loss_access`,\n`.model_access`, `.step_loss`, etc. Therefore, to use a PyTorch LR Scheduler it's as simple as getting the underlying\noptimizer and passing it to the LR Scheduler constructor:\n\n```python\nfrom torch.optim.lr_scheduler import OneCycleLR\n\n\nscheduler = OneCycleLR(\n  stoke_obj.optimizer, \n  max_lr=0.001, \n  pct_start = 0.9, \n  epochs=100, \n  steps_per_epoch=len(data_loader)\n)\n```\n\n#### Run a Training Loop\n\nAt this point, we've successfully configured `stoke`! Since `stoke` handled wrapping/building your `torch.nn.Module` and \n`torch.utils.data.DataLoader`, device placement is handled automatically (in our example the model and data are moved\nto GPUs). The following simple training loop should look fairly standard, except that the model `forward`, `loss`, \n`backward`, and `step` calls are all called on the `Stoke` object instead of each individual component (as it \ninternally maintains the model, loss, and optimizer and all necessary code for all \nbackends/functionality/extensions). In addition, we use one of many helper functions built into `stoke` to print the \nsynced and gradient accumulated loss across all devices (an all-reduce across all devices with ReduceOp.SUM and divided\nby world_size -- that is print only on rank 0 by default)\n\n```python\nepoch = 0\n# Iterate until number epochs\nwhile epoch < 100:\n    # Loop through the dataset\n    for x, y in data_loader:\n        # Use the Stoke wrapped version(s) of model, loss, backward, and step\n        # Forward\n        out = stoke_obj.model(x)\n        # Loss\n        loss = stoke_obj.loss(out, y.to(dtype=torch.float).unsqueeze(1))\n        # Detach loss and sync across devices -- only after grad accum step has been called \n        stoke_obj.print_mean_accumulated_synced_loss()\n        # Backward\n        stoke_obj.backward(loss)\n        # stoke_obj.dump_model_grads()\n        # Optimizer Step\n        stoke_obj.step()\n        # Scheduler Step -> Note this is the order for PyTorch 1.10, for < 1.10 the scheduler step is before the\n        # optimizer step\n        scheduler.step()\n    epoch += 1\n```\n\n#### Save/Load\n\n`stoke` provides a unified interface to save and load model checkpoints regardless of backend/functionality/extensions.\nSimply call the `save` or `load` methods on the `Stoke` object.\n\n```python\n# Save the model w/ a dummy extra dict\npath, tag = stoke_obj.save(\n    path='/path/to/save/dir',\n    name='my-checkpoint-name',\n    extras={'foo': 'bar'}\n    )\n\n# Attempt to load a saved checkpoint -- returns the extras dictionary\nextras = stoke_obj.load(\n    path=path,\n    tag=tag\n)\n```\n\n### Launchers\n\nSee the documentation [here](https://fidelity.github.io/stoke/docs/Launchers/)\n\n## Compatibility Matrix\n\nCertain combinations of backends/functionality are not compatible with each other. The below table indicates which \ncombinations should work together:\n\n| Backends/Devices | CPU | GPU | PyTorch DDP | Deepspeed DDP | Horovod | Deepspeed FP16 | Native AMP | NVIDIA APEX | Deepspeed ZeRO | Fairscale |\n| ---------------- | --- | --- | ----------- | ------------- | ------- | -------------- | ---------  | ----------- | -------------- | --------- |\n| CPU | | | | | | | | | |\n| GPU | | | &#10004; | &#10004; | &#10004; | &#10004; | &#10004; | &#10004; | &#10004; | |\n| PyTorch DDP | | &#10004; | | | | | &#10004; | &#10004; | | &#10004; |\n| Deepspeed DDP | | &#10004; | | | | &#10004; | | | &#10004; | |\n| Horovod | | &#10004; | | | | | &#10004; | &#10004; | | |\n| DeepspeedFP16 | | &#10004; | | &#10004; | | &#10004; | | | &#10004; | |\n| Native AMP | | &#10004; | &#10004; | | &#10004; | | | | | &#10004; |\n| NVIDIA APEX | | &#10004; | &#10004; | | &#10004; | | | | | |\n| Deepspeed ZeRO | | &#10004; | | &#10004; | | &#10004; | | | | |\n| Fairscale | | &#10004; | &#10004; | | | | &#10004; | | | |\n\n\n___\n`stoke` is developed and maintained by the **Artificial Intelligence Center of Excellence at Fidelity Investments**.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/fidelity/stoke",
    "keywords": "deep learning,pytorch,AI,gpu,ddp,horovod,deepspeed,fairscale,distributed,fp16,mixed-precision,apex,amp",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "stoke",
    "package_url": "https://pypi.org/project/stoke/",
    "platform": null,
    "project_url": "https://pypi.org/project/stoke/",
    "project_urls": {
      "Bug Tracker": "https://github.com/fidelity/stoke/issues",
      "Documentation": "https://fidelity.github.io/stoke/",
      "Homepage": "https://github.com/fidelity/stoke",
      "Source": "https://github.com/fidelity/stoke"
    },
    "release_url": "https://pypi.org/project/stoke/0.2.1/",
    "requires_dist": [
      "attrs (>=20.3.0)",
      "deepspeed (>=0.6.4)",
      "fairscale (>=0.4.6)",
      "horovod (>=0.21.2)",
      "torch (>=1.8.1)",
      "mypy-extensions ; python_version < \"3.8\"",
      "mpi4py (==3.0.3) ; extra == 'mpi'"
    ],
    "requires_python": ">=3.6",
    "summary": "Lightweight wrapper for PyTorch that provides a simple unified interface for context switching between devices (CPU, GPU), distributed modes (DDP, Horovod), mixed-precision (AMP, Apex), and extensions (fairscale, deepspeed).",
    "version": "0.2.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13879343,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "82e2c1c2cd8401c52eb14e9d408839c1539adcb6123a1fb03b2616596a506db7",
          "md5": "a65d4130cc882285dd1daa0126ddae13",
          "sha256": "32c3ad2fb5a7f9644c8e84d3505fab9be099a02316cc05d6acb307f3f6cdf8e9"
        },
        "downloads": -1,
        "filename": "stoke-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a65d4130cc882285dd1daa0126ddae13",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 58100,
        "upload_time": "2021-06-23T20:35:07",
        "upload_time_iso_8601": "2021-06-23T20:35:07.499616Z",
        "url": "https://files.pythonhosted.org/packages/82/e2/c1c2cd8401c52eb14e9d408839c1539adcb6123a1fb03b2616596a506db7/stoke-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d9001287afb0298df7465c993e8fe0231367293814040edbec3e68c87d512c3d",
          "md5": "bc3b73e4fea854dca422bf457a3b5e61",
          "sha256": "650d65d87945cd61bf10896a065c47e858b759102f0c85e90c6647cfc323021a"
        },
        "downloads": -1,
        "filename": "stoke-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bc3b73e4fea854dca422bf457a3b5e61",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 73552,
        "upload_time": "2021-06-23T20:35:09",
        "upload_time_iso_8601": "2021-06-23T20:35:09.108594Z",
        "url": "https://files.pythonhosted.org/packages/d9/00/1287afb0298df7465c993e8fe0231367293814040edbec3e68c87d512c3d/stoke-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "034118ee46a363e3680df0f7529c8695b05cd0ffbb5fcaa96bc2d8f6022bc92e",
          "md5": "f53a14e92161737753cd880fc1edd264",
          "sha256": "572faef41726062a8dea375074fc90ac651874994cbac222125e9ae9e970ef40"
        },
        "downloads": -1,
        "filename": "stoke-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f53a14e92161737753cd880fc1edd264",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 58265,
        "upload_time": "2021-07-28T18:40:42",
        "upload_time_iso_8601": "2021-07-28T18:40:42.399906Z",
        "url": "https://files.pythonhosted.org/packages/03/41/18ee46a363e3680df0f7529c8695b05cd0ffbb5fcaa96bc2d8f6022bc92e/stoke-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "84b959f550ae3ede44238d3c50c740cc151a2dc7e247e8be9004fabb078dd90e",
          "md5": "b7735204587e76422441990df6cb4897",
          "sha256": "855154273b477f8eca59fad8563f9e3e7953f1e88636e2a903e4b992aef07b97"
        },
        "downloads": -1,
        "filename": "stoke-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b7735204587e76422441990df6cb4897",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 73944,
        "upload_time": "2021-07-28T18:40:43",
        "upload_time_iso_8601": "2021-07-28T18:40:43.969789Z",
        "url": "https://files.pythonhosted.org/packages/84/b9/59f550ae3ede44238d3c50c740cc151a2dc7e247e8be9004fabb078dd90e/stoke-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6cd5e51fd4b22772e87ba476f3f8dbe229322614ccea760493ce0119daf6c9f6",
          "md5": "34e01fc1d713972d3ca5cf43212d5377",
          "sha256": "131889522895255e09b03649261ed2a1f6ec5210c310b79cb08bb1bf5d37a5ed"
        },
        "downloads": -1,
        "filename": "stoke-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "34e01fc1d713972d3ca5cf43212d5377",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 58300,
        "upload_time": "2021-08-02T15:48:31",
        "upload_time_iso_8601": "2021-08-02T15:48:31.009685Z",
        "url": "https://files.pythonhosted.org/packages/6c/d5/e51fd4b22772e87ba476f3f8dbe229322614ccea760493ce0119daf6c9f6/stoke-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2e9c1d40346a9b1221ac4d1bc4a753b53c1302188784592e86869d85eaf4d7cc",
          "md5": "96fb808b36a2201e55794bd0a005935b",
          "sha256": "7a6cee37f1ac06b8d57a15013c52d24ae4a2f080931cb1478c203be47b6e21be"
        },
        "downloads": -1,
        "filename": "stoke-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "96fb808b36a2201e55794bd0a005935b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 73976,
        "upload_time": "2021-08-02T15:48:32",
        "upload_time_iso_8601": "2021-08-02T15:48:32.662723Z",
        "url": "https://files.pythonhosted.org/packages/2e/9c/1d40346a9b1221ac4d1bc4a753b53c1302188784592e86869d85eaf4d7cc/stoke-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ae3032bb380aa21df55eb795f4c9dd119fdce6bf55a2849afca9ea2eb8f4f41",
          "md5": "18d118a8913cdda47cec097149b1a4d6",
          "sha256": "93586f8835d3b47c53251c92f3bb1fba98578e27666cb66f4ecc995dc5da3822"
        },
        "downloads": -1,
        "filename": "stoke-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "18d118a8913cdda47cec097149b1a4d6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 62881,
        "upload_time": "2021-10-26T15:09:10",
        "upload_time_iso_8601": "2021-10-26T15:09:10.047337Z",
        "url": "https://files.pythonhosted.org/packages/5a/e3/032bb380aa21df55eb795f4c9dd119fdce6bf55a2849afca9ea2eb8f4f41/stoke-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d26bdfa1439dc4c2cfb01aa636be5c9f0d8d42fd7f416841ce9490bc0e9f2b7d",
          "md5": "b63d922b93e411c5495e7f2d34ff665e",
          "sha256": "d2f9599ae8025ee21dcdcdc35056cce1b96c6f3985a112f836c96119fc76ca02"
        },
        "downloads": -1,
        "filename": "stoke-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b63d922b93e411c5495e7f2d34ff665e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 79257,
        "upload_time": "2021-10-26T15:09:12",
        "upload_time_iso_8601": "2021-10-26T15:09:12.330722Z",
        "url": "https://files.pythonhosted.org/packages/d2/6b/dfa1439dc4c2cfb01aa636be5c9f0d8d42fd7f416841ce9490bc0e9f2b7d/stoke-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a0d19d98336a7f2a27e67957488174c53f65eddd14d8cca738320466e247c2d2",
          "md5": "5066f67e7f7c71198347eec4076aa747",
          "sha256": "f924dca95e4eb561cc9aa840e04a9991f7564c3cbfff677c010463a99a623838"
        },
        "downloads": -1,
        "filename": "stoke-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5066f67e7f7c71198347eec4076aa747",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 63527,
        "upload_time": "2022-05-20T15:38:41",
        "upload_time_iso_8601": "2022-05-20T15:38:41.580020Z",
        "url": "https://files.pythonhosted.org/packages/a0/d1/9d98336a7f2a27e67957488174c53f65eddd14d8cca738320466e247c2d2/stoke-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a54530f85b9658adec6a5a8909c8af74de68059f53888ead1d643f470da0c83e",
          "md5": "fdaf04c78e75eec197b37ccd2697c910",
          "sha256": "b85bb6436da5a22ffb7efc5bc7ff51d9aea59cbe833e64337a82a8f8105fa479"
        },
        "downloads": -1,
        "filename": "stoke-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "fdaf04c78e75eec197b37ccd2697c910",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 80449,
        "upload_time": "2022-05-20T15:38:43",
        "upload_time_iso_8601": "2022-05-20T15:38:43.634976Z",
        "url": "https://files.pythonhosted.org/packages/a5/45/30f85b9658adec6a5a8909c8af74de68059f53888ead1d643f470da0c83e/stoke-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a0d19d98336a7f2a27e67957488174c53f65eddd14d8cca738320466e247c2d2",
        "md5": "5066f67e7f7c71198347eec4076aa747",
        "sha256": "f924dca95e4eb561cc9aa840e04a9991f7564c3cbfff677c010463a99a623838"
      },
      "downloads": -1,
      "filename": "stoke-0.2.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "5066f67e7f7c71198347eec4076aa747",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 63527,
      "upload_time": "2022-05-20T15:38:41",
      "upload_time_iso_8601": "2022-05-20T15:38:41.580020Z",
      "url": "https://files.pythonhosted.org/packages/a0/d1/9d98336a7f2a27e67957488174c53f65eddd14d8cca738320466e247c2d2/stoke-0.2.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a54530f85b9658adec6a5a8909c8af74de68059f53888ead1d643f470da0c83e",
        "md5": "fdaf04c78e75eec197b37ccd2697c910",
        "sha256": "b85bb6436da5a22ffb7efc5bc7ff51d9aea59cbe833e64337a82a8f8105fa479"
      },
      "downloads": -1,
      "filename": "stoke-0.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "fdaf04c78e75eec197b37ccd2697c910",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 80449,
      "upload_time": "2022-05-20T15:38:43",
      "upload_time_iso_8601": "2022-05-20T15:38:43.634976Z",
      "url": "https://files.pythonhosted.org/packages/a5/45/30f85b9658adec6a5a8909c8af74de68059f53888ead1d643f470da0c83e/stoke-0.2.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}