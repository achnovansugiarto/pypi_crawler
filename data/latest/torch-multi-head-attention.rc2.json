{
  "info": {
    "author": "CyberZHG",
    "author_email": "CyberZHG@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "# PyTorch Multi-Head Attention\n\n[![Travis](https://travis-ci.org/CyberZHG/torch-multi-head-attention.svg)](https://travis-ci.org/CyberZHG/torch-multi-head-attention)\n[![Coverage](https://coveralls.io/repos/github/CyberZHG/torch-multi-head-attention/badge.svg?branch=master)](https://coveralls.io/github/CyberZHG/torch-multi-head-attention)\n\n## Install\n\n```bash\npip install torch-multi-head-attention\n```\n\n## Usage\n\n```python\nfrom torch_multi_head_attention import MultiHeadAttention\n\nMultiHeadAttention(in_features=768, head_num=12)\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/CyberZHG/torch-multi-head-attention",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torch-multi-head-attention",
    "package_url": "https://pypi.org/project/torch-multi-head-attention/",
    "platform": "",
    "project_url": "https://pypi.org/project/torch-multi-head-attention/",
    "project_urls": {
      "Homepage": "https://github.com/CyberZHG/torch-multi-head-attention"
    },
    "release_url": "https://pypi.org/project/torch-multi-head-attention/0.15.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Multi-head attention implemented in PyTorch",
    "version": "0.15.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4860010,
  "releases": {
    "0.15.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fec66bf35cf1292e4a7756c534fa4373f539a946904435338e5b1d0a6ae444a6",
          "md5": "8c15db37e0ca2bdc088d89da7c0f30ba",
          "sha256": "7e51ea6a54b4ee16134c00ea8930e444757f326951ba9cde709410e7c11c9fe1"
        },
        "downloads": -1,
        "filename": "torch-multi-head-attention-0.15.0.tar.gz",
        "has_sig": false,
        "md5_digest": "8c15db37e0ca2bdc088d89da7c0f30ba",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 3762,
        "upload_time": "2019-02-21T09:58:05",
        "upload_time_iso_8601": "2019-02-21T09:58:05.196854Z",
        "url": "https://files.pythonhosted.org/packages/fe/c6/6bf35cf1292e4a7756c534fa4373f539a946904435338e5b1d0a6ae444a6/torch-multi-head-attention-0.15.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.15.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8d73b0734654ec4c950270d32c3d4ffb7460e63df229021a52386bf86356e815",
          "md5": "e24c9e56e808eee69921d26768f8bcca",
          "sha256": "e181602fe1ef6da8322cb6bc1ffb41f52d3658c54e3937040e8f186754bb3056"
        },
        "downloads": -1,
        "filename": "torch-multi-head-attention-0.15.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e24c9e56e808eee69921d26768f8bcca",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 3592,
        "upload_time": "2019-02-24T04:44:07",
        "upload_time_iso_8601": "2019-02-24T04:44:07.255356Z",
        "url": "https://files.pythonhosted.org/packages/8d/73/b0734654ec4c950270d32c3d4ffb7460e63df229021a52386bf86356e815/torch-multi-head-attention-0.15.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8d73b0734654ec4c950270d32c3d4ffb7460e63df229021a52386bf86356e815",
        "md5": "e24c9e56e808eee69921d26768f8bcca",
        "sha256": "e181602fe1ef6da8322cb6bc1ffb41f52d3658c54e3937040e8f186754bb3056"
      },
      "downloads": -1,
      "filename": "torch-multi-head-attention-0.15.1.tar.gz",
      "has_sig": false,
      "md5_digest": "e24c9e56e808eee69921d26768f8bcca",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 3592,
      "upload_time": "2019-02-24T04:44:07",
      "upload_time_iso_8601": "2019-02-24T04:44:07.255356Z",
      "url": "https://files.pythonhosted.org/packages/8d/73/b0734654ec4c950270d32c3d4ffb7460e63df229021a52386bf86356e815/torch-multi-head-attention-0.15.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}