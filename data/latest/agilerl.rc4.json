{
  "info": {
    "author": "Nick Ustaran-Anderegg",
    "author_email": "dev@agilerl.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: Other/Proprietary License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.11",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# AgileRL\n<p align=\"center\">\n  <img src=https://user-images.githubusercontent.com/47857277/222710068-e09a4e3c-368c-458a-9e01-b68674806887.png height=\"120\">\n</p>\n<p align=\"center\"><b>Reinforcement learning streamlined.</b><br>Easier and faster reinforcement learning with RLOps. Visit our <a href=\"https://agilerl.com\">website</a>. View <a href=\"https://agilerl.readthedocs.io/en/latest/\">documentation</a>.<br>Join the <a href=\"https://discord.gg/eB8HyTA2ux\">Discord Server</a> to collaborate.</p>\n\n<div align=\"center\">\n\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Documentation Status](https://readthedocs.org/projects/agilerl/badge/?version=latest)](https://agilerl.readthedocs.io/en/latest/?badge=latest)\n[![Downloads](https://static.pepy.tech/badge/agilerl)](https://pypi.python.org/pypi/agilerl/)\n[![Discord](https://dcbadge.vercel.app/api/server/eB8HyTA2ux?style=flat)](https://discord.gg/eB8HyTA2ux)\n\n</div>\n\nThis is a Deep Reinforcement Learning library focused on improving development by introducing RLOps - MLOps for reinforcement learning.\n  \nThis library is initially focused on reducing the time taken for training models and hyperparameter optimisation (HPO) by pioneering evolutionary HPO techniques for reinforcement learning.<br>\nEvolutionary HPO has been shown to drastically reduce overall training times by automatically converging on optimal hyperparameters, without requiring numerous training runs.<br>\nWe are constantly adding more algorithms, with a view to add hierarchical and multi-agent algorithms soon.\n\n## Get Started\nInstall as a package with pip: \n```bash\npip install agilerl\n```\nOr install in development mode: (Recommended due to nascent nature of this library)\n```bash\ngit clone https://github.com/AgileRL/AgileRL.git && cd AgileRL\npip install -r requirements.txt\n```\n\n## Algorithms implemented (more coming soon!)\n  * DQN\n  * DDPG\n\n## Train an agent\nBefore starting training, there are some meta-hyperparameters and settings that must be set. These are defined in <code>INIT_HP</code>, for general parameters, and <code>MUTATION_PARAMS</code>, which define the evolutionary probabilities. For example:\n```python\nINIT_HP = {\n    'ENV_NAME': 'LunarLander-v2',   # Gym environment name\n    'ALGO': 'DQN',                  # Algorithm\n    'HIDDEN_SIZE': [64,64],         # Actor network hidden size\n    'BATCH_SIZE': 256,              # Batch size\n    'LR': 1e-3,                     # Learning rate\n    'EPISODES': 2000,               # Max no. episodes\n    'TARGET_SCORE': 200.,           # Early training stop at avg score of last 100 episodes\n    'GAMMA': 0.99,                  # Discount factor\n    'MEMORY_SIZE': 10000,           # Max memory buffer size\n    'LEARN_STEP': 1,                # Learning frequency\n    'TAU': 1e-3,                    # For soft update of target parameters\n    'TOURN_SIZE': 2,                # Tournament size\n    'ELITISM': True,                # Elitism in tournament selection\n    'POP_SIZE': 6,                  # Population size\n    'EVO_EPOCHS': 20,               # Evolution frequency\n    'POLICY_FREQ': 2,               # Policy network update frequency\n    'WANDB': True                   # Log with Weights and Biases\n}\n```\n```python\nMUTATION_PARAMS = {\n    # Relative probabilities\n    'NO_MUT': 0.4,                              # No mutation\n    'ARCH_MUT': 0.2,                            # Architecture mutation\n    'NEW_LAYER': 0.2,                           # New layer mutation\n    'PARAMS_MUT': 0.2,                          # Network parameters mutation\n    'ACT_MUT': 0,                               # Activation layer mutation\n    'RL_HP_MUT': 0.2,                           # Learning HP mutation\n    'RL_HP_SELECTION': ['lr', 'batch_size'],    # Learning HPs to choose from\n    'MUT_SD': 0.1,                              # Mutation strength\n    'RAND_SEED': 1,                             # Random seed\n}\n```\nFirst, use <code>utils.initialPopulation</code> to create a list of agents - our population that will evolve and mutate to the optimal hyperparameters.\n```python\nfrom agilerl.utils import makeVectEnvs, initialPopulation\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nenv = makeVectEnvs(env_name=INIT_HP['ENV_NAME'], num_envs=16)\ntry:\n    num_states = env.single_observation_space.n\n    one_hot = True\nexcept:\n    num_states = env.single_observation_space.shape[0]\n    one_hot = True\ntry:\n    num_actions = env.single_action_space.n\nexcept:\n    num_actions = env.single_action_space.shape[0]\n\nagent_pop = initialPopulation(INIT_HP['ALGO'],\n  num_states,\n  num_actions,\n  one_hot,\n  INIT_HP,\n  INIT_HP['POP_SIZE'],\n  device=device)\n```\nNext, create the tournament, mutations and experience replay buffer objects that allow agents to share memory and efficiently perform evolutionary HPO.\n```python\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.hpo.mutation import Mutations\nimport torch\n\nfield_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\nmemory = ReplayBuffer(num_actions, INIT_HP['MEMORY_SIZE'], field_names=field_names, device=device)\n\ntournament = TournamentSelection(INIT_HP['TOURN_SIZE'],\n    INIT_HP['ELITISM'],\n    INIT_HP['POP_SIZE'],\n    INIT_HP['EVO_EPOCHS'])\n    \nmutations = Mutations(algo=INIT_HP['ALGO'],\n    no_mutation=MUTATION_PARAMS['NO_MUT'], \n    architecture=MUTATION_PARAMS['ARCH_MUT'], \n    new_layer_prob=MUTATION_PARAMS['NEW_LAYER'], \n    parameters=MUTATION_PARAMS['PARAMS_MUT'], \n    activation=MUTATION_PARAMS['ACT_MUT'], \n    rl_hp=MUTATION_PARAMS['RL_HP_MUT'], \n    rl_hp_selection=MUTATION_PARAMS['RL_HP_SELECTION'], \n    mutation_sd=MUTATION_PARAMS['MUT_SD'], \n    rand_seed=MUTATION_PARAMS['RAND_SEED'],\n    device=device)\n```\nThe easiest training loop implementation is to use our <code>training.train()</code> function. It requires the <code>agent</code> have functions <code>getAction()</code> and <code>learn().</code>\n```python\nfrom agilerl.training.train import train\n\ntrained_pop, pop_fitnesses = train(env,\n    INIT_HP['ENV_NAME'],\n    INIT_HP['ALGO'],\n    agent_pop,\n    memory=memory,\n    n_episodes=INIT_HP['EPISODES'],\n    evo_epochs=INIT_HP['EVO_EPOCHS'],\n    evo_loop=1,\n    target=INIT_HP['TARGET_SCORE'],\n    chkpt=INIT_HP['SAVE_CHKPT'],\n    tournament=tournament,\n    mutation=mutations,\n    wb=INIT_HP['WANDB'],\n    device=device)\n```\n\n### Custom Training Loop\nAlternatively, use a custom training loop. Combining all of the above:\n\n```python\nfrom agilerl.utils import makeVectEnvs, initialPopulation\nfrom agilerl.components.replay_buffer import ReplayBuffer\nfrom agilerl.hpo.tournament import TournamentSelection\nfrom agilerl.hpo.mutation import Mutations\nimport gymnasium as gym\nimport numpy as np\nimport torch\n\nINIT_HP = {\n            'HIDDEN_SIZE': [64,64], # Actor network hidden size\n            'BATCH_SIZE': 128,      # Batch size\n            'LR': 1e-3,             # Learning rate\n            'GAMMA': 0.99,          # Discount factor\n            'LEARN_STEP': 1,        # Learning frequency\n            'TAU': 1e-3             # For soft update of target network parameters\n            }\n\nenv = makeVectEnvs('LunarLander-v2', num_envs=16)   # Create environment\ntry:\n    num_states = env.single_observation_space.n         # Discrete observation space\n    one_hot = True                                      # Requires one-hot encoding\nexcept:\n    num_states = env.single_observation_space.shape[0]  # Continuous observation space\n    one_hot = False                                     # Does not require one-hot encoding\ntry:\n    num_actions = env.single_action_space.n             # Discrete action space\nexcept:\n    num_actions = env.single_action_space.shape[0]      # Continuous action space\n\npop = initialPopulation(algo='DQN',                 # Algorithm\n                        num_states=num_states,      # State dimension\n                        num_actions=num_actions,    # Action dimension\n                        one_hot=one_hot,            # One-hot encoding\n                        INIT_HP=INIT_HP,            # Initial hyperparameters\n                        population_size=6,          # Population size\n                        device=torch.device(\"cuda\"))\n\nfield_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\nmemory = ReplayBuffer(n_actions=num_actions,    # Number of agent actions\n                      memory_size=10000,        # Max replay buffer size\n                      field_names=field_names,  # Field names to store in memory\n                      device=torch.device(\"cuda\"))\n\ntournament = TournamentSelection(tournament_size=2, # Tournament selection size\n                                 elitism=True,      # Elitism in tournament selection\n                                 population_size=6, # Population size\n                                 evo_step=1)        # Evaluate using last N fitness scores\n\nmutations = Mutations(algo='DQN',                           # Algorithm\n                      no_mutation=0.4,                      # No mutation\n                      architecture=0.2,                     # Architecture mutation\n                      new_layer_prob=0.2,                   # New layer mutation\n                      parameters=0.2,                       # Network parameters mutation\n                      activation=0,                         # Activation layer mutation\n                      rl_hp=0.2,                            # Learning HP mutation\n                      rl_hp_selection=['lr', 'batch_size'], # Learning HPs to choose from\n                      mutation_sd=0.1,                      # Mutation strength\n                      rand_seed=1,                          # Random seed\n                      device=torch.device(\"cuda\"))\n\nmax_episodes = 1000 # Max training episodes\nmax_steps = 500     # Max steps per episode\n\n# Exploration params\neps_start = 1.0     # Max exploration\neps_end = 0.1       # Min exploration\neps_decay = 0.995   # Decay per episode\nepsilon = eps_start\n\nevo_epochs = 5      # Evolution frequency\nevo_loop = 1        # Number of evaluation episodes\n\nenv = makeVectEnvs('LunarLander-v2', num_envs=16)   # Create environment\n\n# TRAINING LOOP\nfor idx_epi in range(max_episodes):\n    for agent in pop:   # Loop through population\n        state = env.reset()[0]  # Reset environment at start of episode\n        score = 0\n        for idx_step in range(max_steps):\n            action = agent.getAction(state, epsilon)    # Get next action from agent\n            next_state, reward, done, _, _ = env.step(action)   # Act in environment\n            \n            # Save experience to replay buffer\n            memory.save2memoryVectEnvs(state, action, reward, next_state, done)\n\n            # Learn according to learning frequency\n            if memory.counter % agent.learn_step == 0 and len(memory) >= agent.batch_size:\n                experiences = memory.sample(agent.batch_size) # Sample replay buffer\n                agent.learn(experiences)    # Learn according to agent's RL algorithm\n            \n            state = next_state\n            score += reward\n\n    epsilon = max(eps_end, epsilon*eps_decay) # Update epsilon for exploration\n\n    # Now evolve population if necessary\n    if (idx_epi+1) % evo_epochs == 0:\n        \n        # Evaluate population\n        fitnesses = [agent.test(env, max_steps=max_steps, loop=evo_loop) for agent in pop]\n\n        print(f'Episode {idx_epi+1}/{max_episodes}')\n        print(f'Fitnesses: {[\"%.2f\"%fitness for fitness in fitnesses]}')\n        print(f'100 fitness avgs: {[\"%.2f\"%np.mean(agent.fitness[-100:]) for agent in pop]}')\n\n        # Tournament selection and population mutation\n        elite, pop = tournament.select(pop)\n        pop = mutations.mutation(pop)\n```\n\nView <a href=\"https://agilerl.readthedocs.io/en/latest/\">documentation</a>.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "agilerl",
    "package_url": "https://pypi.org/project/agilerl/",
    "platform": null,
    "project_url": "https://pypi.org/project/agilerl/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/agilerl/0.1.3/",
    "requires_dist": [
      "fastrand (>=1.3.0,<2.0.0)",
      "gymnasium (>=0.27.1,<0.28.0)",
      "matplotlib (>=3.4.3,<4.0.0)",
      "numpy (>=1.22.4,<2.0.0)",
      "torch (>=1.13.1,<2.0.0)",
      "tqdm (>=4.62.2,<5.0.0)",
      "wandb (>=0.13.1,<0.14.0)"
    ],
    "requires_python": ">=3.8,<4.0",
    "summary": "AgileRL is a deep reinforcement learning library focused on improving RL development through RLOps.",
    "version": "0.1.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17322577,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d460711465b34ffa718eac7c139a3968f8d733dfade487dcd48dc69af94daa84",
          "md5": "e5d545fbfc8ef10bb32f1eaf3e581ce3",
          "sha256": "1d3418f383b82497a3ffd9d26b97703b67952af57d8df10dd345a793b086a3b1"
        },
        "downloads": -1,
        "filename": "agilerl-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e5d545fbfc8ef10bb32f1eaf3e581ce3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<4.0",
        "size": 25757,
        "upload_time": "2023-03-07T16:35:08",
        "upload_time_iso_8601": "2023-03-07T16:35:08.614344Z",
        "url": "https://files.pythonhosted.org/packages/d4/60/711465b34ffa718eac7c139a3968f8d733dfade487dcd48dc69af94daa84/agilerl-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a44c7d4c1adfb28e230993af1204812245993ee51ac9236a854811995365b132",
          "md5": "407053eaddc0bae1d681e834d7899494",
          "sha256": "e86f711744f4af6614e26043d8d9bea887aa45ae8aa6d75182eff6756e55ad3f"
        },
        "downloads": -1,
        "filename": "agilerl-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "407053eaddc0bae1d681e834d7899494",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<4.0",
        "size": 20447,
        "upload_time": "2023-03-07T16:35:11",
        "upload_time_iso_8601": "2023-03-07T16:35:11.071509Z",
        "url": "https://files.pythonhosted.org/packages/a4/4c/7d4c1adfb28e230993af1204812245993ee51ac9236a854811995365b132/agilerl-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d8ec44d699f8a612d05a2913226d3dca65685c89d1a089c7f1cf8054229ae710",
          "md5": "9ee856ea14b4ba0212385c95e551894e",
          "sha256": "c9a6b6228f96f4d48959f536a2eeefe9e7049f820a132eadd8d5b78058d294cb"
        },
        "downloads": -1,
        "filename": "agilerl-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9ee856ea14b4ba0212385c95e551894e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<4.0",
        "size": 25745,
        "upload_time": "2023-03-07T17:14:44",
        "upload_time_iso_8601": "2023-03-07T17:14:44.009297Z",
        "url": "https://files.pythonhosted.org/packages/d8/ec/44d699f8a612d05a2913226d3dca65685c89d1a089c7f1cf8054229ae710/agilerl-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63810b11ec5e90dbd375ef15dcc0e435a9ea678ab46215da9ae9f5890f5d5c02",
          "md5": "a84c5ff2a9e616011600d98970fefc64",
          "sha256": "4dbbd8f3aad576abda862af6c93312db6841f0f91e04df1ec241cad2780a3d7a"
        },
        "downloads": -1,
        "filename": "agilerl-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a84c5ff2a9e616011600d98970fefc64",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<4.0",
        "size": 20473,
        "upload_time": "2023-03-07T17:14:45",
        "upload_time_iso_8601": "2023-03-07T17:14:45.566357Z",
        "url": "https://files.pythonhosted.org/packages/63/81/0b11ec5e90dbd375ef15dcc0e435a9ea678ab46215da9ae9f5890f5d5c02/agilerl-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "56e1d68a09b51b33f6d8e76b7ccc3f078162b8b7adc98b448977efe878a20b31",
          "md5": "bf173e64e7d97e741555c11d2193b45a",
          "sha256": "430545a67a27b9434aa1ca45ba3830efeed98e5728bfe77f8da8ee75671ec2ad"
        },
        "downloads": -1,
        "filename": "agilerl-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bf173e64e7d97e741555c11d2193b45a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<4.0",
        "size": 28715,
        "upload_time": "2023-03-09T15:33:26",
        "upload_time_iso_8601": "2023-03-09T15:33:26.209928Z",
        "url": "https://files.pythonhosted.org/packages/56/e1/d68a09b51b33f6d8e76b7ccc3f078162b8b7adc98b448977efe878a20b31/agilerl-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "df918ce0faf2506f5627d0845a2107bdcc36d1f90d1685fdca6a8118263bfa97",
          "md5": "54add5a700ad73438ebd16417628f65e",
          "sha256": "82e71d7718832f568f58567d5074c14aab5bfb59397af6c3f80b26a64bcec4a7"
        },
        "downloads": -1,
        "filename": "agilerl-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "54add5a700ad73438ebd16417628f65e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<4.0",
        "size": 23075,
        "upload_time": "2023-03-09T15:33:28",
        "upload_time_iso_8601": "2023-03-09T15:33:28.394032Z",
        "url": "https://files.pythonhosted.org/packages/df/91/8ce0faf2506f5627d0845a2107bdcc36d1f90d1685fdca6a8118263bfa97/agilerl-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "225474e26b703d7f5f2658f2c0dfe92cb5acc4068ba7a838696a7abacc1f1ca9",
          "md5": "abe879daf42e89742fa7f34684387bfb",
          "sha256": "85fa685e836aac948c42ac2b62b4372da13285889c52f05c32789bd4342d6495"
        },
        "downloads": -1,
        "filename": "agilerl-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "abe879daf42e89742fa7f34684387bfb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<4.0",
        "size": 29278,
        "upload_time": "2023-03-16T17:42:57",
        "upload_time_iso_8601": "2023-03-16T17:42:57.627670Z",
        "url": "https://files.pythonhosted.org/packages/22/54/74e26b703d7f5f2658f2c0dfe92cb5acc4068ba7a838696a7abacc1f1ca9/agilerl-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dd6249e3e4ffe3dc4073eb120a3c1a0b86f8d8bda15a9419e6ce69ed91440fc7",
          "md5": "c859c3af3f73b5d8fb074bc0857197cd",
          "sha256": "bd64b71dc1f043652174bb7a030838886ca5c05feb3bd014887442cc10893643"
        },
        "downloads": -1,
        "filename": "agilerl-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "c859c3af3f73b5d8fb074bc0857197cd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<4.0",
        "size": 23487,
        "upload_time": "2023-03-16T17:42:58",
        "upload_time_iso_8601": "2023-03-16T17:42:58.996939Z",
        "url": "https://files.pythonhosted.org/packages/dd/62/49e3e4ffe3dc4073eb120a3c1a0b86f8d8bda15a9419e6ce69ed91440fc7/agilerl-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "225474e26b703d7f5f2658f2c0dfe92cb5acc4068ba7a838696a7abacc1f1ca9",
        "md5": "abe879daf42e89742fa7f34684387bfb",
        "sha256": "85fa685e836aac948c42ac2b62b4372da13285889c52f05c32789bd4342d6495"
      },
      "downloads": -1,
      "filename": "agilerl-0.1.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "abe879daf42e89742fa7f34684387bfb",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8,<4.0",
      "size": 29278,
      "upload_time": "2023-03-16T17:42:57",
      "upload_time_iso_8601": "2023-03-16T17:42:57.627670Z",
      "url": "https://files.pythonhosted.org/packages/22/54/74e26b703d7f5f2658f2c0dfe92cb5acc4068ba7a838696a7abacc1f1ca9/agilerl-0.1.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dd6249e3e4ffe3dc4073eb120a3c1a0b86f8d8bda15a9419e6ce69ed91440fc7",
        "md5": "c859c3af3f73b5d8fb074bc0857197cd",
        "sha256": "bd64b71dc1f043652174bb7a030838886ca5c05feb3bd014887442cc10893643"
      },
      "downloads": -1,
      "filename": "agilerl-0.1.3.tar.gz",
      "has_sig": false,
      "md5_digest": "c859c3af3f73b5d8fb074bc0857197cd",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8,<4.0",
      "size": 23487,
      "upload_time": "2023-03-16T17:42:58",
      "upload_time_iso_8601": "2023-03-16T17:42:58.996939Z",
      "url": "https://files.pythonhosted.org/packages/dd/62/49e3e4ffe3dc4073eb120a3c1a0b86f8d8bda15a9419e6ce69ed91440fc7/agilerl-0.1.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}