{
  "info": {
    "author": "Ivan Pashkevich, Roman Sorokin",
    "author_email": "ivan.ig.pashkevich@gmail.com, sorokin.r.v.97@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "## quokkas\r\n\r\nData analysis tool that you didn't know you needed.\r\n\r\nQuokkas is a powerful pandas-based data analysis tool. In addition to the well-known pandas functionality, it provides \r\ntools for data preprocessing, pipelining and explorative data analysis.\r\n\r\nLet's have a short overview of these three pillars.\r\n\r\n### Preprocessing\r\n\r\nWith quokkas, it is incredibly easy to scale, impute, encode (incl. dates), normalize, trim, and winsorize your data. \r\nThese operations are not only easy to use - they are fast, robust and preserve your DataFrame structure / typing. \r\n\r\nFor instance, if you wish to standardize your data, you can simply do it like that:\r\n\r\n```\r\nimport quokkas as qk\r\ndf = qk.read_csv('42.csv')\r\ndf = df.scale('standard') \r\n# or simply df.scale('standard', inplace=True)\r\n```\r\n\r\nBy default, each transformation has an `auto=True` parameter. This parameter ensures that the transformations are \r\napplied only to \"relevant\" columns. For instance, scale, normalize, winsorize and trim are applied only to numeric \r\ncolumns, encode is applied to columns with \"few\" distinct values, and impute is strategy- and type-dependent (see more \r\nin our preprocessing deep-dive).\r\n\r\nAdditionally, by default the transformations do not include the target column(s) - which you can set via \r\n`df.targetize('target_column_name')` or `df.targetize(['target_one_column_name, 'target_two_column_name'])`. You can \r\nchange that behaviour by setting `include_target=True`, like in`df.impute('simple', strategy='default', \r\ninclude_target=True)`,   \r\n\r\nHowever, the user is, of course, able to simply select the columns that they want to transform or ensure that some \r\ncolumns are not transformed:\r\n```\r\n# only 'investor_type' and 'country_of_origin' will be ordinally encoded\r\ndf.encode(kind='ordinal', include=['investor_type', 'country_of_origin'], inplace=True) \r\n# the first argument is always 'kind', so we could also use df.encode('ordinal', ...)\r\n\r\n# all automatically selected columns except 'defcon_state' will be onehot-encoded\r\ndf.encode('onehot', exclude=['defcon_state'], inplace=True)\r\n\r\n# all columns in the dataframe except for 'familial_status' and 'accommodation' will be robustly scaled\r\ndf.scale('robust', auto=False, exclude=['familial_status', 'accommodation'], inplace=True)\r\n```\r\n\r\nAs you might have guessed, this column selection procedure is uniform across all data preprocessing functions. Some \r\npreprocessing functions have several (at times kind-dependent) additional parameters. For user convenience they are \r\nheavily aligned with sklearn preprocessing functionality. \r\n\r\nFor instance, `df.scale('standard')` supports additional boolean `with_mean` and `with_std` parameters, \r\n`df.encode('onehot')` supports additional `categories`, `drop`, `sparse`, `dtype`, `handle_unknown`, `min_frequency` and \r\n`max_categories` parameters, and `df.impute('simple')` supports `strategy`, `missing_values` and `fill_value` \r\nparameters which can be used just like you would use respective sklearn's `StandardScaler`, `OneHotEncoder`, and \r\n`SimpleImputer` parameters.\r\n\r\nBut how could you use the transformed data for, say, training a model? Of course, you could transfer the data to two \r\nnumpy arrays and go from there:\r\n```\r\ny = df['target_name'].to_numpy()\r\nX = df.drop('target_name'], axis=1).to_numpy()\r\n\r\n# which is equivalent to:\r\nX, y = df.separate(to_numpy=True)\r\n# if 'target_name' was targetized\r\n\r\nmodel = GradientBoostingClassifier(loss='log_loss', learning_rate=5e-2)\r\nmodel.fit(X, y)\r\n```\r\nHowever, quokkas provides an even easier way to fit a model to the dataframe. You just need to do the following:\r\n```\r\ndf.fit(GradientBoostingClassifier(loss='log_loss', learning_rate=5e-2))\r\n\r\n# now you can access the trained model via\r\nmodel = df.pipeline.model\r\n\r\n# and you can make predictions for the dataframe like that:\r\ny_pred = df.predict()\r\n\r\n# or, if you wanted to predict the values for another dataframe, you could use:\r\ny_pred = df_test.predict(df.pipeline.model)\r\n```\r\nThis example forces us to think about the following natural problem: sometimes we would like to transform a dataframe in \r\nexactly the same way as another dataframe. This is very easy in quokkas! Let's quickly learn how to do it:\r\n\r\n### Pipelining\r\n\r\nBy default, quokkas pipelines (most of) the dataframe functions. That means that after each transformation, quokkas \r\nsaves the data needed to do exactly the same transform again - which can be used, for instance, on another dataframe. \r\nThis new dataframe can be changed via `df_new.stream(df_old)` - which finds the first operation in `df_old` that wasn't \r\napplied to `df_new` and applies the rest of the `df_old` pipeline to `df_new`.\r\n\r\nHere is an example: say we have some data in a csv file, and we would like to load it, add a couple of columns, \r\npreprocess the data, fit a model, and evaluate the performance on the test dataset. Of course, we want to evaluate the \r\nperformance of the model in a clean way - in particular, we would like to fit all data preprocessors (e.g. scaler) \r\nsolely on the training data, without looking at the test data. Here is how we do it:\r\n\r\n```\r\n# load data\r\ndf = qk.read_csv('data.csv')\r\n\r\n# create a couple of additional variables\r\ndf['sales_cash_ratio'] = df['sales'] / df['cash'] \r\ndf['return'] = df['price'].pct_change()\r\ndf['return_squared'] = df['return'] ** 2\r\n\r\n# we would like to predict the returns for the next period:\r\ndf['target_return'] = df['return'].shift(-1)\r\n\r\n# split the data into train and test sets - convenient functionality, btw!\r\n# default split is 80% train and 20% test\r\ndf_train, df_test = df.train_test_split(random_seed=42)\r\n\r\n# turn on the inplace mode (default inplace=False in all functions) - strictly speaking not necessary,\r\n# could achieve the same by writing inplace=True for each preprocessing function  \r\nqk.Mode.inplace()\r\n# this can be undone with qk.Mode.outplace()\r\n\r\n# targetize the target_return column, impute the missing values for feature columns\r\n# and drop missing values for target - note that the impute function imputes missing values for all \r\n# columns except target, so the only missing values left after imputing will be in the target\r\ndf_train.targetize('target_return').impute().dropna()\r\n\r\n# scale the data robustly, winsorize the data and encode auto-detected values\r\n# and encode dates - note that scaling and winsorization (with default auto=True) \r\n# does not affect non-numeric columns\r\ndf_train.scale('robust').winsorize(limits=(0.005, 0.005)).encode('onehot', drop='first').encode_dates(intrayear=True)\r\n\r\n# fit the model\r\ndf_train.fit(RandomForestRegressor())\r\n\r\n# change df_test exactly like we changed df_train, but without refitting of scalers / encoders\r\n# in a scenario when you want to refit transformers on the new dataframe, you can set fit=True\r\n# by default, stream will also copy the df_train's model\r\ndf_test.stream(df_train.pipeline)\r\n# or, alternatively, df_test.stream(df_train)\r\n\r\n# make predictions\r\npreds = df_test.predict()\r\n\r\n# evaluate the results\r\n_, trues = df_test.separate()\r\nmse = np.mean((preds - trues)**2)\r\n```\r\nSo, quite easy indeed. Above we used the `train_test_split` function to split the data into a training and a test set, \r\nbut what if we wanted to split data into multiple sets, e.g. for validation? For that we can use functions \r\n`train_val_test_split` and `split`. Here are some examples:    \r\n```\r\ndf = df.sample(10000)\r\n\r\n# default sizes for train_val_test_split are (0.8, 0.1, 0.1)\r\naccepts parameters train_size, val_size and test_size and can infer one of them if the others are specified\r\ndf_train, df_val, df_test = df.train_val_test_split(train_size=0.7, val_size=0.1)\r\n\r\n# the same result can be achieved with the split function\r\ndf_train, df_val, df_test = df.split(n_splits=3, sizes=(0.7, 0.1, 0.2))\r\n# equivalent to {n_splits=3, sizes=(7000, 1000, 2000)}, {sizes=(7000, 1000, 2000)}, {n_splits=3, sizes=(7000, 1000)}\r\n# since n_splits can be inferred from sizes and one remaining size can be inferred if n_splits is specified\r\n# if sizes are not specified at all, split function would split the data into n_splits approximately equal parts\r\n``` \r\nBy default, splitter `kind` is set to `shuffled`, other options include `sequential` (rows are not shuffled before \r\nsplit), `stratified` (preserves the same proportions of examples in each class of the provided column) and `sorted` (the \r\ndataframe is first sorted by a provided column, then a sequential split is performed). quokkas also offers a way to \r\nperform a k-fold split that is often used for cross-validation:\r\n```\r\ndf.targetize('target_return')\r\nmse = []\r\nfor df_train, df_test in df.kfold(kind='stratified', n_splits=3):\r\n    df_train.scale('robust').winsorize(limits=(0.005, 0.005)).encode('onehot', drop='first').encode_dates(intrayear=True)\r\n    df_train.fit(RandomForestRegressor())\r\n    df_test.stream(df_train)\r\n    preds = df_test.predict()\r\n    trues = df_test.target\r\n    mse.append(np.mean((preds - trues)**2))\r\n```\r\nThere is also a much easier way to perform cross-validation based on k-fold split in quokkas:\r\n```\r\ndef mse(y_true, y_pred):\r\n    return np.mean((y_pred - y_true)**2)\r\nresult = df.cross_validate(estimator=RandomForestRegressor(), scoring=mse, cv=3, target='target_return')\r\n```\r\nYou can obtain the fitted models, training scores and predictions by setting `return_estimator`, \r\n`return_train_score` and `return_predictions` parameters to `True`. The difference between the two approaches is that \r\nwhile using the `cross_validate` function is easier, it doesn't allow us to specify transformations to be performed on \r\nthe training and test data before fitting the model or making predictions - meaning it is still useful in most cases. \r\n\r\nBack to pipelining! Could we transform a completely new dataframe (say, some unlabeled data) in exactly the same way? \r\nWell, almost. As mentioned before, quokkas pipelines most of the dataframe operations. However, it does not keep track \r\nof column operations, or operations that involve other dataframes. How could we manage that?\r\n\r\nWell, nothing is easier! Quokkas `df.map()` method allows us to pipeline any function that we might want to apply to a \r\ndataframe. We could use it like that:\r\n```\r\ndef add_columns(df):\r\n    df['sales_cash_ratio'] = df['sales'] / df['cash'] \r\n    df['return'] = df['price'].pct_change()\r\n    df['return_squared'] = df['return'] ** 2\r\n    df['target_return'] = df['return'].shift(-1)\r\n    return df # it is critical to return the dataframe in a function that will be \"mapped\"\r\n\r\ndf.map(add_columns)\r\ndf.targetize('target_return').scale('robust').encode('onehot', drop='first')\r\n\r\ndf_new = qk.read_csv('test_data.csv')\r\n\r\n# stream all the changes - now with the pipelined column operations!\r\ndf_new.stream(df)\r\npreds = df_new.predict()\r\n```\r\nA trained pipeline of a dataframe can be easily saved (as long as all transformations in it can be pickled). This is \r\nhow we can do that:\r\n```\r\n# save\r\ndf.pipeline.save('path.pkl')\r\n\r\n# load\r\npipeline = qk.Pipeline.load('path.pkl')\r\n\r\n# visualize the pipeline\r\nprint(pipeline)\r\n\r\n# apply loaded pipeline to a new dataframe\r\ndf_new.stream(pipeline)\r\n```\r\nAs discussed above, all quokkas-native preprocessing functions (i.e. encode, scale, encode_dates, impute, winsorize, \r\ntrim) are saved in the pipeline, and an arbitrary function on a dataframe can be added to the pipeline via map. Most of \r\nthe dataframe's member functions that transform the dataframe in some way are also added to the pipeline (\"pipelined\") \r\nautomatically. Here is a list of the pipelined functions:\r\n```\r\ndf.abs() # this function got an additional inplace parameter compared to pd implementation\r\ndf.aggregate()\r\ndf.apply()\r\ndf.applymap()\r\ndf.asfreq()\r\ndf.astype()\r\ndf.bfill()\r\ndf.clip()\r\ndf.corr()\r\ndf.cov()\r\ndf.diff()\r\ndf.drop_duplicates()\r\ndf.drop()\r\ndf.droplevel()\r\ndf.dropna()\r\ndf.explode()\r\ndf.ffill()\r\ndf.fillna()\r\ndf.filter()\r\ndf.interpolate()\r\ndf.melt()\r\ndf.pipe()\r\ndf.pivot()\r\ndf.query()\r\ndf.rename_axis()\r\ndf.rename()\r\ndf.reorder_levels()\r\ndf.replace()\r\ndf.reset_index()\r\ndf.round()\r\ndf.select_dtypes()\r\ndf.shift()\r\ndf.sort_index()\r\ndf.sort_values()\r\ndf.stack()\r\ndf.swapaxes()\r\ndf.swaplevel()\r\ndf.targetize()\r\ndf.to_datetime() # similar in functionality to pd.to_datetime), but the user provides column labels instead of dataframe\r\ndf.to_period()\r\ndf.to_timestamp()\r\ndf.transform()\r\ndf.transpose()\r\ndf.unstack()\r\n```\r\nThe following member functions preserve the pipeline without adding themselves to it:\r\n```\r\ndf.align()\r\ndf.append()\r\ndf.asof()\r\ndf.combine()\r\ndf.combine_first()\r\ndf.corrwith()\r\ndf.dot()\r\ndf.get()\r\ndf.iloc[]\r\ndf.join()\r\ndf.loc[]\r\ndf.mask()\r\ndf.merge()\r\ndf.reindex()\r\ndf.sample()\r\ndf.set_axis()\r\ndf.set_index()\r\ndf.update()\r\ndf.where()\r\ndf.__getitem__() # i.e. df[['column_one', 'column_two']] preserves pipeline\r\n```\r\nAdditionally, all arithmetic operations preserve the pipeline of the left element. As you might have noted, all \r\noperations that require another dataframe / series to work are not pipelined. This ensures that the pipeline does not \r\nbecome too large. Of course, if the user wants to pipeline these operations, they can do it via map - as dicussed above.\r\n\r\nIf the user does not wish to pipeline a certain operation, they could turn a pipeline of the dataframe off. There are \r\ntwo principal ways to do that:\r\n```\r\n# disable the pipeline\r\ndf.pipeline.disable()\r\n\r\ndf.abs(inplace=True) # won't be pipelined\r\n\r\n# enable the pipeline\r\ndf.pipeline.enable()\r\ndf.abs(inplace=True) # will be pipelined\r\n\r\n# the pipeline can also be disabled via context manager:\r\nwith df.pipeline:\r\n    df.abs(inplace=True) # will not be pipelined\r\n```\r\n\r\nEvery selection operation preserves the pipeline (provided that the result of the operation is a dataframe). In \r\nparticular, each time `df.iloc[]` is called, the pipeline of the original dataframe is copied. This makes those \r\nselection operations a little bit slower. There is a solution for that: quokkas provides a functionality to completely \r\nlock all pipeline operations via `qk.Lock.global_lock()` (which can be reversed with `qk.Lock.global_unlock()`). There \r\nis also a convenient context manager:\r\n```\r\nwith qk.Lock.lock():\r\n    df = df.scale('minmax')encode('ordinal').encode_dates(intraweek=True) # none of the operations will be pipelined\r\n```\r\nNote the difference: disabling the pipeline prevents transformations from being added to the pipeline, while the global \r\nlock prevents any operation on the pipeline. In particular, even when using operations that would generally preserve the \r\npipeline, with a global lock the pipeline might not be preserved!\r\n\r\nNow that we have discussed how the pipelining works for quokkas dataframes, we can move to the last important feature of \r\nthis package:\r\n\r\n### Exploratory Data Analysis\r\nQuokkas provides some very useful (in our unbiased opinion) capabilities to help the user understand the data better. \r\nSome provided functions are fairly standard:\r\nfor instance, the user can visualize the correlation matrix, create scatter plots for features / target variable \r\n(recommended if the target values are continuous), create density plots of features for each distinct value of the \r\ntarget variable as well as plot missing values for all features (and, if necessary, target). Here is an example of the \r\ninterface:\r\n```\r\n# correlation plot\r\ndf.plot_correlation(auto=True, include_target=False, absolute=True, style='seaborn')\r\n\r\n# scatter plot, n_line_items corresponds to the number of plots in one line\r\ndf.plot_scatter(include=['col_1', 'col_2', 'col_3', 'col_4'], auto=False, n_line_items=2)\r\n\r\n# density plot (kde)\r\ndf.plot_density(auto=True, n_line_items=3)\r\n\r\n# missing values plot, reverse=False means that the shares of missing values will be plotted\r\n# otherwise, shares of present values would be plotted instead\r\ndf.plot_missing_values(reverse=False, figsize=(12, 5))\r\n```\r\nOf course, the target for scatter and density plots should be provided to the dataframe via the targetize member \r\nfunction. An attentive reader might guess that the 'include' / 'exclude' / 'auto' logic here is the same as for the \r\npreprocessing functionality. By default, 'auto' is enabled, so in most cases the user does not need to provide any \r\narguments at all. Every charting function in quokkas allows the user to choose the style of the chart (string \r\ncorresponding to one of the plt.styles via style argument) and the figure size (via figsize argument). \r\n\r\nAdditionally, quokkas provides a bit of non-standard charting functionality: the user may wish to view how the feature \r\nvalues depend on the values of the target. The function `df.plot_feature_means()` does exactly that. If the target \r\nvariable is continuous, the user may provide an integer 'buckets' argument - the target variable values will be split\r\nin that many quantiles and the mean of each variable will be plotted for each of the quantiles. In the case when the \r\ntarget variable is categorical, the means of feature values will be plotted for each distinct value of target variable.\r\n\r\nThe user can specify if the target variable should be considered continuous or categorical via 'kind' argument: e.g. \r\n`df.plot_feature_means(kind='categorical')` or `df.plot_feature_means(kind='continuous')`. By default, the kind \r\nparameter is set to 'default', which means that quokkas will attempt to infer the type of the target variable itself.\r\n\r\nThis plot can be quantified in a simple way as well - for that, the user can use the `df.feature_importance()` function. \r\nThis function calculates the variance of the means of the standardized feature values among different buckets / distinct \r\nvalues, corrects that variance with the expected variance of the means of the buckets / distinct values and returns the \r\nshare of this corrected variance for each feature.\r\n\r\nThe last EDA function that we will cover is very simple: `df.suggest_categorical(strategy=...)` suggests the features \r\nthat should be considered categorical. It has the following strategies: 'count', 'type' and 'count&type'. If 'count' is \r\nselected, the decision will be based on the number of distinct feature values (if there are fewer than min(cat_number, \r\ncat_share*n_rows), the feature will be considered categorical, where cat_number and cat_share are parameters with \r\ndefault values 20 and 0.1). If 'type' is selected, the categorical features will be selected based on column type, and \r\nif 'count&type' is selected, all columns that would be selected under 'type' and 'count' strategies would be selected.\r\n\r\nWe hope that you have a lot of fun working with quokkas! If you have any issues or suggestions - please feel free to \r\ncontact us! We will do our best to help!\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "quokkas",
    "package_url": "https://pypi.org/project/quokkas/",
    "platform": null,
    "project_url": "https://pypi.org/project/quokkas/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/quokkas/0.0.4/",
    "requires_dist": [
      "pandas (>=1.3.5)",
      "numpy (>=1.19)",
      "scikit-learn (>=1.0.1)",
      "seaborn (>=0.11.2)",
      "scipy (>=1.7.0)",
      "joblib (>=1.1.0)"
    ],
    "requires_python": ">=3.7",
    "summary": "Data analysis tool that you didn't know you needed",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15932179,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bc0ed00033e6d5efddc6856cc8671eb36c2795bf6ea21dec489a81447e1c3b81",
          "md5": "1b4f47930fe04be9615f3185038298c7",
          "sha256": "4bba2da1329ff2084aa521697dc668c295d12f68a7ee1e860ae7f26365a00c49"
        },
        "downloads": -1,
        "filename": "quokkas-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1b4f47930fe04be9615f3185038298c7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 745339,
        "upload_time": "2022-10-16T17:49:16",
        "upload_time_iso_8601": "2022-10-16T17:49:16.061001Z",
        "url": "https://files.pythonhosted.org/packages/bc/0e/d00033e6d5efddc6856cc8671eb36c2795bf6ea21dec489a81447e1c3b81/quokkas-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4f1a5a5966803ff84ad223e5c56ebbb63942eb61485670779d59179d79b0a5c3",
          "md5": "e02e08f0e70a448e258f026872839556",
          "sha256": "b188f1a3a454f0feb9d572d55f597679fd5c9d54bd3c94f692cd1c3bff653243"
        },
        "downloads": -1,
        "filename": "quokkas-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e02e08f0e70a448e258f026872839556",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 694228,
        "upload_time": "2022-10-16T17:49:21",
        "upload_time_iso_8601": "2022-10-16T17:49:21.350178Z",
        "url": "https://files.pythonhosted.org/packages/4f/1a/5a5966803ff84ad223e5c56ebbb63942eb61485670779d59179d79b0a5c3/quokkas-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8f6b167276112531db032d477d4cc7bfff15055712d94dd0eeb67b6bbc687c7f",
          "md5": "a5d36a9ef54193e5b32806ac2b73f974",
          "sha256": "761e9c29480117db4b9f09f9891e3e8b638eb7970ca683d49befbb319e058013"
        },
        "downloads": -1,
        "filename": "quokkas-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a5d36a9ef54193e5b32806ac2b73f974",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 738822,
        "upload_time": "2022-10-27T19:13:21",
        "upload_time_iso_8601": "2022-10-27T19:13:21.297659Z",
        "url": "https://files.pythonhosted.org/packages/8f/6b/167276112531db032d477d4cc7bfff15055712d94dd0eeb67b6bbc687c7f/quokkas-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "43805b132a5be7d3b53671de9e9cdbdf5ab697c26f86ec5b1a573ff2ace42f5b",
          "md5": "2e2f6fc8ad6756859ae4b2c2fc874f29",
          "sha256": "b89f3c4c65577ca610ecf736c91a7516df5fb5f267a53616b6ac38e3ab6fb4b0"
        },
        "downloads": -1,
        "filename": "quokkas-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "2e2f6fc8ad6756859ae4b2c2fc874f29",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 686698,
        "upload_time": "2022-10-27T19:13:32",
        "upload_time_iso_8601": "2022-10-27T19:13:32.877632Z",
        "url": "https://files.pythonhosted.org/packages/43/80/5b132a5be7d3b53671de9e9cdbdf5ab697c26f86ec5b1a573ff2ace42f5b/quokkas-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ce651e5440273c07960abff5fae39839a243136b7fcb38be7e0bcb13b6ecd9ca",
          "md5": "b857f6c4a08cc281d7105b3061c185f6",
          "sha256": "781282439916d5bad2d44cd6eb2e82afabe7ec4ce02d1a509d7c7ffdf633e851"
        },
        "downloads": -1,
        "filename": "quokkas-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b857f6c4a08cc281d7105b3061c185f6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 955319,
        "upload_time": "2022-11-27T11:02:12",
        "upload_time_iso_8601": "2022-11-27T11:02:12.998692Z",
        "url": "https://files.pythonhosted.org/packages/ce/65/1e5440273c07960abff5fae39839a243136b7fcb38be7e0bcb13b6ecd9ca/quokkas-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ff8d595f791192619ec5a9c659addcf6ef6fd7c2708832eb3d5126aac099a999",
          "md5": "97ae9363dd1895bb9a6df5749f25b1c2",
          "sha256": "81eaa9a217a19a93458fa9f89e0aed9dc5ba5e2bfa106178e3546efd614ad856"
        },
        "downloads": -1,
        "filename": "quokkas-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "97ae9363dd1895bb9a6df5749f25b1c2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 810639,
        "upload_time": "2022-11-27T11:02:18",
        "upload_time_iso_8601": "2022-11-27T11:02:18.120414Z",
        "url": "https://files.pythonhosted.org/packages/ff/8d/595f791192619ec5a9c659addcf6ef6fd7c2708832eb3d5126aac099a999/quokkas-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "40024d14993c52523917521051c35bd345829507c4c90bf60491b123a6a55963",
          "md5": "06939b500fe6b2815b491ef54e4b11d8",
          "sha256": "6f5f0d311a43af10e7deb6d13a02aeb456f388e696b03e91034227d3f461fb06"
        },
        "downloads": -1,
        "filename": "quokkas-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "06939b500fe6b2815b491ef54e4b11d8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 961955,
        "upload_time": "2022-11-29T19:48:03",
        "upload_time_iso_8601": "2022-11-29T19:48:03.061080Z",
        "url": "https://files.pythonhosted.org/packages/40/02/4d14993c52523917521051c35bd345829507c4c90bf60491b123a6a55963/quokkas-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "35221025cf98073a6518215a1a94d64717fbb6920fd2e63e7c094d641ffd9e78",
          "md5": "e4574915ff40d4943cd3179db919c2b6",
          "sha256": "f91d67947ceb3455d715efb0d9573897305ec2d931d5a18635eeb5f40f6e60bb"
        },
        "downloads": -1,
        "filename": "quokkas-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "e4574915ff40d4943cd3179db919c2b6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 817463,
        "upload_time": "2022-11-29T19:48:09",
        "upload_time_iso_8601": "2022-11-29T19:48:09.779449Z",
        "url": "https://files.pythonhosted.org/packages/35/22/1025cf98073a6518215a1a94d64717fbb6920fd2e63e7c094d641ffd9e78/quokkas-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "40024d14993c52523917521051c35bd345829507c4c90bf60491b123a6a55963",
        "md5": "06939b500fe6b2815b491ef54e4b11d8",
        "sha256": "6f5f0d311a43af10e7deb6d13a02aeb456f388e696b03e91034227d3f461fb06"
      },
      "downloads": -1,
      "filename": "quokkas-0.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "06939b500fe6b2815b491ef54e4b11d8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 961955,
      "upload_time": "2022-11-29T19:48:03",
      "upload_time_iso_8601": "2022-11-29T19:48:03.061080Z",
      "url": "https://files.pythonhosted.org/packages/40/02/4d14993c52523917521051c35bd345829507c4c90bf60491b123a6a55963/quokkas-0.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "35221025cf98073a6518215a1a94d64717fbb6920fd2e63e7c094d641ffd9e78",
        "md5": "e4574915ff40d4943cd3179db919c2b6",
        "sha256": "f91d67947ceb3455d715efb0d9573897305ec2d931d5a18635eeb5f40f6e60bb"
      },
      "downloads": -1,
      "filename": "quokkas-0.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "e4574915ff40d4943cd3179db919c2b6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 817463,
      "upload_time": "2022-11-29T19:48:09",
      "upload_time_iso_8601": "2022-11-29T19:48:09.779449Z",
      "url": "https://files.pythonhosted.org/packages/35/22/1025cf98073a6518215a1a94d64717fbb6920fd2e63e7c094d641ffd9e78/quokkas-0.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}