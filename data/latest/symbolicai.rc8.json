{
  "info": {
    "author": "",
    "author_email": "Marius-Constantin Dinu <office@alphacoreai.eu>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# **SymbolicAI**\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/symai_logo.png\" width=\"200px\">\n\n## **A Neuro-Symbolic Perspective on Large Language Models (LLMs)**\n\n*Building applications with LLMs at its core through our `Symbolic API` leverages the power of classical and differentiable programming in Python.*\n\nRead further [**documentation here**](https://symbolicai.readthedocs.io/).\n\n[![PyPI version](https://badge.fury.io/py/symbolicai.svg)](https://badge.fury.io/py/symbolicai) [![License](https://img.shields.io/badge/License-BSD_3--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/dinumariusc.svg?style=social&label=Follow%20%40DinuMariusC)](https://twitter.com/DinuMariusC) [![Twitter](https://img.shields.io/twitter/url/https/twitter.com/symbolicapi.svg?style=social&label=Follow%20%40SymbolicAI)](https://twitter.com/SymbolicAPI) [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/Xpitfire/symbolicai/issues)\n[![Discord](https://img.shields.io/discord/768087161878085643?label=Discord&logo=Discord&logoColor=white)](https://discord.gg/QYMNnh9ra8) [![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FXpitfire%2Fsymbolicai&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com) [![GitHub forks](https://img.shields.io/github/forks/Xpitfire/symbolicai.svg?style=social&label=Fork&maxAge=2592000)](https://GitHub.com/Xpitfire/symbolicai) [![GitHub stars](https://img.shields.io/github/stars/Xpitfire/symbolicai.svg?style=social&label=Star&maxAge=2592000)](https://GitHub.com/Xpitfire/symbolicai/stargazers/)\n\n\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/preview.gif\">\n\n## Abstract\n\nConceptually, SymbolicAI is a framework that uses machine learning - and specifically LLMs - at its core, and composes operations based on task-specific prompting. We adopt a divide and conquer approach to decompose a complex problem into smaller problems. Therefore, each operation solves a simple task. By re-combining these operations we can solve the complex problem. Furthermore, our design principles allow us to transition between differentiable and classical programming, and to leverage the power of both worlds.\n\n## üìñ Table of Contents\n\n- [**SymbolicAI**](#symbolicai)\n  - [**A Neuro-Symbolic Perspective on Large Language Models (LLMs)**](#a-neuro-symbolic-perspective-on-large-language-models-llms)\n  - [Abstract](#abstract)\n  - [üìñ Table of Contents](#-table-of-contents)\n  - [üîß Get Started](#-get-started)\n    - [‚û°Ô∏è Quick Install](#Ô∏è-quick-install)\n    - [API Keys](#api-keys)\n    - [*\\[Optional\\]* Installs](#optional-installs)\n  - [ü¶ñ Apps](#-apps)\n    - [Shell Command Tool](#shell-command-tool)\n    - [Chatbot](#chatbot)\n    - [üíØ Other Use Cases](#-other-use-cases)\n    - [Community demos](#community-demos)\n  - [ü§∑‚Äç‚ôÇÔ∏è Why SymbolicAI?](#Ô∏è-why-symbolicai)\n  - [ Tell me some more fun facts!](#-tell-me-some-more-fun-facts)\n  - [üò∂‚Äçüå´Ô∏è How does it work?](#Ô∏è-how-does-it-work)\n    - [üìö Symbolic operations](#-symbolic-operations)\n    - [Ranking objects](#ranking-objects)\n    - [Evaluating Expressions by best effort](#evaluating-expressions-by-best-effort)\n    - [Dynamic casting](#dynamic-casting)\n    - [Fuzzy Comparisons](#fuzzy-comparisons)\n    - [üß† Causal Reasoning](#-causal-reasoning)\n  - [üò∑ Operations](#-operations)\n    - [üß™ Custom Operations](#-custom-operations)\n    - [Few-shot operations](#few-shot-operations)\n  - [Prompt Design](#prompt-design)\n  - [üòë Expressions](#-expressions)\n    - [Sequence expressions](#sequence-expressions)\n    - [Stream expressions](#stream-expressions)\n  - [‚ùå Error Handling](#-error-handling)\n  - [üï∑Ô∏è Interpretability, Testing \\& Debugging](#Ô∏è-interpretability-testing--debugging)\n    - [Unit Testing Models](#unit-testing-models)\n    - [üî•Debugging](#debugging)\n    - [Example: News Summary](#example-news-summary)\n  - [‚ñ∂Ô∏è Play around with our API](#Ô∏è-play-around-with-our-api)\n  - [üìà Interface for Query and Response Inspection](#-interface-for-query-and-response-inspection)\n  - [ü§ñ Engines](#-engines)\n    - [Symbolic Engine](#symbolic-engine)\n    - [Speech Engine](#speech-engine)\n    - [OCR Engine](#ocr-engine)\n    - [Search Engine](#search-engine)\n    - [WebCrawler Engine](#webcrawler-engine)\n    - [Drawing Engine](#drawing-engine)\n    - [File Engine](#file-engine)\n    - [Indexing Engine](#indexing-engine)\n    - [CLIP Engine](#clip-engine)\n    - [Local Neuro-Symbolic Engine](#local-neuro-symbolic-engine)\n    - [Custom Engine](#custom-engine)\n  - [‚ö°Limitations](#limitations)\n  - [ü•† Future Work](#-future-work)\n  - [Conclusion](#conclusion)\n  - [üë• References, Related Work \\& Credits](#-references-related-work--credits)\n    - [Comparison to other frameworks](#comparison-to-other-frameworks)\n    - [Acknowledgements](#acknowledgements)\n    - [Contribution](#contribution)\n    - [üìú Citation](#-citation)\n    - [üìù License](#-license)\n    - [Like this project?](#like-this-project)\n    - [üì´ Contact](#-contact)\n\n\n## üîß Get Started\n\n### ‚û°Ô∏è Quick Install\n\n```bash\npip install symbolicai\n```\n\nOne can run our framework in two ways:\n\n* using local engines (`experimental`) that are run on your local machine ([see Local Neuro-Symbolic Engine section](#local-neuro-symbolic-engine)), or\n* using engines powered by external APIs, i.e. using OpenAI's API ([see API Keys](#api-keys)).\n\n### API Keys\n\nBefore the first run, define exports for the required `API keys` to enable the respective engines. This will register the keys in the internally for subsequent runs. By default `SymbolicAI` currently uses OpenAI's neural engines, i.e. GPT-3 Davinci-003, DALL¬∑E 2 and Embedding Ada-002, for the neuro-symbolic computations, image generation and embeddings computation respectively. However, these modules can easily be replaced with open-source alternatives. Examples are \n- [OPT](https://huggingface.co/docs/transformers/model_doc/opt) or [Bloom](https://huggingface.co/bigscience/bloom) for neuro-symbolic computations, \n- [Craiyon](https://www.craiyon.com/) for image generation, \n- and any [BERT variants](https://huggingface.co/models) for semantic embedding computations. \n\nTo set the OpenAI API Keys use the following command:\n\n```bash\n# Linux / MacOS\nexport OPENAI_API_KEY=\"<OPENAI_API_KEY>\"\n\n# Windows (PowerShell)\n$Env:OPENAI_API_KEY=\"<OPENAI_API_KEY>\"\n\n# Jupyter Notebooks (important: do not use quotes)\n%env OPENAI_API_KEY=<OPENAI_API_KEY>\n```\n\n\n**To get started import our library by using:**\n\n```python\nimport symai as ai\n```\n\nOverall, the following engines are currently supported:\n\n* **Neuro-Symbolic Engine**: [OpenAI's LLMs (GPT-3)](https://beta.openai.com/docs/introduction/overview) \n* **Embedding Engine**: [OpenAI's Embedding API](https://beta.openai.com/docs/introduction/overview)\n* **[Optional] Symbolic Engine**: [WolframAlpha](https://www.wolframalpha.com/)\n* **[Optional] Search Engine**: [SerpApi](https://serpapi.com/)\n* **[Optional] OCR Engine**: [APILayer](https://apilayer.com/ocr)\n* **[Optional] SpeechToText Engine**: [OpenAI's Whisper](https://openai.com/blog/whisper/)\n* **[Optional] WebCrawler Engine**: [Selenium](https://selenium-python.readthedocs.io/)\n* **[Optional] Image Rendering Engine**: [DALL¬∑E 2](https://openai.com/dall-e-2/)\n* **[Optional] Indexing Engine**: [Pinecone](https://app.pinecone.io/)\n* **[Optional] [CLIP](https://openai.com/blog/clip/) Engine**: ü§ó [Hugging Face](https://huggingface.co/) (experimental image and text embeddings)\n\n\n### *[Optional]* Installs\n\nSymbolicAI uses multiple engines to process text, speech and images. We also include search engine access to retrieve information from the web. To use all of them, you will need to install also the following dependencies or assign the API keys to the respective engines. \n\nIf you want to use the `WolframAlpha Engine`, `Search Engine` or `OCR Engine` you will need to export the following API keys:\n\n```bash\n# Linux / MacOS\nexport SYMBOLIC_ENGINE_API_KEY=\"<WOLFRAMALPHA_API_KEY>\"\nexport SEARCH_ENGINE_API_KEY=\"<SERP_API_KEY>\"\nexport OCR_ENGINE_API_KEY=\"<APILAYER_API_KEY>\"\nexport INDEXING_ENGINE_API_KEY=\"<PINECONE_API_KEY>\"\n\n# Windows (PowerShell)\n$Env:SYMBOLIC_ENGINE_API_KEY=\"<WOLFRAMALPHA_API_KEY>\"\n$Env:SEARCH_ENGINE_API_KEY=\"<SERP_API_KEY>\"\n$Env:OCR_ENGINE_API_KEY=\"<APILAYER_API_KEY>\"\n$Env:INDEXING_ENGINE_API_KEY=\"<PINECONE_API_KEY>\"\n```\n\nTo use them, you will also need to install the following dependencies:\n\n\n* **SpeechToText Engine**: `ffmpeg` for audio processing (based on OpenAI's [whisper](https://openai.com/blog/whisper/))\n\n```bash\n# Linux\nsudo apt update && sudo apt install ffmpeg\n\n# MacOS\nbrew install ffmpeg\n\n# Windows\nchoco install ffmpeg\n```\n\n[Note] Additionally, you need to install the newest version directly from their repository, since the version available via `pip` is outdated:\n\n```bash\npip install git+https://github.com/openai/whisper.git\n```\n\n* **WebCrawler Engine**: For `selenium`, download the corresponding driver version by setting the `SELENIUM_CHROME_DRIVER_VERSION` environment variable. Currently we use Chrome as the default browser. This means that the Chrome version major number must match the ChromeDriver version. All versions are available [here](https://chromedriver.chromium.org/downloads). For example, if you use chrome version `109.0.5414.74`, you can set any `109.x.x.x` version for the `chromedriver`. In this case the `109.0.5414.74` is available on the selenium page, therefore the environment variable is set to it:\n\n```bash\n# Linux / MacOS\nexport SELENIUM_CHROME_DRIVER_VERSION=\"109.0.5414.74\"\n\n# Windows (PowerShell)\n$Env:SELENIUM_CHROME_DRIVER_VERSION=\"109.0.5414.74\"\n```\n\n----\n\nAlternatively, you can specify in your project path a `symai.config.json` file with all the engine properties. This will replace the environment variables. See the following configuration file as an example:\n\n```json\n{\n    \"NEUROSYMBOLIC_ENGINE_API_KEY\": \"<OPENAI_API_KEY>\",\n    \"NEUROSYMBOLIC_ENGINE_MODEL\": \"text-davinci-003\",\n    \"SYMBOLIC_ENGINE_API_KEY\": \"<WOLFRAMALPHA_API_KEY>\",\n    \"EMBEDDING_ENGINE_API_KEY\": \"<OPENAI_API_KEY>\",\n    \"EMBEDDING_ENGINE_MODEL\": \"text-embedding-ada-002\",\n    \"IMAGERENDERING_ENGINE_API_KEY\": \"<OPENAI_API_KEY>\",\n    \"VISION_ENGINE_MODEL\": \"openai/clip-vit-base-patch32\",\n    \"SEARCH_ENGINE_API_KEY\": \"<SERP_API_KEY>\",\n    \"SEARCH_ENGINE_MODEL\": \"google\",\n    \"OCR_ENGINE_API_KEY\": \"<APILAYER_API_KEY>\",\n    \"SPEECH_ENGINE_MODEL\": \"base\",\n    \"SELENIUM_CHROME_DRIVER_VERSION\": \"110.0.5481.30\",\n    \"INDEXING_ENGINE_API_KEY\": \"<PINECONE_API_KEY>\",\n    \"INDEXING_ENGINE_ENVIRONMENT\": \"us-west1-gcp\"\n}\n```\n\n## ü¶ñ Apps\n\nOver the course of th next weeks, we will expand our experimental demo apps and provide a set of useful tools that showcase how to interact with our framework. These apps are made available by calling the `sym+<shortcut-name-of-app>` command in your `terminal` or `PowerShell`.\n\n### Shell Command Tool\n\nYou can start a basic shell command support tool that translates natural language commands into shell commands. To start the shell command tool, simply run:\n\n```bash\nsymsh \"<your-query>\"\n```\n\nYou can also use the `--help` flag to get more information about the tool and available arguments.\n\n```bash\nsymsh --help\n```\n\nHere is an example of how to use the tool:\n\n```bash\n$> symsh \"PowerShell edit registiry entry\"\n\n# :Output:\n# Set-ItemProperty -Path <path> -Name <name> -Value <value>\n\n$> symsh \"Set-ItemProperty -Path <path> -Name <name> -Value <value>\" --add \"path='/Users/myuser' name=Demo value=SymbolicAI\"\n\n# :Output:\n# Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI\n\n$> symsh \"Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI\" --del \"string quotes\"\n\n# :Output:\n# Set-ItemProperty -Path /Users/myuser -Name Demo -Value SymbolicAI\n\n$> symsh \"Set-ItemProperty -Path '/Users/myuser' -Name Demo -Value SymbolicAI\" --convert \"linux\"\n\n# :Output:\n# export Demo=\"SymbolicAI\"\n```\n\n### Chatbot\n\nYou can start a basic conversation with `Symbia`. `Symbia` is a chatbot that uses `SymbolicAI` to detect the content of your request and switch between different contextual modes to answer your questions. These mode include search engines, speech engines and more. To start the chatbot, simply run:\n\n```bash\nsymchat\n```\n\nThis will start now a chatbot interface:\n\n```bash\nSymbia: Hi there! I'm Symbia, your virtual assistant. How may I help you?\n$> \n```\n\nYou can exit the conversation by either typing `exit`, `quit` or pressing `Ctrl+C`.\n\n### üíØ Other Use Cases\n\nWe compiled a few examples to show how to use our Symbolic API. You can find them in the `notebooks` folder.\n\n- *Basics*: See our basics notebook to get familiar with our API structure ([notebooks/Basics.ipynb](notebooks/Basics.ipynb))\n- *Queries*: See our query manipulation notebook for contextualized operations ([notebooks/Queries.ipynb](notebooks/Queries.ipynb))\n- *News & Docs Generation*: See our news and documentation generation notebook for stream processing ([notebooks/News.ipynb](notebooks/News.ipynb))\n- *ChatBot*: See how to implement a custom chatbot based on semantic narrations ([notebooks/ChatBot.ipynb](notebooks/ChatBot.ipynb))\n\n\nYou can solve many more problems with our Symbolic API. We are looking forward to see what you will build with it. Keep us posted on our shared community space on [Discord: AI Is All You Need / SymbolicAI](https://discord.gg/QYMNnh9ra8).\n\n\n### Community demos\n\nWe are listing all your cool demos and tools that you build with our framework. If you want to add your project just PM on Twitter at [@SymbolicAPI](https://twitter.com/SymbolicAPI) or via [Discord](https://discord.gg/QYMNnh9ra8).\n\n## ü§∑‚Äç‚ôÇÔ∏è Why SymbolicAI?\n\nSymbolicAI tries to close the gap between classical programming or Software 1.0 and modern data-driven programming (aka Software 2.0). It is a framework that allows to build software applications, which are able to utilize the power of large language models (LLMs) wtih composability and inheritance - two powerful concepts from the object-oriented classical programming paradigm.\n\nThis allows to move along the spectrum between the classical programming realm and data-driven programming realm as illustrated in the following figure:\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img5.png\" width=\"720px\">\n\nAs briefly mentioned, we adopt a divide and conquer approach to decompose a complex problem into smaller problems. We then use the expressiveness and flexibility of LLMs to evaluate these sub-problems and by re-combining these operations we can solve the complex problem. \n\nIn this turn, and with enough data, we can gradually transition between general purpose LLMs with `zero` and `few-shot` learning capabilities, and specialized fine-tuned models to solve specific problems (see above). This means that each operations could be designed to use a model with fine-tuned task-specific behavior. \n\n## <img src=\"https://media.giphy.com/media/mGcNjsfWAjY5AEZNw6/giphy.gif\" width=\"50\"> Tell me some more fun facts!\n\nIn its essence, SymbolicAI was inspired by the [`neuro-symbolic programming paradigm`](https://arxiv.org/abs/2210.05050).\n\n**Neuro-symbolic programming** is a paradigm for artificial intelligence and cognitive computing that combines the strengths of both deep neural networks and symbolic reasoning.\n\n**Deep neural networks** are a type of machine learning algorithms that are inspired by the structure and function of biological neural networks. They are particularly good at tasks such as image recognition, natural language processing etc. However, they are not as good at tasks that require explicit reasoning, such as long-term planning, problem solving, and understanding causal relationships.\n\n**Symbolic reasoning**, on the other hand uses formal languages and logical rules to represent knowledge and perform tasks such as planning, problem solving, and understanding causal relationships. Symbolic reasoning systems are good at tasks that require explicit reasoning, but are not as good at tasks that require pattern recognition or generalization, such as image recognition or natural language processing.\n\n**Neuro-symbolic programming** aims to combine the strengths of both neural networks and symbolic reasoning to create AI systems that can perform a wide range of tasks. One way this is done is by using neural networks to extract information from data and then using symbolic reasoning to make inferences and decisions based on that information. Another way is to use symbolic reasoning to guide the generative process of neural networks and make them more interpretable.\n\n**Embedded accelerators for LLMs** will, in our opinion, be ubiquitous in future computation platforms, such as wearables, smartphones, tablets or notebooks. They will contain models similar to GPT-3, ChatGPT, OPT or Bloom. \n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img1.png\" width=\"720px\">\n\nThese LLMs will be able to perform a wide range of computations, such as natural language understanding or decision making. Furthermore, neuro-symbolic computation engines will be able to learn concepts how to tackle unseen tasks and solve complex problems by querying various data sources for solutions and executing logical statements on top. \nIn this turn, to ensure the generated content is in alignment with our goals, we need to develop ways to instruct, steer and control the generative processes of machine learning models. Therefore, our approach is an attempt to enable active and transparent flow control of these generative processes.\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img7.png\" width=\"720px\">\n\nAs shown in the figure above, one can think of this generative process as shifting a probability mass of an input stream of data towards an output stream of data, in a contextualized manner. With properly designed conditions and expressions, one can also validate and steer the behavior towards a desired outcome, or repeat expressions that failed to fulfil our requirements. Our approach is to define a set of `fuzzy` operations that manipulate the data stream and conditions the LLMs to align with our goals. In essence, we consider all data objects, such as strings, letters, integers, arrays, etc. as symbols and we see natural language as the main interface to interact with. See the following figure:\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img10.png\" width=\"720px\">\n\nWe show that as long as we can express our goals in natural language, we can use the power of LLMs for neuro-symbolic computations.\nIn this turn, we create operations that manipulate these symbols to generate new symbols from them. Each symbol can be interpreted as a statement. Multiple statements can be combined to form a logical expression.\n\nTherefore, by chaining statements together we can build causal relationships and computations, instead of relying only on inductive approaches. Consequently, the outlook towards an updated computational stack resembles a neuro-symbolic computation engine at its core and, in combination with established frameworks, enables new applications. \n\n\n## üò∂‚Äçüå´Ô∏è How does it work?\n\nWe now show how we define our `Symbolic API`, which is based on object-oriented and compositional design patterns. The `Symbol` class is the base class for all functional operations, which we refer to as a terminal symbol in the context of symbolic programming (fully resolved expressions). The Symbol class holds helpful operations that can be interpreted as expressions to manipulate its content and evaluate to new Symbols. \n\n### üìö Symbolic operations\n\nLet us now define a Symbol and perform some basic manipulations. We start with a translation operation:\n\n```python\nsym = ai.Symbol(\"Welcome to our tutorial.\")\nsym.translate('German')\n```\n```bash\n:[Output]: \n<class 'symai.expressions.Symbol'>(value=Willkommen zu unserem Tutorial.)\n```\n\n### Ranking objects\n\nOur API can also perform basic data-agnostic operations to `filter`, `rank` or `extract` patterns. For example, we can rank a list of numbers:\n\n```python\nsym = ai.Symbol(numpy.array([1, 2, 3, 4, 5, 6, 7]))\nres = sym.rank(measure='numerical', order='descending')\n```\n```bash\n:[Output]: \n<class 'symai.expressions.Symbol'>(value=['7', '6', '5', '4', '3', '2', '1'])\n```\n\n### Evaluating Expressions by best effort\n\nAs an inspiration, we relate to an approach demonstrated by [word2vec](https://arxiv.org/abs/1301.3781). \n\n**Word2Vec** generates dense vector representations of words by training a shallow neural network to predict a word given its neighbors in a text corpus. The resulting vectors are then used in a wide range of natural language processing applications, such as sentiment analysis, text classification, and clustering.\n\nBelow we can see an example how one can perform operations on the word embeddings (colored boxes).\nThe words are tokenized and mapped to a vector space, where we can perform semantic operations via vector arithmetics. \n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img3.png\" width=\"450px\">\n\nSimilar to word2vec we intend to perform contextualized operations on different symbols, however, instead of operating in the vector space, we operate in the natural language domain. This gives us the ability to perform arithmetics on words, sentences, paragraphs, etc. and verify the results in a human readable format. \n\nThe following examples show how to evaluate such an expression via a string representation:\n\n```python\nSymbol('King - Man + Women').expression()\n```\n```bash\n:[Output]:\n<class 'symai.expressions.Symbol'>(value=Queen)\n```\n\n### Dynamic casting\n\nWe can also subtract sentences from each other, where our operations condition the neural computation engine to evaluate the Symbols by best effort. In the following example, it determines that the word `enemy` is present in the sentence, therefore deletes it and replaces it with the word `friend` (which is added):\n\n```python\nres = ai.Symbol('Hello my enemy') - 'enemy' + 'friend'\n```\n```bash\n:[Output]: \n<class 'symai.expressions.Symbol'>(value=Hello my friend)\n```\n\nWhat we also see is that the API performs dynamic casting, when data types are combined with a Symbol object. If an overloaded operation of the Symbol class is used, the Symbol class can automatically cast the second object to a Symbol. This is a convenient modality to perform operations between `Symbol`objects and other types of data, such as strings, integers, floats, lists, etc. without bloating the syntax.\n\n### Fuzzy Comparisons\n\nIn this example we are fuzzily comparing two number objects, where the Symbol variant is only an approximation of `numpy.pi`. Given the context of the fuzzy equals `==` operation, this comparison still succeeds and returns `True`. \n\n```python\nsym = ai.Symbol('3.1415...')\nsym == numpy.pi\n```\n```bash\n:[Output]:\nTrue\n```\n\n### üß† Causal Reasoning\n\nOur framework was built with the intention to enable reasoning capabilities on top of statistical inference of LLMs. Therefore, we can also perform deductive reasoning operations with our Symbol objects. For example, we can define a set of operations with rules that define the causal relationship between two symbols. The following example shows how the `&` is used to compute the logical implication of two symbols. \n\n```python\nres = Symbol('The horn only sounds on Sundays.') & Symbol('I hear the horn.')\n```\n```bash\n:[Output]:\n<class 'symai.expressions.Symbol'>(value=It is Sunday.)\n```\n\nThe current `&`-operation overloads the `and` logical operator and sends `few-shot` prompts how to evaluate the statement to the neural computation engine. However, we can define more sophisticated logical operators for `and`, `or` and `xor` via formal proof statements and use the neural engines to parse data structures prior to our expression evaluation. Therefore, one can also define custom operations to perform more complex and robust logical operations, including constraints to validate the outcomes and ensure a desired behavior. \n\nTo provide a more complete picture, we also sketch more comprehensive causal examples below, where one tries to obtain logical answers, based on questions of the kind:\n\n```python\n# 1) \"A line parallel to y = 4x + 6 passes through (5, 10). What is the y-coordinate of the point where this line crosses the y-axis?\"\n# 2) \"Bob has two sons, John and Jay. Jay has one brother and father. The father has two sons. Jay's brother has a brother and a father. Who is Jay's brother.\"\n# 3) \"is 1000 bigger than 1063.472?\"\n```\nTo give an rough idea of how we would approach this with our framework is by, first, using a chain of operations to detect the neural engine that is best suited to handle this task, and second, prepare the input for the respective engine. Let's see an example:\n\n```python\nval = \"<one of the examples above>\"\n\n# First define a class that inherits from the Expression class\nclass ComplexExpression(Expression): # more to the Expression class in later sections\n    # write a method that returns the causal evaluation\n    def causal_expression(self):\n        pass # see below for implementation\n\n# instantiate an object of the class\nexpr = ComplexExpression(val)\n# set WolframAlpha as the main expression engine to use\nexpr.command(engines=['symbolic'], expression_engine='wolframalpha')\n# evaluate the expression\nres = expr.causal_expression()\n```\n\nNow, the implementation of `causal_expression` could in principle look like this:\n\n```python\ndef causal_expression(self):\n    # very which case to use `self.value` contains the input\n    if self.isinstanceof('mathematics'):\n        # get the mathematical formula\n        formula = self.extract('mathematical formula')\n        # verify which problem type we have\n        if formula.isinstanceof('linear function'):\n            # prepare for wolframalpha\n            question = self.extract('question sentence')\n            req = question.extract('what is requested?')\n            x = self.extract('coordinate point (.,.)') # get coordinate point / could also ask for other points\n            query = formula @ f', point x = {x}' @ f', solve {req}' # concatenate to the question and formula\n            res = query.expression(query) # send prepared query to wolframalpha\n\n        elif formula.isinstanceof('number comparison'):\n            res = formula.expression() # send directly to wolframalpha\n\n        ... # more cases\n\n    elif self.isinstanceof('linguistic problem'):\n        sentences = self / '.' # first split into sentences\n        graph = {} # define graph\n        for s in sentences:\n            sym = Symbol(s)\n            relations = sym.extract('connected entities (e.g. A has three B => A | A: three B)') / '|' # and split by pipe\n            for r in relations:\n                ... # add relations and populate graph => alternatively, read also about CycleGT\n\n    ... # more cases\n    return res\n```\n\nThe above example shows how we can use the `causal_expression` expression method to step-wise iterate and extract information which we can then either manually or using external solvers resolve. \n\n**Attention:** We hint the reader that this is a very rough sketch and that the implementation of the `causal_expression` method would need much more engineering effort. Furthermore, the currently used GPT-3 LLM backend often fails to extract the correct information or resolve the right comparison. However, we strongly believe in the advances of the field and that this will change in the future, specifically with fine-tuned models like ChatGPT with Reinforcement Learning from Human Feedback (RLHF).\n\nLastly, it is also noteworthy that given enough data, we could fine-tune methods that extract information or build our knowledge graph from natural language. This would enable us to perform more complex reasoning tasks, such as the ones mentioned above. Therefore, we also point the reader to recent publications for translating [Text-to-Graphs](https://aclanthology.org/2020.webnlg-1.8.pdf). This means that in the attempt to answer the query, we can simply traverse the graph and extract the information we need.\n\n\nIn the next section, we will explore operations.\n\n## üò∑ Operations\n\nOperations are at the core of our framework. They are the building blocks of our API and are used to define the behavior of our symbols. We can think of operations as contextualized functions that take in a `Symbol` object, send it to the neuro-symbolic engine for evaluation, and return one or multiple new objects (mainly new symbols; but not necessarily limited to that). Another fundamental property is polymorphism, which means that operations can be applied to different types of data, such as strings, integers, floats, lists, etc. with different behaviors, depending on the object instance. \n\nThe way we execute operations is by using the `Symbol` object `value` attribute containing the original data type that is then sent as a string representations to the engines to perform the operations. Therefore all values are casted to a string representation. This also means, that for custom objects one needs to define a proper `__str__` method to cast the object to a string representation and ensure preservation of the semantics of that object. \n\nLastly, we need to talk about inheritance. Our API is built on top of the `Symbol` class, which is the base class of all operations. This means that all operations are inherited from the `Symbol` class. This provides a convenient modality to add new custom operations by sub-classing `Symbol`, yet, ensuring to always have a set of base operations at our disposal without bloating the syntax or re-implementing many existing functionalities. This also means that we can define contextualized operations with individual constraints, prompt designs and therefore behaviors by simply sub-classing the `Symbol` class and overriding the corresponding method. However, we recommend sub-classing the `Expression` class as we will see later, it adds additional functionalities.\n\nHere is an example of how to define a custom `==` operation by overriding the `__eq__` method and providing a custom prompt object with a list of examples:\n\n```python\nclass Demo(ai.Symbol):\n    def __eq__(self, other) -> bool:\n        @ai.equals(examples=ai.Prompt([\n              \"1 == 'ONE' =>True\",\n              \"'six' == 7 =>False\",\n              \"'Acht' == 'eight' =>True\",\n              ...\n          ])\n        )\n        def _func(_, other) -> bool:\n            return False # default behavior on failure\n        return _func(self, other)\n```\n\nAs shown in the above example, this is also the way we implemented the basic operations in `Symbol`, by defining local functions that are then decorated with the respective operation decorator from the `symai/core.py` file. The `symai/core.py` is a collection of pre-defined operation decorators that we can quickly apply to any function. The reason why we use locally defined functions instead of directly decorating the main methods, is that we do not necessarily want that all our operations are sent to the neural engine and could implement a default behavior. Another reason is that we want to cast return types of the operation outcome to symbols or other derived classes thereof. This is done by using the `self._sym_return_type(...)` method and can give contextualized behavior based on the defined return type. See more details in the actual [`Symbol` class](https://github.com/Xpitfire/symbolicai/blob/main/symai/symbol.py).\n\nIn the next section, we will show that almost all operations in `symai/core.py` are derived from the more generic `few_shot` decorator.\n\n\n### üß™ Custom Operations\n\nOne can also define customized operations. For example, let us define a custom operation to generate a random integer between 0 and 10:\n\n```python\nclass Demo(ai.Symbol):\n    def __init__(self, value = '') -> None:\n        super().__init__(value)\n    \n    @ai.zero_shot(prompt=\"Generate a random integer between 0 and 10.\",\n                  constraints=[\n                      lambda x: x >= 0,\n                      lambda x: x <= 10\n                  ])\n    def get_random_int(self) -> int:\n        pass\n```\n\nAs we show, the Symbolic API uses Python `Decorators` to define operations. The `@ai.zero_shot` decorator is used to define a custom operation that does not require any demonstration examples, since the prompt is expressive enough. In the shown example, the `zero_shot` decorator takes in two arguments: `prompt` and `constraints`. The former is used to define the prompt that conditions our desired operation behavior. The latter is used to define validation constraints of the computed outcome, to ensure it fulfills our expectations.\n\nIf the constraint is not fulfilled, the above implementation would reach out to the specified `default` implementation or default value. If no default implementation or value was found, the Symbolic API would raise an `ConstraintViolationException`.\n\nWe also see that in the above example the return type is defined as `int`. Therefore, the resulting value from the wrapped function will be of type int. This works because our implementation uses auto-casting to a user specified return data type. If the cast fails, the Symbolic API will raise a `ValueError`. If no return type is specified, the return type will be `Any`.\n\n### Few-shot operations\n\nThe `@ai.few_shot` decorator is the a generalized version of `@ai.zero_shot` and is used to define a custom operation that requires demonstration examples. To give a more complete picture, we present the function signature of the `few_shot` decorator:\n\n```python\ndef few_shot(prompt: str,\n             examples: Prompt, \n             constraints: List[Callable] = [],\n             default: Optional[object] = None, \n             limit: int = 1,\n             pre_processor: Optional[List[PreProcessor]] = None,\n             post_processor: Optional[List[PostProcessor]] = None,\n             **wrp_kwargs):\n```\n\nThe `prompt` and `constraints` attributes behavior is similar to the `zero_shot` decorator. The `examples` and `limit` arguments are new. The `examples` argument is used to define a list of demonstrations that are used to condition the neural computation engine. The `limit` argument is used to define the maximum number of examples that are returned, give that there are more results. The `pre_processor` argument takes a list of `PreProcessor` objects which can be used to pre-process the input before it is fed into the neural computation engine. The `post_processor` argument takes a list of `PostProcessor` objects which can be used to post-process the output before it is returned to the user. The `wrp_kwargs` argument is used to pass additional arguments to the wrapped method, which are also stream-lined towards the neural computation engine and other engines.\n\nTo give a more holistic picture ouf our conceptional implementation, see the following flow diagram containing the most important classes:\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img9.png\" width=\"600px\">\n\nThe colors indicate logical groups of data processing steps. `Yellow` indicates the input and output data. `Blue` indicates places you can customize or prepare the input of your engine. `Green` indicates post-processing steps of the engine response. `Red` indicates the application of constraints (which also includes the attempted casting of the `return type signature`, if specified in the decorated method). `Grey` indicates the custom method which defines all properties, therefore has access to all the above mentioned objects.\n\nTo conclude this section, here is an example how to write a custom Japanese name generator with our `@ai.zero_shot` decorator:\n\n```python\nimport symai as ai\nclass Demo(ai.Symbol):\n    @ai.few_shot(prompt=\"Generate Japanese names: \",\n                 examples=ai.Prompt(\n                   [\"ÊÑõÂ≠ê\", \"ÂíåËä±\", \"‰∏ÄÈÉé\", \"ÂíåÊûù\"]\n                 ),\n                 limit=2,\n                 constraints=[lambda x: len(x) > 1])\n    def generate_japanese_names(self) -> list:\n        return ['ÊÑõÂ≠ê', 'ÂíåËä±'] # dummy implementation\n```\n\nShould the neural computation engine not be able to compute the desired outcome, it will reach out to the `default` implementation or default value. If no default implementation or value was found, the method call will raise an exception.\n\n\n## Prompt Design\n\nThe way all the above operations are performed is by using a `Prompt` class. The Prompt class is a container for all the information that is needed to define a specific operation. The Prompt class is also the base class for all other Prompt classes. \n\nHere is an example how to define a Prompt to enforce the neural computation engine for comparing two values:\n\n```python\nclass CompareValues(ai.Prompt):\n    def __init__(self) -> ai.Prompt:\n        super().__init__([\n            \"4 > 88 =>False\",\n            \"-inf < 0 =>True\",\n            \"inf > 0 =>True\",\n            \"4 > 3 =>True\",\n            \"1 < 'four' =>True\",\n            ...\n        ])\n```\n\nFor example, when calling the `<=` operation on two Symbols, the neural computation engine will evaluate the symbols in the context of the `CompareValues` prompt.\n\n```python\nres = ai.Symbol(1) <= ai.Symbol('one')\n```\n\nThis statement evaluates to `True`, since the fuzzy compare operation was conditions our engine to compare the two Symbols based on their semantic meaning.\n\n```bash\n:[Output]:\nTrue\n```\n\nIn a more general notion, depending on the context hierarchy of the expression class and used operations the semantics of the Symbol operations may vary. To better illustrate this, we show our conceptual prompt design in the following figure:\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img4.png\" width=\"350px\">\n\nThe figure shows our hierarchical prompt design as a container of all the information that is provided to the neural computation engine to define a task-specific operation. The `Yellow` and `Green` highlighted boxes indicate mandatory string placements. The dashed boxes are optional placeholders. and the `Red` box indicates the starting point of the model prediction.\n\nConceptually we consider three main prompt designs: `Context-based Prompts`, `Operational Prompts`, and `Templates`. The prompts can be curated either by inheritance or by composition. For example, the `Static Context` can be defined by inheriting from the `Expression` class and overriding the `static_context` property. An `Operation` and `Template` prompt can be created by providing an `PreProcessor` to modify the input data. \n\nWe will now explain each prompt concept in more details:\n\n- The `Context-based Prompts (Static, Dynamic and Attachment)` are considered optional and can be defined in a static manner, either by sub-classing the Expression class and overriding the `static_context` property, or at runtime by updating the `dynamic_context` property or passing an `attach` kwargs to a method. Here is an example how to use the `attach` kwargs via the method signature:\n  ```python\n  # creating a query to ask if an issue was resolve or not\n  sym = Symbol(\"<some-community-conversation>\")\n  q = sym.query(\"Was the issue resolved?\")\n  # write manual condition to check if the issue was resolved\n  if 'not resolved' in q:\n      # do a new query but attach the previous query answer to the new query\n      sym.query(\"What was the resolution?\", attach=q)\n      ...\n  else:\n      pass # all good\n  ```\n  Regardless of how we set the context, our contextualized prompt defines the desired behavior of the Expression operations. For example, if we want to operate in the context of a domain-specific language, without having to override each base class method. See more details in [this notebook](notebooks/Queries.ipynb).\n\n- The `Operation` prompts define the behavior of an atomic operation and is therefore mandatory to express the nature of such an operation. For example, the `+`-operation is used to add two Symbols together and therefore the `+`-operation prompt explains its behavior. `Examples` defines another optional structure that provides the neural computation engine with a set of demonstrations that are used to properly condition the engine. For example, the `+`-operation prompt can be conditioned on how to add numbers by providing a set of demonstrations, such as `1 + 1 = 2`, `2 + 2 = 4`, etc.\n\n- The `Template` prompts are optional and encapsulates the resulting prediction to enforce a specific format. For example, to generate HTML tags we can use a curated `<html>{{placeholder}}</html>` template. This template will enforce the neural computation engine to start the generation process already in the context of a HTML tags format, and not produce irrelevant descriptions about its task.\n\n\n## üòë Expressions\n\nAn `Expression` is a non-terminal symbol, which can be further evaluated. It inherits all the properties from Symbol and overrides the `__call__` method to evaluate its expressions or values. From the `Expression` class, all other expressions are derived. The Expression class also adds additional capabilities i.e. to `fetch` data from URLs, `search` on the internet or `open` files. These operations are specifically separated from `Symbol` since they do not use the `value` attribute of the Symbol class. \n\nSymbolicAI' API closely follows best practices and ideas from `PyTorch`, therefore, one can build complex expressions by combining multiple expressions as a computational graph. Each Expression has its own `forward` method, which has to be overridden. The `forward` method is used to define the behavior of the expression. The `forward` method is called by the `__call__` method, which is inherited from the `Expression` base class. The `__call__` evaluates an expression and returns the result from the implemented `forward` method. This design pattern is used to evaluate the expressions in a lazy manner, which means that the expression is only evaluated when the result is needed. This is a very important feature, since it allows us to chain complex expressions together. We already implemented many useful expressions, which can be imported from the `symai.components` file.\n\nOther important properties that are inherited from the Symbol class are `_sym_return_type` and `static_context`. These two properties define the context in which the current Expression operates, as described in the [Prompt Design](#prompt-design) section. The static_context therefore influences all operations of the current Expression sub-class. The _sym_return_type ensures that after each evaluation of an Expression, we obtain the desired return object type. This is usually implemented to return the current type, but can be set to return a different type. \n\nExpressions can of course have more complex structures and be further sub-classed, such as shown in the example of the `Sequence` expression in the following figure:\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img2.png\" width=\"720px\">\n\nA Sequence expression can hold multiple expressions, which are evaluated at runtime.\n\n### Sequence expressions\n\nHere is an example how to define a Sequence expression:\n\n```python\n# first import all expressions\nfrom symai import *\n# define a sequence of expressions\nSequence(\n    Clean(),\n    Translate(),\n    Outline(),\n    Compose('Compose news:'),\n)\n```\n\n### Stream expressions\n\nAs we saw earlier, we can create contextualized prompts to define the behavior of operations on our neural engine. However, this also takes away a lot of the available context size and since e.g. the GPT-3 Davinci context length is limited to 4097 tokens, this might quickly become a problem. Luckily, we can use the `Stream` processing expression. This expression opens up a data stream and performs chunk-based operations on the input stream. \n\nA Stream expression can easily be wrapped around other expressions. For example, the chunks can be processed with a `Sequence` expression, that allows multiple chained operations in sequential manner. Here is an example how to define such a Stream expression:\n\n```python\nStream(Sequence(\n    Clean(),\n    Translate(),\n    Outline(),\n    Embed()\n))\n```\nThe shown example opens a stream, passes a `Sequence` object which cleans, translates, outlines and embeds the input. \nInternally, the stream operation estimates the available model context size and chunks the long input text into smaller chunks, which are passed to the inner expression. The returned object type is a `generator`. \n\nThe issue with this approach is, that the resulting chunks are processed independently. This means there is no shared context or information among chunks. To solve this issue, we can use the `Cluster` expression instead, where the independent chunks are merged based on their similarity. We illustrate this in the following figure:\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img6.png\" width=\"720px\">\n\nIn the shown example all individual chunks are merged by clustering the information within each chunk. This gives us a way to consolidate contextually related information and merge them in a meaningful way. Furthermore, the clustered information can then be labeled by streaming through the content of each cluster and extracting the most relevant labels, providing us with interpretable node summaries.\n\nThe full example is shown below:\n\n```python\nstream = Stream(Sequence(\n    Clean(),\n    Translate(),\n    Outline(),\n))\nsym = Symbol('<some long text>')\nres = Symbol(list(stream(sym)))\nexpr = Cluster()\nexpr(res)\n```\n\nIn a next step, we could recursively repeat this process on each summary node, therefore, build a hierarchical clustering structure. Since each Node resembles a summarized sub-set of the original information we can use the summary as an index. The resulting tree can then be used to navigate and retrieve the original information, turning the large data stream problem into a search problem.\n\nAlternatively, we could use vector-base similarity search to find similar nodes.\nFor searching in a vector space we can use dedicated libraries such as [Annoy](https://github.com/spotify/annoy), [Faiss](https://github.com/facebookresearch/faiss) or [Milvus](https://github.com/milvus-io/milvus). \n\n## ‚ùå Error Handling\n\nA key idea of the SymbolicAI API is to be able to generate code. This in turn means that errors may occur, which we need to handle in a contextual manner. As a future vision, we even want our API to self extend and therefore need to be able to resolve issues automatically. To do so, we propose the `Try` expression, which has a fallback statements built-in and retries an execution with dedicated error analysis and correction. This expression analyses the input and the error, and conditions itself to resolve the error by manipulating the original code. If the fallback expression succeeds, the result is returned. Otherwise, this process is repeated for the number of `retries` specified. If the maximum number of retries is reached and the problem was not resolved, the error is raised again. \n\nLet us assume, we have some executable code that was previously generated. However, by the nature of generative processes syntax errors may occur. By using the `Execute` expression, we can evaluate our generated code, which takes in a symbol and tries to execute it. Naturally, this will fail. However, in the following example the `Try` expression resolves this syntactic error and the receive a computed result.\n\n```python\nexpr = Try(expr=Execute())\nsym = Symbol('a = int(\"3,\")') # some code with a syntax error\nres = expr(sym)\n```\n\nThe resulting output is the evaluated code, which was corrected:\n\n```bash\n:Output:\na = 3\n```\n\nWe are aware that not all errors are as simple as the shown syntactic error example, which can be resolved automatically. Many errors occur due to semantic misconceptions. Such issues require contextual information. Therefore, we are further exploring means towards more sophisticated error handling mechanism.\nThis includes also the usage of streams and clustering to resolve errors in a more hierarchical contextual manner. It is also noteworthy that neural computations engines need to be further improved to better detect and resolve errors.\n\n\n## üï∑Ô∏è Interpretability, Testing & Debugging\n\nPerhaps one of the greatest benefits of using neuro-symbolic programming is, that we can get a clear understanding of how well our LLMs understand simple operations. Specifically we gain knowledge about if, and at which point they fail, enabling us to follow their StackTraces and determine the failure points. In our case, neuro-symbolic programming allows us to debug the model predictions based on dedicated unit test for simple operations. To detect conceptual misalignments we can also use a chain of neuro-symbolic operations and validate the generative process. This is of course not a perfect solution, since the verification may also be error prone, but it gives us at least a principle way to detect conceptual flaws and biases in our LLMs.\n\n### Unit Testing Models\n\nSince our premise is to divide and conquer complex problems, we can curate conceptual unit test and target very specific and tracktable sub-problems. The resulting measure, i.e. success rate of the model prediction, can then be used to evaluate their performance, and hint towards undesired flaws or biases.\n\nThis allows us to design domain-specific benchmarks and see how well general learners, such as GPT-3, adapt with certain prompts to a set of tasks. \n\nFor example, we can write a fuzzy comparison operation, that can take in digits and strings alike, and perform a semantic comparison. LLMs can then be asked to evaluate these expressions. Often times, these LLMs still fail to understand the semantic equivalence of tokens in digits vs strings and give wrong answers. \n\nThe following code snipped shows a unit test to perform semantic comparison of numbers (between digits and strings):\n\n```python\nimport unittest\nfrom symai import *\n\nclass TestComposition(unittest.TestCase):\n  def test_compare(self):\n      res = Symbol(10) > Symbol(5)\n      self.assertTrue(res)\n      res = Symbol(1) < Symbol('five')\n      self.assertTrue(res)\n      ...\n```\n\n### üî•Debugging\n\nWhen creating very complex expressions, we debug them by using the `Trace` expression, which allows to print out the used expressions, and follow the StackTrace of the neuro-symbolic operations. Combined with the `Log` expression, which creates a dump of all prompts and results to a log file, we can analyze where our models potentially failed.\n\n\n### Example: News Summary\n\nIn the following example we create a news summary expression that crawls the given URL and streams the site content through multiple expressions. The outcome is a news website that is created based on the crawled content. The `Trace` expression allows to follow the StackTrace of the operations and see what operations are currently executed. If we open the `outputs/engine.log` file we can see the dumped traces with all the prompts and results.\n\n```python\n# crawling the website and creating an own website based on its facts\nnews = News(url='https://www.cnbc.com/cybersecurity/',\n            pattern='cnbc',\n            filters=ExcludeFilter('sentences about subscriptions, licensing, newsletter'),\n            render=True)\nexpr = Log(Trace(news))\nres = expr()\n```\n\nHere is the corresponding StackTrace of the model:\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/assets/images/img8.png\" width=\"900px\">\n\nThe above code creates a webpage with the crawled content from the original source. See the preview below, the entire [rendered webpage image here](https://raw.githubusercontent.com/Xpitfire/symbolicai/main/examples/results/news.png) and resulting [code of webpage here](https://raw.githubusercontent.com/Xpitfire/symbolicai/main/examples/results/news.html. \n\n\n<img src=\"https://raw.githubusercontent.com/Xpitfire/symbolicai/main/examples/results/news_prev.png\" width=\"900px\">\n\n## ‚ñ∂Ô∏è Play around with our API\n\nLaunch and explore the notebook here:\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/Xpitfire/symbolicai/HEAD)\n\nThere are many more examples in the [examples folder](examples/) and in the [notebooks folder](notebooks/). You can also explore the test cases in the [tests folder](tests/).\n\n## üìà Interface for Query and Response Inspection\n\nSymbolicAI is by design a data-driven framework. This means that we can collect data from API interactions while we provide the requested responses. For very agile, dynamic adaptations or prototyping we can integrate user desired behavior quickly into existing prompts. However, we can also log the user queries and model predictions to make them available for post-processing. Therefore, we can customize and improve the model's responses based on real-world data.\n\nIn the following example, we show how we can use an `Output` expression to pass a handler function and access input prompts of the model and model predictions. These, can be used for data collection and later fine-tuning stages. The handler function provides a dictionary and offers keys for `input` and `output` values. The content can then be sent to a data pipeline for further processing.\n\n```python\nsym = Symbol('Hello World!')\ndef handler(res):\n    input_ = res['input']\n    output = res['output']\nexpr = Output(expr=sym.translate, \n              handler=handler, \n              verbose=True)\nres = expr('German')\n```\n\nSince we called verbose, we can also see the console print of the `Output` expression:\n\n```bash\nInput: (['Translate the following text into German:\\n\\nHello World!'],)\nExpression: <bound method Symbol.translate of <class 'symai.symbol.Symbol'>(value=Hello World!)>\nargs: ('German',) kwargs: {'input_handler': <function OutputEngine.forward.<locals>.input_handler at ...\nDictionary: {'wrp_self': <class 'symai.components.Output'>(value=None), 'func': <function Symbol.output.<locals>._func at ...\nOutput: Hallo Welt!\n```\n\n\n## ü§ñ Engines\n\nDue to limited compute resources we currently rely on OpenAI's GPT-3 API for the neuro-symbolic engine. However, given the right compute resources, it is possible to use local machines to avoid high latencies and costs, with alternative engines such as OPT or Bloom. This would allow for recursive executions, loops, and more complex expressions.\n\n\nFurthermore, as we interpret all objects as symbols only with a different encodings, we integrated a set of useful engines that transform these objects to the natural language domain to perform our operations.\n\n### Symbolic Engine\n\nAlthough in our work, we mainly focus on how LLMs can evaluate symbolic expressions, many formal statements were already well implemented in existing symbolic engines, like WolframAlpha. Therefore, given an API KEY from WolframAlpha, we can use their engine by setting the `expression_engine` attribute. This avoids error prune evaluations from neuro-symbolic engines for mathematical operations. The following example shows how to use WolframAlpha to compute the result of the variable `x`:\n\n```python\nexpr = Expression()\nexpr.command(engines=['symbolic'], expression_engine='wolframalpha')\nres = expr.expression('x^2 + 2x + 1')\n```\n\n```bash\n:Output:\nx = -1\n```\n\n### Speech Engine\n\nTo interpret audio files we can perform speech transcription by using `whisper`. The following example shows how to transcribe an audio file and return the text:\n\n```python\nexpr = Expression()\nres = expr.speech('examples/audio.mp3')\n```\n\n```bash\n:Output:\nI may have overslept.\n```\n\n### OCR Engine\n\nTo \"read\" text from images we can perform optical character recognition (OCR) with `APILayer`. The following example shows how to transcribe an image and return the text:\n\n```python\nexpr = Expression()\nres = expr.ocr('https://media-cdn.tripadvisor.com/media/photo-p/0f/da/22/3a/rechnung.jpg')\n```\n\nThe OCR engine returns a dictionary with a key `all_text` where the full text is stored. See more details in their documentation [here](https://apilayer.com/marketplace/image_to_text-api).\n\n```bash\n:Output:\nChina Restaurant\\nMaixim,s\\nSegeberger Chaussee 273\\n22851 Norderstedt\\nTelefon 040/529 16 2 ...\n```\n\n\n### Search Engine\n\nTo obtain fact-based content we perform search queries via `SerpApi` with a `Google` backend. The following example shows how to search for a query and return the results:\n\n```python\nexpr = Expression()\nres = expr.search('Birthday of Barack Obama')\n```\n\n```bash\n:Output:\nAugust 4, 1961\n```\n\n### WebCrawler Engine\n\nTo access any data source from the web, we can use `Selenium`. The following example shows how to crawl a website and return the results:\n\n```python\nexpr = Expression()\nres = expr.fetch(url=\"https://www.google.com/\", \n                 pattern=\"google\")\n```\nThe `pattern` property can be used to detect if the document as been loaded correctly. If the pattern is not found, the crawler will timeout and return an empty result.\n\n```bash\n:Output:\nGoogleKlicke hier, wenn du nach einigen Sekunden nicht automatisch weitergeleitet wirst.GmailBilderAnmelden ...\n```\n\n### Drawing Engine\n\nTo render nice images from text description we use `DALL¬∑E 2`. The following example shows how to draw a text description and return the image:\n\n```python\nexpr = Expression('a cat with a hat')\nres = expr.draw()\n```\n\n```bash\n:Output:\nhttps://oaidalleapiprodscus.blob.core.windows.net/private/org-l6FsXDfth6Uct ...\n```\n\nDon't worry, we would never hide an image of a cat with a hat from you. Here is the image preview and [link](https://camo.githubusercontent.com/4f607176e782700befd732212c198b12c3923bf9c25f548aa444c92f6bcb97d9/68747470733a2f2f6f616964616c6c6561706970726f64736375732e626c6f622e636f72652e77696e646f77732e6e65742f707269766174652f6f72672d6c36467358446674683655637479777441504e746248364b2f757365722d76726c58594933793375484c6557374f6b594a64374b32632f696d672d7530523372394b515130736f716e7830774c7361335368532e706e673f73743d323032332d30312d3133543139253341313625334130305a2673653d323032332d30312d3133543231253341313625334130305a2673703d722673763d323032312d30382d30362673723d6226727363643d696e6c696e6526727363743d696d6167652f706e6726736b6f69643d36616161646564652d346662332d343639382d613866362d36383464373738366230363726736b7469643d61343863636135362d653664612d343834652d613831342d39633834393635326263623326736b743d323032332d30312d3133543138253341313925334133365a26736b653d323032332d30312d3134543138253341313925334133365a26736b733d6226736b763d323032312d30382d3036267369673d466c3133556f51694c646a6e42716e59473674746e6e455666716247546975596b2f7067706170385625324259253344):\n\n<img src=\"https://camo.githubusercontent.com/4f607176e782700befd732212c198b12c3923bf9c25f548aa444c92f6bcb97d9/68747470733a2f2f6f616964616c6c6561706970726f64736375732e626c6f622e636f72652e77696e646f77732e6e65742f707269766174652f6f72672d6c36467358446674683655637479777441504e746248364b2f757365722d76726c58594933793375484c6557374f6b594a64374b32632f696d672d7530523372394b515130736f716e7830774c7361335368532e706e673f73743d323032332d30312d3133543139253341313625334130305a2673653d323032332d30312d3133543231253341313625334130305a2673703d722673763d323032312d30382d30362673723d6226727363643d696e6c696e6526727363743d696d6167652f706e6726736b6f69643d36616161646564652d346662332d343639382d613866362d36383464373738366230363726736b7469643d61343863636135362d653664612d343834652d613831342d39633834393635326263623326736b743d323032332d30312d3133543138253341313925334133365a26736b653d323032332d30312d3134543138253341313925334133365a26736b733d6226736b763d323032312d30382d3036267369673d466c3133556f51694c646a6e42716e59473674746e6e455666716247546975596b2f7067706170385625324259253344\" width=\"200px\">\n\n\n### File Engine\n\nTo perform file operations we use the file system of the OS. At the moment, we support only PDF files and plain text files. This is a very early stage and we are working on more sophisticated file system access and also remote storage. The following example shows how to read a PDF file and return the text:\n\n```python\nexpr = Expression()\nres = expr.open('./LICENSE')\n```\n\n```bash\n:Output:\nBSD 3-Clause License\\n\\nCopyright (c) 2023 ...\n```\n\n### Indexing Engine\nWe use `Pinecone` to index and search for text. The following example shows how to store text as an index and then retrieve the most related match of it:\n\n```python\nexpr = Expression()\nexpr.add(Expression('Hello World!').zip())\nexpr.add(Expression('I like cookies!').zip())\nres = expr.get(Expression('hello').embed().value).ast()\nres['matches'][0]['id']\n```\n\n```bash\n:Output:\nHello World\n```\n\nHere the `zip` method creates a pair of strings and embedding vectors. Afterwards they are added to the index. The line with `get` basically retrieves the original source based on the vector value of `hello` and uses `ast` to cast the value to a dictionary.\n\nOne can set several kwargs for the indexing engine. See the `symai/backend/engine_index.py` file for more details.\n\n\n\n### CLIP Engine\n\nTo perform text-based image few-shot classification we use `CLIP`. This implementation is very experimental and conceptually does not fully integrate the way we intend it, since the embeddings of CLIP and GPT-3 are not aligned (embeddings of the same word are not identical for both models). Aligning them is an open problem for future research. For example, one could learn linear projections from one embedding space to the other.\n\nThe following example shows how to classify the image of our generated cat from above and return the results as an array of probabilities:\n\n```python\nexpr = Expression()\nres = expr.vision('https://oaidalleapiprodscus.blob.core.windows.net/private/org-l6FsXDfth6...', \n                  ['cat', 'dog', 'bird', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe'])\n```\n\n```bash\n:Output:\narray([[9.72840726e-01, 6.34790864e-03, 2.59368378e-03, 3.41371237e-03,\n        3.71197984e-03, 8.53193272e-03, 1.03346225e-04, 2.08464009e-03,\n        1.77942711e-04, 1.94185617e-04]], dtype=float32)\n```\n\n### Local Neuro-Symbolic Engine\n\nOne can use the a locally hosted instance for the Neuro-Symbolic Engine. Out of the box we provide a Hugging Face client-server backend and host the model `EleutherAI/gpt-j-6B` to perform the inference. As the name suggests this is a six billion parameter model and requires a GPU with ~16GB RAM to run properly. The following example shows how to host and configure the usage of the local Neuro-Symbolic Engine.\n\nFist we start the backend server:\n\n```bash\n# optional: set cache folder for transformers (Linux/MacOS)\nexport TRANSFORMERS_CACHE=\"<path-to-cache-folder>\"\n# start server backend (default model is EleutherAI/gpt-j-6B)\nsymsvr\n# initialize server with client call\nsymclient\n```\n\nThen use once the following code to set up the local engine:\n\n```python\nfrom symai.backend.engine_nesy_client import NeSyClientEngine\n# setup local engine\nengine = NeSyClientEngine()\nsetting = Expression()\nsetting.setup(engines={'neurosymbolic': engine})\n```\n\nNow you can use the local engine to perform symbolic computation:\n```python\n# do some symbolic computation with the local engine\nsym = Symbol('cats are cute')\nres = sym.compose()\n...\n```\n\n\n### Custom Engine\n\nIf you want to replace or extend the functionality of our framework, you can do this by customizing the existing engines or creating new engines. The `Symbol` class provides for this functionality some helper methods, such as `command` and `setup`. The `command` method can pass on configurations (as `**kwargs`) to the engines and change functionalities or parameters. The `setup` method can be used to re-initialize an engine with your custom engine implementation which must sub-class the `Engine` class. Both methods can be specified to address one, more or all engines.\n\nHere is an example how to initialize your own engine. We will sub-class the existing `GPT3Engine` and override the `prepare` method. This method is called before the neural computation and can be used to modify the parameters of the actual input prompt that will be passed in for execution. In this example, we will replace the prompt with dummy text for illustration purposes:\n\n\n```python\nfrom symai.backend.engine_gpt3 import GPT3Engine\nclass DummyEngine(GPT3Engine):\n    def prepare(self, args, kwargs, wrp_params):\n        wrp_params['prompts'] = ['Go wild and generate something!']\ncustom_engine = DummyEngine()\nsym = Symbol()\nsym.setup(engines={'neurosymbolic': custom_engine})\nres = sym.compose()\n```\n\nTo configure an engine, we can use the `command` method. In this example, we will enable `verbose` mode, where the engine will print out what methods it is executing and the parameters it is using. This is useful for debugging purposes:\n\n```python\nsym = Symbol('Hello World!')\nsym.command(engines=['neurosymbolic'], verbose=True)\nres = sym.translate('German')\n```\n\n```bash\n:Output:\n<symai.backend.engine_gpt3.GPT3Engine object at 0, <function Symbol.translate.<locals>._func at 0x7fd68ba04820>, {'wrp_self': <class 'symai.symbol.S ['\\n\\nHallo Welt!']\n```\n\nHere is the list of names of the engines that are currently supported:\n\n* `neurosymbolic` - GPT-3\n* `symbolic` - WolframAlpha\n* `ocr` - Optical Character Recognition\n* `vision` - CLIP\n* `speech` - Whisper\n* `embedding` - OpenAI Embeddings API (`ada-002`)\n* `userinput` - User Command Line Input\n* `search` - SerpApi (Google search)\n* `crawler` - Selenium\n* `execute` - Python Interpreter\n* `index` - Pinecone\n* `open` - File System\n* `output` - Output Callbacks (e.g. for printing to console or storage)\n* `imagerendering` - DALL¬∑E 2\n\nFinally, let's assume you want to create a entirely new engine, but still keep our workflow, then you can use the `_process_query` function from `symai/functional.py` and pass in your engine including all other specified objects (i.e. Prompt, PreProcessor, etc.; see also section [Custom Operations](#üß™-custom-operations)).\n\n## ‚ö°Limitations\n\nUff... this is a long list. We are still in the early stages of development and are working hard to overcome these limitations. Just to name a few:\n\nEngineering challenges:\n* Our framework is still in its early stages of development and is not yet meant for production use. For example, the Stream class only estimates the prompt size by an approximation, which sometimes can fail. One can also create more sophisticated prompt hierarchies and dynamically adjust the global context based on a state-based approach. This would allow making consistent predictions even for long text streams.\n* Many operations need to be further improved: verified for biases, fairness, robustness, etc.\n* The code may not be complete and is not yet optimized for speed and memory usage, and uses API-based LLMs due to limitations of compute resources.\n* Code coverage is not yet complete and we are still working on the documentation.\n* Integrate with a more diverse set of models from [Hugging Face](https://huggingface.co/) or other platforms.\n* Currently we did not account for multi-threading and multi-processing.\n\nResearch challenges:\n* To reliably use our framework, one needs to further explore how to fine-tune LLMs to specifically solve many of the proposed operations in a more robust and efficient way. \n* The experimental integration of CLIP is meant to align image and text embeddings. To enable decision-making of LLMs based on observations and perform symbolic operations on objects in images or videos would be a huge leap forward. This would perfectly integrate with reinforcement learning approaches and enable us to control policies in a systematic way (see also [GATO](https://www.deepmind.com/publications/a-generalist-agent)). Therefore, we need to train large multi-modal variants with image / video data and text data, describing in high details the scenes to obtain neuro-symbolic computation engines that can perform semantic operations similar to `move-towards-tree`, `open-door`, etc.\n* Generalist LLMs are still highly over-parameterized and hardware has not yet caught up to host these models on arbitrary day-to-day machines. This limits the applicability of our approach not only on small data streams, but also gives high latencies and therefore limits the amount of complexity and expressiveness we can achieve with our expressions.\n\n\n## ü•† Future Work\n\nWe are constantly working on improving the framework and are open to any suggestions, feedback or comments. However, we try to think ahead of time and have some general ideas for future work in mind:\n\n* Meta-Learning Semantic Concepts on top of Neuro-Symbolic Expressions\n* Self-evolving and self-healing API\n* Integrate our neuro-symbolic framework with Reinforcement Learning\n\nWe believe that LLMs as neuro-symbolic computation engines enable us a new class of applications, with tools and APIs that can self-analyze and self-heal. We are excited to see what the future brings and are looking forward to your feedback and contributions.\n\n## Conclusion\n\nWe have presented a neuro-symbolic view on LLMs and showed how they can be a central pillar for many multi-modal operations. We gave an technical report on how to utilize our framework and also hinted at the capabilities and prospects of these models to be leveraged by modern software development. \n\n\n\n## üë• References, Related Work \\& Credits\n\nThis project is inspired by the following works, but not limited to them:\n\n* [Newell and Simon's Logic Theorist: Historical Background and Impact on Cognitive Modeling](https://www.researchgate.net/publication/276216226_Newell_and_Simon's_Logic_Theorist_Historical_Background_and_Impact_on_Cognitive_Modeling)\n* [Search and Reasoning in Problem Solving](https://www.sciencedirect.com/science/article/abs/pii/S0004370283800034)\n* [The Algebraic Theory of Context-Free Languages](http://www-igm.univ-mlv.fr/~berstel/Mps/Travaux/A/1963-7ChomskyAlgebraic.pdf)\n* [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/abs/2301.05062)\n* [How can computers get common sense?](https://www.science.org/doi/10.1126/science.217.4566.1237)\n* [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)\n* [SymPy: symbolic computing in Python](https://github.com/sympy/sympy)\n* [Neuro-symbolic programming](https://arxiv.org/abs/2210.05050)\n* [Fuzzy Sets](https://web.archive.org/web/20150813153834/http://www.cs.berkeley.edu/~zadeh/papers/Fuzzy%20Sets-Information%20and%20Control-1965.pdf)\n* [An early approach toward graded identity and graded membership in set theory](https://www.sciencedirect.com/science/article/abs/pii/S0165011409005326?via%3Dihub)\n* [From Statistical to Causal Learning](https://arxiv.org/abs/2204.00607)\n* [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n* [Deep reinforcement learning from human preferences](https://arxiv.org/abs/1706.03741)\n* [Aligning Language Models to Follow Instructions](https://openai.com/blog/instruction-following/)\n* [Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n* [Measuring and Narrowing the Compositionality Gap in Language Models](https://ofir.io/self-ask.pdf)\n* [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)\n* [Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing](https://arxiv.org/abs/2107.13586)\n* [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)\n* [Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing](https://arxiv.org/abs/2212.10678)\n* [Connectionism and Cognitive Architecture: A Critical Analysis](https://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf)\n* [Unit Testing for Concepts in Neural Networks](https://arxiv.org/abs/2208.10244)\n* [Teaching Algorithmic Reasoning via In-context Learning](https://arxiv.org/abs/2211.09066)\n* [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)\n* [Wolfram|Alpha as the Way to Bring Computational Knowledge Superpowers to ChatGPT](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/)\n* [Build a GitHub support bot with GPT3, LangChain, and Python](https://dagster.io/blog/chatgpt-langchain)\n\n\n### Comparison to other frameworks\n\nSince an often received request is to state the differences between our project and LangChain, this is a short list and by no means complete to contrast ourselves to other frameworks:\n\n* We focus on cognitive science and cognitive architectures research, and therefore, do not consider our framework as a production-ready implementation. We believe that the current state of the art in LLMs is not yet ready for general purpose tasks, and therefore, we focus on the advances of concept learning, reasoning and flow control of the generative process.\n* We consider LLMs as one type of neuro-symbolic computation engines, which could be of any shape or form, such as knowledge graphs, rule-based systems, etc., therefore, not necessarily limited to Transformers or LLMs.\n* We focus on advancing the development of programming languages and new programming paradigms, and subsequently its programming stack, including neuro-symbolic design patterns to integrate with operators, inheritance, polymorphism, compositionality, etc. Classical object-oriented and compositional design pattern have been well studied in the literature, however, we bring a novel view on how LLMs integrate and augment fuzzy logic and neuro-symbolic computation.\n* We do not consider our main attention towards prompt engineering. Our proposed prompt design helps the purpose to combine object-oriented paradigms with machine learning models. We believe that prompt misalignments in their current form will alleviate with further advances in Reinforcement Learning from Human Feedback and other value alignment methods. Therefore, these approaches will solve the necessity to prompt engineer or the ability to prompt hack statements.\nConsequently, this will result to much shorter zero- or few-shot examples (at least for small enough tasks). This is where we see the power of a divide a conquer approach, performing basic operations and re-combining them to solve the complex tasks.\n* We see operators / methods as being able to move along a spectrum between prompting and fine-tuning, based on task-specific requirements and availability of data. We believe that this is a more general approach, compared to prompting frameworks.\n* We propose a general approach how to handle large context sizes and how to transform a data stream problem into a search problem, related to the **reasoning as a search problem** in [Search and Reasoning in Problem Solving](https://www.sciencedirect.com/science/article/abs/pii/S0004370283800034).\n\nWe also want to state, that we highly value and support the further development of LangChain. We believe that for the community they offer very important contributions and help advance the commercialization of LLMs. We hope that our work can be seen as complementary, and future outlook on how we would like to use machine learning models as an integral part of programming languages and therefore its entire computational stack.\n\n### Acknowledgements\n\nAlso this is a long list. Great thanks to my colleagues and friends at the [Institute for Machine Learning at Johannes Kepler University (JKU), Linz](https://www.jku.at/institut-fuer-machine-learning/) for their great support and feedback; great thanks to [Dynatrace Research](https://engineering.dynatrace.com/research/) for supporting this project. Thanks also to the [AI Austria RL Community](https://aiaustria.com/rl-community). Thanks to all the people who contributed to this project. Be it by providing feedback, bug reports, code, or just by using the framework. We are very grateful for your support. \n\nAnd finally, thanks to the open source community for making their APIs and tools available to the public, including (but not exclusive to) [PyTorch](https://pytorch.org/), [Hugging Face](https://huggingface.co/), [OpenAI](https://openai.com/), [GitHub](https://github.com/), [Microsoft Research](https://www.microsoft.com/en-us/research/), and many more. \n\n\nSpecial thanks to the contributions from [Kajetan Schweighofer](https://www.linkedin.com/in/kajetan-schweighofer-a61113202/?originalSubdomain=at), [Markus Hofmarcher](https://www.linkedin.com/in/markus-hofmarcher-2722141b8/?originalSubdomain=at), [Thomas Natschl√§ger](https://www.linkedin.com/in/thomas-natschlaeger/?originalSubdomain=at) and [Sepp Hochreiter](https://scholar.google.at/citations?user=tvUH3WMAAAAJ&hl=en).\n\n\n### Contribution\n\nIf you want to contribute to this project, please read the [CONTRIBUTING.md](CONTRIBUTING.md) file for details on our code of conduct, and the process for submitting pull requests to us. Any contributions are highly appreciated.\n\n### üìú Citation\n\n```bibtex\n@software{Dinu_SymbolicAI_2022,\n  author = {Dinu, Marius-Constantin},\n  title = {{SymbolicAI: A Neuro-Symbolic Perspective on Large Language Models (LLMs)}},\n  url = {https://github.com/Xpitfire/symbolicai},\n  month = {11},\n  year = {2022}\n}\n```\n\n### üìù License\n\nThis project is licensed under the BSD-3-Clause License - see the [LICENSE](LICENSE) file for details.\n\n### Like this project?\n\nIf you like this project, leave a star ‚≠êÔ∏è and share it with your friends and colleagues.\nAnd if you want to support this project even further, please consider donating to support the continuous development of this project. Thank you!\n\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/donate/?hosted_button_id=WCWP5D2QWZXFQ)\n\nWe are also looking for contributors or investors to grow and support this project. If you are interested, please contact us.\n\n### üì´ Contact\n\nIf you have any questions about this project, please contact us via [email](mailto:office@alphacoreai.eu), on our [website](https://alphacoreai.eu/symbolic-ai/) or find us on Discord:\n\n[![Discord](https://img.shields.io/discord/768087161878085643?label=Discord&logo=Discord&logoColor=white)](https://discord.gg/QYMNnh9ra8)\n\nIf you want to contact me directly, you can reach me directly on [LinkedIn](https://www.linkedin.com/in/mariusconstantindinu/), on [Twitter](https://twitter.com/DinuMariusC), or at my personal [website](https://www.dinu.at/).\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "symbolic programming,machine learning",
    "license": "BSD 3-Clause License Copyright (c) 2023, Marius-Constantin Dinu All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: * Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
    "maintainer": "",
    "maintainer_email": "",
    "name": "symbolicai",
    "package_url": "https://pypi.org/project/symbolicai/",
    "platform": null,
    "project_url": "https://pypi.org/project/symbolicai/",
    "project_urls": {
      "GitHub": "https://github.com/Xpitfire/symai",
      "Homepage": "https://alphacoreai.eu/"
    },
    "release_url": "https://pypi.org/project/symbolicai/0.2.13/",
    "requires_dist": [
      "natsort",
      "numpy",
      "tqdm",
      "python-box",
      "wolframalpha",
      "rpyc",
      "pandas",
      "scikit-learn",
      "torch",
      "torchvision",
      "pyyaml",
      "transformers",
      "sympy",
      "openai",
      "google-search-results",
      "PyPDF2",
      "ipython",
      "accelerate",
      "sentencepiece",
      "beautifulsoup4",
      "selenium",
      "webdriver-manager",
      "whisper",
      "pinecone-client"
    ],
    "requires_python": ">=3.7",
    "summary": "A Neuro-Symbolic Framework for Python",
    "version": "0.2.13",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17073713,
  "releases": {
    "0.2.10": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6b847ec354c6c70a6f593dea8f575babfbc4621b27d11c61aa49272866ef0c7b",
          "md5": "892b5f61ff04293641afc635764ca448",
          "sha256": "ca2322a20a45c72f18d283a0f1b7f6b345a0a0706f2b566e0e3fc075602053e3"
        },
        "downloads": -1,
        "filename": "symbolicai-0.2.10-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "892b5f61ff04293641afc635764ca448",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 4116149,
        "upload_time": "2023-02-07T08:23:24",
        "upload_time_iso_8601": "2023-02-07T08:23:24.713904Z",
        "url": "https://files.pythonhosted.org/packages/6b/84/7ec354c6c70a6f593dea8f575babfbc4621b27d11c61aa49272866ef0c7b/symbolicai-0.2.10-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8acc72697f5bf294805a0adb16db45c35dfc7039b7cc6dbda709dd9599a4db58",
          "md5": "b5e4882b8d33c36d7b08356f16427ba3",
          "sha256": "d944f2a91dad406176a1b02e6d43dab71e5879b23456a1584fb63df09a2075b9"
        },
        "downloads": -1,
        "filename": "symbolicai-0.2.11-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b5e4882b8d33c36d7b08356f16427ba3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 4122140,
        "upload_time": "2023-02-12T20:42:40",
        "upload_time_iso_8601": "2023-02-12T20:42:40.908220Z",
        "url": "https://files.pythonhosted.org/packages/8a/cc/72697f5bf294805a0adb16db45c35dfc7039b7cc6dbda709dd9599a4db58/symbolicai-0.2.11-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.13": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "20d058f0be1c31e861776d7d2d09934b8b1ab2bad65c13d0366e6966a79b5dae",
          "md5": "6256476215daa6ed298d784cbff25fe1",
          "sha256": "2c91a08fca9eff50dbd1173c0db57c318d3c9059dec767785ccb513708d97bfd"
        },
        "downloads": -1,
        "filename": "symbolicai-0.2.13-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6256476215daa6ed298d784cbff25fe1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 4127194,
        "upload_time": "2023-02-27T23:56:22",
        "upload_time_iso_8601": "2023-02-27T23:56:22.301292Z",
        "url": "https://files.pythonhosted.org/packages/20/d0/58f0be1c31e861776d7d2d09934b8b1ab2bad65c13d0366e6966a79b5dae/symbolicai-0.2.13-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a58f53eb771cd3b648d591473245262cc430d658e2c3051a266ed459b45afc52",
          "md5": "18e287dd4534c52336ff223c3b627a15",
          "sha256": "3b634ed609f0235b41875d18c624ad4c9e493bb097e65187284d0bf88eb547d9"
        },
        "downloads": -1,
        "filename": "symbolicai-0.2.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "18e287dd4534c52336ff223c3b627a15",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 4097284,
        "upload_time": "2023-01-20T11:12:12",
        "upload_time_iso_8601": "2023-01-20T11:12:12.263563Z",
        "url": "https://files.pythonhosted.org/packages/a5/8f/53eb771cd3b648d591473245262cc430d658e2c3051a266ed459b45afc52/symbolicai-0.2.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bf2e40fb7a314742a2c64ecf8c509b3d5420a6511ffdefe60affa827ed1ee29c",
          "md5": "90c1d27a1ca4792f81bbd40608e3b33e",
          "sha256": "80a1a135f726950625e6b0b2307f9295cb045e5d74da923f6efc9f4950250a9c"
        },
        "downloads": -1,
        "filename": "symbolicai-0.2.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "90c1d27a1ca4792f81bbd40608e3b33e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 4098810,
        "upload_time": "2023-01-21T19:56:37",
        "upload_time_iso_8601": "2023-01-21T19:56:37.128909Z",
        "url": "https://files.pythonhosted.org/packages/bf/2e/40fb7a314742a2c64ecf8c509b3d5420a6511ffdefe60affa827ed1ee29c/symbolicai-0.2.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08261b4cba7e69f4fa6d222881c14dcf8d64156f2f6d205e15b008914c428849",
          "md5": "f3bf627555034fd5ccd4c3aca752c971",
          "sha256": "f613a8602aa7d03b82c7ea91294218e6c0f60e73bf7b7576ca0414ff3980f5a9"
        },
        "downloads": -1,
        "filename": "symbolicai-0.2.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f3bf627555034fd5ccd4c3aca752c971",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 4100816,
        "upload_time": "2023-01-23T23:41:13",
        "upload_time_iso_8601": "2023-01-23T23:41:13.811031Z",
        "url": "https://files.pythonhosted.org/packages/08/26/1b4cba7e69f4fa6d222881c14dcf8d64156f2f6d205e15b008914c428849/symbolicai-0.2.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5914798e49c20397acd30771571396d3eda6fd597a03aa173eaff2f7291228e3",
          "md5": "15299decb46e405143214824ea50108a",
          "sha256": "e9a000ce8cd5a5e1ccd7a8a1f4390f0282e89656d781ca074af6d230214041c1"
        },
        "downloads": -1,
        "filename": "symbolicai-0.2.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "15299decb46e405143214824ea50108a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 4111528,
        "upload_time": "2023-02-03T12:29:33",
        "upload_time_iso_8601": "2023-02-03T12:29:33.654722Z",
        "url": "https://files.pythonhosted.org/packages/59/14/798e49c20397acd30771571396d3eda6fd597a03aa173eaff2f7291228e3/symbolicai-0.2.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d420d69129c72a905a1d7f198c70e94d4b5d966f97686ef827699d68d5da2a6c",
          "md5": "2955f62d8e88319698c6312c2b91f25d",
          "sha256": "0f45a391ac79739be6f0817d984795e24b2a99f0423bedac7a6e11770670afbd"
        },
        "downloads": -1,
        "filename": "symbolicai-0.2.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2955f62d8e88319698c6312c2b91f25d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 4116082,
        "upload_time": "2023-02-04T16:31:12",
        "upload_time_iso_8601": "2023-02-04T16:31:12.797310Z",
        "url": "https://files.pythonhosted.org/packages/d4/20/d69129c72a905a1d7f198c70e94d4b5d966f97686ef827699d68d5da2a6c/symbolicai-0.2.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "20d058f0be1c31e861776d7d2d09934b8b1ab2bad65c13d0366e6966a79b5dae",
        "md5": "6256476215daa6ed298d784cbff25fe1",
        "sha256": "2c91a08fca9eff50dbd1173c0db57c318d3c9059dec767785ccb513708d97bfd"
      },
      "downloads": -1,
      "filename": "symbolicai-0.2.13-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "6256476215daa6ed298d784cbff25fe1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 4127194,
      "upload_time": "2023-02-27T23:56:22",
      "upload_time_iso_8601": "2023-02-27T23:56:22.301292Z",
      "url": "https://files.pythonhosted.org/packages/20/d0/58f0be1c31e861776d7d2d09934b8b1ab2bad65c13d0366e6966a79b5dae/symbolicai-0.2.13-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}