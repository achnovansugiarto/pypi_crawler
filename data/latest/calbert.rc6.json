{
  "info": {
    "author": "Aditeya Baral",
    "author_email": "aditeya.baral@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# CalBERT - Code-mixed Apaptive Language representations using BERT\r\n\r\nThis repository contains the source code\r\nfor [CalBERT - Code-mixed Apaptive Language representations using BERT](http://ceur-ws.org/Vol-3121/short3.pdf),\r\npublished at AAAI-MAKE 2022, Stanford University.\r\n\r\nCalBERT can be used to adapt existing Transformer language representations into another similar language by minimising\r\nthe semantic space between equivalent sentences in those languages, thus allowing the Transformer to learn\r\nrepresentations for words across two languages. It relies on a novel pre-training architecture named Siamese Pre-training to learn task-agnostic and language-agnostic\r\nrepresentations. For more information, please refer to the paper.\r\n\r\nThis framework allows you to perform CalBERT's Siamese Pre-training to learn representations for your own data and can be used to obtain dense vector representations for words, sentences or paragraphs. The base models used to \r\ntrain CalBERT consist of BERT-based Transformer models such as BERT, RoBERTa, XLM, XLNet, DistilBERT, and so on. \r\nCalBERT achieves state-of-the-art results on the SAIL and IIT-P Product Reviews datasets. CalBERT is also one of the\r\nonly models able to learn code-mixed language representations without the need for traditional pre-training methods and \r\nis currently one of the few models available for Indian code-mixing such as Hinglish.\r\n\r\n# Installation\r\n\r\nWe recommend `Python 3.9` or higher for CalBERT.\r\n\r\n## Install PyTorch\r\n\r\nFollow [PyTorch - Get Started](https://pytorch.org/get-started/locally/) for further details on how to install PyTorch \r\nwith or without CUDA.\r\n\r\n## Install CalBERT\r\n\r\n### Install with pip\r\n   ```bash\r\n   pip install calbert\r\n   ```\r\n\r\n### Install from source\r\nYou can also clone the current version from the [repository](https://github.com/aditeyabaral/calbert) and then directly \r\ninstall the package.\r\n   ```bash\r\n   pip install -e .\r\n   ```\r\n\r\n# Getting Started\r\n\r\nYou can read the [docs](https://calbert.readthedocs.io/en/latest/) to learn more about how to train CalBERT for your own\r\nuse case.\r\n\r\nThe following example shows you how to use CalBERT to obtain sentence embeddings.\r\n\r\n# Training\r\n\r\nThis framework allows you to also train your own CalBERT models on your own code-mixed data so you can learn\r\nembeddings for your custom code-mixed languages. There are various options to choose from in order to get the best\r\nembeddings for your language.\r\n\r\nFirst, initialise a model with the base Transformer\r\n```python\r\nfrom calbert import CalBERT\r\nmodel = CalBERT('bert-base-uncased')\r\n```\r\n\r\nCreate a CalBERTDataset using your sentences\r\n```python\r\nfrom calbert import CalBERTDataset\r\nbase_language_sentences = [\r\n   \"I am going to Delhi today via flight\",\r\n   \"This movie is awesome!\"\r\n]\r\ntarget_language_sentences = [\r\n   \"Main aaj flight lekar Delhi ja raha hoon.\",\r\n   \"Mujhe yeh movie bahut awesome lagi!\"\r\n]\r\ndataset = CalBERTDataset(base_language_sentences, target_language_sentences)\r\n```\r\n\r\nThen create a trainer and train the model\r\n```python\r\nfrom calbert import SiamesePreTrainer\r\ntrainer = SiamesePreTrainer(model, dataset)\r\ntrainer.train()\r\n```\r\n\r\n# Performance\r\n\r\nOur models achieve state-of-the-art results on the SAIL and IIT-P Product Reviews datasets.\r\n\r\nMore information will be added soon.\r\n\r\n# Application and Uses\r\n\r\nThis framework can be used for:\r\n\r\n- Computing code-mixed as well as plain sentence embeddings\r\n- Obtaining semantic similarities between any two sentences\r\n- Other textual tasks such as clustering, text summarization, semantic search and many more.\r\n\r\n# Citing and Authors\r\n\r\nIf you find this repository useful, please cite our publication [CalBERT - Code-mixed Apaptive Language representations using BERT](http://ceur-ws.org/Vol-3121/short3.pdf).\r\n\r\n```bibtex\r\n@inproceedings{calbert-baral-et-al-2022,\r\n  author    = {Aditeya Baral and\r\n               Aronya Baksy and\r\n               Ansh Sarkar and\r\n               Deeksha D and\r\n               Ashwini M. Joshi},\r\n  editor    = {Andreas Martin and\r\n               Knut Hinkelmann and\r\n               Hans{-}Georg Fill and\r\n               Aurona Gerber and\r\n               Doug Lenat and\r\n               Reinhard Stolle and\r\n               Frank van Harmelen},\r\n  title     = {CalBERT - Code-Mixed Adaptive Language Representations Using {BERT}},\r\n  booktitle = {Proceedings of the {AAAI} 2022 Spring Symposium on Machine Learning\r\n               and Knowledge Engineering for Hybrid Intelligence {(AAAI-MAKE} 2022),\r\n               Stanford University, Palo Alto, California, USA, March 21-23, 2022},\r\n  series    = {{CEUR} Workshop Proceedings},\r\n  volume    = {3121},\r\n  publisher = {CEUR-WS.org},\r\n  year      = {2022},\r\n  url       = {http://ceur-ws.org/Vol-3121/short3.pdf},\r\n  timestamp = {Fri, 22 Apr 2022 14:55:37 +0200}\r\n}\r\n```\r\n\r\n# Contact\r\n\r\nPlease feel free to contact us by emailing us to report any issues or suggestions, or if you have any further\r\nquestions.\r\n\r\nContact: - [Aditeya Baral](https://aditeyabaral.github.io/), [aditeya.baral@gmail.com](mailto:aditeya.baral@gmail.com)\r\n\r\nYou can also contact the other maintainers listed below.\r\n\r\n- [Aronya Baksy](mailto:abaksy@gmail.com)\r\n- [Ansh Sarkar](mailto:anshsarkar1@gmail.com)\r\n- [Deeksha D](mailto:deekshad132@gmail.com)\r\n\r\n\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://pypi.org/project/calbert/",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/aditeyabaral/calbert",
    "keywords": "NLP deep learning transformer pytorch BERT",
    "license": "MIT",
    "maintainer": "Aronya Baksy",
    "maintainer_email": "abaksy@gmail.com",
    "name": "calbert",
    "package_url": "https://pypi.org/project/calbert/",
    "platform": null,
    "project_url": "https://pypi.org/project/calbert/",
    "project_urls": {
      "Download": "https://pypi.org/project/calbert/",
      "Homepage": "https://github.com/aditeyabaral/calbert"
    },
    "release_url": "https://pypi.org/project/calbert/1.0.3/",
    "requires_dist": null,
    "requires_python": ">=3.7.0",
    "summary": "Code-mixed Adaptive Language representations using BERT",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15058877,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e3cd6f31caf03f797e99686c4af50f1bdce677d84a7a64e05d99ec0af2818b0a",
          "md5": "33a5921e2f8fc681f382e149a18de95a",
          "sha256": "430fffbc15a6cddfd62b4faef6625852545c13e678165828bd7d14056489667e"
        },
        "downloads": -1,
        "filename": "calbert-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "33a5921e2f8fc681f382e149a18de95a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13663,
        "upload_time": "2022-07-12T17:54:59",
        "upload_time_iso_8601": "2022-07-12T17:54:59.120508Z",
        "url": "https://files.pythonhosted.org/packages/e3/cd/6f31caf03f797e99686c4af50f1bdce677d84a7a64e05d99ec0af2818b0a/calbert-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ac9be53ccc04a1035d7045ed02ad8f8cd1c58b4f87a6d0025f364e12a28bce75",
          "md5": "06744a19376f997f6318b501259352b0",
          "sha256": "efb9658a79206a7cc88a3bc4542dd915b611afb7eeeee48135cff4fc0d66305d"
        },
        "downloads": -1,
        "filename": "calbert-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "06744a19376f997f6318b501259352b0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14149,
        "upload_time": "2022-07-14T14:50:45",
        "upload_time_iso_8601": "2022-07-14T14:50:45.093343Z",
        "url": "https://files.pythonhosted.org/packages/ac/9b/e53ccc04a1035d7045ed02ad8f8cd1c58b4f87a6d0025f364e12a28bce75/calbert-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7a3bed85806ff565a02cec37e4c321fcf9d2d5607a2dcd7c137edaaf6162b1d4",
          "md5": "23918d3a56eb6df9bb9ae5201ca7c3a3",
          "sha256": "d930f24705a38e704c0c42e6d499686bec38af349be3e71edaacf1be8060cbab"
        },
        "downloads": -1,
        "filename": "calbert-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "23918d3a56eb6df9bb9ae5201ca7c3a3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14069,
        "upload_time": "2022-07-14T16:10:27",
        "upload_time_iso_8601": "2022-07-14T16:10:27.434869Z",
        "url": "https://files.pythonhosted.org/packages/7a/3b/ed85806ff565a02cec37e4c321fcf9d2d5607a2dcd7c137edaaf6162b1d4/calbert-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f34fa2697a6761826373709d814761febbf2197f6ba0e4ff6d034d72aa415f68",
          "md5": "864b3f1f482e9bb0329b86169dcc8089",
          "sha256": "edece7dd49e3454e910f973fda1ab33c54a79b9604231a6aa64a591709e15268"
        },
        "downloads": -1,
        "filename": "calbert-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "864b3f1f482e9bb0329b86169dcc8089",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 14609,
        "upload_time": "2022-07-19T17:28:05",
        "upload_time_iso_8601": "2022-07-19T17:28:05.973170Z",
        "url": "https://files.pythonhosted.org/packages/f3/4f/a2697a6761826373709d814761febbf2197f6ba0e4ff6d034d72aa415f68/calbert-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6fa4ce80b9c37f01b32c239de5547ea6a7209fa3d6c900fe2138c3466c41804f",
          "md5": "15096a2809b66ec1bb01eb996c4e498b",
          "sha256": "4dace2ae8e9f3e076761997826ad7c9d808efd06b8c581984851e4a5176195d5"
        },
        "downloads": -1,
        "filename": "calbert-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "15096a2809b66ec1bb01eb996c4e498b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 14893,
        "upload_time": "2022-07-21T15:59:52",
        "upload_time_iso_8601": "2022-07-21T15:59:52.114701Z",
        "url": "https://files.pythonhosted.org/packages/6f/a4/ce80b9c37f01b32c239de5547ea6a7209fa3d6c900fe2138c3466c41804f/calbert-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3f986cb30589b2d05193ed037655a619c2ea994b01fedb9bdbcbfb42e38da689",
          "md5": "2e1bcdd41c39b33a17b9aea9905117e5",
          "sha256": "cc25e36ebeb5a4987d672bacdc4d135ba851c445e9f606fb52a34a8434700144"
        },
        "downloads": -1,
        "filename": "calbert-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "2e1bcdd41c39b33a17b9aea9905117e5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 15347,
        "upload_time": "2022-09-11T15:31:38",
        "upload_time_iso_8601": "2022-09-11T15:31:38.346432Z",
        "url": "https://files.pythonhosted.org/packages/3f/98/6cb30589b2d05193ed037655a619c2ea994b01fedb9bdbcbfb42e38da689/calbert-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3f986cb30589b2d05193ed037655a619c2ea994b01fedb9bdbcbfb42e38da689",
        "md5": "2e1bcdd41c39b33a17b9aea9905117e5",
        "sha256": "cc25e36ebeb5a4987d672bacdc4d135ba851c445e9f606fb52a34a8434700144"
      },
      "downloads": -1,
      "filename": "calbert-1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "2e1bcdd41c39b33a17b9aea9905117e5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.0",
      "size": 15347,
      "upload_time": "2022-09-11T15:31:38",
      "upload_time_iso_8601": "2022-09-11T15:31:38.346432Z",
      "url": "https://files.pythonhosted.org/packages/3f/98/6cb30589b2d05193ed037655a619c2ea994b01fedb9bdbcbfb42e38da689/calbert-1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}