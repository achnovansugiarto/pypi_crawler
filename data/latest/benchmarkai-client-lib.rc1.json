{
  "info": {
    "author": "Edison Gustavo Muenz",
    "author_email": "edisongustavo@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: Implementation :: CPython"
    ],
    "description": "A small library for publishing metrics when running a workflow using the Benchmark AI infrastructure.\n\n# Installation\n\n```\npip install benchmarkai-client-lib\n```\n\n# API\n\nThe library provides a single method:\n\n```\ndef emit(metrics: Dict[str, float])\n```\n\nPS: This library also works with Python 2, which has no type system.\n\n## Example usage\n\n```python\nfrom benchmarkai import emit\n\ndef my_fancy_benchmark():\n    import time\n    begin = time.time()\n    for epoch in range(100):\n        execute_training()\n    elapsed_time = time.time() - begin\n    emit({\"alexnet.batchsize16.latency\": elapsed_time})\n```\n\n## Performance\n\nAs the author you don't need to worry about `emit()` being slow since it does 2 things:\n\n- serializes `metrics` into JSON\n- writes this string into a file (see the internal details)\n\nSo it will be as slow as these operations can be, which are quite fast.\n\n## Accepted values\n\nThe dictionary provided by the `metrics` parameter is in the form:\n\n`\"metric-name\" => value`\n\n- `metric-name` is a `string`\n- `value` is a `float`\n\n# Running tests\n\nTo set up the environment to run the tests, the file `test-environment.yml` is provided. This conda environment file\nhas all dependencies required to run the tests.\n\n```\n$ conda env create -f test-environment.yml\n$ conda activate benchmark-ai-client-library-tests\n$ tox\n```\n\n# Internal details\n\n## Modes\n\nThe library can be used in these modes as of now:\n\n1. Stdout printing (this is the **default** mode)\n1. Benchmark AI infrastructure\n\nTo control which behavior is desired, the environment variable `BENCHMARK_AI` can be set to one of the values:\n\n- `stdout`: The **default** mode. In this configuration, the `emit()` method is equivalent to:\n    ```python\n    def emit(metrics):\n        import json\n        print(json.dumps(metrics))\n    ```\n\n- `fifo`: When running inside Benchmark AI infrastructure this is the mode used.\n\n### Fifo mode\n\nIn this mode, the `emit()` method writes JSON string to a file that was created as a \n[named pipe](https://docs.python.org/3.7/library/os.html#os.mkfifo). It does **NOT** write to `stdout`.\n\nKeep in mind that the client library does **NOT** create the *named pipe* file, this is delegated to the\n\"daemon process\", which will be running alongside the benchmark process in the Benchmark AI infrastructure.\n\nThe daemon process is responsible to:\n\n- create the *named pipe*\n- read the contents from this pipe\n- post the contents of this pipe to the metrics servers (eg.: Elastic Search, Cloudwatch, etc.)\n\nThe first time that `emit()` is called, it checks for the existence of the file and uses that. If it doesn't exist, it\nwaits for some time before giving up and throwing an exception `FifoNotCreatedInTimeError`.\n\n#### Fifo mode configurations\n\nThese environment variables control the behavior of the client library:\n\n- `BENCHMARK_AI_FIFO_FILEPATH`: The path to the file that will be created as a *named pipe*.\n                                Default value: `/tmp/benchmark-ai-fifo`\n- `BENCHMARK_AI_FIFO_MAX_WAIT_TIME`: The maximum amount of time that the client library will wait for the\n                                     *named pipe file* to be created.\n                                     Type is **float**, unit is **seconds**, default value: `10`.\n- `BENCHMARK_AI_FIFO_WAIT_TIME_STEP`: The amount of time between each check that the client library will wait before\n                                      checking for the existence of the *named pipe file* again.\n                                      Type is **float**, unit is **seconds**, default value: `0.5`.\n\n**IMPORTANT**: These environment variables are for internal usage of the Benchmark AI infrastructure. They are listed\n               here for documentation purposes only.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/MXNetEdge/benchmark-ai",
    "keywords": "",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "benchmarkai-client-lib",
    "package_url": "https://pypi.org/project/benchmarkai-client-lib/",
    "platform": "",
    "project_url": "https://pypi.org/project/benchmarkai-client-lib/",
    "project_urls": {
      "Homepage": "https://github.com/MXNetEdge/benchmark-ai"
    },
    "release_url": "https://pypi.org/project/benchmarkai-client-lib/0.1/",
    "requires_dist": null,
    "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
    "summary": "A client library for posting metrics that uses the Benchmark AI framework.",
    "version": "0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4873932,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "93ff3acf083f8c8c25fe8e8043e98fc2d3fb61d1b55f23a470ac210954c4e82f",
          "md5": "1efa1e34a87adf0aa69390e09df275b9",
          "sha256": "7d5118375a2f81b7d8bb7bbe1e3ad7319f17e79cdcb1bdb30c41d2cd9d28c566"
        },
        "downloads": -1,
        "filename": "benchmarkai-client-lib-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1efa1e34a87adf0aa69390e09df275b9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 4304,
        "upload_time": "2019-02-27T12:13:09",
        "upload_time_iso_8601": "2019-02-27T12:13:09.988146Z",
        "url": "https://files.pythonhosted.org/packages/93/ff/3acf083f8c8c25fe8e8043e98fc2d3fb61d1b55f23a470ac210954c4e82f/benchmarkai-client-lib-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "93ff3acf083f8c8c25fe8e8043e98fc2d3fb61d1b55f23a470ac210954c4e82f",
        "md5": "1efa1e34a87adf0aa69390e09df275b9",
        "sha256": "7d5118375a2f81b7d8bb7bbe1e3ad7319f17e79cdcb1bdb30c41d2cd9d28c566"
      },
      "downloads": -1,
      "filename": "benchmarkai-client-lib-0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "1efa1e34a87adf0aa69390e09df275b9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
      "size": 4304,
      "upload_time": "2019-02-27T12:13:09",
      "upload_time_iso_8601": "2019-02-27T12:13:09.988146Z",
      "url": "https://files.pythonhosted.org/packages/93/ff/3acf083f8c8c25fe8e8043e98fc2d3fb61d1b55f23a470ac210954c4e82f/benchmarkai-client-lib-0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}