{
  "info": {
    "author": "Yeonwoo Sung",
    "author_email": "neos960518@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# GLOM\n\nPyTorch implementation of [GLOM](https://arxiv.org/abs/2102.12627), Geoffrey Hinton's new idea that integrates concepts from neural fields, top-down-bottom-up processing, and attention (consensus between columns).\n\n## 1. Overview\n\nAn implementation of Geoffrey Hinton's paper \"How to represent part-whole hierarchies in a neural network\" for MNIST Dataset.\n\n## 2. Usage\n\n### 2 - 1. PyTorch version\n\n```python\nimport torch\nfrom pyglom import GLOM\n\nmodel = GLOM(\n    dim = 512,         # dimension\n    levels = 6,        # number of levels\n    image_size = 224,  # image size\n    patch_size = 14    # patch size\n)\n\nimg = torch.randn(1, 3, 224, 224)\nlevels = model(img, iters = 12) # (1, 256, 6, 512) - (batch - patches - levels - dimension)\n```\n\nPass the `return_all = True` keyword argument on forward, and you will be returned all the column and level states per iteration, (including the initial state, number of iterations + 1). You can then use this to attach any losses to any level outputs at any time step.\n\nIt also gives you access to all the level data across iterations for clustering, from which one can inspect for the theorized islands in the paper.\n\n```python\nimport torch\nfrom pyglom import GLOM\n\nmodel = GLOM(\n    dim = 512,         # dimension\n    levels = 6,        # number of levels\n    image_size = 224,  # image size\n    patch_size = 14    # patch size\n)\n\nimg = torch.randn(1, 3, 224, 224)\nall_levels = model(img, iters = 12, return_all = True) # (13, 1, 256, 6, 512) - (time, batch, patches, levels, dimension)\n\n# get the top level outputs after iteration 6\ntop_level_output = all_levels[7, :, :, -1] # (1, 256, 512) - (batch, patches, dimension)\n```\n\nDenoising self-supervised learning for encouraging emergence, as described by Hinton\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom einops.layers.torch import Rearrange\n\nfrom pyglom import GLOM\n\nmodel = GLOM(\n    dim = 512,         # dimension\n    levels = 6,        # number of levels\n    image_size = 224,  # image size\n    patch_size = 14    # patch size\n)\n\nimg = torch.randn(1, 3, 224, 224)\nnoised_img = img + torch.randn_like(img)\n\nall_levels = model(noised_img, return_all = True)\n\npatches_to_images = nn.Sequential(\n    nn.Linear(512, 14 * 14 * 3),\n    Rearrange('b (h w) (p1 p2 c) -> b c (h p1) (w p2)', p1 = 14, p2 = 14, h = (224 // 14))\n)\n\ntop_level = all_levels[7, :, :, -1]  # get the top level embeddings after iteration 6\nrecon_img = patches_to_images(top_level)\n\n# do self-supervised learning by denoising\n\nloss = F.mse_loss(img, recon_img)\nloss.backward()\n```\n\nYou can pass in the state of the column and levels back into the model to continue where you left off (perhaps if you are processing consecutive frames of a slow video, as mentioned in the paper)\n\n```python\nimport torch\nfrom pyglom import GLOM\n\nmodel = GLOM(\n    dim = 512,\n    levels = 6,\n    image_size = 224,\n    patch_size = 14\n)\n\nimg1 = torch.randn(1, 3, 224, 224)\nimg2 = torch.randn(1, 3, 224, 224)\nimg3 = torch.randn(1, 3, 224, 224)\n\nlevels1 = model(img1, iters = 12)                   # image 1 for 12 iterations\nlevels2 = model(img2, levels = levels1, iters = 10) # image 2 for 10 iteratoins\nlevels3 = model(img3, levels = levels2, iters = 6)  # image 3 for 6 iterations\n```\n\n### 2 - 2. PyTorch-Lightning version\n\nThe pyglom also provides the GLOM model that is implemented with PyTorch-Lightning.\n\n```python\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nimport os\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\n\nfrom pyglom.glom import LightningGLOM\n\n\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor()\n]))\ntrain, val = random_split(dataset, [55000, 5000])\n\nglom = LightningGLOM(\n    dim=256,         # dimension\n    levels=6,        # number of levels\n    image_size=256,  # image size\n    patch_size=16,   # patch size\n    img_channels=1\n)\n\ngpus = torch.cuda.device_count()\ntrainer = pl.Trainer(gpus=gpus, max_epochs=5)\ntrainer.fit(glom, DataLoader(train, batch_size=8, num_workers=2), DataLoader(val, batch_size=8, num_workers=2))\n```\n\n## 3. ToDo\n\n- contrastive / consistency regularization of top-ish levels\n\n## 4. Citations\n\n```bibtex\n@misc{hinton2021represent,\n    title   = {How to represent part-whole hierarchies in a neural network}, \n    author  = {Geoffrey Hinton},\n    year    = {2021},\n    eprint  = {2102.12627},\n    archivePrefix = {arXiv},\n    primaryClass = {cs.CV}\n}\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/YeonwooSung/GLOM",
    "keywords": "AI,artificial intelligence,deep learning,glom,neuralnet,neural network,NN",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pyglom",
    "package_url": "https://pypi.org/project/pyglom/",
    "platform": "",
    "project_url": "https://pypi.org/project/pyglom/",
    "project_urls": {
      "Homepage": "https://github.com/YeonwooSung/GLOM"
    },
    "release_url": "https://pypi.org/project/pyglom/0.0.3/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "PyTorch implementation of GLOM. PyTorch-Lightning version is also available.",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10160277,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fb1de4bce6021d30aa685e72b879650f9fab08e20a517caa6ac2ee34ebc8be78",
          "md5": "404c810ebc99c8992ebcaaf01f1c443f",
          "sha256": "1b50c6996b4d9d0fce4a714cf61566d9121a97477e09ede446d02edc9483bb34"
        },
        "downloads": -1,
        "filename": "pyglom-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "404c810ebc99c8992ebcaaf01f1c443f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5384,
        "upload_time": "2021-04-23T12:31:36",
        "upload_time_iso_8601": "2021-04-23T12:31:36.103551Z",
        "url": "https://files.pythonhosted.org/packages/fb/1d/e4bce6021d30aa685e72b879650f9fab08e20a517caa6ac2ee34ebc8be78/pyglom-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ee3c9047e2e64473f019dd5fe1247013bf7d684cbf780e11fe5c7c9e6219005",
          "md5": "72b9edd2a752878eed766121e60ff325",
          "sha256": "a5952af845282a05d98918137a08af9abfabad448c900e8c276f2c6d7c1e863a"
        },
        "downloads": -1,
        "filename": "pyglom-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "72b9edd2a752878eed766121e60ff325",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6818,
        "upload_time": "2021-04-24T10:29:04",
        "upload_time_iso_8601": "2021-04-24T10:29:04.332901Z",
        "url": "https://files.pythonhosted.org/packages/5e/e3/c9047e2e64473f019dd5fe1247013bf7d684cbf780e11fe5c7c9e6219005/pyglom-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d5d8722b76f9c13888f317c9e80dd16485d86f499eacb9e435a3f32603aba9cb",
          "md5": "d63b6ef288e986eed35e88fffa10c0ec",
          "sha256": "2d228416305c5f41031b6d8aefbfef1658c8750cea358b2eea5c781287b716ab"
        },
        "downloads": -1,
        "filename": "pyglom-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "d63b6ef288e986eed35e88fffa10c0ec",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6641,
        "upload_time": "2021-04-24T15:40:38",
        "upload_time_iso_8601": "2021-04-24T15:40:38.500194Z",
        "url": "https://files.pythonhosted.org/packages/d5/d8/722b76f9c13888f317c9e80dd16485d86f499eacb9e435a3f32603aba9cb/pyglom-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d5d8722b76f9c13888f317c9e80dd16485d86f499eacb9e435a3f32603aba9cb",
        "md5": "d63b6ef288e986eed35e88fffa10c0ec",
        "sha256": "2d228416305c5f41031b6d8aefbfef1658c8750cea358b2eea5c781287b716ab"
      },
      "downloads": -1,
      "filename": "pyglom-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "d63b6ef288e986eed35e88fffa10c0ec",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 6641,
      "upload_time": "2021-04-24T15:40:38",
      "upload_time_iso_8601": "2021-04-24T15:40:38.500194Z",
      "url": "https://files.pythonhosted.org/packages/d5/d8/722b76f9c13888f317c9e80dd16485d86f499eacb9e435a3f32603aba9cb/pyglom-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}