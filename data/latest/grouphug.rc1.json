{
  "info": {
    "author": "Sander Land",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: Other/Proprietary License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.11",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "\n# grouphug\n\nGroupHug is a library with extensions to ðŸ¤— transformers for multitask language modelling.\nIn addition, it contains utilities that ease data preparation, training, and inference.\n\n## Overview\n\nThe package is optimized for training a single language model to make quick and robust predictions for a wide variety of related tasks at once,\n as well as to investigate the regularizing effect of training a language modelling task at the same time.\n\nYou can train on multiple datasets, with each dataset containing an arbitrary subset of your tasks. Supported tasks include: \n\n* A single language modelling task (Masked language modelling, Masked token detection, Causal language modelling).\n  * The default collator included handles most preprocessing for these heads automatically.\n* Any number of classification tasks, including single- and multi-label classification and regression\n  * A utility function that automatically creates a classification head from your data. \n  * Additional options such as hidden layer size, additional input variables, and class weights.\n* You can also define your own model heads.\n\n## Quick Start\n\nThe project is based on Python 3.8+ and PyTorch 1.10+. To install it, simply use:\n\n`pip install grouphug`\n\n### Documentation\n\nDocumentation can be generated from docstrings using `make html` in the `docs` directory, but this is not yet on a hosted site. \n\n### Example usage\n\n```python\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\n\nfrom grouphug import AutoMultiTaskModel, ClassificationHeadConfig, DatasetFormatter, LMHeadConfig, MultiTaskTrainer\n\n# load some data. 'label' gets renamed in huggingface, so is better avoided as a feature name.\ntask_one = load_dataset(\"tweet_eval\",'emoji').rename_column(\"label\", \"tweet_label\")\nboth_tasks = pd.DataFrame({\"text\": [\"yay :)\", \"booo!\"], \"sentiment\": [\"pos\", \"neg\"], \"tweet_label\": [0,14]})\n\n# create a tokenizer\nbase_model = \"prajjwal1/bert-tiny\"\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\n# preprocess your data: tokenization, preparing class variables\nformatter = DatasetFormatter().tokenize().encode(\"sentiment\")\n# data converted to a DatasetCollection: essentially a dict of DatasetDict\ndata = formatter.apply({\"one\": task_one, \"both\": both_tasks}, tokenizer=tokenizer, test_size=0.05)\n\n# define which model heads you would like\nhead_configs = [\n    LMHeadConfig(weight=0.1),  # default is BERT-style masked language modelling\n    ClassificationHeadConfig.from_data(data, \"sentiment\"),  # detects dimensions and type\n    ClassificationHeadConfig.from_data(data, \"tweet_label\"),  # detects dimensions and type\n]\n# create the model, optionally saving the tokenizer and formatter along with it\nmodel = AutoMultiTaskModel.from_pretrained(base_model, head_configs, formatter=formatter, tokenizer=tokenizer)\n# create the trainer\ntrainer = MultiTaskTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_data=data[:, \"train\"],\n    eval_data=data[[\"one\"], \"test\"],\n    eval_heads={\"one\": [\"tweet_label\"]},  # limit evaluation to one classification task\n)\ntrainer.train()\n```\n\n### Tutorials\n\nSee [examples](./examples) for a few notebooks that demonstrate the key features.\n\n## Supported Models\n\nThe package has support for the following base models:\n\n* Bert, DistilBert, Roberta/DistilRoberta, XLM-Roberta \n* Deberta/DebertaV2\n* Electra\n* GPT2, GPT-J, GPT-NeoX, OPT\n\nExtending it to support other models is possible by simply inheriting from `_BaseMultiTaskModel`, although language modelling head weights may not always load. \n\n## Limitations\n\n* The package only supports PyTorch, and will not work with other frameworks. There are no plans to change this.\n* Grouphug was developed and tested with ðŸ¤— transformers 4.19-4.22. We will aim to test and keep compatibility with the latest version, but it is still recommended to lock the latest working versions. \n\nSee the [contributing page](CONTRIBUTING.md) if you are interested in contributing.\n\n## License\n\ngrouphug was initially developed at [Chatdesk](http://www.chatdesk.com) and is licensed under the Apache 2 [license](LICENSE).\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/sanderland/grouphug",
    "keywords": "transformers,language modelling,machine learning,classification",
    "license": "Apache2",
    "maintainer": "",
    "maintainer_email": "",
    "name": "grouphug",
    "package_url": "https://pypi.org/project/grouphug/",
    "platform": null,
    "project_url": "https://pypi.org/project/grouphug/",
    "project_urls": {
      "Homepage": "https://github.com/sanderland/grouphug"
    },
    "release_url": "https://pypi.org/project/grouphug/0.8.0/",
    "requires_dist": [
      "Unidecode (>=1.3.4,<2.0.0)",
      "datasets (>=2.0.0,<3.0.0)",
      "demoji (>=1.1.0,<2.0.0)",
      "evaluate (>=0.3.0,<0.4.0)",
      "numpy (>=1.21,<2.0)",
      "regex (>=2022.3.15,<2023.0.0)",
      "sentencepiece (>=0.1.96,<0.2.0)",
      "torch (>=1.10.0,<2.0.0)",
      "transformers (>=4.20.0,<5.0.0)"
    ],
    "requires_python": ">=3.8,<4.0",
    "summary": "GroupHug is a library with extensions to ðŸ¤— transformers for multitask language modelling.",
    "version": "0.8.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15409126,
  "releases": {
    "0.8.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a5be704204143ff6a44d6de4d60b302cf36f0178301bec3c15e8988e52d01329",
          "md5": "ecc74c99725a2f055363b2326d8c417d",
          "sha256": "24fa4d411a968b945913b10002689e60594539595155aac38ff3896cd4954582"
        },
        "downloads": -1,
        "filename": "grouphug-0.8.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ecc74c99725a2f055363b2326d8c417d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<4.0",
        "size": 40317,
        "upload_time": "2022-10-14T09:51:36",
        "upload_time_iso_8601": "2022-10-14T09:51:36.785096Z",
        "url": "https://files.pythonhosted.org/packages/a5/be/704204143ff6a44d6de4d60b302cf36f0178301bec3c15e8988e52d01329/grouphug-0.8.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "57e52736f18d4a17df60886f510ee84640cca3f6251a8c66a737a397ec94ceba",
          "md5": "070706faa514436982c7a0f7e1a3a4fc",
          "sha256": "1915843708704e005d0153ed71f6f755dcaf41d308c483e2a648e06b5c1d40d4"
        },
        "downloads": -1,
        "filename": "grouphug-0.8.0.tar.gz",
        "has_sig": false,
        "md5_digest": "070706faa514436982c7a0f7e1a3a4fc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<4.0",
        "size": 36537,
        "upload_time": "2022-10-14T09:51:38",
        "upload_time_iso_8601": "2022-10-14T09:51:38.472141Z",
        "url": "https://files.pythonhosted.org/packages/57/e5/2736f18d4a17df60886f510ee84640cca3f6251a8c66a737a397ec94ceba/grouphug-0.8.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a5be704204143ff6a44d6de4d60b302cf36f0178301bec3c15e8988e52d01329",
        "md5": "ecc74c99725a2f055363b2326d8c417d",
        "sha256": "24fa4d411a968b945913b10002689e60594539595155aac38ff3896cd4954582"
      },
      "downloads": -1,
      "filename": "grouphug-0.8.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ecc74c99725a2f055363b2326d8c417d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8,<4.0",
      "size": 40317,
      "upload_time": "2022-10-14T09:51:36",
      "upload_time_iso_8601": "2022-10-14T09:51:36.785096Z",
      "url": "https://files.pythonhosted.org/packages/a5/be/704204143ff6a44d6de4d60b302cf36f0178301bec3c15e8988e52d01329/grouphug-0.8.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "57e52736f18d4a17df60886f510ee84640cca3f6251a8c66a737a397ec94ceba",
        "md5": "070706faa514436982c7a0f7e1a3a4fc",
        "sha256": "1915843708704e005d0153ed71f6f755dcaf41d308c483e2a648e06b5c1d40d4"
      },
      "downloads": -1,
      "filename": "grouphug-0.8.0.tar.gz",
      "has_sig": false,
      "md5_digest": "070706faa514436982c7a0f7e1a3a4fc",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8,<4.0",
      "size": 36537,
      "upload_time": "2022-10-14T09:51:38",
      "upload_time_iso_8601": "2022-10-14T09:51:38.472141Z",
      "url": "https://files.pythonhosted.org/packages/57/e5/2736f18d4a17df60886f510ee84640cca3f6251a8c66a737a397ec94ceba/grouphug-0.8.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}