{
  "info": {
    "author": "Laurits Skov and Moises Coll Macia",
    "author_email": "lauritsskov2@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# Introgression detection\nThese are the scripts needed to infere archaic introgression in modern populations using an unadmixed outgroup.\n\n## Installation\n\nRun the following to install:\n\n```python\nconda create -n myenv python=3.7\nconda activate myenv\npip install myHmm # I haven't figured out how to make it a conda package\n```\nIf you want to work with bcf/vcf files I would also install vcftools and bcftools\n```python\nconda install -c bioconda vcftools bcftools\n```\n\n\n![Overview of model](https://user-images.githubusercontent.com/30321818/43464826-4d11d46c-94dc-11e8-8f1a-6851aa5d9125.jpg)\n\nThe way the model works is by removing variation found in an outgroup population and then using the remaining variants to group the genome into regions of different variant density. If the model works well we would expect that introgressed regions have higher variant density than non-introgressed - because they have spend more time accumulation variation that is not found in the outgroup.\n\nAn example on simulated data is provided below:\n\n![het_vs_archaic](https://user-images.githubusercontent.com/30321818/46877046-217eff80-ce40-11e8-9010-edb544e3e1ee.png)\n\nIn this example we zoom in on 1 Mb of simulated data for a haploid genome. The top panel shows the coalescence times with the outgroup across the region and the green segment is an archaic introgressed segment. Notice how much more deeper the coalescence time with the outgroup is. The second panel shows that probability of being in the archaic state. We can see that the probability is much higher in the archaic segment, demonstrating that in this toy example the model is working like we would hope. The next panel is the snp density if you dont remove all snps found in the outgroup. By looking at this one cant tell where the archaic segments begins and ends, or even if there is one. The bottom panel is the snp density when all variation in the outgroup is removed. Notice that now it is much clearer where the archaic segment begins and ends!\n\nThe method is now published in PlosGenetics and can be found here: [Detecting archaic introgression using an unadmixed outgroup](https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1007641) This paper is describing and evaluating the method. \n\n## Usage\n```\nScript for identifying introgressed archaic segments\n\nTurorial:\nmyHMM make_test_data \nmyHMM train  -obs=obs.txt -weights=weights.bed -mutrates=mutrates.bed -param=Initialguesses.json -out=trained.json \nmyHMM decode -obs=obs.txt -weights=weights.bed -mutrates=mutrates.bed -param=trained.json\n\n\nTurorial with 1000 genomes data:\nmyHMM create_outgroup -ind=individuals.json -vcf=*.bcf -weights=strickmask.bed -out=outgroup.txt -ancestral=homo_sapiens_ancestor_GRCh37_e71/homo_sapiens_ancestor_*.fa\nmyHMM mutation_rate -outgroup=outgroup.txt  -weights=strickmask.bed -window_size=1000000 -out mutationrate.bed\nmyHMM create_ingroup  -ind=individuals.json -vcf=*.bcf -weights=strickmask.bed -out=obs -outgroup=outgroup.txt -ancestral=homo_sapiens_ancestor_GRCh37_e71/homo_sapiens_ancestor_*.fa\n\nmyHMM train  -obs=obs.HG00096.txt -weights=strickmask.bed -mutrates=mutationrate.bed -out=trained.HG00096.json \nmyHMM decode -obs=obs.HG00096.txt -weights=strickmask.bed -mutrates=mutationrate.bed -param=trained.HG00096.json \n```\n\n\n\n## Quick tutorial\n\nLets make some test data and start using the program.\n```\n> myHMM make_test_data\nmaking test data...\ncreating 2 chromosomes with 50 Mb of test data (100K bins) with the following parameters..\n\nState names: ['Human' 'Archaic']\nStarting_probabilities: [0.98 0.02]\nTransition matrix: \n[[9.999e-01 1.000e-04]\n [2.000e-02 9.800e-01]]\nEmission values: [0.04 0.4 ]\n```\nThis will generate 4 files, obs.txt, weights.bed, mutrates.bed and Initialguesses.json. \n\nobs.txt. These are the mutation that are left after removing variants which are found in the outgroup.\n```\nchrom  pos     ancestral_base  genotype\nchr1   5212    A               AG\nchr1   32198   A               AG\nchr1   65251   C               CG\nchr1   117853  A               AG\nchr1   122518  T               TC\nchr1   142322  T               TC\nchr1   144695  C               CG\nchr1   206370  T               TG\nchr1   218969  A               AT\n```\nweights.bed. This is the parts of the genome that we can accurately map to - in this case we have simulated the data and can accurately access the entire genome.\n```\nchr1\t1\t50000000\nchr2\t1\t50000000\n```\nmutrates.bed. This is the normalized mutation rate across the genome (in bins of 1 Mb).\n```\nchr1  0        1000000   1\nchr1  1000000  2000000   1\nchr1  2000000  3000000   1\nchr1  3000000  4000000   1\nchr1  4000000  5000000   1\nchr1  5000000  6000000   1\nchr1  6000000  7000000   1\nchr1  7000000  8000000   1\nchr1  8000000  9000000   1\nchr1  9000000  10000000  1\n```\nInitialguesses.json. This is our initial guesses when training the model - note these are different from those we simulated from.\n```json\n{\n  \"state_names\": [\"Human\",\"Archaic\"],\n  \"starting_probabilities\": [0.5,0.5],\n  \"transitions\": [[0.99,0.01],[0.02,0.98]],\n  \"emissions\": [0.03,0.3]\n}\n```\nWe can find the best fitting parameters using BaumWelsch training - note you can try to ommit the weights and mutrates arguments. Since this is simulated data the mutation is constant across the genome and we can asses the entire genome. Also notice how the parameters approach the parameters the data was generated from (jubii).\n```bash\n> myHMM train  -obs=obs.txt -weights=weights.bed -mutrates=mutrates.bed -param=Initialguesses.json -out=trained.json\n\niteration    loglikelihood  start1  start2  emis1   emis2   trans1_1  trans2_2\n0            -18123.4475    0.5     0.5     0.03    0.3     0.99      0.98\n1            -17506.0219    0.96    0.04    0.035   0.2202  0.9969    0.9242\n2            -17487.797     0.971   0.029   0.0369  0.2235  0.9974    0.9141\n3            -17477.1367    0.976   0.024   0.0375  0.2404  0.9978    0.9105\n4            -17466.6961    0.98    0.02    0.0379  0.2627  0.9982    0.9102\n5            -17456.1508    0.983   0.017   0.0382  0.2877  0.9985    0.9123\n6            -17445.5098    0.986   0.014   0.0385  0.3146  0.9988    0.9172\n7            -17434.9006    0.988   0.012   0.0388  0.3426  0.9991    0.9248\n8            -17424.6966    0.99    0.01    0.039   0.3705  0.9993    0.9348\n9            -17415.623     0.991   0.009   0.0393  0.3968  0.9995    0.9464\n10           -17408.6263    0.992   0.008   0.0395  0.4195  0.9997    0.9579\n11           -17404.3022    0.993   0.007   0.0396  0.4367  0.9998    0.9673\n12           -17402.299     0.994   0.006   0.0397  0.4477  0.9998    0.9738\n13           -17401.6146    0.994   0.006   0.0398  0.4537  0.9999    0.9774\n14           -17401.4336    0.994   0.006   0.0398  0.4566  0.9999    0.9793\n15           -17401.3933    0.994   0.006   0.0398  0.4578  0.9999    0.9802\n16           -17401.3851    0.994   0.006   0.0398  0.4584  0.9999    0.9806\n17           -17401.3835    0.994   0.006   0.0398  0.4586  0.9999    0.9807\n18           -17401.3832    0.994   0.006   0.0398  0.4587  0.9999    0.9808\n19           -17401.3832    0.994   0.006   0.0398  0.4588  0.9999    0.9808\n\n\n# run without mutrate and weights (only do this for simulated data)\n> myHMM train  -obs=obs.txt -param=Initialguesses.json -out=trained.json\n```\n\nWe can now decode the data with the best parameters that maximize the likelihood and find the archaic segments:\n\n```bash\n> myHMM decode -obs=obs.txt -weights=weights.bed -mutrates=mutrates.bed -param=trained.json\n\nchrom        start     end       length    state    snps  mean_prob\nchr1         0         7233000   7234000   Human    287   0.9995\nchr1         7234000   7246000   13000     Archaic  9     0.90427\nchr1         7247000   21618000  14372000  Human    610   0.99946\nchr1         21619000  21673000  55000     Archaic  22    0.9697\nchr1         21674000  26859000  5186000   Human    204   0.99878\nchr1         26860000  26941000  82000     Archaic  36    0.971\nchr1         26942000  49989000  23048000  Human    863   0.99982\nchr2         0         6793000   6794000   Human    237   0.99972\nchr2         6794000   6822000   29000     Archaic  14    0.95461\nchr2         6823000   12646000  5824000   Human    244   0.99927\nchr2         12647000  12745000  99000     Archaic  55    0.97413\nchr2         12746000  15461000  2716000   Human    125   0.99881\nchr2         15462000  15547000  86000     Archaic  38    0.93728\nchr2         15548000  32626000  17079000  Human    709   0.99951\nchr2         32627000  32695000  69000     Archaic  31    0.98305\nchr2         32696000  41087000  8392000   Human    360   0.9995\nchr2         41088000  41178000  91000     Archaic  43    0.96093\nchr2         41179000  49952000  8774000   Human    328   0.9979\nchr2         49953000  49977000  25000     Archaic  13    0.98501\n\n# Again here you could ommit weights and mutationrates. Actually one could also ommit trained.json because then the model defaults to using the parameters we used the generated the data\n> myHMM decode -obs=obs.txt\n```\n\n\n\n\n## Example with 1000 genomes data\nI thought it would be nice to have an entire reproduceble example of how to use this model. From a common starting point such as a VCF file (well a BCF file in this case) to the final output.  The reason for using BCF files is because it is MUCH faster to extract data for each individual. You can convert a vcf file to a bcf file like this:\n\n```bash\nbcftools view file.vcf -l 1 -O b > file.bcf\nbcftools index file.bcf\n```\n\nIn this example I will analyse an individual (HG00096) from the 1000 genomes project phase 3.\n\nFirst we will need to know which 1) bases can be called in the genome and 2) which variants are found in the outgroup. So let's start out by downloading the files from the following directories.\nTo download callability regions, ancestral alleles information, ingroup outgroup information call this command:\n\n```bash\n\n\n# bcffiles (hg19)\nftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/bcf_files/\n\n# callability (remember to remove chr in the beginning of each line to make it compatible with hg19 e.g. chr1 > 1)\nftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/supporting/accessible_genome_masks/20141020.strict_mask.whole_genome.bed\nsed 's/^chr\\|%$//g' 20141020.strict_mask.whole_genome.bed | awk '{print $1\"\\t\"$2\"\\t\"$3}' > strickmask.bed\n\n# outgroup information\nftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel\n\n# Ancestral information\nftp://ftp.ensembl.org/pub/release-74/fasta/ancestral_alleles/homo_sapiens_ancestor_GRCh37_e71.tar.bz2\n```\n\nFor this example we will use all individuals from 'YRI','MSL' and 'ESN' as outgroup individuals. While we will only be decoding hG00096 in this example you can add as many individuals as you want to the ingroup.  \n\n```json\n{\n  \"ingroup\": [\n    \"HG00096\",\n    \"HG00097\"\n  ],\n  \"outgroup\": [\n    \"HG02922\",\n    \"HG02923\",\n    ...\n    \"HG02944\",\n    \"HG02946\"]\n}\n```\nThis is how we would call archaic segments in an individual. First we need to find a set of variants found in the outgroup. We can use the wildcard character to loop through all bcf files. If you dont have ancestral information you can skip the ancestral argument. \n```bash\n(took an hour) > myHMM create_outgroup -ind=individuals.json -vcf=*.bcf -weights=strickmask.bed -out=outgroup.txt -ancestral=homo_sapiens_ancestor_GRCh37_e71/homo_sapiens_ancestor_*.fa\n(took 30 sec) > myHMM mutation_rate -outgroup=outgroup.txt  -weights=strickmask.bed -window_size=1000000 -out mutationrate.bed\n```\n\n\nKeep variants that are not found to be derived in the outgroup for each individual in ingroup. You can also speficy a single individual or a comma separated list of individuals.\n```bash\n(took 20 min) > myHMM create_ingroup  -ind=individuals.json -vcf=*.bcf -weights=strickmask.bed -out=obs -outgroup=outgroup.txt -ancestral=homo_sapiens_ancestor_GRCh37_e71/homo_sapiens_ancestor_*.fa\n\n(took 20 min) > myHMM create_ingroup  -ind=HG00096 -vcf=*.bcf -weights=strickmask.bed -out=obs -outgroup=outgroup.txt -ancestral=homo_sapiens_ancestor_GRCh37_e71/homo_sapiens_ancestor_*.fa\n\n(took 20 min) > myHMM create_ingroup  -ind=HG00096,HG00097 -vcf=*.bcf -weights=strickmask.bed -out=obs -outgroup=outgroup.txt -ancestral=homo_sapiens_ancestor_GRCh37_e71/homo_sapiens_ancestor_*.fa\n```\nNow for training the HMM parameters and decoding\n```bash\n(took 3 min) > myHMM train  -obs=obs.HG00096.txt -weights=strickmask.bed -mutrates=mutationrate.bed -out=trained.HG00096.json \n\niteration  loglikelihood  start1  start2  emis1   emis2   trans1_1  trans2_2\n0          -538494.225    0.98    0.02    0.04    0.4     0.9999    0.98\n1          -531761.7479   0.957   0.043   0.0543  0.385   0.9994    0.9859\n2          -531645.8243   0.955   0.045   0.0545  0.3911  0.9992    0.9836\n3          -531598.8719   0.955   0.045   0.0544  0.3945  0.9991    0.9818\n4          -531577.0133   0.954   0.046   0.0542  0.3961  0.9991    0.9806\n5          -531566.2204   0.954   0.046   0.0541  0.3968  0.999     0.9797\n6          -531560.6752   0.954   0.046   0.054   0.3972  0.999     0.979\n7          -531557.7466   0.954   0.046   0.054   0.3973  0.999     0.9786\n8          -531556.1697   0.954   0.046   0.0539  0.3973  0.9989    0.9782\n9          -531555.3088   0.954   0.046   0.0539  0.3972  0.9989    0.978\n10         -531554.8342   0.953   0.047   0.0539  0.3971  0.9989    0.9778\n11         -531554.5707   0.953   0.047   0.0539  0.3971  0.9989    0.9777\n12         -531554.4236   0.953   0.047   0.0538  0.397   0.9989    0.9776\n13         -531554.3412   0.953   0.047   0.0538  0.3969  0.9989    0.9775\n14         -531554.295    0.953   0.047   0.0538  0.3969  0.9989    0.9774\n15         -531554.2689   0.953   0.047   0.0538  0.3969  0.9989    0.9774\n16         -531554.2542   0.953   0.047   0.0538  0.3968  0.9989    0.9774\n17         -531554.2459   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n18         -531554.2412   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n19         -531554.2386   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n20         -531554.2371   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n21         -531554.2363   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n22         -531554.2358   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n23         -531554.2355   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n24         -531554.2354   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n25         -531554.2353   0.953   0.047   0.0538  0.3968  0.9989    0.9773\n```\n\n```bash\n(took 30 sec) > myHMM decode -obs=obs.HG00096.txt -weights=strickmask.bed -mutrates=mutationrate.bed -param=trained.HG00096.json \n\nchrom  start    end      length   state    snps  mean_prob\n1      0        2788000  2789000  Human    97    0.9816\n1      2789000  2793000  5000     Archaic  3     0.52798\n1      2794000  2988000  195000   Human    11    0.96136\n1      2989000  2996000  8000     Archaic  6     0.62345\n1      2997000  3424000  428000   Human    33    0.99159\n1      3425000  3450000  26000    Archaic  22    0.97091\n1      3451000  4267000  817000   Human    42    0.98602\n1      4268000  4269000  2000     Archaic  2     0.50461\n1      4270000  4301000  32000    Human    2     0.80669\n1      4302000  4359000  58000    Archaic  20    0.7938\n1      4360000  4499000  140000   Human    5     0.97461\n1      4500000  4509000  10000    Archaic  7     0.8114\n1      4510000  5338000  829000   Human    44    0.98449\n1      5339000  5348000  10000    Archaic  6     0.71886\n1      5349000  9319000  3971000  Human    173   0.99088\n```\n\nThe first Archaic segment with a high mean posterior probability is from 3,425,000 to 3,450,000. And this segment has also been found be other methods:\n\n```bash\n# Conditional random field (S. Sankararaman - 2014)\nchr1     3,421,722-3,450,286 HG00096\n# Sstar (B. Vernot - 2016)\nchr1     3,427,298-3,461,813 HG00096\n# Sprime (S. Browning - 2018)\nchr1     3,418,794-3,457,377 HG00096\n# HMM (L. Skov 2018)\nchr1     3,425,000-3,450,000 HG00096\n```\n\nAnd that is it! Now you have run the model and gotten a set of parameters that you can interpret biologically (see my paper) and you have a list of segments that belong to the human and Archaic state.\n\nIf you have any questions about the use of the scripts, if you find errors or if you have feedback you can contact my here (make an issue) or write to:\n\nlauritsskov2@gmail.com\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/LauritsSkov/Introgression-detection",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "myHmm",
    "package_url": "https://pypi.org/project/myHmm/",
    "platform": null,
    "project_url": "https://pypi.org/project/myHmm/",
    "project_urls": {
      "Homepage": "https://github.com/LauritsSkov/Introgression-detection"
    },
    "release_url": "https://pypi.org/project/myHmm/0.0.2/",
    "requires_dist": [
      "numpy (>=1.15)",
      "scipy (>=1.5)",
      "numba"
    ],
    "requires_python": "",
    "summary": "Find introgressed segments",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13800137,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "90d6c43a2f21ed32a8eec414f7a0ef4bfbf95f58af543c8ac4b7c7a2249fdf17",
          "md5": "0dc075c463aa9e4363b6ddefe6362e70",
          "sha256": "a6b4a446d4164e694fa4b44263141b80f67e770685db7b6e7a56c6801a2a8b87"
        },
        "downloads": -1,
        "filename": "myHmm-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0dc075c463aa9e4363b6ddefe6362e70",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 29655,
        "upload_time": "2022-05-12T23:46:03",
        "upload_time_iso_8601": "2022-05-12T23:46:03.171702Z",
        "url": "https://files.pythonhosted.org/packages/90/d6/c43a2f21ed32a8eec414f7a0ef4bfbf95f58af543c8ac4b7c7a2249fdf17/myHmm-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fa58a4a9bcabfbdca5abbefb49c48fe7839307315278dc75923a64d47fadeb0b",
          "md5": "c750a1f095e890f9883f9deff83d66b1",
          "sha256": "680cbbffa3e0a0fabcdd9c803e7f495ca940d0d688c1d049de80d122a3d782fe"
        },
        "downloads": -1,
        "filename": "myHmm-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c750a1f095e890f9883f9deff83d66b1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 33417,
        "upload_time": "2022-05-12T23:46:05",
        "upload_time_iso_8601": "2022-05-12T23:46:05.584923Z",
        "url": "https://files.pythonhosted.org/packages/fa/58/a4a9bcabfbdca5abbefb49c48fe7839307315278dc75923a64d47fadeb0b/myHmm-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "827856712ade058da25a086d5201382ecc29c6c9efb860a35c063dfc565733f1",
          "md5": "20af80ed26819dc3fae263eb5ecf6af8",
          "sha256": "cef7df314ac4fc6527ca40be74639bf938feec61e383d2f70ddf1a68dfc88881"
        },
        "downloads": -1,
        "filename": "myHmm-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "20af80ed26819dc3fae263eb5ecf6af8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 29709,
        "upload_time": "2022-05-13T00:02:08",
        "upload_time_iso_8601": "2022-05-13T00:02:08.981090Z",
        "url": "https://files.pythonhosted.org/packages/82/78/56712ade058da25a086d5201382ecc29c6c9efb860a35c063dfc565733f1/myHmm-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "48d0b7abcc42a5c0f233ae99cb31445de5a455b7432cc25a75dc3c3c68724592",
          "md5": "3d9021637462c1c75ce0ef2276b051fd",
          "sha256": "a888dc72625ebf4bb88530625484d0944c6433c8c15317ad372c98b8f649ef6a"
        },
        "downloads": -1,
        "filename": "myHmm-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3d9021637462c1c75ce0ef2276b051fd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30219,
        "upload_time": "2022-05-13T00:02:13",
        "upload_time_iso_8601": "2022-05-13T00:02:13.085904Z",
        "url": "https://files.pythonhosted.org/packages/48/d0/b7abcc42a5c0f233ae99cb31445de5a455b7432cc25a75dc3c3c68724592/myHmm-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "827856712ade058da25a086d5201382ecc29c6c9efb860a35c063dfc565733f1",
        "md5": "20af80ed26819dc3fae263eb5ecf6af8",
        "sha256": "cef7df314ac4fc6527ca40be74639bf938feec61e383d2f70ddf1a68dfc88881"
      },
      "downloads": -1,
      "filename": "myHmm-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "20af80ed26819dc3fae263eb5ecf6af8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 29709,
      "upload_time": "2022-05-13T00:02:08",
      "upload_time_iso_8601": "2022-05-13T00:02:08.981090Z",
      "url": "https://files.pythonhosted.org/packages/82/78/56712ade058da25a086d5201382ecc29c6c9efb860a35c063dfc565733f1/myHmm-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "48d0b7abcc42a5c0f233ae99cb31445de5a455b7432cc25a75dc3c3c68724592",
        "md5": "3d9021637462c1c75ce0ef2276b051fd",
        "sha256": "a888dc72625ebf4bb88530625484d0944c6433c8c15317ad372c98b8f649ef6a"
      },
      "downloads": -1,
      "filename": "myHmm-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "3d9021637462c1c75ce0ef2276b051fd",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 30219,
      "upload_time": "2022-05-13T00:02:13",
      "upload_time_iso_8601": "2022-05-13T00:02:13.085904Z",
      "url": "https://files.pythonhosted.org/packages/48/d0/b7abcc42a5c0f233ae99cb31445de5a455b7432cc25a75dc3c3c68724592/myHmm-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}