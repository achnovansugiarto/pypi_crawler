{
  "info": {
    "author": "André Pedersen and David Bouget",
    "author_email": "andrped94@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<div align=\"center\">\n<h1 align=\"center\">GradientAccumulator</h1>\n<h3 align=\"center\">Seemless gradient accumulation for TensorFlow 2</h3>\n\n[![Pip Downloads](https://img.shields.io/pypi/dm/gradient-accumulator?label=pip%20downloads&logo=python)](https://pypi.org/project/gradient-accumulator/)\n[![PyPI version](https://badge.fury.io/py/gradient-accumulator.svg)](https://badge.fury.io/py/gradient-accumulator)\n[![License](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7581815.svg)](https://doi.org/10.5281/zenodo.7581815)\n[![CI](https://github.com/andreped/GradientAccumulator/workflows/CI/badge.svg)](https://github.com/andreped/GradientAccumulator/actions)\n\n**GradientAccumulator** was developed by SINTEF Health due to the lack of an easy-to-use method for gradient accumulation in TensorFlow 2.\n\nThe package is available on PyPI and is compatible with and have been tested against TF >= 2.3 and Python >= 3.6 (tested with 3.6-3.10), and works cross-platform (Ubuntu, Windows, macOS).\n</div>\n\n## What?\nGradient accumulation (GA) enables reduced GPU memory consumption through dividing a batch into smaller reduced batches, and performing gradient computation either in a distributing setting across multiple GPUs or sequentially on the same GPU. When the full batch is processed, the gradients are the _accumulated_ to produce the full batch gradient.\n\n<p align=\"center\">\n<img src=\"assets/grad_accum.png\" width=\"50%\">\n</p>\n\n\n## Why?\nIn TensorFlow 2, there did not exist a plug-and-play method to use gradient accumulation with any custom pipeline. Hence, we have implemented two generic TF2-compatible approaches:\n\n| Method | Usage |\n| - | - |\n| `GradientAccumulateModel` | `model = GradientAccumulateModel(accum_steps=4, inputs=model.input, outputs=model.output)` |\n| `GradientAccumulateOptimizer` | `opt = GradientAccumulateOptimizer(accum_steps=4, optimizer=tf.keras.optimizers.SGD(1e-2))` |\n\nBoth approaches control how frequently the weigths are updated, but in their own way. Approach (1) is for single-GPU only, whereas (2) supports both single-GPU and distributed training (multi-GPU).\n\nOur implementations enable theoretically **infinitely large batch size**, with **identical memory consumption** as for a regular mini batch. If a single GPU is used, this comes at the cost of increased training runtime. Multiple GPUs could be used to increase runtime performance.\n\nAs batch normalization is not natively compatible with GA, support for adaptive gradient clipping has been added as an alternative. We have also added support for mixed precision and both GPU and TPU support.\n\n\n## Install\n\nStable release from PyPI:\n```\npip install gradient-accumulator\n```\n\nOr from source:\n```\npip install git+https://github.com/andreped/GradientAccumulator\n```\n\n## Usage\n```\nfrom gradient_accumulator import GradientAccumulateModel\nfrom tensorflow.keras.models import Model\n\nmodel = Model(...)\nmodel = GradientAccumulateModel(accum_steps=4, inputs=model.input, outputs=model.output)\n```\n\nThen simply use the `model` as you normally would!\n\n<details open>\n<summary>\n\n#### Mixed precision</summary>\n\nThere has also been added experimental support for mixed precision:\n```\nfrom tensorflow.keras import mixed_precision\nfrom tensorflow.keras.optimizers import Adam\n\nmixed_precision.set_global_policy('mixed_float16')\nmodel = GradientAccumulateModel(accum_steps=4, mixed_precision=True, inputs=model.input, outputs=model.output)\n\nopt = Adam(1e-3, epsilon=1e-4)\nopt = mixed_precision.LossScaleOptimizer(opt)\n```\n\nIf using TPUs, use `bfloat16` instead of `float16`, like so:\n```\nmixed_precision.set_global_policy('mixed_bfloat16')\n```\n\nThere is also an example of how to use gradient accumulation with mixed precision [here](https://github.com/andreped/GradientAccumulator/blob/main/tests/test_mixed_precision.py#L58).\n</details>\n\n\n<details open>\n<summary>\n\n#### Distributed training with multiple GPUs</summary>\nIn order to use multiple GPUs, you will have to use the Optimizer wrapper:\n```\nopt = GradientAccumulateOptimizer(accum_steps=4, optimizer=tf.keras.optimizers.SGD(1e-2))\n```\n\nJust remember to wrap the optimizer within the `tf.distribute.MirroredStrategy`. For an example, see [here](https://github.com/andreped/GradientAccumulator/blob/main/tests/test_optimizer_distribute.py).\n\n</details>\n\n<details>\n<summary>\n\n#### Adaptive gradient clipping</summary>\n\nThere has also been added support for adaptive gradient clipping, based on [this](https://github.com/sayakpaul/Adaptive-Gradient-Clipping) implementation:\n```\nmodel = GradientAccumulateModel(accum_steps=4, use_agc=True, clip_factor=0.01, eps=1e-3, inputs=model.input, outputs=model.output)\n```\n\nThe hyperparameters values for `clip_factor` and `eps` presented here are the default values.\n</details>\n\n\n<details>\n<summary>\n\n#### Model format</summary>\n\nIt is recommended to use the SavedModel format when using this implementation. That is because the HDF5 format is only compatible with `TF <= 2.6` when using the model wrapper. However, if you are using older TF versions, both formats work out-of-the-box. The SavedModel format works fine for all versions of TF 2.x\n</details>\n\n\n<details>\n<summary>\n\n#### macOS compatibility</summary>\nNote that GradientAccumulator is perfectly compatible with macOS, both with and without GPUs. In order to have GPU support on macOS, you will need to install the tensorflow-compiled version that is compatible with metal:\n```\npip install tensorflow-metal\n```\n\nGradientAccumulator can be used as usually. However, note that there only exists one tf-metal version, which should be equivalent to TF==2.5.\n</details>\n\n\n<details>\n<summary>\n\n#### TF 1.x</summary>\nFor TF 1, I suggest using the AccumOptimizer implementation in the [H2G-Net repository](https://github.com/andreped/H2G-Net/blob/main/src/utils/accum_optimizers.py#L139) instead, which wraps the optimizer instead of overloading the train_step of the Model itself (new feature in TF2).\n</details>\n\n\n<details>\n<summary>\n\n#### PyTorch</summary>\nFor PyTorch, I would recommend using [accelerate](https://pypi.org/project/accelerate/). HuggingFace :hugs: has a great tutorial on how to use it [here](https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation).\n</details>\n\n\n<details>\n<summary>\n\n#### Troubleshooting</summary>\n\nOverloading of `train_step` method of tf.keras.Model was introduced in TF 2.2, hence, this code is compatible with TF >= 2.2.\n\nAlso, note that TF depends on different python versions. If you are having problems getting TF working, try a different TF version or python version.\n</details>\n\n\n<details>\n<summary>\n\n#### Disclaimer</summary>\nIn theory, one should be able to get identical results for batch training and using gradient accumulation. However, in practice, one may observe a slight difference. One of the cause may be when operations are used (or layers/optimizer/etc) that update for each step, such as Batch Normalization. It is **not** recommended to use BN with GA, as BN would update too frequently. However, you could try to adjust the `momentum` of BN (see [here](https://keras.io/api/layers/normalization_layers/batch_normalization/)).\n\nIt was also observed a small difference when using adaptive optimizers, which I believe might be due to how frequently they are updated. Nonetheless, for the optimizers, the difference was quite small, and one may approximate batch training quite well using our GA implementation, as rigorously tested [here](https://github.com/andreped/GradientAccumulator/tree/main/tests)).\n</details>\n\n\n## Acknowledgements\nThe gradient accumulator model wrapper is based on the implementation presented in [this thread](https://stackoverflow.com/a/66524901) on stack overflow. The adaptive gradient clipping method is based on [the implementation by @sayakpaul](https://github.com/sayakpaul/Adaptive-Gradient-Clipping).\nThe optimizer wrapper is derived from [the implementation by @fsx950223 and @stefan-falk](https://github.com/tensorflow/addons/pull/2525).\n\n\n## How to cite?\nIf you used this package or found the project relevant in your research, please, considering including the following citation:\n\n```\n@software{andre_pedersen_2023_7581815,\n  author       = {André Pedersen and David Bouget},\n  title        = {andreped/GradientAccumulator: v0.3.0},\n  month        = jan,\n  year         = 2023,\n  publisher    = {Zenodo},\n  version      = {v0.3.0},\n  doi          = {10.5281/zenodo.7581815},\n  url          = {https://doi.org/10.5281/zenodo.7581815}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/andreped/GradientAccumulator",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "gradient-accumulator",
    "package_url": "https://pypi.org/project/gradient-accumulator/",
    "platform": null,
    "project_url": "https://pypi.org/project/gradient-accumulator/",
    "project_urls": {
      "Homepage": "https://github.com/andreped/GradientAccumulator"
    },
    "release_url": "https://pypi.org/project/gradient-accumulator/0.3.1/",
    "requires_dist": [
      "tensorflow",
      "tensorflow-addons"
    ],
    "requires_python": ">=3.6",
    "summary": "Package for gradient accumulation in TensorFlow",
    "version": "0.3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16612335,
  "releases": {
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b78c766352961306c60048e87dcc2e16ee5826f828f57277cced6b853bf6aea1",
          "md5": "4645e7f83f6bd4517a2b92833a575397",
          "sha256": "4da1819797b3dc8d899035f22b1e260a3bc0f0936af9f015fdb1ca7b310f94f9"
        },
        "downloads": -1,
        "filename": "gradient-accumulator-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "4645e7f83f6bd4517a2b92833a575397",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 7255,
        "upload_time": "2022-06-05T12:59:34",
        "upload_time_iso_8601": "2022-06-05T12:59:34.678159Z",
        "url": "https://files.pythonhosted.org/packages/b7/8c/766352961306c60048e87dcc2e16ee5826f828f57277cced6b853bf6aea1/gradient-accumulator-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "20814b02c76e7a74c3b915563a0753c5be09659bc3e0e09659a5b2eb583284fb",
          "md5": "e9dd45eef5a95a8af0027ceb50edc3c3",
          "sha256": "b031ac8c81284e6778610fc44b76473213de53c10a142ab4093162408cc5ea4f"
        },
        "downloads": -1,
        "filename": "gradient-accumulator-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "e9dd45eef5a95a8af0027ceb50edc3c3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 7504,
        "upload_time": "2022-06-05T14:31:23",
        "upload_time_iso_8601": "2022-06-05T14:31:23.177104Z",
        "url": "https://files.pythonhosted.org/packages/20/81/4b02c76e7a74c3b915563a0753c5be09659bc3e0e09659a5b2eb583284fb/gradient-accumulator-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f969a8bd73b0449c2dbe6d0b1d209591db84a2dc2f12d6d024830e6607059add",
          "md5": "27b2038f973f9dcec66965f33473671b",
          "sha256": "d9302b01d1d96fd03cc7e248ec48daecb8c1f39ace0d9257cd1418886facfceb"
        },
        "downloads": -1,
        "filename": "gradient_accumulator-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "27b2038f973f9dcec66965f33473671b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 10000,
        "upload_time": "2022-06-20T15:54:18",
        "upload_time_iso_8601": "2022-06-20T15:54:18.065770Z",
        "url": "https://files.pythonhosted.org/packages/f9/69/a8bd73b0449c2dbe6d0b1d209591db84a2dc2f12d6d024830e6607059add/gradient_accumulator-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "99fadef33e49e72a898bcf3ab951f42e6e45a1de2ee606fd45db9d57e6a43237",
          "md5": "9104c2b66ceef80cd875a8fd77682a1b",
          "sha256": "3139c80012025d5d9a6cafd9ea41230093948085e14d05f2b0642cce2fa868af"
        },
        "downloads": -1,
        "filename": "gradient_accumulator-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9104c2b66ceef80cd875a8fd77682a1b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 14105,
        "upload_time": "2022-06-24T16:34:10",
        "upload_time_iso_8601": "2022-06-24T16:34:10.235307Z",
        "url": "https://files.pythonhosted.org/packages/99/fa/def33e49e72a898bcf3ab951f42e6e45a1de2ee606fd45db9d57e6a43237/gradient_accumulator-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb0dab34a9216e0b85d1c81c38fdb2d5a537e100aeeac892053949ef33f341be",
          "md5": "808dad8ef9de0c9550c70a0f161b51bb",
          "sha256": "682c9b8011c33bd495bf825ed08f93e352e03c6af241f90da53ec4ca53ff7a38"
        },
        "downloads": -1,
        "filename": "gradient_accumulator-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "808dad8ef9de0c9550c70a0f161b51bb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 10215,
        "upload_time": "2022-08-25T17:32:46",
        "upload_time_iso_8601": "2022-08-25T17:32:46.871091Z",
        "url": "https://files.pythonhosted.org/packages/bb/0d/ab34a9216e0b85d1c81c38fdb2d5a537e100aeeac892053949ef33f341be/gradient_accumulator-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "40d51f42a066755bbfc053320f025bc223a0436a4c38b51775c79992f2aac3e2",
          "md5": "7b1e73bced277bef5133e222de8e9661",
          "sha256": "c29dec0501f341f5a37d466aefa28cb630bbcbdae64ec4a993ed3dcf9d4b115b"
        },
        "downloads": -1,
        "filename": "gradient-accumulator-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "7b1e73bced277bef5133e222de8e9661",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 6524,
        "upload_time": "2022-11-08T23:48:17",
        "upload_time_iso_8601": "2022-11-08T23:48:17.169688Z",
        "url": "https://files.pythonhosted.org/packages/40/d5/1f42a066755bbfc053320f025bc223a0436a4c38b51775c79992f2aac3e2/gradient-accumulator-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "db6d3a05785162f6aca32a07bda952162ffccd069b724fa583737745d1cde6bd",
          "md5": "f32e24c2c3e2de4b640f15cc28de59ba",
          "sha256": "e58170e8ed858633510545ff4f31df54f9a4ed9f5285dd3a670dc0a78a002512"
        },
        "downloads": -1,
        "filename": "gradient_accumulator-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f32e24c2c3e2de4b640f15cc28de59ba",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 13876,
        "upload_time": "2023-01-29T17:17:32",
        "upload_time_iso_8601": "2023-01-29T17:17:32.599851Z",
        "url": "https://files.pythonhosted.org/packages/db/6d/3a05785162f6aca32a07bda952162ffccd069b724fa583737745d1cde6bd/gradient_accumulator-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d967c4a05bb9770f2f45e5040210cf6c9cefa504bc71b18f7624d4074ec4b252",
          "md5": "9244f4146703dc670be08a71eef6a419",
          "sha256": "1be029eb9038888eabfde621cf6b6ec1602eab55cb349e707318a335b4b9e75e"
        },
        "downloads": -1,
        "filename": "gradient-accumulator-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "9244f4146703dc670be08a71eef6a419",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 13071,
        "upload_time": "2023-01-29T17:17:33",
        "upload_time_iso_8601": "2023-01-29T17:17:33.738947Z",
        "url": "https://files.pythonhosted.org/packages/d9/67/c4a05bb9770f2f45e5040210cf6c9cefa504bc71b18f7624d4074ec4b252/gradient-accumulator-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7c9c2ae03611fd34ee4bb83c16bf7d1d6e8700c338ee3f338769c63f0c58f2a2",
          "md5": "4e084b028f77e36efa6ad44d59d68fed",
          "sha256": "bb6003a6a0bd12551fd36cc0f221634d17bba5b43827f97d1501afcff6331185"
        },
        "downloads": -1,
        "filename": "gradient_accumulator-0.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4e084b028f77e36efa6ad44d59d68fed",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 26741,
        "upload_time": "2023-01-29T22:30:01",
        "upload_time_iso_8601": "2023-01-29T22:30:01.017442Z",
        "url": "https://files.pythonhosted.org/packages/7c/9c/2ae03611fd34ee4bb83c16bf7d1d6e8700c338ee3f338769c63f0c58f2a2/gradient_accumulator-0.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "00889676e08ace14396a7f060d1b66fad7b001c93c9324e3d719ec174c0842df",
          "md5": "6161136518c01adbbbbf26aab837b005",
          "sha256": "f0cd92d0e95082dc36190bcfefad71a2890a16d33b1f74e6e787301898b6879c"
        },
        "downloads": -1,
        "filename": "gradient-accumulator-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "6161136518c01adbbbbf26aab837b005",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 14808,
        "upload_time": "2023-01-29T22:30:02",
        "upload_time_iso_8601": "2023-01-29T22:30:02.931719Z",
        "url": "https://files.pythonhosted.org/packages/00/88/9676e08ace14396a7f060d1b66fad7b001c93c9324e3d719ec174c0842df/gradient-accumulator-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7c9c2ae03611fd34ee4bb83c16bf7d1d6e8700c338ee3f338769c63f0c58f2a2",
        "md5": "4e084b028f77e36efa6ad44d59d68fed",
        "sha256": "bb6003a6a0bd12551fd36cc0f221634d17bba5b43827f97d1501afcff6331185"
      },
      "downloads": -1,
      "filename": "gradient_accumulator-0.3.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4e084b028f77e36efa6ad44d59d68fed",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 26741,
      "upload_time": "2023-01-29T22:30:01",
      "upload_time_iso_8601": "2023-01-29T22:30:01.017442Z",
      "url": "https://files.pythonhosted.org/packages/7c/9c/2ae03611fd34ee4bb83c16bf7d1d6e8700c338ee3f338769c63f0c58f2a2/gradient_accumulator-0.3.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "00889676e08ace14396a7f060d1b66fad7b001c93c9324e3d719ec174c0842df",
        "md5": "6161136518c01adbbbbf26aab837b005",
        "sha256": "f0cd92d0e95082dc36190bcfefad71a2890a16d33b1f74e6e787301898b6879c"
      },
      "downloads": -1,
      "filename": "gradient-accumulator-0.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "6161136518c01adbbbbf26aab837b005",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 14808,
      "upload_time": "2023-01-29T22:30:02",
      "upload_time_iso_8601": "2023-01-29T22:30:02.931719Z",
      "url": "https://files.pythonhosted.org/packages/00/88/9676e08ace14396a7f060d1b66fad7b001c93c9324e3d719ec174c0842df/gradient-accumulator-0.3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}