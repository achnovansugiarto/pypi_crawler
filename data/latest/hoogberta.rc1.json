{
  "info": {
    "author": "Peerachet Porkaew",
    "author_email": "peerachet.porkaew@nectec.or.th",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Text Processing",
      "Topic :: Text Processing :: General"
    ],
    "description": "# HoogBERTa\n\nThis repository includes the Thai pretrained language representation (HoogBERTa_base) and the fine-tuned model for multitask sequence labeling.\n\n# Installation\n\n```\n$ python setup.py install\n```\n\nTo download model, use\n\n```\n>>> import hoogberta\n>>> hoogberta.download() # or hoogberta.download('/home/user/.hoogberta/')\n```\n\n# Usage\n\nsee test.py\n\n\n# Documentation\n\nTo annotate POS, NE and cluase boundary, use the following commands\n\n```python\nfrom hoogberta.multitagger import HoogBERTaMuliTaskTagger\ntagger = HoogBERTaMuliTaskTagger(cuda=False) # or cuda=True\noutput = tagger.nlp(\"วันที่ 12 มีนาคมนี้ ฉันจะไปเที่ยววัดพระแก้ว ที่กรุงเทพ\")\n```\n\nPlease give the \"base path\" parameter if you have changed the \"models\" directory to a different location than the current one, for example. \n\n```python\ntagger = HoogBERTaMuliTaskTagger(cuda=False,base_path=\"/home/user/.hoogberta/\" ) \n```\n\nThe output is a list of annotations (token, POS, NE, MARK). \"MARK\" is annotation for a single white space that can be PUNC (not clause boundary) or MARK (clause boundary). Note that, for clause boundary classification, the current pretrained model works well with inputs containing two clauses. If you want a more precise result, we recommend running tagger.nlp iteratively.\n\nTo extract token features, based on the RoBERTa architecture, use the following commands\n\n```python\nfrom hoogberta.encoder import HoogBERTaEncoder\nencoder = HoogBERTaEncoder(cuda=False) # or cuda=True\ntoken_ids, features = encoder.extract_features(\"วันที่ 12 มีนาคมนี้ ฉันจะไปเที่ยววัดพระแก้ว ที่กรุงเทพ\")\n```\n\nFor batch processing,\n\n```python\ninputText = [\"วันที่ 12 มีนาคมนี้\",\"ฉันจะไปเที่ยววัดพระแก้ว ที่กรุงเทพ\"]\ntoken_ids, features = encoder.extract_features_batch(inputText)\n```\n\nTo use HoogBERTa as an embedding layer, use\n\n```python\ntokens, features = encoder.extract_features_from_tensor(token_ids) # where token_ids is a tensor with type \"long\".\n```\n\n# Citation\n\nPlease cite as:\n\n``` bibtex\n@inproceedings{porkaew2021hoogberta,\n  title = {HoogBERTa: Multi-task Sequence Labeling using Thai Pretrained Language Representation},\n  author = {Peerachet Porkaew, Prachya Boonkwan and Thepchai Supnithi},\n  booktitle = {The Joint International Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP 2021)},\n  year = {2021},\n  address={Online}\n}\n```\n\nDownload full-text [PDF](https://drive.google.com/file/d/1jXYscGOeUASBJI9cMfj2sAzthEFPp9Gr/view?usp=sharing)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/lstnlp/HoogBERTa/archive/refs/tags/v0.1.1.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/lstnlp/HoogBERTa",
    "keywords": "HoogBERTa: Multi-task Sequence Labeling using Thai Pretrained Language Representation",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hoogberta",
    "package_url": "https://pypi.org/project/hoogberta/",
    "platform": "",
    "project_url": "https://pypi.org/project/hoogberta/",
    "project_urls": {
      "Download": "https://github.com/lstnlp/HoogBERTa/archive/refs/tags/v0.1.1.tar.gz",
      "Homepage": "https://github.com/lstnlp/HoogBERTa"
    },
    "release_url": "https://pypi.org/project/hoogberta/0.1.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "HoogBERTa: Multi-task Sequence Labeling using Thai Pretrained Language Representation",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12352615,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "da27edcca992ce5d6224dd3b37879b20de2151491dc1396a37371ad096e19739",
          "md5": "061ce44646907549596b29030edc7f19",
          "sha256": "8d04ade710a8440d360f79bf13e8a38fff47eda64b626975e3041269775e74fb"
        },
        "downloads": -1,
        "filename": "hoogberta-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "061ce44646907549596b29030edc7f19",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4818,
        "upload_time": "2021-12-19T09:37:44",
        "upload_time_iso_8601": "2021-12-19T09:37:44.790594Z",
        "url": "https://files.pythonhosted.org/packages/da/27/edcca992ce5d6224dd3b37879b20de2151491dc1396a37371ad096e19739/hoogberta-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "da27edcca992ce5d6224dd3b37879b20de2151491dc1396a37371ad096e19739",
        "md5": "061ce44646907549596b29030edc7f19",
        "sha256": "8d04ade710a8440d360f79bf13e8a38fff47eda64b626975e3041269775e74fb"
      },
      "downloads": -1,
      "filename": "hoogberta-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "061ce44646907549596b29030edc7f19",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 4818,
      "upload_time": "2021-12-19T09:37:44",
      "upload_time_iso_8601": "2021-12-19T09:37:44.790594Z",
      "url": "https://files.pythonhosted.org/packages/da/27/edcca992ce5d6224dd3b37879b20de2151491dc1396a37371ad096e19739/hoogberta-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}