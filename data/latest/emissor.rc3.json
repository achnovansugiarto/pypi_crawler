{
  "info": {
    "author": "CLTL",
    "author_email": "piek.vossen@vu.nl",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Multimodal Interaction Data Representation\n\nWe propose a generic and simple structure for representing multimodal interaction data. \nThis data can be derived from human-human, human-agent and agent-agent interactions, \nwhere agents can be robots or virtual agents. Our motivation for doing this is that \nit can be used to hold, combine and share data across many different experiments and allows to compare these.\nOur motivation for this representation can be found in our seminal paper: Baez et al. 2021 (see references below).\n\nThis README explains the data folder in this repository, which illustrates how to structure data.\nThis data can be rendered by interacting systems that record the interaction and it can be annotated \nwith interpretations (by people and/or systems). In the **representation** module of this repository, \nwe provide python classes that represent the data elements, which are used to create and load data. \nWe also provide a basic annotation tool for creating and annotating scenarios. \nIn the future, we will release here public data sets used in other experiments converted to our proposed format.\nThis data can also be loaded and annotated.\n\nThe formats and classes for representing data are derived from presentations that were developed \nin various others projects and combined in the [NewsReader](www.newsreader-project.eu): the Simple Event Model (SEM, Hage et al. 2011), \nthe NewsReader Annotation Format (NAF, Fokkens et al, 2014), the Grounded Annotation Framework (GAF, Fokkens et al, 2013, 2014) \nand its successor the Grounded Annotation and Source Perspective model (GRaSP, Son et al, 2016, Fokkens et al, 2017). \nThe core idea behind the latter two is that we create a relation between a signal and the interpretation of that signal. \nThese interpretations are seen as annotations that express a relation between a **segment** of the signal \n(e.g. a bounding box in an image or an offset position and length in a text) and some interpretation label defined in another framework. \nWe use the Simple Event Model or **SEM**  to model the interpretation of situations depicted in the multimodal signal. \nThese situations are represented in RDF as instances of people, objects, relations and properties. \nAnnotations relate these instances to specific segments in the signals.\n\nThis README explains the data laysers and representations in more detail and is further divided into the following subsections:\n<ol>\n<li>Overall data view\n<li>Scenario structure\n<li>Context\n<li>Content\n</li>Annotation\n</ol>\n\n## 1. Overall data view\n\nFigure-1 shows an overview of the different data layers in our representation. From the top, we see different layers for segments,\nrulers, containers. A segment can consists of smaller segments and there are different subtypes\nof segments per modality of the data. A ruler is made up of a specific unit of segments, which can be used to index a signal as a set of\nsegments with a particular granularity. Again, we can have subtypes of rulers for each modality. Next, we have containers for each modality.\nContainers ground the data in a spatial and temporal world and can be used to define the order of segments, as in a sequence, \nor a position of regions.\n\nA scenario inherits from both a temporal and spatial container. It is further defined with specific atttributes and the signals\nof a certain modality. These signals consist of segments. Segments that are annotated are mentions. The annotation relates a segment to an interpretation.\nSome of these annotations are instances in the model of the world and others are labels that represent concepts. Here we show different types of\nconcepts, such as tokens, faces, and named entity expressions, and types of instances, such as objects, friends and persons.\n\n![Entity relationship diagram](doc/Datarepresentation.png \"Overview of data elements and relations\")\nFigure-1: Overview of data elements and relations.\n\n \n## 2. Scenario structure\nWe consider an interaction as a scenario. Scenarios are stored as subfolders within the data folder. \nWithin a scenario folder, we store multimodal data in four media subfolders as separate files: text, video, image, audio. \nFurthermore, JSON files for each modality define the metadata and the annotations. There is one JSON file per modality.\nThe JSON file contain meta data on the source, creation time, the scenario it is part of, the owner, license, etc. The annotations\nin the JSON file define a segment in a specific data file and the interpretation label of the segment, e.g. a person, object, an emotion,\na part-of-speech, named-entity-expressions, etc. A specific folder \"rdf\" contains the RDF triples extracted from the annotated signals.\nFor example, an utterance in a conversation may mention somebody's age, which yields an RDF triple with the person's URI as the subject, \nthe has-age property and the actual age as a value.\n\nFinally, there is a separate JSON file with meta data on the complete scenario. This scenario JSON defines the temporal and spatial ruler\nwithin which the scenario is located (date, begin and end time, geo-location, place-name), the participants of the interaction \n(e.g. the agent and the human speaker) and any other people and objects that participate in the scene.\nThis scenario JSON file has the same name as the folder name of the scenario. \n\n**An example**\nAssume our scenario folder has the name \"my-first-scenario\". This is how its structure could look like:\n\n```\nmy-first-scenario\n\tmy-first-scenario.json --> overall data on the scenario\n\n\ttext.json -->  meta data and annotations on the conversation and segments within, typically the utterances of each turn form a unit\n\tvideo.json --> meta data and annotations of video fragments and segments within\n\taudio.json --> meta data and annotations of audio fragments and segments within\n\timage.json --> meta data and annotations of images and segments within\n    \n    text\n        #### conversations in text\n\t    conversation-as-text.csv --> csv file with utterances from conversations that take place in this scenario in text format\n\n    image\n        #### stills from the video signals\n        image1.jpg --> images representing stills of situations, possibly drawn from a video\n\t    image2.jpg\n\t    image3.jpg\n  \n    video\n        #### video shoot of the scenario\n        interaction-video.mpeg4 --> video with interaction, either the agent view or another camera view\n\n    audio\n        #### audio files possibly representing speech\n        audio1.wav --> audio files representing sound events, possibly speech acts or utterances \n\t    audio2.wav\n\t    audio3.wav\n\n    rdf\n        #### \n        some-triples.trig\n```\n\n## 2. Context\nThe file \"my-first-scenario.json\" describes the scenario in terms of meta data using standard data categories (e.g. Dublin core and CMDI) \nbut also the 'spatial and temporal containers' within which the scenario takes place.\n\n<ol>\n    <li>Spatial container: geo-location and coordinates that define the area, possibly the name that identifies the area\n    <li>Temporal container: date and begin and end time within which events take place\n</ol>\n\nThe spatial and temporal containers define the primary context for interpreting data in the scenario. For example, knowing the date\nand the location, a system can infer it is winter or summer, guess what the weather is like, whether it is morning or evening, or\nthat you birthday is soon.\n\nIn addition, the scenario can also have a specification of the participants, such as:\n\n<ol>\n    <li>Identity of the agent\n    <li>Identity of the speaker\n    <li>Any other people present and their spatial orientation\n    <li>Objects and their spatial orientation\n</ol>\n\nFinally, the JSON file of the scenario provides an overview of all the data files in the folder and their grounding to the spatial and temporal containers. \n\n## 3. Content\nThe content of the scenario is represented by a series of files in the scenario folder representing the data in different modalities. \nIn the above example, we have separate files for the conversations, a video stream of the interaction, images of scenarios, and audio files. \nEach modality has a JSON file that describes the data that contain signals and any interpretation of the signal in the form of an annotation.\nAny signal is grounded in the spatial and temporal container using specific data elements in the JSON file. \n\nAlthough the data can be streamed as in video and audio, any system needs to define units within the stream to interpret states \nand changes between these states. Therefore, we can not only represent a scenario by the video but also through \na collection of stills in the form of images taken at different time points, as a collection of audio files for speech interaction \nor as the transcribed text of the audio to represent a dialogue. \n\nThrough the spatial and temporal grounding of each of these data files (treated as a signal), \nthey can be organised through a a two dimensional matrix (T x M), where T is the temporal ruler segmenting time in Tn units and M \nis a series of modalities with data files at the time points in the ruler. \nThe next example shows such a Matrix with a temporal ruler for 6 time points and 4 modalities of signals grounded to these units:\n\n```\ntime | video | audio | text  | image |\n-------------------------------------\n1:02 | ...   |       |       |       |\n1:03 | ...   | wav   |       | jpg   |\n1:04 | ...   |       | utter | jpg   |\n1:05 | ...   | wav   | utter |       |\n1:06 | ...   |       |       | jpg   |\n1:07 | ...   | wav   | utter |       |\n```\n\nWe assume that the video is a continuous stream from begin to end which is not cut to the time points. \nHowever, the other modalities fill separate slots at the time points, where we do not necessarily have data at each time point. \nThe temporal ruler (the first column) aligns the different signals across the modalities. \nhis temporal ruler can have any granularity, in this example it is by minutes.\n\n## 4. Segments, rulers and containers\nThe maximal segment of a data file for a modality is the actual data file itself. The JSON file for that modality defines for every data file\nthe modality, the temporal ruler identity in which it is grounded and the start and end point of the maximal segment it represents.\nBelow we show the JSON structure for one **image** file, where the start and end are represented here in milliseconds.\n\n```example\n\"modality\": \"IMAGE\",\n    \"time\": {\n        \"type\": \"TemporalRuler\",\n        \"container_id\": \"test-scenario-2\",\n        \"start\": 1603139705,\n        \"end\": 1603140000\n    },\n    \"files\": [\n        \"data/test-scenes/test-scenario-2/image/piek-sunglasses.jpg\"\n    ],\n```\nWithin the overall segment of the grounded image file, we can define smaller spatial segments by annotation.\nFor images such annotations could be linked to bounding boxes defined in the image, as shown next, where the bounds\nindicate the coordinates that make up this segment and two annotations are provided, one for the person identified and one\nfor the emotion expressed by the face:\n\n```example\n    \"mentions\": [\n        {\n            \"segment\": {\n                \"type\": \"MultiIndex\",\n                \"container_id\": \"0c1b7ffd-d22b-41c5-a55d-0f4a16b9ad89\",\n                \"bounds\": [\n                    [\n                        10,\n                        521\n                    ],\n                    [\n                        15,\n                        518\n                    ]\n                ]\n            },\n            \"annotations\": [\n                {\n                    \"type\": \"PersonAnnotation\",\n                    \"value\": \"leolaniWorld:piek\",\n                    \"id\": \"e136ee3c-ccaa-456d-9124-bc1af60ad424#PERSON1\",\n                    \"source\": \"face_recognition\",\n                    \"confidence\": 0.76,\n                    \"timestamp\": 1604957866.524172\n                }, {\n                    \"type\": \"EmotionAnnotation\",\n                    \"value\": \"ANGRY\",\n                    \"id\": \"e136ee3c-ccaa-456d-9124-bc1af60ad424#EMO1\",\n                    \"source\": \"annotator_1\",\n                    \"confidence\": 0.76,\n                    \"timestamp\": 1604957866.5242481\n                }\n            ]\n        }\n\n        .... etc....\n    ]\n```\n\nWe can have any number of segments with any number of annotations defined for any number of image files. \nAnnotated segments are listed as mentions in the JSON file.\n\nThe JSON for **text** data has a similar structure for the data file except that we have a single CSV file with all the text utterances rather\nthan a separate data file for each utterance. This is just for pragmatic reasons to make it easier to process Natural Language data.\nAs a result of that, we separate the data by the rows and columns in the CSV file as shown in the JSON fragment below:\n\n```example\n{\n    \"modality\": \"TEXT\",\n    \"time\": {\n        \"type\": \"TemporalRuler\",\n        \"container_id\": \"test_scenario\",\n        \"start\": 1603139850,\n        \"end\": 1603149890\n    },\n    \"files\": [\n        \"data/test-scenes/test_scenario/text/chat1.csv#1#1\"\n    ],\n```\n\nWe see that the file \"chat1.csv\" which is in the **text** folder is indexed with \"#1#1\", which refers to the second row and the second\ncolumn of the csv file that contains the text of an utterance (#0#0 would refer to the header and the first column). \nOther columns can be used to represent the speaker (column 0 in our example) of the utterance and the\nstart and end time of speaking (possibly derived from an audio file). The complete conversation for a scenario can thus \nbe represented through a single CSV file with all utterances, identifiers and temporal grounding on separate rows.\n\nNote: In the case of text, it is also possible to represent the data only in the JSON file and not separately in a CSV file.\n\nNote: Text as data can be derived from the audio data, in which case it is a form of annotation, or it can be raw data (without audio)\nfor example taken from chat platforms, forums or novels.\n\nAn annotation of the above text fragment is shown below. The segment is defined by the start and end offset position in the text\nutterance from row 1 and column 1 in the CSV file. The annotation of the segment shown here defines the offset range as a Token which\nisolated the word \"This\".\n\n```example\n    \"mentions\": [\n        {\n            \"segment\": {\n                \"type\": \"Index\",\n                \"container_id\": \"265a5bd0-a8b7-4de6-9f4d-2c4d8d200f70\",\n                \"start\": 0,\n                \"end\": 4\n            },\n            \"annotations\": [\n                {   \"type\": \"Token\",\n                    \"value\": \"This\",\n                    \"id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t1\",\n                    \"source\": \"treebank_tokenizer\",\n                    \"timestamp\": 1604957866.557157\n                }\n            ]\n        }\n        .... etc....\n    ]\n```\nMore details on the annotations are given in the next section.\n\n## 5. Annotations\nAs explained above, annotations define a relation between a segment and an interpretation. Each annotation has the following attributes:\n\n<ul>\n<li> type: kind of annotation\n<li> value: the actual label, which can be a JSON structure or some defined label or identifier\n<li> source: name of the software or person that created the annotation\n<li> timestamp: indicate when the annotation was created</li>\n<li> (OPTIONAL) id: local identifier that differentiates annotations within a JSON file</li>\n</ul>\n\nThe above examples illustrate these attributes and possible values.\n\nThe id attribute holds a unique identifiers for each annotation. This allows us to build annotations on top of other annotations, as\nNLP pipelines, modules typically take the output of one annotation as the input for the next annotation \n(following the layered annotation framework as defined by Ide and Romary 2007). \nFor example, we can first define the tokens of a text as segments grounded through\noffsets and next refer to these tokens to add another annotation. \nThis is shown in the next example, in which a PersonAnnotation is given\nfor a segment that is defined elsewhere as the token \"My\":\n\n```example\n        {\n            \"segment\": {\n                \"type\": \"Index\",\n                \"container_id\": \"265a5bd0-a8b7-4de6-9f4d-2c4d8d200f70\",\n                \"start\": 0,\n                \"end\": 1\n            },\n            \"annotations\": [\n                {   \"type\": \"Token\",\n                    \"value\": \"My\",\n                    \"id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t1\",\n                    \"source\": \"treebank_tokenizer\",\n                    \"confidence\": 0.76,\n                    \"timestamp\": 1604957866.557157\n                }\n            ]\n        },\n        {\n            \"segment\": {\n                \"type\": \"AtomicRuler\",\n                \"container_id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t1\"\n            },\n\n            \"annotations\": [\n                {\n                    \"type\": \"PersonAnnotation\",\n                    \"value\": \"leolaniWorld:piek\",\n                    \"source\": \"entity_linking\",\n                    \"confidence\": 0.76,\n                    \"timestamp\": 1604957866.524172\n                },\n            ]\n        }\n```\n\nSimilarly, we can provide a whole set of segments by listing the identifiers that represent the mention of an annotation.\nIn the next example, we show a set of four segments that have been annotated as representing a \"Claim\" by a \"textToTriple\" module:\n\n```example\n       {\n            \"segment\": {\n                \"type\": \"AtomicRuler\",\n                \"container_id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t1\",\n                \"container_id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t2\",\n                \"container_id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t3\",\n                \"container_id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t4\",\n                \"container_id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t5\"\n            },\n            \"annotations\": [\n                {\n                    \"type\": \"Claim\",\n                    \"value\": \"leolaniWorld:piek-daughter-niqee\",\n                    \"source\": \"textToTriple\",\n                    \"confidence\": 0.76,\n                    \"timestamp\": 1604957866.524172\n                }\n            ]\n        }\n```\n\n\n### 5.1 Mentions\nThe annotation labels can represent any type of interpretation, ranging from part-of-speech, syntactic dependencies, pixel colour, pitch, loudness,\nshape, emotions, people, objects or events. Since our models need to relate interpretations across modalities, \nsome of these annotations identify instances of people and objects \ndepicted in the visual world. Consider the following examples: \n\n* \"That is my son sitting next to me\"\n* \"Sam is eating a sandwich\"\n\nAn annotation of these utterances could define the tokens \"my son\" as making reference to a person who is a male child of me (the speaker). \nSimilarly, the token \"Sam\" can be annotated as the name of a person. \n\nSimilarly, we can annotate certain areas in images as representing people of certain age or gender, surrounded by objects that are annotated \nwith object labels. By annotating segments in the signal with interpretation labels, we indicate that these segment **mention** things in signals.\n\n### 5.2. Identities\nIt is however not enough to mark \"my son\" as making reference to a person or \"Sam\" as a named entity expression. \nWe also need to link these expressions to the actual people in our shared world. \nThe GAF/GRaSP framework (Fokkens et al, 2013, 2017) therefore uses so-called **grasp:denotes** and **grasp:denotesBy** relations to mentions\nand identifiers in a semantic model of the world. Following Semantic Web practices and standards, individuals are represented \nthrough unique resource identifiers or URIs (also called IRIs). In the above example, we have seen the annotation of the Token \"my\" \nby making reference to the value \"leolaniWorld:piek\". The latter being short-cut identifier for an instance within the name-space \"leolaniWorld\".\nAny name space and any identifier will do here. This could also have been a DBPedia URI or any other identifier in \nthe Semantic Web Linked Open Data Cloud.\n\n{\n            \"segment\": {\n                \"type\": \"AtomicRuler\",\n                \"container_id\": \"2ac58d89-76e8-41c5-8997-cd36e38fab9e#t9\"\n            },\n\n            \"annotations\": [\n                {\n                    \"type\": \"PersonAnnotation\",\n                    \"value\": \"leolaniWorld:sam\",\n                    \"source\": \"entity_linking\",\n                    \"confidence\": 0.76,\n                    \"timestamp\": 1604957866.524172\n                },\n            ]\n        }\n        \n\nIdentity across these URIs establishes **co-reference** relations across different mentions. If the tokens \"piek\" and \"my\" are\nannotated with the same URI they automatically become co-referential. By following the same procedure for other modalities, \nwe can thus annotate visual data with similar identifiers, creating cross-modal co-reference.\n\n### 5.3. Properties and relations as RDF triples\nBy using URIs for referential annotations of segments, we can also represent properties and relations of and between identified people and objects.\nThese properties and relations, such as 'wearing a hat\", \"\"eating a sandwich\" or \"throwing a ball\", can be expressed in the utterances and in images.\nWhen an annotator (human or machine) detects these properties, the multimodal signals yield triples (in RDF format) that represent these as\ninterpretations of states of the (multimodal) world. Such RDF triples consist of a subject URI, a predicate or property and an object URI or value, \nas shown below (leaving out the name spaces):\n\n```example\n    :piek   :has-gender         :male\n    :piek   :is-father-of       :sam\n    :piek   :shows-emotion      \"smile\"\n```\n\nThrough the referential grounding discussed in the previous section, we can use multimodal signals to generate triples expressing properties and\nrelations across different modalities and different data files. Shared identifiers (URIs) aggregate of these eproperties and relations\nresulting in a world model over time. The triples stored in a data base (triple store) \nlikewise reflect this cumulation over time, while each triple can still be grounded to a segment in a modality. By faceting the triples in time, they\nalso model changes of states, such as smiling first and being angry next.\n\nThe GRaSP framework considers the triples depicted in segments of any modality as claims. Different sources and different perceptions can make different claims\nabout the same triple relation. For example, source could dispute the gender of \":piek\" or his emotion at any moment in time.\n\n### 5.4 Claims\nIn our triple representation, we therefore used **named-graphs** that embed the extracted triples within \"claims\".\nBelow, we show a representation of triples in which claims are listed through URIs as well such that we can express \nproperties about them and also embed world relations and properties within a named claim-graph:\n\n```example\n\n    leolaniWorld:piek-from-schaesberg {\n\t    :piek :born-in :schaesberg .\n    }\n\n\n    leolaniWorld:piek-from-landgraaf {\n\t    :piek :born-in :landgraaf .\n    }\n\n    leolaniWorld:Claims {\n\n\tleolaniWorld:piek-from-schaesberg a grasp:Statement ;\n\t                                  grasp:denotedBy leolaniTalk:2ac58d89-76e8-41c5-8997-cd36e38fab9e#t9 .\n\n  \tleolaniWorld:piek-from-landgraaf  a grasp:Statement ;\n                                      grasp:denotedBy leolaniTalk:2ac58d89-76e8-41c5-8997-cd36e38fab9e#t45 .\n\n    } \n\n    leolaniTalk:Perspectives {\n        leolaniTalk:2ac58d89-76e8-41c5-8997-cd36e38fab9e#t9 \n                                      a grasp:Mention ;\n                                      grasp:wasAttributedTo :piek .\n\n        leolaniTalk:2ac58d89-76e8-41c5-8997-cd36e38fab9e#t45  \n                                      a grasp:Mention ;\n                                      grasp:wasAttributedTo :carl.\n    }\n\n```\n\nThis RDF representation uses name spaces: **leolaniWorld** and **leolaniTalk**, that are defined in a robot platform\ncalled **Leolani**: http://makerobotstalk.nl. The implementation is further defined in the following Github: https://github.com/cltl/pepper\n  \nIn this example, there are two claims: one claim with the URI *leolaniWorld:piek-from-schaesberg* that embeds the triple \nthat \":piek\" was born in the city \"schaesberg\" and another claim with the URI *leolaniWorld:piek-from-landgraaf* \nwhich embeds the triple that he was born in \"landgraaf\". \nWe furthermore see that these claims are instances of the class \"grasp:Statement\" and they\nare grounded through \"gaf:denotedBy\" links to identifiers grouped within the \"leolaniTalk:Perspectives\" subgraph, where\nthey are defined as \"mentions\" and have a grasp:wasAttrubutedTo relation to the speaker.\n\nIf we return to the JSON structure for our text fragment above, we see in the last examples that we can annotate tokens with claim\nidentifiers that are also used in the triples that explicitly model the claims. In that way, we do not need to represent the triples\nexplicitly in the JSON file, but we can annotate segments through URIs that are further defined in the associated triple file.\n\n## References\n```\n\ninproceedings@{emissor:2021,\n        title = {EMISSOR: A platform for capturing multimodal interactions as Episodic Memories and Interpretations with Situated Scenario-based Ontological References},\n        author = {Selene Baez Santamaria and Thomas Baier and Taewoon Kim and Lea Krause and Jaap Kruijt and Piek Vossen},\n        url={https://mmsr-workshop.github.io/programme},\n        booktitle = {Processings of the MMSR workshop \"Beyond Language: Multimodal Semantic Representations\", IWSC2021},\n        year = {2021}\n\n\n@incollection{ide2007towards,\n  title={Towards International Standards for Language Resources Nancy Ide and Laurent Romary},\n  author={Ide, Nancy and Romary, Laurent},\n  booktitle={Evaluation of text and speech systems},\n  pages={263--284},\n  year={2007},\n  publisher={Springer}\n}\n\n@inproceedings{fokkens2014naf,\n  title={NAF and GAF: Linking linguistic annotations},\n  author={Fokkens, Antske and Soroa, Aitor and Beloki, Zuhaitz and Ockeloen, Niels and Rigau, German and Van Hage, Willem Robert and Vossen, Piek},\n  booktitle={Proceedings 10th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic Annotation},\n  pages={9--16},\n  year={2014}\n}\n\n@article{fokkensnaf,\n  title={NAF: the NLP Annotation Format Technical Report NWR-2014-3},\n  author={Fokkens, Antske and Soroa, Aitor and Beloki, Zuhaitz and Rigau, German and van Hage, Willem Robert and Vossen, Piek and Donostia, Basque Country}\n}\n\n@inproceedings{fokkens2013gaf,\n  title={GAF: A grounded annotation framework for events},\n  author={Fokkens, Antske and Van Erp, Marieke and Vossen, Piek and Tonelli, Sara and Van Hage, Willem Robert and Serafini, Luciano and Sprugnoli, Rachele and Hoeksema, Jesper},\n  booktitle={Workshop on Events: Definition, Detection, Coreference, and Representation},\n  pages={11--20},\n  year={2013}\n}\n\n@inproceedings{van2016grasp,\n  title={GRaSP: A Multilayered Annotation Scheme for Perspectives},\n  author={van Son, Chantal and Caselli, Tommaso and Fokkens, Antske and Maks, Isa and Morante, Roser and Aroyo, Lora and Vossen, Piek},\n  booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},\n  pages={1177--1184},\n  year={2016}\n}\n\n@article{van2011design,\n  title={Design and use of the Simple Event Model (SEM)},\n  author={Van Hage, Willem Robert and Malais{\\'e}, V{\\'e}ronique and Segers, Roxane and Hollink, Laura and Schreiber, Guus},\n  journal={Journal of Web Semantics},\n  volume={9},\n  number={2},\n  pages={128--136},\n  year={2011},\n  publisher={Elsevier}\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/cltl/EMISSOR",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "emissor",
    "package_url": "https://pypi.org/project/emissor/",
    "platform": null,
    "project_url": "https://pypi.org/project/emissor/",
    "project_urls": {
      "Homepage": "https://github.com/cltl/EMISSOR"
    },
    "release_url": "https://pypi.org/project/emissor/0.0.dev6/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Representation of multi-modal datasets",
    "version": "0.0.dev6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15240936,
  "releases": {
    "0.0.dev4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d9354ebf92dd5a8b89cf80f4a846851f9af5c213ff156a44a3c026dea2f741ce",
          "md5": "4d5f5afd1bc47ba12b224e3f04c914d0",
          "sha256": "9fc75c7b7388855196c7536a1883e4e05a3f978b639dde314b7e2cc11612e9e0"
        },
        "downloads": -1,
        "filename": "emissor-0.0.dev4.tar.gz",
        "has_sig": false,
        "md5_digest": "4d5f5afd1bc47ba12b224e3f04c914d0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 345474,
        "upload_time": "2021-10-06T09:44:03",
        "upload_time_iso_8601": "2021-10-06T09:44:03.449305Z",
        "url": "https://files.pythonhosted.org/packages/d9/35/4ebf92dd5a8b89cf80f4a846851f9af5c213ff156a44a3c026dea2f741ce/emissor-0.0.dev4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.dev5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7c8c410aa796ca7ffd10f39fd832b864fbfd9227354c204e48d6b2c5bd3b8c1b",
          "md5": "4a20c35040d3e9af1e6498248538250c",
          "sha256": "0c76a1cbe55b3fefd7cc07c2becf45fb2aa9041afc6bb3545c3cb99b35dd3c15"
        },
        "downloads": -1,
        "filename": "emissor-0.0.dev5.tar.gz",
        "has_sig": false,
        "md5_digest": "4a20c35040d3e9af1e6498248538250c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 345022,
        "upload_time": "2021-10-27T10:59:23",
        "upload_time_iso_8601": "2021-10-27T10:59:23.809834Z",
        "url": "https://files.pythonhosted.org/packages/7c/8c/410aa796ca7ffd10f39fd832b864fbfd9227354c204e48d6b2c5bd3b8c1b/emissor-0.0.dev5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.dev6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c830a8ecbbf652bb75ab8caa3c46026f6a48da3db047b7ce9d8296b7c1f07f19",
          "md5": "66a47dc38466704de38ae056383adce3",
          "sha256": "932e0a16e9e90372f4da3621a8cdf555349986ded699d4a7ed71fa0961aedf1a"
        },
        "downloads": -1,
        "filename": "emissor-0.0.dev6.tar.gz",
        "has_sig": false,
        "md5_digest": "66a47dc38466704de38ae056383adce3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 347391,
        "upload_time": "2022-09-28T12:35:37",
        "upload_time_iso_8601": "2022-09-28T12:35:37.342765Z",
        "url": "https://files.pythonhosted.org/packages/c8/30/a8ecbbf652bb75ab8caa3c46026f6a48da3db047b7ce9d8296b7c1f07f19/emissor-0.0.dev6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c830a8ecbbf652bb75ab8caa3c46026f6a48da3db047b7ce9d8296b7c1f07f19",
        "md5": "66a47dc38466704de38ae056383adce3",
        "sha256": "932e0a16e9e90372f4da3621a8cdf555349986ded699d4a7ed71fa0961aedf1a"
      },
      "downloads": -1,
      "filename": "emissor-0.0.dev6.tar.gz",
      "has_sig": false,
      "md5_digest": "66a47dc38466704de38ae056383adce3",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 347391,
      "upload_time": "2022-09-28T12:35:37",
      "upload_time_iso_8601": "2022-09-28T12:35:37.342765Z",
      "url": "https://files.pythonhosted.org/packages/c8/30/a8ecbbf652bb75ab8caa3c46026f6a48da3db047b7ce9d8296b7c1f07f19/emissor-0.0.dev6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}