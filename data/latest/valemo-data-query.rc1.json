{
  "info": {
    "author": "Arthur_LAFFARGUE",
    "author_email": "arthur.laffargue@valemo.fr",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# VALEMO_DATA_QUERY\n## _Outils de requêtes de données sur Python pour Valemo_\n\n## Fonctionnalités\n\n#### azure_datalake_gen2\n\n- Rechercher les fichiers .csv dans une architecture partitionnée du datalake gen2 ; \n- Filtrer les partitions et sélectionner les dossiers ; \n- Télécharger les .csv depuis le datalake et les agréger en dataframes Pandas ; \n- Transformer les données et préparer des analyses ; \n\n#### user_credentials\n\n- enregistrer vos informations de connexion et les importer facilement ;\n \n## Installation\nInstaller _valemo-data-query_ via command prompt\n```sh\npip install --upgrade valemo-data-query\n```\n\nDésinstaller _valemo-data-query_ via command prompt\n```sh\npip uninstall valemo-data-query\n```\n\n_valemo-data-query_ requiert : \n- Un abonnement Azure. Consultez la page d'obtention d’un essai gratuit d’[Azure](https://azure.microsoft.com/fr-fr/free/) ;\n- Un compte de stockage doté d’un espace de noms hiérarchique activé. Pour créer un test, suivez [ces](https://docs.microsoft.com/fr-fr/azure/storage/blobs/create-data-lake-storage-account) instructions ; \n- Pour plus d'informations sur la gestion de datalake gen2 de microsoft via Python voir cette [page](https://docs.microsoft.com/fr-fr/azure/storage/blobs/data-lake-storage-directory-file-acl-python) ; \n- Installation à jour de Python 3.8 ou supérieure. Distribution [Anaconda](https://www.anaconda.com/products/individual) recommandée. ; \n- Librairie Pandas version 1.1.4 ou supérieure ; \n- Librairie Numpy version 1.21.3 ou supérieure ;\n- Librairie Azure (azure-storage-file-datalake) version 12.5.0 ;  \n\n## Exemple : enregistrer ses connexions \n```python\nfrom valemo_data_query import user_credentials\n\ncredentials = user_credentials()\n\n# Déclarer une nouvelle connexion \ndatalake_1 = {\"account\" : \"datalake_name\",\n                \"key\" : \"password\"}\ncredentials.add_datasource(datalake_1 ,\"datalake_1\")\n\ndatalake_2 = {\"account\" : \"datalake_name_2\",\n                \"key\" : \"password\"}\ncredentials.add_datasource(datalake_2 ,\"datalake_2\")\n\n# Récupérer une connexion \ndataconnexion = credentials.get_credentials\ndatalake_1 = dataconnexion[\"datalake_1\"]\n\n#Supprimer toutes les connexions \ncredentials.clear_credentials()\n\n```\n\n## Exemple : Azure datalake - gérer les partitions\nAprès avoir configurer ses connexions au datalake Azure gen2, la fonction _read_csv_partition_ reconstruit l'arborescence des fichiers csv sans lire le contenu. Lors de cette phase des filtres peuvent être appliqués basés sur la reconnaissance des hiérarchies _projet=_, _year=_ et _month=_. La méthode statique _get_file_partition_df_ renvoie les partitions des fichiers csv. Un deuxième filtre basé sur ce dataframe est applicable avec _apply_user_partition_filter_ avant la lecture  et la concaténation des données via la méthode _concatenate_pandas_.\n\n```python\nfrom valemo_data_query import azure_datalake_gen2, user_credentials\ndatalake_connexion = user_credentials().get_credentials[\"datalake_1\"]\n\n\"\"\"\nOn recherche les fichiers csv dans le dossier : \n    \"detailed/wind/metrics/csv/project=X/year=Y/month=Z/...\"\n    Avec X,Y,Z : \n     X = [\"BAM\",\"DSN\",\"PUDP\",\"PAACS\"]\n     Y = [\"2021\",\"2022\"]\n     Z : Mois [de 06 à 10]\n\"\"\"\nbasepath = \"detailed/wind/metrics/csv/\"\nazure_query = azure_datalake_gen2(datalake_connexion,basepath)\nazure_query.filter_partitionProject([\"BAM\",\"DSN\",\"PUDP\",\"PAACS\"])\nazure_query.filter_partitionYear([\"2021\",\"2022\"])\n# Pour les mois on pourrait spécifier le filtre via .filter_partitionMonth\n# ou utiliser .apply_user_partition_filter basé sur le dataframe des \n# partitions renvoyé par la méthode .get_file_partition_df\n\nazure_query.read_csv_partition() #recherche de l'arborescence des fichiers csv\npartition_df = azure_query.get_file_partition_df\n\"\"\"\n    filepath               partition  ...                              year       month\n0   detailed/wind/metr...  project=BAM/year=2021/month=10/day=01  ...  year=2021  month=10\n1   detailed/wind/metr...  project=BAM/year=2021/month=10/day=02  ...  year=2021  month=10\n2   detailed/wind/metr...  project=BAM/year=2021/month=10/day=03  ...  year=2021  month=10\n3   detailed/wind/metr...  project=BAM/year=2021/month=10/day=04  ...  year=2021  month=10\n\"\"\"\nmonth_number = partition_df[\"month\"].str.replace(\"month=\",\"\").astype(int) \nfilter_month = (month_number>=6)&(month_number<=10)\nazure_query.apply_user_partition_filter(filter_month)\n\nazure_query.concatenate_pandas(aggregation_level=['year','month']) #concaténer par mois\nazure_query.write_concatenated_dataset_csv(\"my/local_path/\",\n                                        file_prefixe=\"Wind\")\n# génère plusieurs fichiers \n# CONCAT DATASET ... \n# my/local_path/Wind_year=2021_month=06.csv\n# my/local_path/Wind_year=2021_month=07.csv\n# ... \n```\n\n## Exemple : Azure datalake - retravailler les données\nLorsque l'arborescence des fichiers ne nécessite pas de filtre via _apply_user_partition_filter_, il est possible de rechercher les partitions et lire les fichiers csv trouvés dynamiquement en utilisant _concatenate_pandas_ sans _read_csv_partition_. Les fonctions _select_columns_ et _dataframe_transformation_ permettent de gérer les dataframes lus de manière dynamique à chaque lecture de fichiers. \nIl est possible de différer ces opérations après la lecture des données (après concatenate_pandas). L'option _return_copy_ renvoie une copie des transformations appliquées en différé.  \n```python\nfrom valemo_data_query import azure_datalake_gen2, user_credentials\nimport pandas as pd\ndatalake_connexion = user_credentials().get_credentials[\"datalake_1\"]\n\n\nbasepath = \"consolidated/pv/metrics/subs/csv/\"\nprojets = [\"Project1\",\"Project2\"]\nannees = [\"2021\"]\nmois = [\"09\",\"10\"]\nquery_azure = azure_datalake_gen2(credentials_dict,basepath)\n\n# Filtrage\nquery_azure.filter_partitionProject(projets)\nquery_azure.filter_partitionYear(annees)\nquery_azure.filter_partitionMonth(mois)\n\n\n#Transformation des données et sélection\n\nselect_col = [\"projectCode\",\"localTimestamp\",\"activePower\",\"gii\"]\n\ndef daily_metrics_func(data:pd.DataFrame):\n    \"\"\"\n    agrégation des données par jour, par parc\n    \"\"\"\n    data[\"day\"] = data[\"localTimestamp\"].astype(\"datetime64[D]\")\n    aggdata = data.drop(columns='localTimestamp').\\\n                   groupby([\"projectCode\",\"day\"]).\\\n                   sum().\\\n                   rename(columns={\"activePower\":\"activeEnergy\",\n                                   \"irradiance\":\"irradianceEnergy\"}).\\\n                   reset_index(drop=False)\n    aggdata[[\"activeEnergy\",\n            \"irradianceEnergy\"]] = aggdata[[\"activeEnergy\",\n                                            \"irradianceEnergy\"]]/(1000*6)\n    return aggdata\n\n#Les transformations + sélections sont appliquées à la lecture. \nquery_azure.select_columns(select_col)\nquery_azure.dataframe_transformation(daily_metrics_func)\n\n\n#Lecture des fichiers\n\nquery_azure.concatenate_pandas(aggregation_level=['project','year',\"month\"],\n                                convert_dtypes=True)\n\nquery_azure.write_concatenated_dataset_csv(\"dataset/\")\n\n\n\n```\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "GNU",
    "maintainer": "",
    "maintainer_email": "",
    "name": "valemo-data-query",
    "package_url": "https://pypi.org/project/valemo-data-query/",
    "platform": "",
    "project_url": "https://pypi.org/project/valemo-data-query/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/valemo-data-query/1.0.1/",
    "requires_dist": [
      "pandas",
      "azure-storage-file-datalake",
      "numpy",
      "parquet"
    ],
    "requires_python": "",
    "summary": "",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12046558,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5b4435f630617d33993c9cd9295d9032c3cff242710caafcf1031a481aa6fc2a",
          "md5": "e2bac3dae8b895ce29471e56c1ab64ce",
          "sha256": "cb07cd97cca4fe405a6319658a92f6467bd7258b28195625179fc137075c26c9"
        },
        "downloads": -1,
        "filename": "valemo_data_query-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e2bac3dae8b895ce29471e56c1ab64ce",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 23373,
        "upload_time": "2021-11-17T09:35:52",
        "upload_time_iso_8601": "2021-11-17T09:35:52.871268Z",
        "url": "https://files.pythonhosted.org/packages/5b/44/35f630617d33993c9cd9295d9032c3cff242710caafcf1031a481aa6fc2a/valemo_data_query-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5b4435f630617d33993c9cd9295d9032c3cff242710caafcf1031a481aa6fc2a",
        "md5": "e2bac3dae8b895ce29471e56c1ab64ce",
        "sha256": "cb07cd97cca4fe405a6319658a92f6467bd7258b28195625179fc137075c26c9"
      },
      "downloads": -1,
      "filename": "valemo_data_query-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e2bac3dae8b895ce29471e56c1ab64ce",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 23373,
      "upload_time": "2021-11-17T09:35:52",
      "upload_time_iso_8601": "2021-11-17T09:35:52.871268Z",
      "url": "https://files.pythonhosted.org/packages/5b/44/35f630617d33993c9cd9295d9032c3cff242710caafcf1031a481aa6fc2a/valemo_data_query-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}