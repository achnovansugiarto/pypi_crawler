{
  "info": {
    "author": "Patrice Lopez",
    "author_email": "patrice.lopez@science-miner.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: POSIX",
      "Programming Language :: Python :: 3.5"
    ],
    "description": "[![PyPI version](https://badge.fury.io/py/article_dataset_builder.svg)](https://badge.fury.io/py/article_dataset_builder)\n[![License](http://img.shields.io/:license-apache-blue.svg)](http://www.apache.org/licenses/LICENSE-2.0.html)\n\n# Open Access scholar PDF harvester and ingester\n\nPython utility for harvesting efficiently a large Open Access collection of scholar PDF (fault tolerant, can be resumed, parallel download and ingestion) and for transforming them into structured XML adapted to text mining and information retrieval applications.\n\nInput currently supported:\n\n- list of DOI in a file, one DOI per line\n- list of PMID in a file, one DOI per line\n- list of PMC ID in a file, one DOI per line\n- metadata csv input file from [CORD-19 dataset](https://pages.semanticscholar.org/coronavirus-research), see the CORD-19 result section below to see the capacity of the tool to get more full texts and better data quality that the official dataset\n\nThe harvesting is following fair-use (which means that it covers non re-sharable articles) and it is exploiting various Open Access sources. The harvesting thus should result in a close-to-optimal discovery of OA full texts. For instance, from the same CORD-19 metadata file, the tool can harvest 35.5% more usable full text than available in the CORD-19 dataset (140,322 articles with at least one usable full text versus 103,587 articles with at least one usable full text for the CORD-19 dataset version 2020-09-11), see statistics [here](https://github.com/kermitt2/article_dataset_builder#results-with-cord-19). \n\nTo do:\n- Apache Airflow for more complex task workflow and automated periodic incremental harvesting\n- Consolidate/resolve bibliographical references obtained via Pub2TEI\n- Upload harvested resources on SWIFT object storage (OpenStack) in addition to local and S3 storage\n\n## What this doing is doing\n\n- Perform some metadata enrichment/agregation via [biblio-glutton](https://github.com/kermitt2/biblio-glutton) & [CrossRef web API](https://github.com/CrossRef/rest-api-doc) and output consolidated metadata in a json file \n\n- Harvest PDF from the specification of the article set (list of strong identifiers or basic metadata provided in a csv file), typically PDF available in Open Access PDF via the [Unpaywall API](https://unpaywall.org/products/api) and some heuristics\n\n- Optionally, perform [Grobid](https://github.com/kermitt2/grobid) full processing of PDF (including bibliographical reference consolidation and OA access resolution of the cited references), converting them into structured XML TEI\n\n- For PMC files (Open Access set only), harvest also XML JATS (NLM) files and perform optionally a conversion into XML TEI (same TEI customization as Grobid) via [Pub2TEI](https://github.com/kermitt2/Pub2TEI)\n\nIn addition, optionally: \n\n- Generate thumbnails for article (based on the first page of the PDF), small/medium/large \n\n- Upload the generated dataset on S3, instead of the local file system\n\n- Generate json PDF annotations (with coordinates) for inline reference markers and bibliographical references (see [here](https://grobid.readthedocs.io/en/latest/Grobid-service/#apireferenceannotations))\n\n## Requirements\n\nThe utility has been tested with Python 3.5+. It is developed for a deployment on a POSIX/Linux server (it uses `imagemagick` as external process to generate thumbnails and `wget`). An S3 account and bucket must have been created for non-local storage of the data collection. \n\nIf you wish to generate thumbnails for article as they are harvested, install `imagemagick`:\n\n- on Linux Ubuntu:\n\n```console\nsudo apt update\nsudo apt build-dep imagemagick\n```\n\n- on macos:\n\n```console\nbrew install libmagic\n```\n\n## Installation\n\n### Article dataset builder\n\n#### Local install\n\nFor installing the tool locally and use the current master version, get the github repo:\n\n```sh\ngit clone https://github.com/kermitt2/article_dataset_builder\ncd delft\n```\n\nCreate a virtual environment and install the Python mess:\n\n```console\nvirtualenv --system-site-packages -p python3.8 env\nsource env/bin/activate\npython3 -m pip install -r requirements.txt\n```\n\nFinally install the project, preferably in editable state\n\n```sh\npython3 -m pip  install -e .\n```\n\n#### Using PyPI package\n\nPyPI packages are available for stable versions. Latest stable version is `0.2.1`:\n\n```\npython3 -m pip install article-dataset-builder==0.2.1\n```\n\n### Third party web services\n\nDepending on the process you are interested to apply to the articles as they are harvested, the following tools need to be installed/accessed and running, with access information specified in the configuration file (`config.json`):\n\n- [biblio-glutton](https://github.com/kermitt2/biblio-glutton), for metadata retrieval and aggregation\n\nIt should be possible to use the public demo instance of [biblio-glutton](https://github.com/kermitt2/biblio-glutton), as default configured in the `config.json` file (the tool scale at more than 6000 queries per second). However in combaination with [Grobid](https://github.com/kermitt2/grobid), we strongly recommand to install a local instance, because the online public demo will not be able to scale and won't be reliable given that it is more or less always overloaded. \n\n- [Grobid](https://github.com/kermitt2/grobid), for converting PDF into XML TEI\n\nThis tool requires Java 8 to 11. \n\n- [Pub2TEI](https://github.com/kermitt2/Pub2TEI), for converting PMC XML files into XML TEI\n\nAs [biblio-glutton](https://github.com/kermitt2/biblio-glutton) is using dataset dumps, there is a gap of several months in term of bibliographical data freshness. So, complementary, the [CrossRef web API](https://github.com/CrossRef/rest-api-doc) and [Unpaywall API](https://unpaywall.org/products/api) services are used to cover the gap. For these two services, you need to indicate your email in the config file (`config.json`) to follow the etiquette policy of these two services. If the configuration parameters for `biblio-glutton` are empty, only the CrossRef REST API will be used. \n\nAn important parameter in the `config.json` file is the number of parallel document processing that is allowed, this is specified by the attribute `batch_size`, default value being `10` (so 10 documents max downloaded in parallel with distinct threads/workers and processed by Grobid in parallel). You can set this number according to your available number of threads. If you do not apply Grobid on the downloaded PDF, you can raise the `batch_size` parameter significantly, for example to `50`, which means then 100 paralell download. Be careful that parallel download from the same source might be blocked or might result in black-listing for some OA publisher sites, so it might be better to keep `batch_size` reasonable even when only donwloading.  \n\nFor downloading preferably the fulltexts available at PubMed Central from the NIH site (PDF and JATS XML files) rather than on publisher sites, the Open Access list file from PMC that maps PMC identifiers to PMC resource archive URL will be downloaded automatically. You can also download it manually as follow:\n\n```console\ncd resources\nwget https://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_file_list.txt\n```\n\nWhen this file is available under `resources/oa_file_list.txt`, an index will be built at first launch and the harvester will prioritize the access to the NIH resources. \n\n## Docker\n\nTBD\n\n## Usage\n\n```\npython3 article_dataset_builder/harvest.py --help\nusage: harvest.py [-h] [--dois DOIS] [--cord19 CORD19] [--pmids PMIDS] [--pmcids PMCIDS]\n                  [--config CONFIG] [--reset] [--reprocess] [--thumbnail] [--annotation]\n                  [--diagnostic] [--dump] [--grobid]\nScholar PDF harvester and converter\n\noptional arguments:\n  -h, --help       show this help message and exit\n  --dois DOIS      path to a file describing a dataset articles as a simple list of DOI (one per\n                   line)\n  --cord19 CORD19  path to the csv file describing the CORD-19 dataset articles\n  --pmids PMIDS    path to a file describing a dataset articles as a simple list of PMID (one\n                   per line)\n  --pmcids PMCIDS  path to a file describing a dataset articles as a simple list of PMC ID (one\n                   per line)\n  --config CONFIG  path to the config file, default is ./config.json\n  --reset          ignore previous processing states, and re-init the harvesting process from\n                   the beginning\n  --reprocess      reprocessed existing failed entries\n  --thumbnail      generate thumbnail files for the front page of the harvested PDF\n  --annotation     generate bibliographical annotations with coordinates for the harvested PDF\n  --diagnostic     perform a full consistency diagnostic on the harvesting and transformation\n                   process\n  --dump           write all the consolidated metadata in json in the file\n                   consolidated_metadata.json\n  --grobid         process downloaded files with Grobid to generate full text XML\n```\n\nFill the file `config.json` with relevant service and parameter url.\n\nFor example to harvest a list of DOI (one DOI per line):\n\n```console\npython3 article_dataset_builder/harvest.py --dois test/dois.txt \n```\n\nSimilarly for a list of PMID or PMC ID with Grobid conversion of the PDF as the are downloaded:\n\n```console\npython3 article_dataset_builder/harvest.py --pmids test/pmids.txt --grobid\npython3 article_dataset_builder/harvest.py --pmcids test/pmcids.txt --grobid\n```\n\nFor example for the [CORD-19 dataset](https://pages.semanticscholar.org/coronavirus-research), you can use the [metadata.csv](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html) (last tested version from 2020-06-29) file by running: \n\n```console\npython3 harvest.py --cord19 metadata.csv  \n```\n\nAn harvesting will generate a JSONL mapping file with the UUID associated with to the Open Access full text resource and the main identifiers of the entries under the data path specified in the configuration file and named `map.json`. The harvesting uploads full text files, converted tei.xml files and other optional files either in the local file system (under data_path indicated in the `config.json` file) or on a S3 bucket if the fields are filled in `config.json`. \n\nYou can set a specific config file name with `--config` :\n\n```console\npython3 harvest.py --cord19 metadata.csv --config my_config.json    \n```\n\nTo resume an interrupted processing, **simply re-run the same command**. \n\nTo re-process the failed articles of an harvesting, use:\n\n```console\npython3 harvest.py --reprocess --config my_config.json  \n```\n\nTo reset entirely an existing harvesting and re-start an harvesting from zero (it means all the already harvested PDF will be deleted!): \n\n```console\npython3 harvest.py --cord19 metadata.csv --reset --config my_config.json  \n```\n\nTo download full texts (PDF and JATS/NLM) with GROBID processing, use `--grobid` parameter:\n\n```console\npython3 harvest.py --cord19 metadata.csv --config my_config.json --grobid\n```\n\nTo create a full dump of the consolidated metadata for all the scholar articles (including the UUID identifier and the state of processing), add the parameter `--dump`:\n\n```console\npython3 harvest.py --dump --config my_config.json  \n```\n\nThe generated metadata file is named `consolidated_metadata.json`.\n\nFor producing the thumbnail images of the article first page, use `--thumbnail` argument. This option requires `imagemagick` installed on your system and will produce 3 PNG files of size height x150, x300 and x500. These thumbnails can be interesting for offering a preview to an article for an application using these data.\n\n```console\npython3 harvest.py --cord19 metadata.csv --thumbnail --config my_config.json  \n```\n\nFor producing PDF annotations in JSON format corresponding to the bibliographical information (reference markers in the article and bibliographical references in the bibliographical section), use the argument `--annotation`. See more information about these annotations [here](https://grobid.readthedocs.io/en/latest/Coordinates-in-PDF/). They allow to enrich the display of PDF, and make them more interactive. \n\n```console\npython3 harvest.py --cord19 metadata.csv --annotation --config my_config.json  \n```\n\nFinally you can run a short diagnostic/reporting on the latest harvesting by adding `--diagnostic` (combined to any previous harvesting command line, or just as unique parameter, to make a diagnostic to the realized harvesting):\n\n```console\npython3 harvest.py --diagnostic --config my_config.json  \n```\n\n## Generated files\n\n### Default \n\nStructure of the generated files for an article having as UUID identifier `98da17ff-bf7e-4d43-bdf2-4d8d831481e5`\n\n```\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5.pdf\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5.json\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5.grobid.tei.xml\n```\n\nThe `*.json` file above gives the consolidated metadata of the harvested item, based on CrossRef entries, with additional information provided by `biblio-glutton`, status of the harvesting and GROBID processing and UUID (field `id`). \n\nOptional additional files:\n\n```\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5.nxml\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5.pub2tei.tei.xml\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5-ref-annotations.json\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5-thumb-small.png\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5-thumb-medium.png\n98/da/17/ff/98da17ff-bf7e-4d43-bdf2-4d8d831481e5/98da17ff-bf7e-4d43-bdf2-4d8d831481e5-thumb-large.png\n```\n\nFile with extension `*.nxml` are JATS XML files downloaded from PubMed Central (Open Access set only). File with extension `*.pub2tei.tei.xml` are converted JATS file into TEI XML (same XML full text format as Grobid TEI) with Pub2TEI. File with suffix `*-ref-annotations.json` are JSON PDF coordinates of structures recognized by GROBID. Files with extension `*.png` are thumbnail images of the first page of the harvested PDF.\n\nThe UUID identifier for a particular article is given in the generated `map.json` file under the data path, associated to the every document identifiers. The UUID is also given in the `consolidated_metadata.csv` file (obtained with option `--dump`, see above).\n\n## CORD-19 example\n\nThe tool can realize its own harvesting and ingestion of CORD-19 papers based on an official version of the `metadata.csv` file of CORD-19. It provides two main advantages as compared to the official CORD-19 dataset:\n\n- Harvest around 35% more usable full texts for the CORD-19 `2020-09-11` version, around 25% more for the CORD-19 `2021-07-26` version\n- Structure the full texts into high quality TEI format (both from PDF and from JATS), with much more information than the JSON format of the CORD-19 dataset. The JATS conversion into TEI in particular does not lose any information from the original XML file.\n\nBe sure to install the latest available version of GROBID, many improvements have been added to the tool regarding the support of bioRxiv and medRxiv preprints.\n\nTo launch the harvesting (see above for more details):\n\n```console\npython3 harvest.py --cord19 metadata.csv --grobid\n```\n\nFor the CORD-19 dataset, for simplification and clarity, we reuse the `cord id` which is a random string 8 characters in `[0-9a-z]`: \n\n```\n00/0a/je/vz/000ajevz/000ajevz.pdf\n00/0a/je/vz/000ajevz/000ajevz.json\n00/0a/je/vz/000ajevz/000ajevz.grobid.tei.xml\n```\n\nOptional additional files:\n\n```\n00/0a/je/vz/000ajevz/000ajevz.nxml\n00/0a/je/vz/000ajevz/000ajevz.pub2tei.tei.xml\n00/0a/je/vz/000ajevz/000ajevz-ref-annotations.json\n00/0a/je/vz/000ajevz/000ajevz-thumb-small.png\n00/0a/je/vz/000ajevz/000ajevz-thumb-medium.png\n00/0a/je/vz/000ajevz/000ajevz-thumb-large.png\n```\n\nFor harvesting and structuring, you only need the metadata file of the CORD-19 dataset, available at: \n\n  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/<date_iso_str>/metadata.csv\n\nwhere `<date_iso_str>` should match a release date indicated on the [CORD-19 release page](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html).\n\nFor running the coverage script, which compares the full text coverage of the official CORD-19 dataset with the one produced by the present tool, you will need the full CORD-19 dataset. \n\n## On harvesting and ingesting the CORD-19 dataset\n\n### Adding a local PDF repository for Elsevier OA COVID papers\n\nThe [CORD-19 dataset](https://pages.semanticscholar.org/coronavirus-research) includes more than 19k articles corresponding to a set of Elsevier articles on COVID-19 [recently put in Open Access](https://www.elsevier.com/connect/coronavirus-information-center). As Unpaywall does not cover these OA articles (on 2020-03-23 at least), you will need to download first these PDF and indicates to the harvesting tool where the local repository of PDF is located: \n\n- download the PDF files on the COVID-19 FTP server: \n\n```console\nsftp public@coronacontent.np.elsst.com\n```\n\nIndicate `beat_corona` as password. See the [instruction page](https://www.elsevier.com/connect/coronavirus-information-center#researchers) in case of troubles. \n\n```console\ncd pdf\nmget *\n```\n\n- indicate the local repository where you have downloaded the dataset in the `config.json` file:\n\n```json\n\"cord19_elsevier_pdf_path\": \"/the/path/to/the/pdf\"\n```\n\nThat's it. The file `./elsevier_covid_map_28_06_2020.csv.gz` contains a map of DOI and PII (the Elsevier article identifiers) for these OA articles. \n\n### Incremental harvesting\n\nCORD-19 is updated regularly. Suppose that you have harvested one release of the CORD-19 full texts and a few weeks later you would like to refresh your local corpus. Incremental harvesting is supported, so only the new entries will be uploaded and ingested. \n\nIf the harvesting was done with one version of the metadata file `metadata-2020-09-11.csv` (from the `2020-09-11` release):\n\n```console\npython3 article_dataset_builder/harvest.py --cord19 metadata-2020-09-11.csv --config my_config.json --grobid\n```\n\nThe incremental update will be realized with a new version of the metadata file simply by specifying it:\n\n```console\npython3 article_dataset_builder/harvest.py --cord19 metadata-2021-03-22.csv --config my_config.json --grobid\n```\n\nThe constraint is that the same data repository path is kept in the config file. The repository and its state will be reused to check if an entry has already been harvested or not.\n\nAs an alternative, it is also possible to point to a local old data directory in the config file, with parameter `legacy_data_path`. Before trying to download a file from the internet, the harvester will first check in this older data directory if the PDF files are not already locally available based on the same identifiers. \n\n### Results with CORD-19\n\nHere are the results regarding the CORD-19 from __2020-09-11__ ([cord-19_2020-09-11.tar.gz](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-09-11.tar.gz)) (4.6GB) to illustrate the interest of this harvester tool. We used the present tool using the CORD-19 metadata file (`metadata.csv`), re-harvested the full texts and converted all into the same target TEI XML format (without information loss with respect to the available publisher XML and GROBID PDF-to-XML conversion). \n\n|   | official CORD-19 | this harvester |\n|---|---|---|\n| total entries | 253,454 | 253,454 | \n| without `cord id` duplicates | 241,335 | 241,335 |\n| without duplicates | - | 161,839 |\n| entries with valid OA URL | - | 141,142 |\n| entries with successfully downloaded PDF | - | 139,565 | \n| entries with structured full texts via GROBID | 94,541 (PDF JSON) | 138,440 (TEI XML) |\n| entries with structured full texts via PMC JATS | 77,115 (PMC JSON) | 104,288 (TEI XML) |\n| __total entries with at least one structured full text__ | __103,587 (PDF JSON or PMC JSON)__ | __140,322 (TEI XML)__ |\n\nOther information for this harvester: \n\n- total OA URL not found or invalid: 20,697 (out of the 161,839 distinct articles)\n- 760 GROBID PDF to TEI XML conversion failures (the average failure on random downloaded scholar PDF is normally around 1%, so we are at 0.5% and this is good)\n- 45 Pub2TEI tranformations (NLM (JATS) -> TEI XML) reported as containing some kind of failure \n\nOther main differences include:\n\n- the XML TEI contain richer structured full text (section titles, notes, formulas, etc.), \n- usage of up-to-date GROBID models for PDF conversion (with extra medRxiv and bioRxiv training data), \n- PMC JATS files conversion with [Pub2TEI](https://github.com/kermitt2/Pub2TEI) (normally without information loss because the TEI custumization we are using superseeds the structures covered by JATS). Note that a conversion from PMC JATS files has been introduced in CORD-19 from version 6. \n- full consolidation of the bibliographical references with publisher metadata, DOI, PMID, PMC ID, etc. when available (if you are into citation graphs)\n- consolidation of article metadata with CrossRef and PubMed aggregations for the entries \n- optional coordinates of structures on the original PDF\n- optional thumbnails for article preview\n\nSome [notes on CORD-19 harvesting](notes-cord19.md), including numbers for update version `2021-07-26`.\n\n## Converting the PMC XML JATS files into XML TEI\n\nAfter the harvesting and processing realised by `article_dataset_builder/harvest.py`, it is possible to convert of PMC XML JATS files into XML TEI. This will provide better XML quality than what can be extracted automatically by Grobid from the PDF. This conversion allows to have all the documents in the same XML TEI customization format. As the TEI format superseeds JATS, there is no loss of information from the JATS file. It requires [Pub2TEI](https://github.com/kermitt2/Pub2TEI) to be installed and the path to Pub2TEI `pub2tei_path` to be set in the `config.json` file of the `article_dataset_builder` project.\n\nTo launch the conversion under the default `data/` directory:\n\n```console\npython3 article_dataset_builder/nlm2tei.py\n```\n\nIf a custom config file and custom `data/` path are used:\n\n```console\npython3 article_dataset_builder/nlm2tei.py --config ./my_config.json\n```\n\nThis will apply Pub2TEI (a set of XSLT) to all the harvested `*.nxml` files and add to the document repository a new file TEI file, for instance for a CORD-19 entry:\n\n```\n00/0a/je/vz/000ajevz/000ajevz.pub2tei.tei.xml\n```\n\nNote that Pub2TEI supports a lot of other publisher's XML formats (and variants of these formats), so the principle and current tool could be used to transform different publisher XML formats into a single one (TEI) - not just NLM/JATS, facilitating and centralizing further ingestion and process by avoiding to write complicated XML parsers for each case. \n\n## Checking CORD-19 dataset coverage\n\nThe following script checks the number of duplicated `cord id` (also done by the normal harvester), but also count the number of articles with at least one JSON full text file:\n\n\n```\nusage: article_dataset_builder/check_cord19_coverage.py [-h] [--documents DOCUMENTS]\n                                [--metadata METADATA]\n\nCOVIDataset harvester\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --documents DOCUMENTS\n                        path to the official CORD-19 uncompressed document dataset\n  --metadata METADATA   path to the CORD-19 CSV metadata file\n```\n\nFor example:\n\n```\npython3 article_dataset_builder/check_cord19_coverage.py --metadata cord-19/2021-03-22/metadata.csv --documents cord-19/2021-03-22/ --config my_config.json\n```\n\nThe path for `--documents` is the path where the folder `document_parses` is located. \n\n\n## Troubleshooting with imagemagick\n\nRecent update (end of October 2018) of imagemagick is breaking the normal conversion usage. Basically the converter does not convert by default for security reason related to server usage. For non-server mode as involved in our module, it is not a problem to allow PDF conversion. For this, simply edit the file `/etc/ImageMagick-6/policy.xml` (or `/etc/ImageMagick/policy.xml`) and put into comment the following line: \n\n```\n<!-- <policy domain=\"coder\" rights=\"none\" pattern=\"PDF\" /> -->\n```\n\n## How to cite \n\nFor citing this software work, please refer to the present GitHub project, together with the [Software Heritage](https://www.softwareheritage.org/) project-level permanent identifier. For example, with BibTeX:\n\n```bibtex\n@misc{articledatasetbuilder,\n    title = {Article Dataset Builder},\n    howpublished = {\\url{https://github.com/kermitt2/article_dataset_builder}},\n    publisher = {GitHub},\n    year = {2020--2023},\n    archivePrefix = {swh},\n    eprint = {1:dir:adc1581a092560c0ac4a82256c0c905859ec15fc}\n}\n```\n\n## License and contact\n\nDistributed under [Apache 2.0 license](http://www.apache.org/licenses/LICENSE-2.0).\n\nMain author and contact: Patrice Lopez (<patrice.lopez@science-miner.com>)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/kermitt2/article_dataset_builder",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "article-dataset-builder",
    "package_url": "https://pypi.org/project/article-dataset-builder/",
    "platform": null,
    "project_url": "https://pypi.org/project/article-dataset-builder/",
    "project_urls": {
      "Homepage": "https://github.com/kermitt2/article_dataset_builder"
    },
    "release_url": "https://pypi.org/project/article-dataset-builder/0.2.3/",
    "requires_dist": null,
    "requires_python": ">=3.5",
    "summary": "Open Access scholar PDF harvester, metadata aggregator and full-text ingester",
    "version": "0.2.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17192215,
  "releases": {
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "45e39212da29e09801c153ef35ed8eec06ee8c40f65e92c816b3db82c7ee364c",
          "md5": "028dc2c0ec180ef79ccb578651ce82c0",
          "sha256": "abe84cb3b6067dd42083e53d561fea36fefcb4a0b9002486a1ae5f3d85c73a31"
        },
        "downloads": -1,
        "filename": "article_dataset_builder-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "028dc2c0ec180ef79ccb578651ce82c0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 42465,
        "upload_time": "2023-03-05T14:49:32",
        "upload_time_iso_8601": "2023-03-05T14:49:32.670827Z",
        "url": "https://files.pythonhosted.org/packages/45/e3/9212da29e09801c153ef35ed8eec06ee8c40f65e92c816b3db82c7ee364c/article_dataset_builder-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "60ca7326aad9a2a7e331a8eb0b96b3ffe1921dc4a7ade19dcff58752b9d5e327",
          "md5": "eeb097d26e19a619a2555cb220f4366c",
          "sha256": "13121bf1b6ace7ff748bd13cc94f20f4d970ad6072ed2a2eb9f044720691e483"
        },
        "downloads": -1,
        "filename": "article_dataset_builder-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "eeb097d26e19a619a2555cb220f4366c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 43525,
        "upload_time": "2023-03-07T14:44:52",
        "upload_time_iso_8601": "2023-03-07T14:44:52.083205Z",
        "url": "https://files.pythonhosted.org/packages/60/ca/7326aad9a2a7e331a8eb0b96b3ffe1921dc4a7ade19dcff58752b9d5e327/article_dataset_builder-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "81086af2ed5bbc2670331daf38e7164918e60369aeae64e7eca0bdcb92094e54",
          "md5": "62d98856eb120d58a70cf782825d00a4",
          "sha256": "ed6c560d43ddf6b607fb97d32e9fe0fa061d99f1be494010b8cc0daffa67eade"
        },
        "downloads": -1,
        "filename": "article_dataset_builder-0.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "62d98856eb120d58a70cf782825d00a4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 43530,
        "upload_time": "2023-03-07T15:09:39",
        "upload_time_iso_8601": "2023-03-07T15:09:39.851562Z",
        "url": "https://files.pythonhosted.org/packages/81/08/6af2ed5bbc2670331daf38e7164918e60369aeae64e7eca0bdcb92094e54/article_dataset_builder-0.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "81086af2ed5bbc2670331daf38e7164918e60369aeae64e7eca0bdcb92094e54",
        "md5": "62d98856eb120d58a70cf782825d00a4",
        "sha256": "ed6c560d43ddf6b607fb97d32e9fe0fa061d99f1be494010b8cc0daffa67eade"
      },
      "downloads": -1,
      "filename": "article_dataset_builder-0.2.3.tar.gz",
      "has_sig": false,
      "md5_digest": "62d98856eb120d58a70cf782825d00a4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 43530,
      "upload_time": "2023-03-07T15:09:39",
      "upload_time_iso_8601": "2023-03-07T15:09:39.851562Z",
      "url": "https://files.pythonhosted.org/packages/81/08/6af2ed5bbc2670331daf38e7164918e60369aeae64e7eca0bdcb92094e54/article_dataset_builder-0.2.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}