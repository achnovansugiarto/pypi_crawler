{
  "info": {
    "author": "Hyunwoong Ko",
    "author_email": "gusdnd852@naver.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.2",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries"
    ],
    "description": "# Asian Bart\n- `asian-bart` is package of Bart model for Asian languages.\n- `asian-bart` supports English, Chinese, Korean, japanese, Total (=ECJK)\n- We made `asian-bart` using [mBart](https://arxiv.org/abs/2001.08210) by embedding layer pruning. \n<br><br><br>\n\n## Installation\n```console\npip install asian-bart\n```\n<br><br>\n\n## Model specification\n- ECJK model\n  - vocab size: 57k\n  - model size: 413M\n  - languages: En, Zh, Ja, Ko\n  - architecture: Transformer 12 Encoder + 12 Decoder\n  - name: `hyunwoongko/asian-bart-ecjk`\n<br><br>  \n- English model\n  - vocab size: 32k\n  - model size: 387M\n  - languages: English (`en_XX`)\n  - architecture: Transformer 12 Encoder + 12 Decoder\n  - name: `hyunwoongko/asian-bart-en`\n<br><br>\n- Chinese model\n  - vocab size: 20k\n  - model size: 375M\n  - languages: Chinese (`zh_CN`)\n  - architecture: Transformer 12 Encoder + 12 Decoder\n  - name: `hyunwoongko/asian-bart-zh`\n<br><br>\n- Japanese model\n  - vocab size: 13k\n  - model size: 368M\n  - languages: Japanese (`ja_XX`)\n  - architecture: Transformer 12 Encoder + 12 Decoder\n  - name: `hyunwoongko/asian-bart-ja`\n <br><br>\n- Korean model\n  - vocab size: 8k\n  - model size: 363M\n  - languages: Korean (`ko_KR`)\n  - architecture: Transformer 12 Encoder + 12 Decoder\n  - name: `hyunwoongko/asian-bart-ko`\n<br><br>\n\n## Usage\n- The `asian-bart` is made using mbart, so you have to follow mbart's input rules:\n  - source: `text` + `</s>` + `lang_code`\n  - target: `lang_code` + `text` + `</s>`\n- For more details, please check the content of the [mbart paper](https://arxiv.org/abs/2001.08210).\n<br><br>\n\n### Usage of tokenizer\n- tokenization of `(single language, single text)`\n```python\n>>> from asian_bart import AsianBartTokenizer\n>>> tokenizer = AsianBartTokenizer.from_pretrained(\"hyunwoongko/asian-bart-ecjk\")\n>>> tokenizer.prepare_seq2seq_batch(\n...     src_texts=\"hello.\",\n...     src_langs=\"en_XX\",\n... )\n```\n```\n{\n  'input_ids': tensor([[37199, 35816,     2, 57521]]), \n  'attention_mask': tensor([[1, 1, 1, 1]])\n}\n```\n<br>\n\n- batch tokenization of `(single language, mutiple texts)`\n```python\n>>> from asian_bart import AsianBartTokenizer\n>>> tokenizer = AsianBartTokenizer.from_pretrained(\"hyunwoongko/asian-bart-ecjk\")\n>>> tokenizer.prepare_seq2seq_batch(\n...     src_texts=[\"hello.\", \"how are you?\", \"good.\"],\n...     src_langs=\"en_XX\",\n... )\n```\n```\n{\n  'input_ids': tensor([[37199, 35816,     2, 57521,     1,     1],\n                       [38248, 46819, 39446, 36209,     2, 57521],\n                       [40010, 39539,     2, 57521,     1,     1]]), \n\n  'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n                            [1, 1, 1, 1, 1, 1],\n                            [1, 1, 1, 1, 0, 0]])\n}\n```\n<br>\n\n- batch tokenization of `(multiple languages, multiple texts)`\n```python\n>>> from asian_bart import AsianBartTokenizer\n>>> tokenizer = AsianBartTokenizer.from_pretrained(\"hyunwoongko/asian-bart-ecjk\")\n>>> tokenizer.prepare_seq2seq_batch(\n...     src_texts=[\"hello.\", \"반가워\", \"你好\", \"こんにちは\"],\n...     src_langs=[\"en_XX\", \"ko_KR\", \"zh_CN\", \"ja_XX\"],\n... )\n```\n```\n{\n  'input_ids': tensor([[37199, 35816, 39539,     2, 57521,     1,     1,     1],\n                       [22880, 49591,  3901,     2, 57523,     1,     1,     1],\n                       [50356,  7929,     2, 57524,     1,     1,     1,     1],\n                       [42990, 19092, 51547, 36821, 33899, 37382,     2, 57522]]), \n\n   'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0],\n                             [1, 1, 1, 1, 1, 0, 0, 0],\n                             [1, 1, 1, 1, 0, 0, 0, 0],\n                             [1, 1, 1, 1, 1, 1, 1, 1]])\n}\n```\n<br>\n\n- seq2seq tokenization of `(source text, target text)`\n```python\n>>> from asian_bart import AsianBartTokenizer\n>>> tokenizer = AsianBartTokenizer.from_pretrained(\"hyunwoongko/asian-bart-ecjk\")\n>>> tokenizer.prepare_seq2seq_batch(\n...     src_texts=\"반가워\",\n...     src_langs=\"ko_KR\",\n...     tgt_texts=\"hello.\",\n...     tgt_langs=\"en_XX\",\n... )\n```\n```\n{\n  'input_ids': tensor([[22880, 49591,  3901,     2, 57523]]), \n  'attention_mask': tensor([[1, 1, 1, 1, 1]]), \n  'labels': tensor([[37199, 35816, 39539,     2, 57521]])\n}\n```\n<br>\n\n- all above batch tokenization settings work the same about target texts\n```python\n>>> from asian_bart import AsianBartTokenizer\n>>> tokenizer = AsianBartTokenizer.from_pretrained(\"hyunwoongko/asian-bart-ecjk\")\n>>> tokenizer.prepare_seq2seq_batch(\n...     src_texts=[\"hello.\", \"반가워\", \"你好\", \"こんにちは\"],\n...     src_langs=[\"en_XX\", \"ko_KR\", \"zh_CN\", \"ja_XX\"],\n...     tgt_texts=[\"hello.\", \"반가워\", \"你好\", \"こんにちは\"],\n...     tgt_langs=[\"en_XX\", \"ko_KR\", \"zh_CN\", \"ja_XX\"],\n... )\n```\n```\n{\n  'input_ids': tensor([[37199, 35816, 39539,     2, 57521,     1,     1,     1],\n                      [22880, 49591,  3901,     2, 57523,     1,     1,     1],\n                      [50356,  7929,     2, 57524,     1,     1,     1,     1],\n                      [42990, 19092, 51547, 36821, 33899, 37382,     2, 57522]]), \n\n  'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0],\n                            [1, 1, 1, 1, 1, 0, 0, 0],\n                            [1, 1, 1, 1, 0, 0, 0, 0],\n                            [1, 1, 1, 1, 1, 1, 1, 1]]), \n\n  'labels': tensor([[37199, 35816, 39539,     2, 57521,     1,     1,     1],\n                    [22880, 49591,  3901,     2, 57523,     1,     1,     1],\n                    [50356,  7929,     2, 57524,     1,     1,     1,     1],\n                    [42990, 19092, 51547, 36821, 33899, 37382,     2, 57522]])\n}\n```\n<br><br>\n\n### Usage of models\n- Interfaces of all functions are the same as mbart model on Huggingface transformers.\n- Here is an example of using a asian bart model. (ecjk model)\n- Other language work the same way. change both model and tokenizer's `from_pretrained`.\n    - English only: `from_pretrained(\"hyunwoongko/asian-bart-en\")`\n    - Chinese only: `from_pretrained(\"hyunwoongko/asian-bart-zh\")`\n    - Japanese only: `from_pretrained(\"hyunwoongko/asian-bart-ja\")`\n    - Korean only: `from_pretrained(\"hyunwoongko/asian-bart-ko\")`\n```python\n# import modules\n>>> import torch\n>>> from asian_bart import AsianBartTokenizer, AsianBartForConditionalGeneration\n>>> from transformers.models.bart.modeling_bart import shift_tokens_right\n\n# create model and tokenizer\n>>> tokenizer = AsianBartTokenizer.from_pretrained(\"hyunwoongko/asian-bart-ecjk\")\n>>> model = AsianBartForConditionalGeneration.from_pretrained(\"hyunwoongko/asian-bart-ecjk\")\n\n# tokenize texts\n>>> tokens = tokenizer.prepare_seq2seq_batch(\n...     src_texts=\"Kevin is the <mask> man in the world.\",\n...     src_langs=\"en_XX\",\n...     tgt_texts=\"Kevin is the most kind man in the world.\",\n...     tgt_langs=\"en_XX\",                  \n... )\n\n>>> input_ids = tokens[\"input_ids\"]\n>>> attention_mask = tokens[\"attention_mask\"]\n>>> labels = tokens[\"labels\"]\n>>> decoder_input_ids = shift_tokens_right(labels, tokenizer.pad_token_id)\n\n# forwarding model for training\n>>> output = model(\n...     input_ids=input_ids,\n...     attention_mask=attention_mask,\n...     decoder_input_ids=decoder_input_ids,\n... )\n\n# compute loss\n>>> lm_logits = outputs[0]\n>>> loss_function = torch.nn.CrossEntropyLoss(\n...     ignore_index=tokenizer.pad_token_id\n... )\n\n>>> loss = loss_function(\n...     lm_logits.view(-1, lm_logits.shape[-1]), \n...     labels.view(-1)\n... )\n\n# generate text\n>>> output = model.generate(\n...     input_ids=input_ids,\n...     attention_mask=attention_mask,\n...     decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"],\n... )\n```\n<br><br>\n\n### Downstream tasks\n- You can train various downstream tasks with asian bart.\n- All interfaces have the same usage as the Huggingface transformers.\n- Supported classes: \n  - `AsianBartTokenizer`\n  - `AsianBartModel`\n  - `AsianBartForCausalLM`\n  - `AsianBartForQuestionAnswering`\n  - `AsianBartForConditionalGeneration`\n  - `AsianBartForSequenceClassification`\n<br><br><br>\n\n## License\n```\nCopyright 2021 Hyunwoong Ko.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/hyunwoongko/asian-bart",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "asian-bart",
    "package_url": "https://pypi.org/project/asian-bart/",
    "platform": "",
    "project_url": "https://pypi.org/project/asian-bart/",
    "project_urls": {
      "Homepage": "https://github.com/hyunwoongko/asian-bart"
    },
    "release_url": "https://pypi.org/project/asian-bart/1.0.2/",
    "requires_dist": [
      "transformers (>=4)",
      "torch",
      "sentencepiece"
    ],
    "requires_python": ">=3",
    "summary": "Asian language bart models (En, Ja, Ko, Zh, ECJK)",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10609326,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "539f2f094144cad64bd5e515e4e1f726c12fad9d2160fe7ad7d484137c1b311e",
          "md5": "7e0ade5d2a5a69699ab3952893ab5f83",
          "sha256": "b9c199e24513f7babfb23fb3bfe299f386659da530cd8b2001f41732e0495797"
        },
        "downloads": -1,
        "filename": "asian_bart-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7e0ade5d2a5a69699ab3952893ab5f83",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 8045,
        "upload_time": "2021-04-01T14:05:55",
        "upload_time_iso_8601": "2021-04-01T14:05:55.579190Z",
        "url": "https://files.pythonhosted.org/packages/53/9f/2f094144cad64bd5e515e4e1f726c12fad9d2160fe7ad7d484137c1b311e/asian_bart-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "41ff2f7659b517978c44cc4fb1c247c3e6d2d76c0466f459b157a43fb6beaaf9",
          "md5": "2182a3c2bd9bc471718a036e23c4358a",
          "sha256": "3cc90067328557ea097e7420eab67c965118c7c63b96d5587cc2c8208b8bbd6f"
        },
        "downloads": -1,
        "filename": "asian_bart-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2182a3c2bd9bc471718a036e23c4358a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 8062,
        "upload_time": "2021-04-03T13:51:37",
        "upload_time_iso_8601": "2021-04-03T13:51:37.855389Z",
        "url": "https://files.pythonhosted.org/packages/41/ff/2f7659b517978c44cc4fb1c247c3e6d2d76c0466f459b157a43fb6beaaf9/asian_bart-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1192a4d6b8c58d735660fbae8061396df4bfa52e57f909e42bec1191ebf0ede2",
          "md5": "13af06634a0805cadcb364c2f092dfe1",
          "sha256": "93a5d5279c95067186eb6f046b962b0bcbf306b8de57a637f2cb60a0048fd6e0"
        },
        "downloads": -1,
        "filename": "asian_bart-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "13af06634a0805cadcb364c2f092dfe1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 8072,
        "upload_time": "2021-06-10T08:20:28",
        "upload_time_iso_8601": "2021-06-10T08:20:28.034912Z",
        "url": "https://files.pythonhosted.org/packages/11/92/a4d6b8c58d735660fbae8061396df4bfa52e57f909e42bec1191ebf0ede2/asian_bart-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1192a4d6b8c58d735660fbae8061396df4bfa52e57f909e42bec1191ebf0ede2",
        "md5": "13af06634a0805cadcb364c2f092dfe1",
        "sha256": "93a5d5279c95067186eb6f046b962b0bcbf306b8de57a637f2cb60a0048fd6e0"
      },
      "downloads": -1,
      "filename": "asian_bart-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "13af06634a0805cadcb364c2f092dfe1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3",
      "size": 8072,
      "upload_time": "2021-06-10T08:20:28",
      "upload_time_iso_8601": "2021-06-10T08:20:28.034912Z",
      "url": "https://files.pythonhosted.org/packages/11/92/a4d6b8c58d735660fbae8061396df4bfa52e57f909e42bec1191ebf0ede2/asian_bart-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}