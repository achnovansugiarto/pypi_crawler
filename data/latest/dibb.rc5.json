{
  "info": {
    "author": "Giuseppe Cuccu",
    "author_email": "giuseppe.cuccu@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# DiBB\n\n_Distributing Black-Box optimization_\n\n## TL;DR\n\nDiBB generates and runs a Partially-Separable, parallelized and distributed version of your favorite Black Box optimization algorithm -- including Evolutionary Algorithms such as Evolution Strategies for continuous optimization.\n\n```bash\n# also installs CMA-ES, a solid choice for the underlying BBO\npip install dibb[cma]`\n```\n\n```python\n# Python\nfrom dibb import DiBB\n# If a Ray cluster with 3 machines is already running, the computation will be\n# distributed as 2 block workers (5 dims each) and 1 head node\n# If no Ray cluster is running, the three processes will spawn locally\ndibb = DiBB(ndims=10, nblocks=2, fit_fn=lambda x: sum(x**2), verbose=True)\ndibb.run(ngens=10)\n```\n\nPaper + bibtex are at the bottom of this file, or check out [Giuse's page](https://exascale.info/giuse).\n\n**Powerusers:** don't miss out on the *Advanced Usage* section below.\n\n\n## Installation\n\nYou can install DiBB from Pypi using `pip install dibb[cma]` or `dibb[lmmaes]`, which installs both DiBB and a solid underlying optimizer ([CMA-ES](https://github.com/CMA-ES/pycma) or [LM-MA-ES](https://github.com/giuse/lmmaes) respectively). Running `pip install dibb[all]` installs all currently available optimizers.  \n\nIf you just `pip install dibb`, it will install only DiBB (and Parameter Guessing); you can then install separately an optimizer of your choice, just make sure that\n[there is already a wrapper available for it](dibb/opt_wrappers/__init__.py) -- if not, [writing your own](dibb/opt_wrappers/README_interface.md) is very easy.\n\nTo contribute, you can also clone the repo from GitHub\n`git clone https://github.com/giuse/dibb`\nthen install it locally with\n`pip install -e <path_to>/dibb`.\n\n## Local usage\nDiBB is compatible with several traditional workflows from the larger family of Black Box Optimization methods. Here's an example:\n\n```python\n# Let's define a simple function to optimize:\nimport numpy as np\ndef sphere(coords): np.dot(coords, coords)\n\n# You can use the classic `fmin`, if you need\nfrom dibb import fmin\nfmin(sphere, np.random.rand(10))\n# ...but that is not particularly interesting\n\n# Let's fire up a couple of *blocks* instead:\nfrom dibb import DiBB\ndibb = DiBB(ndims=10, nblocks=2, fit_fn=sphere, verbose=True)\ndibb.run(ngens=100)\n```\n\nWhen launching DiBB using the last command, you should notice the overhead of Ray starting up with multiple workers to handle the blocks. It is just a few seconds, and already allows you to play with dimensionality `ndims` much larger than you could with CMA-ES alone.\nThe algorithm used is more correctly the partially-separable version of CMA created by DiBB: *PS-CMA-ES*. [Check out the paper](https://exascale.info/assets/pdf/cuccu2022gecco.pdf) for more details.\n\n## Cluster usage\n\nTo enable the full potential of DiBB you should get your hands on a few machines (one per each block, plus one for the head process), then:\n\n- `pip install ray` on each of them\n- Set up basic SSH key-pair authentication (here's a [quick script](https://github.com/giuse/devops/blob/master/pair_ssh_keys.sh)) \n- Mark down the IP addresses\n- Customize a copy of [`ray_cluster_config.yaml`](ray_cluster_config.yaml)\n(instructions inside)\n- Initialize the cluster by running (locally) `ray up ray_cluster_config.yaml`\n\nNow you can run the same code again, only this time it will find and utilize your new cluster rather than launching local processes. Notice the overhead is very small.\n\n```python\n# With a Ray cluster with 3 machines already running, this code will now\n# run two blocks on two machines, plus the head node on a third machine\nfrom dibb import DiBB\ndibb = DiBB(ndims=10, nblocks=2, fit_fn=sphere, verbose=True)\ndibb.run(ngens=100)\n```\n\nA more flexible way to call DiBB with multiple options is to use a `dict` (check `DiBB.defaults` for an example):\n\n```python\nfrom dibb import DiBB\ndibb_config = {\n    'ndims' : 10,\n    'nblocks' : 2,\n    'fit_fn' : sphere,\n    'verbose' : True,\n    'stopping_conditions' : {'ngens' : 100},\n}\ndibb = DiBB(**dibb_config).run()\n```\n\nYou can find the complete list of accepted parameters, their descriptions and default values, in [DiBB's main file](dibb/dibb.py)\n\n## Neuroevolution example in 5 minutess\n\n*Neuroevolution* means training neural networks using evolutionary computation algorithms. This is not a supervised learning technique, so you need no labels and no differentiability. In a RL control problem for example, you can train a neural network to use as the policy (*Direct Policy Search*), and entirely ditch value functions, Markov chains and the whole classic RL framework.\n\nFirst you will need to install the requirements (besides DiBB of course): 1. an optimizer, 2. a neural network and 3. a RL control environment.\n\n```bash\npip install \"dibb[cma] tinynet gym[classic_control]\"\n```\n_(the quotes are here to escape the square parenthesis for `zsh`, which is currently the default shell on Mac)_\n\nThen copy+paste the example below to a `.py` file and run it.\nIt should not take long, even on your local machine -- or you can launch a cluster of 3 machines first (using `ray_cluster_config.yaml`): the example below will run on the cluster with no further changes.\n\n```python\n# INSTALL REQUIREMENTS FIRST: dibb (with optimizer), neural network, RL environment:\n# $ pip install \"dibb[cma] tinynet gym[classic_control]\"\n\nimport numpy as np\nimport ray\nimport tinynet  # https://github.com/giuse/tinynet/\nimport gym      # https://www.gymlibrary.ml/environments/classic_control/\nimport warnings # silence some gym warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning, module='gym')\nfrom dibb import DiBB\n\n# Set up the environment and network\nenv = gym.make(\"CartPole-v1\")\nnactions = env.action_space.n\nninputs = env.reset().size\n# Just 2 neurons (for the Cartpole only has 2 actions: left and right)\n# with linear activation `f(x)=x` are already enough\nnet = tinynet.FFNN([ninputs, nactions], act_fn=lambda x: x)\n\n# The fitness function accumulates the episodic reward (basic gym gameplay)\ndef play_gym(ind, render=False):\n    obs = env.reset(seed=1) # fix random seed: faster training but less generalization\n    net.set_weights(ind) # set the weights into the network\n    score = 0\n    done = False\n    while not done:\n        if render: env.render() # you can watch it play!\n        action = net.activate(obs).argmax() # pick action of neuron with highest act\n        obs, rew, done, info = env.step(action)\n        # With NE we ignore the availability of per-action reward (it's rarely \n        # available in nature anyway), and just accumulate it over the episode\n        score += rew\n    return score\n\n###### THIS IS THE ACTUAL DIBB-SPECIFIC CODE ######\n\n# Call `DiBB.defaults` for the full list, check `dibb/dibb.py` for descriptions\ndibb_config = {\n    'fit_fn' : play_gym,\n    'minim_task' : False, # IMPORTANT: in this task we want to _maximize_ the score!\n    'ndims' : net.nweights,\n    'nblocks' : net.noutputs, # Let's use a block for each output neuron (2 here)\n    'optimizer' : 'default', # CMA or LMMAES if installed, else Parameter Guessing\n    # 'optimizer_options' : {'popsize' : 50} # Ray manages parallel fitness evals\n}\ndibb = DiBB(**dibb_config).run(ngens=15) # Cartpole is not a challenge for DPS/NE\n\n###################################################\n\n# Watch the best individual play!\nbest_fit = play_gym(dibb.best_ind, render=True)\nprint(\"Best fitness:\", best_fit)\n\n# You can even resume the run for a few more generations if needed:\n# print(\"Resume training for 15 more generations\")\n# dibb.run(ngens=15)\n# print(\"Best fitness:\", play_gym(dibb.best_ind, render=True))\n\n# Keep the console open at the end of the run, so you can play more with DiBB!\nprint(\"\\n\\nDropping into an interactive console now, feel free to inspect the `dibb` object at your leisure. For example, try running `dibb.defaults` to see all options and their default values; `dibb.cfg` for the values actually used for this run; `dibb.result` for a dict of the final mean and best individual found; `dibb.comm_dict` to see the communication dictionary with the BlockWorkers.\\n\")\nimport os; os.environ['PYTHONINSPECT'] = 'TRUE'\n```\n\n# Glossary\n\n- **DiBB:** Distributed Black Box (optimization)\n- **Block:** subset of parameters assumed to be correlated (intra-block); parameters in different blocks are assumed separable (inter-block). Notice that in most applications both full correlation and full separability between parameters are very unlikely. With DiBB you can better approximate real characteristics, and distribute the computation of blocks to different, dedicated machines as a bonus. Check the paper to know more.\n- **Optimizer:** the underlying black box optimization algorithm that will be used on a block level. Notice that using an optimizer with DiBB automatically generates a partially-separable version of that algorithm: for example, using CMA-ES with DiBB you are actually running PS-CMA-ES.\n- **Block Worker (BW):** the class, process and Ray actor that runs the base optimizer on one block. They are distributed to their own dedicated machines when a cluster is available. BWs always run asynchronously from each other, leveraging the assumption of block separability.\n- **Fitness (function):** the objective function for the optimization. In classical Reinforcement Learning, the term \"reward\" indicates a reinforcement available per each interaction (denoted by states and actions). However, this is commonly not available in real-world processes. Black-box optimization instead uses a \"fitness\" evaluation, which is a score of proficiency of one solution (as it is a multi-agent setup) on the task. The quickest way to build a fitness from an environment coded with classic RL in mind is to accumulate the reward throughout the episode, though customizing a more proper definition of \"fitness\" beyond fixed extrinsic reward schemes can often improve performance quite drastically -- so keep that in mind.\n- **Trial:** one evaluation of one individual on the task, running the fitness function. If the environment fitness is non-deterministic (e.g. random initial conditions), you can provide a better estimation of the individual's performance by averaging across multiple trials. DiBB has an option to do this automatically.\n- **Fitness Evaluator (FE):** each BW can spawn a pool of FE, sized to the machine's computational resources (e.g. number of CPUs or GPUs). All fitness computation then runs through the FEs, as an effective way to manage (potentially complex and external) available resources. For example: load-scaling when the fitness function is resource intensive, such as a robotic control task running on a separate physics simulator.\n- **Random seed:** setting a random seed for the (pseudo-)random number generator enforces reproducibility of the whole run. If tackling a non-deterministic problem using `ntrials>1`, each trial sets a separate seed out of a sequence (`[rseed, rseed+1, rseed+2, ... , rseed+ntrials-1]`).\n\n## Instantiation hierarchy\n- The cluster, if available, should be launched from a local machine, and include at least `nblocks+1` machines (one for each BW + one for the head/DiBB)\n- If no cluster is available, the run is local: all \"machines\" mentioned below become one and the same (the very local machine), hosting all computation.\n- DiBB runs on the head machine, which also hosts the Ray _object store_ (hosting the shared data: check out the [Ray documentation](https://docs.ray.io/en/latest/ray-core/objects/serialization.html#id1) for further details)\n- At initialization, the DiBB object instantiates a list of BlockWorkers (BWs), one for each parameters block, each assigned to a dedicated machine on the Ray cluster (if available)\n- At initialization, each BW creates one instance of the underlying optimizer, which is dedicated to the subset of parameters of its assigned block -- more precisely, it will instantiation the corresponding `opt_wrapper`, ensuring a compatible interface\n- Each BW can be set up to execute the fitness function either on its own (either sequentially or in parallel), or instantiate a pool of FitnessEvaluators (FE), typically as many as the available computational resources (e.g. CPU cores or GPUs) -- see Advanced Usage below for more info\n- If using the FEs, these run on the same machine of the BW that instantiated them: at any given moment, the machine resources are either dedicated to fitness evaluation or to (underlying) BBO update\n- Further distributing the fitness computation to extra available machines is a planned feature for a future release\n\n## Run hierarchy\n- DiBB instantiates the BWs, then remains at watch, executing the function `print_fn(dibb)` (if enabled) at custom regular intervals\n- If chosen, each BW instantiates immediately an independent, machine-local pool of FEs to evaluate the individuals generated by its optimizer\n- The BW then immediately starts the generation loop of the underlying BBO, using a classic \"ask&tell\" interface:\n    - `bbo.ask()` needs to return a population of individuals, which are then evaluated on the fitness function by the BW (either locally or on the FEs)\n    - In case of multiple trials per individual, the BW aggregates the resulting episodic fitness scores using a customizable aggregation function\n    - `bbo.tell(inds, fits)` then uses the individuals and fitness scores to execute one update step of the BBO algorithm\n- The BWs run asynchronously from each other, leveraging the assumption of inter-block separability. Potentially they could even run on machines with different performance, and the blocks can differ in size, though special care is then necessary to ensure load balance\n- The BWs can individually decide to pause their execution, typically temporarily if convergence is achieved in one block (as the _moving target_ problem should allow resuming the search shortly), or because of an inescapable termination criterion -- either way, this is communicated to the main DiBB process, which can itself request the BWs termination as needed, such as if they all reach termination.\n\n## Advanced Usage\n\n- The complete list of available options to DiBB is available directly in the code to avoid duplication and simplify maintenance; you can check the top of [the main DiBB file](dibb/dibb.py). You can ask directly the DiBB class (`DiBB.defaults`) or even the module (`from dibb import options`), but the source has more useful descriptions for each item.\n- The `printfn` option accepts a callable object (function, method, lambda, even a class implementing `__call__()`) which should take a DiBB object as parameter: this is then executed at regular intervals by the main node while waiting for the workers to finish (run report)\n- Termination conditions can be passed more explicitly using the `stopping_conditions` option (e.g. `DiBB(stopping_conditions={'nevals' : 10e5})`\n- The `hooks` parameter accepts callables (either single or lists of them) which are executed before each generation (for the BW) or before each fitness evaluation (for the FE). This is often useful to customize the algorithm tracking and behavior, giving the user a direct \"hook\" into the algorithm's mechanics\n- For reproducibility we use `pipenv` for development. If you wish to contribute a pull request (or simply to run the tests), do a `python3 -m pip install pipenv` followed by `python3 -m pipenv sync` to install the exact same environment we use for development (tip: don't use `pipenv install`, as it will update all libraries and edit the `Pipfile.lock`). You can then run your virtualenv shell with `python3 -m pipenv shell`, or run your code with `python3 -m pipenv run python your_experiment.py`\n- To run DiBB on a cluster:\n    1. Download the [Ray cluster configuration template](ray_cluster_config.yaml)\n    2. Open it in a text editor and follow instructions: you need to replace all placeholders (format looks like this: `%replace_me%`) with the actual data of your cluster (machine IPs, ray command, etc.)\n    3. Save, close, and keep on your local machine, which should be able to reach the network hosting the cluster machines\n    4. Install DiBB, Ray and all libraries for your experiment Ray on all machines of your cluster, including any external resource you need for your fitness evaluation\n    5. Launch the cluster by running `ray up ray_cluster_config.yaml` on your local machine\n    6. Launch your code by using [Ray's job submission API](https://docs.ray.io/en/latest/cluster/job-submission.html#ray-job-cli). For a quick run (or to avoid problems e.g. with virtual environments through the job submission API) you can also simply connect to the head node with `ssh` and launch your code from there. Done!\n- You can run fitness computation either: 1. on the BlockWorker, sequentially; 2. on the BlockWorker, in parallel (the default); 3. on the FitnessEvaluator pool, ideal for complex fitnesses such as needing to launch external simulators on each machine in the cluster\n- To support complex fitness evaluation, especially on the FEs, and specifically to avoid the problems in pickling (serialization) complex fitness objects for distribution on the cluster: you can pass the `fit_fn` as a *string*, in which case it will be sent as a string and then `eval()`ed on the target machine. To fully leverage this, try the following:\n    1. Create a `Fitness` class\n    2. In its `__init__`, launch the simulator or connect to external resources\n    3. Define a `__call__(self, ind)` method to take an individual and return its fitness (on one run; use the `ntrials` option for nondeterministic fitnesses)\n    4. Pass to DiBB a string with a call to the constructor: `DiBB(fit_fn=\"Fitness()\")`\n    5. DiBB will send the string to the remote machines, which will then run`fitness = eval(\"Fitness()\")`, thus creating locally the exact environment required for individual evaluation\n- Debugging is best done following [Ray's guidelines and tools](https://docs.ray.io/en/latest/ray-observability/ray-debugging.html). Particularly, be aware that Ray silently collects uncaught exceptions on the remote machines, and upon errors it does not flush the local stdout buffers to the main node. \n\nThis means that if your code breaks a remote machine in the cluster, it will look like DiBB is failing silently and without support. There is nothing we can do about it (yet). Please be patient, either trust DiBB and simplify your fitness, or carefully debug and submit a fix as pull request if it's our fault. Thank you!\n\nHere's a snippet to help:\n\n```python\n# Classic printf debugging, add as many as needed\nimport inspect\nlineno = inspect.getframeinfo(inspect.currentframe()).lineno\nprint(f\"\\n\\n\\tline {lineno} of file {__file__}\\n\\n\" )\n```\n\nthen check the logs (\"workers\" here are machines in a Ray cluster):\n\n```bash\nfor log in $(find /tmp/ray/session_latest/logs -name 'worker*'); do\n    echo -e \"\\n\\n-------- $log --------\\n\"\n    cat $log\ndone\n```\n\n## Acknowledgements\n\nThis work has been published at GECCO 2022, The 24th Genetic and Evolutionary Computation Conference.\n[[Download the paper here]](https://exascale.info/assets/pdf/cuccu2022gecco.pdf), bibtex here:\n\n```bibtex\n@inproceedings{cuccu2022gecco,\n  author    = {Cuccu, Giuseppe and Rolshoven, Luca and Vorpe, Fabien and Cudr\\'{e}-Mauroux, Philippe and Glasmachers, Tobias},\n  title     = {{DiBB}: Distributing Black-Box Optimization},\n  year      = {2022},\n  isbn      = {9781450392372},\n  publisher = {Association for Computing Machinery},\n  address   = {New York, NY, USA},\n  doi       = {10.1145/3512290.3528764},\n  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},\n  pages     = {341–349},\n  numpages  = {9},\n  keywords  = {parallelization, neuroevolution, evolution strategies, distributed algorithms, black-box optimization},\n  location  = {Boston, Massachusetts},\n  series    = {GECCO '22},\n  url       = {https://exascale.info/assets/pdf/cuccu2022gecco.pdf}\n}\n```\n\nThe experiment code to reproduce our COCO results is [available here](https://github.com/eXascaleInfolab/dibb_coco), created and maintained by Luca [(@rolshoven)](https://github.com/rolshoven).\n\nSince 2021 -- initially as part of his Master thesis, then out of his personal quest for excellence -- Luca Sven Rolshoven [(@rolshoven)](https://github.com/rolshoven) has contributed to this project with engaging discussions, reliable debugging, the original \"hooks\" feature, [fundamental testing](https://github.com/eXascaleInfolab/dibb_coco), and managing our cluster of 25 decommissioned office computers :)  \nThe git history of this repository has been wiped at publication for privacy concerns, but his contribution should not go unacknowledged nor underestimated. Thanks Luca!\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/giuse/dibb",
    "keywords": "black-box optimization,distributed computation",
    "license": "License :: OSI Approved :: MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dibb",
    "package_url": "https://pypi.org/project/dibb/",
    "platform": null,
    "project_url": "https://pypi.org/project/dibb/",
    "project_urls": {
      "Homepage": "https://github.com/giuse/dibb"
    },
    "release_url": "https://pypi.org/project/dibb/0.29.8/",
    "requires_dist": [
      "numpy",
      "ray[default]",
      "lmmaes ; extra == 'all'",
      "cma ; extra == 'all'",
      "cma ; extra == 'cma'",
      "lmmaes ; extra == 'lmmaes'"
    ],
    "requires_python": ">=3.6",
    "summary": "DiBB: Distributing Black-Box optimization",
    "version": "0.29.8",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14599692,
  "releases": {
    "0.29.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d5cd953f519c654e66fdf2f4c137606750cdcfa7396b8091f01dcf353f4a81cd",
          "md5": "d51835ca22b8f2988871611114756ec1",
          "sha256": "6ed0084bf3120dd39de8d4cb2d3dfcfed9b02a8c624acde4c2a648452e234201"
        },
        "downloads": -1,
        "filename": "dibb-0.29.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d51835ca22b8f2988871611114756ec1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27533,
        "upload_time": "2022-06-12T17:52:45",
        "upload_time_iso_8601": "2022-06-12T17:52:45.437990Z",
        "url": "https://files.pythonhosted.org/packages/d5/cd/953f519c654e66fdf2f4c137606750cdcfa7396b8091f01dcf353f4a81cd/dibb-0.29.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cf67792f56c49acc2867f436981d31e064fde9c4d00df467a1e84db6e053b38a",
          "md5": "b234c495d8f4d5caaa53122be53923ca",
          "sha256": "1227d12c9031d8428e68154b1d50d928022b9bd990ae79b16c79efa7260b81db"
        },
        "downloads": -1,
        "filename": "dibb-0.29.4.tar.gz",
        "has_sig": false,
        "md5_digest": "b234c495d8f4d5caaa53122be53923ca",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 33075,
        "upload_time": "2022-06-12T17:52:47",
        "upload_time_iso_8601": "2022-06-12T17:52:47.566134Z",
        "url": "https://files.pythonhosted.org/packages/cf/67/792f56c49acc2867f436981d31e064fde9c4d00df467a1e84db6e053b38a/dibb-0.29.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.29.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "75f330b35f6a38f2f72ec20242aa920aff22de10bfbd517ba7b3f512381684d9",
          "md5": "b0f797d906d579d9571467ddd8b9a251",
          "sha256": "58ae7512886e367cb2a4c11ab965f94b670abb55dff409d494d22622ef639cd8"
        },
        "downloads": -1,
        "filename": "dibb-0.29.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b0f797d906d579d9571467ddd8b9a251",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27389,
        "upload_time": "2022-06-13T10:40:53",
        "upload_time_iso_8601": "2022-06-13T10:40:53.552237Z",
        "url": "https://files.pythonhosted.org/packages/75/f3/30b35f6a38f2f72ec20242aa920aff22de10bfbd517ba7b3f512381684d9/dibb-0.29.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "99682f4befd59c5fc7285150fbe8db82ada6945230d58488288398692882ad4a",
          "md5": "c281275286f464f6d7e058e395a244a0",
          "sha256": "4e1c9245a3dfbcc37f89b0cb6b5489a90eaa37940c02fd159105f7dca0602eec"
        },
        "downloads": -1,
        "filename": "dibb-0.29.5.tar.gz",
        "has_sig": false,
        "md5_digest": "c281275286f464f6d7e058e395a244a0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 32903,
        "upload_time": "2022-06-13T10:40:55",
        "upload_time_iso_8601": "2022-06-13T10:40:55.748505Z",
        "url": "https://files.pythonhosted.org/packages/99/68/2f4befd59c5fc7285150fbe8db82ada6945230d58488288398692882ad4a/dibb-0.29.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.29.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e8c135016298f421aee8a9d703862705c51bff3e07858a29964def8e31289554",
          "md5": "5367e3f27dfdbcf3a650603280f56590",
          "sha256": "392603f039a33ce3c166d2b8514cfdb0a1b3721326a737a3333a0a9966720d57"
        },
        "downloads": -1,
        "filename": "dibb-0.29.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5367e3f27dfdbcf3a650603280f56590",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27468,
        "upload_time": "2022-07-11T14:28:08",
        "upload_time_iso_8601": "2022-07-11T14:28:08.528430Z",
        "url": "https://files.pythonhosted.org/packages/e8/c1/35016298f421aee8a9d703862705c51bff3e07858a29964def8e31289554/dibb-0.29.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4761f29a68e1d43949af0fb52344924f04eef7639683778c1640cd3d6cc4f801",
          "md5": "f00ededce44beb9ebc61845d1d51dec6",
          "sha256": "6e048f6a9884962b9078f256d2ad572597dd20cc69e99ed0e9195f581ef8745f"
        },
        "downloads": -1,
        "filename": "dibb-0.29.6.tar.gz",
        "has_sig": false,
        "md5_digest": "f00ededce44beb9ebc61845d1d51dec6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 33040,
        "upload_time": "2022-07-11T14:28:10",
        "upload_time_iso_8601": "2022-07-11T14:28:10.608126Z",
        "url": "https://files.pythonhosted.org/packages/47/61/f29a68e1d43949af0fb52344924f04eef7639683778c1640cd3d6cc4f801/dibb-0.29.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.29.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9108cb0978d10ec67183438b411968058c9b2f41e01dd8f2532c83b0c6401c00",
          "md5": "fcbaab935d93bc42af35766622904041",
          "sha256": "f1e0c14362ebe647b961ef338ec3eec49406174383905bddbed6d3e9eb4a4c05"
        },
        "downloads": -1,
        "filename": "dibb-0.29.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fcbaab935d93bc42af35766622904041",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27564,
        "upload_time": "2022-07-11T15:29:52",
        "upload_time_iso_8601": "2022-07-11T15:29:52.575634Z",
        "url": "https://files.pythonhosted.org/packages/91/08/cb0978d10ec67183438b411968058c9b2f41e01dd8f2532c83b0c6401c00/dibb-0.29.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d0cf6bb33b983432ac220cc8f1cc8421841c46506b74de8998e55814ea933acf",
          "md5": "2c5acc14d591b43b8e502500039e07aa",
          "sha256": "3c16017937361a043256585e7b0f001e89e1d08f497946fa113477249b56fd51"
        },
        "downloads": -1,
        "filename": "dibb-0.29.7.tar.gz",
        "has_sig": false,
        "md5_digest": "2c5acc14d591b43b8e502500039e07aa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 33233,
        "upload_time": "2022-07-11T15:29:54",
        "upload_time_iso_8601": "2022-07-11T15:29:54.495619Z",
        "url": "https://files.pythonhosted.org/packages/d0/cf/6bb33b983432ac220cc8f1cc8421841c46506b74de8998e55814ea933acf/dibb-0.29.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.29.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e6d2b5cca7ac9b08e88184e4d82c4275aee4bd3b2170ef296395d94a14fd019",
          "md5": "8401cf9e3784efb043c690a9bb484613",
          "sha256": "4d151ee3f29349b2b0e9e62c86218de8380a9b5db835be69b1a3f91457bfb15a"
        },
        "downloads": -1,
        "filename": "dibb-0.29.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8401cf9e3784efb043c690a9bb484613",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27564,
        "upload_time": "2022-07-30T16:08:38",
        "upload_time_iso_8601": "2022-07-30T16:08:38.542225Z",
        "url": "https://files.pythonhosted.org/packages/5e/6d/2b5cca7ac9b08e88184e4d82c4275aee4bd3b2170ef296395d94a14fd019/dibb-0.29.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cac4f44f92eec6d5c499162f3c3e99be146cebde4c2013c907de1b406ba77c5d",
          "md5": "7de7da6bc4db95f2649ac7bbeaacdaa5",
          "sha256": "d60928bbe7cb53730d51cfa9f67274bc11b1806b61c4d3b095fad25f08359ce4"
        },
        "downloads": -1,
        "filename": "dibb-0.29.8.tar.gz",
        "has_sig": false,
        "md5_digest": "7de7da6bc4db95f2649ac7bbeaacdaa5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 33240,
        "upload_time": "2022-07-30T16:08:41",
        "upload_time_iso_8601": "2022-07-30T16:08:41.795738Z",
        "url": "https://files.pythonhosted.org/packages/ca/c4/f44f92eec6d5c499162f3c3e99be146cebde4c2013c907de1b406ba77c5d/dibb-0.29.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5e6d2b5cca7ac9b08e88184e4d82c4275aee4bd3b2170ef296395d94a14fd019",
        "md5": "8401cf9e3784efb043c690a9bb484613",
        "sha256": "4d151ee3f29349b2b0e9e62c86218de8380a9b5db835be69b1a3f91457bfb15a"
      },
      "downloads": -1,
      "filename": "dibb-0.29.8-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8401cf9e3784efb043c690a9bb484613",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 27564,
      "upload_time": "2022-07-30T16:08:38",
      "upload_time_iso_8601": "2022-07-30T16:08:38.542225Z",
      "url": "https://files.pythonhosted.org/packages/5e/6d/2b5cca7ac9b08e88184e4d82c4275aee4bd3b2170ef296395d94a14fd019/dibb-0.29.8-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cac4f44f92eec6d5c499162f3c3e99be146cebde4c2013c907de1b406ba77c5d",
        "md5": "7de7da6bc4db95f2649ac7bbeaacdaa5",
        "sha256": "d60928bbe7cb53730d51cfa9f67274bc11b1806b61c4d3b095fad25f08359ce4"
      },
      "downloads": -1,
      "filename": "dibb-0.29.8.tar.gz",
      "has_sig": false,
      "md5_digest": "7de7da6bc4db95f2649ac7bbeaacdaa5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 33240,
      "upload_time": "2022-07-30T16:08:41",
      "upload_time_iso_8601": "2022-07-30T16:08:41.795738Z",
      "url": "https://files.pythonhosted.org/packages/ca/c4/f44f92eec6d5c499162f3c3e99be146cebde4c2013c907de1b406ba77c5d/dibb-0.29.8.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}