{
  "info": {
    "author": "TUNiB",
    "author_email": "contact@tunib.ai",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries"
    ],
    "description": "<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/38183241/125905410-1ee984a3-c5a9-4d8c-ba40-46fca740f514.png\" width=380>\n</p>\n<p align=\"center\">\n<a href=\"https://github.com/tunib-ai/parallelformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/tunib-ai/parallelformers.svg\" /></a> <a href=\"https://github.com/tunib-ai/parallelformers/blob/master/LICENSE\"><img alt=\"Apache 2.0\" src=\"https://img.shields.io/badge/license-Apache%202.0-blue.svg\"/></a> <a href=\"https://tunib-ai.github.io/parallelformers\"><img alt=\"Docs\" src=\"https://img.shields.io/badge/docs-passing-success.svg\"/></a> <a href=\"https://github.com/tunib-ai/parallelformers/issues\"><img alt=\"Issues\" src=\"https://img.shields.io/github/issues/tunib-ai/parallelformers\"/></a>\n\n</p>\n<br>\n\n\n- Parallelformers, which is based on [Megatron LM](https://github.com/NVIDIA/Megatron-LM), is designed to make model parallelization easier.\n- You can parallelize various models in [HuggingFace Transformers](https://github.com/huggingface/transformers) on multiple GPUs with **a single line of code.**\n- Currently, Parallelformers **only supports inference**. Training features are NOT included.\n\n<br>\n\n\n### What's New:\n* October 24, 2021 [Docker support](https://github.com/tunib-ai/parallelformers#are-you-getting-some-errors-in-docker-container).\n* July 28, 2021 [Released a tech blog](https://tunib.tistory.com/entry/Parallelformers-Journey-to-deploying-big-modelsTUNiB?category=899987).\n* July 18, 2021 [Released Parallelformers 1.0](https://github.com/tunib-ai/parallelformers/releases/tag/1.0).\n\n<br>\n\n## Why Parallelformers?\nYou can load a model that is too large for a single GPU. For example, using Parallelformers, you can load a model of 12GB on two 8 GB GPUs. In addition, you can save your precious money because usually multiple smaller size GPUs are less costly than a single larger size GPU.\n\n## Installation\nParallelformers can be easily installed using the `pip` package manager. All the dependencies such as [torch](https://pypi.org/project/torch/), [transformers](https://pypi.org/project/transformers/), and [dacite](https://pypi.org/project/dacite/) should be installed automatically with the following command. Be careful that the name is plural.\n```console\npip install parallelformers\n```\n\n## Getting Started\n#### 1. Create a HuggingFace transformers model. \nYou don't need to call `.half()` or `.cuda()` as those functions will be invoked automatically. It is more memory efficient to start parallelization on the CPU.\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n```\n\n#### 2. Put the `model` in the `parallelize()` function.\n```python\nfrom parallelformers import parallelize\n\nparallelize(model, num_gpus=2, fp16=True, verbose='detail')\n```\n\nSince `nvidia-smi` shows the reserved cache area, it is difficult to check the exact allocated memory. To check the allocated memory state well, **you can set the verbose option as `'detail'` or `'simple'`.** (default is `None`).\nIf you want to set a random seed value, input the seed value using `parallelize(..., seed=YOUR_SEED)`.\n```\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |    2721 MB |    2967 MB |    2967 MB |  251905 KB |\n|       from large pool |    2720 MB |    2966 MB |    2966 MB |  251904 KB |\n|       from small pool |       1 MB |       1 MB |       1 MB |       1 KB |\n|---------------------------------------------------------------------------|\n\nGPU:0 => 2.72GB\n```\n```\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 1                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |    2721 MB |    2967 MB |    2967 MB |  251905 KB |\n|       from large pool |    2720 MB |    2966 MB |    2966 MB |  251904 KB |\n|       from small pool |       1 MB |       1 MB |       1 MB |       1 KB |\n|---------------------------------------------------------------------------|\n\nGPU:1 => 2.72GB\n```\n\n#### 3. Do Inference as usual. \nYou don't have to call `.cuda()` when creating input tokens. **Note that you should input both input tokens and attention masks to the model.** (`**inputs` is the recommended way for this)\n```python\ninputs = tokenizer(\"Parallelformers is\", return_tensors=\"pt\")\n\noutputs = model.generate(\n    **inputs,\n    num_beams=5,\n    no_repeat_ngram_size=4,\n    max_length=15,\n)\n\nprint(f\"Output: {tokenizer.batch_decode(outputs)[0]}\")\n``` \n```\nOutput: Parallelformers is an open-source library for parallel programming ...\n```\n\n#### 4. Deploy the model to the server as usual. \nThe parallelization process does not affect the web server because they are automatically synchronized.\n```python\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n@app.route(\"/generate_text/<text>\")\ndef generate_text(text):\n    inputs = tokenizer(text, return_tensors=\"pt\")\n    \n    outputs = model.generate(\n        **inputs,\n        num_beams=5,\n        no_repeat_ngram_size=4,\n        max_length=15,\n    )\n    \n    outputs = tokenizer.batch_decode(\n        outputs,\n        skip_special_tokens=True,\n    )\n    \n    return {\n        \"inputs\": text,\n        \"outputs\": outputs[0],\n    }\n\n\napp.run(host=\"0.0.0.0\", port=5000)\n```\n\nYou can send a request to the web server as follows:\n```\n$ curl -X get \"YOUR_IP:5000/generate_text/Messi\"\n```\nAnd the following result should be returned.\n```\n{\"inputs\": \"Messi\", \"outputs\": \"Messi is the best player in the world right now. He is the\"}\n```\n\n#### 5. Check the current GPU states.\nYou can check GPU states using `.memory_allocated()`, `.memory_reserved()` and `.memory_chached()` to make sure the parallelization is successful.\n```python\nmodel.memory_allocated()\nmodel.memory_reserved()\nmodel.memory_chached()\n```\n```\n{'cuda:0':XXXXXX, 'cuda:1':XXXXXX}\n```\n\n#### 6. Manage the model parallelization states.\nYou can manage model parallelization states using `.cuda()`, `.cpu()` and `.to()`. **The model parallelization process ends if you call those functions.**\n```python\nmodel.cuda()\n\nprint(torch.cuda.memory_summary(0))\nprint(torch.cuda.memory_summary(1))\n```\nCheck the allocated memory status using `torch.cuda.memory_summary()`.\n```\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |    5121 MB |    5121 MB |    5121 MB |    1024 B  |\n|       from large pool |    5120 MB |    5120 MB |    5120 MB |       0 B  |\n|       from small pool |       1 MB |       1 MB |       1 MB |    1024 B  |\n|---------------------------------------------------------------------------|\n\nGPU0 => 5.12GB\n```\n```\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 1                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |       0 B  |    1024 B  |    1024 B  |    1024 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |    1024 B  |    1024 B  |    1024 B  |\n|---------------------------------------------------------------------------|\n\nGPU1 => 0.00GB\n```\nIf you switch to the CPU mode, it works like this.\n```python\nmodel.cpu()\n\nprint(torch.cuda.memory_summary(0))\nprint(torch.cuda.memory_summary(1))\n```\n```\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 0                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |       0 B  |    5121 MB |    5121 MB |    5121 MB |\n|       from large pool |       0 B  |    5120 MB |    5120 MB |    5120 MB |\n|       from small pool |       0 B  |       1 MB |       1 MB |       1 MB |\n|---------------------------------------------------------------------------|\n\nGPU0 => 0.00GB\n```\n```\n|===========================================================================|\n|                  PyTorch CUDA memory summary, device ID 1                 |\n|---------------------------------------------------------------------------|\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n|===========================================================================|\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n|---------------------------------------------------------------------------|\n| Allocated memory      |       0 B  |    1024 B  |    1024 B  |    1024 B  |\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n|       from small pool |       0 B  |    1024 B  |    1024 B  |    1024 B  |\n|---------------------------------------------------------------------------|\n\nGPU1 => 0.00GB\n```\n\n## Do your processes die or stop working?\nMany issues have pointed this out. And I've found that running code inside the context of `if __name__ == '__main__'` solves a lot of problems. So if you run have some problems about processes, try writing your code inside the context of it.\n\n## Are you getting some errors in docker container?\nI recently found out that ALL errors that occur in environments with limited resources such as docker containers are due to **shared memory size**. So, if you want to use larger models with parallelformers in docker containers, **INCREASE the size of shared memory by `--shm-size=?gb` or REMOVE the limitation of shared memory size by `--ipc=host`**. the larger shared memory size is required if you want to use larger model.\n\n## Supported Models\nCurrently, most models in Huggingface transformers are supported. All layers in the models listed below can be parallelized.\nThey include vision models like `ViT`, `CLIP` and speech models like `Wav2Vec2` as well as language models.\n\n<details>\n  <summary>Fully Supported Models</summary>\n\n* ALBERT\n* BART\n* BARThez (=BERT)\n* BERT\n* BERTweet (=BERT)\n* BertJapanese (=BERT)\n* BertGeneration\n* Blenderbot\n* Blenderbot Samll\n* BORT (=BERT)\n* CamemBERT (=RoBERTa)\n* CLIP\n* CPM\n* CTRL\n* DeBERTa\n* DeBERTa-v2\n* DeiT\n* DETR\n* DialoGPT (=GPT2)\n* DistilBERT\n* DPR (=BERT)\n* ELECTRA\n* FlauBERT (=XLM)\n* FSMT\n* Funnel Transformer\n* herBERT (=RoBERTa)\n* I-BERT\n* LayoutLM\n* LED\n* Longformer\n* LUKE\n* LXMERT\n* MarianMT\n* M2M100\n* MBart\n* Mobile BERT\n* MPNet\n* MT5 (=T5)\n* Megatron BERT (=BERT)\n* Megatron GPT2 (=GPT2)\n* OpenAI GPT\n* OpenAI GPT2\n* OPT\n* GPTNeo\n* GPTJ  \n* Hubert\n* Pegasus\n* PhoBERT (=RoBERTa)\n* Reformer\n* RetriBERT\n* RoBERTa\n* RoFormer\n* Speech2Text\n* T5\n* ByT5 (=T5)\n* TAPAS\n* TransformerXL\n* ViT\n* VisualBERT\n* Wav2Vec2\n* XLM\n* XLM-RoBERTa (=RoBERTa)\n* XLNet\n* XLSR-Wave2Vec2\n  \n</details>\n\n\nAt present the following models are [partly supported or not supported](FAQ.md#q-why-are-some-models-not-supported). \n\n<details> \n  <summary>Partly Supported Models</summary>\n\n* BigBird \n* BigBirdPegasus\n* ConvBERT\n* ProphetNet \n* XLM-ProphetNet\n\n</details>\n\n<details> \n  <summary>Unsupported Models</summary>\n\n* SqueezeBERT\n* RAG\n  \n</details>\n\n## Advanced Usage\nRefer to [POLICY.md](POLICY.md)\n\n## FAQ\nRefer to [FAQ.md](FAQ.md).\n\n## Contributing\nRefer to [CONTRIBUTING.md](CONTRIBUTING.md)\n\n## Documentation\nFor more detailed information, see [full documentation](https://tunib-ai.github.io/parallelformers/)\n\n## Citation\nIf you find this library useful, please consider citing:\n\n```\n@misc{parallelformers,\n  author       = {Ko, Hyunwoong},\n  title        = {Parallelformers: An Efficient Model Parallelization Toolkit for Deployment},\n  howpublished = {\\url{https://github.com/tunib-ai/parallelformers}},\n  year         = {2021},\n}\n```\n\nThis library is cited by:\n\n- [Few-Shot Bot: Prompt-Based Learning for Dialogue Systems, Madotto et al, 2021](https://arxiv.org/abs/2110.08118)\n- [AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing, Kalyan et al, 2021](https://arxiv.org/abs/2108.05542)\n\n## LICENSE\n`Parallelformers` is licensed under the terms of the Apache License 2.0.\n\nCopyright 2021 TUNiB inc. http://www.tunib.ai. All Rights Reserved.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/tunib-ai/parallelformers",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "parallelformers",
    "package_url": "https://pypi.org/project/parallelformers/",
    "platform": null,
    "project_url": "https://pypi.org/project/parallelformers/",
    "project_urls": {
      "Homepage": "https://github.com/tunib-ai/parallelformers"
    },
    "release_url": "https://pypi.org/project/parallelformers/1.2.7/",
    "requires_dist": null,
    "requires_python": ">=3.6.0",
    "summary": "An Efficient Model Parallelization Toolkit for Deployment",
    "version": "1.2.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14569535,
  "releases": {
    "0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d4a604a7e6efd982a5bb37ae581e822e6afb077262f9d06487eda93a3363144",
          "md5": "74c3499e28f1dd56f7b7493dd42b9f25",
          "sha256": "15745373ec927ed9fbb00f68a5d656a1479b93b24df7763f280fdaf619c46b5f"
        },
        "downloads": -1,
        "filename": "parallelformers-0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "74c3499e28f1dd56f7b7493dd42b9f25",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 3217,
        "upload_time": "2021-06-23T14:24:31",
        "upload_time_iso_8601": "2021-06-23T14:24:31.288482Z",
        "url": "https://files.pythonhosted.org/packages/5d/4a/604a7e6efd982a5bb37ae581e822e6afb077262f9d06487eda93a3363144/parallelformers-0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c26213418b22fd12e2859f916198112a49c54df3487cb19bb790a82f9f4dc779",
          "md5": "b9ae45551acf9392b17fc7fafc733e5a",
          "sha256": "2166ebb3e27a43069ff911305940e7b91f85b70920a2a31c9d9779e52c3c8f32"
        },
        "downloads": -1,
        "filename": "parallelformers-1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b9ae45551acf9392b17fc7fafc733e5a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 110627,
        "upload_time": "2021-07-17T23:25:54",
        "upload_time_iso_8601": "2021-07-17T23:25:54.805165Z",
        "url": "https://files.pythonhosted.org/packages/c2/62/13418b22fd12e2859f916198112a49c54df3487cb19bb790a82f9f4dc779/parallelformers-1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5164181f1f20cc35cdf053f2935e8f1203621b5d4e0722078fa71a3261692c33",
          "md5": "2537c66f036a436b6b44b46f3724876e",
          "sha256": "9d247d2a94ece627254b6c4ce260f5efcfa1dce1ebae2ac1eb64044673fb809c"
        },
        "downloads": -1,
        "filename": "parallelformers-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2537c66f036a436b6b44b46f3724876e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 110684,
        "upload_time": "2021-07-19T11:49:13",
        "upload_time_iso_8601": "2021-07-19T11:49:13.089853Z",
        "url": "https://files.pythonhosted.org/packages/51/64/181f1f20cc35cdf053f2935e8f1203621b5d4e0722078fa71a3261692c33/parallelformers-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3cd0851bb7e9f6cc17096b45d1dd45677a1f02674c80c7b979347b818f2a303f",
          "md5": "3131bcfbdc486899e7dc6d55556fd75f",
          "sha256": "d6f9713b59cf960444c1ba80ff88b0d854a8fb03177a4638996c55c2ef4d66f9"
        },
        "downloads": -1,
        "filename": "parallelformers-1.0b0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3131bcfbdc486899e7dc6d55556fd75f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 61424,
        "upload_time": "2021-07-07T04:29:08",
        "upload_time_iso_8601": "2021-07-07T04:29:08.377355Z",
        "url": "https://files.pythonhosted.org/packages/3c/d0/851bb7e9f6cc17096b45d1dd45677a1f02674c80c7b979347b818f2a303f/parallelformers-1.0b0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b484e63d61b979da40c723d926c27f91d9743ae2827b6e195c04f8dc5ce47e05",
          "md5": "7edf51b130a5dd9675a88382b5a71ec9",
          "sha256": "230bd59f700958c198442008526e22ff2bfc66991f3f7c546d7c411d4556112d"
        },
        "downloads": -1,
        "filename": "parallelformers-1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7edf51b130a5dd9675a88382b5a71ec9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 47736,
        "upload_time": "2021-12-06T00:05:27",
        "upload_time_iso_8601": "2021-12-06T00:05:27.186398Z",
        "url": "https://files.pythonhosted.org/packages/b4/84/e63d61b979da40c723d926c27f91d9743ae2827b6e195c04f8dc5ce47e05/parallelformers-1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4de6753379729ec09b264cb3b3817493112e03f8cbdfb66f65f848613ec21ae8",
          "md5": "d9a8fa547cc0047be1761676c68cc875",
          "sha256": "818e08876b8f9dc07cd1e99f6f4cfc1fa3cac5036c39c63e5906b0252f1f56d8"
        },
        "downloads": -1,
        "filename": "parallelformers-1.1b0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d9a8fa547cc0047be1761676c68cc875",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 61429,
        "upload_time": "2021-07-07T17:06:58",
        "upload_time_iso_8601": "2021-07-07T17:06:58.450825Z",
        "url": "https://files.pythonhosted.org/packages/4d/e6/753379729ec09b264cb3b3817493112e03f8cbdfb66f65f848613ec21ae8/parallelformers-1.1b0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5af85d13ff2960a49400a655dd358e4a54d55b3c88c07b56ac4932e937c9bf7d",
          "md5": "85b147e4b82efdd12a3eda282268fb59",
          "sha256": "ea2b054b02a63dda8873a1da21e408f6c22a309a0ea58ea1e3a1aece3ed5e2a4"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "85b147e4b82efdd12a3eda282268fb59",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 47703,
        "upload_time": "2021-12-17T09:27:54",
        "upload_time_iso_8601": "2021-12-17T09:27:54.361689Z",
        "url": "https://files.pythonhosted.org/packages/5a/f8/5d13ff2960a49400a655dd358e4a54d55b3c88c07b56ac4932e937c9bf7d/parallelformers-1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "37303fca4f17e3607fbe64af58b5851a29381e1c2a77392fa10941a8977efaa0",
          "md5": "af2f057ea32a1dbb25913783f33f6a46",
          "sha256": "10f2f44fad99d57ab8454abd742f804d553f1a5fb86d9ba89c4697f88f7f1e2b"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "af2f057ea32a1dbb25913783f33f6a46",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 48646,
        "upload_time": "2021-12-28T19:22:04",
        "upload_time_iso_8601": "2021-12-28T19:22:04.858419Z",
        "url": "https://files.pythonhosted.org/packages/37/30/3fca4f17e3607fbe64af58b5851a29381e1c2a77392fa10941a8977efaa0/parallelformers-1.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "04f87425d84eb13995a36084e50c576a6bdc5f4238aef1997b9f1826724549d8",
          "md5": "03f9c127a09e61f17006ed47a46d7e22",
          "sha256": "27dd4490239d99e137c5870f5de399ef9b18ec903c092a6ba0df0362129a832a"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "03f9c127a09e61f17006ed47a46d7e22",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 48640,
        "upload_time": "2021-12-28T19:29:44",
        "upload_time_iso_8601": "2021-12-28T19:29:44.369870Z",
        "url": "https://files.pythonhosted.org/packages/04/f8/7425d84eb13995a36084e50c576a6bdc5f4238aef1997b9f1826724549d8/parallelformers-1.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "61d3381df89fafd820c7fb4135c2f3d6a04389b4c707dcf15c6911f116216c44",
          "md5": "23be8d390aca7dc6a31810d1c5781f0a",
          "sha256": "bb52b198c9bc8ab069be4c10c744521134f69d70f6beb26f57295a93bbe66bff"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "23be8d390aca7dc6a31810d1c5781f0a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 48504,
        "upload_time": "2021-12-29T11:47:00",
        "upload_time_iso_8601": "2021-12-29T11:47:00.323336Z",
        "url": "https://files.pythonhosted.org/packages/61/d3/381df89fafd820c7fb4135c2f3d6a04389b4c707dcf15c6911f116216c44/parallelformers-1.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "becdb273b6bfeb6f792cb5c772ffbc4b76b269253bfc8c7d2a23ba55726772eb",
          "md5": "41130372b80bccd87da0d0298a670d09",
          "sha256": "77c34eae1aaf6d7775047505bfbb197fbc0845195ca45e12fd58d7d18ac0a51c"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2.4.tar.gz",
        "has_sig": false,
        "md5_digest": "41130372b80bccd87da0d0298a670d09",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 48622,
        "upload_time": "2021-12-29T21:47:16",
        "upload_time_iso_8601": "2021-12-29T21:47:16.658281Z",
        "url": "https://files.pythonhosted.org/packages/be/cd/b273b6bfeb6f792cb5c772ffbc4b76b269253bfc8c7d2a23ba55726772eb/parallelformers-1.2.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9a0413de6f37f665b3120c50d6710eb3568551986ef04a6d288d2a1d5aafccb6",
          "md5": "cd1b320ad3666f89492b9a2d8a56c696",
          "sha256": "ee1fcccf70cf9bc1cf0dbe50037fa4ed455ae1aa1833538001942f61644e49d1"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2.5.tar.gz",
        "has_sig": false,
        "md5_digest": "cd1b320ad3666f89492b9a2d8a56c696",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 48611,
        "upload_time": "2022-07-26T16:29:49",
        "upload_time_iso_8601": "2022-07-26T16:29:49.478385Z",
        "url": "https://files.pythonhosted.org/packages/9a/04/13de6f37f665b3120c50d6710eb3568551986ef04a6d288d2a1d5aafccb6/parallelformers-1.2.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d39f794639cd719c350a4b45c1c1e18d4ef4d2978725414af363b0dc4b34fb23",
          "md5": "eae1219530428fd680292ce73af30986",
          "sha256": "170c2e954f1476d6e4ed1d4afe1dd348ad5a74c044bbe45e9aac039788ecf34a"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2.6.tar.gz",
        "has_sig": false,
        "md5_digest": "eae1219530428fd680292ce73af30986",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 48634,
        "upload_time": "2022-07-27T19:30:32",
        "upload_time_iso_8601": "2022-07-27T19:30:32.752648Z",
        "url": "https://files.pythonhosted.org/packages/d3/9f/794639cd719c350a4b45c1c1e18d4ef4d2978725414af363b0dc4b34fb23/parallelformers-1.2.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "53c8038a2c2e1ec70304cb10f6d030170f445bd3354fcfec7aa5a83956ae7a6a",
          "md5": "79305432cd08131a70715004b5db0455",
          "sha256": "893fc007b7ca12037defc4fae6bcec489bfaf5c00fd9a9b990bf463b35db8851"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2.7.tar.gz",
        "has_sig": false,
        "md5_digest": "79305432cd08131a70715004b5db0455",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 48880,
        "upload_time": "2022-07-27T19:50:34",
        "upload_time_iso_8601": "2022-07-27T19:50:34.597654Z",
        "url": "https://files.pythonhosted.org/packages/53/c8/038a2c2e1ec70304cb10f6d030170f445bd3354fcfec7aa5a83956ae7a6a/parallelformers-1.2.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5cddaca8e4d2b275850bed03fee8ce8ec99f0f68441a3b7175156c3abf8226b8",
          "md5": "48a92f020d69eb05b58b9be8bbf24ca3",
          "sha256": "3ce9cb5ff619fa76f69e6d60a75ce2f7e23990e54ed9bb67c8a97b9f72ad32df"
        },
        "downloads": -1,
        "filename": "parallelformers-1.2b0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "48a92f020d69eb05b58b9be8bbf24ca3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 61482,
        "upload_time": "2021-07-07T18:14:14",
        "upload_time_iso_8601": "2021-07-07T18:14:14.335200Z",
        "url": "https://files.pythonhosted.org/packages/5c/dd/aca8e4d2b275850bed03fee8ce8ec99f0f68441a3b7175156c3abf8226b8/parallelformers-1.2b0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f96797d4c637fc892ec29d9910caa3aa491adb0707c88827a60328222728ac80",
          "md5": "a31ccbb994e8d5b99521ae6704443200",
          "sha256": "955d2c9c015fcd945d26bd7c139dba356e0ac9b5ef23a6c4768a2d327bbc5647"
        },
        "downloads": -1,
        "filename": "parallelformers-1.3b0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a31ccbb994e8d5b99521ae6704443200",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 61509,
        "upload_time": "2021-07-07T22:08:36",
        "upload_time_iso_8601": "2021-07-07T22:08:36.256641Z",
        "url": "https://files.pythonhosted.org/packages/f9/67/97d4c637fc892ec29d9910caa3aa491adb0707c88827a60328222728ac80/parallelformers-1.3b0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.4b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ec1ee397a9e7826a04c506435b3693ebee37710d6422f87cdf8c71cd2a75c1fb",
          "md5": "53d1060293c06a6e571ac6d755c0d593",
          "sha256": "c356772b6078e2c629e2b0b4da524a505a0654f2bad234d3ca1fe26b2b7a1918"
        },
        "downloads": -1,
        "filename": "parallelformers-1.4b0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "53d1060293c06a6e571ac6d755c0d593",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 61493,
        "upload_time": "2021-07-08T02:03:23",
        "upload_time_iso_8601": "2021-07-08T02:03:23.302347Z",
        "url": "https://files.pythonhosted.org/packages/ec/1e/e397a9e7826a04c506435b3693ebee37710d6422f87cdf8c71cd2a75c1fb/parallelformers-1.4b0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.5b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "71e06dc2b7f9b48fbaa6f6fae5c17179a26477f8a3854704deb87f99a00ba8dc",
          "md5": "d5952a7be19f31fc659cb91edcb68bc8",
          "sha256": "e3f2303450c3da236a6b3712bd5f4d7581164781d919018c788045f68fdab7a7"
        },
        "downloads": -1,
        "filename": "parallelformers-1.5b0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d5952a7be19f31fc659cb91edcb68bc8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 61520,
        "upload_time": "2021-07-08T02:38:35",
        "upload_time_iso_8601": "2021-07-08T02:38:35.424622Z",
        "url": "https://files.pythonhosted.org/packages/71/e0/6dc2b7f9b48fbaa6f6fae5c17179a26477f8a3854704deb87f99a00ba8dc/parallelformers-1.5b0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.6b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9f76022bbd7fbd16a13e366bbfe917790a6ef87fd4b31301a8fa333f638bc478",
          "md5": "4f603030d834147f8ac2cf4c78bf6767",
          "sha256": "63c6126b72a51f40497abaee7f501f631727a58086e538e19c5add4d511df29e"
        },
        "downloads": -1,
        "filename": "parallelformers-1.6b0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4f603030d834147f8ac2cf4c78bf6767",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 61502,
        "upload_time": "2021-07-08T02:46:14",
        "upload_time_iso_8601": "2021-07-08T02:46:14.594434Z",
        "url": "https://files.pythonhosted.org/packages/9f/76/022bbd7fbd16a13e366bbfe917790a6ef87fd4b31301a8fa333f638bc478/parallelformers-1.6b0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "53c8038a2c2e1ec70304cb10f6d030170f445bd3354fcfec7aa5a83956ae7a6a",
        "md5": "79305432cd08131a70715004b5db0455",
        "sha256": "893fc007b7ca12037defc4fae6bcec489bfaf5c00fd9a9b990bf463b35db8851"
      },
      "downloads": -1,
      "filename": "parallelformers-1.2.7.tar.gz",
      "has_sig": false,
      "md5_digest": "79305432cd08131a70715004b5db0455",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 48880,
      "upload_time": "2022-07-27T19:50:34",
      "upload_time_iso_8601": "2022-07-27T19:50:34.597654Z",
      "url": "https://files.pythonhosted.org/packages/53/c8/038a2c2e1ec70304cb10f6d030170f445bd3354fcfec7aa5a83956ae7a6a/parallelformers-1.2.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}