{
  "info": {
    "author": "Pukar Acharya",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Overview\n-----------------------------------------------------------------------------\n\nurl_crawler is a Python library to crawl the details of a URL. \n\n## Usage\n\n    from url_crawler import url_crawler\n    '''\n      url -> string URL to crawl for information.\n    '''\n    package_details = url_crawler(url)\n    \n    print(package_details.url)\n    print(package_details.domain)\n    print(package_details.check_https)\n    print(package_details.dot_count)\n    print(package_details.digit_count)\n    print(package_details.url_length)\n    \n**Utilities**\n\n| Name           | Output | Description  |\n| ------------- | -----| -----|\n| url | str | Returns the string url. |\n| domain | str | Returns the domain of the url. |\n| registrar | str | Returns the registrar for the given URL. |\n| registered_country | str | Returns the registered domain country of the given URL. |\n| whois | dict | Returns the whois information of the given URL. |\n| registration_date | int | Returns the number of days since registration of the given URL. |\n| expiry_date | int | Returns the number of days to expiration of the given URL. |\n| intended_lifespan | int | Returns the number of days of intended lifespan of the given URL. |\n| dot_count | int | Returns the dot(.) count in the given URL. |\n| digit_count | int | Returns the digit count in the given URL. |\n| url_length | int | Returns the length of the given URL. |\n| fragments_count | int | Returns the fragment counts in the given URL. |\n| entropy | int | Returns the entropy of the given URL. |\n| check_http | bool | Checks for http headers in the given URL. |\n| check_http | bool | Checks for https headers in the given URL. |\n| url_response | bool | Checks for the URL response. |\n| check_encoding | bool | Checks for encoding in in the given URL. |\n| check_client | bool | Checks for client keyword in the given URL. |\n| check_admin | bool | Checks for admin keyword in the given URL. |\n| check_server | bool | Checks for server keyword in the given URL. |\n| check_login | bool | Checks for login keyword in the given URL. |\n| check_ports | bool | Checks for any ports in the given URL. |\n\n## Requirements\n\nThe `requirements.txt` file has details of all Python libraries for this package, and can be installed using \n```\npip install -r requirements.txt\n```\n\n## Organization\n\n    ├── src\n    │   ├── url_crawler\n              ├── init             <- init\n              ├── url_crawler      <- package source code for URL crawler\n    ├── setup.py             <- setup file \n    ├── LICENSE              <- LICENSE\n    ├── README.md            <- README\n    ├── CONTRIBUTING.md      <- contribution\n    ├── test.py              <- test cases for unit testing\n    ├── requirements.txt     <- requirements file for reproducing the code package\n\n## License\n\nMIT\n\n## Contributions\n\nFor steps on code contribution, please see [CONTRIBUTING](./CONTRIBUTING.md).\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/iampukar/url_crawler",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "url-crawler",
    "package_url": "https://pypi.org/project/url-crawler/",
    "platform": null,
    "project_url": "https://pypi.org/project/url-crawler/",
    "project_urls": {
      "Bug Tracker": "https://github.com/iampukar/url_crawler/issues",
      "Homepage": "https://github.com/iampukar/url_crawler"
    },
    "release_url": "https://pypi.org/project/url-crawler/1.0.0/",
    "requires_dist": null,
    "requires_python": ">=3.9.7",
    "summary": "A Python library to crawl the details of a URL.",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13345705,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "92d5def49b1434b576cda9f116d8b013c5d84394f289180cfc7ca41b8f7c74d6",
          "md5": "fae1443c47e01ebda9a59ea6ff953e60",
          "sha256": "6b5835cf494b9bbc83dc9518ed48747414a547402ca911c1887cf8ebced1ffc0"
        },
        "downloads": -1,
        "filename": "url_crawler-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fae1443c47e01ebda9a59ea6ff953e60",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9.7",
        "size": 4406,
        "upload_time": "2022-03-30T12:16:28",
        "upload_time_iso_8601": "2022-03-30T12:16:28.015831Z",
        "url": "https://files.pythonhosted.org/packages/92/d5/def49b1434b576cda9f116d8b013c5d84394f289180cfc7ca41b8f7c74d6/url_crawler-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9054be2bbcc941df501e27d5b25723c539fb11b8b673c7fd1769a7046d005694",
          "md5": "75a8e61fd51689cdff447deb4b4d25b6",
          "sha256": "21c7dfcd132ad400d95df2336a1e4f8b2b66d8c3bda49cb7a28ec676b1400dfa"
        },
        "downloads": -1,
        "filename": "url_crawler-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "75a8e61fd51689cdff447deb4b4d25b6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9.7",
        "size": 4067,
        "upload_time": "2022-03-30T12:16:29",
        "upload_time_iso_8601": "2022-03-30T12:16:29.747819Z",
        "url": "https://files.pythonhosted.org/packages/90/54/be2bbcc941df501e27d5b25723c539fb11b8b673c7fd1769a7046d005694/url_crawler-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "92d5def49b1434b576cda9f116d8b013c5d84394f289180cfc7ca41b8f7c74d6",
        "md5": "fae1443c47e01ebda9a59ea6ff953e60",
        "sha256": "6b5835cf494b9bbc83dc9518ed48747414a547402ca911c1887cf8ebced1ffc0"
      },
      "downloads": -1,
      "filename": "url_crawler-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fae1443c47e01ebda9a59ea6ff953e60",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.9.7",
      "size": 4406,
      "upload_time": "2022-03-30T12:16:28",
      "upload_time_iso_8601": "2022-03-30T12:16:28.015831Z",
      "url": "https://files.pythonhosted.org/packages/92/d5/def49b1434b576cda9f116d8b013c5d84394f289180cfc7ca41b8f7c74d6/url_crawler-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9054be2bbcc941df501e27d5b25723c539fb11b8b673c7fd1769a7046d005694",
        "md5": "75a8e61fd51689cdff447deb4b4d25b6",
        "sha256": "21c7dfcd132ad400d95df2336a1e4f8b2b66d8c3bda49cb7a28ec676b1400dfa"
      },
      "downloads": -1,
      "filename": "url_crawler-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "75a8e61fd51689cdff447deb4b4d25b6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.9.7",
      "size": 4067,
      "upload_time": "2022-03-30T12:16:29",
      "upload_time_iso_8601": "2022-03-30T12:16:29.747819Z",
      "url": "https://files.pythonhosted.org/packages/90/54/be2bbcc941df501e27d5b25723c539fb11b8b673c7fd1769a7046d005694/url_crawler-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}