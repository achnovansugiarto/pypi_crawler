{
  "info": {
    "author": "Seonghyeon Boris Moon",
    "author_email": "boris.moon514@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# connlp\nA bunch of python codes to analyze text data in the construction industry.  \nMainly reconstitute the pre-exist python libraries for Natural Language Processing (NLP).\n\n## _Project Information_\n- Supported by C!LAB (@Seoul Nat'l Univ.)\n\n## _Contributors_\n- Seonghyeon Boris Moon (blank54@snu.ac.kr, https://github.com/blank54/)\n- Sehwan Chung (hwani751@snu.ac.kr)\n- Jungyeon Kim (janykjy@snu.ac.kr)\n\n# Initialize\n\n## _Setup_\n\nInstall _**connlp**_ with _pip_.\n\n```shell\npip install connlp\n```\n\nInstall _requirements.txt_.\n\n```shell\ncd WORKSPACE\nwget -O requirements_connlp.txt https://raw.githubusercontent.com/blank54/connlp/master/requirements.txt\npip install -r requirements_connlp.txt\n```\n\n## _Test_\n\nIf the code below runs with no error, _**connlp**_ is installed successfully.\n\n```python\nfrom connlp.test import hello\nhello()\n\n# 'Helloworld'\n```\n\n# Preprocess\n\nPreprocessing module supports English and Korean.  \nNOTE: No plan for other languages (by 2021.04.02.).\n\n## _Normalizer_\n\n_**Normalizer**_ normalizes the input text by eliminating trash characters and remaining numbers, alphabets, and punctuation marks.\n\n```python\nfrom connlp.preprocess import Normalizer\nnormalizer = Normalizer()\n\nnormalizer.normalize(text='I am a boy!')\n\n# 'i am a boy'\n```\n\n## _EnglishTokenizer_\n\n_**EnglishTokenizer**_ tokenizes the input text in English based on word spacing.  \nThe ngram-based tokenization is in preparation.\n\n```python\nfrom connlp.preprocess import EnglishTokenizer\ntokenizer = EnglishTokenizer()\n\ntokenizer.tokenize(text='I am a boy!')\n\n# ['I', 'am', 'a', 'boy!']\n```\n\n## _KoreanTokenizer_\n\n_**KoreanTokenizer**_ tokenizes the input text in Korean, and is based on either pre-trained or unsupervised approaches.\n\nYou are recommended to use pre-trained method unless you have a large size of corpus. This is the default setting.\n\nIf you want to use a pre-trained tokenizer, you have to select which analyzer you want to use. Available analyzers are based on KoNLPy (https://konlpy.org/ko/latest/api/konlpy.tag/), a python package for Korean language processing. The default analyzer is _**Hannanum**_\n\n```python\nfrom connlp.preprocess import KoreanTokenizer\ntokenizer = KoreanTokenizer(pre_trained=True, analyzer='Hannanum')\n```\n\nIf your corpus is big, you may use an unsupervised method, which is based on _**soynlp**_ (https://github.com/lovit/soynlp), an unsupervised text analyzer in Korean.\n\n```python\nfrom connlp.preprocess import KoreanTokenizer\ntokenizer = KoreanTokenizer(pre_trained=False)\n```\n\n### _train_\n\nIf your _**KoreanTokenizer**_ are pre-trained, you can neglect this step.\n\nOtherwhise (i.e. you are using an unsupervised approach), the _**KoreanTokenizer**_ object first needs to be trained on (unlabeled) corpus. 'Word score' is calculated for every subword in the corpus.\n\n```python\nfrom connlp.preprocess import KoreanTokenizer\ntokenizer = KoreanTokenizer(pre_trained=False)\n\ndocs = ['코퍼스의 첫 번째 문서입니다.', '두 번째 문서입니다.', '마지막 문서']\n\ntokenizer.train(text=docs)\nprint(tokenizer.word_score)\n\n# {'서': 0.0, '코': 0.0, '째': 0.0, '.': 0.0, '의': 0.0, '마': 0.0, '막': 0.0, '번': 0.0, '문': 0.0, '코퍼': 1.0, '번째': 1.0, '마지': 1.0, '문서': 1.0, '코퍼스': 1.0, '문서입': 0.816496580927726, '마지막': 1.0, '코퍼스의': 1.0, '문서입니': 0.8735804647362989, '문서입니다': 0.9036020036098448, '문서입니다.': 0.9221079114817278}\n```\n\n### _tokenize_\n\nIf you are using a pre-trained _**KoreanTokenizer**_, the selected KoNLPy analyzer will tokenize the input sentence based on morphological analysis.\n\n```python\nfrom connlp.preprocess import KoreanTokenizer\ntokenizer = KoreanTokenizer(pre_trained=True, analyzer='Hannanum')\ndoc = docs[0] # '코퍼스의 첫 번째 문서입니다.'\ntokenizer.tokenize(doc)\n\n# ['코퍼스', '의', '첫', '번째', '문서', '입니다', '.']\n```\n\nIf you are using an unsupervised _**KoreanTokenizer**_, tokenization is based on the 'word score' calculated from _**KoreanTokenizer.train**_ method.\n\nFor each blank-separated token, a subword that has the maximum 'word score' is selectd as an individual 'word' and separated with the remaining part.\n\n```python\nfrom connlp.preprocess import KoreanTokenizer\ntokenizer = KoreanTokenizer(pre_trained=False)\ndoc = docs[0] # '코퍼스의 첫 번째 문서입니다.'\ntokenizer.tokenize(doc)\n\n# ['코퍼스의', '첫', '번째', '문서', '입니다.']\n```\n\n## _StopwordRemover_\n\n_**StopwordRemover**_ removes stopwords from a given sentence based on the user-customized stopword list.  \nBefore utilizing _**StopwordRemover**_ the user should normalize and tokenize the docs.\n\n```python\nfrom connlp.preprocess import Normalizer, EnglishTokenizer, StopwordRemover\nnormalizer = Normalizer()\neng_tokenizer = EnglishTokenizer()\nstopword_remover = StopwordRemover()\n\ndocs = ['I am a boy!', 'He is a boy..', 'She is a girl?']\ntokenized_docs = []\n\nfor doc in eng_docs:\n    normalized_doc = normalizer.normalize(text=doc)\n    tokenized_doc = eng_tokenizer.tokenize(text=normalized_doc)\n    tokenized_docs.append(tokenized_doc)\n\nprint(docs)\nprint(tokenized_docs)\n\n# ['I am a boy!', 'He is a boy..', 'She is a girl?']\n# [['i', 'am', 'a', 'boy'], ['he', 'is', 'a', 'boy'], ['she', 'is', 'a', 'girl']]\n```\n\nThe user should prepare a customized stopword list (i.e., _stoplist_).  \nThe _stoplist_ should include user-customized stopwords divided by '\\n' and the file should be in \".txt\" format.\n\n```text\na\nis\nam\n```\n\nInitiate the _**StopwordRemover**_ with appropriate filepath of user-customized stopword list.  \nIf the stoplist is absent at the filepath, the stoplist would be ramain as a blank list.\n\n```python\nfpath_stoplist = 'test/thesaurus/stoplist.txt'\nstopword_remover.initiate(fpath_stoplist=fpath_stoplist)\n\nprint(stopword_remover)\n\n# <connlp.preprocess.StopwordRemover object at 0x7f163e70c050>\n```\n\nThe user can count the word frequencies and figure out additional stopwords based on the results.\n\n```python\nstopword_remover.count_freq_words(docs=tokenized_docs)\n\n# ========================================\n# Word counts\n#   | [1] a: 3\n#   | [2] boy: 2\n#   | [3] is: 2\n#   | [4] i: 1\n#   | [5] am: 1\n#   | [6] he: 1\n#   | [7] she: 1\n#   | [8] girl: 1\n```\n\nAfter finally updating the _stoplist_, use _**remove**_ method to remove the stopwords from text.\n\n```python\nstopword_removed_docs = []\n    for doc in tokenized_docs:\n        stopword_removed_docs.append(stopword_remover.remove(sent=doc))\n\nprint(stopword_removed_docs)\n\n# [['i', 'boy'], ['he', 'boy'], ['she', 'girl']]\n```\n\nThe user can check which stopword was removed with _**check_removed_words**_ methods.\n\n```python\nstopword_remover.check_removed_words(docs=tokenized_docs, stopword_removed_docs=stopword_removed_docs)\n\n# ========================================\n# Check stopwords removed\n#   | [1] BEFORE: a(3) ->\n#   | [2] BEFORE: boy -> AFTER: boy(2)\n#   | [3] BEFORE: is(2) ->\n#   | [4] BEFORE: i -> AFTER: i(1)\n#   | [5] BEFORE: am(1) ->\n#   | [6] BEFORE: he -> AFTER: he(1)\n#   | [7] BEFORE: she -> AFTER: she(1)\n#   | [8] BEFORE: girl -> AFTER: girl(1)\n```\n\n# Embedding\n\n## _Vectorizer_\n\n_**Vectorizer**_ includes several text embedding methods that have been commonly used for decades.  \n\n### _tfidf_\n\nTF-IDF is the most commonly used technique for word embedding.  \nThe TF-IDF model counts the term frequency(TF) and inverse document frequency(IDF) from the given documents.  \nThe results included the followings.  \n- TF-IDF Vectorizer (a class of sklearn.feature_extraction.text.TfidfVectorizer')\n- TF-IDF Matrix\n- TF-IDF Vocabulary\n\n```python\nfrom connlp.preprocess import EnglishTokenizer\nfrom connlp.embedding import Vectorizer\ntokenizer = EnglishTokenizer()\nvectorizer = Vectorizer()\n\ndocs = ['I am a boy', 'He is a boy', 'She is a girl']\ntfidf_vectorizer, tfidf_matrix, tfidf_vocab = vectorizer.tfidf(docs=docs)\ntype(tfidf_vectorizer)\n\n# <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n```\n\nThe user can get a document vector by indexing the _**tfidf_matrix**_.\n\n```python\ntfidf_matrix[0]\n\n# (0, 2)    0.444514311537431\n# (0, 0)    0.34520501686496574\n# (0, 1)    0.5844829010200651\n# (0, 5)    0.5844829010200651\n```\n\nThe _**tfidf_vocab**_ returns an index for every token.\n\n```python\nprint(tfidf_vocab)\n\n# {'i': 5, 'am': 1, 'a': 0, 'boy': 2, 'he': 4, 'is': 6, 'she': 7, 'girl': 3}\n```\n\n### _word2vec_\n\nWord2Vec is a distributed representation language model for word embedding.  \nThe Word2vec model trains tokenized docs and returns word vectors.  \nThe result is a class of 'gensim.models.word2vec.Word2Vec'.\n\n```python\nfrom connlp.preprocess import EnglishTokenizer\nfrom connlp.embedding import Vectorizer\ntokenizer = EnglishTokenizer()\nvectorizer = Vectorizer()\n\ndocs = ['I am a boy', 'He is a boy', 'She is a girl']\ntokenized_docs = [tokenizer.tokenize(text=doc) for doc in docs]\nw2v_model = vectorizer.word2vec(docs=tokenized_docs)\ntype(w2v_model)\n\n# <class 'gensim.models.word2vec.Word2Vec'>\n```\n\nThe user can get a word vector by _**.wv**_ method.\n\n```python\nw2v_model.wv['boy']\n\n# [-2.0130998e-03 -3.5652996e-03  2.7793974e-03 ...]\n```\n\nThe Word2Vec model provides the _topn_-most similar word vectors.\n\n```python\nw2v_model.wv.most_similar('boy', topn=3)\n\n# [('He', 0.05311150848865509), ('a', 0.04154288396239281), ('She', -0.029122961685061455)]\n```\n\n### _word2vec (update)_\n\nThe user can update the Word2Vec model with new data.\n\n```python\nnew_docs = ['Tom is a man', 'Sally is not a boy']\ntokenized_new_docs = [tokenizer.tokenize(text=doc) for doc in new_docs]\nw2v_model_updated = vectorizer.word2vec_update(w2v_model=w2v_model, new_docs=tokenized_new_docs)\n\nw2v_model_updated.wv['man']\n\n# [4.9649975e-03  3.8002312e-04 -1.5773597e-03 ...]\n```\n\n### _doc2vec_\n\nDoc2Vec is a distributed representation language model for longer text (e.g., sentence, paragraph, document) embedding.  \nThe Doc2vec model trains tokenized docs with tags and returns document vectors.  \nThe result is a class of 'gensim.models.doc2vec.Doc2Vec'.\n\n```python\nfrom connlp.preprocess import EnglishTokenizer\nfrom connlp.embedding import Vectorizer\ntokenizer = EnglishTokenizer()\nvectorizer = Vectorizer()\n\ndocs = ['I am a boy', 'He is a boy', 'She is a girl']\ntagged_docs = [(idx, tokenizer.tokenize(text=doc)) for idx, doc in enumerate(docs)]\nd2v_model = vectorizer.doc2vec(tagged_docs=tagged_docs)\ntype(d2v_model)\n\n# <class 'gensim.models.doc2vec.Doc2Vec'>\n```\n\nThe Doc2Vec model can infer a new document.\n\n```python\ntest_doc = ['My', 'name', 'is', 'Peter']\nd2v_model.infer_vector(doc_words=test_doc)\n\n# [4.8494316e-03 -4.3647490e-03  1.1437446e-03 ...]\n```\n\n# Analysis\n\n## _TopicModel_\n\n_**TopicModel**_ is a class for topic modeling based on gensim LDA model.  \nIt provides a simple way to train lda model and assign topics to docs.  \n\nBefore using LDA topic modeling, the user should install the following packages.\n\n```shell\npip install pyldavis==2.1.2\n```\n\n_**TopicModel**_ requires two instances.  \n- a dict of docs whose keys are the tag\n- the number of topics for modeling\n\n```python\nfrom connlp.analysis_lda import TopicModel\n\nnum_topics = 2\ndocs = {'doc1': ['I', 'am', 'a', 'boy'],\n        'doc2': ['He', 'is', 'a', 'boy'],\n        'doc3': ['Cat', 'on', 'the', 'table'],\n        'doc4': ['Mike', 'is', 'a', 'boy'],\n        'doc5': ['Dog', 'on', 'the', 'table'],\n        }\n\nlda_model = TopicModel(docs=docs, num_topics=num_topics)\n```\n\n### _learn_\n\nThe user can train the model with _learn_ method.\nUnless parameters being provided by the user, the model trains based on default parameters.  \n\nAfter _learn_, _**TopicModel**_ provides _model_ instance that is a class of <'gensim.models.ldamodel.LdaModel'>\n\n\n```python\nparameters = {\n    'iterations': 100,\n    'alpha': 0.7,\n    'eta': 0.05,\n}\nlda_model.learn(parameters=parameters)\ntype(lda_model.model)\n\n# <class 'gensim.models.ldamodel.LdaModel'>\n```\n\n### _coherence_\n\n_**TopicModel**_ provides coherence value for model evaluation.  \nThe coherence value is automatically calculated right after model training.\n\n```python\nprint(lda_model.coherence)\n\n# 0.3607990279229385\n```\n\n### _assign_\n\nThe user can easily assign the most proper topic to each doc using _assign_ method.  \nAfter _assign_, the _**TopicModel**_ provides _tag2topic_ and _topic2tag_ instances for convenience.\n\n```python\nlda_model.assign()\n\nprint(lda_model.tag2topic)\nprint(lda_model.topic2tag)\n\n# defaultdict(<class 'int'>, {'doc1': 1, 'doc2': 1, 'doc3': 0, 'doc4': 1, 'doc5': 0})\n# defaultdict(<class 'list'>, {1: ['doc1', 'doc2', 'doc4'], 0: ['doc3', 'doc5']})\n```\n\n## _NamedEntityRecognition_\n\nBefore using NER modules, the user should install proper versions of TensorFlow and Keras.  \n\n```shell\npip install config==0.4.2 gensim==3.8.1 gpustat==0.6.0 GPUtil==1.4.0 h5py==2.10.0 JPype1==0.7.1 Keras==2.2.4 konlpy==0.5.2 nltk==3.4.5 numpy==1.18.1 pandas==1.0.1 scikit-learn==0.22.1 scipy==1.4.1 silence-tensorflow==1.1.1 soynlp==0.0.493 tensorflow==1.14.0 tensorflow-gpu==1.14.0\n```\n\nThe modules might require the module of _keras-contrib_.  \nThe user can install the module by following the below.  \n\n```shell\ngit clone https://www.github.com/keras-team/keras-contrib.git \ncd keras-contrib \npython setup.py install\n```\n\n### _Labels_\n\n_**NER_Model**_ is a class to conduct named entity recognition using Bi-directional Long-Short Term Memory (Bi-LSTM) and Conditional Random Field (CRF).  \n\nAt the beginning, appropriate labels are required.  \nThe labels should be numbered with start of 0.\n\n```python\nfrom connlp.analysis_ner import NER_Labels\n\nlabel_dict = {'NON': 0,     #None\n              'PER': 1,     #PERSON\n              'FOD': 2,}    #FOOD\n\nner_labels = NER_Labels(label_dict=label_dict)\n```\n\n### _Corpus_\n\nNext, the user should prepare data including sentences and labels, of which each data being matched by the same tag.  \nThe tokenized sentences and labels are then combined via _**NER_LabeledSentence**_.  \nWith the data, labels, and a proper size of _max_sent_len_ (i.e., the maximum length of sentence for analysis), _**NER_Corpus**_ would be developed.  \nOnce the corpus was developed, every data of sentences and labels would be padded with the length of _max_sent_len_.  \n\n```python\nfrom connlp.preprocess import EnglishTokenizer\nfrom connlp.analysis_ner import NER_LabeledSentence, NER_Corpus\ntokenizer = EnglishTokenizer()\n\ndata_sents = {'sent1': 'Sam likes pizza',\n              'sent2': 'Erik eats pizza',\n              'sent3': 'Erik and Sam are drinking soda',\n              'sent4': 'Flora cooks chicken',\n              'sent5': 'Sam ordered a chicken',\n              'sent6': 'Flora likes chicken sandwitch',\n              'sent7': 'Erik likes to drink soda'}\ndata_labels = {'sent1': [1, 0, 2],\n               'sent2': [1, 0, 2],\n               'sent3': [1, 0, 1, 0, 0, 2],\n               'sent4': [1, 0, 2],\n               'sent5': [1, 0, 0, 2],\n               'sent6': [1, 0, 2, 2],\n               'sent7': [1, 0, 0, 0, 2]}\n\ndocs = []\nfor tag, sent in data_sents.items():\n    words = [str(w) for w in tokenizer.tokenize(text=sent)]\n    labels = data_labels[tag]\n    docs.append(NER_LabeledSentence(tag=tag, words=words, labels=labels))\n\nmax_sent_len = 10\nner_corpus = NER_Corpus(docs=docs, ner_labels=ner_labels, max_sent_len=max_sent_len)\ntype(ner_corpus)\n\n# <class 'connlp.analysis_ner.NER_Corpus'>\n```\n\n### _Word Embedding_\n\nEvery word in the _**NER_Corpus**_ should be embedded into numeric vector space.  \nThe user can conduct embedding with Word2Vec which is provided in _**Vectorizer**_ of _**connlp**_.  \nNote that the embedding process of _**NER_Corpus**_ only requires the dictionary of word vectors and the feature size.  \n\n```python\nfrom connlp.preprocess import EnglishTokenizer\nfrom connlp.embedding import Vectorizer\ntokenizer = EnglishTokenizer()\nvectorizer = Vectorizer()\n\ntokenized_sents = [tokenizer.tokenize(sent) for sent in data_sents.values()]\nw2v_model = vectorizer.word2vec(docs=tokenized_sents)\n\nword2vector = vectorizer.get_word_vectors(w2v_model)\nfeature_size = w2v_model.vector_size\nner_corpus.word_embedding(word2vector=word2vector, feature_size=feature_size)\nprint(ner_corpus.X_embedded)\n\n# [[[-2.40120804e-03  1.74632657e-03  ...]\n#   [-3.57543468e-03  2.86567654e-03  ...]\n#   ...\n#   [ 0.00000000e+00  0.00000000e+00  ...]] ...]\n```\n\n### _Model Initialization_\n\nThe parameters for Bi-LSTM and model training should be provided, however, they can be composed of a single dictionary.  \nThe user should initialize the _**NER_Model**_ with _**NER_Corpus**_ and the parameters.\n\n```python\nfrom connlp.analysis_ner import NER_Model\n\nparameters = {\n    # Parameters for Bi-LSTM.\n    'lstm_units': 512,\n    'lstm_return_sequences': True,\n    'lstm_recurrent_dropout': 0.2,\n    'dense_units': 100,\n    'dense_activation': 'relu',\n\n    # Parameters for model training.\n    'test_size': 0.3,\n    'batch_size': 1,\n    'epochs': 100,\n    'validation_split': 0.1,\n}\n\nner_model = NER_Model()\nner_model.initialize(ner_corpus=ner_corpus, parameters=parameters)\ntype(ner_model)\n\n# <class 'connlp.analysis_ner.NER_Model'>\n```\n\n### _Model Training_\n\nThe user can train the _**NER_Model**_ with customized parameters.  \nThe model automatically gets the dataset from the _**NER_Corpus**_.  \n\n```python\nner_model.train(parameters=parameters)\n\n# Train on 3 samples, validate on 1 samples\n# Epoch 1/100\n# 3/3 [==============================] - 3s 1s/step - loss: 1.4545 - crf_viterbi_accuracy: 0.3000 - val_loss: 1.0767 - val_crf_viterbi_accuracy: 0.8000\n# Epoch 2/100\n# 3/3 [==============================] - 0s 74ms/step - loss: 0.8602 - crf_viterbi_accuracy: 0.7000 - val_loss: 0.5287 - val_crf_viterbi_accuracy: 0.8000\n# ...\n```\n\n### _Model Evaluation_\n\nThe model performance can be shown in the aspects of confusion matrix and F1 score.\n\n```python\nner_model.evaluate()\n\n# |--------------------------------------------------\n# |Confusion Matrix:\n# [[ 3  0  3  6]\n#  [ 1  3  0  4]\n#  [ 0  0  2  2]\n#  [ 4  3  5 12]]\n# |--------------------------------------------------\n# |F1 Score: 0.757\n# |--------------------------------------------------\n# |    [NON]: 0.600\n# |    [PER]: 0.857\n# |    [FOD]: 0.571\n```\n\n### _Save_\n\nThe user can save the _**NER_Model**_.  \nThe model would save the model itself (\"\\<FileName\\>.pk\") and the dataset (\"\\<FileName\\>-dataset.pk\") that was used in model development.  \nNote that the directory should exist before saving the model.  \n\n```python\nfrom connlp.util import makedir\n\nfpath_model = 'test/ner/model.pk'\nmakedir(fpath=fpath_model)\nner_model.save(fpath_model=fpath_model)\n```\n\n### _Load_\n\nIf the user wants to load the already trained model, just call the model and load.  \n\n```python\nfpath_model = 'test/ner/model.pk'\nner_model = NER_Model()\nner_model.load(fpath_model=fpath_model, ner_corpus=ner_corpus, parameters=parameters)\n```\n\n### _Prediction_\n\n_**NER_Model**_ can conduct a new NER task on the given sentence.  \nThe result is a class of _**NER_Result**_.  \n\n```python\nfrom connlp.preprocess import EnglishTokenizer\nvectorizer = Vectorizer()\n\nnew_sent = 'Tom eats apple'\ntokenized_sent = tokenizer.tokenize(new_sent)\nner_result = ner_model.predict(sent=tokenized_sent)\nprint(ner_result)\n\n# Tom/PER eats/NON apple/FOD\n```\n\n## _Web Crawling_\n\nThe _**connlp**_ currently provides web crawling for Naver news articles.\n\n### _Query_\n\nThe user should prepare the proper queries first.  \nA single text file(.txt) should include every information of the query as below.  \n- Date Start\n- Date End\n- Keywords\n\nThe web crawler utilizes the keywords separated with '\\n\\n' in the same time.  \nMeanwhile, the web crawler utilizes the keywords separated with '\\n' as a different queries.\n\nFor example, if the queries are determined as below, the web crawler would search the articles with six queries: \"smart+construction+safety at 20210718\", \"smart+construction+management at 20210718\", \"smart+construction+safety at 20210719\", ...\n\n```plain\n20210718\n20210720\n\nsmart\n\nconstruction\n\nsafety\nmanagement\n```\n\nThe _**NewsQueryParser**_ parses the queries into appropriate formats.\n\n```python\nfrom connlp.web_crawling import NewsQueryParser\nquery_parser = NewsQueryParser()\n\nfpath_query = 'FILEPATH_OF_YOUR_QUERY'\nquery_list, date_list = query_parser.parse(fpath_query=fpath_query)\n```\n\n### _URLs_\n\nFor the second step, the web crawler parses the web page that shows the list of news articles.  \n_**NaverNewsListScraper**_ provides the function of parsing the list page.  \nThe user is recommended to save the url lists and load them later.\n\n```python\nfrom connlp.web_crawling import NaverNewsListScraper\nlist_scraper = NaverNewsListScraper()\n\nfor date in sorted(date_list, reverse=False):\n    for query in query_list:\n        url_list = list_scraper.get_url_list(query=query, date=date)\n```\n\n### _Articles_\n\nThe last step is to parse the article page and get information from the article.  \n_**NaverNewsArticleParser**_ returns a class of _**Article**_ for a given article.  \nRemember to extend the query list of the article.\n\n```python\nfrom connlp.web_crawling import NaverNewsArticleParser\narticle_parser = NaverNewsArticleParser()\n\nquery_list, _ = query_parser.urlname2query(fname_url_list=fname_url_list)\nfor url in url_list:\n    article = article_parser.parse(url=url)\n    article.extend_query(query_list)\n```\n\n### _Status_\n\n_**NewsStatus**_ provides the status of the crawled corpus for given directories.\n\n```python\nfrom connlp.web_crawling import NewsStatus\nnews_status = NewsStatus()\n\nfdir_queries = 'DIRECTORY_FOR_QUERIES'\nfdir_url_list = 'DIRECTORY_FOR_URLS'\nfdir_article = 'DIRECTORY_FOR_ARTICLES'\n\nnews_status.queries(fdir_queries=fdir_queries)\nnews_status.urls(fdir_urls=fdir_url_list)\nnews_status.articles(fdir_articles=fdir_article)\n```\n\n# Visualization\n\n## _Visualizer_\n\n_**Visualizer**_ includes several simple tools for text visualization.\n\nInstall the following packages.\n\n```\npip install networkx wordcloud\n```\n\n### _network_\n\n_**network**_ method provides a word network for tokenized docs.\n\n```python\nfrom connlp.preprocess import EnglishTokenizer\nfrom connlp.visualize import Visualizer\ntokenizer = EnglishTokenizer()\nvisualizer = Visualizer()\n\ndocs = ['I am a boy', 'She is a girl']\ntokenized_docs = [tokenizer.tokenize(text=doc) for doc in docs]\nword_network = visualizer.network(docs=tokenized_docs, show=True)\n```\n\nThe word network is a _matplotlib.pyplot_ object.  \nThe user can save the figure by _.savefig()_ method.\n\n```python\nword_network.savefig(FILEPATH)\n```\n\n### _wordcloud_\n\n_**wordcloud**_ method provides a word cloud for tokenized docs.\n\n```python\nfrom connlp.preprocess import EnglishTokenizer\nfrom connlp.visualize import Visualizer\ntokenizer = EnglishTokenizer()\nvisualizer = Visualizer()\n\ndocs = ['I am a boy', 'She is a girl']\ntokenized_docs = [tokenizer.tokenize(text=doc) for doc in docs]\nwordcloud = visualizer.wordcloud(docs=tokenized_docs, show=True)\n```\n\nThe wordcloud is a _matplotlib.pyplot_ object.  \nThe user can save the figure by _.savefig()_ method.\n\n```python\nwordcloud.savefig(FILEPATH)\n```\n\n# Extracting Text\n\n## _TextConverter_\n\n_**TextConverter**_ includes several methods that extract raw text from various types of files (e.g. PDF, HWP) and/or converts the files into plain text files (e.g. TXT).\n\n### _hwp2txt_\n\n_**hwp2txt**_ method converts a HWP file into a plain text file.\nDependencies: pyhwp package\n\nInstall pyhwp (you need to install the pre-release version)\n\n```\npip install --pre pyhwp\n```\n\nExample\n\n```python\nfrom connlp.text_extract import TextConverter\nconverter = TextConverter()\n\nhwp_fpath = '/data/raw/hwp_file.hwp'\noutput_fpath = '/data/processed/extracted_text.txt'\n\nconverter.hwp2txt(hwp_fpath, output_fpath) # returns 0 if no error occurs\n```\n\n# GPU Utils\n\n## _GPUMonitor_\n\n_**GPUMonitor**_ generates a class to monitor and display the GPU status based on nvidia-smi.  \nRefer to \"https://github.com/anderskm/gputil\" and \"https://data-newbie.tistory.com/561\" for usages.\n\nInstall _GPUtils_ module with _pip_.\n\n```\npip install GPUtil\n```\n\nWrite your code between the initiation of the _**GPUMonitor**_ and _**monitor.stop()**_.\n\n```python\nfrom connlp.util import GPUMonitor\n\nmonitor = GPUMonitor(delay=3)\n# >>>Write your code here<<<\nmonitor.stop()\n\n# | ID | GPU | MEM |\n# ------------------\n# |  0 |  0% |  0% |\n# |  1 |  1% |  0% |\n# |  2 |  0% | 94% |\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/blank54/connlp.git",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "connlp",
    "package_url": "https://pypi.org/project/connlp/",
    "platform": "",
    "project_url": "https://pypi.org/project/connlp/",
    "project_urls": {
      "Homepage": "https://github.com/blank54/connlp.git"
    },
    "release_url": "https://pypi.org/project/connlp/0.0.18/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A bunch of python codes to analyze text data in the construction industry. Mainly reconstitute the pre-exist python libraries for Natural Language Processing (NLP)",
    "version": "0.0.18",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11002236,
  "releases": {
    "0.0.10": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "344794248d268f0fa9336cfba328529d68ba5af0e7975e1c055e12e9421d252b",
          "md5": "b7cfaa720b5b138b66b84ed9de03db67",
          "sha256": "0482becc0361a3be4d30fb03a1cb013518732fd3d7cc72294d43e820dce05fe1"
        },
        "downloads": -1,
        "filename": "connlp-0.0.10-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b7cfaa720b5b138b66b84ed9de03db67",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16152,
        "upload_time": "2021-05-26T13:17:29",
        "upload_time_iso_8601": "2021-05-26T13:17:29.881787Z",
        "url": "https://files.pythonhosted.org/packages/34/47/94248d268f0fa9336cfba328529d68ba5af0e7975e1c055e12e9421d252b/connlp-0.0.10-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fa975721fa4837c1fad34abb5db8ba746b5932d5ecf251e9f3c06bea692a692f",
          "md5": "93e7dc52815f51b8fb0ab242735846ec",
          "sha256": "4a69e8ce8b680770671f731aa73f6b7a7aec46465d26c2f132e6be6d729b85cc"
        },
        "downloads": -1,
        "filename": "connlp-0.0.10.tar.gz",
        "has_sig": false,
        "md5_digest": "93e7dc52815f51b8fb0ab242735846ec",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13491,
        "upload_time": "2021-05-26T13:17:35",
        "upload_time_iso_8601": "2021-05-26T13:17:35.968464Z",
        "url": "https://files.pythonhosted.org/packages/fa/97/5721fa4837c1fad34abb5db8ba746b5932d5ecf251e9f3c06bea692a692f/connlp-0.0.10.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "386d019dc71c5580be7e966b2b5e05b947ff0d4d217cb9f2b560e5b6da9eb485",
          "md5": "98a5b5270a1eef3edfe06f0b6a03649e",
          "sha256": "11aa72e9e814427aea598c9c22cbe5f56ba5f5cc7e5e4a25767cfb23deb960e5"
        },
        "downloads": -1,
        "filename": "connlp-0.0.11-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "98a5b5270a1eef3edfe06f0b6a03649e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21507,
        "upload_time": "2021-06-10T07:06:16",
        "upload_time_iso_8601": "2021-06-10T07:06:16.478185Z",
        "url": "https://files.pythonhosted.org/packages/38/6d/019dc71c5580be7e966b2b5e05b947ff0d4d217cb9f2b560e5b6da9eb485/connlp-0.0.11-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bfda02e1e0d46f9fe4103551ece384e2f862f2227d3133b5e63545ae9500c523",
          "md5": "61e2b9f7dc7dcbd0be441afdf96e6442",
          "sha256": "29ffb2ca21cf464300c03010b0164966cbbc2447b0c6522a47091f6339d94890"
        },
        "downloads": -1,
        "filename": "connlp-0.0.11.tar.gz",
        "has_sig": false,
        "md5_digest": "61e2b9f7dc7dcbd0be441afdf96e6442",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 21998,
        "upload_time": "2021-06-10T07:06:22",
        "upload_time_iso_8601": "2021-06-10T07:06:22.766322Z",
        "url": "https://files.pythonhosted.org/packages/bf/da/02e1e0d46f9fe4103551ece384e2f862f2227d3133b5e63545ae9500c523/connlp-0.0.11.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.12": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8855d67ec3400e23f5331aedcf24987fe6d6eef21ad994f3235c563974b775af",
          "md5": "9e10b825a8150f12a73442d80146c5e0",
          "sha256": "f57000aa8a30aa48adcc66d942d8558f05321994866eed5c7391f25ab1124195"
        },
        "downloads": -1,
        "filename": "connlp-0.0.12-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9e10b825a8150f12a73442d80146c5e0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 22412,
        "upload_time": "2021-07-12T03:29:02",
        "upload_time_iso_8601": "2021-07-12T03:29:02.830885Z",
        "url": "https://files.pythonhosted.org/packages/88/55/d67ec3400e23f5331aedcf24987fe6d6eef21ad994f3235c563974b775af/connlp-0.0.12-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bd1a28b0a15320718e620595e04342a89cd574df686174759355e9a5ce734590",
          "md5": "9138390daa1f943a26a4a46d6ce48f04",
          "sha256": "572e978a5777f488e629954a5e1fa80eff2f869699fef84a571966e775ed7b58"
        },
        "downloads": -1,
        "filename": "connlp-0.0.12.tar.gz",
        "has_sig": false,
        "md5_digest": "9138390daa1f943a26a4a46d6ce48f04",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23027,
        "upload_time": "2021-07-12T03:29:11",
        "upload_time_iso_8601": "2021-07-12T03:29:11.138074Z",
        "url": "https://files.pythonhosted.org/packages/bd/1a/28b0a15320718e620595e04342a89cd574df686174759355e9a5ce734590/connlp-0.0.12.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.13": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9a7601f83a4bccb7eb2275b325d5a6fb6048c631f9cc8c47560d9e043d952a29",
          "md5": "236b321c43b417559f4976d29bb7d10f",
          "sha256": "0258b6af0a6ddebf3f4150e4ff10268eb2ee73d80f69f7abf97dba5eba158224"
        },
        "downloads": -1,
        "filename": "connlp-0.0.13-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "236b321c43b417559f4976d29bb7d10f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 22968,
        "upload_time": "2021-07-12T03:31:45",
        "upload_time_iso_8601": "2021-07-12T03:31:45.350783Z",
        "url": "https://files.pythonhosted.org/packages/9a/76/01f83a4bccb7eb2275b325d5a6fb6048c631f9cc8c47560d9e043d952a29/connlp-0.0.13-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "051162ec8b3df83c076aa4f047e70a463b2d1d4eb2acb90a45c30f28f0ef8f03",
          "md5": "23d6a52d3f150d47b52a9918a55cf8d2",
          "sha256": "4d17e67b9d9d473dcd90bb707e86eb812925fd1b49f65d64d02f584986abed06"
        },
        "downloads": -1,
        "filename": "connlp-0.0.13.tar.gz",
        "has_sig": false,
        "md5_digest": "23d6a52d3f150d47b52a9918a55cf8d2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24473,
        "upload_time": "2021-07-12T03:31:55",
        "upload_time_iso_8601": "2021-07-12T03:31:55.108401Z",
        "url": "https://files.pythonhosted.org/packages/05/11/62ec8b3df83c076aa4f047e70a463b2d1d4eb2acb90a45c30f28f0ef8f03/connlp-0.0.13.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.14": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "60d5996cc32f10d913262bc098e0c03fc954e09198839cd210f2863461e8fae9",
          "md5": "1d6e7765cdf7135c82012559aa2c9f73",
          "sha256": "0782a2f481f53d44036c8f2f7c792761df261818e774bd41a5c82c159b058e88"
        },
        "downloads": -1,
        "filename": "connlp-0.0.14-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1d6e7765cdf7135c82012559aa2c9f73",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 24772,
        "upload_time": "2021-07-13T04:17:28",
        "upload_time_iso_8601": "2021-07-13T04:17:28.527548Z",
        "url": "https://files.pythonhosted.org/packages/60/d5/996cc32f10d913262bc098e0c03fc954e09198839cd210f2863461e8fae9/connlp-0.0.14-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "75dada2d006711d0e8be20b49751806beba2e7dc1f0e4c4b7060c2790c353e03",
          "md5": "6b4f2565f3fccbe81e7e487e174f6e12",
          "sha256": "e0bfe0e0f7b1d3d0545f6a8d94b40f0b2b1ec5e8cb3e0adda27184308dc85a51"
        },
        "downloads": -1,
        "filename": "connlp-0.0.14.tar.gz",
        "has_sig": false,
        "md5_digest": "6b4f2565f3fccbe81e7e487e174f6e12",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 27356,
        "upload_time": "2021-07-13T04:17:39",
        "upload_time_iso_8601": "2021-07-13T04:17:39.879217Z",
        "url": "https://files.pythonhosted.org/packages/75/da/da2d006711d0e8be20b49751806beba2e7dc1f0e4c4b7060c2790c353e03/connlp-0.0.14.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.15": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c169e75568da812a9eb4d75803d3bc257786b057359fddbaeb5f2c781872b3b8",
          "md5": "f718e59c682917d5e9ab73924d83947f",
          "sha256": "160430c853fae0fad32bdff9d1b8688248bd76acf25a40cc141756722109fee7"
        },
        "downloads": -1,
        "filename": "connlp-0.0.15-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f718e59c682917d5e9ab73924d83947f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 30245,
        "upload_time": "2021-07-14T06:58:59",
        "upload_time_iso_8601": "2021-07-14T06:58:59.946925Z",
        "url": "https://files.pythonhosted.org/packages/c1/69/e75568da812a9eb4d75803d3bc257786b057359fddbaeb5f2c781872b3b8/connlp-0.0.15-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "519ce4f4d17d8b4735dc8753708c1fd8f01833d11115b5ad81167d149e547e66",
          "md5": "74b329f73701fd7d587cb444d512f623",
          "sha256": "993d8af313986d79942b4527bfbc186a96268586a6f73f141079a8951579ec83"
        },
        "downloads": -1,
        "filename": "connlp-0.0.15.tar.gz",
        "has_sig": false,
        "md5_digest": "74b329f73701fd7d587cb444d512f623",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 28184,
        "upload_time": "2021-07-14T06:59:10",
        "upload_time_iso_8601": "2021-07-14T06:59:10.858272Z",
        "url": "https://files.pythonhosted.org/packages/51/9c/e4f4d17d8b4735dc8753708c1fd8f01833d11115b5ad81167d149e547e66/connlp-0.0.15.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.16": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ef36920a61a3bef84b636e26f1cbf73b5605569056093c7e8b7ff8356c655528",
          "md5": "e2b716a4863d8953280e65c8fc3852a4",
          "sha256": "e5b15dffb7ae388e7a14bd9dd6e6a7f8fe82c60bbd409db4f7ff839c8a80ccf0"
        },
        "downloads": -1,
        "filename": "connlp-0.0.16-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e2b716a4863d8953280e65c8fc3852a4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 30973,
        "upload_time": "2021-07-20T04:01:13",
        "upload_time_iso_8601": "2021-07-20T04:01:13.153168Z",
        "url": "https://files.pythonhosted.org/packages/ef/36/920a61a3bef84b636e26f1cbf73b5605569056093c7e8b7ff8356c655528/connlp-0.0.16-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bad36581286a30b42cf88821731cd6d1fcf75fb48ba54372bf37414f92af648c",
          "md5": "b64f7ea066c34723b6d0626df57832bb",
          "sha256": "785ce9a79fc3f0531d6fb86149d395c4d85443025545f3dde8ee6c4c7f4f14ff"
        },
        "downloads": -1,
        "filename": "connlp-0.0.16.tar.gz",
        "has_sig": false,
        "md5_digest": "b64f7ea066c34723b6d0626df57832bb",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 29826,
        "upload_time": "2021-07-20T04:01:24",
        "upload_time_iso_8601": "2021-07-20T04:01:24.346473Z",
        "url": "https://files.pythonhosted.org/packages/ba/d3/6581286a30b42cf88821731cd6d1fcf75fb48ba54372bf37414f92af648c/connlp-0.0.16.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.17": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "650398fe94c9ca59e0ca97496136e700328f64d1762f7402c1fa7ee6742ca262",
          "md5": "e623e2c2aec7be8eaf409a3b5356e51d",
          "sha256": "480bd79ec88c2e94345b641eb59b30d93ec7a98b22fec28d439a848b7316dd1b"
        },
        "downloads": -1,
        "filename": "connlp-0.0.17-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e623e2c2aec7be8eaf409a3b5356e51d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 31044,
        "upload_time": "2021-07-20T06:00:31",
        "upload_time_iso_8601": "2021-07-20T06:00:31.518786Z",
        "url": "https://files.pythonhosted.org/packages/65/03/98fe94c9ca59e0ca97496136e700328f64d1762f7402c1fa7ee6742ca262/connlp-0.0.17-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b73f5b1436faa77d13752f1f77e268f91d5952e65ca29a21b72fdc0f998ca989",
          "md5": "f2df12bffdd69ab98e1ea3da07f30cb4",
          "sha256": "8db0aaf6893b3dfe467810035e8d67513a4b7dff055271358ddfcd54df4e2044"
        },
        "downloads": -1,
        "filename": "connlp-0.0.17.tar.gz",
        "has_sig": false,
        "md5_digest": "f2df12bffdd69ab98e1ea3da07f30cb4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30036,
        "upload_time": "2021-07-20T06:00:45",
        "upload_time_iso_8601": "2021-07-20T06:00:45.274800Z",
        "url": "https://files.pythonhosted.org/packages/b7/3f/5b1436faa77d13752f1f77e268f91d5952e65ca29a21b72fdc0f998ca989/connlp-0.0.17.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.18": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0ab94a20f780a76f56b45734d7fe06faef8a6787fff25d8d1c2b1a40028dc60f",
          "md5": "a8aaf3ebfe9c51f02138d234f03a3ff9",
          "sha256": "32a83bfef902f871969d4aacc4403279e9aed645fb1e8e45a23c430e1f1b0c2b"
        },
        "downloads": -1,
        "filename": "connlp-0.0.18-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a8aaf3ebfe9c51f02138d234f03a3ff9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 35238,
        "upload_time": "2021-07-26T05:08:08",
        "upload_time_iso_8601": "2021-07-26T05:08:08.376717Z",
        "url": "https://files.pythonhosted.org/packages/0a/b9/4a20f780a76f56b45734d7fe06faef8a6787fff25d8d1c2b1a40028dc60f/connlp-0.0.18-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "01402280fd618332097867a92c3b13ed1f1aa2e2dcca5f035d201b9daf8647a7",
          "md5": "15ba22874354e5c4ca050aaa403d6cbc",
          "sha256": "7d44c1ade7de442306c4b99d725ca69a9f923e3d96a0c5a89b3bd4b782387835"
        },
        "downloads": -1,
        "filename": "connlp-0.0.18.tar.gz",
        "has_sig": false,
        "md5_digest": "15ba22874354e5c4ca050aaa403d6cbc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 36132,
        "upload_time": "2021-07-26T05:08:22",
        "upload_time_iso_8601": "2021-07-26T05:08:22.796052Z",
        "url": "https://files.pythonhosted.org/packages/01/40/2280fd618332097867a92c3b13ed1f1aa2e2dcca5f035d201b9daf8647a7/connlp-0.0.18.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "943c53c28baed66d8b89dabdaa7365e6400458ded517cfffb62ad45d0bc90fbd",
          "md5": "221d58d6f1695e29c844ae34f6e6eef1",
          "sha256": "ba8984cba0a1f14662c4c7323462a788cb8d62fbd9cb235189e8c1921529a0dd"
        },
        "downloads": -1,
        "filename": "connlp-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "221d58d6f1695e29c844ae34f6e6eef1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10508,
        "upload_time": "2021-04-02T04:54:38",
        "upload_time_iso_8601": "2021-04-02T04:54:38.686960Z",
        "url": "https://files.pythonhosted.org/packages/94/3c/53c28baed66d8b89dabdaa7365e6400458ded517cfffb62ad45d0bc90fbd/connlp-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6c1deabfdaa1f349090309322fd5a1b03a8cc451c5505fec774402a802504e7f",
          "md5": "a2e16fea865d7790f7ac007601d6593c",
          "sha256": "a05533a28233877b062736c2ae91086d80b04161d9c80e4978e1eab6fecc4f73"
        },
        "downloads": -1,
        "filename": "connlp-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "a2e16fea865d7790f7ac007601d6593c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5276,
        "upload_time": "2021-04-02T04:54:42",
        "upload_time_iso_8601": "2021-04-02T04:54:42.576943Z",
        "url": "https://files.pythonhosted.org/packages/6c/1d/eabfdaa1f349090309322fd5a1b03a8cc451c5505fec774402a802504e7f/connlp-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0597509daf5c229b1c54e8e5e6ca7310e8a78f7810e6b8182952ec398eaaed12",
          "md5": "bc4d4aee5d527dcb6a9157d8c80b6257",
          "sha256": "ed3591a7bb92e3e1b71505c37284158c8a4f42eb490ee694bd4212e117b64fc4"
        },
        "downloads": -1,
        "filename": "connlp-0.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bc4d4aee5d527dcb6a9157d8c80b6257",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 11383,
        "upload_time": "2021-04-02T05:47:35",
        "upload_time_iso_8601": "2021-04-02T05:47:35.747943Z",
        "url": "https://files.pythonhosted.org/packages/05/97/509daf5c229b1c54e8e5e6ca7310e8a78f7810e6b8182952ec398eaaed12/connlp-0.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63a94bb7e5b8214e55e61bcd63649bcebe367ae65b5c96b5a7a8d2c2c3bf680a",
          "md5": "79b03178791b4a770290a1e87fb1cc0a",
          "sha256": "0b979f810b29d39a58d040937ad45a77e0847924222a7121575b84ea602634d8"
        },
        "downloads": -1,
        "filename": "connlp-0.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "79b03178791b4a770290a1e87fb1cc0a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6009,
        "upload_time": "2021-04-02T05:48:18",
        "upload_time_iso_8601": "2021-04-02T05:48:18.071987Z",
        "url": "https://files.pythonhosted.org/packages/63/a9/4bb7e5b8214e55e61bcd63649bcebe367ae65b5c96b5a7a8d2c2c3bf680a/connlp-0.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5465df3bd5ccea2de12a8e43c67885f59059ae31df7e2f76a5a60756da0fbc4f",
          "md5": "9c11259ca8367e2b0c49ae4ab0064e1b",
          "sha256": "5de8efe7c12c105070ff2ea2b65ca4e1711972e6365a18c9c5efba883461e7f3"
        },
        "downloads": -1,
        "filename": "connlp-0.0.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9c11259ca8367e2b0c49ae4ab0064e1b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12029,
        "upload_time": "2021-04-13T03:27:29",
        "upload_time_iso_8601": "2021-04-13T03:27:29.140698Z",
        "url": "https://files.pythonhosted.org/packages/54/65/df3bd5ccea2de12a8e43c67885f59059ae31df7e2f76a5a60756da0fbc4f/connlp-0.0.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8c621a085fb0fd4a628363ef3af82c8aef07e2e457be753aa1e48c4bf33c7d76",
          "md5": "cef0daec442a48d1a518e1cef60295aa",
          "sha256": "6c69f7c7c97575b7d33e5dee3951c9dbcc2739a94ce17e942a6b3f4b23f82e64"
        },
        "downloads": -1,
        "filename": "connlp-0.0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "cef0daec442a48d1a518e1cef60295aa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6417,
        "upload_time": "2021-04-13T03:27:31",
        "upload_time_iso_8601": "2021-04-13T03:27:31.783018Z",
        "url": "https://files.pythonhosted.org/packages/8c/62/1a085fb0fd4a628363ef3af82c8aef07e2e457be753aa1e48c4bf33c7d76/connlp-0.0.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5859c7d5728fcd8b483b44b65aa5ef01558ee95e7413982219f2c6009a9fb01f",
          "md5": "b6cd9104756e3d034b42e3efde6c1ad9",
          "sha256": "0ec260421268fb94759c38350ec747b3bdc48ca3ebd61cd1c9767a7e7c50892a"
        },
        "downloads": -1,
        "filename": "connlp-0.0.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b6cd9104756e3d034b42e3efde6c1ad9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 14424,
        "upload_time": "2021-05-10T06:36:48",
        "upload_time_iso_8601": "2021-05-10T06:36:48.497951Z",
        "url": "https://files.pythonhosted.org/packages/58/59/c7d5728fcd8b483b44b65aa5ef01558ee95e7413982219f2c6009a9fb01f/connlp-0.0.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "155e4fb103df341626b642689aad0e3ee8edb686102748d4d76d3e0cfb775603",
          "md5": "4df7030bae7baa16db88f3a5877196fd",
          "sha256": "64c8592ecb219316cfbda416f71735167bcafb9155567060fe94db9fd457d660"
        },
        "downloads": -1,
        "filename": "connlp-0.0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "4df7030bae7baa16db88f3a5877196fd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9818,
        "upload_time": "2021-05-10T06:36:51",
        "upload_time_iso_8601": "2021-05-10T06:36:51.992249Z",
        "url": "https://files.pythonhosted.org/packages/15/5e/4fb103df341626b642689aad0e3ee8edb686102748d4d76d3e0cfb775603/connlp-0.0.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0ab94a20f780a76f56b45734d7fe06faef8a6787fff25d8d1c2b1a40028dc60f",
        "md5": "a8aaf3ebfe9c51f02138d234f03a3ff9",
        "sha256": "32a83bfef902f871969d4aacc4403279e9aed645fb1e8e45a23c430e1f1b0c2b"
      },
      "downloads": -1,
      "filename": "connlp-0.0.18-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a8aaf3ebfe9c51f02138d234f03a3ff9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 35238,
      "upload_time": "2021-07-26T05:08:08",
      "upload_time_iso_8601": "2021-07-26T05:08:08.376717Z",
      "url": "https://files.pythonhosted.org/packages/0a/b9/4a20f780a76f56b45734d7fe06faef8a6787fff25d8d1c2b1a40028dc60f/connlp-0.0.18-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "01402280fd618332097867a92c3b13ed1f1aa2e2dcca5f035d201b9daf8647a7",
        "md5": "15ba22874354e5c4ca050aaa403d6cbc",
        "sha256": "7d44c1ade7de442306c4b99d725ca69a9f923e3d96a0c5a89b3bd4b782387835"
      },
      "downloads": -1,
      "filename": "connlp-0.0.18.tar.gz",
      "has_sig": false,
      "md5_digest": "15ba22874354e5c4ca050aaa403d6cbc",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 36132,
      "upload_time": "2021-07-26T05:08:22",
      "upload_time_iso_8601": "2021-07-26T05:08:22.796052Z",
      "url": "https://files.pythonhosted.org/packages/01/40/2280fd618332097867a92c3b13ed1f1aa2e2dcca5f035d201b9daf8647a7/connlp-0.0.18.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}