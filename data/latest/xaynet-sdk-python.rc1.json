{
  "info": {
    "author": "Xayn Engineering <engineering@xaynet.dev>",
    "author_email": "Xayn Engineering <engineering@xaynet.dev>",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "![Xaynet banner](../../assets/xaynet_banner.png)\n\n## Installation\n\n**Prerequisites**\n\n- Python 3.6 or higher\n\n**Install it from source**\n\n```bash\n# first install rust via https://rustup.rs/\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# clone the xaynet repository\ngit clone https://github.com/xaynetwork/xaynet.git\ncd xaynet/bindings/python\n\n# create and activate a virtual environment e.g.\npyenv virtualenv xayn\npyenv activate xayn\n\n# install maturin\npip install maturin==0.9.0\npip install justbackoff\n\n# install xaynet-sdk\nmaturin develop\n```\n\n## Participant API(s)\n\nThe Python SDK that consists of two experimental Xaynet participants `ParticipantABC`\nand `AsyncParticipant`.\n\nThe word `Async` does not refer to either `asyncio` or asynchronous federated learning.\nIt refers to the property when a local model can be set. In `ParticipantABC`\nthe local model can only be set if the participant was selected an update participant\nwhile in `AsyncParticipant` the model can be set at any time.\n\n### `ParticipantABC`\n\nThe `ParticipantABC` API is similar to the old one which we introduced in\n[`v0.8.0`](https://github.com/xaynetwork/xaynet/blob/v0.8.0/python/sdk/xain_sdk/participant.py#L24).\nAside from some changes to the method signature, the biggest change is that the participant\nnow runs in its own thread.\n\nTo migrate from `v0.8.0` to `v0.11.0` please follow the [migration guide](./migration_guide.md).\n\n![ParticipantABC](../../assets/python_participant.svg)\n\n**Public API of `ParticipantABC`  and `InternalParticipant`**\n\n```python\ndef spawn_participant(\n    coordinator_url: str,\n    participant: ParticipantABC,\n    args: Tuple = (),\n    kwargs: dict = {},\n    state: Optional[List[int]] = None,\n    scalar: float = 1.0,\n):\n    \"\"\"\n    Spawns a `InternalParticipant` in a separate thread and returns a participant handle.\n    If a `state` is passed, this state is restored, otherwise a new `InternalParticipant`\n    is created.\n\n    Args:\n        coordinator_url: The url of the coordinator.\n        participant: A class that implements `ParticipantABC`.\n        args: The args that get passed to the constructor of the `participant` class.\n        kwargs: The kwargs that get passed to the constructor of the `participant` class.\n        state: A serialized participant state. Defaults to `None`.\n        scalar: The scalar used for masking. Defaults to `1.0`.\n\n    Note:\n        The `scalar` is used later when the models are aggregated in order to scale their weights.\n        It can be used when you want to weight the participants updates differently.\n\n        For example:\n        If not all participant updates should be weighted equally but proportionally to their\n        training samples, the scalar would be set to `scalar = 1 / number_of_samples`.\n\n    Returns:\n        The `InternalParticipant`.\n\n    Raises:\n        CryptoInit: If the initialization of the underling crypto library has failed.\n        ParticipantInit: If the participant cannot be initialized. This is most\n            likely caused by an invalid `coordinator_url`.\n        ParticipantRestore: If the participant cannot be restored due to invalid\n            serialized state. This exception can never be thrown if the `state` is `None`.\n        Exception: Any exception that can be thrown during the instantiation of `participant`.\n    \"\"\"\n\nclass ParticipantABC(ABC):\n    def train_round(self, training_input: Optional[TrainingInput]) -> TrainingResult:\n        \"\"\"\n        Trains a model. `training_input` is the deserialized global model\n        (see `deserialize_training_input`). If no global model exists\n        (usually in the first round), `training_input` will be `None`.\n        In this case the weights of the model should be initialized and returned.\n\n        Args:\n            self: The participant.\n            training_input: The deserialized global model (weights of the global model) or None.\n\n        Returns:\n            The updated model weights (the local model).\n        \"\"\"\n\n    def serialize_training_result(self, training_result: TrainingResult) -> list:\n        \"\"\"\n        Serializes the `training_result` into a `list`. The data type of the\n        elements must match the data type defined in the coordinator configuration.\n\n        Args:\n            self: The participant.\n            training_result: The `TrainingResult` of `train_round`.\n\n        Returns:\n            The `training_result` as a `list`.\n        \"\"\"\n\n    def deserialize_training_input(self, global_model: list) -> TrainingInput:\n        \"\"\"\n        Deserializes the `global_model` from a `list` to the type of `TrainingInput`.\n        The data type of the elements matches the data type defined in the coordinator\n        configuration. If no global model exists (usually in the first round), the method will\n        not be called by the `InternalParticipant`.\n\n        Args:\n            self: The participant.\n            global_model: The global model.\n\n        Returns:\n            The `TrainingInput` for `train_round`.\n        \"\"\"\n\n    def participate_in_update_task(self) -> bool:\n        \"\"\"\n        A callback used by the `InternalParticipant` to determine whether the\n        `train_round` method should be called. This callback is only called\n        if the participant is selected as an update participant. If `participate_in_update_task`\n        returns `False`, `train_round` will not be called by the `InternalParticipant`.\n\n        If the method is not overridden, it returns `True` by default.\n\n        Returns:\n            Whether the `train_round` method should be called when the participant\n            is an update participant.\n        \"\"\"\n\n    def on_new_global_model(self, global_model: Optional[TrainingInput]) -> None:\n        \"\"\"\n        A callback that is called by the `InternalParticipant` once a new global model is\n        available. If no global model exists (usually in the first round), `global_model` will\n        be `None`. If a global model exists, `global_model` is already the deserialized\n        global model. (See `deserialize_training_input`)\n\n        If the method is not overridden, it does nothing by default.\n\n        Args:\n            self: The participant.\n            global_model: The deserialized global model or `None`.\n        \"\"\"\n\n    def on_stop(self) -> None:\n        \"\"\"\n        A callback that is called by the `InternalParticipant` before the `InternalParticipant`\n        thread is stopped.\n\n        This callback can be used, for example, to show performance values ​​that have been\n        collected in the participant over the course of the training rounds.\n\n        If the method is not overridden, it does nothing by default.\n\n        Args:\n            self: The participant.\n        \"\"\"\n\nclass InternalParticipant:\n    def stop(self) -> List[int]:\n        \"\"\"\n        Stops the execution of the participant and returns its serialized state.\n        The serialized state can be passed to the `spawn_participant` function\n        to restore a participant.\n\n        After calling `stop`, the participant is consumed. Every further method\n        call on the handle of `InternalParticipant` leads to an `UninitializedParticipant`\n        exception.\n\n        Note:\n            The serialized state contains unencrypted **private key(s)**. If used\n            in production, it is important that the serialized state is securely saved.\n\n        Returns:\n            The serialized state of the participant.\n        \"\"\"\n```\n\n### `AsyncParticipant`\n\nWe noticed that the API of `ParticipantABC`/`InternalParticipant` reduces a fair amount of\ncode on the user side, however, it may not be flexible enough to cover some of the following\nuse cases:\n\n1. The user wants to use the global/local model in a different thread.\n\n    It is possible to provide methods for this on the `InternalParticipant` but they are not\n    straight forward to implement. To make them thread-safe, it is probably necessary to use\n    synchronization primitives but this would make the `InternalParticipant` more complicated.\n    In addition, questions arise such as: Would the user want to be able to get\n    the current local model at any time or would they like to be notified as soon as a new\n    local model is available.\n\n2. Train a model without the participant\n\n    Since the training of the model is embedded in the `ParticipantABC`, this will probably lead to\n    code duplication if the user wants to perform the training without the participant. Furthermore,\n    the embedding of the training in the `ParticipantABC` can also be a problem once the participant\n    is integrated into an existing application, considering the code for the training has to be\n    moved into the `train_round` method, which can lead to significant changes to the existing code.\n\n3. Custom exception handling\n\n    Last but not least, the question arises how we can inform the user that an exception has been\n    thrown. We do not want the participant to be terminated with every exception but we want to\n    give the user the opportunity to respond appropriately.\n\nThe main issue we saw is that the participant is responsible for training the model\nand to run the PET protocol. Therefore, we offer a second API in which the training\nof the model is no longer part of the participant. This results in a simpler and more flexible API,\nbut it comes with the tradeoff that the user needs to perform the de/serialization of the\nglobal/local on their side.\n\n![AsyncParticipant](../../assets/python_async_participant.svg)\n\n**Public API of `AsyncParticipant`**\n\n```python\ndef spawn_async_participant(coordinator_url: str, state: Optional[List[int]] = None, scalar: float = 1.0)\n    -> (AsyncParticipant, threading.Event):\n    \"\"\"\n    Spawns a `AsyncParticipant` in a separate thread and returns a participant handle\n    together with a global model notifier. If a `state` is passed, this state is restored,\n    otherwise a new participant is created.\n\n    The global model notifier sets the flag once a new global model is available.\n    The flag is also set when the global model is `None` (usually in the first round).\n    The flag is reset once the method `get_global_model` has been called but it is also possible\n    to reset the flag manually by calling\n    [`clear()`](https://docs.python.org/3/library/threading.html#threading.Event.clear).\n\n    Args:\n        coordinator_url: The url of the coordinator.\n        state: A serialized participant state. Defaults to `None`.\n        scalar: The scalar used for masking. Defaults to `1.0`.\n\n    Note:\n        The `scalar` is used later when the models are aggregated in order to scale their weights.\n        It can be used when you want to weight the participants updates differently.\n\n        For example:\n        If not all participant updates should be weighted equally but proportionally to their\n        training samples, the scalar would be set to `scalar = 1 / number_of_samples`.\n\n    Returns:\n        A tuple which consists of an `AsyncParticipant` and a global model notifier.\n\n    Raises:\n        CryptoInit: If the initialization of the underling crypto library has failed.\n        ParticipantInit: If the participant cannot be initialized. This is most\n            likely caused by an invalid `coordinator_url`.\n        ParticipantRestore: If the participant cannot be restored due to invalid\n            serialized state. This exception can never be thrown if the `state` is `None`.\n    \"\"\"\n\nclass AsyncParticipant:\n    def get_global_model(self) -> Optional[list]:\n        \"\"\"\n        Fetches the current global model. This method can be called at any time. If no global\n        model exists (usually in the first round), the method returns `None`.\n\n        Returns:\n            The current global model or `None`. The data type of the elements matches the data\n            type defined in the coordinator configuration.\n\n        Raises:\n            GlobalModelUnavailable: If the participant cannot connect to the coordinator to get\n                the global model.\n            GlobalModelDataTypeMisMatch: If the data type of the global model does not match\n                the data type defined in the coordinator configuration.\n        \"\"\"\n\n    def set_local_model(self, local_model: list):\n        \"\"\"\n        Sets a local model. This method can be called at any time. Internally the\n        participant first caches the local model. As soon as the participant is selected as an\n        update participant, the currently cached local model is used. This means that the cache\n        is empty after this operation.\n\n        If a local model is already in the cache and `set_local_model` is called with a new local\n        model, the current cached local model will be replaced by the new one.\n        If the participant is an update participant and there is no local model in the cache,\n        the participant waits until a local model is set or until a new round has been started.\n\n        Args:\n            local_model: The local model. The data type of the elements must match the data\n            type defined in the coordinator configuration.\n\n        Raises:\n            LocalModelLengthMisMatch: If the length of the local model does not match the\n                length defined in the coordinator configuration.\n            LocalModelDataTypeMisMatch: If the data type of the local model does not match\n                the data type defined in the coordinator configuration.\n        \"\"\"\n\n    def stop(self) -> List[int]:\n        \"\"\"\n        Stops the execution of the participant and returns its serialized state.\n        The serialized state can be passed to the `spawn_async_participant` function\n        to restore a participant.\n\n        After calling `stop`, the participant is consumed. Every further method\n        call on the handle of `AsyncParticipant` leads to an `UninitializedParticipant`\n        exception.\n\n        Note:\n            The serialized state contains unencrypted **private key(s)**. If used\n            in production, it is important that the serialized state is securely saved.\n\n        Returns:\n            The serialized state of the participant.\n        \"\"\"\n```\n\n## Enable logging of `xaynet-mobile`\n\nIf you are interested in what `xaynet-mobile` is doing under the hood,\nyou can turn on the logging via the environment variable `XAYNET_CLIENT`.\n\nFor example:\n\n`XAYNET_CLIENT=info python examples/participate_in_update.py`\n\n## How can I ... ?\n\nWe have created a few [examples](./examples/README.md) that show the basic methods in action.\nBut if something is missing, not very clear or not working properly, please let us know\nby opening an issue.\n\nWe are happy to help and open to ideas or feedback :)\n",
    "description_content_type": "text/markdown; charset=UTF-8; variant=GFM",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://xaynet.dev/",
    "keywords": "federated-learning fl ai machine-learning",
    "license": "Apache-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "xaynet-sdk-python",
    "package_url": "https://pypi.org/project/xaynet-sdk-python/",
    "platform": "",
    "project_url": "https://pypi.org/project/xaynet-sdk-python/",
    "project_urls": {
      "Homepage": "https://xaynet.dev/"
    },
    "release_url": "https://pypi.org/project/xaynet-sdk-python/0.1.0/",
    "requires_dist": [
      "justbackoff (==0.6.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "The Xayn Network project is building a privacy layer for machine learning so that AI projects can meet compliance such as GDPR and CCPA. The approach relies on Federated Learning as enabling technology that allows production AI applications to be fully privacy compliant.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9163857,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "70b16eb72ad2de007a98b3a8a18e12ab764ec0440bd1d7c3aa0edc8f97fcef6b",
          "md5": "e6d030043f7eebe41be7b3624970b04a",
          "sha256": "3c20515bf9c33cbff70bfd8c65ac80cdb12a1fb1bc9bc5e82f195bd4b1f3bf25"
        },
        "downloads": -1,
        "filename": "xaynet_sdk_python-0.1.0-cp36-abi3-macosx_10_7_x86_64.whl",
        "has_sig": false,
        "md5_digest": "e6d030043f7eebe41be7b3624970b04a",
        "packagetype": "bdist_wheel",
        "python_version": "cp36",
        "requires_python": ">=3.6",
        "size": 2487446,
        "upload_time": "2021-01-18T11:35:17",
        "upload_time_iso_8601": "2021-01-18T11:35:17.207504Z",
        "url": "https://files.pythonhosted.org/packages/70/b1/6eb72ad2de007a98b3a8a18e12ab764ec0440bd1d7c3aa0edc8f97fcef6b/xaynet_sdk_python-0.1.0-cp36-abi3-macosx_10_7_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eda3814e03f0020a500f8426e34d5be5f65b67c3cccaa90dedc495cd1f2dccee",
          "md5": "e294a80e61000cfba5c2547e2ab9384f",
          "sha256": "c528174f4a86364b87e417019159138aafd6900cf00ebd4eb9a3806d85d48889"
        },
        "downloads": -1,
        "filename": "xaynet_sdk_python-0.1.0-cp36-abi3-manylinux2010_x86_64.whl",
        "has_sig": false,
        "md5_digest": "e294a80e61000cfba5c2547e2ab9384f",
        "packagetype": "bdist_wheel",
        "python_version": "cp36",
        "requires_python": ">=3.6",
        "size": 4185558,
        "upload_time": "2021-01-18T14:49:29",
        "upload_time_iso_8601": "2021-01-18T14:49:29.624097Z",
        "url": "https://files.pythonhosted.org/packages/ed/a3/814e03f0020a500f8426e34d5be5f65b67c3cccaa90dedc495cd1f2dccee/xaynet_sdk_python-0.1.0-cp36-abi3-manylinux2010_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "70b16eb72ad2de007a98b3a8a18e12ab764ec0440bd1d7c3aa0edc8f97fcef6b",
        "md5": "e6d030043f7eebe41be7b3624970b04a",
        "sha256": "3c20515bf9c33cbff70bfd8c65ac80cdb12a1fb1bc9bc5e82f195bd4b1f3bf25"
      },
      "downloads": -1,
      "filename": "xaynet_sdk_python-0.1.0-cp36-abi3-macosx_10_7_x86_64.whl",
      "has_sig": false,
      "md5_digest": "e6d030043f7eebe41be7b3624970b04a",
      "packagetype": "bdist_wheel",
      "python_version": "cp36",
      "requires_python": ">=3.6",
      "size": 2487446,
      "upload_time": "2021-01-18T11:35:17",
      "upload_time_iso_8601": "2021-01-18T11:35:17.207504Z",
      "url": "https://files.pythonhosted.org/packages/70/b1/6eb72ad2de007a98b3a8a18e12ab764ec0440bd1d7c3aa0edc8f97fcef6b/xaynet_sdk_python-0.1.0-cp36-abi3-macosx_10_7_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eda3814e03f0020a500f8426e34d5be5f65b67c3cccaa90dedc495cd1f2dccee",
        "md5": "e294a80e61000cfba5c2547e2ab9384f",
        "sha256": "c528174f4a86364b87e417019159138aafd6900cf00ebd4eb9a3806d85d48889"
      },
      "downloads": -1,
      "filename": "xaynet_sdk_python-0.1.0-cp36-abi3-manylinux2010_x86_64.whl",
      "has_sig": false,
      "md5_digest": "e294a80e61000cfba5c2547e2ab9384f",
      "packagetype": "bdist_wheel",
      "python_version": "cp36",
      "requires_python": ">=3.6",
      "size": 4185558,
      "upload_time": "2021-01-18T14:49:29",
      "upload_time_iso_8601": "2021-01-18T14:49:29.624097Z",
      "url": "https://files.pythonhosted.org/packages/ed/a3/814e03f0020a500f8426e34d5be5f65b67c3cccaa90dedc495cd1f2dccee/xaynet_sdk_python-0.1.0-cp36-abi3-manylinux2010_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}