{
  "info": {
    "author": "Oliver Mathias",
    "author_email": "mathias@chapman.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.7",
      "Topic :: Software Development :: Build Tools"
    ],
    "description": "<img src=\"https://media.giphy.com/media/26AHE5D3mbfAm1M4M/giphy.gif\"  align=right  />\n\n# Predictionary\nA python package that allows for the easy creation of LSTM text prediction datasets. With just a few lines of code,\nyou can begin training your network on PDFs, .txt files, and UTF-8 hosted raw text online!\n\n\n## ðŸš© Table of contents\n* [Install](#install)\n* [Import](#import)\n* [Source](#source)\n* [Data](#data)\n* [Visualize](#visualize)\n* [Results](#Results)\n* [License](#license)\n\n## Install\nThe easiest way to install this package is to simply use pip\n```\npip install predictionary\n```\nYou could also just as easily clone this repo.\n\n## Import\nThe following lines import all classes and methods from the Predictionary package.\n```\nfrom predictionary.source import load_raw_text\nfrom predictionary.vata import process_text, word_to_int, int_to_word, make_data, make_sentences, encode_sentences\nfrom predictionary.visualize import make_prevelence_dict, plot_top\n```\nLet's go through the features, one method at a time.\n\n## Source\n* ### load_raw_text\n  The Source class has only one method, \"load_raw_text.\" This method serves to pull in the raw text data from the web, PDFs, or local text files. It can also merge the data, or return a list of separated data.\n\n  **Arguments**\n    * **URL(s)** - The first argument to pass is a list of the URLs or paths of the file(s) you want to pull from\n    * **methods** - This variable corresponds to the type of your source, for example \"web\" means the first value passed will be a list of URL(s), \"text\" will be passed local .txt file paths, and \"pdf\" will be passed the path(s) of local .pdf files.\n    * **merge_data** - The final variable to set is merge_data, if set if \"True\" or left un-instantiated, if will merge all the sources in the list passed and return a single string at the end. If set to \"False\", if will return a list of length len(URL(s)), with each source's data\n\n    <br>\n\n    **Note** Be sure to enter the correct identifier in the \"method\" variable when using \"load_raw_text.\"\n\n    * #### Web\n    The web value allows us to pull data from hosted .txt files such as the ones of project gutenberg.\n    ```\n    text = load_raw_text([\"https://www.gutenberg.org/files/76/76-0.txt\", \"https://www.gutenberg.org/files/6130/6130-0.txt\"], method=\"web\", merge_data=True)\n    ```\n    In the example above, we're grabbing data from 2 books hosted on project gutenburg, and telling the package to merge the data from both books. This command will return a single string with our data in it to be processed. If we instead used \"merge_data=False\", we would be returned a list of 2 such strings that we must then process individually.\n\n    * #### Text\n    This source method allows users to pull in data from their local .txt files.\n    ```\n    text = load_raw_text([r'C:\\Users\\Chuggy\\Documents\\hello.txt', r'C:\\Users\\Chuggy\\Documents\\hello1.txt'], method=\"text\", merge_data=False)\n    ```\n    In this example we're pulling from two local files, \"hello.txt\", and \"hello1.txt.\"\n    We're also telling the package to return a 2 element list of the 2 files' data by specifying \"merge_data=False.\"\n\n    * #### PDF\n    The final source method allows users to pull data straight from PDFs.\n    ```\n    text = load_raw_text([r'C:\\Users\\Chuggy\\Downloads\\flashboys.pdf'], method=\"pdf\")\n    ```\n    Here we are using the default values for merging, so the files would have been merged if we had specified 2 different files, however we only have one here so we're fine. As always, we specify the input method as \"pdf\", and in this case are returned a string containing the raw text from our source.\n\n## Data\n* ### process_text\n  This method is the main text \"cleaner\" of the package. It will take in the raw text data returned by the \"Source\" class and turn it into either characters or words that have been cleaned (read 'separated') of their punctuation.\n\n  **Arguments**\n   * **text** - The first argument to pass is the raw text string that was returned from the \"Source\" class.\n   * **split_by_words** - This argument will determine if the package splits up the text by words or by characters.\n   * **keep_spaces** - The final argument in this method determines if the input data should contain spaces, or simply the words / chars.\n\n  <br>\n\n * #### split_by_words = True\n\n    Example Usage:\n    ```\n    fully_processed_text = process_text(text, split_by_words=True)\n    ```\n    Returns:\n    ```\n    ['the', ' ', 'project', ' ', 'gutenberg', ' ', 'ebook', ' ', 'of', ' ', 'adventures', ' ', 'of', ' ', 'huckleberry', ...]\n    ```\n    The above example is one where we split the raw text from the source into words.\n\n  * #### split_by_words = False\n\n    Example Usage:\n    ```\n    fully_processed_text = process_text(text, split_by_words=False)\n    ```\n    Returns:\n    ```\n    ['t', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', ...]\n    ```\n    In this example we split the raw text by characters.\n\n  * #### split_by_words = True, keep_spaces=False\n\n    Example Usage:\n    ```\n    fully_processed_text = process_text(text, split_by_words=False)\n    ```\n    Returns:\n    ```\n    ['the', 'project', 'gutenberg', 'ebook', 'of', 'adventures', 'of', 'huckleberry', ...]\n    ```\n    In this example we wanted just the words with no spaces.\n\n\n* ### word_to_int\n  This method serves to create a dictionary for the word_to_int process necessary for our data to flow into an LSTM architecture.\n\n  **Arguments**\n   * **fully_processed_text** - Takes in the fully processed text returned by the process_text method.\n\n   <br>\n\n  Example Usage:\n  ```\n  word_to_int_dictionary = word_to_int(fully_processed_text)\n  ```\n  Returns:\n  ```\n  {'': 0, '\\r': 1, '\\n': 2, ' ': 3, '!': 4, '#': 5, '$': 6, '%': 7, ...}\n  ```\n* ### int_to_word\n  This method serves to create an inverse dictionary from the word_to_int method for turning\n  the argmax of the model's one-hot array outputs back into chars/words.\n\n  **Arguments**\n   * **word_to_int_dictionary** - Takes in the word_to_int_dictionary from the word_to_int method and reverses it.\n\n  <br>\n\n  Example Usage:\n  ```\n  int_to_word_dictionary = int_to_word(word_to_int_dictionary)\n  ```\n  Returns:\n  ```\n  {0: '', 1: '\\r', 2: '\\n', 3: ' ', 4: '!', 5: '#', 6: '$', 7: '%', ...}\n  ```\n* ### make_data\n  This method is the main workhorse for this Data class. It serves to take in the processed text from the process text method and turn it into fully trainable LSTM text prediction datasets.\n\n  **Arguments**\n   * **fully_processed_text** - Pass in the fully processed text from the process_text method.\n   * **int_to_word_dictionary** - Pass in the int_to_word_dictionary from the int_to_word         dictionary creating method.\n   * **sequence_length** - The length of chars/words you want each training example to be.\n   * **X_one_hot** - A boolean value to determine if you want your training sequences to be passed to your network in the form of ints or one-hot-arrays.\n   * **Y_one_hot** - A boolean value to determine if you want your target data to be passed to your network in the form of ints or one-hot-arrays.\n   * **split_train_test** - A Boolean that if set to True, the method will pass back data split into X_train, Y_train, X_test, Y_test form, OR if set to False, will pass back data in the X, Y form.\n   * **test_train_split_ratio** - A float value that allows the user to set the ratio of testing data to training data.\n   * **save_locally** - A boolean value that if set to True, will save created arrays in .npy format locally.\n  <br>\n\n  Example Usage\n  ```\n  X_train, X_test, y_train, y_test = make_data(fully_processed_text, int_to_word_dictionary, sequence_length=68, X_one_hot=True, split_train_test=True, test_train_split_ratio=0.33, save_locally=False)\n  ```\n  Returns:\n  ```\n  What it Actually Returns:\n  X_train, X_test, y_train, y_test\n\n  **The Features of What it Returns***:\n\n  Number of Train Examples:  385072\n  Number of Test Examples:  189663\n\n  X_train.shape:\n  (385072, 69, 60)\n\n  X_train[0]:\n  [[0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   ...\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]\n   [0. 0. 0. ... 0. 0. 0.]]\n\n  y_train.shape:\n  (385072, 60)\n\n  y_train[0]:\n  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n  ```\n  In the above example we created our data to contain 68 chars/words per sequence, be one-hot encoded, have a 33% test to train data ration, and not save to our local machine.\n\n  Example Usage\n  ```\n  X, y = make_data(fully_processed_text, int_to_word_dictionary, sequence_length=69, X_one_hot=False, split_train_test=False, save_locally=True))\n  ```\n  Returns:\n  ```\n  What it Actually Returns:\n  X, y\n\n  **The Features of What it Returns***:\n\n  Number of Train Examples:  574735\n  Number of Test Examples: 0\n\n  X.shape:\n  (574735, 69, 1)\n\n  X[0]:\n  [[53]\n   [41]\n   [38]\n   [ 3]\n   [49]\n   [51]\n   [48]\n   [43]\n   [38]\n    .\n    .\n    .\n       ]\n\n  y_train.shape:\n  (574735, 1)\n\n  y_train[0]:\n  [128]\n\n  ```\n  In the above example we created our data to contain 69 chars/words per sequence, be NOT one-hot encoded, for Y NOT to be one-hot encoded, NOT be split into test and train datasets, and to be saved to our local machine.\n\n* ### make_sentences\n  This method was a bit of an afterthought in this package, and was included upon the request of a friend. It splits the fully_processed_text into sentences by identifying splitter locations through chars like [. ! ?]\n\n  **Arguments**\n   * **fully_processed_text** - Takes in the fully processed text returned by the process_text method.\n  <br>\n\n  Example Usage\n  ```\n  train sentences = make_sentences(fully_processed_text)\n  ```\n  Returns:\n  ```\n  [' ', 'i', ' ', 'n', 'e', 'v', 'e', 'r', ' ', 's', 'e', 'e', ' ', 's', 'u', 'c', 'h', ' ', 'a', ' ', 's', 'o', 'n', '.']\n  ```\n\n* ### encode_sentences\n  This method is just an encoder for the make_sentences method. Turning them into integer representations of each word, using the int_to_word_dictionary.\n\n  **Arguments**\n   * **fully_processed_text** - Takes in the fully processed text returned by the process_text method.\n   * **int_to_word_dictionary** - Takes in the int_to_word_dictionary returned by the int_to_word method.\n  <br>\n\n  Example Usage\n  ```\n  encoded = encode_sentences(sentences, int_to_word_dictionary)\n  ```\n  Returns:\n  ```\n  [3, 42, 3, 47, 38, 55, 38, 51, 3, 52, 38, 38, 3, 52, 54, 36, 41, 3, 34, 3, 52, 48, 47, 15]\n  ```\n\n  **Note**, these final two methods in the Data class leave it up to the user to create target data, as there are many different ways to train on sentence data using LSTMs.\n\n## Visualize\n* ### make_prevelence_dict\n  This is a quick method that takes in all the chars/words in the fully processed text, and returns a dictionary based on prevalence sorted in descending order. This is a great tool for visualizing what your data looks like and catching possible overfitting causes.\n\n  **Arguments**\n   * **fully_processed_text** - Takes in the fully processed text returned by the process_text method.\n  <br>\n\n  Example Usage\n  ```\n  prevelence_dictionary = make_prevelence_dict(fully_processed_text)\n  ```\n  Returns:\n  ```\n  [(' ', 104548), ('e', 49605), ('t', 42825), ('o', 37018), ('a', 36947), ('n', 33119), ('i', 28636), ('h', 26660), ('s', 25503), ('d', 23906), ('r', 20554), ('l', 17637), ('u', 14114), ('w', 13419), ('g', 10906), ...]\n  ```\n* ### plot_top\n  Now that we have our sort prevalence dictionary, we can make a simple bar chart to visualize the dictionary better.\n\n  **Arguments**\n   * **number** - Takes in a number for the top 'n' chars/words you want to display.\n   * **prevelence_dictionary** - Takes in the prevelence_dictionary from the make_prevelence_dict method.\n  <br>\n\n  Example Usage\n  ```\n  plot_top(10, prevelence_dictionary)\n  ```\n  Returns:\n  ![](https://i.imgur.com/O3l6NCn.png)\n\n\n## Results\nFor the following example, I used Predictionary to pull data from project gutenberg on 5 books. Moby Dick, Alice in Wonderland, Huckleberry Finn, The Strange Case of Dr. Jekyll and Mr. Hyde, and the Iliad. These books totaled over 3,000,000 sequence examples for the network to learn. I used a 2 layer LSTM with over 200,000 weights and the following architecture...\n```\nModel: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            (None, 50, 1)        0                                            \n__________________________________________________________________________________________________\npermute_4 (Permute)             (None, 1, 50)        0           input_4[0][0]                    \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 1, 50)        2550        permute_4[0][0]                  \n__________________________________________________________________________________________________\nattention_prob (Permute)        (None, 50, 1)        0           dense_9[0][0]                    \n__________________________________________________________________________________________________\nmultiply_4 (Multiply)           (None, 50, 1)        0           input_4[0][0]                    \n                                                                 attention_prob[0][0]             \n__________________________________________________________________________________________________\nlstm_7 (LSTM)                   (None, 50, 128)      66560       multiply_4[0][0]                 \n__________________________________________________________________________________________________\ndropout_7 (Dropout)             (None, 50, 128)      0           lstm_7[0][0]                     \n__________________________________________________________________________________________________\nlstm_8 (LSTM)                   (None, 128)          131584      dropout_7[0][0]                  \n__________________________________________________________________________________________________\ndropout_8 (Dropout)             (None, 128)          0           lstm_8[0][0]                     \n__________________________________________________________________________________________________\ndense_10 (Dense)                (None, 100)          12900       dropout_8[0][0]                  \n__________________________________________________________________________________________________\ndense_11 (Dense)                (None, 70)           7070        dense_10[0][0]                   \n==================================================================================================\nTotal params: 220,664\nTrainable params: 220,664\nNon-trainable params: 0\n```\nThis network was EXTREMELY slow to train on my machine so I capped the number of epochs to about 35, which came out to around 40 hours.\n\nHere are a few input seeds, and the computer's predictions...\n\nInput:\n```\n\"The old woman talked surgeon's astronomy in the back country, and by j...\"\n```\nOutput:\n```\n\"...ove the soul the last the milking of the contrast in the coursers he dropped it out the new charms\"\n```\n\nInput:\n```\n\"The old woman talked surgeon's astronomy in the back country, and by j...\"\n```\nOutput:\n```\n\"...ove without paused great achilles lies and i show one murmer all brightly enough\"\n```\nAs you can see, not great or coherent by any stretch of the imagination, but not bad for a model that only sees 50 characters at a time and has >3M training examples.\n\n## ðŸ“œ License\n\nMIT License\n\nCopyright (c) 2019 Oliver Mathias\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/OliverMathias/Predictionary/archive/v0.1.5.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/OliverMathias/Predictionary",
    "keywords": "LSTM,pipeline,data,text,prediction",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "predictionary",
    "package_url": "https://pypi.org/project/predictionary/",
    "platform": "",
    "project_url": "https://pypi.org/project/predictionary/",
    "project_urls": {
      "Download": "https://github.com/OliverMathias/Predictionary/archive/v0.1.5.tar.gz",
      "Homepage": "https://github.com/OliverMathias/Predictionary"
    },
    "release_url": "https://pypi.org/project/predictionary/0.1.5/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A python package that allows for the easy creation of LSTM text prediction datasets",
    "version": "0.1.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6084385,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cb0bf6f03c04800ba8b6681f5f37321ae748e7a33b033fbcba5e2e85265cb10e",
          "md5": "9b97964f48f952bbf1bb421bc2a6fd5d",
          "sha256": "b4a8609699a3941add288bfbbd19920ccc9cc935ace28a8c95a74b533bd913bd"
        },
        "downloads": -1,
        "filename": "Predictionary-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9b97964f48f952bbf1bb421bc2a6fd5d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5600,
        "upload_time": "2019-11-03T02:27:46",
        "upload_time_iso_8601": "2019-11-03T02:27:46.496906Z",
        "url": "https://files.pythonhosted.org/packages/cb/0b/f6f03c04800ba8b6681f5f37321ae748e7a33b033fbcba5e2e85265cb10e/Predictionary-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "03ebdfdcea9ef7d65c2c6e25aabf0a8cbccd3a12ba0ab18678f4cea68bdbeb00",
          "md5": "efc8569ddcd43c470780bcb3b3081351",
          "sha256": "f844a021b85dde0c543c5f8e6baed8976913e4483b3035a31563b6ab569b37ef"
        },
        "downloads": -1,
        "filename": "Predictionary-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "efc8569ddcd43c470780bcb3b3081351",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5529,
        "upload_time": "2019-11-04T18:34:30",
        "upload_time_iso_8601": "2019-11-04T18:34:30.753563Z",
        "url": "https://files.pythonhosted.org/packages/03/eb/dfdcea9ef7d65c2c6e25aabf0a8cbccd3a12ba0ab18678f4cea68bdbeb00/Predictionary-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0d1e05ea522b66dc45cf0b2b88d8344f19c9fe43727e7b528cb5df8fc6402fd3",
          "md5": "ccea22d9987f0ee66d9178adfed61e50",
          "sha256": "b3dc5b782f15a266b02acdcc02d4aa8e959bb709204109c66578db8203c10021"
        },
        "downloads": -1,
        "filename": "Predictionary-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "ccea22d9987f0ee66d9178adfed61e50",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 15330,
        "upload_time": "2019-11-04T18:50:32",
        "upload_time_iso_8601": "2019-11-04T18:50:32.966118Z",
        "url": "https://files.pythonhosted.org/packages/0d/1e/05ea522b66dc45cf0b2b88d8344f19c9fe43727e7b528cb5df8fc6402fd3/Predictionary-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ff3279e59caac292722aabb4c6799491122ae556b28aa9732dbb4ea4bd90f2ec",
          "md5": "e3bcdf41bbdb7f5366969c6c6bb7290d",
          "sha256": "524b9a4ef9f74cf980ddef705f3975b2fdeeeb6ae646e32d54576c43eba47880"
        },
        "downloads": -1,
        "filename": "Predictionary-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "e3bcdf41bbdb7f5366969c6c6bb7290d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 17344,
        "upload_time": "2019-11-05T05:54:28",
        "upload_time_iso_8601": "2019-11-05T05:54:28.874783Z",
        "url": "https://files.pythonhosted.org/packages/ff/32/79e59caac292722aabb4c6799491122ae556b28aa9732dbb4ea4bd90f2ec/Predictionary-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "653da25cae544564cb219e442c95dbacb181e33b114ccace6d9b92567ee8e2cd",
          "md5": "4bd33075f21da39ea18a0aec94614dd0",
          "sha256": "69c18f66476def66dc8b49acdeaf56006cb2e1c0ef0fd743361a0cdfd4eeec1d"
        },
        "downloads": -1,
        "filename": "predictionary-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "4bd33075f21da39ea18a0aec94614dd0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 17320,
        "upload_time": "2019-11-06T02:40:23",
        "upload_time_iso_8601": "2019-11-06T02:40:23.892177Z",
        "url": "https://files.pythonhosted.org/packages/65/3d/a25cae544564cb219e442c95dbacb181e33b114ccace6d9b92567ee8e2cd/predictionary-0.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "653da25cae544564cb219e442c95dbacb181e33b114ccace6d9b92567ee8e2cd",
        "md5": "4bd33075f21da39ea18a0aec94614dd0",
        "sha256": "69c18f66476def66dc8b49acdeaf56006cb2e1c0ef0fd743361a0cdfd4eeec1d"
      },
      "downloads": -1,
      "filename": "predictionary-0.1.5.tar.gz",
      "has_sig": false,
      "md5_digest": "4bd33075f21da39ea18a0aec94614dd0",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 17320,
      "upload_time": "2019-11-06T02:40:23",
      "upload_time_iso_8601": "2019-11-06T02:40:23.892177Z",
      "url": "https://files.pythonhosted.org/packages/65/3d/a25cae544564cb219e442c95dbacb181e33b114ccace6d9b92567ee8e2cd/predictionary-0.1.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}