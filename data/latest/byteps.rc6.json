{
  "info": {
    "author": "Bytedance Inc.",
    "author_email": "lab-hr@bytedance.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "# BytePS\n\n[![Build Status](https://travis-ci.org/bytedance/byteps.svg?branch=master)](https://travis-ci.org/bytedance/byteps)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n![Pypi](https://img.shields.io/pypi/v/byteps.svg)\n\nBytePS is a high performance and general distributed training framework. It supports TensorFlow, Keras, PyTorch, and MXNet, and can run on either TCP or RDMA network.\n\nBytePS outperforms existing open-sourced distributed training frameworks by a large margin. For example, on BERT-large training, BytePS can achieve ~90% scaling efficiency with 256 GPUs (see below), which is much higher than [Horovod](https://github.com/horovod/horovod)+[NCCL](https://github.com/NVIDIA/nccl). In certain scenarios, BytePS can double the training speed compared with Horovod+NCCL.\n\n## News\n- [BytePS paper](https://www.usenix.org/conference/osdi20/presentation/jiang) has been accepted to OSDI'20. The code to reproduce the end-to-end evaluation is available [here](https://github.com/byteps/examples).\n- Support [gradient compression](https://github.com/bytedance/byteps/pull/225).\n- [v0.2.4](https://github.com/bytedance/byteps/tree/v0.2.4)\n    * Fix compatibility issue with tf2 + standalone keras\n    * Add support for tensorflow.keras\n    * Improve robustness of broadcast\n- [v0.2.3](https://github.com/bytedance/byteps/tree/v0.2.3)\n    * Add DistributedDataParallel module for PyTorch\n    * Fix the problem of different CPU tensor using the same name\n    * Add skip_synchronize api for PyTorch\n    * Add the option for lazy/non-lazy init\n- [v0.2.0](https://github.com/bytedance/byteps/tree/v0.2)\n    * Largely improve RDMA performance by enforcing page aligned memory.\n    * Add IPC support for RDMA. Now support colocating servers and workers without sacrificing much performance.\n    * Fix a hanging bug in BytePS server.\n    * Fix RDMA-related segmentation fault problem during fork() (e.g., used by PyTorch data loader).\n    * New feature: Enable mixing use of colocate and non-colocate servers, along with a smart tensor allocation strategy.\n    * New feature: Add ``bpslaunch`` as the command to launch tasks.\n    * Add support for pip install: ``pip3 install byteps``\n\n## Performance\n\nWe show our experiment on BERT-large training, which is based on GluonNLP toolkit. The model uses mixed precision.\n\nWe use Tesla V100 32GB GPUs and set batch size equal to 64 per GPU. Each machine has 8 V100 GPUs (32GB memory) with NVLink-enabled. Machines are inter-connected with 100 Gbps RDMA network. This is the same hardware setup you can get on [AWS](https://aws.amazon.com/about-aws/whats-new/2018/12/introducing-amazon-ec2-p3dn-instances-our-most-powerful-gpu-instance-yet/).\n\nBytePS achieves ~90% scaling efficiency for BERT-large with 256 GPUs. The code is available [here](https://github.com/ymjiang/gluon-nlp/tree/bert-byteps/scripts/bert). As a comparison, Horovod+NCCL has only ~70% scaling efficiency even after expert parameter tunning.\n\n![BERT-Large](https://user-images.githubusercontent.com/13852819/69874496-1ca43600-12f6-11ea-997b-b023e4c93360.png)\n\n\nWith slower network, BytePS offers even more performance advantages -- up to 2x of Horovod+NCCL. You can find more evaluation results at [performance.md](docs/performance.md).\n\n## Goodbye MPI, Hello Cloud\n\nHow can BytePS outperform Horovod by so much? One of the main reasons is that BytePS is designed for cloud and shared clusters, and throws away MPI.\n\nMPI was born in the HPC world and is good for a cluster built with homogeneous hardware and for running a single job. However, cloud (or in-house shared clusters) is different.\n\nThis leads us to rethink the best communication strategy, as explained in [here](docs/rationale.md). In short, BytePS only uses NCCL inside a machine, while re-implements the inter-machine communication.\n\nBytePS also incorporates many acceleration techniques such as hierarchical strategy, pipelining, tensor partitioning, NUMA-aware local communication, priority-based scheduling, etc.\n\n## Quick Start\n\nWe provide a [step-by-step tutorial](docs/step-by-step-tutorial.md) for you to run benchmark training tasks. The simplest way to start is to use our [docker images](docker). Refer to [Documentations](docs) for how to [launch distributed jobs](docs/running.md) and more [detailed configurations](docs/env.md). After you can start BytePS, read [best practice](docs/best-practice.md) to get the best performance.\n\nBelow, we explain how to install BytePS by yourself. There are two options.\n\n### Install by pip\n\n```\npip3 install byteps\n```\n\n### Build from source code\n\nYou can try out the latest features by directly installing from master branch:\n\n```\ngit clone --recursive https://github.com/bytedance/byteps\ncd byteps\npython3 setup.py install\n```\n\nNotes for above two options:\n- BytePS assumes that you have already installed one or more of the following frameworks: TensorFlow / PyTorch / MXNet.\n- BytePS depends on CUDA and NCCL. You should specify the NCCL path with `export BYTEPS_NCCL_HOME=/path/to/nccl`. By default it points to `/usr/local/nccl`.\n- The installation requires gcc>=4.9. If you are working on CentOS/Redhat and have gcc<4.9, you can try `yum install devtoolset-7` before everything else. In general, we recommend using gcc 4.9 for best compatibility ([how to pin gcc](https://github.com/bytedance/byteps/blob/3fba75def0d81c1d3225f8f397cc985200f57de7/docker/Dockerfile.mxnet#L72-L80)).\n- RDMA support: During setup, the script will automatically detect the RDMA header file. If you want to use RDMA, make sure your RDMA environment has been properly installed and tested before install ([install on Ubuntu-18.04](https://github.com/bytedance/byteps/blob/3fba75def0d81c1d3225f8f397cc985200f57de7/docker/Dockerfile.mxnet#L29-L33)).\n\n## Examples\n\nBasic examples are provided under the [example](example) folder. \n\nTo reproduce the end-to-end evaluation in our OSDI'20 paper, find the code at this [repo](https://github.com/byteps/examples).\n\n## Use BytePS in Your Code\n\nThough being totally different at its core, BytePS is highly compatible with Horovod interfaces (Thank you, Horovod community!). We chose Horovod interfaces in order to minimize your efforts for testing BytePS.\n\nIf your tasks only rely on Horovod's allreduce and broadcast, you should be able to switch to BytePS in 1 minute. Simply replace `import horovod.tensorflow as hvd` by `import byteps.tensorflow as bps`, and then replace all `hvd` in your code by `bps`. If your code invokes `hvd.allreduce` directly, you should also replace it by `bps.push_pull`.\n\nMany of our examples were copied from Horovod and modified in this way. For instance, compare the MNIST example for [BytePS](https://github.com/bytedance/byteps/blob/master/example/tensorflow/tensorflow_mnist.py) and [Horovod](https://github.com/horovod/horovod/blob/master/examples/tensorflow_mnist.py).\n\nBytePS also supports other native APIs, e.g., PyTorch Distributed Data Parallel and TensorFlow Mirrored Strategy. See [DistributedDataParallel.md](docs/DistributedDataParallel.md) and [MirroredStrategy.md](docs/MirroredStrategy.md) for usage.\n\n## Limitations and Future Plans\nBytePS does not support pure CPU training for now. One reason is that the [cheap PS assumption](docs/rationale.md) of BytePS do not hold for CPU training. Consequently, you need CUDA and NCCL to build and run BytePS.\n\nWe would like to have below features, and there is no fundamental difficulty to implement them in BytePS architecture. However, they are not implemented yet:\n* Sparse model training\n* Fault-tolerance\n* Straggler-mitigation\n\n## Publications\n\n1. [OSDI'20] \"[A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters](https://www.usenix.org/conference/osdi20/presentation/jiang)\". Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, Chuanxiong Guo. \n\n2. [SOSP'19] \"[A Generic Communication Scheduler for Distributed DNN Training Acceleration](https://i.cs.hku.hk/~cwu/papers/yhpeng-sosp19.pdf)\". Yanghua Peng, Yibo Zhu, Yangrui Chen, Yixin Bao, Bairen Yi, Chang Lan, Chuan Wu, Chuanxiong Guo. (Code is at [bytescheduler branch](https://github.com/bytedance/byteps/tree/bytescheduler/bytescheduler))",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/bytedance/byteps",
    "keywords": "",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "byteps",
    "package_url": "https://pypi.org/project/byteps/",
    "platform": "",
    "project_url": "https://pypi.org/project/byteps/",
    "project_urls": {
      "Homepage": "https://github.com/bytedance/byteps"
    },
    "release_url": "https://pypi.org/project/byteps/0.2.5.post15/",
    "requires_dist": null,
    "requires_python": ">=2.7.0",
    "summary": "A high-performance cross-framework Parameter Server for Deep Learning",
    "version": "0.2.5.post15",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11071217,
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "35d827cc102ecfe6f21d44cffad004c2e5c2605b516bf36066ede4388ff5a17c",
          "md5": "38bf3bbdc414479471a69ef46699fd5a",
          "sha256": "f4cd357e933ab665d65c680d524d86c4a2e29fb325d0b442b00e30ae83eb27be"
        },
        "downloads": -1,
        "filename": "byteps-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "38bf3bbdc414479471a69ef46699fd5a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7.0",
        "size": 231569,
        "upload_time": "2020-02-14T12:44:44",
        "upload_time_iso_8601": "2020-02-14T12:44:44.749559Z",
        "url": "https://files.pythonhosted.org/packages/35/d8/27cc102ecfe6f21d44cffad004c2e5c2605b516bf36066ede4388ff5a17c/byteps-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7174fb0c1edcc8ebf58a7000446536924458a940a43b5072eac91942155f1330",
          "md5": "099f66230085443978913c6dd559445b",
          "sha256": "d7f428df67283565dd74e042cd459d2c5d4f954cfe4c7aaa3d2c4c10d0eccecc"
        },
        "downloads": -1,
        "filename": "byteps-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "099f66230085443978913c6dd559445b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7.0",
        "size": 231764,
        "upload_time": "2020-03-11T08:57:51",
        "upload_time_iso_8601": "2020-03-11T08:57:51.389870Z",
        "url": "https://files.pythonhosted.org/packages/71/74/fb0c1edcc8ebf58a7000446536924458a940a43b5072eac91942155f1330/byteps-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ddbb797649314195b8e40a26f62bbd02a6ea3bc658e9d60c7c8a269ed18b0c8a",
          "md5": "c91172ec229f64bf6e166e69a1caad51",
          "sha256": "a2b0c1197fb3588a6e2a36be2f47777d4a8a87987888a4fe9f4b2059854b5050"
        },
        "downloads": -1,
        "filename": "byteps-0.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "c91172ec229f64bf6e166e69a1caad51",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7.0",
        "size": 242374,
        "upload_time": "2020-05-14T08:39:49",
        "upload_time_iso_8601": "2020-05-14T08:39:49.730724Z",
        "url": "https://files.pythonhosted.org/packages/dd/bb/797649314195b8e40a26f62bbd02a6ea3bc658e9d60c7c8a269ed18b0c8a/byteps-0.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "076fb5ce9c339e6c4289f0a7c5ee599db33ab0636e12c4b19a883f2968142f31",
          "md5": "033e535c9ea41a49be4316e5537fc0fd",
          "sha256": "d625a081f8d419be019cdc9d1e8392291dc184ed1ff909d9b2daebd4afcba43a"
        },
        "downloads": -1,
        "filename": "byteps-0.2.4.tar.gz",
        "has_sig": false,
        "md5_digest": "033e535c9ea41a49be4316e5537fc0fd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7.0",
        "size": 246532,
        "upload_time": "2020-08-27T15:42:13",
        "upload_time_iso_8601": "2020-08-27T15:42:13.734510Z",
        "url": "https://files.pythonhosted.org/packages/07/6f/b5ce9c339e6c4289f0a7c5ee599db33ab0636e12c4b19a883f2968142f31/byteps-0.2.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.5.post1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "af974db0d6402908939ab618b16b823fb9bbc08106cf5188d006285791b8d135",
          "md5": "86fc01662ed021825e0c32c3999f1c87",
          "sha256": "78a8f581c8db6c4c6d9044f88f7d6f6c15757f5d56a7912449dec5aa94c4c7a7"
        },
        "downloads": -1,
        "filename": "byteps-0.2.5.post1.tar.gz",
        "has_sig": false,
        "md5_digest": "86fc01662ed021825e0c32c3999f1c87",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7.0",
        "size": 299494,
        "upload_time": "2020-11-04T00:05:46",
        "upload_time_iso_8601": "2020-11-04T00:05:46.067111Z",
        "url": "https://files.pythonhosted.org/packages/af/97/4db0d6402908939ab618b16b823fb9bbc08106cf5188d006285791b8d135/byteps-0.2.5.post1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.5.post15": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "131a215b1dde04b99cf13ded3e87eb4abcf521fa24cca184a13ef3e6ee324e8c",
          "md5": "2be57409a12e9de88c5fea1360f2f188",
          "sha256": "9d6a3c5be4c4b5a172739744bb32725c433f9a9f63794e1885e7376ff4eea99f"
        },
        "downloads": -1,
        "filename": "byteps-0.2.5.post15.tar.gz",
        "has_sig": false,
        "md5_digest": "2be57409a12e9de88c5fea1360f2f188",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7.0",
        "size": 4568030,
        "upload_time": "2021-08-02T17:37:42",
        "upload_time_iso_8601": "2021-08-02T17:37:42.077191Z",
        "url": "https://files.pythonhosted.org/packages/13/1a/215b1dde04b99cf13ded3e87eb4abcf521fa24cca184a13ef3e6ee324e8c/byteps-0.2.5.post15.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "131a215b1dde04b99cf13ded3e87eb4abcf521fa24cca184a13ef3e6ee324e8c",
        "md5": "2be57409a12e9de88c5fea1360f2f188",
        "sha256": "9d6a3c5be4c4b5a172739744bb32725c433f9a9f63794e1885e7376ff4eea99f"
      },
      "downloads": -1,
      "filename": "byteps-0.2.5.post15.tar.gz",
      "has_sig": false,
      "md5_digest": "2be57409a12e9de88c5fea1360f2f188",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=2.7.0",
      "size": 4568030,
      "upload_time": "2021-08-02T17:37:42",
      "upload_time_iso_8601": "2021-08-02T17:37:42.077191Z",
      "url": "https://files.pythonhosted.org/packages/13/1a/215b1dde04b99cf13ded3e87eb4abcf521fa24cca184a13ef3e6ee324e8c/byteps-0.2.5.post15.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}