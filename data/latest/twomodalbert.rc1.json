{
  "info": {
    "author": "",
    "author_email": "Zuzanna Deutschman <zuza.deu@gmail.com>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "![Alt text](https://github.com/zuzadeu/twomodalbert/blob/develop/images/2022-11-09-19-01-23-image.png)\n\n*Fine-tune a neural network with two text modes of tunable weights*\n\n`pip install twomodalbert`\n\n# Introduction\n\nLet's consider whether we want to classify the below sample as positive or negative.\n\n| context                                          | text                            |\n| ------------------------------------------------ | ------------------------------- |\n| This product is horrible, which I didn't expect. | Its website is just incredible! |\n\nWhen only the message is considered, you can see its sentiment is positive due to the word 'incredible'. However, when the context is considered, sentiment will be rather negative.\n\nThe TwoModalBERT package allows us to quickly run an experiment with the two-modal neural network architecture described in [this section](https://github.com/zuzadeu/twomodalbert#neural-network-architecture). It allows quickly constructing a model on top of `PyTorch` and `transformers` libraries and enables experimenting with weights of two input texts. So, how to use the package?\n\n# Usage\n\n1. First, create a `config.ini` file in your working directory (parameters are described [here](https://github.com/zuzadeu/twomodalbert#parameters-in-configini)). \n   \n   ```ini\n   [GENERAL]\n    EPOCHS = 3\n    RANDOM_SEED = 42\n    BATCH_SIZE = 16\n    MAX_SEQ_LEN = 200\n    NUM_WORKERS = 2\n    PRETRAINED_MODEL_NAME_OR_PATH = bert-base-uncased\n    MODEL_SAVE_PATH = best_model_state.bin\n   ```\n\n2. Read the file.\n   \n   ```python\n    from configparser import ConfigParser\n    config = ConfigParser()\n    config.read(\"config.ini\")\n   ```\n\n3. Set the device.\n   \n   ```python\n   os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n   DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n   ```\n\n## Train\n\n1. initialize  DataPreparation and Trainer modules.\n   \n   ```python\n   from twomodalbert.DataPreparation import TwoModalDapaPreparation\n   from twomodalbert.Trainer import TwoModalBertTrainer\n   \n   DataPreparation = TwoModalDataPreparation(config=config)\n   Trainer = TwoModalBertTrainer(device=DEVICE, config=config)\n   ```\n\n2. Split input `df`  with *text*, *context*, *label* columns and create data loaders.\n   \n   ```python\n   (\n       train_data_loader,\n       train,\n       val_data_loader,\n       val,\n       test_data_loader,\n       test,\n   ) = DataPreparation.prepare_data(\n       df,\n       text_column=\"text\",\n       context_column=\"context\",\n       label_column=\"label\",\n       train_size=0.8,\n       val_size=0.1,\n   )\n   ```\n\n3. Train the model (nn parameters described in [this section](https://github.com/zuzadeu/twomodalbert#neural-network-architecture))\n   \n   ```python\n   model, history = Trainer.train_model(\n       train_data_loader,\n       train,\n       val_data_loader,\n       val,\n       text_size=100,\n       context_size=50,\n       binary=False,\n       text_p=0.3,\n       context_p=0.3,\n       output_p=0.3,\n   \n   )\n   ```\n\n## Predict\n\n1. Load the model.\n   \n   ```python\n   model = TwoModalBERTModel(\n       text_size=100,\n       context_size=50,\n       binary=False,\n       text_p=0.3,\n       context_p=0.3,\n       output_p=0.3,\n   )\n   \n   model.load_state_dict(torch.load(config[\"GENERAL\"][\"MODEL_SAVE_PATH\"]))\n   ```\n\n2. Evaluate it on a test set (choose any metric from `sklearn`).\n   \n   ```python\n   y_pred, y_test = test_model(model, test_data_loader, DEVICE)\n   ```\n\n3. Run the model on two text inputs\n   \n   ```python\n   text = \"Its website is just incredible!\"\n   \n   context = \"This product is horrible, which I didn't expect.\"\n   \n   predict_on_text(model, text, context, DEVICE)\n   ```\n\n# Neural network architecture\n\nBelow you can find how TwoModalBERT is constructed and what are the class parameters.\n\n![Alt text](https://github.com/zuzadeu/twomodalbert/blob/develop/images/2022-11-01-17-55-38-image.png)\n\nFirst of all, on top of the last BERT layer, a linear layer is added. To be precise, it is added on top of the CLS token. As the CLS token aggregates the entire sequence representation, it is often used in a classification task.\n\nThe linear layer transforms input features with hidden size relevant to the BERT model, which is usually 768 for the models available in the *transformers* package, into features with hidden size equal to predefined `context_size` and `text_size`.\n\nIn the next step, the dropout layers with probabilities `context_p` and `text_p` are added on the top. Why? Because it makes the neural network less sensitive to the specific weights of neurons and not prone to overfitting.\n\nFinally, both branches created similarly are combined and followed by another dropout layer of `output_p` and an activation function (Sigmoid if `binary`, else Softmax). \n\n# Parameters in `config.ini`\n\nSettings to be defined in the  `config.ini` file:\n\n| Variable                      | Description                                                                                        | Default Value     |\n| ----------------------------- | -------------------------------------------------------------------------------------------------- | ----------------- |\n| EPOCHS                        | the number of complete passes through the training dataset                                         | 3                 |\n| RANDOM_SEED                   | a number used to initialize a pseudorandom number generator                                        | 42                |\n| BARCH_SIZE                    | the number of training samples to work through before the model’s internal parameters are updated  | 16                |\n| MAX_SEQ_LEN                   | the maximum length in number of tokens for the inputs to the transformer model                     | 100               |\n| NUM_WORKERS                   | the number of processes that generate batches in parallel                                          | 2                 |\n| PRETRAINED_MODEL_NAME_OR_PATH | of a pre-trained model configuration to load from cache or download (equivalent to `transformers`) | bert-base-uncased |\n| MODEL_SAVE_PATH               | the model save path                                                                                | best_model.bin    |\n\n# Requirenments\n\n- `configparser-5.3.0`\n\n- `scikit-learn-1.0.2`\n\n- `torch-1.12.1+cu113`\n\n- `transformers-4.24.0`\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "twomodalbert",
    "package_url": "https://pypi.org/project/twomodalbert/",
    "platform": null,
    "project_url": "https://pypi.org/project/twomodalbert/",
    "project_urls": {
      "Bug Tracker": "https://github.com/zuzadeu/twomodalbert/issues",
      "Homepage": "https://github.com/zuzadeu/twomodalbert"
    },
    "release_url": "https://pypi.org/project/twomodalbert/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "A package to build neural networks based on two separate text fields.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15746137,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5843e9864b090fa4bd5a89035efcd7380a9c6f6cfd3fb4954c96fa39ad035d40",
          "md5": "c18467063a2a1dd151071bfefb3d242e",
          "sha256": "3e6d11001b787195c45200b5f5f3ca79e8cc81c8ee1f86420da075040c311bd8"
        },
        "downloads": -1,
        "filename": "twomodalbert-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c18467063a2a1dd151071bfefb3d242e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 9174,
        "upload_time": "2022-11-12T17:18:11",
        "upload_time_iso_8601": "2022-11-12T17:18:11.030099Z",
        "url": "https://files.pythonhosted.org/packages/58/43/e9864b090fa4bd5a89035efcd7380a9c6f6cfd3fb4954c96fa39ad035d40/twomodalbert-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "406ad1e0074b76bdbea3e4fdd893524d9b38647d0d7c9984236f33413c84b412",
          "md5": "3288dd0386601cbddcb441670d5409c2",
          "sha256": "95ac66f6d22ebe4d5c06465ec17166fd53b617760abf6e401ac37ddc0b8bac20"
        },
        "downloads": -1,
        "filename": "twomodalbert-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3288dd0386601cbddcb441670d5409c2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 241681,
        "upload_time": "2022-11-12T17:18:13",
        "upload_time_iso_8601": "2022-11-12T17:18:13.821707Z",
        "url": "https://files.pythonhosted.org/packages/40/6a/d1e0074b76bdbea3e4fdd893524d9b38647d0d7c9984236f33413c84b412/twomodalbert-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5843e9864b090fa4bd5a89035efcd7380a9c6f6cfd3fb4954c96fa39ad035d40",
        "md5": "c18467063a2a1dd151071bfefb3d242e",
        "sha256": "3e6d11001b787195c45200b5f5f3ca79e8cc81c8ee1f86420da075040c311bd8"
      },
      "downloads": -1,
      "filename": "twomodalbert-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c18467063a2a1dd151071bfefb3d242e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 9174,
      "upload_time": "2022-11-12T17:18:11",
      "upload_time_iso_8601": "2022-11-12T17:18:11.030099Z",
      "url": "https://files.pythonhosted.org/packages/58/43/e9864b090fa4bd5a89035efcd7380a9c6f6cfd3fb4954c96fa39ad035d40/twomodalbert-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "406ad1e0074b76bdbea3e4fdd893524d9b38647d0d7c9984236f33413c84b412",
        "md5": "3288dd0386601cbddcb441670d5409c2",
        "sha256": "95ac66f6d22ebe4d5c06465ec17166fd53b617760abf6e401ac37ddc0b8bac20"
      },
      "downloads": -1,
      "filename": "twomodalbert-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "3288dd0386601cbddcb441670d5409c2",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 241681,
      "upload_time": "2022-11-12T17:18:13",
      "upload_time_iso_8601": "2022-11-12T17:18:13.821707Z",
      "url": "https://files.pythonhosted.org/packages/40/6a/d1e0074b76bdbea3e4fdd893524d9b38647d0d7c9984236f33413c84b412/twomodalbert-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}