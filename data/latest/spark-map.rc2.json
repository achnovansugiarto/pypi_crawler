{
  "info": {
    "author": "",
    "author_email": "Pedro Faria <pedropark99@gmail.com>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "\r\n\r\n<!-- badges: start -->\r\n[![pytest](https://github.com/pedropark99/spark_map/actions/workflows/pytest.yml/badge.svg)](https://github.com/pedropark99/spark_map/actions/workflows/pytest.yml)\r\n<!-- badges: end -->\r\n\r\n# Overview <img src=\"doc/spark-map-logo.png\" align=\"right\" style=\"height:200px\" />\r\n\r\n`spark_map` is a python package that offers some tools that help you to apply a function over multiple columns of Apache Spark DataFrames, using `pyspark`. The package offers two main functions (or \"two main methods\") to distribute your calculations, which are `spark_map()` and `spark_across()`. Furthermore, the package offers several methods to map (or select) the columns to which you want to apply your calculations (these methods are called *mapping methods* in the package).\r\n\r\n# Installation\r\n\r\nTo get the latest version of `spark_map` at PyPI, use:\r\n\r\n```bash\r\npip install spark-map\r\n```\r\n\r\n# Documentation\r\n\r\nThe full documentation for `spark_map` package is available at the [website of the package](https://pedropark99.github.io/spark_map/). To access it, just use the `Function Reference` and `Articles` menus located at the top navigation bar of the website.\r\n\r\n\r\n\r\n# What problem `spark_map` solves?\r\n\r\nWhen you work a lot with data pipelines using Apache Spark and `pyspark`, at some day, you might find yourself writing a very long `agg()` statement to aggregate multiple columns of my Spark DataFrame with the same function, like this one below:\r\n\r\n```python\r\nfrom pyspark.sql.functions import sum, column\r\naggregates = (\r\n    spark.table('cards.detailed_sales_per_user')\r\n        .groupBy('day')\r\n        .agg(\r\n            sum(column('cards_lite')).alias('cards_lite'),\r\n            sum(column('cards_silver')).alias('cards_silver'),\r\n            sum(column('cards_gold')).alias('cards_gold'),\r\n            sum(column('cards_premium')).alias('cards_premium'),\r\n            sum(column('cards_enterprise')).alias('cards_enterprise'),\r\n            sum(column('cards_business')).alias('cards_business')\r\n        )\r\n)\r\n```\r\nThe problem with this code is that: it is not elegant; and it is error-prone. Because it involves copy and paste, and very subtle changes in each line. Following the golden rule of DRY (*do not repeat yourself*), we need a better way to write this code. That is the exact problem that `spark_map` solves for you!\r\n\r\nWhen you want to apply the same function (like `sum()`) over multiple columns of a Spark DataFrame (like `spark.table('cards.detailed_sales_per_user')`) that might be grouped by a variable (like `day`), you can use the `spark_map` package, to declare this operation in a much better, elegant and concise way, by using the `spark_map()` function.\r\n\r\n```python\r\nfrom spark_map.functions import spark_map\r\nfrom spark_map.mapping import starts_with\r\ngrouped_by_day = spark.table('cards.detailed_sales_per_user')\\\r\n    .groupBy('day')\r\n\r\naggregates = spark_map(grouped_by_day, starts_with('cards'), sum)\r\n```\r\n\r\n# How `spark_map()` works ?\r\n\r\nThe `spark_map()` function receives three inputs, which are `table` (i.e. the Spark DataFrame you want to use), `mapping` (i.e. a \"mapping\" that describes which columns you want to apply your function), and `function` (i.e. the function you want to apply to each column in the Spark DataFrame).\r\n\r\nIn short, the `starts_with('cards')` section in the above example tells `spark_map()` that you want to apply the input function on all columns of `grouped_by_day` whose name starts with the text `'cards'`. In other words, all `spark_map()` does is to apply the function you want (in the above example this function is `sum()`) to whatever column it finds in the input DataFrame which fits in the description of your mapping method.\r\n\r\nYou can use different mapping methods to select the columns of your DataFrame, and the package offers several built-in methods which can be very useful for you, which are available through the `spark_map.mapping` module of the package. You can select columns based on:\r\n\r\n- `at_position()`: their position (i.e. 3rd, 4th and 5th columns);\r\n- `matches()`: a regex to which their match;\r\n- `are_of_type()`: the type of data their store (i.e. all columns of type `int`);\r\n- `starts_with()` and `ends_with()`: its name starting or ending with a particular pattern;\r\n- `all_of()`: its name being inside a specific list of options;\r\n\r\n# Check the documentation for more examples and details\r\n\r\nThe [website](https://pedropark99.github.io/spark_map) have documentation for all functions of the package. If you have any trouble to understand or to find examples, is a good idea to check the [Function Reference](https://pedropark99.github.io/spark_map/reference-en.html) of the package, to see examples and more details about how each function works.\r\n\r\nTo understand how the mapping methods works, and how you can create your own mapping method, a good place to start is to read the article [Building the mapping](https://pedropark99.github.io/spark_map/english/articles/building-mapping.html) available at the website of the package.\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "Copyright 2022 Pedro Duarte Faria  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
    "maintainer": "",
    "maintainer_email": "",
    "name": "spark-map",
    "package_url": "https://pypi.org/project/spark-map/",
    "platform": null,
    "project_url": "https://pypi.org/project/spark-map/",
    "project_urls": {
      "Homepage": "https://pedropark99.github.io/spark_map/",
      "Issues": "https://github.com/pedropark99/spark_map/issues",
      "Repo": "https://github.com/pedropark99/spark_map"
    },
    "release_url": "https://pypi.org/project/spark-map/0.2.77/",
    "requires_dist": [
      "pyspark",
      "setuptools",
      "toml"
    ],
    "requires_python": ">=3.7",
    "summary": "Pyspark implementation of `map()` function for spark DataFrames",
    "version": "0.2.77",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16621080,
  "releases": {
    "0.2.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1df91f2c9ff2474e2768e70fd5edbaff54ba93b8875c98cefaa946061a072fb8",
          "md5": "f4de08b63032541208219d0082507f96",
          "sha256": "63d2e46615227920e9bc5570a987b3d4f3dc5f9df04cb541069574c1f4022905"
        },
        "downloads": -1,
        "filename": "spark_map-0.2.7.tar.gz",
        "has_sig": false,
        "md5_digest": "f4de08b63032541208219d0082507f96",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4996,
        "upload_time": "2022-12-21T23:36:40",
        "upload_time_iso_8601": "2022-12-21T23:36:40.409556Z",
        "url": "https://files.pythonhosted.org/packages/1d/f9/1f2c9ff2474e2768e70fd5edbaff54ba93b8875c98cefaa946061a072fb8/spark_map-0.2.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.77": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ce3bd2f1fc8740e3bd635757c5b934418cc29994af03405cad15682870d7a439",
          "md5": "454b2385494c8597f0b8f1893149b76e",
          "sha256": "ed0bebf942728591c48c98ffa40ad327b272a674648fb400e32c72eebb2bd5d9"
        },
        "downloads": -1,
        "filename": "spark_map-0.2.77-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "454b2385494c8597f0b8f1893149b76e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 8050,
        "upload_time": "2023-01-30T15:56:44",
        "upload_time_iso_8601": "2023-01-30T15:56:44.463454Z",
        "url": "https://files.pythonhosted.org/packages/ce/3b/d2f1fc8740e3bd635757c5b934418cc29994af03405cad15682870d7a439/spark_map-0.2.77-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6202b141cabda8738705a82ff23e8f5499dfd60ec661e8f45d139e300b8b67e1",
          "md5": "fefaca823b282ffbdcb358bacb2a456c",
          "sha256": "9862c7505685794664f70c9951fcbad4556a6cd0c011d596e4f40b9f40785de8"
        },
        "downloads": -1,
        "filename": "spark_map-0.2.77.tar.gz",
        "has_sig": false,
        "md5_digest": "fefaca823b282ffbdcb358bacb2a456c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 9842,
        "upload_time": "2023-01-30T15:56:45",
        "upload_time_iso_8601": "2023-01-30T15:56:45.954213Z",
        "url": "https://files.pythonhosted.org/packages/62/02/b141cabda8738705a82ff23e8f5499dfd60ec661e8f45d139e300b8b67e1/spark_map-0.2.77.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ce3bd2f1fc8740e3bd635757c5b934418cc29994af03405cad15682870d7a439",
        "md5": "454b2385494c8597f0b8f1893149b76e",
        "sha256": "ed0bebf942728591c48c98ffa40ad327b272a674648fb400e32c72eebb2bd5d9"
      },
      "downloads": -1,
      "filename": "spark_map-0.2.77-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "454b2385494c8597f0b8f1893149b76e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 8050,
      "upload_time": "2023-01-30T15:56:44",
      "upload_time_iso_8601": "2023-01-30T15:56:44.463454Z",
      "url": "https://files.pythonhosted.org/packages/ce/3b/d2f1fc8740e3bd635757c5b934418cc29994af03405cad15682870d7a439/spark_map-0.2.77-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6202b141cabda8738705a82ff23e8f5499dfd60ec661e8f45d139e300b8b67e1",
        "md5": "fefaca823b282ffbdcb358bacb2a456c",
        "sha256": "9862c7505685794664f70c9951fcbad4556a6cd0c011d596e4f40b9f40785de8"
      },
      "downloads": -1,
      "filename": "spark_map-0.2.77.tar.gz",
      "has_sig": false,
      "md5_digest": "fefaca823b282ffbdcb358bacb2a456c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 9842,
      "upload_time": "2023-01-30T15:56:45",
      "upload_time_iso_8601": "2023-01-30T15:56:45.954213Z",
      "url": "https://files.pythonhosted.org/packages/62/02/b141cabda8738705a82ff23e8f5499dfd60ec661e8f45d139e300b8b67e1/spark_map-0.2.77.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}