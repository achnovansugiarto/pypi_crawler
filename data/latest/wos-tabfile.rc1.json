{
  "info": {
    "author": "Donghua Chen",
    "author_email": "douglaschan@126.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Software Development :: Build Tools"
    ],
    "description": "# WoS Tab File\r\n\r\nExtract information from the exported Web of Science's tab delimited text file\r\n\r\n## Installation\r\n```pip\r\npip install wos-tabfile\r\n```\r\n\r\n## Basic Usage\r\n1. Retrieve publication year and numbers of citations in different indicators. \r\n```python\r\nfrom wostabfile.core import WosTabFile\r\nfrom collections import OrderedDict\r\n\r\n# data source\r\nroot_path = \"data/social network\"\r\nfile_path = root_path + \"/part1.txt\"\r\n\r\n# header fields in the text file\r\n# Tag from: https://images.webofknowledge.com/images/help/WOS/hs_wos_fieldtags.html\r\nwos_fields = [\"PY\", \"NR\", \"TC\", \"U1\", \"U2\"]\r\n# load data by specific keys\r\nwtf = WosTabFile(file_path=file_path)\r\n\r\ntable = wtf.generate_table(wos_fields)\r\n\r\nprint()\r\n# group by year using count for numbers of publications per year\r\nnew_table=wtf.group_by(table,key_index=0,value_index=1,method=\"count\")\r\nnew_table = OrderedDict(sorted(new_table.items()))\r\n\r\nprint(\"Year\\tNumber of publication\")\r\nfor year in new_table:\r\n    print(f'{year}\\t{new_table[year]}')\r\n\r\n```\r\n\r\n2. Retrieve keywords and its frequency from the bibliometric data\r\n\r\n```python\r\nfrom wostabfile.core import WosTabFile\r\nfrom nltk.stem import WordNetLemmatizer\r\nfrom nltk.tokenize import word_tokenize\r\n\r\nroot_path = \"data/social network\"\r\nwos_fields = [\"DE\"]\r\nwtf = WosTabFile(file_path=root_path)\r\ndict_words={}\r\n\r\ndef singularize(text):\r\n    wnl = WordNetLemmatizer()\r\n    tokens = [token.lower() for token in word_tokenize(text)]\r\n    lemmatized_words = [wnl.lemmatize(token) for token in tokens]\r\n    return (' '.join(lemmatized_words)).strip()\r\n\r\ndef process_row(rows):\r\n    global dict_words\r\n    for ks in rows:\r\n        wlist=ks[0]\r\n        for w in wlist.split(\";\"):\r\n            w=w.strip()\r\n            if w==\"\":\r\n                continue\r\n            w=singularize(w)\r\n            if w not in dict_words.keys():\r\n                dict_words[w]=1\r\n            else:\r\n                dict_words[w]+=1\r\n\r\ntable = wtf.generate_table_by_folder(wos_fields,func=process_row)\r\ndict_words =  dict(sorted(dict_words.items(),reverse=True, key=lambda item: item[1]))\r\n\r\nprint(\"Word\\tTerm Count\")\r\nfor k in list(dict_words.keys())[:20]:\r\n    print(f'{k}\\t{dict_words[k]}')\r\nprint()\r\nprint(\"Number of unique words: \",len(dict_words.keys()))\r\n\r\n```\r\n\r\nSample bibliometric data in the above examples can be downloaded in this \r\n[link](https://github.com/dhchenx/dhchenx.github.io/blob/master/projects/wos-tabfile/social-network.zip?raw=true).\r\n\r\n## License\r\nThe `wos-tabfile` project is provided by [Donghua Chen](https://github.com/dhchenx). \r\n\r\n\r\n\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "Web of Science,bibliometric analysis,WoS keyword analysis,WoS file",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "wos-tabfile",
    "package_url": "https://pypi.org/project/wos-tabfile/",
    "platform": "",
    "project_url": "https://pypi.org/project/wos-tabfile/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/wos-tabfile/0.0.1a1/",
    "requires_dist": null,
    "requires_python": ">=3.6, <4",
    "summary": "Extract information from the exported Web of Science's tab delimited text file",
    "version": "0.0.1a1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12362414,
  "releases": {
    "0.0.1a1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2e20ebb4425b8f599e160ea84b440ec09a2d69102288bca007573160814b1ddd",
          "md5": "a28f05506cc8abd5cbf6e3ee9b1a9f0b",
          "sha256": "2cff64c480c1f52dc565ecd16c6da0909094f3c2e8083e63b293e2e75182063d"
        },
        "downloads": -1,
        "filename": "wos-tabfile-0.0.1a1.tar.gz",
        "has_sig": false,
        "md5_digest": "a28f05506cc8abd5cbf6e3ee9b1a9f0b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6, <4",
        "size": 8728,
        "upload_time": "2021-12-20T16:03:28",
        "upload_time_iso_8601": "2021-12-20T16:03:28.457632Z",
        "url": "https://files.pythonhosted.org/packages/2e/20/ebb4425b8f599e160ea84b440ec09a2d69102288bca007573160814b1ddd/wos-tabfile-0.0.1a1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2e20ebb4425b8f599e160ea84b440ec09a2d69102288bca007573160814b1ddd",
        "md5": "a28f05506cc8abd5cbf6e3ee9b1a9f0b",
        "sha256": "2cff64c480c1f52dc565ecd16c6da0909094f3c2e8083e63b293e2e75182063d"
      },
      "downloads": -1,
      "filename": "wos-tabfile-0.0.1a1.tar.gz",
      "has_sig": false,
      "md5_digest": "a28f05506cc8abd5cbf6e3ee9b1a9f0b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6, <4",
      "size": 8728,
      "upload_time": "2021-12-20T16:03:28",
      "upload_time_iso_8601": "2021-12-20T16:03:28.457632Z",
      "url": "https://files.pythonhosted.org/packages/2e/20/ebb4425b8f599e160ea84b440ec09a2d69102288bca007573160814b1ddd/wos-tabfile-0.0.1a1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}