{
  "info": {
    "author": "Peng Shiyu",
    "author_email": "pengshiyuyx@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "PageParser\n==========\n\n|Build Status| |GitHub|\n\n项目简介\n--------\n\n项目名称：六行代码写爬虫\n\n英文名称：PageParser\n\n项目简介：一个爬虫使用的网页解析包，实现最大限度的代码复用\n\n项目目标：不懂网页解析也能写爬虫\n\n安装模块\n--------\n\n::\n\n    pip install page-parser\n\n最小项目示例：\n\n.. code:: python\n\n    import requests\n    from page_parser import BaiduParser\n\n    # 1、下载网页\n    response = requests.get(\"https://www.baidu.com/\")\n    html = response.content.decode(\"utf-8\")\n\n    # 2、解析网页\n    items = BaiduParser.parse_index(html)\n\n    # 3、输出数据\n    for item in items: print(item)\n    # {'title': '百度一下，你就知道'}\n\n支持网页\n--------\n\n+--------+------------+--------------------+-----------------------------------------------------+\n| 序号   | 网站       | 网页名称           | 网页地址                                            |\n+========+============+====================+=====================================================+\n| 1      | 百度       | 主页               | https://www.baidu.com/                              |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 2      | 豆瓣       | 电影 正在热映      | https://movie.douban.com/                           |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 3      | 拉勾       | 招聘职位列表页     | https://www.lagou.com/zhaopin/                      |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 4      | 企查查     | 融资事件页         | https://www.qichacha.com/elib\\_financing            |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 5      | 西刺代理   | 主页               | http://www.xicidaili.com/                           |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 6      | 西刺代理   | 国内高匿代理       | http://www.xicidaili.com/nn/                        |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 7      | 西刺代理   | 国内普通代理       | http://www.xicidaili.com/nt/                        |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 8      | 西刺代理   | 国内HTTPS代理      | http://www.xicidaili.com/wn/                        |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 9      | 西刺代理   | 国内HTTP代理       | http://www.xicidaili.com/wt/                        |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 10     | 搜狗搜索   | 微信公众号搜索页   | https://weixin.sogou.com/weixin?type=1&query=百度   |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 11     | 煎蛋网     | 主页列表           | http://jandan.net/                                  |\n+--------+------------+--------------------+-----------------------------------------------------+\n| 12     | 伯乐在线   | python栏目         | http://python.jobbole.com/                          |\n+--------+------------+--------------------+-----------------------------------------------------+\n\n使用示例\n--------\n\n.. code:: python\n\n    # -*- coding: utf-8 -*-\n\n    import requests\n    from page_parser import BaiduParser\n\n    # 1、下载网页\n    url = \"https://www.baidu.com/\"\n    response = requests.get(url)\n    response.encoding = response.apparent_encoding\n\n    # 2、解析网页\n    items = BaiduParser.parse_index(response.text)\n\n    # 3、输出数据\n    for item in items:\n        print(item)\n\n    # {'title': '百度一下，你就知道'}\n\n网络爬虫工作流程：\n------------------\n\n::\n\n    页面下载器 -> 页面解析器 -> 数据存储\n\n``页面下载器``: 主要涉及防爬攻破，方法各异，爬虫的难点也在此\n\n``页面解析器``:\n一般页面在一段时间内是固定的，每个人下载页面后都需要解析出页面内容，属于重复工作\n\n``数据存储``: 不管是存储到什么文件或数据库，主要看业务需求\n\n此项目就是将这项工作抽离出来，让网络爬虫程序重点关注于：网页下载，而不是重复的网页解析\n\n项目说明\n--------\n\n此项目可以和python 的requests 和scrapy 配合使用\n\n当然如果要和其他编程语言使用，可以使用flask等网络框架再次对此项目进行封装，提供网络接口即可\n\n发起人：mouday\n\n发起时间：2018-10-13\n\n需要更多的人一起来维护\n\n贡献代码\n--------\n\n贡献的代码统一放入文件夹：page\\_parser\n\n代码示例，如没有更好的理由，应该按照下面的格式，便于使用者调用\n\nbaidu\\_parser.py\n\n.. code:: python\n\n\n    # -*- coding: utf-8 -*-\n\n    # @Date    : 2018-10-13\n    # @Author  : Peng Shiyu\n\n    from parsel import Selector\n\n\n    class BaiduParser(object):\n        \"\"\"\n        百度网：https://www.baidu.com/\n        \"\"\"\n\n        @staticmethod\n        def parse_index(html):\n            \"\"\"\n            解析主页：https://www.baidu.com/\n            2018-10-13 pengshiyuyx@gmai.com\n            :param html: {str} 网页文本\n            :return: {iterator} 抽取的内容\n            \"\"\"\n            sel = Selector(html)\n            title = sel.css(\"title::text\").extract_first()\n            item = {\n                \"title\": title\n            }\n            yield item\n\n\n    if __name__ == '__main__':\n        import requests\n        response = requests.get(\"https://www.baidu.com/\")\n        response.encoding = response.apparent_encoding\n        items = BaiduParser.parse_index(response.text)\n        for item in items:\n            print(item)\n\n        # {'title': '百度一下，你就知道'}\n\n说明：\n------\n\n原则：\n~~~~~~\n\n1. 按照网站分类建立解析类\n\n2. 解析方法包含在解析类中 为方便调用需要静态方法\n\n3. 因为网页解析有时效性，所以必须\\ ``注明日期``\n\n命名规则：\n~~~~~~~~~~\n\n例如:\n\n::\n\n    文件名：baidu_parser\n    类名：BaiduParser\n    方法名：parse_index\n\n其他\n~~~~\n\n1. 必要的代码注释\n\n2. 必要的测试代码\n\n3. 其他必要的代码\n\n加入我们\n--------\n\n基本要求\n~~~~~~~~\n\n1. python的基本语法 + 面向对象 + 迭代器（yield）\n2. 掌握的库：requests、parsel、scrapy（了解即可）\n3. 解析库统一使用parsel（基于xpath），简单高效，与scrapy无缝衔接\n4. 不太懂也没关系，自己看参考文章，只要愿意学就会，瞬间提升自己\n\n参考文章：\n\n1. `Python编程：class类面向对象 <https://blog.csdn.net/mouday/article/details/79002712>`__\n\n2. `Python编程：生成器yield与yield\n   from区别简单理解 <https://blog.csdn.net/mouday/article/details/80760973>`__\n\n3. `Python爬虫：requests库基本使用 <https://blog.csdn.net/mouday/article/details/80087627>`__\n\n4. `Python网络爬虫之scrapy框架 <https://blog.csdn.net/mouday/article/details/79736108>`__\n\n5. `Python爬虫：xpath常用方法示例 <https://blog.csdn.net/mouday/article/details/80364436>`__\n\n6. `python爬虫：scrapy框架xpath和css选择器语法 <https://blog.csdn.net/mouday/article/details/80455560>`__\n\n联系方式\n~~~~~~~~\n\nPageParser QQ群号: 932301512\n\n.. figure:: images/page-parser-min.jpeg\n   :alt: \n\n.. |Build Status| image:: https://travis-ci.org/mouday/PageParser.svg?branch=master\n   :target: https://travis-ci.org/mouday/PageParser\n.. |GitHub| image:: https://img.shields.io/github/license/mashape/apistatus.svg\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/mouday/PageParser",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "page-parser",
    "package_url": "https://pypi.org/project/page-parser/",
    "platform": "",
    "project_url": "https://pypi.org/project/page-parser/",
    "project_urls": {
      "Homepage": "https://github.com/mouday/PageParser"
    },
    "release_url": "https://pypi.org/project/page-parser/0.0.4/",
    "requires_dist": [
      "parsel (>=1.4.0)",
      "requests (>=2.18.4)"
    ],
    "requires_python": "",
    "summary": "web crawler or spider parse page",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4966287,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1d183f0ff2f366ba2495e6cfa1f15f623a3e6ce36cc1cf50f254768ba2784061",
          "md5": "7fb555f80ca9c034e88e37873f8414b2",
          "sha256": "cf1486e545d0a16f02138ce2b9fc07f78c1f5ff2f2634ecac6451b54ddfef4a8"
        },
        "downloads": -1,
        "filename": "page_parser-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7fb555f80ca9c034e88e37873f8414b2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5066,
        "upload_time": "2018-10-15T07:57:21",
        "upload_time_iso_8601": "2018-10-15T07:57:21.659118Z",
        "url": "https://files.pythonhosted.org/packages/1d/18/3f0ff2f366ba2495e6cfa1f15f623a3e6ce36cc1cf50f254768ba2784061/page_parser-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cec16b611999aac0372c4b2fe28cab02b8ba796ac0ff0871874578e3bf9e27c0",
          "md5": "f138b0594c97070015ade754be9dbb15",
          "sha256": "86cfd50d015efe09072766deb188121b07c5be6916838830b2d13b8c23a2e690"
        },
        "downloads": -1,
        "filename": "page-parser-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f138b0594c97070015ade754be9dbb15",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 3263,
        "upload_time": "2018-10-15T07:57:23",
        "upload_time_iso_8601": "2018-10-15T07:57:23.166073Z",
        "url": "https://files.pythonhosted.org/packages/ce/c1/6b611999aac0372c4b2fe28cab02b8ba796ac0ff0871874578e3bf9e27c0/page-parser-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8f2d76862275e2c041143dc5befb5fb7295a5308ba59dddb49f6dcaf49c59293",
          "md5": "3a3a235c893bfaf85ae1efd28536c9f8",
          "sha256": "c34f3104a557b51aa279019c7dd749b51b465fe70825ba865e220c47acc17b58"
        },
        "downloads": -1,
        "filename": "page_parser-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3a3a235c893bfaf85ae1efd28536c9f8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 99894,
        "upload_time": "2018-10-15T08:32:34",
        "upload_time_iso_8601": "2018-10-15T08:32:34.036890Z",
        "url": "https://files.pythonhosted.org/packages/8f/2d/76862275e2c041143dc5befb5fb7295a5308ba59dddb49f6dcaf49c59293/page_parser-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "feef66e5f6c0656e25261b41767acb5f1750a02fa74db393910b6bd7d201b643",
          "md5": "3c225fd10bcdf70e35b3580b16f08027",
          "sha256": "1874d3990a466bd03b09ffe4c977acabcbc81dda2b3fabf40907a7384ffc430f"
        },
        "downloads": -1,
        "filename": "page_parser-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3c225fd10bcdf70e35b3580b16f08027",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 96216,
        "upload_time": "2018-10-15T08:32:35",
        "upload_time_iso_8601": "2018-10-15T08:32:35.945169Z",
        "url": "https://files.pythonhosted.org/packages/fe/ef/66e5f6c0656e25261b41767acb5f1750a02fa74db393910b6bd7d201b643/page_parser-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7bd9c6800f0f27c220a857b92c68295590a9b8aaf84dce8faedc8df96d6eb62a",
          "md5": "99a823c9f617d681e59b9f20d8d58847",
          "sha256": "cc2d96ea1ff8a5c413bffe7c421a164320b66340802a105acea75910de0d50b4"
        },
        "downloads": -1,
        "filename": "page_parser-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "99a823c9f617d681e59b9f20d8d58847",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 32847,
        "upload_time": "2018-10-17T05:35:34",
        "upload_time_iso_8601": "2018-10-17T05:35:34.860863Z",
        "url": "https://files.pythonhosted.org/packages/7b/d9/c6800f0f27c220a857b92c68295590a9b8aaf84dce8faedc8df96d6eb62a/page_parser-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "72885ad59981d212f69883038ba267acf421b7dc5957c0e492d8bfcb490fbcd9",
          "md5": "c12b7ffe00f2353ab6d76e06453769e3",
          "sha256": "2c0738742b503fe603c404ff1bb50a41bbf1405b4e07c81660087125b234fde9"
        },
        "downloads": -1,
        "filename": "page_parser-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "c12b7ffe00f2353ab6d76e06453769e3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 26232,
        "upload_time": "2018-10-17T05:35:36",
        "upload_time_iso_8601": "2018-10-17T05:35:36.600162Z",
        "url": "https://files.pythonhosted.org/packages/72/88/5ad59981d212f69883038ba267acf421b7dc5957c0e492d8bfcb490fbcd9/page_parser-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5cec9640dfcbb0440a7bf94deb317615800768d57b2b27bb3fa9d0e8353296cb",
          "md5": "8bc631693a0ad573083a2073e67940bc",
          "sha256": "0679f154273e3f71773071c9651df2e3f07d999859c8a4eb8c492dcdbf897d48"
        },
        "downloads": -1,
        "filename": "page_parser-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8bc631693a0ad573083a2073e67940bc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 14828,
        "upload_time": "2019-03-21T02:15:04",
        "upload_time_iso_8601": "2019-03-21T02:15:04.057976Z",
        "url": "https://files.pythonhosted.org/packages/5c/ec/9640dfcbb0440a7bf94deb317615800768d57b2b27bb3fa9d0e8353296cb/page_parser-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "46669a32790324fe241c3c4cee6eb9b9e3605ea83bcc312aa56e00958353e182",
          "md5": "d97c0d866d041b86cfb11928dd6ffed5",
          "sha256": "7bffbb1b502f9c0c7a260aefba280a700bea4c95310b05e2271863f9be1a731e"
        },
        "downloads": -1,
        "filename": "page_parser-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "d97c0d866d041b86cfb11928dd6ffed5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7737,
        "upload_time": "2019-03-21T02:15:06",
        "upload_time_iso_8601": "2019-03-21T02:15:06.702225Z",
        "url": "https://files.pythonhosted.org/packages/46/66/9a32790324fe241c3c4cee6eb9b9e3605ea83bcc312aa56e00958353e182/page_parser-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5cec9640dfcbb0440a7bf94deb317615800768d57b2b27bb3fa9d0e8353296cb",
        "md5": "8bc631693a0ad573083a2073e67940bc",
        "sha256": "0679f154273e3f71773071c9651df2e3f07d999859c8a4eb8c492dcdbf897d48"
      },
      "downloads": -1,
      "filename": "page_parser-0.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8bc631693a0ad573083a2073e67940bc",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 14828,
      "upload_time": "2019-03-21T02:15:04",
      "upload_time_iso_8601": "2019-03-21T02:15:04.057976Z",
      "url": "https://files.pythonhosted.org/packages/5c/ec/9640dfcbb0440a7bf94deb317615800768d57b2b27bb3fa9d0e8353296cb/page_parser-0.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "46669a32790324fe241c3c4cee6eb9b9e3605ea83bcc312aa56e00958353e182",
        "md5": "d97c0d866d041b86cfb11928dd6ffed5",
        "sha256": "7bffbb1b502f9c0c7a260aefba280a700bea4c95310b05e2271863f9be1a731e"
      },
      "downloads": -1,
      "filename": "page_parser-0.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "d97c0d866d041b86cfb11928dd6ffed5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 7737,
      "upload_time": "2019-03-21T02:15:06",
      "upload_time_iso_8601": "2019-03-21T02:15:06.702225Z",
      "url": "https://files.pythonhosted.org/packages/46/66/9a32790324fe241c3c4cee6eb9b9e3605ea83bcc312aa56e00958353e182/page_parser-0.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}