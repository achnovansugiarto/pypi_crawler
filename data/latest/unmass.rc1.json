{
  "info": {
    "author": "Thamme Gowda",
    "author_email": "tgowdan@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# MASS\n\n[MASS](https://arxiv.org/pdf/1905.02450.pdf) is a novel pre-training method for sequence to sequence based language generation tasks.\n It randomly masks a sentence fragment in the encoder, and then predicts it in the decoder.\n\n![img](figs/mass.png)\n\nMASS can be applied on cross-lingual tasks such as neural machine translation (NMT), \nand monolingual tasks such as text summarization. \nThe current codebase supports unsupervised NMT (implemented based on XLM).\n\nCredits: the original developers/researchers:\n```\nfacebookresearch/XLM\n   |---microsoft/MASS\n        |---<this>\n```\n\n## Unsupervised NMT\n\nUnsupervised Neural Machine Translation just uses monolingual data to train the models.\n During MASS pre-training, the source and target languages are pre-trained in one model, with the \n corresponding language embeddings to differentiate the languages.\n  During MASS fine-tuning, back-translation is used to train the unsupervised models. \n  We provide pre-trained and fine-tuned models:\n\n| Languages | Pre-trained Model | Fine-tuned Model | BPE codes | Vocabulary |\n|-----------|:-----------------:|:----------------:| ---------:| ----------:|\n| EN - FR   | [MODEL](https://modelrelease.blob.core.windows.net/mass/mass_enfr_1024.pth)    |   [MODEL](https://modelrelease.blob.core.windows.net/mass/mass_ft_enfr_1024.pth)   | [BPE codes](https://dl.fbaipublicfiles.com/XLM/codes_enfr) | [Vocabulary](https://dl.fbaipublicfiles.com/XLM/vocab_enfr) |\n| EN - DE   | [MODEL](https://modelrelease.blob.core.windows.net/mass/mass_ende_1024.pth) | [MODEL](https://modelrelease.blob.core.windows.net/mass/mass_ft_ende_1024.pth) | [BPE codes](https://dl.fbaipublicfiles.com/XLM/codes_ende) | [Vocabulary](https://dl.fbaipublicfiles.com/XLM/vocab_ende) |\n| En - RO   | [MODEL](https://modelrelease.blob.core.windows.net/mass/mass_enro_1024.pth) | [MODEL](https://modelrelease.blob.core.windows.net/mass/mass_ft_enro_1024.pth) | [BPE_codes](https://dl.fbaipublicfiles.com/XLM/codes_enro) | [Vocabulary](https://dl.fbaipublicfiles.com/XLM/vocab_enro) |\n\nWe are also preparing larger models on more language pairs, and will release them in the future.\n\n### Dependencies\nCurrently we implement MASS for unsupervised NMT based on the codebase of [XLM](https://github.com/facebookresearch/XLM). The depencies are as follows:\n- Python 3\n- NumPy\n- PyTorch (version 0.4 and 1.0)\n- fastBPE (for BPE codes)\n- Moses (for tokenization)\n- Apex (for fp16 training)\n\n### Data Ready\n\nWe use the same BPE codes and vocabulary with XLM. Here we take English-French as an example.\n\n```\ncd MASS\n\nwget https://dl.fbaipublicfiles.com/XLM/codes_enfr\nwget https://dl.fbaipublicfiles.com/XLM/vocab_enfr\n\n./get-data-nmt.sh --src en --tgt fr --reload_codes codes_enfr --reload_vocab vocab_enfr\n```\n\n### Pre-training:\n```\npython train.py                                      \\\n--exp_name unsupMT_enfr                              \\\n--data_path ./data/processed/en-fr/                  \\\n--lgs 'en-fr'                                        \\\n--mass_steps 'en,fr'                                 \\\n--encoder_only false                                 \\\n--emb_dim 1024                                       \\\n--n_layers 6                                         \\\n--n_heads 8                                          \\\n--dropout 0.1                                        \\\n--attention_dropout 0.1                              \\\n--gelu_activation true                               \\\n--tokens_per_batch 3000                              \\\n--optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 \\\n--epoch_size 200000                                  \\\n--max_epoch 100                                      \\\n--eval_bleu true                                     \\\n--word_mass 0.5                                      \\\n--min_len 5                                          \\\n```\n\n\nDuring the pre-training prcess, even without any back-translation, you can observe the model can achieve some intial BLEU scores:\n```\nepoch -> 4\nvalid_fr-en_mt_bleu -> 10.55\nvalid_en-fr_mt_bleu ->  7.81\ntest_fr-en_mt_bleu  -> 11.72\ntest_en-fr_mt_bleu  ->  8.80\n```\n#### Distributed Training\n\nTo use *multiple GPUs* e.g. 3 GPUs **on same node**\n```\nexport NGPU=3; CUDA_VISIBLE_DEVICES=0,1,2 python -m torch.distributed.launch --nproc_per_node=$NGPU train.py [...args]\n```\nTo use *multiple GPUS* across **many nodes**, use Slurm to request multi-node job and launch the above command. \nThe code automatically detects the SLURM_* environment vars to distribute the training.\n\n\n### Fine-tuning \nAfter pre-training, we use back-translation to fine-tune the pre-trained model on unsupervised machine translation:\n```\nMODEL=mass_enfr_1024.pth\n\npython train.py \\\n  --exp_name unsupMT_enfr                              \\\n  --data_path ./data/processed/en-fr/                  \\\n  --lgs 'en-fr'                                        \\\n  --bt_steps 'en-fr-en,fr-en-fr'                       \\\n  --encoder_only false                                 \\\n  --emb_dim 1024                                       \\\n  --n_layers 6                                         \\\n  --n_heads 8                                          \\\n  --dropout 0.1                                        \\\n  --attention_dropout 0.1                              \\\n  --gelu_activation true                               \\\n  --tokens_per_batch 2000                              \\\n  --batch_size 32\t                                     \\\n  --bptt 256                                           \\\n  --optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 \\\n  --epoch_size 200000                                  \\\n  --max_epoch 30                                       \\\n  --eval_bleu true                                     \\\n  --reload_model \"$MODEL,$MODEL\"                       \\\n```\n\nWe also provide a demo to use MASS pre-trained model on the WMT16 en-ro bilingual dataset. We provide pre-trained and fine-tuned models:\n\n| Model | Ro-En BLEU (with BT) |\n|:---------:|:----:|\n| Baseline | 34.0 |\n| XLM | 38.5 |\n| [MASS](https://modelrelease.blob.core.windows.net/mass/mass_mt_enro_1024.pth) | 39.1 |\n\n\nDownload dataset by the below command:\n```\nwget https://dl.fbaipublicfiles.com/XLM/codes_enro\nwget https://dl.fbaipublicfiles.com/XLM/vocab_enro\n\n./get-data-bilingual-enro-nmt.sh --src en --tgt ro --reload_codes codes_enro --reload_vocab vocab_enro\n```\n\nAfter download the mass pre-trained model from the above link. And use the following command to fine tune:\n```\nMODEL=mass_enro_1024.pth\n\npython train.py \\\n\t--exp_name unsupMT_enro                              \\\n\t--data_path ./data/processed/en-ro                   \\\n\t--lgs 'en-ro'                                        \\\n\t--bt_steps 'en-ro-en,ro-en-ro'                       \\\n\t--encoder_only false                                 \\\n\t--mt_steps 'en-ro,ro-en'                             \\\n\t--emb_dim 1024                                       \\\n\t--n_layers 6                                         \\\n\t--n_heads 8                                          \\\n\t--dropout 0.1                                        \\\n\t--attention_dropout 0.1                              \\\n\t--gelu_activation true                               \\\n\t--tokens_per_batch 2000                              \\\n\t--batch_size 32                                      \\\n\t--bptt 256                                           \\\n\t--optimizer adam_inverse_sqrt,beta1=0.9,beta2=0.98,lr=0.0001 \\\n\t--epoch_size 200000                                  \\\n\t--max_epoch 50                                       \\\n\t--eval_bleu true                                     \\\n\t--reload_model \"$MODEL,$MODEL\"\n```\n\n\n## Training Details \n\n`MASS-base-uncased` uses 32x NVIDIA 32GB V100 GPUs and trains on (Wikipekia + BookCorpus, 16GB) for 20 epochs (float32), batch size is simulated as 4096.\n\n### Other questions\n> 1. Q: When I run this program in multi-gpus or multi-nodes, the program reports errors like `ModuleNotFouldError: No module named 'mass'`.   \n  A: This seems a bug in python `multiprocessing/spawn.py`, a direct solution is to move these files into each relative folder under fairseq. Do not forget to modify the import path in the code.\n\n\n\n## Reference\n\nIf you find MASS useful in your work, you can cite the paper as below:\n\n    @inproceedings{song2019mass,\n        title={MASS: Masked Sequence to Sequence Pre-training for Language Generation},\n        author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},\n        booktitle={International Conference on Machine Learning},\n        pages={5926--5936},\n        year={2019}\n    }\n\n## Related Works\n\n* [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/pdf/2004.09297.pdf), by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu. GitHub: https://github.com/microsoft/MPNet\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/thammegowda/unmass",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "unmass",
    "package_url": "https://pypi.org/project/unmass/",
    "platform": "any",
    "project_url": "https://pypi.org/project/unmass/",
    "project_urls": {
      "Homepage": "https://github.com/thammegowda/unmass"
    },
    "release_url": "https://pypi.org/project/unmass/0.1.0/",
    "requires_dist": [
      "ruamel.yaml (>=0.16.10)",
      "sacrebleu (>=1.4.6)",
      "sacremoses (>=0.0.43)",
      "nlcodec (>=0.2.3)",
      "torch (>=1.4)"
    ],
    "requires_python": ">=3.7",
    "summary": "UNMASS - Unsupervised NMT with Masked Sequence-to-Sequence training",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8037635,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9d8b00b55116bb99ef85184c9f0ab882f424097c9de89bb5681621a28f194444",
          "md5": "89b0649a6a791f80c6d1ae88bd3823b0",
          "sha256": "aba11d51260a75118d3f5b1c68d32bf4c59968ae8f58c41cfe69642d02cd82de"
        },
        "downloads": -1,
        "filename": "unmass-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "89b0649a6a791f80c6d1ae88bd3823b0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 67844,
        "upload_time": "2020-08-25T18:43:09",
        "upload_time_iso_8601": "2020-08-25T18:43:09.219776Z",
        "url": "https://files.pythonhosted.org/packages/9d/8b/00b55116bb99ef85184c9f0ab882f424097c9de89bb5681621a28f194444/unmass-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c80e06607790252232db1bdafe0e6a5d586bbe773b6a6a86f9be72699f2a32f6",
          "md5": "6f958881590b332fc7bc7643e9540843",
          "sha256": "d1aa3b436e275f98f6de427bfa5c66e203cfa5acbe1b8c581fb8b0eb7e922bbe"
        },
        "downloads": -1,
        "filename": "unmass-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "6f958881590b332fc7bc7643e9540843",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 54529,
        "upload_time": "2020-08-25T18:43:13",
        "upload_time_iso_8601": "2020-08-25T18:43:13.510777Z",
        "url": "https://files.pythonhosted.org/packages/c8/0e/06607790252232db1bdafe0e6a5d586bbe773b6a6a86f9be72699f2a32f6/unmass-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9d8b00b55116bb99ef85184c9f0ab882f424097c9de89bb5681621a28f194444",
        "md5": "89b0649a6a791f80c6d1ae88bd3823b0",
        "sha256": "aba11d51260a75118d3f5b1c68d32bf4c59968ae8f58c41cfe69642d02cd82de"
      },
      "downloads": -1,
      "filename": "unmass-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "89b0649a6a791f80c6d1ae88bd3823b0",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 67844,
      "upload_time": "2020-08-25T18:43:09",
      "upload_time_iso_8601": "2020-08-25T18:43:09.219776Z",
      "url": "https://files.pythonhosted.org/packages/9d/8b/00b55116bb99ef85184c9f0ab882f424097c9de89bb5681621a28f194444/unmass-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c80e06607790252232db1bdafe0e6a5d586bbe773b6a6a86f9be72699f2a32f6",
        "md5": "6f958881590b332fc7bc7643e9540843",
        "sha256": "d1aa3b436e275f98f6de427bfa5c66e203cfa5acbe1b8c581fb8b0eb7e922bbe"
      },
      "downloads": -1,
      "filename": "unmass-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "6f958881590b332fc7bc7643e9540843",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 54529,
      "upload_time": "2020-08-25T18:43:13",
      "upload_time_iso_8601": "2020-08-25T18:43:13.510777Z",
      "url": "https://files.pythonhosted.org/packages/c8/0e/06607790252232db1bdafe0e6a5d586bbe773b6a6a86f9be72699f2a32f6/unmass-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}