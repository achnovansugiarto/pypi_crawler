{
  "info": {
    "author": "Ramon Invarato Menendez",
    "author_email": "r.invarato@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Sort in disk\nSort a bulk of data in disk (RAM memory free) and optionally in parallel (Use available resources).\n\nIt is recomendable use `sorted_in_disk` to sort bulk of data to disk, because `sorted` save data RAM memory that is \npretty limited.\n\nThis is a good way to work with big data without RAM memory limits (hard disk have more size and cheaper than \nRAM memory).\n\n`sorted_in_disk` algorithm return data as soon as posible and only work the minimum necessary, that mean sorted is in \nreal time while return sorted data to you (in injection time perform a quick and minimum sorted work).\n\n\n## Table of contents\n * [Technologies](#technologies)\n * [Setup](#setup)\n * [Quick use](#quick-use)\n * [Algorithm lifecycle](#algorithm-lifecycle)\n * [Performance](#performance)\n * [Documentation](#documentation)\n * [Limitations](#limitations)\n * [About this algorithm](#about-this-algorithm)\n * [Is useful for you?](#is-useful-for-you)\n * [Contact](#contact)\n\n\n## Technologies\nProject create with:\n * Python 3\n\n\n## Setup\nInstall it locally using PyPI (last release in https://pypi.org/project/sorted-in-disk/):\n\n```\npip install sorted-in-disk\n```\n\n### Information about dependencies\nMandatory dependencies (auto-import with PIP):\n * `easy_binary_file`: to manage binary files\n * `quick_queue`: to pass quick values between processes\n\nOptional dependencies (only hand-installable):\n * `psutil`: to check current memory of process and save to disk (if you do not have `psutil`, then when certain \n             number of items will be reached, then save to disk). Not mandatory because I detected some \n             incompatibilities issues in some systems (`psutil` need extra permissions). Optionally you can install \n             `psutil` with:\n```\npip install psutil\n```\n\n\n## Quick use\nImport:\n```python\nfrom sorted_in_disk import sorted_in_disk\n```\n\nExample of inject a list of string lines with pipe `|` delimiter where key is first part:\n```python\nunsorted_data = [\n    \"key3|value3\",\n    \"key1|value1\",\n    \"key2|value2\"\n]\n\nsid = sorted_in_disk(unsorted_data,\n                     key=lambda line: line.split(\"|\")[0])\n\nfor element in sid:\n    print(element)\n```\n\nYou can read from `sorted_in_disk` like a dict style with `values`, `keys`, `items` methods. Example:\n```python\nfor key, value in sorted_in_disk(...).items():\n    pass\n```\n\n`SortInDisk` implements `__iter__` (with `values`). Example:\n```python\nfor value in sorted_in_disk(...):\n    pass\n```\n\nIf you have a big text file where each line has a key to sort, you can read file line to line quickly with\n`read_iter_from_file` in `sorted_in_disk.utils` package to pass an iterable for lines in the file (this way not read \nfull file in one time, read line per line in a generator; only consume one line size in RAM memory). \nExample (key supposes file have lines similar to \"key1|value1\"):\n```python\nfrom sorted_in_disk.utils import read_iter_from_file\n\niterable_with_unsorted_data = read_iter_from_file(\"path/to/your/file/to/read\")\n\nsid = sorted_in_disk(iterable_with_unsorted_data,\n                     key=lambda line: line.split(\"|\")[0])\n\nfor sorted_line in sid:\n    print(sorted_line)\n```\n\n\n### In comparison with sorted method\n`sorted_in_disk` is similar to oficial `sorted` method (https://docs.python.org/3/library/functions.html#sorted), with\n`iterable`, `key` and `reverse` args. \n\nExample for comparison of common `sorted` use: \n```python\nunsorted_data = [\n    \"key3|value3\",\n    \"key1|value1\",\n    \"key2|value2\"\n]\n\nfor element in sorted(unsorted_data,\n                      key=lambda line: line.split(\"|\")[0]):\n    print(element)\n```\n\nExample of `sorted_in_disk` use:\n```python\nfor element in sorted_in_disk(unsorted_data,\n                              key=lambda line: line.split(\"|\")[0]):\n    print(element)\n```\n\n\n## Algorithm lifecycle\n`sort_in_disk` method creates one instance of `SortInDisk` object.\n\nThe sort in disk method have a lifecycle:\n * **Clean before**: Check if not exist previous structure in disk and clean if it is necessary\n * **Injection**: Data unsorted is inject in this structure\n * **Reader**: Data sorted is read from this structure\n * **Clean after**: clean if it is necessary\n\n\n### Clean before\nOne live instance of `SortInDisk` own a temporal dir path to create temporal files.\n\nNo more than one instance of `SortInDisk` at time may own to same temporal dir path.\n\nFor this reason is important check if not exist previous temporal files generated \nfor `sort_in_disk` in same folder in disk.\n\nOne instance of `SortInDisk can:\n * **Create** a new temporal dir: Not exist other temporal dir with same name. This is a clean state.\n * **Append** more data to previous temporal dir: If exist a temporal dir with same name and not other instance that\n    pointing to this dir are live, then this new instance can own previous temporal dir to append more data. \n    This is enable if `sorted_in_disk` have arg `append` to `True`.\n * **Delete** previous temporal dir: delete all, similar to clean state and begin.\n    This is enable if `sorted_in_disk` have arg `append` to `False`.\n * **Rename** a new temporal dir: in case of exist a previous temporal dir with same name, create a new temporal dir\n    with an increment number and use this new as clean state.\n    This is enable if `sorted_in_disk` have arg `ensure_different_dirs` to `True`.\n\n\n### Injection\nData can be injected in multiprocess or mono-process (in the main thread) way.\n\nEach thread to create a new raw file with all injected data.\n\nAlso, each thread create a cache in RAM memory with index of keys to point data and extra information.\n\nBy other hand, it is created one file more with general information.\n\nIf you have `psutil` installed, then if this cache has more size than `max_write_process_size` and \n`count_insert_to_check` is reached (to reduce check and increase injection speed), then keys cached are pre-sorted \nand saved to disk and create a new empty cache.\n\nIn other way, if you do not have `psutil` installed (or if `max_write_process_size` is `None`), then if\n`count_insert_to_check` is reached, then keys cached are pre-sorted and saved to disk and create a new empty cache.\n\nYou can enable mono-process if `sorted_in_disk` have arg `write_processes` to `0`.\n\nYou can enable multiprocess if `sorted_in_disk` have arg `write_processes` to `None` to auto-determinate physical\nprocessors in current system or define a number of processors. Depend on your velocity of main-thread reading data you \ntake advantage of multiprocess or not. If multiprocess is enable, then when main-thread end of inject data next code \nwill be not blocked; it is necessary join with `join_multiprocess()`, some methods call to `join_multiprocess()` as\n`__iter__`, `values`, `keys`, `items` or `__len__`. Multiprocess have a queue determine with `queue_max_size` to work \nwith data between processes (it can be regulated, but it is better not to put neither too much to have enough \nRAM memory nor too little to do not have idle process).\n\nTo sum up process control:\n * `write_processes = 0` is mono-process injection.\n * `write_processes = None` is multi-process injection, one process per physical processor detected in system.\n * `write_processes = 4` is multi-process injection, 4 process to inject data.\n * `write_processes = [\"path/tmp_process_1\", \"path/tmp_process_2\", \"path/tmp_process_3\"]` is multi-process injection, \n    3 process to inject data, one per directory.\n\nTo sum up memory control:\n * `max_write_process_size = None` or not `psutil`, and `count_insert_to_check = 1000000` then save index to disk when\n 1000000 values binjected, clean and continue.\n * `max_write_process_size = 1024*1024*1024` or not `psutil`, and `count_insert_to_check = 1000000` then check if 1 GB are\n reached each 100000 values inject, if precess reach 1 GB, then index save to disk, clean and continue.\n * `queue_max_size` only if multi-process injection is enable. If `queue_max_size=1000` then main process put in the \n queue max 1000 values, those values will be taking by consumption processes. If data does not fit in the queue then \n main  process will go to idle until the queue have space.\n\n\n### Reader\nData can read in multiprocess or mono-process (in the main thread) way.\n\nIn one thread, the header of data read from all files and sorted these headers. Then it return the first header,\nchoose next header from file and sort other time all headers and repeat.\n\nIn this point have data in real time (data streaming). Data is not necessary full sorted to sure a correct sort.\n\nIf full data read, then file will be closed (and go to after clean step in life cycle).\n\nYou can enable mono-process if `sorted_in_disk` have arg `read_process` to `False`.\n\nYou can enable multiprocess if `sorted_in_disk` have arg `read_process` to `True`. In this case, data is\nsorted and enqueue until these sorted data will read in main-thread. This is useful if we want to take advantage of\nthe data preparation between data consumption. Multiprocess have a queue determine with `iter_m_queue_max_size` to \nwork with data between processes (it can be regulated, but it is better not to put neither too much to have enough \nRAM memory nor too little to do not have idle process).\n\nTo sum up:\n * `read_process = False` is mono-process reader.\n * `read_process = True` is multi-process reader (one process more to prepare a bulk of sorted data).\n * `iter_m_queue_max_size` only if multi-process reader is enable. If `iter_m_queue_max_size=1000` then second process \n have a queue of 1000 positions to put data, if data does not fit in the queue then process will go to idle until queue \n main process take data from the queue and leave space.\n\n### Clean after\nDepend on we want append more data or maintain sorted work in disk, maybe we do not want clean anything.\n\nIn the end on one instance of `SortInDisk can:\n * **Maintain** the temporal dir: This not delete temporal files to use other times in the future. If we want clear \n    data in the future, we will be able to delete the temporal folder from disk or use `sorted_in_disk` other time \n    and change the `only_one_read` to `True` or use `clear` method by hand. \n    To enable maintain the temporal dir if `sorted_in_disk` have arg `only_one_read` to `False`.\n * **Delete** the temporal dir: delete all temporal data, clean state.\n    This is enable if `sorted_in_disk` have arg `only_one_read` to `True`.\n\n\n## Performance\n\n### Hardware\n`sorted_in_disk` take advantage of you Hardware, then this is more quickly if it is well configured and run in best\nhardware.\n\nYou need to know:\n* **RAM memory**: it is used to accelerate injection, multiprocess queues and pre-sorted, when condition of variables \n`count_insert_to_check` and `queue_max_size` are met, then will dump to disk to liberate RAM memory. In conclusion,\nif you have more RAM memory you can configure more `count_insert_to_check` and `queue_max_size` to work more in \nfaster RAM memory. Example:\n```python\nsid = sorted_in_disk(...,\n                     count_insert_to_check=1000000,\n                     max_write_process_size=1024 * 1024 * 1024)\n```\n\n* **Processes**: To inject/write is a good idea divide work in processes one per processor \n(with `write_processes` variable) if it is possible to increase the velocity of sorted in disk. But, you need to\ncalibrate the best number of processes in your system if you want to reach the max performance; because, maybe more\nprocesses are no more quickly if you have RAM memory or disk limitations. To read, if `read_process=True` you\nhave a dedicated precess to read while you consume data and have a buffer of prepared sorted data.\n\n* **Disk**: It is better work in SSD disk than HHD disk to prevent a bottleneck of processes. To increase the \ndisk performance you can read data from different disk (depend on `iterable` read data) where the sorted is \ntaking place. Configure main path with `tmp_dir` (or/and define one list with paths to other disks with \n`write_processes`). Example:\n```python\nsid = sorted_in_disk(...,\n                     tmp_dir=\"/path/to/tmp_dir\")\n```\n\nTo start I recommend you test first with mono-process (`write_processes=0`) in injection and read\n(`read_process=False`).\n```python\nsid = sorted_in_disk(...,\n                     write_processes=0)\n```\n\nAfter, you can test with multiprocess (example for 4 inject processes `write_processes=4`,\nor create one per physical processor with `write_processes=None`). If you think velocity of disk is enough,\nyou can enable multiprocess and test with different configurations. If disk velocity is not enough then is \nquickly and easy use mono-process.\n```python\nsid = sorted_in_disk(...,\n                     write_processes=4)\n```\n\nIf you think your bottleneck is disk, and you want to enable multiprocess, then you can try to pass a list with \npaths to `write_processes`. Each element of this list create a process than save data in each assigned path \n(`tmp_dir` continue saving a small file with state, but not bulk of data). For example, you have tree disks, \n\"disk_HDD1\" and \"disk_HDD2\" are HDD and \"disk_SSD\" is a SSD, then you can define one process per HDD disk and two \nprocess to SSD because it is quickly (in total 4 injection process; similar to `write_processes=4` but each\nprocess have one path defined to save data):\n```python\nsid = sorted_in_disk(...,\n                     tmp_dir=\"/path/to/tmp_dir\",\n                     write_processes = [\"disk_HDD1/path/tmp_HDD1\", \n                                        \"disk_SSD/path/tmp_SSD\", \n                                        \"disk_SSD/path/tmp_SSD\", \n                                        \"disk_HDD2/path/tmp_HDD2\"])\n```\n\nTo read, if you need to perform hard operations with each returned data, it is better to enable multi-process in read.\n```python\nsid = sorted_in_disk(...,\n                     read_process=True)\n```\n\nIn other case, it is quick enough mono-process.\n```python\nsid = sorted_in_disk(...,\n                     read_process=False)\n```\n\nMore advanced performance settings in `sorted_in_disk` if you use multi-process, then you can configure the queue \nto read or write (both queues are independents). \n\nFor example to precise fit configuration of write-queue (if `write_processes=0` write-queue not work):\n```python\nsid = sorted_in_disk(...,\n                     write_processes=None,  # Write multiprocess enabled if this is different to 0 (None, 1, 2, 3, etc...)\n                     queue_max_size=1000,\n                     size_bucket_list=None,\n                     min_size_bucket_list=10,\n                     max_size_bucket_list=None)\n```\n\nOr an example to precise fit configuration of read-queue (if `read_process=False` read-queue not work):\n```python\nsid = sorted_in_disk(...,\n                     read_process=True,  # Read multiprocess enabled if this is True\n                     iter_m_queue_max_size=1000,\n                     iter_min_size_bucket_list=10,\n                     iter_max_size_bucket_list=None)\n```\n\n### Reuse pre-sorted work\nYou can use many times one sorted work from disk (if `only_one_read=False`), but this is not a data base. Example:\n```python\nsid = sorted_in_disk(...,\n                     only_one_read=False)\n```\n\nWhen data is in disk have a minimum sorted work, but it is not finally sort. When you read data perform complete sort \nin real time (to have sorted data as soon as posible). Due to, if you want use several times sorted work, maybe is good\nidea save result to file and read from this one (if you want to take advantage of same read iteration, with Python \ngenerators in streaming configuration you can save data to disk while you use data at same time).\n\n\n### Performance test\nHardware where the tests have been done:\n * Processor: Intel i5 3.2GHz 4-core\n * Operating System: Windows 10 x64\n * RAM Memory: 8 GB\n\nUse different configurations in `python3 tests\\complex_example.py`\n\nInject N elements in `sorted_in_disk`.\n\n\n#### 6,360,077 elements (1 GB of data) <All data sorted in same HHD disk using 1,16 GB>\n\n**Mono-Process**: main process (injection in same main process):\n```\nsorted_in_disk write and pre-sort: 0:01:41.586510\n```\nExample to instance this execution:\n```python\nsid = sorted_in_disk(iterable_1gb,\n                     key=lambda line: line.split(\"|\")[2],\n                     write_processes=0)\n```\n\n**+1 Process**: main process and 1 process for injection \n(time: Mono-Process = 1 Process x 3.3 faster):\n```\nsorted_in_disk write and pre-sort: 0:00:30.138000\n```\nExample to instance this execution:\n```python\nsid = sorted_in_disk(iterable_1gb,\n                     key=lambda line: line.split(\"|\")[2],\n                     write_processes=1)\n```\n\n**+4 Processes**: main process and 4 processes for injection \n(time: Mono-Process = 4 Processes x 3.5 faster):\n```\nsorted_in_disk write and pre-sort: 0:00:28.697512\n```\n```python\nsid = sorted_in_disk(iterable_1gb,\n                     key=lambda line: line.split(\"|\")[2],\n                     write_processes=None)\n```\n\n\n#### 314,610,000 elements (50 GB of data) <All data sorted in same HHD disk using 66 GB>\n\n**Mono-Process**: main process (injection in same main process; max 2 GB RAM memory consumption):\n```\nsorted_in_disk write and pre-sort: 2:01:26.222081\n```\nExample to instance this execution:\n```python\nsid = sorted_in_disk(iterable_50gb,\n                     key=lambda line: line.split(\"|\")[2],\n                     write_processes=0)\n```\n\n**+4 Processes**: main process and 4 processes for injection (max 5 GB RAM memory consumption)\n(time: Mono-Process = 4 Processes x 2 faster):\n```\nsorted_in_disk write and pre-sort: 1:16:38.016659\n```\n```python\nsid = sorted_in_disk(iterable_50gb,\n                     key=lambda line: line.split(\"|\")[2],\n                     write_processes=None)\n```\n\n### Readed\n**Reminder**: `sorted_in_disk` write and do pre-sort operations while consume all data, after read in realtime when data \nis consumed by third code. In the following individual performance tests it is only indicated \"write and pre-sort\" \nheavy work require consume all data and block main process until all data will inject.\n\nIn all cases, **read sorted data in realtime** (reading in same main process from all files): \n```\n100,000 elements each 4 seconds approximately\n```\n\nTo configure read in same process (with block while process of sort improved):\n```python\nsid = sorted_in_disk(iterable,\n                     ...\n                     read_process=False)\n```\n\nTo configure read in different process (without block while process of sort improved):\n```python\nsid = sorted_in_disk(iterable,\n                     ...\n                     read_process=True)\n```\n\n\n## Documentation\nThis is a review of class and functions content inside.\n\n**You have complete docstring documentation in code and more examples/tests in doctest format.**\n\n\n### Function:\n * `sorted_in_disk`: Main method to create a SortedInDisk object configured\n\nHelpers methods public to take advantage of this package (those are not main package use): \n * `create_tmp_folder`: Helper to create a temporal folder\n * `delete_tmp_folder`: Delete temporal files created\n\n\n#### sorted_in_disk args\nArgs to configure **common sorted args**:\n * `iterable`: iterable to sort in disk.\n        This iterable should by a generator for big data due to RAM memory limitation.\n * `key`: key specifies a function of one argument that is used to extract a comparison key from each element in\n        iterable (for example, `key=str.lower` or `key=lambda e: e.split(\",\")[3]`).\n        The default value is None (compare the elements directly).\n * `value`: value specifies a function of one argument that is used to extract a value from each element in\n            iterable (for example, `key=lambda e: e.split(\",\")[1:]`).\n * `reverse`: reverse is a boolean value.\n\nArgs to configure **temporal directories**:\n        If set to `True`, then the list elements sorted as if each comparison were reversed.\n * `tmp_dir`: Path to dir where save temporal files. If None this creates a folder and overwrite if\n        exist previously. By default: create a sortInDiskTmps folder in the current directory.\n * `ensure_different_dirs`: True to add incremental counter to folder if exists previously.\n        Useful if you use several instances of SortedInDisk at same time.\n        Note: conflict if `append` is `True`, because this creates a new name and not delete\n        previously file (example if True: if exist `/path/folder/` it create a new `/paht/folder(1)/`). \n        By default: `False`\n * `append`: True to clean folder tmp_dir if existe previously. By default: `False`\n * `only_one_read`: True to clean folder tmp_dir when you consume all data. If it is True only works if you read all \n        returned data, if you not read all, then you need to clear instance to auto. By default: `True`\n\nArgs to configure **write/injection**:\n * `count_insert_to_check`: counter to check if process have more size in memory than max_write_process_size.\n        By default: `1000000`\n * `max_write_process_size`: max size in bytes to dump cache memory values to disk\n        (only execute when `count_insert_to_check` is reached). If None, then not import psutil and then only\n        check with `count_insert_to_check`. By default: `1024*1024*1024`   # 1Gib\n * `ensure_space`: True to ensure disk space but is slowly. If not space then process launch warning message\n           and wait for space. If False, then get and IOException if not enough space. By defatul: `False`\n * `write_processes`: number of process to execute. If None then it is number of CPUs. If you pass one list \n                     with paths pointing to folders, then each path implements one process (each process save data in \n                     its own path; you can use one path to several processes if you define same path several times in \n                     the list) and these paths are managed by `sorted_in_disk` (`tmp_dir` continues to be used for \n                     save general state information). By default: `0`\n * `queue_max_size`: (only if `write_processes!=0`) max number of elements in queue. If None then is the max by default.\n        By default: `1000`\n * `size_bucket_list`: None to enable sensor size bucket list (require `maxsize>0`). If a number is defined\n                              here then use this number to size_bucket_list and disable sensor. If `maxsize<=0`\n                              and `size_bucket_list==None` then size_bucket_list is default to `1000`; other wise,\n                              if maxsize<=0 and size_bucket_list is defined, then use this number. By default: `None`\n * `min_size_bucket_list`: (only if sensor is enabled) min size bucket list.\n                                  `Min == 1` and `max == max_size_bucket_list - 1`. By default: `10`\n * `max_size_bucket_list`: (only if sensor is enabled) max size bucket list. If `None` is infinite.\n                                  By defatult: `None`\nArgs to configure **read**:\n * `read_process`: `True` to get and prepare data in other process, `False` to use this one.\n        By default: `False`\n * `iter_m_queue_max_size`: (only if `enable_multiprocessing` is `True`) max number of elements in queue. If None\n        then is the max by default. By default: `1000`\n * `iter_min_size_bucket_list`: (only if sensor is enabled) min size bucket list.\n                         `Min == 1` and `max == iter_max_size_bucket_list` - 1. By default: `10`\n * `iter_max_size_bucket_list`: (only if sensor is enabled) max size bucket list. If `None` is infinite.\n                                 By default: `None`\nArgs to debug:\n * `logging_level`: Level of log. Only to debug or to remove psutil warning. By default: `logging.WARNING`\n\n### Class:\n * `SortedInDisk`: Instance an object to work with data in a specific temporal folder.\n    * `save_and_sort`: Choose `save_and_sort_multiprocess` of `save_and_sort_mono` depend on `write_processes`\n    * `__iter__`: Sorted iterable of lines (same as `values` method).\n    * `__len__`: Get number of elements in this structure.\n    * `items`: Get a sorted iterable from disk to return sorted tuples of key and line, in each petition this get \n               one sorted\n    * `values`: Get a sorted iterable from disk to return sorted lines, in each petition this get one sorted line.\n    * `keys`: Get a sorted iterable from disk to return sorted keys of lines, in each petition this get one sorted key.\n    * `join_multiprocess`: Wait to end of all processes (only it is important if multiprocess injection is enable).\n    * `clear`: Clear file and delete temporal files\n    * `visor`: Visor of information in state file.\n    * Other methods invoked in previous methods (public for package extension proposals): \n        * `delete_tmp`: Delete temporal files created (use `clear` to use instance state)\n        * `get_dict_saved_info`: Get dict with general information. If not exist, create a new empty.\n        * `set_dict_saved_info`: Save in disk a new dict with general information.\n        * `save_and_sort_multiprocess`: Consume an iterable to be sorted in multiprocess way. Take analysis in this \n                                        iterable and save to disk (in temporal files).\n        * `save_and_sort_mono`: Consume an iterable to be sorted. Take analysis in this iterable and save to disk \n                                (in temporal files). Mono-thread, this one execute in the current thread.\n\n### Utils functions:\nSome tools to make work easier to read a file from disk to use `sorted_in_disk` and others.\n * `write_iter_in_file`: Write a iterable as text line in file\n * `read_iter_from_file`: Read a iterable where each element is a text line in file\n * `human_size`: Return a human size readable from bytes\n\n#### How to read a file and sort quickly\nYou have a file similar to this content, and you want to sort with \"keyN\":\n```\nvalA,key3,valB\nvalC,key1,valD\nvalE,key2,valF\n```\n\nYou can inject use util `read_iter_from_file` in this way:\n```python\nfrom sorted_in_disk.utils import read_iter_from_file\n\nsid = sorted_in_disk(read_iter_from_file(\"path/to/file/to/read\"),\n                     key=lambda line: line.split(\",\")[1])\n```\n\nAnd to write sorted content in a different file:\n```python\nfrom sorted_in_disk.utils import write_iter_in_file\n\ncount = write_iter_in_file(\"path/to/file/to/write\", sid)\n\nprint(\"Total sorted lines: {}\".format(count))\n```\n\n\n\n## Limitations\nNot mix mono-process and multiprocess configuration in same temporal file (only if you use `only_one_read=False` \nand `append=True`). Each mode (mono-process or multiprocess) work similar but save data in different ways to improve\nperformance of each thread.\n\n\n## About this algorithm\nThis is an own algorithm create with my own experience in big data. This is invented by me and my experience, I do not \ninvestigated thirds or used any others to create this one. In addition, I use QuickQueue for Python than I invented and\ndeveloped too.\n\n\n## Is useful for you?\nMaybe you would be so kind to consider the amount of hours puts in, the great effort and the resources expended in \ndoing this project. Thank you.\n\n[![paypal](https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=PWRRXZ2HETVG8&source=url)\n\n\n## Contact\nContact me via: r.invarato@gmail.com\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Invarato/sort_in_disk_project",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "sorted-in-disk",
    "package_url": "https://pypi.org/project/sorted-in-disk/",
    "platform": "",
    "project_url": "https://pypi.org/project/sorted-in-disk/",
    "project_urls": {
      "Homepage": "https://github.com/Invarato/sort_in_disk_project"
    },
    "release_url": "https://pypi.org/project/sorted-in-disk/1.0.3/",
    "requires_dist": [
      "easy-binary-file (>=1.0.4)",
      "quick-queue (>=1.0.5)"
    ],
    "requires_python": ">=3.0",
    "summary": "Sort a bulk of data in disk and parallel (RAM memory free)",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8037721,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "50dbabf9b244495e4e44396dfb64257f918f95bfe008ab4d4bb61e1211348d42",
          "md5": "353a9bf3e548e18cec64e3b8bc1c7830",
          "sha256": "21ea9aee38137ba90996521cf0a80eb7a317fed743624be773b40a50fc0b8328"
        },
        "downloads": -1,
        "filename": "sorted_in_disk-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "353a9bf3e548e18cec64e3b8bc1c7830",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.0",
        "size": 17613,
        "upload_time": "2020-08-06T19:26:49",
        "upload_time_iso_8601": "2020-08-06T19:26:49.975983Z",
        "url": "https://files.pythonhosted.org/packages/50/db/abf9b244495e4e44396dfb64257f918f95bfe008ab4d4bb61e1211348d42/sorted_in_disk-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "44d98f49a83e7168206df46b79afe833f2e4d1e78b9afc7181ba919a768bf59a",
          "md5": "c13cc389855e30728793151672a346a2",
          "sha256": "febeb0e39ab1f0cff8b0dd3d343e0eb83d94d2f4bfc2fc927fec0c8b9c3d2847"
        },
        "downloads": -1,
        "filename": "sorted-in-disk-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c13cc389855e30728793151672a346a2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.0",
        "size": 21136,
        "upload_time": "2020-08-06T19:26:52",
        "upload_time_iso_8601": "2020-08-06T19:26:52.330871Z",
        "url": "https://files.pythonhosted.org/packages/44/d9/8f49a83e7168206df46b79afe833f2e4d1e78b9afc7181ba919a768bf59a/sorted-in-disk-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "94abb799dc486ce2c7371e487181b0996e13639073d810158b0e821964ec0c18",
          "md5": "fd281495897687ed225b331c4cb17662",
          "sha256": "0fda0c0bcb3a9f1a0ea6c45f3315df9efda71bea718f04ba63042d0368b74916"
        },
        "downloads": -1,
        "filename": "sorted_in_disk-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fd281495897687ed225b331c4cb17662",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.0",
        "size": 20359,
        "upload_time": "2020-08-18T15:39:17",
        "upload_time_iso_8601": "2020-08-18T15:39:17.658567Z",
        "url": "https://files.pythonhosted.org/packages/94/ab/b799dc486ce2c7371e487181b0996e13639073d810158b0e821964ec0c18/sorted_in_disk-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63a2935eb83ff2de75fb702cb3d2c3fb74b52c62ea5beb72261390ed2ad7b6f1",
          "md5": "bd4b024c609240b91e922c039bb72cb0",
          "sha256": "29d110a31ca144bd64b78a9b2404a073199c55b0dee117ae94b4a42d32fa0343"
        },
        "downloads": -1,
        "filename": "sorted-in-disk-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "bd4b024c609240b91e922c039bb72cb0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.0",
        "size": 26176,
        "upload_time": "2020-08-18T15:39:19",
        "upload_time_iso_8601": "2020-08-18T15:39:19.339343Z",
        "url": "https://files.pythonhosted.org/packages/63/a2/935eb83ff2de75fb702cb3d2c3fb74b52c62ea5beb72261390ed2ad7b6f1/sorted-in-disk-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2c3d2dd6e901873607c326915e147be77f7737a8b95156e6450fccccdc36fd9a",
          "md5": "4a6e2f210317bad0552b9a436f5c2b3d",
          "sha256": "e1bc2efc61c625d0995771acff2d11b3f8cc38fe141853579227dc493a025810"
        },
        "downloads": -1,
        "filename": "sorted_in_disk-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4a6e2f210317bad0552b9a436f5c2b3d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.0",
        "size": 23344,
        "upload_time": "2020-08-25T18:28:12",
        "upload_time_iso_8601": "2020-08-25T18:28:12.732556Z",
        "url": "https://files.pythonhosted.org/packages/2c/3d/2dd6e901873607c326915e147be77f7737a8b95156e6450fccccdc36fd9a/sorted_in_disk-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "236cfff482b633a221d873f9f9af4e3da2c0edbf31a4c582e8f59c77bf69df82",
          "md5": "10c08e8e817ddbcb0cbfa19e7a45f520",
          "sha256": "0d87fbe2700d1b0c610c48ca0bca8d6e647b93ed97152805c369b01015d3a04d"
        },
        "downloads": -1,
        "filename": "sorted-in-disk-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "10c08e8e817ddbcb0cbfa19e7a45f520",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.0",
        "size": 30782,
        "upload_time": "2020-08-25T18:28:14",
        "upload_time_iso_8601": "2020-08-25T18:28:14.102790Z",
        "url": "https://files.pythonhosted.org/packages/23/6c/fff482b633a221d873f9f9af4e3da2c0edbf31a4c582e8f59c77bf69df82/sorted-in-disk-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "01b8fe54a5c90896b4c24fcb95bb510526c04ebaddfff1b487a36f38891509a1",
          "md5": "06c1c5d99e3934768cf17ca5faadb38d",
          "sha256": "7b6a7e73029798f754b2d366fdbf9672447ac3b252daafb1eb570bad9fcd5f83"
        },
        "downloads": -1,
        "filename": "sorted_in_disk-1.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "06c1c5d99e3934768cf17ca5faadb38d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.0",
        "size": 23503,
        "upload_time": "2020-08-25T18:54:13",
        "upload_time_iso_8601": "2020-08-25T18:54:13.536736Z",
        "url": "https://files.pythonhosted.org/packages/01/b8/fe54a5c90896b4c24fcb95bb510526c04ebaddfff1b487a36f38891509a1/sorted_in_disk-1.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "69efe9d139ada7db63e885daf627900e60775d66989953ac1715be0b7c586455",
          "md5": "ebe923106c61098b81fae0204df322d8",
          "sha256": "7b81a6c6f4fddc2efe4c2c3bb1eaf3259e71fcfb12895b4ca2fb7345ea20303a"
        },
        "downloads": -1,
        "filename": "sorted-in-disk-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "ebe923106c61098b81fae0204df322d8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.0",
        "size": 31510,
        "upload_time": "2020-08-25T18:54:15",
        "upload_time_iso_8601": "2020-08-25T18:54:15.122101Z",
        "url": "https://files.pythonhosted.org/packages/69/ef/e9d139ada7db63e885daf627900e60775d66989953ac1715be0b7c586455/sorted-in-disk-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "01b8fe54a5c90896b4c24fcb95bb510526c04ebaddfff1b487a36f38891509a1",
        "md5": "06c1c5d99e3934768cf17ca5faadb38d",
        "sha256": "7b6a7e73029798f754b2d366fdbf9672447ac3b252daafb1eb570bad9fcd5f83"
      },
      "downloads": -1,
      "filename": "sorted_in_disk-1.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "06c1c5d99e3934768cf17ca5faadb38d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.0",
      "size": 23503,
      "upload_time": "2020-08-25T18:54:13",
      "upload_time_iso_8601": "2020-08-25T18:54:13.536736Z",
      "url": "https://files.pythonhosted.org/packages/01/b8/fe54a5c90896b4c24fcb95bb510526c04ebaddfff1b487a36f38891509a1/sorted_in_disk-1.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "69efe9d139ada7db63e885daf627900e60775d66989953ac1715be0b7c586455",
        "md5": "ebe923106c61098b81fae0204df322d8",
        "sha256": "7b81a6c6f4fddc2efe4c2c3bb1eaf3259e71fcfb12895b4ca2fb7345ea20303a"
      },
      "downloads": -1,
      "filename": "sorted-in-disk-1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "ebe923106c61098b81fae0204df322d8",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.0",
      "size": 31510,
      "upload_time": "2020-08-25T18:54:15",
      "upload_time_iso_8601": "2020-08-25T18:54:15.122101Z",
      "url": "https://files.pythonhosted.org/packages/69/ef/e9d139ada7db63e885daf627900e60775d66989953ac1715be0b7c586455/sorted-in-disk-1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}