{
  "info": {
    "author": "Lux",
    "author_email": "1223411083@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "﻿## 仅需一行代码写爬虫--simple_crawl\n\n---\n\n### simple_crawl\n- 仅需一行代码即可达到爬虫效果\n- 项目地址(欢迎star):[https://github.com/Amiee-well/crawl](https://github.com/Amiee-well/crawl)\n\n### 使用方法\n**pip install simple_crawl**\n```python\nfrom simple_crawl import request\nrequest.parse(\n\turl=['https://www.douban.com/group/explore?start={}'.format(i) for i in range(0,180,30)],\n        #login=\"taobao\",\n        type_url = \"text\",\n        Parsing = 'xpath',\n        label = {\n            'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n            'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n            'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n            },\n        write='result.txt',\n        next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n        page=[True,\"now_url.txt\"],\n        #clean=True,\n        texting=True,\n        status=\"aiohttp\",\n        Thread_num=3,\n        cpu_count=1,\n        sem=5,\n        write_SQL={\n            'host':'localhost',\n            'post':'3306',\n            'user':'root',\n            'password':'123456',\n            'db':'example',\n            'table':'example'\n            }\n).run()\n```\n---\n介绍一下crawl参数设置：\n\n```python\n'''\n单行代码爬虫程序执行\n:param status:启用爬虫类型,支持普通爬虫、多进程爬虫、多线程爬虫、异步爬虫、异步多进程爬虫,参数请参考文档\n:param url:即将请求的url地址,仅支持get请求\n:param type_url:请求url后返回格式,支持text和json格式返回\n:param Thread_num:即将启动多线程个数,默认为1单线程\n:param sem:协程信号量,控制协程数,防止爬的过快,默认为5\n:param cpu_count:运行爬虫使用多进程cpu核数,默认为系统核数一半\n:param login:模拟网站登陆,保存登陆信息\n:param Parsing:爬虫方式,支持re、xpath以及bs4方法\n:param texting:是否启用连续爬虫,爬虫程序异常报错后重新启动爬虫,\n                       多次报错结束程序,默认否\n:param label:选择器内容,字典格式保存,\n                     字典值为列表格式,第一个参数为选择器,第二个参数为转换类型\n                     第一个参数必填,第二个参数默认str类型\n:param write:是否写入文件,支持txt格式、csv格式、json格式以及pkl格式,默认否\n:param next_url:是否跨页爬虫,选择器内容使爬虫继续翻页爬虫\n:param page:是否选择断续笔记接手下次爬虫处理,默认否\n:param clean:是否进行简单类型数据清洗,默认否\n:param write_sql:是否写入数据库,默认否\n                         'host'默认为'localhost','post'默认'3306','user'默认'root',\n                         'password':'密码','db':'数据库','table':'数据表',\n                         检测库是否存在,若不存在则创建,若存在则直接插入,\n                         检测表是否存在,若不存在则创建,若存在则直接插入\n:return True\n'''\n```\n---\n## 介绍玩法\n接下来介绍的均为调用第三方库的情况下运行：\n\n```python\nfrom simple_crawl import request\n```\n\n---\n### 第一种玩法：输出源代码\n调用requests库进行源代码请求。\n\n特点：\n请求失败则调用ip池处理重新请求访问，\n出现五次失败默认网址输入错误。\n支持text以及json字符串返回。默认text。\n\n缺点：\n暂时只能进行get请求，不支持post访问\n\n：return text or json\n```python\nrequest.parse(\n\turl = \"https://www.douban.com/group/explore\",\n\ttype_url = \"text\"\n).run()\n# return text\n```\n---\n### 第二种玩法：模拟网站登陆并保存信息\n调用DecryptLogin库请求登陆访问。\n\n> ps：DecryptLogin库为大佬皮卡丘写的一个很不错的模拟网站登陆库，在此引用一下，因为单行爬虫原因不支持账号密码登陆，我将大佬写的二维码登陆使用过来了。再次感谢大佬开源\n> 在此放出文档地址\n> DecryptLogin库中文文档：[https://httpsgithubcomcharlespikachudecryptlogin.readthedocs.io/zh/latest/](https://httpsgithubcomcharlespikachudecryptlogin.readthedocs.io/zh/latest/)\n> \n\n特点：\n将DecryptLogin库中二维码继承到此库（非二次开发）\n支持QQ群、QQ空间、QQ安全中心、淘宝、京东和斗鱼登陆（大佬登陆库中所有的二维码登陆）\n保存session.pkl信息到本地方便下次登陆运行\n\n缺点：\nsession.pkl登陆信息过时无法自动删除。\n导致下次登陆疑似cookie无法正常登陆。\n\n:return session\n\n```python\nrequest.parse(\n\t# 臭不要脸的推广一下我的店铺\n\turl=\"https://shop574805287.taobao.com/\",\n\tlogin=\"taobao\"\n).run()\n# return text\n```\n---\n### 第三种玩法：爬取网站信息\n爬虫库自然少不了爬虫的过程\n\n特点：\n支持re库，xpath解析以及bs4选择器。\n爬取方法为字典格式。单方面输出。\n字典键为保存的字段名称。\n字典值为列表格式：第一个参数为选择器，第二个参数为转换类型。第一个参数必填，第二个参数默认str类型。\n\n缺点：暂无（等待小伙伴们发现）\n\n：return reptile_results\n```python\nrequest.parse(\n\turl='https://www.douban.com/group/explore',\n\t# 字符串格式,选择器方法。\n    Parsing = 'xpath',\n    # 字典格式,参数如上。\n    label = {\n        'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n        'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n        'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n        }\n).run()\n# return reptile_results（list）\n```\n---\n### 第四种玩法：自由保存信息\n目前版本支持保存txt、csv、json以及pkl四大主流文件。日后版本更新将发布更为全面的保存方法。\n\n特点：\n写入文件均为数据格式传入文件。\n且输入格式规范方便阅读and省事。\n\n缺点：\n保存格式仅四种，\n不方便用户之后读写操作。\n\n：return file\n\n```python\nrequest.parse(\n\turl='https://www.douban.com/group/explore',\n    Parsing = 'xpath',\n    label = {\n        'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n        'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n        'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n        },\n    # 字符串格式,具体保存位置填写\n    write='result.pkl'\n).run()\n# return file\n```\n---\n### 第五种玩法：读取下一页 url 地址继续爬虫\n这也是每个人都担心的问题，仅仅一行代码怎么可能翻页爬虫。这还真能做到哦~\n\n特点：\n继承之前的Parsing参数选择器选择方法。\n在这里可读取到解析后的下一页 url 网址。\n方可继续进行爬虫处理。方便用户使用。\n\n缺点：\n若爬虫时下一页 url 地址改变，便结束爬虫。\n只能爬取所给 url 地址中的信息。\n无法进行某一界面的多个网页爬取返回。\n造成访问页面单一流失。\n\n：return None\n\n```python\nrequest.parse(\n\turl='https://www.douban.com/group/explore',\n    Parsing = 'xpath',\n    label = {\n        'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n        'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n        'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n        },\n    write='result.pkl',\n    # 字符串格式,根据Parsing方法继续请求下一页a中href\n    next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n).run()\n# return None \n```\n---\n### 第六种玩法：爬虫网页保存\n听说过爬虫断续重连的朋友应该懂得这是个什么神仙操作。每次爬虫运行期间好好的，睡一觉醒来发现代码报错了。。。这就是个很难受的事，还不知道之前爬取到哪一页了，只能重新爬虫了啊！\n\n特点：\n持续输出断续笔记。\n将此次爬虫的 url 地址保存到一个文本文档内部。\n下次读取文本文档即可快速进行直接断掉的爬虫 url 地址继续爬取所需。\n\n缺点：\n读取内容不单一。\n导致下次爬虫无法正确读取上次爬虫留下的痕迹。\n\n：return url_file\n\n```python\nrequest.parse(\n    url='https://www.douban.com/group/explore',\n    type_url='text',\n    #login='taobao',\n    Parsing = 'xpath',\n    label = {\n        'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n        'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n        'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n        },\n    write='result.pkl',\n    next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n    # 第一个参数为是否需要断续笔记。\n    # 第二个参数为断续笔记保存位置。\n    page=[True,'url_page.txt']\n).run()\n# return url_file\n```\n### 第七种玩法：简单数据清洗\n数据拿下后，直接保存到本地有些大佬可能觉得很辣鸡，连清洗都不清洗就存入本地了？那得拿到多少废数据脏数据。那么接下来介绍一下清洗参数。\n\n特点：\n本人曾写过一个底层数据清洗。\n能将列表格式数据进行归分清洗。\n主要内容请参考另一篇文章\n如下连接：[数据清洗](https://blog.csdn.net/qq_45414559/article/details/105907938)\n\n缺点：\n数据清洗格式简单。\n数据清洗内容单一。\n无法完全做到绝对清洗。\n有待改善。\n\n：return keyword_list, value_list\n\n```python\nrequest.parse(\n    url='https://www.douban.com/group/explore',\n    Parsing = 'xpath',\n    label = {\n        'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n        'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n        'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n        },\n    write='result.pkl',\n    next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n    page=[True,'url_page.txt'],\n    # bool类型,默认不清洗\n    clean=True\n).run()\n```\n---\n### 第八种玩法：爬虫存入数据库\n存到txt、存到csv、存到json、存到pkl，那也太low了吧。现在流行的数据库用不了么？那是不可能的。。\n\n特点：\n信息存入MySQL数据库。\n可连接docker远程数据库。\n数据库的库名可以不存在。\n数据库的表名可以不存在。\n根据之前传入字典键与值参数判断表类型。\n自由建立数据表传入信息。\n\n缺点：\n仅支持MySQL数据库。\n\n：return SQL\n\n```python\nrequest.parse(\n    url='https://www.douban.com/group/explore',\n    Parsing = 'xpath',\n    label = {\n        'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n        'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n        'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n        },\n    write='result.pkl',\n    next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n    page=[True,'url_page.txt'],\n    clean=True,\n    # 字典格式,\n    # host可有可无,默认localhost\n    # post可有可无,默认3306\n    # user可有可无,默认root\n    # password必要参数,数据库连接密码\n    # db必要参数,数据库即将存入的库名\n    # table必要参数,数据库即将存入的表名\n    write_SQL={\n        'host':'localhost',\n        'post':'3306',\n        'user':'root',\n        'password':'123456',\n        'db':'example',\n        'table':'example'\n        }\n    ).run()\n```\n---\n### 第九种玩法：重新启动爬虫\n爬虫最让人厌烦的就是被一个不痛不痒的错误信息给终止爬虫了，比如意外的远程断开链接等低级非代码错误，报错之后还得重新启动断续爬虫，就显得很麻烦。我做了一期爬虫程序异常报错后重新启动爬虫，多次报错结束程序。\n\n特点：\n检测报错重新启动爬虫\n无需手动处理错误信息\n\n缺点：\n无法收集子线程错误。\n\n：return None\n\n```python\nrequest.parse(\n\turl=['https://www.douban.com/group/explore?start={}'.format(i) for i in range(0,180,30)],\n        #login=\"taobao\",\n        type_url = \"text\",\n        Parsing = 'xpath',\n        label = {\n            'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n            'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n            'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n            },\n        write='result.txt',\n        next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n        page=[True,\"now_url.txt\"],\n        # texting 参数为是否启用连续爬虫\n        # 爬虫程序异常报错后重新启动爬虫\n        texting=True,\n        ###\n        write_SQL={\n            'host':'localhost',\n            'post':'3306',\n            'user':'root',\n            'password':'123456',\n            'db':'example',\n            'table':'example'\n            }\n).run()\n```\n---\n### 第十种玩法：多线程爬虫\n特点：\n爬虫速度加快，\n更好的利用了线程。\n\n缺点：暂无\n\n：return None\n\n```python\nrequest.parse(\n\turl=['https://www.douban.com/group/explore?start={}'.format(i) for i in range(0,180,30)],\n        #login=\"taobao\",\n        type_url = \"text\",\n        Parsing = 'xpath',\n        label = {\n            'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n            'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n            'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n            },\n        write='result.txt',\n        next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n        page=[True,\"now_url.txt\"],\n        #clean=True,\n        texting=True,\n        # status=\"threads\" 启用多线程爬虫\n        # Thread_num 为线程数目,默认为1 单线程\n        status=\"threads\",\n        Thread_num=3,\n        ###\n        write_SQL={\n            'host':'localhost',\n            'post':'3306',\n            'user':'root',\n            'password':'123456',\n            'db':'example',\n            'table':'example'\n            }\n).run()\n```\n\n---\n### 第十一种玩法：多进程爬虫\n特点：\n爬虫速度加快，\n更好的利用了进程。\n\n缺点：暂无\n\n：return None\n\n```python\nfrom simple_crawl import request\nrequest.parse(\n\turl=['https://www.douban.com/group/explore?start={}'.format(i) for i in range(0,180,30)],\n        #login=\"taobao\",\n        type_url = \"text\",\n        Parsing = 'xpath',\n        label = {\n            'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n            'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n            'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n            },\n        write='result.txt',\n        next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n        page=[True,\"now_url.txt\"],\n        #clean=True,\n        texting=True,\n        # status=\"multiprocessing\" 启用多进程爬虫\n        # cpu_count 为启动代码核心数,默认为系统核数一半\n        status=\"multiprocessing\",\n        cpu_count=2,\n        ###\n        write_SQL={\n            'host':'localhost',\n            'post':'3306',\n            'user':'root',\n            'password':'123456',\n            'db':'example',\n            'table':'example'\n            }\n).run()\n```\n\n---\n### 第十二种玩法：异步多线程爬虫\n特点：\n爬虫速度加快，\n异步使得爬虫无等待时间，\n同时使用多线程速度明显加快。\n\n缺点：暂无\n\n：return None\n\n```python\nfrom simple_crawl import request\nrequest.parse(\n\turl=['https://www.douban.com/group/explore?start={}'.format(i) for i in range(0,180,30)],\n        #login=\"taobao\",\n        type_url = \"text\",\n        Parsing = 'xpath',\n        label = {\n            'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n            'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n            'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n            },\n        write='result.txt',\n        next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n        page=[True,\"now_url.txt\"],\n        #clean=True,\n        texting=True,\n        # sem 参数为异步引擎数目,默认为5\n        # 其他参数同上\n        status=\"aiohttp\",\n        Thread_num=3,\n        sem=5,\n        ###\n        write_SQL={\n            'host':'localhost',\n            'post':'3306',\n            'user':'root',\n            'password':'123456',\n            'db':'example',\n            'table':'example'\n            }\n).run()\n```\n\n---\n### 第十三种玩法：异步多进程爬虫\n特点：\n爬虫速度加快，\n异步使得爬虫无等待时间，\n同时使用多进程速度明显加快。\n\n缺点：暂无\n\n：return None\n\n```python\nfrom simple_crawl import request\nrequest.parse(\n\turl=['https://www.douban.com/group/explore?start={}'.format(i) for i in range(0,180,30)],\n        #login=\"taobao\",\n        type_url = \"text\",\n        Parsing = 'xpath',\n        label = {\n            'url':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/@href',str],\n            'name':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/h3/a/text()',str],\n            'Author':['//*[@id=\"content\"]/div/div[1]/div[1]/div[1]/div[2]/div[2]/span[1]/a/text()',str]\n            },\n        write='result.txt',\n        next_url='//*[@id=\"content\"]/div/div[1]/div[2]/span[4]/a/@href',\n        page=[True,\"now_url.txt\"],\n        #clean=True,\n        texting=True,\n        # 参数如上\n        status=\"between\",\n        cpu_count=1,\n        sem=5,\n        ###\n        write_SQL={\n            'host':'localhost',\n            'post':'3306',\n            'user':'root',\n            'password':'123456',\n            'db':'example',\n            'table':'example'\n            }\n).run()\n\n```\n\n---\n### 功能介绍完毕🤞\n\n### 最后还是希望你们能给我点一波小小的关注。\n### 奉上自己诚挚的爱心💖\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Amiee-well/crawl",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "simple-crawl",
    "package_url": "https://pypi.org/project/simple-crawl/",
    "platform": "",
    "project_url": "https://pypi.org/project/simple-crawl/",
    "project_urls": {
      "Blog": "https://blog.csdn.net/qq_45414559/article/details/106005684",
      "Homepage": "https://github.com/Amiee-well/crawl"
    },
    "release_url": "https://pypi.org/project/simple-crawl/0.0.9/",
    "requires_dist": [
      "requests",
      "DecryptLogin",
      "cleancc",
      "lxml",
      "beautifulsoup4",
      "pymysql",
      "aiohttp",
      "asyncio"
    ],
    "requires_python": "",
    "summary": "Only need one line to crawl",
    "version": "0.0.9",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7291268,
  "releases": {
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cd7f3c3a5ca2ce78ab52d199de2606f6c329d3ba41ac20c7d3906b26605bc915",
          "md5": "dd247693254e8676c6aa4f004e9e86fd",
          "sha256": "2dbb8d5da1f5d2a06832b2033985e7260323c207498fdb865bc6e5d686e8972c"
        },
        "downloads": -1,
        "filename": "simple_crawl-0.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dd247693254e8676c6aa4f004e9e86fd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12990,
        "upload_time": "2020-05-09T13:24:15",
        "upload_time_iso_8601": "2020-05-09T13:24:15.716521Z",
        "url": "https://files.pythonhosted.org/packages/cd/7f/3c3a5ca2ce78ab52d199de2606f6c329d3ba41ac20c7d3906b26605bc915/simple_crawl-0.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d06798fad29672ac66a8df694276148909af3626ba49d27b6b33e95b561e4ac1",
          "md5": "c3359ca965d2867403129e4b45fb0316",
          "sha256": "3c4b6f10f29d1935995df1594392c8e37eaed1c345f842d4ed2c63e86a052275"
        },
        "downloads": -1,
        "filename": "simple_crawl-0.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "c3359ca965d2867403129e4b45fb0316",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14630,
        "upload_time": "2020-05-09T13:33:10",
        "upload_time_iso_8601": "2020-05-09T13:33:10.226351Z",
        "url": "https://files.pythonhosted.org/packages/d0/67/98fad29672ac66a8df694276148909af3626ba49d27b6b33e95b561e4ac1/simple_crawl-0.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "df958eeb801f1d56ee22a25729cac3a08c09a5e738b34968d601931103f9ce06",
          "md5": "e35ba92c4472c09a064a02174ca55291",
          "sha256": "20a013900af78f4b5476279c3e1ffad59597d75260dca49d40c8a28a74d1c6d1"
        },
        "downloads": -1,
        "filename": "simple_crawl-0.0.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e35ba92c4472c09a064a02174ca55291",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 29947,
        "upload_time": "2020-05-21T04:43:38",
        "upload_time_iso_8601": "2020-05-21T04:43:38.470077Z",
        "url": "https://files.pythonhosted.org/packages/df/95/8eeb801f1d56ee22a25729cac3a08c09a5e738b34968d601931103f9ce06/simple_crawl-0.0.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "71847739337b8c29661b59eed4521f4f73b2b4e7d5f2389e35bc330b7cf3366b",
          "md5": "87136688e7e9297623df962781fd678b",
          "sha256": "7c376ed7bc2e0a20a4ad3ec9ea585476a80bd12544d5f91c4fd175f32c87fd70"
        },
        "downloads": -1,
        "filename": "simple_crawl-0.0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "87136688e7e9297623df962781fd678b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 21374,
        "upload_time": "2020-05-21T04:43:46",
        "upload_time_iso_8601": "2020-05-21T04:43:46.101973Z",
        "url": "https://files.pythonhosted.org/packages/71/84/7739337b8c29661b59eed4521f4f73b2b4e7d5f2389e35bc330b7cf3366b/simple_crawl-0.0.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "df958eeb801f1d56ee22a25729cac3a08c09a5e738b34968d601931103f9ce06",
        "md5": "e35ba92c4472c09a064a02174ca55291",
        "sha256": "20a013900af78f4b5476279c3e1ffad59597d75260dca49d40c8a28a74d1c6d1"
      },
      "downloads": -1,
      "filename": "simple_crawl-0.0.9-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e35ba92c4472c09a064a02174ca55291",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 29947,
      "upload_time": "2020-05-21T04:43:38",
      "upload_time_iso_8601": "2020-05-21T04:43:38.470077Z",
      "url": "https://files.pythonhosted.org/packages/df/95/8eeb801f1d56ee22a25729cac3a08c09a5e738b34968d601931103f9ce06/simple_crawl-0.0.9-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "71847739337b8c29661b59eed4521f4f73b2b4e7d5f2389e35bc330b7cf3366b",
        "md5": "87136688e7e9297623df962781fd678b",
        "sha256": "7c376ed7bc2e0a20a4ad3ec9ea585476a80bd12544d5f91c4fd175f32c87fd70"
      },
      "downloads": -1,
      "filename": "simple_crawl-0.0.9.tar.gz",
      "has_sig": false,
      "md5_digest": "87136688e7e9297623df962781fd678b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 21374,
      "upload_time": "2020-05-21T04:43:46",
      "upload_time_iso_8601": "2020-05-21T04:43:46.101973Z",
      "url": "https://files.pythonhosted.org/packages/71/84/7739337b8c29661b59eed4521f4f73b2b4e7d5f2389e35bc330b7cf3366b/simple_crawl-0.0.9.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}