{
  "info": {
    "author": "Jérémie Gince",
    "author_email": "gincejeremie@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<!--- # <p align=\"center\"> AutoMLpy </p>) -->\n\n<p align=\"center\"> <img width=\"900\" height=\"400\" src=\"https://github.com/JeremieGince/AutoMLpy/blob/main/images/logo_001.png?raw=true\"> </p>\n\n---------------------------------------------------------------------------\n\nThis package is an automatic machine learning module whose function is to optimize the hyper-parameters \nof an automatic learning model. \n\nIn this package you can find: a grid search method, a random search algorithm and a Gaussian process search method. \nEverything is implemented to be compatible with the _Tensorflow_, _pyTorch_ and _sklearn_ libraries. \n\n\n# Installation\n\n## Latest stable version:\n```\npip install AutoMLpy\n```\n\n## Latest unstable version:\n0. Download the .whl file [here](https://github.com/JeremieGince/AutoMLpy/blob/main/dist/AutoMLpy-0.0.3-py3-none-any.whl);\n1. Copy the path of this file on your computer;\n2. pip install it with ``` pip install [path].whl ```\n\n\n ---------------------------------------------------------------------------\n# Example - MNIST optimization with Tensorflow & Keras\n\nHere you can see an example on how to optimize a model made with Tensorflow and Keras on the popular dataset MNIST.\n\n## Imports\n\nWe start by importing some useful stuff.\n\n\n```python\n# Some useful packages\nfrom typing import Union, Tuple\nimport time\nimport numpy as np\nimport pandas as pd\nimport pprint\n\n# Tensorflow\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\n# Importing the HPOptimizer and the RandomHpSearch from the AutoMLpy package.\nfrom AutoMLpy import HpOptimizer, RandomHpSearch\n\n```\n\n## Dataset\n\nNow we load the MNIST dataset in the tensorflow way.\n\n\n```python\ndef normalize_img(image, label):\n    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n    return tf.cast(image, tf.float32) / 255., label\n\ndef get_tf_mnist_dataset(**kwargs):\n    # https://www.tensorflow.org/datasets/keras_example\n    (ds_train, ds_test), ds_info = tfds.load(\n        'mnist',\n        split=['train', 'test'],\n        shuffle_files=True,\n        as_supervised=True,\n        with_info=True,\n    )\n\n    # Build training pipeline\n    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    ds_train = ds_train.cache()\n    ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n    ds_train = ds_train.batch(128)\n    ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n\n    # Build evaluation pipeline\n    ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    ds_test = ds_test.batch(128)\n    ds_test = ds_test.cache()\n    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n\n    return ds_train, ds_test\n```\n\n## Keras Model\n\nNow we make a function that return a keras model given a set of hyper-parameters (hp).\n\n\n```python\ndef get_tf_mnist_model(**hp):\n\n    if hp.get(\"use_conv\", False):\n        model = tf.keras.models.Sequential([\n            # Convolution layers\n            tf.keras.layers.Conv2D(10, 3, padding=\"same\", input_shape=(28, 28, 1)),\n            tf.keras.layers.MaxPool2D((2, 2)),\n            tf.keras.layers.Conv2D(50, 3, padding=\"same\"),\n            tf.keras.layers.MaxPool2D((2, 2)),\n\n            # Dense layers\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(120, activation='relu'),\n            tf.keras.layers.Dense(84, activation='relu'),\n            tf.keras.layers.Dense(10)\n        ])\n    else:\n        model = tf.keras.models.Sequential([\n            tf.keras.layers.Flatten(input_shape=(28, 28)),\n            tf.keras.layers.Dense(120, activation='relu'),\n            tf.keras.layers.Dense(84, activation='relu'),\n            tf.keras.layers.Dense(10)\n        ])\n\n    return model\n\n```\n\n## The Optimizer Model\n\nIt's time to implement the optimizer model. You just have to implement the following methods: \"build_model\",\n\"fit_dataset_model_\" and \"score_on_dataset\". Those methods must respect their signature and output type. The objective \nhere is to make the building, the training and the score phase depend on some hyper-parameters. So the optimizer can \nuse those to find the best set of hp.\n\n\n```python\nclass KerasMNISTHpOptimizer(HpOptimizer):\n    def build_model(self, **hp) -> tf.keras.Model:\n        model = get_tf_mnist_model(**hp)\n\n        model.compile(\n            optimizer=tf.keras.optimizers.SGD(\n                learning_rate=hp.get(\"learning_rate\", 1e-3),\n                nesterov=hp.get(\"nesterov\", True),\n                momentum=hp.get(\"momentum\", 0.99),\n            ),\n            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n            metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n        )\n        return model\n\n    def fit_dataset_model_(\n            self,\n            model: tf.keras.Model,\n            dataset,\n            **hp\n    ) -> tf.keras.Model:\n        history = model.fit(\n            dataset,\n            epochs=hp.get(\"epochs\", 1),\n            verbose=False,\n        )\n        return model\n\n    def score_on_dataset(\n            self,\n            model: tf.keras.Model,\n            dataset,\n            **hp\n    ) -> float:\n        test_loss, test_acc = model.evaluate(dataset, verbose=0)\n        return test_acc\n\n```\n\n## Execution & Optimization\n\nFirst thing after creating our classes is to load the dataset in memory.\n\n\n```python\nmnist_train, mnist_test = get_tf_mnist_dataset()\nmnist_hp_optimizer = KerasMNISTHpOptimizer()\n```\n\nAfter you will define your hyper-parameters space with a dictionary like this.\n\n\n```python\nhp_space = dict(\n    epochs=list(range(1, 16)),\n    learning_rate=np.linspace(1e-4, 1e-1, 50),\n    nesterov=[True, False],\n    momentum=np.linspace(0.01, 0.99, 50),\n    use_conv=[True, False],\n)\n```\n\nIt's time to define your hp search algorithm and give it your budget in time and iteration. Here we will test for \n10 minutes and 100 iterations maximum.\n\n\n```python\nparam_gen = RandomHpSearch(hp_space, max_seconds=60*10, max_itr=100)\n```\n\nFinally, you start the optimization by giving your parameter generator to the optimize method. Note that the \n\"stop_criterion\" argument is to stop the optimization when the given score is reached. It's really useful to save some \ntime.\n\n\n```python\nsave_kwargs = dict(\n    save_name=f\"tf_mnist_hp_opt\",\n    title=\"Random search: MNIST\",\n)\n\nparam_gen = mnist_hp_optimizer.optimize_on_dataset(\n    param_gen, mnist_train, save_kwargs=save_kwargs,\n    stop_criterion=1.0,\n)\n```\n\n\n## Testing\n\nNow, you can test the optimized hyper-parameters by fitting again with the full train dataset. Yes with the full \ndataset, because in the optimization phase a cross-validation is made which crop your train dataset by half. Plus, \nit's time to test the fitted model on the test dataset.\n\n\n```python\nopt_hp = param_gen.get_best_param()\n\nmodel = mnist_hp_optimizer.build_model(**opt_hp)\nmnist_hp_optimizer.fit_dataset_model_(\n    model, mnist_train, **opt_hp\n)\n\ntest_acc = mnist_hp_optimizer.score_on_dataset(\n    model, mnist_test, **opt_hp\n)\n\nprint(f\"test_acc: {test_acc*100:.3f}%\")\n```\n\n\nThe optimized hyper-parameters:\n\n\n```python\npp = pprint.PrettyPrinter(indent=4)\npp.pprint(opt_hp)\n```\n\n\n## Visualization\n\nYou can visualize the optimization with an interactive html file.\n\n\n```python\nfig = param_gen.write_optimization_to_html(show=True, dark_mode=True, **save_kwargs)\n```\n\n## Optimisation table\n```python\nopt_table = param_gen.get_optimization_table()\n```\n\n## Saving ParameterGenerator\n```python\nparam_gen.save_history(**save_kwargs)\nsave_path = param_gen.save_obj(**save_kwargs)\n```\n\n## Loading ParameterGenerator\n```python\nparam_gen = RandomHpSearch.load_obj(save_path)\n```\n\n## Re-lunch optimisation with loaded ParameterGenerator\n```python\n# Change the budget to be able to optimize again\nparam_gen.max_itr = param_gen.max_seconds + 100\nparam_gen.max_seconds = param_gen.max_seconds + 60\n\nparam_gen = mnist_hp_optimizer.optimize_on_dataset(\n    param_gen, mnist_train, save_kwargs=save_kwargs,\n    stop_criterion=1.0, reset_gen=False,\n)\n\nopt_hp = param_gen.get_best_param()\n\nprint(param_gen.get_optimization_table())\npp.pprint(param_gen.history)\npp.pprint(opt_hp)\n```\n\n ---------------------------------------------------------------------------\n # Other examples\n Examples on how to use this package are in the folder [./examples](https://github.com/JeremieGince/AutoMLpy/blob/main/examples). \n There you can find the previous example with [_Tensorflow_](https://github.com/JeremieGince/AutoMLpy/blob/main/examples/tensorflow_example.ipynb) \n and an example with [_pyTorch_](https://github.com/JeremieGince/AutoMLpy/blob/main/examples/pytorch_example.ipynb).\n\n\n\n# License\n[Apache License 2.0](LICENSE.md)\n\n# Citation\n```\n@article{Gince,\n  title={Implémentation du module AutoMLpy, un outil d’apprentissage machine automatique},\n  author={Jérémie Gince},\n  year={2021},\n  publisher={ULaval},\n  url={https://github.com/JeremieGince/AutoMLpy},\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/JeremieGince/AutoMLpy",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "AutoMLpy",
    "package_url": "https://pypi.org/project/AutoMLpy/",
    "platform": "",
    "project_url": "https://pypi.org/project/AutoMLpy/",
    "project_urls": {
      "Homepage": "https://github.com/JeremieGince/AutoMLpy"
    },
    "release_url": "https://pypi.org/project/AutoMLpy/0.0.31/",
    "requires_dist": [
      "wheel (>=0.36.2)",
      "tqdm (>=4.56.0)",
      "numpy (>=1.19.5)",
      "scikit-learn (>=0.24.1)",
      "scipy (>=1.6.0)",
      "plotly (>=4.14.3)",
      "pandas (>=1.2.1)",
      "dash (>=1.19.0)",
      "matplotlib (>=3.3.3)",
      "psutil (>=5.8.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "This package is an automatic machine learning module whose function is to optimize the hyper-parameters of an automatic learning model. Code at: https://github.com/JeremieGince/AutoMLpy .",
    "version": "0.0.31",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11468372,
  "releases": {
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4d828f6a0454b15773804ee4565cf9cfb7dac09cc44457a2fa3b631b9fe350d9",
          "md5": "2a32064a95b71989dbe665853547b7ab",
          "sha256": "b0f0c7bafcc54da1905254e94f706a2b17c80e2c80621b6618410acd972fe5a6"
        },
        "downloads": -1,
        "filename": "AutoMLpy-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2a32064a95b71989dbe665853547b7ab",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 25375,
        "upload_time": "2021-03-19T23:33:15",
        "upload_time_iso_8601": "2021-03-19T23:33:15.958546Z",
        "url": "https://files.pythonhosted.org/packages/4d/82/8f6a0454b15773804ee4565cf9cfb7dac09cc44457a2fa3b631b9fe350d9/AutoMLpy-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "84aa7424f154f7fb49a32ab304d15c0e9c84666b5cc30ce55f7351b9c73b726f",
          "md5": "33d965ad848c97c1e02b2b1be3094890",
          "sha256": "519dd1b312a51c0fcf63cca0ffbd94479bd8e2b5badeb30ee3b979de8d442f0b"
        },
        "downloads": -1,
        "filename": "AutoMLpy-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "33d965ad848c97c1e02b2b1be3094890",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 21372,
        "upload_time": "2021-03-19T23:33:17",
        "upload_time_iso_8601": "2021-03-19T23:33:17.349291Z",
        "url": "https://files.pythonhosted.org/packages/84/aa/7424f154f7fb49a32ab304d15c0e9c84666b5cc30ce55f7351b9c73b726f/AutoMLpy-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.31": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1be25480b1161b0f37c8b562f62b57fd620ad0e71575932b324bb0d0368090ee",
          "md5": "90c8e54171c31f48c0202c7cfa4baa9a",
          "sha256": "2d580547f15ca39faf8d7e7a3b22454955e8a058d1b52176757ccb2a3d3b1ea3"
        },
        "downloads": -1,
        "filename": "AutoMLpy-0.0.31-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "90c8e54171c31f48c0202c7cfa4baa9a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27870,
        "upload_time": "2021-09-16T14:14:03",
        "upload_time_iso_8601": "2021-09-16T14:14:03.566863Z",
        "url": "https://files.pythonhosted.org/packages/1b/e2/5480b1161b0f37c8b562f62b57fd620ad0e71575932b324bb0d0368090ee/AutoMLpy-0.0.31-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2ee2501e377f7efefe5fedd91cab15109906c2d57727194fbd15f4c44d8d08fc",
          "md5": "52f8de179336ddba5ea9126d98a781b7",
          "sha256": "b1bfe1e2ed1601040d117c56d5c9dd452c55f8eaed051add8b68490302fd4c95"
        },
        "downloads": -1,
        "filename": "AutoMLpy-0.0.31.tar.gz",
        "has_sig": false,
        "md5_digest": "52f8de179336ddba5ea9126d98a781b7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 23672,
        "upload_time": "2021-09-16T14:14:05",
        "upload_time_iso_8601": "2021-09-16T14:14:05.286305Z",
        "url": "https://files.pythonhosted.org/packages/2e/e2/501e377f7efefe5fedd91cab15109906c2d57727194fbd15f4c44d8d08fc/AutoMLpy-0.0.31.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1be25480b1161b0f37c8b562f62b57fd620ad0e71575932b324bb0d0368090ee",
        "md5": "90c8e54171c31f48c0202c7cfa4baa9a",
        "sha256": "2d580547f15ca39faf8d7e7a3b22454955e8a058d1b52176757ccb2a3d3b1ea3"
      },
      "downloads": -1,
      "filename": "AutoMLpy-0.0.31-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "90c8e54171c31f48c0202c7cfa4baa9a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 27870,
      "upload_time": "2021-09-16T14:14:03",
      "upload_time_iso_8601": "2021-09-16T14:14:03.566863Z",
      "url": "https://files.pythonhosted.org/packages/1b/e2/5480b1161b0f37c8b562f62b57fd620ad0e71575932b324bb0d0368090ee/AutoMLpy-0.0.31-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2ee2501e377f7efefe5fedd91cab15109906c2d57727194fbd15f4c44d8d08fc",
        "md5": "52f8de179336ddba5ea9126d98a781b7",
        "sha256": "b1bfe1e2ed1601040d117c56d5c9dd452c55f8eaed051add8b68490302fd4c95"
      },
      "downloads": -1,
      "filename": "AutoMLpy-0.0.31.tar.gz",
      "has_sig": false,
      "md5_digest": "52f8de179336ddba5ea9126d98a781b7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 23672,
      "upload_time": "2021-09-16T14:14:05",
      "upload_time_iso_8601": "2021-09-16T14:14:05.286305Z",
      "url": "https://files.pythonhosted.org/packages/2e/e2/501e377f7efefe5fedd91cab15109906c2d57727194fbd15f4c44d8d08fc/AutoMLpy-0.0.31.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}