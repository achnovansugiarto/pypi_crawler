{
  "info": {
    "author": "xu",
    "author_email": "charlesxu86@163.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "BERT for TensorFlow v2\n======================\n\nThis repo contains a `TensorFlow 2.0`_ `Keras`_ implementation of `google-research/bert`_\nwith support for loading of the original `pre-trained weights`_,\nand producing activations **numerically identical** to the one calculated by the original model.\n\n`ALBERT`_ and `adapter-BERT`_ are also supported by setting the corresponding\nconfiguration parameters (``shared_layer=True``, ``embedding_size`` for `ALBERT`_\nand ``adapter_size`` for `adapter-BERT`_). Setting both will result in an adapter-ALBERT\nby sharing the BERT parameters across all layers while adapting every layer with layer specific adapter.\n\nThe implementation is build from scratch using only basic tensorflow operations,\nfollowing the code in `google-research/bert/modeling.py`_\n(but skipping dead code and applying some simplifications). It also utilizes `kpe/params-flow`_ to reduce\ncommon Keras boilerplate code (related to passing model and layer configuration arguments).\n\n`bert4tf`_ should work with both `TensorFlow 2.0`_ and `TensorFlow 1.14`_ or newer.\n\nNEWS\n----\n\n\nLICENSE\n-------\n\nMIT. See `License File <https://github.com/kpe/bert-for-tf2/blob/master/LICENSE.txt>`_.\n\nInstall\n-------\n\n``bert4tf`` is on the Python Package Index (PyPI):\n\n::\n\n    pip install bert4tf\n\n\nUsage\n-----\n\nBERT in `bert4tf` is implemented as a Keras layer. You could instantiate it like this:\n\n.. code:: python\n\n  from bert4tf import BertModelLayer\n\n  l_bert = BertModelLayer(**BertModelLayer.Params(\n    vocab_size               = 16000,        # embedding params\n    use_token_type           = True,\n    use_position_embeddings  = True,\n    token_type_vocab_size    = 2,\n\n    num_layers               = 12,           # transformer encoder params\n    hidden_size              = 768,\n    hidden_dropout           = 0.1,\n    intermediate_size        = 4*768,\n    intermediate_activation  = \"gelu\",\n\n    adapter_size             = None,         # see arXiv:1902.00751 (adapter-BERT)\n\n    shared_layer             = False,        # True for ALBERT (arXiv:1909.11942)\n    embedding_size           = None,         # None for BERT, wordpiece embedding size for ALBERT\n\n    name                     = \"bert\"        # any other Keras layer params\n  ))\n\nor by using the ``bert_config.json`` from a `pre-trained google model`_:\n\n.. code:: python\n\n  import bert4tf\n\n  model_dir = \".models/uncased_L-12_H-768_A-12\"\n\n  bert_params = bert.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert4tf.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n\nnow you can use the BERT layer in your Keras model like this:\n\n.. code:: python\n\n  from tensorflow import keras\n\n  max_seq_len = 128\n  l_input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n  l_token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32')\n\n  # using the default token_type/segment id 0\n  output = l_bert(l_input_ids)                              # output: [batch_size, max_seq_len, hidden_size]\n  model = keras.Model(inputs=l_input_ids, outputs=output)\n  model.build(input_shape=(None, max_seq_len))\n\n  # provide a custom token_type/segment id as a layer input\n  output = l_bert([l_input_ids, l_token_type_ids])          # [batch_size, max_seq_len, hidden_size]\n  model = keras.Model(inputs=[l_input_ids, l_token_type_ids], outputs=output)\n  model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n\nif you choose to use `adapter-BERT`_ by setting the `adapter_size` parameter,\nyou would also like to freeze all the original BERT layers by calling:\n\n.. code:: python\n\n  l_bert.apply_adapter_freeze()\n\nand once the model has been build or compiled, the original pre-trained weights\ncan be loaded in the BERT layer:\n\n.. code:: python\n\n  import bert4tf\n\n  bert_ckpt_file   = os.path.join(model_dir, \"bert_model.ckpt\")\n  bert.load_stock_weights(l_bert, bert_ckpt_file)\n\n**N.B.** see `tests/test_bert_activations.py`_ for a complete example.\n\nFAQ\n---\n1. How to use BERT with the `google-research/bert`_ pre-trained weights?\n\n.. code:: python\n\n  model_name = \"uncased_L-12_H-768_A-12\"\n  model_dir = bert4tf.fetch_google_bert_model(model_name, \".models\")\n  model_ckpt = os.path.join(model_dir, \"bert_model.ckpt\")\n\n  bert_params = bert4tf.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert4tf.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n  # use in Keras Model here, and call model.build()\n\n  bert.load_bert_weights(l_bert, model_ckpt)      # should be called after model.build()\n\n2. How to use ALBERT with the `google-research/albert`_ pre-trained weights?\n\n.. code:: python\n\n  model_name = \"albert_base\"\n  model_dir    = bert4tf.fetch_tfhub_albert_model(model_name, \".models\")\n  model_params = bert4tf.albert_params(model_name)\n  l_bert = bert.BertModelLayer.from_params(model_params, name=\"albert\")\n\n  # use in Keras Model here, and call model.build()\n\n  bert.load_albert_weights(l_bert, albert_dir)      # should be called after model.build()\n\n3. How to use ALBERT with the `albert_zh` pre-trained weights?\n\nsee `tests/nonci/test_albert.py <https://github.com/kpe/bert-for-tf2/blob/master/tests/nonci/test_albert.py>`_:\n\n.. code:: python\n\n  model_name = \"albert_base\"\n  model_dir = bert4tf.fetch_brightmart_albert_model(model_name, \".models\")\n  model_ckpt = os.path.join(model_dir, \"albert_model.ckpt\")\n\n  bert_params = bert.params_from_pretrained_ckpt(model_dir)\n  l_bert = bert.BertModelLayer.from_params(bert_params, name=\"bert\")\n\n  # use in a Keras Model here, and call model.build()\n\n  bert.load_albert_weights(l_bert, model_ckpt)      # should be called after model.build()\n\n4. How to tokenize the input for the `google-research/bert`_ models?\n\n  do_lower_case = not (model_name.find(\"cased\") == 0 or model_name.find(\"multi_cased\") == 0)\n  bert.bert_tokenization.validate_case_matches_checkpoint(do_lower_case, model_ckpt)\n  vocab_file = os.path.join(model_dir, \"vocab.txt\")\n  tokenizer = bert4tf.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)\n  tokens = tokenizer.tokenize(\"Hello, BERT-World!\")\n  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n5. How to tokenize the input for the `google-research/albert`_ models?\n\n  import sentencepiece as spm\n\n  spm_model = os.path.join(model_dir, \"assets\", \"30k-clean.model\")\n  sp = spm.SentencePieceProcessor()\n  sp.load(spm_model)\n  do_lower_case = True\n\n  processed_text = bert.albert_tokenization.preprocess_text(\"Hello, World!\", lower=do_lower_case)\n  token_ids = bert4tf.albert_tokenization.encode_ids(sp, processed_text)\n\n\nReferrence\n---\n1. kpe\n\nResources\n---------\n\n- `BERT`_ - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n- `adapter-BERT`_ - adapter-BERT: Parameter-Efficient Transfer Learning for NLP\n- `ALBERT`_ - ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations\n- `google-research/bert`_ - the original `BERT`_ implementation\n- `google-research/albert`_ - the original `ALBERT`_ implementation by Google\n- `kpe/params-flow`_ - A Keras coding style for reducing `Keras`_ boilerplate code in custom layers by utilizing `kpe/py-params`_\n\n.. _`kpe/params-flow`: https://github.com/kpe/params-flow\n.. _`kpe/py-params`: https://github.com/kpe/py-params\n.. _`bert4tf`: https://github.com/charlesXu86/Bert4tf\n\n.. _`Keras`: https://keras.io\n.. _`pre-trained weights`: https://github.com/google-research/bert#pre-trained-models\n.. _`google-research/bert`: https://github.com/google-research/bert\n.. _`google-research/bert/modeling.py`: https://github.com/google-research/bert/blob/master/modeling.py\n.. _`BERT`: https://arxiv.org/abs/1810.04805\n.. _`pre-trained google model`: https://github.com/google-research/bert\n.. _`tests/test_bert_activations.py`: https://github.com/kpe/bert-for-tf2/blob/master/tests/test_compare_activations.py\n.. _`TensorFlow 2.0`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf\n.. _`TensorFlow 1.14`: https://www.tensorflow.org/versions/r1.14/api_docs/python/tf\n\n.. _`google-research/adapter-bert`: https://github.com/google-research/adapter-bert/\n.. _`adapter-BERT`: https://arxiv.org/abs/1902.00751\n.. _`ALBERT`: https://arxiv.org/abs/1909.11942\n.. _`google ALBERT weights`: https://github.com/google-research/google-research/tree/master/albert\n.. _`google-research/albert`: https://github.com/google-research/google-research/tree/master/albert\n.. _`TFHub/albert`: https://tfhub.dev/google/albert_base/1\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/charlesXu86/Bert4tf",
    "keywords": "bert4tf,tensorflow2",
    "license": "MIT Licence",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bert4tf",
    "package_url": "https://pypi.org/project/bert4tf/",
    "platform": "any",
    "project_url": "https://pypi.org/project/bert4tf/",
    "project_urls": {
      "Homepage": "https://github.com/charlesXu86/Bert4tf"
    },
    "release_url": "https://pypi.org/project/bert4tf/2.0.2/",
    "requires_dist": [
      "tensorflow"
    ],
    "requires_python": "",
    "summary": "bert for tensorflow2",
    "version": "2.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6360239,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b8f112fb62ae056b6d66b8718b785924201fcc6b870edd401d395b6e366b91ca",
          "md5": "ed9bf5e8b252c2c3993a6fc279204970",
          "sha256": "5b79d80afcc0f9c6f0a4a80f106605ecb811bac0485c6cb7216b1f7e0d660303"
        },
        "downloads": -1,
        "filename": "bert4tf-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ed9bf5e8b252c2c3993a6fc279204970",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 35268,
        "upload_time": "2019-11-03T07:41:56",
        "upload_time_iso_8601": "2019-11-03T07:41:56.832857Z",
        "url": "https://files.pythonhosted.org/packages/b8/f1/12fb62ae056b6d66b8718b785924201fcc6b870edd401d395b6e366b91ca/bert4tf-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7fbf84fb2d006309f365a279d88624c2f0e6aa7a09627ffba1aa07ffe29ea764",
          "md5": "fe33baabffd5d701324c1fb48be3a61d",
          "sha256": "2db83b38c49f76276bc3e9c6e5e62d1dd830acf3638511efd8491f2a527d99f0"
        },
        "downloads": -1,
        "filename": "bert4tf-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fe33baabffd5d701324c1fb48be3a61d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 36758,
        "upload_time": "2019-11-12T09:48:41",
        "upload_time_iso_8601": "2019-11-12T09:48:41.724833Z",
        "url": "https://files.pythonhosted.org/packages/7f/bf/84fb2d006309f365a279d88624c2f0e6aa7a09627ffba1aa07ffe29ea764/bert4tf-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4af3471a553a4c13a9519afba4b17fe0b7b2443d9d9a220eac33a1bacabfac80",
          "md5": "cf4827e11bad3b6f94e9d138da0f9b15",
          "sha256": "99820a0c4243eeb330882eed28f9de69b9b5058ee2f13d902a00303f1549d2b2"
        },
        "downloads": -1,
        "filename": "bert4tf-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cf4827e11bad3b6f94e9d138da0f9b15",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 36994,
        "upload_time": "2019-11-12T11:12:36",
        "upload_time_iso_8601": "2019-11-12T11:12:36.486628Z",
        "url": "https://files.pythonhosted.org/packages/4a/f3/471a553a4c13a9519afba4b17fe0b7b2443d9d9a220eac33a1bacabfac80/bert4tf-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5c26970de459cf79542a3daa196ed1486dda118e056247edfc62d5d4b052cca5",
          "md5": "5ec9c12220d0a21bd480c23be10bafad",
          "sha256": "aa8ad9737883ab63d5bc85f2c5de533b318a20a755b04a1061bf807b2b918bbc"
        },
        "downloads": -1,
        "filename": "bert4tf-1.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5ec9c12220d0a21bd480c23be10bafad",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 37004,
        "upload_time": "2019-11-13T03:12:30",
        "upload_time_iso_8601": "2019-11-13T03:12:30.132130Z",
        "url": "https://files.pythonhosted.org/packages/5c/26/970de459cf79542a3daa196ed1486dda118e056247edfc62d5d4b052cca5/bert4tf-1.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b6d0d54e390ca1d97bbde943e308e80e70a2ebe456e287fff545024bb45566bb",
          "md5": "32d9a266af52ac0564b600263d02a5d0",
          "sha256": "0c19ec08479fa0612535e0324592296139ebf0d9932eea9a549cf408ff146d11"
        },
        "downloads": -1,
        "filename": "bert4tf-2.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "32d9a266af52ac0564b600263d02a5d0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 38844,
        "upload_time": "2019-12-26T02:48:18",
        "upload_time_iso_8601": "2019-12-26T02:48:18.882699Z",
        "url": "https://files.pythonhosted.org/packages/b6/d0/d54e390ca1d97bbde943e308e80e70a2ebe456e287fff545024bb45566bb/bert4tf-2.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b6d0d54e390ca1d97bbde943e308e80e70a2ebe456e287fff545024bb45566bb",
        "md5": "32d9a266af52ac0564b600263d02a5d0",
        "sha256": "0c19ec08479fa0612535e0324592296139ebf0d9932eea9a549cf408ff146d11"
      },
      "downloads": -1,
      "filename": "bert4tf-2.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "32d9a266af52ac0564b600263d02a5d0",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 38844,
      "upload_time": "2019-12-26T02:48:18",
      "upload_time_iso_8601": "2019-12-26T02:48:18.882699Z",
      "url": "https://files.pythonhosted.org/packages/b6/d0/d54e390ca1d97bbde943e308e80e70a2ebe456e287fff545024bb45566bb/bert4tf-2.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}