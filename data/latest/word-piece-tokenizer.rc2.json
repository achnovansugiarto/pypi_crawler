{
  "info": {
    "author": "",
    "author_email": "Jing Hua <mail@dev.tjh.sg>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3"
    ],
    "description": "# A Lightweight Word Piece Tokenizer\n\n[![PyPI version shields.io](https://img.shields.io/pypi/v/word-piece-tokenizer.svg)](https://pypi.org/project/word-piece-tokenizer/)\n\nThis library is an implementation of a modified version of [Huggingface's Bert Tokenizer](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer) in pure python.\n\n## Table of Contents\n\n1. [Usage](#usage)\n   - [Installing](#installing)\n   - [Example](#example)\n   - [Running Tests](#running-tests)\n1. [Making it Lightweight](#making-it-lightweight)\n   - [Optional Features](#optional-features)\n   - [Unused Features](#unused-features)\n1. [Matching Algorithm](#matching-algorithm)\n   - [The Trie](#the-trie)\n\n## Usage\n\n### Installing\n\nInstall and update using [pip](https://pip.pypa.io/en/stable/getting-started/)\n\n```shell\npip install word-piece-tokenizer\n```\n\n### Example\n\n```python\nfrom word_piece_tokenizer import WordPieceTokenizer\ntokenizer = WordPieceTokenizer()\n\nids = tokenizer.tokenize('reading a storybook!')\n# [101, 3752, 1037, 2466, 8654, 999, 102]\n\ntokens = tokenizer.convert_ids_to_tokens(ids)\n# ['[CLS]', 'reading', 'a', 'story', '##book', '!', '[SEP]']\n\ntokenizer.convert_tokens_to_string(tokens)\n# '[CLS] reading a storybook ! [SEP]'\n```\n\n### Running Tests\n\nTest the tokenizer against hugging's face implementation:\n\n```bash\npip install transformers\npython tests/tokenizer_test.py\n```\n\n<br/>\n\n## Making It Lightweight\n\nTo make the tokenizer more lightweight and versatile for usage such as embedded systems and browsers, the tokenizer has been stripped of optional and unused features.\n\n### Optional Features\n\nThe following features has been enabled by default instead of being configurable:\n\n| Category      | Feature                                                                                                                                                                                 |\n| ------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Tokenizer     | - The tokenizer utilises the pre-trained [bert-based-uncased](https://huggingface.co/bert-base-uncased) vocab list.<br>- Basic tokenization is performed before word piece tokenization |\n| Text Cleaning | - Chinese characters are padded with whitespace<br>- Characters are converted to lowercase<br>- Input string is stripped of accent                                                      |\n\n### Unused Features\n\nThe following features has been removed from the tokenizer:\n\n- `pad_token`, `mask_token`, and special tokens\n- Ability to add new tokens to the tokenizer\n- Ability to never split certain strings (`never_split`)\n- Unused functions such as `build_inputs_with_special_tokens`, `get_special_tokens_mask`, `get_vocab`, `save_vocabulary`, and more...\n\n<br/>\n\n## Matching Algorithm\n\nThe tokenizer's _longest substring token matching_ algorithm is implemented using a `trie` instead of _greedy longest-match-first_\n\n### The Trie\n\nThe original `Trie` class has been modified to adapt to the modified _longest substring token matching_ algorithm.\n\nInstead of a `split` function that seperates the input string into substrings, the new trie implements a `getLongestMatchToken` function that returns the _token value `(int)`_ of the longest substring match, and the _remaining unmatched substring `(str)`_\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "word,piece,tokenizer,transformer,bert,uncased,nlp,natural,language,processing",
    "license": "CC0 1.0 Universal",
    "maintainer": "",
    "maintainer_email": "",
    "name": "word-piece-tokenizer",
    "package_url": "https://pypi.org/project/word-piece-tokenizer/",
    "platform": null,
    "project_url": "https://pypi.org/project/word-piece-tokenizer/",
    "project_urls": {
      "Repository": "https://github.com/ztjhz/word-piece-tokenizer"
    },
    "release_url": "https://pypi.org/project/word-piece-tokenizer/1.0.1/",
    "requires_dist": [
      "transformers ; extra == 'test'"
    ],
    "requires_python": ">=3.9",
    "summary": "A Lightweight Word Piece Tokenizer",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15223996,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b1730e2ddab45ad7a8af6ae463074cc2af0d68ad994b28eb6ac3605dc86b81d9",
          "md5": "dd8fdb7869587f653ef0a26189f5fe7a",
          "sha256": "d2de03358bcbe01acc9827f3e7256e0cc4debcdcc48dd170cd8f4039abdc926f"
        },
        "downloads": -1,
        "filename": "word_piece_tokenizer-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dd8fdb7869587f653ef0a26189f5fe7a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 119620,
        "upload_time": "2022-09-24T08:33:54",
        "upload_time_iso_8601": "2022-09-24T08:33:54.481654Z",
        "url": "https://files.pythonhosted.org/packages/b1/73/0e2ddab45ad7a8af6ae463074cc2af0d68ad994b28eb6ac3605dc86b81d9/word_piece_tokenizer-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1a12ef710a371775acac63ac07aa75caeae9a3e6d985d284576b91c23a7f57c5",
          "md5": "b35878715c2d9c1a7dd4e4d97f5b6d1a",
          "sha256": "9eec64839995153eda13de124407b6f97056ff416f6ed99825621d01259175ae"
        },
        "downloads": -1,
        "filename": "word-piece-tokenizer-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b35878715c2d9c1a7dd4e4d97f5b6d1a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 119825,
        "upload_time": "2022-09-24T08:33:57",
        "upload_time_iso_8601": "2022-09-24T08:33:57.410489Z",
        "url": "https://files.pythonhosted.org/packages/1a/12/ef710a371775acac63ac07aa75caeae9a3e6d985d284576b91c23a7f57c5/word-piece-tokenizer-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "48f74bd06cb1294f0f6b133e415a1226c4e4f2a8ae01f1da869315f82674eed8",
          "md5": "a95321ddddc3bdeac359711ed1cdf72e",
          "sha256": "655b8e918cebddfd1a13d11d77ca2c670939443e3dc9c4802d4472cc3eb6322c"
        },
        "downloads": -1,
        "filename": "word_piece_tokenizer-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a95321ddddc3bdeac359711ed1cdf72e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 119886,
        "upload_time": "2022-09-27T06:56:17",
        "upload_time_iso_8601": "2022-09-27T06:56:17.487740Z",
        "url": "https://files.pythonhosted.org/packages/48/f7/4bd06cb1294f0f6b133e415a1226c4e4f2a8ae01f1da869315f82674eed8/word_piece_tokenizer-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c1e57a38a1cc6fe9d729c0ccbb92273bc6b0ffb8e1d4d9c76c3ee3b522b8fc8b",
          "md5": "e059e09ca96646a424f66a8868d67164",
          "sha256": "46348d0a75fb4df364fa20c1dcf050b813debe7dae9e2971cadd05e3c8c0b4ba"
        },
        "downloads": -1,
        "filename": "word-piece-tokenizer-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e059e09ca96646a424f66a8868d67164",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 120267,
        "upload_time": "2022-09-27T06:56:20",
        "upload_time_iso_8601": "2022-09-27T06:56:20.375605Z",
        "url": "https://files.pythonhosted.org/packages/c1/e5/7a38a1cc6fe9d729c0ccbb92273bc6b0ffb8e1d4d9c76c3ee3b522b8fc8b/word-piece-tokenizer-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "48f74bd06cb1294f0f6b133e415a1226c4e4f2a8ae01f1da869315f82674eed8",
        "md5": "a95321ddddc3bdeac359711ed1cdf72e",
        "sha256": "655b8e918cebddfd1a13d11d77ca2c670939443e3dc9c4802d4472cc3eb6322c"
      },
      "downloads": -1,
      "filename": "word_piece_tokenizer-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a95321ddddc3bdeac359711ed1cdf72e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.9",
      "size": 119886,
      "upload_time": "2022-09-27T06:56:17",
      "upload_time_iso_8601": "2022-09-27T06:56:17.487740Z",
      "url": "https://files.pythonhosted.org/packages/48/f7/4bd06cb1294f0f6b133e415a1226c4e4f2a8ae01f1da869315f82674eed8/word_piece_tokenizer-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c1e57a38a1cc6fe9d729c0ccbb92273bc6b0ffb8e1d4d9c76c3ee3b522b8fc8b",
        "md5": "e059e09ca96646a424f66a8868d67164",
        "sha256": "46348d0a75fb4df364fa20c1dcf050b813debe7dae9e2971cadd05e3c8c0b4ba"
      },
      "downloads": -1,
      "filename": "word-piece-tokenizer-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "e059e09ca96646a424f66a8868d67164",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.9",
      "size": 120267,
      "upload_time": "2022-09-27T06:56:20",
      "upload_time_iso_8601": "2022-09-27T06:56:20.375605Z",
      "url": "https://files.pythonhosted.org/packages/c1/e5/7a38a1cc6fe9d729c0ccbb92273bc6b0ffb8e1d4d9c76c3ee3b522b8fc8b/word-piece-tokenizer-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}