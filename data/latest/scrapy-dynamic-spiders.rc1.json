{
  "info": {
    "author": "Derek Harootune Otis",
    "author_email": "dharootuneotis@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "Scrapy Dynamic Spiders\n---\n\nScrapy configures its Crawler objects using class variables associated with each Spider class. Most of these can be\nmeaningfully changed using the Spider's constructor, or even with other Spider class methods. However, some class\nvariables, most notably custom_settings, are accessed *before* Spider instantiation. Crawl rules are similarly accessed\nbefore spider instantiation, though a protected method does allow for rule re-compilation with somewhat unpredictable\nresults. In certain cases, this can lead to the creation of large numbers of subclasses made solely to handle slightly \ndifferent websites or use cases.\n\nDynamic Spiders offers a simple, extensible API for generating spider subclasses based on existing ones, as well as \nan API for running crawls in a synchronous environment (using [Crochet](https://github.com/itamarst/crochet)).\n\n### Use Cases\n\n- Mutate your pipeline without putting a million boolean flags in your code\n- Rapidly generate large numbers of spider subclasses based on external data\n- Run crawls from within a blocking event loop\n- Run crawls tailored to user input\n\n### The APIs\n\nWhat follows is a brief overview of the provided APIs.\n\n#### SpiderClsFactory\n```python3\nclass scrapy_dynamic_spiders.factories.SpiderClsFactory(custom_settings=None, settings_ow=False)\n```\nThis is the basic SpiderClsFactory, which can handle Spider, XmlFeedSpider, CsvFeedSpider, and SitemapSpider subclasses.\nIt allows for dynamic definition of custom_settings, which is the only thing in these generic spider classes which\nhave effects before instantiation.\n\n*Attributes*\n\n``custom_settings`` - a custom_settings dictionary.  \n``settings_ow`` - determines how custom_settings are handled. If ``True``, the provided settings overwrite any other\ncustom settings native to the template class. If ``False``, the provided settings are merged with a copy of the native \nsettings, with preference for provided settings.\n\n*Methods*\n\n``construct_spiderself, (spidercls) -> type`` - construct_spider returns a subclass of the template ``spidercls`` with \nits ``custom settings`` class variable customized based on the Factory's current attributes.\n\n*Extension*\n\nYou should subclass SpiderFactory if you wish to customize class variables besides ``custom_settings``. This is done in\nthe ``construct_spider`` method, which you will thus need to overwrite. There is one relevant private method which you\nmay wish to use.\n\n``_construct_custom_settings(self, spidercls) -> dict`` - returns a new ``custom_settings`` dictionary based off of\nexisting ``custom_settings`` and the Factory's current attributes.\n\n<br>  \n\n#### CrawlSpiderClsFactory\nInherits from SpiderClsFactory.\n```python3\nclass scrapy_dynamic_spiders.factories.CrawlSpiderClsFactory(custom_settings=None, settings_ow=False,\n                                         extractor_configs=None, rule_configs=None, rule_ow:=False)\n```\nThis subclass handles rule construction for CrawlSpiders and CrawlSpider subclasses. It uses two lists of dictionaries,\n``extractor_configs`` and ``rule_configs`` to create rules dynamically. Each ``extractor_config`` dictionary provides\nkwargs to a LinkExtractor, whose corresponding Rule is provided keyword arguments by the dictionary at the matching\nindex position in ``rule_configs``. If there are fewer  ``extractor_configs`` than ``rule_configs``, the last\n``extractor_config`` is used for all extra ``rule_configs``.\n\n*Attributes*\n\n``extractor_configs`` - a list of dictionaries. Each dictionary should contain kwargs for a LinkExtractor object.  \n``rule_configs`` - a list of dictionaries. Each dictionary should contain kwargs for a Rule object.  \n``rule_ow`` - if ``True``, any exiting rules are replaced by the dynamically generated rules. If ``false``, new rules\nare appended to a copy of the existing rule list.\n\n*Extension*\n\nExtending this subclass should be similar to extending SpiderFactory. There is one private method which you may wish\nto use.\n\n``_construct_rule_list(self, spidercls) -> List[Rule]`` - returns a new list of Rule objects based on those in the\ntemplate spidercls and the Factory's current attributes.\n\n<br>\n\n#### SpiderWrangler\n```python3\nclass scrapy_dynamic_spiders.wranglers.SpiderWrangler(settings, spidercls = None, clsfactory = None, gen_spiders = True)\n```\n\nThis class allows you to run scrapy crawls sequentially, within a synchronous context. It does this through a crochet-\nmanaged Twisted Reactor. Because of this, it executes crawls using a composited CrawlerRunner object, rather than a \nCrawlerProcess object. SpiderWranglers offer built-in support for generating spider subclasses using a SpiderClsFactory,\nthough this behavior is optional.\n\n*Parameters*\n\n``settings`` - a Scrapy Settings object. Required to initialize the composited CrawlerRunner.\n\n*Attributes*\n\n``gen_spiders`` - If ``True``, crawls made using the ``start_crawl`` method will use a spider subclass generated using\nthe composited SpiderClsFactory. If ``False``, crawls will use the Spider class referenced in the Wrangler's ``spidercls``\nattribute. Attempting to crawl with ``gen_spiders=True`` without a ``clsfactory`` raises an error.\n\n``spidercls`` - The Spider class which will be used for crawling or generating subclasses for crawling.\n\n``clsfactory`` - A SpiderClsFactory or subclass instance.\n\n*Methods*\n\n``start_crawl(self, *args, **kwargs) -> Deferred`` - Initiates a crawl which blocks in a synchronous context. Unlike\nScrapy's Core API, this allows you to perform multiple crawls in sequence without having to manually manipulate your\nReactor. ``*args`` and ``**kwargs`` are passed to the Spider's constructor. The Spider class used is either ``spidercls``\nor a dynamically-generated subclass of it.\n\n*Extension*\n\nExtension of SpiderWrangler is generally straightforward. However, modifying the behavior of ``start_crawl`` requires\nextra attention. ``start_crawl`` is decorated using crochet's ``wait_for`` decorator. To modify the timeout specified by\nthis decorator, you will need to override the existing ``start_crawl`` method. To simplify this, the primary functionality\nof ``start_crawl`` is encapusalated in the private method ``_start_crawl``. Changing the behavior of the decorator thus\nrequires overriding ``start_crawl`` with a new instance of the ``wait_for`` decorator, and calling ``_start_crawl`` in\nthe body of the function.\n\n``_start_crawl(self, *args, **kwargs) -> Deferred`` - Wraps the actual functionality of ``start_crawl``. Return the value\nreturn by this function, or a mutation of it, when overriding ``start_crawl``.\n\n<br>\n\n### Tests\nRun tests like so.\n```bash\npython -m unittests tests\n```\n\n<br>\n\n### Known Issues\n- Running tests results in unclosed sockets. Currently investigating.\n\n<br>\n\n### Changelog\n\n**v1.0.0a1**\n- Initial alpha release\n\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/harootune/scrapy_dynamic_spiders",
    "keywords": "scrapy spiders web-scraping",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-dynamic-spiders",
    "package_url": "https://pypi.org/project/scrapy-dynamic-spiders/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-dynamic-spiders/",
    "project_urls": {
      "Homepage": "https://github.com/harootune/scrapy_dynamic_spiders",
      "Source": "https://github.com/harootune/scrapy_dynamic_spiders"
    },
    "release_url": "https://pypi.org/project/scrapy-dynamic-spiders/1.0.0a1/",
    "requires_dist": [
      "scrapy",
      "crochet"
    ],
    "requires_python": "~=3.6",
    "summary": "Dynamically generate spider subclasses. Run crawls sequentially with crochet. Do both.",
    "version": "1.0.0a1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7626372,
  "releases": {
    "1.0.0a1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ed4e93cb6356d0ddb42b83b8a3744c20789141de058ec4b5499e06d5f1e491fb",
          "md5": "41450cfad421ebee6f0c4f1a53a79ecf",
          "sha256": "3cad84e8159a7b8e4bcd60484f1e454f91ab3c47ad4eecf8bbedf14638eb55a3"
        },
        "downloads": -1,
        "filename": "scrapy_dynamic_spiders-1.0.0a1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "41450cfad421ebee6f0c4f1a53a79ecf",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.6",
        "size": 11394,
        "upload_time": "2020-07-03T21:35:10",
        "upload_time_iso_8601": "2020-07-03T21:35:10.567536Z",
        "url": "https://files.pythonhosted.org/packages/ed/4e/93cb6356d0ddb42b83b8a3744c20789141de058ec4b5499e06d5f1e491fb/scrapy_dynamic_spiders-1.0.0a1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bd93b7f48bf8d55f0b34978f3c23f1dc78633e8d836e855df9f792a1c5fbfa26",
          "md5": "3dbb2dd3375d4aec57d06eba461693f5",
          "sha256": "c94d5b3f58e2660eec0ed6b9c735a986ca3512b9b226a5c045c873930524fa48"
        },
        "downloads": -1,
        "filename": "scrapy_dynamic_spiders-1.0.0a1.tar.gz",
        "has_sig": false,
        "md5_digest": "3dbb2dd3375d4aec57d06eba461693f5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.6",
        "size": 10185,
        "upload_time": "2020-07-03T21:35:12",
        "upload_time_iso_8601": "2020-07-03T21:35:12.612295Z",
        "url": "https://files.pythonhosted.org/packages/bd/93/b7f48bf8d55f0b34978f3c23f1dc78633e8d836e855df9f792a1c5fbfa26/scrapy_dynamic_spiders-1.0.0a1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ed4e93cb6356d0ddb42b83b8a3744c20789141de058ec4b5499e06d5f1e491fb",
        "md5": "41450cfad421ebee6f0c4f1a53a79ecf",
        "sha256": "3cad84e8159a7b8e4bcd60484f1e454f91ab3c47ad4eecf8bbedf14638eb55a3"
      },
      "downloads": -1,
      "filename": "scrapy_dynamic_spiders-1.0.0a1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "41450cfad421ebee6f0c4f1a53a79ecf",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "~=3.6",
      "size": 11394,
      "upload_time": "2020-07-03T21:35:10",
      "upload_time_iso_8601": "2020-07-03T21:35:10.567536Z",
      "url": "https://files.pythonhosted.org/packages/ed/4e/93cb6356d0ddb42b83b8a3744c20789141de058ec4b5499e06d5f1e491fb/scrapy_dynamic_spiders-1.0.0a1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bd93b7f48bf8d55f0b34978f3c23f1dc78633e8d836e855df9f792a1c5fbfa26",
        "md5": "3dbb2dd3375d4aec57d06eba461693f5",
        "sha256": "c94d5b3f58e2660eec0ed6b9c735a986ca3512b9b226a5c045c873930524fa48"
      },
      "downloads": -1,
      "filename": "scrapy_dynamic_spiders-1.0.0a1.tar.gz",
      "has_sig": false,
      "md5_digest": "3dbb2dd3375d4aec57d06eba461693f5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "~=3.6",
      "size": 10185,
      "upload_time": "2020-07-03T21:35:12",
      "upload_time_iso_8601": "2020-07-03T21:35:12.612295Z",
      "url": "https://files.pythonhosted.org/packages/bd/93/b7f48bf8d55f0b34978f3c23f1dc78633e8d836e855df9f792a1c5fbfa26/scrapy_dynamic_spiders-1.0.0a1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}