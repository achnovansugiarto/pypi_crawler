{
  "info": {
    "author": "InstaDeep Ltd",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Environment :: Console",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# MARL-eval\n\n<h2 align=\"center\">\n    <p>A tool for standardised experiment data aggregation for cooperative multi-agent reinforcement learning</p>\n</h2>\n\n## Welcome to MARL-eval üß™\n`marl-eval` is an open-source Python package for simplifying and standardising the statistical aggregation and plotting of cooperative multi-agent reinforcement learning experimental data.\n\nThis repo is the official implementation of the data aggregation guideline proposed in the paper titled _Towards a Standardised Performance Evaluation Protocol for Cooperative MARL_ by [Gorsane et al. (2022)](https://arxiv.org/abs/2209.10485) published at the 36th Conference on Neural Information Processing Systems.\n\n## Standing on the shoulders of giants ‚õ∞Ô∏è\nThe tools here build upon the tools in the [rliable](https://github.com/google-research/rliable) repo which goes along with the work done by [Agarwal et al. (2022)](https://arxiv.org/abs/2108.13264) in the paper titled _Deep Reinforcement Learning at the Edge of the Statistical Precipice_. For any details on the types of plots produced, please see their work.\n\n## Overview ü¶ú\n\n- ü™Ñ **Easy data processing**: Easy-to-use functions that will process all raw experiment data and prepare it for downstream use of the `rliable` tools.\n- üìä **Easy data plotting**: Easy-to-use functions built on top of the `rliable` tools which will handle the plotting of all processed data and produce tabular results in both `.csv` and `LaTeX` formats.\n\n\n## Installation üé¨\n\nThe latest release of the `marl-eval` can be installed as follows:\n```bash\npip install id-marl-eval\n```\nOr to install directly from source:\n\n```bash\npip install \"git+https://github.com/instadeepai/marl-eval.git\"\n```\nIt should be noted that we have tested `marl-eval` on Python 3.8.\n\n## Quickstart ‚ö°\n\nWe have a quickstart notebook available [here](examples/quickstart.ipynb), alternatively please see the following code snippet for an example of how to process data and to produce a performance profile plot:\n\n```python\n# Relevant imports\nfrom marl_eval.plotting_tools.plotting import (\n    aggregate_scores,\n    performance_profiles,\n    probability_of_improvement,\n    sample_efficiency_curves,\n)\nfrom marl_eval.utils.data_processing_utils import (\n    create_matrices_for_rliable,\n    data_process_pipeline,\n)\n\n# Specify any metrics that should be normalised\nMETRICS_TO_NORMALIZE = [\"return\"]\n\n# Read in and process data\nwith open(\"data/raw_experiment_results.json\", \"r\") as f:\n    raw_data = json.load(f)\n\nprocessed_data = data_process_pipeline(\n    raw_data=raw_data, metrics_to_normalize=METRICS_TO_NORMALIZE\n)\n\nenvironment_comparison_matrix, sample_effeciency_matrix = create_matrices_for_rliable(\n    data_dictionary=processed_data,\n    environment_name=\"env_1\",\n    metrics_to_normalize=METRICS_TO_NORMALIZE,\n)\n\n# Generate performance profile plot\nfig = performance_profiles(\n    environment_comparison_matrix,\n    metric_name=\"return\",\n    metrics_to_normalize=METRICS_TO_NORMALIZE,\n)\n```\nLeading to the following plot:\n<p align=\"center\">\n    <a href=\"docs/images/return_performance_profile.png\">\n        <img src=\"docs/images/return_performance_profile.png\" alt=\"Performance profile\" width=\"50%\"/>\n    </a>\n</p>\n\nFor a more detailed example illustrating how multiple plots may be made for various metrics as well as how to aggregate data for a single task in a given environment, please see our [quickstart notebook](examples/quickstart.ipynb) or the following [example script](https://github.com/instadeepai/marl-eval/blob/develop/examples/simple_example.py).\n\n## Usage üßë‚Äçüíª\n\nIn order to use the tools, raw experiment data must be in the suggested format and stored in a json file. If given in the correct format, `marl-eval` will aggregate experiment data, plot the results and produce aggregated tabular results as a `.csv` file, in LaTeX table formatting and in the terminal.\n\n### Data Structure for Raw Experiment data üìí\n\nIn order to use the tools we suggest effectively, raw data json files are required to have the following structure :\n\n```\n{\n    \"environment_name\" : {\n        \"task_name\" : {\n            \"algorithm_name\": {\n                \"run_1\": {\n                    \"step_1\" : {\n                        \"step_count\": <int>,\n                        \"metric_1\": [<number_evaluation_episodes>],\n                        \"metric_2\": [<number_evaluation_episodes>],\n                    }\n                    .\n                    .\n                    .\n                    \"step_k\" : {\n                        \"step_count\": <int>,\n                        \"metric_1\": [<number_evaluation_episodes>],\n                        \"metric_2\": [<number_evaluation_episodes>],\n                    }\n                    \"absolute_metrics\": {\n                        \"metric_1\": [<number_evaluation_episodes>*10],\n                        \"metric_2\": [<number_evaluation_episodes>*10]\n                    }\n\n                }\n                .\n                .\n                .\n                \"run_n\": {\n                    \"step_1\" : {\n                        \"step_count\": <int>,\n                        \"metric_1\": [<number_evaluation_episodes>],\n                        \"metric_2\": [<number_evaluation_episodes>],\n                    }\n                    .\n                    .\n                    .\n                    \"step_k\" : {\n                        \"step_count\": <int>,\n                        \"metric_1\": [<number_evaluation_episodes>],\n                        \"metric_2\": [<number_evaluation_episodes>],\n                    }\n                    \"absolute_metrics\": {\n                        \"metric_1\": [<number_evaluation_episodes>*10],\n                        \"metric_2\": [<number_evaluation_episodes>*10]\n                    }\n                }\n            }\n        }\n    }\n}\n```\nHere `run_1` to `run_n` correspond to the number of independent runs in a given experiment and `step_1` to `step_k` correspond to the number of logged steps in a given environment. We do not require an independent run to explicitly be named run, users may also name a run using the value of a particular seed that was used as a string. `step_count` corresponds to the amount of steps taken by agents in the environment when logging occurs and the values logged for each relevant metric for a given logging step should be a list containing either 1 element for a metric such as a win rate which gets computed over multiple episodes or as many elements as evaluation episodes that we run at the logging step. The final logging step for a given run should contain the `absolute_metrics` values for the given metric in an experiment with these lists containing either 1 element or 10 times as many elements as evaluation episodes at each logging step. For an explanation of the `absolute metric` please see [paragraph 1 on page 9 here](https://arxiv.org/pdf/2209.10485.pdf).\n\n> üöß **Important note on data structure** üöß\n>\n> Due to the underlying statistical aggregation relying on `numpy` array operations it is required that all data contain the same number of data points. This implies that, for a given environment, it is required that all experiment trials should be done using the same algorithms, on the same tasks, for the same number of independent runs and for the same amount of evaluation steps. The code will currently check that these conditions are met and will not be able to progress otherwise. In the case that this happens, the `check_data` method of the [`DiagnoseData`](marl_eval/utils/diagnose_data_errors.py) class will be able to tell a user exactly what is causing the issues in their raw experiment data.\n\n### Metrics to be normalised during data processing ‚öóÔ∏è\nCertain metrics, like episode returns, are required to be normalised during data processing. In order to achieve this it is required that users give these metric names, in the form of strings in a python list, to the `data_process_pipeline` function, the `create_matrices_for_rliable` function and all plotting functions as an argument. In the case where no normalisation is required this argument may be omitted.\n\n## Contributing ü§ù\n\nPlease read our [contributing docs](./CONTRIBUTING.md) for details on how to submit pull requests, our Contributor License Agreement and community guidelines.\n\n## Citing MARL-eval\n\nIf you use any of these tools in your work and find them useful, please cite the accompanying [paper](https://arxiv.org/abs/2209.10485):\n\n```bibtex\n@article{gorsane2022towards,\n  title={Towards a Standardised Performance Evaluation Protocol for Cooperative MARL},\n  author={Gorsane, Rihab and Mahjoub, Omayma and de Kock, Ruan and Dubb, Roland and Singh, Siddarth and Pretorius, Arnu},\n  journal={arXiv preprint arXiv:2209.10485},\n  year={2022}\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "multi-agent reinforcement-learning python machine learning",
    "license": "Apache License, Version 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "id-marl-eval",
    "package_url": "https://pypi.org/project/id-marl-eval/",
    "platform": null,
    "project_url": "https://pypi.org/project/id-marl-eval/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/id-marl-eval/0.0.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A Python library for Multi-Agent Reinforcement Learning evaluation.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15866121,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f24f6d558cc57f1780368d3a14b2c798e5cf179f4f0f718d4af033d029fe147e",
          "md5": "d9a478f1660f99674a6c3daf24b20b62",
          "sha256": "3917fddf18c06e833b5b41f1401305178587acd779cf14bb49ca275381a94cf2"
        },
        "downloads": -1,
        "filename": "id-marl-eval-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "d9a478f1660f99674a6c3daf24b20b62",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 19613,
        "upload_time": "2022-11-23T11:50:13",
        "upload_time_iso_8601": "2022-11-23T11:50:13.020821Z",
        "url": "https://files.pythonhosted.org/packages/f2/4f/6d558cc57f1780368d3a14b2c798e5cf179f4f0f718d4af033d029fe147e/id-marl-eval-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f24f6d558cc57f1780368d3a14b2c798e5cf179f4f0f718d4af033d029fe147e",
        "md5": "d9a478f1660f99674a6c3daf24b20b62",
        "sha256": "3917fddf18c06e833b5b41f1401305178587acd779cf14bb49ca275381a94cf2"
      },
      "downloads": -1,
      "filename": "id-marl-eval-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "d9a478f1660f99674a6c3daf24b20b62",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 19613,
      "upload_time": "2022-11-23T11:50:13",
      "upload_time_iso_8601": "2022-11-23T11:50:13.020821Z",
      "url": "https://files.pythonhosted.org/packages/f2/4f/6d558cc57f1780368d3a14b2c798e5cf179f4f0f718d4af033d029fe147e/id-marl-eval-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}