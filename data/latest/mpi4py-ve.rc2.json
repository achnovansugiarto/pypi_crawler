{
  "info": {
    "author": "NEC",
    "author_email": "dev-nlcpy@sxarr.jp.nec.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "License :: OSI Approved :: BSD License",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: C",
      "Programming Language :: Cython",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering",
      "Topic :: Software Development"
    ],
    "description": "#########\nmpi4py-ve \n#########\n\n*mpi4py-ve* is an extension to *mpi4py*, which provides Python bindings for the Message Passing Interface (MPI).\nThis package also supports to communicate array objects of `NLCPy <https://sxauroratsubasa.sakura.ne.jp/documents/nlcpy/en/>`_ (nlcpy.ndarray) between MPI processes on x86 servers of SX-Aurora TSUBASA systems.\nCombining NLCPy with *mpi4py-ve* enables Python scripts to utilize multi-VE computing power.\nThe current version of *mpi4py-ve* is based on *mpi4py* version 3.0.3.\nFor details of API references, please refer to `mpi4py manual <https://mpi4py.readthedocs.io/en/stable/>`_.\n\n************\nRequirements\n************\n\nBefore the installation, the following components are required to be installed on your x86 Node of SX-Aurora TSUBASA.\n\n- `Alternative VE Offloading (AVEO) <https://sxauroratsubasa.sakura.ne.jp/documents/veos/en/aveo/index.html>`_\n\t- required version: >= 2.13.0\n\n- `NEC MPI <https://sxauroratsubasa.sakura.ne.jp/documents/mpi/g2am01e-NEC_MPI_User_Guide_en/frame.html>`_\n\t- required NEC MPI version: > 2.22.0 (for Mellanox OFED 4.x) or >= 3.1.0 (for Mellanox OFED 5.x)\n\n- `Python <https://www.python.org/>`_\n        - required version: 3.6, 3.7, or 3.8\n\n- `NumPy <https://www.numpy.org/>`_\n        - required version: v1.17, v1.18, v1.19, or v1.20\n\n- `NLC(optional) <https://sxauroratsubasa.sakura.ne.jp/documents/sdk/SDK_NLC/UsersGuide/main/en/index.html>`_\n\t- required version: >= 2.3.0\n\n- `NLCPy(optional) <https://sxauroratsubasa.sakura.ne.jp/documents/nlcpy/en/>`_\n        - required version: >= 2.2.0\n\nSince December 2022, mpi4py-ve has been provided as a software of NEC SDK (NEC Software Development Kit for Vector Engine).\nIf NEC SDK on your machine has been properly installed or updated after that, mpi4py-ve is available by using /usr/bin/python3 command.\n\n******************\nInstall from wheel\n******************\n\nYou can install *mpi4py-ve* by executing either of the following commands.\n\n- Install from PyPI\n\n    ::\n\n    $ pip install mpi4py-ve\n\n- Install from your local computer\n\n    1. Download `the wheel package <https://github.com/SX-Aurora/mpi4py-ve/releases>`_ from GitHub.\n\n    2. Put the wheel package to your any directory.\n\n    3. Install the local wheel package via pip command.\n\n        ::\n\n        $ pip install <path_to_wheel>\n\nThe shared objects for Vector Host, which are included in the wheel package, are compiled by gcc 4.8.5 and tested by using following softwares:\n    +---------+--------------------+\n    | NEC MPI | v2.22.0 and V3.1.0 |\n    +---------+--------------------+\n    | NumPy   | v1.19.2            |\n    +---------+--------------------+\n    | NLCPy   | v2.2.0             |\n    +---------+--------------------+\n\n***********************************\nInstall from source (with building)\n***********************************\n\nBefore building this package, you need to execute the environment setup script *necmpivars.sh* or *necmpivars.csh* once advance.\n\n* When using *sh* or its variant:\n\n    ::\n\n    $ source /opt/nec/ve/mpi/X.X.X/bin/necmpivars.sh\n\n* When using *csh* or its variant:\n\n    ::\n\n    $ source /opt/nec/ve/mpi/X.X.X/bin/necmpivars.csh\n\nHere, X.X.X denotes the version number of NEC MPI.\n\nAfter that, execute the following commands:\n\n    ::\n\n    $ git clone https://github.com/SX-Aurora/mpi4py-ve.git\n    $ cd mpi4py-ve\n    $ python setup.py build --mpi=necmpi\n    $ python setup.py install \n\n*******\nExample\n*******\n\n**Transfer Array**\n\nTransfers an NLCPy's ndarray from MPI rank 0 to 1 by using comm.Send() and comm.Recv():\n\n.. code-block:: python\n\n    from mpi4pyve import MPI\n    import nlcpy as vp\n\n    comm = MPI.COMM_WORLD\n    size = comm.Get_size()\n    rank = comm.Get_rank()\n\n    if rank == 0:\n        x = vp.array([1,2,3], dtype=int)\n        comm.Send(x, dest=1)\n\n    elif rank == 1:\n        y = vp.empty(3, dtype=int)\n        comm.Recv(y, source=0)\n\n\n**Sum of Numbers**\n\nSums the numbers locally, and reduces all the local sums to the root rank (rank=0):\n\n.. code-block:: python\n\n    from mpi4pyve import MPI\n    import nlcpy as vp\n\n    comm = MPI.COMM_WORLD\n    size = comm.Get_size()\n    rank = comm.Get_rank()\n\n    N = 1000000000\n    begin = N * rank // size\n    end = N * (rank + 1) // size\n\n    sendbuf = vp.arange(begin, end).sum()\n    recvbuf = comm.reduce(sendbuf, MPI.SUM, root=0)\n\nThe following table shows the performance results[msec] on VE Type 20B:\n\n+------+------+------+------+------+------+------+------+ \n| np=1 | np=2 | np=3 | np=4 | np=5 | np=6 | np=7 | np=8 |\n+------+------+------+------+------+------+------+------+\n| 35.8 | 19.0 | 12.6 | 10.1 |  8.1 |  7.0 |  6.0 |  5.5 |\n+------+------+------+------+------+------+------+------+\n\n*********\nExecution\n*********\n\nWhen executing Python script using *mpi4py-ve*, use *mpirun* command of NEC MPI on an x86 server of SX-Aurora TSUBASA.\nBefore running the Python script, you need to execute the environment the following setup scripts once advance.\n\n* When using *sh* or its variant:\n\n    ::\n\n    $ source /opt/nec/ve/mpi/X.X.X/bin/necmpivars.sh gnu 4.8.5\n    $ source /opt/nec/ve/nlc/Y.Y.Y/bin/nlcvars.sh\n\n* When using *csh* or its variant:\n\n    ::\n\n    $ source /opt/nec/ve/mpi/X.X.X/bin/necmpivars.csh gnu 4.8.5\n    $ source /opt/nec/ve/nlc/Y.Y.Y/bin/nlcvars.csh\n\nHere, X.X.X and Y.Y.Y denote the version number of NEC MPI and NLC, respectively.\n\nWhen using the *mpirun* command:\n\n    ::\n\n    $ mpirun -veo -np N $(which python) sample.py\n\n| Here, N is the number of MPI processes that are created on an x86 server.\n| NEC MPI 2.21.0 or later supports the environment variable `NMPI_USE_COMMAND_SEARCH_PATH`.\n| If `NMPI_USE_COMMAND_SEARCH_PATH` is set to `ON` and the Python command path is added to the environment variable PATH, you do not have to specify with the full path.\n\n    ::\n\n    $ export NMPI_USE_COMMAND_SEARCH_PATH=ON\n    $ mpirun -veo -np N python sample.py\n\n| For details of mpirun command, refer to `NEC MPI User's Guide <https://sxauroratsubasa.sakura.ne.jp/documents/mpi/g2am01e-NEC_MPI_User_Guide_en/frame.html>`_.\n\n******************\nExecution Examples\n******************\n\nThe following examples show how to launch MPI programs that use mpi4py-ve and NLCPy on the SX-Aurora TSUBASA.\n\n| *ncore* : Number of cores per VE.\n| a.py: Python script using mpi4py-ve and NLCPy.\n| \n\n* Interactive Execution\n\n  * Execution on one VE\n\n    Example of using 4 processes on local VH and 4 VE processes (*ncore* / 4 OpenMP parallel per process) on VE#0 of local VH\n\n    ::\n\n      $ mpirun -veo -np 4 python a.py\n\n  * Execution on multiple VEs on a VH\n\n    Example of using 4 processes on local VH and 4 VE processes (1 process per VE, *ncore* OpenMP parallel per process) on VE#0 to VE#3 of local VH\n\n    ::\n\n      $ VE_NLCPY_NODELIST=0,1,2,3 mpirun -veo -np 4 python a.py\n\n\n    Example of using 32 processes on local VH and 32 VE processes (8 processes per VE, *ncore* / 8 OpenMP parallel per process) on VE#0 to VE# 3 of local VH\n\n    ::\n\n      $ VE_NLCPY_NODELIST=0,1,2,3 mpirun -veo -np 32 python a.py\n\n  * Execution on multiple VEs on multiple VHs\n\n    Example of using a total of 32 processes on two VHs host1 and host2, and a total of 32 VE processes on VE#0 and VE#1 of each VH (8 processes per VE, *ncore* / 8 OpenMP parallel per process)\n\n    ::\n\n      $ VE_NLCPY_NODELIST=0,1 mpirun -hosts host1,host2 -veo -np 32 python a.py\n\n* NQSV Request Execution\n\n  * Execution on a specific VH, on a VE\n\n    Example of using 32 processes on logical VH#0 and 32 VE processes on logical VE#0 to logical VE#3 on logical VH#0 (8 processes per VE, *ncore* / 8 OpenMP parallel per process)\n\n    ::\n\n      #PBS -T necmpi\n      #PBS -b 2 # The number of logical hosts\n      #PBS --venum-lhost=4 # The number of VEs per logical host\n      #PBS --cpunum-lhost=32 # The number of CPUs per logical host\n\n      source /opt/nec/ve/mpi/2.22.0/bin/necmpivars.sh\n      export NMPI_USE_COMMAND_SEARCH_PATH=ON\n      mpirun -host 0 -veo -np 32 python a.py\n\n  * Execution on a specific VH, on a specific VE\n\n    Example of using 16 processes on logical VH#0, 16 VE processes in total on logical VE#0 and logical VE#3 on logical VH#0 (8 processes per VE, *ncore* / 8 OpenMP parallel per process)\n\n    ::\n\n      #PBS -T necmpi\n      #PBS -b 2 # The number of logical hosts\n      #PBS --venum-lhost=4 # The number of VEs per logical host\n      #PBS --cpunum-lhost=16 # The number of CPUs per logical host\n\n      source /opt/nec/ve/mpi/2.22.0/bin/necmpivars.sh\n      export NMPI_USE_COMMAND_SEARCH_PATH=ON\n      VE_NLCPY_NODELIST=0,3 mpirun -host 0 -veo -np 16 python a.py\n\n  * Execution on all assigned VEs\n\n    Example of using 32 processes in total on 4 VHs and using 32 VE processes in total from logical VE#0 to logical VE#7 on each of VHs (1 process per VE, *ncore* OpenMP parallel per process).\n\n    ::\n\n      #PBS -T necmpi\n      #PBS -b 4 # The number of logical hosts\n      #PBS --venum-lhost=8 # The number of VEs per logical host\n      #PBS --cpunum-lhost=8 # The number of CPUs per logical host\n      #PBS --use-hca=2 # The number of HCAs\n\n      source /opt/nec/ve/mpi/2.22.0/bin/necmpivars.sh\n      export NMPI_USE_COMMAND_SEARCH_PATH=ON\n      mpirun -veo -np 32 python a.py\n\n*********\nProfiling\n*********\nNEC MPI provides the facility of displaying MPI communication information. \nThere are two formats of MPI communication information available as follows:\n\n+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ \n| Reduced Format  | The maximum, minimum, and average values of MPI communication information of all MPI processes are displayed.                                                                        |\n+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Extended Format | MPI communication information of each MPI process is displayed in the ascending order of their ranks in the communicator MPI_COMM_WORLD after the information in the reduced format. |\n+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\nYou can control the display and format of MPI communication information by setting the environment variable NMPI_COMMINF at runtime as shown in the following table.\n\nThe Settings of NMPI_COMMINF:\n\n+--------------+-----------------------+ \n| NMPI_COMMINF | Displayed Information |\n+--------------+-----------------------+\n| NO           | (Default) No Output   |\n+--------------+-----------------------+\n| YES          | Reduced Format        |\n+--------------+-----------------------+\n| ALL          | Extended Format       |\n+--------------+-----------------------+\n\nWhen using the *mpirun* command:\n\n    ::\n\n    $ export NMPI_COMMINF=ALL\n    $ mpirun -veo -np N python sample.py\n\n***************************************************\nUse mpi4py-ve with homebrew classes (without NLCPy)\n***************************************************\n\nBelow links would be useful to use *mpi4py-ve* with homebrew classes (without NLCPy):\n\n* `use mpi4py-ve with homebrew classes (without NLCPy) <https://github.com/SX-Aurora/mpi4py-ve/blob/v1.0.0/docs/vai_spec_example.rst>`_\n\n***************\nOther Documents\n***************\n\nBelow links would be useful to understand *mpi4py-ve* in more detail:\n\n* `mpi4py-ve tutorial <https://github.com/SX-Aurora/mpi4py-ve/blob/v1.0.0/docs/index.rst>`_\n\n***********\nRestriction\n***********\n* The current version of *mpi4py-ve* does not support some functions that are listed in the section \"List of Unsupported Functions\" of `mpi4py-ve tutorial <https://github.com/SX-Aurora/mpi4py-ve/blob/v1.0.0/docs/index.rst>`_.\n* Communication of type bool between NumPy and NLCPy will fail because of the different number of bytes.\n\n*******\nNotices\n*******\n* If you import NLCPy before calling MPI_Init()/MPI_Init_thread(), a runtime error will be raised.\n\n    Not recommended usage: ::\n\n        $ mpirun -veo -np 1 $(which python) -c \"import nlcpy; from mpi4pyve import MPI\"\n        RuntimeError: NLCPy must be import after MPI initialization\n\n    Recommended usage: ::\n\n        $ mpirun -veo -np 1 $(which python) -c \"from mpi4pyve import MPI; import nlcpy\" \n\n    MPI_Init() or MPI_Init_thread() is called when you import the MPI module from the mpi4pyve package.\n\n* If you use the Lock/Lock_all function for one-sided communication using NLCPy array data, you need to put in NLCPy synchronization control.\n\n    Synchronization usage:\n\n    .. code-block:: python\n\n        import mpi4pyve\n        from mpi4pyve import MPI\n        import nlcpy as vp\n\n        comm = MPI.COMM_WORLD\n        size = comm.Get_size()\n        rank = comm.Get_rank()\n\n        array = vp.array(0, dtype=int)\n\n        if rank == 0:\n            win_n = MPI.Win.Create(array,  comm=MPI.COMM_WORLD)\n        else:\n            win_n = MPI.Win.Create(None, comm=MPI.COMM_WORLD)\n        if rank == 0:\n            array.fill(1)\n            array.venode.synchronize()\n            comm.Barrier()\n        if rank != 0:\n           comm.Barrier()\n            win_n.Lock(MPI.LOCK_EXCLUSIVE, 0)\n            win_n.Get([array, MPI.INT], 0)\n            win_n.Unlock(0)\n            assert array == 1\n        comm.Barrier()\n        win_n.Free()\n\n*******\nLicense\n*******\n\n| The 2-clause BSD license (see *LICENSE* file).\n| *mpi4py-ve* is derived from mpi4py (see *LICENSE_DETAIL/LICENSE_DETAIL* file).\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "https://github.com/SX-Aurora/mpi4py-ve/releases/",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/SX-Aurora/mpi4py-ve/",
    "keywords": "scientific computing,parallel computing,message passing interface,MPI",
    "license": "BSD",
    "maintainer": "NEC",
    "maintainer_email": "dev-nlcpy@sxarr.jp.nec.com",
    "name": "mpi4py-ve",
    "package_url": "https://pypi.org/project/mpi4py-ve/",
    "platform": "Linux",
    "project_url": "https://pypi.org/project/mpi4py-ve/",
    "project_urls": {
      "Download": "https://github.com/SX-Aurora/mpi4py-ve/releases/",
      "Homepage": "https://github.com/SX-Aurora/mpi4py-ve/"
    },
    "release_url": "https://pypi.org/project/mpi4py-ve/1.0.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Python bindings for MPI",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15937046,
  "releases": {
    "0.1.0b1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7791eb7104e74479794289a24c2ab21f366ca892d5a0455c055099cad7174c61",
          "md5": "d62f4c30f556a15a9f7233f9b979dd2a",
          "sha256": "8042912c721688b65c57b129e44173d7596ef5296afb2ec5c3ef81a8cae38ad0"
        },
        "downloads": -1,
        "filename": "mpi4py_ve-0.1.0b1-cp36-cp36m-manylinux1_x86_64.whl",
        "has_sig": false,
        "md5_digest": "d62f4c30f556a15a9f7233f9b979dd2a",
        "packagetype": "bdist_wheel",
        "python_version": "cp36",
        "requires_python": null,
        "size": 3062895,
        "upload_time": "2022-03-31T00:17:50",
        "upload_time_iso_8601": "2022-03-31T00:17:50.174861Z",
        "url": "https://files.pythonhosted.org/packages/77/91/eb7104e74479794289a24c2ab21f366ca892d5a0455c055099cad7174c61/mpi4py_ve-0.1.0b1-cp36-cp36m-manylinux1_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7bb4d2457da192962847199e23fd52327f55689dff8e4788f97aba18c1686688",
          "md5": "6900b5df20c2c9913ba1876cff439866",
          "sha256": "40b32cb885ac1e31225c1d32100e74c525ec3939719d4a109092058bc2b28c33"
        },
        "downloads": -1,
        "filename": "mpi4py_ve-0.1.0b1-cp37-cp37m-manylinux1_x86_64.whl",
        "has_sig": false,
        "md5_digest": "6900b5df20c2c9913ba1876cff439866",
        "packagetype": "bdist_wheel",
        "python_version": "cp37",
        "requires_python": null,
        "size": 3087840,
        "upload_time": "2022-03-31T00:18:42",
        "upload_time_iso_8601": "2022-03-31T00:18:42.615206Z",
        "url": "https://files.pythonhosted.org/packages/7b/b4/d2457da192962847199e23fd52327f55689dff8e4788f97aba18c1686688/mpi4py_ve-0.1.0b1-cp37-cp37m-manylinux1_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a20512fea13056b61dfba7283506ee12e2f53a97c2f281622b676a237836d26d",
          "md5": "e44329deb4e90b0b3cfe6ac283ac4e0b",
          "sha256": "8f97b94d5d5beb879d568e79a662d42009c98474240e21b314adc34cdf5f0c8e"
        },
        "downloads": -1,
        "filename": "mpi4py_ve-0.1.0b1-cp38-cp38-manylinux1_x86_64.whl",
        "has_sig": false,
        "md5_digest": "e44329deb4e90b0b3cfe6ac283ac4e0b",
        "packagetype": "bdist_wheel",
        "python_version": "cp38",
        "requires_python": null,
        "size": 3282023,
        "upload_time": "2022-03-31T00:24:34",
        "upload_time_iso_8601": "2022-03-31T00:24:34.557861Z",
        "url": "https://files.pythonhosted.org/packages/a2/05/12fea13056b61dfba7283506ee12e2f53a97c2f281622b676a237836d26d/mpi4py_ve-0.1.0b1-cp38-cp38-manylinux1_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0813a1c44ee56661fff88518361f5f0ffab76cfd2695e63931d8151e68130e94",
          "md5": "2bdc6f50b115ad8dee142ff03afc8bd3",
          "sha256": "afce58beb30c75ffd3e2cb0ec35799c079f92afd578bbfdc72828dcb6cb897bf"
        },
        "downloads": -1,
        "filename": "mpi4py_ve-1.0.0-cp36-cp36m-manylinux1_x86_64.whl",
        "has_sig": false,
        "md5_digest": "2bdc6f50b115ad8dee142ff03afc8bd3",
        "packagetype": "bdist_wheel",
        "python_version": "cp36",
        "requires_python": null,
        "size": 2449881,
        "upload_time": "2022-11-30T03:01:25",
        "upload_time_iso_8601": "2022-11-30T03:01:25.638480Z",
        "url": "https://files.pythonhosted.org/packages/08/13/a1c44ee56661fff88518361f5f0ffab76cfd2695e63931d8151e68130e94/mpi4py_ve-1.0.0-cp36-cp36m-manylinux1_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b30dd330310d3d2ad221bdf4c8986edd5ca51293c72344d42640cdcd0f51e819",
          "md5": "efa49040502c17879ba2d05066d58475",
          "sha256": "49eb2410de0d68f19007f7a046c9ba2a01e6df99e41d3d27ef5b34a1e8e8229c"
        },
        "downloads": -1,
        "filename": "mpi4py_ve-1.0.0-cp37-cp37m-manylinux1_x86_64.whl",
        "has_sig": false,
        "md5_digest": "efa49040502c17879ba2d05066d58475",
        "packagetype": "bdist_wheel",
        "python_version": "cp37",
        "requires_python": null,
        "size": 2439185,
        "upload_time": "2022-11-30T03:03:58",
        "upload_time_iso_8601": "2022-11-30T03:03:58.165231Z",
        "url": "https://files.pythonhosted.org/packages/b3/0d/d330310d3d2ad221bdf4c8986edd5ca51293c72344d42640cdcd0f51e819/mpi4py_ve-1.0.0-cp37-cp37m-manylinux1_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a2eae8548eadcba2a54357f84675a9626f92ae0a775e40dd68cb8e49818f8944",
          "md5": "f32c86dcbe9bdbec163b3c7502233cfd",
          "sha256": "ccd9a8119d2f696ae1164ac42ea1c78007467fefbdc52334fda78b740bda1c6a"
        },
        "downloads": -1,
        "filename": "mpi4py_ve-1.0.0-cp38-cp38-manylinux1_x86_64.whl",
        "has_sig": false,
        "md5_digest": "f32c86dcbe9bdbec163b3c7502233cfd",
        "packagetype": "bdist_wheel",
        "python_version": "cp38",
        "requires_python": null,
        "size": 2504263,
        "upload_time": "2022-11-30T03:04:23",
        "upload_time_iso_8601": "2022-11-30T03:04:23.051941Z",
        "url": "https://files.pythonhosted.org/packages/a2/ea/e8548eadcba2a54357f84675a9626f92ae0a775e40dd68cb8e49818f8944/mpi4py_ve-1.0.0-cp38-cp38-manylinux1_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0813a1c44ee56661fff88518361f5f0ffab76cfd2695e63931d8151e68130e94",
        "md5": "2bdc6f50b115ad8dee142ff03afc8bd3",
        "sha256": "afce58beb30c75ffd3e2cb0ec35799c079f92afd578bbfdc72828dcb6cb897bf"
      },
      "downloads": -1,
      "filename": "mpi4py_ve-1.0.0-cp36-cp36m-manylinux1_x86_64.whl",
      "has_sig": false,
      "md5_digest": "2bdc6f50b115ad8dee142ff03afc8bd3",
      "packagetype": "bdist_wheel",
      "python_version": "cp36",
      "requires_python": null,
      "size": 2449881,
      "upload_time": "2022-11-30T03:01:25",
      "upload_time_iso_8601": "2022-11-30T03:01:25.638480Z",
      "url": "https://files.pythonhosted.org/packages/08/13/a1c44ee56661fff88518361f5f0ffab76cfd2695e63931d8151e68130e94/mpi4py_ve-1.0.0-cp36-cp36m-manylinux1_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b30dd330310d3d2ad221bdf4c8986edd5ca51293c72344d42640cdcd0f51e819",
        "md5": "efa49040502c17879ba2d05066d58475",
        "sha256": "49eb2410de0d68f19007f7a046c9ba2a01e6df99e41d3d27ef5b34a1e8e8229c"
      },
      "downloads": -1,
      "filename": "mpi4py_ve-1.0.0-cp37-cp37m-manylinux1_x86_64.whl",
      "has_sig": false,
      "md5_digest": "efa49040502c17879ba2d05066d58475",
      "packagetype": "bdist_wheel",
      "python_version": "cp37",
      "requires_python": null,
      "size": 2439185,
      "upload_time": "2022-11-30T03:03:58",
      "upload_time_iso_8601": "2022-11-30T03:03:58.165231Z",
      "url": "https://files.pythonhosted.org/packages/b3/0d/d330310d3d2ad221bdf4c8986edd5ca51293c72344d42640cdcd0f51e819/mpi4py_ve-1.0.0-cp37-cp37m-manylinux1_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a2eae8548eadcba2a54357f84675a9626f92ae0a775e40dd68cb8e49818f8944",
        "md5": "f32c86dcbe9bdbec163b3c7502233cfd",
        "sha256": "ccd9a8119d2f696ae1164ac42ea1c78007467fefbdc52334fda78b740bda1c6a"
      },
      "downloads": -1,
      "filename": "mpi4py_ve-1.0.0-cp38-cp38-manylinux1_x86_64.whl",
      "has_sig": false,
      "md5_digest": "f32c86dcbe9bdbec163b3c7502233cfd",
      "packagetype": "bdist_wheel",
      "python_version": "cp38",
      "requires_python": null,
      "size": 2504263,
      "upload_time": "2022-11-30T03:04:23",
      "upload_time_iso_8601": "2022-11-30T03:04:23.051941Z",
      "url": "https://files.pythonhosted.org/packages/a2/ea/e8548eadcba2a54357f84675a9626f92ae0a775e40dd68cb8e49818f8944/mpi4py_ve-1.0.0-cp38-cp38-manylinux1_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}