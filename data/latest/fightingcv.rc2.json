{
  "info": {
    "author": "xmu-xiaoma666",
    "author_email": "julien@huggingface.co",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "\r\n<img src=\"./FightingCVimg/LOGO.gif\" height=\"200\" width=\"400\"/>\r\n\r\nÁÆÄ‰Ωì‰∏≠Êñá | [English](./README_EN.md)\r\n\r\n# FightingCV ‰ª£Á†ÅÂ∫ìÔºå ÂåÖÂê´ [***Attention***](#attention-series),[***Backbone***](#backbone-series), [***MLP***](#mlp-series), [***Re-parameter***](#re-parameter-series), [**Convolution**](#convolution-series)\r\n\r\n![](https://img.shields.io/badge/fightingcv-v0.0.1-brightgreen)\r\n![](https://img.shields.io/badge/python->=v3.0-blue)\r\n![](https://img.shields.io/badge/pytorch->=v1.4-red)\r\n\r\n<!--\r\n-------\r\n*If this project is helpful to you, welcome to give a ***star***.* \r\n\r\n*Don't forget to ***follow*** me to learn about project updates.*\r\n\r\n-->\r\n\r\n\r\n\r\n\r\nHelloÔºåÂ§ßÂÆ∂Â•ΩÔºåÊàëÊòØÂ∞èÈ©¨üöÄüöÄüöÄ\r\n\r\n***For Â∞èÁôΩÔºàLike MeÔºâÔºö***\r\nÊúÄËøëÂú®ËØªËÆ∫ÊñáÁöÑÊó∂ÂÄô‰ºöÂèëÁé∞‰∏Ä‰∏™ÈóÆÈ¢òÔºåÊúâÊó∂ÂÄôËÆ∫ÊñáÊ†∏ÂøÉÊÄùÊÉ≥ÈùûÂ∏∏ÁÆÄÂçïÔºåÊ†∏ÂøÉ‰ª£Á†ÅÂèØËÉΩ‰πüÂ∞±ÂçÅÂá†Ë°å„ÄÇ‰ΩÜÊòØÊâìÂºÄ‰ΩúËÄÖreleaseÁöÑÊ∫êÁ†ÅÊó∂ÔºåÂç¥ÂèëÁé∞ÊèêÂá∫ÁöÑÊ®°ÂùóÂµåÂÖ•Âà∞ÂàÜÁ±ª„ÄÅÊ£ÄÊµã„ÄÅÂàÜÂâ≤Á≠â‰ªªÂä°Ê°ÜÊû∂‰∏≠ÔºåÂØºËá¥‰ª£Á†ÅÊØîËæÉÂÜó‰ΩôÔºåÂØπ‰∫éÁâπÂÆö‰ªªÂä°Ê°ÜÊû∂‰∏çÁÜüÊÇâÁöÑÊàëÔºå**ÂæàÈöæÊâæÂà∞Ê†∏ÂøÉ‰ª£Á†Å**ÔºåÂØºËá¥Âú®ËÆ∫ÊñáÂíåÁΩëÁªúÊÄùÊÉ≥ÁöÑÁêÜËß£‰∏ä‰ºöÊúâ‰∏ÄÂÆöÂõ∞Èöæ„ÄÇ\r\n\r\n***For ËøõÈò∂ËÄÖÔºàLike YouÔºâÔºö***\r\nÂ¶ÇÊûúÊääConv„ÄÅFC„ÄÅRNNËøô‰∫õÂü∫Êú¨ÂçïÂÖÉÁúãÂÅöÂ∞èÁöÑLegoÁßØÊú®ÔºåÊääTransformer„ÄÅResNetËøô‰∫õÁªìÊûÑÁúãÊàêÂ∑≤ÁªèÊê≠Â•ΩÁöÑLegoÂüéÂ†°„ÄÇÈÇ£‰πàÊú¨È°πÁõÆÊèê‰æõÁöÑÊ®°ÂùóÂ∞±ÊòØ‰∏Ä‰∏™‰∏™ÂÖ∑ÊúâÂÆåÊï¥ËØ≠‰πâ‰ø°ÊÅØÁöÑLegoÁªÑ‰ª∂„ÄÇ**ËÆ©ÁßëÁ†îÂ∑•‰ΩúËÄÖ‰ª¨ÈÅøÂÖçÂèçÂ§çÈÄ†ËΩÆÂ≠ê**ÔºåÂè™ÈúÄÊÄùËÄÉÂ¶Ç‰ΩïÂà©Áî®Ëøô‰∫õ‚ÄúLegoÁªÑ‰ª∂‚ÄùÔºåÊê≠Âª∫Âá∫Êõ¥Â§öÁªöÁÉÇÂ§öÂΩ©ÁöÑ‰ΩúÂìÅ„ÄÇ\r\n\r\n***For Â§ßÁ•ûÔºàMay Be Like YouÔºâÔºö***\r\nËÉΩÂäõÊúâÈôêÔºå**‰∏çÂñúËΩªÂñ∑**ÔºÅÔºÅÔºÅ\r\n\r\n***For AllÔºö***\r\nÊú¨È°πÁõÆÂ∞±ÊòØË¶ÅÂÆûÁé∞‰∏Ä‰∏™Êó¢ËÉΩ**ËÆ©Ê∑±Â∫¶Â≠¶‰π†Â∞èÁôΩ‰πüËÉΩÊêûÊáÇ**ÔºåÂèàËÉΩ**ÊúçÂä°ÁßëÁ†îÂíåÂ∑•‰∏öÁ§æÂå∫**ÁöÑ‰ª£Á†ÅÂ∫ì„ÄÇ‰Ωú‰∏∫[**FightingCVÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/m9RiivbbDPdjABsTd6q8FA)Âíå **[FightingCV-Paper-Reading](https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading)** ÁöÑË°•ÂÖÖÔºåÊú¨È°πÁõÆÁöÑÂÆóÊó®ÊòØ‰ªé‰ª£Á†ÅËßíÂ∫¶ÔºåÂÆûÁé∞üöÄ**ËÆ©‰∏ñÁïå‰∏äÊ≤°ÊúâÈöæËØªÁöÑËÆ∫Êñá**üöÄ„ÄÇ\r\n\r\nÔºàÂêåÊó∂‰πüÈùûÂ∏∏Ê¨¢ËøéÂêÑ‰ΩçÁßëÁ†îÂ∑•‰ΩúËÄÖÂ∞ÜËá™Â∑±ÁöÑÂ∑•‰ΩúÁöÑÊ†∏ÂøÉ‰ª£Á†ÅÊï¥ÁêÜÂà∞Êú¨È°πÁõÆ‰∏≠ÔºåÊé®Âä®ÁßëÁ†îÁ§æÂå∫ÁöÑÂèëÂ±ïÔºå‰ºöÂú®readme‰∏≠Ê≥®Êòé‰ª£Á†ÅÁöÑ‰ΩúËÄÖ~Ôºâ\r\n\r\n\r\n\r\n\r\n\r\n\r\n## ÊäÄÊúØ‰∫§ÊµÅ <img title=\"\" src=\"https://user-images.githubusercontent.com/48054808/157800467-2a9946ad-30d1-49a9-b9db-ba33413d9c90.png\" alt=\"\" width=\"20\">\r\n\r\nÊ¨¢ËøéÂ§ßÂÆ∂ÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑Ôºö**FightingCV**\r\n\r\n\r\n\r\n| FightingCVÂÖ¨‰ºóÂè∑ | Â∞èÂä©ÊâãÂæÆ‰ø° ÔºàÂ§áÊ≥®„Äê**ÂÖ¨Âè∏/Â≠¶Ê†°+ÊñπÂêë+ID**„ÄëÔºâ|\r\n:-------------------------:|:-------------------------:\r\n<img src='./FightingCVimg/FightingCV.jpg' width='200px'>  |  <img src='./FightingCVimg/xiaozhushou.jpg' width='200px'> \r\n\r\n- ÂÖ¨‰ºóÂè∑**ÊØèÂ§©**ÈÉΩ‰ºöËøõË°å**ËÆ∫Êñá„ÄÅÁÆóÊ≥ïÂíå‰ª£Á†ÅÁöÑÂπ≤Ë¥ßÂàÜ‰∫´**Âì¶~\r\n\r\n- **‰∫§ÊµÅÁæ§ÊØèÂ§©ÂàÜ‰∫´‰∏Ä‰∫õÊúÄÊñ∞ÁöÑËÆ∫ÊñáÂíåËß£Êûê**ÔºåÊ¨¢ËøéÂ§ßÂÆ∂‰∏ÄËµ∑**Â≠¶‰π†‰∫§ÊµÅ**Âìà~~~\r\nÔºàÂä†‰∏çËøõÂéªÂèØ‰ª•Âä†ÂæÆ‰ø°Ôºö**775629340**ÔºåËÆ∞ÂæóÂ§áÊ≥®„Äê**ÂÖ¨Âè∏/Â≠¶Ê†°+ÊñπÂêë+ID**„ÄëÔºâ\r\n\r\n<!-- ![](./FightingCVimg/wechat.jpg) -->\r\n\r\n- Âº∫ÁÉàÊé®ËçêÂ§ßÂÆ∂ÂÖ≥Ê≥®[**Áü•‰πé**](https://www.zhihu.com/people/jason-14-58-38/posts)Ë¥¶Âè∑Âíå[**FightingCVÂÖ¨‰ºóÂè∑**](https://mp.weixin.qq.com/s/m9RiivbbDPdjABsTd6q8FA)ÔºåÂèØ‰ª•Âø´ÈÄü‰∫ÜËß£Âà∞ÊúÄÊñ∞‰ºòË¥®ÁöÑÂπ≤Ë¥ßËµÑÊ∫ê„ÄÇ\r\n\r\n\r\n-------\r\n\r\n\r\n\r\n# ÁõÆÂΩï\r\n\r\n- [Attention Series](#attention-series)\r\n    - [1. External Attention Usage](#1-external-attention-usage)\r\n\r\n    - [2. Self Attention Usage](#2-self-attention-usage)\r\n\r\n    - [3. Simplified Self Attention Usage](#3-simplified-self-attention-usage)\r\n\r\n    - [4. Squeeze-and-Excitation Attention Usage](#4-squeeze-and-excitation-attention-usage)\r\n\r\n    - [5. SK Attention Usage](#5-sk-attention-usage)\r\n\r\n    - [6. CBAM Attention Usage](#6-cbam-attention-usage)\r\n\r\n    - [7. BAM Attention Usage](#7-bam-attention-usage)\r\n    \r\n    - [8. ECA Attention Usage](#8-eca-attention-usage)\r\n\r\n    - [9. DANet Attention Usage](#9-danet-attention-usage)\r\n\r\n    - [10. Pyramid Split Attention (PSA) Usage](#10-Pyramid-Split-Attention-Usage)\r\n\r\n    - [11. Efficient Multi-Head Self-Attention(EMSA) Usage](#11-Efficient-Multi-Head-Self-Attention-Usage)\r\n\r\n    - [12. Shuffle Attention Usage](#12-Shuffle-Attention-Usage)\r\n    \r\n    - [13. MUSE Attention Usage](#13-MUSE-Attention-Usage)\r\n  \r\n    - [14. SGE Attention Usage](#14-SGE-Attention-Usage)\r\n\r\n    - [15. A2 Attention Usage](#15-A2-Attention-Usage)\r\n\r\n    - [16. AFT Attention Usage](#16-AFT-Attention-Usage)\r\n\r\n    - [17. Outlook Attention Usage](#17-Outlook-Attention-Usage)\r\n\r\n    - [18. ViP Attention Usage](#18-ViP-Attention-Usage)\r\n\r\n    - [19. CoAtNet Attention Usage](#19-CoAtNet-Attention-Usage)\r\n\r\n    - [20. HaloNet Attention Usage](#20-HaloNet-Attention-Usage)\r\n\r\n    - [21. Polarized Self-Attention Usage](#21-Polarized-Self-Attention-Usage)\r\n\r\n    - [22. CoTAttention Usage](#22-CoTAttention-Usage)\r\n\r\n    - [23. Residual Attention Usage](#23-Residual-Attention-Usage)\r\n  \r\n    - [24. S2 Attention Usage](#24-S2-Attention-Usage)\r\n\r\n    - [25. GFNet Attention Usage](#25-GFNet-Attention-Usage)\r\n\r\n    - [26. Triplet Attention Usage](#26-TripletAttention-Usage)\r\n\r\n    - [27. Coordinate Attention Usage](#27-Coordinate-Attention-Usage)\r\n\r\n    - [28. MobileViT Attention Usage](#28-MobileViT-Attention-Usage)\r\n\r\n    - [29. ParNet Attention Usage](#29-ParNet-Attention-Usage)\r\n\r\n    - [30. UFO Attention Usage](#30-UFO-Attention-Usage)\r\n\r\n    - [31. ACmix Attention Usage](#31-Acmix-Attention-Usage)\r\n  \r\n    - [32. MobileViTv2 Attention Usage](#32-MobileViTv2-Attention-Usage)\r\n\r\n    - [33. DAT Attention Usage](#33-DAT-Attention-Usage)\r\n\r\n    - [34. CrossFormer Attention Usage](#34-CrossFormer-Attention-Usage)\r\n\r\n    - [35. MOATransformer Attention Usage](#35-MOATransformer-Attention-Usage)\r\n\r\n    - [36. CrissCrossAttention Attention Usage](#36-CrissCrossAttention-Attention-Usage)\r\n\r\n    - [37. Axial_attention Attention Usage](#37-Axial_attention-Attention-Usage)\r\n\r\n- [Backbone Series](#Backbone-series)\r\n\r\n    - [1. ResNet Usage](#1-ResNet-Usage)\r\n\r\n    - [2. ResNeXt Usage](#2-ResNeXt-Usage)\r\n\r\n    - [3. MobileViT Usage](#3-MobileViT-Usage)\r\n\r\n    - [4. ConvMixer Usage](#4-ConvMixer-Usage)\r\n\r\n    - [5. ShuffleTransformer Usage](#5-ShuffleTransformer-Usage)\r\n\r\n    - [6. ConTNet Usage](#6-ConTNet-Usage)\r\n\r\n    - [7. HATNet Usage](#7-HATNet-Usage)\r\n\r\n    - [8. CoaT Usage](#8-CoaT-Usage)\r\n\r\n    - [9. PVT Usage](#9-PVT-Usage)\r\n\r\n    - [10. CPVT Usage](#10-CPVT-Usage)\r\n\r\n    - [11. PIT Usage](#11-PIT-Usage)\r\n\r\n    - [12. CrossViT Usage](#12-CrossViT-Usage)\r\n\r\n    - [13. TnT Usage](#13-TnT-Usage)\r\n\r\n    - [14. DViT Usage](#14-DViT-Usage)\r\n\r\n    - [15. CeiT Usage](#15-CeiT-Usage)\r\n\r\n    - [16. ConViT Usage](#16-ConViT-Usage)\r\n\r\n    - [17. CaiT Usage](#17-CaiT-Usage)\r\n\r\n    - [18. PatchConvnet Usage](#18-PatchConvnet-Usage)\r\n\r\n    - [19. DeiT Usage](#19-DeiT-Usage)\r\n\r\n    - [20. LeViT Usage](#20-LeViT-Usage)\r\n\r\n    - [21. VOLO Usage](#21-VOLO-Usage)\r\n    \r\n    - [22. Container Usage](#22-Container-Usage)\r\n\r\n    - [23. CMT Usage](#23-CMT-Usage)\r\n\r\n    - [24. EfficientFormer Usage](#24-EfficientFormer-Usage)\r\n\r\n\r\n- [MLP Series](#mlp-series)\r\n\r\n    - [1. RepMLP Usage](#1-RepMLP-Usage)\r\n\r\n    - [2. MLP-Mixer Usage](#2-MLP-Mixer-Usage)\r\n\r\n    - [3. ResMLP Usage](#3-ResMLP-Usage)\r\n\r\n    - [4. gMLP Usage](#4-gMLP-Usage)\r\n\r\n    - [5. sMLP Usage](#5-sMLP-Usage)\r\n\r\n    - [6. vip-mlp Usage](#6-vip-mlp-Usage)\r\n\r\n- [Re-Parameter(ReP) Series](#Re-Parameter-series)\r\n\r\n    - [1. RepVGG Usage](#1-RepVGG-Usage)\r\n\r\n    - [2. ACNet Usage](#2-ACNet-Usage)\r\n\r\n    - [3. Diverse Branch Block(DDB) Usage](#3-Diverse-Branch-Block-Usage)\r\n\r\n- [Convolution Series](#Convolution-series)\r\n\r\n    - [1. Depthwise Separable Convolution Usage](#1-Depthwise-Separable-Convolution-Usage)\r\n\r\n    - [2. MBConv Usage](#2-MBConv-Usage)\r\n\r\n    - [3. Involution Usage](#3-Involution-Usage)\r\n\r\n    - [4. DynamicConv Usage](#4-DynamicConv-Usage)\r\n\r\n    - [5. CondConv Usage](#5-CondConv-Usage)\r\n\r\n***\r\n\r\n\r\n# Attention Series\r\n\r\n- Pytorch implementation of [\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks---arXiv 2021.05.05\"](https://arxiv.org/abs/2105.02358)\r\n\r\n- Pytorch implementation of [\"Attention Is All You Need---NIPS2017\"](https://arxiv.org/pdf/1706.03762.pdf)\r\n\r\n- Pytorch implementation of [\"Squeeze-and-Excitation Networks---CVPR2018\"](https://arxiv.org/abs/1709.01507)\r\n\r\n- Pytorch implementation of [\"Selective Kernel Networks---CVPR2019\"](https://arxiv.org/pdf/1903.06586.pdf)\r\n\r\n- Pytorch implementation of [\"CBAM: Convolutional Block Attention Module---ECCV2018\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\r\n\r\n- Pytorch implementation of [\"BAM: Bottleneck Attention Module---BMCV2018\"](https://arxiv.org/pdf/1807.06514.pdf)\r\n\r\n- Pytorch implementation of [\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020\"](https://arxiv.org/pdf/1910.03151.pdf)\r\n\r\n- Pytorch implementation of [\"Dual Attention Network for Scene Segmentation---CVPR2019\"](https://arxiv.org/pdf/1809.02983.pdf)\r\n\r\n- Pytorch implementation of [\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30\"](https://arxiv.org/pdf/2105.14447.pdf)\r\n\r\n- Pytorch implementation of [\"ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28\"](https://arxiv.org/abs/2105.13677)\r\n\r\n- Pytorch implementation of [\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021\"](https://arxiv.org/pdf/2102.00240.pdf)\r\n\r\n- Pytorch implementation of [\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17\"](https://arxiv.org/abs/1911.09483)\r\n\r\n- Pytorch implementation of [\"Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23\"](https://arxiv.org/pdf/1905.09646.pdf)\r\n\r\n- Pytorch implementation of [\"A2-Nets: Double Attention Networks---NIPS2018\"](https://arxiv.org/pdf/1810.11579.pdf)\r\n\r\n\r\n- Pytorch implementation of [\"An Attention Free Transformer---ICLR2021 (Apple New Work)\"](https://arxiv.org/pdf/2105.14103v1.pdf)\r\n\r\n\r\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24\"](https://arxiv.org/abs/2106.13112) \r\n  [„ÄêËÆ∫ÊñáËß£Êûê„Äë](https://zhuanlan.zhihu.com/p/385561050)\r\n\r\n\r\n- Pytorch implementation of [Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23](https://arxiv.org/abs/2106.12368) \r\n  [„ÄêËÆ∫ÊñáËß£Êûê„Äë](https://mp.weixin.qq.com/s/5gonUQgBho_m2O54jyXF_Q)\r\n\r\n\r\n- Pytorch implementation of [CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09](https://arxiv.org/abs/2106.04803) \r\n  [„ÄêËÆ∫ÊñáËß£Êûê„Äë](https://zhuanlan.zhihu.com/p/385578588)\r\n\r\n\r\n- Pytorch implementation of [Scaling Local Self-Attention for Parameter Efficient Visual Backbones---CVPR2021 Oral](https://arxiv.org/pdf/2103.12731.pdf)  [„ÄêËÆ∫ÊñáËß£Êûê„Äë](https://zhuanlan.zhihu.com/p/388598744)\r\n\r\n\r\n\r\n- Pytorch implementation of [Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02](https://arxiv.org/abs/2107.00782)  [„ÄêËÆ∫ÊñáËß£Êûê„Äë](https://zhuanlan.zhihu.com/p/389770482) \r\n\r\n\r\n- Pytorch implementation of [Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292)  [„ÄêËÆ∫ÊñáËß£Êûê„Äë](https://zhuanlan.zhihu.com/p/394795481) \r\n\r\n\r\n- Pytorch implementation of [Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \r\n\r\n\r\n- Pytorch implementation of [S¬≤-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) [„ÄêËÆ∫ÊñáËß£Êûê„Äë](https://zhuanlan.zhihu.com/p/397003638) \r\n\r\n- Pytorch implementation of [Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \r\n\r\n- Pytorch implementation of [Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021](https://arxiv.org/abs/2010.03045) \r\n\r\n- Pytorch implementation of [Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021](https://arxiv.org/abs/2103.02907)\r\n\r\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2110.02178)\r\n\r\n- Pytorch implementation of [Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\r\n\r\n- Pytorch implementation of [UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2109.14382)\r\n\r\n- Pytorch implementation of [Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06](https://arxiv.org/abs/2206.02680)\r\n\r\n- Pytorch implementation of [On the Integration of Self-Attention and Convolution---ArXiv 2022.03.14](https://arxiv.org/pdf/2111.14556.pdf)\r\n\r\n- Pytorch implementation of [CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION---ICLR 2022](https://arxiv.org/pdf/2108.00154.pdf)\r\n\r\n- Pytorch implementation of [Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903)\r\n\r\n- Pytorch implementation of [CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\r\n\r\n- Pytorch implementation of [Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)\r\n***\r\n\r\n\r\n### 1. External Attention Usage\r\n#### 1.1. Paper\r\n[\"Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\"](https://arxiv.org/abs/2105.02358)\r\n\r\n#### 1.2. Overview\r\n![](./model/img/External_Attention.png)\r\n\r\n#### 1.3. Usage Code\r\n```python\r\nfrom model.attention.ExternalAttention import ExternalAttention\r\nimport torch\r\n\r\ninput=torch.randn(50,49,512)\r\nea = ExternalAttention(d_model=512,S=8)\r\noutput=ea(input)\r\nprint(output.shape)\r\n```\r\n\r\n***\r\n\r\n\r\n### 2. Self Attention Usage\r\n#### 2.1. Paper\r\n[\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf)\r\n\r\n#### 1.2. Overview\r\n![](./model/img/SA.png)\r\n\r\n#### 1.3. Usage Code\r\n```python\r\nfrom model.attention.SelfAttention import ScaledDotProductAttention\r\nimport torch\r\n\r\ninput=torch.randn(50,49,512)\r\nsa = ScaledDotProductAttention(d_model=512, d_k=512, d_v=512, h=8)\r\noutput=sa(input,input,input)\r\nprint(output.shape)\r\n```\r\n\r\n***\r\n\r\n### 3. Simplified Self Attention Usage\r\n#### 3.1. Paper\r\n[None]()\r\n\r\n#### 3.2. Overview\r\n![](./model/img/SSA.png)\r\n\r\n#### 3.3. Usage Code\r\n```python\r\nfrom model.attention.SimplifiedSelfAttention import SimplifiedScaledDotProductAttention\r\nimport torch\r\n\r\ninput=torch.randn(50,49,512)\r\nssa = SimplifiedScaledDotProductAttention(d_model=512, h=8)\r\noutput=ssa(input,input,input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n### 4. Squeeze-and-Excitation Attention Usage\r\n#### 4.1. Paper\r\n[\"Squeeze-and-Excitation Networks\"](https://arxiv.org/abs/1709.01507)\r\n\r\n#### 4.2. Overview\r\n![](./model/img/SE.png)\r\n\r\n#### 4.3. Usage Code\r\n```python\r\nfrom model.attention.SEAttention import SEAttention\r\nimport torch\r\n\r\ninput=torch.randn(50,512,7,7)\r\nse = SEAttention(channel=512,reduction=8)\r\noutput=se(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n### 5. SK Attention Usage\r\n#### 5.1. Paper\r\n[\"Selective Kernel Networks\"](https://arxiv.org/pdf/1903.06586.pdf)\r\n\r\n#### 5.2. Overview\r\n![](./model/img/SK.png)\r\n\r\n#### 5.3. Usage Code\r\n```python\r\nfrom model.attention.SKAttention import SKAttention\r\nimport torch\r\n\r\ninput=torch.randn(50,512,7,7)\r\nse = SKAttention(channel=512,reduction=8)\r\noutput=se(input)\r\nprint(output.shape)\r\n\r\n```\r\n***\r\n\r\n### 6. CBAM Attention Usage\r\n#### 6.1. Paper\r\n[\"CBAM: Convolutional Block Attention Module\"](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)\r\n\r\n#### 6.2. Overview\r\n![](./model/img/CBAM1.png)\r\n\r\n![](./model/img/CBAM2.png)\r\n\r\n#### 6.3. Usage Code\r\n```python\r\nfrom model.attention.CBAM import CBAMBlock\r\nimport torch\r\n\r\ninput=torch.randn(50,512,7,7)\r\nkernel_size=input.shape[2]\r\ncbam = CBAMBlock(channel=512,reduction=16,kernel_size=kernel_size)\r\noutput=cbam(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n### 7. BAM Attention Usage\r\n#### 7.1. Paper\r\n[\"BAM: Bottleneck Attention Module\"](https://arxiv.org/pdf/1807.06514.pdf)\r\n\r\n#### 7.2. Overview\r\n![](./model/img/BAM.png)\r\n\r\n#### 7.3. Usage Code\r\n```python\r\nfrom model.attention.BAM import BAMBlock\r\nimport torch\r\n\r\ninput=torch.randn(50,512,7,7)\r\nbam = BAMBlock(channel=512,reduction=16,dia_val=2)\r\noutput=bam(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n### 8. ECA Attention Usage\r\n#### 8.1. Paper\r\n[\"ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\"](https://arxiv.org/pdf/1910.03151.pdf)\r\n\r\n#### 8.2. Overview\r\n![](./model/img/ECA.png)\r\n\r\n#### 8.3. Usage Code\r\n```python\r\nfrom model.attention.ECAAttention import ECAAttention\r\nimport torch\r\n\r\ninput=torch.randn(50,512,7,7)\r\neca = ECAAttention(kernel_size=3)\r\noutput=eca(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n### 9. DANet Attention Usage\r\n#### 9.1. Paper\r\n[\"Dual Attention Network for Scene Segmentation\"](https://arxiv.org/pdf/1809.02983.pdf)\r\n\r\n#### 9.2. Overview\r\n![](./model/img/danet.png)\r\n\r\n#### 9.3. Usage Code\r\n```python\r\nfrom model.attention.DANet import DAModule\r\nimport torch\r\n\r\ninput=torch.randn(50,512,7,7)\r\ndanet=DAModule(d_model=512,kernel_size=3,H=7,W=7)\r\nprint(danet(input).shape)\r\n\r\n```\r\n\r\n***\r\n\r\n### 10. Pyramid Split Attention Usage\r\n\r\n#### 10.1. Paper\r\n[\"EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network\"](https://arxiv.org/pdf/2105.14447.pdf)\r\n\r\n#### 10.2. Overview\r\n![](./model/img/psa.png)\r\n\r\n#### 10.3. Usage Code\r\n```python\r\nfrom model.attention.PSA import PSA\r\nimport torch\r\n\r\ninput=torch.randn(50,512,7,7)\r\npsa = PSA(channel=512,reduction=8)\r\noutput=psa(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n### 11. Efficient Multi-Head Self-Attention Usage\r\n\r\n#### 11.1. Paper\r\n[\"ResT: An Efficient Transformer for Visual Recognition\"](https://arxiv.org/abs/2105.13677)\r\n\r\n#### 11.2. Overview\r\n![](./model/img/EMSA.png)\r\n\r\n#### 11.3. Usage Code\r\n```python\r\n\r\nfrom model.attention.EMSA import EMSA\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(50,64,512)\r\nemsa = EMSA(d_model=512, d_k=512, d_v=512, h=8,H=8,W=8,ratio=2,apply_transform=True)\r\noutput=emsa(input,input,input)\r\nprint(output.shape)\r\n    \r\n```\r\n\r\n***\r\n\r\n\r\n### 12. Shuffle Attention Usage\r\n\r\n#### 12.1. Paper\r\n[\"SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS\"](https://arxiv.org/pdf/2102.00240.pdf)\r\n\r\n#### 12.2. Overview\r\n![](./model/img/ShuffleAttention.jpg)\r\n\r\n#### 12.3. Usage Code\r\n```python\r\n\r\nfrom model.attention.ShuffleAttention import ShuffleAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\n\r\ninput=torch.randn(50,512,7,7)\r\nse = ShuffleAttention(channel=512,G=8)\r\noutput=se(input)\r\nprint(output.shape)\r\n\r\n    \r\n```\r\n\r\n\r\n***\r\n\r\n\r\n### 13. MUSE Attention Usage\r\n\r\n#### 13.1. Paper\r\n[\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\"](https://arxiv.org/abs/1911.09483)\r\n\r\n#### 13.2. Overview\r\n![](./model/img/MUSE.png)\r\n\r\n#### 13.3. Usage Code\r\n```python\r\nfrom model.attention.MUSEAttention import MUSEAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\n\r\ninput=torch.randn(50,49,512)\r\nsa = MUSEAttention(d_model=512, d_k=512, d_v=512, h=8)\r\noutput=sa(input,input,input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n### 14. SGE Attention Usage\r\n\r\n#### 14.1. Paper\r\n[Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks](https://arxiv.org/pdf/1905.09646.pdf)\r\n\r\n#### 14.2. Overview\r\n![](./model/img/SGE.png)\r\n\r\n#### 14.3. Usage Code\r\n```python\r\nfrom model.attention.SGE import SpatialGroupEnhance\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(50,512,7,7)\r\nsge = SpatialGroupEnhance(groups=8)\r\noutput=sge(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n### 15. A2 Attention Usage\r\n\r\n#### 15.1. Paper\r\n[A2-Nets: Double Attention Networks](https://arxiv.org/pdf/1810.11579.pdf)\r\n\r\n#### 15.2. Overview\r\n![](./model/img/A2.png)\r\n\r\n#### 15.3. Usage Code\r\n```python\r\nfrom model.attention.A2Atttention import DoubleAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(50,512,7,7)\r\na2 = DoubleAttention(512,128,128,True)\r\noutput=a2(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n\r\n\r\n### 16. AFT Attention Usage\r\n\r\n#### 16.1. Paper\r\n[An Attention Free Transformer](https://arxiv.org/pdf/2105.14103v1.pdf)\r\n\r\n#### 16.2. Overview\r\n![](./model/img/AFT.jpg)\r\n\r\n#### 16.3. Usage Code\r\n```python\r\nfrom model.attention.AFT import AFT_FULL\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(50,49,512)\r\naft_full = AFT_FULL(d_model=512, n=49)\r\noutput=aft_full(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n### 17. Outlook Attention Usage\r\n\r\n#### 17.1. Paper\r\n\r\n\r\n[VOLO: Vision Outlooker for Visual Recognition\"](https://arxiv.org/abs/2106.13112)\r\n\r\n\r\n#### 17.2. Overview\r\n![](./model/img/OutlookAttention.png)\r\n\r\n#### 17.3. Usage Code\r\n```python\r\nfrom model.attention.OutlookAttention import OutlookAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(50,28,28,512)\r\noutlook = OutlookAttention(dim=512)\r\noutput=outlook(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n\r\n***\r\n\r\n\r\n\r\n\r\n\r\n\r\n### 18. ViP Attention Usage\r\n\r\n#### 18.1. Paper\r\n\r\n\r\n[Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\r\n\r\n\r\n#### 18.2. Overview\r\n![](./model/img/ViP.png)\r\n\r\n#### 18.3. Usage Code\r\n```python\r\n\r\nfrom model.attention.ViP import WeightedPermuteMLP\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(64,8,8,512)\r\nseg_dim=8\r\nvip=WeightedPermuteMLP(512,seg_dim)\r\nout=vip(input)\r\nprint(out.shape)\r\n\r\n```\r\n\r\n\r\n***\r\n\r\n\r\n\r\n\r\n\r\n### 19. CoAtNet Attention Usage\r\n\r\n#### 19.1. Paper\r\n\r\n\r\n[CoAtNet: Marrying Convolution and Attention for All Data Sizes\"](https://arxiv.org/abs/2106.04803) \r\n\r\n\r\n#### 19.2. Overview\r\nNone\r\n\r\n\r\n#### 19.3. Usage Code\r\n```python\r\n\r\nfrom model.attention.CoAtNet import CoAtNet\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,3,224,224)\r\nmbconv=CoAtNet(in_ch=3,image_size=224)\r\nout=mbconv(input)\r\nprint(out.shape)\r\n\r\n```\r\n\r\n\r\n***\r\n\r\n\r\n\r\n\r\n\r\n\r\n### 20. HaloNet Attention Usage\r\n\r\n#### 20.1. Paper\r\n\r\n\r\n[Scaling Local Self-Attention for Parameter Efficient Visual Backbones\"](https://arxiv.org/pdf/2103.12731.pdf) \r\n\r\n\r\n#### 20.2. Overview\r\n\r\n![](./model/img/HaloNet.png)\r\n\r\n#### 20.3. Usage Code\r\n```python\r\n\r\nfrom model.attention.HaloAttention import HaloAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,512,8,8)\r\nhalo = HaloAttention(dim=512,\r\n    block_size=2,\r\n    halo_size=1,)\r\noutput=halo(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n\r\n***\r\n\r\n### 21. Polarized Self-Attention Usage\r\n\r\n#### 21.1. Paper\r\n\r\n[Polarized Self-Attention: Towards High-quality Pixel-wise Regression\"](https://arxiv.org/abs/2107.00782)  \r\n\r\n\r\n#### 21.2. Overview\r\n\r\n![](./model/img/PoSA.png)\r\n\r\n#### 21.3. Usage Code\r\n```python\r\n\r\nfrom model.attention.PolarizedSelfAttention import ParallelPolarizedSelfAttention,SequentialPolarizedSelfAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,512,7,7)\r\npsa = SequentialPolarizedSelfAttention(channel=512)\r\noutput=psa(input)\r\nprint(output.shape)\r\n\r\n\r\n```\r\n\r\n\r\n***\r\n\r\n\r\n### 22. CoTAttention Usage\r\n\r\n#### 22.1. Paper\r\n\r\n[Contextual Transformer Networks for Visual Recognition---arXiv 2021.07.26](https://arxiv.org/abs/2107.12292) \r\n\r\n\r\n#### 22.2. Overview\r\n\r\n![](./model/img/CoT.png)\r\n\r\n#### 22.3. Usage Code\r\n```python\r\n\r\nfrom model.attention.CoTAttention import CoTAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(50,512,7,7)\r\ncot = CoTAttention(dim=512,kernel_size=3)\r\noutput=cot(input)\r\nprint(output.shape)\r\n\r\n\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n### 23. Residual Attention Usage\r\n\r\n#### 23.1. Paper\r\n\r\n[Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) \r\n\r\n\r\n#### 23.2. Overview\r\n\r\n![](./model/img/ResAtt.png)\r\n\r\n#### 23.3. Usage Code\r\n```python\r\n\r\nfrom model.attention.ResidualAttention import ResidualAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(50,512,7,7)\r\nresatt = ResidualAttention(channel=512,num_class=1000,la=0.2)\r\noutput=resatt(input)\r\nprint(output.shape)\r\n\r\n\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n\r\n### 24. S2 Attention Usage\r\n\r\n#### 24.1. Paper\r\n\r\n[S¬≤-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072) \r\n\r\n\r\n#### 24.2. Overview\r\n\r\n![](./model/img/S2Attention.png)\r\n\r\n#### 24.3. Usage Code\r\n```python\r\nfrom model.attention.S2Attention import S2Attention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(50,512,7,7)\r\ns2att = S2Attention(channels=512)\r\noutput=s2att(input)\r\nprint(output.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n\r\n### 25. GFNet Attention Usage\r\n\r\n#### 25.1. Paper\r\n\r\n[Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) \r\n\r\n\r\n#### 25.2. Overview\r\n\r\n![](./model/img/GFNet.jpg)\r\n\r\n#### 25.3. Usage Code - Implemented by [Wenliang Zhao (Author)](https://scholar.google.com/citations?user=lyPWvuEAAAAJ&hl=en)\r\n\r\n```python\r\nfrom model.attention.gfnet import GFNet\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nx = torch.randn(1, 3, 224, 224)\r\ngfnet = GFNet(embed_dim=384, img_size=224, patch_size=16, num_classes=1000)\r\nout = gfnet(x)\r\nprint(out.shape)\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n### 26. TripletAttention Usage\r\n\r\n#### 26.1. Paper\r\n\r\n[Rotate to Attend: Convolutional Triplet Attention Module---CVPR 2021](https://arxiv.org/abs/2010.03045) \r\n\r\n#### 26.2. Overview\r\n\r\n![](./model/img/triplet.png)\r\n\r\n#### 26.3. Usage Code - Implemented by [digantamisra98](https://github.com/digantamisra98)\r\n\r\n```python\r\nfrom model.attention.TripletAttention import TripletAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\ninput=torch.randn(50,512,7,7)\r\ntriplet = TripletAttention()\r\noutput=triplet(input)\r\nprint(output.shape)\r\n```\r\n\r\n\r\n***\r\n\r\n\r\n### 27. Coordinate Attention Usage\r\n\r\n#### 27.1. Paper\r\n\r\n[Coordinate Attention for Efficient Mobile Network Design---CVPR 2021](https://arxiv.org/abs/2103.02907)\r\n\r\n\r\n#### 27.2. Overview\r\n\r\n![](./model/img/CoordAttention.png)\r\n\r\n#### 27.3. Usage Code - Implemented by [Andrew-Qibin](https://github.com/Andrew-Qibin)\r\n\r\n```python\r\nfrom model.attention.CoordAttention import CoordAtt\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninp=torch.rand([2, 96, 56, 56])\r\ninp_dim, oup_dim = 96, 96\r\nreduction=32\r\n\r\ncoord_attention = CoordAtt(inp_dim, oup_dim, reduction=reduction)\r\noutput=coord_attention(inp)\r\nprint(output.shape)\r\n```\r\n\r\n***\r\n\r\n\r\n### 28. MobileViT Attention Usage\r\n\r\n#### 28.1. Paper\r\n\r\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2103.02907)\r\n\r\n\r\n#### 28.2. Overview\r\n\r\n![](./model/img/MobileViTAttention.png)\r\n\r\n#### 28.3. Usage Code\r\n\r\n```python\r\nfrom model.attention.MobileViTAttention import MobileViTAttention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    m=MobileViTAttention()\r\n    input=torch.randn(1,3,49,49)\r\n    output=m(input)\r\n    print(output.shape)  #output:(1,3,49,49)\r\n    \r\n```\r\n\r\n***\r\n\r\n\r\n### 29. ParNet Attention Usage\r\n\r\n#### 29.1. Paper\r\n\r\n[Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)\r\n\r\n\r\n#### 29.2. Overview\r\n\r\n![](./model/img/ParNet.png)\r\n\r\n#### 29.3. Usage Code\r\n\r\n```python\r\nfrom model.attention.ParNetAttention import *\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(50,512,7,7)\r\n    pna = ParNetAttention(channel=512)\r\n    output=pna(input)\r\n    print(output.shape) #50,512,7,7\r\n    \r\n```\r\n\r\n***\r\n\r\n\r\n### 30. UFO Attention Usage\r\n\r\n#### 30.1. Paper\r\n\r\n[UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2110.07641)\r\n\r\n\r\n#### 30.2. Overview\r\n\r\n![](./model/img/UFO.png)\r\n\r\n#### 30.3. Usage Code\r\n\r\n```python\r\nfrom model.attention.UFOAttention import *\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(50,49,512)\r\n    ufo = UFOAttention(d_model=512, d_k=512, d_v=512, h=8)\r\n    output=ufo(input,input,input)\r\n    print(output.shape) #[50, 49, 512]\r\n    \r\n```\r\n\r\n-\r\n\r\n### 31. ACmix Attention Usage\r\n\r\n#### 31.1. Paper\r\n\r\n[On the Integration of Self-Attention and Convolution](https://arxiv.org/pdf/2111.14556.pdf)\r\n\r\n#### 31.2. Usage Code\r\n\r\n```python\r\nfrom model.attention.ACmix import ACmix\r\nimport torch\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(50,256,7,7)\r\n    acmix = ACmix(in_planes=256, out_planes=256)\r\n    output=acmix(input)\r\n    print(output.shape)\r\n    \r\n```\r\n\r\n### 32. MobileViTv2 Attention Usage\r\n\r\n#### 32.1. Paper\r\n\r\n[Separable Self-attention for Mobile Vision Transformers---ArXiv 2022.06.06](https://arxiv.org/abs/2206.02680)\r\n\r\n\r\n#### 32.2. Overview\r\n\r\n![](./model/img/MobileViTv2.png)\r\n\r\n#### 32.3. Usage Code\r\n\r\n```python\r\nfrom model.attention.MobileViTv2Attention import MobileViTv2Attention\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(50,49,512)\r\n    sa = MobileViTv2Attention(d_model=512)\r\n    output=sa(input)\r\n    print(output.shape)\r\n    \r\n```\r\n\r\n### 33. DAT Attention Usage\r\n\r\n#### 33.1. Paper\r\n\r\n[Vision Transformer with Deformable Attention---CVPR2022](https://arxiv.org/abs/2201.00520)\r\n\r\n#### 33.2. Usage Code\r\n\r\n```python\r\nfrom model.attention.DAT import DAT\r\nimport torch\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = DAT(\r\n        img_size=224,\r\n        patch_size=4,\r\n        num_classes=1000,\r\n        expansion=4,\r\n        dim_stem=96,\r\n        dims=[96, 192, 384, 768],\r\n        depths=[2, 2, 6, 2],\r\n        stage_spec=[['L', 'S'], ['L', 'S'], ['L', 'D', 'L', 'D', 'L', 'D'], ['L', 'D']],\r\n        heads=[3, 6, 12, 24],\r\n        window_sizes=[7, 7, 7, 7] ,\r\n        groups=[-1, -1, 3, 6],\r\n        use_pes=[False, False, True, True],\r\n        dwc_pes=[False, False, False, False],\r\n        strides=[-1, -1, 1, 1],\r\n        sr_ratios=[-1, -1, -1, -1],\r\n        offset_range_factor=[-1, -1, 2, 2],\r\n        no_offs=[False, False, False, False],\r\n        fixed_pes=[False, False, False, False],\r\n        use_dwc_mlps=[False, False, False, False],\r\n        use_conv_patches=False,\r\n        drop_rate=0.0,\r\n        attn_drop_rate=0.0,\r\n        drop_path_rate=0.2,\r\n    )\r\n    output=model(input)\r\n    print(output[0].shape)\r\n    \r\n```\r\n\r\n### 34. CrossFormer Attention Usage\r\n\r\n#### 34.1. Paper\r\n\r\n[CROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING ON CROSS-SCALE ATTENTION---ICLR 2022](https://arxiv.org/pdf/2108.00154.pdf)\r\n\r\n#### 34.2. Usage Code\r\n\r\n```python\r\nfrom model.attention.Crossformer import CrossFormer\r\nimport torch\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = CrossFormer(img_size=224,\r\n        patch_size=[4, 8, 16, 32],\r\n        in_chans= 3,\r\n        num_classes=1000,\r\n        embed_dim=48,\r\n        depths=[2, 2, 6, 2],\r\n        num_heads=[3, 6, 12, 24],\r\n        group_size=[7, 7, 7, 7],\r\n        mlp_ratio=4.,\r\n        qkv_bias=True,\r\n        qk_scale=None,\r\n        drop_rate=0.0,\r\n        drop_path_rate=0.1,\r\n        ape=False,\r\n        patch_norm=True,\r\n        use_checkpoint=False,\r\n        merge_size=[[2, 4], [2,4], [2, 4]]\r\n    )\r\n    output=model(input)\r\n    print(output.shape)\r\n    \r\n```\r\n\r\n### 35. MOATransformer Attention Usage\r\n\r\n#### 35.1. Paper\r\n\r\n[Aggregating Global Features into Local Vision Transformer](https://arxiv.org/abs/2201.12903)\r\n\r\n#### 35.2. Usage Code\r\n\r\n```python\r\nfrom model.attention.MOATransformer import MOATransformer\r\nimport torch\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = MOATransformer(\r\n        img_size=224,\r\n        patch_size=4,\r\n        in_chans=3,\r\n        num_classes=1000,\r\n        embed_dim=96,\r\n        depths=[2, 2, 6],\r\n        num_heads=[3, 6, 12],\r\n        window_size=14,\r\n        mlp_ratio=4.,\r\n        qkv_bias=True,\r\n        qk_scale=None,\r\n        drop_rate=0.0,\r\n        drop_path_rate=0.1,\r\n        ape=False,\r\n        patch_norm=True,\r\n        use_checkpoint=False\r\n    )\r\n    output=model(input)\r\n    print(output.shape)\r\n    \r\n```\r\n\r\n### 36. CrissCrossAttention Attention Usage\r\n\r\n#### 36.1. Paper\r\n\r\n[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)\r\n\r\n#### 36.2. Usage Code\r\n\r\n```python\r\nfrom model.attention.CrissCrossAttention import CrissCrossAttention\r\nimport torch\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(3, 64, 7, 7)\r\n    model = CrissCrossAttention(64)\r\n    outputs = model(input)\r\n    print(outputs.shape)\r\n    \r\n```\r\n\r\n### 37. Axial_attention Attention Usage\r\n\r\n#### 37.1. Paper\r\n\r\n[Axial Attention in Multidimensional Transformers](https://arxiv.org/abs/1912.12180)\r\n\r\n#### 37.2. Usage Code\r\n\r\n```python\r\nfrom model.attention.Axial_attention import AxialImageTransformer\r\nimport torch\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(3, 128, 7, 7)\r\n    model = AxialImageTransformer(\r\n        dim = 128,\r\n        depth = 12,\r\n        reversible = True\r\n    )\r\n    outputs = model(input)\r\n    print(outputs.shape)\r\n    \r\n```\r\n\r\n***\r\n\r\n\r\n# Backbone Series\r\n\r\n- Pytorch implementation of [\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\r\n\r\n- Pytorch implementation of [\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\r\n\r\n- Pytorch implementation of [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\r\n\r\n- Pytorch implementation of [Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\r\n\r\n- Pytorch implementation of [Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer---ArXiv 2021.06.07](https://arxiv.org/abs/2106.03650)\r\n\r\n- Pytorch implementation of [ConTNet: Why not use convolution and transformer at the same time?---ArXiv 2021.04.27](https://arxiv.org/abs/2104.13497)\r\n\r\n- Pytorch implementation of [Vision Transformers with Hierarchical Attention---ArXiv 2022.06.15](https://arxiv.org/abs/2106.03180)\r\n\r\n- Pytorch implementation of [Co-Scale Conv-Attentional Image Transformers---ArXiv 2021.08.26](https://arxiv.org/abs/2104.06399)\r\n\r\n- Pytorch implementation of [Conditional Positional Encodings for Vision Transformers](https://arxiv.org/abs/2102.10882)\r\n\r\n- Pytorch implementation of [Rethinking Spatial Dimensions of Vision Transformers---ICCV 2021](https://arxiv.org/abs/2103.16302)\r\n\r\n- Pytorch implementation of [CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification---ICCV 2021](https://arxiv.org/abs/2103.14899)\r\n\r\n- Pytorch implementation of [Transformer in Transformer---NeurIPS 2021](https://arxiv.org/abs/2103.00112)\r\n\r\n- Pytorch implementation of [DeepViT: Towards Deeper Vision Transformer](https://arxiv.org/abs/2103.11886)\r\n\r\n- Pytorch implementation of [Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)\r\n***\r\n\r\n- Pytorch implementation of [ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)\r\n\r\n- Pytorch implementation of [Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\r\n\r\n- Pytorch implementation of [Going deeper with Image Transformers---ICCV 2021 (Oral)](https://arxiv.org/abs/2103.17239)\r\n\r\n- Pytorch implementation of [Training data-efficient image transformers & distillation through attention---ICML 2021](https://arxiv.org/abs/2012.12877)\r\n\r\n- Pytorch implementation of [LeViT: a Vision Transformer in ConvNet‚Äôs Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\r\n\r\n- Pytorch implementation of [VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/abs/2106.13112)\r\n\r\n- Pytorch implementation of [Container: Context Aggregation Network---NeuIPS 2021](https://arxiv.org/abs/2106.01401)\r\n\r\n- Pytorch implementation of [CMT: Convolutional Neural Networks Meet Vision Transformers---CVPR 2022](https://arxiv.org/abs/2107.06263)\r\n\r\n- Pytorch implementation of [Vision Transformer with Deformable Attention---CVPR 2022](https://arxiv.org/abs/2201.00520)\r\n\r\n- Pytorch implementation of [EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)\r\n\r\n\r\n### 1. ResNet Usage\r\n#### 1.1. Paper\r\n[\"Deep Residual Learning for Image Recognition---CVPR2016 Best Paper\"](https://arxiv.org/pdf/1512.03385.pdf)\r\n\r\n#### 1.2. Overview\r\n![](./model/img/resnet.png)\r\n![](./model/img/resnet2.jpg)\r\n\r\n#### 1.3. Usage Code\r\n```python\r\n\r\nfrom model.backbone.resnet import ResNet50,ResNet101,ResNet152\r\nimport torch\r\nif __name__ == '__main__':\r\n    input=torch.randn(50,3,224,224)\r\n    resnet50=ResNet50(1000)\r\n    # resnet101=ResNet101(1000)\r\n    # resnet152=ResNet152(1000)\r\n    out=resnet50(input)\r\n    print(out.shape)\r\n\r\n```\r\n\r\n\r\n### 2. ResNeXt Usage\r\n#### 2.1. Paper\r\n\r\n[\"Aggregated Residual Transformations for Deep Neural Networks---CVPR2017\"](https://arxiv.org/abs/1611.05431v2)\r\n\r\n#### 2.2. Overview\r\n![](./model/img/resnext.png)\r\n\r\n#### 2.3. Usage Code\r\n```python\r\n\r\nfrom model.backbone.resnext import ResNeXt50,ResNeXt101,ResNeXt152\r\nimport torch\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(50,3,224,224)\r\n    resnext50=ResNeXt50(1000)\r\n    # resnext101=ResNeXt101(1000)\r\n    # resnext152=ResNeXt152(1000)\r\n    out=resnext50(input)\r\n    print(out.shape)\r\n\r\n\r\n```\r\n\r\n\r\n\r\n### 3. MobileViT Usage\r\n#### 3.1. Paper\r\n\r\n[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2020.10.05](https://arxiv.org/abs/2103.02907)\r\n\r\n#### 3.2. Overview\r\n![](./model/img/mobileViT.jpg)\r\n\r\n#### 3.3. Usage Code\r\n```python\r\n\r\nfrom model.backbone.MobileViT import *\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n\r\n    ### mobilevit_xxs\r\n    mvit_xxs=mobilevit_xxs()\r\n    out=mvit_xxs(input)\r\n    print(out.shape)\r\n\r\n    ### mobilevit_xs\r\n    mvit_xs=mobilevit_xs()\r\n    out=mvit_xs(input)\r\n    print(out.shape)\r\n\r\n\r\n    ### mobilevit_s\r\n    mvit_s=mobilevit_s()\r\n    out=mvit_s(input)\r\n    print(out.shape)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n### 4. ConvMixer Usage\r\n#### 4.1. Paper\r\n[Patches Are All You Need?---ICLR2022 (Under Review)](https://openreview.net/forum?id=TVHS5Y4dNvM)\r\n#### 4.2. Overview\r\n![](./model/img/ConvMixer.png)\r\n\r\n#### 4.3. Usage Code\r\n```python\r\n\r\nfrom model.backbone.ConvMixer import *\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    x=torch.randn(1,3,224,224)\r\n    convmixer=ConvMixer(dim=512,depth=12)\r\n    out=convmixer(x)\r\n    print(out.shape)  #[1, 1000]\r\n\r\n\r\n```\r\n\r\n### 5. ShuffleTransformer Usage\r\n#### 5.1. Paper\r\n[Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer](https://arxiv.org/pdf/2106.03650.pdf)\r\n\r\n#### 5.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.ShuffleTransformer import ShuffleTransformer\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    sft = ShuffleTransformer()\r\n    output=sft(input)\r\n    print(output.shape)\r\n\r\n\r\n```\r\n\r\n### 6. ConTNet Usage\r\n#### 6.1. Paper\r\n[ConTNet: Why not use convolution and transformer at the same time?](https://arxiv.org/abs/2104.13497)\r\n\r\n#### 6.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.ConTNet import ConTNet\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == \"__main__\":\r\n    model = build_model(use_avgdown=True, relative=True, qkv_bias=True, pre_norm=True)\r\n    input = torch.randn(1, 3, 224, 224)\r\n    out = model(input)\r\n    print(out.shape)\r\n\r\n\r\n```\r\n\r\n### 7 HATNet Usage\r\n#### 7.1. Paper\r\n[Vision Transformers with Hierarchical Attention](https://arxiv.org/abs/2106.03180)\r\n\r\n#### 7.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.HATNet import HATNet\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    hat = HATNet(dims=[48, 96, 240, 384], head_dim=48, expansions=[8, 8, 4, 4],\r\n        grid_sizes=[8, 7, 7, 1], ds_ratios=[8, 4, 2, 1], depths=[2, 2, 6, 3])\r\n    output=hat(input)\r\n    print(output.shape)\r\n\r\n\r\n```\r\n\r\n### 8 CoaT Usage\r\n#### 8.1. Paper\r\n[Co-Scale Conv-Attentional Image Transformers](https://arxiv.org/abs/2104.06399)\r\n\r\n#### 8.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.CoaT import CoaT\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = CoaT(patch_size=4, embed_dims=[152, 152, 152, 152], serial_depths=[2, 2, 2, 2], parallel_depth=6, num_heads=8, mlp_ratios=[4, 4, 4, 4])\r\n    output=model(input)\r\n    print(output.shape) # torch.Size([1, 1000])\r\n\r\n```\r\n\r\n### 9 PVT Usage\r\n#### 9.1. Paper\r\n[PVT v2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/pdf/2106.13797.pdf)\r\n\r\n#### 9.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.PVT import PyramidVisionTransformer\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = PyramidVisionTransformer(\r\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1])\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n\r\n### 10 CPVT Usage\r\n#### 10.1. Paper\r\n[Conditional Positional Encodings for Vision Transformers](https://arxiv.org/abs/2102.10882)\r\n\r\n#### 10.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.CPVT import CPVTV2\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = CPVTV2(\r\n        patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[8, 8, 4, 4], qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1])\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 11 PIT Usage\r\n#### 11.1. Paper\r\n[Rethinking Spatial Dimensions of Vision Transformers](https://arxiv.org/abs/2103.16302)\r\n\r\n#### 11.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.PIT import PoolingTransformer\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = PoolingTransformer(\r\n        image_size=224,\r\n        patch_size=14,\r\n        stride=7,\r\n        base_dims=[64, 64, 64],\r\n        depth=[3, 6, 4],\r\n        heads=[4, 8, 16],\r\n        mlp_ratio=4\r\n    )\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 12 CrossViT Usage\r\n#### 12.1. Paper\r\n[CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification](https://arxiv.org/abs/2103.14899)\r\n\r\n#### 12.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.CrossViT import VisionTransformer\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == \"__main__\":\r\n    input=torch.randn(1,3,224,224)\r\n    model = VisionTransformer(\r\n        img_size=[240, 224],\r\n        patch_size=[12, 16], \r\n        embed_dim=[192, 384], \r\n        depth=[[1, 4, 0], [1, 4, 0], [1, 4, 0]],\r\n        num_heads=[6, 6], \r\n        mlp_ratio=[4, 4, 1], \r\n        qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\r\n    )\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 13 TnT Usage\r\n#### 13.1. Paper\r\n[Transformer in Transformer](https://arxiv.org/abs/2103.00112)\r\n\r\n#### 13.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.TnT import TNT\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = TNT(\r\n        img_size=224, \r\n        patch_size=16, \r\n        outer_dim=384, \r\n        inner_dim=24, \r\n        depth=12,\r\n        outer_num_heads=6, \r\n        inner_num_heads=4, \r\n        qkv_bias=False,\r\n        inner_stride=4)\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 14 DViT Usage\r\n#### 14.1. Paper\r\n[DeepViT: Towards Deeper Vision Transformer](https://arxiv.org/abs/2103.11886)\r\n\r\n#### 14.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.DViT import DeepVisionTransformer\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = DeepVisionTransformer(\r\n        patch_size=16, embed_dim=384, \r\n        depth=[False] * 16, \r\n        apply_transform=[False] * 0 + [True] * 32, \r\n        num_heads=12, \r\n        mlp_ratio=3, \r\n        qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\r\n        )\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 15 CeiT Usage\r\n#### 15.1. Paper\r\n[Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)\r\n\r\n#### 15.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.CeiT import CeIT\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = CeIT(\r\n        hybrid_backbone=Image2Tokens(),\r\n        patch_size=4, \r\n        embed_dim=192, \r\n        depth=12, \r\n        num_heads=3, \r\n        mlp_ratio=4, \r\n        qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\r\n        )\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 16 ConViT Usage\r\n#### 16.1. Paper\r\n[ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases](https://arxiv.org/abs/2103.10697)\r\n\r\n#### 16.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.ConViT import VisionTransformer\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = VisionTransformer(\r\n        num_heads=16,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\r\n        )\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 17 CaiT Usage\r\n#### 17.1. Paper\r\n[Going deeper with Image Transformers](https://arxiv.org/abs/2103.17239)\r\n\r\n#### 17.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.CaiT import CaiT\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = CaiT(\r\n        img_size= 224,\r\n        patch_size=16, \r\n        embed_dim=192, \r\n        depth=24, \r\n        num_heads=4, \r\n        mlp_ratio=4, \r\n        qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\r\n        init_scale=1e-5,\r\n        depth_token_only=2\r\n        )\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 18 PatchConvnet Usage\r\n#### 18.1. Paper\r\n[Augmenting Convolutional networks with attention-based aggregation](https://arxiv.org/abs/2112.13692)\r\n\r\n#### 18.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.PatchConvnet import PatchConvnet\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = PatchConvnet(\r\n        patch_size=16,\r\n        embed_dim=384,\r\n        depth=60,\r\n        num_heads=1,\r\n        qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\r\n        Patch_layer=ConvStem,\r\n        Attention_block=Conv_blocks_se,\r\n        depth_token_only=1,\r\n        mlp_ratio_clstk=3.0,\r\n    )\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 19 DeiT Usage\r\n#### 19.1. Paper\r\n[Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\r\n\r\n#### 19.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.DeiT import DistilledVisionTransformer\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = DistilledVisionTransformer(\r\n        patch_size=16, \r\n        embed_dim=384, \r\n        depth=12, \r\n        num_heads=6, \r\n        mlp_ratio=4, \r\n        qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6)\r\n        )\r\n    output=model(input)\r\n    print(output[0].shape)\r\n\r\n```\r\n\r\n### 20 LeViT Usage\r\n#### 20.1. Paper\r\n[LeViT: a Vision Transformer in ConvNet‚Äôs Clothing for Faster Inference](https://arxiv.org/abs/2104.01136)\r\n\r\n#### 20.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.LeViT import *\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    for name in specification:\r\n        input=torch.randn(1,3,224,224)\r\n        model = globals()[name](fuse=True, pretrained=False)\r\n        model.eval()\r\n        output = model(input)\r\n        print(output.shape)\r\n\r\n```\r\n\r\n### 21 VOLO Usage\r\n#### 21.1. Paper\r\n[VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/abs/2106.13112)\r\n\r\n#### 21.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.VOLO import VOLO\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = VOLO([4, 4, 8, 2],\r\n                 embed_dims=[192, 384, 384, 384],\r\n                 num_heads=[6, 12, 12, 12],\r\n                 mlp_ratios=[3, 3, 3, 3],\r\n                 downsamples=[True, False, False, False],\r\n                 outlook_attention=[True, False, False, False ],\r\n                 post_layers=['ca', 'ca'],\r\n                 )\r\n    output=model(input)\r\n    print(output[0].shape)\r\n\r\n```\r\n\r\n### 22 Container Usage\r\n#### 22.1. Paper\r\n[Container: Context Aggregation Network](https://arxiv.org/abs/2106.01401)\r\n\r\n#### 22.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.Container import VisionTransformer\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = VisionTransformer(\r\n        img_size=[224, 56, 28, 14], \r\n        patch_size=[4, 2, 2, 2], \r\n        embed_dim=[64, 128, 320, 512], \r\n        depth=[3, 4, 8, 3], \r\n        num_heads=16, \r\n        mlp_ratio=[8, 8, 4, 4], \r\n        qkv_bias=True,\r\n        norm_layer=partial(nn.LayerNorm, eps=1e-6))\r\n    output=model(input)\r\n    print(output.shape)\r\n\r\n```\r\n\r\n### 23 CMT Usage\r\n#### 23.1. Paper\r\n[CMT: Convolutional Neural Networks Meet Vision Transformers](https://arxiv.org/abs/2107.06263)\r\n\r\n#### 23.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.CMT import CMT_Tiny\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = CMT_Tiny()\r\n    output=model(input)\r\n    print(output[0].shape)\r\n\r\n```\r\n\r\n### 24 EfficientFormer Usage\r\n#### 24.1. Paper\r\n[EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)\r\n\r\n#### 24.2. Usage Code\r\n```python\r\n\r\nfrom model.backbone.EfficientFormer import EfficientFormer\r\nimport torch\r\nfrom torch import nn\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = EfficientFormer(\r\n        layers=EfficientFormer_depth['l1'],\r\n        embed_dims=EfficientFormer_width['l1'],\r\n        downsamples=[True, True, True, True],\r\n        vit_num=1,\r\n    )\r\n    output=model(input)\r\n    print(output[0].shape)\r\n\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n# MLP Series\r\n\r\n- Pytorch implementation of [\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition---arXiv 2021.05.05\"](https://arxiv.org/pdf/2105.01883v1.pdf)\r\n\r\n- Pytorch implementation of [\"MLP-Mixer: An all-MLP Architecture for Vision---arXiv 2021.05.17\"](https://arxiv.org/pdf/2105.01601.pdf)\r\n\r\n- Pytorch implementation of [\"ResMLP: Feedforward networks for image classification with data-efficient training---arXiv 2021.05.07\"](https://arxiv.org/pdf/2105.03404.pdf)\r\n\r\n- Pytorch implementation of [\"Pay Attention to MLPs---arXiv 2021.05.17\"](https://arxiv.org/abs/2105.08050)\r\n\r\n\r\n- Pytorch implementation of [\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?---arXiv 2021.09.12\"](https://arxiv.org/abs/2109.05422)\r\n\r\n### 1. RepMLP Usage\r\n#### 1.1. Paper\r\n[\"RepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition\"](https://arxiv.org/pdf/2105.01883v1.pdf)\r\n\r\n#### 1.2. Overview\r\n![](./model/img/repmlp.png)\r\n\r\n#### 1.3. Usage Code\r\n```python\r\nfrom model.mlp.repmlp import RepMLP\r\nimport torch\r\nfrom torch import nn\r\n\r\nN=4 #batch size\r\nC=512 #input dim\r\nO=1024 #output dim\r\nH=14 #image height\r\nW=14 #image width\r\nh=7 #patch height\r\nw=7 #patch width\r\nfc1_fc2_reduction=1 #reduction ratio\r\nfc3_groups=8 # groups\r\nrepconv_kernels=[1,3,5,7] #kernel list\r\nrepmlp=RepMLP(C,O,H,W,h,w,fc1_fc2_reduction,fc3_groups,repconv_kernels=repconv_kernels)\r\nx=torch.randn(N,C,H,W)\r\nrepmlp.eval()\r\nfor module in repmlp.modules():\r\n    if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d):\r\n        nn.init.uniform_(module.running_mean, 0, 0.1)\r\n        nn.init.uniform_(module.running_var, 0, 0.1)\r\n        nn.init.uniform_(module.weight, 0, 0.1)\r\n        nn.init.uniform_(module.bias, 0, 0.1)\r\n\r\n#training result\r\nout=repmlp(x)\r\n#inference result\r\nrepmlp.switch_to_deploy()\r\ndeployout = repmlp(x)\r\n\r\nprint(((deployout-out)**2).sum())\r\n```\r\n\r\n### 2. MLP-Mixer Usage\r\n#### 2.1. Paper\r\n[\"MLP-Mixer: An all-MLP Architecture for Vision\"](https://arxiv.org/pdf/2105.01601.pdf)\r\n\r\n#### 2.2. Overview\r\n![](./model/img/mlpmixer.png)\r\n\r\n#### 2.3. Usage Code\r\n```python\r\nfrom model.mlp.mlp_mixer import MlpMixer\r\nimport torch\r\nmlp_mixer=MlpMixer(num_classes=1000,num_blocks=10,patch_size=10,tokens_hidden_dim=32,channels_hidden_dim=1024,tokens_mlp_dim=16,channels_mlp_dim=1024)\r\ninput=torch.randn(50,3,40,40)\r\noutput=mlp_mixer(input)\r\nprint(output.shape)\r\n```\r\n\r\n***\r\n\r\n### 3. ResMLP Usage\r\n#### 3.1. Paper\r\n[\"ResMLP: Feedforward networks for image classification with data-efficient training\"](https://arxiv.org/pdf/2105.03404.pdf)\r\n\r\n#### 3.2. Overview\r\n![](./model/img/resmlp.png)\r\n\r\n#### 3.3. Usage Code\r\n```python\r\nfrom model.mlp.resmlp import ResMLP\r\nimport torch\r\n\r\ninput=torch.randn(50,3,14,14)\r\nresmlp=ResMLP(dim=128,image_size=14,patch_size=7,class_num=1000)\r\nout=resmlp(input)\r\nprint(out.shape) #the last dimention is class_num\r\n```\r\n\r\n***\r\n\r\n### 4. gMLP Usage\r\n#### 4.1. Paper\r\n[\"Pay Attention to MLPs\"](https://arxiv.org/abs/2105.08050)\r\n\r\n#### 4.2. Overview\r\n![](./model/img/gMLP.jpg)\r\n\r\n#### 4.3. Usage Code\r\n```python\r\nfrom model.mlp.g_mlp import gMLP\r\nimport torch\r\n\r\nnum_tokens=10000\r\nbs=50\r\nlen_sen=49\r\nnum_layers=6\r\ninput=torch.randint(num_tokens,(bs,len_sen)) #bs,len_sen\r\ngmlp = gMLP(num_tokens=num_tokens,len_sen=len_sen,dim=512,d_ff=1024)\r\noutput=gmlp(input)\r\nprint(output.shape)\r\n```\r\n\r\n***\r\n\r\n### 5. sMLP Usage\r\n#### 5.1. Paper\r\n[\"Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?\"](https://arxiv.org/abs/2109.05422)\r\n\r\n#### 5.2. Overview\r\n![](./model/img/sMLP.jpg)\r\n\r\n#### 5.3. Usage Code\r\n```python\r\nfrom model.mlp.sMLP_block import sMLPBlock\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(50,3,224,224)\r\n    smlp=sMLPBlock(h=224,w=224)\r\n    out=smlp(input)\r\n    print(out.shape)\r\n```\r\n\r\n### 6. vip-mlp Usage\r\n#### 6.1. Paper\r\n[\"Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition\"](https://arxiv.org/abs/2106.12368)\r\n\r\n#### 6.2. Usage Code\r\n```python\r\nfrom model.mlp.vip-mlp import VisionPermutator\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(1,3,224,224)\r\n    model = VisionPermutator(\r\n        layers=[4, 3, 8, 3], \r\n        embed_dims=[384, 384, 384, 384], \r\n        patch_size=14, \r\n        transitions=[False, False, False, False],\r\n        segment_dim=[16, 16, 16, 16], \r\n        mlp_ratios=[3, 3, 3, 3], \r\n        mlp_fn=WeightedPermuteMLP\r\n    )\r\n    output=model(input)\r\n    print(output.shape)\r\n```\r\n\r\n\r\n# Re-Parameter Series\r\n\r\n- Pytorch implementation of [\"RepVGG: Making VGG-style ConvNets Great Again---CVPR2021\"](https://arxiv.org/abs/2101.03697)\r\n\r\n- Pytorch implementation of [\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks---ICCV2019\"](https://arxiv.org/abs/1908.03930)\r\n\r\n- Pytorch implementation of [\"Diverse Branch Block: Building a Convolution as an Inception-like Unit---CVPR2021\"](https://arxiv.org/abs/2103.13425)\r\n\r\n\r\n***\r\n\r\n### 1. RepVGG Usage\r\n#### 1.1. Paper\r\n[\"RepVGG: Making VGG-style ConvNets Great Again\"](https://arxiv.org/abs/2101.03697)\r\n\r\n#### 1.2. Overview\r\n![](./model/img/repvgg.png)\r\n\r\n#### 1.3. Usage Code\r\n```python\r\n\r\nfrom model.rep.repvgg import RepBlock\r\nimport torch\r\n\r\n\r\ninput=torch.randn(50,512,49,49)\r\nrepblock=RepBlock(512,512)\r\nrepblock.eval()\r\nout=repblock(input)\r\nrepblock._switch_to_deploy()\r\nout2=repblock(input)\r\nprint('difference between vgg and repvgg')\r\nprint(((out2-out)**2).sum())\r\n```\r\n\r\n\r\n\r\n***\r\n\r\n### 2. ACNet Usage\r\n#### 2.1. Paper\r\n[\"ACNet: Strengthening the Kernel Skeletons for Powerful CNN via Asymmetric Convolution Blocks\"](https://arxiv.org/abs/1908.03930)\r\n\r\n#### 2.2. Overview\r\n![](./model/img/acnet.png)\r\n\r\n#### 2.3. Usage Code\r\n```python\r\nfrom model.rep.acnet import ACNet\r\nimport torch\r\nfrom torch import nn\r\n\r\ninput=torch.randn(50,512,49,49)\r\nacnet=ACNet(512,512)\r\nacnet.eval()\r\nout=acnet(input)\r\nacnet._switch_to_deploy()\r\nout2=acnet(input)\r\nprint('difference:')\r\nprint(((out2-out)**2).sum())\r\n\r\n```\r\n\r\n\r\n\r\n***\r\n\r\n### 2. Diverse Branch Block Usage\r\n#### 2.1. Paper\r\n[\"Diverse Branch Block: Building a Convolution as an Inception-like Unit\"](https://arxiv.org/abs/2103.13425)\r\n\r\n#### 2.2. Overview\r\n![](./model/img/ddb.png)\r\n\r\n#### 2.3. Usage Code\r\n##### 2.3.1 Transform I\r\n```python\r\nfrom model.rep.ddb import transI_conv_bn\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,64,7,7)\r\n#conv+bn\r\nconv1=nn.Conv2d(64,64,3,padding=1)\r\nbn1=nn.BatchNorm2d(64)\r\nbn1.eval()\r\nout1=bn1(conv1(input))\r\n\r\n#conv_fuse\r\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\r\nconv_fuse.weight.data,conv_fuse.bias.data=transI_conv_bn(conv1,bn1)\r\nout2=conv_fuse(input)\r\n\r\nprint(\"difference:\",((out2-out1)**2).sum().item())\r\n```\r\n\r\n##### 2.3.2 Transform II\r\n```python\r\nfrom model.rep.ddb import transII_conv_branch\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,64,7,7)\r\n\r\n#conv+conv\r\nconv1=nn.Conv2d(64,64,3,padding=1)\r\nconv2=nn.Conv2d(64,64,3,padding=1)\r\nout1=conv1(input)+conv2(input)\r\n\r\n#conv_fuse\r\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\r\nconv_fuse.weight.data,conv_fuse.bias.data=transII_conv_branch(conv1,conv2)\r\nout2=conv_fuse(input)\r\n\r\nprint(\"difference:\",((out2-out1)**2).sum().item())\r\n```\r\n\r\n##### 2.3.3 Transform III\r\n```python\r\nfrom model.rep.ddb import transIII_conv_sequential\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,64,7,7)\r\n\r\n#conv+conv\r\nconv1=nn.Conv2d(64,64,1,padding=0,bias=False)\r\nconv2=nn.Conv2d(64,64,3,padding=1,bias=False)\r\nout1=conv2(conv1(input))\r\n\r\n\r\n#conv_fuse\r\nconv_fuse=nn.Conv2d(64,64,3,padding=1,bias=False)\r\nconv_fuse.weight.data=transIII_conv_sequential(conv1,conv2)\r\nout2=conv_fuse(input)\r\n\r\nprint(\"difference:\",((out2-out1)**2).sum().item())\r\n```\r\n\r\n##### 2.3.4 Transform IV\r\n```python\r\nfrom model.rep.ddb import transIV_conv_concat\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,64,7,7)\r\n\r\n#conv+conv\r\nconv1=nn.Conv2d(64,32,3,padding=1)\r\nconv2=nn.Conv2d(64,32,3,padding=1)\r\nout1=torch.cat([conv1(input),conv2(input)],dim=1)\r\n\r\n#conv_fuse\r\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\r\nconv_fuse.weight.data,conv_fuse.bias.data=transIV_conv_concat(conv1,conv2)\r\nout2=conv_fuse(input)\r\n\r\nprint(\"difference:\",((out2-out1)**2).sum().item())\r\n```\r\n\r\n##### 2.3.5 Transform V\r\n```python\r\nfrom model.rep.ddb import transV_avg\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,64,7,7)\r\n\r\navg=nn.AvgPool2d(kernel_size=3,stride=1)\r\nout1=avg(input)\r\n\r\nconv=transV_avg(64,3)\r\nout2=conv(input)\r\n\r\nprint(\"difference:\",((out2-out1)**2).sum().item())\r\n```\r\n\r\n\r\n##### 2.3.6 Transform VI\r\n```python\r\nfrom model.rep.ddb import transVI_conv_scale\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,64,7,7)\r\n\r\n#conv+conv\r\nconv1x1=nn.Conv2d(64,64,1)\r\nconv1x3=nn.Conv2d(64,64,(1,3),padding=(0,1))\r\nconv3x1=nn.Conv2d(64,64,(3,1),padding=(1,0))\r\nout1=conv1x1(input)+conv1x3(input)+conv3x1(input)\r\n\r\n#conv_fuse\r\nconv_fuse=nn.Conv2d(64,64,3,padding=1)\r\nconv_fuse.weight.data,conv_fuse.bias.data=transVI_conv_scale(conv1x1,conv1x3,conv3x1)\r\nout2=conv_fuse(input)\r\n\r\nprint(\"difference:\",((out2-out1)**2).sum().item())\r\n```\r\n\r\n\r\n\r\n\r\n\r\n# Convolution Series\r\n\r\n- Pytorch implementation of [\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications---CVPR2017\"](https://arxiv.org/abs/1704.04861)\r\n\r\n- Pytorch implementation of [\"Efficientnet: Rethinking model scaling for convolutional neural networks---PMLR2019\"](http://proceedings.mlr.press/v97/tan19a.html)\r\n\r\n- Pytorch implementation of [\"Involution: Inverting the Inherence of Convolution for Visual Recognition---CVPR2021\"](https://arxiv.org/abs/2103.06255)\r\n\r\n- Pytorch implementation of [\"Dynamic Convolution: Attention over Convolution Kernels---CVPR2020 Oral\"](https://arxiv.org/abs/1912.03458)\r\n\r\n- Pytorch implementation of [\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference---NeurIPS2019\"](https://arxiv.org/abs/1904.04971)\r\n\r\n***\r\n\r\n### 1. Depthwise Separable Convolution Usage\r\n#### 1.1. Paper\r\n[\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\"](https://arxiv.org/abs/1704.04861)\r\n\r\n#### 1.2. Overview\r\n![](./model/img/DepthwiseSeparableConv.png)\r\n\r\n#### 1.3. Usage Code\r\n```python\r\nfrom model.conv.DepthwiseSeparableConvolution import DepthwiseSeparableConvolution\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,3,224,224)\r\ndsconv=DepthwiseSeparableConvolution(3,64)\r\nout=dsconv(input)\r\nprint(out.shape)\r\n```\r\n\r\n***\r\n\r\n\r\n### 2. MBConv Usage\r\n#### 2.1. Paper\r\n[\"Efficientnet: Rethinking model scaling for convolutional neural networks\"](http://proceedings.mlr.press/v97/tan19a.html)\r\n\r\n#### 2.2. Overview\r\n![](./model/img/MBConv.jpg)\r\n\r\n#### 2.3. Usage Code\r\n```python\r\nfrom model.conv.MBConv import MBConvBlock\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,3,224,224)\r\nmbconv=MBConvBlock(ksize=3,input_filters=3,output_filters=512,image_size=224)\r\nout=mbconv(input)\r\nprint(out.shape)\r\n\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n### 3. Involution Usage\r\n#### 3.1. Paper\r\n[\"Involution: Inverting the Inherence of Convolution for Visual Recognition\"](https://arxiv.org/abs/2103.06255)\r\n\r\n#### 3.2. Overview\r\n![](./model/img/Involution.png)\r\n\r\n#### 3.3. Usage Code\r\n```python\r\nfrom model.conv.Involution import Involution\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\ninput=torch.randn(1,4,64,64)\r\ninvolution=Involution(kernel_size=3,in_channel=4,stride=2)\r\nout=involution(input)\r\nprint(out.shape)\r\n```\r\n\r\n***\r\n\r\n\r\n### 4. DynamicConv Usage\r\n#### 4.1. Paper\r\n[\"Dynamic Convolution: Attention over Convolution Kernels\"](https://arxiv.org/abs/1912.03458)\r\n\r\n#### 4.2. Overview\r\n![](./model/img/DynamicConv.png)\r\n\r\n#### 4.3. Usage Code\r\n```python\r\nfrom model.conv.DynamicConv import *\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(2,32,64,64)\r\n    m=DynamicConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\r\n    out=m(input)\r\n    print(out.shape) # 2,32,64,64\r\n\r\n```\r\n\r\n***\r\n\r\n\r\n### 5. CondConv Usage\r\n#### 5.1. Paper\r\n[\"CondConv: Conditionally Parameterized Convolutions for Efficient Inference\"](https://arxiv.org/abs/1904.04971)\r\n\r\n#### 5.2. Overview\r\n![](./model/img/CondConv.png)\r\n\r\n#### 5.3. Usage Code\r\n```python\r\nfrom model.conv.CondConv import *\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import functional as F\r\n\r\n\r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n    input=torch.randn(2,32,64,64)\r\n    m=CondConv(in_planes=32,out_planes=64,kernel_size=3,stride=1,padding=1,bias=False)\r\n    out=m(input)\r\n    print(out.shape)\r\n\r\n```\r\n\r\n\r\n## È°πÁõÆÊé®Ëçê\r\n\r\n-------\r\n\r\n\r\nüî•üî•üî• ÈáçÁ£ÖÔºÅÔºÅÔºÅ‰Ωú‰∏∫È°πÁõÆË°•ÂÖÖÔºåÊõ¥Â§öËÆ∫ÊñáÂ±ÇÈù¢ÁöÑËß£ÊûêÔºåÂèØ‰ª•ÂÖ≥Ê≥®Êñ∞ÂºÄÊ∫êÁöÑÈ°πÁõÆ **[FightingCV-Paper-Reading](https://github.com/xmu-xiaoma666/FightingCV-Paper-Reading)** ÔºåÈáåÈù¢Ê±áÈõÜÂíåÊï¥ÁêÜ‰∫ÜÂêÑÂ§ßÈ°∂‰ºöÈ°∂ÂàäÁöÑËÆ∫ÊñáËß£Êûê\r\n\r\n\r\nüî•üî•üî• ÈáçÁ£ÖÔºÅÔºÅÔºÅÊúÄËøëÂÖ®Êñ∞ÂºÄÊ∫ê‰∫Ü‰∏Ä‰∏™ **[YOLOAir](https://github.com/iscyy/yoloair)** ÁõÆÊ†áÊ£ÄÊµã‰ª£Á†ÅÂ∫ì ÔºåÈáåÈù¢ÈõÜÊàê‰∫ÜÂ§öÁßçYOLOÊ®°ÂûãÔºåÂåÖÊã¨YOLOv5, YOLOv7,YOLOR, YOLOX,YOLOv4, YOLOv3‰ª•ÂèäÂÖ∂‰ªñYOLOÊ®°ÂûãÔºåËøòÂåÖÊã¨Â§öÁßçÁé∞ÊúâAttentionÊú∫Âà∂„ÄÇ\r\n\r\n![acff451cb342be80bff0963e0a03138](https://user-images.githubusercontent.com/33897496/187075538-746de6eb-308e-4e42-b9f9-939904c9a7f9.jpg)\r\n\r\n\r\n\r\nüî•üî•üî• **ECCV2022ËÆ∫ÊñáÊ±áÊÄªÔºö[ECCV2022-Paper-List](https://github.com/xmu-xiaoma666/ECCV2022-Paper-List/blob/master/README.md)**\r\n\r\n<!-- ![image](https://user-images.githubusercontent.com/33897496/184842902-9acff374-b3e7-401a-80fd-9d484e40c637.png) -->\r\n\r\n\r\n\r\n\r\n\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/xmu-xiaoma666/External-Attention-pytorch",
    "keywords": "AttentionBackbone",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "fightingcv",
    "package_url": "https://pypi.org/project/fightingcv/",
    "platform": null,
    "project_url": "https://pypi.org/project/fightingcv/",
    "project_urls": {
      "Homepage": "https://github.com/xmu-xiaoma666/External-Attention-pytorch"
    },
    "release_url": "https://pypi.org/project/fightingcv/1.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.7.0",
    "summary": "FightingCV Codebase For Attention,Backbone, MLP, Re-parameter, Convolution",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15222554,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a132a7eb6c5689c6257beff97ce6edf910c846deb5b79e2c3ce72407a142209c",
          "md5": "93a9588d5f6bbbad2a58b139f63c1982",
          "sha256": "5b7f737862ae5ada8764a58ce1daee3b3544c0870e8a22f46c221dfadb496159"
        },
        "downloads": -1,
        "filename": "fightingcv-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "93a9588d5f6bbbad2a58b139f63c1982",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 42425,
        "upload_time": "2022-09-27T02:50:43",
        "upload_time_iso_8601": "2022-09-27T02:50:43.181800Z",
        "url": "https://files.pythonhosted.org/packages/a1/32/a7eb6c5689c6257beff97ce6edf910c846deb5b79e2c3ce72407a142209c/fightingcv-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "beb9a38f1cb12c142616ab5e5e6d270d96917157b0c73230b005302f1bff0ca1",
          "md5": "15839c8e1fd8c3b2b559177be9a255c4",
          "sha256": "6bc05461918641813e4e1096581a0fac94be910432b034dfc8cb24f80f7db82e"
        },
        "downloads": -1,
        "filename": "fightingcv-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "15839c8e1fd8c3b2b559177be9a255c4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 151476,
        "upload_time": "2022-09-27T03:04:02",
        "upload_time_iso_8601": "2022-09-27T03:04:02.262086Z",
        "url": "https://files.pythonhosted.org/packages/be/b9/a38f1cb12c142616ab5e5e6d270d96917157b0c73230b005302f1bff0ca1/fightingcv-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "beb9a38f1cb12c142616ab5e5e6d270d96917157b0c73230b005302f1bff0ca1",
        "md5": "15839c8e1fd8c3b2b559177be9a255c4",
        "sha256": "6bc05461918641813e4e1096581a0fac94be910432b034dfc8cb24f80f7db82e"
      },
      "downloads": -1,
      "filename": "fightingcv-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "15839c8e1fd8c3b2b559177be9a255c4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.0",
      "size": 151476,
      "upload_time": "2022-09-27T03:04:02",
      "upload_time_iso_8601": "2022-09-27T03:04:02.262086Z",
      "url": "https://files.pythonhosted.org/packages/be/b9/a38f1cb12c142616ab5e5e6d270d96917157b0c73230b005302f1bff0ca1/fightingcv-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}