{
  "info": {
    "author": "GenerallyIntelligent",
    "author_email": "tannersims@generallyintelligent.me",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Hate Datasets Compilation\nContains 17 different hate, violence, and discrimination speech datasets, along with anotations on where they were found, the data format and method for collection and labeling. Each dataset is kept in the original file structure and placed inside the respective folder's `data` file, in case a more recent version is obtained. (Links for the source are in each ABOUT.md file)\n\n## Installing\nTo use the hatecomp datasets or models, simply run the following command in a python environment of your choice:\n```shell\npip install hatecomp\n```\n\nIf you do not have pytorch already installed, it is recommended to do so using conda. Visit the pytorch [website](https://pytorch.org/) for more information.\n\nOnce it has finished downloading, you can start loading in datasets and models. Below are a couple of examples to get you started. For more advanced usage, please see the `train.py` file in the root of this repo. `hatecomp` follows the huggingface training API, so most everything that works with huggingface will work here.\n\n## Examples\nHere are a couple examples of how to use the hatecomp library.\n\n### Working with a Dataset\nLoading datasets is very simple. Each has its own downloading script that will run lazily when you try to create the dataset. If you would like to, you can specify where to download and if the dataset should download. By default the datasets only download when they cannot find the necessary files in the given location.\n```python\nfrom hatecomp.datasets import Vicomtech\n\n# load a dataset from the default location,\n# or download the dataset in the default location\ndataset = Vicomtech()\nexample = dataset[0]\n\n# load a dataset from a specified location,\n# or download to that location\ndataset = Vicomtech(root = \"my/special/dataset/path\")\nexample = dataset[0]\n\n# only load a dataset if it can be found at the given location\ndataset = Vicomtech(root = \"my/special/dataset/path\", download = False)\nexample = dataset[0]\n```\n\nThe datasets also come equipped with a couple of handy features designed especially for NLP use and convenience.\n\n```python\nfrom hatecomp.datasets import Vicomtech\n\n# Mapping a function over the dataset data (usually text, unless the dataset has already been mapped)\n# Note that the map function can support batching if your mapped function supports it.\ndef my_tokenizing_function(some_string):\n    return 0\ndataset = Vicomtech()\ntokenized_dataset = dataset.map(function = my_tokenizing_function, batched = False)\n\n# Splitting the dataset\ntrain_split, test_split = tokenized_dataset.spit(test_proportion = 0.1)\n```\n\n### Using a model\nThe process for using a model is very similar to the huggingface Trainer API.\n```python\nfrom hatecomp.datasets import Vicomtech\nfrom hatecomp.training import Trainer, TrainingArguments\nfrom hatecomp.base.utils import tokenize_bookends\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nraw_dataset = Vicomtech()\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\nnum_classes = raw_dataset.num_classes()\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-base\", num_labels = num_classes\n)\n\ntokenizer_function = lambda tokenization_input: tokenize_bookends(\n    tokenization_input, model.config.max_position_embeddings, tokenizer\n)\ntokenized_dataset = raw_dataset.map(tokenizer_function, batched=True)\ntrain_split, test_split = tokenized_dataset.split()\n\ntraining_args = TrainingArguments(\"test-run\")\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_split,\n    eval_dataset=test_split\n)\ntrainer.train()\n```\n\n## Datasets\nAdditional information about each dataset can be found in the corresponding ABOUT.md file. \n\n### Functionality\nCurrently the following datasets are implemented:\n- ZeerakTalat NAACL\n- ZeerakTalat NLPCSS\n- HASOC\n- Vicomtech\n\nSeveral more have downloaders already, and are close to completion\n\n### Notes\nTwo of the dataset, the `MLMA Dataset` and `Online Intervention Dataset`, only contain hateful posts, instead labeling other features such as the target of hate.\n\n## Training\n`hatecomp` provides some basic training tools to integrate into the [huggingface](https://github.com/huggingface) :hugs: Trainer API. A full example of how to train a model using the hatecomp datasets can be found in `train.py`\n\n### Results\nHere is a list of results achieved on various datasets with Huggingface models, along with the SOTA performance (as best I could find). Some references only have accuracy, others only have F1. The hatecomp performance is measured with the appropriate metric for whatever SOTA could be found. (Dataset names link to the locations where each SOTA reference was found. If you are looking for citations, please refer to the `About.md` for each dataset)\n\n| Dataset | Metric | SOTA | hatecomp/huggingface |\n| -- | -- | -- | -- |\n| [Vicomtech](https://arxiv.org/pdf/1809.04444.pdf) | Accuracy | 0.79 | **0.93** |\n| [ZeerakTalat-NAACL](https://aclanthology.org/N16-2013.pdf) | F1 | 0.74 | **0.94** |\n| [ZeerakTalat-NLPCSS](https://aclanthology.org/W16-5618.pdf) | F1 | 0.53 | NA |\n| [HASOC](https://arxiv.org/pdf/2108.05927.pdf) | F1 (Macro) | 0.53 | NA |\n| [TwitterSexism](https://aclanthology.org/W17-2902.pdf) | F1 (Macro) | 0.87 | **0.99** |\n\n(If you know of a better SOTA than what is listed here, please create an issue or pull request.)\n\nAlso note that some of these datasets require tweet data. For these, a large number of tweet_ids return Unauthorized from the twitter API, so the data which the hatecomp models trained on is a subset of the total dataset. More information can be found in the following table:\n\n| Dataset | Total Size | Successfully Downloaded Tweets | Available Training Portion |\n| -- | -- | -- | -- |\n| ZeerakTalat-NAACL | 16907 | 7210 | 0.4264 |\n| ZeerakTalat-NLPCSS | 6909 | 4190 | 0.6064 |\n| TwitterSexism | 10583 | 5054 | 0.4775 |\n\nThis info is valid as of Feb 2022, and is probably subject to change as Twitter continues to lock down their API.\n\n## TODO\nImplement a multi-task model, both for datasets with multiple tasks, and for the entire collection of datasets.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/GenerallyIntelligent/hatecomp",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hatecomp",
    "package_url": "https://pypi.org/project/hatecomp/",
    "platform": "",
    "project_url": "https://pypi.org/project/hatecomp/",
    "project_urls": {
      "Homepage": "https://github.com/GenerallyIntelligent/hatecomp"
    },
    "release_url": "https://pypi.org/project/hatecomp/0.3.0/",
    "requires_dist": [
      "aiometer (<1.0.0,>=0.3.0)",
      "httpx (<1.0.0,>=0.21.1)",
      "requests (<3.0.0,>=2.0.0)",
      "torch (<2.0.0,>=1.7.0)",
      "tqdm (<5.0.0,>=4.1.0)",
      "tweepy (<5.0.0,>=4.0.0)",
      "transformers (<5.0.0,>=4.16.0)"
    ],
    "requires_python": "",
    "summary": "Collection of pytorch datasets for hate speech and toxic internet discourse",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12922730,
  "releases": {
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8acc74bf984f42525a0ba6fb71af6b087f35a4d0029550fe69882a6e3854d5c3",
          "md5": "6387e950c4f2302beafba2986b2702d2",
          "sha256": "0cd7425096daec55f16ba4b17ea508032226af73d6d08c88f6343d9c202df040"
        },
        "downloads": -1,
        "filename": "hatecomp-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6387e950c4f2302beafba2986b2702d2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 22954,
        "upload_time": "2022-02-17T15:54:53",
        "upload_time_iso_8601": "2022-02-17T15:54:53.883876Z",
        "url": "https://files.pythonhosted.org/packages/8a/cc/74bf984f42525a0ba6fb71af6b087f35a4d0029550fe69882a6e3854d5c3/hatecomp-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fd796e5778a2016ab40508c5929d01255c4c2f496ac1c2ad53e4e7b1187f308a",
          "md5": "ede1fa30b9d4bc09da98b7bfc7bcc9c4",
          "sha256": "4734d7c0cf97533ff1e2caf528e26eb1098490394f939076e817e98620751022"
        },
        "downloads": -1,
        "filename": "hatecomp-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ede1fa30b9d4bc09da98b7bfc7bcc9c4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 17110,
        "upload_time": "2022-02-17T15:54:55",
        "upload_time_iso_8601": "2022-02-17T15:54:55.653378Z",
        "url": "https://files.pythonhosted.org/packages/fd/79/6e5778a2016ab40508c5929d01255c4c2f496ac1c2ad53e4e7b1187f308a/hatecomp-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8acc74bf984f42525a0ba6fb71af6b087f35a4d0029550fe69882a6e3854d5c3",
        "md5": "6387e950c4f2302beafba2986b2702d2",
        "sha256": "0cd7425096daec55f16ba4b17ea508032226af73d6d08c88f6343d9c202df040"
      },
      "downloads": -1,
      "filename": "hatecomp-0.3.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "6387e950c4f2302beafba2986b2702d2",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 22954,
      "upload_time": "2022-02-17T15:54:53",
      "upload_time_iso_8601": "2022-02-17T15:54:53.883876Z",
      "url": "https://files.pythonhosted.org/packages/8a/cc/74bf984f42525a0ba6fb71af6b087f35a4d0029550fe69882a6e3854d5c3/hatecomp-0.3.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fd796e5778a2016ab40508c5929d01255c4c2f496ac1c2ad53e4e7b1187f308a",
        "md5": "ede1fa30b9d4bc09da98b7bfc7bcc9c4",
        "sha256": "4734d7c0cf97533ff1e2caf528e26eb1098490394f939076e817e98620751022"
      },
      "downloads": -1,
      "filename": "hatecomp-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ede1fa30b9d4bc09da98b7bfc7bcc9c4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 17110,
      "upload_time": "2022-02-17T15:54:55",
      "upload_time_iso_8601": "2022-02-17T15:54:55.653378Z",
      "url": "https://files.pythonhosted.org/packages/fd/79/6e5778a2016ab40508c5929d01255c4c2f496ac1c2ad53e4e7b1187f308a/hatecomp-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}