{
  "info": {
    "author": "Serge Smertin",
    "author_email": "serge.smertin@databricks.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "Intended Audience :: System Administrators",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.11",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# Databricks SDK for Python\n\n**Stability**: [Experimental](https://docs.databricks.com/release-notes/release-types.html)\n\nThe Databricks SDK for Python includes functionality to accelerate development with [Python](https://www.python.org/) for the Databricks Lakehouse.\nIt covers all public [Databricks REST API](https://docs.databricks.com/dev-tools/api/index.html) operations.\nThe SDK's internal HTTP client is robust and handles failures on different levels by performing intelligent retries.\n\n## Contents\n\n- [Getting started](#getting-started)\n- [Authentication](#authentication)\n- [Code examples](#code-examples)\n- [Long-running operations](#long-running-operations)\n- [Paginated responses](#paginated-responses)\n- [Single-sign-on with OAuth](#single-sign-on-sso-with-oauth)\n- [Logging](#logging)\n- [Integration with `dbutils`](#interaction-with-dbutils)\n- [Interface stability](#interface-stability)\n- [Disclaimer](#disclaimer)\n\n## Getting started\n\n1. Please install Databricks SDK for Python via `pip install databricks-sdk` and instantiate `WorkspaceClient`:\n\n```python\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nfor c in w.clusters.list():\n    print(c.cluster_name)\n```\n\nDatabricks SDK for Python is compatible with Python 3.7 _(until [June 2023](https://devguide.python.org/versions/))_, 3.8, 3.9, 3.10, and 3.11.\n\n## Authentication\n\nIf you use Databricks [configuration profiles](https://docs.databricks.com/dev-tools/auth.html#configuration-profiles)\nor Databricks-specific [environment variables](https://docs.databricks.com/dev-tools/auth.html#environment-variables)\nfor [Databricks authentication](https://docs.databricks.com/dev-tools/auth.html), the only code required to start\nworking with a Databricks workspace is the following code snippet, which instructs the Databricks SDK for Python to use\nits [default authentication flow](#default-authentication-flow):\n\n```python\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nw. # press <TAB> for autocompletion\n```\n\nThe conventional name for the variable that holds the workspace-level client of the Databricks SDK for Python is `w`, which is shorthand for `workspace`.\n\n### In this section\n\n- [Default authentication flow](#default-authentication-flow)\n- [Databricks native authentication](#databricks-native-authentication)\n- [Azure native authentication](#azure-native-authentication)\n- [Overriding .databrickscfg](#overriding-databrickscfg)\n- [Additional authentication configuration options](#additional-authentication-configuration-options)\n\n### Default authentication flow\n\nIf you run the [Databricks Terraform Provider](https://registry.terraform.io/providers/databrickslabs/databricks/latest),\nthe [Databricks SDK for Go](https://github.com/databricks/databricks-sdk-go), the [Databricks CLI](https://docs.databricks.com/dev-tools/cli/index.html),\nor applications that target the Databricks SDKs for other languages, most likely they will all interoperate nicely together.\nBy default, the Databricks SDK for Python tries the following [authentication](https://docs.databricks.com/dev-tools/auth.html) methods,\nin the following order, until it succeeds:\n\n1. [Databricks native authentication](#databricks-native-authentication)\n2. [Azure native authentication](#azure-native-authentication)\n4. If the SDK is unsuccessful at this point, it returns an authentication error and stops running.\n\nYou can instruct the Databricks SDK for Python to use a specific authentication method by setting the `auth_type` argument\nas described in the following sections.\n\nFor each authentication method, the SDK searches for compatible authentication credentials in the following locations,\nin the following order. Once the SDK finds a compatible set of credentials that it can use, it stops searching:\n\n1. Credentials that are hard-coded into configuration arguments.\n\n   :warning: **Caution**: Databricks does not recommend hard-coding credentials into arguments, as they can be exposed in plain text in version control systems. Use environment variables or configuration profiles instead.\n\n2. Credentials in Databricks-specific [environment variables](https://docs.databricks.com/dev-tools/auth.html#environment-variables).\n3. For Databricks native authentication, credentials in the `.databrickscfg` file's `DEFAULT` [configuration profile](https://docs.databricks.com/dev-tools/auth.html#configuration-profiles) from its default file location (`~` for Linux or macOS, and `%USERPROFILE%` for Windows).\n4. For Azure native authentication, the SDK searches for credentials through the Azure CLI as needed.\n\nDepending on the Databricks authentication method, the SDK uses the following information. Presented are the `WorkspaceClient` and `AccountClient` arguments (which have corresponding `.databrickscfg` file fields), their descriptions, and any corresponding environment variables.\n\n### Databricks native authentication\n\nBy default, the Databricks SDK for Python initially tries [Databricks token authentication](https://docs.databricks.com/dev-tools/api/latest/authentication.html) (`auth_type='pat'` argument). If the SDK is unsuccessful, it then tries Databricks basic (username/password) authentication (`auth_type=\"basic\"` argument).\n\n- For Databricks token authentication, you must provide `host` and `token`; or their environment variable or `.databrickscfg` file field equivalents.\n- For Databricks basic authentication, you must provide `host`, `username`, and `password` _(for AWS workspace-level operations)_; or `host`, `account_id`, `username`, and `password` _(for AWS, Azure, or GCP account-level operations)_; or their environment variable or `.databrickscfg` file field equivalents.\n\n| Argument     | Description | Environment variable |\n|--------------|-------------|-------------------|\n| `host`       | _(String)_ The Databricks host URL for either the Databricks workspace endpoint or the Databricks accounts endpoint. | `DATABRICKS_HOST` |     \n| `account_id` | _(String)_ The Databricks account ID for the Databricks accounts endpoint. Only has effect when `Host` is either `https://accounts.cloud.databricks.com/` _(AWS)_, `https://accounts.azuredatabricks.net/` _(Azure)_, or `https://accounts.gcp.databricks.com/` _(GCP)_. | `DATABRICKS_ACCOUNT_ID` |\n| `token`      | _(String)_ The Databricks personal access token (PAT) _(AWS, Azure, and GCP)_ or Azure Active Directory (Azure AD) token _(Azure)_. | `DATABRICKS_TOKEN` |\n| `username`   | _(String)_ The Databricks username part of basic authentication. Only possible when `Host` is `*.cloud.databricks.com` _(AWS)_. | `DATABRICKS_USERNAME` |\n| `password`   | _(String)_ The Databricks password part of basic authentication. Only possible when `Host` is `*.cloud.databricks.com` _(AWS)_. | `DATABRICKS_PASSWORD` |\n\nFor example, to use Databricks token authentication:\n\n```python\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host=input('Databricks Workspace URL: '), token=input('Token: '))\n```\n\n### Azure native authentication\n\nBy default, the Databricks SDK for Python first tries Azure client secret authentication (`auth_type='azure-client-secret'` argument). If the SDK is unsuccessful, it then tries Azure CLI authentication (`auth_type='azure-cli'` argument). See [Manage service principals](https://learn.microsoft.com/azure/databricks/administration-guide/users-groups/service-principals).\n\nThe Databricks SDK for Python picks up an Azure CLI token, if you've previously authenticated as an Azure user by running `az login` on your machine. See [Get Azure AD tokens for users by using the Azure CLI](https://learn.microsoft.com/azure/databricks/dev-tools/api/latest/aad/user-aad-token).\n\nTo authenticate as an Azure Active Directory (Azure AD) service principal, you must provide one of the following. See also [Add a service principal to your Azure Databricks account](https://learn.microsoft.com/azure/databricks/administration-guide/users-groups/service-principals#add-sp-account):\n\n- `azure_resource_id`, `azure_client_secret`, `azure_client_id`, and `azure_tenant_id`; or their environment variable or `.databrickscfg` file field equivalents.\n- `azure_resource_id` and `azure_use_msi`; or their environment variable or `.databrickscfg` file field equivalents.\n\n| Argument              | Description | Environment variable |\n|-----------------------|-------------|----------------------|\n| `azure_resource_id`   | _(String)_ The Azure Resource Manager ID for the Azure Databricks workspace, which is exchanged for a Databricks host URL. | `DATABRICKS_AZURE_RESOURCE_ID` |\n| `azure_use_msi`       | _(Boolean)_ `true` to use Azure Managed Service Identity passwordless authentication flow for service principals. _This feature is not yet implemented in the Databricks SDK for Python._ | `ARM_USE_MSI` |\n| `azure_client_secret` | _(String)_ The Azure AD service principal's client secret. | `ARM_CLIENT_SECRET` |\n| `azure_client_id`     | _(String)_ The Azure AD service principal's application ID. | `ARM_CLIENT_ID` |\n| `azure_tenant_id`     | _(String)_ The Azure AD service principal's tenant ID. | `ARM_TENANT_ID` |\n| `azure_environment`   | _(String)_ The Azure environment type (such as Public, UsGov, China, and Germany) for a specific set of API endpoints. Defaults to `PUBLIC`. | `ARM_ENVIRONMENT` |\n\nFor example, to use Azure client secret authentication:\n\n```python\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(host=input('Databricks Workspace URL: '),\n                    azure_workspace_resource_id=input('Azure Resource ID: '),\n                    azure_tenant_id=input('AAD Tenant ID: '),\n                    azure_client_id=input('AAD Client ID: '),\n                    azure_client_secret=input('AAD Client Secret: '))\n```\n\n### Overriding `.databrickscfg`\n\nFor [Databricks native authentication](#databricks-native-authentication), you can override the default behavior for using `.databrickscfg` as follows:\n\n| Argument      | Description | Environment variable |\n|---------------|-------------|----------------------|\n| `profile`     | _(String)_ A connection profile specified within `.databrickscfg` to use instead of `DEFAULT`. | `DATABRICKS_CONFIG_PROFILE` |\n| `config_file` | _(String)_ A non-default location of the Databricks CLI credentials file. | `DATABRICKS_CONFIG_FILE` |\n\nFor example, to use a profile named `MYPROFILE` instead of `DEFAULT`:\n\n```python\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(profile='MYPROFILE')\n# Now call the Databricks workspace APIs as desired...\n```\n\n### Additional authentication configuration options\n\nFor all authentication methods, you can override the default behavior in client arguments as follows:\n\n| Argument                | Description | Environment variable   |\n|-------------------------|-------------|------------------------|\n| `auth_type`             | _(String)_ When multiple auth attributes are available in the environment, use the auth type specified by this argument. This argument also holds the currently selected auth. | `DATABRICKS_AUTH_TYPE` |\n| `http_timeout_seconds`  | _(Integer)_ Number of seconds for HTTP timeout. Default is _60_. | _(None)_               |\n| `retry_timeout_seconds` | _(Integer)_ Number of seconds to keep retrying HTTP requests. Default is _300 (5 minutes)_. | _(None)_               |\n| `debug_truncate_bytes`  | _(Integer)_ Truncate JSON fields in debug logs above this limit. Default is 96. | `DATABRICKS_DEBUG_TRUNCATE_BYTES` |\n| `debug_headers`         | _(Boolean)_ `true` to debug HTTP headers of requests made by the application. Default is `false`, as headers contain sensitive data, such as access tokens. | `DATABRICKS_DEBUG_HEADERS` |\n| `rate_limit`            | _(Integer)_ Maximum number of requests per second made to Databricks REST API. | `DATABRICKS_RATE_LIMIT` |\n\nFor example, to turn on debug HTTP headers:\n\n```python\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(debug_headers=True)\n# Now call the Databricks workspace APIs as desired...\n```\n\n## Code examples\n\nTo find code examples that demonstrate how to call the Databricks SDK for Python, see the top-level [examples](/examples) folder within this repository\n\n## Long-running operations\n\nWhen you invoke a long-running operation, the SDK provides a high-level API to _trigger_ these operations and _wait_ for the related entities\nto reach the correct state or return the error message in case of failure. All long-running operations return generic `Wait` instance with `result()`\nmethod to get a result of long-running operation, once it's finished. Databricks SDK for Python picks the most reasonable default timeouts for\nevery method, but sometimes you may find yourself in a situation, where you'd want to provide `datatime.timedelta()` as the value of `timeout`\nargument to `result()` method.\n\nThere are a number of long-runng opereations in Databricks APIs such as managing:\n* Clusters,\n* Command execution\n* Jobs\n* Libraries\n* Delta Live Tables pipelines\n* Databricks SQL warehouses.\n\nFor example, in the Clusters API, once you create a cluster, you receive a cluster ID, and the cluster is in the `PENDING` state Meanwhile\nDatabricks takes care of provisioning virtual machines from the cloud provider in the background. The cluster is\nonly usable in the `RUNNING` state and so you have to wait for that state to be reached.\n\nAnother example is the API for running a job or repairing the run: right after\nthe run starts, the run is in the `PENDING` state. The job is only considered to be finished when it is in either\nthe `TERMINATED` or `SKIPPED` state. Also you would likely need the error message if the long-running\noperation times out failed with an error code. Other times you may want to configure a custom timeout other than\nthe default of 20 minutes.\n\nIn the following example, `w.clusters.create` returns `ClusterInfo` only once the cluster is in the `RUNNING` state,\notherwise it will timeout in 10 minutes:\n\n```python\nimport datetime\nimport logging\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\ninfo = w.clusters.create_and_wait(cluster_name='Created cluster',\n                                  spark_version='12.0.x-scala2.12',\n                                  node_type_id='m5d.large',\n                                  autotermination_minutes=10,\n                                  num_workers=1,\n                                  timeout=datetime.timedelta(minutes=10))\nlogging.info(f'Created: {info}')\n```\n\nPlease look at the `examples/starting_job_and_waiting.py` for a more advanced usage:\n\n```python\nimport datetime\nimport logging\nimport time\n\nfrom databricks.sdk import WorkspaceClient\nimport databricks.sdk.service.jobs as j\n\nw = WorkspaceClient()\n\n# create a dummy file on DBFS that just sleeps for 10 seconds\npy_on_dbfs = f'/home/{w.current_user.me().user_name}/sample.py'\nwith w.dbfs.open(py_on_dbfs, write=True, overwrite=True) as f:\n    f.write(b'import time; time.sleep(10); print(\"Hello, World!\")')\n\n# trigger one-time-run job and get waiter object\nwaiter = w.jobs.submit(run_name=f'py-sdk-run-{time.time()}', tasks=[\n    j.RunSubmitTaskSettings(\n        task_key='hello_world',\n        new_cluster=j.BaseClusterInfo(\n            spark_version=w.clusters.select_spark_version(long_term_support=True),\n            node_type_id=w.clusters.select_node_type(local_disk=True),\n            num_workers=1\n        ),\n        spark_python_task=j.SparkPythonTask(\n            python_file=f'dbfs:{py_on_dbfs}'\n        ),\n    )\n])\n\nlogging.info(f'starting to poll: {waiter.run_id}')\n\n# callback, that receives a polled entity between state updates\ndef print_status(run: j.Run):\n    statuses = [f'{t.task_key}: {t.state.life_cycle_state}' for t in run.tasks]\n    logging.info(f'workflow intermediate status: {\", \".join(statuses)}')\n\n# If you want to perform polling in a separate thread, process, or service,\n# you can use w.jobs.wait_get_run_job_terminated_or_skipped(\n#   run_id=waiter.run_id,\n#   timeout=datetime.timedelta(minutes=15),\n#   callback=print_status) to achieve the same results.\n#\n# Waiter interface allows for `w.jobs.submit(..).result()` simplicity in\n# the scenarios, where you need to block the calling thread for the job to finish.\nrun = waiter.result(timeout=datetime.timedelta(minutes=15),\n                    callback=print_status)\n\nlogging.info(f'job finished: {run.run_page_url}')\n```\n\n## Paginated responses\n\nOn the platform side the Databricks APIs have different wait to deal with pagination:\n* Some APIs follow the offset-plus-limit pagination\n* Some start their offsets from 0 and some from 1\n* Some use the cursor-based iteration\n* Others just return all results in a single response\n\nThe Databricks SDK for Python hides this  complexity\nunder `Iterator[T]` abstraction, where multi-page results `yield` items. Python typing helps to auto-complete\nthe individual item fields.\n\n```python\nimport logging\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient()\nfor repo in w.repos.list():\n    logging.info(f'Found repo: {repo.path}')\n```\n\nPlease look at the `examples/last_job_runs.py` for a more advanced usage:\n\n```python\nimport logging\nfrom collections import defaultdict\nfrom datetime import datetime, timezone\nfrom databricks.sdk import WorkspaceClient\n\nlatest_state = {}\nall_jobs = {}\ndurations = defaultdict(list)\n\nw = WorkspaceClient()\nfor job in w.jobs.list():\n    all_jobs[job.job_id] = job\n    for run in w.jobs.list_runs(job_id=job.job_id, expand_tasks=False):\n        durations[job.job_id].append(run.run_duration)\n        if job.job_id not in latest_state:\n            latest_state[job.job_id] = run\n            continue\n        if run.end_time < latest_state[job.job_id].end_time:\n            continue\n        latest_state[job.job_id] = run\n\nsummary = []\nfor job_id, run in latest_state.items():\n    summary.append({\n        'job_name': all_jobs[job_id].settings.name,\n        'last_status': run.state.result_state,\n        'last_finished': datetime.fromtimestamp(run.end_time/1000, timezone.utc),\n        'average_duration': sum(durations[job_id]) / len(durations[job_id])\n    })\n\nfor line in sorted(summary, key=lambda s: s['last_finished'], reverse=True):\n    logging.info(f'Latest: {line}')\n```\n\n## Single-Sign-On (SSO) with OAuth\n\n### Authorization Code flow with PKCE\n\nFor a regular web app running on a server, it's recommended to use the Authorization Code Flow to obtain an Access Token\nand a Refresh Token. This method is considered safe because the Access Token is transmitted directly to the server\nhosting the app, without passing through the user's web browser and risking exposure.\n\nTo enhance the security of the Authorization Code Flow, the PKCE (Proof Key for Code Exchange) mechanism can be\nemployed. With PKCE, the calling application generates a secret called the Code Verifier, which is verified by\nthe authorization server. The app also creates a transform value of the Code Verifier, called the Code Challenge,\nand sends it over HTTPS to obtain an Authorization Code. By intercepting the Authorization Code, a malicious attacker\ncannot exchange it for a token without possessing the Code Verifier.\n\nThe [presented sample](https://github.com/databricks/databricks-sdk-py/blob/main/examples/flask_app_with_oauth.py)\nis a Python3 script that uses the Flask web framework along with Databricks SDK for Python to demonstrate how to\nimplement the OAuth Authorization Code flow with PKCE security. It can be used to build an app where each user uses\ntheir identity to access Databricks resources. The script can be executed with or without client and secret credentials\nfor a custom OAuth app.\n\nDatabricks SDK for Python exposes the `oauth_client.initiate_consent()` helper to acquire user redirect URL and initiate\nPKCE state verification. Application developers are expected to persist `RefreshableCredentials` in the webapp session\nand restore it via `RefreshableCredentials.from_dict(oauth_client, session['creds'])` helpers.\n\nWorks for both AWS and Azure. Not supported for GCP at the moment.\n\n```python\nfrom databricks.sdk.oauth import OAuthClient\noauth_client = OAuthClient(host='<workspace-url>',\n                           client_id='<oauth client ID>',\n                           redirect_url=f'http://host.domain/callback',\n                           scopes=['clusters'])\n\nimport secrets\nfrom flask import Flask, render_template_string, request, redirect, url_for, session\n\nAPP_NAME = 'flask-demo'\napp = Flask(APP_NAME)\napp.secret_key = secrets.token_urlsafe(32)\n\n@app.route('/callback')\ndef callback():\n    from databricks.sdk.oauth import Consent\n    consent = Consent.from_dict(oauth_client, session['consent'])\n    session['creds'] = consent.exchange_callback_parameters(request.args).as_dict()\n    return redirect(url_for('index'))\n\n@app.route('/')\ndef index():\n    if 'creds' not in session:\n        consent = oauth_client.initiate_consent()\n        session['consent'] = consent.as_dict()\n        return redirect(consent.auth_url)\n\n    from databricks.sdk import WorkspaceClient\n    from databricks.sdk.oauth import RefreshableCredentials\n\n    credentials_provider = RefreshableCredentials.from_dict(oauth_client, session['creds'])\n    workspace_client = WorkspaceClient(host=oauth_client.host,\n                                       product=APP_NAME,\n                                       credentials_provider=credentials_provider)\n\n    return render_template_string('...', w=workspace_client)\n```\n\n### SSO for local scripts on development machines\n\nFor applications, that do run on developer workstations, Databricks SDK for Python provides `auth_type='external-browser'`\nutility, that opens up a browser for a user to go through SSO flow. Azure support is still in the early experimental\nstage.\n\n```python\nfrom databricks.sdk import WorkspaceClient\n\nhost = input('Enter Databricks host: ')\n\nw = WorkspaceClient(host=host, auth_type='external-browser')\nclusters = w.clusters.list()\n\nfor cl in clusters:\n    print(f' - {cl.cluster_name} is {cl.state}')\n```\n\n### Creating custom OAuth applications\n\nIn order to use OAuth with Databricks SDK for Python, you should use `account_client.custom_app_integration.create` API.\n\n```python\nimport logging, getpass\nfrom databricks.sdk import AccountClient\naccount_client = AccountClient(host='https://accounts.cloud.databricks.com',\n                               account_id=input('Databricks Account ID: '),\n                               username=input('Username: '),\n                               password=getpass.getpass('Password: '))\n\nlogging.info('Enrolling all published apps...')\naccount_client.o_auth_enrollment.create(enable_all_published_apps=True)\n\nstatus = account_client.o_auth_enrollment.get()\nlogging.info(f'Enrolled all published apps: {status}')\n\ncustom_app = account_client.custom_app_integration.create(\n    name='awesome-app',\n    redirect_urls=[f'https://host.domain/path/to/callback'],\n    confidential=True)\nlogging.info(f'Created new custom app: '\n             f'--client_id {custom_app.client_id} '\n             f'--client_secret {custom_app.client_secret}')\n```\n\n## Logging\n\nThe Databricks SDK for Python seamlessly integrates with the standard [Logging facility for Python](https://docs.python.org/3/library/logging.html).\nThis allows developers to easily enable and customize logging for their Databricks Python projects.\nTo enable debug logging in your Databricks Python project, you can follow the example below:\n\n```python\nimport logging, sys\nlogging.basicConfig(stream=sys.stderr,\n                    level=logging.INFO,\n                    format='%(asctime)s [%(name)s][%(levelname)s] %(message)s')\nlogging.getLogger('databricks.sdk').setLevel(logging.DEBUG)\n\nfrom databricks.sdk import WorkspaceClient\nw = WorkspaceClient(debug_truncate_bytes=1024, debug_headers=False)\nfor cluster in w.clusters.list():\n    logging.info(f'Found cluster: {cluster.cluster_name}')\n```\n\nIn the above code snippet, the logging module is imported and the `basicConfig()` method is used to set the logging level to `DEBUG`.\nThis will enable logging at the debug level and above. Developers can adjust the logging level as needed to control the verbosity of the logging output.\nThe SDK will log all requests and responses to standard error output, using the format `> ` for requests and `< ` for responses.\nIn some cases, requests or responses may be truncated due to size considerations. If this occurs, the log message will include\nthe text `... (XXX additional elements)` to indicate that the request or response has been truncated. To increase the truncation limits,\ndevelopers can set the `debug_truncate_bytes` configuration property or the `DATABRICKS_DEBUG_TRUNCATE_BYTES` environment variable.\nTo protect sensitive data, such as authentication tokens, passwords, or any HTTP headers, the SDK will automatically replace these\nvalues with `**REDACTED**` in the log output. Developers can disable this redaction by setting the `debug_headers` configuration property to `True`.\n\n```text\n2023-03-22 21:19:21,702 [databricks.sdk][DEBUG] GET /api/2.0/clusters/list\n< 200 OK\n< {\n<   \"clusters\": [\n<     {\n<       \"autotermination_minutes\": 60,\n<       \"cluster_id\": \"1109-115255-s1w13zjj\",\n<       \"cluster_name\": \"DEFAULT Test Cluster\",\n<       ... truncated for brevity\n<     },\n<     \"... (47 additional elements)\"\n<   ]\n< }\n```\n\nOverall, the logging capabilities provided by the Databricks SDK for Python can be a powerful tool for monitoring and troubleshooting your\nDatabricks Python projects. Developers can use the various logging methods and configuration options provided by the SDK to customize\nthe logging output to their specific needs.\n\n## Interaction with `dbutils`\n\nYou can use the client-side implementation of [`dbutils`](https://docs.databricks.com/dev-tools/databricks-utils.html) by accessing `dbutils` property on the `WorkspaceClient`.\nMost of the `dbutils.fs` operations and `dbutils.secrets` are implemented natively in Python within Databricks SDK. Non-SDK implementations still require a Databricks cluster,\nthat you have to specify through `cluster_id` configuration attribute or `DATABRICKS_CLUSTER_ID` environment variable. Don't worry if cluster is not running: internally,\nDatabricks SDK for Python calls `w.clusters.ensure_cluster_is_running()`.\n\n```python\nfrom databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\ndbutils = w.dbutils\n\nfiles_in_root = dbutils.fs.ls('/')\nprint(f'number of files in root: {len(files_in_root)}')\n```\n\nAlternatively, you can import `dbutils` from `databricks.sdk.runtime` module, but you have to make sure that all configuration is already [present in the environment variables](#default-authentication-flow):\n\n```python\nfrom databricks.sdk.runtime import dbutils\n\nfor secret_scope in dbutils.secrets.listScopes():\n    for secret_metadata in dbutils.secrets.list(secret_scope.name):\n        print(f'found {secret_metadata.key} secret in {secret_scope.name} scope')\n```\n\n## Interface stability\n\nDuring the [Experimental](https://docs.databricks.com/release-notes/release-types.html) period, Databricks is actively working on stabilizing the Databricks SDK for Python's interfaces. API clients for all services are generated from specification files that are synchronized from the main platform. You are highly encouraged to pin the exact dependency version and read the [changelog](https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md) where Databricks documents the changes. Databricks may have minor [documented](https://github.com/databricks/databricks-sdk-py/blob/main/CHANGELOG.md) backward-incompatible changes, such as renaming the methods or some type names to bring more consistency.\n\n## Disclaimer\n- The product is in preview and not intended to be used in production;\n- The product may change or may never be released;\n- While we will not charge separately for this product right now, we may charge for it in the future. You will still incur charges for DBUs.\n- There's no formal support or SLAs for the preview - so please reach out to your account or other contact with any questions or feedback; and\n- We may terminate the preview or your access at any time;\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/databricks/databricks-sdk-py",
    "keywords": "databricks sdk",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "databricks-sdk",
    "package_url": "https://pypi.org/project/databricks-sdk/",
    "platform": null,
    "project_url": "https://pypi.org/project/databricks-sdk/",
    "project_urls": {
      "Homepage": "https://github.com/databricks/databricks-sdk-py"
    },
    "release_url": "https://pypi.org/project/databricks-sdk/0.0.7/",
    "requires_dist": [
      "requests (>=2.28.1)",
      "autoflake ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "pycodestyle ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "pytest-mock ; extra == 'dev'",
      "pytest-xdist ; extra == 'dev'",
      "wheel ; extra == 'dev'",
      "yapf ; extra == 'dev'"
    ],
    "requires_python": ">=3.7",
    "summary": "Databricks SDK for Python (Experimental)",
    "version": "0.0.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17499818,
  "releases": {
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "87c3367e46700eece6ad68a57cfc31c354558d75aa9b04b122700585b4ce07a5",
          "md5": "51eeca0f0d45e34ddfe0ab7259fc63aa",
          "sha256": "6a4d5707cbf61b350e0e2318e29a6f796dce0bf5d3cd3ca7a99868ae3bc97276"
        },
        "downloads": -1,
        "filename": "databricks_sdk-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "51eeca0f0d45e34ddfe0ab7259fc63aa",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 208843,
        "upload_time": "2023-03-24T17:37:34",
        "upload_time_iso_8601": "2023-03-24T17:37:34.047634Z",
        "url": "https://files.pythonhosted.org/packages/87/c3/367e46700eece6ad68a57cfc31c354558d75aa9b04b122700585b4ce07a5/databricks_sdk-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4681dc75704d42013e763b0795828bc33f940d142ccafe208023a8acadd4840e",
          "md5": "50ae13ba0528de62049e6b9e5f89ec26",
          "sha256": "7dbdf6d20ba487b22927ac21d6052d9239acf54039e1f65b8601191096db9adc"
        },
        "downloads": -1,
        "filename": "databricks_sdk-0.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "50ae13ba0528de62049e6b9e5f89ec26",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 211722,
        "upload_time": "2023-03-29T21:15:47",
        "upload_time_iso_8601": "2023-03-29T21:15:47.331800Z",
        "url": "https://files.pythonhosted.org/packages/46/81/dc75704d42013e763b0795828bc33f940d142ccafe208023a8acadd4840e/databricks_sdk-0.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4681dc75704d42013e763b0795828bc33f940d142ccafe208023a8acadd4840e",
        "md5": "50ae13ba0528de62049e6b9e5f89ec26",
        "sha256": "7dbdf6d20ba487b22927ac21d6052d9239acf54039e1f65b8601191096db9adc"
      },
      "downloads": -1,
      "filename": "databricks_sdk-0.0.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "50ae13ba0528de62049e6b9e5f89ec26",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 211722,
      "upload_time": "2023-03-29T21:15:47",
      "upload_time_iso_8601": "2023-03-29T21:15:47.331800Z",
      "url": "https://files.pythonhosted.org/packages/46/81/dc75704d42013e763b0795828bc33f940d142ccafe208023a8acadd4840e/databricks_sdk-0.0.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}