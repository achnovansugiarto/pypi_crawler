{
  "info": {
    "author": "Alison Ferrenha",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.11",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "gpt3_tokenizer\n===============\n| An `OpenAI`_ GPT3 helper library for encoding/decoding strings and counting tokens.\n| Counting tokens gives the same output as OpenAI's `tokenizer`_\n|\n| Tested with versions: **2.7.12**, **2.7.18** and all **3.x.x** versions\n\nInstalling\n--------------\n.. code-block:: bash\n\n    pip install gpt3_tokenizer\n\n    \nExamples\n---------------------\n\n**Encoding/decoding a string**\n\n.. code-block:: python\n\n    import gpt3_tokenizer\n\n    a_string = \"That's my beautiful and sweet string\"\n    encoded = gpt3_tokenizer.encode(a_string) # outputs [2504, 338, 616, 4950, 290, 6029, 4731]\n    decoded = gpt3_tokenizer.decode(encoded) # outputs \"That's my beautiful and sweet string\"\n\n**Counting tokens**\n\n.. code-block:: python\n\n    import gpt3_tokenizer\n\n    a_string = \"That's my beautiful and sweet string\"\n    tokens_count = gpt3_tokenizer.count_tokens(a_string) # outputs 7\n\n.. _tokenizer: https://platform.openai.com/tokenizer\n.. _OpenAI: https://openai.com/",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/alisonjf/gpt3-tokenizer",
    "keywords": "openai,gpt,gpt-3,gpt3,gpt4,gpt-4,tokenizer",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "gpt3-tokenizer",
    "package_url": "https://pypi.org/project/gpt3-tokenizer/",
    "platform": null,
    "project_url": "https://pypi.org/project/gpt3-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/alisonjf/gpt3-tokenizer",
      "Repository": "https://github.com/alisonjf/gpt3-tokenizer"
    },
    "release_url": "https://pypi.org/project/gpt3-tokenizer/0.1.3/",
    "requires_dist": [
      "future (>=0.18.3,<0.19.0)",
      "six (>=1.16.0,<2.0.0)",
      "regex (==2021.11.10) ; python_version < \"3\"",
      "regex ; python_version >= \"3\""
    ],
    "requires_python": ">=2.7",
    "summary": "Encoder/Decoder and tokens counter for GPT3",
    "version": "0.1.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17439631,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "76559bc7de2dad3bfb159f3e0c8c4451549b8149da879f3649570799e32d00eb",
          "md5": "ad2e100b72e60ca7313f4e4948645f51",
          "sha256": "e6f5c4416c2b3251d845e27b36a2b6b3b49c5bd5264ef7d59bb311a9b44083e6"
        },
        "downloads": -1,
        "filename": "gpt3_tokenizer-0.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ad2e100b72e60ca7313f4e4948645f51",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=2.7",
        "size": 567702,
        "upload_time": "2023-03-24T15:02:41",
        "upload_time_iso_8601": "2023-03-24T15:02:41.654058Z",
        "url": "https://files.pythonhosted.org/packages/76/55/9bc7de2dad3bfb159f3e0c8c4451549b8149da879f3649570799e32d00eb/gpt3_tokenizer-0.1.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d1886a413a48c3d62d63f0a04c10b66ab85c8280fd2ef7dbee84457abde6dfbd",
          "md5": "3debdfaaa4fb8ff656066d7de3c996ff",
          "sha256": "f8f8c3c0ce798052bbe26aeb9fbf555c4cbbcb3b27e13ce356435c60436cfe33"
        },
        "downloads": -1,
        "filename": "gpt3_tokenizer-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3debdfaaa4fb8ff656066d7de3c996ff",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 560568,
        "upload_time": "2023-03-24T15:02:45",
        "upload_time_iso_8601": "2023-03-24T15:02:45.073854Z",
        "url": "https://files.pythonhosted.org/packages/d1/88/6a413a48c3d62d63f0a04c10b66ab85c8280fd2ef7dbee84457abde6dfbd/gpt3_tokenizer-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1c8304830bef74a62a7c985d6e236331588b064b087facf78f176dd6a1b0652f",
          "md5": "c6f01a429ab5f8c90e9f6cb1691031be",
          "sha256": "76eb85cf96ff983b1bbd158e7d69fec44ea51a932af73f6548fe25b122fab5dc"
        },
        "downloads": -1,
        "filename": "gpt3_tokenizer-0.1.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c6f01a429ab5f8c90e9f6cb1691031be",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=2.7",
        "size": 567813,
        "upload_time": "2023-03-24T18:18:47",
        "upload_time_iso_8601": "2023-03-24T18:18:47.243163Z",
        "url": "https://files.pythonhosted.org/packages/1c/83/04830bef74a62a7c985d6e236331588b064b087facf78f176dd6a1b0652f/gpt3_tokenizer-0.1.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5495724907d0ffd9d2ff8b919ca2f46ded3b43ab1643ec7c738ef56b443e4ab1",
          "md5": "044210b1b86004cbef8957294cc88a4d",
          "sha256": "b5b58d656af1665cad1bad77452c27c4b958f83edb59fc694b05adf3f46e50d0"
        },
        "downloads": -1,
        "filename": "gpt3_tokenizer-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "044210b1b86004cbef8957294cc88a4d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 560690,
        "upload_time": "2023-03-24T18:18:51",
        "upload_time_iso_8601": "2023-03-24T18:18:51.546553Z",
        "url": "https://files.pythonhosted.org/packages/54/95/724907d0ffd9d2ff8b919ca2f46ded3b43ab1643ec7c738ef56b443e4ab1/gpt3_tokenizer-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ddafaec919d10f5d8c6cbff09408fdcd8dcc88836da8317e29113dc1a5a21164",
          "md5": "566c793028232a992583a453c9954ab4",
          "sha256": "bd04b9287dc130dbcd1ca161c9cdc013c0da90a44296b308ceb34463423d67cf"
        },
        "downloads": -1,
        "filename": "gpt3_tokenizer-0.1.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "566c793028232a992583a453c9954ab4",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=2.7",
        "size": 567876,
        "upload_time": "2023-03-24T21:29:51",
        "upload_time_iso_8601": "2023-03-24T21:29:51.005573Z",
        "url": "https://files.pythonhosted.org/packages/dd/af/aec919d10f5d8c6cbff09408fdcd8dcc88836da8317e29113dc1a5a21164/gpt3_tokenizer-0.1.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f0dd376c1c6977ba6e0ee676271172c528f51b8e06f3c8df76c27432d12a5763",
          "md5": "6bfeea23de24535c0878d951e7abccf2",
          "sha256": "ae31ab1e175c6179ae91ae514ad664dce05808d987585fed4376022da76f2c00"
        },
        "downloads": -1,
        "filename": "gpt3_tokenizer-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "6bfeea23de24535c0878d951e7abccf2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 560703,
        "upload_time": "2023-03-24T21:29:54",
        "upload_time_iso_8601": "2023-03-24T21:29:54.587809Z",
        "url": "https://files.pythonhosted.org/packages/f0/dd/376c1c6977ba6e0ee676271172c528f51b8e06f3c8df76c27432d12a5763/gpt3_tokenizer-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1ea79b825973eb7933cec48dfdce0db81ef6e901971f6e8bb2c440617676e2c0",
          "md5": "1b365c634f166655dd84070790da8c41",
          "sha256": "5af4b2b7f0ec533792cf133a66feab6c482a0434721abecc608e5b26d7b98e7f"
        },
        "downloads": -1,
        "filename": "gpt3_tokenizer-0.1.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1b365c634f166655dd84070790da8c41",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=2.7",
        "size": 567876,
        "upload_time": "2023-03-25T04:58:59",
        "upload_time_iso_8601": "2023-03-25T04:58:59.916604Z",
        "url": "https://files.pythonhosted.org/packages/1e/a7/9b825973eb7933cec48dfdce0db81ef6e901971f6e8bb2c440617676e2c0/gpt3_tokenizer-0.1.3-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "14dbcfeb12acecaa1c26578cf3918efd824dada9bd51f36ae5e257e1042ef26e",
          "md5": "4a3d965df5c22a8e1bc02509253347bb",
          "sha256": "6edef0c8594f34bbe623692a74bb7ab843b8291cce9cede7ea2ab0926c525131"
        },
        "downloads": -1,
        "filename": "gpt3_tokenizer-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "4a3d965df5c22a8e1bc02509253347bb",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 560716,
        "upload_time": "2023-03-25T04:59:03",
        "upload_time_iso_8601": "2023-03-25T04:59:03.432640Z",
        "url": "https://files.pythonhosted.org/packages/14/db/cfeb12acecaa1c26578cf3918efd824dada9bd51f36ae5e257e1042ef26e/gpt3_tokenizer-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1ea79b825973eb7933cec48dfdce0db81ef6e901971f6e8bb2c440617676e2c0",
        "md5": "1b365c634f166655dd84070790da8c41",
        "sha256": "5af4b2b7f0ec533792cf133a66feab6c482a0434721abecc608e5b26d7b98e7f"
      },
      "downloads": -1,
      "filename": "gpt3_tokenizer-0.1.3-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1b365c634f166655dd84070790da8c41",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": ">=2.7",
      "size": 567876,
      "upload_time": "2023-03-25T04:58:59",
      "upload_time_iso_8601": "2023-03-25T04:58:59.916604Z",
      "url": "https://files.pythonhosted.org/packages/1e/a7/9b825973eb7933cec48dfdce0db81ef6e901971f6e8bb2c440617676e2c0/gpt3_tokenizer-0.1.3-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "14dbcfeb12acecaa1c26578cf3918efd824dada9bd51f36ae5e257e1042ef26e",
        "md5": "4a3d965df5c22a8e1bc02509253347bb",
        "sha256": "6edef0c8594f34bbe623692a74bb7ab843b8291cce9cede7ea2ab0926c525131"
      },
      "downloads": -1,
      "filename": "gpt3_tokenizer-0.1.3.tar.gz",
      "has_sig": false,
      "md5_digest": "4a3d965df5c22a8e1bc02509253347bb",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=2.7",
      "size": 560716,
      "upload_time": "2023-03-25T04:59:03",
      "upload_time_iso_8601": "2023-03-25T04:59:03.432640Z",
      "url": "https://files.pythonhosted.org/packages/14/db/cfeb12acecaa1c26578cf3918efd824dada9bd51f36ae5e257e1042ef26e/gpt3_tokenizer-0.1.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}