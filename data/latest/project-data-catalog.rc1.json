{
  "info": {
    "author": null,
    "author_email": null,
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Data Catalog\n\nA catalog to define, create, store, and access datasets.\n\nThis python package aims to streamline data engineering and data analysis during data science projects:\n\n- organize all datasets used or created by your project,\n- define datasets as a transformation of others,\n- easily propagate updates when datasets are updated,\n- avoid boilerplate code,\n- access datasets from anywhere in your code, without having to remember file paths,\n- share dataset definitions within a project team,\n- document datasets,\n- and enable smooth transitions from exploration to deployment.\n\nMany data cataloging python packages exist (kedro, prefect, ...) for slightly different use cases. This package is tailored for managing datasets during data science projects. The emphasis is on minimal boilerplate, easy data access, and no-effort updates.\n\n\n## Installation\n\nUse a python environment compatible with this project, e.g. with conda:\n```\nconda create -n my_env python \"pandas>=0.19\" \"dask>=0.2.0\" \"s3fs>=0.2.0\" \"pytz>=2011k\" pytest pyarrow\n```\n\nInstall this package:\n```\npip install git+https://github.com/numerical-io/data_catalog.git@main\n```\n\n## Example 1: a catalog of datasets\n\nA data catalog is defined in a python module or package. The following catalog defines three classes, each representing a dataset:\n\n- DatasetA, defined by code (`create` function),\n- DatasetB, defined as a transformation of DatasetA (`create` function and `parents` attribute),\n- DatasetC, defined and read from a CSV file (`relative_path` attribute).\n\nEach class inherits from a class defining the data format on disk. This example uses CSV and parquet files.\n\n```python\n# example_catalog.py\n\nimport pandas as pd\nfrom data_catalog.datasets import CsvDataset, ParquetDataset\n\n\nclass DatasetA(CsvDataset):\n    \"\"\"A dataset defined in code, and saved as a CSV file.\n    \"\"\"\n\n    def create(self):\n        df_a = pd.DataFrame({\"col1\": [1, 2, 3]})\n        return df_a\n\n    read_kwargs = {\"index_col\": 0}\n\n\nclass DatasetB(ParquetDataset):\n    \"\"\"A dataset defined from another dataset, and saved as a Parquet file.\n    \"\"\"\n\n    parents = [DatasetA]\n\n    def create(self, df_a):\n        df_b = 2 * df_a\n        return df_b\n\n\nclass DatasetC(CsvDataset):\n    \"\"\"A dataset defined in a CSV file.\n    \"\"\"\n\n    relative_path = \"dataset_c.csv\"\n\n```\n\nThis catalog definition contains all that is needed. The datasets defined in code can be generated by a succession of tasks, that we encode in a task graph. The task graph follows the Dask DAG format, and we execute it with Dask. (The graph itself is otherwise independent from Dask, and could be run by an engine of your choice.)\n\n```python\nfrom data_catalog.taskgraph import create_task_graph\nfrom dask.threaded import get\n\nfrom example_catalog import DatasetA, DatasetB\n\n# Define a context. The context is necessary to instanciate datasets.\n# It contains an URI indicating where to save all the catalog's datasets.\ncontext = {\n    \"catalog_uri\": \"file:///path/to/data/folder\"\n}\n\n# Generate a task graph to create datasets, resolving dependencies between them.\ndatasets = [DatasetA, DatasetB] # leave out DatasetC unless you provide a file dataset_c.csv\ntaskgraph, targets = create_task_graph(datasets, context)\n\n# Let Dask generate all datasets on disk\n_ = get(taskgraph, targets)\n```\n\nOnce the files are created, you can access datasets from anywhere in your project.\n\n```python\ndataset_b = DatasetB(context)\n\n# Load into a dataframe\ndf = dataset_b.read()\n\n# View its description\ndataset_b.description()\n\n# View the file path\ndataset_b.path()\n\n```\n\n## Example 2: a catalog with collections of datasets\n\nSometimes data is available as a collection of identically formatted files. Collections of datasets are available to handle this case.\n\nCollections can be defined in a catalog as follows:\n\n```python\n# example_catalog.py\n\nimport pandas as pd\nfrom data_catalog.datasets import ParquetDataset\nfrom data_catalog.collections import FileCollection, same_key_in\n\n\nclass CollectionA(FileCollection):\n    \"\"\"A collection of datasets saved as Parquet files.\n    \"\"\"\n\n    def keys(self):\n        return [\"file_1\", \"file_2\", \"file_3\"]\n\n    class Item(ParquetDataset):\n        def create(self):\n            df = pd.DataFrame({\"col1\": [1, 2, 3]})\n            return df\n\n\nclass CollectionB(FileCollection):\n    \"\"\"A collection defined from CollectionA.\n\n    Each item corresponds to one item in CollectionA.\n    \"\"\"\n\n    def keys(self):\n        return [\"file_1\", \"file_2\", \"file_3\"]\n\n    class Item(ParquetDataset):\n        parents = [same_key_in(CollectionA)]\n\n        def create(self, df):\n            return 2 * df\n\n\nclass DatasetD(ParquetDataset):\n    \"\"\"A dataset concatenating all items from CollectionA.\n    \"\"\"\n\n    parents = [CollectionA]\n\n    def create(self, collection_a):\n        df = pd.concat(collection_a)\n        return df\n```\n\nThe generation of files is identical as in the previous example:\n\n```python\nfrom data_catalog.taskgraph import create_task_graph\nfrom dask.threaded import get\n\nfrom example_catalog import CollectionA, CollectionB, DatasetD\n\n# Define the catalog's context\ncontext = {\n    \"catalog_uri\": \"file:///path/to/data/folder\"\n}\n\n# Generate the task graph and run it with Dask\ntaskgraph, targets = create_task_graph(\n    [CollectionA, CollectionB, DatasetD], context\n)\n_ = get(taskgraph, targets)\n```\n\nYou can then access data anywhere in your project.\n\n```python\n# Load a collection\nCollectionA(context).read()\n\n# Get a single dataset from a collection.\nitem_2 = CollectionA.get(\"file_2\")\n\n# item_2 is a usual dataset object\ndf = item_2(context).read()\n```\n\nThe task graph only includes necessary updates. If all files exist and parents have older update times than their children, no task will be executed. If however you modify a file, the task graph will contain tasks to update all its descendants. When modifying the code of a dataset, remove the corresponding file to trigger its re-creation, and the updates of all its descendants.\n\n\n## Dataset attributes\n\nWhen defining a dataset class, you can set the following attributes:\n\n- `parents`: A list of dataset/collection classes from which this dataset is defined.\n- `create`: A method to create the dataset. It takes as inputs, aside from `self`, the data loaded from all classes in `parents`. The number of input arguments (not counting `self`) must therefore be equal to the length of `parents`. The method must return the created data.\n- `relative_path`: The file path, relative to the catalog URI.\n- `file_extension`: The file extension.\n- `is_binary_file`: A boolean indicating whether the file is a text or binary file.\n- `read_kwargs`: A dict of keyword arguments for reading the dataset.\n- `write_kwargs`: A dict of keyword arguments for writing the dataset.\n\nAll these attributes are optional, and have default values if omitted.\n\nWhen relative_path is missing, it is inferred from the class name and path in the package. For instance, a CSV dataset `SomeDataset` defined in the submodule `example_catalog.part_one` will have a relative path set to `part_one/SomeDataset.csv`.\n\nIf a docstring is set, it becomes the dataset description available through the `description()` method.\n\nDatasets must inherit from a subclass of `AbstractDataset`. The data catalog provides a few such classes for common cases: `CsvDataset`, `ParquetDataset`, `PickleDataset`, `ExcelDataset`, and `YamlDataset`.\n\n\n## Collection attributes\n\nA collection can have the following attributes:\n\n- `Item`: A nested class defining a dataset in the collection. It is a template for each item in the collection.\n- `keys`: A method returning a list of keys. Each key maps to a collection item. Files in the collection are named after keys, and conversely.\n- `relative_path`: If set, this path refers to the directory containing collection data files. This value is used to define the `relative_path` for each `Item`.\n\nCollections inherit from `FileCollection`.\n\nCollection have a class method `get` that returns dataset classes for given keys.\n\n\n## Managing the catalog\n\nThe data files reside at the URI set in the `context` variable, used for instanciating all objects. The catalog supports, as of now, URI's pointing to local files (`file://`) or to S3 (`s3://`). Note that the catalog itself is defined independently of its location; only data instances are dependent on the context. This facilitates the creation of several copies, e.g. for sharing between different users or versioning datasets.\n\nTo view all datasets and collections defined in a catalog, use the following functions:\n```python\nfrom data_catalog.utils import describe_catalog, list_catalog\n\nimport example_catalog\n\n# Get dataset names and descriptions, in a dict.\ndescribe_catalog(example_catalog)\n\n# List all classes representing datasets and collections.\n# If example_catalog is a package, the list will contain\n# classes from all _imported_ submodules.\nlist_catalog(example_catalog)\n\n```\n\nWhen running the task graph, each task logs messages to a logger named data_catalog. This logger configuration will show messages on sys.stderr:\n\n```python\nimport logging\n\nlogger = logging.getLogger(\"data_catalog\")\n\nlogger.setLevel(logging.INFO)\nlog_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nlog_handler = logging.StreamHandler()\nlog_handler.setLevel(logging.INFO)\nlog_handler.setFormatter(log_formatter)\nlogger.addHandler(log_handler)\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": null,
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": null,
    "keywords": null,
    "license": null,
    "maintainer": null,
    "maintainer_email": null,
    "name": "project-data-catalog",
    "package_url": "https://pypi.org/project/project-data-catalog/",
    "platform": null,
    "project_url": "https://pypi.org/project/project-data-catalog/",
    "project_urls": {
      "Homepage": "https://github.com/numerical-io/data_catalog"
    },
    "release_url": "https://pypi.org/project/project-data-catalog/0.3.1/",
    "requires_dist": [
      "dask>=0.2.0",
      "pandas>=0.19",
      "pytz>=2014",
      "s3fs>=0.2.0"
    ],
    "requires_python": ">=3.7",
    "summary": "A catalog to define, create, store, and access datasets",
    "version": "0.3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17287428,
  "releases": {
    "0.3.1": [
      {
        "comment_text": null,
        "digests": {
          "blake2b_256": "c823bdced0d7418312e0fdddae39237857a52d6418270e7d1e14f046b3be8a1f",
          "md5": "4a936209469c65352d99ba9c616cc6e9",
          "sha256": "2b4eba5f4470834161f2fcaa0746aa317bfe3bcbb7f714198c83833b2025042c"
        },
        "downloads": -1,
        "filename": "project_data_catalog-0.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4a936209469c65352d99ba9c616cc6e9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 16310,
        "upload_time": "2023-03-14T13:11:42",
        "upload_time_iso_8601": "2023-03-14T13:11:42.680366Z",
        "url": "https://files.pythonhosted.org/packages/c8/23/bdced0d7418312e0fdddae39237857a52d6418270e7d1e14f046b3be8a1f/project_data_catalog-0.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": null,
        "digests": {
          "blake2b_256": "0baaf44c5e803d2817b715bd0972c92b7392e5f33e87fe02c156147fbcf1db30",
          "md5": "5f9b9b89f0c94ccf0be10635763330a6",
          "sha256": "d61d54e89e42e610594f24923cc1c81d8e16f6d6bafd1e09cda5721b1ef527cd"
        },
        "downloads": -1,
        "filename": "project_data_catalog-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5f9b9b89f0c94ccf0be10635763330a6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 13851,
        "upload_time": "2023-03-14T13:11:44",
        "upload_time_iso_8601": "2023-03-14T13:11:44.719361Z",
        "url": "https://files.pythonhosted.org/packages/0b/aa/f44c5e803d2817b715bd0972c92b7392e5f33e87fe02c156147fbcf1db30/project_data_catalog-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": null,
      "digests": {
        "blake2b_256": "c823bdced0d7418312e0fdddae39237857a52d6418270e7d1e14f046b3be8a1f",
        "md5": "4a936209469c65352d99ba9c616cc6e9",
        "sha256": "2b4eba5f4470834161f2fcaa0746aa317bfe3bcbb7f714198c83833b2025042c"
      },
      "downloads": -1,
      "filename": "project_data_catalog-0.3.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4a936209469c65352d99ba9c616cc6e9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 16310,
      "upload_time": "2023-03-14T13:11:42",
      "upload_time_iso_8601": "2023-03-14T13:11:42.680366Z",
      "url": "https://files.pythonhosted.org/packages/c8/23/bdced0d7418312e0fdddae39237857a52d6418270e7d1e14f046b3be8a1f/project_data_catalog-0.3.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": null,
      "digests": {
        "blake2b_256": "0baaf44c5e803d2817b715bd0972c92b7392e5f33e87fe02c156147fbcf1db30",
        "md5": "5f9b9b89f0c94ccf0be10635763330a6",
        "sha256": "d61d54e89e42e610594f24923cc1c81d8e16f6d6bafd1e09cda5721b1ef527cd"
      },
      "downloads": -1,
      "filename": "project_data_catalog-0.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "5f9b9b89f0c94ccf0be10635763330a6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 13851,
      "upload_time": "2023-03-14T13:11:44",
      "upload_time_iso_8601": "2023-03-14T13:11:44.719361Z",
      "url": "https://files.pythonhosted.org/packages/0b/aa/f44c5e803d2817b715bd0972c92b7392e5f33e87fe02c156147fbcf1db30/project_data_catalog-0.3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}