{
  "info": {
    "author": "Chris Henn",
    "author_email": "chenn@alum.mit.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 1 - Planning",
      "Environment :: GPU :: NVIDIA CUDA",
      "Intended Audience :: Science/Research",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: Implementation :: CPython"
    ],
    "description": "\n\n<div align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/chrishenn/cuneb-chenn/main/doc/images/package.jpeg?raw=True\" height=\"200\" >\n</div>\n\n<h1 align=\"center\" style=\"margin-top: 0px;\">Example Pytorch-CUDA-CMake Library Pip-Package</h1>\n\n&emsp;\n\nAn example pip-package for a CUDA/C++ Pytorch/Torchlib extension. Allows installing via pip, and importing to Pytorch or Torchscript. We can therefore specify package versions; install python dependencies automatically; and build the package from source on a target machine.\n\nThe benefits of compiling a python model into torchscript are many - see the tutorial for a nice list [(extending torchscript tutorial)](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html) - but my main interest is in the performance and scalability offered by serializing a model to disk (which can be loaded into multiple processes), and running a model in a python-free environment.\n\nIf our model makes use of custom CUDA/C++ projects, it can be tedious and manual work to compile and import them - not to mention keeping track of versions and subversions.\n\nWe choose CMake compilation for our CUDA/C++ library, rather than setuptools. CMake allows for fewer headaches in the extension's project structure - finding includes, headers, and source files is easier - and is generally worth the effort of including another build tool. As the [extending torchscript tutorial](https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html) casually mentions, using setuptools to build a plain shared library can be \"slightly quirky\". For more complex projects, I've found this to be a charming understatement.\n\nCMake functionality in setup.py adapted from [raydouglass/cmake_setuptools](https://github.com/raydouglass/cmake_setuptools).\n\n\n## Using This Example\n\nInstalling the package from source, at install time we must specify the path to our appropriate version of the libtorch library on our machine's filesystem. This example includes dependencies on libtorch-1.8.2, pytorch-1.8.2, cuda-11, cudnn-8.xxx, and gcc~9.\n\nTo install the package from a local folder, open a shell in the package folder; run: \n\n    LIBTORCH_PATH=\"/home/chris/Documents/libtorch\" pip install .\n\nTo call the extension from an environment with this package installed, we can then: \n\n    import modulename\n    output = modulename.get(*args)\n\nTo build a package distribution, including a source and a system-specific wheel, run:\n\n    LIBTORCH_PATH=\"/home/chris/Documents/libtorch\" python -m build\n\nWheels with 'linux-arch' tags cannot be uploaded to pypa - see 'Manylinux' section below. \n\nIf we remove the linux-tagged .whl from the pkg/dist/, we can then upload the package to pypi. Only installation from source will be supported, via the package-version.tar.gz file built by 'build'. Pip will download and unzip the package, enforce dependencies, provide versioning, and install from source.\n\nAlternatively, we can build only the 'source' distribution:\n\n    LIBTORCH_PATH=\"/home/chris/Documents/libtorch\" python setup.py sdist\n\nTo upload the dist/ directory, set `token=our-secret-pypi-api-token` in our shell environment. Then:\n\n    twine upload --skip-existing --verbose -p $token -u __token__ --repository \"cuneb-chenn\" --repository-url https://upload.pypi.org/legacy/ dist/*\n\nThen to install in a fresh environment, pip will download the package source from pypi and build a local wheel to install:\n\n    LIBTORCH_PATH=\"/home/chris/Documents/libtorch\" pip install cuneb-chenn\n\n\n\n\n\n## Adapting This Example\n\nFile structure:\n\n    pkg \n    |-  setup.py\n    |-  src \n    |   |-  module\n    |   |   |-   .env\n    |   |   |-   __init__.py\n    |   |   |-   CMakeLists.txt\n    |   |   |-   file.cpp\n    |   |   |-   file.h\n    |   |   |-   file.cu\n    |   |   |-   file.cuh\n    |-  test\n    |   |-  test_module.py\n\nIn the module/.env file, there are environment variable definitions (PKG_NAME MOD_NAME MOD_PATH OPS_NAME). PKG_NAME, MOD_NAME, and MOD_PATH correspond to package folder name, module folder name, and path to the module folder from setup.py. OPS_NAME defines the torch.ops.OPS_NAME that the extension must be bound to. Setup.py will use python-dotenv to load these variables into the system environment at build time. Then, the installed module will reference them in the same way to find its own *.so file. \n\n    \".env\":\n\n    PKG_NAME=cuneb-chenn\n    MOD_NAME=cuneb\n    MOD_PATH=src/cuneb/\n    OPS_NAME=cuneb_ops\n\nThere are two additional places where these environment vars must be hardcoded: one at the top of setup.py, and one in pkg/module/module.cpp. These module names must agree with those in the .env file.\n\n\n\n\n\n\n\n## ManyLinux\n\nWheels built on manylinux images and repaired with wheel-repair can then be uploaded to pypi. However, manylinux wheels for this package are much too large to upload because of its dependencies (seemingly CUDA-11 is the main offender here).\n\nRun `sudo sh run-manylinux.sh` to start building manylinux wheels.\n\n`run-manylinux.sh` specifies a manylinux docker container inside which the manylinux wheels are built (and tested - todo). You'll need docker installed. I've found manylinux containers built with CUDA installed [(ameli/manylinux-cuda)](https://github.com/ameli/manylinux-cuda) which is very helpful, because the build is cantankerous and slow. However, it would be nice to build a version with an appropriate cudnn installed (todo). \n\n`build-wheels.sh` is run inside the container to build and format the wheels.\n\n### Notes\nUse a machine with more than 64 GB of ram (maybe more like 256 GB) - push this to the build server (super easy CI integration with travis - see [the manylinux github tutorial](https://github.com/pypa/python-manylinux-demo) for info). I copied a starting point for the two manylinux shell scripts from the same.   \n\nThe wheel-size problem appears to crop up frequently. Pypa folks specifically call out Cuda-11 (and later) dependencies as inflating the .whl size [(see here)](https://discuss.python.org/t/what-to-do-about-gpus-and-the-built-distributions-that-support-them/7125). As mentioned in that discussion, Pytorch has had to host and distribute their own python packages because the infrastructure to store and deliver them is too expensive for Pypa. \n\nThis packaging exercise is still worthwhile for learning about CI. In particular, the cpu and memory requirements are quite high - just to build this simple little package! Suitable for an internal build and distribution server only. \n\n\n ## TODO\n\n- TODO: add tests\n- TODO: --use-feature=in-tree-build (will be default behavior in future. See if it breaks the package build)\n\n\n- TODO: build-wheels.sh ends with an EOF error, but runs successfully to completion. Would be nice to finish with a success message rather than an error.\n- TODO: local cudnn install in build-wheel.sh\n- TODO: figure out what those -pypy-something binaries are in build-wheels.h\n- TODO: build docker manylinux images with latest manylinux version, cuda, and cudnn built in\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/chrishenn/cuneb-chenn",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cuneb-chenn",
    "package_url": "https://pypi.org/project/cuneb-chenn/",
    "platform": null,
    "project_url": "https://pypi.org/project/cuneb-chenn/",
    "project_urls": {
      "Homepage": "https://github.com/chrishenn/cuneb-chenn"
    },
    "release_url": "https://pypi.org/project/cuneb-chenn/0.0.4/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A simple package to wrap a pytorch CUDA/C++ extension",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13677484,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "946a12fd826972ee345a8a4576cd9bf71a011e8edf5f0295fdb14646bf4d7c54",
          "md5": "3d120b301e4ff989710820e0380cdba1",
          "sha256": "181052f6b8c0ea18f719fe4b921097874618cceae506bc5d98401838e430b9a8"
        },
        "downloads": -1,
        "filename": "cuneb-chenn-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3d120b301e4ff989710820e0380cdba1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 8668,
        "upload_time": "2022-04-30T17:17:46",
        "upload_time_iso_8601": "2022-04-30T17:17:46.209276Z",
        "url": "https://files.pythonhosted.org/packages/94/6a/12fd826972ee345a8a4576cd9bf71a011e8edf5f0295fdb14646bf4d7c54/cuneb-chenn-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "55ad435abec289172e04e1ee002f3471e65364b12eb54d724adb75fb62713779",
          "md5": "403ebc0b5462a863332f76295f705627",
          "sha256": "4e5d6c0e83dfb93961950427ee2189d9db4305562153147c22b3b15da9b362f4"
        },
        "downloads": -1,
        "filename": "cuneb-chenn-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "403ebc0b5462a863332f76295f705627",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9399,
        "upload_time": "2022-04-30T19:00:34",
        "upload_time_iso_8601": "2022-04-30T19:00:34.242294Z",
        "url": "https://files.pythonhosted.org/packages/55/ad/435abec289172e04e1ee002f3471e65364b12eb54d724adb75fb62713779/cuneb-chenn-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cb8698cedcd69c60c7ae7f1105400bf539e90f5d598436ea69a33a3a33fe8dd0",
          "md5": "5610950d039923d008afee25c09b8571",
          "sha256": "551a5f17419801db701b8a109b0d6fba0e2b2e6784cb1edf038652027de10741"
        },
        "downloads": -1,
        "filename": "cuneb-chenn-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "5610950d039923d008afee25c09b8571",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9375,
        "upload_time": "2022-04-30T19:12:04",
        "upload_time_iso_8601": "2022-04-30T19:12:04.463699Z",
        "url": "https://files.pythonhosted.org/packages/cb/86/98cedcd69c60c7ae7f1105400bf539e90f5d598436ea69a33a3a33fe8dd0/cuneb-chenn-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08ec465b1629b03975bd6ae8bdf24278acb3c681d8a6513c83f322b76d2fd29b",
          "md5": "0110003b8279e6c8a896c55429646bf2",
          "sha256": "ec7fbe75c77c0d9dfe2b04e4403cc11527f8981b344972df73f0e9ae5d23859a"
        },
        "downloads": -1,
        "filename": "cuneb-chenn-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "0110003b8279e6c8a896c55429646bf2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10360,
        "upload_time": "2022-05-01T00:41:19",
        "upload_time_iso_8601": "2022-05-01T00:41:19.778108Z",
        "url": "https://files.pythonhosted.org/packages/08/ec/465b1629b03975bd6ae8bdf24278acb3c681d8a6513c83f322b76d2fd29b/cuneb-chenn-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "08ec465b1629b03975bd6ae8bdf24278acb3c681d8a6513c83f322b76d2fd29b",
        "md5": "0110003b8279e6c8a896c55429646bf2",
        "sha256": "ec7fbe75c77c0d9dfe2b04e4403cc11527f8981b344972df73f0e9ae5d23859a"
      },
      "downloads": -1,
      "filename": "cuneb-chenn-0.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "0110003b8279e6c8a896c55429646bf2",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 10360,
      "upload_time": "2022-05-01T00:41:19",
      "upload_time_iso_8601": "2022-05-01T00:41:19.778108Z",
      "url": "https://files.pythonhosted.org/packages/08/ec/465b1629b03975bd6ae8bdf24278acb3c681d8a6513c83f322b76d2fd29b/cuneb-chenn-0.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}