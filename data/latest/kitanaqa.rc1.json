{
  "info": {
    "author": "Senzeyu Zhang, Mostafa Mirshekari, Aaron Sisto",
    "author_email": "aaron@searchable.ai",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "<p align=\"center\"><img src=\"/assets/img/searchable-logo_full-lockup-horizontal_dark.png\" width=\"460\"></p>\n&nbsp\n<h1 align=\"center\">KitanaQA</h1>\n<p align=\"center\"><b>Tool[KIT] for [A]dversarial Trai[N]ing and [A]ugmentation in [Q]uestion [A]nswering</b></p>\n<p align=\"center\">\n  <a href=\"#about\">About</a> •\n  <a href=\"#features\">Features</a> •\n  <a href=\"#installation\">Install</a> •\n  <a href=\"#getting-started\">Getting Started</a> •\n  <a href=\"#examples\">Examples</a>\n  <br> <br>\n</p>\n\n[![CircleCI](https://circleci.com/gh/searchableai/KitanaQA.svg?style=shield&circle-token=de6470b621d1b07e54466dd087b85b80bcedf36c)](https://github.com/searchableai/KitanaQA)\n\n# About\n\nKitanaQA is an adversarial training and data augmentation framework for fine-tuning Transformer-based language models on question-answering datasets\n\n\n## *Why KitanaQA?*\nWhile NLP models have made incredible progress on curated question-answer datasets in recent years, they are still brittle and unpredictable in production environments, making productization and enterprise adoption problematic. KitanaQA provides resources to \"robustify\" Transformer-based question-answer models against many types of natural and synthetic noise. The major features are:\n1. **Adversarial Training** can increase both robustness and performance of fine-tuned Transformer QA models. Here, we implement *virtual adversarial training*, which introduces embedding-space perturbations during fine-tuning to encourage the model to produce more stable results in the presence of noisy inputs.\n\n  Our experiments with BERT finetuned on the SQuAD v1.1 question answering dataset show a marked improvement in f1 and em scores:\n\n  Model | em | f1\n  --- | --- | ---\n  BERT-base | 80.8 | 88.5\n  **BERT-base (ALUM)** | **81.97** | **88.92**\n\n2. **Augment Your Dataset** to increase model generalizability and robustness using token-level perturbations. While Adversarial Training provides some measure of robustness against bounded perturbations, Augmentation can accomodate a wide range of naturally-occuring noise in user input. We provide tools to augment existing SQuAD-like datasets by perturbing the examples along a number of different dimensions, including synonym replacement, misspelling, repetition and deletion.\n\n3. **Workflow Automation** to prototype robust NLP models faster for research and production. This package is structured for extremely easy use and deployment. Using Prefect Flows, training, evaluation, and model selection can be executed in a single line of code, enabling faster iteration and easier itergration of research into production pipelines.\n\n# Features\n\n## Adversarial Training\nOur implementation is based on the smoothness-inducing regularization approach proposed [here](https://arxiv.org/pdf/1605.07725.pdf). We have updated the implementation for fine-tuning on question-answer datasets, and added additional features like adversarial hyperparameter scheduling, and support for mixed-precision training.\n\n## Adversarial Attack\nA key measure of robustness in neural networks is the so-called white-box adversarial attack. In the context of Transformer-based Question-Answer models, this attack seeks to inject noise into the model's input embeddings and assess performance on the original labels. Here, we implement the projected gradient descent (PGD) attack mechanism, bounded by the norm-ball. Metrics can be calculated for non-adversarial and adversarial evaluation, making robustness studies more streamlined and accessible.\n\n## Data Augmentation\nThe following perturbation methods are available to augment SQuAD-like data:\n- Synonym Replacement (SR) via 1) [constrained word2vec](https://arxiv.org/pdf/1603.00892.pdf), and 2) MLM using BERT\n```diff\n- (original)  How many species of plants were [recorded] in Egypt?\n+ (augmented) How many species of plants were [registered] in Egypt?\n```\n- Random Deletion (RD) using entity-aware term selection\n```diff\n- (original)  How many species of plants [were] recorded in Egypt?\n+ (augmented) How many species of plants [] recorded in Egypt?\n```\n- Random Repetition (RR) using entity-aware term selection\n```diff\n- (original)  How many species of plants [were] recorded in Egypt?\n+ (augmented) How many species of plants [were were] recorded in Egypt?\n```\n- Random Misspelling (RM) using open-source common misspellings datasets\n    -- *sources: [wiki](https://en.wikipedia.org/wiki/Wikipedia:Lists_of_common_misspellings), [brikbeck](https://www.dcs.bbk.ac.uk/~ROGER/corpora.html)*\n```diff\n- (original)  How [many] species of plants were recorded in Egypt?\n+ (augmented) How [mony] species of plants were recorded in Egypt?\n```\nPerturbation types can be flexibly applied in combination with different frequencies for fine-grained control of natural noise profiles\n```diff\n- (original)  How [many] species [of] plants [were] recorded in Egypt?\n+ (augmented) How [mony] species [] plants [] recorded in Egypt?\n```\nEach perturbation type also supports custom term importance sampling, e.g. as generated using a MLM  \n```(How, 0.179), (many, 0.254), (species, 0.123), (of, 0.03), (plants, 0.136) (were, 0.039), (recorded, 0.067), (in, 0.012), (Egypt, 0.159)```\n\n## ML Flows\nUsing the Prefect library, KitanaQA makes it increadibly easy to combine different workflows for end-to-end training/evaluation/model selection. This system also supports rapid iteration in hyperparameter search by easily specifying each experimental condition and deploying independently. You can even get results [reported directly in Slack](https://docs.prefect.io/core/advanced_tutorials/slack-notifications.html)!\n\n# Installation\nEntity-aware data augmentations make use of the John Snow Labs [spark-nlp](https://github.com/JohnSnowLabs/spark-nlp) library, which requires pyspark. To enable this feature, make sure Java v8 is set by default for pyspark compatibility:  \n```\nsudo apt install openjdk-8-jdk\nsudo update-alternatives --config java\njava -version\n```\n\nIt is recommended that you use a [virtual environment](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/) when installing from pip or source. Virtualenv and Conda are good options.\n\nThis package has been tested on Python 3.7+, PyTorch 1.5.1+ and transformers 1.3.1\n\nInstall with pip:  \n```pip install kitanaqa```\n\nInstall from source:  \n```\ngit clone https://github.com/searchableai/KitanaQA.git\ncd KitanaQA\npython setup.py install\n```\n\n# Getting Started  \nTo run training or evaluation from the commandline:  \n```\npython src/kitanaqa/trainer/run_pipeline.py --args=args.json\n```\nSee an example [args.json](examples/commandline/args.json)\n\n# Examples\n\n## *Augmentation*\n- [Generating Augmented Datasets](examples/augment_squad)\n- [Custom Text Perturbations](examples/generate_token_perturbations)\n- [MLM Importance Scores](examples/generate_importance_scores_with_mlm)\n\n## *Training and Evaluation*\n- [Automated Training Pipeline](examples/training_and_evaluation)\n- [Adversarial Training](examples/alum_training_and_evaluation)\n- [Adversarial Attack](examples/adversarial_attack)\n\n## Models Supported\nWe make use of the following models and their respective tokenizers and configurations provided by Hugging Face Inc.\n- ALBERT\n- BERT\n- DistilBERT\n\n### Contributing to KitanaQA\n\nWe welcome suggestions and contributions! Submit an issue or pull request and we will do our best to respond in a timely manner.\nSee [CONTRIBUTING.md](https://github.com/searchableai/KitanaQA/blob/master/CONTRIBUTING.md) for detailed information on contributing.\n\n### Thanks!\n- John Snow Labs\n- Hugging Face Inc.\n- pytorch community\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/searchableai/KitanaQA",
    "keywords": "NLP BERT adversarial training data augmentation deep learning pytorch google",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "kitanaqa",
    "package_url": "https://pypi.org/project/kitanaqa/",
    "platform": "",
    "project_url": "https://pypi.org/project/kitanaqa/",
    "project_urls": {
      "Homepage": "https://github.com/searchableai/KitanaQA"
    },
    "release_url": "https://pypi.org/project/kitanaqa/0.1.0/",
    "requires_dist": [
      "config",
      "nltk",
      "numpy (<1.18.0,>=1.14.0)",
      "stop-words",
      "torch (>=1.5.1)",
      "transformers (==3.1.0)",
      "prefect (==0.13.4)",
      "pendulum (==2.0.5)",
      "dataclasses (==0.6)",
      "pytest ; extra == 'dev'",
      "pytest-pep8 ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'"
    ],
    "requires_python": ">=3.7.0",
    "summary": "Adversarial Training and Data Augmentation for Neural Question-Answering Models",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8667049,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ee051386aae7ac4f810d3dedb915646ca4f09fdfe14eb2cefc4ac82bea98d54e",
          "md5": "1f33dafa4349588ff0d9f0750b40ce36",
          "sha256": "3f009ad6be97f942861e4ddc325c483308c56e3eb78fee4f12d55066271d3368"
        },
        "downloads": -1,
        "filename": "kitanaqa-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1f33dafa4349588ff0d9f0750b40ce36",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 74262015,
        "upload_time": "2020-11-16T01:59:15",
        "upload_time_iso_8601": "2020-11-16T01:59:15.510054Z",
        "url": "https://files.pythonhosted.org/packages/ee/05/1386aae7ac4f810d3dedb915646ca4f09fdfe14eb2cefc4ac82bea98d54e/kitanaqa-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ee051386aae7ac4f810d3dedb915646ca4f09fdfe14eb2cefc4ac82bea98d54e",
        "md5": "1f33dafa4349588ff0d9f0750b40ce36",
        "sha256": "3f009ad6be97f942861e4ddc325c483308c56e3eb78fee4f12d55066271d3368"
      },
      "downloads": -1,
      "filename": "kitanaqa-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1f33dafa4349588ff0d9f0750b40ce36",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7.0",
      "size": 74262015,
      "upload_time": "2020-11-16T01:59:15",
      "upload_time_iso_8601": "2020-11-16T01:59:15.510054Z",
      "url": "https://files.pythonhosted.org/packages/ee/05/1386aae7ac4f810d3dedb915646ca4f09fdfe14eb2cefc4ac82bea98d54e/kitanaqa-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}