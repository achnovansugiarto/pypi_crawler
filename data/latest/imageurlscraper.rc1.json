{
  "info": {
    "author": "MujyKun",
    "author_email": "thugystory1@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python"
    ],
    "description": "# ImageURlScraper\n\nImageURLScraper is a multi-site image scraper. It automatically detects which site the image is coming from and scrapes it. Only relevant images are scraped from the site and shortened links are automatically unshortened. In the case that you have many links that need to be processed, these links can be distinguished by IDs when requesting the image links.\n\nCurrently Supported Sites:  \n[Asiachan](kpop.asiachan.com)  - Checks all previous and next pages from it's current location.  \n[Google Drive](drive.google.com) - Checks all folders and grabs the first 1000 images in each folder.  \n[Imgur](imgur.com) - Grabs all images in a gallery.  \n\n## Installation\n\nIn a terminal, type ``pip install imageurlscraper``.\n\nIn order to scrape images from Google Drive, the credentials are needed.  \nSteps to add Google Drive credentials:\n\nGo to https://console.developers.google.com/apis/dashboard and at the top click `+ ENABLE APIS AND SERVICES`.  \nNext, search for `Google Drive API`, click it, and then click Enable.  \nSelect a project and then you'll be on a page with your project.  \nYou will see a notice: \"To use this API, you may need credentials. Click 'Create credentials' to get started.\".  \nGo ahead and click `Create Credentials`.  \nYou will be requested information on the type of credentials you need.  \nFor the API, select `Google Drive API`, and select `Other UI` for where the API will be called from.  \nFor the data you will be accessing, select Application data.  \nAfter that, create a service account in the 2nd field. Have the role as `project owner` and make sure the Key type is `JSON`.  \nGet your credentials and rename the JSON file to `credentials.JSON`\nGo to the project source (if you installed by pip, go to a terminal and type `pip show imageurlscraper`)  \nPut the `credentials.json` in the same folder as the `main.py`.\n\n\n\n# Sample Code\n```python\n\"\"\"\nThis sample code links directly to the main function that automatically processes the links \nand returns back a dict with IDs and their image links. The original link will not be shown,\nwhich is why IDs are useful.\nIDs are REQUIRED input alongside their links, although they are only for classifying links.\nLinks can have several IDs if necessary to group them together.\n\"\"\"\nimport imageurlscraper\nimport pprint\npp = pprint.PrettyPrinter(indent=4)\n\nlist_of_links = [\n    # the list must contain an ID along with a link\n    # This ID is helpful for distinguishing certain objects or people.\n    # When the dict is returned.\n    [0, \"https://kpop.asiachan.com/222040\"],\n    [1, 'https://imgur.com/a/mEUURoG'],\n    [2, 'https://bit.ly/36GWd2A'],\n    [3, 'http://imgur.com/a/jRcrF'],\n    # [999, 'https://drive.google.com/drive/folders/1uWIObdgq65-TmBcA8oJIWOnbuuR_H5PB']\n    # This google drive folder has a lot of media and will be skipped for testing purposes. but it can support\n    # google drive links like these and will go through every folder in that folder.\n]\n\n\nscraper = imageurlscraper.main.Scraper()\nall_images = scraper.run(list_of_links)  # a dict with all the links of the images.\npp.pprint(all_images)  \n```\n\n#### Expected Output (dict)\n```\n{   1: [   'https://i.imgur.com/RUb6Xwl.jpg',\n           ...],\n    3: [   'https://i.imgur.com/ILixI73.jpg',\n           ...],\n    4: [   'https://i.imgur.com/X8jZOc7.jpg',\n           ...],\n    5: [   'https://i.imgur.com/L4SFme0.jpg',\n           ...],\n    6: [   'https://i.imgur.com/G2ltCDf.jpg',\n           ...],\n    204: [   'https://static.asiachan.com/Lee.Jueun.full.222040.jpg',\n             ...]\n}\n```\n\n## More Samples\n```Python\nimport imageurlscraper\nscraper = imageurlscraper.main.Scraper()\n\nshortened_link = \"https://bit.ly/311n6vP\"\nunshortened_link = scraper.get_main_link(shortened_link)  # Expected Output -> http://google.com/\n\n\n# Want to process links one by one or do not want to use IDs?\nlink = \"https://imgur.com/a/mEUURoG\"\nimage_links = scraper.process_source(link)  # Expected Output -> A LIST of image links.\n\n\n# Want to run from the sources directly?\nimages = imageurlscraper.asiachan.AsiaChan().get_all_image_links(link)  # Asiachan, expected output -> A LIST of image links.\nimages = imageurlscraper.googledrive.DriveScraper().get_links(link)  # Google Drive, expected output -> A LIST of image links.\nimages = imageurlscraper.imgur.MediaScraper().start(link)  # Imgur, expected output -> A LIST of image links.\n\n```\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/MujyKun/ImageURLScraper/",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "imageurlscraper",
    "package_url": "https://pypi.org/project/imageurlscraper/",
    "platform": "",
    "project_url": "https://pypi.org/project/imageurlscraper/",
    "project_urls": {
      "Homepage": "https://github.com/MujyKun/ImageURLScraper/"
    },
    "release_url": "https://pypi.org/project/imageurlscraper/1.0.0/",
    "requires_dist": [
      "urllib3 (==1.25.7)",
      "beautifulsoup4 (==4.8.2)",
      "google-api-python-client (==1.7.11)",
      "google-auth (==1.10.0)",
      "google-auth-httplib2 (==0.0.3)",
      "google-auth-oauthlib (==0.4.1)"
    ],
    "requires_python": ">=2.7",
    "summary": "Image Scraper for Google Drive, Imgur, AsiaChan, and more.",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7852377,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7b3a2241565a3aa7328f56810f63d451fbec98528c4c9ab21a2634fda1c57649",
          "md5": "bca8e492b4590f685ea0eb9ba19a8ae8",
          "sha256": "5a9b349fae06a949319f2674b8d80408a9443f99533da5b154ceb1e292a18eba"
        },
        "downloads": -1,
        "filename": "imageurlscraper-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bca8e492b4590f685ea0eb9ba19a8ae8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=2.7",
        "size": 11371,
        "upload_time": "2020-07-31T06:03:01",
        "upload_time_iso_8601": "2020-07-31T06:03:01.338124Z",
        "url": "https://files.pythonhosted.org/packages/7b/3a/2241565a3aa7328f56810f63d451fbec98528c4c9ab21a2634fda1c57649/imageurlscraper-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9b1abde0109fc413ea85b8fbb7097b0928d46b8ca94a1f9c94984bb923ef1e0a",
          "md5": "308772d175bbf6e401711a09d5ed2053",
          "sha256": "138e04560918d97e7e3d046bc53c98cf16afa222f9defcb908096598b3ece18e"
        },
        "downloads": -1,
        "filename": "imageurlscraper-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "308772d175bbf6e401711a09d5ed2053",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7",
        "size": 7325,
        "upload_time": "2020-07-31T06:03:03",
        "upload_time_iso_8601": "2020-07-31T06:03:03.464016Z",
        "url": "https://files.pythonhosted.org/packages/9b/1a/bde0109fc413ea85b8fbb7097b0928d46b8ca94a1f9c94984bb923ef1e0a/imageurlscraper-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7b3a2241565a3aa7328f56810f63d451fbec98528c4c9ab21a2634fda1c57649",
        "md5": "bca8e492b4590f685ea0eb9ba19a8ae8",
        "sha256": "5a9b349fae06a949319f2674b8d80408a9443f99533da5b154ceb1e292a18eba"
      },
      "downloads": -1,
      "filename": "imageurlscraper-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "bca8e492b4590f685ea0eb9ba19a8ae8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=2.7",
      "size": 11371,
      "upload_time": "2020-07-31T06:03:01",
      "upload_time_iso_8601": "2020-07-31T06:03:01.338124Z",
      "url": "https://files.pythonhosted.org/packages/7b/3a/2241565a3aa7328f56810f63d451fbec98528c4c9ab21a2634fda1c57649/imageurlscraper-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9b1abde0109fc413ea85b8fbb7097b0928d46b8ca94a1f9c94984bb923ef1e0a",
        "md5": "308772d175bbf6e401711a09d5ed2053",
        "sha256": "138e04560918d97e7e3d046bc53c98cf16afa222f9defcb908096598b3ece18e"
      },
      "downloads": -1,
      "filename": "imageurlscraper-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "308772d175bbf6e401711a09d5ed2053",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=2.7",
      "size": 7325,
      "upload_time": "2020-07-31T06:03:03",
      "upload_time_iso_8601": "2020-07-31T06:03:03.464016Z",
      "url": "https://files.pythonhosted.org/packages/9b/1a/bde0109fc413ea85b8fbb7097b0928d46b8ca94a1f9c94984bb923ef1e0a/imageurlscraper-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}