{
  "info": {
    "author": "Federico Claudi",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "Operating System :: MacOS :: MacOS X",
      "Operating System :: Microsoft :: Windows :: Windows 10",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# principledinvestigator\nA papers recomendation tool\n\nprincipledinvestigator compares papers in your library against a database of scientific papers to find new papers that you might be interested in.\nWhile there's a few services out there that try to do the same, principledinvestigator is unique in several ways:\n* principledinvestigator is completely open source, you can get the code and tweak it to improve the recomendation engine\n* principledinvestigator doesn't just use a single paper or a subset of (overly generic) keywords to find new papers, instead it compares *all* of your papers' abstracts against a database of papers metadata, producing much more relevant results\n\n### disclaimer\nThe dataset used here is a subset of a [larger dataset of scientific papers](https://www.semanticscholar.org/paper/Construction-of-the-Literature-Graph-in-Semantic-Ammar-Groeneveld/649def34f8be52c8b66281af98ae884c09aef38b). The dataset if focused on neuroscience papers published in the latest 30 years. If you want to include older papers or are interested in another field, then follow the instructions to create your custom database. \n\n### (possible) future improvements\n- [ ] use [scibert](https://github.com/allenai/scibert) instead of tf-idf for creating the embedding. This should also make it possible to embed the database's papers before use (unlike tf-idf which needs to run on the entire corpus every time).\n\n### Overview\nThe core feature making principledinvestigator unique among papers recomendation systems is that it analyzes **your entire library** of papers and matches it against a **vast database** of scientific papers to find new relevant papers. This is obviously an improvement compared e.g. to finding papers similar to *one paper you like*. \nIn addition, principledinvestigator doesn't just use things like \"title\", \"authors\", \"keywords\"... to find new matches, instead it finds similar papers using [*Term Frequency-Inverse Document Frequency*](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to asses the similarity across **papers abstracts**, thus using much more information about the papers' content. \n\n### Usage\nFirst, you need to get data about your papers you want to use for the search. The best way is to export your library (or a subset of it) directly to a `.bib` file using your references menager of choice.\n\nThen, you can use...\n\n\n## Making your own database\nprincipledinvestigator uses a subset of the vast and eccelent corpus of scientific publications' metadata from [Semanthic Scholar](https://www.semanticscholar.org/paper/Construction-of-the-Literature-Graph-in-Semantic-Ammar-Groeneveld/649def34f8be52c8b66281af98ae884c09aef38b). \nThe dataset used by principledinvestigator is focused on neuroscience papers written in english and published in the last 30 years. If you wish to include a different set of papers in your database, you can make your custom database and use it with principledinvestigator by executing the following steps.\n\n### 1. Download whole corpus\nYou'll first need to download the whole corpus from Semantic Scholar. You can find the data and download instructions [here](http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/download/). Once the data are downloaded, save them in a folder where you want to base your dataset-creation process\n\n### 2. Uncompressing data\nThe downloaded corpus is compressed. To uncompress the files use `principledinvestigator.database_preprocessing.upack_database` pasing to it the path to the folder where you've downloaded the data.\n\n### 3. Specifying your parameters\nThe selection of a subset of papers from the corpus is based on a set of parameters (e.g. year of publication) matched against criteria specified (and described) in `principledinvestigator.settings`. Edit the criteria to adapt the dataset selection to your needs\n\n### 4. Creating the dataset\nSimply run `principledinvestigator.database_preprocessing.make_database`\n\n### 5. Training doc2vec model\nPapers semanthic similarity is estimated using a doc2vec model trained on the entire dataset.\nAfter modifying the dataset to your needs, you'll have to re-train the model by running `principledinvestigator.doc2vec.train_doc2vec_model`\n\n### summary:\nAn example code for creating your dataset (after having downloaded the corpus and edited the settings)\n``` python\n\nfrom principledinvestigator.database_preprocessing import upack_database, make_database\nfrom principledinvestigator.doc2vec import train_doc2vec_model\nfrom pathlib import Path\n\nfolder = Path('path to your data')\n\n# unpack and create\nunpack_database(folder)\nmake_database(folder)\n\n# train new d2v model\ntrain_doc2vec_model()\n\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/FedeClaudi/principledinvestigator",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "principledinvestigator",
    "package_url": "https://pypi.org/project/principledinvestigator/",
    "platform": "",
    "project_url": "https://pypi.org/project/principledinvestigator/",
    "project_urls": {
      "Homepage": "https://github.com/FedeClaudi/principledinvestigator"
    },
    "release_url": "https://pypi.org/project/principledinvestigator/0/",
    "requires_dist": [
      "pyinspect",
      "gensim",
      "nltk",
      "langdetect",
      "pandas",
      "loguru",
      "requests",
      "myterial",
      "rich",
      "bibtexparser",
      "cython",
      "typer"
    ],
    "requires_python": "<3.9",
    "summary": "not yet",
    "version": "0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9119205,
  "releases": {
    "0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "87edc35f81d1e104e133920a7e5b68d3a6c3ca3ba002149f63cadfe03d9e0720",
          "md5": "73a41d763fb59e8b54b5b3b1a37d7657",
          "sha256": "9784206dc0f58f841b227ebe18db02b366f1b6c47ebc3f51bb4250e082ceabe9"
        },
        "downloads": -1,
        "filename": "principledinvestigator-0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "73a41d763fb59e8b54b5b3b1a37d7657",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<3.9",
        "size": 38801,
        "upload_time": "2021-01-12T23:30:02",
        "upload_time_iso_8601": "2021-01-12T23:30:02.794366Z",
        "url": "https://files.pythonhosted.org/packages/87/ed/c35f81d1e104e133920a7e5b68d3a6c3ca3ba002149f63cadfe03d9e0720/principledinvestigator-0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08f38cdfd95a7457e8603a29b7f9a2f70a176d84f957da7bf0c67592ec15ed97",
          "md5": "76a9cd48220c79798ad1817190603af2",
          "sha256": "b56472fabeb520e8fa064c02e80ed92653de86b9e32ecca51ee62ef66f684dc3"
        },
        "downloads": -1,
        "filename": "principledinvestigator-0.tar.gz",
        "has_sig": false,
        "md5_digest": "76a9cd48220c79798ad1817190603af2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "<3.9",
        "size": 16159,
        "upload_time": "2021-01-12T23:30:04",
        "upload_time_iso_8601": "2021-01-12T23:30:04.083870Z",
        "url": "https://files.pythonhosted.org/packages/08/f3/8cdfd95a7457e8603a29b7f9a2f70a176d84f957da7bf0c67592ec15ed97/principledinvestigator-0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "87edc35f81d1e104e133920a7e5b68d3a6c3ca3ba002149f63cadfe03d9e0720",
        "md5": "73a41d763fb59e8b54b5b3b1a37d7657",
        "sha256": "9784206dc0f58f841b227ebe18db02b366f1b6c47ebc3f51bb4250e082ceabe9"
      },
      "downloads": -1,
      "filename": "principledinvestigator-0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "73a41d763fb59e8b54b5b3b1a37d7657",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "<3.9",
      "size": 38801,
      "upload_time": "2021-01-12T23:30:02",
      "upload_time_iso_8601": "2021-01-12T23:30:02.794366Z",
      "url": "https://files.pythonhosted.org/packages/87/ed/c35f81d1e104e133920a7e5b68d3a6c3ca3ba002149f63cadfe03d9e0720/principledinvestigator-0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "08f38cdfd95a7457e8603a29b7f9a2f70a176d84f957da7bf0c67592ec15ed97",
        "md5": "76a9cd48220c79798ad1817190603af2",
        "sha256": "b56472fabeb520e8fa064c02e80ed92653de86b9e32ecca51ee62ef66f684dc3"
      },
      "downloads": -1,
      "filename": "principledinvestigator-0.tar.gz",
      "has_sig": false,
      "md5_digest": "76a9cd48220c79798ad1817190603af2",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "<3.9",
      "size": 16159,
      "upload_time": "2021-01-12T23:30:04",
      "upload_time_iso_8601": "2021-01-12T23:30:04.083870Z",
      "url": "https://files.pythonhosted.org/packages/08/f3/8cdfd95a7457e8603a29b7f9a2f70a176d84f957da7bf0c67592ec15ed97/principledinvestigator-0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}