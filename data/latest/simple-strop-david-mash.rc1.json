{
  "info": {
    "author": "Wonsun Ahn",
    "author_email": "wonsun.ahn@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "- [Introduction](#introduction)\n  * [What does this package do?](#what-does-this-package-do)\n- [Steps to integrate a change and deploy it](#steps-to-integrate-a-change-and-deploy-it)\n  * [Setting up the Build Environment](#setting-up-the-build-environment)\n      - [Short Intro to Pipenv](#short-intro-to-pipenv)\n  * [Testing](#testing)\n  * [Packaging](#packaging)\n  * [Distributing](#distributing)\n- [Automating CI/CD using Pipelines](#automating-ci-cd-using-pipelines)\n  * [Pipelines](#pipelines)\n  * [The `build` Stage](#the-build-stage)\n  * [The `test` Stage](#the-test-stage)\n  * [The `package` Stage](#the-package-stage)\n  * [The `deploy` Stage](#the-deploy-stage)\n  * [Invoking the Pipeline](#invoking-the-pipeline)\n- [Submission](#submission)\n\n# Introduction\n\nThe goal of this exercise is to create a CI/CD (Continuous Integration /\nContinuous Delivery) pipeline on a GitLab repository such that a source code\npush on the repository automatically triggers a pipeline that performs:\n\n1. Integration: Making sure the source code compiles and all unit tests pass before allowing the push to be merged.\n\n2. Delivery: Packaging the code into a distributable (called a \"wheel\" in\n   Python) and deploying it to the end-users (the Python Package Index\npypi.org repository in case of Python).\n\nThe unit tests (an potentially other tests) in the automated pipeline\nprovide the quality control and confidence necessary to allow developers to\npush code to the central repository directly and have it immediately\ndelivered to the end users.  That is why it is often said that Continuous\nTesting (CT) is a prerequisite for CI and CD.\n\nHaving a single repository for the entire organization cuts down on\noperating costs that comes from having to maintain multiple \"beta branches\"\nfor the multiple beta feature developments going on in the organization, and\nhaving to merge those branches into one deployment version on delivery\ndates.  The difficulty of merging these branches is often the reason for\ndelivery delays.\n\n## What does this package do?\n\nYou can install this package from the python package manager `pip` with `pip install simple-strop`. Once installed, you should be able to see package listed when you do `pip list`.  The package has 2 modules (that aren't tests): `operations.py` and `utils.py`. An example usage is below:\n\n```python\n$ python\n>>> import simple_strop as ss\n>>> sample_string = 'This is my sample string'\n>>> ss.reverse(sample_string)\n'gnirts elpmas ym si sihT'\n>>> ss.piglatin(sample_string)\n'Isthay isway myay amplesay ingstray'\n>>> ss.is_capitalized(sample_string)\nTrue\n>>> ss.is_all_caps(sample_string)\nFalse\n>>> ss.is_vowel('y')\nFalse\n>>> ss.is_consonant('q')\nTrue\n>>> exit()\n```\n\nSo where did the simple-strop package come from?  It came from the Python\nPackage Index (PyPI) repository aforementioned.  The URL for the\nsimple-strop package in PyPI is: https://pypi.org/project/simple-strop/.\n\nBy the end of this exercise, you will have learned how to creat a CI/CD\npipeline that builds a package of your own and deploys it to PyPI.\n\n# Steps to integrate a change and deploy it\n\nOn every source code modification, the following steps must happen to have\nCI/CD: 1) Setting up the build environment, 2) Testing, 3) Packaging, and 4)\nDistributing.  We will describe each step in more detail using our example.\nLater, we will talk about how to automate all these steps as part of a pipeline\nusing GitLab.\n\n## Setting up the Build Environment\n\nOne of the most frequent issues when it comes to porting software from one\nmachine to another is ensuring that the two machines _agree completely_ on what\nthe environment for building that code is. When I say \"environment\", I'm talking\nabout the variables that are set on the system as well as the specific versions\nof packages that are installed on the system.\n\nIf you're running a Linux distribution and I'm running on Windows and our\nmutual friend is running on MacOS, then there is no chance that we can have\n_all_ of our environment variables match (without some sort of\nvirtualization... foreshadowing), but we can ensure that our package versions\nare identical by using a virtual environment manager like [pipenv][1]. Pipenv\nis an easy tool for ensuring that not only do you have the same packages\ninstalled, but that you have the same minute versions of those packages\ninstalled. It takes a little bit of getting used to, but once you've adopted it\nthe benefits are immense!\n\nOnce you start using pipenv, you also have the benefit that your list of\nglobally installed packages (the packages that you install when you just run\n`pip install ...`) will become much shorter.\n\nPipenv is just another python package and can be installed with `pip install pipenv`.\n\n#### Short Intro to Pipenv\n\nThere are a few bread-and-butter commands, so I'll cover those first:\n\n| Command                           | Description                                                                                                                                                                                                                                                                                                                          |\n| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `pipenv shell`                    | This enters your shell into the virtual environment or creates an empty one if the directory you're in doesn't contain a Pipfile.                                                                                                                                                                                                    |\n| `pipenv install <package_name>`   | This installs a package into your environment and creates a new environment if the directory you're in doesn't container a Pipfile. If a Pipfile exists but there is no virtual environment stored on your machine, pipenv creates a virtual environment and populates it with packages from the Pipfile.lock.                       |\n| `pipenv lock`                     | This resolves all of the dependencies of all of the packages you've installed and chooses the most modern versions that don't conflict with other package dependencies. It can't be used unless you have a Pipfile. You can also use it to generate your `requirements.txt`                                                          |\n| `pipenv uninstall <package_name>` | Remove a pacakge from your environment. Can't be used unless you have a Pipfile (with whatever package you're trying to remove)                                                                                                                                                                                                      |\n| `pipenv --rm`                     | Deletes the virtual environment for this directory, but keeps the Pipfile and Pipfile.lock. Useful if you've installed extraneous packages into this environment without using pipenv (like `pip install ...` inside of the pipenv shell) or if you are done working on a project for the time being and want to free up some space. |\n\nSo to get started, you go to the top level of whatever git repository you're using for your code and run `pipenv shell`. This will create and enter a virtual environment that is completely distinct from your base environment. You can install any packages you like without cluttering up your home environment with `pip` or you can add pacakges to your project with `pipenv install`.\n\nFor this project, I added 5 packages: 2 build-time dependencies - build, twine; and 3 development-dependencies - pytest, pytest-cov, coverage. To install run/build time dependencies just use `pipenv install ...`, but if you want to install dependencies that are manually enabled, use `pipenv install --dev ...`. The added benefit is when generating requirements.txt files, your dev packages won't be included (by default).\n\nAfter you install packages with pipenv, it will automatically generate a `Pipfile` and a `Pipfile.lock` (although you can tell pipenv not to lock). The Pipfile is basically just a meta description of what your project needs. It says things like what version of python the project is running in, what the packages are, what the dev dependencies are, and then some information about where pipenv was installed from. The Pipfile.lock is more specific and contains a hash of all of the packages for your project, hashes for verifying the installation of those packages, the specific version numbers of the packages, the list of dependencies for each installed package, hashes for those, their dependencies, etc.\n\n**YOU SHOULD NEVER ATTEMPT TO MANUALLY EDIT YOUR PIPFILE.LOCK**. In the worst case, it might make your environment uninstallable and you will need to remove the lock file (which will forget all of your pinned versions) in order to correct this. Then you might have to deal with situations where new versions of your dependencies were released that break your code... it's bad. Let pipenv handle the lock file and get familiar with commands arguments for the install, uninstall, and lock commands.\n\n## Testing\n\nFor this exercise, we'll focus on unit tests for this package. As you can see\nin the directory, the tests are right along side the normal code in this\npackage. This was a choice I made out of convenience because I wanted to do the\nminimal amount of setup with pytest and coverage and if I were to try and\nseparate the tests from the source, it would create import complications and\nthe coverage would be wrong (by default) and it would just be a whole mess.\n\nYou can run the unit tests yourself locally. First install the environment via\npipenv: `pipenv install --dev`, then you can just run the `pytest` command. If\nyou don't include the `--dev` flag, you won't install the packages for unit\ntesting and it won't work.\n\nPytest is configured to also run a code coverage report and save that into a\ndirectory called `htmlcov`. After running the unit tests, open the file\n`htmlcov/index.html` with a web browser and you'll be able to interactively\nclick through each file, see which lines are covered (spoiler alert, it's all\nof them), and see what the coverage of the whole project is.\n\nYou can get familiar with how coverage is working by commenting out some of the\nfunctions in the `src/simple_strop/test_...` files then running `pytest` again\nto see the coverage re-calculated.\n\n## Packaging\n\nFor the details on how to create a Python package, I'll direct you [to this\nguide][3].  I created the Python package myself using this guide.  You will see\nin the guide that the `setup.cfg` contains configurable metadata about the\npackage.  I'm going to ask you to change the package name from:\n```\nname = simple-strop-wonsun.ahn\n```\nto (where your.name is your actual name):\n```\nname = simple-strop-<your.name>\n```\n\nThis is important since each of you need a unique package name to register on\nPyPI.  Otherwise, PyPI will complain of a name conflict.  For the same reason,\nif you attempt to register the same package twice to PyPI, PyPI will complain.\nSo what should you do if you want to update an existing package?  You need to\nincrement the version number.\n\nOnce you are done editing `setup.cfg`, go ahead and run the build package command in the pipenv virtual environment:\n```\npython -m build\n```\n\nThis will run for a few seconds and when it's done, you'll have a few new\ndirectories: `build/`, `dist/`, `src/simple_strop.<your.name>.egg-info/`. Each of these\nserves its own purpose, but the `dist` directory is how we might actually\n_distribute_ our package.\n\nInside of `dist` there should be two files:\n\n- `simple-strop-<your.name>-<version>.tar.gz`\n- `simple_strop-<your.name>-<version>-py3-none-any.whl`\n\nThe first file is just a compressed archive of the files in this repository\nthat are actually meaningful to the distribution of your project. If you'd\nlike, you can open this file (on linux and mac machines natively, windows if\nyou have gnu installed) with `tar xf <filename>` and then check out the new\ndirectory it created with the same name as the tarball.\n\nThe second file is a \"wheel\". Wheel is python's format for creating installable\npackages. It's a binary file that contains the source code and instructions to\npip (or pipenv) on how to actually install the package, what additional needs\nit has (like adding a file to the user's path like pipenv does), etc.\n\nYou can actually install packages directly from wheel files with `pip install <path_to_wheel>`\n\n## Distributing\n\nOnce you've built the package into `dist`, you can upload it to a package\ndirectory like PyPI!  But we probably shouldn't clutter up the PyPI with dozens\nof copies of the same package from dozens of students.  There is actually a\ntest version of PyPI that developers can use to play around or make sure that\ntheir package is actually all set for distribution _before_ they publicly\npublish it. [You can find TestPyPI here][2].\n\nThe command to upload to TestPyPI is:\n```\npython -m twine upload -r testpypi dist/*\n```\n\nIf you wanted to upload to PyPI, you would omit the `-r testpypi`\n(discouraged).  You'll need to create an account with TestPyPI or PyPI to be\nable to upload packages to either.\n\n# Automating CI/CD using Pipelines\n\nNow we are going to automate all the CI/CD steps we described using pipelines.\n\n## Pipelines\n\nThe last component of this repository is the CI/CD pipeline. Pipelines are a\nsequence of steps organized into a DAG (directed acyclic graph) for managing\nbuilds, tests, releases, and anything else that a project might need to go from\nsource code to distributable. You can see the configuration for this\nrepository's pipeline in the file `.gitlab-ci.yml` in the top level of the\nrepository. The `.yml` extension identifies this file as a \"yaml\" file, almost\nall pipelines are defined in yaml files as they are easy to read and are quite\nflexible. [You can read more about yamls here][4].\n\nAt a minimum, a pipeline should have 2 stages: build and deploy (but test is\nalso highly recommended!!!). In this repo, we actually have 4 because we need\nto build two times: first to build the dependencies that our pipeline will need\nfor testing and second to actually build our source code into the wheel file.\n\nThe stages of the pipeline are defined at the top:\n\n```yaml\nstages:\n  - build\n  - test\n  - package\n  - deploy\n```\n\nEach of these will fulfill one part of the process and the final one should\nactually make our package public. So let's dig into each one and understand\nwhat's happening.\n\n## The `build` Stage\n\nOur build stage only has 1 job in it. A job is a single, focused collection of\ncommands to accomplish a specific goal. It is more focused than stages. This\njob is called `build-dependencies` and it is responsible for installing the\npackages we need for the later stages.\n\nIn this stage, we see the first occurrence of a \"yaml anchor\". Anchors can get\npretty complicated, but the easiest way to think about them are as variables\nbut for yaml files: you define a variable with `&variable_name` and you can\nthen reference it with `*variable_name`. So when we see:\n\n```yaml\n.cache: &cache\n  key: $CI_COMMIT_REF_SLUG\n  policy: pull\n  paths:\n    - $PIP_CACHE_DIR\n    - $PYTHON_PACKAGE_DIR\n\nbuild-dependencies:\n  stage: build\n  cache:\n    <<: *cache\n    policy: pull-push\n```\n\nwe are defining the anchor `cache` at the top to be the object containing three\nkeys: `key`, `policy`, and `paths` where `paths` is an array with two indices.\nWe then use our `cache` anchor inside of the `build-dependencies` object with\nthe \"merge operator\" `<<: *variable_name`.\n\nThis merge operator is saying, \"I want to put the value of this anchor in this\nposition as if it were defined here\". So after the merge operator, if we\nexpanded this file it would look like this:\n\n```yaml\n.cache:\n  key: $CI_COMMIT_REF_SLUG\n  policy: pull\n  paths:\n    - $PIP_CACHE_DIR\n    - $PYTHON_PACKAGE_DIR\n\nbuild-dependencies:\n  stage: build\n  cache:\n    key: $CI_COMMIT_REF_SLUG\n    policy: pull\n    paths:\n      - $PIP_CACHE_DIR\n      - $PYTHON_PACKAGE_DIR\n    policy: pull-push\n```\n\nNotice that the key \"policy\" is repeated. In this instance, we want to override\nit from what is defined in our anchor, so we can just define it again and yaml\nwill forget the previous value.\n\nInside of the `build-dependencies` job, we can see that we have defined it to\nbe in the stage \"build\". We also define how we want the cache to work. Defining\na cache isn't necessary, but it is useful for speeding up repeated builds. In\nthis stage, however, we don't want to use the cache, we want to overwrite the\ncache so that it is fresh for this build. Here, we identify a reusable cache by\nthe git commit the pipeline was run for (`CI_COMMIT_REF_SLUG`) and say that we\nwant to be able to push to this cache.\n\nWe also define our \"artifacts\". Artifacts are files, strings, or directories\nthat are the output of a job and are needed for another job or just for later\nreference. In this case, after we install our packages, we want to keep the\ndirectories where we installed these packages for use in the future stages, so\nwe define those directories as artifacts.\n\nFinally, we define our scripts. Before executing the standard script, we make\nsure to delete our old cache and add our cache to our PYTHONPATH environment\nvariable. This tells python how to find these packages later. Then we run two\n`pip install` commands to install the packages that we defined in our\nrequirements files. Note the usage of variables with `${VARIABLE_NAME}`. This\nallows us to change these values easily in one spot (the variables section) and\nnot have to worry about tracking down all of the places in the pipeline we used\nthem. GitLab's pipelines support variables in almost any value for key value\npairs and as environment variables in scripts.\n\n## The `test` Stage\n\n```yaml\ntest:\n  stage: test\n  cache:\n    <<: *cache\n  artifacts:\n    expire_in: 1 day\n    paths:\n      - htmlcov\n      - .coverage\n  coverage: '/Total coverage: \\d+.\\d+%$/'\n  script:\n    - python -m pytest\n```\n\nNow that we've built our dependencies, we're ready to run our unit tests. You\ncan see that most of the configuration here is very similar to the build stage:\nwe pull in our cache anchor (without overriding the default policy), we define\nthe coverage output as artifacts, and we run a script. The script here is just\n`python -m pytest` because we already defined `.coveragerc` and `pytest.ini` to\nrun our tests with all of the command line flags and configurations that we\nneed. Note: when we were running locally, we ran with just `pytest`. The\ncommand here is logically identical, but a little more specific and helps us\navoid issues with caching in pipelines.\n\nNote the inclusion of the `coverage` line. That is how we tell gitlab to parse\nthe output of this job with the regular expression so that the pipeline can\nreport our code coverage at the end.\n\n## The `package` Stage\n\n```yaml\nbuild-package:\n  stage: package\n  cache:\n    <<: *cache\n  artifacts:\n    expire_in: 1 day\n    paths:\n      - dist\n  script:\n    - python -m build\n```\n\nThis is pretty similar to the previous stages as well, but here we are defining\nour `dist` directory as an artifact. We want to make sure that in the final\nstage, we access it to deploy. We have this stage defined on its own for a few\nreasons: first, we want it to benefit from the cache because it would be very\nannoying to have to rebuild this late in the pipeline; second, we don't want to\nspend the time actually building until we've tested and know that everything is\nworking correctly.\n\n## The `deploy` Stage\n\n```yaml\n.release:\n  stage: deploy\n  when: manual\n  cache:\n    <<: *cache\n  dependencies:\n    - build-package\n\nrelease to testpypi:\n  extends: .release\n  script:\n    - python -m twine upload -u ${TEST_PYPI_USERNAME} -p ${TEST_PYPI_PASSWORD} -r testpypi dist/*\n\nrelease to pypi:\n  extends: .release\n  script:\n    - python -m twine upload -u ${PYPI_USERNAME} -p ${PYPI_PASSWORD} dist/*\n```\n\nFinally, we've built and tested our package and we're ready to hand it off the\nthe world! It's time to introduce one final concept of pipelines: templating.\nTemplates and anchors share the same basic goal to reduce repetition, but they\naccomplish it in different ways. Anchors are single-file _only_. If I defined a\npipeline where one file pulled configuration from another, then I couldn't\nreuse the anchors across files. Also, it looks a little cleaner to use\ntemplates. Whereas before I needed 3 or 4 paragraphs to explain what `<<:` was\ndoing, here you see \"extends\" and already have an idea.\n\nWhen we say \"extends\" here, what we're really saying is \"give me everything\nfrom this job\". So in `release to testpypi`, even though I didn't say it there\nit is _as if_ I had defined `stage: deploy` and `when: manual`. Note, I still\nused an anchor inside of my template and I could even have my template extend\nfrom somewhere else.\n\nThough I didn't include it in this repository, I could have set up each of\nthese stages to be in their own files called something like `.build.yml`,\n`.test.yml`, etc. Then I could have _included_ those files in my\n`.gitlab-ci.yml` pipeline and used them as if they were defined at the top. In\nthat situation, however, I couldn't have used my `cache` anchor so if I wanted\nto share that, I could have broken that out into its own job and had each of\nthese other jobs extend from cache. In this small example, that's not really\nworth it. But if I had a huge project with a really big pipeline, then I might\nconsider refactoring my pipeline in that way.\n\nAdditionally, if I was working in an organization, I probably won't want to\nwrite a pipeline for every project that is more or less the same. I could make\na repo that just holds pipeline templates and include templates from that repo\ninstead! Saves time and ensures that if I ever find a bug in one, I can patch\nall of my pipelines at once.\n\nNow with our releases to TestPyPI and PyPI defined, we are ready to deploy! But\nhang on, let's say that we have something to deploy that might break the\nversion of our code that our users are already using? Generally speaking, it's\na bad idea for pipelines to push all the way to production without having some\nsort of user input so we have also defined the `when` value in `.release` to be\n\"manual\". This will run all of the jobs of the pipeline except for this one,\nthen wait for an authorized user's input before continuing.\n\n## Invoking the Pipeline\n\nSo when does the above pipeline get invoked?  By default, it runs whenever new\nchanges are pushed to the repository.  If you have not already, try committing\nand pushing the changes you made to setup.cfg a while back.\n\nStage the file:\n```\ngit add setup.cfg\n```\nThen commit the file:\n```\ngit commit -m \"Changed name of package\"\n```\nThen push the file:\n```\ngit push\n```\n\nNow, go to your GitLab repository page and click on CI/CD > Pipelines:\n![CI/CD Pipelines](ci_cd_pipelines.png)\n\nYou should a pipeline in the `running` or `passed` state for your commit.  The\nfour circles in the `Stages` column indicate the four stages in your pipeline.\nThe three check marks indicate that the first three stages (build, test,\npackage) have passed.  The last circle with a `>>` sign indicates that the last\ndeploy stage is configured to be manually triggered.  If you want to see more\ndetails on what happened on each stage, you can click on the pipeline link.\n\nIf the pipeline is still running, wait until the first three stages have\ncompleted and it now in passed state.  Now you are ready to trigger the manual\ndeploy stage.  But before you do, there is one thing you have to do.  In the\n`.gitlab-ci.yml` file deploy stage, you will have noticed that four variables\nhave been used without being defined: TEST_PYPI_USERNAME, TEST_PYPI_PASSWORD,\nPYPI_USERNAME, PYPI_PASSWORD.  These variables were intentionally omitted in\nthe script because it is not good practice to expose your credentials in a\npublic repository.  These variables can be provided to the script using the\nCI/CD Settings.\n\nOn your GitLab repository page, click on Settings > CI/CD, and expand the Variables menu:\n![CI/CD Settings](ci_cd_settings.png)\n\nNow add the two variables TEST_PYPI_USERNAME and TEST_PYPI_PASSWORD as shown in\nthe image using the `Add variable` button.  Once you are done, you are now\nready to deploy.\n\nGoing back to the CI/CD > Pipelines menu, click on the `>>` icon representing\nthe last deploy stage, and then click on `release to testpypi`:\n![CI/CD Deploy](ci_cd_deploy.png)\n\nOnce it is done running, you should see the `>>` icon turn into a check mark.\nAnd if you go to https://test.pypi.org and search for the name of your package,\nyou should be able to see it deployed!  You should see something similar to:\nhttps://test.pypi.org/project/simple-strop-wonsun.ahn/\n\nIn this way, every time a change is pushed, all the tests are run and a package\nis created ready for upload automatically.  When you feel a need to deploy the\npackage, you can click on the corresponding version deploy stage.  FYI, you can\nalso invoke the pipeline manually if you wish by clicking on the `Run pipeline`\nbutton.  Also, software organizations often schedule a regular invocation of\nthe pipeline by using the CI/CD > Schedules menu.\n\n# Submission\n\nThere is no submission for this exercise.\n\n[1]: https://pypi.org/project/pipenv/\n[2]: https://test.pypi.org/\n[3]: https://packaging.python.org/tutorials/packaging-projects/\n[4]: https://www.cloudbees.com/blog/yaml-tutorial-everything-you-need-get-started/\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.com/wonsun.ahn/simple-python-package",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "simple-strop-david.mash",
    "package_url": "https://pypi.org/project/simple-strop-david.mash/",
    "platform": "",
    "project_url": "https://pypi.org/project/simple-strop-david.mash/",
    "project_urls": {
      "Homepage": "https://gitlab.com/wonsun.ahn/simple-python-package"
    },
    "release_url": "https://pypi.org/project/simple-strop-david.mash/1.0.7/",
    "requires_dist": null,
    "requires_python": ">=3.8",
    "summary": "A simple python package to demo CI/CD pipelines",
    "version": "1.0.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12052002,
  "releases": {
    "1.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1dcceff180ed4215ed4653d2d4052d5f3765d793390190284d228813ed9ef204",
          "md5": "335910a3aeacbd6ec0c576fd106f986c",
          "sha256": "27b0313c441d1155da4391ef19a123585eb299eaaebc183da31f06b42109bb57"
        },
        "downloads": -1,
        "filename": "simple_strop_david.mash-1.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "335910a3aeacbd6ec0c576fd106f986c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 13737,
        "upload_time": "2021-11-17T20:23:27",
        "upload_time_iso_8601": "2021-11-17T20:23:27.077924Z",
        "url": "https://files.pythonhosted.org/packages/1d/cc/eff180ed4215ed4653d2d4052d5f3765d793390190284d228813ed9ef204/simple_strop_david.mash-1.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e88c5df79776d539dda1db50f8254fd50b58800b10ec01c44261dfe9809f820",
          "md5": "96e7ee8bdc10feefbcfc738ce88e3bea",
          "sha256": "e5194f2e585c0723da32de67343e84fff3aef2b70592be241390ec76a528a9c9"
        },
        "downloads": -1,
        "filename": "simple-strop-david.mash-1.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "96e7ee8bdc10feefbcfc738ce88e3bea",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 21888,
        "upload_time": "2021-11-17T20:23:28",
        "upload_time_iso_8601": "2021-11-17T20:23:28.576425Z",
        "url": "https://files.pythonhosted.org/packages/5e/88/c5df79776d539dda1db50f8254fd50b58800b10ec01c44261dfe9809f820/simple-strop-david.mash-1.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1dcceff180ed4215ed4653d2d4052d5f3765d793390190284d228813ed9ef204",
        "md5": "335910a3aeacbd6ec0c576fd106f986c",
        "sha256": "27b0313c441d1155da4391ef19a123585eb299eaaebc183da31f06b42109bb57"
      },
      "downloads": -1,
      "filename": "simple_strop_david.mash-1.0.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "335910a3aeacbd6ec0c576fd106f986c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 13737,
      "upload_time": "2021-11-17T20:23:27",
      "upload_time_iso_8601": "2021-11-17T20:23:27.077924Z",
      "url": "https://files.pythonhosted.org/packages/1d/cc/eff180ed4215ed4653d2d4052d5f3765d793390190284d228813ed9ef204/simple_strop_david.mash-1.0.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5e88c5df79776d539dda1db50f8254fd50b58800b10ec01c44261dfe9809f820",
        "md5": "96e7ee8bdc10feefbcfc738ce88e3bea",
        "sha256": "e5194f2e585c0723da32de67343e84fff3aef2b70592be241390ec76a528a9c9"
      },
      "downloads": -1,
      "filename": "simple-strop-david.mash-1.0.7.tar.gz",
      "has_sig": false,
      "md5_digest": "96e7ee8bdc10feefbcfc738ce88e3bea",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 21888,
      "upload_time": "2021-11-17T20:23:28",
      "upload_time_iso_8601": "2021-11-17T20:23:28.576425Z",
      "url": "https://files.pythonhosted.org/packages/5e/88/c5df79776d539dda1db50f8254fd50b58800b10ec01c44261dfe9809f820/simple-strop-david.mash-1.0.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}