{
  "info": {
    "author": "",
    "author_email": "GX <GX.Xu@ibm.com>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3"
    ],
    "description": "Transition-based Neural Parser\n============================\n\n\n## transition-neural-parser\n**transition-neural-parser** is a powerful and easy-to-use Python package that provides a state-of-the-art neural transition-based parser for Abstract Meaning Representation (AMR). AMR is a semantic formalism used to represent the meaning of natural language sentences in a structured and machine-readable format. The package is designed to enable users to perform AMR parsing with high accuracy and generate reliable token-to-node alignments, which are crucial for various natural language understanding and generation tasks.\n\n\n## Pip Installation Instructions\nStep 1: Create and activate a new conda environment\nTo ensure compatibility and prevent potential conflicts, create a new conda environment with Python 3.8:\n\n```\nconda create -n amr-parser python=3.8\n```\n\nActivate the newly created environment:\n\n```\nconda activate amr-parser\n```\n\nStep 2: Install the package and dependencies\nInstall the transition-neural-parser package using pip:\n\n```\npip install transition-neural-parser\n```\n\nNext, install the torch-scatter dependency. For Linux servers, use the following command:\n\n```\npip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu117.html\n```\n\n\nFor other operating systems, please visit the official torch-scatter repository to find the appropriate installation instructions.\n\nStep 3: Download a pretrained AMR parser and run inference\nHere is an example of how to download and use a pretrained AMR parser:\n\n```\nfrom transition_amr_parser.parse import AMRParser\n\n# Download and save a model named AMR3.0 to cache\nparser = AMRParser.from_pretrained('AMR3-structbart-L')\ntokens, positions = parser.tokenize('The girl travels and visits places')\n\n# Use parse_sentence() for single sentences or parse_sentences() for a batch\nannotations, machines = parser.parse_sentence(tokens)\n\n# Print Penman notation\nprint(annotations)\n\n# Print Penman notation without JAMR, with ISI\namr = machines.get_amr()\nprint(amr.to_penman(jamr=False, isi=True))\n\n# Plot the graph (requires matplotlib)\namr.plot()\n\n```\n\nThis example demonstrates how to tokenize a sentence, parse it using the pretrained AMR parser, and print the resulting AMR graph in Penman notation. If you have matplotlib installed, you can also visualize the graph.\n\n\n\n## Available Pretrained Models\nThe models downloaded using from_pretrained() method will be stored to the pytorch cache folder as follows:\n```\ncache_dir = torch.hub._get_torch_home()\n```\n\nThis table shows you available pretrained model names to download;\n\n| pretrained model name                       | corresponding file name| \n|:----------------------------------------|:-----------:|\n| AMR3-structbart-L-smpl                | amr3.0-structured-bart-large-neur-al-sampling5-seed42.zip      | \n| AMR3-structbart-L                     | amr3.0-structured-bart-large-neur-al-seed42.zip      | \n| AMR2-structbart-L                     | amr2.0-structured-bart-large-neur-al-seed42.zip      |\n| AMR2-joint-ontowiki-seed42            | amr2joint_ontowiki2_g2g-structured-bart-large-seed42.zip       | \n| AMR2-joint-ontowiki-seed43            | amr2joint_ontowiki2_g2g-structured-bart-large-seed43.zip      | \n| AMR2-joint-ontowiki-seed44            | amr2joint_ontowiki2_g2g-structured-bart-large-seed44.zip      | \n| AMR3-joint-ontowiki-seed42            | amr3joint_ontowiki2_g2g-structured-bart-large-seed42.zip      | \n| AMR3-joint-ontowiki-seed43            | amr3joint_ontowiki2_g2g-structured-bart-large-seed43.zip      | \n| AMR3-joint-ontowiki-seed44            | amr3joint_ontowiki2_g2g-structured-bart-large-seed44.zip      | \n\n## Upcoming Features\n\nThe current release primarily supports model inference using Python scripts. In future versions, we plan to expand the capabilities of this package by:\n\n- Adding training and evaluation scripts for a more comprehensive user experience. Interested users can refer to the [IBM/transition-amr-parser](https://github.com/IBM/transition-amr-parser) repository for training and evaluation in the meantime.\n- Broadening platform support to include MacOS and higher versions of Python, in addition to the current support for the Linux operating system and Python 3.8.\n\n\n\n\n## Release History\n\n- **v1.0.0**: This release corresponds to v0.5.2 in the [IBM/transition-amr-parser](https://github.com/IBM/transition-amr-parser) repository.\n\n\n\n## Research and Evaluation Results\n### Structured-BART \n\nCurrent version (`0.5.2`). Structured-BART [(Zhou et al 2021b)](https://aclanthology.org/2021.emnlp-main.507/) encodes the parser state using specialized cross and self-attention heads and leverages BART's language model to replace the use of subgraph actions and lemmatizer, thus enabling a much simpler oracle with 100% coverage. It yields `84.2` Smatch (`84.7` with silver data and `84.9` with ensemble). Version `0.5.2` introduces the ibm-neural-aligner [(Drozdov et al 2022)](https://arxiv.org/abs/2205.01464) yielding a base AMR3.0 performance of `82.7` (`83.1` with latent alignment training). Structured-BART is also used for [(Lee et al 2022)](https://arxiv.org/abs/2112.07790) which yields a new single model SoTA of `85.7` for AMR2.0 and `84.1` for AMR3.0 by introducing Smatch-based ensemble distillation.\n\n### Action Pointer\n\nCheckout the `action-pointer` branch (derived from version `0.4.2`) for the `Action Pointer Transformer` model [(Zhou et al 2021)](https://www.aclweb.org/anthology/2021.naacl-main.443) from NAACL2021. As the stack-Transformer, APT encodes the parser state in dedicated attention heads. APT uses however actions creating nodes to represent them. This decouples token and node representations yielding much shorter sequences than previous oracles with higher coverage. APT achieves `81.8` Smatch (`83.4` with silver data and partial ensemble) on AMR2.0 test using RoBERTa embeddings and has an efficient shallow decoder. Due to aligner implementation improvements this code reaches `82.1` on AMR2.0 test, better that what is reported in the paper.\n\n### Stack-Transformer\n\nCheckout the `stack-transformer` branch (derived from version `0.3.4`) for the `stack-Transformer` model [(Fernandez Astudillo et al 2020)](https://www.aclweb.org/anthology/2020.findings-emnlp.89) from EMNLP findings 2020. The stack-Transformer masks dedicated cross attention heads to encode the parser state represented by stack and buffer. It yields `80.2` Smatch (`81.3` with self-learning) on AMR2.0 test (this code reaches `80.5` due to the aligner implementation). Stack-Transformer can be used to reproduce our works on self-learning and cycle consistency in AMR parsing [(Lee et al 2020)](https://www.aclweb.org/anthology/2020.findings-emnlp.288/) from EMNLP findings 2020, alignment-based multi-lingual AMR parsing [(Sheth et al 2021)](https://www.aclweb.org/anthology/2021.eacl-main.30/) from EACL 2021 and Knowledge Base Question Answering [(Kapanipathi et al 2021)](https://arxiv.org/abs/2012.01707) from ACL findings 2021.\n\nThe code also contains an implementation of the AMR aligner from [(Naseem et al 2019)](https://www.aclweb.org/anthology/P19-1451/) with the forced-alignment introduced in [(Fernandez Astudillo et al 2020)](https://www.aclweb.org/anthology/2020.findings-emnlp.89).\n\nAside from listed [contributors](https://github.com/IBM/transition-amr-parser/graphs/contributors), the initial commit was developed by Miguel Ballesteros and Austin Blodgett while at IBM.\n\n\n## Evaluating Trained checkpoints\n\nWe offer some trained checkpoints on demand, and their evalution score measured in Smatch is below:\n\n|  paper                                                          |  config(.zip)                                         | beam    | Smatch  |\n|:---------------------------------------------------------------:|:------------------------------------------------------:|:-------:|:-------:|\n| [(Drozdov et al 2022)](https://arxiv.org/abs/2205.01464) MAP    | amr2.0-structured-bart-large-neur-al-seed42            |   10    |   84.0  |\n| [(Drozdov et al 2022)](https://arxiv.org/abs/2205.01464) MAP    | amr3.0-structured-bart-large-neur-al-seed42            |   10    |   82.6  |\n| [(Drozdov et al 2022)](https://arxiv.org/abs/2205.01464) PR     | amr3.0-structured-bart-large-neur-al-sampling5-seed42  |   1     |   82.9  |\n| [(Lee et al 2022)](https://arxiv.org/abs/2112.07790) (ensemble) | amr2joint_ontowiki2_g2g-structured-bart-large          |   10    |   85.9  |  \n| [(Lee et al 2022)](https://arxiv.org/abs/2112.07790) (ensemble) | amr3joint_ontowiki2_g2g-structured-bart-large          |   10    |   84.4  |  \n\nwe also provide the trained `ibm-neural-aligner` under names `AMR2.0_ibm_neural_aligner.zip` and `AMR3.0_ibm_neural_aligner.zip`. For the ensemble we provide the three seeds. Following fairseq conventions, to run the ensemble just give the three checkpoint paths joined by `:` to the normal checkpoint argument `-c`. Note that the checkpoints were trained with the `v0.5.1` tokenizer, this reduces performance by `0.1` on `v0.5.2` tokenized data.\n\nNote that we allways report average of three seeds in papers while these are individual models. A fast way to test models standalone is\n\n    bash tests/standalone.sh configs/<config>.sh\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "transition-neural-parser",
    "package_url": "https://pypi.org/project/transition-neural-parser/",
    "platform": null,
    "project_url": "https://pypi.org/project/transition-neural-parser/",
    "project_urls": {
      "Bug Tracker": "https://github.ibm.com/mnlp/transition-amr-parser/issues",
      "Homepage": "https://github.ibm.com/mnlp/transition-amr-parser/tree/transition-neural-parser"
    },
    "release_url": "https://pypi.org/project/transition-neural-parser/0.5.2.1/",
    "requires_dist": [
      "torch (==1.13.1)",
      "numpy (<=1.23.5)",
      "tqdm (>=4.55.0)",
      "fairseq (==0.10.2)",
      "packaging (>=20.8)",
      "requests (>=2.25.1)",
      "h5py (>=3.0.0)",
      "python-dateutil (>=2.8.1)",
      "penman (>=1.1.0)",
      "smatch (>=1.0.3.2)",
      "ipdb (>=0.13.0)",
      "line-profiler (>=4.0.2)",
      "pyinstrument (>=4.4.0)",
      "boto3 (>=1.26.1)",
      "progressbar"
    ],
    "requires_python": "~=3.8",
    "summary": "The package for transition based nueral AMR parser",
    "version": "0.5.2.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17321776,
  "releases": {
    "0.5.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "33bb1881981d2f1741117891ad1cd4d05a719e7b9c5a862be31205d5a9448bac",
          "md5": "8df9f72fad8e3715b0646414ae2b0689",
          "sha256": "b0556944ab8b2df4839f5abeeb9c574c30f69a920cd74f3027b5f9f78549a7f2"
        },
        "downloads": -1,
        "filename": "transition_neural_parser-0.5.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8df9f72fad8e3715b0646414ae2b0689",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.8",
        "size": 530513,
        "upload_time": "2023-03-16T16:34:34",
        "upload_time_iso_8601": "2023-03-16T16:34:34.191737Z",
        "url": "https://files.pythonhosted.org/packages/33/bb/1881981d2f1741117891ad1cd4d05a719e7b9c5a862be31205d5a9448bac/transition_neural_parser-0.5.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3521ad137d8090089d1ce99d4e94716ac084c52e10ceccb61372bfc5f6c97fcc",
          "md5": "5a6462e568c70aeacb466751408994e1",
          "sha256": "ffa15d8018b329639cbb56b23170e301289b8410edcfebbad7d9a3bc2d6f4df5"
        },
        "downloads": -1,
        "filename": "transition_neural_parser-0.5.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5a6462e568c70aeacb466751408994e1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.8",
        "size": 411153,
        "upload_time": "2023-03-16T16:34:36",
        "upload_time_iso_8601": "2023-03-16T16:34:36.032019Z",
        "url": "https://files.pythonhosted.org/packages/35/21/ad137d8090089d1ce99d4e94716ac084c52e10ceccb61372bfc5f6c97fcc/transition_neural_parser-0.5.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.2.dev0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ad9d676edba0c940e909ed52a5a6585b57f3a864c3311794ea2567a128edf335",
          "md5": "34e845e16c8a56878b504c95343fb5cd",
          "sha256": "5df10d309b118ca321e815791435ff279bbeb1523c881911d0fc2988a690ed36"
        },
        "downloads": -1,
        "filename": "transition_neural_parser-0.5.2.dev0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "34e845e16c8a56878b504c95343fb5cd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.8",
        "size": 530543,
        "upload_time": "2023-03-16T14:41:55",
        "upload_time_iso_8601": "2023-03-16T14:41:55.690592Z",
        "url": "https://files.pythonhosted.org/packages/ad/9d/676edba0c940e909ed52a5a6585b57f3a864c3311794ea2567a128edf335/transition_neural_parser-0.5.2.dev0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "93e9b6afda8c1c9c5f9305b689364773aae6d3b805b01cfd1d147e753f29e2fb",
          "md5": "827bd4e1ce16b76159f395ac68be1b5b",
          "sha256": "a48f383d0152510eeba8a1ad3b8b10a952077990f47f4afbc462df038902016f"
        },
        "downloads": -1,
        "filename": "transition_neural_parser-0.5.2.dev0.tar.gz",
        "has_sig": false,
        "md5_digest": "827bd4e1ce16b76159f395ac68be1b5b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.8",
        "size": 411565,
        "upload_time": "2023-03-16T14:41:57",
        "upload_time_iso_8601": "2023-03-16T14:41:57.747685Z",
        "url": "https://files.pythonhosted.org/packages/93/e9/b6afda8c1c9c5f9305b689364773aae6d3b805b01cfd1d147e753f29e2fb/transition_neural_parser-0.5.2.dev0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "33bb1881981d2f1741117891ad1cd4d05a719e7b9c5a862be31205d5a9448bac",
        "md5": "8df9f72fad8e3715b0646414ae2b0689",
        "sha256": "b0556944ab8b2df4839f5abeeb9c574c30f69a920cd74f3027b5f9f78549a7f2"
      },
      "downloads": -1,
      "filename": "transition_neural_parser-0.5.2.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8df9f72fad8e3715b0646414ae2b0689",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "~=3.8",
      "size": 530513,
      "upload_time": "2023-03-16T16:34:34",
      "upload_time_iso_8601": "2023-03-16T16:34:34.191737Z",
      "url": "https://files.pythonhosted.org/packages/33/bb/1881981d2f1741117891ad1cd4d05a719e7b9c5a862be31205d5a9448bac/transition_neural_parser-0.5.2.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3521ad137d8090089d1ce99d4e94716ac084c52e10ceccb61372bfc5f6c97fcc",
        "md5": "5a6462e568c70aeacb466751408994e1",
        "sha256": "ffa15d8018b329639cbb56b23170e301289b8410edcfebbad7d9a3bc2d6f4df5"
      },
      "downloads": -1,
      "filename": "transition_neural_parser-0.5.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "5a6462e568c70aeacb466751408994e1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "~=3.8",
      "size": 411153,
      "upload_time": "2023-03-16T16:34:36",
      "upload_time_iso_8601": "2023-03-16T16:34:36.032019Z",
      "url": "https://files.pythonhosted.org/packages/35/21/ad137d8090089d1ce99d4e94716ac084c52e10ceccb61372bfc5f6c97fcc/transition_neural_parser-0.5.2.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}