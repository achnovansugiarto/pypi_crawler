{
  "info": {
    "author": "Fabian-Robert StÃ¶ter (Inria)",
    "author_email": "fabian-robert.stoter@inria.fr",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# gbif-dl ðŸŒ± > ðŸ’¾\n\n[![Build Status](https://github.com/plantnet/gbif-dl/workflows/CI/badge.svg)](https://github.com/plantnet/gbif-dl/actions?query=workflow%3ACI+branch%3Amaster+event%3Apush)\n[![Supported Python versions](https://img.shields.io/pypi/pyversions/gbif-dl.svg)](https://pypi.python.org/pypi/gbif-dl)\n[![Documentation Status](https://img.shields.io/badge/docs-api-blue)](https://plantnet.github.io/gbif-dl/)\n\nthis package makes it simpler to obtain media data from the GBIF database to be used for training **machine learning classification** tasks. It wraps the [GBIF API](https://www.gbif.org/developer/summary) and supports directly querying the api to obtain and download a list of urls.\nExisting saved queries can also be obtained using the download api of GBIF simply by providing GBIF DOI key.\nThe package provides an efficient downloader that uses python asyncio modules to speed up downloading of many small files as typically occur in downloads.\n\n## Disclaimer\n\nUnlike GBIF occurrences that all have a creative common license (CC0, CC BY, or CC BY-NC), GBIF [does not give any official recommendation](https://data-blog.gbif.org/post/gbif-multimedia/) for licensing shared media files. The License fields are essentially free text filled in by the data provider. Data providers are strongly encouraged to set their licenses in a machine-readable format, but there is no guarantee. Thus, it is the responsibility of GBIF-DL users to set up the appropriate filters on the license field and to respect the conditions of use of these licenses.\n\nSince we are heading into relatively new territory regarding the use of GBIF media files for machine learning, it is currently unclear how publishers will feel about this when learning that their photos are used in this way. At Pl@ntNet, the choice to publish our images in GBIF has been carefully considered and we are fully aware that they would probably be used for this purpose. But we fully understand that some other data providers didn't think of this and we are interested to have open discussion around these aspects.\n\n## Installation\n\nInstallation can be done via pip.\n\n```\npip install gbif-dl\n```\n\n## Usage\n\nThe usage of `gbif-dl` helps users to create their own GBIF based media pipeline for training machine learning models. The package provides two core functionalities as followed:\n\n1. `gbif-dl.generators`: Generators provide image urls from the GBIF database given queries or a pre-defined URL.\n2. `gbif-dl.io`: Provides efficient media downloading to write the data to a storage device.\n\n### 1. Retrieve media urls from GBIF\n\n`gbif-dl` supports two ways to retrieve image urls. One is to use directly query the gbif api the `gbif_dl.api` module. This is suited for quickly retrieving smaller datasets that do not require extensive query parameters. Another way is to use already the [gbif download workflows](https://www.gbif.org/data-processing) which assemble a [Darwin Core Archives](https://github.com/gbif/ipt/wiki/DwCAHowToGuide) waiting on the gbif servers. These can be downloaded and parsed using the `gbif_dl.dwca` module as explained below.\n\n#### `gbif_dl.generators.api`: getting occurance media URLS by querying GBIF\n\nThe query supports all fields that are supported by the [GBIF occurance API](https://www.gbif.org/developer/occurrence#search). In the following example, we query three plants using the `speciesKey` of GBIF from the list of [top 1200 invasive plant species](https://www.cabi.org/ISC). Also, we are limiting the results by only retrieving results from [Plantnet](https://plantnet.org) _and_ [iNaturalist](https://www.inaturalist.org/). using the `datasetKey`.\n\nThe query is passed as a simple dictionary:\n\n```python\nqueries = {\n    \"speciesKey\": [\n        5352251, # \"Robinia pseudoacacia L\"\n        3190653, # \"Ailanthus altissima (Mill.) Swingle\"\n        3189866  # \"Acer negundo L\"\n    ],\n    \"datasetKey\": [\n        \"7a3679ef-5582-4aaa-81f0-8c2545cafc81\",  # plantnet\n        \"50c9509d-22c7-4a22-a47d-8c48425ef4a7\"  # inaturalist\n    ]\n}\n```\n\nGive this query, we can pass this to the `api.generate_urls` function which returns a python\ngenerator:\n\n```python\nimport gbif_dl\ndata_generator = gbif_dl.api.generate_urls(\n    queries=queries,\n    label=\"speciesKey\",\n)\n```\n\nAdditionally we have to specify the output `label` from the occurances which doesn't\nnecessarily have to be part of the query attributes. The `label` is later used to classify the results and store the data in hierachical structure: `label/image.jpg`.\n\nIterating over the generator now yields the media data returning a few thousand urls.\n\n```python\nfor i in data_generator:\n    print(i)\n```\n\neach return entry is a dictionary of media attributes, to be consumed by the downloader.\n\n```python\n{\n    'url': 'https://bs.plantnet.org/image/o/cfa25c7fb5cdf12719d1345769d3936d0ca73974',\n    'basename': 'fdcc3440ab0e3abf824a5c68c864b018cccfcd3b',\n    'label': '5352251'\n},\n{\n    'url': 'https://static.inaturalist.org/photos/58881180/original.jpeg?1577914533',\n    'basename': '7db818c0708ba859516353ff9b30ef942aca19de',\n    'label': '3189866'\n},\n{\n    'url': 'https://static.inaturalist.org/photos/58866788/original.jpeg?1577898729',\n    'basename': '58ae3ef46e59e9a06d67de09c8b7ef3b8db3c85a',\n    'label': '3189866'\n}\n```\n\n#### Balancing items\n\nVery often users won't be using all media downloads from a given query since this often results in datasets with heavily inbalanced number of samples per label. When generating urls from the API, users can specify certain additional attributes to influence the sampling process. For example, to balance the dataset by the dataset provider and by the species the following arguments can be used:\n\n- `split_streams_by`: splits the query into combination of several substreams where each stream represents the product of the query values. When combined with `nb_samples`, this produces a balanced dataset where each stream yields the same number of samples.\n- `nb_samples`: an integer that limits the total number of samples to be generated from the balanced streams. E.g, this can be used to just get `100` samples from the api. When set to `-1`, the minimum number of samples from all streams is used, hence this results in the **maximum number of balanced** sampled from all streams.\n\nIn the following example, we will receive a balanced dataset assembled from `3 species * 2 datasets = 6 streams` and only get minumum number of total samples from all 6 streams:\n\n```python\ndata_generator = gbif_dl.api.generate_urls(\n    queries=queries,\n    label=\"speciesKey\",\n    nb_samples=-1,\n    split_streams_by=[\"datasetKey\", \"speciesKey\"],\n)\n```\n\nFor other, more advanced, use-cases users can add more constraints:\n\n- `nb_samples_per_stream`: put a hard **limit** on the _maximum number of samples_ to be yielded by a stream.\n- `weighted_streams`: weights each stream by its original distribution. That way users can get a smaller subset of the data but keep the original **unbalanced** distribution of the data.\n\nThe following dataset consist of exactly 1000 samples for which the distribution of `speciesKey` is maintained from the full query of all samples. Furthermore, we only allow a maxmimum of 800 samples per species.\n\n```python\ndata_generator = gbifmediads.api.generate_urls(\n    queries=queries,\n    label=\"speciesKey\",\n    nb_samples=1000,\n    nb_samples_per_stream=800,\n    weighted_streams=True,\n    split_streams_by=[\"speciesKey\"],\n)\n```\n\n### Get URLS using Darwin Core Archives\n\nA url generator can also be created from a GBIF download link given a registered DOI or a GBIF download ID. In the following example we will be downloading and parse DWCA archive [that should yield the same results as in the query example above.](https://www.gbif.org/occurrence/download/0117522-200613084148143).\n\n- `dwca_root_path`: Set root path where to store the DWCA zip files. Defaults to None, which results in the creation of a temporary directory, If the path and DWCA archive already exist, it will not be downloaded again.\n\nThe following example creates a data_generator with the the same output class label as in the example above.\n\n```python\ndata_generator = gbif_dl.dwca.generate_urls(\n    \"10.15468/dl.vnm42s\", dwca_root_path=\"dwcas\", label=\"speciesKey\"\n)\n```\n\n### Downloading images to disk\n\nDownloading from a url generator can simply be done by running.\n\n```python\nstats = gbif_dl.io.download(data_generator, root=\"my_dataset\")\n```\n\nThe downloader provides very fast download speeds by using an async queue. Some fail-safe functionality can be provided by setting the number of `retries` to higher than 1.\n\n### Training Datasets\n\n#### PyTorch\n\n`gbif-dl` makes it simple to train a PyTorch image classification model by using e.g. `torchvision.ImageFolder`. Each item in the `data_generator` can be randomly assigned to a `train` or `test` subset using `random_subsets`. That way users can directly use the subsets.\n\n```python\nimport torchvision\ngbif_dl.io.download(data_generator, root=\"my_dataset\", random_subsets={'train': 0.9, 'test': 0.1})\ntrain_dataset = torchvision.datasets.ImageFolder(root='my_dataset/train', ...)\ntest_dataset = torchvision.datasets.ImageFolder(root='my_dataset/test', ...)\n```\n\n#### Tensorflow\n\nThe simpliest way to generate a `tf.data.Dataset` pipeline from a data generator is to use `tf.keras.preprocessing.image_dataset_from_directory`.\nSimilarily to the pytorch example, users just need to provide the root paths of the downloaded datasets.\n\n```python\nimport tensorflow as tf\ngbif_dl.io.download(data_generator, root=\"my_dataset\", random_subsets={'train': 0.9, 'test': 0.1})\ntrain_dataset = tf.keras.preprocessing.image_dataset_from_directory(root='my_dataset/train', label_mode=\"categorical\", labels=\"inferred\", *args, **kwargs)\ntest_dataset = tf.keras.preprocessing.image_dataset_from_directory(root='my_dataset/test', label_mode=\"categorical\", labels=\"inferred\", *args, **kwargs)\n```\n\n## FAQ\n\n#### Q: Downloading doesn't work from inside a jupyter notebook\n\nThis is a known issue of running asyncio code from within jupyter.\nPlease execute these lines before using gbif-dl\n\n```python\nimport nest_asyncio\nnest_asyncio.apply()\n```\n\n## License\n\nMIT\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/plantnet/gbif-dl",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "gbif-dl",
    "package_url": "https://pypi.org/project/gbif-dl/",
    "platform": null,
    "project_url": "https://pypi.org/project/gbif-dl/",
    "project_urls": {
      "Homepage": "https://github.com/plantnet/gbif-dl"
    },
    "release_url": "https://pypi.org/project/gbif-dl/0.1.1/",
    "requires_dist": [
      "aiofiles (>=0.6.0)",
      "aiohttp (>=3.7.2)",
      "aiohttp-retry (>=2.3)",
      "aiostream (>=0.4.3)",
      "pygbif (>=0.5.0)",
      "requests-cache (==0.7.4)",
      "pescador (>=2.1.0)",
      "python-dwca-reader",
      "filetype (>=1.0.0)",
      "tqdm",
      "typing-extensions ; python_version < \"3.8\"",
      "pdoc3 ; extra == 'docs'",
      "pytest ; extra == 'tests'"
    ],
    "requires_python": ">=3.6",
    "summary": "Machine learning data loaders for GBIF",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15123278,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "22032a6fe05a68989b77458b6dc25bbfe8e838f9057e99bd469b99f81676691f",
          "md5": "d46c5cae8a060e3236e64b1aad36c479",
          "sha256": "ae2a795c499cad2e5afc86fab38e47cdfd8de47fa9b08ba8ecb9bf94053a1ba9"
        },
        "downloads": -1,
        "filename": "gbif-dl-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "d46c5cae8a060e3236e64b1aad36c479",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 18495,
        "upload_time": "2021-02-25T13:04:40",
        "upload_time_iso_8601": "2021-02-25T13:04:40.679620Z",
        "url": "https://files.pythonhosted.org/packages/22/03/2a6fe05a68989b77458b6dc25bbfe8e838f9057e99bd469b99f81676691f/gbif-dl-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e712d4e4653ee83b2409b818df66dab5c96c22dacf8922844e90199bef6584ed",
          "md5": "3f9d55e175c975360f7463fc0e702b0d",
          "sha256": "35000494be5ed33a285661d1f52522c2ec9f486602fab6a56a22f2e32b0e630d"
        },
        "downloads": -1,
        "filename": "gbif_dl-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3f9d55e175c975360f7463fc0e702b0d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 19974,
        "upload_time": "2022-09-16T22:59:01",
        "upload_time_iso_8601": "2022-09-16T22:59:01.352396Z",
        "url": "https://files.pythonhosted.org/packages/e7/12/d4e4653ee83b2409b818df66dab5c96c22dacf8922844e90199bef6584ed/gbif_dl-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f15b4c85863379f68931fe0d1dd7fafe90b10cfc951c3464e512136861afa195",
          "md5": "71bd1e3e71005621c565f0a59b991c9a",
          "sha256": "56d536a54ab72124aa67c0b89d93988c3dc2bc3cc624d22c6f2e98eca4578047"
        },
        "downloads": -1,
        "filename": "gbif-dl-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "71bd1e3e71005621c565f0a59b991c9a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 21501,
        "upload_time": "2022-09-16T22:59:03",
        "upload_time_iso_8601": "2022-09-16T22:59:03.182987Z",
        "url": "https://files.pythonhosted.org/packages/f1/5b/4c85863379f68931fe0d1dd7fafe90b10cfc951c3464e512136861afa195/gbif-dl-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e712d4e4653ee83b2409b818df66dab5c96c22dacf8922844e90199bef6584ed",
        "md5": "3f9d55e175c975360f7463fc0e702b0d",
        "sha256": "35000494be5ed33a285661d1f52522c2ec9f486602fab6a56a22f2e32b0e630d"
      },
      "downloads": -1,
      "filename": "gbif_dl-0.1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3f9d55e175c975360f7463fc0e702b0d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 19974,
      "upload_time": "2022-09-16T22:59:01",
      "upload_time_iso_8601": "2022-09-16T22:59:01.352396Z",
      "url": "https://files.pythonhosted.org/packages/e7/12/d4e4653ee83b2409b818df66dab5c96c22dacf8922844e90199bef6584ed/gbif_dl-0.1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f15b4c85863379f68931fe0d1dd7fafe90b10cfc951c3464e512136861afa195",
        "md5": "71bd1e3e71005621c565f0a59b991c9a",
        "sha256": "56d536a54ab72124aa67c0b89d93988c3dc2bc3cc624d22c6f2e98eca4578047"
      },
      "downloads": -1,
      "filename": "gbif-dl-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "71bd1e3e71005621c565f0a59b991c9a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 21501,
      "upload_time": "2022-09-16T22:59:03",
      "upload_time_iso_8601": "2022-09-16T22:59:03.182987Z",
      "url": "https://files.pythonhosted.org/packages/f1/5b/4c85863379f68931fe0d1dd7fafe90b10cfc951c3464e512136861afa195/gbif-dl-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}