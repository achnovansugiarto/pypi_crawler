{
  "info": {
    "author": "zlyuan",
    "author_email": "1277260932@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "Environment :: Web Environment",
      "Intended Audience :: Developers",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# 分布式爬虫精简框架\n\n\n###  安装\n```\npip install zspider\n```\n\n###  框架依赖\n+ 由于使用了队列系统, 此爬虫框架依赖于ssdb\n\n### 配置爬虫框架\n+ 创建一个目录, 这个目录作为你的爬虫组织主路径\n\n```\nmkdir /home/myspider\nmkdir /home/myspider/log\nmkdir /home/myspider/spiders\n```\n\n+ 创建日志目录, 所有的爬虫的日志都将存放到这个目录\n\n```\ncd /home/myspider\nmkdir log\n```\n\n+ 创建配置文件\n```\ntouch /etc/zspider/config.ini\n# win系统: \"c:/zspider/config.ini\"\n# 其他系统: \"/etc/zspider/config.ini\"\n# 可以使用环境变量 ZSPIDER_CONFIG 指定配置文件路径, 优先级最高\n```\n\n+ 修改配置文件为你的设置\n\n```\n[log]\n;是否输出到流\n;write_stream = true\n;是否输出到文件\n;write_file = true\n;日志文件输出路径\n;write_path = /home/myspider/log\n;日志等级\n;log_level = debug\n;每隔几天就新增一个日志文件\n;log_interval = 1\n;保留几个旧的日志文件\n;log_backupCount = 2\n\n[seed_queue]\n;种子队列查询后缀, 请不要更改这个配置, 除非你完全了解此爬虫框架的队列系统是如何运行的\n;suffix = ['vip', 'd1', 'd2', 'd3', 'seed', 'error']\n\n[public_constant]\n;请求等待时间\n;req_timeout = 20\n;一次完整的任务逻辑中出错时, 等待一定时间后才能开始下一个任务\n;spider_err_wait_time = 3\n;种子队列为空后爬虫休眠时间\n;empty_seed_wait_time = 120\n;默认html编码方式\n;default_html_encoding = 'utf-8'\n;重试等待时间\n;retry_wait_fixed = 0.5\n;最大尝试次数\n;max_attempt_count = 5\n\n[ssdb]\n;是否为集群\ncluster = false\n;主机地址\nhost = 你的ssdb服务主机\n;主机端口\nport = 你的ssdb服务端口\n\n[proxy]\n;use_proxy = false\n;代理获取服务器ip和端口,如(xxx.xxx.xxx:1234)\n;proxy_server =\n;代理获取服务器路径,如(getproxy?format=json)\n;proxy_path =\n;无法获取代理地址时重试等待时间\n;except_wait = 1\n;代理模块包名\n;proxy_pack_name = zspider.proxy_base\n;代理模块名\n;proxy_class_name = proxy_base\n```\n\n# 大功告成, 终于可以开始制作爬虫了\n\n### 一个简单的爬虫\n\n+ 在爬虫目录下创建一个文件 my\\_spider.py, 写入如下内容\n\n```python\nfrom zspider.spider_base import spider_base\n\nclass my_spider(spider_base):\n    spider_name = 'my_spider'\n\n    def start_seed(self):\n        yield self.make_seed('https://pypi.org/project/zspider', self.parser_html, encoding='utf8')\n\n    def parser_html(self, res):\n        self.log.info(res.response_text)\n\nif __name__ == '__main__':\n    a = my_spider()\n    a.yield_for_start_seed()\n    a.run()\n```\n\n+ 运行它, 随后你可以在控制台看到打印出来的网页数据\n\n### 这个爬虫做了什么\n```python\n# 从爬虫框架zspider的spider_base模块中导入spider_base类\nfrom zspider.spider_base import spider_base\n\n# 创建自己的爬虫类并继承spider_base\nclass my_spider(spider_base):\n    #设置爬虫的名称为my_spider, 注意: 如果没有设置名称会报错\n    spider_name = 'my_spider'\n\n    #爬虫开始爬取网页时总有一个最开始的网址给它抓取, 这里提交最开始的网址给种子队列\n    def start_seed(self):\n        #构建并提交一个url种子给种子队列, 这个种子包含了三种基本信息: 抓取的网址是'https://pypi.org/project/zspider', 用parser_html函数来解析网页, 这个网页的编码是utf8\n        yield self.make_seed('https://pypi.org/project/zspider', self.parser_html, encoding='utf8')\n\n    #创建一个解析函数, 此函数接收一个参数, 此参数是_seed.Seed类的实例\n    def parser_html(self, res):\n        #将网页数据打印出来\n        self.log.info(res.response_text)\n\nif __name__ == '__main__':\n    #爬虫实例化\n    a = my_spider()\n    #开始提交初始种子\n    a.yield_for_start_seed()\n    #开启爬虫\n    a.run()\n```\n\n### 让爬虫不停的爬下去\n```\n    #在解析函数中\n    def parser_html(self, res):\n        urls = []\n        #在此处写入自己的解析代码, 获取这个网页的一些url存入到urls中\n        #遍历创建并提交url种子\n        for url in urls:\n            #去重检查, 只有第一次检查的数据才会返回True, 可以防止已经抓取的页面再次抓取导致死循环\n            if self.dup_check(url):\n                #这里没有写encoding, 爬虫框架会对每一个网页自动分析出合适的编码来解析网页, 就像浏览器一样\n                yield self.make_seed(url, self.parser_html)\n\n        self.save_result('你想要保存的数据, 它应该是一个dict')\n```\n\n### spider\\_base内建参数说明\n+ 所有的重复参数遵循就近优先原则\n\n参数名|数据类型|默认值|描述|在make\\_seed中<br>为种子单独设置\n--|:--:|:--:|---|:--:\nspider\\_name | str | | 必须设置 |\nmethod | str | get | 默认请求方式 | 是\nencoding | str | | 默认页面编码解码模式, 设为None由爬虫框架自动判断 | 是\nbase\\_headers | dict | {} | 默认请求头, 允许用户添加或修改请求头, 忽略大小写, 爬虫框架会自动补全必须的请求头字段 | 是\ntime\\_out | float | 20 | 请求超时 | 是\nauto\\_cookie_enable | bool | False | 是否自动管理cookie\ncheck\\_html_func | function<br>str | | 页面解析预处理函数, 可能因为某些原因导致页面内容不是你预想的内容, 你可以在这里检查页面内容, 如果不是你想要的内容请返回False, 爬虫框架会更换代理后重新抓取此页面 | 是\nretry\\_http\\_code | list | [] | 如果页面状态码在这个列表中, 爬虫框架会重新抓取页面, 如404 |\n\n\n### spider\\_base方法说明\n+ run\\_of\\_error\\_parser\n> 仅使用解析错误error\\_parser队列进行抓取, 对调试爬虫很有帮助\n\n+ run\n> 运行爬虫\n\n+ dup\\_check\n> 去重检查, 可以有效避免重复抓取页面\n\n参数名|数据类型|默认值|描述\n--|:--:|:--:|---\nitem | | | 必须参数, 这个参数在运算之前会调用str格式为字符串, 然后加密为md5\\_32位字符串, 只有第一次调用去重检查的数据才会返回True\ncollname | str | None | 可选参数, 去重过滤器使用哪个文档保存被去重的数据, 如果设为None, 文档名默认为<爬虫名:dup>\n\n+ clear\\_all\\_queue\n> 清除队列, 慎用, 此函数会将爬虫的种子完全清除\n\n参数名|数据类型|默认值|描述\n--|:--:|:--:|---\nclear\\_parser\\_error | bool | True | 是否清除解析错误的队列\nclear\\_dup | bool | True | 是否清除去重过滤器文档\n\n\n+ make\\_seed\n> 此函数用户构建一个url种子, 它将和页面相关的一切信息保存到一个种子里面\n\n参数名|数据类型|必须参数|描述\n--|:--:|:--:|---\nurl | str | | 要抓取的网址, 尽量把http或https写入, 这样抓取更准确, 允许为空\nparser\\_func | function<br>str | 是 | 表示页面抓取成功后调用哪个解析函数来解析它, 此函数接收一个参数, 请参考[可继承函数说明.parser\\_response]\ncheck\\_html\\_func | function<br>str | | 页面抓取成功后调用哪个页面解析预处理函数, 此函数接收一个参数, 请参考[可继承函数说明.check\\_has\\_need\\_data]\nmeta | python基本类型 | | 此参数用于保存用户想要传递到下一个解析函数的数据, 没错就和scrapy的meta一样\nua\\_type | str | | 随机浏览器头类型('pc', 'ua\\_type', 'ios')\nmethod | str | | 使用什么方式请求url, get还是post, 如果为空则使用内建的爬虫内建参数method, 还是空则默认为get\nparams | | | 参考requests的params\ncookies | dict | | cookies, 要求爬虫内建参数cookie\\_enable为True\nheaders | dict | | 请求头, 参考爬虫内建参数base\\_headers\ndata | | | 参考requests的data\ntimeout | float | | 页面请求等待时间, 超时则认为请求失败, 设为空或0, 爬虫框架会自动更改为20, 爬虫不应该一直等待一个网页\nencoding | str | | 页面编码解码方式, gbk还是utf8? 设为空则使用爬虫内建参数encoding, 还是空则由爬虫框架智能判断. 当页面是图片或是其他非文本页面时, 设为False关闭此种子的解码功能\nproxies | dict | | 主动为此种子设置代理\n\n\n\n+ yield\\_for\\_start\\_seed\n> 根据start\\_url函数提交初始种子\n\n参数名|数据类型|默认值|描述\n--|:--:|:--:|---\nforce\\_yield | bool | False | 是否强行提交初始种子, 如果为False, 在所有正常的种子抓取完毕之前忽略本次操作, 如果为True, 不管是否存在未抓取完毕的种子都立即提交初始种子\n\n### 可继承函数说明\n+ spider\\_init\n> 爬虫初始化完成后会调用这个函数\n\n+ start\\_seed\n> 提交初始种子函数, yield\\_for\\_start\\_seed方法会调用此函数, 此函数允许提交多个初始种子\n\n+ check\\_has\\_need\\_data\n> 页面解析预处理函数, 要使用此函数必须在构建种子的时候用参数check\\_html\\_func指明, 或者在spider\\_base内建参数中设置默认值, 如果你返回了False, 那么爬虫框架会重新抓取这个页面. 如果你的代码出错了, 爬虫框架会将这个种子放入error队列\n\n参数名|数据类型|描述\n--|:--:|---\nres | seed.Seed | 此值包含了抓取成功的页面数据以及原始种子的信息, 参考seed.Seed参数说明. 如果url是空的, 那么这个参数是一个dict, 请参考原始代码seed.Seed.\\_\\_attrs\\_dict\\_\\_\n\n+ parser\\_response\n> 页面解析函数, 要使用此函数必须在构建种子的时候用参数parser\\_func指明, 你可以在此函数内正式解析页面, 然后使用yield提交种子, 如果你的代码出错了, 爬虫框架会将这个种子放入error\\_parser队列\n\n参数名|数据类型|描述\n--|:--:|---\nres | seed.Seed | 此值包含了抓取成功的页面数据以及原始种子的信息, 参考seed.Seed参数说明. 如果url是空的, 那么这个参数是一个dict, 请参考原始代码seed.Seed.\\_\\_attrs\\_dict\\_\\_\n\n### seed.Seed参数说明\n+ 你构建种子时传入的所有参数都将会成为种子实例的属性, 当爬虫框架使用这个种子成功抓取页面后, 以下属性将可用\n\n参数名|数据类型|描述\n--|:--:|---\nresponse | | 请参考requests.Request\nresponse\\_text | str | 网页源代码\nresponse\\_etree | | 请参考lxml.etree.\\_Element\nresponse\\_json | dict | 将网页源代码视为json格式并获取转换为dict的值\nraw\\_data | bytes | 获取网页原始数据, 如果是图片或其他非文本网页时非常有用\n\n### seed.Seed方法说明\n\n+ url\\_join\n> 补全连接地址为完整的网页地址\n\n参数名|数据类型|描述\n--|:--:|---\nlink | str | 一个连接地址, 它可能是不完整的, 使用此函数后会根据当前页面地址补全为完整的地址\n\n### 代理接口说明\n\n+ 创建一个类, 继承zspider.proxy\\_base模块中的proxy\\_base类\n\n```python\n# my_proxy.py\nfrom zspider.proxy_base import proxy_base\n\nclass my_proxy_interface(proxy_base):\n    def proxy_init(self):\n        # 初始化后会调用这个函数\n        pass\n\n    def get_proxy(self):\n        # 返回当前使用的代理, 字典{\"http\": \"http://主机:端口\", \"https\": \"http://主机:端口\"}\n        pass\n\n    def change_proxy(self):\n        # 要求切换代理地址\n        pass\n\n# 然后修改配置文件中[proxy]表的proxy_pack_name为你的代理接口包名, 修改proxy_class_name为你的代理接口类名\n```\n\n### 更新日志\n发布时间|发布版本|发布说明\n--|:--:|---\n19-03-28 | 1.0.2 | 修复一个bug, 在未设置encoding的情况下页面分析会报错\n19-03-28 | 1.0.1 | 现在终结进程时,当前种子会放入error队列,种子不会丢失了\n19-03-27 | 1.0.0 | 发布正式版, 相对于旧版本(0.1.0)更改了大多数使用方法, 取消了mongo依赖:因为用户不一定要用mongo存数据, 缓存服务由redis改为了ssdb, 配置文件简化为最少需要[ssdb]表, 其他修改请自行阅读说明本文档\n\n- - -\n##### 本项目仅供所有人学习交流使用, 禁止用于商业用途\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://pypi.org/project/zspider/",
    "keywords": "",
    "license": "GNU GENERAL PUBLIC LICENSE",
    "maintainer": "",
    "maintainer_email": "",
    "name": "zspider",
    "package_url": "https://pypi.org/project/zspider/",
    "platform": "all",
    "project_url": "https://pypi.org/project/zspider/",
    "project_urls": {
      "Homepage": "https://pypi.org/project/zspider/"
    },
    "release_url": "https://pypi.org/project/zspider/1.0.2/",
    "requires_dist": [
      "zssdb (>=0.1.1)",
      "zsingleton",
      "zretry (>=0.1.1)",
      "zinifile (>=1.0.4)",
      "requests",
      "zlogger (>=0.1.1)",
      "zconst",
      "lxml",
      "msgpack (>=0.6.1)"
    ],
    "requires_python": "",
    "summary": "分布式爬虫精简框架",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4996430,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2315601807153fd1ad57a7f3bc5ddbd49fda7f878c68b025d9e48c905e97b13d",
          "md5": "796d1ccdcb6a491f95e4b9d7f0fb871c",
          "sha256": "952585be0b988a3ff2cad1d7526c5cf5c1c99f55d9480ee4e89f8a8be7ca8e54"
        },
        "downloads": -1,
        "filename": "zspider-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "796d1ccdcb6a491f95e4b9d7f0fb871c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 34573,
        "upload_time": "2019-03-27T06:37:19",
        "upload_time_iso_8601": "2019-03-27T06:37:19.817577Z",
        "url": "https://files.pythonhosted.org/packages/23/15/601807153fd1ad57a7f3bc5ddbd49fda7f878c68b025d9e48c905e97b13d/zspider-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6e80010031a18c5b3e89a7c3767b21b24db1c3ed72624505bc7818cc77e5f5a3",
          "md5": "24c181c0798088d2472502b6c7c34f1f",
          "sha256": "d88ec380adde143883698b536b74ed56a5026f7a3d5ab512252d8b5925a4dcd2"
        },
        "downloads": -1,
        "filename": "zspider-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "24c181c0798088d2472502b6c7c34f1f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 34710,
        "upload_time": "2019-03-28T02:50:22",
        "upload_time_iso_8601": "2019-03-28T02:50:22.225477Z",
        "url": "https://files.pythonhosted.org/packages/6e/80/010031a18c5b3e89a7c3767b21b24db1c3ed72624505bc7818cc77e5f5a3/zspider-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4ecf8e31c13ab11532bb0cf4ffd1069ddf77838c44bfe4ffb3506c1b00a443d7",
          "md5": "b5b17b870c2c13e8aa9e6fcb2e2b26c9",
          "sha256": "ff605c058e3643d7f46db88c677ddd863a0997b5cabcd567e7f9064953206d72"
        },
        "downloads": -1,
        "filename": "zspider-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b5b17b870c2c13e8aa9e6fcb2e2b26c9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 34755,
        "upload_time": "2019-03-28T07:34:25",
        "upload_time_iso_8601": "2019-03-28T07:34:25.829996Z",
        "url": "https://files.pythonhosted.org/packages/4e/cf/8e31c13ab11532bb0cf4ffd1069ddf77838c44bfe4ffb3506c1b00a443d7/zspider-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4ecf8e31c13ab11532bb0cf4ffd1069ddf77838c44bfe4ffb3506c1b00a443d7",
        "md5": "b5b17b870c2c13e8aa9e6fcb2e2b26c9",
        "sha256": "ff605c058e3643d7f46db88c677ddd863a0997b5cabcd567e7f9064953206d72"
      },
      "downloads": -1,
      "filename": "zspider-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b5b17b870c2c13e8aa9e6fcb2e2b26c9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 34755,
      "upload_time": "2019-03-28T07:34:25",
      "upload_time_iso_8601": "2019-03-28T07:34:25.829996Z",
      "url": "https://files.pythonhosted.org/packages/4e/cf/8e31c13ab11532bb0cf4ffd1069ddf77838c44bfe4ffb3506c1b00a443d7/zspider-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}