{
  "info": {
    "author": "Jungang Zou",
    "author_email": "jungang.zou@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Natural Language :: English",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Information Analysis"
    ],
    "description": "# BMIselect\nThis package provides five Bayesian MI-LASSO models for Bayesian variable selection on multiply-imputed data. The inference for these five models are based on MCMC chains written in Python. They take multiply-imputed dataset as the input and output posterior samples of each parameter.\n\nMultiple imputation(MI) is one of the major tools to deal with missing data. However, if we conduct variable selection methods on each imputed dataset separately, different sets of important variables may be obtained. It is very hard to decide which variable is important based on these different variable sets. MI-LASSO, proposed in 2013, is one of the most popular solutions to this problem. The MI-LASSO method treats the estimated regression coefficients of the same variable across all imputed datasets as a group and applies the group LASSO penalty to yield a consistent variable selection across multiple-imputed datasets. In this package, we extend MI-LASSO into Bayesian framework. To yield various selection effects, we use totally five different prior distributions as follows:\n* Ridge prior (ARD prior)\n* Horseshoe prior\n* Laplace prior\n* Spike-ridge prior\n* Spike-laplace prior\n\nFor more details on the algorithm and its applications, please consult the following paper: \"Variable Selection for Multiply-imputed Data: A Bayesian Framework\" (Arxiv: https://arxiv.org/abs/2211.00114).\n\n## Installation\n\nYou can install this package from pip with\n\n`pip install bmiselect`\n\n\n## Requirement\n* Python >= 3.7\n* pymc3 >= 3.11.5\n* theano-pymc >= 1.1.2\n* mkl >= 2.4.0\n* mkl-service >= 2.4.0\n* numpy\n* matplotlib\n* sklearn\n* pandas\n* seaborn\n* arviz\n* xarray\n* statsmodels\n\n\n\n## Models\nThis package is based on the linear regression model: <img src=\"https://latex.codecogs.com/gif.latex?Y=\\alpha+X\\beta+\\epsilon\" /> \n\nDifferent models impose different group-based prior distributions on <img src=\"https://latex.codecogs.com/gif.latex?\\beta\" />. \n\n<table>\n   <tr>\n      <th align=\"center\">Type</th>\n      <th align=\"center\">Group-based Prior</th>\n     <th align=\"center\">Hyper-parameters</th>\n      <th align=\"center\">Default value</th>\n   </tr>\n   <tr>\n      <td style=\"text-align:center\" align=\"center\" rowspan=\"4\" colspan=\"1\">Shrinkage Model</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\" rowspan=\"2\">Multi-Laplace</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\">r</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\">2</td>\n   </tr>\n   <tr>\n    <td style=\"text-align:center\" align=\"center\" colspan=\"1\">s</td>\n      <td style=\"text-align:center\"  align=\"center\" colspan=\"1\">15</td>\n   </tr>\n   <tr>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\" rowspan=\"1\">ARD(Ridge)</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"2\">No hyper-parameters</td>\n   </tr>\n   <tr>\n    <td style=\"text-align:center\" align=\"center\" colspan=\"1\">Horseshoe</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"2\">No hyper-parameters</td>\n   </tr>\n   <tr>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\" rowspan=\"5\">Discrete Mixture Model</td>\n      <td style=\"text-align:center\" align=\"center\" rowspan=\"2\">Spike-Ridge</td>\n      <td style=\"text-align:center\" align=\"center\" rowspan=\"1\">p0</td>\n      <td style=\"text-align:center\" align=\"center\" rowspan=\"1\">0.5</td>\n   <tr>\n     <td style=\"text-align:center\" align=\"center\" colspan=\"1\">v0</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\">4</td>\n   </tr>\n   <tr>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\" rowspan=\"3\">Spike-Laplace</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\">lambda</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\">6/11</td>\n   </tr>\n   <tr>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\">a</td>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\">1</td>\n   </tr>\n      <tr>\n      <td style=\"text-align:center\" align=\"center\" colspan=\"1\">b</td>\n         <td style=\"text-align:center\" align=\"center\" colspan=\"1\">1</td>\n   </tr>\n</tr>\n</table>\n\nThe inference is done with posterior samples by running MCMC. \n\n## Usage\nExamples about how to use this library are included in `bmiselect/main.py`.\n\n### Input\nAfter installation from pip, we can import Bayesian MI-LASSO classes in Python shell:\n```\nfrom bmiselect.models.ARD import ARD\nfrom bmiselect.models.Horseshoe import Horseshoe\nfrom bmiselect.models.Spike_ridge import Spike_ridge\nfrom bmiselect.models.Laplace import Laplace\nfrom bmiselect.models.Spike_laplace import Spike_laplace\n```\n\n### Initialization\nThen we can use MI dataset to initialize the models:\n```\n# shrinkage models\nmodel1 = Laplace(Y_array, X_array, standardize = True, r = 2, s = 15)\n# model1 = Horseshoe(Y_array, X_array, standardize = True)\n# model1 = Ridge(Y_array, X_array, standardize = True)\n\n# discrete mixture models\nmodel2 = Spike_laplace(Y_array, X_array, standardize = True, lambda_ = 6/11, a = 1, b = 1)\n# model2 = Spike_ridge(Y_array, X_array, standardize = True, p0 = 0.5, v0 = 4)\n```\nHere `Y_array` is a 2-d data array for response variable, its dimension is `(n_imputations, n_samples)`. `X_array` is a 3-d data array for explanatory variables, its dimension is `(n_imputations, n_samples, n_features)`. If the parameter `standardize` is True, X_array is standardized and then used to run MCMC chains. If it is False, the original X_array is used to calculate MCMC chains. Other parameters are hyper-parameters for each model.\n\n\n### Posterior Sampling\nAfter initialization, we can use `sample` function to run MCMC chains and get posterior samples:\n```\nmodel1.sample(n_post = 1000, n_burn = 500, target_accept = 0.9, n_chain = 2, n_thread = 4, max_treedepth = 10, seed = 123)\n# model2.sample(n_post = 1000, n_burn = 500, target_accept = 0.9, n_chain = 2, n_thread = 4, max_treedepth = 10, seed = 123)\n```\nThe parameters for `sample` function are as follows:\n* n_post(required): number of posterior samples for each chain.\n* n_burn(required): number of burn-in samples for each chain.\n* target_accept(default 0.9): target acceptance probability for NUTS.\n* max_treedepth(default 10): maximum tree depth for NUTS.\n* n_chain(default 1): number of parallel chains to run.\n* n_thread(default 4): number of threads to run parallel computing.\n* seed(default None): random seed. If seed is None, seed is equals to the current time in seconds since the Epoch.\n\nWe can use `get_posterior_samples` function to get posterior samples:\n```\nmodel1.get_posterior_samples(var_name = \"beta\", rescale = True)   # get posterior samples for the coefficients vector\nmodel2.get_posterior_samples(var_name = \"alpha\", rescale = True)  # get posterior samples for the intercept\nmodel2.get_posterior_samples(var_name = \"g\", rescale = True)      # get posterior samples for the hidden binary variables in discrete mixture models\n```\nHere `var_name` is the variable we want to sample for. `rescale` specifies whether to return coefficients in the original scale; if it is False, then coefficients corresponding to standardized covariates are return; if it is True, all the coefficients are rescaled to the original scale. If `standardize = False` in initialization stage, `rescale` has no effect. For MI data, we simply mixed up the posterior samples for each grouped coefficient among all MI sets. So the dimension of posterior samples for coefficients vector `beta` is `(n_chains, n_imputations * n_samples, n_features)`. And the dimension of intercept `alpha` is `(n_chains, n_imputations * n_samples)`.\n\n### Summary Statistics\nOur library provides a `summary` function to generate summary statistics for all the variables in the hierachical model:\n```\nsummary_stats1 = model1.summary(rescale = True)\nprint(summary_stats1)\n```\nHere `rescale` is the same as it in function `get_posterior_samples`.\n\n\n### Variable Selection\nUsers can use `select` function to select important variables:\n```\nselect1 = model1.select(value = 0.95, rescale = True) # Credible Interval Criterion for Shrinkage Models\nselect2 = model2.select(value = 0.5,  rescale = True) # Cutting-off point for Discrete Mixture Models\n```\nThe meaning of `value` depends on the type of models. For shrinkage models, `value` is the credible interval criterion for selection. For discrete mixture models, `value` stands for the cutting-off point for selection. For more details, please consult Chapter 3.2 in the paper: \"Variable Selection for Multiply-imputed Data: A Bayesian Framework\" (Arxiv: https://arxiv.org/abs/2211.00114).\n\n\n### Evaluation\nThere are some evaluation functions in this library:\n```\nfrom bmiselect.utils.evaluation import sensitivity, specificity, f1_score, mse\n\nsensitivity(select = select1, truth = truth)                                           # sensitivity\nspecificity(select = select2, truth = truth)                                           # specificity\nf1_score(select = select2, truth = truth)                                              # f1 score\nmse(beta, covariance, select = select1, X = X_array, Y = Y_array, intercept = True)    # mse, given coefficients and covariance matrix of ground truth\n```\nHere `select` and `truth` are binary vectors with length `(n_features)`. `select[i] = True` means i-th variable is selected.\n\n\n### Refitting Linear Regression\nAfter we complete the variable selection by Bayesian MI-LASSO, users can apply `fit_lr` to fit ordinary linear regression separately on each imputed dataset. Alternatively, users can utilize `pooled_coefficients` and `pooled_covariance` to directly get pooled coefficients and covariance matrix with Rubin`s Rule.\n```\nfrom bmiselect.utils.evaluation import fit_lr, pooled_coefficients, pooled_covariance\n\n# refit linear regression by using selected variabels\nlr_models = fit_lr(select1, X_array, Y_array, intercept = True))\nfor lr in lr_models:\n    print(lr.summary())\n\n# get pooled coefficients estimates by using Rubin`s rule\nlr_coef = pooled_coefficients(select2, X_array, Y_array, intercept = True))\nprint(lr_coef)\n\n# get pooled covariance estimates by using Rubin`s rule\nlr_covariance = pooled_covariance(select2, X_array, Y_array, intercept = True))\nprint(lr_covariance)\n```\nIf `intercept = True`, then an intercept is added to each ordinary linear regression respecitively. If `intercept = False`, then no intercept is used in linear regression.\n\n\n## Disclaimer\n\nIf you find there is any bug, please contact me: jungang.zou@gmail.com.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/zjg540066169/Bmiselect",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bmiselect",
    "package_url": "https://pypi.org/project/bmiselect/",
    "platform": null,
    "project_url": "https://pypi.org/project/bmiselect/",
    "project_urls": {
      "Code": "https://github.com/zjg540066169/Bmiselect",
      "Homepage": "https://github.com/zjg540066169/Bmiselect"
    },
    "release_url": "https://pypi.org/project/bmiselect/0.0.9/",
    "requires_dist": [
      "pymc3 (>=3.11.5)",
      "theano-pymc (>=1.1.2)",
      "mkl (>=2.4.0)",
      "mkl-service (>=2.4.0)",
      "numpy",
      "matplotlib",
      "scikit-learn",
      "pandas",
      "seaborn",
      "arviz",
      "xarray",
      "statsmodels"
    ],
    "requires_python": ">=3.7.*",
    "summary": "Bayesian MI-LASSO for variable selection on multiply-imputed data.",
    "version": "0.0.9",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15775398,
  "releases": {
    "0.0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "87677986ad129231e92a8ba815e38672310dc8c5929fcc585c0f946c851a87f6",
          "md5": "c9726382ec747d14239b7fd3e92fe5b4",
          "sha256": "ba5223162e5a8d157f57c62e046a3691954ff2b0347e4f62d907eaee0d2e789c"
        },
        "downloads": -1,
        "filename": "bmiselect-0.0.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c9726382ec747d14239b7fd3e92fe5b4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.*",
        "size": 28268,
        "upload_time": "2022-11-15T14:01:28",
        "upload_time_iso_8601": "2022-11-15T14:01:28.056313Z",
        "url": "https://files.pythonhosted.org/packages/87/67/7986ad129231e92a8ba815e38672310dc8c5929fcc585c0f946c851a87f6/bmiselect-0.0.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "abc8b0a2df930964f55141d03df8bf3e35da0c779e3852628fbf9b7c1af783a2",
          "md5": "cd83f9767c4bb9a36cd8e89275ee43b3",
          "sha256": "3eb3daaa5de529900142f25ee20df752d298995c0412be6bef425692dc53e6df"
        },
        "downloads": -1,
        "filename": "bmiselect-0.0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "cd83f9767c4bb9a36cd8e89275ee43b3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.*",
        "size": 20325,
        "upload_time": "2022-11-15T14:01:30",
        "upload_time_iso_8601": "2022-11-15T14:01:30.590152Z",
        "url": "https://files.pythonhosted.org/packages/ab/c8/b0a2df930964f55141d03df8bf3e35da0c779e3852628fbf9b7c1af783a2/bmiselect-0.0.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "87677986ad129231e92a8ba815e38672310dc8c5929fcc585c0f946c851a87f6",
        "md5": "c9726382ec747d14239b7fd3e92fe5b4",
        "sha256": "ba5223162e5a8d157f57c62e046a3691954ff2b0347e4f62d907eaee0d2e789c"
      },
      "downloads": -1,
      "filename": "bmiselect-0.0.9-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c9726382ec747d14239b7fd3e92fe5b4",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7.*",
      "size": 28268,
      "upload_time": "2022-11-15T14:01:28",
      "upload_time_iso_8601": "2022-11-15T14:01:28.056313Z",
      "url": "https://files.pythonhosted.org/packages/87/67/7986ad129231e92a8ba815e38672310dc8c5929fcc585c0f946c851a87f6/bmiselect-0.0.9-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "abc8b0a2df930964f55141d03df8bf3e35da0c779e3852628fbf9b7c1af783a2",
        "md5": "cd83f9767c4bb9a36cd8e89275ee43b3",
        "sha256": "3eb3daaa5de529900142f25ee20df752d298995c0412be6bef425692dc53e6df"
      },
      "downloads": -1,
      "filename": "bmiselect-0.0.9.tar.gz",
      "has_sig": false,
      "md5_digest": "cd83f9767c4bb9a36cd8e89275ee43b3",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.*",
      "size": 20325,
      "upload_time": "2022-11-15T14:01:30",
      "upload_time_iso_8601": "2022-11-15T14:01:30.590152Z",
      "url": "https://files.pythonhosted.org/packages/ab/c8/b0a2df930964f55141d03df8bf3e35da0c779e3852628fbf9b7c1af783a2/bmiselect-0.0.9.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}