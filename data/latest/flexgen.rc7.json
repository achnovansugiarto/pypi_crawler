{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3"
    ],
    "description": "# FlexGen (Still Working in Progress!)\n\nFlexGen is a high-throughput generation engine for running large language models with limited GPU memory (e.g., a 16GB T4 GPU or a 24GB RTX3090 gaming card!). FlexGen allows **high-throughput** generation by IO-efficient offloading, compression and **large effective batch sizes**.\n\n## Recent Changes (It is getting better thanks to youüôè)\nWe are glad the public has been really excited about FlexGen. However, our work is still under preparation and not ready for public release / announcement yet.\nThanks to early feedback about this project, we realized that early versions of this README and our paper were a bit unclear about the purpose of FlexGen.\n**This is a preliminary effort to lower the resource requirements of LLMs, but it also has a lot of limitations and does not aim to replace use cases when sufficient resources are available.**\nOur primary contributions are increasing throughput on single GPU instances - by effectively increasing the batch size.\nWe're really excited about our techniques for offloading and automatically searching through the design space, as well as our results that suggest it's possible to go down to 4-bit quantization without hurting accuracy.\nThis naturally trades off latency, but we think it's a really interesting direction for future work.\nWe'd like to thank everyone for their feedback - keep it coming!\n\n----------\n\nFlexGen was made possible thanks to a collaboration with\n\n<a href=\"https://cs.stanford.edu/\"><img src=\"https://identity.stanford.edu/wp-content/uploads/sites/3/2020/06/wordmark-nospace-red.png\" height=\"20\"></a> &nbsp;&nbsp;&nbsp; <a href=\"https://sky.cs.berkeley.edu/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/82/University_of_California%2C_Berkeley_logo.svg/1280px-University_of_California%2C_Berkeley_logo.svg.png\" height=\"22\"></a> &nbsp;&nbsp;&nbsp; <a href=\"https://www.together.xyz/\"><img src=\"https://images.squarespace-cdn.com/content/v1/6358bea282189a0adf57fe16/eef09191-631f-40d9-9bfd-f875b25bcf0b/together-logo-black-transparent2.png\" height=\"20\"></a> &nbsp;&nbsp;&nbsp; <a href=\"https://ds3lab.inf.ethz.ch/\"><img src=\"https://user-images.githubusercontent.com/1608867/220273382-c09669b3-42fd-47c2-b88c-7ed55cb43820.png\" height=\"20\"></a>\n\n----------\n\nThe high computational and memory requirements of large language model (LLM) inference traditionally make it feasible only with multiple high-end accelerators.\nFlexGen aims to lower the resource requirements of LLM inference down to a single commodity GPU (e.g., T4, 3090) and allow flexible deployment for various hardware setups. The key technique behind FlexGen is to trade off between **latency** and **throughput**.\n\nThe key features of FlexGen include:  \n\n‚ö° **High-Throughput Offloading**.  \nHigher-throughput generation than other offloading-based systems (e.g., Hugging Face Accelerate, DeepSpeed Zero-Inference) - sometimes by orders of magnitude. The key innovation is a new offloading technique that can effectively increase the batch size. This can be useful for batch inference scenarios, such as benchmarking (e.g., [HELM](https://github.com/stanford-crfm/helm)) and [data wrangling](https://arxiv.org/abs/2205.09911).\n\nüì¶ **Extreme Compression**.  \nCompress both the parameters and attention cache of models, such as OPT-175B, down to 4 bits with negligible accuracy loss.\n\nüöÄ **Scalability**.  \nCome with a distributed pipeline parallelism runtime to allow scaling if more GPUs are given.\n\n‚ùå **Limitation**.  \nAs an offloading-based system running on weak GPUs, FlexGen also has its limitations.\nFlexGen can be significantly slower than the case when you have enough powerful GPUs to hold the whole model, especially for small-batch cases.\nFlexGen is mostly optimized for throughput-oriented batch processing settings (e.g., classifying or extracting information from many documents in batches), on single GPUs.\n\n[**Join Discord**](https://discord.gg/JfphDTkBAh)\n\n## Content\n- [Benchmark Results](#benchmark-results)\n- [Install](#install)\n- [Get Started with a Single GPU](#get-started-with-a-single-gpu)\n- [API Example](#api-example)\n- [Scaling to Distributed GPUs](#scaling-to-distributed-gpus)\n- [Roadmap](#roadmap)\n\n## Benchmark Results\n### Generation Throughput (token/s)\nThe corresponding effective batch size is in the bracket. Please see [here](benchmark/batch_size_table.md) for more details.\n| System | OPT-6.7B | OPT-30B | OPT-175B |\n| ------ | -------- | ------- | -------- |\n| Hugging Face Accelerate   | 25.12 (2 on gpu) | 0.62 (8 on cpu\t) | 0.01 (2 on disk) |\n| DeepSpeed ZeRO-Inference | 9.28 (16 on cpu)  | 0.60 (4 on cpu) | 0.01 (1 on disk) |\n| Petals\\*                 | -     | -    | 0.05 |\n| FlexGen                  | 25.26 (2 on gpu) | 7.32 (144 on cpu) | 0.69 (256 on disk) |\n| FlexGen with Compression | **29.12** (72 on gpu) | **8.38** (512 on cpu) | **1.12** (144 on cpu) |\n\n- Hardware: an NVIDIA T4 (16GB) instance on GCP with 208GB of DRAM and 1.5TB of SSD.  \n- Workload: input sequence length = 512, output sequence length = 32. The batch size is tuned to **a large value** that maximizes the generation throughput for each system.\n- Metric: generation throughput (token/s) = number of the generated tokens / (time for processing prompts + time for generation).  \n\nHow to [reproduce](benchmark/flexgen).\n\n### Latency-Throughput Trade-Off\nThe figure below shows the latency and throughput trade-off of three offloading-based systems on OPT-175B (left) and OPT-30B (right).\nFlexGen achieves a new Pareto-optimal frontier with significatnly higher maximum throughput for both models.\nOther systems cannot further increase throughput due to out-of-memory.\n\"FlexGen(c)\" is FlexGen with compression.\n\n<img src=\"https://github.com/FMInference/FlexGen/blob/main/docs/throughput_vs_latency.jpg\" alt=\"image\" width=\"500\"></img>\n\n## How It Works\nFlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. Through a linear programming optimizer, it searches for the best pattern to store and access the tensors, including weights, activations, and attention key/value (KV) cache. FlexGen further compresses both weights and KV cache to 4 bits with negligible accuracy loss. \n\nOne key idea of FlexGen is to play the latency-throughput trade-off. Achieving low latency is inherently challenging for offloading methods, \nbut the I/O efficiency of offloading can be greatly boosted for throughput-oriented scenarios (see the figure above).\nFlexGen utilizes a block schedule to reuse weight and overlap I/O with computation, as shown in figure (b) below, while other baseline systems use an inefficient row-by-row schedule, as shown in figure (a) below.\n\n<img src=\"https://github.com/FMInference/FlexGen/raw/main/docs/block_schedule.jpg\" alt=\"image\" width=\"500\"></img>\n\n## Install\nRequirements:  \n - PyTorch >= 1.12 [(Help)](https://pytorch.org/get-started/locally/)\n\n### Method 1: With pip\n```\npip install flexgen\n```\n\n### Method 2: From source\n```\ngit clone https://github.com/FMInference/FlexGen.git\ncd FlexGen\npip install -e .\n```\n\n### Optional\nInstall openmpi for multi-gpu execution.\n```\nsudo apt install openmpi-bin\n```\n\n## Get Started with a Single GPU\n\n### OPT-1.3B\nTo get started, you can try a small model like OPT-1.3B first. It fits into a single GPU so no offloading is required.\nFlexGen will automatically download weights from Hugging Face.\n```\npython3 -m flexgen.flex_opt --model facebook/opt-1.3b\n```\n\nYou should see some text generated by OPT-1.3B and the benchmark results.\n\n### OPT-30B\nTo run large models like OPT-30B, you will need to use CPU offloading. You can try commands below.\nThe `--percent` argument specifies the offloading strategy for parameters, attention cache and hidden states separately.\nThe exact meaning of this argument can be found [here](https://github.com/FMInference/FlexGen/blob/9d092d848f106cd9eaf305c12ef3590f7bcb0277/flexgen/flex_opt.py#L1271-L1279).\n```\npython3 -m flexgen.flex_opt --model facebook/opt-30b --percent 0 100 100 0 100 0\n```\n\n### OPT-175B\nTo run OPT-175B, you need to download the weights from [metaseq](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT) and convert the weights into Alpa [format](https://alpa.ai/tutorials/opt_serving.html#convert-opt-175b-weights-into-alpa-formats).\nThe numpy weights should be put under `~/opt_weights/opt-175b-np/` by default. You can check the downloaded weights of smaller models for the required format.\nYou can then try to offloading all weights to disk by\n```\npython3 -m flexgen.flex_opt --model facebook/opt-175b --percent 0 0 100 0 100 0 --offload-dir YOUR_SSD_FOLDER\n```\n\n### How to set the offloading strategy and `--percent`?\nWe will release an automatic policy optimizer later, but now you have to manually try a few strategies.\nThe idea of high-throughput generation is to offload parameters and attention cache as much as possible to the CPU and disk if necessary.\nYou can see the reference strategies in our benchmark [here](https://github.com/FMInference/FlexGen/blob/9d092d848f106cd9eaf305c12ef3590f7bcb0277/benchmark/flexgen/bench_suite.py#L39-L79).\nTo avoid out-of-memory, you can tune the `--percent` of offload more tensors to the CPU and disk.\n\n## Scaling to Distributed GPUs\nIf you have more GPUs, FlexGen can combine offloading with pipeline parallelism to allow scaling.\nFor example, if you have 2 GPUs but the aggregated GPU memory is less than the model size, you still need offloading. FlexGen allow you to do pipeline parallelism with these 2 GPUs to accelerate the generation.\nSee examples [here](https://github.com/FMInference/FlexGen/tree/main/benchmark/flexgen#distributed-gpus).\n\n## API Example\nWe demonstrate the usage of FlexGen API in [completion.py](flexgen/apps/completion.py).\nThis example shows how to run generation for two sentences.\nTo get the best throughput out of FlexGen, you typically need to batch more sentences.\n\n### Generation API\nFlexGen has a generation API following the style of Hugging Face's transformers.\n```python\noutput_ids = model.generate(\n\tinput_ids,\n\tdo_sample=True,\n\ttemperature=0.7,\n\tmax_new_tokens=32,\n\tstop=stop)\n```\n\n### Example Commands\nYou can use the example commands below.\nIf you do not have enough GPU/CPU memory, see the [Handle Out-Of-Memory](#handle-out-of-memory) section.\n\n```\n# Complete with OPT-6.7B. You need at least 15GB of GPU memory.\npython3 -m flexgen.apps.completion --model facebook/opt-6.7b\n```\n\n```\n# Complete with OPT-30B. You need about 90GB of CPU memory.\npython3 -m flexgen.apps.completion --model facebook/opt-30b --percent 0 100 100 0 100 0\n```\n\n```\n# Complete with instruction-tuned OPT-IML-MAX-30B. You need about 90GB of CPU memory.\npython3 -m flexgen.apps.completion --model facebook/opt-iml-max-30b --percent 0 100 100 0 100 0\n```\n\n### Handle Out-Of-Memory\nIf you do not have enough GPU/CPU memory, here are a few things you can try.\nThey save more memory but run slower.\n\n- Do not pin weights by adding `--pin-weight 0`. This can reduce the weight memory usage on CPU by around 20% or more.\n- Enable weight compression by adding `--compress-weight`. This can reduce the weight memory usage by around 70%.\n- Offload all weights to disk by using `--percent 0 0 100 0 100 0`. This requires very little CPU and GPU memory.\n\n### More Applications\nSee [flexgen/apps](flexgen/apps) for more example applications.\n\n## Roadmap\nWe plan to work on the following features. Community contributions are welcome.\n\n- [ ] Support Apple silicon M1/M2 deployment\n- [ ] Support Colab deployment\n- [ ] Support more models (BLOOM, CodeGen, GLM)\n- [ ] Release the cost model and policy optimizer\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "flexgen",
    "package_url": "https://pypi.org/project/flexgen/",
    "platform": null,
    "project_url": "https://pypi.org/project/flexgen/",
    "project_urls": {
      "Bug Tracker": "https://github.com/FMInference/FlexGen/issues",
      "Homepage": "https://github.com/FMInference/FlexGen"
    },
    "release_url": "https://pypi.org/project/flexgen/0.1.7/",
    "requires_dist": [
      "torch (>=1.12)",
      "transformers (>=4.24)",
      "numpy",
      "tqdm",
      "pulp",
      "attrs"
    ],
    "requires_python": ">=3.7",
    "summary": "Running large language models like OPT-175B/GPT-3 on a single GPU. Focusing on high-throughput large-batch generation.",
    "version": "0.1.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17110267,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9c81e84f461c7c706d1c8f579705cfb01180bdf1d34f2753f8530e1d1b87cff1",
          "md5": "0f5feee134e529f99574a9738982f3ba",
          "sha256": "98f20c6fb90f0b1d4028e4d3e5ef799e18c7fe64642ec66ba251641971084d32"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0f5feee134e529f99574a9738982f3ba",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 47083,
        "upload_time": "2023-02-26T02:08:24",
        "upload_time_iso_8601": "2023-02-26T02:08:24.982301Z",
        "url": "https://files.pythonhosted.org/packages/9c/81/e84f461c7c706d1c8f579705cfb01180bdf1d34f2753f8530e1d1b87cff1/flexgen-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "896e42b39c98015a884c5365f384008dc7021135ce31b7b9e71a44a18eaa0466",
          "md5": "bf532ee1ea73bcbd3352440162468990",
          "sha256": "832e204b5616c4bf2d3f08fe2018c5920439a4d270f34488563f7196aff6bc28"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "bf532ee1ea73bcbd3352440162468990",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 46805,
        "upload_time": "2023-02-26T02:08:26",
        "upload_time_iso_8601": "2023-02-26T02:08:26.977831Z",
        "url": "https://files.pythonhosted.org/packages/89/6e/42b39c98015a884c5365f384008dc7021135ce31b7b9e71a44a18eaa0466/flexgen-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "044966441923a7b833f7a01683bb5ed917f0e86e9cbc8ff1fded643756c8956b",
          "md5": "f66b8e816fb4066a13dfc0c0593760f8",
          "sha256": "7be9cd0acea8183d442b1f66a3bc9be8b7e63d953792fe5d958639d2c4594ada"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f66b8e816fb4066a13dfc0c0593760f8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 47152,
        "upload_time": "2023-02-26T02:35:34",
        "upload_time_iso_8601": "2023-02-26T02:35:34.435040Z",
        "url": "https://files.pythonhosted.org/packages/04/49/66441923a7b833f7a01683bb5ed917f0e86e9cbc8ff1fded643756c8956b/flexgen-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "04d43289b780284b918956a480813973145371777b2647aa4e5c5262d5883f4e",
          "md5": "278c75ff59352121de99c60fe5385c98",
          "sha256": "08ec1e70aaf506edaf400f9e58f8564d4ff0151b180d285032b2cbad702c9f6b"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "278c75ff59352121de99c60fe5385c98",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 46931,
        "upload_time": "2023-02-26T02:35:35",
        "upload_time_iso_8601": "2023-02-26T02:35:35.891147Z",
        "url": "https://files.pythonhosted.org/packages/04/d4/3289b780284b918956a480813973145371777b2647aa4e5c5262d5883f4e/flexgen-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ab21178b0df3f0ff56e5d7ff9fe5b2d7a28cade88be4f7acb922b5610b2357aa",
          "md5": "94ea1c4a4064f97ee2ae0cb2f3273960",
          "sha256": "e0a19eec2d8b9eefb08e7a96a999017f73420c4a395bb9716638f0dd927d6775"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "94ea1c4a4064f97ee2ae0cb2f3273960",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 47247,
        "upload_time": "2023-02-26T06:57:17",
        "upload_time_iso_8601": "2023-02-26T06:57:17.552161Z",
        "url": "https://files.pythonhosted.org/packages/ab/21/178b0df3f0ff56e5d7ff9fe5b2d7a28cade88be4f7acb922b5610b2357aa/flexgen-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8c9e42682569fbfb7d1583bbfaec22fd6ed32e988227a4498c2917f86d664dd2",
          "md5": "e580de14623cb0cb2e50d5bc9900f966",
          "sha256": "71ff84ccfd0c98ee64342b0aa446137ff1c2370e91972b8528a6b0c37d6477f9"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "e580de14623cb0cb2e50d5bc9900f966",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 47111,
        "upload_time": "2023-02-26T06:57:19",
        "upload_time_iso_8601": "2023-02-26T06:57:19.159034Z",
        "url": "https://files.pythonhosted.org/packages/8c/9e/42682569fbfb7d1583bbfaec22fd6ed32e988227a4498c2917f86d664dd2/flexgen-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8842bc7c68cddde9714994e148cf1c5c5f1d4811dbab324582bde709b56fab87",
          "md5": "4513cf1d5f8a8e07d819bbd64f657cd6",
          "sha256": "996a0a190f9a247afbaeb6991ea02bd0de17f0a1f53ae1cac413d8d9530d7969"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4513cf1d5f8a8e07d819bbd64f657cd6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 47489,
        "upload_time": "2023-02-26T10:31:22",
        "upload_time_iso_8601": "2023-02-26T10:31:22.828043Z",
        "url": "https://files.pythonhosted.org/packages/88/42/bc7c68cddde9714994e148cf1c5c5f1d4811dbab324582bde709b56fab87/flexgen-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0154a9300aa55de4f51396f50fec11a288c908b6f830440885f7f40757fb4f04",
          "md5": "14828c59687780f441cc3abc5688c1d5",
          "sha256": "7993e7eb416dca36a1bef072e89946fbb6820bd268c0bd2c1fdc8ffe2a0c284c"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "14828c59687780f441cc3abc5688c1d5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 47321,
        "upload_time": "2023-02-26T10:31:24",
        "upload_time_iso_8601": "2023-02-26T10:31:24.731928Z",
        "url": "https://files.pythonhosted.org/packages/01/54/a9300aa55de4f51396f50fec11a288c908b6f830440885f7f40757fb4f04/flexgen-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb0063aa69d149298153446532c31985b829d0d0ea599a9a230ed0eada373ca2",
          "md5": "23a8d77f9b4ab90d1a1a42b978aebc6c",
          "sha256": "b8d1027a666e7e9b63cbfae478898f97e46ad38f2ffc4e5486aa8ac2f11614f4"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "23a8d77f9b4ab90d1a1a42b978aebc6c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 47708,
        "upload_time": "2023-02-26T11:07:31",
        "upload_time_iso_8601": "2023-02-26T11:07:31.946905Z",
        "url": "https://files.pythonhosted.org/packages/bb/00/63aa69d149298153446532c31985b829d0d0ea599a9a230ed0eada373ca2/flexgen-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4f08a019ccbf10603c6f6ae51291893c9e45b9c3f2e7d2e7b5af67aef0e3e814",
          "md5": "5bd83e0ee5327b207e78637dfc0402b0",
          "sha256": "609f7a594ac037575f65b768d0e7d818fbb3bfcc70f03f72e90304c2a4b00f97"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "5bd83e0ee5327b207e78637dfc0402b0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 47319,
        "upload_time": "2023-02-26T11:07:33",
        "upload_time_iso_8601": "2023-02-26T11:07:33.724622Z",
        "url": "https://files.pythonhosted.org/packages/4f/08/a019ccbf10603c6f6ae51291893c9e45b9c3f2e7d2e7b5af67aef0e3e814/flexgen-0.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f9c1f5ed2f9d1ff9b0858740c892b19cb6b554df7ddb6407893860b2a45d41b6",
          "md5": "68a70f8b703015e6d307c87fed6d2ad8",
          "sha256": "521e8d0356ca032b64bb0d94ddcd3b909429880433d6948f45e2dbbd01ab9d70"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "68a70f8b703015e6d307c87fed6d2ad8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 46081,
        "upload_time": "2023-02-27T00:41:33",
        "upload_time_iso_8601": "2023-02-27T00:41:33.539311Z",
        "url": "https://files.pythonhosted.org/packages/f9/c1/f5ed2f9d1ff9b0858740c892b19cb6b554df7ddb6407893860b2a45d41b6/flexgen-0.1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a1a0a37d2ce38cffe7255af42763ff276569e7da5cf9f9046648138fc14ba50e",
          "md5": "4cf00a1237538929bb6fece05ef9b090",
          "sha256": "4a235035e6afc65ee161b0482c100ad11b34923f8cf9237ac5bc0ebb71af42a0"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "4cf00a1237538929bb6fece05ef9b090",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 46790,
        "upload_time": "2023-02-27T00:41:35",
        "upload_time_iso_8601": "2023-02-27T00:41:35.678147Z",
        "url": "https://files.pythonhosted.org/packages/a1/a0/a37d2ce38cffe7255af42763ff276569e7da5cf9f9046648138fc14ba50e/flexgen-0.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "20aea54960a728230f73432a547b07c9369a4c5d58d4e7b93402dbd7b8ffd9b8",
          "md5": "101f6cdd67be21da709d107078e342ac",
          "sha256": "b861e343d78213faa74c904a75b4e6b2edaa8e5f3cbcb403d11ae7b23dd1027b"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "101f6cdd67be21da709d107078e342ac",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 50757,
        "upload_time": "2023-03-01T11:18:25",
        "upload_time_iso_8601": "2023-03-01T11:18:25.895333Z",
        "url": "https://files.pythonhosted.org/packages/20/ae/a54960a728230f73432a547b07c9369a4c5d58d4e7b93402dbd7b8ffd9b8/flexgen-0.1.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "79373ac011e4331a4bdc4ed6e936b1045e8e5a2a7399ff66321c79e575bb38fb",
          "md5": "085e66c750990de4a02f26b673551d17",
          "sha256": "1989e5e4262c31681375b09f85670dca3b699c709701c616b4c5260097114003"
        },
        "downloads": -1,
        "filename": "flexgen-0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "085e66c750990de4a02f26b673551d17",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 50361,
        "upload_time": "2023-03-01T11:18:27",
        "upload_time_iso_8601": "2023-03-01T11:18:27.939753Z",
        "url": "https://files.pythonhosted.org/packages/79/37/3ac011e4331a4bdc4ed6e936b1045e8e5a2a7399ff66321c79e575bb38fb/flexgen-0.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "20aea54960a728230f73432a547b07c9369a4c5d58d4e7b93402dbd7b8ffd9b8",
        "md5": "101f6cdd67be21da709d107078e342ac",
        "sha256": "b861e343d78213faa74c904a75b4e6b2edaa8e5f3cbcb403d11ae7b23dd1027b"
      },
      "downloads": -1,
      "filename": "flexgen-0.1.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "101f6cdd67be21da709d107078e342ac",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 50757,
      "upload_time": "2023-03-01T11:18:25",
      "upload_time_iso_8601": "2023-03-01T11:18:25.895333Z",
      "url": "https://files.pythonhosted.org/packages/20/ae/a54960a728230f73432a547b07c9369a4c5d58d4e7b93402dbd7b8ffd9b8/flexgen-0.1.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "79373ac011e4331a4bdc4ed6e936b1045e8e5a2a7399ff66321c79e575bb38fb",
        "md5": "085e66c750990de4a02f26b673551d17",
        "sha256": "1989e5e4262c31681375b09f85670dca3b699c709701c616b4c5260097114003"
      },
      "downloads": -1,
      "filename": "flexgen-0.1.7.tar.gz",
      "has_sig": false,
      "md5_digest": "085e66c750990de4a02f26b673551d17",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 50361,
      "upload_time": "2023-03-01T11:18:27",
      "upload_time_iso_8601": "2023-03-01T11:18:27.939753Z",
      "url": "https://files.pythonhosted.org/packages/79/37/3ac011e4331a4bdc4ed6e936b1045e8e5a2a7399ff66321c79e575bb38fb/flexgen-0.1.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}