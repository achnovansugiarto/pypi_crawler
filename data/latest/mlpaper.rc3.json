{
  "info": {
    "author": "Ryan Turner",
    "author_email": "rdturnermtl@github.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "******************************\nThe ML Paper Package (mlpaper)\n******************************\n\nEasy benchmarking of machine learning models with sklearn interface with\nstatistical tests built-in.\n\nUsage for classification problems\n=================================\n\nFirst, we consider the ``plot_classifier_comparison.py`` demo file. This extends\nthe standard sklearn `classifier\ncomparison <https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html>`__\nbut also demos the ease of `mlpaper` to create a performance\nreport.\n\nIn this demo, we use the example of the three toy data sets and ten classifiers from the sklearn example:\n\n.. figure:: https://user-images.githubusercontent.com/28273671/88328310-17f51d80-ccdd-11ea-8993-d833cb35c524.png\n   :alt: sklearn\n\nThe `mlpaper` package can benchmark all of the of these methods and created a properly formatted LaTeX table (with error bars) in a few commands.\nThis generates a results table for copy-and-paste into a ML paper `.tex` file in a few commands.\n\nPandas tables with the performance results of all the methods can be\nbuilt by:\n\n.. code:: python\n\n    import mlpaper.classification as btc\n    from mlpaper.classification import STD_BINARY_CURVES, STD_CLASS_LOSS\n\n    performance_df, performance_curves_dict = btc.just_benchmark(\n        X_train,\n        y_train,\n        X_test,\n        y_test,\n        2,\n        classifiers,\n        STD_CLASS_LOSS,\n        STD_BINARY_CURVES,\n        ref_method,\n    )\n\nThis benchmarks all the models in classifiers on the data (``X_train``,\n``y_train``, ``X_test``, ``y_test``) for 2-class classification. It uses\nthe loss function described in the dictionaries ``STD_CLASS_LOSS``, and\nthe curves (e.g., ROC, PR) in ``STD_BINARY_CURVES``. The ``ref_method``\ndefines the model that is the reference to compare against for assessing\nstatistically significant performance gains.\n\nThe `sciprint` module formats these tables for scientific presentation.\nThe performance dictionaries can be converted to cleanly formatted\ntables: correct significant figures, shifting of exponent for\ncompactness, thresholding huge/small (crap limit) results, and correct\nalignment of decimal points, units in headers, etc. Here we use:\n\n.. code:: python\n\n    import mlpaper.sciprint as sp\n\n    print(\n        sp.just_format_it(\n            performance_df,\n            shift_mod=3,\n            unit_dict={\"NLL\": \"nats\"},\n            crap_limit_min={\"AUPRG\": -1},\n            EB_limit={\"AUPRG\": -1},\n            non_finite_fmt={sp.NAN_STR: \"N/A\"},\n            use_tex=False,\n        )\n    )\n\nto export the results in plain text, or for LaTeX we use:\n\n.. code:: python\n\n    import mlpaper.sciprint as sp\n\n    print(\n        sp.just_format_it(\n            performance_df,\n            shift_mod=3,\n            unit_dict={\"NLL\": \"nats\"},\n            crap_limit_min={\"AUPRG\": -1},\n            EB_limit={\"AUPRG\": -1},\n            non_finite_fmt={sp.NAN_STR: \"{--}\"},\n            use_tex=True,\n        )\n    )\n\nOutput\n------\n\nDataset 0 (Moons)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n                              AP        p        AUC        p    AUPRG        p      Brier        p NLL (nats)        p     sphere        p   zero one        p\n    AdaBoost           0.93(16)   <0.0001  0.950(96)  <0.0001  0.90464  <0.0001  0.42(14)   <0.0001  0.368(80)  <0.0001  0.36(15)   <0.0001  0.075(86)  <0.0001\n    Decision Tree      0.95(13)   <0.0001  0.966(70)  <0.0001  0.93860  <0.0001  0.18(25)   <0.0001  0.40(71)    0.4072  0.16(22)   <0.0001  0.050(71)  <0.0001\n    Gaussian Process   0.90(22)   <0.0001  0.95(12)   <0.0001  0.92081  <0.0001  0.27(17)   <0.0001  0.27(11)   <0.0001  0.22(16)   <0.0001  0.025(51)  <0.0001\n    Linear SVM         0.952(99)  <0.0001  0.950(77)  <0.0001  0.88705  <0.0001  0.34(24)   <0.0001  0.29(16)   <0.0001  0.31(24)   <0.0001  0.15(12)    0.0006\n    Naive Bayes        0.957(97)  <0.0001  0.957(68)  <0.0001  0.89782  <0.0001  0.34(25)   <0.0001  0.28(18)   <0.0001  0.31(24)   <0.0001  0.13(11)    0.0002\n    Nearest Neighbors  0.94(14)   <0.0001  0.969(69)  <0.0001  0.93498  <0.0001  0.18(21)   <0.0001  0.42(70)    0.4241  0.15(18)   <0.0001  0.025(51)  <0.0001\n    Neural Net         0.957(91)  <0.0001  0.957(69)  <0.0001  0.89782  <0.0001  0.33(23)   <0.0001  0.28(15)   <0.0001  0.30(22)   <0.0001  0.100(98)  <0.0001\n    QDA                0.951(91)  <0.0001  0.950(80)  <0.0001  0.88517  <0.0001  0.34(27)   <0.0001  0.29(21)    0.0003  0.31(25)   <0.0001  0.15(12)    0.0006\n    RBF SVM            0.93(18)   <0.0001  0.957(94)  <0.0001  0.92081  <0.0001  0.14(20)   <0.0001  0.18(18)   <0.0001  0.12(17)   <0.0001  0.025(51)  <0.0001\n    Random Forest      0.965(82)  <0.0001  0.949(84)  <0.0001  0.92147  <0.0001  0.31(26)   <0.0001  0.52(70)    0.6099  0.28(24)   <0.0001  0.100(98)  <0.0001\n    iid                0.53(16)       N/A  0.5(0)         N/A  0(0)         N/A  1.004(22)      N/A  0.695(11)      N/A  1.005(27)      N/A  0.53(17)       N/A\n\nDataset 0 (Moons) in LaTeX\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    \\begin{tabular}{|l|Sr|Sr|Sr|Sr|Sr|Sr|Sr|}\n    \\toprule\n    {}                &       {AP} &      {p} &      {AUC} &      {p} &  {AUPRG} &      {p} &    {Brier} &      {p} & {NLL (nats)} &      {p} &   {sphere} &      {p} & {zero one} &      {p} \\\\\n    \\midrule\n    AdaBoost          &  0.93(16)  &  <0.0001 &  0.950(96) &  <0.0001 &  0.90464 &  <0.0001 &  0.42(14)  &  <0.0001 &    0.368(80) &  <0.0001 &  0.36(15)  &  <0.0001 &  0.075(86) &  <0.0001 \\\\\n    Decision Tree     &  0.95(13)  &  <0.0001 &  0.966(70) &  <0.0001 &  0.93860 &  <0.0001 &  0.18(25)  &  <0.0001 &    0.40(71)  &   0.4072 &  0.16(22)  &  <0.0001 &  0.050(71) &  <0.0001 \\\\\n    Gaussian Process  &  0.90(22)  &  <0.0001 &  0.95(12)  &  <0.0001 &  0.92081 &  <0.0001 &  0.27(17)  &  <0.0001 &    0.27(11)  &  <0.0001 &  0.22(16)  &  <0.0001 &  0.025(51) &  <0.0001 \\\\\n    Linear SVM        &  0.952(99) &  <0.0001 &  0.950(77) &  <0.0001 &  0.88705 &  <0.0001 &  0.34(24)  &  <0.0001 &    0.29(16)  &  <0.0001 &  0.31(24)  &  <0.0001 &  0.15(12)  &   0.0006 \\\\\n    Naive Bayes       &  0.957(97) &  <0.0001 &  0.957(68) &  <0.0001 &  0.89782 &  <0.0001 &  0.34(25)  &  <0.0001 &    0.28(18)  &  <0.0001 &  0.31(24)  &  <0.0001 &  0.13(11)  &   0.0002 \\\\\n    Nearest Neighbors &  0.94(14)  &  <0.0001 &  0.969(69) &  <0.0001 &  0.93498 &  <0.0001 &  0.18(21)  &  <0.0001 &    0.42(70)  &   0.4241 &  0.15(18)  &  <0.0001 &  0.025(51) &  <0.0001 \\\\\n    Neural Net        &  0.957(91) &  <0.0001 &  0.957(69) &  <0.0001 &  0.89782 &  <0.0001 &  0.33(23)  &  <0.0001 &    0.28(15)  &  <0.0001 &  0.30(22)  &  <0.0001 &  0.100(98) &  <0.0001 \\\\\n    QDA               &  0.951(91) &  <0.0001 &  0.950(80) &  <0.0001 &  0.88517 &  <0.0001 &  0.34(27)  &  <0.0001 &    0.29(21)  &   0.0003 &  0.31(25)  &  <0.0001 &  0.15(12)  &   0.0006 \\\\\n    RBF SVM           &  0.93(18)  &  <0.0001 &  0.957(94) &  <0.0001 &  0.92081 &  <0.0001 &  0.14(20)  &  <0.0001 &    0.18(18)  &  <0.0001 &  0.12(17)  &  <0.0001 &  0.025(51) &  <0.0001 \\\\\n    Random Forest     &  0.965(82) &  <0.0001 &  0.949(84) &  <0.0001 &  0.92147 &  <0.0001 &  0.31(26)  &  <0.0001 &    0.52(70)  &   0.6099 &  0.28(24)  &  <0.0001 &  0.100(98) &  <0.0001 \\\\\n    iid               &  0.53(16)  &     {--} &  0.5(0)    &     {--} &  0(0)    &     {--} &  1.004(22) &     {--} &    0.695(11) &     {--} &  1.005(27) &     {--} &  0.53(17)  &     {--} \\\\\n    \\bottomrule\n    \\end{tabular}\n\nDataset 1 (Circles)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n                               AP        p        AUC        p      AUPRG        p      Brier        p NLL (nats)        p     sphere        p   zero one        p\n    AdaBoost           0.938(82)   <0.0001  0.89(12)   <0.0001  0.76091    <0.0001  0.773(96)  <0.0001  0.576(50)  <0.0001  0.73(12)   <0.0001  0.17(13)   <0.0001\n    Decision Tree      0.86(16)    <0.0001  0.80(13)   <0.0001  0.76316    <0.0001  0.80(52)    0.3009  2.8(18)     0.0270  0.68(45)    0.0792  0.20(13)    0.0003\n    Gaussian Process   0.977(47)   <0.0001  0.964(60)  <0.0001  0.93049    <0.0001  0.39(23)   <0.0001  0.33(14)   <0.0001  0.36(23)   <0.0001  0.100(98)  <0.0001\n    Linear SVM         0.53(18)     0.1621  0.51(21)    0.8580  0.19756     0.3660  1.066(80)   0.1521  0.726(41)   0.1514  1.079(96)   0.1531  0.60(16)    1.0000\n    Naive Bayes        0.9983(82)  <0.0001  0.997(13)  <0.0001  0.996(21)  <0.0001  0.64(20)   <0.0001  0.48(12)   <0.0001  0.63(21)   <0.0001  0.30(15)    0.0003\n    Nearest Neighbors  0.996(15)   <0.0001  0.966(49)  <0.0001  0.991(47)  <0.0001  0.30(16)   <0.0001  0.23(11)   <0.0001  0.28(16)   <0.0001  0.075(86)  <0.0001\n    Neural Net         0.993(23)   <0.0001  0.990(32)  <0.0001  0.982(79)  <0.0001  0.69(14)   <0.0001  0.525(74)  <0.0001  0.65(16)   <0.0001  0.25(15)   <0.0001\n    QDA                0.9983(83)  <0.0001  0.997(11)  <0.0001  0.996(32)  <0.0001  0.63(19)   <0.0001  0.47(11)   <0.0001  0.61(20)   <0.0001  0.28(15)   <0.0001\n    RBF SVM            0.979(44)   <0.0001  0.966(63)  <0.0001  0.93680    <0.0001  0.34(22)   <0.0001  0.29(14)   <0.0001  0.31(22)   <0.0001  0.100(98)  <0.0001\n    Random Forest      0.90(13)    <0.0001  0.85(16)   <0.0001  0.64512     0.0021  0.65(30)    0.0070  0.48(19)    0.0094  0.62(31)    0.0047  0.23(14)    0.0006\n    iid                0.60(16)        N/A  0.5(0)         N/A  0(0)           N/A  1.071(85)      N/A  0.729(43)      N/A  1.08(11)       N/A  0.60(16)       N/A\n\nDataset 1 (Circles) in LaTeX\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    \\begin{tabular}{|l|Sr|Sr|Sr|Sr|Sr|Sr|Sr|}\n    \\toprule\n    {}                &        {AP} &      {p} &      {AUC} &      {p} &    {AUPRG} &      {p} &    {Brier} &      {p} & {NLL (nats)} &      {p} &   {sphere} &      {p} & {zero one} &      {p} \\\\\n    \\midrule\n    AdaBoost          &  0.938(82)  &  <0.0001 &  0.89(12)  &  <0.0001 &  0.76091   &  <0.0001 &  0.773(96) &  <0.0001 &    0.576(50) &  <0.0001 &  0.73(12)  &  <0.0001 &  0.17(13)  &  <0.0001 \\\\\n    Decision Tree     &  0.86(16)   &  <0.0001 &  0.80(13)  &  <0.0001 &  0.76316   &  <0.0001 &  0.80(52)  &   0.3009 &    2.8(18)   &   0.0270 &  0.68(45)  &   0.0792 &  0.20(13)  &   0.0003 \\\\\n    Gaussian Process  &  0.977(47)  &  <0.0001 &  0.964(60) &  <0.0001 &  0.93049   &  <0.0001 &  0.39(23)  &  <0.0001 &    0.33(14)  &  <0.0001 &  0.36(23)  &  <0.0001 &  0.100(98) &  <0.0001 \\\\\n    Linear SVM        &  0.53(18)   &   0.1621 &  0.51(21)  &   0.8580 &  0.19756   &   0.3660 &  1.066(80) &   0.1521 &    0.726(41) &   0.1514 &  1.079(96) &   0.1531 &  0.60(16)  &   1.0000 \\\\\n    Naive Bayes       &  0.9983(82) &  <0.0001 &  0.997(13) &  <0.0001 &  0.996(21) &  <0.0001 &  0.64(20)  &  <0.0001 &    0.48(12)  &  <0.0001 &  0.63(21)  &  <0.0001 &  0.30(15)  &   0.0003 \\\\\n    Nearest Neighbors &  0.996(15)  &  <0.0001 &  0.966(49) &  <0.0001 &  0.991(47) &  <0.0001 &  0.30(16)  &  <0.0001 &    0.23(11)  &  <0.0001 &  0.28(16)  &  <0.0001 &  0.075(86) &  <0.0001 \\\\\n    Neural Net        &  0.993(23)  &  <0.0001 &  0.990(32) &  <0.0001 &  0.982(79) &  <0.0001 &  0.69(14)  &  <0.0001 &    0.525(74) &  <0.0001 &  0.65(16)  &  <0.0001 &  0.25(15)  &  <0.0001 \\\\\n    QDA               &  0.9983(83) &  <0.0001 &  0.997(11) &  <0.0001 &  0.996(32) &  <0.0001 &  0.63(19)  &  <0.0001 &    0.47(11)  &  <0.0001 &  0.61(20)  &  <0.0001 &  0.28(15)  &  <0.0001 \\\\\n    RBF SVM           &  0.979(44)  &  <0.0001 &  0.966(63) &  <0.0001 &  0.93680   &  <0.0001 &  0.34(22)  &  <0.0001 &    0.29(14)  &  <0.0001 &  0.31(22)  &  <0.0001 &  0.100(98) &  <0.0001 \\\\\n    Random Forest     &  0.90(13)   &  <0.0001 &  0.85(16)  &  <0.0001 &  0.64512   &   0.0021 &  0.65(30)  &   0.0070 &    0.48(19)  &   0.0094 &  0.62(31)  &   0.0047 &  0.23(14)  &   0.0006 \\\\\n    iid               &  0.60(16)   &     {--} &  0.5(0)    &     {--} &  0(0)      &     {--} &  1.071(85) &     {--} &    0.729(43) &     {--} &  1.08(11)  &     {--} &  0.60(16)  &     {--} \\\\\n    \\bottomrule\n    \\end{tabular}\n\nDataset 2 (Linear)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n                              AP        p        AUC        p      AUPRG        p      Brier        p NLL (nats)        p     sphere        p   zero one        p\n    AdaBoost           0.984(43)  <0.0001  0.962(87)  <0.0001  0.96274    <0.0001  0.21(23)   <0.0001  0.27(29)    0.0034  0.18(20)   <0.0001  0.050(71)  <0.0001\n    Decision Tree      0.91(14)   <0.0001  0.922(98)  <0.0001  0.88360    <0.0001  0.30(35)    0.0002  1.0(12)     0.5706  0.26(30)   <0.0001  0.075(86)  <0.0001\n    Gaussian Process   0.984(38)  <0.0001  0.977(52)  <0.0001  0.96794    <0.0001  0.25(24)   <0.0001  0.23(17)   <0.0001  0.23(23)   <0.0001  0.075(86)  <0.0001\n    Linear SVM         0.994(26)  <0.0001  0.992(23)  <0.0001  0.989(47)  <0.0001  0.17(14)   <0.0001  0.163(86)  <0.0001  0.16(15)   <0.0001  0.050(71)  <0.0001\n    Naive Bayes        0.992(25)  <0.0001  0.990(32)  <0.0001  0.986(50)  <0.0001  0.18(20)   <0.0001  0.15(15)   <0.0001  0.17(19)   <0.0001  0.050(71)  <0.0001\n    Nearest Neighbors  0.992(25)  <0.0001  0.946(78)  <0.0001  0.985(67)  <0.0001  0.29(30)   <0.0001  0.76(98)    0.9063  0.25(26)   <0.0001  0.075(86)  <0.0001\n    Neural Net         0.987(35)  <0.0001  0.982(40)  <0.0001  0.975(83)  <0.0001  0.24(19)   <0.0001  0.22(12)   <0.0001  0.21(19)   <0.0001  0.050(71)  <0.0001\n    QDA                0.984(42)  <0.0001  0.975(57)  <0.0001  0.96560    <0.0001  0.21(24)   <0.0001  0.23(28)    0.0014  0.19(22)   <0.0001  0.075(86)  <0.0001\n    RBF SVM            0.980(45)  <0.0001  0.970(62)  <0.0001  0.95778    <0.0001  0.21(25)   <0.0001  0.20(21)   <0.0001  0.18(23)   <0.0001  0.050(71)  <0.0001\n    Random Forest      0.990(25)  <0.0001  0.968(58)  <0.0001  0.981(73)  <0.0001  0.25(25)   <0.0001  0.47(70)    0.5055  0.23(23)   <0.0001  0.075(86)  <0.0001\n    iid                0.55(16)       N/A  0.5(0)         N/A  0(0)           N/A  1.018(43)      N/A  0.702(22)      N/A  1.021(52)      N/A  0.55(17)       N/A\n\nDataset 2 (Linear) in LaTeX\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n::\n\n    \\begin{tabular}{|l|Sr|Sr|Sr|Sr|Sr|Sr|Sr|}\n    \\toprule\n    {}                &       {AP} &      {p} &      {AUC} &      {p} &    {AUPRG} &      {p} &    {Brier} &      {p} & {NLL (nats)} &      {p} &   {sphere} &      {p} & {zero one} &      {p} \\\\\n    \\midrule\n    AdaBoost          &  0.984(43) &  <0.0001 &  0.962(87) &  <0.0001 &  0.96274   &  <0.0001 &  0.21(23)  &  <0.0001 &    0.27(29)  &   0.0034 &  0.18(20)  &  <0.0001 &  0.050(71) &  <0.0001 \\\\\n    Decision Tree     &  0.91(14)  &  <0.0001 &  0.922(98) &  <0.0001 &  0.88360   &  <0.0001 &  0.30(35)  &   0.0002 &    1.0(12)   &   0.5706 &  0.26(30)  &  <0.0001 &  0.075(86) &  <0.0001 \\\\\n    Gaussian Process  &  0.984(38) &  <0.0001 &  0.977(52) &  <0.0001 &  0.96794   &  <0.0001 &  0.25(24)  &  <0.0001 &    0.23(17)  &  <0.0001 &  0.23(23)  &  <0.0001 &  0.075(86) &  <0.0001 \\\\\n    Linear SVM        &  0.994(26) &  <0.0001 &  0.992(23) &  <0.0001 &  0.989(47) &  <0.0001 &  0.17(14)  &  <0.0001 &    0.163(86) &  <0.0001 &  0.16(15)  &  <0.0001 &  0.050(71) &  <0.0001 \\\\\n    Naive Bayes       &  0.992(25) &  <0.0001 &  0.990(32) &  <0.0001 &  0.986(50) &  <0.0001 &  0.18(20)  &  <0.0001 &    0.15(15)  &  <0.0001 &  0.17(19)  &  <0.0001 &  0.050(71) &  <0.0001 \\\\\n    Nearest Neighbors &  0.992(25) &  <0.0001 &  0.946(78) &  <0.0001 &  0.985(67) &  <0.0001 &  0.29(30)  &  <0.0001 &    0.76(98)  &   0.9063 &  0.25(26)  &  <0.0001 &  0.075(86) &  <0.0001 \\\\\n    Neural Net        &  0.987(35) &  <0.0001 &  0.982(40) &  <0.0001 &  0.975(83) &  <0.0001 &  0.24(19)  &  <0.0001 &    0.22(12)  &  <0.0001 &  0.21(19)  &  <0.0001 &  0.050(71) &  <0.0001 \\\\\n    QDA               &  0.984(42) &  <0.0001 &  0.975(57) &  <0.0001 &  0.96560   &  <0.0001 &  0.21(24)  &  <0.0001 &    0.23(28)  &   0.0014 &  0.19(22)  &  <0.0001 &  0.075(86) &  <0.0001 \\\\\n    RBF SVM           &  0.980(45) &  <0.0001 &  0.970(62) &  <0.0001 &  0.95778   &  <0.0001 &  0.21(25)  &  <0.0001 &    0.20(21)  &  <0.0001 &  0.18(23)  &  <0.0001 &  0.050(71) &  <0.0001 \\\\\n    Random Forest     &  0.990(25) &  <0.0001 &  0.968(58) &  <0.0001 &  0.981(73) &  <0.0001 &  0.25(25)  &  <0.0001 &    0.47(70)  &   0.5055 &  0.23(23)  &  <0.0001 &  0.075(86) &  <0.0001 \\\\\n    iid               &  0.55(16)  &     {--} &  0.5(0)    &     {--} &  0(0)      &     {--} &  1.018(43) &     {--} &    0.702(22) &     {--} &  1.021(52) &     {--} &  0.55(17)  &     {--} \\\\\n    \\bottomrule\n    \\end{tabular}\n\nROC curves\n\"\"\"\"\"\"\"\"\"\"\n\nThe `just_benchmark` routines also produces ROC curves with error bars from bootstrap analysis, which have been vectorized for speed:\n\n.. figure:: https://user-images.githubusercontent.com/28273671/88328302-13306980-ccdd-11ea-8862-2fd3e92239b3.png\n   :alt: ROC\n\nPrecision-recall curves\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n.. figure:: https://user-images.githubusercontent.com/28273671/88328286-0f9ce280-ccdd-11ea-815e-f3f0ce86d669.png\n   :alt: PR\n\nPrecision-recall-gain curves\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n.. figure:: https://user-images.githubusercontent.com/28273671/88328305-1592c380-ccdd-11ea-8906-79142178322f.png\n   :alt: PRG\n\nUsage for regression problems\n=============================\n\nThe `mlpaper` package can also be applied to a regression problem with:\n\n.. code:: python\n\n    import mlpaper.regression as btr\n\n    full_tbl = btr.just_benchmark(X_train, y_train, X_test, y_test, regressors, STD_REGR_LOSS, \"iid\", pairwise_CI=True)\n\nHere we have used ``pairwise_CI=True`` which makes the confidence\nintervals based on the uncertainty of the loss *difference* to the\nreference method rather than a confidence interval on the actual loss.\n\nOutput\n------\n\nBy extending the sklearn `regression\ndemo <https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_compare_gpr_krr.html#sphx-glr-auto-examples-gaussian-process-plot-compare-gpr-krr-py>`__\nwe can make simple formatted tables:\n\n::\n\n                 MAE       p          MSE        p   NLL (nats)        p\n    BLR  0.96933(30)  0.0979  1.39881(67)   0.0665  1.58842(57)   0.9828\n    GPR  0.75(13)     0.0009  0.75(28)     <0.0001  1.27(12)     <0.0001\n    iid  0.96908         N/A  1.3982           N/A  1.5884           N/A\n\nor in LaTeX:\n\n::\n\n    \\begin{tabular}{|l|Sr|Sr|Sr|}\n    \\toprule\n    {}  &        {MAE} &     {p} &        {MSE} &      {p} & {NLL (nats)} &      {p} \\\\\n    \\midrule\n    BLR &  0.96933(30) &  0.0979 &  1.39881(67) &   0.0665 &  1.58842(57) &   0.9828 \\\\\n    GPR &  0.75(13)    &  0.0009 &  0.75(28)    &  <0.0001 &  1.27(12)    &  <0.0001 \\\\\n    iid &  0.96908     &     N/A &  1.3982      &      N/A &  1.5884      &      N/A \\\\\n    \\bottomrule\n    \\end{tabular}\n\n.. figure:: https://user-images.githubusercontent.com/28273671/88328364-2c391a80-ccdd-11ea-8367-2e53427c184d.png\n   :alt: regression demo\n\nInstallation\n============\n\nOnly ``Python>=3.5`` is officially supported, but older versions of Python likely work as well.\n\nThe core package itself can be installed with:\n\n.. code-block:: bash\n\n   pip install mlpaper\n\nTo also get the dependencies for the demos in the README install with\n\n.. code-block:: bash\n\n   pip install mlpaper[demo]\n\nContributing\n============\n\nThe following instructions have been tested with Python 3.7.4 on Mac OS (10.14.6).\n\nInstall in editable mode\n------------------------\n\nFirst, define the variables for the paths we will use:\n\n.. code-block:: bash\n\n   GIT=/path/to/where/you/put/repos\n   ENVS=/path/to/where/you/put/virtualenvs\n\nThen clone the repo in your git directory ``$GIT``:\n\n.. code-block:: bash\n\n   cd $GIT\n   git clone https://github.com/rdturnermtl/mlpaper.git\n\nInside your virtual environments folder ``$ENVS``, make the environment:\n\n.. code-block:: bash\n\n   cd $ENVS\n   virtualenv mlpaper --python=python3.7\n   source $ENVS/mlpaper/bin/activate\n\nNow we can install the pip dependencies. Move back into your git directory and run\n\n.. code-block:: bash\n\n   cd $GIT/mlpaper\n   pip install -r requirements/base.txt\n   pip install -e .  # Install the package itself\n\nContributor tools\n-----------------\n\nFirst, we need to setup some needed tools:\n\n.. code-block:: bash\n\n   cd $ENVS\n   virtualenv mlpaper_tools --python=python3.7\n   source $ENVS/mlpaper_tools/bin/activate\n   pip install -r $GIT/mlpaper/requirements/tools.txt\n\nTo install the pre-commit hooks for contributing run (in the ``mlpaper_tools`` environment):\n\n.. code-block:: bash\n\n   cd $GIT/mlpaper\n   pre-commit install\n\nTo rebuild the requirements, we can run:\n\n.. code-block:: bash\n\n   cd $GIT/mlpaper\n\n   # Check if there any discrepancies in the .in files\n   pipreqs mlpaper/ --diff requirements/base.in\n   pipreqs tests/ --diff requirements/test.in\n   pipreqs demos/ --diff requirements/demo.in\n   pipreqs docs/ --diff requirements/docs.in\n\n   # Regenerate the .txt files from .in files\n   pip-compile-multi --no-upgrade\n\nGenerating the documentation\n----------------------------\n\nFirst setup the environment for building with ``Sphinx``:\n\n.. code-block:: bash\n\n   cd $ENVS\n   virtualenv mlpaper_docs --python=python3.7\n   source $ENVS/mlpaper_docs/bin/activate\n   pip install -r $GIT/mlpaper/requirements/docs.txt\n\nThen we can do the build:\n\n.. code-block:: bash\n\n   cd $GIT/mlpaper/docs\n   make all\n   open _build/html/index.html\n\nDocumentation will be available in all formats in ``Makefile``. Use ``make html`` to only generate the HTML documentation.\n\nRunning the tests\n-----------------\n\nThe tests for this package can be run with:\n\n.. code-block:: bash\n\n   cd $GIT/mlpaper\n   ./local_test.sh\n\nThe script creates an environment using the requirements found in ``requirements/test.txt``.\nA code coverage report will also be produced in ``$GIT/mlpaper/htmlcov/index.html``.\n\nDeployment\n----------\n\nThe wheel (tar ball) for deployment as a pip installable package can be built using the script:\n\n.. code-block:: bash\n\n   cd $GIT/mlpaper/\n   ./build_wheel.sh\n\nLinks\n=====\n\nThe `source <https://github.com/rdturnermtl/mlpaper/>`_ is hosted on GitHub.\n\nThe `documentation <https://mlpaper.readthedocs.io/en/latest/>`_ is hosted at Read the Docs.\n\nInstallable from `PyPI <https://pypi.org/project/mlpaper/>`_.\n\nLicense\n=======\n\nThis project is licensed under the Apache 2 License - see the LICENSE file for details.",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/rdturnermtl/mlpaper/",
    "keywords": "",
    "license": "Apache v2",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlpaper",
    "package_url": "https://pypi.org/project/mlpaper/",
    "platform": "any",
    "project_url": "https://pypi.org/project/mlpaper/",
    "project_urls": {
      "Homepage": "https://github.com/rdturnermtl/mlpaper/"
    },
    "release_url": "https://pypi.org/project/mlpaper/0.0.3/",
    "requires_dist": null,
    "requires_python": ">=3.5",
    "summary": "Easy benchmarking of machine learning models with sklearn interface with statistical tests built-in.",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7775558,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "43608231cf4daa94713e447a32878fbe7c8e7795cd879389070f67bc1a84708b",
          "md5": "40f6ef311540496b41d2fface9c464c3",
          "sha256": "e3df422e99ca63aa1bfb44b815ccce5e3b3be8800a03216d25a66c4264b5d513"
        },
        "downloads": -1,
        "filename": "mlpaper-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "40f6ef311540496b41d2fface9c464c3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 49526,
        "upload_time": "2020-07-23T02:10:51",
        "upload_time_iso_8601": "2020-07-23T02:10:51.905977Z",
        "url": "https://files.pythonhosted.org/packages/43/60/8231cf4daa94713e447a32878fbe7c8e7795cd879389070f67bc1a84708b/mlpaper-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a5b49e526676d02e03351492f0e249420cb01b0b826e74e0bfd698e53ba23b31",
          "md5": "98e1b1656bcc1ecf9544067ddab0c19c",
          "sha256": "0b84f571ddd75210272dae5763d5232188f2122541b91bb2b7048f5868c6ed2a"
        },
        "downloads": -1,
        "filename": "mlpaper-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "98e1b1656bcc1ecf9544067ddab0c19c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 49517,
        "upload_time": "2020-07-23T03:38:33",
        "upload_time_iso_8601": "2020-07-23T03:38:33.406790Z",
        "url": "https://files.pythonhosted.org/packages/a5/b4/9e526676d02e03351492f0e249420cb01b0b826e74e0bfd698e53ba23b31/mlpaper-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0dcbec9bea40e20dfbd335b27f11e55bc291e7f6d6dd3c477cdf2e813b698122",
          "md5": "cab6b337209373d2f47b1492c74187e0",
          "sha256": "525add6a45c5979fbfe26172838590e66eedf6e80d8d1438102f3649e210320a"
        },
        "downloads": -1,
        "filename": "mlpaper-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "cab6b337209373d2f47b1492c74187e0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 53340,
        "upload_time": "2020-07-24T04:42:14",
        "upload_time_iso_8601": "2020-07-24T04:42:14.363627Z",
        "url": "https://files.pythonhosted.org/packages/0d/cb/ec9bea40e20dfbd335b27f11e55bc291e7f6d6dd3c477cdf2e813b698122/mlpaper-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0dcbec9bea40e20dfbd335b27f11e55bc291e7f6d6dd3c477cdf2e813b698122",
        "md5": "cab6b337209373d2f47b1492c74187e0",
        "sha256": "525add6a45c5979fbfe26172838590e66eedf6e80d8d1438102f3649e210320a"
      },
      "downloads": -1,
      "filename": "mlpaper-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "cab6b337209373d2f47b1492c74187e0",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 53340,
      "upload_time": "2020-07-24T04:42:14",
      "upload_time_iso_8601": "2020-07-24T04:42:14.363627Z",
      "url": "https://files.pythonhosted.org/packages/0d/cb/ec9bea40e20dfbd335b27f11e55bc291e7f6d6dd3c477cdf2e813b698122/mlpaper-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}