{
  "info": {
    "author": "salesforce",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "## BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n\n## Announcement: BLIP is now officially integrated into [LAVIS](https://github.com/salesforce/LAVIS) - a one-stop library for language-and-vision research and applications!\n\n<img src=\"BLIP.gif\" width=\"700\">\n\nThis is the PyTorch code of the <a href=\"https://arxiv.org/abs/2201.12086\">BLIP paper</a> [[blog](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/)]. The code has been tested on PyTorch 1.10.\nTo install the dependencies, run <pre/>pip install -r requirements.txt</pre> \n\nCatalog:\n- [x] Inference demo\n- [x] Pre-trained and finetuned checkpoints\n- [x] Finetuning code for Image-Text Retrieval, Image Captioning, VQA, and NLVR2\n- [x] Pre-training code\n- [x] Zero-shot video-text retrieval\n- [x] Download of bootstrapped pre-training datasets \n\n\n### Inference demo:\nRun our interactive demo using [Colab notebook](https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb) (no GPU needed).\nThe demo includes code for: \n1. Image captioning\n2. Open-ended visual question answering\n3. Multimodal / unimodal feature extraction\n4. Image-text matching\n\nTry out the [Web demo](https://huggingface.co/spaces/Salesforce/BLIP), integrated into [Huggingface Spaces ðŸ¤—](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). \n\nReplicate web demo and Docker image is also available at [![Replicate](https://replicate.com/salesforce/blip/badge)](https://replicate.com/salesforce/blip)\n\n### Pre-trained checkpoints:\nNum. pre-train images | BLIP w/ ViT-B | BLIP w/ ViT-B and CapFilt-L | BLIP w/ ViT-L \n--- | :---: | :---: | :---: \n14M | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_14M.pth\">Download</a>| - | -\n129M | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\">Download</a>| <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\">Download</a> | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth\">Download</a>\n\n### Finetuned checkpoints:\nTask | BLIP w/ ViT-B | BLIP w/ ViT-B and CapFilt-L | BLIP w/ ViT-L \n--- | :---: | :---: | :---:\nImage-Text Retrieval (COCO) | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\">Download</a>| - | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth\">Download</a>\nImage-Text Retrieval (Flickr30k) | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_flickr.pth\">Download</a>|  - | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_flickr.pth\">Download</a>\nImage Captioning (COCO) | - | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth\">Download</a>| <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\">Download</a> | \nVQA | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_vqa.pth\">Download</a>| <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\">Download</a> | - \nNLVR2 | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_nlvr.pth\">Download</a>| - | - \n\n\n### Image-Text Retrieval:\n1. Download COCO and Flickr30k datasets from the original websites, and set 'image_root' in configs/retrieval_{dataset}.yaml accordingly.\n2. To evaluate the finetuned BLIP model on COCO, run:\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_retrieval.py \\\n--config ./configs/retrieval_coco.yaml \\\n--output_dir output/retrieval_coco \\\n--evaluate</pre> \n3. To finetune the pre-trained checkpoint using 8 A100 GPUs, first set 'pretrained' in configs/retrieval_coco.yaml as \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\". Then run:\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_retrieval.py \\\n--config ./configs/retrieval_coco.yaml \\\n--output_dir output/retrieval_coco </pre> \n\n### Image-Text Captioning:\n1. Download COCO and NoCaps datasets from the original websites, and set 'image_root' in configs/caption_coco.yaml and configs/nocaps.yaml accordingly.\n2. To evaluate the finetuned BLIP model on COCO, run:\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_caption.py --evaluate</pre> \n3. To evaluate the finetuned BLIP model on NoCaps, generate results with: (evaluation needs to be performed on official server)\n<pre>python -m torch.distributed.run --nproc_per_node=8 eval_nocaps.py </pre> \n4. To finetune the pre-trained checkpoint using 8 A100 GPUs, first set 'pretrained' in configs/caption_coco.yaml as \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\". Then run:\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_caption.py </pre> \n\n### VQA:\n1. Download VQA v2 dataset and Visual Genome dataset from the original websites, and set 'vqa_root' and 'vg_root' in configs/vqa.yaml.\n2. To evaluate the finetuned BLIP model, generate results with: (evaluation needs to be performed on official server)\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_vqa.py --evaluate</pre> \n3. To finetune the pre-trained checkpoint using 16 A100 GPUs, first set 'pretrained' in configs/vqa.yaml as \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\". Then run:\n<pre>python -m torch.distributed.run --nproc_per_node=16 train_vqa.py </pre> \n\n### NLVR2:\n1. Download NLVR2 dataset from the original websites, and set 'image_root' in configs/nlvr.yaml.\n2. To evaluate the finetuned BLIP model, run\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_nlvr.py --evaluate</pre> \n3. To finetune the pre-trained checkpoint using 16 A100 GPUs, first set 'pretrained' in configs/nlvr.yaml as \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\". Then run:\n<pre>python -m torch.distributed.run --nproc_per_node=16 train_nlvr.py </pre> \n\n### Finetune with ViT-L:\nIn order to finetune a model with ViT-L, simply change the config file to set 'vit' as large. Batch size and learning rate may also need to be adjusted accordingly (please see the paper's appendix for hyper-parameter details). <a href=\"https://github.com/facebookresearch/fairscale\">Gradient checkpoint</a> can also be activated in the config file to reduce GPU memory usage. \n\n### Pre-train:\n1. Prepare training json files where each json file contains a list. Each item in the list is a dictonary with two key-value pairs: {'image': path_of_image, 'caption': text_of_image}. \n2. In configs/pretrain.yaml, set 'train_file' as the paths for the json files .\n3. Pre-train the model using 8 A100 GPUs:\n<pre>python -m torch.distributed.run --nproc_per_node=8 pretrain.py --config ./configs/Pretrain.yaml --output_dir output/Pretrain </pre> \n\n### Zero-shot video-text retrieval:\n1. Download MSRVTT dataset following the instructions from https://github.com/salesforce/ALPRO, and set 'video_root' accordingly in configs/retrieval_msrvtt.yaml.\n2. Install [decord](https://github.com/dmlc/decord) with <pre>pip install decord</pre> \n3. To perform zero-shot evaluation, run\n<pre>python -m torch.distributed.run --nproc_per_node=8 eval_retrieval_video.py</pre> \n\n### Pre-training datasets download:\nWe provide bootstrapped pre-training datasets as json files. Each json file contains a list. Each item in the list is a dictonary with two key-value pairs: {'url': url_of_image, 'caption': text_of_image}. \n\nImage source | Filtered web caption | Filtered synthetic caption by ViT-B | Filtered synthetic caption by ViT-L\n--- | :---: | :---: | :---:\nCC3M+CC12M+SBU |  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_filtered.json\">Download</a>|  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered.json\">Download</a>|  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered_large.json\">Download</a>\nLAION115M | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_filtered.json\">Download</a>|  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered.json\">Download</a>|  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered_large.json\">Download</a>\n\n### Citation\nIf you find this code to be useful for your research, please consider citing.\n<pre>\n@inproceedings{li2022blip,\n      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, \n      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},\n      year={2022},\n      booktitle={ICML},\n}</pre>\n\n### Acknowledgement\nThe implementation of BLIP relies on resources from <a href=\"https://github.com/salesforce/ALBEF\">ALBEF</a>, <a href=\"https://github.com/huggingface/transformers\">Huggingface Transformers</a>, and <a href=\"https://github.com/rwightman/pytorch-image-models/tree/master/timm\">timm</a>. We thank the original authors for their open-sourcing.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/pharmapsychotic/BLIP",
    "keywords": "",
    "license": "BSD-3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "blip-ci",
    "package_url": "https://pypi.org/project/blip-ci/",
    "platform": null,
    "project_url": "https://pypi.org/project/blip-ci/",
    "project_urls": {
      "Homepage": "https://github.com/pharmapsychotic/BLIP"
    },
    "release_url": "https://pypi.org/project/blip-ci/0.0.3/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "BLIP library for use with CLIP Interrogator",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16994663,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c78708df16a6fa5f54b539f12f215a1a41f08a39fa11a9c5c19449db46c7259a",
          "md5": "bf7541a1129971b39bc4640fa4e749f3",
          "sha256": "4a4e9d98f269ceb5e2a86644bdf17600575ba7325bd4c149db28dc10d89f0934"
        },
        "downloads": -1,
        "filename": "blip-ci-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bf7541a1129971b39bc4640fa4e749f3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 42684,
        "upload_time": "2023-02-11T21:31:13",
        "upload_time_iso_8601": "2023-02-11T21:31:13.599674Z",
        "url": "https://files.pythonhosted.org/packages/c7/87/08df16a6fa5f54b539f12f215a1a41f08a39fa11a9c5c19449db46c7259a/blip-ci-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "45940e9268e25b89a8e50869bd6783eb1fac21db73c17f44a8f1754670e63107",
          "md5": "90425474db2e7013e53d76c527a318f7",
          "sha256": "ba2d50c1bde4c6bedf13c5871e20aa430ab591f9eed5a31f557223eb067c6056"
        },
        "downloads": -1,
        "filename": "blip-ci-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "90425474db2e7013e53d76c527a318f7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 43361,
        "upload_time": "2023-02-11T21:54:25",
        "upload_time_iso_8601": "2023-02-11T21:54:25.400490Z",
        "url": "https://files.pythonhosted.org/packages/45/94/0e9268e25b89a8e50869bd6783eb1fac21db73c17f44a8f1754670e63107/blip-ci-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "14a4016b479d287b48eb8d35016f0287303f46e90da1c7cff46d8bfb6d7504e8",
          "md5": "7320374fb98af9517188f8ee88a7d260",
          "sha256": "feb6ec8291d461a134f7cb9dc61f03a66cc1128edd108c3c7e276df638587ee4"
        },
        "downloads": -1,
        "filename": "blip-ci-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "7320374fb98af9517188f8ee88a7d260",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 43368,
        "upload_time": "2023-02-18T02:10:38",
        "upload_time_iso_8601": "2023-02-18T02:10:38.597418Z",
        "url": "https://files.pythonhosted.org/packages/14/a4/016b479d287b48eb8d35016f0287303f46e90da1c7cff46d8bfb6d7504e8/blip-ci-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08df92521a45ae427d5bef26b64cb920688911e4f5709f06500766cbd4d015ff",
          "md5": "81bc960bcc633468f7f12e79264677bf",
          "sha256": "ae17575cfe9ec77c60fef90fe679f366164a6ae1a6d28d48b5b8af9166886dba"
        },
        "downloads": -1,
        "filename": "blip-ci-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "81bc960bcc633468f7f12e79264677bf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 43048,
        "upload_time": "2023-02-23T15:09:54",
        "upload_time_iso_8601": "2023-02-23T15:09:54.399663Z",
        "url": "https://files.pythonhosted.org/packages/08/df/92521a45ae427d5bef26b64cb920688911e4f5709f06500766cbd4d015ff/blip-ci-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "08df92521a45ae427d5bef26b64cb920688911e4f5709f06500766cbd4d015ff",
        "md5": "81bc960bcc633468f7f12e79264677bf",
        "sha256": "ae17575cfe9ec77c60fef90fe679f366164a6ae1a6d28d48b5b8af9166886dba"
      },
      "downloads": -1,
      "filename": "blip-ci-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "81bc960bcc633468f7f12e79264677bf",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 43048,
      "upload_time": "2023-02-23T15:09:54",
      "upload_time_iso_8601": "2023-02-23T15:09:54.399663Z",
      "url": "https://files.pythonhosted.org/packages/08/df/92521a45ae427d5bef26b64cb920688911e4f5709f06500766cbd4d015ff/blip-ci-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}