{
  "info": {
    "author": "Tao Wei",
    "author_email": "taowei@buffalo.edu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# torchtools\r\n\r\n## torchtools.utils.plot_network\r\n\r\n* Highlights:\r\n  * Show **module name, module hyperparameter (e.g. kernel size, stride, padding), output shape** info\r\n\r\n```python\r\nimport torch\r\nimport torchvision.models as models\r\nfrom torchtools.utils import plot_network\r\n\r\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\nmodel = models.resnet18().to(device)\r\ninputs = torch.randn(1, 3, 224, 224).to(device)\r\n\r\nplot_network(model, inputs).save('resnet18.gv')\r\n```\r\n\r\n[resnet18.pdf](images/resnet18.pdf)\r\n\r\n![](images/resnet18_crop.jpg)\r\n\r\n## torchtools.utils.print_summary\r\n\r\n* Highlights:\r\n  * Calculate FLOPs for **RNN, LSTM, GRU**\r\n  * Calculate FLOPs for **Attention** (in Vision Transformer)\r\n\r\n```python\r\nimport torch\r\nimport torchvision.models as models\r\nfrom torchtools.utils import print_summary\r\n\r\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\nmodel = models.resnet18().to(device)\r\ninputs = torch.randn(1, 3, 224, 224).to(device)\r\n\r\nprint_summary(model, inputs)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------------------------------------------------\r\n                            Layer (type)    Output shape     Param shape      Param #     FLOPs basic           FLOPs\r\n=====================================================================================================================\r\n                                 Input *     1x3x224x224\r\n                        conv1 (Conv2d) *    1x64x112x112        64x3x7x7        9,408     118,013,952     118,013,952\r\n                     bn1 (BatchNorm2d) *    1x64x112x112           64+64          128               0       1,605,632\r\n                           relu (ReLU) *    1x64x112x112                            0               0               0\r\n                   maxpool (MaxPool2d) *      1x64x56x56                            0               0               0\r\n               layer1.0.conv1 (Conv2d) *      1x64x56x56       64x64x3x3       36,864     115,605,504     115,605,504\r\n            layer1.0.bn1 (BatchNorm2d) *      1x64x56x56           64+64          128               0         401,408\r\n                  layer1.0.relu (ReLU) *      1x64x56x56                            0               0               0\r\n               layer1.0.conv2 (Conv2d) *      1x64x56x56       64x64x3x3       36,864     115,605,504     115,605,504\r\n            layer1.0.bn2 (BatchNorm2d) *      1x64x56x56           64+64          128               0         401,408\r\n                  layer1.0.relu (ReLU) *      1x64x56x56                            0               0               0\r\n                 layer1.0 (BasicBlock)        1x64x56x56                            0               0               0\r\n               layer1.1.conv1 (Conv2d) *      1x64x56x56       64x64x3x3       36,864     115,605,504     115,605,504\r\n            layer1.1.bn1 (BatchNorm2d) *      1x64x56x56           64+64          128               0         401,408\r\n                  layer1.1.relu (ReLU) *      1x64x56x56                            0               0               0\r\n               layer1.1.conv2 (Conv2d) *      1x64x56x56       64x64x3x3       36,864     115,605,504     115,605,504\r\n            layer1.1.bn2 (BatchNorm2d) *      1x64x56x56           64+64          128               0         401,408\r\n                  layer1.1.relu (ReLU) *      1x64x56x56                            0               0               0\r\n                 layer1.1 (BasicBlock)        1x64x56x56                            0               0               0\r\n                   layer1 (Sequential)        1x64x56x56                            0               0               0\r\n               layer2.0.conv1 (Conv2d) *     1x128x28x28      128x64x3x3       73,728      57,802,752      57,802,752\r\n            layer2.0.bn1 (BatchNorm2d) *     1x128x28x28         128+128          256               0         200,704\r\n                  layer2.0.relu (ReLU) *     1x128x28x28                            0               0               0\r\n               layer2.0.conv2 (Conv2d) *     1x128x28x28     128x128x3x3      147,456     115,605,504     115,605,504\r\n            layer2.0.bn2 (BatchNorm2d) *     1x128x28x28         128+128          256               0         200,704\r\n        layer2.0.downsample.0 (Conv2d) *     1x128x28x28      128x64x1x1        8,192       6,422,528       6,422,528\r\n   layer2.0.downsample.1 (BatchNorm2d) *     1x128x28x28         128+128          256               0         200,704\r\n      layer2.0.downsample (Sequential)       1x128x28x28                            0               0               0\r\n                  layer2.0.relu (ReLU) *     1x128x28x28                            0               0               0\r\n                 layer2.0 (BasicBlock)       1x128x28x28                            0               0               0\r\n               layer2.1.conv1 (Conv2d) *     1x128x28x28     128x128x3x3      147,456     115,605,504     115,605,504\r\n            layer2.1.bn1 (BatchNorm2d) *     1x128x28x28         128+128          256               0         200,704\r\n                  layer2.1.relu (ReLU) *     1x128x28x28                            0               0               0\r\n               layer2.1.conv2 (Conv2d) *     1x128x28x28     128x128x3x3      147,456     115,605,504     115,605,504\r\n            layer2.1.bn2 (BatchNorm2d) *     1x128x28x28         128+128          256               0         200,704\r\n                  layer2.1.relu (ReLU) *     1x128x28x28                            0               0               0\r\n                 layer2.1 (BasicBlock)       1x128x28x28                            0               0               0\r\n                   layer2 (Sequential)       1x128x28x28                            0               0               0\r\n               layer3.0.conv1 (Conv2d) *     1x256x14x14     256x128x3x3      294,912      57,802,752      57,802,752\r\n            layer3.0.bn1 (BatchNorm2d) *     1x256x14x14         256+256          512               0         100,352\r\n                  layer3.0.relu (ReLU) *     1x256x14x14                            0               0               0\r\n               layer3.0.conv2 (Conv2d) *     1x256x14x14     256x256x3x3      589,824     115,605,504     115,605,504\r\n            layer3.0.bn2 (BatchNorm2d) *     1x256x14x14         256+256          512               0         100,352\r\n        layer3.0.downsample.0 (Conv2d) *     1x256x14x14     256x128x1x1       32,768       6,422,528       6,422,528\r\n   layer3.0.downsample.1 (BatchNorm2d) *     1x256x14x14         256+256          512               0         100,352\r\n      layer3.0.downsample (Sequential)       1x256x14x14                            0               0               0\r\n                  layer3.0.relu (ReLU) *     1x256x14x14                            0               0               0\r\n                 layer3.0 (BasicBlock)       1x256x14x14                            0               0               0\r\n               layer3.1.conv1 (Conv2d) *     1x256x14x14     256x256x3x3      589,824     115,605,504     115,605,504\r\n            layer3.1.bn1 (BatchNorm2d) *     1x256x14x14         256+256          512               0         100,352\r\n                  layer3.1.relu (ReLU) *     1x256x14x14                            0               0               0\r\n               layer3.1.conv2 (Conv2d) *     1x256x14x14     256x256x3x3      589,824     115,605,504     115,605,504\r\n            layer3.1.bn2 (BatchNorm2d) *     1x256x14x14         256+256          512               0         100,352\r\n                  layer3.1.relu (ReLU) *     1x256x14x14                            0               0               0\r\n                 layer3.1 (BasicBlock)       1x256x14x14                            0               0               0\r\n                   layer3 (Sequential)       1x256x14x14                            0               0               0\r\n               layer4.0.conv1 (Conv2d) *       1x512x7x7     512x256x3x3    1,179,648      57,802,752      57,802,752\r\n            layer4.0.bn1 (BatchNorm2d) *       1x512x7x7         512+512        1,024               0          50,176\r\n                  layer4.0.relu (ReLU) *       1x512x7x7                            0               0               0\r\n               layer4.0.conv2 (Conv2d) *       1x512x7x7     512x512x3x3    2,359,296     115,605,504     115,605,504\r\n            layer4.0.bn2 (BatchNorm2d) *       1x512x7x7         512+512        1,024               0          50,176\r\n        layer4.0.downsample.0 (Conv2d) *       1x512x7x7     512x256x1x1      131,072       6,422,528       6,422,528\r\n   layer4.0.downsample.1 (BatchNorm2d) *       1x512x7x7         512+512        1,024               0          50,176\r\n      layer4.0.downsample (Sequential)         1x512x7x7                            0               0               0\r\n                  layer4.0.relu (ReLU) *       1x512x7x7                            0               0               0\r\n                 layer4.0 (BasicBlock)         1x512x7x7                            0               0               0\r\n               layer4.1.conv1 (Conv2d) *       1x512x7x7     512x512x3x3    2,359,296     115,605,504     115,605,504\r\n            layer4.1.bn1 (BatchNorm2d) *       1x512x7x7         512+512        1,024               0          50,176\r\n                  layer4.1.relu (ReLU) *       1x512x7x7                            0               0               0\r\n               layer4.1.conv2 (Conv2d) *       1x512x7x7     512x512x3x3    2,359,296     115,605,504     115,605,504\r\n            layer4.1.bn2 (BatchNorm2d) *       1x512x7x7         512+512        1,024               0          50,176\r\n                  layer4.1.relu (ReLU) *       1x512x7x7                            0               0               0\r\n                 layer4.1 (BasicBlock)         1x512x7x7                            0               0               0\r\n                   layer4 (Sequential)         1x512x7x7                            0               0               0\r\n           avgpool (AdaptiveAvgPool2d) *       1x512x1x1                            0               0               0\r\n                           fc (Linear) *          1x1000   1000x512+1000      513,000         512,000         513,000\r\n                              (ResNet)            1x1000                            0               0               0\r\n---------------------------------------------------------------------------------------------------------------------\r\nTotal params: 11,689,512 (44.591949462890625 MB)\r\nTotal params (with aux): 11,689,512 (44.591949462890625 MB)\r\n    Trainable params: 11,689,512 (44.591949462890625 MB)\r\n    Non-trainable params: 0 (0.0 MB)\r\nTotal flops (basic): 1,814,073,344 (1.814073344 billion)\r\nTotal flops: 1,819,041,768 (1.819041768 billion)\r\n---------------------------------------------------------------------------------------------------------------------\r\nNOTE:\r\n    *: leaf modules\r\n    Flops is measured in multiply-adds. Multiply, divide, exp are treated the same for calculation, add is ignored except for bias.\r\n    Flops (basic) only calculates for convolution and linear layers (not inlcude bias)\r\n    Flops additionally calculates for bias, normalization (BatchNorm, LayerNorm, GroupNorm), RNN (RNN, LSTM, GRU) and attention layers\r\n        - activations (e.g. ReLU), operations implemented as functionals (e.g. add in a residual architecture) are not \r\n          calculated as they are usually neglectable.\r\n        - complex custom module may need manual calculation for correctness (refer to RNN, LSTM, GRU, Attention as examples).\r\n---------------------------------------------------------------------------------------------------------------------\r\nOut[1]: \r\n{'flops': 1819041768,\r\n 'flops_basic': 1814073344,\r\n 'params': 11689512,\r\n 'params_with_aux': 11689512}\r\n```\r\n\r\n```python\r\nimport torch\r\nimport timm.models as models\r\nfrom torchtools.utils import print_summary\r\n\r\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\nmodel = models.vit_base_patch16_224().to(device)\r\ninputs = torch.randn(1, 3, 224, 224).to(device)\r\n\r\nprint_summary(model, inputs)\r\n```\r\n\r\n```\r\n---------------------------------------------------------------------------------------------------------------------\r\n                            Layer (type)    Output shape     Param shape      Param #     FLOPs basic           FLOPs\r\n=====================================================================================================================\r\n                                 Input *     1x3x224x224\r\n             patch_embed.proj (Conv2d) *     1x768x14x14 768x3x16x16+768      590,592     115,605,504     115,756,032\r\n           patch_embed.norm (Identity) *       1x196x768                            0               0               0\r\n              patch_embed (PatchEmbed)         1x196x768                            0               0               0\r\n                    pos_drop (Dropout) *       1x197x768                            0               0               0\r\n            blocks.0.norm1 (LayerNorm) *       1x197x768         768+768        1,536               0         302,592\r\n            blocks.0.attn.qkv (Linear) *      1x197x2304   2304x768+2304    1,771,776     348,585,984     349,039,872\r\n     blocks.0.attn.attn_drop (Dropout) *    1x12x197x197                            0               0               0\r\n           blocks.0.attn.proj (Linear) *       1x197x768     768x768+768      590,592     116,195,328     116,346,624\r\n     blocks.0.attn.proj_drop (Dropout) *       1x197x768                            0               0               0\r\n             blocks.0.attn (Attention)         1x197x768                            0               0     179,763,288\r\n         blocks.0.drop_path (Identity) *       1x197x768                            0               0               0\r\n            blocks.0.norm2 (LayerNorm) *       1x197x768         768+768        1,536               0         302,592\r\n             blocks.0.mlp.fc1 (Linear) *      1x197x3072   3072x768+3072    2,362,368     464,781,312     465,386,496\r\n               blocks.0.mlp.act (GELU) *      1x197x3072                            0               0               0\r\n          blocks.0.mlp.drop1 (Dropout) *      1x197x3072                            0               0               0\r\n             blocks.0.mlp.fc2 (Linear) *       1x197x768    768x3072+768    2,360,064     464,781,312     464,932,608\r\n          blocks.0.mlp.drop2 (Dropout) *       1x197x768                            0               0               0\r\n                    blocks.0.mlp (Mlp)         1x197x768                            0               0               0\r\n         blocks.0.drop_path (Identity) *       1x197x768                            0               0               0\r\n                      blocks.0 (Block)         1x197x768                            0               0               0\r\n  ...\r\n           blocks.11.norm1 (LayerNorm) *       1x197x768         768+768        1,536               0         302,592\r\n           blocks.11.attn.qkv (Linear) *      1x197x2304   2304x768+2304    1,771,776     348,585,984     349,039,872\r\n    blocks.11.attn.attn_drop (Dropout) *    1x12x197x197                            0               0               0\r\n          blocks.11.attn.proj (Linear) *       1x197x768     768x768+768      590,592     116,195,328     116,346,624\r\n    blocks.11.attn.proj_drop (Dropout) *       1x197x768                            0               0               0\r\n            blocks.11.attn (Attention)         1x197x768                            0               0     179,763,288\r\n        blocks.11.drop_path (Identity) *       1x197x768                            0               0               0\r\n           blocks.11.norm2 (LayerNorm) *       1x197x768         768+768        1,536               0         302,592\r\n            blocks.11.mlp.fc1 (Linear) *      1x197x3072   3072x768+3072    2,362,368     464,781,312     465,386,496\r\n              blocks.11.mlp.act (GELU) *      1x197x3072                            0               0               0\r\n         blocks.11.mlp.drop1 (Dropout) *      1x197x3072                            0               0               0\r\n            blocks.11.mlp.fc2 (Linear) *       1x197x768    768x3072+768    2,360,064     464,781,312     464,932,608\r\n         blocks.11.mlp.drop2 (Dropout) *       1x197x768                            0               0               0\r\n                   blocks.11.mlp (Mlp)         1x197x768                            0               0               0\r\n        blocks.11.drop_path (Identity) *       1x197x768                            0               0               0\r\n                     blocks.11 (Block)         1x197x768                            0               0               0\r\n                   blocks (Sequential)         1x197x768                            0               0               0\r\n                      norm (LayerNorm) *       1x197x768         768+768        1,536               0         302,592\r\n                 pre_logits (Identity) *           1x768                            0               0               0\r\n                         head (Linear) *          1x1000   1000x768+1000      769,000         768,000         769,000\r\n                   (VisionTransformer)            1x1000 1x1x768+1x197x768      152,064               0               0\r\n---------------------------------------------------------------------------------------------------------------------\r\nTotal params: 86,567,656 (330.2294006347656 MB)\r\nTotal params (with aux): 86,567,656 (330.2294006347656 MB)\r\n    Trainable params: 86,567,656 (330.2294006347656 MB)\r\n    Non-trainable params: 0 (0.0 MB)\r\nTotal flops (basic): 16,848,500,736 (16.848500736 billion)\r\nTotal flops: 19,029,716,488 (19.029716488 billion)\r\n---------------------------------------------------------------------------------------------------------------------\r\nNOTE:\r\n    *: leaf modules\r\n    Flops is measured in multiply-adds. Multiply, divide, exp are treated the same for calculation, add is ignored except for bias.\r\n    Flops (basic) only calculates for convolution and linear layers (not inlcude bias)\r\n    Flops additionally calculates for bias, normalization (BatchNorm, LayerNorm, GroupNorm), RNN (RNN, LSTM, GRU) and attention layers\r\n        - activations (e.g. ReLU), operations implemented as functionals (e.g. add in a residual architecture) are not \r\n          calculated as they are usually neglectable.\r\n        - complex custom module may need manual calculation for correctness (refer to RNN, LSTM, GRU, Attention as examples).\r\n---------------------------------------------------------------------------------------------------------------------\r\nOut[2]: \r\n{'flops': 19029716488,\r\n 'flops_basic': 16848500736,\r\n 'params': 86567656,\r\n 'params_with_aux': 86567656}\r\n```\r\n\r\n\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/taoari/torchtools",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torch-print-summary",
    "package_url": "https://pypi.org/project/torch-print-summary/",
    "platform": null,
    "project_url": "https://pypi.org/project/torch-print-summary/",
    "project_urls": {
      "Homepage": "https://github.com/taoari/torchtools"
    },
    "release_url": "https://pypi.org/project/torch-print-summary/1.0.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "PyTorch Tools",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13143230,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "599866e61b90ce521fb6032b0b4e293c3581e4a4c8dfcd35a28adbb0e19fd372",
          "md5": "01427bdf055cbdab3914ffae2bd7b50b",
          "sha256": "77615919bb243a36507472fcaf8f78020ccccf28b5267307d4e5e2a4bfc1997e"
        },
        "downloads": -1,
        "filename": "torch_print_summary-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "01427bdf055cbdab3914ffae2bd7b50b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5283,
        "upload_time": "2022-02-09T22:45:09",
        "upload_time_iso_8601": "2022-02-09T22:45:09.893261Z",
        "url": "https://files.pythonhosted.org/packages/59/98/66e61b90ce521fb6032b0b4e293c3581e4a4c8dfcd35a28adbb0e19fd372/torch_print_summary-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "56b442004bbea59a5ae4feee04684a1cacb0ed5244f5395f574dfba693887487",
          "md5": "bc8e9db4a6586f6d6b873649ee13ce70",
          "sha256": "f9d9b397c4dbb6f3e6691051c53fd63483ff1318b1a158b889b43e4dc381ea17"
        },
        "downloads": -1,
        "filename": "torch-print-summary-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bc8e9db4a6586f6d6b873649ee13ce70",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4842,
        "upload_time": "2022-02-09T22:45:11",
        "upload_time_iso_8601": "2022-02-09T22:45:11.411709Z",
        "url": "https://files.pythonhosted.org/packages/56/b4/42004bbea59a5ae4feee04684a1cacb0ed5244f5395f574dfba693887487/torch-print-summary-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e2468936e8d8b1579c7c837a96454fd66538df1654ded1d9926c0c6514ff26ec",
          "md5": "3780caad047f91da1f12c9e7f44603f0",
          "sha256": "1b7e35477386cdfa1c5a7df60e1b349e6c0873906f51bc9e6df9cdfbfaf077a1"
        },
        "downloads": -1,
        "filename": "torch-print-summary-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3780caad047f91da1f12c9e7f44603f0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 15955,
        "upload_time": "2022-03-10T22:32:29",
        "upload_time_iso_8601": "2022-03-10T22:32:29.057721Z",
        "url": "https://files.pythonhosted.org/packages/e2/46/8936e8d8b1579c7c837a96454fd66538df1654ded1d9926c0c6514ff26ec/torch-print-summary-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e2468936e8d8b1579c7c837a96454fd66538df1654ded1d9926c0c6514ff26ec",
        "md5": "3780caad047f91da1f12c9e7f44603f0",
        "sha256": "1b7e35477386cdfa1c5a7df60e1b349e6c0873906f51bc9e6df9cdfbfaf077a1"
      },
      "downloads": -1,
      "filename": "torch-print-summary-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "3780caad047f91da1f12c9e7f44603f0",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 15955,
      "upload_time": "2022-03-10T22:32:29",
      "upload_time_iso_8601": "2022-03-10T22:32:29.057721Z",
      "url": "https://files.pythonhosted.org/packages/e2/46/8936e8d8b1579c7c837a96454fd66538df1654ded1d9926c0c6514ff26ec/torch-print-summary-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}