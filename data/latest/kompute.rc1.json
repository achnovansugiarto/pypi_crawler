{
  "info": {
    "author": "Alejandro Saucedo",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "\n![GitHub](https://img.shields.io/badge/Version-0.4.1-green.svg)\n![GitHub](https://img.shields.io/badge/C++-14â€”20-purple.svg)\n![GitHub](https://img.shields.io/badge/Build-cmake-red.svg)\n![GitHub](https://img.shields.io/badge/Python-3.5â€”3.8-blue.svg)\n![GitHub](https://img.shields.io/badge/License-Apache-black.svg)\n\n<table>\n<tr>\n\n<td width=\"20%\">\n<img src=\"https://raw.githubusercontent.com/EthicalML/vulkan-kompute/master/docs/images/kompute.jpg\">\n</td>\n\n<td>\n\n<h1>Vulkan Kompute</h1>\n<h3>The General Purpose Vulkan Compute Framework for C++ and Python.</h3>\n\n</td>\n\n</tr>\n</table>\n\n<h4>Blazing fast, mobile-enabled, asynchronous, and optimized for advanced GPU processing usecases.</h4>\n\nðŸ”‹ [Documentation](https://kompute.cc) ðŸ’» [Blog Post](https://medium.com/@AxSaucedo/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a) âŒ¨ [Examples](#more-examples) ðŸ’¾\n\n\n## Principles & Features\n\n* [Single header](#setup) for simple import with flexible build-system configuration\n* Multi-language support with C++ as core SDK as well as [optimized Python bindings](#python-package)\n* [Asynchronous & parallel processing](#asynchronous-and-parallel-operations) support through GPU family queues\n* [Mobile enabled](#mobile-enabled) with examples in Android studio across several architectures\n* BYOV: [Bring-your-own-Vulkan design](#motivations) to play nice with existing Vulkan applications\n* Explicit relationships for GPU and host [memory ownership and memory management](https://kompute.cc/overview/memory-management.html)\n* [Short code examples](#simple-examples) showing the core features \n* Longer tutorials for [machine learning ðŸ¤–](https://towardsdatascience.com/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a), [mobile development ðŸ“±](https://towardsdatascience.com/gpu-accelerated-machine-learning-in-your-mobile-applications-using-the-android-ndk-vulkan-kompute-1e9da37b7617) and [game development ðŸŽ®](https://towardsdatascience.com/supercharging-game-development-with-gpu-accelerated-ml-using-vulkan-kompute-the-godot-game-engine-4e75a84ea9f0).\n\n![](https://raw.githubusercontent.com/ethicalml/vulkan-kompute/master/docs/images/komputer-2.gif)\n\n## Getting Started\n\n### Setup\n\nKompute is provided as a single header file [`Kompute.hpp`](#setup). See [build-system section](#build-overview) for configurations available.\n\n\n### Your First Kompute (Simple Version)\n\nThis simple example will show the basics of Kompute through the high level API.\n\n1. Create Kompute Manager with default settings (device 0 and first compute compatible queue)\n2. Create and initialise Kompute Tensors through manager\n3. Run multiplication operation synchronously\n4. Map results back from GPU memory to print the results\n\nView the [extended version](#your-first-kompute-extended-version) or [more examples](#simple-examples).\n\n```c++\nint main() {\n\n    // 1. Create Kompute Manager with default settings (device 0 and first compute compatible queue)\n    kp::Manager mgr; \n\n    // 2. Create and initialise Kompute Tensors through manager\n    auto tensorInA = mgr.buildTensor({ 2., 2., 2. });\n    auto tensorInB = mgr.buildTensor({ 1., 2., 3. });\n    auto tensorOut = mgr.buildTensor({ 0., 0., 0. });\n\n    // 3. Run multiplication operation synchronously\n    mgr.evalOpDefault<kp::OpMult<>>(\n        { tensorInA, tensorInB, tensorOut })\n\n    // 4. Map results back from GPU memory to print the results\n    mgr.evalOpDefault<kp::OpTensorSyncLocal>({ tensorInA, tensorInB, tensorOut })\n\n    // Prints the output which is Output: { 2, 4, 6 }\n    std::cout << fmt::format(\"Output: {}\", \n        tensorOut.data()) << std::endl;\n}\n```\n\n## Your First Kompute (Extended Version)\n\nWe will now show the [same example as above](#your-first-kompute-simple-version) but leveraging more advanced Kompute features:\n\n1. Create Kompute Manager with explicit device 0 and single queue of familyIndex 2\n2. Explicitly create Kompute Tensors without initializing in GPU\n3. Initialise the Kompute Tensor in GPU memory and map data into GPU\n4. Run operation with custom compute shader code asynchronously with explicit dispatch layout\n5. Create managed sequence to submit batch operations to the CPU\n6. Map data back to host by running the sequence of batch operations\n\nView [more examples](https://kompute.cc/overview/advanced-examples.html#simple-examples).\n\n```c++\nint main() {\n\n    // 1. Create Kompute Manager with explicit device 0 and single queue of familyIndex 2\n    kp::Manager mgr(0, { 2 });\n\n    // 2. Explicitly create Kompute Tensors without initializing in GPU\n    auto tensorInA = std::make_shared<kp::Tensor>(kp::Tensor({ 2., 2., 2. }));\n    auto tensorInB = std::make_shared<kp::Tensor>(kp::Tensor({ 1., 2., 3. }));\n    auto tensorOut = std::make_shared<kp::Tensor>(kp::Tensor({ 0., 0., 0. }));\n\n    // 3. Initialise the Kompute Tensor in GPU memory and map data into GPU\n    mgr.evalOpDefault<kp::OpTensorCreate>({ tensorInA, tensorInB, tensorOut });\n\n    // 4. Run operation with custom compute shader code asynchronously with explicit dispatch layout\n    mgr.evalOpAsyncDefault<kp::OpAlgoBase<3, 1, 1>>(\n        { tensorInA, tensorInB, tensorOut }, \n        shaderData); // \"shaderData\" defined is below and can be glsl/spirv string, or path to file\n\n    // 4.1. Before submitting sequence batch we wait for the async operation\n    mgr.evalOpAwaitDefault();\n\n    // 5. Create managed sequence to submit batch operations to the CPU\n    std::shared_ptr<kp::Sequence> sq = mgr.getOrCreateManagedSequence(\"seq\");\n\n    // 5.1. Explicitly begin recording batch commands\n    sq->begin();\n\n    // 5.2. Record batch commands\n    sq->record<kp::OpTensorSyncLocal({ tensorInA });\n    sq->record<kp::OpTensorSyncLocal({ tensorInB });\n    sq->record<kp::OpTensorSyncLocal({ tensorOut });\n\n    // 5.3. Explicitly stop recording batch commands\n    sq->end();\n\n    // 6. Map data back to host by running the sequence of batch operations\n    sq->eval();\n\n    // Prints the output which is Output: { 2, 4, 6 }\n    std::cout << fmt::format(\"Output: {}\", \n        tensorOut.data()) << std::endl;\n}\n```\n\nYour shader can be provided as raw glsl/hlsl string, SPIR-V bytes array (using our CLI), or string path to file containing either. Below are the examples of the valid ways of providing shader.\n\n#### Passing raw GLSL/HLSL string\n\n```c++\nstatic std::string shaderString = (R\"(\n    #version 450\n\n    layout (local_size_x = 1) in;\n\n    // The input tensors bind index is relative to index in parameter passed\n    layout(set = 0, binding = 0) buffer bina { float tina[]; };\n    layout(set = 0, binding = 1) buffer binb { float tinb[]; };\n    layout(set = 0, binding = 2) buffer bout { float tout[]; };\n\n    void main() {\n        uint index = gl_GlobalInvocationID.x;\n        tout[index] = tina[index] * tinb[index];\n    }\n)\");\nstatic std::vector<char> shaderData(shaderString.begin(), shaderString.end());\n```\n\n#### Passing SPIR-V Bytes array \n\nYou can use the Kompute [shader-to-cpp-header CLI](https://kompute.cc/overview/shaders-to-headers.html) to convert your GLSL/HLSL or SPIR-V shader into C++ header file (see documentation link for more info). This is useful if you want your binary to be compiled with all relevant artifacts.\n\n```c++\nstatic std::vector<uint8_t> shaderData = { 0x03, //... spirv bytes go here)\n```\n\n#### Path to file containing raw glsl/hlsl or SPIRV bytes\n\n```c++\nstatic std::string shaderData = \"path/to/shader.glsl\";\n// Or SPIR-V\nstatic std::string shaderData = \"path/to/shader.glsl.spv\";\n```\n\n## More examples\n\n### Simple examples\n\n* [Pass shader as raw string](https://kompute.cc/overview/advanced-examples.html#simple-shader-example)\n* [Record batch commands with a Kompute Sequence](https://kompute.cc/overview/advanced-examples.html#record-batch-commands)\n* [Run Asynchronous Operations](https://kompute.cc/overview/advanced-examples.html#asynchronous-operations)\n* [Run Parallel Operations Across Multiple GPU Queues](https://kompute.cc/overview/advanced-examples.html#parallel-operations)\n* [Create your custom Kompute Operations](https://kompute.cc/overview/advanced-examples.html#your-custom-kompute-operation)\n* [Implementing logistic regression from scratch](https://kompute.cc/overview/advanced-examples.html#logistic-regression-example)\n\n### End-to-end examples\n\n* [Machine Learning Logistic Regression Implementation](https://towardsdatascience.com/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a)\n* [Parallelizing GPU-intensive Workloads via Multi-Queue Operations](https://towardsdatascience.com/parallelizing-heavy-gpu-workloads-via-multi-queue-operations-50a38b15a1dc)\n* [Android NDK Mobile Kompute ML Application](https://towardsdatascience.com/gpu-accelerated-machine-learning-in-your-mobile-applications-using-the-android-ndk-vulkan-kompute-1e9da37b7617)\n* [Game Development Kompute ML in Godot Engine](https://towardsdatascience.com/supercharging-game-development-with-gpu-accelerated-ml-using-vulkan-kompute-the-godot-game-engine-4e75a84ea9f0)\n\n\n## Architectural Overview\n\nThe core architecture of Kompute includes the following:\n* [Kompute Manager](https://kompute.cc/overview/reference.html#manager) - Base orchestrator which creates and manages device and child components\n* [Kompute Sequence](https://kompute.cc/overview/reference.html#sequence) - Container of operations that can be sent to GPU as batch\n* [Kompute Operation (Base)](https://kompute.cc/overview/reference.html#algorithm) - Base class from which all operations inherit\n* [Kompute Tensor](https://kompute.cc/overview/reference.html#tensor) - Tensor structured data used in GPU operations\n* [Kompute Algorithm](https://kompute.cc/overview/reference.html#algorithm) - Abstraction for (shader) code executed in the GPU\n\nTo see a full breakdown you can read further in the [C++ Class Reference](https://kompute.cc/overview/reference.html).\n\n<table>\n<th>\nFull Vulkan Components\n</th>\n<th>\nSimplified Kompute Components\n</th>\n<tr>\n<td width=30%>\n\n\n<img width=\"100%\" src=\"https://raw.githubusercontent.com/ethicalml/vulkan-kompute/master/docs/images/kompute-vulkan-architecture.jpg\">\n\n<br>\n<br>\n(very tiny, check the <a href=\"https://ethicalml.github.io/vulkan-kompute/overview/reference.html\">full reference diagram in docs for details</a>)\n<br>\n<br>\n\n<img width=\"100%\" src=\"https://raw.githubusercontent.com/ethicalml/vulkan-kompute/master/docs/images/suspicious.jfif\">\n\n</td>\n<td>\n<img width=\"100%\" src=\"https://raw.githubusercontent.com/ethicalml/vulkan-kompute/master/docs/images/kompute-architecture.jpg\">\n</td>\n</tr>\n</table>\n\n\n## Asynchronous and Parallel Operations\n\nKompute provides flexibility to run operations in an asynrchonous way through Vulkan Fences. Furthermore, Kompute enables for explicit allocation of queues, which allow for parallel execution of operations across queue families.\n\nThe image below provides an intuition on how Kompute Sequences can be allocated to different queues to enable parallel execution based on hardware. You can see the [hands on example](https://kompute.cc/overview/advanced-examples.html#parallel-operations), as well as the [detailed documentation page](https://kompute.cc/overview/async-parallel.html) describing how it would work using an NVIDIA 1650 as an example. \n\n![](https://raw.githubusercontent.com/ethicalml/vulkan-kompute/master/docs/images/queue-allocation.jpg)\n\n## Mobile Enabled\n\nKompute has been optimized to work in mobile environments. The [build system](#build-overview) enables for dynamic loading of the Vulkan shared library for Android environments, together with a working [Android NDK Vulkan wrapper](https://github.com/EthicalML/vulkan-kompute/tree/master/vk_ndk_wrapper_include) for the CPP headers.\n\n<table>\n<tr>\n\n<td width=\"70%\">\n<p>\nFor a full deep dive you can read the blog post \"<a href=\"https://towardsdatascience.com/gpu-accelerated-machine-learning-in-your-mobile-applications-using-the-android-ndk-vulkan-kompute-1e9da37b7617\">Supercharging your Mobile Apps with On-Device GPU Accelerated Machine Learning</a>\". \n\nYou can also access the <a href=\"https://github.com/EthicalML/vulkan-kompute/tree/v0.4.0/examples/android/android-simple\">end-to-end example code</a> in the repository, which can be run using android studio.\n\n</p>\n\n\n<img src=\"https://raw.githubusercontent.com/EthicalML/vulkan-kompute/android-example/docs/images/android-editor.jpg\">\n\n</td>\n\n\n<td width=\"30%\">\n<img src=\"https://raw.githubusercontent.com/EthicalML/vulkan-kompute/android-example/docs/images/android-kompute.jpg\">\n</td>\n\n</tr>\n</table>\n\n## Python Package\n\nBesides the C++ core SDK you can also use the Python package of Kompute, which exposes the same core functionality, and supports interoperability with Python objects like Lists, Numpy Arrays, etc.\n\nYou can install from the repository by running:\n\n```\npip install .\n```\n\nFor further details you can read the [Python Package documentation](https://kompute.cc/overview/python-package.html) or the [Python Class Reference documentation](https://kompute.cc/overview/python-reference.html).\n\n### Python Example (Simple)\n\nThen you can interact with it from your interpreter. Below is the same sample as above \"Your First Kompute (Simple Version)\" but in Python:\n\n```python\nmgr = Manager()\n\n# Can be initialized with List[] or np.Array\ntensor_in_a = Tensor([2, 2, 2])\ntensor_in_b = Tensor([1, 2, 3])\ntensor_out = Tensor([0, 0, 0])\n\nmgr.eval_tensor_create_def([tensor_in_a, tensor_in_b, tensor_out])\n\nshaderFilePath = \"shaders/glsl/opmult.comp\"\nmgr.eval_async_algo_file_def([tensor_in_a, tensor_in_b, tensor_out], shaderFilePath)\n\n# Alternatively can pass raw string/bytes:\n# shaderFileData = \"\"\" shader code here... \"\"\"\n# mgr.eval_algo_data_def([tensor_in_a, tensor_in_b, tensor_out], list(shaderFileData))\n\nmgr.eval_await_def()\n\nmgr.eval_tensor_sync_local_def([tensor_out])\n\nassert tensor_out.data() == [2.0, 4.0, 6.0]\n```\n\n### Python Example (Extended)\n\nSimilarly you can find the same extended example as above:\n\n```python\nmgr = Manager(0, [2])\n\n# Can be initialized with List[] or np.Array\ntensor_in_a = Tensor([2, 2, 2])\ntensor_in_b = Tensor([1, 2, 3])\ntensor_out = Tensor([0, 0, 0])\n\nshaderFilePath = \"../../shaders/glsl/opmult.comp\"\n\nmgr.eval_tensor_create_def([tensor_in_a, tensor_in_b, tensor_out])\n\nseq = mgr.create_sequence(\"op\")\n\nmgr.eval_async_algo_file_def([tensor_in_a, tensor_in_b, tensor_out], shaderFilePath)\nmgr.eval_await_def()\n\nseq.begin()\nseq.record_tensor_sync_local([tensor_in_a])\nseq.record_tensor_sync_local([tensor_in_b])\nseq.record_tensor_sync_local([tensor_out])\nseq.end()\n\nseq.eval()\n\nassert tensor_out.data() == [2.0, 4.0, 6.0]\n```\n\nFor further details you can read the [Python Package documentation](https://kompute.cc/overview/python-package.html) or the [Python Class Reference documentation](https://kompute.cc/overview/python-reference.html).\n\n## Build Overview\n\nThe build system provided uses `cmake`, which allows for cross platform builds.\n\nThe top level `Makefile` provides a set of optimized configurations for development as well as the docker image build, but you can start a build with the following command:\n\n```\n   cmake -Bbuild\n```\n\nYou also are able to add Kompute in your repo with `add_subdirectory` - the [Android example CMakeLists.txt file](https://github.com/EthicalML/vulkan-kompute/blob/7c8c0eeba2cdc098349fcd999102bb2cca1bf711/examples/android/android-simple/app/src/main/cpp/CMakeLists.txt#L3) shows how this would be done.\n\nFor a more advanced overview of the build configuration check out the [Build System Deep Dive](https://kompute.cc/overview/build-system.html) documentation.\n\n## Kompute Development\n\nWe appreciate PRs and Issues. If you want to contribute try checking the \"Good first issue\" tag, but even using Vulkan Kompute and reporting issues is a great contribution!\n\n### Contributing\n\n#### Dev Dependencies\n\n* Testing\n    + GTest\n* Documentation\n    + Doxygen (with Dot)\n    + Sphynx\n\n#### Development\n\n* Follows Mozilla C++ Style Guide https://www-archive.mozilla.org/hacking/mozilla-style-guide.html\n    + Uses post-commit hook to run the linter, you can set it up so it runs the linter before commit\n    + All dependencies are defined in vcpkg.json \n* Uses cmake as build system, and provides a top level makefile with recommended command\n* Uses xxd (or xxd.exe windows 64bit port) to convert shader spirv to header files\n* Uses doxygen and sphinx for documentation and autodocs\n* Uses vcpkg for finding the dependencies, it's the recommended set up to retrieve the libraries\n\n##### Updating documentation\n\nTo update the documentation you will need to:\n* Run the gendoxygen target in the build system\n* Run the gensphynx target in the build-system \n* Push to github pages with `make push_docs_to_ghpages`\n\n##### Running tests\n\nTo run tests you can use the helper top level Makefile\n\nFor visual studio you can run\n\n```\nmake vs_cmake\nmake vs_run_tests VS_BUILD_TYPE=\"Release\"\n```\n\nFor unix you can run\n\n```\nmake mk_cmake MK_BUILD_TYPE=\"Release\"\nmake mk_run_tests\n```\n\n## Motivations\n\nThis project started after seeing that a lot of new and renowned ML & DL projects like Pytorch, Tensorflow, Alibaba DNN, Tencent NCNN - among others - have either integrated or are looking to integrate the Vulkan SDK to add mobile (and cross-vendor) GPU support.\n\nThe Vulkan SDK offers a great low level interface that enables for highly specialized optimizations - however it comes at a cost of highly verbose code which requires 500-2000 lines of code to even begin writing application code. This has resulted in each of these projects having to implement the same baseline to abstract the non-compute related features of Vulkan. This large amount of non-standardised boiler-plate can result in limited knowledge transfer, higher chance of unique framework implementation bugs being introduced, etc.\n\nWe are currently developing Vulkan Kompute not to hide the Vulkan SDK interface (as it's incredibly well designed) but to augment it with a direct focus on Vulkan's GPU computing capabilities. [This article](https://towardsdatascience.com/machine-learning-and-data-processing-in-the-gpu-with-vulkan-kompute-c9350e5e5d3a) provides a high level overview of the motivations of Kompute, together with a set of hands on examples that introduce both GPU computing as well as the core Vulkan Kompute architecture.\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "kompute",
    "package_url": "https://pypi.org/project/kompute/",
    "platform": "",
    "project_url": "https://pypi.org/project/kompute/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/kompute/0.4.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Vulkan Kompute: Blazing fast, mobile-enabled, asynchronous, and optimized for advanced GPU processing usecases.",
    "version": "0.4.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8581656,
  "releases": {
    "0.4.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ca6d9038901b9017977dc628f48a2512dcb0c57e5c5c7f4ca6422b9c4e16a44b",
          "md5": "87c20a1c73dd9622e315cbe8735d09ed",
          "sha256": "2f1441994c942c45d8e87d3148015e3458d1a01cd71f6333f6eca72c90bd8d54"
        },
        "downloads": -1,
        "filename": "kompute-0.4.1-cp38-cp38-win_amd64.whl",
        "has_sig": false,
        "md5_digest": "87c20a1c73dd9622e315cbe8735d09ed",
        "packagetype": "bdist_wheel",
        "python_version": "cp38",
        "requires_python": null,
        "size": 201305,
        "upload_time": "2020-11-04T21:01:27",
        "upload_time_iso_8601": "2020-11-04T21:01:27.418792Z",
        "url": "https://files.pythonhosted.org/packages/ca/6d/9038901b9017977dc628f48a2512dcb0c57e5c5c7f4ca6422b9c4e16a44b/kompute-0.4.1-cp38-cp38-win_amd64.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "88deea432a0d7680fc676740b82fd17466c4980153556b8e7850da051058c954",
          "md5": "e9d6ab80ae0a89d0c3941d2a9fb27fba",
          "sha256": "42bf206ac885ed8f3733e4167409ee49791a22a2750cba300634db1b39286295"
        },
        "downloads": -1,
        "filename": "kompute-0.4.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e9d6ab80ae0a89d0c3941d2a9fb27fba",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 2769164,
        "upload_time": "2020-11-04T21:01:34",
        "upload_time_iso_8601": "2020-11-04T21:01:34.402419Z",
        "url": "https://files.pythonhosted.org/packages/88/de/ea432a0d7680fc676740b82fd17466c4980153556b8e7850da051058c954/kompute-0.4.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ca6d9038901b9017977dc628f48a2512dcb0c57e5c5c7f4ca6422b9c4e16a44b",
        "md5": "87c20a1c73dd9622e315cbe8735d09ed",
        "sha256": "2f1441994c942c45d8e87d3148015e3458d1a01cd71f6333f6eca72c90bd8d54"
      },
      "downloads": -1,
      "filename": "kompute-0.4.1-cp38-cp38-win_amd64.whl",
      "has_sig": false,
      "md5_digest": "87c20a1c73dd9622e315cbe8735d09ed",
      "packagetype": "bdist_wheel",
      "python_version": "cp38",
      "requires_python": null,
      "size": 201305,
      "upload_time": "2020-11-04T21:01:27",
      "upload_time_iso_8601": "2020-11-04T21:01:27.418792Z",
      "url": "https://files.pythonhosted.org/packages/ca/6d/9038901b9017977dc628f48a2512dcb0c57e5c5c7f4ca6422b9c4e16a44b/kompute-0.4.1-cp38-cp38-win_amd64.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "88deea432a0d7680fc676740b82fd17466c4980153556b8e7850da051058c954",
        "md5": "e9d6ab80ae0a89d0c3941d2a9fb27fba",
        "sha256": "42bf206ac885ed8f3733e4167409ee49791a22a2750cba300634db1b39286295"
      },
      "downloads": -1,
      "filename": "kompute-0.4.1.tar.gz",
      "has_sig": false,
      "md5_digest": "e9d6ab80ae0a89d0c3941d2a9fb27fba",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 2769164,
      "upload_time": "2020-11-04T21:01:34",
      "upload_time_iso_8601": "2020-11-04T21:01:34.402419Z",
      "url": "https://files.pythonhosted.org/packages/88/de/ea432a0d7680fc676740b82fd17466c4980153556b8e7850da051058c954/kompute-0.4.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}