{
  "info": {
    "author": "Sebastien M. Popoff",
    "author_email": "sebastien.popoff@espci.psl.eu",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# complexPyTorch\n\nA high-level toolbox for using complex valued neural networks in PyTorch.\n\nBefore version 1.7 of PyTroch, complex tensor were not supported. \nThe initial version of **complexPyTorch** represented complex tensor using two tensors, one for the real and one for the imaginary part.\nSince version 1.7, compex tensors of type `torch.complex64` are allowed, but only a limited number of operation are supported.\nThe current version **complexPyTorch** use complex tensors (hence requires PyTorch version >= 1.7) and add support for various operations and layers.\n\n## Installation\n```bash\npip install complexPyTorch\n```\n\n## Complex Valued Networks with PyTorch\n\nArtificial neural networks are mainly used for treating data encoded in real values, such as digitized images or sounds. \nIn such systems, using complex-valued tensor would be quite useless. \nHowever, for physic related topics, in particular when dealing with wave propagation, using complex values is interesting as the physics typically has linear, hence more simple, behavior when considering complex fields. \ncomplexPyTorch is a simple implementation of complex-valued functions and modules using the high-level API of PyTorch. \nFollowing [[C. Trabelsi et al., International Conference on Learning Representations, (2018)](https://openreview.net/forum?id=H1T2hmZAb)], it allows the following layers and functions to be used with complex values:\n* Linear\n* Conv2d\n* MaxPool2d\n* Relu (&#8450;Relu)\n* BatchNorm1d (Naive and Covariance approach)\n* BatchNorm2d (Naive and Covariance approach)\n\n## Citating the code\n\nIf the code was helpful to your work, please consider citing it:\n\n[![DOI](https://img.shields.io/badge/DOI-10.1103%2FPhysRevX.11.021060-blue)](https://doi.org/10.1103/PhysRevX.11.021060)\n\n\n## Syntax and usage\n\nThe syntax is supposed to copy the one of the standard real functions and modules from PyTorch. \nThe names are the same as in `nn.modules` and `nn.functional` except that they start with `Complex` for Modules, e.g. `ComplexRelu`, `ComplexMaxPool2d` or `complex_` for functions, e.g. `complex_relu`, `complex_max_pool2d`.\nThe only usage difference is that the forward function takes two tensors, corresponding to real and imaginary parts, and returns two ones too.\n\n## BatchNorm\n\nFor all other layers, using the recommendation of [[C. Trabelsi et al., International Conference on Learning Representations, (2018)](https://openreview.net/forum?id=H1T2hmZAb)], the calculation can be done in a straightforward manner using functions and modules form `nn.modules` and `nn.functional`. \nFor instance, the function `complex_relu` in `complexFunctions`, or its associated module `ComplexRelu` in `complexLayers`, simply performs `relu` both on the real and imaginary part and returns the two tensors.\nThe complex BatchNorm proposed in [[C. Trabelsi et al., International Conference on Learning Representations, (2018)](https://openreview.net/forum?id=H1T2hmZAb)] requires the calculation of the inverse square root of the covariance matrix.\nThis is implemented in `ComplexbatchNorm1D` and `ComplexbatchNorm2D` but using the high-level PyTorch API, which is quite slow.\nThe gain of using this approach, however, can be experimentally marginal compared to the naive approach which consists in simply performing the BatchNorm on both the real and imaginary part, which is available using `NaiveComplexbatchNorm1D` or `NaiveComplexbatchNorm2D`.\n\n\n## Example\n\nFor illustration, here is a small example of a complex model.\nNote that in that example, complex values are not particularly useful, it just shows how one can handle complex ANNs.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom complexPyTorch.complexLayers import ComplexBatchNorm2d, ComplexConv2d, ComplexLinear\nfrom complexPyTorch.complexFunctions import complex_relu, complex_max_pool2d\n\nbatch_size = 64\ntrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\ntrain_set = datasets.MNIST('../data', train=True, transform=trans, download=True)\ntest_set = datasets.MNIST('../data', train=False, transform=trans, download=True)\n\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size= batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size= batch_size, shuffle=True)\n\nclass ComplexNet(nn.Module):\n    \n    def __init__(self):\n        super(ComplexNet, self).__init__()\n        self.conv1 = ComplexConv2d(1, 10, 5, 1)\n        self.bn  = ComplexBatchNorm2d(10)\n        self.conv2 = ComplexConv2d(10, 20, 5, 1)\n        self.fc1 = ComplexLinear(4*4*20, 500)\n        self.fc2 = ComplexLinear(500, 10)\n             \n    def forward(self,x):\n        x = self.conv1(x)\n        x = complex_relu(x)\n        x = complex_max_pool2d(x, 2, 2)\n        x = self.bn(x)\n        x = self.conv2(x)\n        x = complex_relu(x)\n        x = complex_max_pool2d(x, 2, 2)\n        x = x.view(-1,4*4*20)\n        x = self.fc1(x)\n        x = complex_relu(x)\n        x = self.fc2(x)\n        x = x.abs()\n        x =  F.log_softmax(x, dim=1)\n        return x\n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = ComplexNet().to(device)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device).type(torch.complex64), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print('Train Epoch: {:3} [{:6}/{:6} ({:3.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch,\n                batch_idx * len(data), \n                len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), \n                loss.item())\n            )\n\n# Run training on 50 epochs\nfor epoch in range(50):\n    train(model, device, train_loader, optimizer, epoch)\n```\n       \n\n## Acknowledgments\n\nI want to thank Piotr Bialecki for his invaluable help on the PyTorch forum.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.institut-langevin.espci.fr/spopoff/complexPyTorch",
    "keywords": "pytorch,deep learning,complex values",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "complexPyTorch",
    "package_url": "https://pypi.org/project/complexPyTorch/",
    "platform": "",
    "project_url": "https://pypi.org/project/complexPyTorch/",
    "project_urls": {
      "Homepage": "https://gitlab.institut-langevin.espci.fr/spopoff/complexPyTorch"
    },
    "release_url": "https://pypi.org/project/complexPyTorch/0.4/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A high-level toolbox for using complex valued neural networks in PyTorch.",
    "version": "0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11620999,
  "releases": {
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "64eb182b68dc251a66f9aa1141c95f02c7b6e4876cb5eb396fa7cb1ae9b6d1b9",
          "md5": "088c27ea5f38f522c3a66a55222ef086",
          "sha256": "ac493d2a6d5c20300c5ed1bc2b628d94ee8cc0ad0513006422e4d7fca9e464a7"
        },
        "downloads": -1,
        "filename": "complexPyTorch-0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "088c27ea5f38f522c3a66a55222ef086",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 8348,
        "upload_time": "2021-04-02T19:48:57",
        "upload_time_iso_8601": "2021-04-02T19:48:57.950364Z",
        "url": "https://files.pythonhosted.org/packages/64/eb/182b68dc251a66f9aa1141c95f02c7b6e4876cb5eb396fa7cb1ae9b6d1b9/complexPyTorch-0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7e87921b8a41a607fa19ca81a5c3237a958b32d06296fe6cad4653b7e8dc2ef4",
          "md5": "ffd37f228006374611817dcaab1af3ba",
          "sha256": "cbe3d3a54e20f88a1aa2ef0f3ac7e1c6de7080d39319a5fc602871ed3b6cb92c"
        },
        "downloads": -1,
        "filename": "complexPyTorch-0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "ffd37f228006374611817dcaab1af3ba",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 9784,
        "upload_time": "2021-10-04T07:22:36",
        "upload_time_iso_8601": "2021-10-04T07:22:36.919016Z",
        "url": "https://files.pythonhosted.org/packages/7e/87/921b8a41a607fa19ca81a5c3237a958b32d06296fe6cad4653b7e8dc2ef4/complexPyTorch-0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7e87921b8a41a607fa19ca81a5c3237a958b32d06296fe6cad4653b7e8dc2ef4",
        "md5": "ffd37f228006374611817dcaab1af3ba",
        "sha256": "cbe3d3a54e20f88a1aa2ef0f3ac7e1c6de7080d39319a5fc602871ed3b6cb92c"
      },
      "downloads": -1,
      "filename": "complexPyTorch-0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "ffd37f228006374611817dcaab1af3ba",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 9784,
      "upload_time": "2021-10-04T07:22:36",
      "upload_time_iso_8601": "2021-10-04T07:22:36.919016Z",
      "url": "https://files.pythonhosted.org/packages/7e/87/921b8a41a607fa19ca81a5c3237a958b32d06296fe6cad4653b7e8dc2ef4/complexPyTorch-0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}