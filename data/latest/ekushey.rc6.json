{
  "info": {
    "author": "Karigor",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Ekushey\n\n\"Ekushey\" is the First Structured and Cost-Effective Bangla Natural Language Processing Toolkit\n\n\n\n## Current Modules\n- [Feature Extraction](#feature-extraction)\n```\nfeature_extraction is a Bangla Natural Language Processing based feature extractor\n```\n\n\n### Feature Extraction\n\n  1. [CountVectorizer](#1-countvectorizer)\n  2. [HashVectorizer](#2-hashvectorizer)\n  3. [TfIdf](#3-tfidf)\n  4. [Word Embedding](#4-word-embedding)\n      * [Word2Vec](#word2vec)\n      * [FastText](#fasttext)\n\t  * [GloVe](#glove)\n\n## Installation\n```\npip install ekushey\n```\n## Example\n### 1. CountVectorizer\n  - Fit n Transform\n  - Transform\n  - Get Wordset\n\n**Fit n Transform**\n```py\nfrom ekushey.feature_extraction import CountVectorizer\nct = CountVectorizer()\nX = ct.fit_transform(X) # X is the word features\n```\nOutput:\n```\nthe countVectorized matrix form of given features\n```\n\n**Transform**\n```py\nfrom ekushey.feature_extraction import CountVectorizer\nct = CountVectorizer()\nget_mat = ct.transform(\"রাহাত\")\n```\nOutput:\n```\nthe countVectorized matrix form of given word\n```\n\n**Get Wordset**\n```py\nfrom ekushey.feature_extraction import CountVectorizer\nct = CountVectorizer()\nct.get_wordSet()\n```\nOutput:\n```\nget the raw wordset used in training model\n```\n\n### 2. HashVectorizer\n  - Fit n Transform\n  - Transform\n```py\nfrom ekushey.feature_extraction import HashVectorizer\ncorpus = [\n'আমাদের দেশ বাংলাদেশ', 'আমার বাংলা'\n]\nVectorizer = HashVectorizer()\nn_features = 8\nX = Vectorizer.fit_transform(corpus, n_features)\ncorpus_t = [\"আমাদের দেশ অনেক সুন্দর\"]\nXf = Vectorizer.transform(corpus_t)\n\nprint(X.shape, Xf.shape)\nprint(\"=====================================\")\nprint(X)\nprint(\"=====================================\")\nprint(Xf)\n```\nOutput:\n```\n(2, 8) (1, 8)\n=====================================\n  (0, 7)\t-1.0\n  (1, 7)\t-1.0\n=====================================\n  (0, 0)\t0.5773502691896258\n  (0, 2)\t0.5773502691896258\n  (0, 7)\t-0.5773502691896258\n```\n\n**Get Wordset**\n\n\n### 3. TfIdf\n  - Fit n Transform\n  - Transform\n  - Coefficients\n\n **Fit n Transform**\n```py\nfrom ekushey.feature_extraction import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"কাওছার আহমেদ\", \"শুভ হাইদার\"]\nmatrix1 = k.fit_transform(doc)\nprint(matrix1)\n```\nOutput:\n```\n[[0.150515 0.150515 0.       0.      ]\n [0.       0.       0.150515 0.150515]]\n\n```\n\n**Transform**\n```py\nfrom ekushey.feature_extraction import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"আহমেদ সুমন\", \"কাওছার করিম\"]\nmatrix2 = k.transform(doc)\nprint(matrix2)\n```\nOutput: \n```\n[[0.150515 0.       0.       0.      ]\n [0.       0.150515 0.       0.      ]]\n```\n**Coefficients**\n```py\nfrom ekushey.feature_extraction import TfIdfVectorizer\nk = TfIdfVectorizer()\ndoc = [\"কাওছার আহমেদ\", \"শুভ হাইদার\"]\nk.fit_transform(doc)\nwordset, idf = k.coefficients()\nprint(wordset)\n#Output: ['আহমেদ', 'কাওছার', 'হাইদার', 'শুভ']\n\nprint(idf)\n'''\nOutput: \n{'আহমেদ': 0.3010299956639812, 'কাওছার': 0.3010299956639812, 'হাইদার': 0.3010299956639812, 'শুভ': 0.3010299956639812}\n'''\n\n```\n\n### 4. Word Embedding\n- ### Word2Vec\n    - Training\n    - Get Word Vector\n    - Get Similarity\n    - Get n Similar Words\n    - Get Middle Word\n    - Get Odd Words\n    - Get Similarity Plot\n\n**Training**\n```py\nfrom ekushey.feature_extraction import BN_Word2Vec\n#Training Against Sentences\nw2v = BN_Word2Vec(sentences=[['আমার', 'প্রিয়', 'জন্মভূমি'], ['বাংলা', 'আমার', 'মাতৃভাষা'],['আমার', 'প্রিয়', 'জন্মভূমি'], ['বাংলা', 'আমার', 'মাতৃভাষা'],['আমার', 'প্রিয়', 'জন্মভূমি'], ['বাংলা', 'আমার', 'মাতৃভাষা']])\nw2v.train()\n\n#Training Against one Text Corpus\nw2v = BN_Word2Vec(corpus_file=\"path_to_corpus.txt\")\nw2v.train()\n\n#Training Against Multiple corpuses\n'''\n    path\n      ->corpus\n        ->1.txt\n        ->2.txt\n        ->3.txt\n'''\nw2v = BN_Word2Vec(corpus_path=\"path/corpus\")\nw2v.train(epochs=25)\n\n\n#Training Against a Dataframe Column\nw2v = BN_Word2Vec(df= news_data['text_content'])\nw2v.train(epochs=25)\n```\nAfter training is done the model \"w2v_model\"  along with it's supportive vector files will be saved to current directory.\n\n**If you use any pretrained model, specify it while initializing BN_Word2Vec() . Otherwise no model_name is needed.**\n\n**Get Word Vector**\n```py\nfrom ekushey.feature_extraction import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_wordVector('আমার')\n```\n\n**Get Similarity**\n```py\nfrom ekushey.feature_extraction import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_similarity('ঢাকা', 'রাজধানী')\n```\nOutput: \n```\n67.457879\n```\n\n**Get n Similar Words**\n```py\nfrom ekushey.feature_extraction import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_n_similarWord(['পদ্মা'], n=10)\n```\nOutput: \n```\n[('সেতুর', 0.5857524275779724),\n ('মুলফৎগঞ্জ', 0.5773632526397705),\n ('মহানন্দা', 0.5634652376174927),\n (\"'পদ্মা\", 0.5617109537124634),\n ('গোমতী', 0.5605217218399048),\n ('পদ্মার', 0.5547558069229126),\n ('তুলসীগঙ্গা', 0.5274507999420166),\n ('নদীর', 0.5232067704200745),\n ('সেতু', 0.5225246548652649),\n ('সেতুতে', 0.5192927718162537)]\n```\n\n**Get Middle Word**\n\nGet the probability distribution of the center word given words list.\n```py\nfrom ekushey.feature_extraction import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_outputWord(['ঢাকায়', 'মৃত্যু'], n=2)\n```\nOutput:\n```\n[(\"হয়েছে।',\", 0.05880642), ('শ্রমিকের', 0.05639163)]\n```\n\n**Get Odd Words**\n\nGet the most unmatched word out from given words list\n```py\nfrom ekushey.feature_extraction import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_oddWords(['চাল', 'ডাল', 'চিনি', 'আকাশ'])\n```\nOutput: \n```\n'আকাশ' \n```\n\n**Get Similarity Plot**\n\nCreates a barplot of similar words with their probability \n\n```py\nfrom ekushey.feature_extraction import BN_Word2Vec \nw2v = BN_Word2Vec(model_name='give the model name here')\nw2v.get_similarity_plot('চাউল', 5)\n```\n\n- ### FastText\n    - Training\n    - Get Word Vector\n    - Get Similarity\n    - Get n Similar Words\n    - Get Middle Word\n    - Get Odd Words\n\n\n**Training**\n```py\nfrom ekushey.feature_extraction import BN_FastText\n#Training Against Sentences\nft = ft = BN_FastText(sentences=[['আমার', 'প্রিয়', 'জন্মভূমি'], ['বাংলা', 'আমার', 'মাতৃভাষা'], ['বাংলা', 'আমার', 'মাতৃভাষা'], ['বাংলা', 'আমার', 'মাতৃভাষা'], ['বাংলা', 'আমার', 'মাতৃভাষা'] ])\nft.train()\n\n#Training Against one Text Corpus\nft = BN_FastText(corpus_file=\"path to data or txt file\")\nft.train()\n\n#Training Against Multiple Corpuses\n'''\n    path\n      ->Corpus\n        ->1.txt\n        ->2.txt\n        ->3.txt\n'''\nft = BN_FastText(corpus_path=\"path/Corpus\")\nft.train(epochs=25)\n\n#Training Against a Dataframe Column\nft = BN_FastText(df= news_data['text_content'])\nft.train(epochs=25)\n\n```\nAfter training is done the model \"ft_model\"  along with it's supportive vector files will be saved to current directory.\n\n**If you don't want to train instead use a pretrained model, specify it while initializing BN_FastText() . Otherwise no model_name is needed.**\n\n**Get Word Vector**\n```py\nfrom ekushey.feature_extraction import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_wordVector('আমার')\n```\n\n**Get Similarity**\n```py\nfrom ekushey.feature_extraction import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_similarity('ঢাকা', 'রাজধানী')\n```\nOutput:\n```\n70.56821120\n```\n\n**Get n Similar Words**\n```py\nfrom ekushey.feature_extraction\" import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_n_similarWord(['পদ্মা'], n=10)\n```\n\nOutput: \n```\n[('পদ্মায়', 0.8103810548782349),\n ('পদ্মার', 0.794012725353241),\n ('পদ্মানদীর', 0.7747839689254761),\n ('পদ্মা-মেঘনার', 0.7573559284210205),\n ('পদ্মা.', 0.7470568418502808),\n ('‘পদ্মা', 0.7413997650146484),\n ('পদ্মাসেতুর', 0.716225266456604),\n ('পদ্ম', 0.7154797315597534),\n ('পদ্মহেম', 0.6881639361381531),\n ('পদ্মাবত', 0.6682782173156738)]\n```\n\n**Get Odd Words**\n\nGet the most unmatched word out from given words list\n```py\nfrom \"package_name\" import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_oddWords(['চাল', 'ডাল', 'চিনি', 'আকাশ'])\n```\nOutput:\n```\n'আকাশ' \n```\n\n**Get Similarity Plot**\n\nCreates a barplot of similar words with their probability \n\n```py\nfrom ekushey.feature_extraction import BN_FastText \nft = BN_FastText(model_name='give the model name here')\nft.get_similarity_plot('চাউল', 5)\n```\n\n- ### GloVe\n    - Training\n    - Get n Similar Words\n\n\n**Training**\n```py\nfrom ekushey.feature_extraction import BN_GloVe\n#Training Against Sentences\nglv = BN_GloVe(sentences=[['আমার', 'প্রিয়', 'জন্মভূমি'], ['বাংলা', 'আমার', 'মাতৃভাষা'], ['বাংলা', 'আমার', 'মাতৃভাষা'], ['বাংলা', 'আমার', 'মাতৃভাষা'], ['বাংলা', 'আমার', 'মাতৃভাষা'] ])\nglv.train()\n\n#Training Against one Text Corpus\nglv = BN_GloVe(corpus_file=\"path_to_corpus.txt\")\nglv.train()\n\n#Training Against Multiple Corpuses\n'''\n    path\n      ->Corpus\n        ->1.txt\n        ->2.txt\n        ->3.txt\n'''\nglv = BN_GloVe(corpus_path=\"path/corpus\")\nglv.train(epochs=25)\n\n#Training Against a Dataframe Column\nglv = BN_GloVe(df= news_data['text_content'])\nglv.train(epochs=25)\n\n\n```\nAfter training is done the model \"glove_model\"  along with it's supportive vector files will be saved to current directory.\n\n**If you don't want to train instead use a pretrained model, specify it while initializing BN_FastText() . Otherwise no model_name is needed.**\n\n\n**Get n Similar Words**\n```py\nfrom ekushey.feature_extraction\" import BN_GloVe \nglv = BN_GloVe(model_name='give the model name here')\nglv.get_n_similarWord(['পদ্মা'], n=10)\n```\n\nOutput: \n```\n[('পদ্মায়', 0.8103810548782349),\n ('পদ্মার', 0.794012725353241),\n ('পদ্মানদীর', 0.7747839689254761),\n ('পদ্মা-মেঘনার', 0.7573559284210205),\n ('পদ্মা.', 0.7470568418502808),\n ('‘পদ্মা', 0.7413997650146484),\n ('পদ্মাসেতুর', 0.716225266456604),\n ('পদ্ম', 0.7154797315597534),\n ('পদ্মহেম', 0.6881639361381531),\n ('পদ্মাবত', 0.6682782173156738)]\n```\n\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Kowsher/Bangla-NLP/tree/master/Bangla%20Feature%20Extraction",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ekushey",
    "package_url": "https://pypi.org/project/ekushey/",
    "platform": "",
    "project_url": "https://pypi.org/project/ekushey/",
    "project_urls": {
      "Homepage": "https://github.com/Kowsher/Bangla-NLP/tree/master/Bangla%20Feature%20Extraction"
    },
    "release_url": "https://pypi.org/project/ekushey/0.6/",
    "requires_dist": [
      "scipy",
      "gensim",
      "numpy",
      "matplotlib",
      "scikit-learn",
      "glove-python"
    ],
    "requires_python": "",
    "summary": "",
    "version": "0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7973212,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4138bd4cd2a8ea5dade77b2a1b76c1bd8d97ea3469761d817c0e5cc73f22dda5",
          "md5": "ef78ee7bd41f9dfa0eecd0d0521c8bb9",
          "sha256": "1fbfe877d152b93f8c43ec6658231f4593c5efd3bbf3142cd2f537b385285793"
        },
        "downloads": -1,
        "filename": "ekushey-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ef78ee7bd41f9dfa0eecd0d0521c8bb9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 2629,
        "upload_time": "2020-04-06T18:26:50",
        "upload_time_iso_8601": "2020-04-06T18:26:50.920983Z",
        "url": "https://files.pythonhosted.org/packages/41/38/bd4cd2a8ea5dade77b2a1b76c1bd8d97ea3469761d817c0e5cc73f22dda5/ekushey-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4609cdc67858a43a8718929b0c3e546ca4ca1cfa8a21ccd286b2abb202b78e76",
          "md5": "ea5699efc6b66fc2a280c3413a74ed00",
          "sha256": "037cbcfec7b5e1524293b040420521d700e73890b714afc1962b213ead0a16c6"
        },
        "downloads": -1,
        "filename": "ekushey-0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ea5699efc6b66fc2a280c3413a74ed00",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 8990,
        "upload_time": "2020-04-17T18:29:43",
        "upload_time_iso_8601": "2020-04-17T18:29:43.371590Z",
        "url": "https://files.pythonhosted.org/packages/46/09/cdc67858a43a8718929b0c3e546ca4ca1cfa8a21ccd286b2abb202b78e76/ekushey-0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6bf810dd297b125507e92b8c8debc283db26788774bebc0d26c5b7328af2a43e",
          "md5": "8a3d2fded4eb41621dc23a1ec9877079",
          "sha256": "00f5ec85380fac9c82fc249945657d7aa946b25b090fbe2f78cf0786f6fa2fae"
        },
        "downloads": -1,
        "filename": "ekushey-0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8a3d2fded4eb41621dc23a1ec9877079",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 8994,
        "upload_time": "2020-04-17T18:36:42",
        "upload_time_iso_8601": "2020-04-17T18:36:42.859706Z",
        "url": "https://files.pythonhosted.org/packages/6b/f8/10dd297b125507e92b8c8debc283db26788774bebc0d26c5b7328af2a43e/ekushey-0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "979beef725c8267adb8757fefe4e81994ab7ac47f398136fe41d637d6f7add1f",
          "md5": "37f80fea0575383df9612aef6f53949b",
          "sha256": "cdbc99f1b1237937cab48c3201e090fba8955c8dbf7e8886b7a441a26028d900"
        },
        "downloads": -1,
        "filename": "ekushey-0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "37f80fea0575383df9612aef6f53949b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 8980,
        "upload_time": "2020-04-17T19:12:08",
        "upload_time_iso_8601": "2020-04-17T19:12:08.857646Z",
        "url": "https://files.pythonhosted.org/packages/97/9b/eef725c8267adb8757fefe4e81994ab7ac47f398136fe41d637d6f7add1f/ekushey-0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3c710c79b27fe3b693adc335e579b25705bf5725404ba211031d98eb78fc8d56",
          "md5": "0a5fa252ac21f683b10e6df0c4a0c870",
          "sha256": "7bd96abda42ab4ef3a1793229c6b35deff4db1c69fd34554860b916e21ee1270"
        },
        "downloads": -1,
        "filename": "ekushey-0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0a5fa252ac21f683b10e6df0c4a0c870",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 167904,
        "upload_time": "2020-05-30T07:54:47",
        "upload_time_iso_8601": "2020-05-30T07:54:47.600637Z",
        "url": "https://files.pythonhosted.org/packages/3c/71/0c79b27fe3b693adc335e579b25705bf5725404ba211031d98eb78fc8d56/ekushey-0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0671e9961fe1bfc12e32046e7a24e6429e2ab49158bd65b73db3db9c4d0f5216",
          "md5": "81670da52464892fe09f1e7f079c363b",
          "sha256": "182263dbdae1eef67c9c83b35fd36e93f06d029ce5fff2ffbdff6ed0c56670a1"
        },
        "downloads": -1,
        "filename": "ekushey-0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "81670da52464892fe09f1e7f079c363b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 167906,
        "upload_time": "2020-05-30T07:57:19",
        "upload_time_iso_8601": "2020-05-30T07:57:19.976567Z",
        "url": "https://files.pythonhosted.org/packages/06/71/e9961fe1bfc12e32046e7a24e6429e2ab49158bd65b73db3db9c4d0f5216/ekushey-0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0671e9961fe1bfc12e32046e7a24e6429e2ab49158bd65b73db3db9c4d0f5216",
        "md5": "81670da52464892fe09f1e7f079c363b",
        "sha256": "182263dbdae1eef67c9c83b35fd36e93f06d029ce5fff2ffbdff6ed0c56670a1"
      },
      "downloads": -1,
      "filename": "ekushey-0.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "81670da52464892fe09f1e7f079c363b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 167906,
      "upload_time": "2020-05-30T07:57:19",
      "upload_time_iso_8601": "2020-05-30T07:57:19.976567Z",
      "url": "https://files.pythonhosted.org/packages/06/71/e9961fe1bfc12e32046e7a24e6429e2ab49158bd65b73db3db9c4d0f5216/ekushey-0.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}