{
  "info": {
    "author": "Junyi Zhong",
    "author_email": "jyzh@yahoo.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# probability-calibration Python Library\nSample × Category Probability Calibration with Two Dimensions (ProbCalib2D)\n\nThis repository contains library code (Calibration Folder) to evaluate calibration and to measure the calibration error of models, including confidence intervals, models outcome probability give more information, not traditionally measure on model's accuracy. Generally, calibration is a pos-processing way to take an existing model and correct its uncertainties to make them more reliable and trustworthy.\n\n## Problem Setting\n\nDeep Convolutional Neural Networks have achieved very good performance in text or image classification tasks. However, the problem of probability calibration leads to myriad problems in safety-critical machine learning application filed. The predictions of model can be over-confident if without calibration or mis-calibration.  As a consequence ,it is necessary to evaluate the model calibration. There is still a main limitation, which is the calibration only adapted for one dimension. The aim is to find calibration methods that take into account both dimensions simultaneously. \n\n### Installation\n```\npip install probability-calibration\n```\n### Multi-label vs. Multi-class Classification\n**Multi-label**: In the complete and “non-exclusive”,  or multi-label setting,  zero,one or more categories can be associated to a given sample (e.g., Pascal VOC [3]). The labelvector associated to a given sample is still a binary one but does not necessarily correspond toany one-hot encoding\n\nMulti-label classification is a predictive modeling task that involves predicting zero or moremutually non-exclusive class labels. When designing a CNN model to perform a classificationtask (e.g. classifying objects in cifar10 dataset or classifying handwritten digits) we want to tellour model whether it is allowed to choose many answers (e.g.  both frog and cat) or only oneanswer (e.g. the digit “8.”\n\n**Multi-class**: In the complete and “exclusive”, or multi-class setting, each of thesamples is associated to exactly one category (e.g., MNIST [2], SVHN [5], CIFAR [4], CUBcategories [8],ILSVRC [7], and many other standard collections).\nThis section will discuss how we can achieve this goal by applying either a sigmoid or a softmax function to our classifier’s raw output values.\n\n### Applying Sigmoid versus Softmax\nGenerally, we use softmax activation instead of sigmoid with the cross-entropy loss because softmax activation distributes the probability throughout each output node. If it is a binary classification, using sigmoid is same as softmax. For multi-class classification use sofmax with cross-entropy.\n\nAt the end of a neural network classifier, you’ll get a vector of “raw output values”: for example [-0.5, 1.2, -0.1, 2.4] if your neural network has four outputs . We’d like to convert these raw values into an understandable format: probabilities. After all, it makes more sense to tell a patient that their risk of diabetes is 91\\% rather than “2.4” (which looks arbitrary.)\n\nWe convert a classifier’s raw output values into probabilities using either a sigmoid function or a softmax function.\n\n\n### Applying SoftMax normalization versus Platt Scaling\nIs Platt scaling can well calibrated? For anwsering this question, we are going to take Softmax to compare methods like Platt scaling. These scores can be used for ranking test images according to their likeliness to contain a given target concept (search task) or for ranking target concepts according to their likeliness to be visible in a given image (classification task). \n\n## Evaluating Calibration \nIn order to rectify the problem of the uncertainty of model, these works resulted in two common ways of measuring calibration: reliability diagrams [17] and estimates of the squared expected calibration error (ECE)[17].\n\nBoth tasks are usually evaluated with different metrics. In the multi-class setting, the classification performance is generally evaluated using the top-N accuracy, usually with N = 1.\n\nThe retrieval performance is generally evaluated using the mean average precision (MAP).\nThe MAP may also be used for the evaluation of the classification performance in the multilabel setting, like for Pascal VOC, as there may be more than one correct category associated to a given sample\n\nVisualizing calibration with reliability diagrams.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Jooeys/ProbCalib2D",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "probability-calibration",
    "package_url": "https://pypi.org/project/probability-calibration/",
    "platform": "",
    "project_url": "https://pypi.org/project/probability-calibration/",
    "project_urls": {
      "Homepage": "https://github.com/Jooeys/ProbCalib2D"
    },
    "release_url": "https://pypi.org/project/probability-calibration/0.0.1/",
    "requires_dist": [
      "numpy",
      "sklearn",
      "parameterized"
    ],
    "requires_python": ">=3.6",
    "summary": "Utilities to calibrate model outcome probability and evaluate calibration.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10088848,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4c026c80ed89488f93b3a513439e46ff88bac9d98491a08e333ceff1315128e4",
          "md5": "a6fb13aa539f2f9fb935891c984ad176",
          "sha256": "0b096f8c4362c9f9248d34042c396418e7e8390f0f34e336f80fc8a98629cd3e"
        },
        "downloads": -1,
        "filename": "probability_calibration-0.0.1-py2-none-any.whl",
        "has_sig": false,
        "md5_digest": "a6fb13aa539f2f9fb935891c984ad176",
        "packagetype": "bdist_wheel",
        "python_version": "py2",
        "requires_python": ">=3.6",
        "size": 5369,
        "upload_time": "2021-04-16T21:28:54",
        "upload_time_iso_8601": "2021-04-16T21:28:54.166078Z",
        "url": "https://files.pythonhosted.org/packages/4c/02/6c80ed89488f93b3a513439e46ff88bac9d98491a08e333ceff1315128e4/probability_calibration-0.0.1-py2-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "17f9ee61d78bf11ccab07df392e6fe22c23e10f1b96303bf24275ca1115b571b",
          "md5": "3408d2d54b0d5b6a3ac108a9e88d19d8",
          "sha256": "60805ff91d2bf2ca7283fb848b11a67c32b11d824c2a0876ce0382f75233c958"
        },
        "downloads": -1,
        "filename": "probability-calibration-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3408d2d54b0d5b6a3ac108a9e88d19d8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 4534,
        "upload_time": "2021-04-16T21:28:56",
        "upload_time_iso_8601": "2021-04-16T21:28:56.162866Z",
        "url": "https://files.pythonhosted.org/packages/17/f9/ee61d78bf11ccab07df392e6fe22c23e10f1b96303bf24275ca1115b571b/probability-calibration-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4c026c80ed89488f93b3a513439e46ff88bac9d98491a08e333ceff1315128e4",
        "md5": "a6fb13aa539f2f9fb935891c984ad176",
        "sha256": "0b096f8c4362c9f9248d34042c396418e7e8390f0f34e336f80fc8a98629cd3e"
      },
      "downloads": -1,
      "filename": "probability_calibration-0.0.1-py2-none-any.whl",
      "has_sig": false,
      "md5_digest": "a6fb13aa539f2f9fb935891c984ad176",
      "packagetype": "bdist_wheel",
      "python_version": "py2",
      "requires_python": ">=3.6",
      "size": 5369,
      "upload_time": "2021-04-16T21:28:54",
      "upload_time_iso_8601": "2021-04-16T21:28:54.166078Z",
      "url": "https://files.pythonhosted.org/packages/4c/02/6c80ed89488f93b3a513439e46ff88bac9d98491a08e333ceff1315128e4/probability_calibration-0.0.1-py2-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "17f9ee61d78bf11ccab07df392e6fe22c23e10f1b96303bf24275ca1115b571b",
        "md5": "3408d2d54b0d5b6a3ac108a9e88d19d8",
        "sha256": "60805ff91d2bf2ca7283fb848b11a67c32b11d824c2a0876ce0382f75233c958"
      },
      "downloads": -1,
      "filename": "probability-calibration-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "3408d2d54b0d5b6a3ac108a9e88d19d8",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 4534,
      "upload_time": "2021-04-16T21:28:56",
      "upload_time_iso_8601": "2021-04-16T21:28:56.162866Z",
      "url": "https://files.pythonhosted.org/packages/17/f9/ee61d78bf11ccab07df392e6fe22c23e10f1b96303bf24275ca1115b571b/probability-calibration-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}