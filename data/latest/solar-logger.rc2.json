{
  "info": {
    "author": "Zach Sanson",
    "author_email": "zac@sanson.co.nz",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.10"
    ],
    "description": "# Solar Logger\n\n![slice3](https://user-images.githubusercontent.com/47839859/161478218-66dbca5d-5277-479e-8144-1c017c92fbd3.png)\n<!-- [![Pylint](https://github.com/WibblyGhost/solar_logger/actions/workflows/pylint.yml/badge.svg)](https://github.com/WibblyGhost/solar_logger/actions/workflows/pylint.yml)\n[![Docker Package Deployment](https://github.com/WibblyGhost/solar_logger/actions/workflows/docker-publish.yml/badge.svg)](https://github.com/WibblyGhost/solar_logger/actions/workflows/docker-publish.yml) -->\n[![Docker Package Deployment](https://github.com/WibblyGhost/solar_logger/actions/workflows/workflow.yml/badge.svg)](https://github.com/WibblyGhost/solar_logger/actions/workflows/workflow.yml)\n\n## Info\n\nThis project is a multi-step program which relies on a MQTT backend to read information from an Outback solar controller which sends statistics of current battery status, input voltages etc. This program subscribes to the MQTT broker to retrieve the information broadcast and deciphers the raw byte streams into a readable form. It then converts the data into points to allow insertion into a time series database (InfluxDB) where the data can be stored, modeled and queried. The database will link to a Grafana website which will graph, model and compare the data on a privately accessible site.\n\nThe program makes use of multi-threaded applications for receiving MQTT data packets and uploading the packets into InfluxDB. This is done by transferring packets from one thread into the other by the use of `Queues`. The MQTT listener and Influx `write_api` both run concurrently.\n\n## Docker Setup\n\nTo make installation easy there is a PowerShell file that will run the Docker pull from my hub repository, and run all basic configurations needed to start SolarLogger. If you want to customize images or run the other programs held within this repository I would recommend editing and using the `docker-compose.yml`. You can pull my Docker image using the image tag `wibblyghost/solar-logger`.\n\nBoth `docker-compose.yml` and `solar-logger-build.ps1` can be customized to your personal preferences.\n\n**Note:** To run the SolarLogger you require a `.env` file in the same directory that you run the `solar-logger-build.ps1` file, otherwise change the `$EnvFile` variable to reflect the true path.\n\n```powershell\n# NOTES\n# $(pwd) - Expands to working directory on Linux or Mac\n# ${pwd} - Expands to working directory on Windows IN POWERSHELL\n\n$CurrentDir = ${pwd}\n$EnvFile = \".env\"\n$IsFromDockerHub = $TRUE\n$VersionTag = \"latest\"\n$RestartMode = \"unless-stopped\"\n\n\nif (!(${IsFromDockerHub})) {\n    # Start by building an image of SolarLogger locally\n    docker build . -f solar.dockerfile -t solar-logger-local\n}\n\n# Before running the Docker images I would suggest creating the config and output volumes first\n# Otherwise the config.ini won't get copied across\nif (!(Test-Path -Path \"${CurrentDir}/docker-solar-logger/output\")) {\n    mkdir -p \"${CurrentDir}/docker-solar-logger/output\"\n}\nif (!(Test-Path -Path \"${CurrentDir}/docker-solar-logger/config\")) {\n    mkdir -p \"${CurrentDir}/docker-solar-logger/config\"\n}\n\n\n# CONFIG VOLUMES\n# docker volume create \\\ndocker volume create --driver local --opt type=none --opt device=\"${CurrentDir}/docker-solar-logger/config\" --opt o=bind SolarLogger-Config\n\n# OUTPUT VOLUMES\ndocker volume create --driver local --opt type=none --opt device=\"${CurrentDir}/docker-solar-logger/output\" --opt o=bind SolarLogger-Output\n\n\n# Run the Docker image with an environment file, output folder and config folder\nif (${IsFromDockerHub}) {\n    # If the image is built from Docker hub\n    docker run -d --name solar-logger --restart=\"${RestartMode}\" --env-file \"${EnvFile}\" --volume \"SolarLogger-Config:/app/config\" --volume \"SolarLogger-Output:/app/output\" wibblyghost/solar-logger:\"${VersionTag}\"\n} else {\n    # If the image is built locally\n    docker run -d --name solar-logger --restart=\"$RestartMode\" --env-file \"${EnvFile}\" --volume \"SolarLogger-Config:/app/config\" --volume \"SolarLogger-Output:/app/output\" solar-logger-local\n}\n```\n\nIn a recent update I've also added a script `influxdb-build.ps1` to pull and create a InfluxDB image through docker also.\n\n```PowerShell\n# NOTES\n# $(pwd) - Expands to working directory on Linux or Mac\n# ${pwd} - Expands to working directory on Windows IN POWERSHELL\n\n$CurrentDir = ${pwd}\n$RestartMode = \"unless-stopped\"\n\n\n# Before running the Docker images I would suggest creating the config and output volumes first\n# Otherwise the config.ini won't get copied across\nif (!(Test-Path -Path \"${CurrentDir}/docker-influxdb/data-volume\")) {\n    mkdir -p \"${CurrentDir}/docker-influxdb/data-volume\"\n}\nif (!(Test-Path -Path \"${CurrentDir}/docker-influxdb/config\")) {\n    mkdir -p \"${CurrentDir}/docker-influxdb/config\"\n}\n\n# Create docker volumes that mount to the current directory\n# /var/lib/influxdb2\ndocker volume create --driver local --opt type=none --opt device=\"${CurrentDir}/docker-influxdb/data-volume\" --opt o=bind InfluxDB-DataVolume\n# /etc/influxdb2\ndocker volume create --driver local --opt type=none --opt device=\"${CurrentDir}/docker-influxdb/config\" --opt o=bind InfluxDB-Config\n\n# Run the build command and start InfluxDB\ndocker run -d --name influx-db -p 8086:8086 -p 8088:8088 --restart=\"${RestartMode}\" --volume \"InfluxDB-DataVolume:/var/lib/influxdb2\" --volume \"InfluxDB-Config:/etc/influxdb2\" influxdb:2.1.1\n```\n\n## Logging\n\nAll programs below are implemented with a file logger which can be configured through the `config.ini` file, this can be used for program info or debugging purposes. If file logging is enabled all logs will be written in the `output` folder although if using within a Docker instance it will be written to `/docker-name/output` instead.\n\n**Note:** By default a `/docker-name/output` volume is created.\n\n## Solar Logger\n\n### Setup\n\nTo start, fill out the `.env` template file with personal secrets and copy them to the base directory. Then after running a Docker compose it will use the environmental variables to start the service and start writing data into Influx. You can run the Docker build commands either through the `docker-compose.yml` or `solar-logger-build.ps1` files and both will mount a config and output volume under `/docker-solar-logger/`.\n\n### Summary\n\nThe Solar Logger acts as a bridge between a **MQTT** broker and a *time series database* **InfluxDB**. It does this by subscribing to the MQTT broker, decoding the packets and uploading the results to Influx.\n\nThe program makes use of multi-threading to keep listening and writing services active concurrently. The MQTT and Influx methods both make use of threading through the `solar_logger` runtime.\n\n**Usage:** To use this program you must set up an Influx controller which connects to the Influx database and also set up a MQTT subscriber. The MQTT subscriber requires an Influx controller instance for it to run since it uses the controller to write data as it receives the data points. There are already pre-defined classes that I've create that will help you achieve this.\n\n### Code Run Through\n\n#### MQTT\n\nFirstly we create a new thread for the MQTT listening service to run on, then initialize the service with all required connection credentials. This can be done through the `SecretStore` and `MqttConnector` class.\n\nThen we can run the MQTT listening service by creating a *MQTT Client*.\n\n```python\nsecret_store = SecretStore(has_mqtt_access=True)\nmqtt_connector = MqttConnector(\n    secret_store=secret_store,\n)\nmqtt_client = mqtt_connector.get_mqtt_client()\n```\n\nThen you must start the listening service.\n\n```python\nmqtt_client.loop_start()\n```\n\n**Note:** `loop_start()` actually creates another thread since on-top of out already created `MQTT-Thread`. But due to the complexity of setting up MQTT's `read_loop()`, I've decided to keep the separate thread instead.\n\nFrom this point onwards the threads just works in the background, listening, decoding packets and pushing the packets onto a globally available `Queue`.\n\n**Notes:**\n`_on_connect()` runs when the MQTT subscriber firstly connects to the MQTT broker to choose what subscription to listen to.\n\n`_on_message()` runs every time the MQTT subscriber receives a message from the broker.\n\n#### Influx\n\nThis part of the program is much simpler than the MQTT listener service. Firstly we create another new thread for the InfluxDB service to run on, then initialize the service with all required connection credentials. This can be done through the `SecretStore` and `InfluxConnector` class.\n\n```python\nsecret_store = SecretStore(has_influx_access=True)\ninflux_connector = InfluxConnector(secret_store=secret_store)\n```\n\nSince InfluxDB doesn't require an ongoing connection we only need to make a connection when *writing* or *querying* the server.\n\nWe do however run a quick `health_check()` on creation to check the endpoint is alive.\n\n ```python\n influx_connector.health_check()\n ```\n\nFollowing the check we run a blocking loop which constantly checks for items in the `Queue` and writes the points to the Influx server.\n\n```python\nqueue_package: QueuePackage = THREADED_QUEUE.get(timeout=1.0)\ntry:\n    influx_connector.write_points(queue_package=queue_package)\n```\n\n\n## InfluxDB\n\n### Setup\n\nThis project comes with a mostly pre-built InfluxDB instance that you can run up or copy to a Docker server. This can be run through the `docker-compose.yml` or `influxdb-build.ps1` and customized to accept initial setup variables. All Influx configurations will be written to the folder `/docker-influxdb/`. *(If this is your first time running the `docker-compose.yml` I would suggest uncommenting the following code.)* After configuring InfluxDB all following Docker restarts should keep their configs and data through the mounted volumes in `/docker-influxdb/`.\n\n```yml\nInfluxDB:\n    # For first time setup use these variables\n    environment:\n      - DOCKER_INFLUXDB_INIT_MODE=$influx_mode\n      - DOCKER_INFLUXDB_INIT_USERNAME=$influx_username\n      - DOCKER_INFLUXDB_INIT_PASSWORD=$influx_password\n      - DOCKER_INFLUXDB_INIT_ORG=$influx_org\n      - DOCKER_INFLUXDB_INIT_BUCKET=$influx_bucket\n      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=$influx_token\n```\n\n## Influx Queries *(Docker Compose Only)*\n\n### Summary\n\nThis is a simple program that is meant to provide minimal user experience to a pythonic way to Query an Influx database.\n\nIt contains methods to allow for different aggregation, sorting, range fields and more. The program will handle creating and running the queries.\n\n**Usage:** To use the Influx query program you must run the Python in an interactive instance:\n\n`python -i .\\influx_query.py`\n\nUpon startup you will get the opportunity to create and modify queries. To start run `QueryBuilder.help()` for valid query commands. Then to generate a query string you create a QueryBuilder instance and add strings to your query like below:\n\nYou can currently apply the following fields to a query:\n\n* `from(bucket:bucket_name)`, field for choosing the source bucket **(required)**.\n* `range(start: -20m)`, field for choosing how long you want to poll the database **(required)**.\n* `append_filter(field_1, value_1, joiner, new_band)`, takes the following parameters; field type, field value, joiner *('AND' or 'OR')*, new line boolean *(adds a newline to the filter)*.\n* `append_aggregate(collection_window, aggregate_function)`, take the following parameters; how long to apply the aggregation over, type of aggregation function *(e.g. min/max)*.\n* `append_sort(field, desc)`, takes the following parameters; field to sort by, show in descending order.\n\nWhen running the query the program will take the query options from config.ini to decide what format to return the data in.\nWhen querying the Influx database you can use three data types to assign the result to, those being the following:\n\n* Raw CSV file\n* Flux file *(returns a Python dictionary)*\n* Stream file *(returns a FluxRecord object)*\n\n*Currently, the program doesn't support parsing of stream files but will handle writing CSV files and printing Flux files.*\n\nAfter building up a query you can submit the query by running `execute_query(query)`.\n\n## Configurations\n\nAll debugging and querying options can be changed through the config file. If file logging is set to false then the generated output will be set to only use standard output. The file logging uses rotational logging meaning that it will create a new log after a set file size has been reached.\n\n```ini\n[influx_debugger]\n; Logging levels: DEBUG, INFO, WARNING, ERROR, CRITICAL\ndebug_level     = INFO\nfile_logging    = true\n; Log rotation can be set to time_based or size_based\nlog_rotation    = size_based\nfile_location   = output/\nfile_name       = influx_logs.log\nformat          = %%(asctime)s, %%(name)s, %%(levelname)s, %%(message)s\ndateformat      = %%d/%%m/%%Y, %%H:%%M:%%S\n; Rotating file loggers require the following configs\nmax_file_no     = 5\ntime_cutover    = \"midnight\"\nmax_file_bytes  = 5242880\n\n\n[solar_debugger]\n;Logging levels: DEBUG, INFO, WARNING, ERROR, CRITICAL\ndebug_level     = INFO\nfile_logging    = true\n; Log rotation can be set to time_based or size_based\nlog_rotation    = size_based\nfile_location   = output/\nfile_name       = solar_logs.log\nformat          = %%(asctime)s, %%(name)s, %%(levelname)s, %%(message)s\ndateformat      = %%d/%%m/%%Y, %%H:%%M:%%S\n; Rotating file loggers require the following configs\nmax_file_no     = 5\ntime_cutover    = \"midnight\"\nmax_file_bytes  = 5242880\n\n\n[query_settings]\n; Can be either 'csv, 'flux' or 'stream'\nquery_mode      = flux\n; Following three values are only required for CSV's\ncsv_location    = output/\ncsv_name        = query_result.csv\ncsv_mode        = w\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/WibblyGhost/solar_logger",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "solar-logger",
    "package_url": "https://pypi.org/project/solar-logger/",
    "platform": null,
    "project_url": "https://pypi.org/project/solar-logger/",
    "project_urls": {
      "Bug Tracker": "https://github.com/WibblyGhost/solar_logger/issues",
      "Homepage": "https://github.com/WibblyGhost/solar_logger"
    },
    "release_url": "https://pypi.org/project/solar-logger/1.2.0/",
    "requires_dist": null,
    "requires_python": ">=3.10",
    "summary": "MQTT to Influx DB converter for Outback solar controller",
    "version": "1.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15584779,
  "releases": {
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "afd8b85258a442cdff6944987d94e26586b69e99a3289bf9caeb97a2929d56c0",
          "md5": "288f00262edeed00b20b3368962578ad",
          "sha256": "23e39a9e5865e4da901a09f0d91bed327668aef816d8545da1db20be9040a0c3"
        },
        "downloads": -1,
        "filename": "solar_logger-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "288f00262edeed00b20b3368962578ad",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.10",
        "size": 22709,
        "upload_time": "2022-09-17T00:08:14",
        "upload_time_iso_8601": "2022-09-17T00:08:14.622760Z",
        "url": "https://files.pythonhosted.org/packages/af/d8/b85258a442cdff6944987d94e26586b69e99a3289bf9caeb97a2929d56c0/solar_logger-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8abcc57cd608bdaf60b8139ddd687d85411bea80261d682b4f5f5f506ebd7040",
          "md5": "1f7f2b0c85f67a917bb763992a7d9862",
          "sha256": "fd3b95f18b5993cf65418547401ceba72c59b2e9c0d81f8eefed3cb033bc1c32"
        },
        "downloads": -1,
        "filename": "solar_logger-1.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "1f7f2b0c85f67a917bb763992a7d9862",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.10",
        "size": 23551,
        "upload_time": "2022-10-30T08:16:21",
        "upload_time_iso_8601": "2022-10-30T08:16:21.663462Z",
        "url": "https://files.pythonhosted.org/packages/8a/bc/c57cd608bdaf60b8139ddd687d85411bea80261d682b4f5f5f506ebd7040/solar_logger-1.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8abcc57cd608bdaf60b8139ddd687d85411bea80261d682b4f5f5f506ebd7040",
        "md5": "1f7f2b0c85f67a917bb763992a7d9862",
        "sha256": "fd3b95f18b5993cf65418547401ceba72c59b2e9c0d81f8eefed3cb033bc1c32"
      },
      "downloads": -1,
      "filename": "solar_logger-1.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "1f7f2b0c85f67a917bb763992a7d9862",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.10",
      "size": 23551,
      "upload_time": "2022-10-30T08:16:21",
      "upload_time_iso_8601": "2022-10-30T08:16:21.663462Z",
      "url": "https://files.pythonhosted.org/packages/8a/bc/c57cd608bdaf60b8139ddd687d85411bea80261d682b4f5f5f506ebd7040/solar_logger-1.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}