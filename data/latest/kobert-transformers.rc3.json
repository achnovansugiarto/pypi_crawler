{
  "info": {
    "author": "Jangwon Park",
    "author_email": "adieujw@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# KoBERT-Transformers\n\n`KoBERT` & `DistilKoBERT` on 🤗 Huggingface Transformers 🤗\n\nKoBERT 모델은 [공식 레포](https://github.com/SKTBrain/KoBERT)의 것과 동일합니다. 본 레포는 **Huggingface tokenizer의 모든 API를 지원**하기 위해서 제작되었습니다.\n\n## 🚨 중요! 🚨\n\n### 🙏 TL;DR\n\n1. `transformers` 는 `v3.0` 이상을 반드시 설치!\n2. `tokenizer`는 본 레포의 `kobert_transformers/tokenization_kobert.py`를 사용!\n\n### 1. Tokenizer 호환\n\n`Huggingface Transformers`가 `v2.9.0`부터 tokenization 관련 API가 일부 변경되었습니다. 이에 맞춰 기존의 `tokenization_kobert.py`를 상위 버전에 맞게 수정하였습니다.\n\n### 2. Embedding의 padding_idx 이슈\n\n이전부터 `BertModel`의 `BertEmbeddings`에서 `padding_idx=0`으로 **Hard-coding**되어 있었습니다. (아래 코드 참고)\n\n```python\nclass BertEmbeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n```\n\n그러나 Sentencepiece의 경우 기본값으로 `pad_token_id=1`, `unk_token_id=0`으로 설정이 되어 있고 (이는 KoBERT도 동일), 이를 그대로 사용하는 BertModel의 경우 원치 않은 결과를 가져올 수 있습니다.\n\nHuggingface에서도 최근에 해당 이슈를 인지하여 이를 수정하여 `v2.9.0`에 반영하였습니다. ([관련 PR #3793](https://github.com/huggingface/transformers/pull/3793)) config에 `pad_token_id=1` 을 추가 가능하여 이를 해결할 수 있게 하였습니다.\n\n```python\nclass BertEmbeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n```\n\n그러나 `v.2.9.0`에서 `DistilBERT`, `ALBERT` 등에는 이 이슈가 해결되지 않아 직접 PR을 올려 처리하였고 ([관련 PR #3965](https://github.com/huggingface/transformers/pull/3965)), **`v2.9.1`에 최종적으로 반영되어 배포되었습니다.**\n\n아래는 이전과 현재 버전의 차이점을 보여주는 코드입니다.\n\n```python\n# Transformers v2.7.0\n>>> from transformers import BertModel, DistilBertModel\n>>> model = BertModel.from_pretrained(\"monologg/kobert\")\n>>> model.embeddings.word_embeddings\nEmbedding(8002, 768, padding_idx=0)\n>>> model = DistilBertModel.from_pretrained(\"monologg/distilkobert\")\n>>> model.embeddings.word_embeddings\nEmbedding(8002, 768, padding_idx=0)\n\n\n### Transformers v2.9.1\n>>> from transformers import BertModel, DistilBertModel\n>>> model = BertModel.from_pretrained(\"monologg/kobert\")\n>>> model.embeddings.word_embeddings\nEmbedding(8002, 768, padding_idx=1)\n>>> model = DistilBertModel.from_pretrained(\"monologg/distilkobert\")\n>>> model.embeddings.word_embeddings\nEmbedding(8002, 768, padding_idx=1)\n```\n\n## KoBERT / DistilKoBERT on 🤗 Transformers 🤗\n\n### Dependencies\n\n- torch>=1.1.0\n- transformers>=3,<5\n\n### How to Use\n\n```python\n>>> from transformers import BertModel, DistilBertModel\n>>> bert_model = BertModel.from_pretrained('monologg/kobert')\n>>> distilbert_model = DistilBertModel.from_pretrained('monologg/distilkobert')\n```\n\n**Tokenizer를 사용하려면, [`kobert_transformers/tokenization_kobert.py`](https://github.com/monologg/KoBERT-Transformers/blob/master/kobert_transformers/tokenization_kobert.py) 파일을 복사한 후, `KoBertTokenizer`를 임포트하면 됩니다.**\n\n- KoBERT와 DistilKoBERT 모두 동일한 토크나이저를 사용합니다.\n- **기존 KoBERT의 경우 Special Token이 제대로 분리되지 않는 이슈**가 있어서 해당 부분을 수정하여 반영하였습니다. ([Issue link](https://github.com/SKTBrain/KoBERT/issues/11))\n\n```python\n>>> from tokenization_kobert import KoBertTokenizer\n>>> tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일\n>>> tokenizer.tokenize(\"[CLS] 한국어 모델을 공유합니다. [SEP]\")\n>>> ['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n>>> tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]'])\n>>> [2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]\n```\n\n## Kobert-Transformers (Pip library)\n\n[![PyPI](https://img.shields.io/pypi/v/kobert-transformers)](https://pypi.org/project/kobert-transformers/)\n[![license](https://img.shields.io/badge/license-Apache%202.0-red)](https://github.com/monologg/DistilKoBERT/blob/master/LICENSE)\n[![Downloads](https://pepy.tech/badge/kobert-transformers)](https://pepy.tech/project/kobert-transformers)\n\n- `tokenization_kobert.py`를 랩핑한 파이썬 라이브러리\n- KoBERT, DistilKoBERT를 Huggingface Transformers 라이브러리 형태로 제공\n- `v0.5.1`에서는 `transformers v3.0` 이상으로 기본 설치합니다. (`transformers v4.0` 까지는 이슈 없이 사용 가능)\n\n### Install Kobert-Transformers\n\n```bash\npip3 install kobert-transformers\n```\n\n### How to Use\n\n```python\n>>> import torch\n>>> from kobert_transformers import get_kobert_model, get_distilkobert_model\n>>> model = get_kobert_model()\n>>> model.eval()\n>>> input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n>>> attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n>>> token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n>>> sequence_output, pooled_output = model(input_ids, attention_mask, token_type_ids)\n>>> sequence_output[0]\ntensor([[-0.2461,  0.2428,  0.2590,  ..., -0.4861, -0.0731,  0.0756],\n        [-0.2478,  0.2420,  0.2552,  ..., -0.4877, -0.0727,  0.0754],\n        [-0.2472,  0.2420,  0.2561,  ..., -0.4874, -0.0733,  0.0765]],\n       grad_fn=<SelectBackward>)\n```\n\n```python\n>>> from kobert_transformers import get_tokenizer\n>>> tokenizer = get_tokenizer()\n>>> tokenizer.tokenize(\"[CLS] 한국어 모델을 공유합니다. [SEP]\")\n['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]']\n>>> tokenizer.convert_tokens_to_ids(['[CLS]', '▁한국', '어', '▁모델', '을', '▁공유', '합니다', '.', '[SEP]'])\n[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]\n```\n\n## Reference\n\n- [KoBERT](https://github.com/SKTBrain/KoBERT)\n- [DistilKoBERT](https://github.com/monologg/DistilKoBERT)\n- [Huggingface Transformers](https://github.com/huggingface/transformers)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/monologg/KoBERT-Transformers",
    "keywords": "distilkobert kobert bert pytorch transformers lightweight",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "kobert-transformers",
    "package_url": "https://pypi.org/project/kobert-transformers/",
    "platform": "",
    "project_url": "https://pypi.org/project/kobert-transformers/",
    "project_urls": {
      "Homepage": "https://github.com/monologg/KoBERT-Transformers"
    },
    "release_url": "https://pypi.org/project/kobert-transformers/0.5.1/",
    "requires_dist": [
      "torch (>=1.1.0)",
      "transformers (<5,>=3)",
      "sentencepiece (>=0.1.91)"
    ],
    "requires_python": ">=3.6",
    "summary": "Transformers library for KoBERT, DistilKoBERT",
    "version": "0.5.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10661145,
  "releases": {
    "0.4.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f36df4e21513c1f26cacd68c144a428ccaa90dd92d85985e878976ebbaf06624",
          "md5": "dc05ff5d8d3d4deb6886411c6b7165ba",
          "sha256": "518d622054ce0965c7853eb9a0710051b8db295f7f6c477f584473ba3f64330e"
        },
        "downloads": -1,
        "filename": "kobert_transformers-0.4.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dc05ff5d8d3d4deb6886411c6b7165ba",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 12308,
        "upload_time": "2020-05-14T11:33:08",
        "upload_time_iso_8601": "2020-05-14T11:33:08.819817Z",
        "url": "https://files.pythonhosted.org/packages/f3/6d/f4e21513c1f26cacd68c144a428ccaa90dd92d85985e878976ebbaf06624/kobert_transformers-0.4.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ea69aa41197a950299eebc1e2991fdb6a7d8d2fa80fa1efe8b580bffddaf43af",
          "md5": "ba05b79a0e42fb03b3523a77ca4e5e38",
          "sha256": "aa7760853be5d1ec8ab600b11f1b6626123583c3c3f2f9adc36a5bd7b2e2f0ac"
        },
        "downloads": -1,
        "filename": "kobert_transformers-0.5.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ba05b79a0e42fb03b3523a77ca4e5e38",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 12411,
        "upload_time": "2021-06-16T09:09:15",
        "upload_time_iso_8601": "2021-06-16T09:09:15.099044Z",
        "url": "https://files.pythonhosted.org/packages/ea/69/aa41197a950299eebc1e2991fdb6a7d8d2fa80fa1efe8b580bffddaf43af/kobert_transformers-0.5.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "36043b41292198e1c7429c2104dcb05b8912ddb18582c8021b324d233313a807",
          "md5": "33625e69e5551325dbe9edbcb7151136",
          "sha256": "4aba0fa8f4f1eb477c5da91962090fcc958158d6e45bffe6e56e5d90c38bb578"
        },
        "downloads": -1,
        "filename": "kobert-transformers-0.5.1.tar.gz",
        "has_sig": false,
        "md5_digest": "33625e69e5551325dbe9edbcb7151136",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 7792,
        "upload_time": "2021-06-16T09:09:16",
        "upload_time_iso_8601": "2021-06-16T09:09:16.527051Z",
        "url": "https://files.pythonhosted.org/packages/36/04/3b41292198e1c7429c2104dcb05b8912ddb18582c8021b324d233313a807/kobert-transformers-0.5.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.1rc1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3c2cb211dd166554838054041c2fae999ed31997b8faa8e2766309a202719a57",
          "md5": "f7aad9919e55fe92cf2043062dfebcba",
          "sha256": "fc787cd692fec40d27ff69b825681a262b4e1c0dea497f9f59708c366193359e"
        },
        "downloads": -1,
        "filename": "kobert_transformers-0.5.1rc1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f7aad9919e55fe92cf2043062dfebcba",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 12448,
        "upload_time": "2021-06-16T09:07:01",
        "upload_time_iso_8601": "2021-06-16T09:07:01.925482Z",
        "url": "https://files.pythonhosted.org/packages/3c/2c/b211dd166554838054041c2fae999ed31997b8faa8e2766309a202719a57/kobert_transformers-0.5.1rc1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0380cd9a474ebb3ed7e859b2e56fcc70be1b259e0d84b6506387446526409210",
          "md5": "f0ac21754e019d861e9267a6519560af",
          "sha256": "ebbdc8bec199262df8ac9559d6b90aa200e6485eaba11af1368307aa7415ec9c"
        },
        "downloads": -1,
        "filename": "kobert-transformers-0.5.1rc1.tar.gz",
        "has_sig": false,
        "md5_digest": "f0ac21754e019d861e9267a6519560af",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 7808,
        "upload_time": "2021-06-16T09:07:03",
        "upload_time_iso_8601": "2021-06-16T09:07:03.489748Z",
        "url": "https://files.pythonhosted.org/packages/03/80/cd9a474ebb3ed7e859b2e56fcc70be1b259e0d84b6506387446526409210/kobert-transformers-0.5.1rc1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ea69aa41197a950299eebc1e2991fdb6a7d8d2fa80fa1efe8b580bffddaf43af",
        "md5": "ba05b79a0e42fb03b3523a77ca4e5e38",
        "sha256": "aa7760853be5d1ec8ab600b11f1b6626123583c3c3f2f9adc36a5bd7b2e2f0ac"
      },
      "downloads": -1,
      "filename": "kobert_transformers-0.5.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ba05b79a0e42fb03b3523a77ca4e5e38",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 12411,
      "upload_time": "2021-06-16T09:09:15",
      "upload_time_iso_8601": "2021-06-16T09:09:15.099044Z",
      "url": "https://files.pythonhosted.org/packages/ea/69/aa41197a950299eebc1e2991fdb6a7d8d2fa80fa1efe8b580bffddaf43af/kobert_transformers-0.5.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "36043b41292198e1c7429c2104dcb05b8912ddb18582c8021b324d233313a807",
        "md5": "33625e69e5551325dbe9edbcb7151136",
        "sha256": "4aba0fa8f4f1eb477c5da91962090fcc958158d6e45bffe6e56e5d90c38bb578"
      },
      "downloads": -1,
      "filename": "kobert-transformers-0.5.1.tar.gz",
      "has_sig": false,
      "md5_digest": "33625e69e5551325dbe9edbcb7151136",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 7792,
      "upload_time": "2021-06-16T09:09:16",
      "upload_time_iso_8601": "2021-06-16T09:09:16.527051Z",
      "url": "https://files.pythonhosted.org/packages/36/04/3b41292198e1c7429c2104dcb05b8912ddb18582c8021b324d233313a807/kobert-transformers-0.5.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}