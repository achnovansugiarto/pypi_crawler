{
  "info": {
    "author": "Mihail Dungarov",
    "author_email": "deeplearnmd@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# PictureText\nPictureText converts a list of short documents to an interactive tree map with minimal code. It defaults to SBERT for text representation, leverages Hierarchical Agglomerative Clustering (HAC) for grouping and tree maps to visualize text interactively.\n\n<p align=\"center\">\n  <img src=\"assets/cover.gif\" width=1000>\n</p>\n\nGiven a corpus of short documents (think news headlines) it can group them into hierarchical groups, that semantically belong together. It also allows the reader to explore each group in more detail by going deeper into a hierarchy and dynamically pulling out of it when needed.\n\nThe approach is intended for grouping large sets of non-domain specific short texts. For instance: news headlines, natural language questions and social media posts would be good candidates.\n\n## Getting started\n\n### Installation\n```py\nconda create --name pt python=3.6\nconda install -n pt nb_conda_kernels\nconda activate pt\npip install picture_text\n```\n\n### A simple example\nConsider the default values and their result\n```py\ntxt=['The cat sits outside',\n     'A man is playing guitar',\n     'I love pasta',\n     'The new movie is awesome',\n     'The cat plays in the garden',\n     'A woman watches TV',\n     'The new movie is so great',\n     'Do you like pizza?',\n     'Burgers are my favorite',\n     'I like chips',\n     'I will have french fries with my burger'\n       ]\nfrom picture_text.picture_text import PictureText\n\n# initializing just sets the text corpus\npt = PictureText(txt) \n\n# Calling the method does the heavy lifting: \n# 1. HAC \n# 2. text embedding \npt() \n\n# This step puts it all together:\n# 1. converts HAC into a treemap format\n# 2. determines a summary for each cluster and \n# 3. draws & return a treemap\npt.make_picture() \n```\n<p align=\"left\">\n  <img src=\"assets/default_settings.png\" width=500>\n</p>\n\n### Demo\nCheckout the Colab notebook for interactive examples\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mTrwk9hYl7bXYUr7e5hbCzv7Bim9ML8Y?usp=sharing)\n\n## Outline of approach\n<p align=\"left\">\n  <img src=\"assets/solution_steps.png\" width=1000>\n</p>\n\n- Perform any required preprocessing to get to a list of document strings\n- Embed / Encode all documents with the method of choice, by default I use [SBERT](https://www.sbert.net)\n- Use HAC to get a “linkage” table of hierarchical assignments of each point to the rest of the data. Here I use [fastcluster](http://danifold.net/fastcluster.html), ward linkage by default.\n- Convert to layers for treemap. Iteratively create “layers” by selecting a set number of splits to each layer\n- Summarize. Generate a summary for each layer. In the default setting, I use the point closest to the average of the cluster. Using the average of the cluster to represent its centroid is used in a number of few-shot, unsupervised settings\n- Draw treemap. Use [plotly](https://plotly.com/)'s [treemap](https://plotly.com/python/treemaps/) for interactive visualization\n\n## Customization\n\nConsider the default values and their result\n```python\nfrom picture_text.picture_text import PictureText\npt = PictureText(txt)\npt(hac_method='ward', hac_metric='euclidean')\npt.make_picture(layer_depth = 6,\n                layer_min_size = 0.1,\n                layer_max_extension = 1,\n                treemap_average_score = None, \n                treemap_maxdepth=3,)\n```\n<p align=\"left\">\n  <img src=\"assets/default_settings.png\" width=500>\n</p>\n\n### Selecting Layer Settings\nChanging `layer_depth` parameter sets the number of layers produced by the split.\n```python\npt.make_picture(layer_depth = 1)\n```\n<p align=\"left\">\n  <img src=\"assets/layer_depth1.png\" width=500>\n</p>\n\nChanging `layer_min_size` parameters determines what is the minimal acceptable size of a new cluster for each layer. By default `layer_min_size` is 0.1 (or 10%) meaning if a layer has a cluster smaller than 10% we will try to find another cluster to add to the layer hoping that the next one will be bigger. We will do so up to increasing the relative number of additional clusters up to 1 (or 100%, `layer_max_extension` = 1). Increasing both of these significantly basically means that we get a lot more clusters a lot earlier.\n\n```python\npt.make_picture(layer_depth = 1,\n                layer_min_size = 0.9,\n                layer_max_extension = 3,\n                )\n```\n<p align=\"left\">\n  <img src=\"assets/min_size.png\" width=500>\n</p>\n\n### Selecting Clustering Settings\n\nThe defaults are the following\n\n```python\npt = PictureText(txt)\npt(hac_method='ward', hac_metric='euclidean')\n```\n\nHowever, those get fed directly into fastcluster, hence all choices from the fastcluster documentation are available here too.\n\n## BYO-NLP\nThe key features to this sort of approach are the embeddings as well as the method of multi-doc summarization. You can use your NLP tools of choice there.\n\n### Text embeddings\nThe default set of text embeddings is via [SBERT](https://www.sbert.net)'s `distilbert-base-nli-stsb-mean-tokens`.\n\n```python\nfrom picture_text.picture_text import sbert_encoder\npt = PictureText(txt)\npt(encoder=sbert_encoder)\n```\nHowever, any mapping of a list of text to encoding can be used instead.\n```py\ndef silly_encoder(text_list):\n    return [[1]]*len(text_list)\n\npt(encoder=silly_encoder)\npt.make_picture()\n```\n<p align=\"left\">\n  <img src=\"assets/silly_encoder.png\" width=500>\n</p>\n\n### Summarizer\nThe default sumary method is to take the cluster member closest to the cluster averag. However, any mapping of a list of texts and embeddings into a text summary can be used instead.\n```python\ndef silly_summarizer(txt,txt_embeddings):\n   return \"All the same to me\", 0\npt.make_picture(summarizer = silly_summarizer,)\n```\n<p align=\"left\">\n  <img src=\"assets/silly_summarizer.png\" width=500>\n</p>\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/md-experiments/picture_text",
    "keywords": "hierarchical agglomerative clustering,hac,treemap,interactive visualization,data visualization,deep learning,machine learning,transformers,sentence-transformers,BERT,SBERT,nlp,natural language processing,text,ai,ml",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "picture-text",
    "package_url": "https://pypi.org/project/picture-text/",
    "platform": "",
    "project_url": "https://pypi.org/project/picture-text/",
    "project_urls": {
      "Homepage": "https://github.com/md-experiments/picture_text"
    },
    "release_url": "https://pypi.org/project/picture-text/0.1.0/",
    "requires_dist": [
      "numpy (>=1.14.0)",
      "pandas (>=0.20.0)",
      "fastcluster (>=1.1.26)",
      "sentence-transformers (>=0.3.4)",
      "transformers (>=3.0.2)",
      "scipy (>=1.1.0)",
      "matplotlib (>=3.0.0)",
      "plotly (>=4.10.0)"
    ],
    "requires_python": "",
    "summary": "Interactive tree-maps for text corpora with SBERT & Hierarchical Clustering (HAC)",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8389092,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fa61d2749f92b65e7154d9cb7d4c4e9c12505f3240aad3a5c248860492c56da4",
          "md5": "1f401c1d1509b55266427771488d84a6",
          "sha256": "91a36bc4896c0e4a584c2a8e2e6889f56d5ceefe7bdfaa4e0ddaa62c4387bdbc"
        },
        "downloads": -1,
        "filename": "picture_text-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1f401c1d1509b55266427771488d84a6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 17783,
        "upload_time": "2020-10-11T21:09:09",
        "upload_time_iso_8601": "2020-10-11T21:09:09.263554Z",
        "url": "https://files.pythonhosted.org/packages/fa/61/d2749f92b65e7154d9cb7d4c4e9c12505f3240aad3a5c248860492c56da4/picture_text-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "12f6de894b1bc5f283df364a7d2c4825e9ee30237b38d6770def70995a1003a7",
          "md5": "3c75d571a70c7c680837b852c7fcbc2c",
          "sha256": "6b5d130e866acc15a88a02a5d0632db48fbbe5d48467f1cc811ad8f39dc5110d"
        },
        "downloads": -1,
        "filename": "picture_text-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3c75d571a70c7c680837b852c7fcbc2c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16261,
        "upload_time": "2020-10-11T21:09:11",
        "upload_time_iso_8601": "2020-10-11T21:09:11.652344Z",
        "url": "https://files.pythonhosted.org/packages/12/f6/de894b1bc5f283df364a7d2c4825e9ee30237b38d6770def70995a1003a7/picture_text-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fa61d2749f92b65e7154d9cb7d4c4e9c12505f3240aad3a5c248860492c56da4",
        "md5": "1f401c1d1509b55266427771488d84a6",
        "sha256": "91a36bc4896c0e4a584c2a8e2e6889f56d5ceefe7bdfaa4e0ddaa62c4387bdbc"
      },
      "downloads": -1,
      "filename": "picture_text-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1f401c1d1509b55266427771488d84a6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 17783,
      "upload_time": "2020-10-11T21:09:09",
      "upload_time_iso_8601": "2020-10-11T21:09:09.263554Z",
      "url": "https://files.pythonhosted.org/packages/fa/61/d2749f92b65e7154d9cb7d4c4e9c12505f3240aad3a5c248860492c56da4/picture_text-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "12f6de894b1bc5f283df364a7d2c4825e9ee30237b38d6770def70995a1003a7",
        "md5": "3c75d571a70c7c680837b852c7fcbc2c",
        "sha256": "6b5d130e866acc15a88a02a5d0632db48fbbe5d48467f1cc811ad8f39dc5110d"
      },
      "downloads": -1,
      "filename": "picture_text-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "3c75d571a70c7c680837b852c7fcbc2c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 16261,
      "upload_time": "2020-10-11T21:09:11",
      "upload_time_iso_8601": "2020-10-11T21:09:11.652344Z",
      "url": "https://files.pythonhosted.org/packages/12/f6/de894b1bc5f283df364a7d2c4825e9ee30237b38d6770def70995a1003a7/picture_text-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}