{
  "info": {
    "author": "Francesco Saverio Zuppichini & Francesco Cicala",
    "author_email": "francesco.zuppichini@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "```python\n%load_ext autoreload\n%autoreload 2\n```\n\n# Glasses ðŸ˜Ž\n\n![alt](https://github.com/FrancescoSaverioZuppichini/glasses/blob/develop/docs/_static/images/background.png?raw=true)\n\n[![codecov](https://codecov.io/gh/FrancescoSaverioZuppichini/glasses/branch/develop/graph/badge.svg)](https://codecov.io/gh/FrancescoSaverioZuppichini/glasses)\n\nCompact, concise and customizable \ndeep learning computer vision library\n\n**So far I have the [following](#pretrained-models) pretrainde weights. I am working on porting more. They are hosted on GitHub if < 100MB and on AWS (thaks to Francis Ukpeh) if > 100MB.***\n\nDoc is [here](https://francescosaveriozuppichini.github.io/glasses/index.html)\n\n## TL;TR\n\nThis library has\n\n- human readable code, no *research code*\n- common component are shared across [models](#Models)\n- [same APIs](#classification) for all models (you learn them once and they are always the same)\n- clear and easy to use model constomization (see [here](#block))\n- [classification](#classification) and [segmentation](#segmentation) \n- emoji in the name ;)\n\nArchitectures implemented so far:\n\n- [Training data-efficient image transformers & distillation through attention](https://arxiv.org/pdf/2010.11929.pdf)\n- [Vision Transformer -  An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale](https://arxiv.org/pdf/2010.11929.pdf)\n- [ResNeSt: Split-Attention Networks](https://arxiv.org/abs/2004.08955) \n- [AlexNet-  ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n- [DenseNet - Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)\n- [EfficientNet - EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)\n- [EfficientNetLite - Higher accuracy on vision models with EfficientNet-Lite](https://blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-efficientnet-lite.html)\n- [FishNet - FishNet: A Versatile Backbone for Image, Region, and Pixel Level Prediction\n](https://arxiv.org/abs/1901.03495)\n- [MobileNet - MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/pdf/1801.04381.pdf)\n- [RegNet - Designing Network Design Spaces](https://arxiv.org/abs/2003.13678)\n- [ResNet - Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n- [ResNetD - Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/pdf/1812.01187.pdf)\n- [ResNetXt - Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf)\n- [SEResNet - Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n- [VGG - Very Deep Convolutional Networks For Large-scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf)\n- [WideResNet - Wide Residual Networks](https://arxiv.org/pdf/1605.07146.pdf)\n- [FPN - Feature Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144)\n- [PFPN - Panoptic Feature Pyramid Networks](https://arxiv.org/pdf/1901.02446.pdf)\n- [UNet - U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n- [Squeeze and Excitation - Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n- [ECA - ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks](https://arxiv.org/pdf/1910.03151.pdf)\n\n## Installation\n\nYou can install `glasses` using pip by running\n\n```\npip install git+https://github.com/FrancescoSaverioZuppichini/glasses\n```\n\n### Motivations\n\nAlmost all existing implementations of the most famous model are written with very bad coding practices, what today is called *research code*. I struggled myself to understand some of the implementations that in the end were just a few lines of code. \n\nMost of them are missing a global structure, they used tons of code repetition, they are not easily customizable and not tested. Since I do computer vision for living, so I needed a way to make my life easier.\n\n## Getting started\n\nThe API are shared across **all** models!\n\n\n```python\nimport torch\nfrom glasses.models import AutoModel, AutoConfig\nfrom torch import nn\n# load one model\nmodel = AutoModel.from_pretrained('resnet18')\ncfg = AutoConfig.from_name('resnet18')\nmodel.summary(device='cpu' ) # thanks to torchsummary\nAutoModel.models() # 'resnet18', 'resnet26', 'resnet26d', 'resnet34', 'resnet50', ...\n```\n\n### Interpretability\n\n\n```python\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom glasses.interpretability import GradCam, SaliencyMap\nfrom torchvision.transforms import Normalize\nr = requests.get('https://i.insider.com/5df126b679d7570ad2044f3e?width=700&format=jpeg&auto=webp')\nim = Image.open(BytesIO(r.content))\n# un normalize when done\npostprocessing = Normalize(-cfg.mean / cfg.std, (1.0 / cfg.std))\n# apply preprocessing\nx =  cfg.transform(im).unsqueeze(0)\n_ = model.interpret(x, using=GradCam(), postprocessing=postprocessing).show()\n```\n\n![alt](https://github.com/FrancescoSaverioZuppichini/glasses/blob/develop/docs/_static/images/grad_cam.png?raw=true)\n\n## Classification\n\n\n```python\nfrom glasses.models import ResNet\n# change activation\nResNet.resnet18(activation = nn.SELU)\n# change number of classes\nResNet.resnet18(n_classes=100)\n# freeze only the convolution weights\nmodel = ResNet.resnet18(pretrained=True)\nmodel.freeze(who=model.encoder)\n# get the last layer, usuful to hook to it if you want to get the embeeded vector\nmodel.encoder.layers[-1]\n# what about resnet with inverted residuals?\nfrom glasses.models.classification.efficientnet import InvertedResidualBlock\nResNet.resnet18(block = InvertedResidualBlock)\n```\n\n## Segmentation\n\n\n```python\nfrom functools import partial\nfrom glasses.models.segmentation.unet import UNet, UNetDecoder\n# vanilla Unet\nunet = UNet()\n# let's change the encoder\nunet = UNet.from_encoder(partial(AutoModel.from_name, 'efficientnet_b1'))\n# mmm I want more layers in the decoder!\nunet = UNet(decoder=partial(UNetDecoder, widths=[256, 128, 64, 32, 16]))\n# maybe resnet was better\nunet = UNet(encoder=lambda **kwargs: ResNet.resnet26(**kwargs).encoder)\n# same API\nunet.summary(input_shape=(1,224,224))\n```\n\n### More examples\n\n\n```python\n# change the decoder part\nmodel = ResNet.resnet18(pretrained=True)\nmy_head = nn.Sequential(\n    nn.AdaptiveAvgPool2d((1,1)),\n    nn.Flatten(),\n    nn.Linear(model.encoder.widths[-1], 512),\n    nn.Dropout(0.2),\n    nn.ReLU(),\n    nn.Linear(512, 1000))\n\nmodel.head = my_head\n\nx = torch.rand((1,3,224,224))\nmodel(x).shape #torch.Size([1, 1000])\n```\n\n## Pretrained Models\n\n**I am currently working on the pretrained models and the best way to make them available**\n\nThis is a list of all the pretrained models available so far!. They are all trained on *ImageNet*.\n\nI used a `batch_size=64` and a GTX 1080ti to evaluale the models.\n\n|                        |    top1 |    top5 |     time |   batch_size |\n|:-----------------------|--------:|--------:|---------:|-------------:|\n| efficientnet_b3        | 0.82034 | 0.9603  | 199.599  |           64 |\n| regnety_032            | 0.81958 | 0.95964 | 136.518  |           64 |\n| deit_small_patch16_224 | 0.81082 | 0.95316 | 132.868  |           64 |\n| resnet50d              | 0.80492 | 0.95128 |  97.5827 |           64 |\n| cse_resnet50           | 0.80292 | 0.95048 | 108.765  |           64 |\n| efficientnet_b2        | 0.80126 | 0.95124 | 127.177  |           64 |\n| resnext101_32x8d       | 0.7921  | 0.94556 | 290.38   |           64 |\n| wide_resnet101_2       | 0.7891  | 0.94344 | 277.755  |           64 |\n| wide_resnet50_2        | 0.78464 | 0.94064 | 201.634  |           64 |\n| efficientnet_b1        | 0.7831  | 0.94096 |  98.7143 |           64 |\n| resnet152              | 0.7825  | 0.93982 | 186.191  |           64 |\n| regnetx_032            | 0.7792  | 0.93996 | 319.558  |           64 |\n| resnext50_32x4d        | 0.77628 | 0.9368  | 114.325  |           64 |\n| regnety_016            | 0.77604 | 0.93702 |  96.547  |           64 |\n| efficientnet_b0        | 0.77332 | 0.93566 |  67.2147 |           64 |\n| resnet101              | 0.77314 | 0.93556 | 134.148  |           64 |\n| densenet161            | 0.77146 | 0.93602 | 239.388  |           64 |\n| resnet34d              | 0.77118 | 0.93418 |  59.9938 |           64 |\n| densenet201            | 0.76932 | 0.9339  | 158.514  |           64 |\n| regnetx_016            | 0.76684 | 0.9328  |  91.7536 |           64 |\n| resnet26d              | 0.766   | 0.93188 |  70.6453 |           64 |\n| regnety_008            | 0.76238 | 0.93026 |  54.1286 |           64 |\n| resnet50               | 0.76012 | 0.92934 |  89.7976 |           64 |\n| densenet169            | 0.75628 | 0.9281  | 127.077  |           64 |\n| resnet26               | 0.75394 | 0.92584 |  65.5801 |           64 |\n| resnet34               | 0.75096 | 0.92246 |  56.8985 |           64 |\n| regnety_006            | 0.75068 | 0.92474 |  55.5611 |           64 |\n| regnetx_008            | 0.74788 | 0.92194 |  57.9559 |           64 |\n| densenet121            | 0.74472 | 0.91974 | 104.13   |           64 |\n| deit_tiny_patch16_224  | 0.7437  | 0.91898 |  66.662  |           64 |\n| vgg19_bn               | 0.74216 | 0.91848 | 169.357  |           64 |\n| regnety_004            | 0.73766 | 0.91638 |  68.4893 |           64 |\n| regnetx_006            | 0.73682 | 0.91568 |  81.4703 |           64 |\n| vgg16_bn               | 0.73476 | 0.91536 | 150.317  |           64 |\n| vgg19                  | 0.7236  | 0.9085  | 155.851  |           64 |\n| regnetx_004            | 0.72298 | 0.90644 |  58.0049 |           64 |\n| vgg16                  | 0.71628 | 0.90368 | 135.398  |           64 |\n| vgg13_bn               | 0.71618 | 0.9036  | 129.077  |           64 |\n| vgg11_bn               | 0.70408 | 0.89724 |  86.9459 |           64 |\n| vgg13                  | 0.69984 | 0.89306 | 116.052  |           64 |\n| regnety_002            | 0.6998  | 0.89422 |  46.804  |           64 |\n| resnet18               | 0.69644 | 0.88982 |  46.2029 |           64 |\n| vgg11                  | 0.68872 | 0.88658 |  79.4136 |           64 |\n| regnetx_002            | 0.68658 | 0.88244 |  45.9211 |           64 |\n\nAssuming you want to load `efficientnet_b1`:\n\n\n```python\nfrom glasses.models import EfficientNet, AutoModel, AutoConfig\n\n# load it using AutoModel\nmodel = AutoModel.from_pretrained('efficientnet_b1')\n# or from its own class\nmodel = EfficientNet.efficientnet_b1(pretrained=True)\n# you may also need to get the correct transformation that must be applied on the input\ncfg = AutoConfig.from_name('efficientnet_b1')\ntransform = cfg.transform\n```\n\n    INFO:root:Loaded efficientnet_b1 pretrained weights.\n    INFO:root:Loaded efficientnet_b1 pretrained weights.\n\n\nIn this case, `transform` is \n\n```\nCompose(\n    Resize(size=240, interpolation=PIL.Image.BICUBIC)\n    CenterCrop(size=(240, 240))\n    ToTensor()\n    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n)\n```\n\n## Deep Customization\n\nAll models are composed by sharable parts:\n- `Block`\n- `Layer`\n- `Encoder`\n- `Head`\n- `Decoder`\n\n### Block\n\nEach model has its building block, they are noted by `*Block`. In each block, all the weights are in the `.block` field. This makes it very easy to customize one specific model. \n\n\n```python\nfrom glasses.models.classification.vgg import VGGBasicBlock\nfrom glasses.models.classification.resnet import ResNetBasicBlock, ResNetBottleneckBlock, ResNetBasicPreActBlock, ResNetBottleneckPreActBlock\nfrom glasses.models.classification.senet import SENetBasicBlock, SENetBottleneckBlock\nfrom glasses.models.classification.resnetxt import ResNetXtBottleNeckBlock\nfrom glasses.models.classification.densenet import DenseBottleNeckBlock\nfrom glasses.models.classification.wide_resnet import WideResNetBottleNeckBlock\nfrom glasses.models.classification.efficientnet import EfficientNetBasicBlock\n```\n\nFor example, if we want to add Squeeze and Excitation to the resnet bottleneck block, we can just\n\n\n```python\nfrom glasses.nn.att import SpatialSE\nfrom  glasses.models.classification.resnet import ResNetBottleneckBlock\n\nclass SEResNetBottleneckBlock(ResNetBottleneckBlock):\n    def __init__(self, in_features: int, out_features: int, squeeze: int = 16, *args, **kwargs):\n        super().__init__(in_features, out_features, *args, **kwargs)\n        # all the weights are in block, we want to apply se after the weights\n        self.block.add_module('se', SpatialSE(out_features, reduction=squeeze))\n\nSEResNetBottleneckBlock(32, 64)\n```\n\nThen, we can use the class methods to create the new models following the existing architecture blueprint, for example, to create `se_resnet50`\n\n\n```python\nResNet.resnet50(block=ResNetBottleneckBlock)\n```\n\nThe cool thing is each model has the same api, if I want to create a vgg13 with the `ResNetBottleneckBlock` I can just\n\n\n```python\nfrom glasses.models import VGG\nmodel = VGG.vgg13(block=SEResNetBottleneckBlock)\nmodel.summary()\n```\n\nSome specific model can require additional parameter to the block, for example `MobileNetV2` also required a `expansion` parameter so our `SEResNetBottleneckBlock` won't work. \n\n### Layer\n\nA `Layer` is a collection of blocks, it is used to stack multiple blocks together following some logic. For example, `ResNetLayer`\n\n\n```python\nfrom glasses.models.classification.resnet import ResNetLayer\n\nResNetLayer(64, 128, depth=2)\n```\n\n### Encoder\n\nThe encoder is what encoders a vector, so the convolution layers. It has always two very important parameters.\n\n- widths\n- depths\n\n\n**widths** is the wide at each layer, so how much features there are\n**depths** is the depth at each layer, so how many blocks there are\n\nFor example, `ResNetEncoder` will creates multiple `ResNetLayer` based on the len of `widths` and `depths`. Let's see some example.\n\n\n```python\nfrom glasses.models.classification.resnet import ResNetEncoder\n# 3 layers, with 32,64,128 features and 1,2,3 block each\nResNetEncoder(\n    widths=[32,64,128],\n    depths=[1,2,3])\n\n```\n\nAll encoders are subclass of `Encoder` that allows us to hook on specific stages to get the featuers. All you have to do is first call `.features` to notify the model you want to receive the features, and then pass an input.\n\n\n```python\nenc = ResNetEncoder()\nenc.features\nenc(torch.randn((1,3,224,224)))\nprint([f.shape for f in enc.features])\n```\n\n**Remember** each model has always a `.decoder` field\n\n\n```python\nfrom glasses.models import ResNet\n\nmodel = ResNet.resnet18()\nmodel.encoder.widths[-1]\n```\n\nThe encoder knows the number of output features, you can access them by\n\n#### Features\n\nEach encoder can return a list of features accessable by the `.features` field. You need to call it once before in order to notify the encoder we wish to also store the features\n\n\n```python\nfrom glasses.models.classification.resnet import ResNetEncoder\n\nx = torch.randn(1,3,224,224)\nenc = ResNetEncoder()\nenc.features # call it once\nenc(x)\nfeatures = enc.features # now we have all the features from each layer (stage)\n[print(f.shape) for f in features]\n# torch.Size([1, 64, 112, 112])\n# torch.Size([1, 64, 56, 56])\n# torch.Size([1, 128, 28, 28])\n# torch.Size([1, 256, 14, 14])\n```\n\n### Head\n\nHead is the last part of the model, it usually perform the classification\n\n\n```python\nfrom glasses.models.classification.resnet import ResNetHead\n\n\nResNetHead(512, n_classes=1000)\n```\n\n### Decoder\n\nThe decoder takes the last feature from the `.encoder` and decode it. This is usually done in `segmentation` models, such as Unet.\n\n\n```python\nfrom glasses.models.segmentation.unet import UNetDecoder\nx = torch.randn(1,3,224,224)\nenc = ResNetEncoder()\nenc.features # call it once\nx = enc(x)\nfeatures = enc.features\n# we need to tell the decoder the first feature size and the size of the lateral features\ndec = UNetDecoder(start_features=enc.widths[-1],\n                  lateral_widths=enc.features_widths[::-1])\nout = dec(x, features[::-1])\nout.shape\n```\n\n**This object oriented structure allows to reuse most of the code across the models**\n\n### Models\n\nThe models so far\n\nðŸ˜¥ = I don't have enough GPU RAM \n\n| name                   | Parameters   | Size (MB)   |\n|:-----------------------|:-------------|:------------|\n| resnet18               | 11,689,512   | 44.59       |\n| resnet26               | 15,995,176   | 61.02       |\n| resnet26d              | 16,014,408   | 61.09       |\n| resnet34               | 21,797,672   | 83.15       |\n| resnet34d              | 21,816,904   | 83.22       |\n| resnet50               | 25,557,032   | 97.49       |\n| resnet50d              | 25,576,264   | 97.57       |\n| resnet101              | 44,549,160   | 169.94      |\n| resnet152              | 60,192,808   | 229.62      |\n| resnet200              | 64,673,832   | 246.71      |\n| se_resnet18            | 11,776,552   | 44.92       |\n| se_resnet34            | 21,954,856   | 83.75       |\n| se_resnet50            | 28,071,976   | 107.09      |\n| se_resnet101           | 49,292,328   | 188.04      |\n| se_resnet152           | 66,770,984   | 254.71      |\n| cse_resnet18           | 11,778,592   | 44.93       |\n| cse_resnet34           | 21,958,868   | 83.77       |\n| cse_resnet50           | 28,088,024   | 107.15      |\n| cse_resnet101          | 49,326,872   | 188.17      |\n| cse_resnet152          | 66,821,848   | 254.91      |\n| resnext50_32x4d        | 25,028,904   | 95.48       |\n| resnext101_32x8d       | 88,791,336   | 338.71      |\n| resnext101_32x16d      | 194,026,792  | 740.15      |\n| resnext101_32x32d      | 468,530,472  | 1787.30     |\n| resnext101_32x48d      | 828,411,176  | 3160.14     |\n| regnetx_002            | 2,684,792    | 10.24       |\n| regnetx_004            | 5,157,512    | 19.67       |\n| regnetx_006            | 6,196,040    | 23.64       |\n| regnetx_008            | 7,259,656    | 27.69       |\n| regnetx_016            | 9,190,136    | 35.06       |\n| regnetx_032            | 15,296,552   | 58.35       |\n| regnety_002            | 3,162,996    | 12.07       |\n| regnety_004            | 4,344,144    | 16.57       |\n| regnety_006            | 6,055,160    | 23.10       |\n| regnety_008            | 6,263,168    | 23.89       |\n| regnety_016            | 11,202,430   | 42.73       |\n| regnety_032            | 19,436,338   | 74.14       |\n| resnest14d             | 10,611,688   | 40.48       |\n| resnest26d             | 17,069,448   | 65.11       |\n| resnest50d             | 27,483,240   | 104.84      |\n| resnest50d_1s4x24d     | 25,677,000   | 97.95       |\n| resnest50d_4s2x40d     | 30,417,592   | 116.03      |\n| resnest101e            | 48,275,016   | 184.15      |\n| resnest200e            | 70,201,544   | 267.80      |\n| resnest269e            | 7,551,112    | 28.81       |\n| wide_resnet50_2        | 68,883,240   | 262.77      |\n| wide_resnet101_2       | 126,886,696  | 484.03      |\n| densenet121            | 7,978,856    | 30.44       |\n| densenet169            | 14,149,480   | 53.98       |\n| densenet201            | 20,013,928   | 76.35       |\n| densenet161            | 28,681,000   | 109.41      |\n| fishnet99              | 16,630,312   | 63.44       |\n| fishnet150             | 24,960,808   | 95.22       |\n| vgg11                  | 132,863,336  | 506.83      |\n| vgg13                  | 133,047,848  | 507.54      |\n| vgg16                  | 138,357,544  | 527.79      |\n| vgg19                  | 143,667,240  | 548.05      |\n| vgg11_bn               | 132,868,840  | 506.85      |\n| vgg13_bn               | 133,053,736  | 507.56      |\n| vgg16_bn               | 138,365,992  | 527.82      |\n| vgg19_bn               | 143,678,248  | 548.09      |\n| efficientnet_b0        | 5,288,548    | 20.17       |\n| efficientnet_b1        | 7,794,184    | 29.73       |\n| efficientnet_b2        | 9,109,994    | 34.75       |\n| efficientnet_b3        | 12,233,232   | 46.67       |\n| efficientnet_b4        | 19,341,616   | 73.78       |\n| efficientnet_b5        | 30,389,784   | 115.93      |\n| efficientnet_b6        | 43,040,704   | 164.19      |\n| efficientnet_b7        | 66,347,960   | 253.10      |\n| efficientnet_b8        | ðŸ˜¥           | ðŸ˜¥          |\n| efficientnet_l2        | ðŸ˜¥           | ðŸ˜¥          |\n| efficientnet_lite0     | 4,652,008    | 17.75       |\n| efficientnet_lite1     | 5,416,680    | 20.66       |\n| efficientnet_lite2     | 6,092,072    | 23.24       |\n| efficientnet_lite3     | 8,197,096    | 31.27       |\n| efficientnet_lite4     | 13,006,568   | 49.62       |\n| vit_small_patch16_224  | 48,602,344   | 185.40      |\n| vit_base_patch16_224   | 86,415,592   | 329.65      |\n| vit_base_patch16_384   | 86,415,592   | 329.65      |\n| vit_base_patch32_384   | 88,185,064   | 336.40      |\n| vit_huge_patch16_224   | 631,823,080  | 2410.21     |\n| vit_huge_patch32_384   | 634,772,200  | 2421.46     |\n| vit_large_patch16_224  | 304,123,880  | 1160.14     |\n| vit_large_patch16_384  | 304,123,880  | 1160.14     |\n| vit_large_patch32_384  | 306,483,176  | 1169.14     |\n| deit_tiny_patch16_224  | 5,872,400    | 22.40       |\n| deit_small_patch16_224 | 22,359,632   | 85.30       |\n| deit_base_patch16_224  | 87,184,592   | 332.58      |\n| mobilenetv2            | 3,504,872    | 13.37       |\n| unet                   | 23,202,530   | 88.51       |\n| deit_base_patch16_384  | 87,184,592   | 332.58      |\n\n## Credits\n\nMost of the weights were trained by other people and adapted to glasses. It is worth cite\n\n- [pytorch-image-models](https://github.com/rwightman/pytorch-image-models)\n- [torchvision](hhttps://github.com/pytorch/vision)\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/FrancescoSaverioZuppichini/glasses",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "glasses",
    "package_url": "https://pypi.org/project/glasses/",
    "platform": "",
    "project_url": "https://pypi.org/project/glasses/",
    "project_urls": {
      "Homepage": "https://github.com/FrancescoSaverioZuppichini/glasses"
    },
    "release_url": "https://pypi.org/project/glasses/0.0.6/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Compact, concise and customizable deep learning computer vision",
    "version": "0.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9222687,
  "releases": {
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4626fe24208494ceaa51d345d10d91fc42f74c566594ba2cb50a5cffe23c6d54",
          "md5": "6c0fc8a198ec04d30ac27b42cc377dfd",
          "sha256": "1fc95e29458e5d8e752078bb3580bd5026d7a92584b811867aa4b869059266c6"
        },
        "downloads": -1,
        "filename": "glasses-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6c0fc8a198ec04d30ac27b42cc377dfd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 46086,
        "upload_time": "2020-10-26T22:29:22",
        "upload_time_iso_8601": "2020-10-26T22:29:22.207496Z",
        "url": "https://files.pythonhosted.org/packages/46/26/fe24208494ceaa51d345d10d91fc42f74c566594ba2cb50a5cffe23c6d54/glasses-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "887c366a223f1d15b92d83864e18fc308df3f7722f78db7627cab13b12203d43",
          "md5": "d60d474a727829708d4adce1e875a0a6",
          "sha256": "4fd60380b298bd2e8a860f6285ef810cf87f01d0c95782d18b19624b81d9204a"
        },
        "downloads": -1,
        "filename": "glasses-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "d60d474a727829708d4adce1e875a0a6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 37771,
        "upload_time": "2020-10-26T22:29:24",
        "upload_time_iso_8601": "2020-10-26T22:29:24.414667Z",
        "url": "https://files.pythonhosted.org/packages/88/7c/366a223f1d15b92d83864e18fc308df3f7722f78db7627cab13b12203d43/glasses-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "33235925370ff0fc3d9fa61f2839c31bebec659af816fc6ed79333217eaedead",
          "md5": "7fb0eecf359964fc073a0e30b90601b6",
          "sha256": "a9d5df09d509e4417ee069f32d7adaf72b7739547cbd5481efab4afa8f031a1e"
        },
        "downloads": -1,
        "filename": "glasses-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7fb0eecf359964fc073a0e30b90601b6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 52621,
        "upload_time": "2020-10-30T14:50:49",
        "upload_time_iso_8601": "2020-10-30T14:50:49.123704Z",
        "url": "https://files.pythonhosted.org/packages/33/23/5925370ff0fc3d9fa61f2839c31bebec659af816fc6ed79333217eaedead/glasses-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d4eb3fd13926bed0514a4d1a8910fbb2c9b3102672be4c310b344f6f1e4644b",
          "md5": "3f10c4d6e3f1648565ea75ef67e30b66",
          "sha256": "92e8a7fba8e96814dab6b507499478bdf95c2c535e22685865034b65c4692ef8"
        },
        "downloads": -1,
        "filename": "glasses-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "3f10c4d6e3f1648565ea75ef67e30b66",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 44465,
        "upload_time": "2020-10-30T14:50:50",
        "upload_time_iso_8601": "2020-10-30T14:50:50.718785Z",
        "url": "https://files.pythonhosted.org/packages/5d/4e/b3fd13926bed0514a4d1a8910fbb2c9b3102672be4c310b344f6f1e4644b/glasses-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2da00179ba059640a8f6e43154c6fcf31c439db1a0c7c72093f927f61d850f17",
          "md5": "0aa4e9df6cd0d6e25241efbe2cfc7a4c",
          "sha256": "198f2c9bd19c946c9bbfc64eb2badcbf9f012c6a2efafc8646c9c12350576cab"
        },
        "downloads": -1,
        "filename": "glasses-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0aa4e9df6cd0d6e25241efbe2cfc7a4c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 63955,
        "upload_time": "2020-12-26T15:59:34",
        "upload_time_iso_8601": "2020-12-26T15:59:34.336105Z",
        "url": "https://files.pythonhosted.org/packages/2d/a0/0179ba059640a8f6e43154c6fcf31c439db1a0c7c72093f927f61d850f17/glasses-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5b0431e6f9ba756456b9aaf3c0e5e91eb5775328f663cd0966c5a08dcd910951",
          "md5": "6c50b9fe02efbb5ba56141ba03aad18e",
          "sha256": "2111af96392b593a91f0cb07252ea9b62e1a5702b5be8e8c9453bdd6049cf5d8"
        },
        "downloads": -1,
        "filename": "glasses-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "6c50b9fe02efbb5ba56141ba03aad18e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 57999,
        "upload_time": "2020-12-26T15:59:35",
        "upload_time_iso_8601": "2020-12-26T15:59:35.600025Z",
        "url": "https://files.pythonhosted.org/packages/5b/04/31e6f9ba756456b9aaf3c0e5e91eb5775328f663cd0966c5a08dcd910951/glasses-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2ad56af5533745919800349f4ea7fcc4fbe296a69f2eb0d28159aeba3effa733",
          "md5": "2ea8b29c7877a135a27b77ebe1946532",
          "sha256": "40b518300a025bf3fcdcfc5591ad7dcd3e8c6fb70f57ac9f5d18949f8e7b94b5"
        },
        "downloads": -1,
        "filename": "glasses-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2ea8b29c7877a135a27b77ebe1946532",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 76622,
        "upload_time": "2021-01-25T11:37:57",
        "upload_time_iso_8601": "2021-01-25T11:37:57.250313Z",
        "url": "https://files.pythonhosted.org/packages/2a/d5/6af5533745919800349f4ea7fcc4fbe296a69f2eb0d28159aeba3effa733/glasses-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a57b1fe68a45965d650f3ee829beac1901cebd33f4a3589966db9e31a25d0a99",
          "md5": "e530741e2bd978b3f3fea9f2ba69fdc7",
          "sha256": "a9bb8117b54e91b99aa92ad55ecdc4946a6a166f9b39afb6eba1110754d778ef"
        },
        "downloads": -1,
        "filename": "glasses-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "e530741e2bd978b3f3fea9f2ba69fdc7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 69674,
        "upload_time": "2021-01-25T11:37:58",
        "upload_time_iso_8601": "2021-01-25T11:37:58.313256Z",
        "url": "https://files.pythonhosted.org/packages/a5/7b/1fe68a45965d650f3ee829beac1901cebd33f4a3589966db9e31a25d0a99/glasses-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2ad56af5533745919800349f4ea7fcc4fbe296a69f2eb0d28159aeba3effa733",
        "md5": "2ea8b29c7877a135a27b77ebe1946532",
        "sha256": "40b518300a025bf3fcdcfc5591ad7dcd3e8c6fb70f57ac9f5d18949f8e7b94b5"
      },
      "downloads": -1,
      "filename": "glasses-0.0.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "2ea8b29c7877a135a27b77ebe1946532",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 76622,
      "upload_time": "2021-01-25T11:37:57",
      "upload_time_iso_8601": "2021-01-25T11:37:57.250313Z",
      "url": "https://files.pythonhosted.org/packages/2a/d5/6af5533745919800349f4ea7fcc4fbe296a69f2eb0d28159aeba3effa733/glasses-0.0.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a57b1fe68a45965d650f3ee829beac1901cebd33f4a3589966db9e31a25d0a99",
        "md5": "e530741e2bd978b3f3fea9f2ba69fdc7",
        "sha256": "a9bb8117b54e91b99aa92ad55ecdc4946a6a166f9b39afb6eba1110754d778ef"
      },
      "downloads": -1,
      "filename": "glasses-0.0.6.tar.gz",
      "has_sig": false,
      "md5_digest": "e530741e2bd978b3f3fea9f2ba69fdc7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 69674,
      "upload_time": "2021-01-25T11:37:58",
      "upload_time_iso_8601": "2021-01-25T11:37:58.313256Z",
      "url": "https://files.pythonhosted.org/packages/a5/7b/1fe68a45965d650f3ee829beac1901cebd33f4a3589966db9e31a25d0a99/glasses-0.0.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}