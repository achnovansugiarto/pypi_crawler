{
  "info": {
    "author": "Christine Allen",
    "author_email": "ca.magallen@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3"
    ],
    "description": "# fairMLHealth\nTools and tutorials for variation analysis in healthcare machine learning.\n\n## New in Version 1.0.0!\nWe reorganized the library to make things more intuitive and added some useful capabilities:\n* Evaluate measures for regression analysis\n* Stack stratified tables by cohort groups\n* Apply a single **compare** function to evaluate one or multiple models\n\n## Tool Contents\n- ### [Documentation and References](./docs/README.md)\n    - [Evaluation of Fairness](./docs/resources/Evaluating_Fairness.md)\n    - [Quick Reference of Measures](./docs/resources/Measures_QuickReference.md)\n    - [References and Resources](./docs/resources/References_and_Resources.md)\n    - [Our Publications](./docs/publications/README.md)\n\n- ### [Examples and Tutorials](examples_and_tutorials/README.md)\n    - Tutorials for measuring and analyzing fairness as it applies to machine learning\n    - Examples for using the templates and tools\n\n- ### [FairMLHealth](fairmlhealth/README.md)\n    - **Measure**:\n        - Measure healthcare-specific fairness metrics for classification and regression models.\n    - **Compare**:\n        - Compare protected attributes and other features of a model or several models for potential biases in outcomes.\n    - **Visualize**:\n        - Visualize biases across multiple models through helpful reports & customizable highlights.\n\n- ### [Templates](templates/README.md)\n    - Quickstart notebooks that serve as skeletons for your model analysis\n\n## Installation <a id=\"installation_instructions\"></a>\nInstalling with pip:\n\n    python -m pip install fairmlhealth\n\nInstalling directly from GitHub:\n\n    python -m pip install git+https://github.com/KenSciResearch/fairMLHealth\n\nInstalling from a local copy of the repo:\n\n    pip install <path_to_fairMLHealth_dir>\n\n### Troubleshooting Installation Issues\n\n*Trouble with the Installation?*\n     - Step 1: Verify that you're using a compatible version of python.\n     - Step 2: If step 1 does not resolve your issue, verify that all required packages are properly installed.\n     - Step 3: For some metrics, FairMLHealth relies on AIF360, which has a few known installation gotchas. If you are having trouble with your installation, first check [AIF360's Troubleshooting Tips](https://github.com/Trusted-AI/AIF360#troubleshooting).\n\nIf you are not able to resolve your issue through the troubleshooting tips above, please let us know through the [Discussion Board](https://github.com/KenSciResearch/fairMLHealth/discussions) or by submitting an issue using the [Issue Template](./docs/code_contributions/ISSUE_TEMPLATE.md) found in our [Documentation folder](./docs/README.md).\n\n## FairMLHealth Usage\nBelow are some quickstart examples of our most popular features. More information about these and other examples can be found in our [examples_and_tutorials](./examples_and_tutorials) folder! These specific examples are based on our ToolUsage notebooks, for which we've provided online access in Jupyter's nbviewer via the following links:\n* [Tool Usage for Binary Classification](https://nbviewer.jupyter.org/github/KenSciResearch/fairMLHealth/blob/master/examples_and_tutorials/Example-ToolUsage_BinaryClassification.ipynb)\n* [Tool Usage for Regression](https://nbviewer.jupyter.org/github/KenSciResearch/fairMLHealth/blob/master/examples_and_tutorials/Example-ToolUsage_Regression.ipynb)\n\nNote that while the examples below use pandas, the library is designed to accept either pandas objects or numpy arrays.\n### Example Setup\n\n```python\nfrom fairmlhealth import report, measure\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# First we'll create a semi-randomized dataframe with specific columns for our attributes of interest\nrng = np.random.RandomState(506)\nN = 240\nX = pd.DataFrame({'col1': rng.randint(1, 4, N),\n                  'col2': rng.randint(1, 75, N),\n                  'col3': rng.randint(0, 2, N),\n                  'gender': [0, 1]*int(N/2),\n                  'ethnicity': [1, 1, 0, 0]*int(N/4),\n                  'other': [1, 0, 1, 0, 1, 1, 0, 1]*int(N/8)\n                 })\n```\n\n### Generalized Reports - report.py\nAs in previous versions, the latest version of fairMLHealth has tools to create generalized reports of model bias and performance. *An important update starting in Version 1.0.0 is that all of these features are now contained in the **report.py** module (previously named model_comparison.py).*\n\n#### Comparing Binary Classifications\nThe primary reporting tool is now the **compare** function, which can be used to generate side-by-side comparisons for any number of models, and for either binary classifcation or for regression problems. Model performance metrics such as accuracy and precision (or MAE and RSquared for regression problems) are also provided to facilitate comparison. Below is an example output comparing the two example models defined above.\n\nA flagging protocol is applied by default to highlight any cells with values that are out of range.  This can be turned off by passing ***flag_oor = False*** to report.compare().\n\n``` python\n\n# Ceate a randomized target variable\ny = pd.Series(X['col3'].values + rng.randint(0, 2, N), name='Example_Target').clip(upper=1)\n\n# Third, we'll split the data and use it to train two generic models\nsplits = train_test_split(X, y, stratify=y, test_size=0.5, random_state=60)\nX_train, X_test, y_train, y_test = splits\n\nmodel_1 = BernoulliNB().fit(X_train, y_train)\nmodel_2 = DecisionTreeClassifier().fit(X_train, y_train)\n\n# Generate the report\nfairness_measures = report.compare(X_test, y_test, X_test['gender', model_1)\n\n```\nNote that the Equal Odds Ratio has been dropped from the example below. This because the false positive rate is approximately zero for both the entire dataset and for the privileged class, leading to a zero in the denominator of the False Positive Rate Ratio. The result is therefore undefined and cannot be compared in the Equal Odds Ratio.\n\n<img src=\"./docs/img/tool_examples/bin_oneModel.png\"\n     alt=\"single model compare example\"\n     width=\"800px\"\n    />\n\n\nThe **compare** tool can also be used to measure two different models or two different protected attributes. Protected attributes are measured separately and cannot yet be combined together with the **compare** tool, although they can be grouped as cohorts in the stratified tables [as shown below](#cohort).\n\n\n```python\n# Example with multiple models\nreport.compare(test_data = X_test,\n               targets = y_test,\n               protected_attr = X_test['gender'],\n               models = {'Any Name 1':model_1, 'Model 2':model_2})\n```\n<img src=\"./docs/img/tool_examples/bin_multiModel.png\"\n     alt=\"two model comparison example\"\n     width=\"90%\"\n     />\n\n```python\n# Example with different protected attributes.\n# Note that the same model is passed with two different keys to clarify the column names.\nreport.compare(X_test, y_test,\n               [X_test['gender'], X_test['ethnicity']],\n               {'Gender':model_1, 'Ethnicity':model_1})\n```\n<img src=\"./docs/img/tool_examples/bin_multiAttribute.png\"\n     alt=\"multiple attribute comparison example\"\n     width=\"90%\"\n     />\n\n#### Comparing Regressions\n\nHere is an example applying the same function for a regression model. Note that the \"fair\" range to be used for evaluation of regression metrics does requires judgement on the part of the user. Default ranges have been set to [0.8, 1.2] for ratios, 10% of the available target range for *Mean Prediction Difference*, and 10% of the available MAE range for *MAE Difference*. If the default flags do not meet your needs, they can be turned off by passing ***flag_oor = False*** to report.compare(). More information is available in our [Evaluating Fairness Documentation](./docs/resources/Evaluating_Fairness.md#regression_ranges).\n\n``` python\nfrom sklearn.linear_model import LinearRegression\n\n# Create a randomized target variable. In this case we'll add some correlation with existing variables\ny = pd.Series((X['col3']+X['gender']).values + rng.uniform(0, 6, N), name='Example_Continuous_Target')\n\n# Split the data and use it to train a regression model\nsplits = train_test_split(X, y, test_size=0.5, random_state=42)\nX_train, X_test, y_train, y_test = splits\nregression_model = LinearRegression().fit(X_train, y_train)\n\n# Generate the report.\n# Note that for regression models, the prediction type (pred_type) must be declared as such.\nreport.compare(X_test, y_test, X_test['gender'], regression_model, pred_type=\"regression\")\n```\n<img src=\"./docs/img/tool_examples/reg_oneModel.png\"\n     alt=\"regression comparison example\"\n     width=\"90%\"\n     />\n\n``` python\n# Display the same report with no flags and no model performance\nreport.compare(X_test, y_test, X_test['gender'], regression_model, pred_type=\"regression\",\n                flag_oor=False, skip_performance=True))\n```\n<img src=\"./docs/img/tool_examples/reg_skipPerformance.png\"\n     alt=\"regression comparison example with skipped performance\"\n     width=\"90%\"\n     />\n\n\n### Detailed Analyses - measure.py\n\nFairMLHealth also provides tools for detailed analysis of model variance by way of stratified data, performance, and bias tables. Beyond evaluating fairness, these tools are intended for flexible use in any generic assessment of model bais to evaluate multiple features at once. *An important update starting in Version 1.0.0 is that all of these features are now contained in the **measure.py** module (previously named reports.py).*\n\nAll tables display a summary row for \"All Features, All Values\". This summary can be turned off by passing ***add_overview=False*** to measure.data().\n\n#### Stratified Data Tables\n\nThe stratified data table can be used to evaluate data against one or multiple targets. Two methods are available for identifying which features to assess, as shown in the first example below.\n\n```python\n# The following two function calls will produce the same output table shown below\n\n# Arguments Option 1: pass full set of data, subsetting with *features* argument\nmeasure.data(X_test, y_test, features=['gender'])\n\n# Arguments Option 2: pass the data subset of interest without using the *features* argument\nmeasure.data(X_test[['gender']], y_test)\n\n```\n<img src=\"./docs/img/tool_examples/data_table.png\"\n     alt=\"data table example\"\n     width=\"90%\"\n     />\n\n```python\n# Display a similar report for multiple targets, dropping the summary row\nmeasure.data(X=X_test, # used to define rows\n             Y=X_test, # used to define columns\n             features=['gender', 'col1'], # optional subset of X\n             targets=['col2', 'col3'], # optional subset of Y\n             add_overview=False # turns off \"All Features, All Values\" row\n             )\n\n```\n<img src=\"./docs/img/tool_examples/data_table_multiTarget_noOverview.png\"\n     alt=\"multi-target data table example\"\n     width=\"90%\"\n     />\n\n#### Stratified Performance Tables\n\nThe stratified performance table evaluates model performance specific to each feature-value subset. These tables are compatible with both classification and regression models. For classification models with the *predict_proba()* method, additional ROC_AUC and PR_AUC values will be included if possible.\n\n```python\n# Binary classification performance table with probabilities included\nmeasure.performance(X_test[['gender']],\n                    y_true=y_test,\n                    y_pred=model_1.predict(X_test),\n                    y_prob=model_1.predict_proba(X_test)[:,1])\n```\n<img src=\"./docs/img/tool_examples/bin_performance_probs.png\"\n     alt=\"performance table example, binary classification\"\n     width=\"90%\"\n     />\n\n```python\n# Regression example\nmeasure.performance(X_test[['gender']],\n                    y_true=y_test,\n                    y_pred=regression_model.predict(X_test),\n                    pred_type=\"regression\")\n```\n<img src=\"./docs/img/tool_examples/reg_performance.png\"\n     alt=\"performance table example, regression\"\n     width=\"90%\"\n     />\n\n#### Stratified Bias Tables\n\nThe stratified bias analysis table apply fairness-related metrics for each feature-value pair. It assumes a given feature-value as the \"privileged\" group relative to all other possible values for the feature. For example, row **2** in the table below displays measures for **\"col1\"** with a value of **\"2\"**. For this row, \"2\" is considered to be the privileged group, while all other non-null values (namely \"1\" and \"3\") are considered unprivileged.\n\nTo simplify the table, fairness measures have been reduced to their component parts. For example, the Equal Odds Ratio has been reduced to the True Positive Rate (TPR) Ratio and False Positive Rate (FPR) Ratio.\n\n```python\n# Binary classification example\n# Note that flag_oor is set to False by default for this feature\nmeasure.bias(X_test[['gender', 'col1']], y_test, model_1.predict(X_test))\n```\n<img src=\"./docs/img/tool_examples/bin_bias_noFlag.png\"\n     alt=\"bias table example, binary classification\"\n     width=\"90%\"\n     />\n\nNote that the *flag* function is compatible with both **measure.bias()** and **measure.summary()** (which is demonstrated in the next section). However, to enable colored cells the tool returns a pandas Styler rather than a DataTable. For this reason, *flag_oor* is set to False by default (as shown in the example above). Flagging can be turned on by passing *flag_oor=True* to either function. As an added feature, optional custom ranges can be passed to either **measure.bias()** or **measure.summary()** to facilitate regression evaluation, shown in the example below.\n\n```python\n# Custom \"fair\" ranges may be passed as dictionaries of tuples whose keys are case-insensive measure names\nmy_ranges = {'MAE Difference':(-0.1, 0.1),  'mean prediction difference':(-2, 2)}\n\n# Note that flag_oor is set to False by default for this feature\nmeasure.bias(X_test[['gender', 'col1']],\n             y_test,\n             regression_model.predict(X_test),\n             pred_type=\"regression\",\n             flag_oor=True,\n             custom_ranges=my_ranges)\n```\n<img src=\"./docs/img/tool_examples/reg_bias_customBounds.png\"\n     alt=\"bias table example, regression\"\n     width=\"90%\"\n     />\n\n\n#### Summary Table\n\nThe **measure** module also contains a summary function that works similarly to report.compare(). While it can only be applied to one model at a time, it can accept custom \"fair\" ranges, and accept cohort groups as will be [shown in the next section](#cohort).\n```python\n# Example summary output for the regression model with custom ranges\nmeasure.summary(X_test[['gender', 'col1']],\n                y_test,\n                regression_model.predict(X_test),\n                prtc_attr=X_test['gender'],\n                pred_type=\"regression\",\n                flag_oor=True,\n                custom_ranges={ 'mean prediction difference':(-0.5, 2)})\n```\n<img src=\"./docs/img/tool_examples/reg_summary_customBounds.png\"\n     alt=\"summary example\"\n     width=\"90%\"\n     />\n\n#### <a name=\"cohort\"></a>Analysis by Cohorts\n\nAll of the tables generated by measure.py can be passed an optional *cohort_labels* argument specifying additional labels for each observation by which the analysis should be grouped. Cohorts for which there is insufficient data to run the analysis are simply skipped.\n\n\n```python\n# Example of cohorts applied to bias function\ncohort_labels = X_test['gender']\nmeasure.bias(X_test['col3'], y_test, model_1.predict(X_test),\n                    flag_oor=True, cohort_labels=cohort_labels)\n```\n<img src=\"./docs/img/tool_examples/bin_bias_singleCohort.png\"\n     alt=\"bias table example with single cohorts\"\n     width=\"90%\"\n     />\n\n```python\n# Example of cohorts applied to summary function\n# Note that performance measures and flagging are turned off\nmeasure.summary(X_test[['col2']],\n                y_test,\n                model_1.predict(X_test),\n                prtc_attr=X_test['gender'],\n                pred_type=\"classification\",\n                flag_oor=False,\n                skip_performance=True,\n                cohort_labels=X_test[['ethnicity', 'col3']]\n               )\n```\n<img src=\"./docs/img/tool_examples/bin_summary_multiCohort.png\"\n     alt=\"summary example with multiple cohorts\"\n     width=\"90%\"\n     />\n\n\n## Additional Library Resources\nMore information about these and other examples can be found in our [examples_and_tutorials](./examples_and_tutorials) folder! These specific examples are based on our ToolUsage notebooks, for which we've provided online access in Jupyter's nbviewer via the following links:\n* [Tool Usage for Binary Classification](./examples_and_tutorials/Example-ToolUsage_BinaryClassification.ipynb)\n* [Tool Usage for Regression](./examples_and_tutorials/Example-ToolUsage_Regression.ipynb)\n\nFor a deep discussion of fairness evaluation, see [Evaluating Fairness](./docs/resources/Evaluating_Fairness.md) in our [documentation and resources](./docs/resources) section. In the same folder you'll find a [Measures QuickReference](./docs/resources/Measures_QuickReference.md), plus additional [References and Resources](./docs/resources/References_and_Resources.md)\n\nFor an active notebook demonstrating the fairness evaluation process, see the [Tutorial for Evaluating Fairness in Binary Classification](https://nbviewer.jupyter.org/github/KenSciResearch/fairMLHealth/blob/master/examples_and_tutorials/Tutorial-EvaluatingFairnessInBinaryClassification.ipynb) and [Tutorial for Evaluating Fairness in Regression](https://nbviewer.jupyter.org/github/KenSciResearch/fairMLHealth/blob/master/examples_and_tutorials/Tutorial-EvaluatingFairnessInRegression.ipynb) (nbviewer links), the notebooks for which are located in our [examples_and_tutorials](./examples_and_tutorials). These are best used with our [ICHI2021 FairnessInHealthcareML Slides.pdf](./docs/publications/ICHI2021-FairnessInHealthcareML-Slides.pdf), which can be found in the [Publications]((./docs/publications/) folder.\n\nTemplates are available in the [templates folder](./templates):\n* [Binary Classification Assessment Template](./templates/Template-BinaryClassificationAssessment.ipynb)\n* [Regression Assessment Template](./templates/Template-RegressionAssessment.ipynb)\n\n\n\n\n\n## Connect with Us!\nThis is a work in progress. By making this information as accessible as possible, we hope to promote an industry based on equity and empathy. But building that industry takes time, and it takes the support of the community. Please connect with us so that we can support each other to advance machine learning and healthcare!\n\n- For problems with the source code or documentation, please submit inquiries using our [Issue Template](./docs/code_contributions/ISSUE_TEMPLATE.md) or [Feature Request Template](./docs/code_contributions/FEATURE_REQUEST.md) through GitHub's [Issue Tracker](https://github.com/KenSciResearch/fairMLHealth/issues).\n- Other comments, ideas, inquiries, suggestions, feedback and requests are welcome through the [Discussion Page](https://github.com/KenSciResearch/fairMLHealth/discussions).\n- See the [Contributing Guidelines](./docs/code_contributions/CONTRIBUTING.md) for more information.\n\n## Citations\n### Repository\nAllen,  C.,  Ahmad,  M.A., Eckert, C.,  Hu,  J.,   Kumar,  V. , & Teredesai, A. (2020). _fairML-Health: Tools and tutorials for fairness evaluation in healthcare machine learning._ https://github.com/KenSciResearch/fairMLHealth.\n```\n@misc{fairMLHealth,\n    title={fairMLHealth: Tools and tutorials for fairness evaluation in healthcare machine learning.},\n    author={Allen, C. and Ahmad, M.A. and Eckert, C. and Hu, J. and Kumar, V. and Teredesai, A.},\n    year={2020},\n    publisher = {GitHub},\n    journal = {GitHub repository},\n    howpublished = {\\url{https://github.com/KenSciResearch/fairMLHealth}}\n}\n```\n\n### KDD2020 Tutorial Presentation\nAhmad, M.A., Patel, A., Eckert, C., Kumar, V., Allen, C. & Teredesai, A. (2020, August). [Fairness in Machine Learning for Healthcare.](./docs/publications/KDD2020-FairnessInHealthcareML-Slides.pdf) In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_ (pp. 3529-3530).\n\nSee also: [Publications](./docs/publications)\n\n```\n@incollection{FMLH_KDD2020,\n    title = {Fairness in Machine Learning for Healthcare},\n    author = {Ahmad, M.A. and Eckert, C. and Kumar, V. and Patel, A. and Allen, C. and Teredesai, A.},\n    year = 2020,\n    month = {August},\n    booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},\n    pages = {3529--3530}\n}\n```\n\n### Courses that use FairMLHealth\n* TCSS 593: *Research Seminar In Data Science* Spring 2021 Department of Computer Science, University of Washington Tacoma\n* EE 520: *Predictive Learning From Data Spring 2021* Department of Electrical Engineering, University of Washington Bothell\n* CSS 581: *Machine Learning Autumn 2020* Department of Computer Science, University of Washington Bothell\n\n## Key Contributors\n* [Muhammad Aurangzeb Ahmad](http://www.aurumahmad.com)\n* [Christine Allen](https://github.com/camagallen)\n* Carly Eckert\n* Juhua Hu\n* Vikas Kumar\n* Arpit Patel\n* Ankur Teredesai\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/KenSciResearch/fairMLHealth",
    "keywords": "healthcare,machine learning,fairness,fair ML,responsible AI",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "fairmlhealth",
    "package_url": "https://pypi.org/project/fairmlhealth/",
    "platform": "",
    "project_url": "https://pypi.org/project/fairmlhealth/",
    "project_urls": {
      "GitHub Pages": "https://kensciresearch.github.io/fairMLHealth",
      "Homepage": "https://github.com/KenSciResearch/fairMLHealth",
      "KenSci": "https://www.kensci.com"
    },
    "release_url": "https://pypi.org/project/fairmlhealth/1.0.2/",
    "requires_dist": [
      "pandas (>=1.0.3)",
      "aif360 (<=0.4.0,>=0.3.0)",
      "ipython",
      "jupyter",
      "numpy (>=1.16)",
      "requests",
      "scipy (<1.6.0,>=1.4.1)",
      "scikit-learn (>=0.21)",
      "pytest (==5.4.2) ; extra == 'dev'",
      "ipyparallel ; extra == 'dev'",
      "nbformat ; extra == 'dev'",
      "nbconvert ; extra == 'dev'",
      "regex ; extra == 'dev'",
      "matplotlib ; extra == 'dev'",
      "seaborn ; extra == 'dev'",
      "xgboost ; extra == 'dev'",
      "pypiwin32 ; (platform_system == \"Windows\") and extra == 'dev'",
      "pywin32 ; (platform_system == \"Windows\") and extra == 'dev'",
      "LICENSE.txt ; extra == 'license_files'",
      "pytest (==5.4.2) ; extra == 'tests'",
      "ipyparallel ; extra == 'tests'",
      "nbformat ; extra == 'tests'",
      "nbconvert ; extra == 'tests'",
      "regex ; extra == 'tests'",
      "pypiwin32 ; (platform_system == \"Windows\") and extra == 'tests'",
      "pywin32 ; (platform_system == \"Windows\") and extra == 'tests'",
      "matplotlib ; extra == 'tutorials'",
      "seaborn ; extra == 'tutorials'",
      "xgboost ; extra == 'tutorials'"
    ],
    "requires_python": "<4,>=3.6",
    "summary": "Health-centered variation analysis",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11461395,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "34c0a6f1d2a0fd653fd8028620bd4510995cad2936852f25ae33309fe3436705",
          "md5": "8f66614432bcc0fa187c01ea9c60ebe0",
          "sha256": "5cd5c5fc3414d3c7f5ea2b935ebea5587c9d3ba42c3dde1a6d767c3571e7eb7c"
        },
        "downloads": -1,
        "filename": "fairmlhealth-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8f66614432bcc0fa187c01ea9c60ebe0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.6",
        "size": 61626,
        "upload_time": "2021-09-03T00:01:57",
        "upload_time_iso_8601": "2021-09-03T00:01:57.994853Z",
        "url": "https://files.pythonhosted.org/packages/34/c0/a6f1d2a0fd653fd8028620bd4510995cad2936852f25ae33309fe3436705/fairmlhealth-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7a0b1a202a7339ee40e999634a7a355400cab80f64b1c54e471f95ec1fec3d8e",
          "md5": "fd86307540a2444b73deeeb0ad60d420",
          "sha256": "de4dc490102bbcd216961578e4e592b167666dc428821c5656e3e67e27a28007"
        },
        "downloads": -1,
        "filename": "fairmlhealth-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "fd86307540a2444b73deeeb0ad60d420",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "<4,>=3.6",
        "size": 59787,
        "upload_time": "2021-09-03T00:02:01",
        "upload_time_iso_8601": "2021-09-03T00:02:01.218781Z",
        "url": "https://files.pythonhosted.org/packages/7a/0b/1a202a7339ee40e999634a7a355400cab80f64b1c54e471f95ec1fec3d8e/fairmlhealth-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2fbc91e7b79f4641a8f05e2bfff613c0d3aff1923181693fa39e0f877de19a35",
          "md5": "204d7e21e6cddaf09294afdcd0bf827e",
          "sha256": "5a3b0a6c6e8f4aac36652e405786e84e6b19df6b5c6ca134eb124d07edfa2059"
        },
        "downloads": -1,
        "filename": "fairmlhealth-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "204d7e21e6cddaf09294afdcd0bf827e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.6",
        "size": 60591,
        "upload_time": "2021-09-03T17:15:01",
        "upload_time_iso_8601": "2021-09-03T17:15:01.550782Z",
        "url": "https://files.pythonhosted.org/packages/2f/bc/91e7b79f4641a8f05e2bfff613c0d3aff1923181693fa39e0f877de19a35/fairmlhealth-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0d114c912181c5eec17eae0b26280fc30bf21d61c4047566020e656910376a1f",
          "md5": "6c7a5feaa83beafa758dc4b3a9023f40",
          "sha256": "86f1d7779e237902425b61cbb115114af2e9bfd688dc86d90a037411df7a8504"
        },
        "downloads": -1,
        "filename": "fairmlhealth-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "6c7a5feaa83beafa758dc4b3a9023f40",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "<4,>=3.6",
        "size": 57721,
        "upload_time": "2021-09-03T17:15:04",
        "upload_time_iso_8601": "2021-09-03T17:15:04.306671Z",
        "url": "https://files.pythonhosted.org/packages/0d/11/4c912181c5eec17eae0b26280fc30bf21d61c4047566020e656910376a1f/fairmlhealth-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f554fd7c80ecf32bf39fd6b703e33c0d5c146ecbc400cbb9624e37b7ac58a3c5",
          "md5": "fad6a5ce0a965820edb793c945b08a80",
          "sha256": "b0691be7781150c6762569ab39d5303e3e32edf0a9f950917c8132aa63c14422"
        },
        "downloads": -1,
        "filename": "fairmlhealth-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fad6a5ce0a965820edb793c945b08a80",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.6",
        "size": 60706,
        "upload_time": "2021-09-15T19:01:25",
        "upload_time_iso_8601": "2021-09-15T19:01:25.180431Z",
        "url": "https://files.pythonhosted.org/packages/f5/54/fd7c80ecf32bf39fd6b703e33c0d5c146ecbc400cbb9624e37b7ac58a3c5/fairmlhealth-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ba6084a6120870b09c8dad1b7d9a97628a9b0a100f7d3b4f2ec538a5098d6c47",
          "md5": "2831562f8ea7905893d2147633b7f88f",
          "sha256": "9a57cfc6304e1e5702b6e834f8131bada9bad55a6c319d61cfbf8215874a2c49"
        },
        "downloads": -1,
        "filename": "fairmlhealth-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "2831562f8ea7905893d2147633b7f88f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "<4,>=3.6",
        "size": 59399,
        "upload_time": "2021-09-15T19:01:27",
        "upload_time_iso_8601": "2021-09-15T19:01:27.866637Z",
        "url": "https://files.pythonhosted.org/packages/ba/60/84a6120870b09c8dad1b7d9a97628a9b0a100f7d3b4f2ec538a5098d6c47/fairmlhealth-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f554fd7c80ecf32bf39fd6b703e33c0d5c146ecbc400cbb9624e37b7ac58a3c5",
        "md5": "fad6a5ce0a965820edb793c945b08a80",
        "sha256": "b0691be7781150c6762569ab39d5303e3e32edf0a9f950917c8132aa63c14422"
      },
      "downloads": -1,
      "filename": "fairmlhealth-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fad6a5ce0a965820edb793c945b08a80",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "<4,>=3.6",
      "size": 60706,
      "upload_time": "2021-09-15T19:01:25",
      "upload_time_iso_8601": "2021-09-15T19:01:25.180431Z",
      "url": "https://files.pythonhosted.org/packages/f5/54/fd7c80ecf32bf39fd6b703e33c0d5c146ecbc400cbb9624e37b7ac58a3c5/fairmlhealth-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ba6084a6120870b09c8dad1b7d9a97628a9b0a100f7d3b4f2ec538a5098d6c47",
        "md5": "2831562f8ea7905893d2147633b7f88f",
        "sha256": "9a57cfc6304e1e5702b6e834f8131bada9bad55a6c319d61cfbf8215874a2c49"
      },
      "downloads": -1,
      "filename": "fairmlhealth-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "2831562f8ea7905893d2147633b7f88f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "<4,>=3.6",
      "size": 59399,
      "upload_time": "2021-09-15T19:01:27",
      "upload_time_iso_8601": "2021-09-15T19:01:27.866637Z",
      "url": "https://files.pythonhosted.org/packages/ba/60/84a6120870b09c8dad1b7d9a97628a9b0a100f7d3b4f2ec538a5098d6c47/fairmlhealth-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}