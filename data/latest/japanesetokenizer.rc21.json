{
  "info": {
    "author": "Kensuke Mitsuzawa",
    "author_email": "kensuke.mit@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "|MIT License|\\ |Build Status|\n\nWhat’s this?\n============\n\nThis is simple python-wrapper for Japanese Tokenizers(A.K.A Tokenizer)\n\nThis project aims to call tokenizers and split a sentence into tokens as\neasy as possible.\n\nAnd, this project supports various Tokenization tools common interface.\nThus, it’s easy to compare output from various tokenizers.\n\nThis project is available also in\n`Github <https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers>`__.\n\nIf you find any bugs, please report them to github issues. Or any pull\nrequests are welcomed!\n\nRequirements\n============\n\n-  Python 2.7\n-  Python 3.x\n\n   -  checked in 3.5, 3.6, 3.7\n\nFeatures\n========\n\n-  simple/common interface among various tokenizers\n-  simple/common interface for filtering with stopwords or\n   Part-of-Speech condition\n-  simple interface to add user-dictionary(mecab only)\n\nSupported Tokenizers\n--------------------\n\nMecab\n~~~~~\n\n`Mecab <http://mecab.googlecode.com/svn/trunk/mecab/doc/index.html?sess=3f6a4f9896295ef2480fa2482de521f6>`__\nis open source tokenizer system for various language(if you have\ndictionary for it)\n\nSee `english\ndocumentation <https://github.com/jordwest/mecab-docs-en>`__ for detail\n\nJuman\n~~~~~\n\n`Juman <http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN>`__ is a\ntokenizer system developed by Kurohashi laboratory, Kyoto University,\nJapan.\n\nJuman is strong for ambiguous writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.\n\nAnd, Juman tells you semantic meaning of words.\n\n.. _juman-1:\n\nJuman++\n~~~~~~~\n\n`Juman++ <http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN++>`__ is a\ntokenizer system developed by Kurohashi laboratory, Kyoto University,\nJapan.\n\nJuman++ is succeeding system of Juman. It adopts RNN model for\ntokenization.\n\nJuman++ is strong for ambigious writing style in Japanese, and is strong\nfor new-comming words thanks to Web based huge dictionary.\n\nAnd, Juman tells you semantic meaning of words.\n\nNote: New Juman++ dev-version(later than 2.x) is available at\n`Github <https://github.com/ku-nlp/jumanpp>`__\n\nKytea\n~~~~~\n\n`Kytea <http://www.phontron.com/kytea/>`__ is tokenizer tool developped\nby Graham Neubig.\n\nKytea has a different algorithm from one of Mecab or Juman.\n\nSetting up\n==========\n\nTokenizers auto-install\n-----------------------\n\n::\n\n   make install\n\nmecab-neologd dictionary auto-install\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n::\n\n   make install_neologd\n\nTokenizers manual-install\n-------------------------\n\n.. _mecab-1:\n\nMeCab\n~~~~~\n\nSee `here <https://github.com/jordwest/mecab-docs-en>`__ to install\nMeCab system.\n\nMecab Neologd dictionary\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nMecab-neologd dictionary is a dictionary-extension based on\nipadic-dictionary, which is basic dictionary of Mecab.\n\nWith, Mecab-neologd dictionary, you’re able to parse new-coming words\nmake one token.\n\nHere, new-coming words is such like, movie actor name or company name…..\n\nSee `here <https://github.com/neologd/mecab-ipadic-neologd>`__ and\ninstall mecab-neologd dictionary.\n\n.. _juman-2:\n\nJuman\n~~~~~\n\n::\n\n   wget -O juman7.0.1.tar.bz2 \"http://nlp.ist.i.kyoto-u.ac.jp/DLcounter/lime.cgi?down=http://nlp.ist.i.kyoto-u.ac.jp/nl-resource/juman/juman-7.01.tar.bz2&name=juman-7.01.tar.bz2\"\n   bzip2 -dc juman7.0.1.tar.bz2  | tar xvf -\n   cd juman-7.01\n   ./configure\n   make   \n   [sudo] make install\n\n.. _juman-3:\n\nJuman++\n-------\n\n-  GCC version must be >= 5\n\n::\n\n   wget http://lotus.kuee.kyoto-u.ac.jp/nl-resource/jumanpp/jumanpp-1.02.tar.xz\n   tar xJvf jumanpp-1.02.tar.xz\n   cd jumanpp-1.02/\n   ./configure\n   make\n   [sudo] make install\n\n.. _kytea-1:\n\nKytea\n-----\n\nInstall Kytea system\n\n::\n\n   wget http://www.phontron.com/kytea/download/kytea-0.4.7.tar.gz\n   tar -xvf kytea-0.4.7.tar\n   cd kytea-0.4.7\n   ./configure\n   make\n   make install\n\nKytea has `python wrapper <https://github.com/chezou/Mykytea-python>`__\nthanks to michiaki ariga. Install Kytea-python wrapper\n\n::\n\n   pip install kytea\n\ninstall\n-------\n\n::\n\n   [sudo] python setup.py install\n\nNote\n~~~~\n\nDuring install, you see warning message when it fails to install\n``pyknp`` or ``kytea``.\n\nif you see these messages, try to re-install these packages manually.\n\nUsage\n=====\n\nTokenization Example(For python3.x. To see exmaple code for Python2.x,\nplaese see\n`here <https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers/blob/master/examples/examples.py>`__)\n\n::\n\n   import JapaneseTokenizer\n   input_sentence = '10日放送の「中居正広のミになる図書館」（テレビ朝日系）で、SMAPの中居正広が、篠原信一の過去の勘違いを明かす一幕があった。'\n   # ipadic is well-maintained dictionary #\n   mecab_wrapper = JapaneseTokenizer.MecabWrapper(dictType='ipadic')\n   print(mecab_wrapper.tokenize(input_sentence).convert_list_object())\n\n   # neologd is automatically-generated dictionary from huge web-corpus #\n   mecab_neologd_wrapper = JapaneseTokenizer.MecabWrapper(dictType='neologd')\n   print(mecab_neologd_wrapper.tokenize(input_sentence).convert_list_object())\n\nFiltering example\n-----------------\n\n::\n\n   import JapaneseTokenizer\n   # with word filtering by stopword & part-of-speech condition #\n   print(mecab_wrapper.tokenize(input_sentence).filter(stopwords=['テレビ朝日'], pos_condition=[('名詞', '固有名詞')]).convert_list_object())\n\nPart-of-speech structure\n------------------------\n\nMecab, Juman, Kytea have different system of Part-of-Speech(POS).\n\nYou can check tables of Part-of-Speech(POS)\n`here <http://www.unixuser.org/~euske/doc/postag/>`__\n\nSimilar Package\n===============\n\nnatto-py\n--------\n\nnatto-py is sophisticated package for tokenization. It supports\nfollowing features\n\n-  easy interface for tokenization\n-  importing additional dictionary\n-  partial parsing mode\n\nLICENSE\n=======\n\nMIT license\n\nFor developers\n==============\n\nYou could build an environment which has dependencies to test this\npackage.\n\nSimply, you build docker image and run docker container.\n\nDev environment\n---------------\n\nDevelop environment is defined with ``test/docker-compose-dev.yml``.\n\nWith the docker-compose.yml file, you could call python2.7 or python3.7\n\nIf you’re using Pycharm Professional edition, you could set\ndocker-compose.yml as remote interpreter.\n\nTo call python2.7, set ``/opt/conda/envs/p27/bin/python2.7``\n\nTo call python3.7, set ``/opt/conda/envs/p37/bin/python3.7``\n\nTest environment\n----------------\n\nThese commands checks from procedures of package install until test of\npackage.\n\n.. code:: bash\n\n   $ docker-compose build\n   $ docker-compose up\n\n.. |MIT License| image:: http://img.shields.io/badge/license-MIT-blue.svg?style=flat\n   :target: LICENSE\n.. |Build Status| image:: https://travis-ci.org/Kensuke-Mitsuzawa/JapaneseTokenizers.svg?branch=master\n   :target: https://travis-ci.org/Kensuke-Mitsuzawa/JapaneseTokenizers\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers",
    "keywords": "MeCab,和布蕪,Juman,Japanese morphological analyzer,NLP,形態素解析,自然言語処理",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "JapaneseTokenizer",
    "package_url": "https://pypi.org/project/JapaneseTokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/JapaneseTokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/Kensuke-Mitsuzawa/JapaneseTokenizers"
    },
    "release_url": "https://pypi.org/project/JapaneseTokenizer/1.6/",
    "requires_dist": [
      "future",
      "jaconv (>=0.2)",
      "mecab-python3",
      "pexpect",
      "pip (>=8.1.0)",
      "pyknp (>=0.4.1)",
      "pypandoc",
      "six"
    ],
    "requires_python": "",
    "summary": "",
    "version": "1.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4983270,
  "releases": {
    "0.6a1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5b96cc92357c7e7261c791db6f769bda1cd2c1f4547ea64bd1b75227c2818643",
          "md5": "e55f95650df77ef126ee23b2cc88ad20",
          "sha256": "c15a23d01f1ad997049e1f89333bb421b98fd5f0a2fbe7ae005662a9cd5a383a"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-0.6a1.tar.gz",
        "has_sig": false,
        "md5_digest": "e55f95650df77ef126ee23b2cc88ad20",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10587,
        "upload_time": "2016-03-05T15:16:52",
        "upload_time_iso_8601": "2016-03-05T15:16:52.996917Z",
        "url": "https://files.pythonhosted.org/packages/5b/96/cc92357c7e7261c791db6f769bda1cd2c1f4547ea64bd1b75227c2818643/JapaneseTokenizer-0.6a1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c67ae5b1c02a0e1c055d17f171fe04925556f419b59f871902a77f6e83a048af",
          "md5": "bdba410c246604b53599e6431ee8ba97",
          "sha256": "82a76d0309a31c09f653b3ba0d1ac99e42a0c9a80e55b5f48b52072a30c84214"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "bdba410c246604b53599e6431ee8ba97",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 15964,
        "upload_time": "2016-03-06T05:06:38",
        "upload_time_iso_8601": "2016-03-06T05:06:38.110908Z",
        "url": "https://files.pythonhosted.org/packages/c6/7a/e5b1c02a0e1c055d17f171fe04925556f419b59f871902a77f6e83a048af/JapaneseTokenizer-0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "13616586dc1b41cdcf05f1f49ea3d92cc4e14341c7879dcce33fab693383da25",
          "md5": "aecb2a18cabf7ab5fcae6dd5147390e8",
          "sha256": "35297b98855676f04f910e4025e7888be9ef69bc3bfb2fad5dc9b34740053274"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "aecb2a18cabf7ab5fcae6dd5147390e8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 15860,
        "upload_time": "2016-04-02T19:53:42",
        "upload_time_iso_8601": "2016-04-02T19:53:42.365687Z",
        "url": "https://files.pythonhosted.org/packages/13/61/6586dc1b41cdcf05f1f49ea3d92cc4e14341c7879dcce33fab693383da25/JapaneseTokenizer-0.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d56ae52ba521d54169b40acf7cb025b68e1e7119e28da6be9416c99054111bcd",
          "md5": "965d1893a35b65ed9c81d09492834f3b",
          "sha256": "f7b4ea9753a8e8a5894ba0ccc1f0b9713aa78f62cc7dc49dbb4fa86ef06c14de"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "965d1893a35b65ed9c81d09492834f3b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16284,
        "upload_time": "2016-04-04T17:23:05",
        "upload_time_iso_8601": "2016-04-04T17:23:05.158729Z",
        "url": "https://files.pythonhosted.org/packages/d5/6a/e52ba521d54169b40acf7cb025b68e1e7119e28da6be9416c99054111bcd/JapaneseTokenizer-0.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "54b579590c298941926efd0b4fbe4906979bfcb16199dd371e46d2a88144b8c4",
          "md5": "2b10f181b00cc626eed106965163d8e3",
          "sha256": "71f5675bd8b6815fcc8242f48be9a4b6197294e2626683a7431380469cfc78f6"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2b10f181b00cc626eed106965163d8e3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 17118,
        "upload_time": "2016-08-03T04:28:53",
        "upload_time_iso_8601": "2016-08-03T04:28:53.319387Z",
        "url": "https://files.pythonhosted.org/packages/54/b5/79590c298941926efd0b4fbe4906979bfcb16199dd371e46d2a88144b8c4/JapaneseTokenizer-1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0a0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "82478727b3a859e6c3cff4c3fdb2a5fc068d355bd346a22db38730e3ae9bd1ce",
          "md5": "87fa79c3609690d6b86295c494af7d6f",
          "sha256": "fa3ec1860b5df9688d0d2c0037af19b568be2663e8b06e9814effb258cf82a4b"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.0a0.tar.gz",
        "has_sig": false,
        "md5_digest": "87fa79c3609690d6b86295c494af7d6f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 17105,
        "upload_time": "2016-06-19T08:18:31",
        "upload_time_iso_8601": "2016-06-19T08:18:31.008233Z",
        "url": "https://files.pythonhosted.org/packages/82/47/8727b3a859e6c3cff4c3fdb2a5fc068d355bd346a22db38730e3ae9bd1ce/JapaneseTokenizer-1.0a0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b6b2089232b3c76d3e802e0fd1c17eb7d96f9bbdc4034d8a55b3bfa18d241d64",
          "md5": "b6d953a9ed3efe28dfbc1a5c9f3287a2",
          "sha256": "fdb6b46290ae0907e62d16d32c762b6c7f4f433619c22c34519e32d6be253d21"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.0b0.tar.gz",
        "has_sig": false,
        "md5_digest": "b6d953a9ed3efe28dfbc1a5c9f3287a2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 17113,
        "upload_time": "2016-06-22T02:34:50",
        "upload_time_iso_8601": "2016-06-22T02:34:50.998148Z",
        "url": "https://files.pythonhosted.org/packages/b6/b2/089232b3c76d3e802e0fd1c17eb7d96f9bbdc4034d8a55b3bfa18d241d64/JapaneseTokenizer-1.0b0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "872b6ad05fb2afe606550828fba2c1da98629bf2f34a5523535d7214ea9c15c0",
          "md5": "da97ce49089ebc76f08aa476a9c1cdce",
          "sha256": "7d9540ee440c9393bdf4232cc694d5bc8b6ba60b5bbde87edaf06381d046263d"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "da97ce49089ebc76f08aa476a9c1cdce",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16609,
        "upload_time": "2016-12-08T18:25:46",
        "upload_time_iso_8601": "2016-12-08T18:25:46.695603Z",
        "url": "https://files.pythonhosted.org/packages/87/2b/6ad05fb2afe606550828fba2c1da98629bf2f34a5523535d7214ea9c15c0/JapaneseTokenizer-1.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2a260c91f48b42d8059f969c2ab9a56b29b351eac15a139744f17fb98fa0accb",
          "md5": "30d57d0c20f73a976f3be343738f5f55",
          "sha256": "2ea333d144fd9f0ba44e6001d74d63855beb3a30a4c87924938130664231518f"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.2.5.tar.gz",
        "has_sig": false,
        "md5_digest": "30d57d0c20f73a976f3be343738f5f55",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 19943,
        "upload_time": "2016-12-28T07:03:26",
        "upload_time_iso_8601": "2016-12-28T07:03:26.757392Z",
        "url": "https://files.pythonhosted.org/packages/2a/26/0c91f48b42d8059f969c2ab9a56b29b351eac15a139744f17fb98fa0accb/JapaneseTokenizer-1.2.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eb9c9197907e3cd0c13f03cd887efef234bc0fe359a05a844e7b82405ddab282",
          "md5": "e6389d1aa28127631fb9595e19a6fd23",
          "sha256": "f2010906f997bc36db4fddb438abaf6d6380bca065beefd2859428fda0a5c0f4"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.2.6.tar.gz",
        "has_sig": false,
        "md5_digest": "e6389d1aa28127631fb9595e19a6fd23",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 20216,
        "upload_time": "2017-01-11T21:26:02",
        "upload_time_iso_8601": "2017-01-11T21:26:02.153647Z",
        "url": "https://files.pythonhosted.org/packages/eb/9c/9197907e3cd0c13f03cd887efef234bc0fe359a05a844e7b82405ddab282/JapaneseTokenizer-1.2.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08ede970d4d49554e7ddf5ed3edec1e5f1d8265c47e44c6de0b4e71191a15c7d",
          "md5": "4caa2f2fd7e0a4e71d0d95b319e2f960",
          "sha256": "bddfc293a9c221f37b631c226568361c2599e74d49c4e0cb57ff859df725d4b8"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.2.7.tar.gz",
        "has_sig": false,
        "md5_digest": "4caa2f2fd7e0a4e71d0d95b319e2f960",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 20224,
        "upload_time": "2017-01-13T00:37:10",
        "upload_time_iso_8601": "2017-01-13T00:37:10.425560Z",
        "url": "https://files.pythonhosted.org/packages/08/ed/e970d4d49554e7ddf5ed3edec1e5f1d8265c47e44c6de0b4e71191a15c7d/JapaneseTokenizer-1.2.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7919c950a7b3ba817a9fc56e9d44d6698a401e876f54de404a760f8a1100a34a",
          "md5": "13b4072229b14619ae7b8cd672c47fbe",
          "sha256": "b51dd61c3a4192bb70d47310e67d42b29b8e6ec7702b556f4b718836fc772ccf"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "13b4072229b14619ae7b8cd672c47fbe",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23199,
        "upload_time": "2017-02-23T03:41:02",
        "upload_time_iso_8601": "2017-02-23T03:41:02.236513Z",
        "url": "https://files.pythonhosted.org/packages/79/19/c950a7b3ba817a9fc56e9d44d6698a401e876f54de404a760f8a1100a34a/JapaneseTokenizer-1.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c006e6629d1113b7f9061702969154baed98cdb75d3648ff3d0270d87b17a7a5",
          "md5": "f70e1e0d82111714c1958084eef1c813",
          "sha256": "3608d6135eb00d6b59fccf675c894ff978f22359ed37a99fde8bb006b6d948f0"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f70e1e0d82111714c1958084eef1c813",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 28932,
        "upload_time": "2017-06-29T11:05:09",
        "upload_time_iso_8601": "2017-06-29T11:05:09.142771Z",
        "url": "https://files.pythonhosted.org/packages/c0/06/e6629d1113b7f9061702969154baed98cdb75d3648ff3d0270d87b17a7a5/JapaneseTokenizer-1.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b41c9939737367a9fbd76d83fb3785676f1c25f441e73612e2b8ef7cd0f96ca4",
          "md5": "3c19ae9d41dc190c59be3664e3c9b659",
          "sha256": "f18bdbb1883a02d2cacfa42cf41b3d5198a804b986c06ec5f61b37c4d0ca0a82"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.3.tar.gz",
        "has_sig": false,
        "md5_digest": "3c19ae9d41dc190c59be3664e3c9b659",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 29010,
        "upload_time": "2017-09-11T09:30:36",
        "upload_time_iso_8601": "2017-09-11T09:30:36.530709Z",
        "url": "https://files.pythonhosted.org/packages/b4/1c/9939737367a9fbd76d83fb3785676f1c25f441e73612e2b8ef7cd0f96ca4/JapaneseTokenizer-1.3.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ab73cfc8a35e964e78dbc8a72cd631b4f84d837279ae34501381a115d83f6c58",
          "md5": "29140724b608a9a2b8232c7c6bfb35d8",
          "sha256": "d0f89a39f575af46b1ada42b0cd6980e56ecad7aa74397a13fa7c04df7ffa896"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.4.tar.gz",
        "has_sig": false,
        "md5_digest": "29140724b608a9a2b8232c7c6bfb35d8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 25226,
        "upload_time": "2017-09-21T08:10:52",
        "upload_time_iso_8601": "2017-09-21T08:10:52.647747Z",
        "url": "https://files.pythonhosted.org/packages/ab/73/cfc8a35e964e78dbc8a72cd631b4f84d837279ae34501381a115d83f6c58/JapaneseTokenizer-1.3.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0cb7c54719202bbd67b7a44ba1d53f58d5557261d4a523fe1bfe1aeceab9ca65",
          "md5": "e24bafd9da63971195ce7f1590e7487c",
          "sha256": "5276308eae6a34221446069a254ef98e20f3a5b90f2e5ab8c682d45a269295b8"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.5.tar.gz",
        "has_sig": false,
        "md5_digest": "e24bafd9da63971195ce7f1590e7487c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 25785,
        "upload_time": "2017-09-27T03:30:33",
        "upload_time_iso_8601": "2017-09-27T03:30:33.996137Z",
        "url": "https://files.pythonhosted.org/packages/0c/b7/c54719202bbd67b7a44ba1d53f58d5557261d4a523fe1bfe1aeceab9ca65/JapaneseTokenizer-1.3.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1f080b839490cd42fce5b727bbf43500d2bd268584c0519720cd1b03738899d0",
          "md5": "4528fc7cca5ee812f63f6183435eb2c5",
          "sha256": "55cd3f4b0979609bddadd88cedd31afdc6cec39f5b2c4225b814b58944bf9a92"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.6.tar.gz",
        "has_sig": false,
        "md5_digest": "4528fc7cca5ee812f63f6183435eb2c5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 26011,
        "upload_time": "2017-11-01T11:05:19",
        "upload_time_iso_8601": "2017-11-01T11:05:19.951464Z",
        "url": "https://files.pythonhosted.org/packages/1f/08/0b839490cd42fce5b727bbf43500d2bd268584c0519720cd1b03738899d0/JapaneseTokenizer-1.3.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0d2388e38a5a9f874d6c2bba08908d76b449cb300985c8c882a32b8d2c0a4434",
          "md5": "e9367af771a29abc9249eb23ad1ed60f",
          "sha256": "8d296055cf8a4e8249773287ca22704fb63adebed3bca5a249128aea64d87796"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.3.7.tar.gz",
        "has_sig": false,
        "md5_digest": "e9367af771a29abc9249eb23ad1ed60f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 26109,
        "upload_time": "2018-02-27T01:58:58",
        "upload_time_iso_8601": "2018-02-27T01:58:58.011697Z",
        "url": "https://files.pythonhosted.org/packages/0d/23/88e38a5a9f874d6c2bba08908d76b449cb300985c8c882a32b8d2c0a4434/JapaneseTokenizer-1.3.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1c8845abab146379806f748fe2af0969b4d739a47efdd8e04017a138d0a1b78d",
          "md5": "658c071f213905c766dca30dbe990ba4",
          "sha256": "6f8af3f172ca535d0d37aff2ec9e9ad619d069422859219fac8a0ed5b9ae7de1"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "658c071f213905c766dca30dbe990ba4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 25609,
        "upload_time": "2018-12-24T19:37:11",
        "upload_time_iso_8601": "2018-12-24T19:37:11.647869Z",
        "url": "https://files.pythonhosted.org/packages/1c/88/45abab146379806f748fe2af0969b4d739a47efdd8e04017a138d0a1b78d/JapaneseTokenizer-1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "df23399685ed7438030f01417bb1d2c0e76d0bcb60f4d8f73f2d6449d52b8c86",
          "md5": "0d64248c5c514fa6abf0293ae7d80f33",
          "sha256": "c8636309b672ff6fd893fd388d0757c4b026320d55c2d10081df10ad1508b126"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "0d64248c5c514fa6abf0293ae7d80f33",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 28160,
        "upload_time": "2019-01-21T07:14:05",
        "upload_time_iso_8601": "2019-01-21T07:14:05.051165Z",
        "url": "https://files.pythonhosted.org/packages/df/23/399685ed7438030f01417bb1d2c0e76d0bcb60f4d8f73f2d6449d52b8c86/JapaneseTokenizer-1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2963b71a2fa2ba0ad681d0877bab89b56d162b0e07533b99c5f3477f9a3df7f0",
          "md5": "2023647d0aea042e51fac3be0450dee8",
          "sha256": "72e54e004071d6e26e53ee6fbf384ac696260a4ccd98517b460c7322d600c2cb"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2023647d0aea042e51fac3be0450dee8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 44447,
        "upload_time": "2019-03-25T16:00:13",
        "upload_time_iso_8601": "2019-03-25T16:00:13.465345Z",
        "url": "https://files.pythonhosted.org/packages/29/63/b71a2fa2ba0ad681d0877bab89b56d162b0e07533b99c5f3477f9a3df7f0/JapaneseTokenizer-1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "134e758f36d3d7f51d9c10d07d46a265f3f2337062b1913c231223156b719d0f",
          "md5": "f5d47f7d1d6bd0381f4ff56f25a52dee",
          "sha256": "c9b93fb9d355a10b4e6484ac25febcecee00e40e42e6ad38beb2258f9e8d7900"
        },
        "downloads": -1,
        "filename": "JapaneseTokenizer-1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "f5d47f7d1d6bd0381f4ff56f25a52dee",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 29519,
        "upload_time": "2019-03-25T16:00:16",
        "upload_time_iso_8601": "2019-03-25T16:00:16.301362Z",
        "url": "https://files.pythonhosted.org/packages/13/4e/758f36d3d7f51d9c10d07d46a265f3f2337062b1913c231223156b719d0f/JapaneseTokenizer-1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2963b71a2fa2ba0ad681d0877bab89b56d162b0e07533b99c5f3477f9a3df7f0",
        "md5": "2023647d0aea042e51fac3be0450dee8",
        "sha256": "72e54e004071d6e26e53ee6fbf384ac696260a4ccd98517b460c7322d600c2cb"
      },
      "downloads": -1,
      "filename": "JapaneseTokenizer-1.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "2023647d0aea042e51fac3be0450dee8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 44447,
      "upload_time": "2019-03-25T16:00:13",
      "upload_time_iso_8601": "2019-03-25T16:00:13.465345Z",
      "url": "https://files.pythonhosted.org/packages/29/63/b71a2fa2ba0ad681d0877bab89b56d162b0e07533b99c5f3477f9a3df7f0/JapaneseTokenizer-1.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "134e758f36d3d7f51d9c10d07d46a265f3f2337062b1913c231223156b719d0f",
        "md5": "f5d47f7d1d6bd0381f4ff56f25a52dee",
        "sha256": "c9b93fb9d355a10b4e6484ac25febcecee00e40e42e6ad38beb2258f9e8d7900"
      },
      "downloads": -1,
      "filename": "JapaneseTokenizer-1.6.tar.gz",
      "has_sig": false,
      "md5_digest": "f5d47f7d1d6bd0381f4ff56f25a52dee",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 29519,
      "upload_time": "2019-03-25T16:00:16",
      "upload_time_iso_8601": "2019-03-25T16:00:16.301362Z",
      "url": "https://files.pythonhosted.org/packages/13/4e/758f36d3d7f51d9c10d07d46a265f3f2337062b1913c231223156b719d0f/JapaneseTokenizer-1.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}