{
  "info": {
    "author": "Fiona Pigott, Jeff Kolb, Josh Montague, Aaron Gonzales, Jim Moffitt",
    "author_email": "dev-support@twitter.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": ".. .. image:: https://img.shields.io/endpoint?url=https%3A%2F%2Ftwbadges.glitch.me%2Fbadges%2Fv2\n..   :target: https://developer.twitter.com/en/docs/twitter-api\n..   :alt: Twitter API v2\n\nPython client for the Twitter API v2 search endpoints\n===========================================================\n\nWelcome to the ``v2`` branch of the Python search client. This branch was born from the main branch that supports\npremium and enterprise tiers of Twitter search. This branch supports the `Twitter API v2 'recent' and 'all' search endpoints <https://developer.twitter.com/en/docs/twitter-api/tweets/search/introduction>`__ only, and drops support for the premium and enterprise tiers.\n\nThis project serves as a wrapper for the all Twitter API v2 search endpoints, including both the endpoints that return Tweets and the endpoints that return *counts* of Tweets. This wrapper provides a command-line utility and a Python library you can integrate into your own scripts.\n\nThis client library supports these endpoints:\n\n- /2/tweets/search/recent\n- /2/tweets/counts/recent\n- /2/tweets/search/all\n- /2/tweets/counts/all\n\nThe search endpoint you want to hit is specified in the library's YAML file:\n\n.. code:: yaml\n\n  search_tweets_v2:\n    endpoint:  https://api.twitter.com/2/tweets/search/recent #Or https://api.twitter.com/2/tweets/search/all\n\n\nThe 'recent' search endpoint provides Tweets from the past 7 days. The 'all' search endpoint, launched in January 2021 as part of the 'academic research' tier of Twitter API v2 access, provides access to all publicly avaialble Tweets posted since March 2006.\n\nTo learn more about the Twitter academic research program, see this [Twitter blog post](https://blog.twitter.com/developer/en_us/topics/tips/2021/enabling-the-future-of-academic-research-with-the-twitter-api.html).\n\nTo download and install this package, go to: https://pypi.org/project/searchtweets-v2/\n\nIf you are looking for the original version that works with premium and enterprise versions of search, head on over to\nthe main or ``enterprise-premium`` branch. (Soon, the v2 version will be promoted to the main branch.)\n\n\nFeatures\n========\n\n- Supports Twitter API v2 'recent' and 'all' search.\n- [Update] Supports the Tweet 'counts' endpoint, which can reduce API call usage and provide rapid insights if you only need Tweet volumes and not Tweet payloads. \n- Supports the configuration of v2 `expansions <https://developer.twitter.com/en/docs/twitter-api/expansions>`_ and `fields <https://developer.twitter.com/en/docs/twitter-api/fields>`_.\n- Supports multiple output formats: \n  * Original API responses (new default)\n  * Stream of messages (previous default in versions <1.0.7)\n  * New 'atomic' format with expansions included in tweets.\n- Supports a new \"polling\" mode using the ``since-id`` search request parameter. The ``since-id``, along with the new ``until-id`` provide a way to navigate the public Tweet archive by Tweet ID.\n- Supports additional ways to specify ``start-time`` and ``end-time`` request parameters:\n\n  - #d - For example, '2d' sets ``start-time`` to (exactly) two days ago.\n  - #h - For example, '12h' sets ``start-time`` to (exactly) twelve hours ago.\n  - #m - For example, '15m' sets ``start-time`` to (exactly) fifteen minutes ago.\n\n  These are handy for kicking off searches with a backfill period, and also work with the ``end-time`` request parameter.\n\nThese features were inherited from the enterprise/premium version:\n\n-  Command-line utility is pipeable to other tools (e.g., ``jq``).\n-  Automatically handles pagination of search results with specifiable limits.\n-  Delivers a stream of data to the user for low in-memory requirements.\n-  Handles OAuth 2 and Bearer Token authentication.\n-  Flexible usage within a python program.\n\n\nTwitter API v2 search updates\n====================================\n\nTwitter API v2 represents an opportunity to apply previous learnings from building Twitter API v1.1. and the premium and enterprise tiers of endpoints, and redesign and rebuild from the ground up. While building this v2 version of the `search-tweets-python` library,\nwe took the opportunity to update fundamental things. This library provides example scripts, and one example is updating their command-line arguments to better match new v2 conventions. Instead of setting search periods with `start-datetime` and `end-datetime`,\nthey have been shortened to match current search request parameters: `start-time` and `end-time`. Throughout the code, we no longer use parlance that references `rules` and `PowerTrack`, and now reference `queries` and the v2 recent search endpoint.\n\nWhen migrating this Python search client to v2 from the enterprise and premium tiers, the following updates were made:\n\n- Added support for GET requests (and removed POST support for now).\n- Added support for ``since_id`` and ``until_id`` request parameters.\n- Updated pagination details.\n- Updated app command-line parlance:\n      -  --start-datetime → --start-time\n      -  --end-datetime → --end-time\n      -  --filter-rule → --query\n      -  --max-results → --max-tweets\n      -  --count-bucket → --granularity [Updated 2021-06] \n      - Dropped --account-type. No longer required since support for Premium and Enterprise search tiers have been dropped.\n\nIn this spirit of updating the parlance used, note that a core method provided by searchtweets/result_stream.py has been renamed. The method `gen_rule_payload` has been updated to `gen_request_parameters`. \n\n**One key update is handling the changes in how the search endpoint returns its data.** The v2 search endpoint returns matching Tweets in a `data` array, along with an `includes` array that provides supporting objects that result from specifying `expansions`.\nThese expanded objects include Users, referenced Tweets, and attached media.  In addition to the `data` and `includes` arrays, the search endpoint also provides a `meta` object that provides the max and min Tweet IDs included in the response,\nalong with a `next_token` if there is another 'page' of data to request.\n\nCurrently, the v2 client returns the original API responses. Optionally, it can output a stream of Tweet objects with all expansions included in each tweet. Alternatively, it can output a stream of messages, yielding the individual Tweet objects, arrays of User, Tweet, and media objects from the `includes` array, followed by the `meta` object. This matches the behavior of the original search client, and was the default output format in versions 1.0.7 and earlier.\n\nFinally, the original version of search-tweets-python used a `Tweet Parser <https://twitterdev.github.io/tweet_parser/>`__ to help manage the differences between two different JSON formats (\"original\" and \"Activity Stream\"). With v2, there is just one version of Tweet JSON, so this Tweet Parser is not used.\nIn the original code, this Tweet parser was envoked with a `tweetify=True directive. With this v2 version, this use of the Tweet Parser is turned off by instead using `tweetify=False`.\n\n\nCommand-line options\n====================\n\n.. code:: bash\n\n  usage: search_tweets.py \n                        [-h] [--credential-file CREDENTIAL_FILE] [--credential-file-key CREDENTIAL_YAML_KEY] [--env-overwrite ENV_OVERWRITE] [--config-file CONFIG_FILENAME] [--query QUERY]\n                        [--start-time START_TIME] [--end-time END_TIME] [--since-id SINCE_ID] [--until-id UNTIL_ID] [--results-per-call RESULTS_PER_CALL] [--expansions EXPANSIONS]\n                        [--tweet-fields TWEET_FIELDS] [--user-fields USER_FIELDS] [--media-fields MEDIA_FIELDS] [--place-fields PLACE_FIELDS] [--poll-fields POLL_FIELDS]\n                        [--output-format OUTPUT_FORMAT] [--max-tweets MAX_TWEETS] [--max-pages MAX_PAGES] [--results-per-file RESULTS_PER_FILE] [--filename-prefix FILENAME_PREFIX]\n                        [--no-print-stream] [--print-stream] [--extra-headers EXTRA_HEADERS] [--debug]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --credential-file CREDENTIAL_FILE\n                        Location of the yaml file used to hold your credentials.\n  --credential-file-key CREDENTIAL_YAML_KEY\n                        the key in the credential file used for this session's credentials. Defaults to search_tweets_v2\n  --env-overwrite ENV_OVERWRITE\n                        Overwrite YAML-parsed credentials with any set environment variables. See API docs or readme for details.\n  --config-file CONFIG_FILENAME\n                        configuration file with all parameters. Far, easier to use than the command-line args version., If a valid file is found, all args will be populated, from there. Remaining\n                        command-line args, will overrule args found in the config, file.\n  --query QUERY         Search query. (See: https://developer.twitter.com/en/docs/labs/recent-search/guides/search-queries)\n  --granularity GRANULARITY\n                        Set this to make a 'counts' request. 'Bucket' size for\n                        the search counts API. Options: day, hour, minute.\n                        Aligned to midnight UTC.\n  --start-time START_TIME\n                        Start of datetime window, format 'YYYY-mm-DDTHH:MM' (default: -7 days for /recent, -30 days for /all)\n  --end-time END_TIME   End of datetime window, format 'YYYY-mm-DDTHH:MM' (default: to 30 seconds before request time)\n  --since-id SINCE_ID   Tweet ID, will start search from Tweets after this one. (See: https://developer.twitter.com/en/docs/labs/recent-search/guides/pagination)\n  --until-id UNTIL_ID   Tweet ID, will end search from Tweets before this one. (See: https://developer.twitter.com/en/docs/labs/recent-search/guides/pagination)\n  --results-per-call RESULTS_PER_CALL\n                        Number of results to return per call (default 10; max 100) - corresponds to 'max_results' in the API\n  --expansions EXPANSIONS\n                        A comma-delimited list of expansions. Specified expansions results in full objects in the 'includes' response object.\n  --tweet-fields TWEET_FIELDS\n                        A comma-delimited list of Tweet JSON attributes to include in endpoint responses. (API default:\"id,text\")\n  --user-fields USER_FIELDS\n                        A comma-delimited list of User JSON attributes to include in endpoint responses. (API default:\"id\")\n  --media-fields MEDIA_FIELDS\n                        A comma-delimited list of media JSON attributes to include in endpoint responses. (API default:\"id\")\n  --place-fields PLACE_FIELDS\n                        A comma-delimited list of Twitter Place JSON attributes to include in endpoint responses. (API default:\"id\")\n  --poll-fields POLL_FIELDS\n                        A comma-delimited list of Twitter Poll JSON attributes to include in endpoint responses. (API default:\"id\")\n  --output-format OUTPUT_FORMAT\n                        Set output format: 'r' Unmodified API [R]esponses. (default). 'a' [A]tomic Tweets: Tweet objects with expansions inline. 'm' [M]essage stream: Tweets, expansions, and\n                        pagination metadata as a stream of messages.\n  --max-tweets MAX_TWEETS\n                        Maximum number of Tweets to return for this session of requests.\n  --max-pages MAX_PAGES\n                        Maximum number of pages/API calls to use for this session.\n  --results-per-file RESULTS_PER_FILE\n                        Maximum tweets to save per file.\n  --filename-prefix FILENAME_PREFIX\n                        prefix for the filename where tweet json data will be stored.\n  --no-print-stream     disable print streaming\n  --print-stream        Print tweet stream to stdout\n  --extra-headers EXTRA_HEADERS\n                        JSON-formatted str representing a dict of additional HTTP request headers\n  --debug               print all info and warning messages\n\n\n\nInstallation\n=============\n\nThe updated Pypi install package for the v2 version is at:\n\nhttps://pypi.org/project/searchtweets-v2/\n\nAnother option to work directly with this code by cloning the repository, installing the required Python packages, setting up your credentials, and start making requests.\nFor those not using the Pypi package, and instead are cloning the repository, a ``requirements.txt`` is provided. Dependencies can be installed with the ``pip install -r requirements.txt`` command.\n\nTo confirm the your code is ready to go, run the ``$python3 scripts/search-tweets.py -h`` command. You should see the help details shown above.\n\n\n\nCredential Handling\n===================\n\nThe Twitter API v2 search endpoints uses app-only authentication. You have the choice to configure your application consumer key and secret, or a Bearer Token you have generated. If you supply the application key and secret, the client will generate a Bearer Token for you.\n\nMany developers might find providing your application key and secret more straightforward and letting this library manage your Bearer Token generation for you. Please see `HERE <https://developer.twitter.com/en/docs/basics/authentication/oauth-2-0>`_ for an overview of the app-only authentication method.\n\nWe support both YAML-file based methods and environment variables for storing credentials, and provide flexible handling with sensible defaults.\n\nYAML method\n===========\n\nThe simplest credential file should look like this:\n\n.. code:: yaml\n\n  search_tweets_v2:\n    endpoint:  https://api.twitter.com/2/tweets/search/recent\n    consumer_key: <CONSUMER_KEY>\n    consumer_secret: <CONSUMER_SECRET>\n    bearer_token: <BEARER_TOKEN>\n\nBy default, this library expects this file at \"~/.twitter_keys.yaml\", but you can pass the relevant location as needed, either with the --credential-file flag for the command-line app or as demonstrated below in a Python program.\n\nBoth above examples require no special command-line arguments or in-program arguments. The credential parsing methods, unless otherwise specified, will look for a YAML key called search_tweets_v2.\n\nFor developers who have multiple endpoints and/or search products, you can keep all credentials in the same file and specify specific keys to use. --credential-file-key specifies this behavior in the command line app. An example:\n\n.. code:: yaml\n\n  search_tweets_v2:\n    endpoint: https://api.twitter.com/2/tweets/search/recent\n    consumer_key: <KEY>\n    consumer_secret: <SECRET>\n    (optional) bearer_token: <TOKEN>\n\n  search_tweets_labsv2:\n    endpoint: https://api.twitter.com/labs/2/tweets/search\n    consumer_key: <KEY>\n    consumer_secret: <SECRET>\n    (optional) bearer_token: <TOKEN>\n\nEnvironment Variables\n=====================\n\nIf you want or need to pass credentials via environment variables, you can set the appropriate variables:\n\n::\n\n  export SEARCHTWEETS_ENDPOINT=\n  export SEARCHTWEETS_BEARER_TOKEN=\n  export SEARCHTWEETS_CONSUMER_KEY=\n  export SEARCHTWEETS_CONSUMER_SECRET=\n\nThe ``load_credentials`` function will attempt to find these variables if it cannot load fields from the YAML file, and it will **overwrite any credentials from the YAML file that are present as environment variables** if they have been parsed. This behavior can be changed by setting the ``load_credentials`` parameter ``env_overwrite`` to ``False``.\n\nThe following cells demonstrates credential handling in the Python library.\n\n.. code:: python\n\n  from searchtweets import load_credentials\n\n.. code:: python\n\n  load_credentials(filename=\"./search_tweets_creds_example.yaml\",\n                   yaml_key=\"search_tweets_v2_example\",\n                   env_overwrite=False)\n\n::\n\n  {'bearer_token': '<A_VERY_LONG_MAGIC_STRING>',\n   'endpoint': 'https://api.twitter.com/2/tweets/search/recent',\n   'extra_headers_dict': None}\n\nEnvironment Variable Overrides\n------------------------------\n\nIf we set our environment variables, the program will look for them\nregardless of a YAML file's validity or existence.\n\n.. code:: python\n\n   import os\n   os.environ[\"SEARCHTWEETS_BEARER_TOKEN\"] = \"<ENV_BEARER_TOKEN>\"\n   os.environ[\"SEARCHTWEETS_ENDPOINT\"] = \"<https://endpoint>\"\n\n   load_credentials(filename=\"nothing_here.yaml\", yaml_key=\"no_key_here\")\n\n::\n\n   cannot read file nothing_here.yaml\n   Error parsing YAML file; searching for valid environment variables\n\n::\n\n   {'bearer_token': '<ENV_BEARER_TOKEN>',\n    'endpoint': '<https://endpoint>'}\n\nCommand-line app\n----------------\n\nthe flags:\n\n-  ``--credential-file <FILENAME>``\n-  ``--credential-file-key <KEY>``\n-  ``--env-overwrite``\n\nare used to control credential behavior from the command-line app.\n\n--------------\n\nUsing the Comand Line Application\n=================================\n\nThe library includes an application, ``search_tweets.py``, that provides rapid access to Tweets. When you use ``pip`` to install this package, ``search_tweets.py`` is installed globally. The file is located in the ``scripts/`` directory for those who want to run it locally.\n\nNote that the ``--results-per-call`` flag specifies an argument to the API, not as a hard max to number of results returned from this program. The argument ``--max-tweets`` defines the maximum number of results to return from a single run of the ``search-tweets.py``` script. All examples assume that your credentials are set up correctly in the default location - ``.twitter_keys.yaml`` or in environment variables.\n\n**Stream json results to stdout without saving**\n\n.. code:: bash\n\n  search_tweets.py \\\n    --max-tweets 10000 \\\n    --results-per-call 100 \\\n    --query \"(snow OR rain) has:media -is:retweet\" \\\n    --print-stream\n\n**Stream json results to stdout and save to a file**\n\n.. code:: bash\n\n  search_tweets.py \\\n    --max-tweets 10000 \\\n    --results-per-call 100 \\\n    --query \"(snow OR rain) has:media -is:retweet\" \\\n    --filename-prefix weather_pics \\\n    --print-stream\n\n**Save to file without output**\n\n.. code:: bash\n\n  search_tweets.py \\\n    --max-tweets 10000 \\\n    --results-per-call 100 \\\n    --query \"(snow OR rain) has:media -is:retweet\" \\\n    --filename-prefix weather_pics \\\n    --no-print-stream\n\nOne or more custom headers can be specified from the command line, using the ``--extra-headers`` argument and a JSON-formatted string representing a dictionary of extra headers:\n\n.. code:: bash\n\n  search_tweets.py \\\n    --query \"(snow OR rain) has:media -is:retweet\" \\\n    --extra-headers '{\"<MY_HEADER_KEY>\":\"<MY_HEADER_VALUE>\"}'\n\nOptions can be passed via a configuration file (either ini or YAML). Example files can be found in the ``config/api_config_example.config`` or ``config/api_yaml_example.yaml`` files, which might look like this:\n\n.. code:: bash\n\n  [search_rules]\n  start_time = 2020-05-01\n  end_time = 2020-05-01\n  query = (snow OR rain) has:media -is:retweet\n\n  [search_params]\n  results_per_call = 100\n  max_tweets = 10000\n\n  [output_params]\n  save_file = True\n  filename_prefix = weather_pics\n  results_per_file = 10000000\n\nOr this:\n\n.. code:: bash\n\n  search_rules:\n      start_time: 2020-05-01\n      end_time: 2020-05-01 01:01\n      query: (snow OR rain) has:media -is:retweet\n\n  search_params:\n      results_per_call: 100\n      max_results: 500\n\n  output_params:\n      save_file: True\n      filename_prefix: (snow OR rain) has:media -is:retweet\n      results_per_file: 10000000\n\nCustom headers can be specified in a config file, under a specific credentials key:\n\n.. code:: yaml\n\n  search_tweets_v2:\n    endpoint: <FULL_URL_OF_ENDPOINT>\n    bearer_token: <AAAAAloooooogString>\n    extra_headers:\n      <MY_HEADER_KEY>: <MY_HEADER_VALUE>\n\nWhen using a config file in conjunction with the command-line utility, you need to specify your config file via the ``--config-file`` parameter. Additional command-line arguments will either be added to the config file args or overwrite the config file args if both are specified and present.\n\nExample:\n\n::\n\n  search_tweets.py \\\n    --config-file myapiconfig.config \\\n    --no-print-stream\n\n------------------\n\nUsing the Twitter Search APIs' Python Wrapper\n=============================================\n\nWorking with the API within a Python program is straightforward.\n\nWe'll assume that credentials are in the default location,\n``~/.twitter_keys.yaml``.\n\n.. code:: python\n\n   from searchtweets import ResultStream, gen_request_parameters, load_credentials\n\n\nTwitter API v2 Setup\n--------------------\n\n.. code:: python\n\n   search_args = load_credentials(\"~/.twitter_keys.yaml\",\n                                          yaml_key=\"search_tweets_v2\",\n                                          env_overwrite=False)\n\n\nThere is a function that formats search API rules into valid json queries called ``gen_request_parameters``. It has sensible defaults, such as pulling more Tweets per call than the default 10, and not including dates. Discussing the finer points of\ngenerating search rules is out of scope for these examples; we encourage you to see the docs to learn the nuances within, but for now let's see what a query looks like.\n\n.. code:: python\n\n   query = gen_request_parameters(\"snow\", results_per_call=100) \n   print(query)\n\n::\n\n   {\"query\":\"snow\",\"max_results\":100}\n\nThis rule will match tweets that have the text ``snow`` in them.\n\nFrom this point, there are two ways to interact with the API. There is a quick method to collect smaller amounts of Tweets to memory that requires less thought and knowledge, and interaction with the ``ResultStream`` object which will be introduced later.\n\nFast Way\n--------\n\nWe'll use the ``search_args`` variable to power the configuration point for the API. The object also takes a valid search query and has options to cutoff search when hitting limits on both number of Tweets and endpoint calls.\n\nWe'll be using the ``collect_results`` function, which has three parameters.\n\n-  query: a valid search query, referenced earlier\n-  max_results: as the API handles pagination, it will stop collecting\n   when we get to this number\n-  result_stream_args: configuration args that we've already specified.\n\nLet's see how it goes:\n\n.. code:: python\n\n   from searchtweets import collect_results\n\n.. code:: python\n\n   tweets = collect_results(query,\n                            max_tweets=100,\n                            result_stream_args=search_args) # change this if you need to\n\nAn overwhelming number of Tweet attributes are made available directly, as such:\n\n.. code:: python\n\n   [print(tweet.text, end='\\n\\n') for tweet in tweets[0:10]]\n\n::\n\n   @CleoLoughlin Rain after the snow? Do you have ice now?\n\n   @koofltxr Rain, 134340, still with you, winter bear, Seoul, crystal snow, sea, outro:blueside\n\n   @TheWxMeister Sorry it ruined your camping. I was covering plants in case we got snow in the Mountain Shadows area. Thankfully we didn\\u2019t. At least it didn\\u2019t stick to the ground. The wind was crazy! Got just over an inch of rain. Looking forward to better weather.\n\n   @brettlorenzen And, the reliability of \\u201cNeither snow nor rain nor heat nor gloom of night stays these couriers (the #USPS) from the swift completion of their appointed rounds.\\u201d\n\n   Because black people get killed in the rain, black lives matter in the rain. It matters all the time. Snow, rain, sleet, sunny days. We're not out here because it's sunny. We're not out here for fun. We're out here because black lives matter. \n\n   Some of the master copies of the film \\u201cGone With the Wind\\u201d are archived at the @librarycongress near \\u201cSnow White and the Seven Dwarfs\\u201d and \\u201cSingin\\u2019 in the Rain.\\u201d GWTW isn\\u2019t going to vanish off the face of the earth.\n\n   Snow Man\\u306eD.D.\\u3068\\nSixTONES\\u306eImitation Rain\\n\\u6d41\\u308c\\u305f\\u301c\n\n   @Nonvieta Yup I work in the sanitation industry. I'm in the office however. Life would not go on without our garbage men and women out there. All day everyday rain snow or shine they out there.\n\n   This picture of a rainbow in WA proves nothing. How do we know if this rainbow was not on Mars or the ISS? Maybe it was drawn in on the picture. WA has mail-in voting so we do have to worry aboug rain, snow, poll workers not showing up or voting machines broke on election day !! https://t.co/5WdHx0acS0 https://t.co/BEKtTpBW9g\n\n   Weather in Oslo at 06:00: Clear Temp: 10.6\\u00b0C Min today: 9.1\\u00b0C Rain today:0.0mm Snow now: 0.0cm Wind N Conditions: Clear Daylight:18:39 hours Sunset: 22:36\n\nVoila, we have some Tweets. For interactive environments and other cases where you don't care about collecting your data in a single load or don't need to operate on the stream of Tweets directly, I recommend using this convenience function.\n\nWorking with the ResultStream\n-----------------------------\n\nThe ResultStream object will be powered by the ``search_args``, and takes the query and other configuration parameters, including a hard stop on number of pages to limit your API call usage.\n\n.. code:: python\n\n   rs = ResultStream(request_parameters=query,\n                     max_results=500,\n                     max_pages=1,\n                     **search_args)\n\n   print(rs)\n\n ::\n\n    ResultStream: \n   \t{\n       \"endpoint\":\"https:\\/\\/api.twitter.com\\/2\\/tweets\\/search\\/recent\",\n       \"request_parameters\":{\n           \"query\":\"snow\",\n           \"max_results\":100\n       },\n       \"tweetify\":false,\n       \"max_results\":1000\n   }\n\nThere is a function, ``.stream``, that seamlessly handles requests and pagination for a given query. It returns a generator, and to grab our 1000 Tweets that mention ``snow`` we can do this:\n\n.. code:: python\n\n   tweets = list(rs.stream())\n\n.. code:: python\n\n   # using unidecode to prevent emoji/accents printing \n   [print(tweet) for tweet in tweets[0:10]]\n\n::\n\n{\"id\": \"1270572563505254404\", \"text\": \"@CleoLoughlin Rain after the snow? Do you have ice now?\"}\n{\"id\": \"1270570767038599168\", \"text\": \"@koofltxr Rain, 134340, still with you, winter bear, Seoul, crystal snow, sea, outro:blueside\"}\n{\"id\": \"1270570621282340864\", \"text\": \"@TheWxMeister Sorry it ruined your camping. I was covering plants in case we got snow in the Mountain Shadows area. Thankfully we didn\\u2019t. At least it didn\\u2019t stick to the ground. The wind was crazy! Got just over an inch of rain. Looking forward to better weather.\"}\n{\"id\": \"1270569070287630337\", \"text\": \"@brettlorenzen And, the reliability of \\u201cNeither snow nor rain nor heat nor gloom of night stays these couriers (the #USPS) from the swift completion of their appointed rounds.\\u201d\"}\n{\"id\": \"1270568690447257601\", \"text\": \"\\\"Because black people get killed in the rain, black lives matter in the rain. It matters all the time. Snow, rain, sleet, sunny days. We're not out here because it's sunny. We're not out here for fun. We're out here because black lives matter.\\\" @wisn12news https://t.co/3kZZ7q2MR9\"}\n{\"id\": \"1270568607605575680\", \"text\": \"Some of the master copies of the film \\u201cGone With the Wind\\u201d are archived at the @librarycongress near \\u201cSnow White and the Seven Dwarfs\\u201d and \\u201cSingin\\u2019 in the Rain.\\u201d GWTW isn\\u2019t going to vanish off the face of the earth.\"}\n{\"id\": \"1270568437916426240\", \"text\": \"Snow Man\\u306eD.D.\\u3068\\nSixTONES\\u306eImitation Rain\\n\\u6d41\\u308c\\u305f\\u301c\"}\n{\"id\": \"1270568195519373313\", \"text\": \"@Nonvieta Yup I work in the sanitation industry. I'm in the office however. Life would not go on without our garbage men and women out there. All day everyday rain snow or shine they out there.\"}\n{\"id\": \"1270567737283117058\", \"text\": \"This picture of a rainbow in WA proves nothing. How do we know if this rainbow was not on Mars or the ISS? Maybe it was drawn in on the picture. WA has mail-in voting so we do have to worry aboug rain, snow, poll workers not showing up or voting machines broke on election day !! https://t.co/5WdHx0acS0 https://t.co/BEKtTpBW9g\"}\n{\"id\": \"1270566386524356608\", \"text\": \"Weather in Oslo at 06:00: Clear Temp: 10.6\\u00b0C Min today: 9.1\\u00b0C Rain today:0.0mm Snow now: 0.0cm Wind N Conditions: Clear Daylight:18:39 hours Sunset: 22:36\"}\n\n\nCounts Endpoint\n---------------\n\nWe can also use the Search API Counts endpoint to get counts of Tweets\nthat match our rule. Each request will return up to *30 days* of results, and\neach count request can be done on a minutely, hourly, or daily basis.\nThe underlying ``ResultStream`` object will handle converting your\nendpoint to the count endpoint, and you have to specify the\n``count_bucket`` argument when making a rule to use it.\n\nThe process is very similar to grabbing Tweets, but has some minor\ndifferences.\n\n.. code:: python\n\n   count_rule = gen_request_parameters(\"snow has:media\", granularity=\"day\")\n\n   counts = collect_results(count_rule, result_stream_args=enterprise_search_args)\n\nOur results are pretty straightforward and can be rapidly used.\n\n::\n\n{\"data\": [{\"end\": \"2021-05-24T00:00:00.000Z\", \"start\": \"2021-05-23T00:00:00.000Z\", \"tweet_count\": 45}, {\"end\": \"2021-05-25T00:00:00.000Z\", \"start\": \"2021-05-24T00:00:00.000Z\", \"tweet_count\": 38}, {\"end\": \"2021-05-26T00:00:00.000Z\", \"start\": \"2021-05-25T00:00:00.000Z\", \"tweet_count\": 62}, {\"end\": \"2021-05-27T00:00:00.000Z\", \"start\": \"2021-05-26T00:00:00.000Z\", \"tweet_count\": 136}, {\"end\": \"2021-05-28T00:00:00.000Z\", \"start\": \"2021-05-27T00:00:00.000Z\", \"tweet_count\": 154}, {\"end\": \"2021-05-29T00:00:00.000Z\", \"start\": \"2021-05-28T00:00:00.000Z\", \"tweet_count\": 101}, {\"end\": \"2021-05-30T00:00:00.000Z\", \"start\": \"2021-05-29T00:00:00.000Z\", \"tweet_count\": 104}, {\"end\": \"2021-05-31T00:00:00.000Z\", \"start\": \"2021-05-30T00:00:00.000Z\", \"tweet_count\": 60}, {\"end\": \"2021-06-01T00:00:00.000Z\", \"start\": \"2021-05-31T00:00:00.000Z\", \"tweet_count\": 70}, {\"end\": \"2021-06-02T00:00:00.000Z\", \"start\": \"2021-06-01T00:00:00.000Z\", \"tweet_count\": 73}, {\"end\": \"2021-06-03T00:00:00.000Z\", \"start\": \"2021-06-02T00:00:00.000Z\", \"tweet_count\": 80}, {\"end\": \"2021-06-04T00:00:00.000Z\", \"start\": \"2021-06-03T00:00:00.000Z\", \"tweet_count\": 426}, {\"end\": \"2021-06-05T00:00:00.000Z\", \"start\": \"2021-06-04T00:00:00.000Z\", \"tweet_count\": 99}, {\"end\": \"2021-06-06T00:00:00.000Z\", \"start\": \"2021-06-05T00:00:00.000Z\", \"tweet_count\": 82}, {\"end\": \"2021-06-07T00:00:00.000Z\", \"start\": \"2021-06-06T00:00:00.000Z\", \"tweet_count\": 74}, {\"end\": \"2021-06-08T00:00:00.000Z\", \"start\": \"2021-06-07T00:00:00.000Z\", \"tweet_count\": 137}, {\"end\": \"2021-06-09T00:00:00.000Z\", \"start\": \"2021-06-08T00:00:00.000Z\", \"tweet_count\": 129}, {\"end\": \"2021-06-10T00:00:00.000Z\", \"start\": \"2021-06-09T00:00:00.000Z\", \"tweet_count\": 76}, {\"end\": \"2021-06-11T00:00:00.000Z\", \"start\": \"2021-06-10T00:00:00.000Z\", \"tweet_count\": 103}, {\"end\": \"2021-06-12T00:00:00.000Z\", \"start\": \"2021-06-11T00:00:00.000Z\", \"tweet_count\": 106}, {\"end\": \"2021-06-13T00:00:00.000Z\", \"start\": \"2021-06-12T00:00:00.000Z\", \"tweet_count\": 81}, {\"end\": \"2021-06-14T00:00:00.000Z\", \"start\": \"2021-06-13T00:00:00.000Z\", \"tweet_count\": 76}, {\"end\": \"2021-06-15T00:00:00.000Z\", \"start\": \"2021-06-14T00:00:00.000Z\", \"tweet_count\": 118}, {\"end\": \"2021-06-16T00:00:00.000Z\", \"start\": \"2021-06-15T00:00:00.000Z\", \"tweet_count\": 109}, {\"end\": \"2021-06-17T00:00:00.000Z\", \"start\": \"2021-06-16T00:00:00.000Z\", \"tweet_count\": 97}, {\"end\": \"2021-06-18T00:00:00.000Z\", \"start\": \"2021-06-17T00:00:00.000Z\", \"tweet_count\": 139}, {\"end\": \"2021-06-19T00:00:00.000Z\", \"start\": \"2021-06-18T00:00:00.000Z\", \"tweet_count\": 150}, {\"end\": \"2021-06-20T00:00:00.000Z\", \"start\": \"2021-06-19T00:00:00.000Z\", \"tweet_count\": 64}, {\"end\": \"2021-06-21T00:00:00.000Z\", \"start\": \"2021-06-20T00:00:00.000Z\", \"tweet_count\": 73}, {\"end\": \"2021-06-22T00:00:00.000Z\", \"start\": \"2021-06-21T00:00:00.000Z\", \"tweet_count\": 103}, {\"end\": \"2021-06-23T00:00:00.000Z\", \"start\": \"2021-06-22T00:00:00.000Z\", \"tweet_count\": 101}], \"meta\": {\"total_tweet_count\": 3266}}\n\nContributing\n============\n\nAny contributions should follow the following pattern:\n\n1. Make a feature or bugfix branch, e.g.,\n   ``git checkout -b my_new_feature``\n2. Make your changes in that branch\n3. Ensure you bump the version number in ``searchtweets/_version.py`` to\n   reflect your changes. We use `Semantic\n   Versioning <https://semver.org>`__, so non-breaking enhancements\n   should increment the minor version, e.g., ``1.5.0 -> 1.6.0``, and\n   bugfixes will increment the last version, ``1.6.0 -> 1.6.1``.\n4. Create a pull request\n\nAfter the pull request process is accepted, package maintainers will\nhandle building documentation and distribution to Pypi.\n\nFor reference, distributing to Pypi is accomplished by the following\ncommands, ran from the root directory in the repo:\n\n.. code:: bash\n\n   python setup.py bdist_wheel\n   python setup.py sdist\n   twine upload dist/*\n\nIf you receive an error during the ``twine upload`` step, it may due to the README.rst\nhaving something invalid in its RST format. Using a RST linter will help fix that.\n\nAlso, as Pypi updates are made, you may want to clear out previous versions from the package.\nThis can be done with this command: ``rm -rf build dist *.egg-info``\n\nHow to build the documentation:\n\nBuilding the documentation requires a few Sphinx packages to build the\nwebpages:\n\n.. code:: bash\n\n   pip install sphinx\n   pip install sphinx_bootstrap_theme\n   pip install sphinxcontrib-napoleon\n\nThen (once your changes are committed to master) you should be able to\nrun the documentation-generating bash script and follow the\ninstructions:\n\n.. code:: bash\n\n   bash build_sphinx_docs.sh master searchtweets\n\nNote that this README is also generated, and so after any README changes\nyou'll need to re-build the README (you need pandoc version 2.1+ for\nthis) and commit the result:\n\n.. code:: bash\n\n   bash make_readme.sh\n\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/twitterdev/search-tweets-python",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "searchtweets-v2",
    "package_url": "https://pypi.org/project/searchtweets-v2/",
    "platform": "",
    "project_url": "https://pypi.org/project/searchtweets-v2/",
    "project_urls": {
      "Homepage": "https://github.com/twitterdev/search-tweets-python"
    },
    "release_url": "https://pypi.org/project/searchtweets-v2/1.1.1/",
    "requires_dist": [
      "requests",
      "pyyaml",
      "python-dateutil"
    ],
    "requires_python": ">=3.3",
    "summary": "Wrapper for Twitter API v2 search endpoints (both 'search Tweets' and 'count Tweets' endpoints).",
    "version": "1.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10799692,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ccc010d01b337e084acb7319699d887e7131930a90f742aa5cece3434a46f103",
          "md5": "6f2f9c2e22c9dfa78ced70fcc0cec385",
          "sha256": "cb12e7e14671aa4bcad056b6e48405612c61026fd1ad3a8eb17046501bd5451e"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6f2f9c2e22c9dfa78ced70fcc0cec385",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 27851,
        "upload_time": "2020-08-12T19:05:05",
        "upload_time_iso_8601": "2020-08-12T19:05:05.157139Z",
        "url": "https://files.pythonhosted.org/packages/cc/c0/10d01b337e084acb7319699d887e7131930a90f742aa5cece3434a46f103/searchtweets_v2-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6edecc65d3385a53cd8512c8b13c1ded3c578c7d1516da305f239fea76c3aaa3",
          "md5": "8c3e6152c7324e88fadeec5c4cb5d48e",
          "sha256": "8cd6df44c5eaee79f284d45ff277e4668cabd9a55c5869841c0d5d07ee210a5e"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "8c3e6152c7324e88fadeec5c4cb5d48e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 32078,
        "upload_time": "2020-08-12T19:05:12",
        "upload_time_iso_8601": "2020-08-12T19:05:12.166978Z",
        "url": "https://files.pythonhosted.org/packages/6e/de/cc65d3385a53cd8512c8b13c1ded3c578c7d1516da305f239fea76c3aaa3/searchtweets-v2-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "622cf20c348ab92a4990239caae6bccb556fd96d612fa2c0921ac0698ac7c4a1",
          "md5": "095733178314e0f0573f972c29b010c8",
          "sha256": "0f1c538f032894432e8630aa02d860604c452700ba2182b4dae97730dce42f3b"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "095733178314e0f0573f972c29b010c8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 28027,
        "upload_time": "2020-08-25T21:38:33",
        "upload_time_iso_8601": "2020-08-25T21:38:33.134635Z",
        "url": "https://files.pythonhosted.org/packages/62/2c/f20c348ab92a4990239caae6bccb556fd96d612fa2c0921ac0698ac7c4a1/searchtweets_v2-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ba62cc3e8a027e56783f0f051b618e5176e6df80064f6a569cb6915a339d6c6d",
          "md5": "8daef0f1b2176beb0bc6fd0b0511386a",
          "sha256": "6f74303df2b38d01e73dfb3c84fb36addcc59aba97ee472bd193ed0fe3e4f199"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "8daef0f1b2176beb0bc6fd0b0511386a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 32187,
        "upload_time": "2020-08-25T21:38:34",
        "upload_time_iso_8601": "2020-08-25T21:38:34.906812Z",
        "url": "https://files.pythonhosted.org/packages/ba/62/cc3e8a027e56783f0f051b618e5176e6df80064f6a569cb6915a339d6c6d/searchtweets-v2-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "80b42b2e165e3814579b356693163eee25ffd843b95ad864fea41ff83cc90527",
          "md5": "1bdc62a8f431cec18596d03e7ea84aae",
          "sha256": "92b32844d7ccb8ef3c3f40b6402594acded52c29c17011ec8b78c06d818844a9"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1bdc62a8f431cec18596d03e7ea84aae",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 28590,
        "upload_time": "2020-08-26T15:54:07",
        "upload_time_iso_8601": "2020-08-26T15:54:07.267339Z",
        "url": "https://files.pythonhosted.org/packages/80/b4/2b2e165e3814579b356693163eee25ffd843b95ad864fea41ff83cc90527/searchtweets_v2-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "91a5095879bd30d3bc580248b211394ea3e2a6e1f760eb6f7c8fc4e17d223d6d",
          "md5": "1813bebff4176bec7c63fdde279c47f7",
          "sha256": "f9837c250ac8b754647ed7349e05c898742ea641d1c7f65eb3eb06c375a14276"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1813bebff4176bec7c63fdde279c47f7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 34001,
        "upload_time": "2020-08-26T15:54:08",
        "upload_time_iso_8601": "2020-08-26T15:54:08.826100Z",
        "url": "https://files.pythonhosted.org/packages/91/a5/095879bd30d3bc580248b211394ea3e2a6e1f760eb6f7c8fc4e17d223d6d/searchtweets-v2-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d2c6e996f0e6ff474c7ef908544c8f25e97eb732c2985192c8e86cbde78dad43",
          "md5": "2e9f7962c89a24ebe2f70669bc771b7f",
          "sha256": "1e95f7a34570b356989918695aa3ae94fe7f4372a4a5247d990a30a9b8285c58"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2e9f7962c89a24ebe2f70669bc771b7f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 28469,
        "upload_time": "2020-08-26T17:15:44",
        "upload_time_iso_8601": "2020-08-26T17:15:44.175327Z",
        "url": "https://files.pythonhosted.org/packages/d2/c6/e996f0e6ff474c7ef908544c8f25e97eb732c2985192c8e86cbde78dad43/searchtweets_v2-1.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "39ce96b45f96914c183ab4b983c7ca34726f5d04939d60aae81d062895d6fe43",
          "md5": "1ddbd047b229c42914d2468609101ab1",
          "sha256": "9f31f72eb3255524f5ff9d76f0089cb3b775464189892182e784000371e1d517"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "1ddbd047b229c42914d2468609101ab1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 33884,
        "upload_time": "2020-08-26T17:15:45",
        "upload_time_iso_8601": "2020-08-26T17:15:45.653625Z",
        "url": "https://files.pythonhosted.org/packages/39/ce/96b45f96914c183ab4b983c7ca34726f5d04939d60aae81d062895d6fe43/searchtweets-v2-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cc7cdc4e7c3508e6850841f72df04b16ee552b8ce788d8d6adca118b6069d8c3",
          "md5": "4abdb7bde0819039b9d3d82903527fd9",
          "sha256": "dd143f89742e8b40c4e476f1a17aae636cfbc0a3df8ef16906da9bc5c91af07f"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4abdb7bde0819039b9d3d82903527fd9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 28633,
        "upload_time": "2020-09-16T20:02:01",
        "upload_time_iso_8601": "2020-09-16T20:02:01.491740Z",
        "url": "https://files.pythonhosted.org/packages/cc/7c/dc4e7c3508e6850841f72df04b16ee552b8ce788d8d6adca118b6069d8c3/searchtweets_v2-1.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ab6cb77cb30c41d84ae9c270e3660338bce492d2c8ccd7c7533fe85bfdeaf3a6",
          "md5": "0dfa95b68570ccd11aed63d72d7eb16b",
          "sha256": "6b8f3b784e9f8c4e13bf85627420c58587d2eae6442ae01ced9de8490fda21f3"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "0dfa95b68570ccd11aed63d72d7eb16b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 34512,
        "upload_time": "2020-09-16T20:02:03",
        "upload_time_iso_8601": "2020-09-16T20:02:03.238781Z",
        "url": "https://files.pythonhosted.org/packages/ab/6c/b77cb30c41d84ae9c270e3660338bce492d2c8ccd7c7533fe85bfdeaf3a6/searchtweets-v2-1.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c3ec267aba21ff3a61336c30f30f2d367416591e7e0ef0a458869b58ff06545b",
          "md5": "3b7bb5476b89b645ed572af3f715ba49",
          "sha256": "5beabbd1089662f514c678f1afcf61141de108153f2674a0697c0206ef47c98b"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3b7bb5476b89b645ed572af3f715ba49",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 29130,
        "upload_time": "2021-02-19T18:06:57",
        "upload_time_iso_8601": "2021-02-19T18:06:57.911757Z",
        "url": "https://files.pythonhosted.org/packages/c3/ec/267aba21ff3a61336c30f30f2d367416591e7e0ef0a458869b58ff06545b/searchtweets_v2-1.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "61b982705a28b8056590a464775ca9d627df5b02ac33c133831b0bc9529be9a5",
          "md5": "1904b0709e22382d18035a04eda3a8b8",
          "sha256": "4d632ba2ba4e28c5870aaac30cb93a4ac055a5f9ce8c1ccc3bb7b5f99b015c31"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "1904b0709e22382d18035a04eda3a8b8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 38396,
        "upload_time": "2021-02-19T18:06:59",
        "upload_time_iso_8601": "2021-02-19T18:06:59.307779Z",
        "url": "https://files.pythonhosted.org/packages/61/b9/82705a28b8056590a464775ca9d627df5b02ac33c133831b0bc9529be9a5/searchtweets-v2-1.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "730927e5797911cdc6fd35aa195cc53d7d3e58182761e395ceb37331d2536f95",
          "md5": "f3adb9a5c15021405218831859e3a0a7",
          "sha256": "ebcf2cf73f30eb6ebb9d3bbf579e8ee6872d3752f136c46bbf80f39c2944a4fd"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f3adb9a5c15021405218831859e3a0a7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 29136,
        "upload_time": "2021-02-23T19:20:18",
        "upload_time_iso_8601": "2021-02-23T19:20:18.726175Z",
        "url": "https://files.pythonhosted.org/packages/73/09/27e5797911cdc6fd35aa195cc53d7d3e58182761e395ceb37331d2536f95/searchtweets_v2-1.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d6d9335769edb059645f2044e3344b7f3173c0bfaed971a37c5406cebe717388",
          "md5": "0222b3dca73c7ce55a8313c0ddbd7443",
          "sha256": "06c2131e89ed369dd48c621c041a2f478398cd20dafa50f4d5d5e73d8d82b6ba"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "0222b3dca73c7ce55a8313c0ddbd7443",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 38411,
        "upload_time": "2021-02-23T19:20:20",
        "upload_time_iso_8601": "2021-02-23T19:20:20.263019Z",
        "url": "https://files.pythonhosted.org/packages/d6/d9/335769edb059645f2044e3344b7f3173c0bfaed971a37c5406cebe717388/searchtweets-v2-1.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d61039bc8e59d1dd000fdf393ceb534eb681eef07acf77f8af9b12389fd5c9a5",
          "md5": "f31e5aa86687b0c6f237a97386f1f393",
          "sha256": "f6e3f6f352161393c763263ec180d3b582db9b82a7fbd61cb78fa6378ca67517"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f31e5aa86687b0c6f237a97386f1f393",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 29127,
        "upload_time": "2021-03-02T03:36:38",
        "upload_time_iso_8601": "2021-03-02T03:36:38.924241Z",
        "url": "https://files.pythonhosted.org/packages/d6/10/39bc8e59d1dd000fdf393ceb534eb681eef07acf77f8af9b12389fd5c9a5/searchtweets_v2-1.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a3a03256bc5a0498f25ee466850a0a1a327730a76b333fd2f9dfe3c00d8cb34a",
          "md5": "4e931f38c8a108614a50d225d8eb9881",
          "sha256": "232dfa52d9ab769ba3aff0f9b2976fc9543200b79c35d75c941a7fa1f8b735a2"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "4e931f38c8a108614a50d225d8eb9881",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 38399,
        "upload_time": "2021-03-02T03:36:40",
        "upload_time_iso_8601": "2021-03-02T03:36:40.514630Z",
        "url": "https://files.pythonhosted.org/packages/a3/a0/3256bc5a0498f25ee466850a0a1a327730a76b333fd2f9dfe3c00d8cb34a/searchtweets-v2-1.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7c4d640756145e1c87a981c4ece4b306b16d72716839bec3ebacb17a3124e2c3",
          "md5": "4ed4cceb24e292895d5654a8c587b1ed",
          "sha256": "f96204b27610d92e193d164db8127a97b69748e90147be6bc26296733d121115"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4ed4cceb24e292895d5654a8c587b1ed",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 32843,
        "upload_time": "2021-06-24T19:20:06",
        "upload_time_iso_8601": "2021-06-24T19:20:06.744520Z",
        "url": "https://files.pythonhosted.org/packages/7c/4d/640756145e1c87a981c4ece4b306b16d72716839bec3ebacb17a3124e2c3/searchtweets_v2-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2dd44fb418f1ad72971fbdd9cfe5596688f2282545e2a5bb4f03d55269ad47c0",
          "md5": "0f1dad5d540a874dfa1b59131c86e3d7",
          "sha256": "65d354f8341153a04d85c261bd59a58364604ed591cae5505ad1182225646029"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0f1dad5d540a874dfa1b59131c86e3d7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 46602,
        "upload_time": "2021-06-24T19:20:58",
        "upload_time_iso_8601": "2021-06-24T19:20:58.333017Z",
        "url": "https://files.pythonhosted.org/packages/2d/d4/4fb418f1ad72971fbdd9cfe5596688f2282545e2a5bb4f03d55269ad47c0/searchtweets-v2-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1a5c13dc2c33acc2f05422bdddd374da53c91fee97a241501a8e334800bab600",
          "md5": "4bfeb2172617eb39c173a435fe815344",
          "sha256": "037c09691d8909baaa5517ca4a84e48dea741dd09b11c2da43b4a54b4ed454d6"
        },
        "downloads": -1,
        "filename": "searchtweets_v2-1.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4bfeb2172617eb39c173a435fe815344",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.3",
        "size": 32937,
        "upload_time": "2021-07-01T19:14:11",
        "upload_time_iso_8601": "2021-07-01T19:14:11.978243Z",
        "url": "https://files.pythonhosted.org/packages/1a/5c/13dc2c33acc2f05422bdddd374da53c91fee97a241501a8e334800bab600/searchtweets_v2-1.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f82c38f379cadad089f05725d00c953f8d1a7d3a62fb283cb13b88aa85baf600",
          "md5": "22e3f66b32559c57abaef2469689741e",
          "sha256": "b267d1649479121d4942ded75075b30475d4701f5a5d7d356e08bee8816ed016"
        },
        "downloads": -1,
        "filename": "searchtweets-v2-1.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "22e3f66b32559c57abaef2469689741e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.3",
        "size": 46764,
        "upload_time": "2021-07-01T19:14:14",
        "upload_time_iso_8601": "2021-07-01T19:14:14.219540Z",
        "url": "https://files.pythonhosted.org/packages/f8/2c/38f379cadad089f05725d00c953f8d1a7d3a62fb283cb13b88aa85baf600/searchtweets-v2-1.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1a5c13dc2c33acc2f05422bdddd374da53c91fee97a241501a8e334800bab600",
        "md5": "4bfeb2172617eb39c173a435fe815344",
        "sha256": "037c09691d8909baaa5517ca4a84e48dea741dd09b11c2da43b4a54b4ed454d6"
      },
      "downloads": -1,
      "filename": "searchtweets_v2-1.1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4bfeb2172617eb39c173a435fe815344",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.3",
      "size": 32937,
      "upload_time": "2021-07-01T19:14:11",
      "upload_time_iso_8601": "2021-07-01T19:14:11.978243Z",
      "url": "https://files.pythonhosted.org/packages/1a/5c/13dc2c33acc2f05422bdddd374da53c91fee97a241501a8e334800bab600/searchtweets_v2-1.1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f82c38f379cadad089f05725d00c953f8d1a7d3a62fb283cb13b88aa85baf600",
        "md5": "22e3f66b32559c57abaef2469689741e",
        "sha256": "b267d1649479121d4942ded75075b30475d4701f5a5d7d356e08bee8816ed016"
      },
      "downloads": -1,
      "filename": "searchtweets-v2-1.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "22e3f66b32559c57abaef2469689741e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.3",
      "size": 46764,
      "upload_time": "2021-07-01T19:14:14",
      "upload_time_iso_8601": "2021-07-01T19:14:14.219540Z",
      "url": "https://files.pythonhosted.org/packages/f8/2c/38f379cadad089f05725d00c953f8d1a7d3a62fb283cb13b88aa85baf600/searchtweets-v2-1.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}