{
  "info": {
    "author": "Juncong Moo;Meta AI",
    "author_email": "JuncongMoo@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.11",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# ðŸ¦™ LLaMA - Run LLM in A Single 4GB GPU\n\n\n> ðŸ“¢ `pyllama` is a hacked version of `LLaMA` based on original Facebook's implementation but more convenient to run in a Single consumer grade GPU.\n\n> The Hugging Face's LLaMA implementation is available at `pyllama.hf`.\n\n## ðŸ“¥ Installation\n\nIn a conda env with pytorch / cuda available, run\n```\npip install pyllama -U\n```\n\n> ðŸ If you have installed llama library from other sources, please uninstall the previous llama library and use `pip install pyllama -U` to install the latest version.\n\n\n## ðŸ“¦ Download Model Files\n\n### ðŸ§˜â€â™€ï¸ Official Way\n\nIn order to download the checkpoints and tokenizer, fill this [google form](https://forms.gle/jk851eBVbX1m5TAv5)\n\nOnce your request is approved, you will receive links to download the tokenizer and model files.\nEdit the `download.sh` script with the signed url provided in the email to download the model weights and tokenizer.\n\n### ðŸ’ Community Way\n\n- 1. pyllama\n\nThere is another high-speed way to download the checkpoints and tokenizers. There are four models(7B,13B,30B,65B) available. To download all of them, run:\n\n```bash\npython -m llama.download\n```\n\nTo download only the 7B model files to your current directory, run:\n\n```bash\npython -m llama.download --model_size 7B\n```\n\nTo download only the 7B and 30B model files to folder `/tmp/pyllama_data`, run:\n\n```bash\npython -m llama.download --model_size 7B,30B --folder /tmp/pyllama_data\n```\n\nThe help doc is:\n```bash\n$python -m llama.download --help\nusage: download.py [-h] [--model_size MODEL_SIZE] [--folder FOLDER]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --model_size MODEL_SIZE\n                        The size of the models that you want to download. A comma separated\n                        string of any of \"7B\", \"13B\", \"30B\", \"65B\". Totally 219G disk space\n                        is needed to download them all. If you only want to download the 7B\n                        model, just put \"7B\" here.\n  --folder FOLDER       The target folder for the download files\n```\n\n- Sample Screenshot\n\n![](docs/download.png)\n\n- 2. Bittorrent\n\nðŸ”¥ In order to download the checkpoints and tokenizer, use this BitTorrent link: \"[magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&dn=LLaMA](magnet:?xt=urn:btih:ZXXDAUWYLRUXXBHUYEMS6Q5CE5WA3LVA&dn=LLaMA)\".\n\n\n## ðŸ’Ž Quantize LLaMA to run in a 4GB GPU\n\n`pyllama` support quantization of 2/3/4/8-bit so that you can run model in a 4G memory GPU.\n\n> You need to run `export HUGGING_FACE_HUB_TOKEN=XXX` to be able to access Hugging Face's data. You also need to install [gptq](https://pypi.org/project/gptq/) with command `pip install gptq`.\n\n```bash\npython -m llama.llama_quant --help\nusage: llama_quant.py [-h] [--ckpt_dir CKPT_DIR] [--tokenizer_path TOKENIZER_PATH] \n                      [--seed SEED] [--nsamples NSAMPLES] [--percdamp PERCDAMP]\n                      [--nearest] [--wbits {2,3,4,8,16}] [--groupsize GROUPSIZE]\n                      [--save SAVE] [--load LOAD] [--benchmark BENCHMARK] [--check]\n                      [--cuda CUDA] [--eval]\n                      {wikitext2,ptb,c4}\n\npositional arguments:\n  {wikitext2,ptb,c4}    Where to extract calibration data from.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --ckpt_dir CKPT_DIR\n  --tokenizer_path TOKENIZER_PATH\n  --seed SEED           Seed for sampling the calibration data.\n  --nsamples NSAMPLES   Number of calibration data samples.\n  --percdamp PERCDAMP   Percent of the average Hessian diagonal to use for dampening.\n  --nearest             Whether to run the RTN baseline.\n  --wbits {2,3,4,8}  bits for quauntization\n  --groupsize GROUPSIZE\n                        Groupsize to use for quantization; default uses full row.\n  --save SAVE           Save quantized checkpoint under this name, eg pyllama-7B4b.pt.\n  --load LOAD           Load quantized model.\n  --benchmark BENCHMARK\n                        Number of tokens to use for benchmarking.\n  --check               Whether to compute perplexity during benchmarking for verification.\n  --cuda CUDA           GPU device string, 'cuda:0' by default.\n  --eval                Evaluate the model with dataset wikitext2, ptb and c4\n```\n\n- Quantize 7B model to 8-bit\n\n```bash\npython -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 8 --save pyllama-7B8b.pt\n```\n\n- Quantize 7B model to 4-bit with groupsize 128 (the recommended setup ðŸ”¥)\n\n```bash\npython -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 4 --groupsize 128 --save pyllama-7B4b.pt\n```\n\n- Quantize 7B model to 2-bit\n\n```bash\npython -m llama.llama_quant decapoda-research/llama-7b-hf c4 --wbits 2 --save pyllama-7B2b.pt\n```\n\nThe download links for quantized LLaMA files are below:\n\n- 7B\n\n| Quant Type   |      Size      |  Link | MD5 |Loss | Password |\n|----------|:-------------:|------:|------:|------:|--:|\n| 2-bit |  2160484475 | [ðŸ”—](https://pan.baidu.com/s/1zOdKOHnSCsz6TFix2NTFtg) | 4c7215d28c1f650218c43fc46402cec5|- | 8g9d |\n| 3-bit |  - | - | -|- |-|\n| 4-bit |  3779485819 | - | cce9a3b522ddf5c011ee0174b2ff3dfb|- |-|\n| 8-bit |  7017493231 | - | 2648b09597cf8f9e0d1a04cb70b71cab|- |-|\n\n\nIt took me 2 hours 40 mins to quantize the 65B model to 4bit. The file size is reduced from 122GB to 32GB.\n\n> The following suggestions are recommended for LLM qunatization:\n> 1. By default, use 4-bit quantization for LLM inference as it offers the total model bits and zero-shot accuracy trade-offs.\n> 2. Use a block size of 128 or lower to stabilize 4-bit quantization and improve zero-shot performance.\n> 3. Use a floating point or quantile quantization data type. In some cases, integer data types might be preferable to improve inference latency depending on the implementation and hardware support.\n\n## ðŸ”® Single GPU Inference\n\n### ðŸ¥¥ Without Quantization\n\nSet the environment variables `CKPT_DIR` as your llamm model folder, for example `/llama_data/7B`, and `TOKENIZER_PATH` as your tokenizer's path, such as `/llama_data/tokenizer.model`.\n\nAnd then run the following command:\n\n```bash\npython inference.py --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH\n```\n\nThe following is an example of LLaMA running in a 8GB single GPU.\n\n![LLaMA Inference](https://raw.githubusercontent.com/juncongmoo/pyllama/main/docs/llama_inference.png)\n\n### ðŸ¥ With Quantization\n\nWith quantization, you can run LLaMA with a 4GB memory GPU.\n\n- pyllama can run 7B model with 6GB GPU memory.\n\n![4bit-quant-6GB](https://github.com/juncongmoo/pyllama/blob/main/docs/pyllama_7B_6GB.png)\n\n- pyllama can run 7B model with 3.2GB GPU memory.\n\n![2bit-quant-6GB](https://github.com/juncongmoo/pyllama/blob/main/docs/pyllama_7B_3GB.png)\n\n### ðŸ’¡ Tips\n\n- To load KV cache in CPU, run `export KV_CAHCHE_IN_GPU=0` in the shell.\n\n- To profile CPU/GPU/Latency, run:\n\n```bash\npython inference_driver.py --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH\n```\n\nA sample result is like:\n\n![LLaMA Inference](https://raw.githubusercontent.com/juncongmoo/pyllama/main/docs/llama_profiling.png)\n\n- Tune `max_seq_len` and `max_batch_size` to reduce memory consumption to be able to run in GPU. Refer to: [this post](https://github.com/juncongmoo/pyllama/issues/9)!\n\n### ðŸ‰ Start a gradio webui\n\n\n```bash\n$ cd apps/gradio\n$ python webapp_single.py  --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH\n```\n\nYou should see something like this in your browser:\n\n![LLaMA Inference](https://raw.githubusercontent.com/juncongmoo/pyllama/main/docs/llama_webui.png)\n\n### ðŸ“ Start a web server\n\nThe following command will start a flask web server:\n\n```bash\n$ cd apps/flask\n$ python web_server_single.py  --ckpt_dir $CKPT_DIR --tokenizer_path $TOKENIZER_PATH\n```\n\n## ðŸ’ Multiple GPU Inference\n\n### ðŸ§˜â€â™€ï¸ Official Way\n\nTo use the original META's model parallel, please set environment variable `PYLLAMA_META_MP` like:\n\n```\nexport PYLLAMA_META_MP=1\n```\n\nWith this environment variable set, you can `import llama` and the original META version's llama will be imported.\n\nThe provided `example.py` can be run on a single or multi-gpu node with `torchrun` and will output completions for two pre-defined prompts. Using `TARGET_FOLDER` as defined in `download.sh`:\n\n```bash\ntorchrun --nproc_per_node MP example.py --ckpt_dir $TARGET_FOLDER/model_size \\\n  --tokenizer_path $TARGET_FOLDER/tokenizer.model\n```\n\nDifferent models require different MP values:\n\n|  Model | MP |\n|--------|----|\n| 7B     | 1  |\n| 13B    | 2  |\n| 30B    | 4  |\n| 65B    | 8  |\n\n### ðŸ’ Community Way\n\nThere are two steps to run LLaMA in multi-GPU environment.\n\n- Convert original LLaMA model\n\n```bash\n$python -m llama.convert_llama --help\nusage: convert_llama.py [-h] [--ckpt_dir CKPT_DIR] [--tokenizer_path TOKENIZER_PATH]\n                        [--model_size {7B,13B,30B,65B}] [--output_dir OUTPUT_DIR]\n                        [--max_batch_size MAX_BATCH_SIZE] [--to {hf,fb}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --ckpt_dir CKPT_DIR\n  --tokenizer_path TOKENIZER_PATH\n  --model_size {7B,13B,30B,65B}\n  --output_dir OUTPUT_DIR\n                        Location to write HF model and tokenizer\n  --max_batch_size MAX_BATCH_SIZE\n  --to {hf,fb}\n```\n\n- Run with HF's accelerate with multiple GPUs\n\n```bash\n$python -m llama.llama_multigpu --help\nusage: llama_multigpu.py [-h] [--state_dict_dir STATE_DICT_DIR] [--model_size {7B,13B,30B,65B}]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --state_dict_dir STATE_DICT_DIR\n  --model_size {7B,13B,30B,65B}\n```\n\n![](https://github.com/juncongmoo/pyllama/blob/main/docs/llama_multigpu.png)\n\n## ðŸ”¬ Model Fine Tuning\n\n### With [Standford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) Instruction-Following Dataset\n\n- Tokenization\n- Finetuning\n- Efficient FT\n\n## ðŸ§¬ LLaMA model structure\n\n- Meta\n- Hugging Face\n\n```\nhttps://github.com/facebookresearch/llama/blob/main/llama/model.py#LL127C27-L127C27\n```\n\n### Model Card\n\nSee [MODEL_CARD.md](https://github.com/juncongmoo/pyllama/blob/main/MODEL_CARD.md)\n\n### License\n\nSee the [LICENSE](https://github.com/juncongmoo/pyllama/blob/main/LICENSE) file.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/juncongmoo/pyllama",
    "keywords": "LLaMA",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pyllama",
    "package_url": "https://pypi.org/project/pyllama/",
    "platform": null,
    "project_url": "https://pypi.org/project/pyllama/",
    "project_urls": {
      "Homepage": "https://github.com/juncongmoo/pyllama"
    },
    "release_url": "https://pypi.org/project/pyllama/0.0.9/",
    "requires_dist": [
      "torch (>=1.12.0)",
      "fairscale (>=0.4.13)",
      "fire (~=0.5.0)",
      "hiq-python (>=1.1.9)",
      "sentencepiece (==0.1.97)",
      "transformers (>=4.26.0) ; extra == 'full'",
      "gptq (>=0.0.2) ; extra == 'full'",
      "sentencepiece (>=0.1.97) ; extra == 'full'",
      "torch (>=1.12.0) ; extra == 'full'",
      "fairscale (>=0.4.13) ; extra == 'full'",
      "fire (~=0.5.0) ; extra == 'full'",
      "hiq-python (>=1.1.9) ; extra == 'full'",
      "sentencepiece (==0.1.97) ; extra == 'full'",
      "transformers (>=4.26.0) ; extra == 'quant'",
      "gptq (>=0.0.2) ; extra == 'quant'",
      "sentencepiece (>=0.1.97) ; extra == 'quant'"
    ],
    "requires_python": "",
    "summary": "ðŸ¦™ LLaMA: Open and Efficient Foundation Language Models in A Single GPU",
    "version": "0.0.9",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17411248,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a160228356517ce4656b48d1b6eb917191c580b2831691ac14af412ac548943e",
          "md5": "b8af0e6a187ac4d045c51902dca0414a",
          "sha256": "f8ed8d85de938d940c2447f2658c5034f6cbe5ab133111419c1119b59315eef6"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b8af0e6a187ac4d045c51902dca0414a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 18855,
        "upload_time": "2023-02-28T04:01:10",
        "upload_time_iso_8601": "2023-02-28T04:01:10.508849Z",
        "url": "https://files.pythonhosted.org/packages/a1/60/228356517ce4656b48d1b6eb917191c580b2831691ac14af412ac548943e/pyllama-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1.dev0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9e0635abf100823da87f2b0e0195db95f19f4b03fa9b841c9be0a263c268510f",
          "md5": "6237699ca9e94352908f22be5809e22d",
          "sha256": "25be533181a8d00e407e55d9ce2947701d7dbf822ec1de0e7cc16404c0e3ae52"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.1.dev0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6237699ca9e94352908f22be5809e22d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 17992,
        "upload_time": "2023-02-27T17:44:02",
        "upload_time_iso_8601": "2023-02-27T17:44:02.273979Z",
        "url": "https://files.pythonhosted.org/packages/9e/06/35abf100823da87f2b0e0195db95f19f4b03fa9b841c9be0a263c268510f/pyllama-0.0.1.dev0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1.dev1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7cc90b05fb70225813945dd2ed940d1ae44b63eae0509aea03df1fe03b93fda4",
          "md5": "1508f4baa6b04c300d7688dc0357f435",
          "sha256": "df2e49ce33991c05237b66626ee1fe11fe7ce0b493a96b1341ed03d556b4e2e7"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.1.dev1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1508f4baa6b04c300d7688dc0357f435",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 18831,
        "upload_time": "2023-02-27T17:54:00",
        "upload_time_iso_8601": "2023-02-27T17:54:00.495463Z",
        "url": "https://files.pythonhosted.org/packages/7c/c9/0b05fb70225813945dd2ed940d1ae44b63eae0509aea03df1fe03b93fda4/pyllama-0.0.1.dev1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1.dev2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a8ee708a47d46f882d1565acff2162cdaac3d64c4b8876971c1114ac985c130a",
          "md5": "b1e6b2d3ee1cdbe68f349948cec3df3c",
          "sha256": "3164b9aa00ad1cf5378944243cdf10a8e2584d51f9199d5330863d6fb21b995f"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.1.dev2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b1e6b2d3ee1cdbe68f349948cec3df3c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 18839,
        "upload_time": "2023-02-27T17:57:52",
        "upload_time_iso_8601": "2023-02-27T17:57:52.870053Z",
        "url": "https://files.pythonhosted.org/packages/a8/ee/708a47d46f882d1565acff2162cdaac3d64c4b8876971c1114ac985c130a/pyllama-0.0.1.dev2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2db9e1e1e905f3151135da273e5ea1d8db4799413c1d5c62e8cdc756de85b666",
          "md5": "0d30af7f5574008e6cb6b99dc138eed1",
          "sha256": "af64309396ba44e3955774979570a354db1521b129abd972c4c58a8c797a5b3f"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0d30af7f5574008e6cb6b99dc138eed1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21382,
        "upload_time": "2023-03-07T01:41:24",
        "upload_time_iso_8601": "2023-03-07T01:41:24.197363Z",
        "url": "https://files.pythonhosted.org/packages/2d/b9/e1e1e905f3151135da273e5ea1d8db4799413c1d5c62e8cdc756de85b666/pyllama-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2.dev0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f4fa5c232bd43cd43b37a3f8a1c80284b3750791171d35827806d61149e9795c",
          "md5": "5b762110e8c7074b2234e1ea396d1b61",
          "sha256": "7f41c7c2fe3eb967510b8127edfccb99ba6b58e8203604b9ada0b040c2eae322"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.2.dev0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5b762110e8c7074b2234e1ea396d1b61",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 18911,
        "upload_time": "2023-02-28T04:12:23",
        "upload_time_iso_8601": "2023-02-28T04:12:23.192159Z",
        "url": "https://files.pythonhosted.org/packages/f4/fa/5c232bd43cd43b37a3f8a1c80284b3750791171d35827806d61149e9795c/pyllama-0.0.2.dev0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2.dev1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "db62ea70358bd48bb89874624919b15d8490ea4732188d42c67b17e4a3d3a3f9",
          "md5": "46ae9637e2a673afb1200109290a9b57",
          "sha256": "d74bf1a775c6369f75ebc3a12b3bb5baaf200c879d3c45a35d4ffbada35ee39c"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.2.dev1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "46ae9637e2a673afb1200109290a9b57",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21406,
        "upload_time": "2023-03-07T01:37:18",
        "upload_time_iso_8601": "2023-03-07T01:37:18.243422Z",
        "url": "https://files.pythonhosted.org/packages/db/62/ea70358bd48bb89874624919b15d8490ea4732188d42c67b17e4a3d3a3f9/pyllama-0.0.2.dev1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2.dev2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "060fe1bde4b40eb4c8af86f2d68e446c5f175ad25fd2871414e8d37f7eca92e3",
          "md5": "4202058ab0eb78757af0ef93870fc1a7",
          "sha256": "a629a29c77c73bee77bf81c66ed03437617233661a6efc6a6420faa646a134a1"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.2.dev2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4202058ab0eb78757af0ef93870fc1a7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21425,
        "upload_time": "2023-03-07T01:39:29",
        "upload_time_iso_8601": "2023-03-07T01:39:29.681734Z",
        "url": "https://files.pythonhosted.org/packages/06/0f/e1bde4b40eb4c8af86f2d68e446c5f175ad25fd2871414e8d37f7eca92e3/pyllama-0.0.2.dev2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c0ab4fc3ffc0806aa4b2148606363ce2acb6069876c1d3dfca1b46a66571c2c4",
          "md5": "caa0ce4f492ed3a087cf14827ac0db0d",
          "sha256": "0df3c6ba520f51c99162405e2f2fd38189771a6c8cad17d820fe37e83f9241f7"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "caa0ce4f492ed3a087cf14827ac0db0d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 30168,
        "upload_time": "2023-03-13T09:55:14",
        "upload_time_iso_8601": "2023-03-13T09:55:14.106213Z",
        "url": "https://files.pythonhosted.org/packages/c0/ab/4fc3ffc0806aa4b2148606363ce2acb6069876c1d3dfca1b46a66571c2c4/pyllama-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a104034b8a9411f23d48db0513b5623a83947a777b9ce75ac3ee426aaf385a3c",
          "md5": "7e6262c2ebfe92b79e058f92ee90fe60",
          "sha256": "75e2025d0b56b3cd9ab85bb92084705f91f7a332610fbe846fb02cf6164edef8"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7e6262c2ebfe92b79e058f92ee90fe60",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 30446,
        "upload_time": "2023-03-13T22:59:54",
        "upload_time_iso_8601": "2023-03-13T22:59:54.002273Z",
        "url": "https://files.pythonhosted.org/packages/a1/04/034b8a9411f23d48db0513b5623a83947a777b9ce75ac3ee426aaf385a3c/pyllama-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c64cf32f9aacc34deea026d54fb4df1792e4bbddca3c76ef2a2ba11b896c966a",
          "md5": "39f4ba7f45824fc4ab82dcba265cad31",
          "sha256": "e5f093c6ff59adf7983b1f66931b552b060d71d900a5e564fedfb0e89200f87c"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "39f4ba7f45824fc4ab82dcba265cad31",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 36395,
        "upload_time": "2023-03-15T07:53:46",
        "upload_time_iso_8601": "2023-03-15T07:53:46.104697Z",
        "url": "https://files.pythonhosted.org/packages/c6/4c/f32f9aacc34deea026d54fb4df1792e4bbddca3c76ef2a2ba11b896c966a/pyllama-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "499c01d9e8868f9f7fe7e86b1a1ef3660408627c3e2e616c36e4d3a7f68b1679",
          "md5": "1068b86f03a68a1e3d4a946410670ffb",
          "sha256": "c528649c500654977d7cbf97c14c057999ac2baf349ef5b4ff7789bb4439d17f"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1068b86f03a68a1e3d4a946410670ffb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 36653,
        "upload_time": "2023-03-15T18:08:44",
        "upload_time_iso_8601": "2023-03-15T18:08:44.076983Z",
        "url": "https://files.pythonhosted.org/packages/49/9c/01d9e8868f9f7fe7e86b1a1ef3660408627c3e2e616c36e4d3a7f68b1679/pyllama-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5c3d5d5c9245d36b05681f410d29e92cee0371696513bc8108b841438e82cf17",
          "md5": "befdc74ca980ea10f6eaa22d0c3d6edd",
          "sha256": "c89cdc6f2e8f1b9789ba037984c462ea1a988d6c5ba661f505426939daccbdd8"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "befdc74ca980ea10f6eaa22d0c3d6edd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 51224,
        "upload_time": "2023-03-17T22:50:18",
        "upload_time_iso_8601": "2023-03-17T22:50:18.199368Z",
        "url": "https://files.pythonhosted.org/packages/5c/3d/5d5c9245d36b05681f410d29e92cee0371696513bc8108b841438e82cf17/pyllama-0.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "265247329ab6e62801decde0c80e63cc3fc9233fa218298c0cc9e4af5cb8f812",
          "md5": "47cff65d24a2c050168d2f3e66dcbb90",
          "sha256": "8d360a7c7cf25f538fdfea2172b2e01a79a6a0299c453fa7066a67bf5556948d"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "47cff65d24a2c050168d2f3e66dcbb90",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 51226,
        "upload_time": "2023-03-17T22:56:10",
        "upload_time_iso_8601": "2023-03-17T22:56:10.451332Z",
        "url": "https://files.pythonhosted.org/packages/26/52/47329ab6e62801decde0c80e63cc3fc9233fa218298c0cc9e4af5cb8f812/pyllama-0.0.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e0ef13c767fdc40c5382e824ffb3c214671a738ab0b64171abd220746b037089",
          "md5": "e81fcc8e3d410ea017e570c3850d113d",
          "sha256": "c7b008f0d3a819cc90de58e533f8a961dc149ab1ae14b140f56a5cb320b92601"
        },
        "downloads": -1,
        "filename": "pyllama-0.0.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e81fcc8e3d410ea017e570c3850d113d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 51510,
        "upload_time": "2023-03-23T07:01:37",
        "upload_time_iso_8601": "2023-03-23T07:01:37.432570Z",
        "url": "https://files.pythonhosted.org/packages/e0/ef/13c767fdc40c5382e824ffb3c214671a738ab0b64171abd220746b037089/pyllama-0.0.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e0ef13c767fdc40c5382e824ffb3c214671a738ab0b64171abd220746b037089",
        "md5": "e81fcc8e3d410ea017e570c3850d113d",
        "sha256": "c7b008f0d3a819cc90de58e533f8a961dc149ab1ae14b140f56a5cb320b92601"
      },
      "downloads": -1,
      "filename": "pyllama-0.0.9-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e81fcc8e3d410ea017e570c3850d113d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 51510,
      "upload_time": "2023-03-23T07:01:37",
      "upload_time_iso_8601": "2023-03-23T07:01:37.432570Z",
      "url": "https://files.pythonhosted.org/packages/e0/ef/13c767fdc40c5382e824ffb3c214671a738ab0b64171abd220746b037089/pyllama-0.0.9-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}