{
  "info": {
    "author": "Hasanain Mehmood",
    "author_email": "hasanain@aicaliber.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<p align=\"center\">\n    <b>\n        <h1 align=\"center\">\n            <em>‚ôæÔ∏è OneSpace ‚ôæÔ∏è</em>\n        </h1>\n    </b>\n</p>\n<p align=\"center\">\n    <em>A high-level Python framework to automate the project lifecycle of Machine and Deep Learning Projects</em>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/onespace\">\n        <img alt=\"PyPI Version\" src=\"https://img.shields.io/pypi/v/onespace?color=g\">\n    </a>\n    <a href=\"https://pypi.org/project/onespace\">\n        <img alt=\"Python Version\" src=\"https://img.shields.io/pypi/pyversions/onespace?color=g\">\n    </a>\n    <a href=\"https://pepy.tech/project/onespace\">\n        <img alt=\"Downloads\" src=\"https://static.pepy.tech/personalized-badge/onespace?period=total&units=international_system&left_color=grey&right_color=brightgreen&left_text=Downloads\">\n    </a>\n    <a href=\"https://github.com/hassi34/onespace\">\n        <img alt=\"Last Commit\" src=\"https://img.shields.io/github/last-commit/hassi34/onespace/main?color=g\">\n    </a>\n    <a href=\"https://github.com/Hassi34/OneSpace/blob/main/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/github/license/hassi34/onespace?color=g\">\n    </a>\n    <a href=\"https://github.com/hassi34/onespace/issues\">\n        <img alt=\"Issue Tracking\" src=\"https://img.shields.io/badge/issue_tracking-github-brightgreen.svg\">\n    </a>\n    <a href=\"https://github.com/hassi34/onespace/issues\">\n        <img alt=\"Open Issues\" src=\"https://img.shields.io/github/issues/hassi34/onespace\">\n    </a>\n    <a href=\"https://pypi.org/project/onespace\">\n        <img alt=\"Github Actions Status\" src=\"https://img.shields.io/github/actions/workflow/status/hassi34/onespace/cicd.yml?branch=main\">\n    </a>\n    <a href=\"https://pypi.org/project/onespace\">\n        <img alt=\"Code Size\" src=\"https://img.shields.io/github/languages/code-size/hassi34/onespace?color=g\">\n    </a>\n    <a href=\"https://pypi.org/project/onespace\">\n        <img alt=\"Repo Size\" src=\"https://img.shields.io/github/repo-size/hassi34/onespace?color=g\">\n    </a>\n</p>\n\n## Overview\n``OneSpace`` enables you to train high performace production ready Machine Learning And Deep Learning Models effortlessly with less than five lines of code. ``OneSpace`` provides you a unified workspace so you can work on all kinds of Machine Learning and Deep Learning problem statements without having to leave your workspace environment. It don't just run everything as a black box to present you results, instead it makes the complete model training process easily explainable using artifacts, logs and plots. ``OneSpace`` also provides you the optional parameters to pass database credentials which will create a table with the project name in the database of your choice and will log all the training activities and the parameters in the database.<br>\nFollowing are the major contents to follow, you can jump to any section:\n\n>   1. [Installation](#install-)\n>   2. [Usage](#use-)\n>   3. [Getting Started with OneSpace (Tutorials)](#tutorials-)<br>\n>      - [Tabular](#tabular-)<br>\n>        - [Training a Regression Model With Tabular Data](#reg-)<br>\n>        - [Training a Classification Model With Tabular Data](#clf-)<br>\n>      - [Computer Vision](#cv-)<br>\n>        - [Training an Image Classification Model with OneSpace(Tensorflow)](#tf-imgclf)<br>\n>        - [Training an Image Classification Model with OneSpace(PyTorch)](#pytorch-imgcls)<br>\n>   4. [Contributing](#contributing-)\n>   5. [Conclusion](#conclusion-)\n### üîó Project Link\n**``OneSpace``** is being distributed through PyPI. Check out the PyPI Package [here](https://pypi.org/project/onespace/)\n\n\n### 1. **Installation**<a id='install-'></a>\nTo avoid any dependency conflict, make sure to create a new Python virtual environment and then install via Pip!\n```bash\npip install onespace\n```\n### 2. **Usage**<a id='use-'></a>\nGet the **[config.py](https://github.com/Hassi34/onespace/blob/main/tabularConfig.py)** and **[training.py](https://github.com/Hassi34/onespace/blob/main/training.py)** files ready. You can get it from this repo or from the following tutorials section. \n- **Prepare ``training.py``**\n```bash\nimport config # In case, you renamed config.py to something else, make sure to use the same name here\nfrom onespace.tabular.regression import Experiment # Importing Experiment class to train a regression model\n\ndef training(config):\n    exp = Experiment(config)\n    exp.run_experiment()\n\nif __name__ == \"__main__\":\n    training(config)\n```\n* Now run the following command in your terminal to start the training job:\n```bash\npython training.py\n```\nPlease following along with these ``quick tutorials``üëá to understand the complete setup and training process.\n### 3. **Getting Started with OneSpace**<a id='tutorials-'></a>\n\n* Ensure you have [Python 3.7+](https://www.python.org/downloads/) installed.\n\n* Create a new Python conda environment for the OneSpace:\n\n```\n$ conda create -n venv  # create venv\n$ conda activate venv  # activate venv\n$ pip install onespace # install onespace\n```\n\n* Create a new Python virtual environment with pip for the OneSpace:\n```\n$ python3 -m venv venv  # create venv\n$ . venv/bin/activate   # activate venv\n$ pip install onespace # install onespace\n```\n#### **Tabular**<a id='tabular-'></a>\nIn this section, you will learn to train a Machine Learning model with ``OneSpace``:\n#### **Training a Regression Model With Tabular Data:**<a id='reg-'></a>\nFirst step is to setup the initial directory. This should present a following tree structure at the beginning of the training:\n```bash\n‚îÇ   .env\n‚îÇ   tabularConfig.py\n‚îÇ   training.py\n‚îÇ\n‚îî‚îÄ‚îÄ‚îÄdata_folder\n        insurance.csv\n``` \nNow let's discuss these files one by one \n* ``.env``<br>\nThis file should only be used when the database integration is required. You will be required to hold the database credentials in ``.env`` as shown below\n```bash\nMONGO_CONN_STR=\"Connection String for MongoDB\"\nMYSQL_HOST=\"database_host(could be an ip or domain)\"\nMYSQL_USER=\"database_user_name\"\nMYSQL_PASS=\"database_password\"\nMYSQL_DB=\"database_name\"\n```\n* ``tabularConfig.py``<br>\nThis file is used to setup the configuration for Experiment. It has standard format as shown below. Parameters could be updated according to the training job requirements\n```python\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Parameters\n#--------------------------------------------------------------------------------------------------------------------------------\ndata_dir = \"data_folder\"\ncsv_file_name = \"insurance.csv\"\ntarget_column =\"charges\"\nautopilot = False    # True if you want to automatically configure everything for the training job and run the job without user interaction else, False.  \neda = False          # Exploratory Data Analysis (EDA)\nmetrics = 'r2_score' # selection_for_classificaton = ['accuracy', 'f1_score', 'recall', 'precision']\n                     # selection_for_regression = ['r2_score', 'mean_absolute_error','mean_squared_error', 'mean_absolute_percentage_error',\n                     # 'median_absolute_error', 'explained_variance_score']\nvalidation_split = 0.20\nscaler = \"RobustScaler\" # available_selections = ['MinMaxScaler', 'StandardScaler', 'MaxAbsScaler', 'RobustScaler']\nimputer = \"SimpleImputer\" # available_selections = ['KNNImputer', ''SimpleImputer']\nPloynomialFeatures = False\nremove_outliers = False\nhandle_imbalance = False # Only applicable to the classification problems.\npca = False               # Principal Component Analysis (PCA).\nfeature_selection = False  # This will use recursive feature elimination (RFE)\n#--------------------------------------------------------------------------------------------------------------------------------\n# Artifacts (Directory names to store the results & resources, can be customized according to the user requirements)\n#--------------------------------------------------------------------------------------------------------------------------------\nproject_name = 'Insurance Project'\nartifacts_dir = \"Artifacts\"\npipelines_dir = \"Pipelines\"\nplots_dir = \"Plots\"\nmodel_name = \"my_test_model\"\nexperiment_name = \"light model testing\"\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Logs (Directory names to record logs, can be customized according to the user requirements)\n#--------------------------------------------------------------------------------------------------------------------------------\nlogs_dir = \"Logs\"\ncsv_logs_dir = \"CSV Logs\"\ncsv_logs_file = \"csv_logs_file\"\ncomments = \"making comparision for optimizers\"\nexecuted_by = 'hasanain'\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Database Integration\n#--------------------------------------------------------------------------------------------------------------------------------\n# Please Note that before making any change in this section, create a .env file and store the mongo db connection string or MySQL credentials in the environment variables \n# Guideline for creating .env is available on project description main page\nfrom dotenv import load_dotenv\nload_dotenv()\ndb_integration_mysql = False\ndb_integration_mongodb = False\n``` \n* ``training.py``<br>\n```python\nimport tabularConfig\nfrom onespace.tabular.regression import Experiment\n\n\ndef training(config):\n    exp = Experiment(config)\n    exp.run_experiment()\n\n\nif __name__ == \"__main__\":\n    training(tabularConfig)\n```\nRun the following command in the terminal to start the training job\n```bash\npython training.py\n```\n**Now let the ``OneSpace`` take care of your end-to-end model training and evaluation process.**<br>\nAfter the training job is completed, the directories in the workspace should look like as follows:\n```bash\n‚îÇ   .env\n‚îÇ   tabularConfig.py\n‚îÇ   training.py\n‚îÇ\n‚îú‚îÄ‚îÄ‚îÄcachedir\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄjoblib\n‚îÇ       ‚îî‚îÄ‚îÄ‚îÄsklearn\n‚îÇ           ‚îî‚îÄ‚îÄ‚îÄpipeline\n‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ_fit_transform_one\n‚îú‚îÄ‚îÄ‚îÄdata_folder\n‚îÇ       insurance.csv\n‚îÇ\n‚îú‚îÄ‚îÄ‚îÄTabular\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄRegression\n‚îÇ       ‚îî‚îÄ‚îÄ‚îÄInsurance Project\n‚îÇ           ‚îú‚îÄ‚îÄ‚îÄArtifacts\n‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄPipelines_on_20221012_at_004113\n‚îÇ           ‚îÇ           GradientBoostingRegressor_on_20221012_at_004124.pkl\n‚îÇ           ‚îÇ           LGBMRegressor_on_20221012_at_004126.pkl\n‚îÇ           ‚îÇ           StackingRegressor_on_20221012_at_004128.pkl\n‚îÇ           ‚îÇ           VotingRegressor_on_20221012_at_004128.pkl\n‚îÇ           ‚îÇ\n‚îÇ           ‚îú‚îÄ‚îÄ‚îÄLogs\n‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄFinal\n‚îÇ           ‚îÇ   ‚îÇ       csv_logs_file.csv\n‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄModel Comparision\n‚îÇ           ‚îÇ   ‚îÇ       modelsComparisionAfterTuning_on_20221012_at_004128..csv\n‚îÇ           ‚îÇ   ‚îÇ       modelsComparisionBeforTuning_on_20221012_at_004116..csv\n‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄPreprocessing\n‚îÇ           ‚îÇ           preprocessed_on_20221012_at_004113..csv\n‚îÇ           ‚îÇ\n‚îÇ           ‚îî‚îÄ‚îÄ‚îÄPlots\n‚îÇ               ‚îú‚îÄ‚îÄ‚îÄEDA\n‚îÇ               ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄEDA_on_20221012_at_004111\n‚îÇ               ‚îÇ           barplot.png\n‚îÇ               ‚îÇ           boxplot.png\n‚îÇ               ‚îÇ           corr_heatmap.png\n‚îÇ               ‚îÇ           countplot.png\n‚îÇ               ‚îÇ           histogram.png\n‚îÇ               ‚îÇ\n‚îÇ               ‚îî‚îÄ‚îÄ‚îÄEvaluation\n‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄEvaluation_on_20221012_at_004120\n‚îÇ                           GradientBoostingRegressor_feature_importance.png\n‚îÇ                           GradientBoostingRegressor_residplot.png\n‚îÇ                           LGBMRegressor_feature_importance.png\n‚îÇ                           LGBMRegressor_residplot.png\n‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ__pycache__\n        tabularConfig.cpython-310.pyc\n\n```\n\n#### **Training a Classification Model With Tabular Data** :<a id='clf-'></a>\nFirst step is to setup the initial directory. This should present a following tree structure at the beginning of the training:\n```bash\n‚îÇ   .env\n‚îÇ   tabularConfig.py\n‚îÇ   training.py\n‚îÇ\n‚îî‚îÄ‚îÄ‚îÄdata_folder\n        titanic.csv\n``` \nNow let's discuss these files one by one \n* ``.env``<br>\nThis file should only be used when the database integration is required. You will be required to hold the database credentials in ``.env`` as shown below\n```bash\nMONGO_CONN_STR=\"Connection String for MongoDB\"\nMYSQL_HOST=\"database_host(could be an ip or domain)\"\nMYSQL_USER=\"database_user_name\"\nMYSQL_PASS=\"database_password\"\nMYSQL_DB=\"database_name\"\n```\n* ``tabularConfig.py``<br>\nThis file is used to setup the configuration for Experiment. It has standard format as shown below. Parameters could be updated according to the training job requirements\n```python\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Parameters\n#--------------------------------------------------------------------------------------------------------------------------------\ndata_dir = \"data_folder\"\ncsv_file_name = \"titanic.csv\"\ntarget_column =\"Survived\"\nautopilot = False    # True if you want to automatically configure everything for the training job and run the job without user interaction else, False.  \neda = True          # Exploratory Data Analysis (EDA)\nmetrics = 'f1_score' # selection_for_classificaton = ['accuracy', 'f1_score', 'recall', 'precision']\n                     # selection_for_regression = ['r2_score', 'mean_absolute_error','mean_squared_error', 'mean_absolute_percentage_error',\n                     # 'median_absolute_error', 'explained_variance_score']\nvalidation_split = 0.20\nscaler = \"RobustScaler\" # available_selections = ['MinMaxScaler', 'StandardScaler', 'MaxAbsScaler', 'RobustScaler']\nimputer = \"SimpleImputer\" # available_selections = ['KNNImputer', ''SimpleImputer']\nPloynomialFeatures = False\nremove_outliers = False\nhandle_imbalance = False # Only applicable to the classification problems.\npca = False               # Principal Component Analysis (PCA).\nfeature_selection = False  # This will use recursive feature elimination (RFE)\n#--------------------------------------------------------------------------------------------------------------------------------\n# Artifacts (Directory names to store the results & resources, can be customized according to the user requirements)\n#--------------------------------------------------------------------------------------------------------------------------------\nproject_name = 'Titanic Project1.0'\nartifacts_dir = \"Artifacts\"\npipelines_dir = \"Pipelines\"\nplots_dir = \"Plots\"\nmodel_name = \"my_test_model\"\nexperiment_name = \"light model testing\"\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Logs (Directory names to record logs, can be customized according to the user requirements)\n#--------------------------------------------------------------------------------------------------------------------------------\nlogs_dir = \"Logs\"\ncsv_logs_dir = \"CSV Logs\"\ncsv_logs_file = \"csv_logs_file\"\ncomments = \"making comparision for optimizers\"\nexecuted_by = 'hasanain'\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Database Integration\n#--------------------------------------------------------------------------------------------------------------------------------\n# Please Note that before making any change in this section, create a .env file and store the mongo db connection string or MySQL credentials in the environment variables \n# Guideline for creating .env is available on project description main page\nfrom dotenv import load_dotenv\nload_dotenv()\ndb_integration_mysql = False\ndb_integration_mongodb = False\n``` \n* ``training.py``<br>\n```python\nimport tabularConfig\nfrom onespace.tabular.classification import Experiment\n\n\ndef training(config):\n    exp = Experiment(config)\n    exp.run_experiment()\n\n\nif __name__ == \"__main__\":\n    training(tabularConfig)\n```\nRun the following command in the terminal to start the training job\n```bash\npython training.py\n```\n**Now let the ``OneSpace`` take care of your end-to-end model training and evaluation process.**<br>\nAfter the training job is completed, the directories in the workspace should look like as follows:\n```bash\n‚îÇ   .env\n‚îÇ   tabularConfig.py\n‚îÇ   training.py\n‚îÇ\n‚îú‚îÄ‚îÄ‚îÄcachedir\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄjoblib\n‚îÇ       ‚îî‚îÄ‚îÄ‚îÄsklearn\n‚îÇ           ‚îî‚îÄ‚îÄ‚îÄpipeline\n‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ_fit_transform_one\n‚îú‚îÄ‚îÄ‚îÄdata_folder\n‚îÇ       titanic.csv\n‚îÇ\n‚îú‚îÄ‚îÄ‚îÄTabular\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄClassification\n‚îÇ       ‚îî‚îÄ‚îÄ‚îÄTitanic Project1.0\n‚îÇ           ‚îú‚îÄ‚îÄ‚îÄArtifacts\n‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄPipelines_on_20221012_at_003107\n‚îÇ           ‚îÇ           ExtraTreesClassifier_on_20221012_at_003128.pkl\n‚îÇ           ‚îÇ           RandomForestClassifier_on_20221012_at_003130.pkl\n‚îÇ           ‚îÇ           StackingClf_on_20221012_at_003136.pkl\n‚îÇ           ‚îÇ           VotingClf_on_20221012_at_003136.pkl\n‚îÇ           ‚îÇ\n‚îÇ           ‚îú‚îÄ‚îÄ‚îÄLogs\n‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄFinal\n‚îÇ           ‚îÇ   ‚îÇ       csv_logs_file.csv\n‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄPreprocessing\n‚îÇ           ‚îÇ           preprocessed_on_20221012_at_003107..csv\n‚îÇ           ‚îÇ\n‚îÇ           ‚îî‚îÄ‚îÄ‚îÄPlots\n‚îÇ               ‚îú‚îÄ‚îÄ‚îÄEDA\n‚îÇ               ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄEDA_on_20221012_at_003048\n‚îÇ               ‚îÇ           boxplot.png\n‚îÇ               ‚îÇ           corr_heatmap.png\n‚îÇ               ‚îÇ           countplot.png\n‚îÇ               ‚îÇ           histogram.png\n‚îÇ               ‚îÇ           pairplot.png\n‚îÇ               ‚îÇ           pie_bar.png\n‚îÇ               ‚îÇ           violinplot.png\n‚îÇ               ‚îÇ\n‚îÇ               ‚îî‚îÄ‚îÄ‚îÄEvaluation\n‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄEvaluation_on_20221012_at_003124\n‚îÇ                           auc_roc_plot.png\n‚îÇ                           ExtraTreesClassifier_confusion_matrix_count.png\n‚îÇ                           ExtraTreesClassifier_confusion_matrix_pct.png\n‚îÇ                           ExtraTreesClassifier_feature_importance.png\n‚îÇ                           RandomForestClassifier_confusion_matrix_count.png\n‚îÇ                           RandomForestClassifier_confusion_matrix_pct.png\n‚îÇ                           RandomForestClassifier_feature_importance.png\n‚îÇ                           StackingClf_confusion_matrix_count.png\n‚îÇ                           StackingClf_confusion_matrix_pct.png\n‚îÇ                           VotingClf_confusion_matrix_count.png\n‚îÇ                           VotingClf_confusion_matrix_pct.png\n‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ__pycache__\n        tabularConfig.cpython-310.pyc\n```\n#### **Computer Vision**<a id='cv-'></a>\nThis API enable users to create image classification models with Tensorflow or PyTorch.\nMore services for computer vision API are being developed.\n#### **Training an Image Classification Model with OneSpace(Tensorflow)** :<a id='tf-imgclf'></a>\nFirst step is to setup the initial directory. This should present a following tree structure at the beginning of the training:\n```bash\n‚îÇ   .env\n‚îÇ   tensorflowConfig.py\n‚îÇ   training.py\n‚îÇ   \n‚îî‚îÄ‚îÄ‚îÄdata_folder\n    ‚îú‚îÄ‚îÄ‚îÄtrain\n    ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄants\n    ‚îÇ   ‚îÇ       0013035.jpg\n    ‚îÇ   ‚îÇ       1030023514_aad5c608f9.jpg\n    ‚îÇ   ‚îÇ       1095476100_3906d8afde.jpg\n    ‚îÇ   ‚îÇ       1099452230_d1949d3250.jpg\n    ‚îÇ   ‚îÇ       116570827_e9c126745d.jpg\n    ‚îÇ   ‚îÇ\n    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄbees\n    ‚îÇ           1092977343_cb42b38d62.jpg\n    ‚îÇ           1093831624_fb5fbe2308.jpg\n    ‚îÇ           1097045929_1753d1c765.jpg\n    ‚îÇ           1232245714_f862fbe385.jpg\n    ‚îÇ           129236073_0985e91c7d.jpg\n    ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄval\n        ‚îú‚îÄ‚îÄ‚îÄants\n        ‚îÇ       10308379_1b6c72e180.jpg\n        ‚îÇ       1053149811_f62a3410d3.jpg\n        ‚îÇ       1073564163_225a64f170.jpg\n        ‚îÇ\n        ‚îî‚îÄ‚îÄ‚îÄbees\n                1032546534_06907fe3b3.jpg\n                10870992_eebeeb3a12.jpg\n                1181173278_23c36fac71.jpg\n\n\n\n``` \nNow let's discuss these files one by one \n* ``.env``<br>\nThis file should only be used when the database integration is required. You will be required to hold the database credentials in ``.env`` as shown below\n```bash\nMONGO_CONN_STR=\"Connection String for MongoDB\"\nMYSQL_HOST=\"database_host(could be an ip or domain)\"\nMYSQL_USER=\"database_user_name\"\nMYSQL_PASS=\"database_password\"\nMYSQL_DB=\"database_name\"\n```\n* ``tensorflowConfig.py``<br>\nThis file is used to setup the configuration for Experiment. It has standard format as shown below. Parameters could be updated according to the training job requirements\n```python\n#--------------------------------------------------------------------------------------------------------------------------------\n# Parameters\n#--------------------------------------------------------------------------------------------------------------------------------\ndata_dir = \"data_folder\" # main directory for data\ntrain_folder_name = \"train\" # name of the training folder. Pass \"None\" if there is no training and validation folder available\nval_folder_name = \"val\"     # name of the validation folder pass \"None\" if there is no training and validation folder available\ntransfer_learning = False    # Pass \"False\" if you want to train the model from scratch\nmodel_architecture = \"MobileNetV3Small\" # available_selections = [\"VGG16\", \"MobileNetV3Large\", \"MobileNetV3Small\", \"DenseNet201\", \"EfficientNetV2L\", \"EfficientNetV2S\", \"ResNet50\", \"ResNet50V2\", \"ResNet152V2\", \"VGG19\", \"Xception\"]\nfreeze_all = True           # Will freeze the weights for all the layers except for last one (Dense Layer)\nfreeze_till = None          # selection = [-1,-2,-3,-4,-5] => if you want to freeze weights until a specific layer, select any value from the list and observe the trainable prams\naugmentation = False         # Pass True if the augmentation should be applied on the images. It helps with regularization.\nepochs = 2                 # Number of Epochs\nbatch_size = 64\ninput_shape = (224, 224, 3) \nactivation = \"relu\"         # available_selections = [\"relu\",\"selu\", \"swish\", \"elu\", \"gelu\", \"tanh\"] => This param is used when there is no transferlearning involved and model is being trained from scratch \nactivation_output = \"softmax\" # available_selections = [\"softmax\",\"sigmoid\", \"hard_sigmoid\"]\nloss_function = \"binary_crossentropy\" # available_selection = [\"binary_crossentropy\",\"categorical_crossentropy\" , \"sparse_categorical_crossentropy\"]\nmetrics = [\"AUC\",\"Recall\" ]   # available_selection = [\"accuracy\", \"AUC\", \"Precision\", \"Recall\" ]\nlr_scheduler = 'InverseTimeDecay' #availabel_selection = [\"InverseTimeDecay\",\"CosineDecay\", \"ExponentialDecay\", \"CosineDecayRestarts\", \"PolynomialDecay\"] # can be observed on tensorboard\noptimizer = \"Adam\"          # available_selection = [\"SGD\", \"Adam\", \"RMSprop\", \"Adadelta\", \"Adagrad\", \"Adamax\", \"Nadam\", \"Ftrl\"]\nvalidation_split = 0.20     # The proportion of data should be used for validation. This param won't be used if a seperate Validation Data Folder is being passed\nes_patience = 5             # Early Stopping Patience\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Artifacts (Directory names to store the results & resources, can be customized according to the user requirements)\n#--------------------------------------------------------------------------------------------------------------------------------\nproject_name = 'AntsBeesProject_tf'\nartifacts_dir = \"Artifacts\"\nmodel_dir = \"Models\"\nplots_dir = \"Plots\"\nmodel_name = \"my_test_model\"\nexperiment_name = \"Exp for Demo\"\nplot_name = \"results_plot\"\nmodel_ckpt_dir = \"Model Checkpoints\"\ncallbacked_model_name = \"model_ckpt\"\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Logs (Directory names to record logs, can be customized according to the user requirements)\n#--------------------------------------------------------------------------------------------------------------------------------\nlogs_dir = \"Logs\"\ntensorboard_root_log_dir = \"Tensorboard Logs\"\ncsv_logs_dir = \"CSV Logs\"\ncsv_logs_file = \"cv_test_logs.csv\"\ncomments = \"Running a demo with transfer learning\"\nexecuted_by = 'Hasanain'\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Database Integration\n#--------------------------------------------------------------------------------------------------------------------------------\n# Please Note that before making any change in this section, create a .env file and store the mongo db connection string or MySQL credentials in the environment variables \n# Guideline for creating .env is available on project description main page\nfrom dotenv import load_dotenv\nload_dotenv()\ndb_integration_mysql = False\ndb_integration_mongodb = False \n``` \n* ``training.py``<br>\n```python\nimport tensorflowConfig\nfrom onespace.tensorflow.cv import Experiment\n\n\ndef training(config):\n    exp = Experiment(config)\n    exp.run_experiment()\n\n\nif __name__ == \"__main__\":\n    training(tensorflowConfig)\n```\nRun the following command in the terminal to start the training job\n```bash\npython training.py\n```\n**Now let the ``OneSpace`` take care of your end-to-end model training and evaluation process.**<br>\nAfter the training job is completed, the directories in the workspace should look like as follows:\n```bash\n‚îÇ   .env\n‚îÇ   tensorflowConfig.py\n‚îÇ   training.py\n‚îÇ   \n‚îú‚îÄ‚îÄ‚îÄComputerVision\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄTensorFlow\n‚îÇ       ‚îî‚îÄ‚îÄ‚îÄAntsBeesProject_tf\n‚îÇ           ‚îú‚îÄ‚îÄ‚îÄArtifacts\n‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄImages\n‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄTraining Images\n‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ       img_grid__on_20221012_at_090752.png\n‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ       \n‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄValidation Images\n‚îÇ           ‚îÇ   ‚îÇ           img_grid__on_20221012_at_090752.png\n‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄModels\n‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄModel Checkpoints\n‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ       model_ckpt__on_20221012_at_090755_.h5\n‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄTrainedModels\n‚îÇ           ‚îÇ   ‚îÇ           my_test_model__on_20221012_at_090800_.h5\n‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄPlots\n‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ‚îÄmodels\n‚îÇ           ‚îÇ       ‚îÇ       my_test_model__on_20221012_at_091731.png\n‚îÇ           ‚îÇ       ‚îÇ\n‚îÇ           ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄresults\n‚îÇ           ‚îÇ               results_plot__on_20221012_at_091731.png\n‚îÇ           ‚îÇ\n‚îÇ           ‚îî‚îÄ‚îÄ‚îÄLogs\n‚îÇ               ‚îú‚îÄ‚îÄ‚îÄCSV Logs\n‚îÇ               ‚îÇ       cv_test_logs.csv\n‚îÇ               ‚îÇ\n‚îÇ               ‚îî‚îÄ‚îÄ‚îÄTensorboard Logs\n‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄlogs__on_20221012_at_090755\n‚îÇ                       ‚îú‚îÄ‚îÄ‚îÄtrain\n‚îÇ                       ‚îÇ       events.out.tfevents.1665540475.MY-LAPTOP.143340.0.v2\n‚îÇ                       ‚îÇ\n‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄvalidation\n‚îÇ                               events.out.tfevents.1665540477.MY-LAPTOP.143430.1.v2\n‚îÇ\n‚îú‚îÄ‚îÄ‚îÄdata_folder\n‚îÇ   ‚îú‚îÄ‚îÄ‚îÄtrain\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄants\n‚îÇ   ‚îÇ   ‚îÇ       0013035.jpg\n‚îÇ   ‚îÇ   ‚îÇ       1030023514_aad5c608f9.jpg\n‚îÇ   ‚îÇ   ‚îÇ       1095476100_3906d8afde.jpg\n‚îÇ   ‚îÇ   ‚îÇ       1099452230_d1949d3250.jpg\n‚îÇ   ‚îÇ   ‚îÇ       116570827_e9c126745d.jpg\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄbees\n‚îÇ   ‚îÇ           1092977343_cb42b38d62.jpg\n‚îÇ   ‚îÇ           1093831624_fb5fbe2308.jpg\n‚îÇ   ‚îÇ           1097045929_1753d1c765.jpg\n‚îÇ   ‚îÇ           1232245714_f862fbe385.jpg\n‚îÇ   ‚îÇ           129236073_0985e91c7d.jpg\n‚îÇ   ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄval\n‚îÇ       ‚îú‚îÄ‚îÄ‚îÄants\n‚îÇ       ‚îÇ       10308379_1b6c72e180.jpg\n‚îÇ       ‚îÇ       1053149811_f62a3410d3.jpg\n‚îÇ       ‚îÇ       1073564163_225a64f170.jpg\n‚îÇ       ‚îÇ\n‚îÇ       ‚îî‚îÄ‚îÄ‚îÄbees\n‚îÇ               1032546534_06907fe3b3.jpg\n‚îÇ               10870992_eebeeb3a12.jpg\n‚îÇ               1181173278_23c36fac71.jpg\n‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ__pycache__\n        tensorflowConfig.cpython-310.pyc\n```\n\n\n#### **Training an Image Classification Model with OneSpace(PyTorch)** :<a id='pytorch-imgcls'></a>\nFirst step is to setup the initial directory. This should present a following tree structure at the beginning of the training:\n```bash\n‚îÇ   .env\n‚îÇ   pytorchConfig.py\n‚îÇ   training.py\n‚îÇ   \n‚îî‚îÄ‚îÄ‚îÄdata_folder\n    ‚îú‚îÄ‚îÄ‚îÄtrain\n    ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄants\n    ‚îÇ   ‚îÇ       0013035.jpg\n    ‚îÇ   ‚îÇ       1030023514_aad5c608f9.jpg\n    ‚îÇ   ‚îÇ       1095476100_3906d8afde.jpg\n    ‚îÇ   ‚îÇ       1099452230_d1949d3250.jpg\n    ‚îÇ   ‚îÇ       116570827_e9c126745d.jpg\n    ‚îÇ   ‚îÇ\n    ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄbees\n    ‚îÇ           1092977343_cb42b38d62.jpg\n    ‚îÇ           1093831624_fb5fbe2308.jpg\n    ‚îÇ           1097045929_1753d1c765.jpg\n    ‚îÇ           1232245714_f862fbe385.jpg\n    ‚îÇ           129236073_0985e91c7d.jpg\n    ‚îÇ\n    ‚îî‚îÄ‚îÄ‚îÄval\n        ‚îú‚îÄ‚îÄ‚îÄants\n        ‚îÇ       10308379_1b6c72e180.jpg\n        ‚îÇ       1053149811_f62a3410d3.jpg\n        ‚îÇ       1073564163_225a64f170.jpg\n        ‚îÇ\n        ‚îî‚îÄ‚îÄ‚îÄbees\n                1032546534_06907fe3b3.jpg\n                10870992_eebeeb3a12.jpg\n                1181173278_23c36fac71.jpg\n``` \nNow let's discuss these files one by one \n* ``.env``<br>\nThis file should only be used when the database integration is required. You will be required to hold the database credentials in ``.env`` as shown below\n```bash\nMONGO_CONN_STR=\"Connection String for MongoDB\"\nMYSQL_HOST=\"database_host(could be an ip or domain)\"\nMYSQL_USER=\"database_user_name\"\nMYSQL_PASS=\"database_password\"\nMYSQL_DB=\"database_name\"\n```\n* ``pytorchConfig.py``<br>\nThis file is used to setup the configuration for Experiment. It has standard format as shown below. Parameters could be updated according to the training job requirements\n```python\n#--------------------------------------------------------------------------------------------------------------------------------\n# Parameters\n#--------------------------------------------------------------------------------------------------------------------------------\ndata_dir = \"data_folder\"\ntrain_folder_name = \"train\"\nval_folder_name = \"val\"\ntransfer_learning = True\nmodel_architecture = \"MobileNet_v3_small\" # available_selections = [\"AlexNet\", \"ConvnNeXt\", \"DenseNet121\", \"DenseNet201\", \"EfficientNet_b7\", \"EfficientNet_v2_s\", \"EfficientNet_v2_m\", \"EfficientNet_v2_l\", \"Wide_Resnet50_2\",\n                                         #\"GoogleNet\", \"Inception_v3\", \"MnasNet0_5\", \"MnasNet1_3\", \"MobileNet_v2\", \"MobileNet_v3_large\", \"MobileNet_v3_small\", \"RegNet_y_32gf\", \"ResNet18\",\n                                         #\"ResNet34\", \"ResNet50\", \"ResNet152\", \"ResNext101_32x8d\", \"ShuffleNet_v2_x1_5\", \"SqueezeNet1_0\", \"VGG11\", \"VGG13\", \"VGG16\", \"VGG19\", \"VisionTransformer\"]\n                                         \naugmentation = False\nepochs = 1\nbatch_size = 32\ninput_shape = (224, 224, 3)\nlr_scheduler = \"ExponentialLR\"           # available_selections = [\"OneCycleLR\",\"StepLR\", \"LambdaLR\", \"ExponentialLR\"]\noptimizer = \"RMSprop\"                        # available_selections = [\"SGD\", \"Adam\", \"Adadelta\", \"Adagrad\", \"RMSprop\"]\nvalidation_split = 0.20\ngrad_clip = 1                        # Should be a value ranging from 0.5 to 1.0 OR None\nweight_decay = 0.001                 # Should be a value ranging from 0.5 to 1.0\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Artifacts (Directory names to store the results & resources, can be customized according to the user requirements)\n#--------------------------------------------------------------------------------------------------------------------------------\nproject_name = 'AntsBeensProject_pytorch'\nartifacts_dir = \"Artifacts\"\nmodel_dir = \"Models\"\nplots_dir = \"Plots\"\nmodel_name = \"antbees_model\"\nexperiment_name = \"Demo Experiment\"\nplot_name = \"results_plot\"\nmodel_ckpt_dir = \"Model Checkpoints\"\ncallbacked_model_name = \"model_ckpt\"\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Logs (Directory names to record logs, can be customized according to the user requirements)\n#--------------------------------------------------------------------------------------------------------------------------------\nlogs_dir = \"Logs\"\ntensorboard_root_log_dir = \"Tensorboard Logs\"\ncsv_logs_dir = \"CSV Logs\"\ncsv_logs_file = \"cv_test_logs.csv\"\ncomments = \"This is a Demo\"\nexecuted_by = 'Hasanain'\n\n#--------------------------------------------------------------------------------------------------------------------------------\n# Database Integration\n#--------------------------------------------------------------------------------------------------------------------------------\n# Please Note that before making any change in this section, create a .env file and store the mongo db connection string or MySQL credentials in the environment variables \n# Guideline for creating .env is available on project description main page\nfrom dotenv import load_dotenv\nload_dotenv()\ndb_integration_mysql = False\ndb_integration_mongodb = False \n``` \n* ``training.py``<br>\n```python\nimport pytorchConfig\nfrom onespace.pytorch.cv import Experiment\n\n\ndef training(config):\n    exp = Experiment(config)\n    exp.run_experiment()\n\n\nif __name__ == \"__main__\":\n    training(pytorchConfig)\n```\nRun the following command in the terminal to start the training job\n```bash\npython training.py\n```\n**Now let the ``OneSpace`` take care of your end-to-end model training and evaluation process.**<br>\nAfter the training job is completed, the directories in the workspace should look like as follows:\n```bash\n‚îÇ   .env\n‚îÇ   pytorchConfig.py\n‚îÇ   training.py\n‚îÇ   \n‚îú‚îÄ‚îÄ‚îÄComputerVision\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄPyTorch\n‚îÇ       ‚îî‚îÄ‚îÄ‚îÄAntsBeensProject_pytorch\n‚îÇ           ‚îú‚îÄ‚îÄ‚îÄArtifacts\n‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄImages\n‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄTraining Images\n‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ       img_grid__on_20221012_at_093114.png\n‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ       \n‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄValidation Images\n‚îÇ           ‚îÇ   ‚îÇ           img_grid__on_20221012_at_093114.png\n‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄModels\n‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄBaseModels\n‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ       MobileNet_v3_small__on_20221012_at_093140_.pth\n‚îÇ           ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄTrainedModels\n‚îÇ           ‚îÇ   ‚îÇ           antbees_model__on_20221012_at_093227_.pth\n‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄPlots\n‚îÇ           ‚îÇ       ‚îî‚îÄ‚îÄ‚îÄresults\n‚îÇ           ‚îÇ               results_plot__on_20221012_at_093227.png\n‚îÇ           ‚îÇ\n‚îÇ           ‚îî‚îÄ‚îÄ‚îÄLogs\n‚îÇ               ‚îú‚îÄ‚îÄ‚îÄCSV Logs\n‚îÇ               ‚îÇ       cv_test_logs.csv\n‚îÇ               ‚îÇ\n‚îÇ               ‚îî‚îÄ‚îÄ‚îÄTensorboard Logs\n‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄlogs__on_20221012_at_093052\n‚îÇ                       ‚îÇ   events.out.tfevents.1665541852.My-Laptop.86003.0\n‚îÇ                       ‚îÇ   projector_config.pbtxt\n‚îÇ                       ‚îÇ\n‚îÇ                       ‚îú‚îÄ‚îÄ‚îÄ00000\n‚îÇ                       ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄdefault\n‚îÇ                       ‚îÇ           metadata.tsv\n‚îÇ                       ‚îÇ           sprite.png\n‚îÇ                       ‚îÇ           tensors.tsv\n‚îÇ                       ‚îÇ\n‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ1665541924.1492205\n‚îÇ                               events.out.tfevents.1665541924.My-Laptop.86003.1\n‚îÇ\n‚îú‚îÄ‚îÄ‚îÄdata_folder\n‚îÇ   ‚îú‚îÄ‚îÄ‚îÄtrain\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ‚îÄants\n‚îÇ   ‚îÇ   ‚îÇ       0013035.jpg\n‚îÇ   ‚îÇ   ‚îÇ       1030023514_aad5c608f9.jpg\n‚îÇ   ‚îÇ   ‚îÇ       1095476100_3906d8afde.jpg\n‚îÇ   ‚îÇ   ‚îÇ       1099452230_d1949d3250.jpg\n‚îÇ   ‚îÇ   ‚îÇ       116570827_e9c126745d.jpg\n‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄbees\n‚îÇ   ‚îÇ           1092977343_cb42b38d62.jpg\n‚îÇ   ‚îÇ           1093831624_fb5fbe2308.jpg\n‚îÇ   ‚îÇ           1097045929_1753d1c765.jpg\n‚îÇ   ‚îÇ           1232245714_f862fbe385.jpg\n‚îÇ   ‚îÇ           129236073_0985e91c7d.jpg\n‚îÇ   ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄval\n‚îÇ       ‚îú‚îÄ‚îÄ‚îÄants\n‚îÇ       ‚îÇ       10308379_1b6c72e180.jpg\n‚îÇ       ‚îÇ       1053149811_f62a3410d3.jpg\n‚îÇ       ‚îÇ       1073564163_225a64f170.jpg\n‚îÇ       ‚îÇ\n‚îÇ       ‚îî‚îÄ‚îÄ‚îÄbees\n‚îÇ               1032546534_06907fe3b3.jpg\n‚îÇ               10870992_eebeeb3a12.jpg\n‚îÇ               1181173278_23c36fac71.jpg\n‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ__pycache__\n        pytorchConfig.cpython-310.pyc\n\n```\n\n### 4. **Contributing**<a id='contributing-'></a>\nYes, Please! We believe that there is alot of oportunity to make Machine Learning more interesting and less complicated for the comunity, so let's make it more efficient, let's go with low-code!!\n\n### 5. **Conclusion**<a id='conclusion-'></a>\nAll the services which are being provided by ``OneSpace`` could be managed in a single directory without having to leave your workspace. A user only needs to take care of two things before running a training job:\n* Setup the relavent configurations in ``config.py``.<br>\n* Check if you are importing the right module in ``training.py``.<br>\n* Now type ``python training.py`` on the terminal and hit enter.<br>\n* Voila, you have a trained production ready model saved in the workspace, Great Work!\n#### **Please give this repository a star if you find our work useful, Thank you! üôè**<br><br>\n\n**Copyright &copy; 2022 OneSpace** <br>\nLet's connect on **[``LinkedIn``](https://www.linkedin.com/in/hasanain-mehmood-a37a4116b/)** <br>\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Hassi34/onespace",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "onespace",
    "package_url": "https://pypi.org/project/onespace/",
    "platform": null,
    "project_url": "https://pypi.org/project/onespace/",
    "project_urls": {
      "Bug Tracker": "https://github.com/Hassi34/onespace/issues",
      "Homepage": "https://github.com/Hassi34/onespace"
    },
    "release_url": "https://pypi.org/project/onespace/0.0.3/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "A high-level Python framework to automate the project lifecycle of Machine and Deep Learning Projects",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16171401,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e0aac318bae81895088fb2f5a7c707c6753913f3de3d77ef60258757d332f0a2",
          "md5": "edb0b431c13ff48bfa3a097ce0fbc84f",
          "sha256": "ff55e551fec399d04907bf5cc97e79167cf66a626297594d7e2769cbacb20ca3"
        },
        "downloads": -1,
        "filename": "onespace-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "edb0b431c13ff48bfa3a097ce0fbc84f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 23931,
        "upload_time": "2022-09-22T08:32:39",
        "upload_time_iso_8601": "2022-09-22T08:32:39.902882Z",
        "url": "https://files.pythonhosted.org/packages/e0/aa/c318bae81895088fb2f5a7c707c6753913f3de3d77ef60258757d332f0a2/onespace-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "331b395a819b52c4ea8d9d956d34c7de4a3466ace00e2bd3677d27ea23fc2d36",
          "md5": "97534bb4e0e406dfcf5541a686b0366f",
          "sha256": "8f4050162bd046f16d8f8215114aeaaf0f113af8160ddafdaaf8339adbcacd1c"
        },
        "downloads": -1,
        "filename": "onespace-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "97534bb4e0e406dfcf5541a686b0366f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 58956,
        "upload_time": "2022-10-12T03:35:15",
        "upload_time_iso_8601": "2022-10-12T03:35:15.535065Z",
        "url": "https://files.pythonhosted.org/packages/33/1b/395a819b52c4ea8d9d956d34c7de4a3466ace00e2bd3677d27ea23fc2d36/onespace-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1.post2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e47a20df7d0c6c5c96e4aec833c21527986f1737c3e0517b21be86a59844290",
          "md5": "931a7f48665c648edb0fd97ccf098dc7",
          "sha256": "2887c4e35e68eade20c10c517eac639ba790cbfa353fd921f50af71e2c182084"
        },
        "downloads": -1,
        "filename": "onespace-0.0.1.post2.tar.gz",
        "has_sig": false,
        "md5_digest": "931a7f48665c648edb0fd97ccf098dc7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 58931,
        "upload_time": "2022-10-12T07:45:43",
        "upload_time_iso_8601": "2022-10-12T07:45:43.261944Z",
        "url": "https://files.pythonhosted.org/packages/5e/47/a20df7d0c6c5c96e4aec833c21527986f1737c3e0517b21be86a59844290/onespace-0.0.1.post2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1b1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "82bb82857e2757d853a049c1c0ab8b6c234976921a62f9b3ac887fcf28f229b3",
          "md5": "1ddae3303f7e347ef8f255ff1ea5ecd4",
          "sha256": "bceaf1a3262ce0989aac7864ffaadf6dd81d2c7c74b1ee1f1ffe4a1137fc76c8"
        },
        "downloads": -1,
        "filename": "onespace-0.0.1b1.tar.gz",
        "has_sig": false,
        "md5_digest": "1ddae3303f7e347ef8f255ff1ea5ecd4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 40978,
        "upload_time": "2022-10-11T10:15:14",
        "upload_time_iso_8601": "2022-10-11T10:15:14.466968Z",
        "url": "https://files.pythonhosted.org/packages/82/bb/82857e2757d853a049c1c0ab8b6c234976921a62f9b3ac887fcf28f229b3/onespace-0.0.1b1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "76a0ceccb079aaf6a6c0665a2ca8de33795f8ce9e611e010b1ee1015b4918f58",
          "md5": "71f9b30d4dd7e8b5d9f7dc6b869b19a9",
          "sha256": "11c488b6314999277ef85f05ad0b69b486b0d44a6c201b72db16fae733dda1dd"
        },
        "downloads": -1,
        "filename": "onespace-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "71f9b30d4dd7e8b5d9f7dc6b869b19a9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 58962,
        "upload_time": "2022-10-14T13:35:10",
        "upload_time_iso_8601": "2022-10-14T13:35:10.490499Z",
        "url": "https://files.pythonhosted.org/packages/76/a0/ceccb079aaf6a6c0665a2ca8de33795f8ce9e611e010b1ee1015b4918f58/onespace-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8c4840638bc4bb1e770264d2a933a7d22acccf1d11ccf056ac932fe015ae92e3",
          "md5": "8bebf0c278858c7a04478363d64eec9f",
          "sha256": "ae42d49b1a6c848ef4b80295308c618accf3263a0b5b2240a7b13b34a064474a"
        },
        "downloads": -1,
        "filename": "onespace-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "8bebf0c278858c7a04478363d64eec9f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 59021,
        "upload_time": "2022-12-21T08:59:35",
        "upload_time_iso_8601": "2022-12-21T08:59:35.896845Z",
        "url": "https://files.pythonhosted.org/packages/8c/48/40638bc4bb1e770264d2a933a7d22acccf1d11ccf056ac932fe015ae92e3/onespace-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3.dev1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "af5646d5ff2487606a89da97a19adea174c64088fc3b87c3f252bcc4826ef004",
          "md5": "bf27d61d89130176ebe6a7f87274ff57",
          "sha256": "bf2116dde31b5913ba3ad98864d3e39272eb957044a54d276a8a41b92a2d95fe"
        },
        "downloads": -1,
        "filename": "onespace-0.0.3.dev1.tar.gz",
        "has_sig": false,
        "md5_digest": "bf27d61d89130176ebe6a7f87274ff57",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 59173,
        "upload_time": "2022-11-11T09:14:49",
        "upload_time_iso_8601": "2022-11-11T09:14:49.793250Z",
        "url": "https://files.pythonhosted.org/packages/af/56/46d5ff2487606a89da97a19adea174c64088fc3b87c3f252bcc4826ef004/onespace-0.0.3.dev1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8c4840638bc4bb1e770264d2a933a7d22acccf1d11ccf056ac932fe015ae92e3",
        "md5": "8bebf0c278858c7a04478363d64eec9f",
        "sha256": "ae42d49b1a6c848ef4b80295308c618accf3263a0b5b2240a7b13b34a064474a"
      },
      "downloads": -1,
      "filename": "onespace-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "8bebf0c278858c7a04478363d64eec9f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 59021,
      "upload_time": "2022-12-21T08:59:35",
      "upload_time_iso_8601": "2022-12-21T08:59:35.896845Z",
      "url": "https://files.pythonhosted.org/packages/8c/48/40638bc4bb1e770264d2a933a7d22acccf1d11ccf056ac932fe015ae92e3/onespace-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}