{
  "info": {
    "author": "Alëna Aksënova",
    "author_email": "loisetoil@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Information Analysis",
      "Topic :: Text Processing :: Linguistic"
    ],
    "description": "Readme last updated: March 23, 2020\n\n\n# [_SigmaPie_](https://github.com/alenaks/SigmaPie) for subregular and subsequential grammar induction\n\n### Installation\n`pip install sigmapie`\n\n### Usage\n\n```python\nimport sigmapie\n# learning UTP pattern with an SP grammar\nlanguage = sigmapie.SP(polar=\"n\")\nlanguage.k = 3\nlanguage.data = [\"HHLLL\", \"LLHHH\", \"LHHHLL\", \"LLL\", \"HHH\"]\nlanguage.alphabet = [\"H\", \"L\"]\nlanguage.learn()\nlanguage.grammar # [('H', 'L', 'H')]\nlanguage.scan(\"HHLLLL\") # True\nlanguage.scan(\"HLLH\") # False\nlanguage.generate_sample(n = 3) # ['LLHH', 'HL', 'HHHL']\n```\n\n## Subregular languages and mappings in phonology\n\nThis toolkit is relevant for anyone who is working or going to work with subregular grammars both from the perspectives of theoretical linguistics and formal language theory.\n\n**Why theoretical linguists might be interested in formal language theory?** <br>\n_Formal language theory_ explains how potentially infinite string sets, or _formal languages_,\ncan be generalized to grammars encoding the desired patterns and what properties those\ngrammars have. It also allows one to compare different grammars regarding parameters such as expressivity.\n\n**The Chomsky hierarchy** aligns the main classes of formal languages with respect to their expressive power [(Chomsky 1959)](http://www.cs.utexas.edu/~cannata/pl/Class%20Notes/Chomsky_1959%20On%20Certain%20Formal%20Properties%20of%20Grammars.pdf).\n\n  * **Regular** grammars are as powerful as finite-state devices or regular expressions: they can \"count\" only until a certain threshold (no `a^n b^n` patterns);\n  * **Context-free** grammars have access to potentially infinite _stack_ that allows them to reproduce patterns that involve center embedding;\n  * **Mildly context-sensitive** grammars are powerful enough to handle cross-serial dependencies such as some types of copying;\n  * **Context-sensitive** grammars can handle non-linear patterns such as `a^2^n` for `n > 0`;\n  * **Recursively enumerable** grammars are as powerful as any theoretically possible computer and generate languages such as `a^n`, where `n` is a prime number.\n\n\nBoth phonology and morphology frequently display properties of regular languages.\n\n**Phonology** does not require the power of center-embedding, which is a property of context-free languages. For example, consider a harmony where the first vowel agrees with the last vowel, the second vowel agrees with the pre-last one, etc.\n    \n    GOOD: \"arugula\", \"tropicalization\", \"electrotelethermometer\", etc.\n    BAD:  any other word violating the rule.\n\n\nWhile it is a theoretically possible pattern, harmonies of that type are unattested in natural languages.\n\n**Morphology** avoids center-embedding as well. In [Aksënova et al. (2016)](https://www.aclweb.org/anthology/W16-2019) we show that it is possible to iterate prefixes with the meaning \"after\" in Russian. In Ilocano, where the same semantics is expressed via a circumfix, its iteration is prohibited.\n    \n    RUSSIAN: \"zavtra\" (tomorrow), \"posle-zavtra\" (the day after tomorrow), \n             \"posle-posle-zavtra\" (the day after the day after tomorrow), ...\n    ILOCANO: \"bigat\" (morning), \"ka-bigat-an\" (the next morning),\n             <*>\"ka-ka-bigat-an-an\" (the morning after the next one).\n\n\nMoreover, typological review of patterns shows that phonology and morphology do not require the full power of regular languages. As an example of an unattested pattern, [Heinz (2011)](http://jeffreyheinz.net/papers/Heinz-2011-CPF.pdf) provides a language where a word must have an even number of vowels to be well-formed.\n\n\nRegular languages can be sub-divided into another nested hierarchy of languages decreasing in their expressive power: **subregular hierarchy**.\nAmong some of the most important characteristics of subregular languages is their learnability only from the positive data: more powerful classes require negative input as well.\n\nHowever, when there is a _mapping_, i.e. a correspondence of input form to its output counterpart, one can use subregular mappings.\nIn linguistics, indeed, mapping underlying representations (URs) to surface forms (SFs) it is a frequent task.\nOne of the ways to encode a rule of a mapping is to use a **transducer**.\nA class of less-than-regular, or subregular transducers are **subsequential transducers**. \nThey deterministically read input strings one-by-one, and output the corresponding part of the output string.\n\nThe _SigmaPie_ toolkit currently contains functionality for the following subregular language and grammar classes:\n  * strictly piecewise (SP);\n  * strictly local (SL);\n  * tier-based strictly local (TSL);\n  * multiple tier-based strictly local (MTSL).\n  \nAdditionally, it implements the OSTIA algorithm that learns subsequential mappings following [Oncina, Garcia and Vidal (1993)](https://www.semanticscholar.org/paper/Learning-Subsequential-Transducers-for-Pattern-Oncina-Garc%C3%ADa/ee7cd648ed7be8a413192cecb956d0c0de29f648) and [Colin de la Higuera (2014)](https://www.cambridge.org/core/books/grammatical-inference/CEEB229AC5A80DFC6436D860AC79434F).\n\n## The functionality of the toolkit\n\n  * **Learners** extract grammars from string sets.\n  * **Scanners** evaluate strings with respect to a given grammar.\n  * **Sample generators** generate stringsets for a given grammar.\n  * **FSM constructors** translate subregular grammars to finite state machines.\n  * **Polarity converters** switch negative grammars to positive, and vice versa.\n\n## Strictly piecewise languages\n\n**Negative strictly piecewise (SP)** grammars prohibit the occurrence of sequences of symbols at an arbitrary distance from each other.\nEvery SP grammar is associated with the value of `k` that defines the size of the longest sequence that this grammar can prohibit. Alternatively, if the grammar is positive, it lists all subsequences that are allowed in well-formed words of the language.\n\n    k = 2\n    POLARITY: negative\n    GRAMMAR:  ab, ba\n    LANGUAGE: accaacc, cbccc, cccacaaaa, ...\n              <*>accacba, <*>bcccacbb, <*>bccccccca, ...\n              \n              \nIn phonology, an example of an SP pattern is _tone plateauing_ discussed in [Jardine (2015,](https://adamjardine.net/files/jardinecomptone-short.pdf) [2016)](https://adamjardine.net/files/jardine2016dissertation.pdf).\nFor example, in Luganda (Bantu) a low tone (L) cannot intervene in-between two high tones (H): L is changed to H in such configurations.\nThe prosodic domain cannot have more than one stretch of H tones.\n\n**Luganda verb and noun combinations** ([Hyman and Katamba (2010)](http://linguistics.berkeley.edu/phonlab/documents/2010/Hyman_Katamba_Paris_PLAR.pdf), cited by [Jardine (2016)](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/B01C656A2B96316F3ADCC836BD2A6244/S0952675716000129a.pdf/computationally_tone_is_different.pdf))\n\n  * /tw-áa-mú-láb-a, walúsimbi/ ---> tw-áá-mu-lab-a, walúsimbi <br>\n    ‘we saw him, Walusimbi’ <br>\n    **HHLLL, LHLL**\n    \n  * /tw-áa-láb-w-a walúsimbi/ ---> tw-áá-láb-wá wálúsimbi <br>\n    ‘we were seen by Walusimbi’ <br>\n    **HHHHHHLL**\n    \n  * /tw-áa-láb-a byaa=walúsimbi/ ---> tw-áá-láb-á byáá-wálúsimbi <br>\n    ‘we saw those of Walusimbi’ <br>\n    **HHHHHHHHLL**\n    \nThis pattern can be described using SP grammar `*HLH`.\n\n### Learning tone plateauing pattern\n\nLet us say that `luganda` represents a \"toy\" example of tone plateauing (TP) pattern.\n\n\n```python\nluganda = [\"LLLL\", \"HHLLL\", \"LHHHLL\", \"LLLLHHHH\"]\n```\n\nOur goal will be to learn the SP generalization behind TP.\n\nNegative and positive SP grammars are implemented as the class `SP()`. The next code cell initialized a positive SP grammar `tp_pattern`.\n\n\n```python\ntp_pattern = SP()\n```\n\n### Attributes of SP grammars\n  * `polar` (\"p\" or \"n\") is the polarity of the grammar, by default, it is \"p\";\n  * `alphabet` (list) is the set of symbols that the grammar uses;\n  * `grammar` (list of tuples) is the list of allowed or prohibited substructures of the language;\n  * `k` (int) is the size of the locality window of the grammar, by default, it is 2;\n  * `data` (list of string) is the learning sample;\n  * `fsm` (FSM object) is the finite state device that corresponds to the grammar.\n  \nThe initial step is to define the training sample and the alphabet.\n\n\n```python\ntp_pattern.data = luganda\ntp_pattern.alphabet = [\"H\", \"L\"]\n```\n\nBy default, the locality window of the grammar is 2.\n\n\n```python\nprint(\"Locality of the SP grammar:\", tp_pattern.k)\n```\n\nSP attributes can be directly accessed. For example, let us change the locality of the window from 2 to 3:\n\n\n```python\ntp_pattern.k = 3\nprint(\"Locality of the SP grammar:\", tp_pattern.k)\n```\n\n### Methods defined for SP grammars\n  * `check_polarity()` and `switch_polarity()` display and change the polarity of the grammar;\n  * `learn()` extracts prohibited or allowed subsequences from the training sample;\n  * `scan(string)` tells if a given string is well-formed with respect to the learned grammar;\n  * `extract_alphabet()` collects alphabet based on the data or grammar attributes;\n  * `generate_sample(n, repeat)` generates $n$ strings based on the given grammar; by default, `repeat` is set to False, and repetitions of the generated strings are not allowed, but this parameter can be set to True;\n  * `fsmize()` creates the corresponding FSM family by following the steps outlined in [Heinz and Rogers (2013)](https://www.aclweb.org/anthology/W13-3007);\n  * `subsequences(string)` returns all `k`-piecewise subsequences of the given string;\n  * `generate_all_ngrams()` generates all possible strings of the length $k$ based on the provided alphabet.\n\n**Checking and changing the polarity of the grammar**\n\nBy default, the grammars are positive. The polarity can be checked by running the `check_polarity` method:\n\n\n```python\nprint(\"Polarity of the grammar:\", tp_pattern.check_polarity())\n```\n\nIf the polarity needs to be changed, this can be done using the `switch_polarity` method. It will automatically switch the grammar, if one is provided or already extracted, to the opposite one.\n\n\n```python\ntp_pattern.switch_polarity()\nprint(\"Polarity of the grammar:\", tp_pattern.check_polarity())\n```\n\n**Learning the SP grammar**\n\nMethod `learn` extracts allowed or prohibited subsequences from the learning sample based on the polarity of the grammar and the locality window.\n\n\n```python\ntp_pattern.learn()\nprint(\"Extracted grammar:\", tp_pattern.grammar)\n```\n\nIndeed, it learned the TP pattern!\n\n$n$-grams are represented as tuples of strings, because in this case, elements of the alphabet are not restricted to characters.\n\n**Scanning strings and telling if they are part of the language**\n\n`scan` takes a string as input and returns True or False depending on the well-formedness of the given strings with respect to the encoded grammar.\n\n\n```python\ntp = [\"HHHLLL\", \"L\", \"HHL\", \"LLHLLL\"]\nno_tp = [\"LLLLHLLLLH\", \"HLLLLLLH\", \"LLLHLLLHLLLHL\"]\n\nprint(\"Tonal plateauing:\")\nfor string in tp:\n    print(\"String\", string, \"is in L(G):\", tp_pattern.scan(string))\n    \nprint(\"\\nNo tonal plateauing:\")\nfor string in no_tp:\n    print(\"String\", string, \"is in L(G):\", tp_pattern.scan(string))\n```\n\n**Generating a data sample**\n\nBased on the learned grammar, a data sample of the desired size can be generated.\n\n\n```python\nsample = tp_pattern.generate_sample(n = 10)\nprint(\"Sample:\", sample)\n```\n\n**Extracting subsequences**\n\nFinally, this toolkit can be used also in order to extract subsequences from the input word by feeding it to the `subsequences` method.\n\n\n```python\ntp_pattern.k = 3\nprint(\"k = 3:\", tp_pattern.subsequences(\"regular\"), \"\\n\")\ntp_pattern.k = 5\nprint(\"k = 5:\", tp_pattern.subsequences(\"regular\"))\n```\n\nWhile SP languages capture multiple long-distance processes such as tone plateauings or some harmonies, they are unable to encode a blocking effect or purely local processes.\n\n## Strictly local languages\n\n**Negative strictly local (SL)** grammars prohibit the occurrence of consecutive substrings consisting of up to `k` symbols. The value of `k` defines the longest substring that cannot be present in a well-formed string of a language. Positive SL grammars define substrings that can be present in the language.\n\nImportantly, to define _first_ and _last_ elements, SL languages use delimiters (\">\" and \"<\") that indicate the beginning and the end of the string.\n\n    k = 2\n    POLARITY: positive\n    GRAMMAR:  >a, ab, ba, b<\n    LANGUAGE: ab, abab, abababab, ...\n              <*>babab, <*>abaab, <*>bababba, ...\n\nIn phonology, very frequently changes involve adjacent segments, and the notion of locality is therefore extremely important. The discussion of local processes in phonology can be found in [(Chandlee 2014)](http://dspace.udel.edu/bitstream/handle/19716/13374/2014_Chandlee_Jane_PhD.pdf).\n\n\n**Russian word-final devoicing**\n\nIn Russian, the final obstruent of a word cannot be voiced. <br>\n  * \"lug\" \\[luK\\] _meadow_ ---> \"lug-a\" \\[luGa\\] _of the meadow_\n  * \"luk\" \\[luK\\] _onion_ ---> \"luk-a\" \\[luKa\\] _of the onion_\n  * \"porog\" \\[paroK\\] _doorstep_ ---> \"porog-a\" \\[paroGa\\] _of the doorstep_\n  * \"porok\" \\[paroK\\] _vice_ ---> \"porok-a\" \\[paroKa\\] _of the vice_\n\n### Learning word-final devoicing\n\nAssume the following toy dataset where the following mapping is defined:\n  * \"a\" stands for a vowel;\n  * \"b\" stands for a voiced obstruent;\n  * \"p\" stands for any other consonant.\n\n\n```python\nrussian = [\"\", \"ababa\", \"babbap\", \"pappa\", \"pabpaapba\" \"aap\"]\n```\n\nIn this term, the Russian word-final devoicing generalization would be _\"do not have \"b\" at the end of the word\"_.\n\nThis pattern can then be described using SL grammar `*b<`.\n\nLet us initialize an SL object.\n\n\n```python\nwf_devoicing = SL()\nwf_devoicing.data = russian\n```\n\n### Attributes of SL grammars\n  * `polar` (\"p\" or \"n\") is the polarity of the grammar, by default, it is \"p\";\n  * `alphabet` (list) is the set of symbols that the grammar uses;\n  * `grammar` (list of tuples) is the list of allowed or prohibited substructures of the language;\n  * `k` (int) is the size of the locality window of the grammar, by default, it is 2;\n  * `data` (list of string) is the learning sample;\n  * `edges` (list of two characters) are the delimiters used by the grammar, the default value is \">\" and \"<\";\n  * `fsm` (FSM object) is the finite state device that corresponds to the grammar.\n  \n### Methods defined for SL grammars\n  * `check_polarity()` and `switch_polarity()` display and change the polarity of the grammar;\n  * `learn()` extracts prohibited or allowed substrings from the training sample;\n  * `scan(string)` tells if a given string is well-formed with respect to a learned grammar;\n  * `extract_alphabet()` collects alphabet based on the data or grammar;\n  * `generate_sample(n, repeat)` generates $n$ strings based on the given grammar; by default, `repeat` is set to False, and repetitions of the generated strings are not allowed, but this parameter can be set to True;\n  * `fsmize()` creates the corresponding FSA;\n  * `clean_grammar()` removes useless k-grams from the grammar.\n\n**Extracting alphabet and learning SL grammar**\n\nAs before, `learn()` extracts dependencies from the data. It simply extracts k-grams of the indicated size from the data, and the default value of `k` is 2.\n\n\n```python\nwf_devoicing.learn()\nprint(\"The grammar is\", wf_devoicing.grammar)\n```\n\nIn order to automatically extract the alphabet from the data, it is possible to run `extract_alphabet()`.\n\n\n```python\nprint(\"The original value of the alphabet is\", wf_devoicing.alphabet)\nwf_devoicing.extract_alphabet()\nprint(\"The modified value of the alphabet is\", wf_devoicing.alphabet)\n```\n\n**Changing polarity of the grammar**\n\nThe grammar outputted above is positive. If we want to capture the pattern using restrictions rather then the allowed substrings, we can `switch_polarity()` of the grammar:\n\n\n```python\nwf_devoicing.switch_polarity()\nprint(\"The grammar is\", wf_devoicing.grammar)\n```\n\n**Scanning strings**\n\nAs before, `scan(string)` method returns True or False depending on the well-formedness of the given string with respect to the learned grammar.\n\n\n```python\nwfd = [\"apapap\", \"papa\", \"abba\"]\nno_wfd = [\"apab\", \"apapapb\"]\n\nprint(\"Word-final devoicing:\")\nfor string in wfd:\n    print(\"String\", string, \"is in L(G):\", wf_devoicing.scan(string))\n    \nprint(\"\\nNo word-final devoicing:\")\nfor string in no_wfd:\n    print(\"String\", string, \"is in L(G):\", wf_devoicing.scan(string))\n```\n\n**Generating data samples**\n\nIf the grammar is non-empty, the data sample can be generated in the same way as before: `generate_sample(n, repeat)`, where `n` is the number of examples that need to be generated, and `repeat` is a flag allowing or prohibiting repetitions of the same strings in the generated data.\n\n\n```python\nsample = wf_devoicing.generate_sample(5, repeat = False)\nprint(sample)\n```\n\n**Cleaning grammar**\n\nPotentially, a grammar that user provides can contain \"useless\" k-grams. For example, consider the following grammar:\n\n\n```python\nsl = SL()\nsl.grammar = [(\">\", \"a\"), (\"b\", \"a\"), (\"a\", \"b\"), (\"b\", \"<\"),\n              (\">\", \"g\"), (\"f\", \"<\"), (\"t\", \"t\")]\nsl.alphabet = [\"a\", \"b\", \"g\", \"f\", \"t\"]\n```\n\nThis grammar contains 3 useless bigrams:\n  \n  * `(\">\", \"g\")` can never be used because nothing can follow \"g\";\n  * `(\"f\", \"<\")` is useless because there is no way to start a string that would lead to \"f\";\n  * `(\"t\", \"t\")` has both problems listed above.\n  \nMethod `clean_grammar()` removes such $n$-grams by constructing a corresponding FSM and trimming its inaccessible states.\n\n\n```python\nprint(\"Old grammar:\", sl.grammar)\nsl.clean_grammar()\nprint(\"Clean grammar:\", sl.grammar)\n```\n\nEven though SP and SL languages can capture a large portion of phonological well-formedness conditions, there are numerous examples of patterns that require increased complexity. For example, **harmony with a blocking effect** cannot be captured using SP grammars because they will \"miss\" a blocker, and cannot be encoded via SL grammars because they cannot be used for long-distance processes.\n\n## Tier-based strictly local languages\n\n**Tier-based strictly local (TSL)** grammars operate just like the strictly local ones, but they have the power to _ignore_ a certain set of symbols completely. The set of symbols that are not ignored are called **tier** symbols, and the ones that do not matter for the well-formedness of strings are the **non-tier** ones [(Heinz et al. 2011)](https://pdfs.semanticscholar.org/b934/bfcc962f65e19ae139426668e8f8054e5616.pdf).\n\nAssume that we have the following sets of tier and non-tier symbols.\n\n    tier = [l, r]\n    non_tier = [c, d]\n    \nNon-tiers symbols are ignored when the strings are being evaluated by TSL grammars, so the alphabets `tier` and `non_tier` define the following mapping:\n\n  * <b>l</b>cc<b>r</b>dc<b>l</b>cddc<b>rl</b>c ---> <b>lrlrl</b>\n  * <b>rl</b>dcd<b>r</b>cc<b>l</b>dcd<b>r</b>d<b>l</b> ---> <b>rlrlrl</b>\n  * cdcddcdcdcdc -> _e_\n\nThe strings on the right-hand side are called _tier images_ of the original strings because they exclude all non-tier symbols. Then the TSL grammars can be defined as _SL grammars that operate on a tier._\n\nContinuing the example above, let's prohibit \"l\" following \"l\" unless \"r\" intervenes, and also ban \"r\" following \"r\" unless \"l\" intervenes (it yields a toy Latin dissimilation pattern). Over the `tier`, the negative TSL grammar with the restrictions `*ll` and `*rr` expresses this rule.\n\nIntuitively, TSL grammars make non-local dependencies local by evaluating only tier images of strings.\n\n**Latin liquid dissimilation**\n\nIn Latin, liquids tend to alternate: if the final liquid of the stem is \"l\", the adjectival affix is realized as \"aris\". And vice versa, if the final liquid is \"r\", the choice of the affix is \"alis\". Consider the examples below.\n\n  * mi<b>l</b>ita<b>r</b>is \\~ <*>mi<b>l</b>ita<b>l</b>is _\"military\"_\n  * f<b>l</b>o<b>r</b>a<b>l</b>is \\~ <*>f<b>l</b>o<b>r</b>a<b>r</b>is _\"floral\"_\n  * p<b>l</b>u<b>r</b>a<b>l</b>is \\~ <*>p<b>l</b>u<b>r</b>a<b>r</b>is _\"plural\"_\n  \nThis pattern is _not SP_ because SP grammars cannot exhibit the blocking effect, and it is _not SL_ either due to its long-distance nature.\n\n\n```python\nlat_dissim = TSL()\n```\n\n### Attributes of TSL grammars\n  * `polar` (\"p\" or \"n\") is the polarity of the grammar, the default value is \"p\";\n  * `alphabet` (list) is the set of symbols that the grammar uses;\n  * `grammar` (list of tuples) is the list of allowed or prohibited substrings of the language;\n  * `k` (int) is the size of the locality window of the grammar, by default, it is $2$;\n  * `data` (list of string) is the learning sample;\n  * `edges` (list of two characters) are the delimiters used by the grammar, the default value is \">\" and \"<\";\n  * `fsm` (FSM object) is the finite state device that corresponds to the grammar;\n  * `tier` (list) is the list of the tier symbols.\n  \n### Methods defined for TSL grammars\n  * `check_polarity()` and `switch_polarity()` display and change the polarity of the grammar;\n  * `learn()` detects tier symbols and learns the tier grammar;\n  * `tier_image(string)` returns the tier image of a given string;\n  * `scan(string)` tells if a given string is well-formed with respect to the learned grammar;\n  * `extract_alphabet()` collects alphabet based on the data or grammar;\n  * `generate_sample(n, repeat)` generates $n$ strings based on the given grammar; by default, `repeat` is set to False, and repetitions of the generated strings are not allowed, but this parameter can be set to True;\n  * `fsmize()` creates the corresponding FSA;\n  * `clean_grammar()` removes useless k-grams from the grammar.\n\n### Learning liquid dissimilation\n\nAssume the toy Latin dissimilation dataset, where we mask every non-liquid as \"c\".\n\n\n```python\nlat_dissim.data = [\"ccc\", \"lccrcccclcr\", \"lrl\", \"rcclc\"]\n```\n\n**Extracting alphabet**\n\nWe don't need to explicitly provide the alphabet. Instead, it can be extracted from the data using the `extract_alphabet()` method.\n\n\n```python\nlat_dissim.extract_alphabet()\nprint(\"Alphabet:\", lat_dissim.alphabet)\n```\n\n**Learning the tier and the grammar**\n\nAfter the alphabet is extracted and the training sample is provided, we can learn the dependency.\n\n\n```python\nlat_dissim.learn()\nprint('Tier:   ', lat_dissim.tier)\nprint('Grammar:', lat_dissim.grammar)\n```\n\n**Switching polarity**\n\nBy-default, the grammars are positive, but this pattern is more clear when represented as a restriction. We can convert the positive grammar to negative using the `switch_polarity()` method.\n\n\n```python\nprint(\"Initial polarity of the grammar:\", lat_dissim.check_polarity(), \"\\n\")\nlat_dissim.switch_polarity()\nprint(\"New polarity of the grammar:\", lat_dissim.check_polarity())\nprint(\"New grammar:\", lat_dissim.grammar)\n```\n\n### Learning stress culminativity\n\nWe can learn a negative grammar directly as well. For example, let us learn a pattern like this:\n\n    aaabaaaa, baaaa, aaaaaba, aaaaaab, ...\n    <*>aababaaa, <*>baaaababb, <*>aaaa, ...\n    \nIn simple words, the desired pattern is _a single \"b\" must be present in a string_. Translating it to a pattern relevant to linguistics would give us _stress culminativity_.\n\n\n```python\nstress = TSL(polar=\"n\")\nstress.data = [\"aaabaaaa\", \"baaaa\", \"aaaaaba\", \"aaaaaab\"]\nstress.extract_alphabet()\nstress.learn()\n\nprint(\"Tier:    \", stress.tier)\nprint(\"Grammar: \", stress.grammar)\n```\n\nThe learned negative TSL grammar prohibits an empty tier (stress must be present in a word) and prohibits a tier where there is more than a single stress.\n\n**Generating sample**\n\nData sample generation is also available for the class of TSL languages. Repetition of the same items within the dataset can be allowed or prohibited by changing the parameter `repeat`.\n\n\n```python\nprint(stress.generate_sample(n=10, repeat=True))\n```\n\n\n```python\nprint(stress.generate_sample(n=10, repeat=False))\n```\n\nThe implemented learning algorithm for k-TSL languages is designed by [McMullin and Jardine (2017)](https://adamjardine.net/files/jardinemcmullin2016tslk.pdf), which is based on [Jardine and Heinz (2016)](http://jeffreyheinz.net/papers/Jardine-Heinz-2016-LTSLL.pdf).\n\nHowever, there are some phonological processes that require more power than TSL. Some languages have more than just a single long-distance assimilation: for example, separate vowel and consonantal harmonies. In this case, one tier is not enough: putting both vowels and consonants on a single tier will create the desired locality neither among vowels nor among consonants. For cases like this, a subregular class of _multiple tier-based strictly local languages_ is especially useful.\n\n## Multiple tier-based strictly local languages\n\nThere are numerous examples from the typological literature that show that there are phonological patterns complexity of which is beyond the power of TSL languages. The example could be any pattern where several long-distance dependencies affect different sets of elements, see [McMullin (2016)](https://www.dropbox.com/s/txmk4efif9f5bvb/McMullin_Dissertation_UBC.pdf?dl=0) and [Aksënova and Deshmukh (2018)](https://www.aclweb.org/anthology/W18-0307.pdf) for examples and discussions of those patterns.\n\n\n**Two features spread, only one of them can be blocked**\n\nThe first example comes from Imdlawn Tashlhiyt [(Hansson 2010)](http://homes.chass.utoronto.ca/~cla-acl/actes2010/CLA2010_Hansson.pdf). Sibilants regressively agree in voicing and anteriority. \n\n  * <b>s</b>-a<b>s:</b>twa _CAUS-settle_\n  * <b>S</b>-fia<b>S</b>r _CAUS-be.full.of.straw_\n  * <b>z</b>-bru<b>z:</b>a _CAUS-crumble_\n  * <b>Z</b>-m:<b>Z</b>dawl _CAUS-stumble_\n\nHowever, while voicing harmony can be blocked by voiceless obstruents, they are transparent for the anteriority agreement.\n\n  * <b>s</b>-m<b>X</b>a<b>z</b>aj _CAUS-loathe.each.other_\n  * <b>S</b>-<b>q</b>u<b>Z:</b>i _CAUS-be.dislocated_\n\nThe blockers need to be projected in order to capture the voicing harmony, however, having those blockers on the tier would make sibilants non-adjacent anymore, and therefore would cause problems for the anteriority harmony.\n\n\n**Vowel harmony and consonant harmony**\n\nIn Bukusu, vowels agree in height, and a liquid \"l\" assimilates to \"r\" if followed by \"r\" somewhere further in the word [(Odden 1994)](https://www.jstor.org/stable/415830?seq=1#metadata_info_tab_contents).\n\n  * <b>r</b><i>ee</i>b-<i>e</i><b>r</b>- _ask-APPL_\n  * <b>l</b><i>i</i>m-<i>i</i><b>l</b>- _cultivate-APPL_\n  * <b>r</b><i>u</i>m-<i>i</i><b>r</b>- _send-APPL_\n  \nThe tier containing both vowels and liquids would not capture this picture. Intervening vowels would make the liquid spreading non-local over the tier, and intervening liquids would cause vowels to be potentially far away from each other over the tier.\n\n\n**Multiple tier-based strictly local** grammars are conjunction of multiple TSL grammars: they consist of several tiers, and restrictions defined for every one of those tiers. For example, consider the following toy example.\n\n\n    Good strings: aaabbabba, oppopooo, aapapapp, obooboboboobbb, ...\n    Bad strings:  <*>aabaoob, <*>paabab, <*>obabooo, ...\n    Generalization: if a string contains \"a\", it cannot contain \"o\", and vice versa;\n                    if a string contains \"p\", it cannot contain \"b\", and vice versa.\n                    \nTwo tiers are required to encode this pattern: a tier of vowels (\"o\" and \"a\"), and a tier of consonants (\"p\" and \"b\"). This restriction can be expressed via the following MTSL grammar,\nwhere the tier containing vowels \"a\" and \"o\" imposes restrictions `*ao` and `*oa`, and the tier of \nconsonants containing `p` and `b` rules out `*pb` and `*bp`.\n\n\n### Learning \"independent\" vowel and consonant harmonies\n\n\n\n```python\ndata = ['aabbaabb', 'abab', 'aabbab', 'abaabb', 'aabaab', 'abbabb', 'ooppoopp',\n        'opop', 'ooppop', 'opoopp', 'oopoop', 'oppopp', 'aappaapp', 'apap',\n        'aappap', 'apaapp', 'aapaap', 'appapp', 'oobboobb', 'obob', 'oobbob',\n        'oboobb', 'ooboob', 'obbobb', 'aabb', 'ab', 'aab', 'abb', 'oopp', 'op',\n        'oop', 'opp', 'oobb', 'ob', 'oob', 'obb', 'aapp', 'ap', 'aap', 'app',\n        'aaa', 'ooo', 'bbb', 'ppp', 'a', 'o', 'b', 'p', '']\n```\n\nThe first step is to initialize the MTSL object.\n\n\n```python\nharmony = MTSL()\n```\n\n### Attributes of MTSL grammars\n  * `polar` (\"p\" or \"n\") is the polarity of the grammar, where \"p\" is the default value;\n  * `alphabet` (list) is the set of symbols that the grammar uses;\n  * `grammar` (dictionary) is a dictionary, where the keys (tuple) are the tier alphabet, and the values (lists) are the restrictions imposed on those tiers;\n  * `k` (int) is the size of the locality window of the grammar, by default, it is 2;\n  * `data` (list of string) is the learning sample;\n  * `edges` (list of two characters) are the delimiters used by the grammar, the default value is \">\" and \"<\".\n  \n### Methods defined for MTSL grammars\n  * `check_polarity()` and `switch_polarity()` display and change the polarity of the grammar;\n  * `learn()` detects the tier symbols and learns the tier grammar;\n  * `scan(string)` tells if a given string is well-formed with respect to a learned grammar;\n  * `extract_alphabet()` collects the alphabet based on the data and grammar.\n\n**Extracting alphabet and learning the grammar**\n\nNow we can initialize the `data` and `alphabet` attributes of the MTSL class, and apply the `learn` method to learn the tiers and the grammars that correspond to them.\n\n\n```python\nharmony.data = data\nharmony.extract_alphabet()\nharmony.learn()\n```\n\nThe value of the attribute `grammar` is represented in the following way:\n\n    G = {\n            tier_1 (tuple): tier_1_restrictions (list),\n            tier_2 (tuple): tier_2_restrictions (list),\n                ...\n            tier_n (tuple): tier_n_restrictions (list)\n        }\n\n\n```python\nfor i in harmony.grammar:\n    print(\"Tier:\", i)\n    print(\"Restrictions;\", harmony.grammar[i], \"\\n\")\n```\n\n**Switching polarity**\n\nThe grammar that is learned by default is positive and is pretty verbose, and can be easily converted to negative by appying the `switch_polarity` method.\n\n\n```python\nprint(\"Old polarity:\", harmony.check_polarity())\nharmony.switch_polarity()\nprint(\"New polarity:\", harmony.check_polarity(), \"\\n\")\n\nfor i in harmony.grammar:\n    print(\"Tier:\", i)\n    print(\"Restrictions;\", harmony.grammar[i], \"\\n\")\n```\n\nThe learning algorithm for 2-MTSL languages is designed by [McMullin et al. (2019)](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&context=scil).\n\n\n**Scanning strings**\n\nAs before, the `scan` method tells if the given string is well-formed with respect to the learned grammar.\n\n\n```python\ngood = [\"apapappa\", \"appap\", \"popo\", \"bbbooo\"]\nbad = [\"aoap\", \"popppa\", \"pabp\", \"popoa\"]\n\nfor s in good:\n    print(\"String\", s, \"is in L(G):\", harmony.scan(s))\nprint()\nfor s in bad:\n    print(\"String\", s, \"is in L(G):\", harmony.scan(s))\n```\n\n**The current state of the MTSL-related research**\n\nWe are currently doing the theoretical work of extending the learning algorithm for MTSL languages from capturing 2-local dependencies to `k`. Therefore this module of the toolkit will be updated as the theoretical work on this language class progresses.\n\n## Learning mappings with OSTIA\n\nNow, let us capture the feature changing process instead of defining the well-formedness conditions. Assuming that the spreading moves left to right, we can then mask all non-initial mentions of vowels and consonants inthe words.\n\n```python\n[(\"baABB\", \"baabb\"), (\"bBBoA\", \"bbboo\"), (\"pBaAA\", \"ppaaa\"), (\"pBBBB\", \"ppppp\")]\n```\n\nFirst of all, let us start by defining toy harmonic classes.\n\n```python\nspecifications = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}\n```\n\nNow, let us generate the training sample.\n\n```python\nnum_examples = 10\nlen_examples = 5\nS = generate_pairs(num_examples, len_examples, specifications)\nshow = 5\nprint(show, \"first pairs of S:\\n\", S[:show])\n5 first pairs of S:\n [('bBaBA', 'bbaba'), ('abBAB', 'abbab'), ('aApBB', 'aappp'), ('pBoBB', 'ppopp'), ('opAAA', 'opooo')]\n```\n\nFirst, let us provide the necessary input to OSTIA and save the resulting machine.\n\n```python\nS = generate_pairs(500, 5, specifications)\nSigma = [\"a\", \"o\", \"A\", \"b\", \"p\", \"B\"]\nGamma = [\"a\", \"o\", \"b\", \"p\"]\nT = ostia(S, Sigma, Gamma)\n```\n\nFor a step-by-step implementation of OSTIA, click [here](https://github.com/alenaks/OSTIA/blob/master/ostia.ipynb).\nIn order to evaluate the performance of the obtained automaton, we can generate more input forms.\n\n```python\ntest = mask_words(generate_words(5, 5, specifications), specifications)\nfor w in test:\n    print(w, \"--->\", T.rewrite(w))\n# pBBoA ---> pppoo\n# opABB ---> opopp\n# obBBB ---> obbbb\n# pBBaA ---> pppaa\n# obABA ---> obobo\n```\n\nThe performance of OSTIA largely depends on the size of the training sample and the length of the words.\nFor example, if the length of words is set to 5, we need to observe at least 200 examples in order to see the stably correct outputs.\nAfter the transducer was extracted, it is possible to explore its structure by simply viweing the list of transitions, states, and state outputs of that machine.\n\n```python\nprint(\"States:\", T.Q)\nprint(\"State outputs:\", T.stout)\nprint(\"\\nTransitions:\", T.E)\n# States: ['', 'o', 'p', 'po']\n# State outputs: {'': '', 'o': '', 'p': '', 'po': ''}\n# Transitions: [['', 'a', 'a', ''], ['', 'o', 'o', 'o'], ['o', 'b', 'b', 'o'], ['', 'p', 'p', 'p'], ['p', 'o', 'o', 'po'], ['po', 'B', 'p', 'po'], ['o', 'A', 'o', 'o'], ['', 'b', 'b', ''], ['p', 'B', 'p', 'p'], ['o', 'p', 'p', 'po'], ['p', 'a', 'a', 'p'], ['po', 'A', 'o', 'po'], ['', 'A', 'a', ''], ['p', 'A', 'a', 'p'], ['o', 'B', 'b', 'o'], ['', 'B', 'b', '']]\n```\n\nWe can visualize this FST, showing that the results of transduction learning are interpretable.\n\n**Acknowledgments** \n\nI am very grateful to [_Thomas Graf_](https://thomasgraf.net/), [_Kyle Gorman_](http://www.wellformedness.com/), [_Jeffrey Heinz_](http://jeffreyheinz.net/), [_Aniello De Santo_](https://aniellodesanto.github.io/about/) and _Ayla Karakaş_ whose input on different parts of this project was extremely helpful.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/alenaks/SigmaPie",
    "keywords": "language grammar subregular subsequential transducer induction",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "SigmaPie",
    "package_url": "https://pypi.org/project/SigmaPie/",
    "platform": "",
    "project_url": "https://pypi.org/project/SigmaPie/",
    "project_urls": {
      "Homepage": "https://github.com/alenaks/SigmaPie"
    },
    "release_url": "https://pypi.org/project/SigmaPie/0.5/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A package for subregular and subsequential grammar induction",
    "version": "0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6867796,
  "releases": {
    "0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dbf87e87002d92ff32821483f53d8806a9d37ad3717f015a7d7c05738a6e8d3b",
          "md5": "af56e6775e4eb26cb0e61e7d62cb3e6f",
          "sha256": "0a0c9cfd9e3f77992d53615cc1522ccd1899f12e051cea6e018d698766708daf"
        },
        "downloads": -1,
        "filename": "SigmaPie-0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "af56e6775e4eb26cb0e61e7d62cb3e6f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30772,
        "upload_time": "2020-03-23T16:57:41",
        "upload_time_iso_8601": "2020-03-23T16:57:41.955485Z",
        "url": "https://files.pythonhosted.org/packages/db/f8/7e87002d92ff32821483f53d8806a9d37ad3717f015a7d7c05738a6e8d3b/SigmaPie-0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5223eefcd2047bdefb4eafc03bec91d0061a9432015c81baf4bb822aca8fb3e8",
          "md5": "c32645ad8079cc9d723b040efcb3a171",
          "sha256": "5ed6a70ec79ed601f12cf3f8b56c1a4b8198c3bd3a790f784776ddaaf050873c"
        },
        "downloads": -1,
        "filename": "SigmaPie-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c32645ad8079cc9d723b040efcb3a171",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 46369,
        "upload_time": "2020-03-23T18:04:26",
        "upload_time_iso_8601": "2020-03-23T18:04:26.250625Z",
        "url": "https://files.pythonhosted.org/packages/52/23/eefcd2047bdefb4eafc03bec91d0061a9432015c81baf4bb822aca8fb3e8/SigmaPie-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "828713f7c1ed4e9bb54f8acecb2d06e485ad071b2ef073d62d3b558053cc656f",
          "md5": "5e6f07b444a18745b8f36383a65eba41",
          "sha256": "3f961ae6b41a48f5113ca1e85b0be537cff70f980ac75abd546d97731ba48b9e"
        },
        "downloads": -1,
        "filename": "SigmaPie-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "5e6f07b444a18745b8f36383a65eba41",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 46380,
        "upload_time": "2020-03-23T18:09:55",
        "upload_time_iso_8601": "2020-03-23T18:09:55.181555Z",
        "url": "https://files.pythonhosted.org/packages/82/87/13f7c1ed4e9bb54f8acecb2d06e485ad071b2ef073d62d3b558053cc656f/SigmaPie-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "22b77a436d537fbfd97fe11823fbeb3b2490731e566ab68c68dc942103faa0a2",
          "md5": "177cd87c5958f5ac931ad852371d77a2",
          "sha256": "8ba292a3da5bae4279c60f504ea59d0e64c2245c8c62def6f735375eb8748cbe"
        },
        "downloads": -1,
        "filename": "SigmaPie-0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "177cd87c5958f5ac931ad852371d77a2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 46387,
        "upload_time": "2020-03-23T18:21:25",
        "upload_time_iso_8601": "2020-03-23T18:21:25.353990Z",
        "url": "https://files.pythonhosted.org/packages/22/b7/7a436d537fbfd97fe11823fbeb3b2490731e566ab68c68dc942103faa0a2/SigmaPie-0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4d349580ebfb584ad289f371cbccc1428334b6fde0c8e5c5e70a36e632be340a",
          "md5": "cb4f7082eb1e727e84205097f64a0d01",
          "sha256": "80130e443f50c620f80d25d3fd64bb7b083a675a882b20e5b6eaca2833438f06"
        },
        "downloads": -1,
        "filename": "SigmaPie-0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "cb4f7082eb1e727e84205097f64a0d01",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 48482,
        "upload_time": "2020-03-23T19:11:50",
        "upload_time_iso_8601": "2020-03-23T19:11:50.701814Z",
        "url": "https://files.pythonhosted.org/packages/4d/34/9580ebfb584ad289f371cbccc1428334b6fde0c8e5c5e70a36e632be340a/SigmaPie-0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a6c2ffcae39b17bfac9161d90bc8ccf31531dca02206c496c00f4786bed409f0",
          "md5": "ba180e7e595b92ea1e899dac7d747a90",
          "sha256": "326d4d5c8d1e237bb97d7a2bcc03738f815a95297b4ad55343d09a7f5f460888"
        },
        "downloads": -1,
        "filename": "SigmaPie-0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "ba180e7e595b92ea1e899dac7d747a90",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 48369,
        "upload_time": "2020-03-23T19:13:53",
        "upload_time_iso_8601": "2020-03-23T19:13:53.614804Z",
        "url": "https://files.pythonhosted.org/packages/a6/c2/ffcae39b17bfac9161d90bc8ccf31531dca02206c496c00f4786bed409f0/SigmaPie-0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a6c2ffcae39b17bfac9161d90bc8ccf31531dca02206c496c00f4786bed409f0",
        "md5": "ba180e7e595b92ea1e899dac7d747a90",
        "sha256": "326d4d5c8d1e237bb97d7a2bcc03738f815a95297b4ad55343d09a7f5f460888"
      },
      "downloads": -1,
      "filename": "SigmaPie-0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "ba180e7e595b92ea1e899dac7d747a90",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 48369,
      "upload_time": "2020-03-23T19:13:53",
      "upload_time_iso_8601": "2020-03-23T19:13:53.614804Z",
      "url": "https://files.pythonhosted.org/packages/a6/c2/ffcae39b17bfac9161d90bc8ccf31531dca02206c496c00f4786bed409f0/SigmaPie-0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}