{
  "info": {
    "author": "Boris Shishov",
    "author_email": "borisshishov@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Software Development :: Libraries"
    ],
    "description": "# tokema\n\ntokema - **toke**n **ma**tching parser.\n\nIt is a pure python, zero-dependency library for building parsers. \ntokema algorithm based on Tomita's GLR* parser.\n\nIt helps to create noise-skipping grammar-based parsers at runtime and apply them to various task like \nentity extraction and token matching.\n\ntokema algorithm is the extended version on Noise-skipping GLR* parser by Lavie Alon and Masaru Tomita:\n\n> Lavie, Alon, and Masaru Tomita.\n> \"10. GLR*-AN EFFICIENT NOISE-SKIPPING PARSING ALGORITHM FOR CONTEXT-FREE GRAMMARS.\"\n> Recent Advances in Parsing Technology 1 (1996): 183.\n\ntokema parsing works with arbitrary tokens - chars, words, structures... you can write a parser for\ntoken stream produced by the tokenizer of your choice.\n\n## GLR\n\nIn general, tokema - is a GLR-based (Generalized LR) context-free parser generator which basically means \nthat parser produced by tokema tries to find most suitable parses (might be multiple) given deterministic grammar.\nGLR-based algorithms evaluate multiple parser states at the same time allowing to parse using incomplete and poorly structured grammars. \n\n## Noise-skipping\n\nThe term \"noise skipping\" means that parser can skip tokens it doesn't understand which is required to solve\nnatural language tasks.\n\n## Extensions\n\nInstead of direct token matching (or dictionary based) like in most traditional parsers tokema\nreplaces terminals and non-terminals in the grammar with queries and resolvers - \nfunctions that matches the input token with the given query.\n\nBy creating these functions you can construct token queries and resolvers of your choice integrating lots of cool features into your parser.\nFor example you can create a custom resolver matches stemmed or lemmatized text tokens.\nOr create a resolver that uses custom dictionaries or even integrate a third party search backend like lucene.\n\n## Installation\n\n    pip install tokema\n\n# Usage\n\nBasic usage:\n```python\nfrom tokema import *\n\n# Define grammar\n# NOTE: you can also define it using python classes only or create you own grammar syntax\nrules = parse_rules_from_string(\"\"\"\nROOT = <EXPR>\nEXPR = {float} + {float}\n\"\"\")\n\n# Parsing table construction\ntable = build_text_parsing_table(rules)\n\n# Input tokens\n# NOTE: In real application you should use some real tokenizer (like nltk)\ntokens = 'this will be ignored 3.1415 and + this 4e-10'.split()\n\n# Actual parsing using token stream and parsing table\n# There may be multiple parses (not in this oversimplified scenario) so parse returns a list\nresults = parse(tokens, table)\n\nfor result in results:\n    # Here you can access the parsed AST (abstract syntax tree)\n    print(result)  # ROOT(EXPR(3.1415, +, 4e-10))\n\n    # EXPR is the first child of ROOT\n    expr = result[0]\n\n    # Convert each value to a float and make sure that result is actually correct\n    assert float(expr[0].value) + float(expr[2].value) == 3.1415 + 4e-10\n\n```\n\nFor more usage scenarios see examples folder\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/bshishov/tokema",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tokema",
    "package_url": "https://pypi.org/project/tokema/",
    "platform": "",
    "project_url": "https://pypi.org/project/tokema/",
    "project_urls": {
      "Homepage": "https://github.com/bshishov/tokema"
    },
    "release_url": "https://pypi.org/project/tokema/0.0.2/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Token matching parser",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9917861,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f40eae7b11b5c8dfbcfaf7e7e103235de3957c179d853fe8ebf2917b27e1f566",
          "md5": "331ed59f5bcc440b4aed4507ffb9acc5",
          "sha256": "ce4be0ae1c0302f350239c1621141be6d585f2ec4972b8ff09e8c64db8cd8a9a"
        },
        "downloads": -1,
        "filename": "tokema-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "331ed59f5bcc440b4aed4507ffb9acc5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 13329,
        "upload_time": "2021-03-28T16:48:37",
        "upload_time_iso_8601": "2021-03-28T16:48:37.590762Z",
        "url": "https://files.pythonhosted.org/packages/f4/0e/ae7b11b5c8dfbcfaf7e7e103235de3957c179d853fe8ebf2917b27e1f566/tokema-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0fad15068770125daf56ab5e53cc3c271620730593c7332cd100b01476b1c2c7",
          "md5": "715fa30dee08ad6bb9d3f860f0238914",
          "sha256": "0b2ae5282d3040356dcdb8efd5d577441b0d0f9237b02a81085c444d052700bf"
        },
        "downloads": -1,
        "filename": "tokema-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "715fa30dee08ad6bb9d3f860f0238914",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 12380,
        "upload_time": "2021-03-28T16:48:38",
        "upload_time_iso_8601": "2021-03-28T16:48:38.424942Z",
        "url": "https://files.pythonhosted.org/packages/0f/ad/15068770125daf56ab5e53cc3c271620730593c7332cd100b01476b1c2c7/tokema-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8e35bbddc3c6e904911a51124205c03fa26cd6563bcc75c965d59020b890dbb5",
          "md5": "a9e233fac37f058f157d893e6e50171d",
          "sha256": "c8cc4e2e5758ff62c7dc153f753e9eb242638a884cf6757f91e8c96bfbc89e0b"
        },
        "downloads": -1,
        "filename": "tokema-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a9e233fac37f058f157d893e6e50171d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 13809,
        "upload_time": "2021-03-29T22:06:15",
        "upload_time_iso_8601": "2021-03-29T22:06:15.175628Z",
        "url": "https://files.pythonhosted.org/packages/8e/35/bbddc3c6e904911a51124205c03fa26cd6563bcc75c965d59020b890dbb5/tokema-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b14fca23b9f97a22d6a594a3cf46ca7a0d3cdab8fd8da1e3bb8c47081efed592",
          "md5": "da4e13c4713cfbb47710ba64023e7727",
          "sha256": "ae74ebc2b1128dffef8cf56f60ee44e2e9bea5fcc4a44233a058277078c89d2f"
        },
        "downloads": -1,
        "filename": "tokema-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "da4e13c4713cfbb47710ba64023e7727",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 12925,
        "upload_time": "2021-03-29T22:06:16",
        "upload_time_iso_8601": "2021-03-29T22:06:16.502789Z",
        "url": "https://files.pythonhosted.org/packages/b1/4f/ca23b9f97a22d6a594a3cf46ca7a0d3cdab8fd8da1e3bb8c47081efed592/tokema-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8e35bbddc3c6e904911a51124205c03fa26cd6563bcc75c965d59020b890dbb5",
        "md5": "a9e233fac37f058f157d893e6e50171d",
        "sha256": "c8cc4e2e5758ff62c7dc153f753e9eb242638a884cf6757f91e8c96bfbc89e0b"
      },
      "downloads": -1,
      "filename": "tokema-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a9e233fac37f058f157d893e6e50171d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 13809,
      "upload_time": "2021-03-29T22:06:15",
      "upload_time_iso_8601": "2021-03-29T22:06:15.175628Z",
      "url": "https://files.pythonhosted.org/packages/8e/35/bbddc3c6e904911a51124205c03fa26cd6563bcc75c965d59020b890dbb5/tokema-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b14fca23b9f97a22d6a594a3cf46ca7a0d3cdab8fd8da1e3bb8c47081efed592",
        "md5": "da4e13c4713cfbb47710ba64023e7727",
        "sha256": "ae74ebc2b1128dffef8cf56f60ee44e2e9bea5fcc4a44233a058277078c89d2f"
      },
      "downloads": -1,
      "filename": "tokema-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "da4e13c4713cfbb47710ba64023e7727",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 12925,
      "upload_time": "2021-03-29T22:06:16",
      "upload_time_iso_8601": "2021-03-29T22:06:16.502789Z",
      "url": "https://files.pythonhosted.org/packages/b1/4f/ca23b9f97a22d6a594a3cf46ca7a0d3cdab8fd8da1e3bb8c47081efed592/tokema-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}