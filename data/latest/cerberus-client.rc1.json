{
  "info": {
    "author": "Naga Ravi Chaitanya Elluri",
    "author_email": "nelluri@redhat.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Programming Language :: Python"
    ],
    "description": "# Cerberus\nGuardian of Kubernetes and OpenShift Clusters\n\nCerberus watches the Kubernetes/OpenShift clusters for dead nodes, system component failures/health and exposes a go or no-go signal which can be consumed by other workload generators or applications in the cluster and act accordingly.\n\n### Workflow\n![Cerberus workflow](media/cerberus-workflow.png)\n\n### Install the dependencies\n```\n$ pip3 install -r requirements.txt\n```\n\n### Usage\n\n#### Config\nSet the supported components to monitor and the tunings like number of iterations to monitor and duration to wait between each check in the config file located at config/config.yaml. A sample config looks like:\n\n```\ncerberus:\n    distribution: openshift                              # Distribution can be kubernetes or openshift\n    kubeconfig_path: ~/.kube/config                      # Path to kubeconfig\n    port: 8080                                           # http server port where cerberus status is published\n    watch_nodes: True                                    # Set to True for the cerberus to monitor the cluster nodes\n    watch_cluster_operators: True                        # Set to True for cerberus to monitor cluster operators. Parameter is optional, will set to True if not specified\n    watch_url_routes:                                    # Route url's you want to monitor\n        - - https://...\n          - Bearer ****                                  # This parameter is optional, specify authorization need for get call to route\n        - - http://...\n    watch_namespaces:                                    # List of namespaces to be monitored\n        -    openshift-etcd\n        -    openshift-apiserver\n        -    openshift-kube-apiserver\n        -    openshift-monitoring\n        -    openshift-kube-controller-manager\n        -    openshift-machine-api\n        -    openshift-kube-scheduler\n        -    openshift-ingress\n        -    openshift-sdn\n    cerberus_publish_status: True                        # When enabled, cerberus starts a light weight http server and publishes the status\n    inspect_components: False                            # Enable it only when OpenShift client is supported to run.\n                                                         # When enabled, cerberus collects logs, events and metrics of failed components\n\n    prometheus_url:                                      # The prometheus url/route is automatically obtained in case of OpenShift, please set it when the distribution is Kubernetes.\n    prometheus_bearer_token:                             # The bearer token is automatically obtained in case of OpenShift, please set it when the distribution is Kubernetes. This is needed to authenticate with prometheus.\n                                                         # This enables Cerberus to query prometheus and alert on observing high Kube API Server latencies.   \n\n    slack_integration: False                             # When enabled, cerberus reports status of failed iterations in the slack channel\n                                                         # The following env vars need to be set: SLACK_API_TOKEN ( Bot User OAuth Access Token ) and SLACK_CHANNEL ( channel to send notifications in case of failures )\n                                                         # When slack_integration is enabled, a cop can be assigned for each day. The cop of the day is tagged while reporting failures in the slack channel. Values are slack member ID's.\n    cop_slack_ID:                                        # (NOTE: Defining the cop id's is optional and when the cop slack id's are not defined, the slack_team_alias tag is used if it is set else no tag is used while reporting failures in the slack channel.)\n        Monday:\n        Tuesday:\n        Wednesday:\n        Thursday:\n        Friday:\n        Saturday:\n        Sunday:\n    slack_team_alias:                                    # The slack team alias to be tagged while reporting failures in the slack channel when no cop is assigned\n\n    custom_checks:                                       # Relative paths of files conataining additional user defined checks\n        -   custom_checks/custom_check_1.py\n        -   custom_check_2.py\n\ntunings:\n    iterations: 5                                        # Iterations to loop before stopping the watch, it will be replaced with infinity when the daemon mode is enabled\n    sleep_time: 60                                       # Sleep duration between each iteration\n    kube_api_request_chunk_size: 250                     # Large requests will be broken into the specified chunk size to reduce the load on API server and improve responsiveness.\n    daemon_mode: True                                    # Iterations are set to infinity which means that the cerberus will monitor the resources forever\n    cores_usage_percentage: 0.5                          # Set the fraction of cores to be used for multiprocessing\n\ndatabase:\n    database_path: /tmp/cerberus.db                      # Path where cerberus database needs to be stored\n    reuse_database: False                                # When enabled, the database is reused to store the failures\n```\n**NOTE**: watch_namespaces support regex patterns. Any valid regex pattern can be used to watch all the namespaces matching the regex pattern. For example, `^openshift-.*$` can be used to watch all namespaces that start with `openshift-` or `openshift` can be used to watch all namespaces that have `openshift` in it.\n\n**NOTE**: The current implementation can monitor only one cluster from one host. It can be used to monitor multiple clusters provided multiple instances of Cerberus are launched on different hosts.\n\n**NOTE**: The components especially the namespaces needs to be changed depending on the distribution i.e Kubernetes or OpenShift. The default specified in the config assumes that the distribution is OpenShift. A config file for Kubernetes is located at config/kubernetes_config.yaml\n\n#### Run\n```\n$ python3 start_cerberus.py --config <config_file_location>\n```\n\n#### Run containerized version\nAssuming that the latest docker ( 17.05 or greater with multi-build support ) is intalled on the host, run:\n```\n$ docker pull quay.io/openshift-scale/cerberus\n$ docker run --name=cerberus --net=host -v <path_to_kubeconfig>:/root/.kube/config -v <path_to_cerberus_config>:/root/cerberus/config/config.yaml -d quay.io/openshift-scale/cerberus:latest\n$ docker logs -f cerberus\n```\n\nSimilarly, podman can be used to achieve the same:\n```\n$ podman pull quay.io/openshift-scale/cerberus\n$ podman run --name=cerberus --net=host -v <path_to_kubeconfig>:/root/.kube/config:Z -v <path_to_cerberus_config>:/root/cerberus/config/config.yaml:Z -d quay.io/openshift-scale/cerberus:latest\n$ podman logs -f cerberus\n```\nThe go/no-go signal ( True or False ) gets published at http://`<hostname>`:8080. Note that the cerberus will only support ipv4 for the time being.\n\n**NOTE**: The report is generated at /root/cerberus/cerberus.report inside the container, it can mounted to a directory on the host in case we want to capture it.\n\n#### Run containerized Cerberus as a Kubernetes/OpenShift deployment\nRefer to the [instructions](https://github.com/openshift-scale/cerberus/blob/master/containers/README.md#cerberus-as-a-kubeapp) for information on how to run cerberus as a KubeApp.\n\n#### Report\nThe report is generated in the run directory and it contains the information about each check/monitored component status per iteration with timestamps. It also displays information about the components in case of failure. For example:\n\n```\n2020-03-26 22:05:06,393 [INFO] Starting ceberus\n2020-03-26 22:05:06,401 [INFO] Initializing client to talk to the Kubernetes cluster\n2020-03-26 22:05:06,434 [INFO] Fetching cluster info\n2020-03-26 22:05:06,739 [INFO] Publishing cerberus status at http://0.0.0.0:8080\n2020-03-26 22:05:06,753 [INFO] Starting http server at http://0.0.0.0:8080\n2020-03-26 22:05:06,753 [INFO] Daemon mode enabled, cerberus will monitor forever\n2020-03-26 22:05:06,753 [INFO] Ignoring the iterations set\n\n2020-03-26 22:05:25,104 [INFO] Iteration 4: Node status: True\n2020-03-26 22:05:25,133 [INFO] Iteration 4: Etcd member pods status: True\n2020-03-26 22:05:25,161 [INFO] Iteration 4: OpenShift apiserver status: True\n2020-03-26 22:05:25,546 [INFO] Iteration 4: Kube ApiServer status: True\n2020-03-26 22:05:25,717 [INFO] Iteration 4: Monitoring stack status: True\n2020-03-26 22:05:25,720 [INFO] Iteration 4: Kube controller status: True\n2020-03-26 22:05:25,746 [INFO] Iteration 4: Machine API components status: True\n2020-03-26 22:05:25,945 [INFO] Iteration 4: Kube scheduler status: True\n2020-03-26 22:05:25,963 [INFO] Iteration 4: OpenShift ingress status: True\n2020-03-26 22:05:26,077 [INFO] Iteration 4: OpenShift SDN status: True\n2020-03-26 22:05:26,077 [INFO] HTTP requests served: 0 \n2020-03-26 22:05:26,077 [INFO] Sleeping for the specified duration: 5\n\n\n2020-03-26 22:05:31,134 [INFO] Iteration 5: Node status: True\n2020-03-26 22:05:31,162 [INFO] Iteration 5: Etcd member pods status: True\n2020-03-26 22:05:31,190 [INFO] Iteration 5: OpenShift apiserver status: True\n127.0.0.1 - - [26/Mar/2020 22:05:31] \"GET / HTTP/1.1\" 200 -\n2020-03-26 22:05:31,588 [INFO] Iteration 5: Kube ApiServer status: True\n2020-03-26 22:05:31,759 [INFO] Iteration 5: Monitoring stack status: True\n2020-03-26 22:05:31,763 [INFO] Iteration 5: Kube controller status: True\n2020-03-26 22:05:31,788 [INFO] Iteration 5: Machine API components status: True\n2020-03-26 22:05:31,989 [INFO] Iteration 5: Kube scheduler status: True\n2020-03-26 22:05:32,007 [INFO] Iteration 5: OpenShift ingress status: True\n2020-03-26 22:05:32,118 [INFO] Iteration 5: OpenShift SDN status: False\n2020-03-26 22:05:32,118 [INFO] HTTP requests served: 1 \n2020-03-26 22:05:32,118 [INFO] Sleeping for the specified duration: 5\n+--------------------------------------------------Failed Components--------------------------------------------------+\n2020-03-26 22:05:37,123 [INFO] Failed openshfit sdn components: ['sdn-xmqhd']\n\n2020-05-23 23:26:43,041 [INFO] ------------------------- Iteration Stats ---------------------------------------------\n2020-05-23 23:26:43,041 [INFO] Time taken to run watch_nodes in iteration 1: 0.0996248722076416 seconds\n2020-05-23 23:26:43,041 [INFO] Time taken to run watch_cluster_operators in iteration 1: 0.3672499656677246 seconds\n2020-05-23 23:26:43,041 [INFO] Time taken to run watch_namespaces in iteration 1: 1.085144281387329 seconds\n2020-05-23 23:26:43,041 [INFO] Time taken to run entire_iteration in iteration 1: 4.107403039932251 seconds\n2020-05-23 23:26:43,041 [INFO] ---------------------------------------------------------------------------------------\n```\n\n#### Slack integration\nThe user has the option to enable/disable the slack integration ( disabled by default ). To use the slack integration, the user has to first create an [app](https://api.slack.com/apps?new_granular_bot_app=1) and add a bot to it on slack. SLACK_API_TOKEN and SLACK_CHANNEL environment variables have to be set. SLACK_API_TOKEN refers to Bot User OAuth Access Token and SLACK_CHANNEL refers to the slack channel ID the user wishes to receive the notifications.\n- Reports when cerberus starts monitoring a cluster in the specified slack channel.\n- Reports the component failures in the slack channel.\n- A cop can be assigned for each day of the week. The cop of the day is tagged while reporting failures in the slack channel instead of everyone. (NOTE: Defining the cop id's is optional and when the cop slack id's are not defined, the slack_team_alias tag is used if it is set else no tag is used while reporting failures in the slack channel.)\n\n#### Go or no-go signal\nWhen the cerberus is configured to run in the daemon mode, it will continuosly monitor the components specified, runs a simple http server at http://0.0.0.0:8080 and publishes the signal i.e True or False depending on the components status. The tools can consume the signal and act accordingly.\n\n#### Failures in a time window\n1. The failures in the past 1 hour can be retrieved in the json format by visiting http://0.0.0.0:8080/history.\n2. The failures in a specific time window can be retrieved in the json format by visiting http://0.0.0.0:8080/history?loopback=<interval>.\n3. The failures between two time timestamps, the failures of specific issues types and the failures related to specific components can be retrieved in the json format by visiting http://0.0.0.0:8080/analyze url. The filters have to be applied to scrape the failures accordingly.\n\n#### Node Problem Detector\n[node-problem-detector](https://github.com/kubernetes/node-problem-detector) aims to make various node problems visible to the upstream layers in cluster management stack.\n\n##### Installation\nPlease follow the instructions in the [installation](https://github.com/kubernetes/node-problem-detector#installation) section to setup Node Problem Detector on Kubernetes. The following instructions are setting it up on OpenShift:\n\n1. Create `openshift-node-problem-detector` namespace [ns.yaml](https://github.com/openshift/node-problem-detector-operator/blob/master/deploy/ns.yaml) with        `oc create -f ns.yaml`\n2. Add cluster role with `oc adm policy add-cluster-role-to-user system:node-problem-detector -z default -n openshift-node-problem-detector`\n3. Add security context constraints with `oc adm policy add-scc-to-user privileged system:serviceaccount:openshift-node-problem-detector:default\n`\n4. Edit [node-problem-detector.yaml](https://github.com/kubernetes/node-problem-detector/blob/master/deployment/node-problem-detector.yaml) to fit your environment.\n5. Edit [node-problem-detector-config.yaml](https://github.com/kubernetes/node-problem-detector/blob/master/deployment/node-problem-detector-config.yaml) to configure node-problem-detector.\n6. Create the ConfigMap with\t`oc create -f node-problem-detector-config.yaml`\n7. Create the DaemonSet with `oc create -f node-problem-detector.yaml`\n\nOnce installed you will see node-problem-detector pods in openshift-node-problem-detector namespace. \nNow enable openshift-node-problem-detector in the [config.yaml](https://github.com/openshift-scale/cerberus/blob/master/config/config.yaml).\nCerberus just monitors `KernelDeadlock` condition provided by the node problem detector as it is system critical and can hinder node performance.\n\n#### Custom checks\nYou can bring in additional checks to monitor components that are not being monitored by Cerberus. This can be accomplished by placing relative paths of files containing additional checks under custom_checks in config file. All the checks should be placed within the main function of the file. If the additional checks need to be considered in determining the go/no-go signal of Cerberus, the main function can return a boolean value for the same. However, it's optional to return a value. Refer to [example_check](https://github.com/openshift-scale/cerberus/blob/master/custom_checks/custom_check_1.py) for an example custom check file.\n\n#### Alerts\nMonitoring metrics and alerting on abnormal behavior is critical as they are the indicators for clusters health. When provided the prometheus url and bearer token in the config, Cerberus looks for KubeAPILatencyHigh alert at the end of each iteration and warns if 99th percentile latency for given requests to the kube-apiserver is above 1 second. It is the official SLI/SLO defined for Kubernetes.\n\n**NOTE**: The prometheus url and bearer token are automatically picked from the cluster if the distibution is OpenShift since it's the default metrics solution. In case of Kubernetes, they need to be provided in the config if prometheus is deployed.\n\n### Use cases\nThere can be number of use cases, here are some of them:\n- We run tools to push the limits of Kubernetes/OpenShift to look at the performance and scalability. There are a number of instances where system components or nodes start to degrade, which invalidates the results and the workload generator continues to push the cluster until it is unrecoverable.\n\n- When running chaos experiments on a kubernetes/OpenShift cluster, they can potentially break the components unrelated to the targeted components which means that the choas experiment won't be able to find it. The go/no-go signal can be used here to decide whether the cluster recovered from the failure injection as well as to decide whether to continue with the next chaos scenario.\n\n### What Kubernetes/OpenShift components can Cerberus monitor?\nFollowing are the components of Kubernetes/OpenShift that Cerberus can monitor today, we will be adding more soon.\n\nComponent                | Description                                                                                                 | Working\n------------------------ | ----------------------------------------------------------------------------------------------------------- | ------------------------- |\nNodes                    | Watches all the nodes including masters, workers as well as nodes created using custom MachineSets          | :heavy_check_mark:        |\nNamespaces               | Watches all the pods including containers running inside the pods in the namespaces specified in the config | :heavy_check_mark:        |\nCluster Operators        | Watches all Cluster Operators                                                                               | :heavy_check_mark:        |\nMaster Nodes Schedule    | Watches schedule of Master Nodes                                                                            | :heavy_check_mark:        |\nRoutes                   | Watches specified routes                                                                                    | :heavy_check_mark:        | \n\n**NOTE**: It supports monitoring pods in any namespaces specified in the config, the watch is enabled for system components mentioned in the [config](https://github.com/openshift-scale/cerberus/blob/master/config/config.yaml) by default as they are critical for running the operations on Kubernetes/OpenShift clusters.\n\n### Blogs and other useful resources\n- https://www.openshift.com/blog/openshift-scale-ci-part-4-introduction-to-cerberus-guardian-of-kubernetes/openshift-clouds \n\n\n",
    "description_content_type": "text/markdown; charset=UTF-8",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/openshift-scale/cerberus",
    "keywords": "",
    "license": "mit",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cerberus-client",
    "package_url": "https://pypi.org/project/cerberus-client/",
    "platform": "",
    "project_url": "https://pypi.org/project/cerberus-client/",
    "project_urls": {
      "Homepage": "https://github.com/openshift-scale/cerberus"
    },
    "release_url": "https://pypi.org/project/cerberus-client/1.0.0/",
    "requires_dist": [
      "setuptools",
      "PyYAML",
      "requests",
      "kubernetes",
      "datetime",
      "configparser",
      "slackclient",
      "pyfiglet",
      "prometheus-api-client"
    ],
    "requires_python": "",
    "summary": "Guardian of Kubernetes and OpenShift Clusters",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7793770,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9a4c97d79ccdd53ffa6f3855cf748aceca50b1b2c4b649d26934f7ae91830498",
          "md5": "bfb03c6669203cc6209b3b86ef5df079",
          "sha256": "c79b8aa3a6bffe861b7bf808e4cce362f9993728603ea0a204ccc2c7e2a5b207"
        },
        "downloads": -1,
        "filename": "cerberus_client-1.0.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bfb03c6669203cc6209b3b86ef5df079",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 27601,
        "upload_time": "2020-07-27T07:35:26",
        "upload_time_iso_8601": "2020-07-27T07:35:26.790779Z",
        "url": "https://files.pythonhosted.org/packages/9a/4c/97d79ccdd53ffa6f3855cf748aceca50b1b2c4b649d26934f7ae91830498/cerberus_client-1.0.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4fb1a5ae8670fc1f29bd0f6aadf735d141702585321e8649609077a69d4daac2",
          "md5": "1b4f554ccfd77bc55ccc47ee9ea48b89",
          "sha256": "265cf74a224a5c70ed83a4db2c7b60c9ccc1de7d100b5ce1ed55f892c8f9f3b5"
        },
        "downloads": -1,
        "filename": "cerberus-client-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "1b4f554ccfd77bc55ccc47ee9ea48b89",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 27309,
        "upload_time": "2020-07-27T07:35:36",
        "upload_time_iso_8601": "2020-07-27T07:35:36.021273Z",
        "url": "https://files.pythonhosted.org/packages/4f/b1/a5ae8670fc1f29bd0f6aadf735d141702585321e8649609077a69d4daac2/cerberus-client-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9a4c97d79ccdd53ffa6f3855cf748aceca50b1b2c4b649d26934f7ae91830498",
        "md5": "bfb03c6669203cc6209b3b86ef5df079",
        "sha256": "c79b8aa3a6bffe861b7bf808e4cce362f9993728603ea0a204ccc2c7e2a5b207"
      },
      "downloads": -1,
      "filename": "cerberus_client-1.0.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "bfb03c6669203cc6209b3b86ef5df079",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 27601,
      "upload_time": "2020-07-27T07:35:26",
      "upload_time_iso_8601": "2020-07-27T07:35:26.790779Z",
      "url": "https://files.pythonhosted.org/packages/9a/4c/97d79ccdd53ffa6f3855cf748aceca50b1b2c4b649d26934f7ae91830498/cerberus_client-1.0.0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4fb1a5ae8670fc1f29bd0f6aadf735d141702585321e8649609077a69d4daac2",
        "md5": "1b4f554ccfd77bc55ccc47ee9ea48b89",
        "sha256": "265cf74a224a5c70ed83a4db2c7b60c9ccc1de7d100b5ce1ed55f892c8f9f3b5"
      },
      "downloads": -1,
      "filename": "cerberus-client-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "1b4f554ccfd77bc55ccc47ee9ea48b89",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 27309,
      "upload_time": "2020-07-27T07:35:36",
      "upload_time_iso_8601": "2020-07-27T07:35:36.021273Z",
      "url": "https://files.pythonhosted.org/packages/4f/b1/a5ae8670fc1f29bd0f6aadf735d141702585321e8649609077a69d4daac2/cerberus-client-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}