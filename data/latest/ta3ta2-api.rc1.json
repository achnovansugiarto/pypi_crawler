{
  "info": {
    "author": "DARPA D3M Program",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License"
    ],
    "description": "# TA3-TA2 API\n\nThis repository contains a TA3-TA2 API protocol specification and implementation using\n[GRPC](http://grpc.io/).\nThe API allows a TA3 to request from TA2 to start a pipeline search process, using\nan optional pipeline template, and after candidate pipelines are found a TA3\ncan request scoring, fitting, or producing data through a pipeline.\n\nThe GRPC protocol specification can be automatically compiled into implementations\nfor multiple programming languages.\nSee below for more information and the [Quickstart](http://grpc.io/docs/quickstart/) for details\nabout GRPC.\n\n## API Structure\n\nTA3-TA2 API calls are defined in the *core* GRPC service which can be found in [`core.proto`](./core.proto) file\nand all TA3 and TA2 systems are expected to implement it and support it. Other `.proto` files provide definitions\nof additional standard messages.\n\nUseful utilities for working with the TA3-TA2 API in Python are available in the included [ta3ta2_api](https://gitlab.com/datadrivendiscovery/ta3ta2-api/tree/dist-python) package.\n\n![Diagram of the overall flow of the API](flow.png)\n\n## GRPC compilation\n\nGRPC provides tooling to compile protocol specification into various target languages. Examples follow.\n\n### Go setup\n\nTo set up GRPC and Protocol Buffers in Go run:\n\n```\ngo get -u github.com/golang/protobuf/proto\ngo get -u github.com/golang/protobuf/protoc-gen-go\ngo get -u google.golang.org/grpc\n```\nNext install protocol buffer compiler:\n\nLinux\n\n```bash\ncurl -OL https://github.com/google/protobuf/releases/download/v3.3.0/protoc-3.3.0-linux-x86_64.zip\nunzip protoc-3.3.0-linux-x86_64.zip -d protoc3\nsudo cp protoc3/bin/protoc /usr/bin/protoc\nsudo cp -r protoc3/include /usr/local\n```\n\nOSX\n\n```bash\ncurl -OL https://github.com/google/protobuf/releases/download/v3.3.0/protoc-3.3.0-osx-x86_64.zip\nunzip protoc-3.3.0-osx-x86_64.zip -d protoc3\nsudo cp protoc3/bin/protoc /usr/bin/protoc\nsudo cp -r protoc3/include /usr/local\n```\n\nCompile the `.proto` file:\n\n```\nprotoc -I /usr/local/include -I . core.proto --go_out=plugins=grpc:.\n```\n\nThe resulting `core.pb.go` file implements the messaging protocol, client and server.\n\n### Python setup\n\nInstall libraries and tools via pip:\n\n```\npython -m pip install grpcio --ignore-installed\npython -m pip install grpcio-tools\n```\n\nCompile the `.proto` file:\n\n```\npython -m grpc_tools.protoc -I . --python_out=. --grpc_python_out=. core.proto\n```\n\nThe created `core_pb2.py` file implements the messaging protocol, and `core_pb2_grpc.py` implements the client and server.\n\n### Javascript/Node.js setup\n\nUse `npm` to get GRPC and Protocol Buffer packages:\n\n```\nnpm install grpc\nnpm install google-protobuf\n```\n\nJust as with Go installation, need to install protocol buffer compiler:\n\nLinux\n\n```bash\ncurl -OL https://github.com/google/protobuf/releases/download/v3.3.0/protoc-3.3.0-linux-x86_64.zip\nunzip protoc-3.3.0-linux-x86_64.zip -d protoc3\nsudo mv protoc3/bin/protoc /usr/bin/protoc\nsudo cp -r protoc3/include /usr/local\n```\n\nOSX\n\n```bash\ncurl -OL https://github.com/google/protobuf/releases/download/v3.3.0/protoc-3.3.0-osx-x86_64.zip\nunzip protoc-3.3.0-osx-x86_64.zip -d protoc3\nsudo mv protoc3/bin/protoc /usr/bin/protoc\nsudo cp -r protoc3/include /usr/local\n```\n\nCompile the `.proto` file:\n\n```\nprotoc -I /usr/local/include -I . core.proto --js_out=import_style=commonjs,binary:.\n```\n\nThe resulting `core_pb.js` file implements the messaging protocol, client and server.\n\n## Pipelines\n\nAPI is centered around a concept of a pipeline. Pipelines are described using a shared\n[D3M pipeline language](https://gitlab.com/datadrivendiscovery/metalearning). Pipeline\ndescriptions are used in two places:\n\n* To describe a pipeline template provided by a TA3 to a TA2.\n* To describe resulting pipelines TA2 finds back to TA3.\n\nGenerally, pipelines always have Dataset container value as input (currently only\none) and predictions as output. This is the only pipeline TA2 is expected to search.\nBut TA3 can fully specify any pipeline for TA2 to execute without any search\n(including a pipeline of just one primitive).\n\nIf pipelines have the associated problem description provided, then this should \napply to the data at the beginning of the pipeline.  This is especially relevant \nfor partially specified pipelines; the problem description for a partially \nspecified pipeline should describe the data at the beginning of the pipeline, \nnot the end of the specified portion.\n\n### Pipeline templates\n\nPipeline templates are based on pipeline description with few differences:\n\n* Templates can accept *multiple* Dataset container values as inputs.\n* There is a special *placeholder pipeline step* which signals where in the pipeline\n  template a TA2 system should insert a standard pipeline it finds.\n* Not all fields in the pipeline description are reasonable (they will be filled out by TA2).\n  Those differences are explained through comments in the [`pipeline.proto`](./pipeline.proto).\n\nA placeholder pipeline step is replaced with a sub-pipeline during pipeline search to form\nfinal pipeline.\n\n### Pipeline template restrictions\n\nWhile the pipeline template language does not restrict the use of a placeholder step, for\nthe purpose of TA3-TA2 API we are currently placing the following restrictions:\n\n* There can be only one placeholder step in a pipeline template, at the top-level of a pipeline (not inside a sub-pipeline).\n* The placeholder step has to have only one input, a Dataset container value, and one output,\n  predictions as a Pandas dataframe. In this way it resembles a standard pipeline.\n* The placeholder can be only the last step in the pipeline.\n* All primitive steps should have all their hyper-parameters fixed (see also `use_default_values_for_free_hyperparams`\n  flag to control this requirement).\n\nThese restrictions effectively mean that a pipeline template can only specify a directed acyclic graph of preprocessing \nprimitives that transforms one or more input Dataset container values into a *single* transformed\nDataset container value, which is the input to the placeholder step (and future sub-pipeline in its place).\nThere are no additional restrictions on the types of individual primitives that can be used within the \npipeline template, although impact on downstream TA2 processing should be assessed before a given \nprimitive is used.\n\nIndividual systems can relax those restrictions. For example, they might allow a placeholder step to\nhave postprocessing primitive steps after it. In this case postprocessing\nprimitives can only transform predictions from a placeholder step into transformed predictions.\nOr individual systems might allow primitive steps to have free hyper-parameters a TA2 system\nshould tune (see `use_default_values_for_free_hyperparams` flag to potentially control this behavior).\nWe expect that some TA2 systems will be able to work with those relaxed requirements, and\nTA3s can use those if available, but it is not expected that every TA2 will.\n\n### Fully specified pipelines\n\nTA3 can also provide a fully specified pipeline in the `SearchSolutions`. This is a pipeline description\nwhich does not have any placeholder step and have all hyper-parameters fixed.\n\nFor fully specified pipelines with fixed hyper-parameters, TA2 will just check that the given\npipeline is valid and return it for it to be directly executed (scored, fitted, called to\nproduce data). This allows fixed computations to be done on data, for example, the pipeline\ncan consist of only one primitive with fixed hyper-parameters to execute that one primitive.\nMoreover, such fully specified pipelines with fixed hyper-parameters can have any\ninputs and any outputs. (Standard pipelines are from a Dataset container value\nto predictions Pandas dataframe.) When non-Dataset inputs are provided, TA2 should attempt\nto convert input value to closest container type value, e.g., GRPC `RAW` list value should\nbe converted to `d3m.container.List` with generated metadata, CSV file read as Pandas\nDataFrame should be converted to `d3m.container.DataFrame` with generated metadata.\n\nIndividual systems can also support pipelines with all primitives specified,\nbut with free (available for tuning) hyper-parameters. In this case, TA2 will only tune\nhyper-parameters and resulting pipelines will have the same structure as given pipeline,\nbut hyper-parameter configuration will differ. If such potential behavior of a system is\nnot desired, a `use_default_values_for_free_hyperparams` flag can be set to true.\n\n## Values\n\nSome messages contain data values which can be passed between TA2 and TA3. There are\nmultiple ways those values can be passed and they are listed in the [`value.proto`](./value.proto)\nfile:\n\n* Put simple raw values directly in the message.\n* If a value is a Dataset container value, read or write it through a dataset URI.\n* Value can also be Python-pickled and stored at a URI or given directly in the message.\n* If value is a tabular container value, it can also be stored as a CSV file.\n* Value can be stored into a shared [Plasma store](https://arrow.apache.org/docs/python/plasma.html),\n  in which case value is represented by its Plasma ObjectID.\n\nBecause not all systems can or are willing to support all ways to pass the value, and we can\nextend them in the future, API supports signaling between TA2 and TA3 which value types are\nallowed/supported through `Hello` call.\n\n## Example call flows\n\n### Basic pipeline search\n\nBelow is an example call flow in which a TA3 system initiates a pipeline search request without any\npreprocessing or postprocessing, and the TA2 system returns two pipelines through a series of streamed\nresponses. Responses for multiple pipelines are transmitted each using one GRPC stream and can be\ninterleaved. Client then requests scores for one.\n\n```mermaid\nsequenceDiagram\n  participant Client\n  participant ScoreSolution\n  participant SearchSolutions\n  Client->>SearchSolutions: SearchSolutionsRequest\n  SearchSolutions-->>Client: SearchSolutionsResponse { search_id = 057cf5... }\n  Client->>+SearchSolutions: GetSearchSolutionsResults(GetSearchSolutionsResultsRequest)\n  SearchSolutions-->>Client: GetSearchSolutionsResultsResponse { solution_id = a5d78d... }\n  SearchSolutions-->>Client: GetSearchSolutionsResultsResponse { solution_id = b6d5e2... }\n  Client->>ScoreSolution: ScoreSolutionRequest { a5d78d... }\n  ScoreSolution-->>Client: ScoreSolutionResponse { request_id = 1d9193... }\n  Client->>+ScoreSolution: GetScoreSolutionResults(GetScoreSolutionResultsRequest)\n  ScoreSolution-->>Client: ScoreSolutionResultsResponse { progress = PENDING }\n  ScoreSolution-->>Client: ScoreSolutionResultsResponse { progress = RUNNING }\n  ScoreSolution-->>Client: ScoreSolutionResultsResponse { progress = COMPLETED, scores }\n  ScoreSolution-->>-Client: (ScoreSolution stream ends)\n  Client->>SearchSolutions: EndSearchSolutions(EndSearchSolutionsRequest)\n  SearchSolutions-->>Client: EndSearchSolutionsResponse\n  SearchSolutions-->>-Client: (GetFoundSolutions stream ends)\n```\n\n```\n1. Client: SearchSolutions(SearchSolutionsRequest) // problem = {...}, template = {...}, inputs = [dataset_uri]\n2. Server: SearchSolutionsResponse // search_id = 057cf581-5d5e-48b2-8867-db72e7d1381d\n3. Client: GetSearchSolutionsResults(GetSearchSolutionsResultsRequest) // search_id = 057cf581-5d5e-48b2-8867-db72e7d1381d\n[SEARCH SOLUTIONS STREAM BEGINS]\n4. Server: GetSearchSolutionsResultsResponse // progress = PENDING\n5. Server: GetSearchSolutionsResultsResponse // progress = RUNNING, solution_id = 5b08f87a-8393-4fa4-95be-91a3e587fe54, internal_score = 0.6, done_ticks = 0.5, all_ticks = 1.0\n6. Server: GetSearchSolutionsResultsResponse // progress = RUNNING, solution_id = 95de692f-ea81-4e7a-bef3-c01f18281bc0, internal_score = 0.8, done_ticks = 1.0, all_ticks = 1.0\n7. Server: GetSearchSolutionsResultsResponse // progress = COMPLETED\n[SEARCH SOLUTIONS STREAM ENDS]\n8. Client: ScoreSolution(ScoreSolutionRequest) // solution_id = 95de692f-ea81-4e7a-bef3-c01f18281bc0, inputs = [dataset_uri], performance_metrics = [ACCURACY]\n9. Server: ScoreSolutionResponse // request_id = 5d919354-4bd3-4155-9295-406d8c02b915\n10. Client: GetScoreSolutionResults(GetScoreSolutionResultsRequest) // request_id = 5d919354-4bd3-4155-9295-406d8c02b915\n[SCORE SOLUTION STREAM BEGINS]\n11. Server: GetScoreSolutionResultsResponse // progress = PENDING\n12. Server: GetScoreSolutionResultsResponse // progress = RUNNING\n13. Server: GetScoreSolutionResultsResponse // progress = COMPLETED, scores = [0.9]\n[SCORE SOLUTION STREAM END]\n14. Client: EndSearchSolutions(EndSearchSolutionsRequest) // search_id = 057cf581-5d5e-48b2-8867-db72e7d1381d\n15. Server: EndSearchSolutionsResponse\n```\n\n### Pass-through execution of a primitive\n\nExample call flow for a TA3 system calling one primitive on a dataset and storing transformed dataset into a\nPlasma store where it can efficiently access it using memory sharing and display it to the user.\nEven if the primitive is just a transformation and fitting is not necessary, TA3 has to fit a solution\nbefore it is able to call produce.\n\nThis example has as an input dataset and as the output dataset as well. This is different from regular\npipelines which take dataset as input and produce predictions as output. The reason is that the\npipeline is full specified by a TA3 system so inputs and outputs can be anything.\n\n```\n1. Client: SearchSolutions(SearchSolutionsRequest) // problem = {...}, template = {...}, inputs = [dataset_uri]\n2. Server: SearchSolutionsResponse // search_id = ae4de7f4-4435-4d86-834b-c183ef85f2d0\n3. Client: GetSearchSolutionsResults(GetSearchSolutionsResultsRequest) // search_id = ae4de7f4-4435-4d86-834b-c183ef85f2d0\n[SEARCH SOLUTIONS STREAM BEGINS]\n4. Server: GetSearchSolutionsResultsResponse // progress = PENDING\n5. Server: GetSearchSolutionsResultsResponse // progress = RUNNING, solution_id = 619e09ee-ccf5-4bd2-935d-41094169b0c5, internal_score = NaN, done_ticks = 1.0, all_ticks = 1.0\n6. Server: GetSearchSolutionsResultsResponse // progress = COMPLETED\n[SEARCH SOLUTIONS STREAM ENDS]\n7. Client: FitSolution(FitSolutionRequest) // solution_id = 619e09ee-ccf5-4bd2-935d-41094169b0c5, inputs = [dataset_uri]\n8. Server: FitSolutionResponse // request_id = e7fe4ef7-8b3a-4365-9fc4-c1a8228c509c\n9. Client: GetFitSolutionResults(GetFitSolutionResultsRequest) // request_id = e7fe4ef7-8b3a-4365-9fc4-c1a8228c509c\n[FIT SOLUTION STREAM BEGINS]\n10. Server: GetFitSolutionResultsResponse // progress = PENDING\n11. Server: GetFitSolutionResultsResponse // progress = RUNNING\n12. Server: GetFitSolutionResultsResponse // progress = COMPLETED, fitted_solution_id = 88d627a4-e4ca-4b1a-9f2e-af9c54dfa860\n[FIT SOLUTION STREAM END]\n13. Client: ProduceSolution(ProduceSolutionRequest) // fitted_solution_id = 88d627a4-e4ca-4b1a-9f2e-af9c54dfa860, inputs = [dataset_uri], expose_outputs = [\"outputs.0\"], expose_value_types = [PLASMA_ID]\n14. Server: ProduceSolutionResponse // request_id = 954b19cc-13d4-4c2a-a98f-8c15498014ac\n15. Client: GetProduceSolutionResults(GetProduceSolutionResultsRequest) // request_id = 954b19cc-13d4-4c2a-a98f-8c15498014ac\n[PRODUCE SOLUTION STREAM BEGINS]\n16. Server: GetProduceSolutionResultsResponse // progress = PENDING\n17. Server: GetProduceSolutionResultsResponse // progress = RUNNING, steps = [progress = PENDING]\n18. Server: GetProduceSolutionResultsResponse // progress = RUNNING, steps = [progress = RUNNING]\n19. Server: GetProduceSolutionResultsResponse // progress = RUNNING, steps = [progress = COMPLETED]\n20. Server: GetProduceSolutionResultsResponse // progress = COMPLETED, steps = [progress = COMPLETED], exposed_outputs = {\"outputs.0\": ObjectID(6811fc1154520d677d58b01a51b47036d5a408a8)}\n[PRODUCE SOLUTION STREAM END]\n21. Client: EndSearchSolutions(EndSearchSolutionsRequest) // search_id = ae4de7f4-4435-4d86-834b-c183ef85f2d0\n22. Server: EndSearchSolutionsResponse\n```\n\n`template` used above could look like (with message shown in JSON):\n\n```json\n{\n  \"inputs\": [\n    {\n      \"name\": \"dataset\"\n    }\n  ],\n  \"outputs\": [\n    {\n      \"name\": \"dataset\",\n      \"data\": \"step.0.produce\"\n    }\n  ],\n  \"steps\": [\n    {\n      \"primitive\": {\n        \"id\": \"f5c2f905-b694-4cf9-b8c3-7cd7cf8d6acf\"\n      },\n      \"arguments\": {\n        \"inputs\": {\n          \"data\": \"inputs.0\"\n        }\n      },\n      \"outputs\": [\n        {\n          \"id\": \"produce\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n## Standard port\n\nA standard port for TA2-TA3 API on which TA2 should listen for connections from a TA3 is 45042.\n\n## Protocol version\n\nTo support easier debugging `SearchSolutionsRequest` and `SearchSolutionsResponse` messages contain a version of the protocol\nused by each party. This can serve to easier understand a potential problem by detecting a version mismatch.\n\nFor this to work, `version` field has to be populated from the value stored in the protocol specification itself.\nWe use [custom options](https://developers.google.com/protocol-buffers/docs/proto#customoptions) for this.\nTo retrieve the version from the protocol specification, you can do the following in Python:\n\n```python\nimport core_pb2\nversion = core_pb2.DESCRIPTOR.GetOptions().Extensions[core_pb2.protocol_version]\n```\n\nIn Go, accessing version is slightly more involved and it is described\n[here](https://gitlab.com/datadrivendiscovery/ta3ta2-api/snippets/1684616).\n\n## Extensions of messages\n\nGRPC and Protocol Buffers support a simple method of extending messages: just define extra fields with custom tags\nin your local version of the protocol. Users for this protocol can do that to experiment with variations of the protocol (and if\nchanges work out, they can submit a merge request for those changes to be included into this specification).\nTo make sure such unofficial fields in messages do not conflict between performers, use values from the\n[allocated tag ranges](./private_tag_ranges.txt) for your organization, or add your organization via a\nmerge request.\n\n## Changelog\n\nSee [HISTORY.md](./HISTORY.md) for summary of changes to the API.\n\n## Repository structure\n\n`master` branch contains latest stable release of the TA3-TA2 API specification.\n`devel` branch is a staging branch for the next release.\n\nReleases are [tagged](https://gitlab.com/datadrivendiscovery/ta3ta2-api/tags).\n\nAt every commit to `master` and `devel` branches we compile `.proto` files and push\ncompiled files to `dist-*` and `dev-dist-*` branches for multiple languages. You can use those\nbranches in your projects directly using `git submodule` or some other similar mechanism.\n\n## Contributing\n\nSee [contributing guide](./CONTRIBUTING.md) for more information how to contribute to the API development.\n\n## About Data Driven Discovery Program\n\nDARPA Data Driven Discovery (D3M) Program is researching ways to get machines to build\nmachine learning pipelines automatically. It is split into three layers:\nTA1 (primitives), TA2 (systems which combine primitives automatically into pipelines\nand executes them), and TA3 (end-users interfaces).",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.com/datadrivendiscovery/ta3ta2-api",
    "keywords": "",
    "license": "Apache-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ta3ta2-api",
    "package_url": "https://pypi.org/project/ta3ta2-api/",
    "platform": "",
    "project_url": "https://pypi.org/project/ta3ta2-api/",
    "project_urls": {
      "Homepage": "https://gitlab.com/datadrivendiscovery/ta3ta2-api"
    },
    "release_url": "https://pypi.org/project/ta3ta2-api/2020.6.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "GRPC protocol for the TA3-TA2 communication API.",
    "version": "2020.6.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7466914,
  "releases": {
    "2020.6.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "36b28e350adc0383973cbeb157fbccc9e9c1f8bdb8039c6cd2948fb277bf05cb",
          "md5": "10f970b5b83930c50db1be95eda42b6c",
          "sha256": "4b12b8b1f286209011146eeb2188348c335dc9288bf282541d3557dd4f32a907"
        },
        "downloads": -1,
        "filename": "ta3ta2_api-2020.6.2.tar.gz",
        "has_sig": false,
        "md5_digest": "10f970b5b83930c50db1be95eda42b6c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 44284,
        "upload_time": "2020-06-02T23:18:51",
        "upload_time_iso_8601": "2020-06-02T23:18:51.365581Z",
        "url": "https://files.pythonhosted.org/packages/36/b2/8e350adc0383973cbeb157fbccc9e9c1f8bdb8039c6cd2948fb277bf05cb/ta3ta2_api-2020.6.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "36b28e350adc0383973cbeb157fbccc9e9c1f8bdb8039c6cd2948fb277bf05cb",
        "md5": "10f970b5b83930c50db1be95eda42b6c",
        "sha256": "4b12b8b1f286209011146eeb2188348c335dc9288bf282541d3557dd4f32a907"
      },
      "downloads": -1,
      "filename": "ta3ta2_api-2020.6.2.tar.gz",
      "has_sig": false,
      "md5_digest": "10f970b5b83930c50db1be95eda42b6c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 44284,
      "upload_time": "2020-06-02T23:18:51",
      "upload_time_iso_8601": "2020-06-02T23:18:51.365581Z",
      "url": "https://files.pythonhosted.org/packages/36/b2/8e350adc0383973cbeb157fbccc9e9c1f8bdb8039c6cd2948fb277bf05cb/ta3ta2_api-2020.6.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}