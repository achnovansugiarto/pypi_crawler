{
  "info": {
    "author": "Original work by Max Woolf, forked by Ceyhun Derinbogaz",
    "author_email": "ceyhun@textcortex.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# aitextgen-aws\n\naitextgen is a wrapper based on the work by Max Woolf. Edited for distributed computing on AWS since current version of the aitextgen does not allow to set the parameter for Pytorch Lightnings Distributed modes.\n\nA robust Python tool for text-based AI training and generation using [OpenAI's](https://openai.com) [GPT-2](https://openai.com/blog/better-language-models/) and [EleutherAI's](https://www.eleuther.ai) [GPT Neo/GPT-3](https://github.com/EleutherAI/gpt-neo) architecture.\n\naitextgen is a Python package that leverages [PyTorch](https://pytorch.org), [Hugging Face Transformers](https://github.com/huggingface/transformers) and [pytorch-lightning](https://github.com/PyTorchLightning/pytorch-lightning) with specific optimizations for text generation using GPT-2, plus _many_ added features. It is the successor to [textgenrnn](https://github.com/minimaxir/textgenrnn) and [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple), taking the best of both packages:\n\n- Finetunes on a pretrained 124M/355M/774M GPT-2 model from OpenAI or a 125M/350M GPT Neo model from EleutherAI...or create your own GPT-2/GPT Neo model + tokenizer and train from scratch!\n- Generates text faster than gpt-2-simple and with better memory efficiency!\n- With Transformers, aitextgen preserves compatibility with the base package, allowing you to use the model for other NLP tasks, download custom GPT-2 models from the HuggingFace model repository, and upload your own models! Also, it uses the included `generate()` function to allow a massive amount of control over the generated text.\n- With pytorch-lightning, aitextgen trains models not just on CPUs and GPUs, but also _multiple_ GPUs and (eventually) TPUs! It also includes a pretty training progress bar, with the ability to add optional loggers.\n- The input dataset is its own object, allowing you to not only easily encode megabytes of data in seconds, cache, and compress it on a local computer before transporting to a remote server, but you are able to _merge_ datasets without biasing the resulting dataset, or _cross-train_ on multiple datasets to create blended output.\n\nYou can read more about aitextgen [in the documentation](https://docs.aitextgen.io/)!\n\n## Demo\n\nYou can play with aitextgen _for free_ with powerful GPUs using these Colaboratory Notebooks!\n\n- [Finetune OpenAI's 124M GPT-2 model (or GPT Neo) on your own dataset (GPU)](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing)\n- [Train a GPT-2 model + tokenizer from scratch (GPU)](https://colab.research.google.com/drive/144MdX5aLqrQ3-YW-po81CQMrD6kpgpYh?usp=sharing)\n\nYou can also play with custom [Reddit](notebooks/reddit_demo.ipynb) and [Hacker News](notebooks/hacker_news_demo.ipynb) demo models on your own PC.\n\n## Installation\n\naitextgen can be installed [from PyPI](https://pypi.org/project/aitextgen/):\n\n```sh\npip3 install aitextgen\n```\n\n## Quick Examples\n\nHere's how you can quickly test out aitextgen on your own computer, even if you don't have a GPU!\n\nFor generating text from a pretrained GPT-2 model:\n\n```py3\nfrom aitextgen import aitextgen\n\n# Without any parameters, aitextgen() will download, cache, and load the 124M GPT-2 \"small\" model\nai = aitextgen()\n\nai.generate()\nai.generate(n=3, max_length=100)\nai.generate(n=3, prompt=\"I believe in unicorns because\", max_length=100)\nai.generate_to_file(n=10, prompt=\"I believe in unicorns because\", max_length=100, temperature=1.2)\n```\n\nYou can also generate from the command line:\n\n```sh\naitextgen generate\naitextgen generate --prompt \"I believe in unicorns because\" --to_file False\n```\n\nWant to train your own mini GPT-2 model on your own computer? You can follow along [in this Jupyter Notebook](/notebooks/training_hello_world.ipynb) or, download this [text file of Shakespeare's plays](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt), cd to that directory in a Terminal, open up a `python3` console and go:\n\n```py3\nfrom aitextgen.TokenDataset import TokenDataset\nfrom aitextgen.tokenizers import train_tokenizer\nfrom aitextgen.utils import GPT2ConfigCPU\nfrom aitextgen import aitextgen\n\n# The name of the downloaded Shakespeare text for training\nfile_name = \"input.txt\"\n\n# Train a custom BPE Tokenizer on the downloaded text\n# This will save one file: `aitextgen.tokenizer.json`, which contains the\n# information needed to rebuild the tokenizer.\ntrain_tokenizer(file_name)\ntokenizer_file = \"aitextgen.tokenizer.json\"\n\n# GPT2ConfigCPU is a mini variant of GPT-2 optimized for CPU-training\n# e.g. the # of input tokens here is 64 vs. 1024 for base GPT-2.\nconfig = GPT2ConfigCPU()\n\n# Instantiate aitextgen using the created tokenizer and config\nai = aitextgen(tokenizer_file=tokenizer_file, config=config)\n\n# You can build datasets for training by creating TokenDatasets,\n# which automatically processes the dataset with the appropriate size.\ndata = TokenDataset(file_name, tokenizer_file=tokenizer_file, block_size=64)\n\n# Train the model! It will save pytorch_model.bin periodically and after completion to the `trained_model` folder.\n# On a 2020 8-core iMac, this took ~25 minutes to run.\nai.train(data, batch_size=8, num_steps=50000, generate_every=5000, save_every=5000)\n\n# Generate text from it!\nai.generate(10, prompt=\"ROMEO:\")\n\n# With your trained model, you can reload the model at any time by\n# providing the folder containing the pytorch_model.bin model weights + the config, and providing the tokenizer.\nai2 = aitextgen(model_folder=\"trained_model\",\n                tokenizer_file=\"aitextgen.tokenizer.json\")\n\nai2.generate(10, prompt=\"ROMEO:\")\n```\n\nWant to run aitextgen and finetune GPT-2? Use the Colab notebooks in the Demos section, or [follow the documentation](https://docs.aitextgen.io/) to get more information and learn some helpful tips!\n\n## Known Issues\n\n- TPUs cannot be used to train a model: although you _can_ train an aitextgen model on TPUs by setting `n_tpu_cores=8` in an appropriate runtime, and the training loss indeed does decrease, there are a number of miscellaneous blocking problems. [[Tracking GitHub Issue](https://github.com/minimaxir/aitextgen/issues/3)]\n\n## Upcoming Features\n\nThe current release (v0.5.X) of aitextgen **is considered to be a beta**, targeting the most common use cases. The Notebooks and examples written so far are tested to work, but more fleshing out of the docs/use cases will be done over the next few months in addition to fixing the known issues noted above.\n\nThe next versions of aitextgen (and one of the reasons I made this package in the first place) will have native support for _schema-based generation_. (See [this repo](https://github.com/minimaxir/gpt-2-keyword-generation) for a rough proof-of-concept.)\n\nAdditionally, I plan to develop an aitextgen [SaaS](https://en.wikipedia.org/wiki/Software_as_a_service) to allow anyone to run aitextgen in the cloud and build APIs/Twitter+Slack+Discord bots with just a few clicks. (The primary constraint is compute cost; if any venture capitalists are interested in funding the development of such a service, let me know.)\n\nI've listed more tentative features in the [UPCOMING](UPCOMING.md) document.\n\n## Ethics\n\naitextgen is a tool primarily intended to help facilitate creative content. It is not a tool intended to deceive. Although parody accounts are an obvious use case for this package, make sure you are _as upfront as possible_ with the methodology of the text you create. This includes:\n\n- State that the text was generated using aitextgen and/or a GPT-2 model architecture. (A link to this repo would be a bonus!)\n- If parodying a person, explicitly state that it is a parody, and reference who it is parodying.\n- If the generated text is human-curated, or if it's unsupervised random output.\n- Indicating who is maintaining/curating the AI-generated text.\n- Make a good-faith effort to remove overfit output from the generated text that matches the input text verbatim.\n\nIt's fun to anthropomorphise the nameless \"AI\" as an abstract genius, but part of the reason I made aitextgen (and all my previous text-generation projects) is to make the technology more accessible and accurately demonstrate both its promise, and its limitations. **Any AI text generation projects that are deliberately deceptive may be disavowed.**\n\n## Maintainer/Creator\n\nMax Woolf ([@minimaxir](https://minimaxir.com))\n\n_Max's open-source projects are supported by his [Patreon](https://www.patreon.com/minimaxir) and [GitHub Sponsors](https://github.com/sponsors/minimaxir). If you found this project helpful, any monetary contributions to the Patreon are appreciated and will be put to good creative use._\n\n## License\n\nMIT\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/cderinbogaz/aitextgen-aws",
    "keywords": "gpt-2,gpt2,text generation,ai",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "aitextgenAws",
    "package_url": "https://pypi.org/project/aitextgenAws/",
    "platform": "",
    "project_url": "https://pypi.org/project/aitextgenAws/",
    "project_urls": {
      "Homepage": "https://github.com/cderinbogaz/aitextgen-aws"
    },
    "release_url": "https://pypi.org/project/aitextgenAws/0.1/",
    "requires_dist": [
      "transformers (>=4.5.1)",
      "fire (>=0.3.0)",
      "pytorch-lightning (>=1.2.7)",
      "torch (>=1.6.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "Fork of aitextgen to set parallel computing settings to be able to run on AWS sagemaker.",
    "version": "0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10370403,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4032e5e4106bd1a71e36568c8bc06512640129e7e5d82261fae6f4f699cf1d50",
          "md5": "e5b4fbbe94429291f2367ca2d337f2ec",
          "sha256": "52cc308912d566930c81b8589b34696a77b36b7fffffd1335922a164ffb9462a"
        },
        "downloads": -1,
        "filename": "aitextgenAws-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e5b4fbbe94429291f2367ca2d337f2ec",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 576005,
        "upload_time": "2021-05-16T12:04:32",
        "upload_time_iso_8601": "2021-05-16T12:04:32.512194Z",
        "url": "https://files.pythonhosted.org/packages/40/32/e5e4106bd1a71e36568c8bc06512640129e7e5d82261fae6f4f699cf1d50/aitextgenAws-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8fbea8559a3a8ec36783709ec4a3340a14bcc9b9f9716032bbd177b1959e03b0",
          "md5": "a7440078020dd2bf4dbd0ae17bbbd5f1",
          "sha256": "d51401def5fa80fcbdd1b3eaac176d3e234de17591773773632943ca3242dc68"
        },
        "downloads": -1,
        "filename": "aitextgenAws-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a7440078020dd2bf4dbd0ae17bbbd5f1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 571858,
        "upload_time": "2021-05-16T12:04:34",
        "upload_time_iso_8601": "2021-05-16T12:04:34.930793Z",
        "url": "https://files.pythonhosted.org/packages/8f/be/a8559a3a8ec36783709ec4a3340a14bcc9b9f9716032bbd177b1959e03b0/aitextgenAws-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4032e5e4106bd1a71e36568c8bc06512640129e7e5d82261fae6f4f699cf1d50",
        "md5": "e5b4fbbe94429291f2367ca2d337f2ec",
        "sha256": "52cc308912d566930c81b8589b34696a77b36b7fffffd1335922a164ffb9462a"
      },
      "downloads": -1,
      "filename": "aitextgenAws-0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e5b4fbbe94429291f2367ca2d337f2ec",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 576005,
      "upload_time": "2021-05-16T12:04:32",
      "upload_time_iso_8601": "2021-05-16T12:04:32.512194Z",
      "url": "https://files.pythonhosted.org/packages/40/32/e5e4106bd1a71e36568c8bc06512640129e7e5d82261fae6f4f699cf1d50/aitextgenAws-0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8fbea8559a3a8ec36783709ec4a3340a14bcc9b9f9716032bbd177b1959e03b0",
        "md5": "a7440078020dd2bf4dbd0ae17bbbd5f1",
        "sha256": "d51401def5fa80fcbdd1b3eaac176d3e234de17591773773632943ca3242dc68"
      },
      "downloads": -1,
      "filename": "aitextgenAws-0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "a7440078020dd2bf4dbd0ae17bbbd5f1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 571858,
      "upload_time": "2021-05-16T12:04:34",
      "upload_time_iso_8601": "2021-05-16T12:04:34.930793Z",
      "url": "https://files.pythonhosted.org/packages/8f/be/a8559a3a8ec36783709ec4a3340a14bcc9b9f9716032bbd177b1959e03b0/aitextgenAws-0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}