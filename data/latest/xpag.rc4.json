{
  "info": {
    "author": "Nicolas Perrin-Gilbert",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# ![alt text](https://raw.githubusercontent.com/perrin-isir/xpag/main/logo.png \"xpag logo\")\n\n![version](https://img.shields.io/badge/version-0.1.5-blue)\n[![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![Documentation](https://img.shields.io/github/actions/workflow/status/perrin-isir/xpag/docs.yml?branch=main&label=docs)](https://perrin-isir.github.io/xpag/)\n[![PyPI version](https://img.shields.io/pypi/v/xpag)](https://pypi.org/project/xpag/)\n\n\n*xpag* (\"e**xp**loring **ag**ents\") is a modular reinforcement learning library with JAX agents, currently in beta version.\n\n-----\n## Install\n\n<details><summary>Option 1: conda (preferred option)</summary>\n<p>\n\t\nThis option is preferred because it relies mainly on conda-forge packages (which among other things simplifies the installation of JAX).\n\n    git clone https://github.com/perrin-isir/xpag.git\n    cd xpag\n    conda update conda\n   \nInstall micromamba if you don't already have it (you can also simply use conda, by replacing below `micromamba create`, `micromamba update` and `micromamba activate` respectively by `conda env create`, `conda env update` and `conda activate`, but this will lead to a significantly slower installation):\n\n    conda install -c conda-forge micromamba\n\nChoose an environment name, for instance `xpagenv`.  \nThe following command creates the `xpagenv` environment with the requirements listed in [environment.yaml](environment.yaml):\n\n    micromamba create --name xpagenv --file environment.yaml\n\nIf you prefer to update an existing environment (`existing_env`):\n\n    micromamba update --name existing_env --file environment.yaml\n\nThen, activate the `xpagenv` environment:\n\n    micromamba activate xpagenv\n\nFinally, install the *xpag* library in the activated environment:\n\n    pip install -e .\n\n</p>\n</details>\n\n<details><summary>Option 2: pip</summary>\n<p>\n\nFor the pip install, you need to properly install JAX yourself. Otherwise, if JAX is installed automatically as a pip dependency of *xpag*, it will probably not work as desired (e.g. it will not be GPU-compatible). So you should install it beforehand, following these guidelines: \n\n[https://github.com/google/jax#installation](https://github.com/google/jax#installation) \n\nThen, install *xpag* with:\n\n    pip install xpag\n\n</p>\n</details>\n\n<details><summary>JAX</summary>\n<p>\n\nTo verify that the JAX installation went well, check the backend used by JAX with the following command:\n```\npython -c \"import jax; print(jax.lib.xla_bridge.get_backend().platform)\"\n```\nIt will print \"cpu\", \"gpu\" or \"tpu\" depending on the platform JAX is using.\n\n</p>\n</details>\n\n<details><summary>Tutorials</summary>\n<p>\n\nThe following libraries, not required by *xpag*, are required for the [tutorials](https://github.com/perrin-isir/xpag-tutorials):\n  - MuJoCo (`pip install mujoco`): see [https://github.com/deepmind/mujoco](https://github.com/deepmind/mujoco)\n  - imageio (`pip install imageio`): see [https://github.com/imageio/imageio](https://github.com/imageio/imageio)\n</p>\n</details>\n\n-----\n## Tutorials\n\nThe *xpag-tutorials* repository contains a list of tutorials (colab notebooks) for *xpag*:  \n[https://github.com/perrin-isir/xpag-tutorials](https://github.com/perrin-isir/xpag-tutorials)\n\n\n-----\n## Short documentation\n<details><summary><B><I>xpag</I>: a platform for goal-conditioned RL</B></summary>\n\n*xpag* allows standard reinforcement learning, but it has been designed with\ngoal-conditioned reinforcement learning (GCRL) in mind (check out the [train_gmazes.ipynb](https://colab.research.google.com/github/perrin-isir/xpag-tutorials/blob/main/train_gmazes.ipynb)\ntutorial for a simple example of GCRL). \n\nIn GCRL, agents have a goal, which is part of the input they take, and the reward mainly depends on \nthe degree of achievement of that goal. Beyond the usual modules in \nRL platforms (environment, agent, buffer/sampler), *xpag* introduces a \nmodule called \"setter\" which, among other things, can help to set and manage\ngoals (for example modifying the goal several times in a single episode).\nAlthough the setter is largely similar to an environment wrapper, it \nis separated from the environment because in some cases it should be considered as \nan independent entity (e.g. a teacher), or as a part of the agent itself.\n\n*xpag* relies on a single reinforcement learning loop (the `learn()`\nfunction in [xpag/tools/learn.py](https://github.com/perrin-isir/xpag/blob/main/xpag/tools/learn.py))\nin which the environment, the agent, the buffer and the setter interact (see below). \nThe `learn()` function  has the following first 3 arguments (returned by [gym_vec_env()](https://github.com/perrin-isir/xpag/blob/main/xpag/wrappers/gym_vec_env.py) and \n[brax_vec_env()](https://github.com/perrin-isir/xpag/blob/main/xpag/wrappers/brax_vec_env.py)):\n  * `env`: the training environment, which runs 1 or more rollouts in parallel.\n  * `eval_env`: the evaluation environment, identical to `env` except that it runs \n  a single rollout.\n  * `env_info`: a dictionary containing information about the environment:\n    * `env_info[\"env_type\"]`: the type of environment; for the moment *xpag* \n  differentiates 3 types of environments: \"Brax\" environments, \"Mujoco\" environments, and\n  \"Gym\" environments. This information is used to adapt the way episodes are saved and replayed.\n    * `env_info[\"name\"]`: the name of the environment.\n    * `env_info[\"is_goalenv\"]`: whether the environment is a goal-based environment or \n  not.\n    * `env_info[\"num_envs\"]`: the number of parallel rollouts in `env`\n    * `env_info[\"max_episode_steps\"]`: the maximum number of steps in episodes (*xpag* \n  does not allow potentially infinite episodes).\n    * `env_info[\"action_space\"]`: the action space (of type [gym.spaces.Space](https://github.com/openai/gym/blob/master/gym/spaces/space.py)) that takes into account parallel rollouts. It can be useful to sample random actions.\n    * `env_info[\"single_action_space\"]`: the action space (of type [gym.spaces.Space](https://github.com/openai/gym/blob/master/gym/spaces/space.py)) for single rollouts.  \n  \n  `learn()` also takes in input the agent, the buffer and the setter and various parameters. Detailed information about the arguments of `learn()` can be\n  found in the code documentation (check [xpag/tools/learn.py](https://github.com/perrin-isir/xpag/blob/main/xpag/tools/learn.py)).\n\nThe components that interact during learning are:\n<details><summary><B>the environment (env)</B></summary>\n\nIn *xpag*, environments must allow parallel rollouts, and *xpag* keeps the same API even in the case of a single rollout,\ni.e. when the number of \"parallel environments\" is 1. Basically, all environments are \n\"vector environments\".\n\n* `env.reset(seed: Optional[Union[int, List[int]]], options: Optional[dict])` -> `observation: Union[np.array, jax.numpy.array], info: dict`  \nFollowing the gym Vector API\n(see [https://www.gymlibrary.dev/api/vector/#vectorenv](https://www.gymlibrary.dev/api/vector/#vectorenv)), environments have \na `reset()` function that returns an `observation` (which is actually a batch of observations for all the \nparallel rollouts) and an optional dictionary `info` (see [https://www.gymlibrary.dev/api/vector/#reset](https://www.gymlibrary.dev/api/vector/#reset)).  \nWe expect `observation` to be a numpy array, or a jax.numpy array, and its first dimension \nselects between parallel rollouts, which means that `observation[i]` is the observation in\nthe i-th rollout. In the case of a single rollout, `observation[0]` is the observation\nin this rollout.\n\n\n* `env.step(action: Union[np.array, jax.numpy.array])` -> `observation, reward, terminated, truncated, info`  \nAgain, following the gym Vector API, environments have a `step()` function that takes\nin input an action (which is actually a batch of actions, one per rollout) and returns:\n`observation`, `reward`, `terminated`, `truncated`, `info` (cf. [https://www.gymlibrary.dev/api/vector/#step](https://www.gymlibrary.dev/api/vector/#step)).\nThere are slight differences with the gym Vector API. First, in *xpag* this API also covers the case\nof a single rollout. Second, *xpag* assumes that `reward` and `done` have shape `(n, 1)`, not\n`(n,)` (where n is the number of parallel rollouts). More broadly, whether they are due to a single rollout or to\nunidimensional elements, single-dimensional entries are not squeezed in *xpag*.\nThird, in *xpag*, `info` is a dictionary, not a tuple of dictionaries\n(however its entries may be tuples). \n\n\n* `env.reset_done(done, seed: Optional[Union[int, List[int]]], options: Optional[dict])` -> `observation, info`   \nThe most significant difference with the gym Vector API is that *xpag* requires a `reset_done()` function which takes a `done` array of Booleans in input and performs a reset for\nthe i-th rollout if and only if `done[i]` is evaluated to True. Besides `done`, the arguments of `reset_done()` are the same as the ones of `reset()`: `seed` and `options`, and its outputs are also the same: `observation`, `info`.\nFor rollouts that are not reset, the returned observation is the same as the observation returned by the last\n`step()`. `reset()` must be called once for the initial reset, and afterwards only `reset_done()` should be used. Auto-resets (automatic resets after terminal transitions) are not allowed in *xpag*. \nThe main reason to prefer `reset_done()` to auto-resets\nis that with auto-resets, terminal transitions must be special and contain additional\ninformation. With `reset_done()`, this is no longer necessary. Furthermore,\nby modifying the `done` array returned by a step of the environment, it becomes possible \nto easily force the termination of an episode, or to force an episode to continue despite \nreaching a terminal transition (but this must be done with caution).\n\n\n* `gym_vec_env(env_name: str, num_envs: int, wrap_function: Callable = None)` -> `env, eval_env, env_info: dict`  \n`brax_vec_env(env_name: str, num_envs: int, wrap_function: Callable = None, *, force_cpu_backend : bool = False)` -> `env, eval_env, env_info: dict`  \nThe [gym_vec_env()](https://github.com/perrin-isir/xpag/blob/main/xpag/wrappers/gym_vec_env.py) and \n[brax_vec_env()](https://github.com/perrin-isir/xpag/blob/main/xpag/wrappers/brax_vec_env.py) functions (see [tutorials](https://github.com/perrin-isir/xpag-tutorials))\ncall wrappers that automatically add the `reset_done()` function to Gym and Brax \nenvironments, and make the wrapped environments fit the *xpag* API.\n\n\n* *Goal-based environments:*  \nGoal-based environments (for GCRL) must have a similar interface to the one defined in \nthe [Gym-Robotics](https://github.com/Farama-Foundation/gym-robotics) library\n(see `GoalEnv` in [core.py](https://github.com/Farama-Foundation/Gym-Robotics/blob/main/gym_robotics/core.py)), with minor differences.\nTheir observation spaces are of type [gym.spaces.Dict](https://github.com/openai/gym/blob/master/gym/spaces/dict.py), with the following keys \nin the `observation` dictionaries: `\"observation\"`, `\"achieved_goal\"`, and `\"desired_goal\"`.\nGoal-based environments must also have in attribute a `compute_reward()` function that computes rewards.\nIn *xpag*, the inputs of `compute_reward()` can be different from the ones considered in \nthe original `GoalEnv` class. For example, in the\n[GoalEnvWrapper](https://github.com/perrin-isir/xpag/blob/main/xpag/wrappers/goalenv_wrapper.py) class,\nwhich can be used to turn standard environments into goal-based environments, the\narguments of `compute_reward()` are assumed to be `achieved_goal` (the goal achieved *after* `step()`),\n`desired_goal` (the desired goal *before* `step()`), `action`, `observation` (the observation *after* `step()`),\n`reward` (the reward of the base environment), `terminated`, `truncated` and `info` (the outputs of the\n`step()` function). In the version of [HER](https://github.com/perrin-isir/xpag/blob/main/xpag/samplers/HER.py)\n  (cf. [https://arxiv.org/pdf/1707.01495.pdf](https://arxiv.org/pdf/1707.01495.pdf)) in *xpag*,\nit is assumed that `compute_reward()` depends only on  `achieved_goal`, `desired_goal`, `action` and `observation`.  \nIn goal-based environments, the multiple observations from parallel rollouts are concatenated as in the gym function `concatenate()`\n(cf. [https://github.com/openai/gym/blob/master/gym/vector/utils/numpy_utils.py](https://github.com/openai/gym/blob/master/gym/vector/utils/numpy_utils.py)), \nwhich means that the batched observations are always single dictionaries in which the \nentries `\"observation\"`, `\"achieved_goal\"` and `\"desired_goal\"` are arrays of observations,\nachieved goals and desired goals.\n\n\n* `info`  \n*xpag* assumes that, in goal-based environments, the `info` dictionary returned by `step()`\nalways contains `info[\"is_success\"]`, an array of Booleans (one per rollout)\nthat are `True` if the corresponding transition is a successfull achievement of the\ndesired goal, and `False` otherwise (*remark:* this does not need to coincide\nwith episode termination).\n\n</details>\n\n<details><summary><B>the agent (agent)</B></summary>\n\n*xpag* only considers off-policy agents. (TODO) \n\n</details>\n\n<details><summary><B>the buffer (buffer)</B></summary> TODO </details>\n<details><summary><B>the sampler (sampler)</B></summary> TODO </details>\n<details><summary><B>the setter (setter)</B></summary> TODO </details>\n\nThe figure below summarizes the RL loop and the interactions between the components:\n(TODO)\n</details>\n\n-----\n## Acknowledgements\n\n* Maintainer and main contributor:\n  - Nicolas Perrin-Gilbert (CNRS, ISIR)\n\n  Other people who contributed to *xpag*:\n  - Olivier Serris (ISIR)\n  - Alexandre Chenu (ISIR)\n  - StÃ©phane Caron (Inria)\n  - Fabian Schramm (Inria)\n\n* The [SAC agent](https://github.com/perrin-isir/xpag/blob/main/xpag/agents/sac) is based on the implementation of SAC in [JAXRL](https://github.com/ikostrikov/jaxrl), and some elements of the [TQC agent](https://github.com/perrin-isir/xpag/blob/main/xpag/agents/tqc) come from the implementation of TQC in [RLJAX](https://github.com/ku2482/rljax).\n\n-----\n## Citing the project\nTo cite this repository in publications:\n\n```bibtex\n@misc{xpag,\n  author = {Perrin-Gilbert, Nicolas},\n  title = {xpag: a modular reinforcement learning library with JAX agents},\n  year = {2022},\n  url = {https://github.com/perrin-isir/xpag}\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/perrin-isir/xpag",
    "keywords": "",
    "license": "LICENSE",
    "maintainer": "",
    "maintainer_email": "",
    "name": "xpag",
    "package_url": "https://pypi.org/project/xpag/",
    "platform": null,
    "project_url": "https://pypi.org/project/xpag/",
    "project_urls": {
      "Homepage": "https://github.com/perrin-isir/xpag"
    },
    "release_url": "https://pypi.org/project/xpag/0.1.5/",
    "requires_dist": [
      "psutil (>=5.8.0)",
      "numpy (>=1.21.5)",
      "matplotlib (>=3.1.3)",
      "joblib (>=1.1.0)",
      "gymnasium (>=0.26.0)",
      "Pillow (>=9.0.1)",
      "ipywidgets (>=7.6.5)",
      "jax (>=0.3.23)",
      "flax (>=0.6.3)",
      "optax (>=0.1.2)",
      "brax (>=0.0.10)",
      "tensorflow-probability (>=0.15.0)",
      "mediapy (>=1.1.4)"
    ],
    "requires_python": "",
    "summary": "xpag: Exploring Agents",
    "version": "0.1.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17498874,
  "releases": {
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1436181ad072428c980b9d2347c7974afbe05bbbc839623d9d2f7e1307230661",
          "md5": "ce44a0928b8afa75e7e485b2dea98ebc",
          "sha256": "8beb564ef70981237fe7a96cc0e887c152ad7950cf4fdcd8f11d857ee4657b66"
        },
        "downloads": -1,
        "filename": "xpag-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ce44a0928b8afa75e7e485b2dea98ebc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 53799,
        "upload_time": "2023-03-21T15:31:13",
        "upload_time_iso_8601": "2023-03-21T15:31:13.920398Z",
        "url": "https://files.pythonhosted.org/packages/14/36/181ad072428c980b9d2347c7974afbe05bbbc839623d9d2f7e1307230661/xpag-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d17ea42a39761c2cecd79a457379a0c269c6fdcd84c191d6397ea7a2f3db03a2",
          "md5": "d7210a303d4f71a543fa56b66daf3beb",
          "sha256": "df0568e3fdda6e957ebc6ea39b5e98c0950e0a3395bf9a94e1a8c7d6d402ce8f"
        },
        "downloads": -1,
        "filename": "xpag-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d7210a303d4f71a543fa56b66daf3beb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 53795,
        "upload_time": "2023-03-22T09:25:51",
        "upload_time_iso_8601": "2023-03-22T09:25:51.353247Z",
        "url": "https://files.pythonhosted.org/packages/d1/7e/a42a39761c2cecd79a457379a0c269c6fdcd84c191d6397ea7a2f3db03a2/xpag-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6c7c4abdc22f055848a8c0eeec7c7460be896d6dbfee8d98a0080a440e4f8abb",
          "md5": "b5105b84425fd8d3992f6ede22d85dbf",
          "sha256": "5418a194cd468ad367809c295518fdb10512b2131057388473ee549cd251b621"
        },
        "downloads": -1,
        "filename": "xpag-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b5105b84425fd8d3992f6ede22d85dbf",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 53848,
        "upload_time": "2023-03-22T09:42:23",
        "upload_time_iso_8601": "2023-03-22T09:42:23.186550Z",
        "url": "https://files.pythonhosted.org/packages/6c/7c/4abdc22f055848a8c0eeec7c7460be896d6dbfee8d98a0080a440e4f8abb/xpag-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5619192a1baac5c6d71f23387080b0d6a63c448e1e03bb87165f366697e28989",
          "md5": "6fdb5056e2949786755517c440198ba5",
          "sha256": "a5a07000aca8488fc3ca91fae8adfe8a65a060b2410f767d8d4c0362bd941a48"
        },
        "downloads": -1,
        "filename": "xpag-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6fdb5056e2949786755517c440198ba5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 55006,
        "upload_time": "2023-03-29T20:02:40",
        "upload_time_iso_8601": "2023-03-29T20:02:40.313831Z",
        "url": "https://files.pythonhosted.org/packages/56/19/192a1baac5c6d71f23387080b0d6a63c448e1e03bb87165f366697e28989/xpag-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5619192a1baac5c6d71f23387080b0d6a63c448e1e03bb87165f366697e28989",
        "md5": "6fdb5056e2949786755517c440198ba5",
        "sha256": "a5a07000aca8488fc3ca91fae8adfe8a65a060b2410f767d8d4c0362bd941a48"
      },
      "downloads": -1,
      "filename": "xpag-0.1.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "6fdb5056e2949786755517c440198ba5",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 55006,
      "upload_time": "2023-03-29T20:02:40",
      "upload_time_iso_8601": "2023-03-29T20:02:40.313831Z",
      "url": "https://files.pythonhosted.org/packages/56/19/192a1baac5c6d71f23387080b0d6a63c448e1e03bb87165f366697e28989/xpag-0.1.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}