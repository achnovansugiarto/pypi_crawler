{
  "info": {
    "author": "Datopian",
    "author_email": "info@datopian.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# AirCan\n\nLoad data into [CKAN DataStore](https://docs.ckan.org/en/2.8/maintaining/datastore.html) using Airflow as the runner. This is a replacement for DataPusher and Xloader.\n\nClean separation of components so you can reuse what you want (e.g., you don't use Airflow but your own runner).\n\n<!-- toc -->\n\n- [AirCan](#-aircan)\n  - [Get Started](#get-started)\n  - [Examples](#examples)\n    - [Example 1: CSV to JSON](#example-1-csv-to-json)\n  - [Using Aircan DAGs](#using-aircan-dags)\n    - [Example 2: Local file to CKAN DataStore using the Datastore API](#example-2-local-file-to-ckan-datastore-using-the-datastore-api)\n      - [Preliminaries: Setup your CKAN instance](#preliminaries-setup-your-ckan-instance)\n    - [Ignore Example 3: Local file to CKAN DataStore using Postgres](#example-3-local-file-to-ckan-datastore-using-postgres)\n      - [Preliminaries: Setup your CKAN instance](#preliminaries-setup-your-ckan-instance-1)\n      - [Doing the load](#doing-the-load)\n    - [Example 2a: Remote file to DataStore](#example-2a-remote-file-to-datastore)\n    - [Example 3: Auto Load file uploaded to CKAN into CKAN DataStore](#examples-3-auto-load-file-uploaded-to-ckan-into-ckan-datastore)\n      - [Run it](#run-it)\n  - [Tutorials](#tutorials)\n    - [Using Google Cloud Composer](#using-google-cloud-composer)\n\n<!-- tocstop -->\n\n## Get Started\n\n* Install **Python** >= 3.5 <= 3.7.x (and make a virtual environment).\n* Clone `aircan` so you have examples available:\n\n  ```bash\n  git clone https://github.com/datopian/aircan\n  ```\n\n* Install and setup **Airflow** (https://airflow.apache.org/docs/stable/installation.html):\n\n  ```bash\n  export AIRFLOW_HOME=~/airflow\n  pip install apache-airflow\n  airflow initdb\n  ```\n\n_Note_: On recent versions of Python (3.7+), you may face the following error when executing `airflow initdb`:\n\n```bash\nModuleNotFoundError: No module named 'typing_extensions'\n```\n\nThis can be solved with `pip install typing_extensions`.\n\n* Then, start the server and visit your Airflow admin UI:\n\n  ```bash\n  airflow webserver -p 8080\n  ```\n\nBy default, the server will be accessible at `http://localhost:8080/` as shown in the output of the terminal where you ran the previous command.\n\n\n\n## Examples\n\n### Example 1: CSV to JSON\n\nIn this example we'll run an AirCan example to convert a CSV to JSON.\n\nAdd the DAG to the default directory for Airflow to recognize it:\n\n```bash\nmkdir ~/airflow/dags/\ncp examples/aircan-example-1.csv ~/airflow/\ncp examples/csv_to_json.py ~/airflow/dags/\n```\n\nTo see this DAG appear in the Airflow admin UI, you may need to restart the server or launch the scheduler to update the list of DAGs (this may take about a minute or two to update, then refresh the page on the Airflow admin UI):\n\n```bash\nairflow scheduler\n```\n\nRun this DAG:\n\n* Enable the dag in the [admin UI](http://localhost:8080/) with this toggle to make it run with the scheduler:\n\n  ![aircan_enable_example_dag_in_scheduler](docs/resources/images/aircan_enable_example_dag_in_scheduler.png)\n* \"Trigger\" the DAG with this button:\n\n  ![aircan_trigger_example_dag_to_run](docs/resources/images/aircan_trigger_example_dag_to_run.png)\n\n* After a moment, check the output. You should see a successful run for this DAG:\n\n  ![aircan_output_example_dag](docs/resources/images/aircan_output_example_dag.png)\n\n* Locate the output on disk at `~/airflow/aircan-example-1.json`\n\n\n\n## Using Aircan DAGs in a local Airflow instance\n\n### Example 2: Local file to CKAN DataStore using the Datastore API\n\nWe'll assume you have:\n\n* a local CKAN setup and running at http://localhost:5000;\n* a dataset (for example, `my-dataset`) with a resource (for example, `my-resource` with the `my-res-id-123` as its `resource_id`);\n* We also need to set up two environment variables for Airflow. Access the [Airflow Variable panel](http://localhost:8080/admin/variable/) and set up `CKAN_SITE_URL` and your `CKAN_SYSADMIN_API_KEY`:\n\n![Variables configuration](docs/resources/images/aircan_variables.png)\n\n#### Single-node DAG\n\n`api_ckan_load_single_node` is a single-node DAG which deletes,creates and loads a resource to a local or remote CKAN instance. You can run the `api_ckan_load_single_node` by following these steps:\n\n1. Open your `airflow.cfg` file (usually located at `~/airflow/airflow.cfg`) and point your DAG folder to AirCan:\n\n```bash\ndags_folder = /your/path/to/aircan\n\n...other configs\n\ndag_run_conf_overrides_params = True\n```\n\nNote: do not point `dags_folder` to `/your/path/to/aircan/aircan/dags`. It must be pointing to the outer `aircan` folder.\n\n2. Verify that Airflow finds the DAGs of Aircan by running `airflow list_dags`. The output should list:\n\n```bash\n-------------------------------------------------------------------\nDAGS\n-------------------------------------------------------------------\nckan_api_load_single_step\n...other DAGs...\n```\n\n3. Make sure you have these environment variables properly set up:\n\n```bash\nexport LC_ALL=en_US.UTF-8\nexport LANG=en_US.UTF-8\nexport OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES\n```\n\n4. Run the Airflow webserver (in case you have skipped the previous example): `airflow webserver`\n\n5. Run the Airflow scheduler: `airflow scheduler`. Make sure the environment variables from (3) are set up.\n\n6. Access the Airflow UI (`http://localhost:8080/`). You should see the DAG `ckan_api_load_single_step` listed.\n\n7. Activate the DAG by hitting on the **Off** button on the interface.\n\n8. Now we can test the DAG. On your terminal, run:\n\n```bash\nairflow test \\\n-tp \"{ \\\"resource_id\\\": \\\"my-res-id-123\\\", \\\n\\\"schema_fields_array\\\": \\\"[ 'field1', 'field2']\\\", \\\n\\\"csv_input\\\": \\\"/path/to/my.csv\\\", \\\n\\\"json_output\\\": \\\"/path/to/my.json\\\" }\" \\\nckan_api_load_single_step full_load_via_api now\n```\n\nMake sure to replace the parameters accordingly.\n\n* `resource_id` is the id of your resource on CKAN.\n* `schema_fields_array` is the header of your CSV file. Everything is being treated as plain text at this time.\n* `csv_input` is the path to the CSV file you want to upload.\n* The DAG will convert your CSV file to a JSON file and then upload it. `json_output` specifies the path where you want to dump your JSON file.\n\n9. Check your CKAN instance and verify that the data has been loaded.\n\n10. Trigger the DAG with the following:\n\n```bash\nairflow trigger_dag ckan_api_load_single_step \\\n --conf='{ \"resource_id\": \"my-res-id-123\", \"schema_fields_array\": [ \"field1\", \"field2\" ], \"csv_input\": \"/path/to.csv\", \"json_output\": \"/path/to.json\" }'\n```\n\nDo not forget to properly replace the parameters with your data and properly escape the special characters.\nAlternatively, you can just run the DAG with the `airflow run` command.\n\n\n`api_ckan_load_single_node` also works for remote CKAN instances. Just set up your Airflow `CKAN_SITE_URL` variable accordingly.\n\n\n#### Multiple-node DAG\n\n`ckan_api_load_multiple_steps` does the same steps of `api_ckan_load_single_node`, but it uses multiple nodes (tasks). You can repeat the steps of the previous section and run `ckan_api_load_multiple_steps`.\n\n\n\n### [Ignore] Example 3: Local file to CKAN DataStore using Postgres\n\nWe'll load a local csv into CKAN DataStore instance.\n\n#### Preliminaries: Setup your CKAN instance\n\nWe'll assume you have:\n\n* a local CKAN setup and running at http://localhost:5000\n* datastore enabled. If you are using Docker, you might need to expose your Postgres Instance port. For example, add the following in your `docker-composer.yml` file:\n\n```yml\ndb:\n  ports:\n      - \"5432:5432\"\n```\n\n(Useful to know: it is possible to access the Postgres on your Docker container. Run `docker ps` and you should see a container named `docker-ckan_db`, which corresponds to the CKAN database. Run `docker exec -it CONTAINER_ID bash` and then `psql -U ckan` to access the corresponding Postgres instance).\n\n* Now you need to set up some information on Airflow. Access your local Airflow Connections panel at http://localhost:8080/admin/connection/. Create a new connection named `ckan_postgres` with your datastore information. For example, assuming your `CKAN_DATASTORE_WRITE_URL=postgresql://ckan:ckan@db/datastore`, use the following schema:\n\n![Connection configuration](docs/resources/images/aircan_connection.png)\n\n* We also need to set up two environment variables for Airflow. Access the Airflow Variable panel and set up `CKAN_SITE_URL` and your `CKAN_SYSADMIN_API_KEY`:\n\n![Variables configuration](docs/resources/images/aircan_variables.png)\n\n[TODO PARAMETERIZE VARS]\n[TODO PARAMETERIZE PATHS]\n\nThen, create a dataset called `aircan-example` using this script:\n\n```bash\ncd aircan\npip install -r requirements-example.txt\npython examples/setup-ckan.py --api-key\n```\n\n#### Doing the load\n\nWe assume you now have a dataset named `my-first-dataset`.\n\nCreate the DAG for loading\n\n```bash\ncp aircan/lib/api_ckan_load.py ~/airflow/dags/\n```\n\nCheck if Airflow recognize your DAG with `airflow list_dags`. You should see a DAG named `ckan_load`.\n\nNow you can test each task individually:\n\n* To delete a datastore, run `airflow test ckan_load delete_datastore_table now`\n* To create a datastore, run `airflow test ckan_load create_datastore_table now`. You can see the corresponding `resource_id` for the datastore on your logs. [TODO JSON is hardcoded now; parameterize on kwargs or some other operator structure]\n* To load a CSV to Postgres, run `airflow test ckan_load load_csv_to_postgres_via_copy now`. [TODO JSON is hardcoded now; insert resource_id on it. File path is also Hardcode, change it]\n* Finally, set your datastore to active: `airflow test ckan_load restore_indexes_and_set_datastore_active now`.\n\nTo run the entire DAG:\n\n* Select the DAG [screenshot]\n* Configure it with a path to ../your/aircan/examples/example1.csv\n* Run it ... [screenshot]\n\nCheck the output\n\n* Visit http://localhost:5000/dataset/aircan-example/ and see the resource named XXX. It will have data in its datastore now!\n\n### Example 2a: Remote file to DataStore\n\nSame as example 2 but use this DAG instead:\n\n```bash\ncp aircan/examples/ckan-datastore-from-remote.py ~/airflow/dags/\n```\n\nPlus set a remote URL for loading.\n\n### Examples 3: Auto Load file uploaded to CKAN into CKAN DataStore\n\nConfigure CKAN to automatically load.\n\n* Setup CKAN - see the previous sections.\n* Also install this extension in your ckan instance: `ckanext-aircan-connector`. **TODO**: add instructions.\n* Configure ckan with location of your airflow instance and the DAG id (`aircan-load-csv`).\n\n#### Run it\n\nRun this script which uploads a CSV file to your ckan instance and will trigger a load to the datastore.\n\n```bash\ncd aircan\npip install -r requirements-example.txt\npython examples/ckan-upload-csv.py\n```\n\n## \n\n### Using Google Cloud Composer\n\n1. Sign up for an account at https://cloud.google.com/composer. Create or select an existing project at Google Cloud Platform. For this example, we use one called `aircan-test-project`.\n2. Create an environment at Google Cloud Composer, either by command line or by UI. Make sure you select **Python 3** when creating the project. Here, we create an environment named `aircan-airflow`.\n![Google Cloud Composer environment configuration](docs/resources/images/composer.png)\n\n\n\nAfter creating your environment, it should appear in your environment list:\n![Google Cloud Composer environment configuration](docs/resources/images/composer2.png)\n\n\n\n3. Override the configuration for `dag_run_conf_overrides_params`:\n![Google Cloud Composer environment configuration](docs/resources/images/composer_airflow_config.png)\n\n\n4. Access the designated DAGs folder (which will be a bucket). Upload the contents of `local/path/to/aircan/aircan` to the bucket:\n![Google Cloud Composer DAGs folder configuration](docs/resources/images/bucket1.png)\n\nThe contents of the subfolder `aircan` must be:\n![Google Cloud Composer DAGs folder configuration](docs/resources/images/bucket2.png)\n\n5. Enter the subdirectory `dags` and delete the `__init__.py` file on this folder. It conflicts with Google Cloud Composer configurations.\n\n6. Similarly to what we did on Example 2, access your Airflow instance (created by Google Cloud Composer) and add `CKAN_SITE_URL` and `CKAN_SYSADMIN_API_KEY` as Variables. Now the DAGs must appear on the UI interface.\n\n7. Let's assume you have a resource on `https://demo.ckan.org/` with `my-res-id-123` as its resource_id. We also assume you have, in the root of your DAG bucket on Google Cloud platform, two files: One CSV file with the resource you want to upload, named `r3.csv`, with two columns, `field1` and `field2`. The other file you must have in the root of your your bucket is `r4.json`, an empty JSON file.\n![Google Cloud Composer DAGs folder configuration](docs/resources/images/setup1.png)\n\nSince our DAGs expect parameters, you'll have to trigger them via CLI:\nFor example, to trigger `api_ckan_load_single_node`, run (from your terminal):\n```bash\ngcloud composer environments run aircan-airflow \\\n     --location us-east1 \\\n     trigger_dag -- ckan_api_load_single_step \\\n      --conf='{ \"resource_id\": \"my-res-id-123\", \"schema_fields_array\": [ \"field1\", \"field2\" ], \"csv_input\": \"/home/airflow/gcs/dags/r3.csv\", \"json_output\": \"/home/airflow/gcs/dags/r4.json\" }'\n```\n\nCheck the logs (tip: filter them by your DAG ID, for example, `ckan_api_load_single_step`). It should updload the data of your `.csv` file to `demo.ckan` successfully.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "aircan,airflow,aircan-lib",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "aircan",
    "package_url": "https://pypi.org/project/aircan/",
    "platform": "",
    "project_url": "https://pypi.org/project/aircan/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/aircan/0.0.4/",
    "requires_dist": [
      "apache-airflow",
      "psycopg2",
      "python-dotenv",
      "requests",
      "sqlalchemy"
    ],
    "requires_python": "<=3.7.8",
    "summary": "",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7705157,
  "releases": {
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "43b7fa4a7867ea85e9fa3a1a1950e1452b9986243119b8cabea52ba57c6b5394",
          "md5": "3589af0c0f798443be05c9e0f7d74773",
          "sha256": "7b746e62aabd0cbd9788151a0de81df35df189248e0c580ac94e1c647bc69d90"
        },
        "downloads": -1,
        "filename": "aircan-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3589af0c0f798443be05c9e0f7d74773",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<=3.7.8",
        "size": 13790,
        "upload_time": "2020-07-01T11:56:42",
        "upload_time_iso_8601": "2020-07-01T11:56:42.897949Z",
        "url": "https://files.pythonhosted.org/packages/43/b7/fa4a7867ea85e9fa3a1a1950e1452b9986243119b8cabea52ba57c6b5394/aircan-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "172e98579f12bbffa4f660e3305cd433f57757ea5e54e97bdc64d6f1d8bbb81f",
          "md5": "729448b3c46d97eb1813d5f212f17583",
          "sha256": "3db5bd12bb1ac0b7ff6c9f3f3c0243470138f3b5e1fd1079e772bcad829108b3"
        },
        "downloads": -1,
        "filename": "aircan-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "729448b3c46d97eb1813d5f212f17583",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "<=3.7.8",
        "size": 15818,
        "upload_time": "2020-07-01T11:56:45",
        "upload_time_iso_8601": "2020-07-01T11:56:45.274019Z",
        "url": "https://files.pythonhosted.org/packages/17/2e/98579f12bbffa4f660e3305cd433f57757ea5e54e97bdc64d6f1d8bbb81f/aircan-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "43b7fa4a7867ea85e9fa3a1a1950e1452b9986243119b8cabea52ba57c6b5394",
        "md5": "3589af0c0f798443be05c9e0f7d74773",
        "sha256": "7b746e62aabd0cbd9788151a0de81df35df189248e0c580ac94e1c647bc69d90"
      },
      "downloads": -1,
      "filename": "aircan-0.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3589af0c0f798443be05c9e0f7d74773",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "<=3.7.8",
      "size": 13790,
      "upload_time": "2020-07-01T11:56:42",
      "upload_time_iso_8601": "2020-07-01T11:56:42.897949Z",
      "url": "https://files.pythonhosted.org/packages/43/b7/fa4a7867ea85e9fa3a1a1950e1452b9986243119b8cabea52ba57c6b5394/aircan-0.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "172e98579f12bbffa4f660e3305cd433f57757ea5e54e97bdc64d6f1d8bbb81f",
        "md5": "729448b3c46d97eb1813d5f212f17583",
        "sha256": "3db5bd12bb1ac0b7ff6c9f3f3c0243470138f3b5e1fd1079e772bcad829108b3"
      },
      "downloads": -1,
      "filename": "aircan-0.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "729448b3c46d97eb1813d5f212f17583",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "<=3.7.8",
      "size": 15818,
      "upload_time": "2020-07-01T11:56:45",
      "upload_time_iso_8601": "2020-07-01T11:56:45.274019Z",
      "url": "https://files.pythonhosted.org/packages/17/2e/98579f12bbffa4f660e3305cd433f57757ea5e54e97bdc64d6f1d8bbb81f/aircan-0.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}