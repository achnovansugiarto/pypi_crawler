{
  "info": {
    "author": "Alex Broley",
    "author_email": "alex.broley@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# data-raven\n\n## Description\nA toolbox of flexible database connectors and test methods used to measure data integrity of datasets and database \ntables.\n* Build data quality tests which can be inserted into an existing Python script or run as a stand-alone script. \n* Send outcome notifications to messaging and logging applications. \n* Halt pipelines and raise exceptions when needed.\n\n## Prerequisites\nPython 3.6+\n\nsqlalchemy>=1.3.19\n\npsycopg2\n\npymysql\n\n## Installing\n`pip install data-raven`\n\n## A simple data quality test script\n\nIn this example we build a script to test the `name`, `price` and `product_id` columns from the Postgres table `Orders`.\nThis table has the following DDL:\n```buildoutcfg\ncreate table Orders (\nid int,\nname varchar(50),\norder_ts varchar(26),\nproduct_id int,\nprice float\n);\n```\n\nHere's the test script.\n```buildoutcfg\nimport os\n\nfrom dataraven.connections import PostgresConnector\nfrom dataraven.data_quality_operators import SQLNullCheckOperator\n\n\ndef main():\n    # initialize logging\n    lazy_logger = lambda msg: print(msg + '\\n')\n\n    # database connection credentials\n    user = os.environ[\"user\"]\n    password = os.environ[\"password\"]\n    host = os.environ[\"host\"]\n    dbname = os.environ[\"dbname\"]\n    port = os.environ[\"port\"]\n\n    # postgres database connector\n    conn = PostgresConnector(user, password, host, dbname, port, logger=lazy_logger)\n    dialect = \"postgres\"\n\n    # test thresholds\n    threshold0 = 0\n    threshold1 = 0.01\n    threshold5 = 0.05\n\n    ##### TEST ORDERS TABLE #####\n    # Table to be tested\n    from_clause = \"test_schema.Orders\"\n\n    # Conditional logic to be applied to input data\n    date = \"2020-09-08\"\n    where_clause = [f\"date(order_ts) = '{date}'\"]\n\n    # Columns to be tested in target table\n    columns = (\"name\", \"product_id\", \"price\")\n\n    # Threshold value to be applied to each column\n    threhold = {\"name\": threshold1, \"product_id\": threshold0, \"price\": threshold5}\n\n    # Hard fail condition set on specific columns\n    hard_fail = {\"product_id\": True}\n\n    # Execute the null check test on each column in columns, on the above table\n    SQLNullCheckOperator(conn, dialect, from_clause, threhold, *columns, where=where_clause, logger=lazy_logger,\n                         hard_fail=hard_fail)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n# Documentation\n## Database Support\n* Postgres\n* MySQL\n\n## Data Quality Tests\nData quality tests are used to measure the integrity of specified columns within a table or document. Every data \nquality test will return `'test_pass'` or `'test_fail'` depending on the given measure and threshold.\n\n### Data Quality Operators\nEach operator will log the test results using the function passed in the `logger` parameter. If no logger is found then\nthese log messages will be swallowed. \n\nEach operator has a `test_results` attribute which exposes the results from the underlying test. `test_results` is a \n`dict` object with the following structure:\n```buildoutcfg\n{\n    COLUMN NAME: {\n        \"result\": 'test_pass' or 'test_fail',\n        \"measure\": THE MEASURED VALUE OF COLUMN NAME,\n        \"threshold\": THE THRESHOLD VALUE SPECIFIED FOR TEST,\n        \"result_msg\": TEST RESULT MESSAGE\n    }\n}   \n```\n\n#### SQL Operators\nAll SQL operators have the following required parameters:\n* `conn` - The database connection object.\n* `dialect` - The SQL dialect for the given database. Accepted values are `postgres` or `mysql`.\n* `from_` - The schema and table name of table to be tested.\n* `threshold` - The threshold specified for a given test or collection of tests. This parameter can be numeric or a \n`dict` object. If `threshold` is numeric then this value will be applied to all columns being tested by the operator. \nIf `threshold` is a `dict` then each `threshold` value will be referenced by column name. All columns being passed to the\noperator must have a specified threshold value. If `threshold` is a `dict` it must have the following structure:\n```buildoutcfg\n{\n    COLUMN NAME: NUMERIC VALUE\n}\n```\n* `columns` - The column names entered as comma separated positional arguments.\n\nAll SQL operators have the following optional parameters:\n* `logger` - The logging function. If None is passed then logged messages will be swallowed.\n* `where` - Conditional logic to be applied to table specified in `from_`.\n* `hard_fail` - Specifies if an operator which has a test which results in `'test_fail'` should terminate the current \nprocess. This parameter\ncan be passed as a literal or a `dict` object. If `hard_fail` is set to `True` then every test being performed by the \ngiven operator which results in `'test_fail'` will terminate the current process. If `hard_fail` is a `dict` object then\neach `hard_fail` value will be referenced by column name. Only those columns with a `hard_fail` value of `True` will \nterminate the process upon test failure. If `hard_fail` is a `dict` it must have the following structure:\n```buildoutcfg\n{\n    COLUMN NAME: BOOLEAN VALUE\n}\n```\n* `use_ansi` - If true then compile measure query to ANSI standards.\n\n`SQLNullCheckOperator` - Test the proportion of null values for each column contained in `columns`. \n\n`SQLDuplicateCheckOperator` - Test the proportion of duplicate values for each column contained in `columns`.\n\n`SQLSetDuplicateCheckOperator` - Test the number of duplicate values across all columns passed to the `columns`\nparameter simultaniously. This measure is equivalent to counting the number of rows returned from a `SELECT DISTINCT` on \nall columns and dividing by the total number of rows.\n\n#### CSV Operators\nAll CSV operators have the following required parameters:\n* `from_` - The path to CSV file to be tested.\n* `threshold` - Same as defined above for SQL operators.\n* `columns` - the column names entered as comma separated positional arguments.\n\nAll CSV operators have the following optional parameters:\n* `delimiter` -  The delimiter used to separate values specified in the file refeneced by the `from_` parameter. \n* `hard_fail` - Same as defined above for SQL operators.\n* `fieldnames` - A sequence of all column names for CSV file specified in `from_` parameter. To be used if the specified\nfile does not have column headers.\n* `reducer_kwargs` - Key word arguments passed to the measure reducer function.\n\n`CSVNullCheckOperator` - Test the proportion of `NULL` values for each column contained in `columns`.\n\n`CSVDuplicateCheckOperator` - Test the proportion of duplicate values for each column contained in `columns`.\n\n`CSVSetDuplicateCheckOperator` - Test the number of duplicate values across all columns passed to the `columns`\nparameter simultaniously.\n\n#### Custom Operators\n`CustomSQLDQOperator` - Executes the test passed by the `custom_test` parameter on each column contained in `columns`. \nThe `CustomSQLDQOperator` class has the following required parameters:\n* `conn` - The database connection object.\n* `custom_test` - The SQL query to be executed. The `custom_test` query is required to return a column labeled `result` \nwhich takes value `'test_pass'` or `'test_fail'`. The `custom_test` query should also return columns `measure`, which\nprovides the measured column value, and `threshold`, which gives the threshold used in the test. If these columns are\npresent then these values will be logged and returned in the `test_results` attribute. If `measure` and `threshold` are\nnot returned by the `custom_test` query then these values will be logged as `None`, and will be given in the \n`test_results` attribute as `None`. `custom_test` can also be a query template with placeholders `{column}` and \n`{threshold}` for variable column names and threshold values.  \n* `description` - The description of the data quality test being performed. The description is may contain \nplaceholders `{column}` and `{threshold}` for the optional parameters `columns` and `threshold`, if they are passed\nto the `CustomSQLDQOperator`. In this case then a test description will be generated for each `column` in `columns` and\nfor each value of `threshold`.\n\nThe `CustomSQLDQOperator` class has the following optional parameters:\n* `columns` - a comma separated list of column arguments.\n* `threhsold` - Same as defined above for SQL operators.\n* `hard_fail` - Same as defined above for SQL operators.\n* `test_desc_kwargs` - Key word arguments for formatting the test description.\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/tekkabroley/data-raven",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "data-raven",
    "package_url": "https://pypi.org/project/data-raven/",
    "platform": "",
    "project_url": "https://pypi.org/project/data-raven/",
    "project_urls": {
      "Homepage": "https://github.com/tekkabroley/data-raven"
    },
    "release_url": "https://pypi.org/project/data-raven/1.0.0/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A Python framework for building data quality tests",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8441458,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "95ac6273b5aef21e83133013cb72ca1f69fc1ec247ffdc79c3aaf761e5f0bba9",
          "md5": "ebfbe275433a0861d9fe584c7ff300cb",
          "sha256": "63ef7a184f868ca09428c17cb139871371a2cb45d766d2065e95404a9a00f87a"
        },
        "downloads": -1,
        "filename": "data_raven-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ebfbe275433a0861d9fe584c7ff300cb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 18097,
        "upload_time": "2020-10-18T20:50:06",
        "upload_time_iso_8601": "2020-10-18T20:50:06.317278Z",
        "url": "https://files.pythonhosted.org/packages/95/ac/6273b5aef21e83133013cb72ca1f69fc1ec247ffdc79c3aaf761e5f0bba9/data_raven-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f15ee04705175b79285edff831b1929f63235ace2b3966a89588ad47d4e0d9ee",
          "md5": "7425b5360bd893349efe0609231da7fd",
          "sha256": "e7a3a07e5610723291d748db1d407cc688024641c8363618a0b69d95d9ecdc9e"
        },
        "downloads": -1,
        "filename": "data-raven-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7425b5360bd893349efe0609231da7fd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 13522,
        "upload_time": "2020-10-18T20:50:08",
        "upload_time_iso_8601": "2020-10-18T20:50:08.574369Z",
        "url": "https://files.pythonhosted.org/packages/f1/5e/e04705175b79285edff831b1929f63235ace2b3966a89588ad47d4e0d9ee/data-raven-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "95ac6273b5aef21e83133013cb72ca1f69fc1ec247ffdc79c3aaf761e5f0bba9",
        "md5": "ebfbe275433a0861d9fe584c7ff300cb",
        "sha256": "63ef7a184f868ca09428c17cb139871371a2cb45d766d2065e95404a9a00f87a"
      },
      "downloads": -1,
      "filename": "data_raven-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ebfbe275433a0861d9fe584c7ff300cb",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 18097,
      "upload_time": "2020-10-18T20:50:06",
      "upload_time_iso_8601": "2020-10-18T20:50:06.317278Z",
      "url": "https://files.pythonhosted.org/packages/95/ac/6273b5aef21e83133013cb72ca1f69fc1ec247ffdc79c3aaf761e5f0bba9/data_raven-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f15ee04705175b79285edff831b1929f63235ace2b3966a89588ad47d4e0d9ee",
        "md5": "7425b5360bd893349efe0609231da7fd",
        "sha256": "e7a3a07e5610723291d748db1d407cc688024641c8363618a0b69d95d9ecdc9e"
      },
      "downloads": -1,
      "filename": "data-raven-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "7425b5360bd893349efe0609231da7fd",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 13522,
      "upload_time": "2020-10-18T20:50:08",
      "upload_time_iso_8601": "2020-10-18T20:50:08.574369Z",
      "url": "https://files.pythonhosted.org/packages/f1/5e/e04705175b79285edff831b1929f63235ace2b3966a89588ad47d4e0d9ee/data-raven-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}