{
  "info": {
    "author": "Damavis",
    "author_email": "info@damavis.com",
    "bugtrack_url": null,
    "classifiers": [
      "Environment :: Plugins",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: Unix",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "[![Build Status](https://api.travis-ci.com/damavis/airflow-hop-plugin.svg?branch=main)](https://app.travis-ci.com/damavis/airflow-hop-plugin)\n[![codecov](https://codecov.io/gh/damavis/airflow-hop-plugin/branch/main/graph/badge.svg?token=IRE2T3NEOD)](https://codecov.io/gh/damavis/airflow-hop-plugin)\n[![PyPI](https://img.shields.io/pypi/v/airflow-hop-plugin)](https://pypi.org/project/airflow-hop-plugin/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/airflow-hop-plugin)](https://pypi.org/project/airflow-hop-plugin/)\n\n# Hop Airflow plugin\n\nThis is an Apache Hop plugin for Apache Airflow in order to orchestrate Apache Hop pipelines and workflows from Airflow.\n\n## Requirements\n\nBefore setting up the plugin you must have completely set up your Hop environment:\n\n- Configured Hop Server\n- Configured remote pipeline configuration\n- Configured remote workflow configuration\n\nTo do so, go to the metadata window in Hop UI by pressing Ctrl+Shift+M or click the ![icono](images/Metadata_icon.png) icon.\n\n![Hop Server](images/Hop_server.png)\n\nDouble click `Hop server` to create a new configuration.\n\n![Remote pipeline Config](images/Pipeline_Run_config.png)\n\nDouble click `Pipeline Run Configuration` to create a new configuration.\n\n![Remote workflow config](images/Workflow_Run_config.png)\n\nDouble click `Workflow Run Configuration` to create a new configuration.\n\n## Set up guide\n\nThe following content will be a \"how to set up the plugin\" guide plus some requirements and\nrestraints when it comes to its usage.\n\n### 1. Generate metadata.json\n\nFor the correct configuration of this plugin a file containing all Hop's metadata must be created\ninside each project directory. This can be done by exporting it from Hop UI.\n\nPlease note that this process must be repeated each time the metadata of a project is modified.\n\n![Metadata option](images/Export_metadata.png)\n\n### 2. Install the plugin\n\nThe first step in order to get this plugin working is to install the package using the following\ncommand:\n\n```\npip install airflow-hop-plugin-custom\n```\n\n### 3. Hop Directory Structure\n\nDue to some technical limitations it's really important for the Hop home directory to have the\nfollowing structure.\n\n```\nhop # This is the hop home directory\n├── ...\n├── config\n│   ├── hop-config.json\n│   ├── example_environment.json # This is where you should save your environment files\n│   ├── metadata\n│   │   └── ...\n│   └── projects\n│       ├── ...\n│       └── example_project # This is how your project's directory should look\n│           ├── metadata.json\n│           ├── metadata\n│           │   └── ...\n│           ├── example_directory\n│           │   └── example_workflow.hwl\n│           └── example_pipeline.hpl\n├── ...\n```\n\nMoreover, please remember to save all projects inside the \"projects\" directory and set a path\nrelative to the hop home directory when configuring them like shown in the following picture:\n\n![Here goes an image](images/project_properties.png)\n\n### 4. Create an Airflow Connection\nTo correctly use the operators you must create a new\n[Airflow connection](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html).\nThere are multiple ways to do so and whichever you want can be used, but it should have these\nvalues for the following attributes:\n\n- Connection ID: 'hop_default'\n- Connection Type: 'http'\n- Login: apache_hop_username\n- Password: apache_hop_password\n- Host: apache_hop_server\n- Port: apache_hop_port\n- Extra: \"hop_home\": \"/path/to/hop-home/\"\n\n Example of a new Airflow connection using Airflow's CLI:\n\n```\nairflow connections add 'hop_default' \\\n    --conn-json '{\n        \"conn_type\": \"http\",\n        \"login\": \"cluster\",\n        \"password\": \"cluster\",\n        \"host\": \"0.0.0.0\",\n        \"port\": 8080,\n        \"schema\": \"\",\n        \"extra\": {\n            \"hop_home\": \"/home/user/hop\"\n        }\n    }'\n```\n\n### 5. Creating a DAG\n\nHere's an example of a DAG:\n\n```python\nfrom airflow_hop.operators import HopPipelineOperator\nfrom airflow_hop.operators import HopWorkflowOperator\n\n# ... #\n\nwith DAG('sample_dag', start_date=datetime(2022,7,26),\n        schedule_interval='@daily', catchup=False) as dag:\n\n    # Define a pipeline\n    first_pipe = HopPipelineOperator(\n        task_id='first_pipe',\n        pipeline='pipelines/first_pipeline.hpl',\n        pipe_config='remote hop server',\n        project_name='default',\n        log_level='Basic')\n\n    # Define a pipeline with parameters\n    second_pipe = HopPipelineOperator(\n        task_id='second_pipe',\n        pipeline='pipelines/second_pipeline.hpl',\n        pipe_config='remote hop server',\n        project_name='default',\n        log_level='Basic',\n        params={'DATE':'{{ ds }}'}) # Date in yyyy-mm-dd format\n\n    # Define a workflow with parameters\n    work_test = HopWorkflowOperator(\n        task_id='work_test',\n        workflow='workflows/workflow_example.hwf',\n        project_name='default',\n        log_level='Basic',\n        params={'DATE':'{{ ds }}'}) # Date in yyyy-mm-dd format\n\n    first_pipe >> second_pipe >> work_test\n```\n\nIt's important to point out that both the workflow and pipeline parameters within their respective\noperators must be a relative path parting from the project's directory.\n\n## Development\n\n### Deploy Apache Hop Server using Docker\n\nRequeriments:\n\n- [docker](https://docs.docker.com/engine/install/)\n- [docker-compose](https://docs.docker.com/compose/install/)\n\nIf you want to use Docker to create the server you can use the following docker-compose\nconfiguration as a template:\n\n```yaml\nservices:\n  apache-hop:\n    image: apache/hop:latest\n    ports:\n      - 8080:8080\n    volumes:\n      - hop_path:/home/hop\n    environment:\n      HOP_SERVER_USER: cluster\n      HOP_SERVER_PASS: cluster\n      HOP_SERVER_PORT: 8080\n      HOP_SERVER_HOSTNAME: 0.0.0.0\n```\n\nOnce done, the Hop server can be started using docker compose.\n\n## License\n\n```\nCopyright 2022 Aneior Studio, SL\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/damavis/airflow-hop-plugin",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "airflow-hop-plugin-custom",
    "package_url": "https://pypi.org/project/airflow-hop-plugin-custom/",
    "platform": null,
    "project_url": "https://pypi.org/project/airflow-hop-plugin-custom/",
    "project_urls": {
      "Homepage": "https://github.com/damavis/airflow-hop-plugin"
    },
    "release_url": "https://pypi.org/project/airflow-hop-plugin-custom/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3",
    "summary": "",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16942316,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "11d295f5cd6ad14db422016036a3e55056220803c62237c9d98d31283335e9bf",
          "md5": "cf60de6f0a4040cceec522095df86a2b",
          "sha256": "33033675cd6e5a6224a3ba6da5e73dcfa4cc75c364a838007ef4cdede9042990"
        },
        "downloads": -1,
        "filename": "airflow-hop-plugin-custom-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "cf60de6f0a4040cceec522095df86a2b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 11358,
        "upload_time": "2023-02-20T11:46:17",
        "upload_time_iso_8601": "2023-02-20T11:46:17.350555Z",
        "url": "https://files.pythonhosted.org/packages/11/d2/95f5cd6ad14db422016036a3e55056220803c62237c9d98d31283335e9bf/airflow-hop-plugin-custom-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "11d295f5cd6ad14db422016036a3e55056220803c62237c9d98d31283335e9bf",
        "md5": "cf60de6f0a4040cceec522095df86a2b",
        "sha256": "33033675cd6e5a6224a3ba6da5e73dcfa4cc75c364a838007ef4cdede9042990"
      },
      "downloads": -1,
      "filename": "airflow-hop-plugin-custom-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "cf60de6f0a4040cceec522095df86a2b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3",
      "size": 11358,
      "upload_time": "2023-02-20T11:46:17",
      "upload_time_iso_8601": "2023-02-20T11:46:17.350555Z",
      "url": "https://files.pythonhosted.org/packages/11/d2/95f5cd6ad14db422016036a3e55056220803c62237c9d98d31283335e9bf/airflow-hop-plugin-custom-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}