{
  "info": {
    "author": "acceldata",
    "author_email": "apisupport@acceldata.io",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Pipeline APIs\n\nAcceldata Torch is a complete solution to observe the quality of the data present in your data lake and warehouse. Using Torch, you can ensure that high-quality data backs your business decisions. Torch provides you with tools to measure the quality of data in a data catalog and to never miss significant data sources. All users including analysts, data scientists, and developers, can rely on Torch to observe the data flowing in the warehouse or data lake and can rest assured that there is no loss of data. \n<br />\nAcceldata SDK is used to trigger torch catalog and pipeline APIs. By creating a Torch client, all the torch apis can be accessed. \n\nInstall `acceldata-sdk` pypi package in a python environment.\n```bash\npip install acceldata-sdk\n```\n\n## Create Torch Client\nTorch client is used to send data to the torch servers. It consists of various methods to communicate with the torch server. Torch client have access to catalog and pipeline APIs. To create a torch client, torch url and API keys are required. To create torch API keys, go to torch ui’s settings and generate keys for the client.\n\nWhile creating a TorchClient connection to torch by default version compatibility checks between torch and sdk is enabled. If we want we can disable that check by passing `do_version_check` as `False.\n\n\n```python\nfrom acceldata_sdk.torch_client import TorchClient\n\ntorch_client = TorchClient(url='https://acceldata.host.dev:9999', access_key='******',\n                         secret_key='*****************', do_version_check=True)\n\n```\n\n## Pipeline API \nThere are various pipeline APIs are supported through Acceldata SDK. Pipeline APIs like create pipeline, add jobs and spans, initiate pipeline run et cetera. Acceldata sdk is able to send various event during span life cycle. Hence, Acceldata sdk has full control over the pipelines.\n\n##### Create Pipeline And Job and span to bound the job\nPipeline represents the ETL pipeline in its entirety and will contain Asset nodes and Jobs associated. The complete pipeline definition forms the Lineage graph for all the data assets.\n</br>\nJob Node or Process Node represents an entity that does some job in the ETL workflow. From this representation, Job’s input is some assets or some other Jobs, and output is few other assets or few other Jobs.\nTorch will use the set of Jobs definition in the workflow to create the Lineage, and also track version changes for the Pipeline.\n\nTo create pipeline and jobs, first create creation object with required parameter. And with use of supported methods by sdk, can do corresponding operation on torch server side.\n\nAcceldata sdk provides CreateJob class which need to be passed to create_job function as a parameter to create a job.\n\nParams for `CreateJob`:\n\n`uid`: uid of the job. It should be unique for the job. It is a mandatory parameter.\n`name`: name of the job. It is a mandatory parameter.\n\n#### NOTE: This changed in 2.4.1 release\n\n`pipeline_run_id`: id of the pipeline_run for which you want to add a job.It is a mandatory parameter if job is being created using pipeline. Its is not needed if job is being created using pipeline_run.\n\n`description`: description of the job\n`inputs`: (list[Node]) input for the job. This can be uid of an asset specified using asset_uid parameter of Node object\n        or it can be uid of another job specified using job_uid parameter of Node object.\n`outputs`: (list[Node]) output for the job.This can be uid of an asset specified using asset_uid parameter of Node object\n        or it can be uid of another job specified using job_uid parameter of Node object.\n`meta`: Metadata of the Job\n`context`: context of the job\n`bounded_by_span`: (Boolean) This has to be set to True if the job has to be bounded with a span. Default value is false. It is an optional parameter\n`span_uid`: (String) This is uid of new span to be created. This is a manadatory parameter if bounded_by_span is set to True\n\n```python\nfrom acceldata_sdk.torch_client import TorchClient\nfrom acceldata_sdk.models.job import CreateJob, JobMetadata, Node\nfrom acceldata_sdk.models.pipeline import CreatePipeline, PipelineMetadata, PipelineRunResult, PipelineRunStatus\n\n# Create pipeline\npipeline = CreatePipeline(\n    uid='monthly_reporting_pipeline',\n    name='Monthly reporting Pipeline',\n    description='Pipeline to create monthly reporting tables',\n    meta=PipelineMetadata('Vaishvik', 'acceldata_sdk_code', '...'),\n    context={'key1': 'value1'}\n)\ntorch_client = TorchClient(url=\"https://torch.acceldata.local\", access_key=\"*******\",\n                          secret_key=\"******************************\",do_version_check=False)\npipeline_response = torch_client.create_pipeline(pipeline=pipeline)\npipeline_run = pipeline_response.create_pipeline_run()\n# Create a job using pipeline object.\n# Passing of pipeline_run_id is mandatory\njob = CreateJob(\n    uid='monthly_sales_aggregate',\n    name='Monthly Sales Aggregate',\n    description='Generates the monthly sales aggregate tables for the complete year',\n    inputs=[Node(asset_uid='datasource-name.database.schema.table_1')],\n    outputs=[Node(job_uid='job2_uid')],\n    meta=JobMetadata('vaishvik', 'backend', 'https://github.com/'),\n    context={'key21': 'value21'},\n    bounded_by_span=True,\n    pipeline_run_id=pipeline_run.id,\n    span_uid=\"test_shubh\"\n)\njob_response = pipeline_response.create_job(job)\n\n# Create a job using pipeline_run object.\n# Passing of pipeline_run_id is not needed\n\njob = CreateJob(\n    uid='monthly_sales_aggregate_r',\n    name='Monthly Sales Aggregate',\n    description='Generates the monthly sales aggregate tables for the complete year',\n    inputs=[Node(asset_uid='datasource-name.database.schema.table_1')],\n    outputs=[Node(job_uid='job2_uid')],\n    meta=JobMetadata('vaishvik', 'backend', 'https://github.com/'),\n    context={'key21': 'value21'}\n)\njob_response_using_run = pipeline_run.create_job(job)\n```\n##### Create Pipeline Run And Generate Spans And Send Span Events\n\nPipeline run indicates the execution of the pipeline. The same pipeline can be executed multiple times and each execution (run) has new snapshot version. Each pipeline run has hierarchical span's group. A Span is a way to group a bunch of metrics, and they are hierarchical. It can be as granular as possible. The APIs will support creating a span object from a pipeline object, and then hierarchical spans are started from parent spans. A Span typically encompasses a process or a task and can be granular. This hierarchical system is powerful enough to model extremely complex pipeline observability flows. Optionally, a span can also be associated with a Job. This way, we can track starting and completion of Job, including the failure tracking. Start and stop are implicitly tracked for a span.\n\nAcceldata sdk also has support for create new pipeline run, add spans in it. During the span life cycle, sdk is able to send some customs and standard span events to collect pipeline run metrics for observability.\n\nParams for create_span function which is available under a pipeline_run\n\n`uid`: uid of the span being created. This should be unique. This is a mandatory parameter.\n`associatedJobUids`: List of job uids with which the span needs to be associated with.\n`context_data`: This is dict of key-value pair providing custom context information related to a span.\n\nParams for create_child_span function which is available under span_context. This is used to create hierarchy of span by creating a span under another span\n\n`uid`: uid of the span being created. This should be unique. This is a mandatory parameter.\n`context_data`: This is dict of key-value pair providing custom context information related to a span.\n`associatedJobUids`: List of job uids with which the span needs to be associated with.\n```python\n\nfrom acceldata_sdk.events.generic_event import GenericEvent\nfrom datetime import datetime\n\n# create a pipeline run of the pipeline\npipeline_run = pipeline_response.create_pipeline_run()\n\n# get root span of a pipeline run\nroot_span = pipeline_run.get_root_span()\n\n# create span in the pipeline run\nspan_context = pipeline_run.create_span(uid='monthly.generate.data.span')\n\n# check current span is root or not\nspan_context.is_root()\n\n# end the span \nspan_context.end()\n\n# check if the current span has children or not\nspan_context.has_children()\n\n# create a child span\nchild_span_context = span_context.create_child_span('monthly.generate.customer.span')\n\n# send custom event\nchild_span_context.send_event(\n    GenericEvent(context_data={'client_time': str(datetime.now()), 'row_count': 100}, \n                 event_uid=\"order.customer.join.result\")\n)\n\n\n# abort span\nchild_span_context.abort()\n\n# failed span\nchild_span_context.failed()\n\n# update a pipeline run of the pipeline\nupdatePipelineRunRes = pipeline_run.update_pipeline_run(context_data={'key1': 'value2', 'name': 'backend'},\n                                                               result=PipelineRunResult.SUCCESS,\n                                                               status=PipelineRunStatus.COMPLETED)\n\n```\n\n##### Get Latest Pipeline Run\nAcceldata sdk can get the latest pipeline run of the pipeline. With use of the latest pipeline run instance, user can continue ETL pipeline and add spans, jobs, events too. Hence, Acceldata sdk has complete access on the torch pipeline service.\nParams for `get_pipeline`:\n\n`pipeline_identity`: String parameter used to filter pipeline. It can be either id or uid of the pipeline.\n\n```python\npipeline = torch_client.get_pipeline('monthly.reporting.pipeline')\npipeline_run = pipeline.get_latest_pipeline_run()\n\n```\n##### Get Pipeline Run with a particular pipeline run id\nAcceldata sdk can get a pipeline run of the pipeline with a particular pipeline run id. With use of the pipeline run \ninstance, user can continue ETL pipeline and add spans, jobs, events too. Hence, Acceldata sdk has complete access on the torch pipeline service.\n\nParams for `get_pipeline_run`:\n\n`pipeline_run_id`: run id of the pipeline run\n`continuation_id`: continuation id of the pipeline run\n`pipeline_id`: id of the pipeline to which the run belongs to\n```python\npipeline_run = torch_client.get_pipeline_run(pipeline_run_id=pipeline_run_id)\npipeline = torch_client.get_pipeline(pipeline_id=pipeline_id)\npipeline_run = torch_client.get_pipeline_run(continuation_id=continuation_id, pipeline_id=pipeline.id)\npipeline_run = pipeline.get_run(continuation_id=continuation_id)\n```\n\n##### Get Pipeline details for a particular pipeline run id\nAcceldata sdk can get Pipeline details for a particular pipeline run.\n\n```python\npipeline_details = pipeline_run.get_details()\n```\n##### Get all spans for a particular pipeline run id\nAcceldata sdk can get all spans for a particular pipeline run id.\n\n```python\npipeline_run_spans = pipeline_run.get_spans()\n```\n##### Get Pipeline Runs for a pipeline\nAcceldata sdk can get all pipeline runs.\nParams for `get_pipeline_runs`:\n\n`pipeline_id`: id of the pipeline\n```python\nruns = torch_client.get_pipeline_runs(pipeline_id)\nruns = pipeline.get_runs()\n```\n\n##### Get all Pipelines\nAcceldata sdk can get all pipelines.\n\n```python\npipelines = torch_client.get_pipelines()\n```\n\n##### Delete a Pipeline\nAcceldata sdk can delete a pipeline.\n```python\ndelete_response = pipeline.delete()\n```\n\n##### Execute policy synchronously and asynchronously\nAcceldata sdk provides utility function `execute_policy` to execute policies synchronously and asynchronously. This will return an object on which `get_result` and `get_status` can be called to get result and status of the execution respectively.\n\nParams for `execute_policy`:\n\n`sync`: Boolean parameter used to decide if the policy should be executed synchronously or asynchronously. It is a mandatory parameter. If its is set to  `True` it will return only after the execution ends. If it is set to `False` it will return immediately after starting the execution.\n\n`policy_type`: Enum parameter used to specify the policy type. It is a mandatory parameter. It is a enum which will take values from constants as PolicyType.DATA_QUALITY or PolicyType.RECONCILIATION.\n\n`policy_id`: String parameter used to specify the policy id to be executed. It is a mandatory parameter. \n\n`incremental`: Boolean parameter used to specify if the policy execution should be incremental or full. Default value is False.\n\n`pipeline_run_id`: Long parameter used to specify Run id of the pipeline run where the policy is being executed. This can\nbe used to link the policy execution with a particular pipeline run.\n\n`failure_strategy`: Enum parameter used to decide the behaviour in case of failure. Default value is DoNotFail.\n\n* `failure_strategy` takes enum of type `FailureStrategy` which can have 3 values DoNotFail, FailOnError and FailOnWarning.\n\n* DoNotFail will never throw. In case of failure it will log the error.\n* FailOnError will Throw exception only if it's an error. In case of warning it return without any errors.\n* FailOnWarning will Throw exception on warning as well as error.\n\nTo get the execution result we can call `get_policy_execution_result` on torch_client or call `get_result` on execution object which will return a result object.\n\nParams for `get_policy_execution_result`:\n\n`policy_type`: Enum parameter used to specify the policy type. It is a mandatory parameter. It is a enum which will take values from constants as PolicyType.DATA_QUALITY or PolicyType.RECONCILIATION.\n\n`execution_id`: String parameter used to specify the execution id to be queried for rsult. It is a mandatory parameter. \n\n`failure_strategy`: Enum parameter used to decide the behaviour in case of failure. Default value is DoNotFail.\n\nParams for `get_result`:\n\n`failure_strategy`: Enum parameter used to decide the behaviour in case of failure. Default value is DoNotFail.\n\nTo get the current status we can call `get_policy_status` on torch_client or call `get_status` on execution object which will get the current `resultStatus` of the execution. \n\nparams for `get_policy_status` :\n`policy_type`: Enum parameter used to specify the policy type. It is a mandatory parameter. It is a enum which will take values from constants as PolicyType.DATA_QUALITY or PolicyType.RECONCILIATION.\n\n`execution_id`: String parameter used to specify the execution id to be queried for rsult. It is a mandatory parameter. \n\n`get_status` does not take any parameter.\n\nAsynchronous execution example\n```python\nfrom acceldata_sdk.torch_client import TorchClient\nimport acceldata_sdk.constants as const\ntorch_credentials = {\n    'url':  'https://torch.acceldata.local:5443/torch',\n    'access_key':'PJSAJALFHSHU',\n    'secret_key': 'E6LLJHKGSHJJTRHGK540E5',\n    'do_version_check': 'True'\n}\ntorch_client = TorchClient(**torch_credentials)\nasync_executor = torch_client.execute_policy(const.PolicyType.DATA_QUALITY, 46, sync=False, failure_strategy=const.FailureStrategy.DoNotFail, pipeline_run_id=None)\n# Wait for execution to get final result\nexecution_result = async_executor.get_result(failure_strategy=const.FailureStrategy.DoNotFail)\n# Get the current status\nexecution_status = async_executor.get_status()\n```\n\nSynchronous execution example.\n```python\nfrom acceldata_sdk.torch_client import TorchClient\nimport acceldata_sdk.constants as const\ntorch_credentials = {\n    'url':  'https://torch.acceldata.local:5443/torch',\n    'access_key':'PJSAJALFHSHU',\n    'secret_key': 'E6LLJHKGSHJJTRHGK540E5',\n    'do_version_check': 'True'\n}\ntorch_client = TorchClient(**torch_credentials)\n# This will wait for execution to get final result\nsync_executor = torch_client.execute_policy(const.PolicyType.DATA_QUALITY, 46, sync=True, failure_strategy=const.FailureStrategy.DoNotFail, pipeline_run_id=None)\n# Wait for execution to get final result\nexecution_result = sync_executor.get_result(failure_strategy=const.FailureStrategy.DoNotFail)\n# Get the current status\nexecution_status = sync_executor.get_status()\n```\n\nCancel execution example.\n```python\nexecution_result = sync_executor.cancel()\n```\n\n\nExample of continuing the same pipeline run across multiple ETL scripts using continuation_id\n\nETL1 - Here a new pipeline_run is created using a continuation_id but pipeline_run is not closed\n```python\nfrom acceldata_sdk.torch_client import TorchClient\nfrom acceldata_sdk.models.pipeline import CreatePipeline, PipelineMetadata, PipelineRunResult, PipelineRunStatus\n\n# Create pipeline\npipeline_uid = 'monthly_reporting_pipeline'\npipeline = CreatePipeline(\n    uid=pipeline_uid,\n    name='Monthly reporting Pipeline',\n    description='Pipeline to create monthly reporting tables',\n    meta=PipelineMetadata('Vaishvik', 'acceldata_sdk_code', '...'),\n    context={'key1': 'value1'}\n)\ntorch_client = TorchClient(url=\"https://torch.acceldata.local\", access_key=\"*******\",\n                          secret_key=\"******************************\",do_version_check=False)\npipeline_response = torch_client.create_pipeline(pipeline=pipeline)\n\n# A new continuation id should be generated on every run. Same continuation id cannot be reused.\ncont_id = \"continuationid_demo_1\"\npipeline_run = pipeline_response.create_pipeline_run(continuation_id=cont_id)\n\n# Make sure pipeline_run is not ended using the update_pipeline_run call so that same run can be used in next ETL script\n```\n\n\nETL2 - This script will continue the same pipeline run from ETL1\n```python\nfrom acceldata_sdk.torch_client import TorchClient\nfrom acceldata_sdk.models.pipeline import PipelineRunResult, PipelineRunStatus\n\ntorch_client = TorchClient(url=\"https://torch.acceldata.local\", access_key=\"*******\",\n                          secret_key=\"******************************\",do_version_check=False)\n\npipeline_uid = 'monthly_reporting_pipeline'\n# First get the same pipeline using the previously used UID. Then we will get the previously started pipeline_run using the continuation_id\npipeline = torch_client.get_pipeline(pipeline_uid)\n\n# continuation_id should be a same ID used in ETL1 script so that same pipeline_run is continued in the pipeline.\ncont_id = \"continuationid_demo_1\"\npipeline_run = pipeline.get_run(continuation_id=cont_id)\n# Use this pipeline run to create span and jobs\n# At the end of this script close the pipeline run using update_pipeline_run if we do not want to continue the same pipeline_run further\nupdatePipelineRunRes = pipeline_run.update_pipeline_run(context_data={'key1': 'value2', 'name': 'backend'},\n                                                               result=PipelineRunResult.SUCCESS,\n                                                               status=PipelineRunStatus.COMPLETED)\n\n```\n\n\n\n# Datasource APIs\n\nAcceldata SDK has full access on catalog APIs as well. \n\n##### Datasource API\nTorch has support for more 15+ datasource crawling support. \n\n```python\n# Get datasource\nds_res = torch_client.get_datasource('snowflake_ds_local')\nds_res = torch_client.get_datasource(5, properties=True)\n\n# Get datasources based on type\ndatasources = torch_client.get_datasources(const.AssetSourceType.SNOWFLAKE)\n\n```\n\n\n##### Assets APIs\nAcceldata sdk has methods to get assets in the given datasource.\n```python\nfrom acceldata_sdk.models.create_asset import AssetMetadata\n\n# Get asset by id/uid\nasset = torchclient.get_asset(1)\nasset = torch_client.get_asset('Feature_bag_datasource.feature_1')\n```\n##### Asset's tags, labels, metadata and sample data\nUser can add tags, labels custom metadata and also get sample data of the asset using sdk.\nTags and labels can be used to filter out asset easily.\n\n```python\n# asset metadata\nfrom acceldata_sdk.models.tags import AssetLabel, CustomAssetMetadata\nasset = torch_client.get_asset(asset_id)\n\n# Get metadata of an asset\nasset.get_metadata()\n\n# Get all tags\ntags = asset.get_tags()\n\n# Add tag asset\ntag_add = asset.add_tag(tag='asset_tag')\n\n# Add asset labels\nlabels = asset.add_labels(labels=[AssetLabel('test1', 'demo1'), AssetLabel('test2', 'demo2')])\n\n# Get asset labels\nlabels = asset.get_labels()\n\n# Add custom metadata\nasset.add_custom_metadata(custom_metadata=[CustomAssetMetadata('testcm1', 'democm1'), CustomAssetMetadata('testcm2', 'democm2')])\n```\n\n##### Crawler Operations\nUser can start crawler as well as check for running crawler status.\n```python\n# Start a crawler\ndatasource.start_crawler()\ntorch_client.start_crawler('datasource_name')\n\n# Get running crawler status\ndatasource.get_crawler_status()\ntorch_client.get_crawler_status('datasource_name')\n\n```\n\n##### Trigger policies, Profiling and sampling of an asset\nCrawled assets can be profiled and sampled with use of spark jobs running on the livy. \nFurthermore, Created policies (Recon + DQ) can be triggered too.\n\n```python\nimport acceldata_sdk.constants as const\n\n# profile an asset, get profile req details, cancel profile\nprofile_res = asset.start_profile(profiling_type=ProfilingType.FULL)\n\nprofile_req_details = profile_res.get_status()\n\ncancel_profile_res = profile_res.cancel()\n\nprofile_res = asset.get_latest_profile_status()\n\nprofile_req_details_by_req_id = torch_client.get_profile_status(asset_id=profile_req_details.assetId,\n                                                                req_id=profile_req_details.id)\n\n# sample data\nsample_data = asset.sample_data()\n\n# Rule execution and status\n# Execute policy\nexecute_dq_rule = torch_client.execute_policy(const.PolicyType.DATA_QUALITY, 1114, incremental=False)\nfailure_strategy = const.FailureStrategy.DoNotFail\n# Get policy execution result\nresult = torch_client.get_policy_execution_result(\n    policy_type=const.PolicyType.DATA_QUALITY,\n    execution_id=execute_dq_rule.id,\n    failure_strategy=failure_strategy\n)\n\n# Get policy and execute\nfrom acceldata_sdk.models.ruleExecutionResult import RuleType, PolicyFilter\n\nrule = torch_client.get_policy(const.PolicyType.RECONCILIATION, \"auth001_reconciliation\")\n\n# Execute policy\nasync_execution = rule.execute(sync=False)\n# Get execution result\nasync_execution_result = async_execution.get_result()\n# Get current execution status\nasync_execution_status = async_execution.get_status()\n# Cancel policy execution job\ncancel_rule = async_execution.cancel()\n\n# List all executions\n# List executions by id\ndq_rule_executions = torch_client.policy_executions(1114, RuleType.DATA_QUALITY)\n# List executions by name\ndq_rule_executions = torch_client.policy_executions('dq-scala', RuleType.DATA_QUALITY)\n\n# List executions by rule\nrecon_rule_executions = rule.get_executions()\nfilter = PolicyFilter(policyType=RuleType.RECONCILIATION, enable=True)\n# List all rules\nrecon_rules = torch_client.list_all_policies(filter=filter)\n```\n\nVersion Log\n==========\n\n0.0.1 (12/09/2022)\n-------------------\n- Acceldata python sdk\n- Support for flow APIs and catalog APIs of the torch\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "acceldata-sdk",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "acceldata-sdk",
    "package_url": "https://pypi.org/project/acceldata-sdk/",
    "platform": null,
    "project_url": "https://pypi.org/project/acceldata-sdk/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/acceldata-sdk/2.6.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Acceldata SDK.",
    "version": "2.6.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17171516,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d14396bdadbfeab4d706ca12d488a09821a58993ce059c5b7151ed0cb9c0a87",
          "md5": "a5ceb85c7809a34ed71b61c2be5d732b",
          "sha256": "560d645fa1607e61b5f1599b9710d4711ea58989b85be6ac2c326f5c0f593aaa"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a5ceb85c7809a34ed71b61c2be5d732b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 67481,
        "upload_time": "2022-09-12T11:54:51",
        "upload_time_iso_8601": "2022-09-12T11:54:51.515113Z",
        "url": "https://files.pythonhosted.org/packages/5d/14/396bdadbfeab4d706ca12d488a09821a58993ce059c5b7151ed0cb9c0a87/acceldata_sdk-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.10": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "767ba3c8bf65771b866204e1448ea153b4a83957ae97b39947218cbe698f3a64",
          "md5": "fd1a6237824682b8a131308208e0e239",
          "sha256": "837f6df30b22c1b2cf6212e01e506e07a714572fdc6c54869f98650fb874f43f"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.10.tar.gz",
        "has_sig": false,
        "md5_digest": "fd1a6237824682b8a131308208e0e239",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 69963,
        "upload_time": "2023-01-11T11:37:44",
        "upload_time_iso_8601": "2023-01-11T11:37:44.999469Z",
        "url": "https://files.pythonhosted.org/packages/76/7b/a3c8bf65771b866204e1448ea153b4a83957ae97b39947218cbe698f3a64/acceldata_sdk-0.0.10.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aebf48d5fe6d41d59a1519887a3e220b0083b81f7dfeb2fb8a348c6970018db4",
          "md5": "2d92518d430d082a45fa506c2865b563",
          "sha256": "ab028c95f568c9dce7e46b686902babab43e7baeb0a5a9b68b7e995315759d3b"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.11.tar.gz",
        "has_sig": false,
        "md5_digest": "2d92518d430d082a45fa506c2865b563",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 71405,
        "upload_time": "2023-01-23T08:34:54",
        "upload_time_iso_8601": "2023-01-23T08:34:54.783663Z",
        "url": "https://files.pythonhosted.org/packages/ae/bf/48d5fe6d41d59a1519887a3e220b0083b81f7dfeb2fb8a348c6970018db4/acceldata_sdk-0.0.11.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.12": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "06130a7f0c1e4e7d9869c4ac678d4a1c134d5dcb88f9128f885dc62eb5da0e66",
          "md5": "f341e7625d08f3524139846dc869d9a3",
          "sha256": "b83375c54c167458266bf9464b665723f081a710c7d278c14638efc56f137825"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.12.tar.gz",
        "has_sig": false,
        "md5_digest": "f341e7625d08f3524139846dc869d9a3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 71592,
        "upload_time": "2023-01-30T10:00:41",
        "upload_time_iso_8601": "2023-01-30T10:00:41.901411Z",
        "url": "https://files.pythonhosted.org/packages/06/13/0a7f0c1e4e7d9869c4ac678d4a1c134d5dcb88f9128f885dc62eb5da0e66/acceldata_sdk-0.0.12.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.13": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b8e70b043f40b3ebc09803f7e0cf3d9420cc5a53944b22322ec7f8dc1ea75bc8",
          "md5": "8dc499b86889efe1481d6768214b580f",
          "sha256": "782a5ac7d4e0a863163284d28623bc8fa9e5350ee0ecc496f428b403eee35dd5"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.13.tar.gz",
        "has_sig": false,
        "md5_digest": "8dc499b86889efe1481d6768214b580f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 65447,
        "upload_time": "2023-03-03T09:05:53",
        "upload_time_iso_8601": "2023-03-03T09:05:53.404014Z",
        "url": "https://files.pythonhosted.org/packages/b8/e7/0b043f40b3ebc09803f7e0cf3d9420cc5a53944b22322ec7f8dc1ea75bc8/acceldata_sdk-0.0.13.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "86e453e3cb278e2f01372b7a0623c49c4d59cb94eae6f59a326970354d462c7a",
          "md5": "d723994fdc6189390cec02c0bf5f1bba",
          "sha256": "20e35b06309334f7fa4f489ec77419e0c5959f93764b2c4f6fa6c06fa5dfada6"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "d723994fdc6189390cec02c0bf5f1bba",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70688,
        "upload_time": "2022-09-27T11:58:49",
        "upload_time_iso_8601": "2022-09-27T11:58:49.205333Z",
        "url": "https://files.pythonhosted.org/packages/86/e4/53e3cb278e2f01372b7a0623c49c4d59cb94eae6f59a326970354d462c7a/acceldata_sdk-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9391cb4d807791da7cdded46cd5d8393a1478391f653e57f0e24d7aeca2f9c15",
          "md5": "4646fb86b9453a8cd5eed3da554c1149",
          "sha256": "060c9271c39619067b8d320f42fcebd07965e74ba8f7252b840c580590a8cb7e"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "4646fb86b9453a8cd5eed3da554c1149",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 72992,
        "upload_time": "2022-10-10T07:26:59",
        "upload_time_iso_8601": "2022-10-10T07:26:59.791617Z",
        "url": "https://files.pythonhosted.org/packages/93/91/cb4d807791da7cdded46cd5d8393a1478391f653e57f0e24d7aeca2f9c15/acceldata_sdk-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "94a2fba11fec41efe1e40e6c98fe31db29dd0b340054c8de7a7187200a94dedb",
          "md5": "cff34885f97507d3e6ad1d20f0957499",
          "sha256": "9c92b204ceb882264ce906d410336899dba00b59d78937b5614caf78c73019ab"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "cff34885f97507d3e6ad1d20f0957499",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 75356,
        "upload_time": "2022-11-07T07:47:44",
        "upload_time_iso_8601": "2022-11-07T07:47:44.858049Z",
        "url": "https://files.pythonhosted.org/packages/94/a2/fba11fec41efe1e40e6c98fe31db29dd0b340054c8de7a7187200a94dedb/acceldata_sdk-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8b15b50b40179a0661355111876ce81d357d0a36323543103e403d7e70aa24a1",
          "md5": "f32ea5ea789031ec4bbfde86fcde15dd",
          "sha256": "ca408b6ad9adf69b62c3bc042a237c665c94f8f1478a974c8fc8a5580156c7a5"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "f32ea5ea789031ec4bbfde86fcde15dd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 75381,
        "upload_time": "2022-11-15T11:30:09",
        "upload_time_iso_8601": "2022-11-15T11:30:09.996465Z",
        "url": "https://files.pythonhosted.org/packages/8b/15/b50b40179a0661355111876ce81d357d0a36323543103e403d7e70aa24a1/acceldata_sdk-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3d1dfaea218d4c3542952e8e7fcfe828b1182e7dd91934bd6f1c686ae796773b",
          "md5": "1fe5ce9680812a6e03c44088a3f0f675",
          "sha256": "82bb21feda31e4748a86a760322907efe2998dc918e27c3b6d96474deaacd614"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "1fe5ce9680812a6e03c44088a3f0f675",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 69509,
        "upload_time": "2022-12-02T09:47:38",
        "upload_time_iso_8601": "2022-12-02T09:47:38.609604Z",
        "url": "https://files.pythonhosted.org/packages/3d/1d/faea218d4c3542952e8e7fcfe828b1182e7dd91934bd6f1c686ae796773b/acceldata_sdk-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2f6d290633f8ab6cc219c601e57d0c4630676286da0f613a943246b207cd19cf",
          "md5": "e7a6e370ab5faf30afd518d349e732e7",
          "sha256": "8b1c282cf29f7a7db78de4b45dc5a0629d59eb9b0a648ba12e6140d6244c7dac"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "e7a6e370ab5faf30afd518d349e732e7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70310,
        "upload_time": "2022-12-12T09:01:23",
        "upload_time_iso_8601": "2022-12-12T09:01:23.733763Z",
        "url": "https://files.pythonhosted.org/packages/2f/6d/290633f8ab6cc219c601e57d0c4630676286da0f613a943246b207cd19cf/acceldata_sdk-0.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fd09b7dbd4bc6f0c9d850829a08c0107028cdf22346c194df7e0b9c69ef03d95",
          "md5": "27146c05e0fe46dd08559f33aa865948",
          "sha256": "624e4a2f500c87cc6b67df06290e4f29ff98737cdf0a788e502e2384b6923b23"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "27146c05e0fe46dd08559f33aa865948",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 73526,
        "upload_time": "2023-01-06T12:32:31",
        "upload_time_iso_8601": "2023-01-06T12:32:31.254379Z",
        "url": "https://files.pythonhosted.org/packages/fd/09/b7dbd4bc6f0c9d850829a08c0107028cdf22346c194df7e0b9c69ef03d95/acceldata_sdk-0.0.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "739d83c981046110a2face67377ef05bd3e73f2e145c9f341091609ebabe9bb6",
          "md5": "99f608d7c4d011e5d37ab93af6ee05ab",
          "sha256": "c3d159fd870580d6cd5acd7281a3d2943677989c21f3e34652219379f01413d0"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-0.0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "99f608d7c4d011e5d37ab93af6ee05ab",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 69960,
        "upload_time": "2023-01-11T09:50:06",
        "upload_time_iso_8601": "2023-01-11T09:50:06.661832Z",
        "url": "https://files.pythonhosted.org/packages/73/9d/83c981046110a2face67377ef05bd3e73f2e145c9f341091609ebabe9bb6/acceldata_sdk-0.0.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0df40eb6756785214f46250d9c95ed07ef0dbd5f131fde77506d862f70f2d605",
          "md5": "ac9ef7272a3f257344369cd3e6ad28a5",
          "sha256": "4244e508d661dc93baca443e7c226e6e7969eefcf4164f4c70115f33dbf21608"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-2.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ac9ef7272a3f257344369cd3e6ad28a5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70682,
        "upload_time": "2022-09-28T05:43:38",
        "upload_time_iso_8601": "2022-09-28T05:43:38.502735Z",
        "url": "https://files.pythonhosted.org/packages/0d/f4/0eb6756785214f46250d9c95ed07ef0dbd5f131fde77506d862f70f2d605/acceldata_sdk-2.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "85a17e1e3fdd409873580b176b787ba2eaedc207406542ffbfcc92accb506e6a",
          "md5": "52052c07c7da2fbf6ad40e280f7c8111",
          "sha256": "b05f412cec0b66afd017d2e1ffce840d25ef30dae357ddb0546ffa6bf8143a6f"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-2.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "52052c07c7da2fbf6ad40e280f7c8111",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 74999,
        "upload_time": "2022-10-18T10:40:17",
        "upload_time_iso_8601": "2022-10-18T10:40:17.403147Z",
        "url": "https://files.pythonhosted.org/packages/85/a1/7e1e3fdd409873580b176b787ba2eaedc207406542ffbfcc92accb506e6a/acceldata_sdk-2.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fd0364bd3e19f1bb217e525cf728615279b8382812af99b3c8ea7086dbf407b8",
          "md5": "a5b905e1c30a323d057a0eeba0981eb6",
          "sha256": "e7087e5d59cf18550f1a0fd028398e21d2c5998b7dd1980dec23b1753b003886"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-2.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a5b905e1c30a323d057a0eeba0981eb6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 75412,
        "upload_time": "2022-11-16T07:39:28",
        "upload_time_iso_8601": "2022-11-16T07:39:28.701368Z",
        "url": "https://files.pythonhosted.org/packages/fd/03/64bd3e19f1bb217e525cf728615279b8382812af99b3c8ea7086dbf407b8/acceldata_sdk-2.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e57f2200cc32be23b676d4def168f664d2cc3b38ee58473a79b93f78b03c31ea",
          "md5": "533ef037692bd5ead1b3be3ffa486f3e",
          "sha256": "6c79aa400875065a226bab47a41590c02815ad8450fe735f09b8a11356b58fa9"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-2.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "533ef037692bd5ead1b3be3ffa486f3e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70224,
        "upload_time": "2022-12-02T10:28:50",
        "upload_time_iso_8601": "2022-12-02T10:28:50.046511Z",
        "url": "https://files.pythonhosted.org/packages/e5/7f/2200cc32be23b676d4def168f664d2cc3b38ee58473a79b93f78b03c31ea/acceldata_sdk-2.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.4.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "176d9965c1318be1e0dafb5ecfe4b519343f7a06972264cedc48552c96d0d32f",
          "md5": "72e29eb02a0c1d1c78919a0cf6c88078",
          "sha256": "38b9f45b4f9979da4d7cb5d243a3d6557e98c59f40a1143d44ecb5f0eab70491"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-2.4.1.tar.gz",
        "has_sig": false,
        "md5_digest": "72e29eb02a0c1d1c78919a0cf6c88078",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70553,
        "upload_time": "2022-12-12T12:25:30",
        "upload_time_iso_8601": "2022-12-12T12:25:30.187617Z",
        "url": "https://files.pythonhosted.org/packages/17/6d/9965c1318be1e0dafb5ecfe4b519343f7a06972264cedc48552c96d0d32f/acceldata_sdk-2.4.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "51690273f1597c04fdaa93009eaadd8f513ae746f270a89b31737105936ab66a",
          "md5": "3b74fc07b71606cb089e823e49941ac7",
          "sha256": "4d925ff7220fecead6eea6a054554aa96d71406a64ea72509160bdf4ccba6654"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-2.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3b74fc07b71606cb089e823e49941ac7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 69958,
        "upload_time": "2023-01-24T08:45:23",
        "upload_time_iso_8601": "2023-01-24T08:45:23.923250Z",
        "url": "https://files.pythonhosted.org/packages/51/69/0273f1597c04fdaa93009eaadd8f513ae746f270a89b31737105936ab66a/acceldata_sdk-2.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ebb8ddb1b3be5878c8ff0ad7e1b1087939402e382583930fe0b98e2678dc9d2e",
          "md5": "4ca9a458e903c88bba816b7312a2a611",
          "sha256": "a0d818d9476cc527ac9b240dfb563aac7ffc0495fa819d00f23ed96138bad642"
        },
        "downloads": -1,
        "filename": "acceldata_sdk-2.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "4ca9a458e903c88bba816b7312a2a611",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 65446,
        "upload_time": "2023-03-06T08:21:15",
        "upload_time_iso_8601": "2023-03-06T08:21:15.348718Z",
        "url": "https://files.pythonhosted.org/packages/eb/b8/ddb1b3be5878c8ff0ad7e1b1087939402e382583930fe0b98e2678dc9d2e/acceldata_sdk-2.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ebb8ddb1b3be5878c8ff0ad7e1b1087939402e382583930fe0b98e2678dc9d2e",
        "md5": "4ca9a458e903c88bba816b7312a2a611",
        "sha256": "a0d818d9476cc527ac9b240dfb563aac7ffc0495fa819d00f23ed96138bad642"
      },
      "downloads": -1,
      "filename": "acceldata_sdk-2.6.0.tar.gz",
      "has_sig": false,
      "md5_digest": "4ca9a458e903c88bba816b7312a2a611",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 65446,
      "upload_time": "2023-03-06T08:21:15",
      "upload_time_iso_8601": "2023-03-06T08:21:15.348718Z",
      "url": "https://files.pythonhosted.org/packages/eb/b8/ddb1b3be5878c8ff0ad7e1b1087939402e382583930fe0b98e2678dc9d2e/acceldata_sdk-2.6.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}