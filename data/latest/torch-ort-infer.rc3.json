{
  "info": {
    "author": "torch-ort contributors",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "[OpenVINO™ Integration with Torch-ORT](https://github.com/pytorch/ort#-inference) accelerates PyTorch models using OpenVINO™ Execution Provider for ONNX Runtime. This product is designed for PyTorch developers who want to get started with OpenVINO™ in their inferencing applications. It delivers OpenVINO™ inline optimizations that enhance inferencing performance with minimal code modifications.\n\nOpenVINO™ Integration with Torch-ORT accelerates inference across many AI models on a variety of Intel® hardware such as:\n\n* Intel® CPUs\n* Intel® integrated GPUs\n* Intel® Movidius™ Vision Processing Units - referred to as VPU.\n\n## Installation\n\n### Requirements\n* Ubuntu 18.04, 20.04\n* Python 3.7, 3.8 or 3.9\n\n#### This package supports:\n* Intel® CPUs\n* Intel® integrated GPUs\n* Intel® Movidius™ Vision Processing Units (VPUs).\n\nThe torch-ort-infer package has dependency on the onnxruntime-openvino package that will be installed by default to run inference workloads. This onnxruntime-openvino package comes with pre-built libraries of OpenVINO™ version 2022.2.0 eliminating the need to install OpenVINO™ separately. The OpenVINO™ libraries are prebuilt with CXX11_ABI flag set to 0.\n\nFor more details, please refer to [OpenVINO™ Execution Provider for ONNX Runtime.](https://onnxruntime.ai/docs/execution-providers/OpenVINO-ExecutionProvider.html)\n\n## Post-installation step\n\nOnce torch-ort-infer is installed, there is a post-installation step:\n\n`python -m torch_ort.configure`\n\n## Usage\n\nBy default, Intel® CPU is used to run inference. However, you can change the default option to either Intel® integrated GPU or Intel® VPU for AI inferencing. Invoke the [provider options](https://github.com/pytorch/ort/blob/main/torch_ort_inference/docs/usage.md#essential-apis) to change the hardware on which inferencing is done.\n\nFor more API calls and environment variables, see [Usage](https://github.com/pytorch/ort/blob/main/torch_ort_inference/docs/usage.md).\n\n## Samples\nFor quick start, explore the [samples](https://github.com/pytorch/ort/tree/main/torch_ort_inference/demos) for few HuggingFace and TorchVision models.\n\n## License\nOpenVINO™ Integration with Torch-ORT is licensed under [MIT](https://github.com/pytorch/ort/blob/main/LICENSE). By contributing to the project, you agree to the license and copyright terms therein and release your contribution under these terms.\n\n## Support\nPlease submit your questions, feature requests and bug reports via [GitHub Issues](https://github.com/pytorch/ort/issues).\n\n## How to Contribute\nWe welcome community contributions to OpenVINO™ Integration with Torch-ORT. If you have an idea for improvement:\n\n* Share your proposal via [GitHub Issues](https://github.com/pytorch/ort/issues).\n* Submit a [Pull Request](https://github.com/pytorch/ort/pulls).\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/pytorch/ort",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torch-ort-infer",
    "package_url": "https://pypi.org/project/torch-ort-infer/",
    "platform": null,
    "project_url": "https://pypi.org/project/torch-ort-infer/",
    "project_urls": {
      "Bug Tracker": "https://github.com/pytorch/ort/issues",
      "Homepage": "https://github.com/pytorch/ort"
    },
    "release_url": "https://pypi.org/project/torch-ort-infer/1.13.1/",
    "requires_dist": [
      "onnxruntime-openvino (>=1.12.0)",
      "torch (==1.12.1)",
      "onnxruntime-openvino (>=1.12.0) ; extra == 'openvino'"
    ],
    "requires_python": ">=3.7",
    "summary": "Accelerate PyTorch models with ONNX Runtime OpenVINO EP",
    "version": "1.13.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15647435,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dee197ebbf93639e513330434a0f97abd405a1ba00887f5a0ff898241ec7bc17",
          "md5": "6033868d26d923a2ee1a1ae7a51d7ef1",
          "sha256": "bdf07ac71cbc7413b15aca66400b0f652c257b6ec347834dc11d2543c82b317f"
        },
        "downloads": -1,
        "filename": "torch_ort_infer-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6033868d26d923a2ee1a1ae7a51d7ef1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 9738,
        "upload_time": "2022-07-14T20:55:06",
        "upload_time_iso_8601": "2022-07-14T20:55:06.414556Z",
        "url": "https://files.pythonhosted.org/packages/de/e1/97ebbf93639e513330434a0f97abd405a1ba00887f5a0ff898241ec7bc17/torch_ort_infer-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.12.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "35452ea593826fabd1ede6e36bda12efbd81f7409f099affe38281ae72f5dd9a",
          "md5": "4ba9f77454bd1cc34fe1beb670f50d90",
          "sha256": "5a36ca8962b183cd414d292ff5cfb8da78856a3ca03a57aca5a30f610ae416c8"
        },
        "downloads": -1,
        "filename": "torch_ort_infer-1.12.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4ba9f77454bd1cc34fe1beb670f50d90",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 10734,
        "upload_time": "2022-08-02T05:03:47",
        "upload_time_iso_8601": "2022-08-02T05:03:47.138733Z",
        "url": "https://files.pythonhosted.org/packages/35/45/2ea593826fabd1ede6e36bda12efbd81f7409f099affe38281ae72f5dd9a/torch_ort_infer-1.12.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.13.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cc29cbd6b2c8e5111e993d79e5e992d288d3dd81a3569461f813e1ec040fbddf",
          "md5": "dfedbe188f37e15781e2c774557ca7a6",
          "sha256": "e36fc35903c80252981cdca07e4b2f48856a3eb3615569ead0013d7e7914f7d4"
        },
        "downloads": -1,
        "filename": "torch_ort_infer-1.13.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dfedbe188f37e15781e2c774557ca7a6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 10759,
        "upload_time": "2022-11-03T22:54:43",
        "upload_time_iso_8601": "2022-11-03T22:54:43.788377Z",
        "url": "https://files.pythonhosted.org/packages/cc/29/cbd6b2c8e5111e993d79e5e992d288d3dd81a3569461f813e1ec040fbddf/torch_ort_infer-1.13.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cc29cbd6b2c8e5111e993d79e5e992d288d3dd81a3569461f813e1ec040fbddf",
        "md5": "dfedbe188f37e15781e2c774557ca7a6",
        "sha256": "e36fc35903c80252981cdca07e4b2f48856a3eb3615569ead0013d7e7914f7d4"
      },
      "downloads": -1,
      "filename": "torch_ort_infer-1.13.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "dfedbe188f37e15781e2c774557ca7a6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 10759,
      "upload_time": "2022-11-03T22:54:43",
      "upload_time_iso_8601": "2022-11-03T22:54:43.788377Z",
      "url": "https://files.pythonhosted.org/packages/cc/29/cbd6b2c8e5111e993d79e5e992d288d3dd81a3569461f813e1ec040fbddf/torch_ort_infer-1.13.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}