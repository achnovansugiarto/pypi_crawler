{
  "info": {
    "author": "Joshua Evans",
    "author_email": "jbe25@bath.ac.uk",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# SimpleOptions\r\n\r\nThis Python package aims to provide a simple framework for implementing and using options in Hierarchical Reinforcement Learning (HRL) projects.\r\n\r\nKey classes:\r\n\r\n- `BaseOption`: An abstract class representing an option with an initiation set, option policy, and termination condition.\r\n- `BaseEnvironment`: An abstract base class representing an agent's environment. The environment specification is based on the OpenAI Gym `Env` specifciation, but does not implement it directly. It supports both primitive actions and options, as well as functionality for constructing State-Transition Graphs (STGs) out-of-the-box using NetworkX.\r\n- `OptionAgent`: A class representing an HRL agent, which can interact with its environment and has access to a number of options. It includes implementations of Macro-Q Learning and Intra-Option learning, with many customisable features.\r\n\r\nThis code was written with tabular, graph-based HRL methods in mind. It's less of a plug-and-play repository, and is intended to be used to as a basic framework for developing your own `BaseOption` and `Environment` implementations.\r\n\r\n## How to Install\r\n\r\nThe easiest way to install this package is to simply run `pip install simpleoptions`.\r\n\r\nIf you want to install from source, download the repository and, in the root directory, run the command `pip install .`\r\n\r\n## How to Use This Code\r\n\r\nBelow, you will find a step-by-step guide introducing the intended workflow for using this code.\r\n\r\n### Step 1: Implement an Environment\r\n\r\nThe first step to using this framework involves defining an environment for your agents to interact with. This can be done by implementing the methods specified in the `BaseEnvironment` class. If you have previously worked with OpenAI Gym, much of this will be familiar to you, although there are a few additional methods on top of the usual `step` and `reset` that you'll need to implement.\r\n\r\n### Step 2: Define Your Options\r\n\r\nYou must now define/discover options for your agent to use when interacting with its environment. How you go about this is up to you &mdash; this framework allows you to train agents using options, not discover them. An ever-growing number of option discovery methods can be found in the hierarchical reinforcement learning literature.\r\n\r\nTo define an option, you need to subclass `BaseOption` and implement the following methods:\r\n\r\n- `initiation` - a method that takes a state as its input, and returns whether the option can be invoked in that state.\r\n- `termination` - a method that takes a state as its input, and returns the probability that the option terminates in that state.\r\n- `policy` - a method that takes a state as its input, and returns the action (either a primitive action or another option) that this option would select in this state.\r\n\r\nThis minimal framework gives you a lot of flexibility in defining your options. For example, your `policy` method could make use of a simple dictionary mapping states to actions, it could be based on some learned action-value function, or any other function of the state.\r\n\r\nAs an example, consider an example option that takes an agent to a sub-goal state from any of the nearest 50 states. `initiation` would return `True` for the nearest 50 states to the subgoal, and `False` otherwise. `termination` would return `0.0` for states in the initiation set, and `1.0` otherwise. `policy` woudl return the primitive action that takes the agent one step along the shortest path to the subgoal state.\r\n\r\nFinally, we also include a `PrimitiveOption` that can be used to represent the primitive actions made available by a given environment.\r\n\r\n### Step 5: Running an Agent Which Uses Options\r\n\r\nOur framework also includes an implementation of Macro-Q Learning and Intra-Option Learning, which can be used to train an agent in an environment with a given set of options.\r\n\r\nOnce you have an environment and a set of options, you can instantiate the `OptionsAgent` class and use its `run_agent` method to train it.\r\n\r\n## Example Implementation\r\n\r\nAn complete end-to-end example implementation of a simple environment and set of options can be found [here](https://github.com/Ueva/BaRL-SimpleOptions/tree/master/example).\r\n\r\n## Additional Environments\r\n\r\nA number of environment implementations based on our `BaseEnvironment` class cna be found [here](https://github.com/Ueva/BaRL_Envs).\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Ueva/BaRL-SimpleOptions",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "simpleoptions",
    "package_url": "https://pypi.org/project/simpleoptions/",
    "platform": null,
    "project_url": "https://pypi.org/project/simpleoptions/",
    "project_urls": {
      "Homepage": "https://github.com/Ueva/BaRL-SimpleOptions"
    },
    "release_url": "https://pypi.org/project/simpleoptions/0.3.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A package which provides a simple framework for working with Options in Reinforcement Learning.",
    "version": "0.3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17510075,
  "releases": {
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "629d00cbc8b14d6645c02db685ce04c7781ee9a1a12fa6b28882719bfd08f944",
          "md5": "a615eef8f083e5264d8487f21c8b9f9f",
          "sha256": "766bd5162e36684f9c9aa68f2d61c89e1adf25913f7414cf0defb983d845aa40"
        },
        "downloads": -1,
        "filename": "simpleoptions-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a615eef8f083e5264d8487f21c8b9f9f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 15426,
        "upload_time": "2023-03-30T12:18:47",
        "upload_time_iso_8601": "2023-03-30T12:18:47.665403Z",
        "url": "https://files.pythonhosted.org/packages/62/9d/00cbc8b14d6645c02db685ce04c7781ee9a1a12fa6b28882719bfd08f944/simpleoptions-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "629d00cbc8b14d6645c02db685ce04c7781ee9a1a12fa6b28882719bfd08f944",
        "md5": "a615eef8f083e5264d8487f21c8b9f9f",
        "sha256": "766bd5162e36684f9c9aa68f2d61c89e1adf25913f7414cf0defb983d845aa40"
      },
      "downloads": -1,
      "filename": "simpleoptions-0.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "a615eef8f083e5264d8487f21c8b9f9f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 15426,
      "upload_time": "2023-03-30T12:18:47",
      "upload_time_iso_8601": "2023-03-30T12:18:47.665403Z",
      "url": "https://files.pythonhosted.org/packages/62/9d/00cbc8b14d6645c02db685ce04c7781ee9a1a12fa6b28882719bfd08f944/simpleoptions-0.3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}