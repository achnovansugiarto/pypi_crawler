{
  "info": {
    "author": "Felix Petersen",
    "author_email": "ads0600@felix-petersen.de",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Scientific/Engineering :: Mathematics",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# difflogic - A Library for Differentiable Logic Gate Networks\n\n![difflogic_logo](difflogic_logo.png)\n\nThis repository includes the official implementation of our NeurIPS 2022 Paper \"Deep Differentiable Logic Gate Networks\"\n(Paper @ [ArXiv](https://arxiv.org/abs/2210.08277)).\n\nThe goal behind differentiable logic gate networks is to solve machine learning tasks by learning combinations of logic\ngates, i.e., so-called logic gate networks. As logic gate networks are conventionally non-differentiable, they can \nconventionally not be trained with methods such as gradient descent. Thus, differentiable logic gate networks are a \ndifferentiable relaxation of logic gate networks which allows efficiently learning of logic gate networks with gradient\ndescent. Specifically, `difflogic` combines real-valued logics and a continuously parameterized relaxation of\nthe network. This allows learning which logic gate (out of 16 possible) is optimal for each neuron.\nThe resulting discretized logic gate networks achieve fast inference speeds, e.g., beyond a million images\nof MNIST per second on a single CPU core.\n\n`difflogic` is a Python 3.6+ and PyTorch 1.9.0+ based library for training and inference with logic gate networks.\nThe library can be installed with:\n```shell\npip install difflogic\n```\n> âš ï¸ Note that `difflogic` requires CUDA, the CUDA Toolkit (for compilation), and `torch>=1.9.0` (matching the CUDA version).\n\nFor additional installation support, see [INSTALLATION_SUPPORT.md](INSTALLATION_SUPPORT.md).\n\n## ðŸŒ± Intro and Training\n\nThis library provides a framework for both training and inference with logic gate networks.\nThe following gives an example of a definition of a differentiable logic network model for the MNIST data set:\n\n```python\nfrom difflogic import LogicLayer, GroupSum\nimport torch\n\nmodel = torch.nn.Sequential(\n    torch.nn.Flatten(),\n    LogicLayer(784, 16_000),\n    LogicLayer(16_000, 16_000),\n    LogicLayer(16_000, 16_000),\n    LogicLayer(16_000, 16_000),\n    LogicLayer(16_000, 16_000),\n    GroupSum(k=10, tau=30)\n)\n```\n\nThis model receives a `784` dimensional input and returns `k=10` values corresponding to the 10 classes of MNIST.\nThe model may be trained, e.g., with a `torch.nn.CrossEntropyLoss` similar to how other neural networks models are trained in PyTorch.\nNotably, the Adam optimizer (`torch.optim.Adam`) should be used for training and the recommended default learning rate is `0.01` instead of `0.001`.\nFinally, it is also important to note that the number of neurons in each layer is much higher for logic gate networks compared to \nconventional MLP neural networks because logic gate networks are very sparse.\n\nTo go into details, for each of these modules, in the following we provide more in-depth examples:\n\n```python\nlayer = LogicLayer(\n    in_dim=784,             # number of inputs\n    out_dim=16_000,         # number of outputs\n    device='cuda',          # the device (cuda / cpu)\n    implementation='cuda',  # the implementation to be used (native cuda / vanilla pytorch)\n    connections='random',   # the method for the random initialization of the connections\n    grad_factor=1.1,        # for deep models (>6 layers), the grad_factor should be increased (e.g., 2) to avoid vanishing gradients\n)\n```\n\nAt this point, it is important to discuss the options for `device` and the provided implementations. Specifically,\n`difflogic` provides two implementations (both of which work with PyTorch):\n\n* **`python`** the Python implementation is a substantially slower implementation that is easy to understand as it is implemented directly in Python with PyTorch and does not require any C++ / CUDA extensions. It is compatible with `device='cpu'` and `device='cuda'`.\n* **`cuda`** is a well-optimized implementation that runs natively on CUDA via custom extensions. This implementation is around 50 to 100 times faster than the python implementation (for large models). It only supports `device='cuda'`. \n\nTo aggregate output neurons into a lower dimensional output space, we can use `GroupSum`, which aggregates a number of output neurons into \na `k` dimensional output, e.g., `k=10` for a 10-dimensional classification setting. \nIt is important to set the parameter `tau`, which the sum of neurons is divided by to keep the range reasonable.\nAs each neuron has a value between 0 and 1 (or in inference a value of 0 or 1), assuming `n` output neurons of the last `LogicLayer`,\nthe range of outputs is `[0, n / k / tau]`.\n\n## ðŸ–¥ Model Inference\n\nDuring training, the model should remain in the PyTorch training mode (`.train()`), which keeps the model differentiable.\nHowever, we can easily switch the model to a hard / discrete / non-differentiable model by calling `model.eval()`, i.e., for inference.\nTypically, this will simply discretize the model but not make it faster per se.\n\nHowever, there are two modes that allow for fast inference:\n\n### `PackBitsTensor`\n\nThe first option is to use a `PackBitsTensor`. \n`PackBitsTensor`s allow efficient dynamic execution of trained logic gate networks on GPU.\n\nA `PackBitsTensor` can package a tensor (of shape `b x n`) with boolean \ndata type in a way such that each boolean entry requires only a single bit (in contrast to the full byte typically\nrequired by a bool) by packing the bits along the batch dimension. If we choose to pack the bits into the `int32` data \ntype (the options are 8, 16, 32, and 64 bits), we would receive a tensor of shape `ceil(b/32) x n` of dtype `int32`.\nTo create a `PackBitsTensor` from a boolean tensor `data`, simply call:\n```python\ndata_bits = difflogic.PackBitsTensor(data)\n```\nTo apply a model to the `PackBitsTensor`, simply call:\n```python\noutput = model(data_bits)\n```\nThis requires that the `model` is in `.eval()` mode, and if supplied with a `PackBitsTensor`, will automatically use \na logic gate-based inference on the tensor. This also requires that `model.implementation = 'cuda'` as the mode is only\nimplemented in CUDA.\nIt is notable that, while the model is in `.eval()` mode, we can still also feed float tensors through the model, in \nwhich case it will simply use a hard variant of the real-valued logics.\n\n### `CompiledLogicNet`\n\nThe second option is to use a `CompiledLogicNet`.\nThis allows especially efficient static execution of a fixed trained logic gate network on CPU.\nSpecifically, `CompiledLogicNet` converts a model into efficient C code and can compile this code into a binary that\ncan then be efficiently run or exported for applications.\nThe following is an example for creating `CompiledLogicNet` from a trained `model`:\n\n```python\ncompiled_model = difflogic.CompiledLogicNet(\n    model=model,            # the trained model (should be a `torch.nn.Sequential` with `LogicLayer`s)\n    num_bits=64,            # the number of bits of the datatype used for inference (typically 64 is fastest, should not be larger than batch size)\n    cpu_compiler='gcc',     # the compiler to use for the c code (alternative: clang)\n    verbose=True            \n)\ncompiled_model.compile(\n    save_lib_path='my_model_binary.so',  # the (optional) location for storing the binary such that it can be reused\n    verbose=True\n)\n\n# to apply the model, we need a 2d numpy array of dtype bool, e.g., via  `data = data.bool().numpy()`\noutput = compiled_model(data)\n```\n\nThis will compile a model into a shared object binary, which is then automatically imported.\nTo export this to other applications, one may either call the shared object binary from another program or export \nthe model into C code via `compiled_model.get_c_code()`.\nA limitation of the current `CompiledLogicNet` is that the compilation time can become long for large models.\n\nWe note that between publishing the paper and the publication of `difflogic`, we have substantially improved the implementations.\nThus, the model inference modes have some deviation from the implementations for the original paper as we have \nfocussed on making it more scalable, efficient, and easier to apply in applications.\nWe have especially focussed on modularity and efficiency for larger models and have opted to polish the presented \nimplementations over publishing a plethora of different competing implementations.\n\n## ðŸ§ª Experiments\n\nIn the following, we present a few example experiments which are contained in the `experiments` directory.\n`main.py` executes the experiments for difflogic and `main_baseline.py` contains regular neural network baselines.\n\n### â˜„ï¸ Adult / Breast Cancer\n\n```shell\npython experiments/main.py  -eid 526010           -bs 100 -t 20 --dataset adult         -ni 100_000 -ef 1_000 -k 256 -l 5 --compile_model\npython experiments/main.py  -eid 526020 -lr 0.001 -bs 100 -t 20 --dataset breast_cancer -ni 100_000 -ef 1_000 -k 128 -l 5 --compile_model\n```\n\n### ðŸ”¢ MNIST\n\n```shell\npython experiments/main.py  -bs 100 -t  10 --dataset mnist20x20 -ni 200_000 -ef 1_000 -k  8_000 -l 6 --compile_model\npython experiments/main.py  -bs 100 -t  30 --dataset mnist      -ni 200_000 -ef 1_000 -k 64_000 -l 6 --compile_model\n# Baselines:\npython experiments/main_baseline.py  -bs 100 --dataset mnist    -ni 200_000 -ef 1_000 -k  128 -l 3\npython experiments/main_baseline.py  -bs 100 --dataset mnist    -ni 200_000 -ef 1_000 -k 2048 -l 7\n```\n\n### ðŸ¶ CIFAR-10\n\n```shell\npython experiments/main.py  -bs 100 -t 100 --dataset cifar-10-3-thresholds  -ni 200_000 -ef 1_000 -k    12_000 -l 4 --compile_model\npython experiments/main.py  -bs 100 -t 100 --dataset cifar-10-3-thresholds  -ni 200_000 -ef 1_000 -k   128_000 -l 4 --compile_model\npython experiments/main.py  -bs 100 -t 100 --dataset cifar-10-31-thresholds -ni 200_000 -ef 1_000 -k   256_000 -l 5\npython experiments/main.py  -bs 100 -t 100 --dataset cifar-10-31-thresholds -ni 200_000 -ef 1_000 -k   512_000 -l 5\npython experiments/main.py  -bs 100 -t 100 --dataset cifar-10-31-thresholds -ni 200_000 -ef 1_000 -k 1_024_000 -l 5\n```\n\n## ðŸ“– Citing\n\n```bibtex\n@inproceedings{petersen2022difflogic,\n  title={{Deep Differentiable Logic Gate Networks}},\n  author={Petersen, Felix and Borgelt, Christian and Kuehne, Hilde and Deussen, Oliver},\n  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},\n  year={2022}\n}\n```\n\n## ðŸ“œ License\n\n`difflogic` is released under the MIT license. See [LICENSE](LICENSE) for additional details about it. \n\nPatent pending.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Felix-Petersen/difflogic",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "difflogic",
    "package_url": "https://pypi.org/project/difflogic/",
    "platform": null,
    "project_url": "https://pypi.org/project/difflogic/",
    "project_urls": {
      "Homepage": "https://github.com/Felix-Petersen/difflogic"
    },
    "release_url": "https://pypi.org/project/difflogic/0.1.0/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16375556,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6050c854cf679c199ab1ecd58613ff9f80db565e519897286e0093ee72c0b8f2",
          "md5": "5d89f5899c3560f72709a09916bd19f2",
          "sha256": "18911108977e4a79d10894d01abd899e31dfef0e7d665d2068cc5c59639cc1bc"
        },
        "downloads": -1,
        "filename": "difflogic-0.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5d89f5899c3560f72709a09916bd19f2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 1299,
        "upload_time": "2022-04-21T11:14:23",
        "upload_time_iso_8601": "2022-04-21T11:14:23.153921Z",
        "url": "https://files.pythonhosted.org/packages/60/50/c854cf679c199ab1ecd58613ff9f80db565e519897286e0093ee72c0b8f2/difflogic-0.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e82981926f059b480b146f537bc528450441c7ad9ae6d2a80a04163764ff642",
          "md5": "645d098acafc86b1a0844dd0c8f5c7d4",
          "sha256": "81013e9c358842d048aea297a06c529ac5393ce841e71cfa88da691cd20ee794"
        },
        "downloads": -1,
        "filename": "difflogic-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "645d098acafc86b1a0844dd0c8f5c7d4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 1433,
        "upload_time": "2022-04-21T11:14:25",
        "upload_time_iso_8601": "2022-04-21T11:14:25.351964Z",
        "url": "https://files.pythonhosted.org/packages/5e/82/981926f059b480b146f537bc528450441c7ad9ae6d2a80a04163764ff642/difflogic-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9c18e7ee8ec8a895f000411853174636f6a90480076da43ca3df40e1e4875fcc",
          "md5": "88804ca7b81736f75fa52521c97dbab7",
          "sha256": "979ce845a494bdf8ebeeb94d61f3bbf1ae018257ee5b048f936567b53fc115a1"
        },
        "downloads": -1,
        "filename": "difflogic-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "88804ca7b81736f75fa52521c97dbab7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 21553,
        "upload_time": "2023-01-10T14:27:11",
        "upload_time_iso_8601": "2023-01-10T14:27:11.097233Z",
        "url": "https://files.pythonhosted.org/packages/9c/18/e7ee8ec8a895f000411853174636f6a90480076da43ca3df40e1e4875fcc/difflogic-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9c18e7ee8ec8a895f000411853174636f6a90480076da43ca3df40e1e4875fcc",
        "md5": "88804ca7b81736f75fa52521c97dbab7",
        "sha256": "979ce845a494bdf8ebeeb94d61f3bbf1ae018257ee5b048f936567b53fc115a1"
      },
      "downloads": -1,
      "filename": "difflogic-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "88804ca7b81736f75fa52521c97dbab7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 21553,
      "upload_time": "2023-01-10T14:27:11",
      "upload_time_iso_8601": "2023-01-10T14:27:11.097233Z",
      "url": "https://files.pythonhosted.org/packages/9c/18/e7ee8ec8a895f000411853174636f6a90480076da43ca3df40e1e4875fcc/difflogic-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}