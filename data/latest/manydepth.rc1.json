{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth\n\n[Jamie Watson](https://scholar.google.com/citations?user=5pC7fw8AAAAJ&hl=en),\n[Oisin Mac Aodha](https://homepages.inf.ed.ac.uk/omacaod/),\n[Victor Prisacariu](https://www.robots.ox.ac.uk/~victor/),\n[Gabriel J. Brostow](http://www0.cs.ucl.ac.uk/staff/g.brostow/) and\n[Michael Firman](http://www.michaelfirman.co.uk) ‚Äì **CVPR 2021**\n\n[[Link to paper]](https://arxiv.org/abs/2104.14540)\n\nWe introduce ***ManyDepth***, an adaptive approach to dense depth estimation that can make use of sequence information at test time, when it is available.\n\n* ‚úÖ **Self-supervised**: We train from monocular video only. No depths or poses are needed at training or test time.\n* ‚úÖ Good depths from single frames; even better depths from **short sequences**.\n* ‚úÖ **Efficient**: Only one forward pass at test time. No test-time optimization needed.\n* ‚úÖ **State-of-the-art** self-supervised monocular-trained depth estimation on KITTI and CityScapes.\n\n\n<p align=\"center\">\n  <a\nhref=\"https://storage.googleapis.com/niantic-lon-static/research/manydepth/manydepth_cvpr_cc.mp4\">\n  <img src=\"assets/video_thumbnail.png\" alt=\"5 minute CVPR presentation video link\" width=\"400\">\n  </a>\n</p>\n\n\n## pip install\n\nManydepth can be installed through pip \n```bash\npip install git+https://github.com/AdityaNG/manydepth@pip-module\n```\n\nRun the webcam demo with :\n```bash\npython -m manydepth\n```\n\nTo use the class to create a manydepth2 object as follows : \n```python\nfrom manydepth  import manydepth\nmd = manydepth()\n# Load in a frame along with previous frame\ndepth = md.eval(frame, prev_frame)\n```\n\n\n## Overview\n\nCost volumes are commonly used for estimating depths from multiple input views:\n\n<p align=\"center\">\n  <img src=\"assets/cost_volume.jpg\" alt=\"Cost volume used for aggreagting sequences of frames\" width=\"700\" />\n</p>\n\nHowever, cost volumes do not easily work with self-supervised training.\n\n<p align=\"center\">\n  <img src=\"assets/baseline.gif\" alt=\"Baseline: Depth from cost volume input without our contributions\" width=\"700\" />\n</p>\n\nIn our paper, we:\n\n* Introduce an adaptive cost volume to deal with unknown scene scales\n* Fix problems with moving objects\n* Introduce augmentations to deal with static cameras and start-of-sequence frames\n\nThese contributions enable cost volumes to work with self-supervised training:\n\n<p align=\"center\">\n  <img src=\"assets/ours.gif\" alt=\"ManyDepth: Depth from cost volume input with our contributions\" width=\"700\" />\n</p>\n\nWith our contributions, short test-time sequences give better predictions than methods which predict depth from just a single frame.\n\n<p align=\"center\">\n  <img src=\"assets/manydepth_vs_monodepth2.jpg\" alt=\"ManyDepth vs Monodepth2 depths and error maps\" width=\"700\" />\n</p>\n\n## ‚úèÔ∏è üìÑ Citation\n\nIf you find our work useful or interesting, please cite our paper:\n\n```latex\n@inproceedings{watson2021temporal,\n    author = {Jamie Watson and\n              Oisin Mac Aodha and\n              Victor Prisacariu and\n              Gabriel Brostow and\n              Michael Firman},\n    title = {{The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth}},\n    booktitle = {Computer Vision and Pattern Recognition (CVPR)},\n    year = {2021}\n}\n```\n\n## üìà Results\n\nOur **ManyDepth** method outperforms all previous methods in all subsections across most metrics, whether or not the baselines use multiple frames at test time.\nSee our paper for full details.\n\n<p align=\"center\">\n  <img src=\"assets/results_table.png\" alt=\"KITTI results table\" width=\"700\" />\n</p>\n\n## üëÄ Reproducing Paper Results\n\nTo recreate the results from our paper, run:\n\n```bash\nCUDA_VISIBLE_DEVICES=<your_desired_GPU> \\\npython -m manydepth.train \\\n    --data_path <your_KITTI_path> \\\n    --log_dir <your_save_path>  \\\n    --model_name <your_model_name>\n```\n\nDepending on the size of your GPU, you may need to set `--batch_size` to be lower than 12. Additionally you can train\na high resolution model by adding `--height 320 --width 1024`.\n\nFor instructions on downloading the KITTI dataset, see [Monodepth2](https://github.com/nianticlabs/monodepth2)\n\nTo train a CityScapes model, run:\n\n```bash\nCUDA_VISIBLE_DEVICES=<your_desired_GPU> \\\npython -m manydepth.train \\\n    --data_path <your_preprocessed_cityscapes_path> \\\n    --log_dir <your_save_path>  \\\n    --model_name <your_model_name> \\\n    --dataset cityscapes_preprocessed \\\n    --split cityscapes_preprocessed \\\n    --freeze_teacher_epoch 5 \\\n    --height 192 --width 512\n```\n\nNote here the `--freeze_teacher_epoch 5` command - we found this to be important for Cityscapes models, due to the large number of images in the training set. \n\nThis assumes you have already preprocessed the CityScapes dataset using SfMLearner's [prepare_train_data.py](https://github.com/tinghuiz/SfMLearner/blob/master/data/prepare_train_data.py) script.\nWe used the following command:\n\n```bash\npython prepare_train_data.py \\\n    --img_height 512 \\\n    --img_width 1024 \\\n    --dataset_dir <path_to_downloaded_cityscapes_data> \\\n    --dataset_name cityscapes \\\n    --dump_root <your_preprocessed_cityscapes_path> \\\n    --seq_length 3 \\\n    --num_threads 8\n```\n\nNote that while we use the `--img_height 512` flag, the `prepare_train_data.py` script will save images which are `1024x384` as it also crops off the bottom portion of the image.\nYou could probably save disk space without a loss of accuracy by preprocessing with `--img_height 256 --img_width 512` (to create `512x192` images), but this isn't what we did for our experiments.\n\n## üíæ Pretrained weights and evaluation\n\nYou can download weights for some pretrained models here:\n\n* [KITTI MR (640x192)](https://storage.googleapis.com/niantic-lon-static/research/manydepth/models/KITTI_MR.zip)\n* [KITTI HR (1024x320)](https://storage.googleapis.com/niantic-lon-static/research/manydepth/models/KITTI_HR.zip)\n* [CityScapes (512x192)](https://storage.googleapis.com/niantic-lon-static/research/manydepth/models/CityScapes_MR.zip)\n\nTo evaluate a model on KITTI, run:\n\n```bash\nCUDA_VISIBLE_DEVICES=<your_desired_GPU> \\\npython -m manydepth.evaluate_depth \\\n    --data_path <your_KITTI_path> \\\n    --load_weights_folder <your_model_path>\n    --eval_mono\n```\n\nMake sure you have first run `export_gt_depth.py` to extract ground truth files.\n\nAnd to evaluate a model on Cityscapes, run:\n\n```bash\nCUDA_VISIBLE_DEVICES=<your_desired_GPU> \\\npython -m manydepth.evaluate_depth \\\n    --data_path <your_cityscapes_path> \\\n    --load_weights_folder <your_model_path>\n    --eval_mono \\\n    --eval_split cityscapes\n```\n\nDuring evaluation, we crop and evaluate on the middle 50% of the images.\n\nWe provide ground truth depth files [HERE](https://storage.googleapis.com/niantic-lon-static/research/manydepth/gt_depths_cityscapes.zip),\nwhich were converted from pixel disparities using intrinsics and the known baseline. Download this and unzip into `splits/cityscapes`.\n\n\nIf you want to evaluate a teacher network (i.e. the monocular network used for consistency loss), then add the flag `--eval_teacher`. This will \nload the weights of `mono_encoder.pth` and `mono_depth.pth`, which are provided for our KITTI models. \n\n## üñº Running on your own images\n\nWe provide some sample code in `test_simple.py` which demonstrates multi-frame inference.\nThis predicts depth for a sequence of two images cropped from a [dashcam video](https://www.youtube.com/watch?v=sF0wXxZwISw).\nPrediction also requires an estimate of the intrinsics matrix, in json format.\nFor the provided test images, we have estimated the intrinsics to be equivalent to those of the KITTI dataset.\nNote that the intrinsics provided in the json file are expected to be in [normalised coordinates](https://github.com/nianticlabs/monodepth2/issues/6#issuecomment-494407590).\n\nDownload and unzip model weights from one of the links above, and then run the following command:\n\n```bash\npython -m manydepth.test_simple \\\n    --target_image_path assets/test_sequence_target.jpg \\\n    --source_image_path assets/test_sequence_source.jpg \\\n    --intrinsics_json_path assets/test_sequence_intrinsics.json \\\n    --model_path path/to/weights\n```\n\nA predicted depth map rendering will be saved to `assets/test_sequence_target_disp.jpeg`.\n\n## üë©‚Äç‚öñÔ∏è License\n\nCopyright ¬© Niantic, Inc. 2021. Patent Pending.\nAll rights reserved.\nPlease see the [license file](LICENSE) for terms.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/AdityaNG/monodepth2",
    "keywords": "",
    "license": "LICENSE",
    "maintainer": "",
    "maintainer_email": "",
    "name": "manydepth",
    "package_url": "https://pypi.org/project/manydepth/",
    "platform": "",
    "project_url": "https://pypi.org/project/manydepth/",
    "project_urls": {
      "Homepage": "https://github.com/AdityaNG/monodepth2"
    },
    "release_url": "https://pypi.org/project/manydepth/1.0.0/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "[CVPR 2021] Self-supervised depth estimation from short sequences",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11922436,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2cba9f7558fe4c9f78cbaa53140504cd52aa543b774341a2747fc10c6b8f04a2",
          "md5": "9ccbcd7fbf27872bbd9e9e1a2467ba93",
          "sha256": "25da7b96e59678ed3dd5d7e2aeb3c806b7f8f2c53069b3c3598b8fbf26b4e233"
        },
        "downloads": -1,
        "filename": "manydepth-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9ccbcd7fbf27872bbd9e9e1a2467ba93",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 44113,
        "upload_time": "2021-11-04T04:33:09",
        "upload_time_iso_8601": "2021-11-04T04:33:09.570272Z",
        "url": "https://files.pythonhosted.org/packages/2c/ba/9f7558fe4c9f78cbaa53140504cd52aa543b774341a2747fc10c6b8f04a2/manydepth-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "52896538b7f86fbf36ad228aeeab7908ccd69e8e6e74d9bc34b3f90136d05b07",
          "md5": "385446555453722a97eee8a570a5f783",
          "sha256": "7bb6549999e08a9e95d50c373c6209917c8b159c4705472bc900e763ada2dbf7"
        },
        "downloads": -1,
        "filename": "manydepth-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "385446555453722a97eee8a570a5f783",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 37109,
        "upload_time": "2021-11-04T04:33:11",
        "upload_time_iso_8601": "2021-11-04T04:33:11.108167Z",
        "url": "https://files.pythonhosted.org/packages/52/89/6538b7f86fbf36ad228aeeab7908ccd69e8e6e74d9bc34b3f90136d05b07/manydepth-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2cba9f7558fe4c9f78cbaa53140504cd52aa543b774341a2747fc10c6b8f04a2",
        "md5": "9ccbcd7fbf27872bbd9e9e1a2467ba93",
        "sha256": "25da7b96e59678ed3dd5d7e2aeb3c806b7f8f2c53069b3c3598b8fbf26b4e233"
      },
      "downloads": -1,
      "filename": "manydepth-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "9ccbcd7fbf27872bbd9e9e1a2467ba93",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 44113,
      "upload_time": "2021-11-04T04:33:09",
      "upload_time_iso_8601": "2021-11-04T04:33:09.570272Z",
      "url": "https://files.pythonhosted.org/packages/2c/ba/9f7558fe4c9f78cbaa53140504cd52aa543b774341a2747fc10c6b8f04a2/manydepth-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "52896538b7f86fbf36ad228aeeab7908ccd69e8e6e74d9bc34b3f90136d05b07",
        "md5": "385446555453722a97eee8a570a5f783",
        "sha256": "7bb6549999e08a9e95d50c373c6209917c8b159c4705472bc900e763ada2dbf7"
      },
      "downloads": -1,
      "filename": "manydepth-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "385446555453722a97eee8a570a5f783",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 37109,
      "upload_time": "2021-11-04T04:33:11",
      "upload_time_iso_8601": "2021-11-04T04:33:11.108167Z",
      "url": "https://files.pythonhosted.org/packages/52/89/6538b7f86fbf36ad228aeeab7908ccd69e8e6e74d9bc34b3f90136d05b07/manydepth-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}