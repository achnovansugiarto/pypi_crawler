{
  "info": {
    "author": "gxtrobot",
    "author_email": "gxtrobot@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "## aspider\n    一个简单好用的Python 异步爬虫\n\n## 简介\n\n    aspider 是一个基于Python asyncio 开发的爬虫工具, 可以通过命令行使用, 不用写任何代码, 也可以调用api, 自定义具体爬取的规则.\n    aspider的设计原则是专注于爬取网页, 本身不包含如解析文本, 数据存储等内容, 这些都可以利用很多已有的工具框架自行加上. aspider 参考了\n    类似flask的路由装饰器的方法, 只需要定义一些方法, 每个方法用于爬取一个相应的正则路径.\n\n    author: gxtrobot\n\n## 系统设计原理\n- 系统使用常用的生产者, 消费者模式, 生产者产生需要爬取的链接并加入队列, 消费者从队列获取链接进行爬取, 并解析其内容和链接, 调用相关的方法, 和用户自定义的方法, 符合要求的链接会加到队列, 这样只通过一个根路径就可以扩展到全站爬取\n- 系统才用asyncio的并发协程, 每个并发协程同时担任消费者和生产者的指责, 通过一个queue(队列)进行交互\n- 系统定义了一个路由(Router), 作为用户唯一需要使用的对象, 路由不需要新建, 系统只有一个Router对象, 直接获取就可以使用\n- 用户所有自定义方法, 都通过路由提供的装饰器来实现, 例如route, 无需创建class或其他复杂的逻辑\n\n\n## 安装\n\nTo install use pip:\n\n    $ pip install aspider\n\nOr clone the repo:\n\n    $ git clone https://github.com/gxtrobot/aspider.git\n    $ python setup.py install\n\n## 开始使用\n\n### 命令行方式\naspider 安装完成后可以直接通过命令 ```aspider``` 使用, 或 ```python -m aspider```\n```\n aspider --help\nusage: aspider [-h] [--max_redirect N] [--max_tries N] [--max_tasks N]\n               [-p PROXY] [-i INCLUDE_REGEX] [-e EXCLUDE_REGEX] [-o OUTPUT]\n               [-c COUNT] [--nostrict] [--no_parse_links] [-v] [-q]\n               [roots [roots ...]]\n\nAn Asyncio Web crawler, version: 0.1.0\n\npositional arguments:\n  roots                 要爬取的网页路径(一般为跟路径), 可以有多个\n\noptional arguments:\n  -h, --help            显示帮助信息\n  --max_tasks N         并发任务数, 默认为5个, 如需要可以提高\n  -p PROXY, --proxy PROXY\n                        代理使用, 例如 http://localhost:1080\n  -i INCLUDE_REGEX, --include INCLUDE_REGEX\n                        需要匹配爬取的网页路径正则, 匹配的网页链接将被加入爬取队列\n  -o OUTPUT, --output OUTPUT\n                        输出的方法, 目前支持这几种[txt, json, stream], stream为命令行输出\n  -c COUNT, --count COUNT\n                        限制爬取的数量, 如指定, 到限额后爬虫自动退出, 队列未完成的会取消\n  --no_parse_links      禁止解析链接, 指定后, 只爬取命令行给定的roots链接, 不会跟踪解\n                        页面包含的链接\n  -v, --verbose         debug 消息数据, 可以有多个v (-v , -vv)\n  -q, --quiet           静默输出, 只打印错误消息\n```\n### 命令行使用例子\n- 豆瓣250电影抓取\n\n运行以下命令, 将会爬取top250的10页html, 并存到json文件中\n\n```\naspider https://movie.douban.com/top250 --include /top250.* --out json\n```\n不过这通常并没太大用处, 因为这样只是抓取完整的html, 只适合有时候需要抓取整个网页, 然后用其他工具处理, 或者不懂python 开发, 可以先用这个抓取网页, 然后用其他工具处理\n\n保存的json文件是json line格式, 就是说每行是一个json对象, 读取时候要相应处理, 每读取一行, 将该行内容转为json对象, python 代码如下\n```\ndef read_json_lines():\n    file = 'top250.json'\n    with open(file) as f:\n        for line in f:\n            d = json.loads(line)\n            print(d.keys()\n```\n\n## API 介绍\n\n### routing 模块 - Router 对象\n\n- route(rule, verify_func=None, no_parse_links=False)\n```\n装饰器, 在用户自定义解析方法使用\n\nArgs:\n  rule: 处理path的正则表达式, 可以包含参数, 使用'<>'包住参数名就可以, 例如 '/page/<no>', 这样用户自定义方法可以接收no参数\n\n  verify_func: 可选, 用户路径验证方法, 如果提供可以自定义是否处理该路径, 方法签名为 verify_url(path), return True or False\n\n  no_parse_links: 可选, 是否处理该页面上的其他链接, 如果需要关闭解析该页面链接, 可以设置为True\n\n用户被装饰方法定义如下:\n\n@router.route('/page/<no>', verify_page_path)\ndef process_page(text, path, no):\n\nArgs:\n    text: 当前要解析页面的html代码, 可以使用其他框架如lxml, request_html, BeautifulSoup\n    path: 可选, 如果有该参数, 系统会自动传递该path参数供使用\n    其他参数, 装饰器的自定义参数\n\n    注意: 前两个参数为系统自动提供, 顺序不能错, 用户自定义参数在前两个之后\n```\n\n### aspider 模块\n该模块提供download方法启动爬虫\n\n- download(loop=None, extra_args=None):\n```\nArgs:\n  loop - 用户提供asyncio loop对象, 一般不需要, 除非是在编写定时任务, 多线程使用, 这时候用户生成一个loop对象并传入\n  extra_args - 其他参数, aspider 命令行使用的参数可以用该dict对象传入\n  其中, 必须的是roots key, 对应为抓取跟路径, 是一个list\n\nReturns:\n  stats - 抓取过程统计对象, 包含抓取的网页数量, 完成时间等数据\n\nexample:\n\n  options = {\n        'roots': ['https://movie.douban.com/top250']\n    }\n  stats = aspider.download(extra_args=options)\n\n```\n\n### reporting 模块\n\n- Stats 对象\n该对象记录抓取过程的统计数据, 如抓取网页数量, 完成时间, 速度等\n\n- report()\n该方法打印这次抓取的记录报告\n\n### API 调用爬虫例子\n\n#### douban top250 电影抓取(代码在example 下))\n1. 先取得router对象\n2. 定义解析方法, 使用router.route装饰器, 提供一个参数为要解析的path正则表达式\n3. 定义一个main方法启动爬虫就可以\n\n```\n'''\n例子: douban top 250 电影名单爬取\n'''\nfrom collections import namedtuple\nfrom aspider.routeing import get_router\nfrom aspider import aspider\nfrom requests_html import HTML\n\nMovie = namedtuple('Movie', ['rank', 'score', 'title'])\n\nrouter = get_router()\n\nroot_url = 'https://movie.douban.com/top250'\n\nmovies_250 = []\n\n\n@router.route('/top250\\?start.+')\ndef process_page(text):\n    html = HTML(html=text)\n    item_css = '#content  ol.grid_view > li'\n    items = html.find(item_css)\n    rank_css = 'em'\n    title_css = '.info  span.title'\n    score_css = '.info  .rating_num'\n    for item in items:\n        rank = int(item.find(rank_css, first=True).text)\n        title = item.find(title_css, first=True).text\n        score = float(item.find(score_css, first=True).text)\n        movies_250.append(Movie(rank, score, title))\n\n\ndef main():\n    options = {\n        'roots': [root_url]\n    }\n    stats = aspider.download(extra_args=options)\n    stats.report()\n    fname = 'top250.txt'\n    sorted_movies_250 = sorted(movies_250, key=lambda m: m.rank)\n    with open(fname, 'w') as f:\n        for movie in sorted_movies_250:\n            print(f'#{movie.rank:<10} {movie.score:<10.2f} - {movie.title}')\n            print(f'#{movie.rank:<10} {movie.score:<10.2f} - {movie.title}', file=f)\n\n\nif __name__ == \"__main__\":\n    main()\n\n```\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/gxtrobot/aspider/tarball/0.1.2",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/gxtrobot/aspider",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "aspider",
    "package_url": "https://pypi.org/project/aspider/",
    "platform": "",
    "project_url": "https://pypi.org/project/aspider/",
    "project_urls": {
      "Download": "https://github.com/gxtrobot/aspider/tarball/0.1.2",
      "Homepage": "https://github.com/gxtrobot/aspider"
    },
    "release_url": "https://pypi.org/project/aspider/0.1.2/",
    "requires_dist": [
      "aiohttp",
      "requests-html"
    ],
    "requires_python": ">=3.6",
    "summary": "a spider using asyncio",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 5963526,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ac49ac01de870212630d33e498e043aa995edeb665f47bfac1f6c90331814f20",
          "md5": "2f5b525c5c37688142eae79e2d0c27e7",
          "sha256": "7da03f9baec45e210efc4e7715b86f97ad6a59742bfb18cfb0eb927b42e27861"
        },
        "downloads": -1,
        "filename": "aspider-0.0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2f5b525c5c37688142eae79e2d0c27e7",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 9425,
        "upload_time": "2019-07-16T15:48:31",
        "upload_time_iso_8601": "2019-07-16T15:48:31.251262Z",
        "url": "https://files.pythonhosted.org/packages/ac/49/ac01de870212630d33e498e043aa995edeb665f47bfac1f6c90331814f20/aspider-0.0.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b7b9e894c0d2ffa0dc9b47b5184be2a7bcf90778a7a0fce6ffcf9911e51ea667",
          "md5": "164550a5f00be2b8510bbd5fc6f5d78d",
          "sha256": "0b230f9503d1612bf02100940f52ba34d933211e5d89cef4a43f87657f91686b"
        },
        "downloads": -1,
        "filename": "aspider-0.0.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "164550a5f00be2b8510bbd5fc6f5d78d",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 13321,
        "upload_time": "2019-09-05T16:14:19",
        "upload_time_iso_8601": "2019-09-05T16:14:19.212963Z",
        "url": "https://files.pythonhosted.org/packages/b7/b9/e894c0d2ffa0dc9b47b5184be2a7bcf90778a7a0fce6ffcf9911e51ea667/aspider-0.0.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "06a14b3aa88e3224e1982d73da964ac5df489b27c2d24df3d86f0d5543d9ad4a",
          "md5": "2405a3427f0e126d4758c86719703a29",
          "sha256": "5e368666ded69bbfac8e667fdce9dc6a014971cdc0cc1f247be03d933bc23f1b"
        },
        "downloads": -1,
        "filename": "aspider-0.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2405a3427f0e126d4758c86719703a29",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.6",
        "size": 15845,
        "upload_time": "2019-10-07T08:03:59",
        "upload_time_iso_8601": "2019-10-07T08:03:59.531931Z",
        "url": "https://files.pythonhosted.org/packages/06/a1/4b3aa88e3224e1982d73da964ac5df489b27c2d24df3d86f0d5543d9ad4a/aspider-0.1.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1d70badeb7f219d9d9f0b78242c14fc7f3d6aacf2bef75fce3e373ad4703e70a",
          "md5": "5c927816dd5ef50eeab61cbc0eb74268",
          "sha256": "313bc2a44aa139c73e8c209196444385f0a2d3e3aef895751165c2c41ad1084f"
        },
        "downloads": -1,
        "filename": "aspider-0.1.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5c927816dd5ef50eeab61cbc0eb74268",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.6",
        "size": 19292,
        "upload_time": "2019-10-09T03:09:51",
        "upload_time_iso_8601": "2019-10-09T03:09:51.537092Z",
        "url": "https://files.pythonhosted.org/packages/1d/70/badeb7f219d9d9f0b78242c14fc7f3d6aacf2bef75fce3e373ad4703e70a/aspider-0.1.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d1850682b9dbd9427fefcae98aa1ebeacdc2cd8fc3532f4146534eaea5808a50",
          "md5": "54fd01fb79ce781aed916c5c4ccce52b",
          "sha256": "d0123f018f4f4c46c7e0faa10d39ae480da5d4ad1783743e731df2bbed518c2f"
        },
        "downloads": -1,
        "filename": "aspider-0.1.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "54fd01fb79ce781aed916c5c4ccce52b",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.6",
        "size": 19307,
        "upload_time": "2019-10-12T08:27:02",
        "upload_time_iso_8601": "2019-10-12T08:27:02.528975Z",
        "url": "https://files.pythonhosted.org/packages/d1/85/0682b9dbd9427fefcae98aa1ebeacdc2cd8fc3532f4146534eaea5808a50/aspider-0.1.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d1850682b9dbd9427fefcae98aa1ebeacdc2cd8fc3532f4146534eaea5808a50",
        "md5": "54fd01fb79ce781aed916c5c4ccce52b",
        "sha256": "d0123f018f4f4c46c7e0faa10d39ae480da5d4ad1783743e731df2bbed518c2f"
      },
      "downloads": -1,
      "filename": "aspider-0.1.2-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "54fd01fb79ce781aed916c5c4ccce52b",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": ">=3.6",
      "size": 19307,
      "upload_time": "2019-10-12T08:27:02",
      "upload_time_iso_8601": "2019-10-12T08:27:02.528975Z",
      "url": "https://files.pythonhosted.org/packages/d1/85/0682b9dbd9427fefcae98aa1ebeacdc2cd8fc3532f4146534eaea5808a50/aspider-0.1.2-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}