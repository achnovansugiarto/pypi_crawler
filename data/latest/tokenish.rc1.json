{
  "info": {
    "author": "Victor Meyer",
    "author_email": "victor_meyer@outlook.fr",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Environment :: Console",
      "Intended Audience :: Developers",
      "Intended Audience :: System Administrators",
      "License :: OSI Approved :: MIT License",
      "Operating System :: MacOS :: MacOS X",
      "Operating System :: POSIX",
      "Programming Language :: Python"
    ],
    "description": "# tokenish\n\n![Code Coverage](https://img.shields.io/badge/Coverage-96%25-brightgreen.svg)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nA simple tool to fill pattern with tokens from file or directory.\n\n- [Installation](#installation)\n- [How to use](#how-to-use)\n- [Example](#example)\n- [Commands](#commands)\n- [License](#license)\n- [Tests](#tests)\n\n## Installation\n\n```sh\npip install tokenish\n```\n\n## How to use\n\n```txt\nusage: tokenish [-h] [-t TOKENS [TOKENS ...]] [-e ENCODING] [-o OUTPUT_DIRECTORY] [-om MAX_FILE_ROWS] pattern\n\nGenerate rows from pattern for each token combinations\n\npositional arguments:\n  pattern               text to fill with links or usernames/passwords\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -t TOKENS [TOKENS ...], --tokens TOKENS [TOKENS ...]\n                        list of tokens file or directory path\n  -e ENCODING, --encoding ENCODING\n                        type of encoding to apply\n  -o OUTPUT_DIRECTORY, --output-directory OUTPUT_DIRECTORY\n                        directory where results will write, print is None\n  -om MAX_FILE_ROWS, --max-file-rows MAX_FILE_ROWS\n                        maximum number of rows per file, default 10 000\n```\n\nAvailable token paths:\n\n- file: all lines in file are consider like tokens.\n- directory: all lines of all files in directory are consider like tokens.\n\nEach token paths should have an associated token in pattern.\nFor example, if two token paths are defined, pattern should contain &TOKEN_0& and &TOKEN_1&.\n\nAvailable encodings:\n\n- base64\n\nThe pattern part to encode must be in the following tag:\n\n```txt\n&ENC[to encode]ODE&\n```\n\n## Example\n\nLet's consider the /tokens directory with the following tree structure:\n\n```txt\n.\n├── passwords\n│   └── passwords_1.txt\n└── usernames\n    ├── usernames_1.txt\n    └── usernames_2.txt\n```\n\nContent of usernames_1.txt:\n\n```txt\nadmin\nroot\nuser\n```\n\nContent of usernames_2.txt:\n\n```txt\nuser1\nuser2\n```\n\nContent of passwords_1.txt:\n\n```txt\n1234\n6712\n```\n\nThe command:\n\n```sh\ntokenish \"Authorization: Basic &TOKEN_0&:&TOKEN_1&\" -t usernames/ passwords/passwords_1.txt\n```\n\nPrint:\n\n```txt\nAuthorization: Basic admin:1234\nAuthorization: Basic admin:6712\nAuthorization: Basic root:1234\nAuthorization: Basic root:6712\nAuthorization: Basic user:1234\nAuthorization: Basic user:6712\nAuthorization: Basic user1:1234\nAuthorization: Basic user1:6712\nAuthorization: Basic user2:1234\nAuthorization: Basic user2:6712\n```\n\nYou can add an encoding like:\n\n```sh\ntokenish \"Authorization: Basic &ENC[&TOKEN_0&:&TOKEN_1&]ODE&\" -t usernames/ passwords/passwords_1.txt -e base64\n```\nPrint:\n\n```txt\nAuthorization: Basic YWRtaW46MTIzNA==\nAuthorization: Basic YWRtaW46NjcxMg==\nAuthorization: Basic cm9vdDoxMjM0\nAuthorization: Basic cm9vdDo2NzEy\nAuthorization: Basic dXNlcjoxMjM0\nAuthorization: Basic dXNlcjo2NzEy\nAuthorization: Basic dXNlcjE6MTIzNA==\nAuthorization: Basic dXNlcjE6NjcxMg==\nAuthorization: Basic dXNlcjI6MTIzNA==\nAuthorization: Basic dXNlcjI6NjcxMg==\n```\n\n## Commands\n\nPrint all combination of username/password, where usernames replace &TOKEN_0& and passwords replace &TOKEN_1&.\n\n```sh\ntokenish \"Authorization: Basic &TOKEN_0&:&TOKEN_1&\" -t /path/to/usernames/dir/ /path/to/passwords.txt\n```\n\nSave all combination of username/password in directory /path/output/ composed by files of 10 000 lines.\n\n```sh\ntokenish \"Authorization: Basic &TOKEN_0&:&TOKEN_1&\" -t /path/to/usernames/dir/ /path/to/passwords.txt -o /path/output/\n```\n\nSave all combination of username/password in directory /path/output/ composed by files of 500 lines.\n\n```sh\ntokenish \"Authorization: Basic &TOKEN_0&:&TOKEN_1&\" -t /path/to/usernames/dir/ /path/to/passwords.txt -o /path/output/ -om 500\n```\n\nPrint all combination of username/password, where usernames replace &TOKEN_0& and passwords replace &TOKEN_1&.\nExpression in tag &ENC[...]ODE& will encode in base64.\n\n```sh\ntokenish \"Authorization: Basic &ENC[&TOKEN_0&:&TOKEN_1&]ODE&\" -t /path/to/usernames/dir/ /path/to/passwords.txt -e base64\n```\n\n## License\n\ntokenish is released under MIT license. See [LICENSE](https://github.com/VictorMeyer77/tokenish/blob/main/LICENSE).\n\n## Tests\n\nRun these commands to run tests and update coverage badge:\n\n```sh\npip install readme-coverage-badger\ncoverage run -m --omit \"/usr/local/*,/usr/lib/*,tests/*,venv/*\" unittest discover tests/tokenish\nreadme-cov\n```\n# Changelog\n\nAll notable changes to this project will be documented in this file.\n\nLog types:\n\n- `added` for new features.\n- `changed` for changes in existing functionality.\n- `fixed` for any bug fixes.\n- `removed` for features removed in this release.\n- `security` to invite users to upgrade in case of vulnerabilities.\n\n## [Unreleased]\n\n### Added\n### changed\n### Fixed\n### removed\n### security\n\n## [0.1](https://github.com/VictorMeyer77/tokenish/releases/tag/0.1) - 2023/03/02\n\n### Added\n\n- Initial release\n\nMIT License\n\nCopyright (c) 2023 Victor Meyer\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
    "description_content_type": "text/markdown; charset=UTF-8",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/VictorMeyer77/tokenish",
    "keywords": "token encode pattern sh",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tokenish",
    "package_url": "https://pypi.org/project/tokenish/",
    "platform": "any",
    "project_url": "https://pypi.org/project/tokenish/",
    "project_urls": {
      "Bug Tracker": "https://github.com/VictorMeyer77/tokenish/issues",
      "Changelog": "https://github.com/VictorMeyer77/tokenish/blob/main/CHANGELOG.md",
      "Documentation": "https://github.com/VictorMeyer77/tokenish/blob/main/README.md",
      "Homepage": "https://github.com/VictorMeyer77/tokenish",
      "Source": "https://github.com/VictorMeyer77/tokenish"
    },
    "release_url": "https://pypi.org/project/tokenish/0.1/",
    "requires_dist": null,
    "requires_python": "<4,>=3.6",
    "summary": "A simple tool to fill pattern with tokens from file or directory.",
    "version": "0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17135598,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9b4c562f6792d6e116f7062da02572c274e37fe69a5211275c95f35357bd4008",
          "md5": "a3079026e2792b49b6d648f42f68d180",
          "sha256": "ff943e443f3675cbe16fbb985bf46d9bd3b07acdd4e6253507ccccb763665485"
        },
        "downloads": -1,
        "filename": "tokenish-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a3079026e2792b49b6d648f42f68d180",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.6",
        "size": 7749,
        "upload_time": "2023-03-02T22:46:56",
        "upload_time_iso_8601": "2023-03-02T22:46:56.256690Z",
        "url": "https://files.pythonhosted.org/packages/9b/4c/562f6792d6e116f7062da02572c274e37fe69a5211275c95f35357bd4008/tokenish-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6e839f9e9997d33fa702ad62aa2d9247c313c389c0a0085783dcbdf0d37ce786",
          "md5": "6b5fc494a679e8e2681e2c66687c2e96",
          "sha256": "2431435c8c5dd3d7f10eedc2f4025e622bb21466a0718296dfd03c4c5b228508"
        },
        "downloads": -1,
        "filename": "tokenish-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "6b5fc494a679e8e2681e2c66687c2e96",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "<4,>=3.6",
        "size": 8285,
        "upload_time": "2023-03-02T22:46:58",
        "upload_time_iso_8601": "2023-03-02T22:46:58.266451Z",
        "url": "https://files.pythonhosted.org/packages/6e/83/9f9e9997d33fa702ad62aa2d9247c313c389c0a0085783dcbdf0d37ce786/tokenish-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9b4c562f6792d6e116f7062da02572c274e37fe69a5211275c95f35357bd4008",
        "md5": "a3079026e2792b49b6d648f42f68d180",
        "sha256": "ff943e443f3675cbe16fbb985bf46d9bd3b07acdd4e6253507ccccb763665485"
      },
      "downloads": -1,
      "filename": "tokenish-0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a3079026e2792b49b6d648f42f68d180",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "<4,>=3.6",
      "size": 7749,
      "upload_time": "2023-03-02T22:46:56",
      "upload_time_iso_8601": "2023-03-02T22:46:56.256690Z",
      "url": "https://files.pythonhosted.org/packages/9b/4c/562f6792d6e116f7062da02572c274e37fe69a5211275c95f35357bd4008/tokenish-0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6e839f9e9997d33fa702ad62aa2d9247c313c389c0a0085783dcbdf0d37ce786",
        "md5": "6b5fc494a679e8e2681e2c66687c2e96",
        "sha256": "2431435c8c5dd3d7f10eedc2f4025e622bb21466a0718296dfd03c4c5b228508"
      },
      "downloads": -1,
      "filename": "tokenish-0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "6b5fc494a679e8e2681e2c66687c2e96",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "<4,>=3.6",
      "size": 8285,
      "upload_time": "2023-03-02T22:46:58",
      "upload_time_iso_8601": "2023-03-02T22:46:58.266451Z",
      "url": "https://files.pythonhosted.org/packages/6e/83/9f9e9997d33fa702ad62aa2d9247c313c389c0a0085783dcbdf0d37ce786/tokenish-0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}