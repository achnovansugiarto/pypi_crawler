{
  "info": {
    "author": "e-dorigatti",
    "author_email": "emilio.dorigatti@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "Hyperband on Snakemake\n===\n\nDo you have a medium-scale hyper-parameter search to run, and need a way to\norchestrate this process? This is the solution. Based on Hyperband [1] and\nSnakemake [2], this tool will orchestrate the whole process for you. How is it\ndifferent from the other implementations you can easily find elsewhere?\n\n 1. The exact configuration used for each hyper-parameter setting is safely\n    stored on disk.\n 2. All intermediate results are saved, too.\n 3. Thanks to Snakemake, training can be offloaded to a cluster manager such as\n    Slurm (see example below), enabling effortless and _massive_ parallel training.\n 4. Even though this tool is written in Python, you can launch training scripts\n    made with _any_ technology: they only need to read a configuration file and\n    write a file with a numerical result.\n\nSaving everything on disk proves advantageous when your hyper-parameter search\nmust run for weeks, as it allows you to check the progress, spot early mistakes,\nperform further iterations after the process ends (e.g., shoot for the very best\nconfiguration through Bayesian optimization, using the hyper-band results to\nbootstrap that model). Importantly, by decoupling training and search, you do\nnot risk losing all your work because of some random bug that would crash the\ntraining process.\n\nNote: to use this tool, you must adapt the included templates to your system.\nSee section _Customization_ below for instructions.\n\n# Installation\nCan be installed via pip:\n\n```\npip install git+https://github.com/e-dorigatti/hyperband-snakemake\n```\n\nOr clone the repo, create a conda environment (optional) and install the\nrequired packages:\n\n```\n> conda create --file packages.txt -c conda-forge -c bioconda -n hyperband-snakemake\n> conda activate hyperband-snakemake\n```\n\n# Example\nBriefly, Hyperband is a random hyper-parameter search algorithm that smartly\nallocates budget to promising configurations. This is done by running all\nconfigurations with a small budget, then obtaining the top half/third/fourth and\nrunning them with two/three/four times larger budget, so that the cost for each\nstage remains the same. To know whether it is better to start with many\nconfigurations and small budget, or fewer initial configuration with larger\nbudget, Hyperband creates several brackets with different trade-offs. A\npractical example follows.\n\nFirst, obtain the plan of the hyper-parameter search. Suppose we want to\nevaluate each hyper-parameter configuration by performing ten-folds\ncross-validation repeated two times. Further, suppose we want to advance the top\nthird of configurations to the next stage (the suggested option), and can spare\n5**3=243 units of budget for each bracket. We set one budget unit to correspond\nto one training epoch, and know that our model requires 10 seconds (0.0028\nhours) to perform one epoch on the dataset. These parameters translate to the\nfollowing search structure:\n\n```\n> python -m hyperband_snakemake generate 5 3 \\\n    --repetitions 2 --folds 10 \\\n    --guaranteed-budget 3 \\\n    --cost-one-epoch-full-dataset 0.0028 \\\n    --output-dir my-search \\\n    --random-seed 123456\n\nHyperband Search (cost: 426.23)\n  Bracket 0 (cost: 73.48)\n    Stage 0 - 243 configurations each with budget 1.0 (cost: 12.25)\n    Stage 1 - 81 configurations each with budget 3.0 (cost: 12.25)\n    Stage 2 - 27 configurations each with budget 9.0 (cost: 12.25)\n    Stage 3 - 9 configurations each with budget 27.0 (cost: 12.25)\n    Stage 4 - 3 configurations each with budget 81.0 (cost: 12.25)\n    Stage 5 - 1 configurations each with budget 243.0 (cost: 12.25)\n  Bracket 1 (cost: 67.44)\n    Stage 0 - 98 configurations each with budget 3.0 (cost: 14.82)\n    Stage 1 - 32 configurations each with budget 9.0 (cost: 14.52)\n    Stage 2 - 10 configurations each with budget 27.0 (cost: 13.61)\n    Stage 3 - 3 configurations each with budget 81.0 (cost: 12.25)\n    Stage 4 - 1 configurations each with budget 243.0 (cost: 12.25)\n  Bracket 2 (cost: 64.86)\n    Stage 0 - 41 configurations each with budget 9.0 (cost: 18.60)\n    Stage 1 - 13 configurations each with budget 27.0 (cost: 17.69)\n    Stage 2 - 4 configurations each with budget 81.0 (cost: 16.33)\n    Stage 3 - 1 configurations each with budget 243.0 (cost: 12.25)\n  Bracket 3 (cost: 73.48)\n    Stage 0 - 18 configurations each with budget 27.0 (cost: 24.49)\n    Stage 1 - 6 configurations each with budget 81.0 (cost: 24.49)\n    Stage 2 - 2 configurations each with budget 243.0 (cost: 24.49)\n  Bracket 4 (cost: 73.48)\n    Stage 0 - 9 configurations each with budget 81.0 (cost: 36.74)\n    Stage 1 - 3 configurations each with budget 243.0 (cost: 36.74)\n  Bracket 5 (cost: 73.48)\n    Stage 0 - 6 configurations each with budget 243.0 (cost: 73.48)\n```\n\nWhere each stage uses the best configurations in the previous stage of the same\nbracket. The indicated cost is in hours (the same unit as the parameter on the\ncommand line), and considers the cross-validation structure (with ten folds, one\nepoch requires about 90% of the time needed for the full dataset, i.e. nine\nseconds. One budget unit then equals 2x10x9=180 seconds, which translates to\n12). The wall-clock time can be of course reduced with parallel training.\n\nThis generator will also create a folder hierarchy under `my-search`, splitting\nbrackets, stages, and configurations. Each folder will contain one random\nconfiguration:\n\n```\n> cat my-search/bracket-2/stage-0/config-16/config\nfolds: 10\nrepetitions: 2\ncv_seed: 109090\nlearning_rate: 0.001\nC: 0.01\nsolver: saga\npenalty: l1\n```\n\nThe whole process can now be run via Snakemake:\n\n```\nsnakemake --snakefile my-search/Snakefile\n```\n\nThe example provided fits a logistic regression model from scikit-learn to the\niris dataset, using one iteration per budget unit. The entire search should require\na few minutes (can be sped up via e.g. `--cores 8`) and result in an accuracy of\n98% (averaged across folds and repetitions), visible from one of the last lines\nof the log:\n\n```\nJob counts:\n        count   jobs\n        1       find_overall_best\n        1\n[('my-search/bracket-1/stage-4/config-0/result', -0.9800000000000001), ...\n```\n\nThe best configuration is also saved in the search root directory (`my-search/config`).\n\nNow, paired to each configuration, you find the negative average accuracy (the\nSnakefile is designed to find the configuration with the minimum result):\n\n```\n> cat my-search/bracket-2/stage-0/config-16/result\n-0.5666666666666667\n```\n\nAnd log file:\n\n```\n> cat my-search/bracket-2/stage-0/config-16/log.log\nINFO:root:Invoked with configuration file dev/sample-hb/bracket-2/stage-0/config-16/config\nINFO:root:Invoked with budget 9\nINFO:root:Loaded configuration: {'folds': 10, 'repetitions': 2, 'cv_seed': 109090, 'learning_rate': 0.001, 'C': 0.01, 'solver': 'saga', 'penalty': 'l1'}\nINFO:root:Accuracy of repetition 0, fold 0: 0.467\nINFO:root:Accuracy of repetition 0, fold 1: 0.533\nINFO:root:Accuracy of repetition 0, fold 2: 0.667\n...\nINFO:root:Accuracy of repetition 1, fold 7: 0.467\nINFO:root:Accuracy of repetition 1, fold 8: 0.533\nINFO:root:Accuracy of repetition 1, fold 9: 0.667\nINFO:root:Mean accuracy: 0.567\n```\n\nNote that, since we used random seeds throughout, these results should be fully\nreproducible.\n\n# Visualizing search status\nSince a Hyperband search can take many days, a simple utility to see the ongoing\nprogress is provided:\n\n```\n> python -m hyperband_snakemake status my-search\nBracket 0 - Stages completed: 0\n  Stage 0 - 81 configurations\n    | Completed (C) | Failed (F) | In progress (R) | Pending (.) | Total |\n    |            12 |        4   |               8 |          57 |    81 |\n\n      ...F. ...C. .CC.. ..CC. ....R\n      ..C.. ..C.. ..... CRR.C C...C\n      ..RR. .F... .R... RF... CF...\n      ...R. .\n\n  Top completed configuration(s):\n    1. 0.1508 - Conf. 68\n    2. 0.1590 - Conf. 50\n    3. 0.1624 - Conf. 56\n\nBracket 1 - Stages completed: 0\n\n(output truncated)\n```\n\nThis will simply scan the directory looking for configuration or result files\nindicating progress. A configuration is deemed \"in progress\" if its folder does not\ncontain the result file, but contains files or folders other than the configuration\nitself, such as log files, TensorBoard's summary folders, etc. A configuration is\ndeemed \"failed\" if the result file contains `nan` or `inf`. Note that configurations\nthat terminated abnormally without writing such a result file, e.g. because of an\nunhandled exception, are still counted as \"in progress\". You should check Snakemake's\nlogs to determine whether a configuration is still running or has failed.\n\n# Customization\nIn its present state, the generator script creates by default the logistic\nregression example just explained, but adapting it to your needs is easy. This\nscript uses [Jinja2][jinja] to render three templates:\n\n 1. The [Snakefile][snake-tmpl], containing the logic to run the script and\n    promote good configurations to the next stage.\n 2. The [bash launch script][run-tmpl] that is invoked by the Snakefile and runs\n    the [sample training script][train-tmpl]. The training script should take as\n    arguments the configuration file and the budget, and write the result to a\n    file named `result` in the same directory as the configuration file (if you\n    are maximizing a metric, write its negation instead).\n 3. The [random configuration][config-tmpl] that is read by the training script.\n    Thanks to Jinja2, the actual generation of random parameters is contained in\n    the template file itself. There can be some custom logic, e.g. in the\n    provided example the type of regularization (L1 or L2) is chosen based on\n    the solver (SAGA or LBFGS). You are of course not restricted to any\n    particular type of configuration, as long as it is text-based. For example,\n    you can directly produce a [Python configuration file][python-cfg] and\n    import it in the training script (if you are using Python, that is). This\n    would save you the effort of writing code to read the configuration and\n    instantiate the specified model.\n\nThese templates are rendered and saved in the output directory. This directory\nis now entirely portable, you just have to update `config['base_dir']` in the\nSnakefile. This variable should contain the path of the output dir, either\nabsolute or relative to where you will run `snakemake`.\n\nYou can pass `--template-dir` to the generator to make it use your custom\ntemplates. Simply create a copy of the provided directory, modify to your needs,\nand run the search:\n\n```\n> cp -r hyperband_snakemake/templates my-templates\n> vim my-templates/config\n> vim my-templates/run.sh\n> vim my-templates/Snakefile\n> python generator.py --template-dir my-templates ...\n> snakemake ...\n```\n\n# Running with Slurm\nOnce you have customized the configuration template, it is quite easy to run a\nhyperband search on a cluster managed by [slurm][slurm], you just need to modify\nthe [launch script template][run-tmpl] to invoke `srun`, like so:\n\n```\nsrun --time 24:00:00 --gpus-per-task 1 --cpus-per-task 6 --mem 92G \\\n    --output \"$1/slurm-%j.out\" --error \"$1/slurm-%j.err\" \\\n    <path-to-python> train.py \"$1/config\" --output-dir \"$1\" --max-epochs \"$2\"\n```\n\nWhere `<path-to-python>` points to the python interpreter in a suitable virtual\nenvironment (but note that you are not restricted to use Python!).\n\nAnother option is to use `sbatch` combined with `--wait`, which has the\nadvantage that you can use a heredoc:\n\n```\nsbatch <options> --wait << EOF\n#!/bin/bash\nconda activate <env>\npython train.py \"$1/config\" --output-dir \"$1\" --max-epochs \"$2\"\nEOF\n```\n\nThen, use `nohup` (or tmux, or screen) to fire-and-forget Snakemake from the\nlogin node:\n\n```\n> nohup snakemake --snakefile my-search/Snakefile --latency-wait 60 -j 100 > my-search/log.out &\n```\n\nWhich will schedule up to 100 jobs at the same time. `latency-wait` tells\nSnakemake to wait for the result files for up to 60 seconds before failing the\njob. It may be necessary to account for possible latencies when the results\nsaved in a networked file-system.\n\n\n# References\n[1]: Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A. and Talwalkar, A., 2017. _Hyperband: A novel bandit-based approach to hyperparameter optimization._ The Journal of Machine Learning Research, 18(1), pp.6765-6816. http://www.jmlr.org/papers/volume18/16-558/16-558.pdf\n\n[2]: Köster, Johannes and Rahmann, Sven. _Snakemake - A scalable bioinformatics workflow engine_. Bioinformatics 2012. https://academic.oup.com/bioinformatics/article/28/19/2520/290322\n\n [jinja]: https://jinja.palletsprojects.com/en/2.11.x/\n [run-tmpl]: hyperband_snakemake/templates/run.sh\n [snake-tmpl]: hyperband_snakemake/templates/Snakefile\n [train-tmpl]: hyperband_snakemake/example/train.py\n [config-tmpl]: hyperband_snakemake/templates/config\n [python-cfg]: https://martin-thoma.com/configuration-files-in-python/#python-configuration-file\n [slurm]: https://slurm.schedmd.com/overview.html",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/e-dorigatti/hyperband-snakemake/",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hyperband-snakemake",
    "package_url": "https://pypi.org/project/hyperband-snakemake/",
    "platform": "",
    "project_url": "https://pypi.org/project/hyperband-snakemake/",
    "project_urls": {
      "Homepage": "https://github.com/e-dorigatti/hyperband-snakemake/"
    },
    "release_url": "https://pypi.org/project/hyperband-snakemake/0.0.2/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Orchestrate hyper-parameters search with snakemake",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8012415,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "adf940675779b78fa6bc35546ff943b14da5de0ce76661844bd1880e299caa9b",
          "md5": "b365d8366aea64fcabf09a421cb442e9",
          "sha256": "e6a3cf570dbc4bf0a72e4dc8b2b3b8c993195c0ee43d88704dc2012da624926a"
        },
        "downloads": -1,
        "filename": "hyperband_snakemake-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b365d8366aea64fcabf09a421cb442e9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 22080,
        "upload_time": "2020-05-29T09:33:42",
        "upload_time_iso_8601": "2020-05-29T09:33:42.522880Z",
        "url": "https://files.pythonhosted.org/packages/ad/f9/40675779b78fa6bc35546ff943b14da5de0ce76661844bd1880e299caa9b/hyperband_snakemake-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "817841e86a10e425efb6b52a31af8415e3210cec16024ffe26222ffe54016be6",
          "md5": "48993a67a16e05c241f5cf8d1ad42d55",
          "sha256": "30dfb41e94236d6368d5a15f2031dd5c3eb7714cc79f291abafdfd1b7bac1bba"
        },
        "downloads": -1,
        "filename": "hyperband-snakemake-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "48993a67a16e05c241f5cf8d1ad42d55",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 10819,
        "upload_time": "2020-05-29T09:33:44",
        "upload_time_iso_8601": "2020-05-29T09:33:44.894893Z",
        "url": "https://files.pythonhosted.org/packages/81/78/41e86a10e425efb6b52a31af8415e3210cec16024ffe26222ffe54016be6/hyperband-snakemake-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cd128d8efddb7a09d0a7d4995addf3f58873e9f5ee53c3dcdd7a39df404b26e8",
          "md5": "62d35d61550e795a88dc172ce6475bbe",
          "sha256": "4a240d7abce8562f6bbc166993e05476b50cac0c81e49465c93f369c93189c99"
        },
        "downloads": -1,
        "filename": "hyperband-snakemake-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "62d35d61550e795a88dc172ce6475bbe",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 16627,
        "upload_time": "2020-08-21T13:32:48",
        "upload_time_iso_8601": "2020-08-21T13:32:48.064162Z",
        "url": "https://files.pythonhosted.org/packages/cd/12/8d8efddb7a09d0a7d4995addf3f58873e9f5ee53c3dcdd7a39df404b26e8/hyperband-snakemake-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cd128d8efddb7a09d0a7d4995addf3f58873e9f5ee53c3dcdd7a39df404b26e8",
        "md5": "62d35d61550e795a88dc172ce6475bbe",
        "sha256": "4a240d7abce8562f6bbc166993e05476b50cac0c81e49465c93f369c93189c99"
      },
      "downloads": -1,
      "filename": "hyperband-snakemake-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "62d35d61550e795a88dc172ce6475bbe",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 16627,
      "upload_time": "2020-08-21T13:32:48",
      "upload_time_iso_8601": "2020-08-21T13:32:48.064162Z",
      "url": "https://files.pythonhosted.org/packages/cd/12/8d8efddb7a09d0a7d4995addf3f58873e9f5ee53c3dcdd7a39df404b26e8/hyperband-snakemake-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}