{
  "info": {
    "author": "Eleanor Holland",
    "author_email": "holland.dwight@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Scientific/Engineering :: Mathematics",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# PyTorch-SM3\n[[source](https://arxiv.org/abs/1901.11150)]\n[[TensorFlow](https://github.com/google-research/google-research/tree/master/sm3)]\n[[notebook](./SM3_example.ipynb)]\n\nImplements the SM3-II adaptive optimization algorithm for PyTorch.\nThis algorithm was designed by Rohan Anil, Vineet Gupta, Tomer Koren, and\nYoram Singer and implemented in TensorFlow.\n\nThe 'Square-root of Minima of Sums of Maxima of Squared-gradients Method'\n(SM3) algorithm is a memory-efficient adaptive optimization algorithm similar\nto Adam and Adagrad with greatly reduced memory usage for history tensors.\nFor an `n x m` matrix, Adam and Adagrad use `O(nm)` memory for history\ntensors, while SM3 uses `O(n+m)` due to the chosen cover. In general, a tensor\nof shape `(n_1, n_2, ..., n_k)` optimized using Adam will use `O(prod n_i)`\nmemory for storage tensors, while the optimization using SM3 will use\n`O(sum n_i)` memory. Despite storing fewer parameters, this optimization\nalgorithm manages to be comparably effective.\n\nThis advantage drastically shrinks when `momentum > 0`. The momentum is\ntracked using a tensor of the same shape as the tensor being optimized. With\nmomentum, SM3 will use just over half as much memory as Adam, and a bit more\nthan Adagrad.\n\nIf the gradient is sparse, then the optimization algorithm will use `O(n_1)`\nmemory as there is only a row cover. The value of `momentum` is ignored in\nthis case.\n\n## Installing\nTo install with `pip`, you can use `pip install torch-SM3`. Alternatively,\nclone the repository and run `python setup.py sdist` and install using the\ngenerated source package.\n\n## Usage\nAfter installing, import the optimizer using `from SM3 import SM3`. The `SM3`\noptimizer that is imported can be used exactly the same way a PyTorch\noptimizer. For example, the optimizer can be constructed using\n`opt = SM3(model.parameters())` with parameter updates being applied using\n`opt.step()`.\n\n## Implementation Differences\nThe algorithm presented by the original authors mentions that the optimization\nalgorithm can be modified to use exponential moving averages. I incorporated\nthis into the optimizer. If `beta = 0`, then the accumulated gradient squares\nmethod (i.e. the default SM3 method) is used. If `beta > 0`, then the updates\nuse exponential moving averages instead. The authors found that `beta = 0` \nwas superior for their experiments in translation and language models.\n\n## Requirements\nThe requirements given in `requirements.txt` are not the absolute minimum -\nthe optimizer may function for earlier versions of PyTorch than 1.4. However,\nthese versions are not tested against. Furthermore, a change in the backend\n`C++` signatures means that the current version of this package may not run\nagainst earlier versions of PyTorch.\n\n# Wisdom from authors\nTheir full advice can be seen in the sources above. Here are two points they\nemphasize and how to incorporate them.\n\n## Learning rate warm-up\nThey prefer using a learning rate that quadratically ramps up to the\nfull learning rate. This is done in the notebook linked above by using the\n`LambdaLR` class. After creating the optimizer, you can use the following:\n```\nlr_lambda = lambda epoch: min(1., (epoch / warm_up_epochs) ** 2)\nscheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n```\nThe authors advocate for this as they found that the early gradients were\ntypically very large in magnitude. By using a warm-up, the accumulated\ngradients are not dominated by the first few updates. After this warm-up,\nthey do not adjust the learning rate.\n\n## Learning rate decay\n[Polyak averaging](https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage)\ncan be useful for training models as the moving average of the parameters\ncan produce better results than the parameters themselves. As this can be\ncostly in memory, an alternative they present is to ramp the learning rate\ndecay to 0 in the last 10% of steps. This can also be achieved using the\n`LambdaLR` class with the following `lambda` function:\n```\nlr_lambda = lambda epoch: min(1., (total_epochs - epoch) / (0.1 * total_epochs))\n```\nTo incorporate both warm-up and decay, we can combine the two functions:\n```\nlr_lambda = lambda epoch: min(1., (epoch / (warm_up_epochs)) ** 2, (epochs - epoch) / (0.1 * epochs))\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Enealor/PyTorch-SM3",
    "keywords": "",
    "license": "Apache-2",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torch-SM3",
    "package_url": "https://pypi.org/project/torch-SM3/",
    "platform": "",
    "project_url": "https://pypi.org/project/torch-SM3/",
    "project_urls": {
      "Homepage": "https://github.com/Enealor/PyTorch-SM3"
    },
    "release_url": "https://pypi.org/project/torch-SM3/0.1.0/",
    "requires_dist": [
      "torch"
    ],
    "requires_python": "",
    "summary": "Adds the memory efficient SM3 optimizer to PyTorch.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7682597,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "90a75f3404c066907f3c92ed6d653980c7ff18cc587758bafaca7fdc0b658612",
          "md5": "dd9adae761807f402ccc8dd5a8732d84",
          "sha256": "5c214703aaac68d5c4511badc8bbc0d093c690e8524f156462ad98fadfd9509f"
        },
        "downloads": -1,
        "filename": "torch_SM3-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dd9adae761807f402ccc8dd5a8732d84",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 9889,
        "upload_time": "2020-07-11T22:44:45",
        "upload_time_iso_8601": "2020-07-11T22:44:45.538432Z",
        "url": "https://files.pythonhosted.org/packages/90/a7/5f3404c066907f3c92ed6d653980c7ff18cc587758bafaca7fdc0b658612/torch_SM3-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "caab4af17fb552caacde153c99e5098b42a31e264a38b8ee2ca7685b31bddb5d",
          "md5": "dea0dbcd88c2715f4fee67f3cfb7ad39",
          "sha256": "2baec737b8144f70ec83f1831d89e53d72965c4637689ed895738a21568acfb0"
        },
        "downloads": -1,
        "filename": "torch-SM3-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "dea0dbcd88c2715f4fee67f3cfb7ad39",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5667,
        "upload_time": "2020-07-11T22:44:47",
        "upload_time_iso_8601": "2020-07-11T22:44:47.378190Z",
        "url": "https://files.pythonhosted.org/packages/ca/ab/4af17fb552caacde153c99e5098b42a31e264a38b8ee2ca7685b31bddb5d/torch-SM3-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "90a75f3404c066907f3c92ed6d653980c7ff18cc587758bafaca7fdc0b658612",
        "md5": "dd9adae761807f402ccc8dd5a8732d84",
        "sha256": "5c214703aaac68d5c4511badc8bbc0d093c690e8524f156462ad98fadfd9509f"
      },
      "downloads": -1,
      "filename": "torch_SM3-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "dd9adae761807f402ccc8dd5a8732d84",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 9889,
      "upload_time": "2020-07-11T22:44:45",
      "upload_time_iso_8601": "2020-07-11T22:44:45.538432Z",
      "url": "https://files.pythonhosted.org/packages/90/a7/5f3404c066907f3c92ed6d653980c7ff18cc587758bafaca7fdc0b658612/torch_SM3-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "caab4af17fb552caacde153c99e5098b42a31e264a38b8ee2ca7685b31bddb5d",
        "md5": "dea0dbcd88c2715f4fee67f3cfb7ad39",
        "sha256": "2baec737b8144f70ec83f1831d89e53d72965c4637689ed895738a21568acfb0"
      },
      "downloads": -1,
      "filename": "torch-SM3-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "dea0dbcd88c2715f4fee67f3cfb7ad39",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 5667,
      "upload_time": "2020-07-11T22:44:47",
      "upload_time_iso_8601": "2020-07-11T22:44:47.378190Z",
      "url": "https://files.pythonhosted.org/packages/ca/ab/4af17fb552caacde153c99e5098b42a31e264a38b8ee2ca7685b31bddb5d/torch-SM3-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}