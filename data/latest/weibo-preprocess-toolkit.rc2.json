{
  "info": {
    "author": "wansho",
    "author_email": "wanshojs@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# weibo-preprocess-toolkit\nWeibo Preprocess Toolkit\n\n## Getting Started\n\n### Installation\n\n```shell\npip install weibo-preprocess-toolkit\n```\n\n### Tutorial\n\n```Python\nfrom weibo_preprocess_toolkit import WeiboPreprocess\n\npreprocess = WeiboPreprocess()\n\ntest_weibo = \"所以我都不喝蒙牛 #南京·大行宫[地点]#，一直不喜欢蒙牛。謝駿毅 赞[122]转发[11] [超话] 收藏09月11日 18:57 \"\n\n# traditional2simplified\nprint(preprocess.traditional2simplified(test_weibo))\n# 所以我都不喝蒙牛 #南京·大行宫[地点]#，一直不喜欢蒙牛。谢骏毅 赞[122]转发[11] [超话] 收藏09月11日 18:57\n\n# clean weibo with simplified Chinese\nprint(preprocess.clean(test_weibo))\n# 所以我都不喝蒙牛 一直不喜欢蒙牛 谢骏毅\n\n# clean weibo \nprint(preprocess.clean(test_weibo, simplified=False))\n# 所以我都不喝蒙牛 一直不喜欢蒙牛 謝駿毅\n\n# seg weibo, keep stop words\nprint(preprocess.cut(test_weibo))\n# ['所以', '我', '都', '不喝', '蒙牛', '#', '南京', '·', '大行宫', '[', '地点', ']', '#', '，', '一直', '不喜欢', '蒙牛', '。', '謝駿毅', '赞', '[', '122', ']', '转发', '[', '11', ']', '[', '超话', ']', '收藏', '09', '月', '11', '日', '18', ':', '57', '\\xa0']\n\n# seg weibo, don't keep stop words\nprint(preprocess.cut(test_weibo, keep_stop_word=False))\n# ['都', '不喝', '蒙牛', '#', '南京', '·', '大行宫', '[', '地点', ']', '#', '，', '不喜欢', '蒙牛', '。', '謝駿毅', '赞', '[', '122', ']', '转发', '[', '11', ']', '[', '超话', ']', '收藏', '09', '月', '11', '日', '18', ':', '57', '\\xa0']\n\n# clean and cut weibo, keep_stop_words, simplified Chinese\nprint(preprocess.preprocess(test_weibo))\n# 所以 我 都 不喝 蒙牛 一直 不喜欢 蒙牛 谢骏毅\nprint(preprocess.preprocess(test_weibo, simplified=False, keep_stop_word=False))\n# 都 不喝 蒙牛 不喜欢 蒙牛 謝駿毅\n```\n\n\n## Introduction\n该工具用于微博文本的预处理：清洗 + 分词。\n\n### Inspiration\n在中文 NLP 领域，文本清洗和分词对于模型的性能有着很大的影响，如果语料库和测试集/**线上环境**的文本清洗规则和分词工具不同，就会导致在语料库上训练出来的模型在测试集上效果很差。举例来说，语料库采用了清洗规则 Clean-A 和 分词工具 Seg-A 来清洗和分词微博，而用户在线上环境采用了另一种清洗规则 Clean-B 和另一种分词工具 Seg-B，那么线上环境就会产生很多不在语料库词典中的**未登陆词（Unknown Words）**，这些未登陆词会导致预先训练好的模型，面对线上环境的另一种规则时，性能变差。\n\n本人在对微博进行情感分析的过程中，总结了较多的微博清洗技巧和分词规则，并总结了一份微博情感分析词典用于优化 jieba 分词。所以我在这里尝试对微博的清洗和分词规则进行整理，同时也是为了保持语料库和线上环境的规则同步，为其他研究者和使用我的模型的人，提供一个和语料库匹配的清洗和分词规则。\n\n### Weibo Cleaning\n\n本人对微博文本的清洗规则进行了整理，主要涉及到如下的规则：\n\n1. 中文繁体转简体\n2. [微博停用词规则1(正则表达式)](weibo_preprocess_toolkit/dictionary/weibo_stopwords1_regex.csv)，包括 url, email, @某人, 地点，…… 等停用词规则\n3. [微博停用词规则2(正则表达式)](weibo_preprocess_toolkit/dictionary/weibo_stopwords2_regex.csv)，包括 时间，数字和微博中常出现的无意义的词等停用词规则\n4. [微博特殊字符](weibo_preprocess_toolkit/dictionary/special_chars.csv)\n5. 其他细节处理\n\n注意：考虑到停用词在词向量训练中衔接上下文的作用，本工具并没有对微博的停用词进行清洗\n\n### Weibo Seg\n\n基于 jieba 分词对微博文本进行分词优化，优化的地方主要有两点：\n\n1. 扩种 jieba 分词词典，构建情感词典，优化情感分析的分词结果\n2. 对否定前缀词进行特殊处理\n\n## Dependencies\n```bash\npip install jieba\n```\n\n## Acknowledgment\n[jieba 结巴中文分词](https://github.com/fxsjy/jieba)\n\n[nstools 中文繁体转简体](https://github.com/skydark/nstools)\n\n[NTUSD 情感词典](https://www.aaai.org/Papers/Symposia/Spring/2006/SS-06-03/SS06-03-020.pdf)\n\n[哈工大停用词表](https://github.com/goto456/stopwords)\n\n## License\n\nMIT\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/wansho/weibo-preprocess-toolkit",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "weibo-preprocess-toolkit",
    "package_url": "https://pypi.org/project/weibo-preprocess-toolkit/",
    "platform": "",
    "project_url": "https://pypi.org/project/weibo-preprocess-toolkit/",
    "project_urls": {
      "Homepage": "https://github.com/wansho/weibo-preprocess-toolkit"
    },
    "release_url": "https://pypi.org/project/weibo-preprocess-toolkit/1.1.0/",
    "requires_dist": [
      "jieba (>=0.39)"
    ],
    "requires_python": ">=3",
    "summary": "Weibo Preprocess Toolkit.",
    "version": "1.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 5526436,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b65fb61b094f36dabee0b69dcd359118a3d94d32c67557fca01cdb8ae35d89f6",
          "md5": "6bc25c42902a29fb06420ab517e9c9f5",
          "sha256": "fa83f4d3d7354d79eef253070cf793dacb99a77285dfe32906500a5c9b333447"
        },
        "downloads": -1,
        "filename": "weibo_preprocess_toolkit-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6bc25c42902a29fb06420ab517e9c9f5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 252123,
        "upload_time": "2019-06-10T05:45:54",
        "upload_time_iso_8601": "2019-06-10T05:45:54.013295Z",
        "url": "https://files.pythonhosted.org/packages/b6/5f/b61b094f36dabee0b69dcd359118a3d94d32c67557fca01cdb8ae35d89f6/weibo_preprocess_toolkit-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "02bfa0e5acd33e5bfda5b7c82e2e860e8c9ee3b577401f61f3fa2cc671b3fe1c",
          "md5": "15365237283b7016aeb0ceaa5c71406f",
          "sha256": "abb3ddd38c1a27697c0ee97c24ca37c2eb5a2324529c48d2e98bf3f23dc09386"
        },
        "downloads": -1,
        "filename": "weibo-preprocess-toolkit-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "15365237283b7016aeb0ceaa5c71406f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 119212,
        "upload_time": "2019-06-10T05:46:01",
        "upload_time_iso_8601": "2019-06-10T05:46:01.516952Z",
        "url": "https://files.pythonhosted.org/packages/02/bf/a0e5acd33e5bfda5b7c82e2e860e8c9ee3b577401f61f3fa2cc671b3fe1c/weibo-preprocess-toolkit-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f2fa1ac334eeeef0546c8c4752435cb037995564272e1edb1b4354bcd4952272",
          "md5": "56df0501228ac7caea820a351a3534c5",
          "sha256": "c1ee6e0d1ca45e47a5afdf7f52f27b6efdca2203669230af56b9caa857fc37c0"
        },
        "downloads": -1,
        "filename": "weibo_preprocess_toolkit-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "56df0501228ac7caea820a351a3534c5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 254817,
        "upload_time": "2019-07-13T08:53:18",
        "upload_time_iso_8601": "2019-07-13T08:53:18.585004Z",
        "url": "https://files.pythonhosted.org/packages/f2/fa/1ac334eeeef0546c8c4752435cb037995564272e1edb1b4354bcd4952272/weibo_preprocess_toolkit-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c7e066ae4ab2cd1412689919135e7b5fbbbb5ee435df82c4df6ead5cdcc31890",
          "md5": "4f935a3be0c7dd3fe960dfb31928054b",
          "sha256": "aa33bcc820c810aa5d4007ef0f903e41f4df66a1029c188e883996f78cb6bf7a"
        },
        "downloads": -1,
        "filename": "weibo-preprocess-toolkit-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "4f935a3be0c7dd3fe960dfb31928054b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 200254,
        "upload_time": "2019-07-13T08:53:21",
        "upload_time_iso_8601": "2019-07-13T08:53:21.304072Z",
        "url": "https://files.pythonhosted.org/packages/c7/e0/66ae4ab2cd1412689919135e7b5fbbbb5ee435df82c4df6ead5cdcc31890/weibo-preprocess-toolkit-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f2fa1ac334eeeef0546c8c4752435cb037995564272e1edb1b4354bcd4952272",
        "md5": "56df0501228ac7caea820a351a3534c5",
        "sha256": "c1ee6e0d1ca45e47a5afdf7f52f27b6efdca2203669230af56b9caa857fc37c0"
      },
      "downloads": -1,
      "filename": "weibo_preprocess_toolkit-1.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "56df0501228ac7caea820a351a3534c5",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3",
      "size": 254817,
      "upload_time": "2019-07-13T08:53:18",
      "upload_time_iso_8601": "2019-07-13T08:53:18.585004Z",
      "url": "https://files.pythonhosted.org/packages/f2/fa/1ac334eeeef0546c8c4752435cb037995564272e1edb1b4354bcd4952272/weibo_preprocess_toolkit-1.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c7e066ae4ab2cd1412689919135e7b5fbbbb5ee435df82c4df6ead5cdcc31890",
        "md5": "4f935a3be0c7dd3fe960dfb31928054b",
        "sha256": "aa33bcc820c810aa5d4007ef0f903e41f4df66a1029c188e883996f78cb6bf7a"
      },
      "downloads": -1,
      "filename": "weibo-preprocess-toolkit-1.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "4f935a3be0c7dd3fe960dfb31928054b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3",
      "size": 200254,
      "upload_time": "2019-07-13T08:53:21",
      "upload_time_iso_8601": "2019-07-13T08:53:21.304072Z",
      "url": "https://files.pythonhosted.org/packages/c7/e0/66ae4ab2cd1412689919135e7b5fbbbb5ee435df82c4df6ead5cdcc31890/weibo-preprocess-toolkit-1.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}