{
  "info": {
    "author": "Antoine Amend",
    "author_email": "antoine.amend@databricks.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: Other/Proprietary License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<p align=\"left\">\n  <img src=\"images/waterbear.png\" width=\"300px\"/>\n</p>\n\n# Waterbear\n\n*Tardigrades can be found in milder environments such as lakes, ponds and meadows, often living near 'lakehouse'. \nThough these species are disarmingly cute, they are also nearly indestructible and can survive in harsh \nenvironments like outer space. This project gives life to the smallest fully functional data processing unit with the \nhighest degrees of resilience and governance standards. We coded our tardigrades to carry the burden of \nenforcing enterprise data models that brings life to an industry regulated data lakehouse.*\n\n___\n\n[![DBR](https://img.shields.io/badge/DBR-9.1_ML-green)]()\n[![codecov](https://codecov.io/gh/databrickslabs/waterbear/branch/master/graph/badge.svg)](https://codecov.io/gh/databrickslabs/watergrade)\n[![PyPI version](https://badge.fury.io/py/dbl-waterbear.svg)](https://badge.fury.io/py/dbl-waterbear)\n\n## Enterprise data models\n\nGiven an enterprise data model, we automatically convert an entity into its spark schema equivalent, extract metadata, \nderive tables expectations as SQL expressions and provision data pipelines that accelerate development of production \nworkflows. Such a foundation allows financial services institutions to bootstrap their \n[Lakehouse for Financial Services](https://databricks.com/solutions/industries/financial-services) with \nresilient data pipelines and minimum development overhead. \n\n### JSON Schema\n\nAdhering to strict industry data standards, our project is supporting data models expressed as \n[JSON Schema](https://json-schema.org/) and was built to ensure full compatibility with recent open source initiatives \nlike the [FIRE](https://suade.org/fire/manifesto/) data model for regulatory reporting.\nIn the example below, we access the spark schema and delta expectations from the `collateral` entity.\n\n```python\nfrom waterbear.convertor import JsonSchemaConvertor\nschema, constraints = JsonSchemaConvertor('fire/model').convert(\"collateral\")\n```\n\n## Execution\n\nEven though records may often \"look\" structured (e.g. reading JSON files or well-defined CSVs), \nenforcing a schema is not just a good practice; in enterprise settings, it guarantees any missing field is still \nexpected, unexpected fields are discarded and data types are fully evaluated (e.g. a date should be treated as a date \nobject and not a string). We retrieve the spark schema required to process a given entity that we can apply on batch \nor on real-time through structured streaming and auto-loader. In the example below, we enforce schema on a batch of \nCSV records, resulting in a schematized dataframe.\n\n```python\nderivative_df = (\n    spark\n        .read\n        .format('csv')  # standard spark formats\n        .schema(schema) # enforcing our data model\n        .load('csv_files')\n)\n```\n\nApplying a schema is one thing, enforcing its constraints is another. Given the schema definition of an entity, \nwe can detect if a field is required or not. Given an enumeration object, we ensure its value consistency \n(e.g. country code). In addition to the technical constraints derived from the schema itself, the model also reports \nbusiness expectations using e.g. `minimum`, `maximum`, `maxItems`, etc. \nAll these technical and business constraints will be programmatically retrieved from our model and interpreted \nas a series of SQL expressions.\n\n```json\n{\n  \"[`high_fives`] VALUE\": \"`high_fives` IS NULL OR `high_fives` BETWEEN 1.0 AND 300.0\",\n  \"[`id`] NULLABLE\": \"`id` IS NOT NULL\",\n  \"[`person`.`username`] MATCH\": \"`person`.`username` IS NULL OR `person`.`username` RLIKE '^[a-z0-9]{2,}$'\",\n  \"[`person`] NULLABLE\": \"`person` IS NOT NULL\",\n  \"[`role`] VALUE\": \"`role` IS NULL OR `role` IN ('SA','CSE','SSA','RSA')\",\n  \"[`skills`] SIZE\": \"`skills` IS NULL OR SIZE(`skills`) >= 1.0\"\n}\n```\n\nAlthough one could apply those expectations through simple user defined functions, we highly recommend\nthe use of [Delta Live Tables](https://databricks.com/product/delta-live-tables) to ensure both reliability and \ntimeliness in financial data pipelines.\n\n### Delta Live Tables\n\nOur first step is to retrieve files landing to our industry lakehouse using Spark auto-loader. \nIn continuous mode, news files will be processed as they unfold, `max_files` at a time. \nIn triggered mode, only new files will be processed since last run. \nUsing Delta Live Tables, we ensure the execution and processing of delta increments, preventing organizations \nfrom having to maintain complex checkpointing mechanisms.\n\n```python\n@dlt.create_table()\ndef bronze():\n    return (\n        spark\n            .readStream\n            .format('csv')   # we read standard sources\n            .schema(schema)  # and enforce schema\n            .convert('/path/to/data/files')\n    )\n```\n\nOur pipeline will evaluate our series of SQL rules against our schematized dataset, \nflagging record breaching any of our expectations through the `expect_all` pattern and reporting on data quality \nin real time. \n\n```python\n@dlt.create_table()\n@dlt.expect_all(constraints) # we enforce expectations\ndef silver():\n  return dlt.read_stream(\"bronze\")\n```\n\n![](images/pipeline_processing.png)\n\n## Testing\n\nFor integration testing, we also provide users with the ability to generate records that match a given schema and \ncomplies with basic expectations (pattern matching is not supported).\n\n```python\nfrom waterbear.generator import JsonRecordGenerator\nxs = JsonRecordGenerator('fire/model').generate(\"collateral\", 5)\nfor x in xs:\n    print(x)\n```\n\n```json\n{\"id\": 6867, \"person\": {\"first_name\": \"vqgjldqqorklmupxibsrdyjw\", \"last_name\": \"vtsnbjuscbkvxyfdxrb\", \"birth_date\": \"2001-07-21\"}, \"skills\": [\"R\"]}\n{\"id\": 3119, \"person\": {\"first_name\": \"vp\", \"last_name\": \"dgipl\", \"birth_date\": \"1972-03-23\"}, \"high_fives\": 71, \"skills\": [\"SCALA\"]}\n{\"id\": 4182, \"person\": {\"first_name\": \"ijlzxxpv\", \"last_name\": \"ldpnnkohf\", \"birth_date\": \"1982-11-10\"}, \"joined_date\": \"2018-06-29\", \"skills\": [\"R\"]}\n{\"id\": 4940, \"person\": {\"first_name\": \"lhklebpkcxp\", \"last_name\": \"jir\", \"birth_date\": \"1998-01-06\"}, \"high_fives\": 213, \"skills\": [\"SQL\"], \"role\": \"RSA\"}\n{\"id\": 5920, \"person\": {\"first_name\": \"njadmuflxqbzc\", \"last_name\": \"arggdbaynulumrchreblfvxfe\", \"birth_date\": \"1997-06-26\", \"username\": \"snuafihfatyf\"}, \"high_fives\": 105, \"skills\": [\"PYTHON\"], \"role\": \"SA\"}\n```\n\n## Building a wheel file\n\n```shell\npython setup.py bdist_wheel --universal\n```\n\n## Using the Project\n\n```shell\npip install dbl-waterbear\n```\n\n## Project support\nPlease note that all projects in the /databrickslabs github account are provided for your exploration only, and are not formally supported by Databricks with Service Level Agreements (SLAs). They are provided AS-IS and we do not make any guarantees of any kind. Please do not submit a support ticket relating to any issues arising from the use of these projects.\n\nAny issues discovered through the use of this project should be filed as GitHub Issues on the Repo. They will be reviewed as time permits, but there are no formal SLAs for support.\n\n## Authors\n<antoine.amend@databricks.com>\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/databrickslabs/waterbear",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dbl-waterbear",
    "package_url": "https://pypi.org/project/dbl-waterbear/",
    "platform": null,
    "project_url": "https://pypi.org/project/dbl-waterbear/",
    "project_urls": {
      "Homepage": "https://github.com/databrickslabs/waterbear"
    },
    "release_url": "https://pypi.org/project/dbl-waterbear/0.1.1/",
    "requires_dist": [
      "pytest ; extra == 'tests'"
    ],
    "requires_python": "",
    "summary": "Automated provisioning of an industry Lakehouse with enterprise data model",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13173128,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cae4d7702a70f1b00fb45051e3b8ae3228094d48b2e854a71a3abc57cb64dc31",
          "md5": "2132c5741835fba02bdc8d1d55e8f5af",
          "sha256": "8621d50b4e087ecb4cf6efd499456e9b7649e0fa4654230a9ae530119e090cbc"
        },
        "downloads": -1,
        "filename": "dbl_waterbear-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2132c5741835fba02bdc8d1d55e8f5af",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16567,
        "upload_time": "2022-03-14T18:17:30",
        "upload_time_iso_8601": "2022-03-14T18:17:30.175020Z",
        "url": "https://files.pythonhosted.org/packages/ca/e4/d7702a70f1b00fb45051e3b8ae3228094d48b2e854a71a3abc57cb64dc31/dbl_waterbear-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6eb1b9c1813dbea7dad02e0fac90097387516ea3e1b95d009a19ece49417552f",
          "md5": "892f00e2b324599374ee6b69e19a38b0",
          "sha256": "c104c4afe0dac58e96219d3a3a82a9630f7ac0f6c4b6f75975eae7b03cf7fdbf"
        },
        "downloads": -1,
        "filename": "dbl_waterbear-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "892f00e2b324599374ee6b69e19a38b0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 19658,
        "upload_time": "2022-03-14T18:44:20",
        "upload_time_iso_8601": "2022-03-14T18:44:20.869174Z",
        "url": "https://files.pythonhosted.org/packages/6e/b1/b9c1813dbea7dad02e0fac90097387516ea3e1b95d009a19ece49417552f/dbl_waterbear-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6eb1b9c1813dbea7dad02e0fac90097387516ea3e1b95d009a19ece49417552f",
        "md5": "892f00e2b324599374ee6b69e19a38b0",
        "sha256": "c104c4afe0dac58e96219d3a3a82a9630f7ac0f6c4b6f75975eae7b03cf7fdbf"
      },
      "downloads": -1,
      "filename": "dbl_waterbear-0.1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "892f00e2b324599374ee6b69e19a38b0",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 19658,
      "upload_time": "2022-03-14T18:44:20",
      "upload_time_iso_8601": "2022-03-14T18:44:20.869174Z",
      "url": "https://files.pythonhosted.org/packages/6e/b1/b9c1813dbea7dad02e0fac90097387516ea3e1b95d009a19ece49417552f/dbl_waterbear-0.1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}