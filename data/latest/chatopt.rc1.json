{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Environment :: GPU :: NVIDIA CUDA",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: System :: Distributed Computing"
    ],
    "description": "# RLHF - Colossal-AI\n\nImplementation of RLHF (Reinforcement Learning with Human Feedback) powered by Colossal-AI. It supports distributed training and offloading, which can fit extremly large models. More details can be found in the [blog](https://www.hpc-ai.tech/blog/colossal-ai-chatgpt).\n\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/chatgpt.png\" width=700/>\n</p>\n\n## Training process (step 3)\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/experience.jpg\" width=500/>\n</p>\n<p align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/train.jpg\" width=500/>\n</p>\n\n\n## Install\n```shell\npip install .\n```\n\n\n## Usage\n\nThe main entrypoint is `Trainer`. We only support PPO trainer now. We support many training strategies:\n\n- NaiveStrategy: simplest strategy. Train on single GPU.\n- DDPStrategy: use `torch.nn.parallel.DistributedDataParallel`. Train on multi GPUs.\n- ColossalAIStrategy: use Gemini and Zero of ColossalAI. It eliminates model duplication on each GPU and supports offload. It's very useful when training large models on multi GPUs.\n\nSimplest usage:\n\n```python\nfrom chatgpt.trainer import PPOTrainer\nfrom chatgpt.trainer.strategies import ColossalAIStrategy\nfrom chatgpt.nn import GPTActor, GPTCritic, RewardModel\nfrom copy import deepcopy\nfrom colossalai.nn.optimizer import HybridAdam\n\nstrategy = ColossalAIStrategy()\n\nwith strategy.model_init_context():\n  # init your model here\n  # load pretrained gpt2\n  actor = GPTActor(pretrained='gpt2')\n  critic = GPTCritic()\n  initial_model = deepcopy(actor).cuda()\n  reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).cuda()\n\nactor_optim = HybridAdam(actor.parameters(), lr=5e-6)\ncritic_optim = HybridAdam(critic.parameters(), lr=5e-6)\n\n# prepare models and optimizers\n(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = strategy.prepare(\n        (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)\n\n# load saved model checkpoint after preparing\nstrategy.load_model(actor, 'actor_checkpoint.pt', strict=False)\n# load saved optimizer checkpoint after preparing\nstrategy.load_optimizer(actor_optim, 'actor_optim_checkpoint.pt')\n\ntrainer = PPOTrainer(strategy,\n                     actor,\n                     critic,\n                     reward_model,\n                     initial_model,\n                     actor_optim,\n                     critic_optim,\n                     ...)\n\ntrainer.fit(dataset, ...)\n\n# save model checkpoint after fitting on only rank0\nstrategy.save_model(actor, 'actor_checkpoint.pt', only_rank0=True)\n# save optimizer checkpoint on all ranks\nstrategy.save_optimizer(actor_optim, 'actor_optim_checkpoint.pt', only_rank0=False)\n```\n\nFor more details, see `examples/`.\n\nWe also support training reward model with true-world data. See `examples/train_reward_model.py`.\n\n## FAQ\n\n### How to save/load checkpoint\n\nTo load pretrained model, you can simply use huggingface pretrained models:\n\n```python\n# load OPT-350m pretrained model\nactor = OPTActor(pretrained='facebook/opt-350m')\n```\n\nTo save model checkpoint:\n\n```python\n# save model checkpoint on only rank0\nstrategy.save_model(actor, 'actor_checkpoint.pt', only_rank0=True)\n```\n\nThis function must be called after `strategy.prepare()`.\n\nFor DDP strategy, model weights are replicated on all ranks. And for ColossalAI strategy, model weights may be sharded, but all-gather will be applied before returning state dict. You can set `only_rank0=True` for both of them, which only saves checkpoint on rank0, to save disk space usage. The checkpoint is float32.\n\nTo save optimizer checkpoint:\n\n```python\n# save optimizer checkpoint on all ranks\nstrategy.save_optimizer(actor_optim, 'actor_optim_checkpoint.pt', only_rank0=False)\n```\n\nFor DDP strategy, optimizer states are replicated on all ranks. You can set `only_rank0=True`. But for ColossalAI strategy, optimizer states are sharded over all ranks, and no all-gather will be applied. So for ColossalAI strategy, you can only set `only_rank0=False`. That is to say, each rank will save a cehckpoint. When loading, each rank should load the corresponding part.\n\nNote that different stategy may have different shapes of optimizer checkpoint.\n\nTo load model checkpoint:\n\n```python\n# load saved model checkpoint after preparing\nstrategy.load_model(actor, 'actor_checkpoint.pt', strict=False)\n```\n\nTo load optimizer checkpoint:\n\n```python\n# load saved optimizer checkpoint after preparing\nstrategy.load_optimizer(actor_optim, 'actor_optim_checkpoint.pt')\n```\n\n## Todo\n\n- [x] implement PPO fine-tuning\n- [x] implement training reward model\n- [x] support LoRA\n- [ ] implement PPO-ptx fine-tuning\n- [ ] integrate with Ray\n- [ ] support more RL paradigms, like Implicit Language Q-Learning (ILQL)\n\n## Invitation to open-source contribution\nReferring to the successful attempts of [BLOOM](https://bigscience.huggingface.co/) and [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion), any and all developers and partners with computing powers, datasets, models are welcome to join and build an ecosystem with Colossal-AI, making efforts towards the era of big AI models from the starting point of replicating ChatGPT!\n\nYou may contact us or participate in the following ways:\n1. Posting an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) or submitting a [PR](https://github.com/hpcaitech/ColossalAI/pulls) on GitHub\n2. Join the Colossal-AI community on\n[Slack](https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w),\nand [WeChat](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png \"qrcode\") to share your ideas.\n3. Check out and fill in the [cooperation proposal](https://www.hpc-ai.tech/partners)\n4. Send your proposal to email contact@hpcaitech.com\n\nThanks so much to all of our amazing contributors!\n\n## Quick Preview\n<p id=\"ChatGPT_scaling\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png\" width=800/>\n</p>\n\n- Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference\n\n<p id=\"ChatGPT-1GPU\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg\" width=450/>\n</p>\n\n- Up to 10.3x growth in model capacity on one GPU\n- A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)\n\n<p id=\"inference\" align=\"center\">\n<img src=\"https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg\" width=600/>\n</p>\n\n- Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU\n- Keep in a sufficiently high running speed\n\n## Citations\n\n```bibtex\n@article{Hu2021LoRALA,\n    title   = {LoRA: Low-Rank Adaptation of Large Language Models},\n    author  = {Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},\n    journal = {ArXiv},\n    year    = {2021},\n    volume  = {abs/2106.09685}\n}\n\n@article{ouyang2022training,\n  title={Training language models to follow instructions with human feedback},\n  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},\n  journal={arXiv preprint arXiv:2203.02155},\n  year={2022}\n}\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/hpcaitech/ChatGPT",
    "keywords": "",
    "license": "Apache Software License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "chatopt",
    "package_url": "https://pypi.org/project/chatopt/",
    "platform": null,
    "project_url": "https://pypi.org/project/chatopt/",
    "project_urls": {
      "Homepage": "https://github.com/hpcaitech/ChatGPT"
    },
    "release_url": "https://pypi.org/project/chatopt/0.0.1a0/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A RLFH implementation (ChatGPT) powered by ColossalAI",
    "version": "0.0.1a0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17002959,
  "releases": {
    "0.0.1a0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "86ea723ca794dffd4330b89b703cb64384315a047981837e641248dd368bfa7e",
          "md5": "85dd56c5882e5eee06c70b882dddc661",
          "sha256": "140e83bffe82d33268d1bb3afbce33acbcc92bbb524d2b48282150b358eb4d45"
        },
        "downloads": -1,
        "filename": "chatopt-0.0.1a0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "85dd56c5882e5eee06c70b882dddc661",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 73137,
        "upload_time": "2023-02-24T00:23:02",
        "upload_time_iso_8601": "2023-02-24T00:23:02.007360Z",
        "url": "https://files.pythonhosted.org/packages/86/ea/723ca794dffd4330b89b703cb64384315a047981837e641248dd368bfa7e/chatopt-0.0.1a0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bd854a1fb6e4a7b6614499cf33e8528b5cb369a776a0fb3a36a559ecbba7a023",
          "md5": "034c41cc47a8426850b7982e2dd83435",
          "sha256": "165cfe529316eb5d345aa914b556a1d55292b14880fcec7c44a7c141111186a8"
        },
        "downloads": -1,
        "filename": "chatopt-0.0.1a0.tar.gz",
        "has_sig": false,
        "md5_digest": "034c41cc47a8426850b7982e2dd83435",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 27223,
        "upload_time": "2023-02-23T15:45:55",
        "upload_time_iso_8601": "2023-02-23T15:45:55.858625Z",
        "url": "https://files.pythonhosted.org/packages/bd/85/4a1fb6e4a7b6614499cf33e8528b5cb369a776a0fb3a36a559ecbba7a023/chatopt-0.0.1a0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "86ea723ca794dffd4330b89b703cb64384315a047981837e641248dd368bfa7e",
        "md5": "85dd56c5882e5eee06c70b882dddc661",
        "sha256": "140e83bffe82d33268d1bb3afbce33acbcc92bbb524d2b48282150b358eb4d45"
      },
      "downloads": -1,
      "filename": "chatopt-0.0.1a0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "85dd56c5882e5eee06c70b882dddc661",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 73137,
      "upload_time": "2023-02-24T00:23:02",
      "upload_time_iso_8601": "2023-02-24T00:23:02.007360Z",
      "url": "https://files.pythonhosted.org/packages/86/ea/723ca794dffd4330b89b703cb64384315a047981837e641248dd368bfa7e/chatopt-0.0.1a0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bd854a1fb6e4a7b6614499cf33e8528b5cb369a776a0fb3a36a559ecbba7a023",
        "md5": "034c41cc47a8426850b7982e2dd83435",
        "sha256": "165cfe529316eb5d345aa914b556a1d55292b14880fcec7c44a7c141111186a8"
      },
      "downloads": -1,
      "filename": "chatopt-0.0.1a0.tar.gz",
      "has_sig": false,
      "md5_digest": "034c41cc47a8426850b7982e2dd83435",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 27223,
      "upload_time": "2023-02-23T15:45:55",
      "upload_time_iso_8601": "2023-02-23T15:45:55.858625Z",
      "url": "https://files.pythonhosted.org/packages/bd/85/4a1fb6e4a7b6614499cf33e8528b5cb369a776a0fb3a36a559ecbba7a023/chatopt-0.0.1a0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}