{
  "info": {
    "author": "Krishna Pillutla",
    "author_email": "pillutla@cs.washington.edu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# SQwash: Distributionally Robust Learning in PyTorch with 1 Additional Line of Code\n\nThis package implements the [superquantile](https://en.wikipedia.org/wiki/Expected_shortfall)\na.k.a. Conditional Value at Risk (CVaR) for distributionally robust learning in PyTorch with GPU support.\nThe superquantile achieves distributional robustness by averaging over \nsome fraction of the worst losses on the minibatch in each step, as illustrated in this figure:\n\n<img src='fig/superquantile2.png' width=500>\n\nFor details, see the [complete documentation](https://krishnap25.github.io/sqwash)\n\n# Installation\nOnce you have PyTorch >=1.7, you can grab SQwash from pip:\n```bash\npip install sqwash\n```\n\nThe only dependency of SQwash is PyTorch, version 1.7 or higher.\nSee [here](https://pytorch.org/) for install instructions.\n\n# Quick Start\n\nAs the name suggests, it requires only a one-line modification to the usual PyTorch training loops.\n\n```python\nfrom sqwash import SuperquantileReducer\ncriterion = torch.nn.CrossEntropyLoss(reduction='none')  # Note: must set `reduction='none'`\nreducer = SuperquantileReducer(superquantile_tail_fraction=0.5)  # define the reducer\n\n# Training loop\nfor x, y in dataloader:\n    y_hat = model(x)\n    batch_losses = criterion(y_hat, y)  # shape: (batch_size,)\n    loss = reducer(batch_losses)  # Additional line to use the superquantile reducer\n    loss.backward()  # Proceed as usual from here\n    ...\n```\n\nThe package also gives a functional version of the reducers, similar to `torch.nn.functional`:\n```python\nimport torch.nn.functional as F\nfrom sqwash import reduce_superquantile\n\nfor x, y in dataloader:\n    y_hat = model(x)\n    batch_losses = F.cross_entropy(y_hat, y, reduction='none')  # must set `reduction='none'`\n    loss = reduce_superquantile(batch_losses, superquantile_tail_fraction=0.5)  # Additional line\n    loss.backward()  # Proceed as usual from here\n    ...\n```\n\nThe package can also be used for distributionally robust learning over \npre-specified groups of data. Simply obtain a tensor of losses for each element of the batch and \nuse the reducers in this pacakge as follows:\n```python\nloss_per_group = ...  # shape: (num_groups,)\nreducer = reduce_superquantile(loss_per_group, superquantile_tail_fraction=0.6)\n```\n\n# Functionality\nThis package provides 3 reducers, which take a tensor of losses on a minibatch and reduce them to a single value. \n- `MeanReducer`: the usual reduction, which is equivalent to specifying `reduction='mean'` in your criterion.\n- `SuperquantileReducer`: computes the superquantile/CVaR of the batch losses.\n- `SuperquantileSmoothReducer`: computes a smooth counterpart of the superquantile/CVaR of the batch losses.\n\nThe reducers take in a batch of losses, so make sure you set the the flag `reduction='none'` in your criterion to get all the losses of your batch.\n\nThese reducers are also available via the functional interface, similar to `torch.nn.functional`:\n- `reduce_mean`\n- `reduce_superquantile`\n- `reduce_superquantile_smooth`\n\n## Setting the parameters\nBoth the `SuperquantileReducer` and the `SuperquantileSmoothReducer` take a parameter called the \n`superquantile_tail_fraction`. This fraction of the worst losses on a batch are taken by the reducer.\nWhen `superquantile_tail_fraction` is taken to be 1, it is equivalent to taking the mean of the batch losses.\nWhen `superquantile_tail_fraction` is 0, it is equivalent to the maximum of the batch losses. \nFor typical applications, we find that values between 0.3 and 0.7 work reasonably. \nIf it is smaller than 0.3, you end up throwing away most of the information in your batch, while \nvalues close to 1 are effectively equivalent to the mean. \n\nThe `SuperquantileSmoothReducer` takes an additional argument `smoothing_parameter`,\nwhich controls how smooth the resulting function is. The smoothing parameter is\ninternally scaled by the batch size due to theoretical considerations (see `qp_solve.py` for details).\nThe extreme regimes for the smoothing parameter are:\n- `smoothing_parameter` is close to 0: we recover exactly the superquantile \n(i.e., the output of `SuperquantileReducer`), \n- `smoothing_parameter` is very large (tending to infinity): we recover the mean (the output of `MeanReducer`).\n\n## Implementation Details\nThe `SuperquantileReducer` requires computing a quantile of the batch losses, which has a computational cost of `O(batch_size)`. One the other hand, the `SuperquantileSmoothReducer` requires solving a quadratic program. By using the special structure of this quadratic program, we can solve it algorithmically by just sorting a vector of size `2*batch_size`. The  algorithm also requires creating a matrix of shape `(2*batch_size, batch_size)`, which can be done very efficiently on the GPU with a < 2% increase in running time for moderate batch sizes.\n\n# Authors\n[Krishna Pillutla](krishnap25.github.io)  \n[Yassine Laguel](https://yassine-laguel.github.io)  \n[Jérôme Malick](https://ljk.imag.fr/membres/Jerome.Malick/)  \n[Zaid Harchaoui](http://faculty.washington.edu/zaid/)  \n\n# License\nSQwash is available under the GPLv3 license. Please see the [license](LICENSE) for details.\n\n# Citation\nIf you found this package useful, please cite the following paper\n```\n@inproceedings{DBLP:conf/ciss/LPMH21,\n  author    = {Yassine Laguel and\n               Krishna Pillutla and\n               J{\\'{e}}r{\\^{o}}me Malick and\n               Zaid Harchaoui},\n  title     = {{A Superquantile Approach to Federated Learning with Heterogeneous\n               Devices}},\n  booktitle = {55th Annual Conference on Information Sciences and Systems, {CISS}\n               2021, Baltimore, MD, USA, March 24-26, 2021},\n  pages     = {1--6},\n  publisher = {{IEEE}},\n  year      = {2021},\n}\n```\n\n# Acknowledgements \nWe acknowledge support from NSF DMS 2023166,\nDMS 1839371, CCF 2019844, the CIFAR program \"Learning\nin Machines and Brains\", faculty research awards, and a JP\nMorgan PhD fellowship. This work has been partially supported\nby MIAI – Grenoble Alpes, (ANR-19-P3IA-0003).\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/krishnap25/sqwash",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "sqwash",
    "package_url": "https://pypi.org/project/sqwash/",
    "platform": "",
    "project_url": "https://pypi.org/project/sqwash/",
    "project_urls": {
      "Bug Tracker": "https://github.com/krishnap25/sqwash/issues",
      "Homepage": "https://github.com/krishnap25/sqwash"
    },
    "release_url": "https://pypi.org/project/sqwash/0.1.0/",
    "requires_dist": [
      "torch (>=1.7.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "Distributionally Robust Learning with the Superquantile",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10947565,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ade2f18459b68bcb764c7d4d156766220aa53d8f360314f9f8710de3860134e0",
          "md5": "e07dd77d6d4a5fb658b76202da2f4773",
          "sha256": "e9acb401dd49ec01d73844ef3b20b0f5e6303894f01c10d5274d5a4557e05bf7"
        },
        "downloads": -1,
        "filename": "sqwash-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e07dd77d6d4a5fb658b76202da2f4773",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 20407,
        "upload_time": "2021-07-19T15:54:58",
        "upload_time_iso_8601": "2021-07-19T15:54:58.885423Z",
        "url": "https://files.pythonhosted.org/packages/ad/e2/f18459b68bcb764c7d4d156766220aa53d8f360314f9f8710de3860134e0/sqwash-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2332df2ea1d4a886c01e0b084552697b27eda29d64077f35a46cffc039504553",
          "md5": "f51118ad2fa9de0f24e39c9a8e6a9a7b",
          "sha256": "7fbd727f9b9153a9dfd14600c2712e34abf6b402c57103d0b330738bb1ab3627"
        },
        "downloads": -1,
        "filename": "sqwash-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f51118ad2fa9de0f24e39c9a8e6a9a7b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 21200,
        "upload_time": "2021-07-19T15:55:00",
        "upload_time_iso_8601": "2021-07-19T15:55:00.443528Z",
        "url": "https://files.pythonhosted.org/packages/23/32/df2ea1d4a886c01e0b084552697b27eda29d64077f35a46cffc039504553/sqwash-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ade2f18459b68bcb764c7d4d156766220aa53d8f360314f9f8710de3860134e0",
        "md5": "e07dd77d6d4a5fb658b76202da2f4773",
        "sha256": "e9acb401dd49ec01d73844ef3b20b0f5e6303894f01c10d5274d5a4557e05bf7"
      },
      "downloads": -1,
      "filename": "sqwash-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e07dd77d6d4a5fb658b76202da2f4773",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 20407,
      "upload_time": "2021-07-19T15:54:58",
      "upload_time_iso_8601": "2021-07-19T15:54:58.885423Z",
      "url": "https://files.pythonhosted.org/packages/ad/e2/f18459b68bcb764c7d4d156766220aa53d8f360314f9f8710de3860134e0/sqwash-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2332df2ea1d4a886c01e0b084552697b27eda29d64077f35a46cffc039504553",
        "md5": "f51118ad2fa9de0f24e39c9a8e6a9a7b",
        "sha256": "7fbd727f9b9153a9dfd14600c2712e34abf6b402c57103d0b330738bb1ab3627"
      },
      "downloads": -1,
      "filename": "sqwash-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "f51118ad2fa9de0f24e39c9a8e6a9a7b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 21200,
      "upload_time": "2021-07-19T15:55:00",
      "upload_time_iso_8601": "2021-07-19T15:55:00.443528Z",
      "url": "https://files.pythonhosted.org/packages/23/32/df2ea1d4a886c01e0b084552697b27eda29d64077f35a46cffc039504553/sqwash-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}