{
  "info": {
    "author": "Tomasz Pietruszka",
    "author_email": "tomek.pietruszka@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Hyperspace explorer\nCollection of tools meant to enable faster progress and reproducibility of results in ML/DL \nprojects and competitions, without assuming too much about the projects themselves and used tools. \n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n\n- [Introduction](#introduction)\n  - [What is it useful for?](#what-is-it-useful-for)\n  - [Goals](#goals)\n  - [Glossary](#glossary)\n- [Setup](#setup)\n  - [Installation](#installation)\n  - [Project structure](#project-structure)\n    - [Adding new parameters, default values](#adding-new-parameters-default-values)\n- [Usage](#usage)\n  - [Running a worker](#running-a-worker)\n  - [Browsing experiment results](#browsing-experiment-results)\n- [Possible access points, usage modes](#possible-access-points-usage-modes)\n  - [CLI](#cli)\n  - [Run queue + workers](#run-queue--workers)\n  - [Interactive prototyping in Jupyter](#interactive-prototyping-in-jupyter)\n  - [Running tests](#running-tests)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Introduction\n### What is it useful for?\nMeant to support a process where we have:\n- a dataset, possibly pre-processed to some degree, that can be deterministically loaded (multiple versions of data? \nloading code should take an argument specifying which version to load)\n- an objective - we only consider supervised ML problems here, and we need some quality metric (be it \naccuracy or AUROC of classification, RMSE of regression, or something else). The metric can be changed, multiple metrics also supported.\n\nA common, useful approach is to then to quickly develop a simple version of the full pipeline:\n1. loading the data\n2. pre-processing, data augmentation, etc\n3. the model logic (if not a ready-made solution - perhaps a custom neural architecture?)\n4. model training procedure\n5. scoring the model on the validation set \n\nThen the real work begins - multiple iterations of:\n1. **developing and testing new versions of parts of the pipeline** - adding dimensions to the hyper-parameter \nspace (e.g. different neural architecture, different kind of model altogether, different pre-trained network, different \ndata pre-processing logic),\n2. **optimizing hyper-parameters**,\n3. inspecting how/when the model fails, and how it works - to guide further development,\n4. (in some cases) periodically pushing new versions to production.\n\nThis package is meant to provide common utilities supporting workflows like those, regardless\nof the frameworks/libraries in use or the structure of the problem to solve. It was born from \ncommon parts of multiple projects, often having a \"weird\" structure - e.g.:\n\n- fitting multiple models with identical hyper-parameters to time-series from corresponding machines,\n- periodic re-training of the model (also for time-series)\n- testing the same approach on multiple subsets of the training set\n\nThe strength of the package lies in its modularity, minimal assumptions about what you need to do \nand how you need to do it. It is meant to get out of your way - your code should still work without it, \nran the way you need - be it CLI, a notebook, etc.\n\n### Goals\nIt should be possible to:\n- store and analyze results of all past experiments,\n- reproduce past results (-> store results together with hyper-parameters, data, versions of code, ...),\n- easily re-run past experiments with some hyper-parameters changed,\n- develop or inspect parts of the pipeline in a notebook, easily run all of it from there (TODO: instructions),\n- extend the code's functionality and the hyper-parameter space; new hyper-params should always be added \n  with default values matching previous behaviour. Replacing pieces of logic with different ones, with different \n  hyper-parameters should be handled as well.\n- queue runs from a notebook (e.g. generate a grid of HPs in some dimensions) and have them\n  executed by worker processes (possibly distributed among multiple machines). Then close the notebook, or \n  add some more runs, never again waiting for computations to finish.\n- install core parts of the project's package and use it in production code (if the code got ugly with too many\n  options, write a new version of it, and compare results with the old version, all in the same environment)\n- **use automatic hyper-parameter tuning algorithms**, informed by all past experiments during development. \n\n### Glossary\n- `Run` - single execution of a `Task` with a specific config (set of parameters), concluded by calculation\nof a quality metric.\\\nExample: training a classifier with the hyper-parameters provided in the config, calculating accuracy\non the validation set.\\\nEach run gets its entry in a selected MongoDB database, in the `runs` collection. It includes\nall parameters, code versions, result metrics, captured output, and possibly custom diagnostic information.\n- `Task` - fully specified problem to solve, and then compare different solutions to. \\\nConsists of a chosen `Scenario` class and parameter values for its constructor.\nEach task should be specified as a .json file and include keys: `Scenario`\n(= dictionary of its settings, starting with `className`) and optionally `seed`.\n- `Scenario` - template of a `Task` without its parameters. When parameterized by `Task` parameters\nand a `Run` config it can be executed, and should return a quality metric. Each `Scenario` is a class,\ninheriting from `scenario_base.Scenario`. Its `.single_run()` method typically contains dataset construction, \nmodel construction, model training and evaluation.\n- `Study` - a project in which we define one or more `Scenarios` to study\nsolutions for, with a shared codebase, database, and virtual environment.\nCorresponds to a Python package, which has a dependency on `hyperspace_explorer`\nand includes a `scenarios` module.\n\n\n## Setup\n### Installation\nInstall with pip, as a normal python package.\n\nOther than all the modules being made available for import, it will also make `hyperspace_worker.py` \navailable in your system's PATH (or a specific virtual environment).\n\nTo use most of this package's functions a running instance of MongoDB will be needed.\n\n### Project structure\nTODO\n\n#### Adding new parameters, default values\nAs we add new parameters to our code (our classes), we must provide default\nvalues that ensure behaviour consistent with the earlier versions of the code.\n\nTo achieve that, default values (they do not have to be optimal/recommended\nones!) live inside the classes. They must not be set in function  - instead,\nthey should be provided by the `.get_default_values()` method of each\n`Configurable`. If using dataclasses, that method is provided automatically.\n\nWhen a worker process starts to process a Run, it fills in all default values\n- they end up stored in the database. It is redundant, but makes result analysis\nmuch easier - they do not have to be aware of the Study-specific codebase.\n\nIf expanding a Scenario to fit more Tasks, and adding parameters to it,\nsimilarly default values should be provided for all of them.\n\nA tool for back-filling new default values to past runs (while keeping original\nconfig in a different field as backup?) would be useful - future development.\n\n## Usage\n### Running a worker\nRun a command `hyperspace_worker.py [path to tasks dir] [mongo db name]`.\n\n**Important:** it has to be ran from a directory containing a `scenarios.py` module, which defines \nexperiment scenarios allowed to be ran within the given project. The `hyperspace_worker.py` file \nshould not be present in the folder.\n\nArguments:\n\n- `path to tasks dir` - path to a directory containing `.json` files, describing each allowed `task` -\na parameterization of a `scenario`\n- `mongo db name` - name of the mongoDB database to store results in\n- optional params: mongoDB URI (if not localhost, or if password is required), interval to query for new tasks\n\n### Browsing experiment results\n\nThis project (ab)uses [Sacred](https://github.com/IDSIA/sacred) to collect and store information about each run.\n\nOne of the benefits: we can use many ready-made dashboards for Sacred,\ne.g.  [Omniboard](https://github.com/vivekratnavel/omniboard) - highly recommended, works out of the box, \nmany impressive features. \n\n\n## Possible access points, usage modes\n### CLI\nTODO\n### Run queue + workers\nStart workers on 1 or more nodes, set them up to use the same database (which\nalso serves as a task queue). Workers are specific to one Study (project) -\nwill only process tasks for the Study they were started for.\n\nExample code, usually ran from a notebook, to submit one task. From here, it is\neasy to e.g. submit a grid of hyper-parameters for the workers to test.\n\n```python\nfrom hyperspace_explorer.queue import RunQueue\nfrom pathlib import Path\n\ntasks_dir = Path.cwd().resolve().parent / 'tasks'  # just an example - relative to the notebook\ndb_name = 'ulmfit_attention'\nmongo_uri = 'localhost:27017'\n\nq = RunQueue(mongo_uri, db_name, tasks_dir)\ntask_name = 'imdb_1k_sample_single'\n\nconf = {\n    'aggregation': {  # different additional parameters are available depending on `className`\n        'className': 'BranchingAttentionAggregation',\n        'agg_layers': [50, 10]\n    },\n    'classifier': { # this dict is passed to a specific function within a scenario, but polymorphism is not needed\n        'lin_ftrs': [],\n        'drop_mult': 0.5,\n    },\n    'training_schedule': { # even if we do not want to change any default parameters, className is required\n        'className': 'DefaultSchedule',\n    }\n}\nq.submit(task_name, conf)\n\n```\nThe code above works with the project: https://github.com/tpietruszka/ulmfit_attention. \nIn this case workers should be ran from within the inner `ulmfit_attention` directory.\n\n### Interactive prototyping in Jupyter\nTODO \n### Running tests\nTODO\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/tpietruszka/hyperspace_explorer",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hyperspace-explorer",
    "package_url": "https://pypi.org/project/hyperspace-explorer/",
    "platform": "",
    "project_url": "https://pypi.org/project/hyperspace-explorer/",
    "project_urls": {
      "Homepage": "https://github.com/tpietruszka/hyperspace_explorer"
    },
    "release_url": "https://pypi.org/project/hyperspace-explorer/0.3.1/",
    "requires_dist": [
      "sacred (>=0.8.1)",
      "pymongo (>=3.9.0)",
      "pandas (>=1.0.1) ; extra == 'analysis'",
      "commitizen (>=1.16.4) ; extra == 'dev'",
      "pytest ; extra == 'dev'"
    ],
    "requires_python": ">=3.7",
    "summary": "Tracking, queueing and distributed execution of ML/DL experiments. Helping define and semi-automatically explore hyper-parameter spaces.",
    "version": "0.3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6883376,
  "releases": {
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ed1f3ba5b388c241fb8cb5a1f4cfb09f76dac0844caadc12dba5b20a78832011",
          "md5": "dea151d038311d4ccae18029f7428ede",
          "sha256": "ec3c5670617c86f72328b843c80924aa7d6c325c0637b4d3858e6e743d40b96d"
        },
        "downloads": -1,
        "filename": "hyperspace_explorer-0.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dea151d038311d4ccae18029f7428ede",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 19652,
        "upload_time": "2020-03-25T18:48:09",
        "upload_time_iso_8601": "2020-03-25T18:48:09.859428Z",
        "url": "https://files.pythonhosted.org/packages/ed/1f/3ba5b388c241fb8cb5a1f4cfb09f76dac0844caadc12dba5b20a78832011/hyperspace_explorer-0.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bc569a85f1abeeba99d9e3b0a314d6d417ec97d63aba68b604de86a6274a5289",
          "md5": "37222cfb9d36aa4bcfe677e70729d2cd",
          "sha256": "85e95c88efaf5529fa0eea0cd1e442f51c74a7f439f5d73384380a397f74091b"
        },
        "downloads": -1,
        "filename": "hyperspace_explorer-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "37222cfb9d36aa4bcfe677e70729d2cd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 16192,
        "upload_time": "2020-03-25T18:48:12",
        "upload_time_iso_8601": "2020-03-25T18:48:12.243593Z",
        "url": "https://files.pythonhosted.org/packages/bc/56/9a85f1abeeba99d9e3b0a314d6d417ec97d63aba68b604de86a6274a5289/hyperspace_explorer-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ed1f3ba5b388c241fb8cb5a1f4cfb09f76dac0844caadc12dba5b20a78832011",
        "md5": "dea151d038311d4ccae18029f7428ede",
        "sha256": "ec3c5670617c86f72328b843c80924aa7d6c325c0637b4d3858e6e743d40b96d"
      },
      "downloads": -1,
      "filename": "hyperspace_explorer-0.3.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "dea151d038311d4ccae18029f7428ede",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 19652,
      "upload_time": "2020-03-25T18:48:09",
      "upload_time_iso_8601": "2020-03-25T18:48:09.859428Z",
      "url": "https://files.pythonhosted.org/packages/ed/1f/3ba5b388c241fb8cb5a1f4cfb09f76dac0844caadc12dba5b20a78832011/hyperspace_explorer-0.3.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bc569a85f1abeeba99d9e3b0a314d6d417ec97d63aba68b604de86a6274a5289",
        "md5": "37222cfb9d36aa4bcfe677e70729d2cd",
        "sha256": "85e95c88efaf5529fa0eea0cd1e442f51c74a7f439f5d73384380a397f74091b"
      },
      "downloads": -1,
      "filename": "hyperspace_explorer-0.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "37222cfb9d36aa4bcfe677e70729d2cd",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 16192,
      "upload_time": "2020-03-25T18:48:12",
      "upload_time_iso_8601": "2020-03-25T18:48:12.243593Z",
      "url": "https://files.pythonhosted.org/packages/bc/56/9a85f1abeeba99d9e3b0a314d6d417ec97d63aba68b604de86a6274a5289/hyperspace_explorer-0.3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}