{
  "info": {
    "author": "Zyte Group Ltd",
    "author_email": "info@zyte.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Environment :: Console",
      "Framework :: Scrapy",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "====================================\nScrapy & Autoextract API integration\n====================================\n\n.. image:: https://img.shields.io/pypi/v/scrapy-autoextract.svg\n   :target: https://pypi.org/project/scrapy-autoextract/\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/scrapy-autoextract.svg\n    :target: https://pypi.org/project/scrapy-autoextract/\n    :alt: Supported Python Versions\n\n.. image:: https://github.com/scrapinghub/scrapy-autoextract/workflows/tox/badge.svg\n   :target: https://github.com/scrapinghub/scrapy-autoextract/actions\n   :alt: Build Status\n\n.. image:: https://codecov.io/gh/scrapinghub/scrapy-autoextract/branch/master/graph/badge.svg?token=D6DQUSkios\n    :target: https://codecov.io/gh/scrapinghub/scrapy-autoextract\n    :alt: Coverage report\n\n\nThis library integrates Zyte's AI Enabled Automatic Data Extraction\ninto a Scrapy spider by two different means:\n\n* with a downloader middleware that injects the AutoExtract responses into ``response.meta['autoextract']``\n  for consumption by the spider.\n* with a `scrapy-poet`_ provider that injects the responses as callback parameters.\n\n\nInstallation\n============\n\n::\n\n    pip install scrapy-autoextract\n\nscrapy-autoextract requires Python 3.6+ for the download middleware and Python 3.7+ for the scrapy-poet provider\n\n\nUsage\n=====\n\nThere are two different ways to consume the AutoExtract API with this library:\n\n* using our Scrapy middleware\n* using our Page Object provider\n\nThe middleware\n--------------\n\nThe middleware is opt-in and can be explicitly enabled per request,\nwith the ``{'autoextract': {'enabled': True}}`` request meta.\nAll the options below can be set either in the project settings file,\nor just for specific spiders, in the ``custom_settings`` dict.\n\nWithin the spider, consuming the AutoExtract result is as easy as::\n\n    def parse(self, response):\n        yield response.meta['autoextract']\n\nConfiguration\n^^^^^^^^^^^^^\n\nAdd the AutoExtract downloader middleware in the settings file::\n\n    DOWNLOADER_MIDDLEWARES = {\n        'scrapy_autoextract.AutoExtractMiddleware': 543,\n    }\n\nNote that this should be the last downloader middleware to be executed.\n\nThe providers\n-------------\n\nAnother way of consuming AutoExtract API is using the Page Objects pattern\nproposed by the `web-poet`_ library and implemented by `scrapy-poet`_.\n\nItems returned by Page Objects are defined in the `autoextract-poet`_\nlibrary.\n\nWithin the spider, consuming the AutoExtract result is as easy as::\n\n    import scrapy\n    from autoextract_poet.pages import AutoExtractArticlePage\n\n    class SampleSpider(scrapy.Spider):\n        name = \"sample\"\n\n        def parse(self, response, article_page: AutoExtractArticlePage):\n            # We're making two requests here:\n            # - one through Scrapy to build the response argument\n            # - the other through the providers to build the article_page argument\n            yield article_page.to_item()\n\nNote that on the example above, we're going to perform two requests:\n\n* one goes through Scrapy (it might use Smart Proxy, Splash or no proxy at all, depending on your configuration)\n* another goes through AutoExtract API using `zyte-autoextract`_\n\nIf you don't need the additional request going through Scrapy,\nyou can annotate the response argument of your callback with ``DummyResponse``.\nThis will ignore the Scrapy request and only the AutoExtract API will be fetched.\n\nFor example::\n\n    import scrapy\n    from autoextract_poet.pages import AutoExtractArticlePage\n    from scrapy_poet import DummyResponse\n\n    class SampleSpider(scrapy.Spider):\n        name = \"sample\"\n\n        def parse(self, response: DummyResponse, article_page: AutoExtractArticlePage):\n            # We're making a single request here to build the article argument\n            yield article_page.to_item()\n\n\nThe examples above extract an article from the page, but you may want to\nextract a different type of item, like a product or a job posting. It is\nas easy as using the correct type annotation in the callback. This\nis how the callback looks like if we need to extract a real state\nfrom the page::\n\n    def parse(self,\n              response: DummyResponse,\n              real_estate_page: AutoExtractRealEstatePage):\n        yield real_estate_page.to_item()\n\nYou can even use ``AutoExtractWebPage`` if what you need is the raw browser HTML to\nextract some additional data. Visit the full list of `supported page types`_\nto get a better idea of the supported pages.\n\nConfiguration\n^^^^^^^^^^^^^\n\nFirst, you need to configure scrapy-poet as described on `scrapy-poet's documentation`_\nand then enable AutoExtract providers by putting the following code to Scrapy's ``settings.py`` file::\n\n    # Install AutoExtract provider\n    SCRAPY_POET_PROVIDERS = {\"scrapy_autoextract.AutoExtractProvider\": 500}\n\n    # Enable scrapy-poet's provider injection middleware\n    DOWNLOADER_MIDDLEWARES = {\n        'scrapy_poet.InjectionMiddleware': 543,\n    }\n\n    # Configure Twisted's reactor for asyncio support on Scrapy\n    TWISTED_REACTOR = 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n\nCurrently, our providers are implemented using asyncio.\nScrapy has introduced asyncio support since version 2.0\nbut as of Scrapy 2.3 you need to manually enable it by configuring Twisted's default reactor.\nCheck `Scrapy's asyncio documentation`_ for more information.\n\nChecklist:\n\n* scrapy-poet is installed and downloader/injector middleware is configured\n* autoextract-poet is installed (page inputs are imported from this lib)\n* providers are configured on settings.py\n* Scrapy's asyncio support is enabled on settings.py\n\nNow you should be ready to use our AutoExtract providers.\n\nExceptions\n^^^^^^^^^^\n\nWhile trying to fetch AutoExtract API, providers might raise some exceptions.\nThose exceptions might come from scrapy-autoextract providers themselves,\n`zyte-autoextract`_, or by other means (e.g. ``ConnectionError``).\nFor example:\n\n* ``autoextract.aio.errors.RequestError``: raised when a `Request-level error`_ is returned\n* ``scrapy_autoextract.errors.QueryError``: raised when a `Query-level error`_ is returned\n\nCheck `zyte-autoextract's async errors`_ for other exception definitions.\n\nYou can capture those exceptions using an error callback (``errback``)::\n\n    import scrapy\n    from autoextract.aio.errors import RequestError\n    from autoextract_poet.pages import AutoExtractArticlePage\n    from scrapy_autoextract.errors import QueryError\n    from scrapy_poet import DummyResponse\n    from twisted.python.failure import Failure\n\n    class SampleSpider(scrapy.Spider):\n        name = \"sample\"\n        urls = [...]\n\n        def start_requests(self):\n            for url in self.urls:\n                yield scrapy.Request(url, callback=self.parse_article,\n                                     errback=self.errback_article)\n\n        def parse_article(self, response: DummyResponse,\n                          article_page: AutoExtractArticlePage):\n            yield article_page.to_item()\n\n        def errback_article(self, failure: Failure):\n            if failure.check(RequestError):\n                self.logger.error(f\"RequestError on {failure.request.url}\")\n\n            if failure.check(QueryError):\n                self.logger.error(f\"QueryError: {failure.value.message}\")\n\nSee `Scrapy documentation <https://docs.scrapy.org/en/latest/topics/request-response.html#using-errbacks-to-catch-exceptions-in-request-processing>`_\nfor more details on how to capture exceptions using request's errback.\n\nSettings\n========\n\nMiddleware settings\n-------------------\n\n- ``AUTOEXTRACT_USER`` [mandatory] is your AutoExtract API key\n- ``AUTOEXTRACT_URL`` [optional] the AutoExtract service url. Defaults to autoextract.scrapinghub.com.\n- ``AUTOEXTRACT_TIMEOUT`` [optional] sets the response timeout from AutoExtract. Defaults to 660 seconds.\n  Can also be defined by setting the \"download_timeout\" in the request.meta.\n- ``AUTOEXTRACT_PAGE_TYPE`` [mandatory] defines the kind of document to be extracted.\n  Current available options are `\"product\"` and `\"article\"`.\n  Can also be defined on ``spider.page_type``, or ``{'autoextract': {'pageType': '...'}}`` request meta.\n  This is required for the AutoExtract classifier to know what kind of page needs to be extracted.\n- `extra` [optional] allows sending extra payload data to your AutoExtract request.\n  Must be specified as ``{'autoextract': {'extra': {}}}`` request meta and must be a dict.\n- ``AUTOEXTRACT_SLOT_POLICY`` [optional] Download concurrency options. Defaults to ``SlotPolicy.PER_DOMAIN``\n  - If set to ``SlotPolicy.PER_DOMAIN``, then consider setting ``SCHEDULER_PRIORITY_QUEUE = 'scrapy.pqueues.DownloaderAwarePriorityQueue'``\n  to make better usage of AutoExtract concurrency and avoid delays.\n\nProvider settings\n-----------------\n\n- ``AUTOEXTRACT_USER`` [optional] is your AutoExtract API key. If not set, it is\n  taken from ZYTE_AUTOEXTRACT_KEY environment variable.\n- ``AUTOEXTRACT_URL`` [optional] the AutoExtract service url.\n  Defaults to the official AutoExtract endpoint.\n- ``AUTOEXTRACT_MAX_QUERY_ERROR_RETRIES`` [optional] Max number of retries for\n  Query-level errors. Defaults to ``0``.\n- ``AUTOEXTRACT_CONCURRENT_REQUESTS_PER_DOMAIN`` [optional] Max number\n  of concurrent requests per domain. If not set, the provider will search\n  for the `CONCURRENT_REQUESTS_PER_DOMAIN` (defaults to ``8``) setting instead.\n- ``AUTOEXTRACT_CACHE_FILENAME`` [optional] Filename of a .sqlite file that will\n  be placed in the ``.scrapy`` folder. File will be created if it doesn't exist.\n  Cache is useful for development; AutoExtract requests bypass standard Scrapy\n  cache when providers are used.\n\nLimitations\n===========\n\nWhen using the AutoExtract middleware, there are some limitations.\n\n* The incoming spider request is rendered by AutoExtract, not just downloaded by Scrapy,\n  which can change the result - the IP is different, headers are different, etc.\n* Only GET requests are supported\n* Custom headers and cookies are not supported (i.e. Scrapy features to set them don't work)\n* Proxies are not supported (they would work incorrectly,\n  sitting between Scrapy and AutoExtract, instead of AutoExtract and website)\n* AutoThrottle extension can work incorrectly for AutoExtract requests,\n  because AutoExtract timing can be much larger than time required to download a page,\n  so it's best to use ``AUTHTHROTTLE_ENABLED=False`` in the settings.\n* Redirects are handled by AutoExtract, not by Scrapy,\n  so these kinds of middlewares might have no effect\n* 429 errors could be handled as standard retries when using Scrapy middleware,\n  but they're handled properly and automatically with scrapy-poet integration,\n  as it relies on `zyte-autoextract`_.\n  You may lose some responses with the middleware approach.\n* Overall, retries have a better behavior with scrapy-poet integration\n  and it includes support for automatic Query-level errors retries with\n  no need to change ``RETRY_HTTP_CODES``.\n* AutoExtract-specific cache (``AUTOEXTRACT_CACHE_FILENAME``) is not supported\n\nWhen using the AutoExtract providers, be aware that:\n\n* With scrapy-poet integration, retry requests don't go through Scrapy\n\n.. _`web-poet`: https://github.com/scrapinghub/web-poet\n.. _`scrapy-poet`: https://github.com/scrapinghub/scrapy-poet\n.. _`autoextract-poet`: https://github.com/scrapinghub/autoextract-poet\n.. _`zyte-autoextract`: https://github.com/zytedata/zyte-autoextract\n.. _`zyte-autoextract's async errors`: https://github.com/zytedata/zyte-autoextract/blob/master/autoextract/aio/errors.py\n.. _`scrapy-poet's documentation`: https://scrapy-poet.readthedocs.io/en/latest/intro/tutorial.html#configuring-the-project\n.. _`Scrapy's asyncio documentation`: https://docs.scrapy.org/en/latest/topics/asyncio.html\n.. _`Request-level error`: https://doc.scrapinghub.com/autoextract.html#request-level\n.. _`Query-level error`: https://doc.scrapinghub.com/autoextract.html#query-level\n.. _`supported page types`: https://autoextract-poet.readthedocs.io/en/stable/_autosummary/autoextract_poet.pages.html#module-autoextract_poet.pages\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/scrapinghub/scrapy-autoextract",
    "keywords": "scrapy autoextract middleware",
    "license": "",
    "maintainer": "Zyte Group Ltd",
    "maintainer_email": "info@zyte.com",
    "name": "scrapy-autoextract",
    "package_url": "https://pypi.org/project/scrapy-autoextract/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-autoextract/",
    "project_urls": {
      "Homepage": "https://github.com/scrapinghub/scrapy-autoextract"
    },
    "release_url": "https://pypi.org/project/scrapy-autoextract/0.7.0/",
    "requires_dist": [
      "autoextract-poet (>=0.3.0)",
      "zyte-autoextract (>=0.7.0)",
      "scrapy-poet (>=0.2.0)",
      "aiohttp",
      "tldextract",
      "sqlitedict (>=1.7.0)"
    ],
    "requires_python": "",
    "summary": "Zyte Automatic Extraction API integration for Scrapy",
    "version": "0.7.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11098496,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0af3501e69ba3bd9f8eff2b6f5c457d40be0a7044f6d59eeb2ece39cd180e49d",
          "md5": "6671c0eb467f0259df37da03cbeba290",
          "sha256": "4a45724527d67ba77670f67ea4221f9070794f6e6e648a898d6cfe14276219b3"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6671c0eb467f0259df37da03cbeba290",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 8158,
        "upload_time": "2019-10-22T10:05:48",
        "upload_time_iso_8601": "2019-10-22T10:05:48.103806Z",
        "url": "https://files.pythonhosted.org/packages/0a/f3/501e69ba3bd9f8eff2b6f5c457d40be0a7044f6d59eeb2ece39cd180e49d/scrapy_autoextract-0.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c2e5abcd25f30b6bdd10b41123dc880070a770586c44ad626b3070f2889ae6d9",
          "md5": "38718acc04040b63b69754c1a097ca4a",
          "sha256": "7f8ee98091d343ed9352eb37f48a7bb16819608fb793c49cb422a156a187f96b"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "38718acc04040b63b69754c1a097ca4a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7114,
        "upload_time": "2019-10-22T10:05:51",
        "upload_time_iso_8601": "2019-10-22T10:05:51.606010Z",
        "url": "https://files.pythonhosted.org/packages/c2/e5/abcd25f30b6bdd10b41123dc880070a770586c44ad626b3070f2889ae6d9/scrapy-autoextract-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1b74982850957d4e5aeaf8cb9f75568cfa913d61eab263b23a0f86bde8f0474d",
          "md5": "b1176fa0612af452bff99c2dda03fadd",
          "sha256": "1c55d448e4bc6cdb008661d60ab7345c3982c97859d0d8aad6f1aebaf37da252"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b1176fa0612af452bff99c2dda03fadd",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 8574,
        "upload_time": "2019-12-09T15:15:58",
        "upload_time_iso_8601": "2019-12-09T15:15:58.099036Z",
        "url": "https://files.pythonhosted.org/packages/1b/74/982850957d4e5aeaf8cb9f75568cfa913d61eab263b23a0f86bde8f0474d/scrapy_autoextract-0.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "87375a8a3a00ff1b6e6e70e9c49815a09587ee2fd7ff012a187e9ce8fd50fb0f",
          "md5": "a0ecd4473d3649f8f14f8a1a6e336563",
          "sha256": "dfc7863a9f18ac0523684c4ccf7504089c9b66787e88d7371f6a7a07762a580e"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "a0ecd4473d3649f8f14f8a1a6e336563",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7588,
        "upload_time": "2019-12-09T15:15:59",
        "upload_time_iso_8601": "2019-12-09T15:15:59.721691Z",
        "url": "https://files.pythonhosted.org/packages/87/37/5a8a3a00ff1b6e6e70e9c49815a09587ee2fd7ff012a187e9ce8fd50fb0f/scrapy-autoextract-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "60c7b1492f1abb90a8f1c486063375e6dab71240d239342dc05ef874b2d4fa61",
          "md5": "8e0e6b2425607aebd650a0b78a6686c8",
          "sha256": "6d406a180219115f1fabf702beee3f43362d81a1299084a0347f2cd10e9c9325"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8e0e6b2425607aebd650a0b78a6686c8",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 8983,
        "upload_time": "2019-12-10T16:04:31",
        "upload_time_iso_8601": "2019-12-10T16:04:31.390860Z",
        "url": "https://files.pythonhosted.org/packages/60/c7/b1492f1abb90a8f1c486063375e6dab71240d239342dc05ef874b2d4fa61/scrapy_autoextract-0.3-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "99ec237c35806c449d80247c98847d9daaa44edd72314c85feaf109755df31bb",
          "md5": "999d1f73543eff23693fe3b6386297a4",
          "sha256": "a44ccceafa8f32241ca579f9eee9f289c31281fcaba55047ac650d24a77ed4ca"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "999d1f73543eff23693fe3b6386297a4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7925,
        "upload_time": "2019-12-10T16:04:34",
        "upload_time_iso_8601": "2019-12-10T16:04:34.729799Z",
        "url": "https://files.pythonhosted.org/packages/99/ec/237c35806c449d80247c98847d9daaa44edd72314c85feaf109755df31bb/scrapy-autoextract-0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2ffd6644893a8a9e17d573d17ec2725c26cabb2d16d3f46b8a82ac1dd7dec8b6",
          "md5": "cf5b837cb129b0c715150b8032e95e7e",
          "sha256": "13e70fcd1368501d1b4c1026fced4595ad6bb9745c7571cf7c1ac5948ea7025d"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.3.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cf5b837cb129b0c715150b8032e95e7e",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 9199,
        "upload_time": "2019-12-17T16:23:19",
        "upload_time_iso_8601": "2019-12-17T16:23:19.214009Z",
        "url": "https://files.pythonhosted.org/packages/2f/fd/6644893a8a9e17d573d17ec2725c26cabb2d16d3f46b8a82ac1dd7dec8b6/scrapy_autoextract-0.3.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0da659e4a730d167ba86385e02057611ed50de15ac35edd39cf33d305c407564",
          "md5": "26ba538ce2debf1e0c9b409d9ae04cad",
          "sha256": "f8810800a376a109acbeadc4217f993b664e91ded9e9ae0f7be9f2369e6f2f6a"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "26ba538ce2debf1e0c9b409d9ae04cad",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 8085,
        "upload_time": "2019-12-17T16:23:20",
        "upload_time_iso_8601": "2019-12-17T16:23:20.271361Z",
        "url": "https://files.pythonhosted.org/packages/0d/a6/59e4a730d167ba86385e02057611ed50de15ac35edd39cf33d305c407564/scrapy-autoextract-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "000bcd50987386dca5f35a383594ba8e27d8a51e96cab58dd330ad8012995618",
          "md5": "343411f75ce26e7a34bb953fe9fcf0d6",
          "sha256": "c593799fe15b06ec56af751d89af7880ddb38cb8bbe3364125dc53505eb2ff82"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.4-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "343411f75ce26e7a34bb953fe9fcf0d6",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 9172,
        "upload_time": "2020-01-30T15:49:40",
        "upload_time_iso_8601": "2020-01-30T15:49:40.349991Z",
        "url": "https://files.pythonhosted.org/packages/00/0b/cd50987386dca5f35a383594ba8e27d8a51e96cab58dd330ad8012995618/scrapy_autoextract-0.4-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "438b105e5dc07fa20c041857f3008d13e3a54d6195dce6d47c1bc6a17e89a2f6",
          "md5": "6de56ca42d2fb5c3f17906afa384a7a6",
          "sha256": "d814eb2d53db50b2dd08d35dfe34c387c79f27d2b13223c67d2ba828ceaeb101"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "6de56ca42d2fb5c3f17906afa384a7a6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 8077,
        "upload_time": "2020-01-30T15:49:41",
        "upload_time_iso_8601": "2020-01-30T15:49:41.366502Z",
        "url": "https://files.pythonhosted.org/packages/43/8b/105e5dc07fa20c041857f3008d13e3a54d6195dce6d47c1bc6a17e89a2f6/scrapy-autoextract-0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d1d6fcfeb46b431528fab55e692f81e74b55600fb543221dcb2b9c91d40bbea7",
          "md5": "8b23285f1920d1d89ab4fa4dfba341af",
          "sha256": "0bd7532fcd7ac895ac22c848571d140c81e7edf22e330cc37f5b364d2b126ff1"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.5.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8b23285f1920d1d89ab4fa4dfba341af",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 24753,
        "upload_time": "2021-01-21T16:08:02",
        "upload_time_iso_8601": "2021-01-21T16:08:02.521807Z",
        "url": "https://files.pythonhosted.org/packages/d1/d6/fcfeb46b431528fab55e692f81e74b55600fb543221dcb2b9c91d40bbea7/scrapy_autoextract-0.5.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "922335dbe851947a285e020129b951c72bcaf0540d633d32c8bce1f3da4d0069",
          "md5": "b2eebb060fd5f260c4d1eba6e617fef9",
          "sha256": "b196a02404cf2f7712ea8276faf7926a266b190d7b4f990775fa80c904ba4ab6"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b2eebb060fd5f260c4d1eba6e617fef9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24894,
        "upload_time": "2021-01-21T16:08:03",
        "upload_time_iso_8601": "2021-01-21T16:08:03.696185Z",
        "url": "https://files.pythonhosted.org/packages/92/23/35dbe851947a285e020129b951c72bcaf0540d633d32c8bce1f3da4d0069/scrapy-autoextract-0.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c314807973de8268281e5eea33a7cec3bfcf42fb6139f037a9ffae9488565f19",
          "md5": "96ca40a104edafe9bde0673db611cd6c",
          "sha256": "edb861bc7bd4e56d61e1d33f903926029190294ed1a4b437b607bbe18b2983bc"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.5.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "96ca40a104edafe9bde0673db611cd6c",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 24761,
        "upload_time": "2021-01-22T21:10:17",
        "upload_time_iso_8601": "2021-01-22T21:10:17.167030Z",
        "url": "https://files.pythonhosted.org/packages/c3/14/807973de8268281e5eea33a7cec3bfcf42fb6139f037a9ffae9488565f19/scrapy_autoextract-0.5.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e1a44ca8103cfce0d99ef85e34d65876fa97691da54049c3723d86254fb4bfa8",
          "md5": "cf4110c7187ebe81d701326d062a78b4",
          "sha256": "1b04c1c75114d9eee0254871725a342d38ef45ab2c51d255aecfc7947b1d5328"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.5.1.tar.gz",
        "has_sig": false,
        "md5_digest": "cf4110c7187ebe81d701326d062a78b4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24926,
        "upload_time": "2021-01-22T21:10:18",
        "upload_time_iso_8601": "2021-01-22T21:10:18.100343Z",
        "url": "https://files.pythonhosted.org/packages/e1/a4/4ca8103cfce0d99ef85e34d65876fa97691da54049c3723d86254fb4bfa8/scrapy-autoextract-0.5.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d8272ae2a662a7570de123993509c2fd17f9343cc6f09a11f600b5b9dc047348",
          "md5": "cbce8b06d06bb5f66dafc47ab1fa0b98",
          "sha256": "7b6fb74da0bf5b49043f66f25e2883123dd1c2f0d587e4fb672cad21be1e2cbb"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.5.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cbce8b06d06bb5f66dafc47ab1fa0b98",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 24761,
        "upload_time": "2021-01-27T18:00:53",
        "upload_time_iso_8601": "2021-01-27T18:00:53.839848Z",
        "url": "https://files.pythonhosted.org/packages/d8/27/2ae2a662a7570de123993509c2fd17f9343cc6f09a11f600b5b9dc047348/scrapy_autoextract-0.5.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "707d3f07059adf50ef9d55c356a498def42b2bd0adc08ff204f3d57d48aa0b21",
          "md5": "935736fa2ce92044bbb9df71ce7fe0e4",
          "sha256": "3266aef5a09a6ef5f07721968e6024204bb78bbf8cb7bdfef462e7b9e3453509"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.5.2.tar.gz",
        "has_sig": false,
        "md5_digest": "935736fa2ce92044bbb9df71ce7fe0e4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24954,
        "upload_time": "2021-01-27T18:00:54",
        "upload_time_iso_8601": "2021-01-27T18:00:54.834988Z",
        "url": "https://files.pythonhosted.org/packages/70/7d/3f07059adf50ef9d55c356a498def42b2bd0adc08ff204f3d57d48aa0b21/scrapy-autoextract-0.5.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2b989685c91cff7cd7b955df131421f021cf48e70ba34cb3ecd970956e99f50b",
          "md5": "e1f8e4e1de48e0d359d9c0528f219796",
          "sha256": "a3e04aea3a782e87ccc46a87e6ca371deef05d9ccbf93ff188afdb79cb43bbd3"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.6.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e1f8e4e1de48e0d359d9c0528f219796",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 24759,
        "upload_time": "2021-06-01T18:48:22",
        "upload_time_iso_8601": "2021-06-01T18:48:22.040528Z",
        "url": "https://files.pythonhosted.org/packages/2b/98/9685c91cff7cd7b955df131421f021cf48e70ba34cb3ecd970956e99f50b/scrapy_autoextract-0.6.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "75fc6c7c7c26df7215aca35f049eb848132db81bb74c30aa5192f12539671d6f",
          "md5": "03959bff09f3ec33cfd92596451fe6d0",
          "sha256": "f0698e3a96794f005c5f2e1091e4bc50f69df2d8023c2800d65ce855365e872a"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "03959bff09f3ec33cfd92596451fe6d0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24932,
        "upload_time": "2021-06-01T18:48:23",
        "upload_time_iso_8601": "2021-06-01T18:48:23.381913Z",
        "url": "https://files.pythonhosted.org/packages/75/fc/6c7c7c26df7215aca35f049eb848132db81bb74c30aa5192f12539671d6f/scrapy-autoextract-0.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bc5e035ea54416dcef2cf41784467f81b34b686694eba883157c9fcc32e2e0d1",
          "md5": "98fe1eee8e930eb0fa9526fa8d332d7a",
          "sha256": "f5c0df2e3871a983070ed207a70d5ab120d7c60ca24eceff43e876c0af478d00"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.6.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "98fe1eee8e930eb0fa9526fa8d332d7a",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 26021,
        "upload_time": "2021-06-02T09:24:24",
        "upload_time_iso_8601": "2021-06-02T09:24:24.169495Z",
        "url": "https://files.pythonhosted.org/packages/bc/5e/035ea54416dcef2cf41784467f81b34b686694eba883157c9fcc32e2e0d1/scrapy_autoextract-0.6.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1940594f46e044e37db6a066593c60305ed8c7047b53c8d123b86e47662a2692",
          "md5": "48c0698be3eec50e66760ad26bc8e977",
          "sha256": "f495690d7a4413b1d6245d789d038e646644020cbb293c6a1e9811dc2cf47038"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.6.1.tar.gz",
        "has_sig": false,
        "md5_digest": "48c0698be3eec50e66760ad26bc8e977",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 26080,
        "upload_time": "2021-06-02T09:24:25",
        "upload_time_iso_8601": "2021-06-02T09:24:25.606631Z",
        "url": "https://files.pythonhosted.org/packages/19/40/594f46e044e37db6a066593c60305ed8c7047b53c8d123b86e47662a2692/scrapy-autoextract-0.6.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.7.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "87b23bc66f8f5079fadc84821030dacbea6349265113579ee57cd1eae2942bbe",
          "md5": "96f3f81315ad9b46c5c389990a3cf850",
          "sha256": "9b116ddb44ff885c3dbfd066dd2fe9360cf01d3fa102f07d869064b739b32d9b"
        },
        "downloads": -1,
        "filename": "scrapy_autoextract-0.7.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "96f3f81315ad9b46c5c389990a3cf850",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 26394,
        "upload_time": "2021-08-05T09:39:49",
        "upload_time_iso_8601": "2021-08-05T09:39:49.823137Z",
        "url": "https://files.pythonhosted.org/packages/87/b2/3bc66f8f5079fadc84821030dacbea6349265113579ee57cd1eae2942bbe/scrapy_autoextract-0.7.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "497b9f44a01a74de46c0792a198762df91f15eb17df7d2ad51085a6cbecb4c6b",
          "md5": "49d24899e835a8655596aed0f64d96ca",
          "sha256": "9fef8bd4508c962c464183187e6d22f1dfa3935ab8cd136018b6f3c11c579b18"
        },
        "downloads": -1,
        "filename": "scrapy-autoextract-0.7.0.tar.gz",
        "has_sig": false,
        "md5_digest": "49d24899e835a8655596aed0f64d96ca",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 26793,
        "upload_time": "2021-08-05T09:39:51",
        "upload_time_iso_8601": "2021-08-05T09:39:51.288201Z",
        "url": "https://files.pythonhosted.org/packages/49/7b/9f44a01a74de46c0792a198762df91f15eb17df7d2ad51085a6cbecb4c6b/scrapy-autoextract-0.7.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "87b23bc66f8f5079fadc84821030dacbea6349265113579ee57cd1eae2942bbe",
        "md5": "96f3f81315ad9b46c5c389990a3cf850",
        "sha256": "9b116ddb44ff885c3dbfd066dd2fe9360cf01d3fa102f07d869064b739b32d9b"
      },
      "downloads": -1,
      "filename": "scrapy_autoextract-0.7.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "96f3f81315ad9b46c5c389990a3cf850",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 26394,
      "upload_time": "2021-08-05T09:39:49",
      "upload_time_iso_8601": "2021-08-05T09:39:49.823137Z",
      "url": "https://files.pythonhosted.org/packages/87/b2/3bc66f8f5079fadc84821030dacbea6349265113579ee57cd1eae2942bbe/scrapy_autoextract-0.7.0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "497b9f44a01a74de46c0792a198762df91f15eb17df7d2ad51085a6cbecb4c6b",
        "md5": "49d24899e835a8655596aed0f64d96ca",
        "sha256": "9fef8bd4508c962c464183187e6d22f1dfa3935ab8cd136018b6f3c11c579b18"
      },
      "downloads": -1,
      "filename": "scrapy-autoextract-0.7.0.tar.gz",
      "has_sig": false,
      "md5_digest": "49d24899e835a8655596aed0f64d96ca",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 26793,
      "upload_time": "2021-08-05T09:39:51",
      "upload_time_iso_8601": "2021-08-05T09:39:51.288201Z",
      "url": "https://files.pythonhosted.org/packages/49/7b/9f44a01a74de46c0792a198762df91f15eb17df7d2ad51085a6cbecb4c6b/scrapy-autoextract-0.7.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}