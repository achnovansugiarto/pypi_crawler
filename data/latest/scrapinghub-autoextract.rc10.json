{
  "info": {
    "author": "Mikhail Korobov",
    "author_email": "kmike84@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "=======================\nscrapinghub-autoextract\n=======================\n\n.. image:: https://img.shields.io/pypi/v/scrapinghub-autoextract.svg\n   :target: https://pypi.python.org/pypi/scrapinghub-autoextract\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/scrapinghub-autoextract.svg\n   :target: https://pypi.python.org/pypi/scrapinghub-autoextract\n   :alt: Supported Python Versions\n\n.. image:: https://github.com/scrapinghub/scrapinghub-autoextract/workflows/tox/badge.svg\n   :target: https://github.com/scrapinghub/scrapinghub-autoextract/actions\n   :alt: Build Status\n\n.. image:: https://codecov.io/github/scrapinghub/scrapinghub-autoextract/coverage.svg?branch=master\n   :target: https://codecov.io/gh/scrapinghub/scrapinghub-autoextract\n   :alt: Coverage report\n\n\nPython client libraries for `Scrapinghub AutoExtract API`_.\nIt allows to extract product, article, job posting, etc.\ninformation from any website - whatever the API supports.\n\nCommand-line utility, asyncio-based library and a simple synchronous wrapper\nare provided by this package.\n\nLicense is BSD 3-clause.\n\n.. _Scrapinghub AutoExtract API: https://scrapinghub.com/autoextract\n\n\nInstallation\n============\n\n::\n\n    pip install scrapinghub-autoextract\n\nscrapinghub-autoextract requires Python 3.6+ for CLI tool and for\nthe asyncio API; basic, synchronous API works with Python 3.5.\n\nUsage\n=====\n\nFirst, make sure you have an API key. To avoid passing it in ``api_key``\nargument with every call, you can set ``SCRAPINGHUB_AUTOEXTRACT_KEY``\nenvironment variable with the key.\n\nCommand-line interface\n----------------------\n\nThe most basic way to use the client is from a command line.\nFirst, create a file with urls, an URL per line (e.g. ``urls.txt``).\nSecond, set ``SCRAPINGHUB_AUTOEXTRACT_KEY`` env variable with your\nAutoExtract API key (you can also pass API key as ``--api-key`` script\nargument).\n\nThen run a script, to get the results::\n\n    python -m autoextract urls.txt --page-type article --output res.jl\n\n.. note::\n    The results can be stored in an order which is different from the input\n    order. If you need to match the output results to the input URLs, the\n    best way is to use ``meta`` field (see below); it is passed through,\n    and returned as-is in ``row[\"query\"][\"userQuery\"][\"meta\"]``.\n\nIf you need more flexibility, you can customize the requests by creating\na JsonLines file with queries: a JSON object per line. You can pass any\nAutoExtract options there. Example - store it in ``queries.jl`` file::\n\n    {\"url\": \"http://example.com\", \"meta\": \"id0\", \"articleBodyRaw\": false}\n    {\"url\": \"http://example.com/foo\", \"meta\": \"id1\", \"articleBodyRaw\": false}\n    {\"url\": \"http://example.com/bar\", \"meta\": \"id2\", \"articleBodyRaw\": false}\n\nSee `API docs`_ for a description of all supported parameters in these query\ndicts. API docs mention batch requests and their limitation\n(no more than 100 queries at time); these limits don't apply to the queries.jl\nfile (i.e. it may have millions of rows), as the command-line script does\nits own batching.\n\n.. _API docs: https://doc.scrapinghub.com/autoextract.html\n\nNote that in the example ``pageType`` argument is omitted; ``pageType``\nvalues are filled automatically from ``--page-type`` command line argument\nvalue. You can also set a different ``pageType`` for a row in ``queries.jl``\nfile; it has a priority over ``--page-type`` passed in cmdline.\n\nTo get results for this ``queries.jl`` file, run::\n\n    python -m autoextract --intype jl queries.jl --page-type article --output res.jl\n\nProcessing speed\n~~~~~~~~~~~~~~~~\n\nEach API key has a limit on RPS. To get your URLs processed faster you can\ntune concurrency options: batch size and a number of connections.\n\nBest options depend on the RPS limit and on websites you're extracting\ndata from. For example, if your API key has a limit of 3RPS, and average\nresponse time you observe for your websites is 10s, then to get to these\n3RPS you may set e.g. batch size = 2, number of connections = 15 - this\nwould allow to process 30 requests in parallel.\n\nTo set these options in the CLI, use ``--n-conn`` and ``--batch-size``\narguments::\n\n    python -m autoextract urls.txt --page-type articles --n-conn 15 --batch-size 2 --output res.jl\n\nIf too many requests are being processed in parallel, you'll be getting\nthrottling errors. They are handled by CLI automatically, but they make\nextraction less efficient; please tune the concurrency options to\nnot hit the throttling errors (HTTP 429) often.\n\nYou may be also limited by the website speed. AutoExtract tries not to hit\nany individual website too hard, but it could be better to limit this on\na client side as well. If you're extracting data from a single website,\nit could make sense to decrease the amount of parallel requests; it can ensure\nhigher success ratio overall.\n\nIf you're extracting data from multiple websites, it makes sense to spread the\nload across time: if you have websites A, B and C, don't send requests in\nAAAABBBBCCCC order, send them in ABCABCABCABC order instead.\n\nTo do so, you can change the order of the queries in your input file.\nAlternatively, you can pass ``--shuffle`` options; it randomly shuffles\ninput queries before sending them to the API:\n\n    python -m autoextract urls.txt --shuffle --page-type articles --output res.jl\n\nRun ``python -m autoextract --help`` to get description of all supported\noptions.\n\nErrors\n~~~~~~\n\nThe following errors could happen while making requests:\n\n- Network errors\n- `Request-level errors`_\n    - Authentication failure\n    - Malformed request\n    - Too many queries in request\n    - Request payload size is too large\n- `Query-level errors`_\n    - Downloader errors\n    - Proxy errors\n    - ...\n\nSome errors can be retried while others can't.\n\nFor example,\nyou can retry a query with a Proxy Timeout error\nbecause this is a temporary error\nand there are chances that this response will be different\nwithin the next retries.\n\nOn the other hand,\nit makes no sense to retry queries that return a 404 Not Found error\nbecause the response is not supposed to change if retried.\n\n.. _Request-level errors: https://doc.scrapinghub.com/autoextract.html#request-level\n.. _Query-level errors: https://doc.scrapinghub.com/autoextract.html#query-level\n\nRetries\n~~~~~~~\n\nBy default, we will automatically retry Network and Request-level errors.\nYou could also enable Query-level errors retries\nby specifying the ``--max-query-error-retries`` argument.\n\nEnable Query-level retries to increase the success rate\nat the cost of more requests being performed\nif you are interested in a higher success rate.\n\n.. code-block::\n\n    python -m autoextract urls.txt --page-type articles --max-query-error-retries 3 --output res.jl\n\nFailing queries are retried\nuntil the max number of retries or a timeout is reached.\nIf it's still not possible to fetch all queries without errors,\nthe last available result is written to the output\nincluding both queries with success and the ones with errors.\n\nSynchronous API\n---------------\n\nSynchronous API provides an easy way to try AutoExtract.\nFor production usage asyncio API is strongly recommended. Currently the\nsynchronous API doesn't handle throttling errors, and has other limitations;\nit is most suited for quickly checking extraction results for a few URLs.\n\nTo send a request, use ``request_raw`` function; consult with the\n`API docs`_ to understand how to populate the query::\n\n    from autoextract.sync import request_raw\n    query = [{'url': 'http://example.com.foo', 'pageType': 'article'}]\n    results = request_raw(query)\n\nNote that if there are several URLs in the query, results can be returned in\narbitrary order.\n\nThere is also a ``autoextract.sync.request_batch`` helper, which accepts URLs\nand page type, and ensures results are in the same order as requested URLs::\n\n    from autoextract.sync import request_batch\n    urls = ['http://example.com/foo', 'http://example.com/bar']\n    results = request_batch(urls, page_type='article')\n\n.. note::\n    Currently request_batch is limited to 100 URLs at time only.\n\nasyncio API\n-----------\n\nBasic usage is similar to the sync API (``request_raw``),\nbut asyncio event loop is used::\n\n    from autoextract.aio import request_raw\n\n    async def foo():\n        query = [{'url': 'http://example.com.foo', 'pageType': 'article'}]\n        results1 = await request_raw(query)\n        # ...\n\nThere is also ``request_parallel_as_completed`` function, which allows\nto process many URLs in parallel, using both batching and multiple\nconnections::\n\n    import sys\n    from autoextract.aio import request_parallel_as_completed, create_session\n    from autoextract import ArticleRequest\n\n    async def extract_from(urls):\n        requests = [ArticleRequest(url) for url in urls]\n        async with create_session() as session:\n            res_iter = request_parallel_as_completed(requests,\n                                        n_conn=15, batch_size=2,\n                                        session=session)\n            for fut in res_iter:\n                try:\n                    batch_result = await fut\n                    for res in batch_result:\n                        # do something with a result, e.g.\n                        print(json.dumps(res))\n                except RequestError as e:\n                    print(e, file=sys.stderr)\n                    raise\n\n``request_parallel_as_completed`` is modelled after ``asyncio.as_completed``\n(see https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed),\nand actually uses it under the hood.\n\nNote ``from autoextract import ArticleRequest`` and its usage in the\nexample above. There are several Request helper classes,\nwhich simplify building of the queries.\n\n``request_parallel_as_completed`` and ``request_raw`` functions handle\nthrottling (http 429 errors) and network errors, retrying a request in\nthese cases.\n\nCLI interface implementation (``autoextract/__main__.py``) can serve\nas an usage example.\n\nRequest helpers\n---------------\n\nTo query AutoExtract you need to create a dict with request parameters, e.g.::\n\n    {'url': 'http://example.com.foo', 'pageType': 'article'}\n\nTo simplify the library usage and avoid typos, scrapinghub-autoextract\nprovides helper classes for constructing these dicts::\n\n* autoextract.Request\n* autoextract.ArticleRequest\n* autoextract.ProductRequest\n* autoextract.JobPostingRequest\n\nYou can pass instances of these classes instead of dicts everywhere when\nrequests dicts are accepted. So e.g. instead of writing this::\n\n    query = [{\"url\": url, \"pageType\": \"article\"} for url in urls]\n\nYou can write this::\n\n    query = [Request(url, pageType=\"article\") for url in urls]\n\nor this::\n\n    query = [ArticleRequest(url) for url in urls]\n\nThere is one difference: ``articleBodyRaw`` parameter is set to ``False``\nby default when Request or its variants are used, while it is ``True``\nby default in the API.\n\nYou can override API params passing a dictionary with extra data using the\n``extra`` argument. Note that it will overwrite any previous configuration\nmade using standard attributes like ``articleBodyRaw`` and ``fullHtml``.\n\nExtra parameters example::\n\n    request = ArticleRequest(\n        url=url,\n        fullHtml=True,\n        extra={\n            \"customField\": \"custom value\",\n            \"fullHtml\": False\n        }\n    )\n\nThis will generate a query that looks like this::\n\n    {\n        \"url\": url,\n        \"pageType\": \"article\",\n        \"fullHtml\": False,  # our extra parameter overrides the previous value\n        \"customField\": \"custom value\"  # not a default param but defined even then\n    }\n\n\nContributing\n============\n\n* Source code: https://github.com/scrapinghub/scrapinghub-autoextract\n* Issue tracker: https://github.com/scrapinghub/scrapinghub-autoextract/issues\n\nUse tox_ to run tests with different Python versions::\n\n    tox\n\nThe command above also runs type checks; we use mypy.\n\n.. _tox: https://tox.readthedocs.io\n\n\nChanges\n=======\n\n0.6.1 (2021-01-27)\n------------------\n\n* fixed ``max_retries`` behaviour. Total attempts must be max_retries + 1\n\n0.6.0 (2020-12-29)\n------------------\n\n* CLI changes: error display in the progress bar is changed;\n  summary is printed after the executions\n* more errors are retried when retrying is enabled, which allows for a higher\n  success rate\n* fixed tcp connection pooling\n* ``autoextract.aio.request_raw`` function allows to pass custom headers\n  to the API (not to remote websites)\n* ``autoextract.aio.request_raw`` now allows to customize the retry\n  behavior, via ``retrying`` argument\n* ``tenacity.RetryError`` is no longer raised by the library; concrete errors\n  are raised instead\n* Python 3.9 support\n* CI is moved from Travis to Github Actions\n\n0.5.2 (2020-11-27)\n------------------\n\n* ``QueryError`` is renamed to ``_QueryError``, as this is not an error\n  users of the library ever see.\n* Retrials were broken by having userAgent in the userQuery API output;\n  temporary workaround is added to make retrials work again.\n\n0.5.1 (2020-08-21)\n------------------\n\n* fix a problem that was preventing calls to ``request_raw`` when ``endpoint`` argument was ``None``\n\n0.5.0 (2020-08-21)\n------------------\n\n* add ``--api-endpoint`` option to the command line utility\n* improves documentation adding details about ``Request``'s extra parameters\n\n0.4.0 (2020-08-17)\n------------------\n\n``autoextract.Request`` helper class now allows to set arbitrary\nparameters for AutoExtract requests - they can be passed in ``extra`` argument.\n\n0.3.0 (2020-07-24)\n------------------\n\nIn this release retry-related features are added or improved.\nIt is now possible to fix some of the temporary errors\nby enabling query-level retries, and the default retry behavior is improved.\n\n* **backwards-incompatible**: autoextract.aio.ApiError is renamed\n  to autoextract.aio.RequestError\n* ``max_query_error_retries`` argument is added to\n  ``autoextract.aio.request_raw`` and\n  ``autoextract.aio.request_parallel_as_completed`` functions; it allows to\n  enable retries of temporary query-level errors returned by the API.\n* CLI: added ``--max-query-error-retries`` option to retry temporary\n  query-level errors.\n* HTTP 500 errors from server are retried now;\n* documentation and test improvements.\n\n0.2.0 (2020-04-15)\n------------------\n\n* asyncio API is rewritten, to simplify use in cases where passing meta\n  is required. ``autoextract.aio.request_parallel_as_completed`` is added,\n  ``autoextract.aio.request_parallel`` and ``autoextract.aio.request_batch``\n  are removed.\n* CLI: it now shows various stats: mean response and connect time,\n  % of throttling errors, % of network and other errors\n* CLI: new ``--intype jl`` option allows to process a .jl file\n  with arbitrary AutoExtract API queries\n* CLI: new ``--shuffle`` option allows to shuffle input data, to spread it\n  more evenly across websites.\n* CLI: it no longer exits on unrecoverable errors, to aid long-running\n  processing tasks.\n* retry logic is adjusted to handle network errors better.\n* ``autoextract.aio.request_raw`` and\n  ``autoextract.aio.request_parallel_as_completed`` functions provide an\n  interface to return statistics about requests made, including retries.\n* autoextract.Request, autoextract.ArticleRequest, autoextract.ProductRequest,\n  autoextract.JobPostingRequest helper classes\n* Documentation improvements.\n\n0.1.1 (2020-03-12)\n------------------\n\n* allow up to 100 elements in a batch, not up to 99\n* custom User-Agent header is added\n* Python 3.8 support is declared & tested\n\n0.1 (2019-10-09)\n----------------\n\nInitial release.\n\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/scrapinghub/scrapinghub-autoextract",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapinghub-autoextract",
    "package_url": "https://pypi.org/project/scrapinghub-autoextract/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapinghub-autoextract/",
    "project_urls": {
      "Homepage": "https://github.com/scrapinghub/scrapinghub-autoextract"
    },
    "release_url": "https://pypi.org/project/scrapinghub-autoextract/0.6.1/",
    "requires_dist": [
      "requests",
      "attrs",
      "runstats",
      "tenacity ; python_version >= \"3.6\"",
      "aiohttp (>=3.6.0) ; python_version >= \"3.6\"",
      "tqdm ; python_version >= \"3.6\""
    ],
    "requires_python": "",
    "summary": "Python interface to Scrapinghub Automatic Extraction API",
    "version": "0.6.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9246209,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "30ef71ab8223947762163e062a0c79ce5019cce474831e77cf19d1fafd97e2d2",
          "md5": "731b2061c99f5b9ef99cf61bed9d2b19",
          "sha256": "f0a9e69c49e5f1e3d1cdfa6069c322b1d9fa8d10c59a422295aa34cf74c14672"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "731b2061c99f5b9ef99cf61bed9d2b19",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 11964,
        "upload_time": "2019-10-09T18:16:23",
        "upload_time_iso_8601": "2019-10-09T18:16:23.270093Z",
        "url": "https://files.pythonhosted.org/packages/30/ef/71ab8223947762163e062a0c79ce5019cce474831e77cf19d1fafd97e2d2/scrapinghub_autoextract-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "811c826a9aa957870fc84f1306ecc3b7d71a9eb4a57b254eb31b5e0813985d1c",
          "md5": "18ad64552554031e4bc6b67efc4d3677",
          "sha256": "672e67b9443aa5ab78345de212b273f92031c95688474b58b0b3fe46ba2d13fa"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "18ad64552554031e4bc6b67efc4d3677",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11042,
        "upload_time": "2019-10-09T18:16:27",
        "upload_time_iso_8601": "2019-10-09T18:16:27.302969Z",
        "url": "https://files.pythonhosted.org/packages/81/1c/826a9aa957870fc84f1306ecc3b7d71a9eb4a57b254eb31b5e0813985d1c/scrapinghub-autoextract-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cc99bf13abb586f316554b9f43be21a74523f7b1a94cabf4e6a2c1950a794fa9",
          "md5": "51b5cca6b7561a08484055916ab3438f",
          "sha256": "d70a2266792950983b8d64503759861d6d2d4fa41d0c48364489fe00de0c2321"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "51b5cca6b7561a08484055916ab3438f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12462,
        "upload_time": "2020-03-12T16:06:21",
        "upload_time_iso_8601": "2020-03-12T16:06:21.444455Z",
        "url": "https://files.pythonhosted.org/packages/cc/99/bf13abb586f316554b9f43be21a74523f7b1a94cabf4e6a2c1950a794fa9/scrapinghub_autoextract-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c38089f406181f8f408c8a763f1484e54ff247cb18ad4488745d9021f528c0cf",
          "md5": "93f43fd475a0158d5fa5590b4b67302b",
          "sha256": "c1dc2f0ec3112513d44b7f43d433247c62fea91c9e487d9bd53491d3a45e3fab"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "93f43fd475a0158d5fa5590b4b67302b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11756,
        "upload_time": "2020-03-12T16:06:22",
        "upload_time_iso_8601": "2020-03-12T16:06:22.740253Z",
        "url": "https://files.pythonhosted.org/packages/c3/80/89f406181f8f408c8a763f1484e54ff247cb18ad4488745d9021f528c0cf/scrapinghub-autoextract-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6650a7f710e92738d7521fadbde072df89d02a405847d9ed344cbf7a3e4c49c4",
          "md5": "413a65c74b7496329a0966c4e77677d1",
          "sha256": "4c5416cfe81609d8a2f0f40d60bc197aee68061ae938a3cd64ea9de0e1aa54d4"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "413a65c74b7496329a0966c4e77677d1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 17797,
        "upload_time": "2020-04-15T19:13:22",
        "upload_time_iso_8601": "2020-04-15T19:13:22.009007Z",
        "url": "https://files.pythonhosted.org/packages/66/50/a7f710e92738d7521fadbde072df89d02a405847d9ed344cbf7a3e4c49c4/scrapinghub_autoextract-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f5a0acd2b913beb1f532513b5953c8ba8795198d964145f16e6cb0ef80d97753",
          "md5": "bce951efbb8d9e6d2be9e1975f049eca",
          "sha256": "78138f5285c73d2baeff1151cc814858b5cb521b65425487810a037f52fe29b5"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bce951efbb8d9e6d2be9e1975f049eca",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 19065,
        "upload_time": "2020-04-15T19:13:23",
        "upload_time_iso_8601": "2020-04-15T19:13:23.395799Z",
        "url": "https://files.pythonhosted.org/packages/f5/a0/acd2b913beb1f532513b5953c8ba8795198d964145f16e6cb0ef80d97753/scrapinghub-autoextract-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7aa3d9961968490c341e6e21c7369522457d0a6dedd9a68df6cc7b3fc94201e1",
          "md5": "f6fefb87c80aaffafedc580da5776257",
          "sha256": "dfae87b886ac4bb3c26235e47e9ceea7dc3c76f9c1e84d5b997f91d9bf4afbc7"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f6fefb87c80aaffafedc580da5776257",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21321,
        "upload_time": "2020-07-24T13:17:02",
        "upload_time_iso_8601": "2020-07-24T13:17:02.716894Z",
        "url": "https://files.pythonhosted.org/packages/7a/a3/d9961968490c341e6e21c7369522457d0a6dedd9a68df6cc7b3fc94201e1/scrapinghub_autoextract-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "18d9b37c3b90a9d72f51eef340d2ed6bc6715950a9ccf22c1a3d9d9143710da1",
          "md5": "a6e61dec0252bb51834b0cb72b7aefbf",
          "sha256": "01537f1b1cd6478a4663d843e09c895517b1c033b353f8cb0f5d5b5978b18a8d"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a6e61dec0252bb51834b0cb72b7aefbf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 25148,
        "upload_time": "2020-07-24T13:17:03",
        "upload_time_iso_8601": "2020-07-24T13:17:03.714788Z",
        "url": "https://files.pythonhosted.org/packages/18/d9/b37c3b90a9d72f51eef340d2ed6bc6715950a9ccf22c1a3d9d9143710da1/scrapinghub-autoextract-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "98eef8ae2f10a867c5c40f1e6e916e1c85fa93169d6579af3f35a7557211f14f",
          "md5": "0f235010eb6c83d5df069dc056f230cc",
          "sha256": "82fcdb4c626f64709cbec492e176c69ec5df73c5d593129b0574527852d72bfe"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.4.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0f235010eb6c83d5df069dc056f230cc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21528,
        "upload_time": "2020-08-17T20:00:45",
        "upload_time_iso_8601": "2020-08-17T20:00:45.363445Z",
        "url": "https://files.pythonhosted.org/packages/98/ee/f8ae2f10a867c5c40f1e6e916e1c85fa93169d6579af3f35a7557211f14f/scrapinghub_autoextract-0.4.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bc38eb3f3b913f0e0564e96bf595bce775ff319302ebd198c2aa27dd3b9b592c",
          "md5": "2e1433baec3dda47492e04bb9fd5385e",
          "sha256": "40ecadd13c86b2c9c0acd97c49a4e3d5742909fff8074d0b919cdc364aa2a15a"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2e1433baec3dda47492e04bb9fd5385e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 25519,
        "upload_time": "2020-08-17T20:00:46",
        "upload_time_iso_8601": "2020-08-17T20:00:46.883835Z",
        "url": "https://files.pythonhosted.org/packages/bc/38/eb3f3b913f0e0564e96bf595bce775ff319302ebd198c2aa27dd3b9b592c/scrapinghub-autoextract-0.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "06e2ac42c6bc8be53417d9ef4e15cc5c2c7fdaafe12adcb60a053504a71c81ce",
          "md5": "35a2e7749bc873e7b3a59ffda074df1e",
          "sha256": "f3500718c2783a39d02ecb1e041cebb673dd1aa6a52632da99dff38d7d7d3b26"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "35a2e7749bc873e7b3a59ffda074df1e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21881,
        "upload_time": "2020-08-22T01:17:24",
        "upload_time_iso_8601": "2020-08-22T01:17:24.393256Z",
        "url": "https://files.pythonhosted.org/packages/06/e2/ac42c6bc8be53417d9ef4e15cc5c2c7fdaafe12adcb60a053504a71c81ce/scrapinghub_autoextract-0.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fd6fdcc6056a0a6de8180efeaea566cf2d4cc8a26e68b6076860d93dc8e58dfe",
          "md5": "2571e83b2d656da6621eed978cdc2c5a",
          "sha256": "de195229cacb99d3d3166a4e3a61a0fd7cf9baa6d7ba637eeb2068bb2b7dbaac"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2571e83b2d656da6621eed978cdc2c5a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 26233,
        "upload_time": "2020-08-22T01:17:25",
        "upload_time_iso_8601": "2020-08-22T01:17:25.485127Z",
        "url": "https://files.pythonhosted.org/packages/fd/6f/dcc6056a0a6de8180efeaea566cf2d4cc8a26e68b6076860d93dc8e58dfe/scrapinghub-autoextract-0.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c735a2cc399dc13928b453b1e79340f09d93d5a6c865d6d6a26ba9d1c7b8fd21",
          "md5": "ffe91ae05b1822a8541906a39ed4b600",
          "sha256": "fa77c84ebce1b0b62cf7b3d99c2a1af536331355fac1d3fa19a4593c3d8ac25a"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.5.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ffe91ae05b1822a8541906a39ed4b600",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21926,
        "upload_time": "2020-08-22T02:28:59",
        "upload_time_iso_8601": "2020-08-22T02:28:59.365061Z",
        "url": "https://files.pythonhosted.org/packages/c7/35/a2cc399dc13928b453b1e79340f09d93d5a6c865d6d6a26ba9d1c7b8fd21/scrapinghub_autoextract-0.5.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d8bf4176b1e7b06b3d4a9c63fb03fb68dde7e63aec29f7cbd86b0af093c8bf84",
          "md5": "fcfcb3243d279737113c8c24b0421e04",
          "sha256": "c2090e37d64f2304b7b1c76b4329c1c79648db75f362e46ec671d7b783a1e971"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.5.1.tar.gz",
        "has_sig": false,
        "md5_digest": "fcfcb3243d279737113c8c24b0421e04",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 26339,
        "upload_time": "2020-08-22T02:29:00",
        "upload_time_iso_8601": "2020-08-22T02:29:00.699311Z",
        "url": "https://files.pythonhosted.org/packages/d8/bf/4176b1e7b06b3d4a9c63fb03fb68dde7e63aec29f7cbd86b0af093c8bf84/scrapinghub-autoextract-0.5.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "77a1eb93333ae2c9d33f62ebda4bb98ac1705e21bf138e4bd9bb5d873b89601f",
          "md5": "dc673105ea367e52222434ec49afbe65",
          "sha256": "155ce7551d4f50841bfcab0ae625cfa73bc629d8a08f7777f08b9a5201dbf594"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.5.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dc673105ea367e52222434ec49afbe65",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 22108,
        "upload_time": "2020-11-27T18:08:00",
        "upload_time_iso_8601": "2020-11-27T18:08:00.785724Z",
        "url": "https://files.pythonhosted.org/packages/77/a1/eb93333ae2c9d33f62ebda4bb98ac1705e21bf138e4bd9bb5d873b89601f/scrapinghub_autoextract-0.5.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c11fd3b75cedcf0df0fdb16fea5aa41b538abe67b6ce964f89ba5051411eb4e5",
          "md5": "9e319c017c4116b2a5d4655d7bd26215",
          "sha256": "af73bbb69b63dd914bc93bbd1ec609d76c0405707c504bce473b42475ced275a"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.5.2.tar.gz",
        "has_sig": false,
        "md5_digest": "9e319c017c4116b2a5d4655d7bd26215",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 26648,
        "upload_time": "2020-11-27T18:08:02",
        "upload_time_iso_8601": "2020-11-27T18:08:02.105235Z",
        "url": "https://files.pythonhosted.org/packages/c1/1f/d3b75cedcf0df0fdb16fea5aa41b538abe67b6ce964f89ba5051411eb4e5/scrapinghub-autoextract-0.5.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8340476744b3dcfaa6eda48bcfade1d918ce3937f10479c04835035aa8d41e1e",
          "md5": "341121a855fd615c4fd3b9f823919502",
          "sha256": "d35fe5839e418a2d7a15955449f2461560254fbc171548c6dbc7b1459f573d64"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.6.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "341121a855fd615c4fd3b9f823919502",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 23624,
        "upload_time": "2020-12-29T16:09:25",
        "upload_time_iso_8601": "2020-12-29T16:09:25.674802Z",
        "url": "https://files.pythonhosted.org/packages/83/40/476744b3dcfaa6eda48bcfade1d918ce3937f10479c04835035aa8d41e1e/scrapinghub_autoextract-0.6.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "668a074f320c12a2f4c63dc92d2e7e88b94a2fe0f060b3f9ce3e73dbd1818a63",
          "md5": "25f18e69f5602b55490d7b5cf060657e",
          "sha256": "8423bc55f886dcaea6d5cb4d5d4062187de2d1f297094cdbf770e91190b9265b"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "25f18e69f5602b55490d7b5cf060657e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30163,
        "upload_time": "2020-12-29T16:09:27",
        "upload_time_iso_8601": "2020-12-29T16:09:27.072653Z",
        "url": "https://files.pythonhosted.org/packages/66/8a/074f320c12a2f4c63dc92d2e7e88b94a2fe0f060b3f9ce3e73dbd1818a63/scrapinghub-autoextract-0.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2fd892ebf33a8355f59799ec28da3a35b6bd79d976c587ad61571b8e4b301df9",
          "md5": "4c5a3b7851e4ba305af8fc953a236a25",
          "sha256": "918056e3afd72926ed005962f8aa5abef95c0f7b488fb65fa17532a3fee2168f"
        },
        "downloads": -1,
        "filename": "scrapinghub_autoextract-0.6.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4c5a3b7851e4ba305af8fc953a236a25",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 23669,
        "upload_time": "2021-01-27T17:13:21",
        "upload_time_iso_8601": "2021-01-27T17:13:21.550064Z",
        "url": "https://files.pythonhosted.org/packages/2f/d8/92ebf33a8355f59799ec28da3a35b6bd79d976c587ad61571b8e4b301df9/scrapinghub_autoextract-0.6.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ec3525532968ed297a5db9dbc0608547fab88a7fd74b06231d94752651f6cc18",
          "md5": "2bdf7829af439453f9023ce3850e9d13",
          "sha256": "0cebe1f002f83b1e0ed1d43c695dc0918b5f20439c7c35bbe8389a09bcb0b55f"
        },
        "downloads": -1,
        "filename": "scrapinghub-autoextract-0.6.1.tar.gz",
        "has_sig": false,
        "md5_digest": "2bdf7829af439453f9023ce3850e9d13",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30284,
        "upload_time": "2021-01-27T17:13:22",
        "upload_time_iso_8601": "2021-01-27T17:13:22.706488Z",
        "url": "https://files.pythonhosted.org/packages/ec/35/25532968ed297a5db9dbc0608547fab88a7fd74b06231d94752651f6cc18/scrapinghub-autoextract-0.6.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2fd892ebf33a8355f59799ec28da3a35b6bd79d976c587ad61571b8e4b301df9",
        "md5": "4c5a3b7851e4ba305af8fc953a236a25",
        "sha256": "918056e3afd72926ed005962f8aa5abef95c0f7b488fb65fa17532a3fee2168f"
      },
      "downloads": -1,
      "filename": "scrapinghub_autoextract-0.6.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4c5a3b7851e4ba305af8fc953a236a25",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 23669,
      "upload_time": "2021-01-27T17:13:21",
      "upload_time_iso_8601": "2021-01-27T17:13:21.550064Z",
      "url": "https://files.pythonhosted.org/packages/2f/d8/92ebf33a8355f59799ec28da3a35b6bd79d976c587ad61571b8e4b301df9/scrapinghub_autoextract-0.6.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ec3525532968ed297a5db9dbc0608547fab88a7fd74b06231d94752651f6cc18",
        "md5": "2bdf7829af439453f9023ce3850e9d13",
        "sha256": "0cebe1f002f83b1e0ed1d43c695dc0918b5f20439c7c35bbe8389a09bcb0b55f"
      },
      "downloads": -1,
      "filename": "scrapinghub-autoextract-0.6.1.tar.gz",
      "has_sig": false,
      "md5_digest": "2bdf7829af439453f9023ce3850e9d13",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 30284,
      "upload_time": "2021-01-27T17:13:22",
      "upload_time_iso_8601": "2021-01-27T17:13:22.706488Z",
      "url": "https://files.pythonhosted.org/packages/ec/35/25532968ed297a5db9dbc0608547fab88a7fd74b06231d94752651f6cc18/scrapinghub-autoextract-0.6.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}