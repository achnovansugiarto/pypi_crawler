{
  "info": {
    "author": "Krzysztof Maziarz",
    "author_email": "krzysztof.maziarz@microsoft.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# MoLeR: A Model for Molecule Generation\n\n[![CI](https://github.com/microsoft/molecule-generation/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/microsoft/molecule-generation/actions/workflows/ci.yml)\n[![license](https://img.shields.io/github/license/microsoft/molecule-generation.svg)](https://github.com/microsoft/molecule-generation/blob/main/LICENSE)\n[![pypi](https://img.shields.io/pypi/v/molecule-generation.svg)](https://pypi.org/project/molecule-generation/)\n[![code style](https://img.shields.io/badge/code%20style-black-202020.svg)](https://github.com/ambv/black)\n\nThis repository contains training and inference code for the MoLeR model introduced in [Learning to Extend Molecular Scaffolds with Structural Motifs](https://arxiv.org/abs/2103.03864). We also include our implementation of CGVAE, but it currently lacks integration with the high-level model interface, and is provided mostly for reference.\n\n## Quick start\n\nThe `molecule_generation` package can be installed via `pip`, but it additionally depends on `rdkit` and (if one wants to use a GPU) on correctly setting up CUDA libraries. One approach to get both is through our minimalistic `conda` environment:\n\n```bash\nconda env create -f environment.yml\nconda activate moler-env\n```\n\nThis environment pins the versions of `python`, `rdkit` and `tensorflow` for reproducibility, but `molecule_generation` is compatible with a range of versions of these dependencies.\n\nTo then install the latest release of `molecule_generation`, simply run\n```bash\npip install molecule-generation\n```\n\nAlternatively, running `pip install -e .` within the root folder installs the latest state of the code, including changes that were merged into `main` but not yet released.\n\nNote that in the instructions above we pinned the `rdkit` version, as this is the version the code has been tested with. However, our code is likely to work with other modern version of `rdkit` as well.\nFinally, if `tensorflow` installation doesn't work out-of-the-box for your particular system, you may need to refer to [the tensorflow website](https://www.tensorflow.org/install) for guidelines.\n\nA MoLeR checkpoint trained using the default hyperparameters is available [here](https://figshare.com/ndownloader/files/34642724). This file needs to be saved in a fresh folder `MODEL_DIR` (e.g., `/tmp/MoLeR_checkpoint`) and be renamed to have the `.pkl` ending (e.g., to `GNN_Edge_MLP_MoLeR__2022-02-24_07-16-23_best.pkl`). Then you can sample 10 molecules by running\n\n```bash\nmolecule_generation sample MODEL_DIR 10\n```\n\nSee the next sections for how to train your own model and run more advanced inference.\n\n## Workflow\n\nWorking with MoLeR can be roughly divided into four stages:\n- *data preprocessing*, where a plain text list of SMILES strings is turned into `*.pkl` files containing descriptions of the molecular graphs and generation traces;\n- *training*, where MoLeR is trained on the preprocessed data until convergence;\n- *inference*, where one loads the model and performs batched encoding, decoding or sampling; and (optionally)\n- *fine-tuning*, where a previously trained model is fine-tuned on new data.\n\nAdditionally, you can visualise the decoding traces and internal action probabilities of the model, which can be useful for debugging.\n\n### Data Preprocessing\n\nTo run preprocessing, your data has to follow a simple GuacaMol format (files `train.smiles`, `valid.smiles` and `test.smiles`, each containing SMILES strings, one per line). Then, you can preprocess the data by running\n\n```\nmolecule_generation preprocess INPUT_DIR OUTPUT_DIR TRACE_DIR\n```\n\nwhere `INPUT_DIR` is the directory containing the three `*.smiles` files, `OUTPUT_DIR` is used for intermediate results, and `TRACE_DIR` for final preprocessed files containing the generation traces. Additionally, the `preprocess` command accepts command-line arguments to override various preprocessing hyperparameters (notably, the size of the motif vocabulary).\nThis step roughly corresponds to applying Algorithm 2 from our paper to each molecule in the input data.\n\nAfter running the above, you should see an output similar to\n\n```\n2022-03-10 11:22:15,927 preprocess.py:239 INFO 1273104 train datapoints, 79568 validation datapoints, 238706 test datapoints loaded, beginning featurization.\n2022-03-10 11:22:15,927 preprocess.py:245 INFO Featurising data...\n2022-03-10 11:22:15,927 molecule_dataset_utils.py:261 INFO Turning smiles into mol\n2022-03-10 11:22:15,927 molecule_dataset_utils.py:79 INFO Initialising feature extractors and motif vocabulary.\n2022-03-10 11:44:17,864 motif_utils.py:158 INFO Motifs in total: 99751\n2022-03-10 11:44:25,755 motif_utils.py:182 INFO Removing motifs with less than 3 atoms\n2022-03-10 11:44:25,755 motif_utils.py:183 INFO Motifs remaining: 99653\n2022-03-10 11:44:25,764 motif_utils.py:190 INFO Truncating the list of motifs to 128 most common\n2022-03-10 11:44:25,764 motif_utils.py:192 INFO Motifs remaining: 128\n2022-03-10 11:44:25,764 motif_utils.py:199 INFO Finished creating the motif vocabulary\n2022-03-10 11:44:25,764 motif_utils.py:200 INFO | Number of motifs: 128\n2022-03-10 11:44:25,764 motif_utils.py:203 INFO | Min frequency: 3602\n2022-03-10 11:44:25,764 motif_utils.py:204 INFO | Max frequency: 1338327\n2022-03-10 11:44:25,764 motif_utils.py:205 INFO | Min num atoms: 3\n2022-03-10 11:44:25,764 motif_utils.py:206 INFO | Max num atoms: 10\n2022-03-10 11:44:25,862 preprocess.py:255 INFO Completed initializing feature extractors; featurising and saving data now.\n Wrote 1273104 datapoints to /guacamol/output/train.jsonl.gz.\n Wrote 79568 datapoints to /guacamol/output/valid.jsonl.gz.\n Wrote 238706 datapoints to /guacamol/output/test.jsonl.gz.\n Wrote metadata to /guacamol/output/metadata.pkl.gz.\n(...proceeds to compute generation traces...)\n```\n\nAfter the preprocessed graphs are saved into `OUTPUT_DIR`, they will be turned into concrete generation traces, which is typically the most compute-intensive part of preprocessing. During that part, the preprocessing code may print errors, noting molecules that could not have been parsed or failed other assertions; MoLeR's preprocessing is robust to such cases, and will simply skip any problematic samples.\n\n### Training\n\nHaving stored some preprocessed data under `TRACE_DIR`, MoLeR can be trained by running\n\n```\nmolecule_generation train MoLeR TRACE_DIR\n```\n\n\nThe `train` command accepts many command-line arguments to override training and architectural hyperparameters, most of which are accessed through passing `--model-params-override`. For example, the following trains a MoLeR model using `GGNN`-style message passing (instead of the default `GNN_Edge_MLP`) and using fewer layers in both the encoder and the decoder GNNs:\n\n```\nmolecule_generation train MoLeR TRACE_DIR \\\n    --model GGNN \\\n    --model-params-override '{\"gnn_num_layers\": 6, \"decoder_gnn_num_layers\": 6}'\n```\n\nAs [tf2-gnn](https://github.com/microsoft/tf2-gnn) is highly flexible, MoLeR supports a vast space of architectural configurations.\n\nAfter running `molecule_generation train`, you should see an output similar to\n\n```\n(...tensorflow messages, hyperparameter dump...)\nInitial valid metric:\nAvg weighted sum. of graph losses:  122.1728\nAvg weighted sum. of prop losses:   0.4712\nAvg node class. loss:                 35.9361\nAvg first node class. loss:           27.4681\nAvg edge selection loss:              1.7522\nAvg edge type loss:                   3.8963\nAvg attachment point selection loss:  1.1227\nAvg KL divergence:                    7335960.5000\nProperty results: sa_score: MAE 11.23, MSE 1416.26 (norm MAE: 13.89) | clogp: MAE 10.87, MSE 4620.69 (norm MAE: 5.98) | mol_weight: MAE 407.42, MSE 185524.38 (norm MAE: 3.70).\n   (Stored model metadata and weights to trained_model/GNN_Edge_MLP_MoLeR__2022-03-01_18-15-14_best.pkl).\n(...training proceeds...)\n```\n\nBy default, training proceeds until there is no improvement in validation loss for 3 consecutive mini-epochs, where a mini-epoch is defined as 5000 training steps; this can be controlled through the `--patience` flag and the `num_train_steps_between_valid` model parameter, respectively.\n\n### Inference\n\nAfter a model has been trained and saved under `MODEL_DIR`, we provide two ways to load it: from CLI or directly from Python.\nCurrently, CLI-based loading does not expose all useful functionalities, and is mostly meant for simple tests.\n\nTo sample molecules from the model using the CLI, simply run\n\n```\nmolecule_generation sample MODEL_DIR NUM_SAMPLES\n```\n\nand, similarly, to encode a list of SMILES stored under `SMILES_PATH` into latent vectors, and store them under `OUTPUT_PATH`\n\n```\nmolecule_generation encode MODEL_DIR SMILES_PATH OUTPUT_PATH\n```\n\nIn all cases `MODEL_DIR` denotes the directory containing the model checkpoint, not the path to the checkpoint itself.\nThe model loader will only look at `*.pkl` files under `MODEL_DIR`, and expect there is _exactly one_ such file, corresponding to the trained checkpoint.\n\nYou can load a model directly from Python via\n\n```python\nfrom molecule_generation import load_model_from_directory\n\nmodel_dir = \"./example_model_directory\"\nexample_smiles = [\"c1ccccc1\", \"CNC=O\"]\n\nwith load_model_from_directory(model_dir) as model:\n    embeddings = model.encode(example_smiles)\n    print(f\"Embedding shape: {embeddings[0].shape}\")\n\n    # Decode without a scaffold constraint.\n    decoded = model.decode(embeddings)\n\n    # The i-th scaffold will be used when decoding the i-th latent vector.\n    decoded_scaffolds = model.decode(embeddings, scaffolds=[\"CN\", \"CCC\"])\n\n    print(f\"Encoded: {example_smiles}\")\n    print(f\"Decoded: {decoded}\")\n    print(f\"Decoded with scaffolds: {decoded_scaffolds}\")\n```\n\nwhich should yield an output similar to\n\n```\nEmbedding shape: (512,)\nEncoded: ['c1ccccc1', 'CNC=O']\nDecoded: ['C1=CC=CC=C1', 'CNC=O']\nDecoded with scaffolds: ['C1=CC=C(CNC2=CC=CC=C2)C=C1', 'CNC(=O)C(C)C']\n```\n\nAs shown above, MoLeR is loaded through a context manager.\nBehind the scenes, the following things happen:\n- First, an appropriate wrapper class is chosen: if the provided directory contains a `MoLeRVae` checkpoint, the returned wrapper will support `encode`, `decode` and `sample`, while `MoLeRGenerator` will only support `sample`.\n- Next, parallel workers are spawned, which await queries for encoding/decoding; these processes continue to live as long as the context is active.\nThe degree of paralellism can be configured using a `num_workers` argument.\n\n### Fine-tuning\n\nFine-tuning proceeds similarly to training from scratch, with a few adjustments.\nFirst, data intended for fine-tuning has to be preprocessed accordingly, by running\n\n```\nmolecule_generation preprocess INPUT_DIR OUTPUT_DIR TRACE_DIR \\\n    --pretrained-model-path CHECKPOINT_PATH\n```\n\nWhere `CHECKPOINT_PATH` points to the file (not directory) corresponding to the model that will later be fine-tuned.\n\nThe `--pretrained-model-path` argument is necessary, as otherwise preprocessing would infer various metadata (e.g. set of atom/motif types) solely from the provided set of SMILES, whereas for fine-tuning this has to be aligned with the metadata that the model was originally trained with.\n\nAfter preprocessing, fine-tuning is run as\n```\nmolecule_generation train MoLeR TRACE_DIR \\\n    --load-saved-model CHECKPOINT_PATH \\\n    --load-weights-only\n```\n\nWhen fine-tuning on a small dataset, it may not be desirable to update the model until convergence.\nTraining duration can be capped by passing `--model-params-override '{\"num_train_steps_between_valid\": 100}'` (to shorten the mini-epochs) and `--max-epochs` (to limit the number of mini-epochs).\n\n### Visualisation\n\nWe support two subtly different modes of visualisation: decoding a given latent vector, and decoding a latent vector created by encoding a given SMILES string. In the former case, the decoder runs as normal during inference; in the latter case we know the ground-truth input, so we teacher-force the correct decoding decisions.\n\nTo enter the visualiser, run either\n\n```\nmolecule_generation visualise cli MODEL_DIR SMILES_OR_SAMPLES_PATH\n```\n\nto get the result printed as plain text in the CLI, or\n\n```\nmolecule_generation visualise html MODEL_DIR SMILES_OR_SAMPLES_PATH OUTPUT_DIR\n```\n\nto get the result saved under `OUTPUT_DIR` as a static HTML webpage.\n\n## Code Structure\n\nAll of our models are implemented in [Tensorflow 2](https://www.tensorflow.org/), and are meant to be easy to extend and build upon. We use [tf2-gnn](https://github.com/microsoft/tf2-gnn) for the core Graph Neural Network components.\n\nThe MoLeR model itself is implemented as a `MoLeRVae` class, inheriting from `GraphTaskModel` in `tf2-gnn`; that base class encapsulates the encoder GNN. The decoder GNN is instantiated as an external `MoLeRDecoder` layer; it also includes batched inference code, which forces the maximum likelihood choice at every step.\n\n## Authors\n\n* [Krzysztof Maziarz](mailto:krzysztof.maziarz@microsoft.com)\n* [Henry Jackson-Flux](mailto:hrjackson@gmail.com)\n* [Marc Brockschmidt](mailto:mabrocks@microsoft.com)\n* [Pashmina Cameron](mailto:Pashmina.Cameron@microsoft.com)\n* [Sarah Lewis](mailto:sarahlewis@microsoft.com)\n* [Marwin Segler](mailto:marwinsegler@microsoft.com)\n* [Megan Stanley](mailto:meganstanley@microsoft.com)\n* [Paweł Czyż](mailto:pawelpiotr.czyz@ai.ethz.ch)\n* [Ashok Thillaisundaram](mailto:ashok@cantab.net)\n\n_Note: as git history was truncated at the point of open-sourcing, GitHub's statistics do not reflect the degree of contribution from some of the authors. All listed above had an impact on the code, and are (approximately) ordered by decreasing contribution._\n\nThe code is maintained by the [Generative Chemistry](https://www.microsoft.com/en-us/research/project/generative-chemistry/)\ngroup at Microsoft Research, Cambridge, UK.\nWe are [hiring](https://www.microsoft.com/en-us/research/project/generative-chemistry/opportunities/).\n\nMoLeR was created as part of our collaboration with Novartis Research. In particular, its design was guided by [Nadine Schneider](mailto:nadine-1.schneider@novartis.com), [Finton Sirockin](mailto:finton.sirockin@novartis.com), [Nikolaus Stiefl](mailto:nikolaus.stiefl@novartis.com), as well as others from Novartis.\n\n## Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### Style Guide\n\n- For code style, use [black](https://pypi.org/project/black/) and [flake8](https://pypi.org/project/flake8/).\n- For commit messages, use imperative style and follow the [semmantic commit messages](https://gist.github.com/joshbuchea/6f47e86d2510bce28f8e7f42ae84c716) template; e.g.\n    > feat(moler_decoder): Improve masking of invalid actions\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/microsoft/molecule-generation/",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "molecule-generation",
    "package_url": "https://pypi.org/project/molecule-generation/",
    "platform": null,
    "project_url": "https://pypi.org/project/molecule-generation/",
    "project_urls": {
      "Homepage": "https://github.com/microsoft/molecule-generation/"
    },
    "release_url": "https://pypi.org/project/molecule-generation/0.3.0/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Implementations of deep generative models of molecules.",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15597352,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8e1276dd1cc827baf25bbd192033be409f1be2e00efa5548008d437311b7aa0b",
          "md5": "ee7874c3804078ca96cb631108a971bf",
          "sha256": "54512a07661a823828fd1bad13ed6964291cf1cae92eae92869a8b6f68b0c7e6"
        },
        "downloads": -1,
        "filename": "molecule_generation-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ee7874c3804078ca96cb631108a971bf",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "==3.7.*",
        "size": 980438,
        "upload_time": "2022-04-25T18:27:42",
        "upload_time_iso_8601": "2022-04-25T18:27:42.182064Z",
        "url": "https://files.pythonhosted.org/packages/8e/12/76dd1cc827baf25bbd192033be409f1be2e00efa5548008d437311b7aa0b/molecule_generation-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ebaa35fe22bcdf6ce6c731063ac353f826e0722ee5473b49a22570b42e7b7e6",
          "md5": "5958cb2846f627f57d76c922b9f252a8",
          "sha256": "0296db7a81c0ebbbd307e4a15e712323abebeadfe2e1facda758ff5c9f362bc1"
        },
        "downloads": -1,
        "filename": "molecule_generation-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "5958cb2846f627f57d76c922b9f252a8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "==3.7.*",
        "size": 946283,
        "upload_time": "2022-04-25T18:27:44",
        "upload_time_iso_8601": "2022-04-25T18:27:44.854285Z",
        "url": "https://files.pythonhosted.org/packages/5e/ba/a35fe22bcdf6ce6c731063ac353f826e0722ee5473b49a22570b42e7b7e6/molecule_generation-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c922f134f0ef834dd16362df53af00d098e001e86f5157cd43c93aaef90d2d8f",
          "md5": "6e2d2b98594bb4ff4d1275432ee70534",
          "sha256": "4eeab038168231c3af4eabe396b6ea42e5f49d522bde98e44393e50555ccc404"
        },
        "downloads": -1,
        "filename": "molecule_generation-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6e2d2b98594bb4ff4d1275432ee70534",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "==3.7.*",
        "size": 986052,
        "upload_time": "2022-07-01T13:44:28",
        "upload_time_iso_8601": "2022-07-01T13:44:28.949134Z",
        "url": "https://files.pythonhosted.org/packages/c9/22/f134f0ef834dd16362df53af00d098e001e86f5157cd43c93aaef90d2d8f/molecule_generation-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "680c65cf38dd77937285cad8a296c48725f4501fde89338001c8d80b2f187154",
          "md5": "a74618dab416de948d34fa55df8b54b9",
          "sha256": "c759e3b133c9720feecb1571317ddac18566c07411e1a07284dfd562d4128710"
        },
        "downloads": -1,
        "filename": "molecule_generation-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a74618dab416de948d34fa55df8b54b9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "==3.7.*",
        "size": 950310,
        "upload_time": "2022-07-01T13:44:31",
        "upload_time_iso_8601": "2022-07-01T13:44:31.381994Z",
        "url": "https://files.pythonhosted.org/packages/68/0c/65cf38dd77937285cad8a296c48725f4501fde89338001c8d80b2f187154/molecule_generation-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "91e0bab7c2c8a95648c8ad2baaab648a18b8cdb366b1a08e03413d30537175af",
          "md5": "466bd15f7a7c10106adb0f759ab0c148",
          "sha256": "88c8e0d8340d569a999c47adc2fb0ffb3275fab196efe8819afc0f07c9f3ae70"
        },
        "downloads": -1,
        "filename": "molecule_generation-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "466bd15f7a7c10106adb0f759ab0c148",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 949817,
        "upload_time": "2022-10-31T14:36:27",
        "upload_time_iso_8601": "2022-10-31T14:36:27.440636Z",
        "url": "https://files.pythonhosted.org/packages/91/e0/bab7c2c8a95648c8ad2baaab648a18b8cdb366b1a08e03413d30537175af/molecule_generation-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "91e0bab7c2c8a95648c8ad2baaab648a18b8cdb366b1a08e03413d30537175af",
        "md5": "466bd15f7a7c10106adb0f759ab0c148",
        "sha256": "88c8e0d8340d569a999c47adc2fb0ffb3275fab196efe8819afc0f07c9f3ae70"
      },
      "downloads": -1,
      "filename": "molecule_generation-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "466bd15f7a7c10106adb0f759ab0c148",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 949817,
      "upload_time": "2022-10-31T14:36:27",
      "upload_time_iso_8601": "2022-10-31T14:36:27.440636Z",
      "url": "https://files.pythonhosted.org/packages/91/e0/bab7c2c8a95648c8ad2baaab648a18b8cdb366b1a08e03413d30537175af/molecule_generation-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}