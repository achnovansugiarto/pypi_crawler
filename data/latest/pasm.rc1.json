{
  "info": {
    "author": "Pranay Manocha",
    "author_email": "pranaymnch@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent"
    ],
    "description": "# Learned Perceptual Audio Similarity Metric (PASM) [[Paper]](https://arxiv.org/abs/2001.04460) [[Webpage]](https://pixl.cs.princeton.edu/pubs/Manocha_2020_ADP/)\n\n**A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences**  \n[Pranay Manocha](https://www.cs.princeton.edu/~pmanocha/), [Adam Finkelstein](https://www.cs.princeton.edu/~af/), [Richard Zhang](http://richzhang.github.io/), [Nicholas J. Bryan](https://ccrma.stanford.edu/~njb/), [Gautham J. Mysore](https://ccrma.stanford.edu/~gautham/Site/Gautham_J._Mysore.html), [Zeyu Jin](https://research.adobe.com/person/zeyu-jin/)  \nAccepted at [Interspeech2020](https://arxiv.org/abs/2001.04460)\n\n<img src='https://richzhang.github.io/index_files/audio_teaser.jpg' width=500>\n\nThis is a Tensorflow implementation (a pytorch implementation is [here](https://github.com/adrienchaton/PerceptualAudio_pytorch)) of our audio perceptual metric. It contains (0) minimal code to run our perceptual metric (LPAM), (1) code to train the perceptual metric on our JND dataset, and (2) an example of using our perceptual metric as a loss function for speech denoising.\n\n## (0) Setup and basic usage\n\nRequired python libraries: Tensorflow with GPU support (>=1.14)(uses tensorflow's slim library so doesnt support tf2.0) + Scipy (>=1.1) + Numpy (>=1.14) + Tqdm (>=4.0.0). To install in your python distribution, run ```pip install -r requirements.txt```.\n\nAdditional notes:\n- Warning: Make sure your libraries (Cuda, Cudnn,...) are compatible with the TensorFlow version you're using or the code will not run.\n- Required software (for resampling): [SoX](http://sox.sourceforge.net/), [FFmpeg](https://www.ffmpeg.org/).\n- Important note: At the moment, this algorithm requires using 32-bit floating-point audio files to perform correctly. You can use sox to convert your file.\n- Tested on Nvidia GeForce RTX 2080 GPU with Cuda (>=9.2) and CuDNN (>=7.3.0). CPU mode should also work with minor changes.\n\n\n## There are two ways to run the code:\n\n### Using pip (Quick Start)\nThis version currently supports **evaluating** the trained metric, as well as using the metric for **backpropogation(as a loss function)**. This version currently does not support a simple interface to **train** the metric. For training, please clone this repo and follow the instructions below.\n```bash\npip install pasm\n```\n\n### Cloning from the repository\nFollow the instructions below to: 1) Evaluate the metric on a few examples, 2) Train a metric on our dataset, 3) Use the metric to optimize a downstream task and 4) Use our pretrained speech enhancement model (trained using our loss function metric).\n\n\n### Minimal basic usage as a distance metric\n\nRunning the command below takes two audio files as input and gives the perceptual distance between the files. It should return **distance = 0.1928**. \n\n```\ncd metric_code\npython metric_use_simple.py --e0 ../sample_audio/ref.wav --e1 ../sample_audio/2.wav\n```\n\nFor loading large number of files, batch processing is more efficient. Refer to at [metric_code/metric_use.py](metric_code/metric_use.py) for more information. In short, you need to change the dataloader function `load_full_data_list()`. You also need to provide the path of the trained model as an input argument.\n\n### Navigating this repository\n\n**PercepAudio** - main directory\n - **metric_code** - Section 1, training our metric on our JND dataset\n - **se_code** - Section 2, training a speech enhancement model using our metric, trained above\n - **dataset** - our JND framework and dataset and text files containing perceptual judgments\n - **pre-model** - sample pre-trained models for easy reference\n - **sample_audio** - sample audio files for comparison\n -  **create_space** - sample code for creating perturbations \n\n## (1) Train a perceptual metric on our JND dataset\n\n- **PercepAudio** (main directory)\n   - **metric_code** \n      - main.py (train the loss function)\n      - metric_use.py (use the trained model as a metric)\n      - dataloader.py (collect and load the audio files)\n      - helper.py (misc helper functions)\n      - network_model.py (NN architecture)\n      - *summaries* folder to store the new trained model with tensorboard files\n\n### Download the JND Dataset\n\nGo to [link](http://percepaudio.cs.princeton.edu/icassp2020_perceptual/audio_perception.zip) to download the dataset (about 23GB). After downloading the dataset, unzip the dataset into the project folder *'PerceptualAudio/dataset'*. Here are the steps to be followed:\n\n```\ngit clone https://github.com/pranaymanocha/PerceptualAudio.git\ncd PerceptualAudio/dataset\nunzip audio_perception.zip\n```\n\nMore information on the JND framework can be found in the paper [here](https://arxiv.org/abs/2001.04460). The text files in the subfolder *dataset* contain information about human perceptual judgments. This sets up the dataset for training the loss function.\n\nFor using a custom dataset, you need to follow the following steps:\n1. Follow a similar framework to obtain human perceptual judgments and store them in the *dataset* subdirectory. Also create a text file containing the results of all human perceptual judgments using a convention *reference_audio_path \\t noisy_audio_path \\t human judgment(same(0)/different(1))*.\nFor an example, please see any text file in *dataset* subdirectory. \n2. Make changes to the dataloader.py function to reflect the new name/path of the folders/text file. \n3. Run the main.py function (after selecting the most appropriate set of parameters). \n\nOnce you train a model, you can use the trained model to infer the distances between audio recordings.\n\n### Using the trained metric for eval\nYou can use one of our trained models as a metric. You can also use your own trained loss function as a metric for evaluation.\n\nFor using a custom dataset, you need to follow the following steps:\n1. Make sure that you have all the right requirements as specified in the *requirements.txt* file on the repo.\n2. Look at *metric_use.py* for more information on how to use the trained model to infer distances between audio files. In short, you need to change the dataloader function (namely function *load_full_data_list()*). You also need to provide the path of the trained model as an input argument. Please look at metric_use.py for full information. \n\n\n### Pretrained Model\n“Off-the-shelf” deep network embeddings have been used as an effective training objective that have been shown to correlate well with human perceptual judgments in the vision setting, even without being\nexplicitly trained on perceptual human judgments. We first investigate if similar trends hold in the audio setting. Hence, we first train a model on two audio datasets: Acoustic scene classification and Domestic audio tagging tasks of the [DCASE 2016 Challenge](https://www.cs.tut.fi/sgn/arg/dcase2016/). We keep the architecture of the model same to compare between different training regimes. More information on training this pretrained \"off-the-shelf\" model can be found in [this](https://github.com/francoisgermain/SpeechDenoisingWithDeepFeatureLosses) repo.\n\n### Final Summary of the models (more info in the paper)\n1. **pretrained** - pretrained \"off-the-shelf\" model\n2. **linear** -  training linear layers over the pretrained \"off-the-shelf\" model\n3. **finetune** - loading the pretrained \"off-the-shelf weights\" but training both the linear layer and the bulk model\n4. **scratch** - training the full model from randomly initialized weights.  \n\n## (2) Speech denoising with our perceptual metric as a loss function\n\nAs an application for our loss function, we use the trained loss function to train a Speech Enhancement Model. We use the Edinburgh Datashare publicly available dataset [here](https://datashare.is.ed.ac.uk/handle/10283/2791). We use the same dataset with any alteration except resampling the dataset at 16KHz.\nDirect links to download the dataset and resampling can be found [here](https://github.com/francoisgermain/SpeechDenoisingWithDeepFeatureLosses). Follow the instructions to download the SE data in the script *download_sedata.sh* in the above repo. In general, we follow the same directory structure.\n\n**PercepAudio** (main directory)\n - **se_code** (train a SE model using the above trained loss function)\n    - se_train.py (train the SE system)\n    - se_infer.py (Infer the SE system)\n    - *dataset*\n      - trainset_clean\n      - trainset_noisy\n      - valset_clean\n      - valset_noisy\n    - data_import.py (Dataloading)\n    - network_model.py (NN architecture)\n    - *summaries* folder to store the new trained model with tensorboard files\n\nAfter you download the dataset, follow this directory structure to copy the audio files accordingly.\n\n### Training the SE Model\nWe make use of our metric as a loss function for training an SE model. After you have downloaded the noisy dataset above (and kept the files at the correct locations), you can start training by running the command:\n```\npython se_train.py --args.....\n```\nThe trained model is stored under the *summaries* folder under the folder name which you specify as an argument. The model is saved as *se_model_'+str(seconds)+'.ckpt'* where seconds is the time in seconds since epoch so that the training can be easily monitered.\n\n### Inferring the SE Model\nAfter you train a SE model, you can use the same trained model to denoise audio files. Simply run \n```\npython se_infer.py --args....\n```\nwith a suitable set of arguements. The denoised files will be stored in the folder name which you specify as an argument in the script. As the SE model is big, it takes a couple of hours to run on a CPU and less than 5 minutes on a GPU.\n\n### Citation\n\nIf you use our code for research, please use the following to cite.\n\n```\n@inproceedings{Manocha:2020:ADP,\n   author = \"Pranay Manocha and Adam Finkelstein and Richard Zhang and Nicholas J.\n      Bryan and Gautham J. Mysore and Zeyu Jin\",\n   title = \"A Differentiable Perceptual Audio Metric Learned from Just Noticeable\n      Differences\",\n   booktitle = \"Interspeech\",\n   year = \"2020\",\n   month = oct\n}\n```\n\n### License\nThe source code is published under the [MIT license](https://choosealicense.com/licenses/mit/). See LICENSE for details. In general, you can use the code for any purpose with proper attribution. If you do something interesting with the code, we'll be happy to know. Feel free to contact us. The primary contact is Pranay Manocha.<br/>\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/pranaymanocha/PerceptualAudio",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pasm",
    "package_url": "https://pypi.org/project/pasm/",
    "platform": "",
    "project_url": "https://pypi.org/project/pasm/",
    "project_urls": {
      "Homepage": "https://github.com/pranaymanocha/PerceptualAudio"
    },
    "release_url": "https://pypi.org/project/pasm/0.0.3/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A pip package for perceptual audio metric",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8347043,
  "releases": {
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ef1315b90244317356569295b1ff1978b7cac1285338b10f8f537af0a2c8ecda",
          "md5": "484e857c55bfee0425e17fc54b419240",
          "sha256": "13f6a505d77dd9e5fe90aa51860bf881e8908e185235f73313890b7c561b9cb3"
        },
        "downloads": -1,
        "filename": "pasm-0.0.3-py2-none-any.whl",
        "has_sig": false,
        "md5_digest": "484e857c55bfee0425e17fc54b419240",
        "packagetype": "bdist_wheel",
        "python_version": "py2",
        "requires_python": null,
        "size": 14550,
        "upload_time": "2020-10-06T02:53:17",
        "upload_time_iso_8601": "2020-10-06T02:53:17.960516Z",
        "url": "https://files.pythonhosted.org/packages/ef/13/15b90244317356569295b1ff1978b7cac1285338b10f8f537af0a2c8ecda/pasm-0.0.3-py2-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "92c151df1a1248df437fe1e977bc3a0d56411dc8196f8855893944994d950e18",
          "md5": "bf4729621ff3cb9e3cb79ff75c74bbf8",
          "sha256": "a6c74c7226795a40907b31f2994786fe73e9c354256a155079b8516812d6136b"
        },
        "downloads": -1,
        "filename": "pasm-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "bf4729621ff3cb9e3cb79ff75c74bbf8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16260,
        "upload_time": "2020-10-06T02:53:19",
        "upload_time_iso_8601": "2020-10-06T02:53:19.011331Z",
        "url": "https://files.pythonhosted.org/packages/92/c1/51df1a1248df437fe1e977bc3a0d56411dc8196f8855893944994d950e18/pasm-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ef1315b90244317356569295b1ff1978b7cac1285338b10f8f537af0a2c8ecda",
        "md5": "484e857c55bfee0425e17fc54b419240",
        "sha256": "13f6a505d77dd9e5fe90aa51860bf881e8908e185235f73313890b7c561b9cb3"
      },
      "downloads": -1,
      "filename": "pasm-0.0.3-py2-none-any.whl",
      "has_sig": false,
      "md5_digest": "484e857c55bfee0425e17fc54b419240",
      "packagetype": "bdist_wheel",
      "python_version": "py2",
      "requires_python": null,
      "size": 14550,
      "upload_time": "2020-10-06T02:53:17",
      "upload_time_iso_8601": "2020-10-06T02:53:17.960516Z",
      "url": "https://files.pythonhosted.org/packages/ef/13/15b90244317356569295b1ff1978b7cac1285338b10f8f537af0a2c8ecda/pasm-0.0.3-py2-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "92c151df1a1248df437fe1e977bc3a0d56411dc8196f8855893944994d950e18",
        "md5": "bf4729621ff3cb9e3cb79ff75c74bbf8",
        "sha256": "a6c74c7226795a40907b31f2994786fe73e9c354256a155079b8516812d6136b"
      },
      "downloads": -1,
      "filename": "pasm-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "bf4729621ff3cb9e3cb79ff75c74bbf8",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 16260,
      "upload_time": "2020-10-06T02:53:19",
      "upload_time_iso_8601": "2020-10-06T02:53:19.011331Z",
      "url": "https://files.pythonhosted.org/packages/92/c1/51df1a1248df437fe1e977bc3a0d56411dc8196f8855893944994d950e18/pasm-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}