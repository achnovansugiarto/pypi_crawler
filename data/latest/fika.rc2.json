{
  "info": {
    "author": "Karthik Bhaskar",
    "author_email": "karthikrajabk@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# Fika\n\n*\"A collection of tools for Data Scientists and ML Engineers to automate their workflow of performing analysis to deploying models and pipelines.\"*\n\nFika is a library/platform that automates your data science and analytical tasks at any stage in the pipeline. Fika is, at its core, a uniform API that helps automate analytical techniques from various libaries such as pandas, sci-kit learn, spacy, etc.\n\n**Fika** in Swedish - *A moment to slow down and appreciate the good things in life*\n\n## Analysis with Fika\n\n```python\nimport fika as fk\nimport pandas as pd\n\nx_train = pd.read_csv('https://raw.githubusercontent.com/karthikraja95/fika/master/examples/data/train.csv') # load data into pandas\n\n# Initialize Data object with training data\n# By default, if no test data (x_test) is provided, then the data is split with 20% going to the test set\n# \n# Specify predictor field as 'Survived'\ndf = fk.Classification(x_train, target='Survived')\n\ndf.x_train # View your training data\ndf.x_test # View your testing data\n\ndf # Glance at your training data\n\ndf[df.Age > 25] # Filter the data\n\ndf.x_train['new_col'] = [1,2] # This is the exact same as the either of code above\ndf.x_test['new_col'] = [1,2]\n\ndf.data_report(title='Titanic Summary', output_file='titanic_summary.html') # Automate EDA with pandas profiling with an autogenerated report\n\ndf.describe() # Display a high level view of your data using an extended version of pandas describe\n\ndf.column_info() # Display info about each column in your data\n\ndf.describe_column('Fare') # Get indepth statistics about the 'Fare' column\n\ndf.mean() # Run pandas functions on the aethos objects\n\ndf.missing_values # View your missing data at anytime\n\ndf.correlation_matrix() # Generate a correlation matrix for your training data\n\ndf.predictive_power() # Calculates the predictive power of each variable\n\ndf.autoviz() # Runs autoviz on the data and runs EDA on your data\n\ndf.pairplot() # Generate pairplots for your training data features at any time\n\ndf.checklist() # Will provide an iteractive checklist to keep track of your cleaning tasks\n\n```\n**NOTE:** One of the benefits of using `fika` is that any method you apply on your train set, gets applied to your test dataset. For any method that requires fitting (replacing missing data with mean), the method is fit on the training data and then applied to the testing data to avoid data leakage.\n\n```python\n# Replace missing values in the 'Fare' and 'Embarked' column with the most common values in each of the respective columns.\ndf.replace_missing_mostcommon('Fare', 'Embarked')\n\n# Replace missing values in the 'Age' column with a random value that follows the probability distribution of the 'Age' column in the training set. \ndf.replace_missing_random_discrete('Age')\n\ndf.drop('Cabin') # Drop the cabin column\n```\n\nAs you've started to notice, alot of tasks to df the data and to explore the data have been reduced down to one command, and are also customizable by providing the respective keyword arguments.\n\n\n```python\n# Create a barplot of the mean surivial rate grouped by age.\ndf.barplot(x='Age', y='Survived', method='mean')\n\n# Plots a scatter plot of Age vs. Fare and colours the dots based off the Survived column.\ndf.scatterplot(x='Age', y='Fare', color='Survived')\n\n# One hot encode the `Person` and `Embarked` columns and then drop the original columns\ndf.onehot_encode('Person', 'Embarked', keep_col=False) \n\n```\n\n## Modelling with Fika\n\n### Running a Single Model\n\nModels can be trained one at a time or multiple at a time. They can also be trained by passing in the params for the sklearn, xgboost, etc constructor, by passing in a gridsearch dictionary & params, cross validating with gridsearch & params.\n\nAfter a model has been ran, it comes with use cases such as plotting RoC curves, calculating performance metrics, confusion matrices, SHAP plots, decision tree plots and other local and global model interpretability use cases.\n\n```python\nlr_model = df.LogisticRegression() # Train a logistic regression model\n\n# Train a logistic regression model with gridsearch\nlr_model = df.LogisticRegression(gridsearch={'penalty': ['l1', 'l2']}, random_state=42)\n\n# Crossvalidate a a logistic regression model, displays the scores and the learning curve and builds the model\nlr_model = df.LogisticRegression()\nlr_model.cross_validate(n_splits=10) # default is strat-kfold for classification  problems\n\n# Build a Logistic Regression model with Gridsearch and then cross validates the best model using stratified K-Fold cross validation.\nlr_model = model.LogisticRegression(gridsearch={'penalty': ['l1', 'l2']}, cv_type=\"strat-kfold\") \n\nlr_model.help_debug() # Interface with items to check for to help debug your model.\n\nlr_model.metrics() # Views all metrics for the model\nlr_model.confusion_matrix()\nlr_model.decision_boundary()\nlr_model.roc_curve()\n```\n\n### Running multiple models in parallel\n\n```python\n# Add a Logistic Regression, Random Forest Classification and a XGBoost Classification model to the queue.\nlr = df.LogisticRegression(random_state=42, model_name='log_reg', run=False)\nrf = df.RandomForestClassification(run=False)\n\n\ndf.run_models() # This will run all queued models in parallel\ndf.run_models(method='series') # Run each model one after the other\n\ndf.compare_models() # This will display each model evaluated against every metric\n\n# Every model is accessed by a unique name that is assiged when you run the model.\n# Default model names can be seen in the function header of each model.\n\ndf.log_reg.confusion_matrix() # Displays a confusion matrix for the logistic regression model\ndf.rf_cls.confusion_matrix() # Displays a confusion matrix for the random forest model\n```\n\n## Model Interpretability\n\nAs mentioned in the Model section, whenever a model is trained you have access to use cases for model interpretability as well. There are prebuild SHAP usecases and an interactive dashboard that is equipped with LIME and SHAP for local model interpretability and Morris Sensitivity for global model interpretability.\n\n```python\nlr_model = model.LogisticRegression(random_state=42)\n\nlr_model.summary_plot() # SHAP summary plot\nlr_model.force_plot() # SHAP force plot\nlr_model.decision_plot() # SHAP decision plot\nlr_model.dependence_plot() # SHAP depencence plot\n\n# Creates an interactive dashboard to interpret predictions of the model\nlr_model.interpret_model() \n```\n\n## Code Generation\n\nCurrently you are only able to export your model to be ran a service, and will be able to automatically generate the required files. The automatic creation of a data pipeline is still in progress.\n\n```python\n\nlr_model.to_service('titanic')\n```\n\nNow navigate to 'your_home_folder'('~' on linux and Users/'your_user_name' on windows)/.fika/projects/titanic/ and you will see the files needed to run the model as a service using FastAPI and uvicorn. \n\n## Setup\n\n**Python Requirements**: 3.6, 3.7\n\nRUN: `pip install aethos`\n\n\n## How to use Fika\n\nTake a look at this [fika.ipynb](https://github.com/karthikraja95/fika/blob/master/examples/fika.ipynb) notebook.\n\n## For Developers\n\nTo install packages `pip3 install -r requirements-dev.txt`",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/karthikraja95/fika",
    "keywords": "datascience,machinelearning,automation,analysis,automl,python,eda",
    "license": "GPL-3.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "fika",
    "package_url": "https://pypi.org/project/fika/",
    "platform": "",
    "project_url": "https://pypi.org/project/fika/",
    "project_urls": {
      "Homepage": "https://github.com/karthikraja95/fika"
    },
    "release_url": "https://pypi.org/project/fika/1.0.1/",
    "requires_dist": null,
    "requires_python": ">= 3.6",
    "summary": "Automate Data Science Workflow from Exploratory Data Analysis to Model Deployment",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11201189,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "98d17babbe67ffbf4e522afb1a9fb2f7810b4b7077cfa6f1c432c570d56a727e",
          "md5": "028e633e8df9efab16c851cd78ed038c",
          "sha256": "96c6f0b85a11b3100f057a39004df87ba2bf40ea78355c81c19f4cc7da784ab3"
        },
        "downloads": -1,
        "filename": "fika-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "028e633e8df9efab16c851cd78ed038c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">= 3.6",
        "size": 119966,
        "upload_time": "2021-08-16T22:12:18",
        "upload_time_iso_8601": "2021-08-16T22:12:18.195590Z",
        "url": "https://files.pythonhosted.org/packages/98/d1/7babbe67ffbf4e522afb1a9fb2f7810b4b7077cfa6f1c432c570d56a727e/fika-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1f1577d461ebfc720bffe98f78f0ff1cd935fab236ab13e40ffc45b0873c9f4e",
          "md5": "ba3a46f2f651b6ebce371192bb2a68c9",
          "sha256": "c82954b0f91f5d5e8df8355510e8d5053b890500a0ba245654c74a46b59f8bc4"
        },
        "downloads": -1,
        "filename": "fika-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ba3a46f2f651b6ebce371192bb2a68c9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">= 3.6",
        "size": 123080,
        "upload_time": "2021-08-17T14:31:28",
        "upload_time_iso_8601": "2021-08-17T14:31:28.826960Z",
        "url": "https://files.pythonhosted.org/packages/1f/15/77d461ebfc720bffe98f78f0ff1cd935fab236ab13e40ffc45b0873c9f4e/fika-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1f1577d461ebfc720bffe98f78f0ff1cd935fab236ab13e40ffc45b0873c9f4e",
        "md5": "ba3a46f2f651b6ebce371192bb2a68c9",
        "sha256": "c82954b0f91f5d5e8df8355510e8d5053b890500a0ba245654c74a46b59f8bc4"
      },
      "downloads": -1,
      "filename": "fika-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "ba3a46f2f651b6ebce371192bb2a68c9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">= 3.6",
      "size": 123080,
      "upload_time": "2021-08-17T14:31:28",
      "upload_time_iso_8601": "2021-08-17T14:31:28.826960Z",
      "url": "https://files.pythonhosted.org/packages/1f/15/77d461ebfc720bffe98f78f0ff1cd935fab236ab13e40ffc45b0873c9f4e/fika-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}