{
  "info": {
    "author": "Zdenko Hrƒçek",
    "author_email": "zdenulo@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# Cloud Workflows SDK\n\nThis is unofficial SDK for [Google Cloud Workflows](https://cloud.google.com/workflows/).  \n**Why to write YAML or JSON file to create a workflow when we could do it in Python?**\n\nThis package behaves like wrapper for various Workflows objects and in the end generates YAML code (no rocket science).  \nIt doesn't provide local simulator. \nMain idea/motivation behind this project:  \n- write workflow in Python (not YAML or JSON)(nothing against YAML or JSON, for some people it isn't so convenient)   \n- reuse existing steps/workflows  \n\nSDK follows closely Workflows naming and conventions\n\ntake a look at [samples](./samples) folder to get the idea how workflows look like.\nAt the moment, this is just simple Proof of Concept to see if/how it could be done and to get feedback. \n\n## Install\nvia pypi\n`pip install cloud-workflows`  \nor from code\n`python setup.py install`\n\n## Get started with workflows\nStructure of library:  \n`workflows` - base objects   \n`workflows.standard` - some functions from Workflows Standard library (not complete)  \n`workflows.connectors` - here are some Google Cloud connectors with subset of functions. Definitely not complete list,\nbut can be easily extended  \n\n## Main components\n`Workflow` - (main) Workflow  \n`Subworkflow`  - Subworkflow definition  \n`Step` - A single step in a workflow  \n`Return` - set variable as return  \n`Raise` - raise exception   \n`Retry` - Rules for retry  \n`Backoff` - back off config for Retry\n\n## Concepts\nAs examples explaining concepts and usage, I am using code snippets, not fully functional workflows. For full functional examples, take a look at \n[samples](./samples)\n\n## Workflow\n\n```python\nw = Workflow()\n```\nusing input parameters\n```python\nw = Workflow(params='[args]')\n```\ninput parameters are enclosed in string and bracket '[args]', similar for multiple input variables \n```python\nw = Workflow(params='[args1, args2]')\n```\nWorkflow consist of steps (one or more). Every Step consists of name, \"operation\" and either 'result' or 'return' and you can\nset 'next' step.\n\n## Step\n\nStep is defined with a name, \"operation\", and optionally next (step).  \nFor example, this step is called 'log_wiki_response' and it calls `SysLog` operation which is shortcut for `sys.log` \nnative Workflows function\n```python\nStep('log_wiki_response', SysLog(json='${wikiResult}'))\n```\n\n```yaml\n- log_wiki_response:\n  call: sys.log\n  args:\n    json: ${wikiResult}\n```\n\nnext Step can be defined as a string \n```python\nStep('getCurrentTime', Http('get', 'https://us-central1-workflowsample.cloudfunctions.net/datetime',\n                result='currentDateTime'), next_='next_step_name')\n```\nor pass Step instance\n```python\nend_step = Step('end_step', Return('output'))\nStep('getCurrentTime', Http('get', 'https://us-central1-workflowsample.cloudfunctions.net/datetime',\n                result='output'), next_=end_step)\n```\n\n## Operations\nStep Operation can variable Assign, Call, TryExceptRetry, For (loop), Switch, Raise, Return, SubWorkflow\n\n### Variables & Expressions\nVariables are defined as strings.  \nSame is with expressions ( ${some_variable} ).  \nVariable is then referenced as a string.  \nTake a look at Future/TODO to read some ideas.  \n\nExample with assigning variable. `project` variable is set with `${sys.get_env(\"GOOGLE_CLOUD_PROJECT_ID\")}`\n\n```python\nstep = Step('asign_variables', Assign(variables={'bucket': 'my-gcs-bucket',\n                                                 'project': '${sys.get_env(\"GOOGLE_CLOUD_PROJECT_ID\")}'}))\n```\nthis generates following \n```yaml\nmain:\n  steps:\n  - asign_variables:\n      assign:\n      - bucket: my-gcs-bucket\n      - project: ${sys.get_env(\"GOOGLE_CLOUD_PROJECT_ID\")}\n```\nor assigning variables as dictionary\n```python\n# assign variable\nStep('step_name', Assign({'myvar_name': 'myvar_value', 'myvar2': 'hello'}))\n```\nwhich generates this yaml\n```yaml\n- step_name:\n      assign:\n      - myvar_name: myvar_value\n      - myvar2: hello\n```\nor defining variable in Call step ('request_result')\n```python\nStep('call_example', Call('http.get', {'url': 'https://www.gcpweekly.com/'}, 'request_result'))\n```\nwhich generates this yaml:\n```yaml\n- http_request_example:\n      call: http.get\n      args:\n        url: https://www.gcpweekly.com/\n      result: request_result\n```\n\n### For Operation (Iteration)\nReference: [https://cloud.google.com/workflows/docs/reference/syntax/iteration](https://cloud.google.com/workflows/docs/reference/syntax/iteration)\nExample \n```python\nFor(list_values='${api_res.body.data}',  # here is list from API response\n    loop_variable='item',  # variable name in loop step\n    steps=[  # steps to execute\n        # prepare row, get desired data\n        Step('set_row', Assign(variables=[\n            {'row':\n                 {'State': '${item.State}',\n                  'Year': '${item.Year}',\n                  'Population': '${item.Population}',\n                  }\n             },\n        ]\n        )),\n        # make BigQuery request\n        Step('insert_bq',\n             BigQueryTableData.insertAll('${gcp_project}', '${bq_dataset}', '${bq_table}', insert_body,\n                                         'bq_insert')),\n        # log response\n        Step('log_item', SysLog(json='${bq_insert}', severity='INFO'))\n    ])))\n```\n`list_values` and `loop_variable` are strings, whereas in `steps` is defined list of Steps to be executed.  \ncode above generates following yaml\n```yaml\nsteps:\n  - extract_data:\n      for:\n        value: item\n        in: ${api_res.body.data}\n        steps:\n        - set_row:\n            assign:\n            - row:\n                State: ${item.State}\n                Year: ${item.Year}\n                Population: ${item.Population}\n        - insert_bq:\n            call: googleapis.bigquery.v2.tabledata.insertAll\n            args:\n              datasetId: ${bq_dataset}\n              projectId: ${gcp_project}\n              tableId: ${bq_table}\n              body:\n                rows:\n                - json: ${row}\n            result: bq_insert\n        - log_item:\n            call: sys.log\n            args:\n              json: ${bq_insert}\n              severity: INFO\n```\n\n### Conditions / Switch\n[https://cloud.google.com/workflows/docs/reference/syntax/conditions](https://cloud.google.com/workflows/docs/reference/syntax/conditions)\n\n`Condition` and `Switch` go together\nSwitch is basically list of Conditions\n`Condition` is string in which logical condition is expressed, for example:  \n`${ocr_status.body.metadata.state!=\"DONE\"}` or  \n`${${first_result.body.SomeField < 10}`  \n\nCondition has 'next' step, which is executed when condition is met, and as input either Step reference can be passed or its' name.  \n\n```python\nwait_step = Step('wait_step', SysSleep(2))\nstep_end = Step('last_step', return_='${output}')\ncheck_status = Step('check_status', Switch([\n    Condition('${ocr_status.body.metadata.state!=\"DONE\"}', next_=wait_step),\n    Condition('${ocr_status.body.metadata.state==\"DONE\"}', next_='last_step')\n]))\n```\nfor full example, take a look at [vision_pdf_ocr.py](./samples/vision_pdf_ocr.py) example\n\n### TryExceptRetry\n[https://cloud.google.com/workflows/docs/reference/syntax/catching-errors](https://cloud.google.com/workflows/docs/reference/syntax/catching-errors)\n\n`TryExceptRetry` encapsulates try/except/retry structure for error handling.  \nThere is a list of try Steps, Steps to execute during 'except' part\n```python\n\nTryExceptRetry([create_bq_table_step,\n                Step('log_response', SysLog(json='${bq_response}')),\n                Step('output', Return('${bq_response}'))\n                ],\n               as_='e',\n               except_steps=[\n                   Step('known_errors', SysLog(json='${e}')),\n               ])\n                )\n```\nthis generates following yaml\n```yaml\n- create_bq_table:\n  try:\n    steps:\n    - create_bq_table_req:\n        call: googleapis.bigquery.v2.tables.insert\n        args:\n          datasetId: ${bq_dataset}\n          projectId: ${gcp_project}\n          body:\n            tableReference:\n              projectId: ${gcp_project}\n              datasetId: ${bq_dataset}\n              tableId: ${bq_table}\n            schema:\n              fields:\n              - name: State\n                type: STRING\n              - name: Year\n                type: STRING\n              - name: Population\n                type: INTEGER\n        result: bq_response\n    - log_response:\n        call: sys.log\n        args:\n          json: ${bq_response}\n    - output:\n        return: ${bq_response}\n  except:\n    as: e\n    steps:\n    - known_errors:\n        call: sys.log\n        args:\n          json: ${e}\n```\n\n### Raise\n[https://cloud.google.com/workflows/docs/reference/syntax/raising-errors](https://cloud.google.com/workflows/docs/reference/syntax/raising-errors)  \nRaising error/exception\n\n```python\ns = Step('raise_string', Raise('my error'))\ns2 = Step('raise_dict', Raise({'code': 55, 'msg': 'my error'}))\n```\n\n```yaml\nmain:\n  steps:\n  - raise_exception:\n      raise: my error\n  - raise_dict:\n      raise:\n        code: 55\n        msg: my error\n```\n\n### Retry\n[https://cloud.google.com/workflows/docs/reference/syntax/retrying](https://cloud.google.com/workflows/docs/reference/syntax/retrying)\n\nRetry allows to define either policy or predicate with other configurations in TryExceptRetry block.  \nFor policy retry values, there is enumeration `RetryPolicy`. This can be used also as predicate\n\n```python\nw = Workflow()\nreq = Http('get', 'https://www.google.com')\ns = Step('policy_example',\n        TryExceptRetry(\n            try_steps=[req],\n            retry=Retry(policy=RetryPolicy.default))\n         )\n\ns2 = Step('predicate_example',\n          TryExceptRetry(try_steps=[req],\n                         retry=Retry(predicate=RetryPolicy.default,\n                                     max_retries=10,\n                                     backoff=BackOff(1, 90, 3))\n                         )\n          )\n```\noutput in yaml:\n```yaml\nmain:\n  steps:\n  - policy_example:\n      try:\n        steps:\n        - call: http.get\n          args:\n            url: https://www.google.com\n      retry: ${http.default_retry}\n  - predicate_example:\n      try:\n        steps:\n        - call: http.get\n          args:\n            url: https://www.google.com\n      retry:\n        predicate: ${http.default_retry}\n        max_retries: 10\n        backoff:\n          initial_delay: 1\n          max_delay: 90\n          multiplier: 3\n```\n\n### Return\nReturn is a wrapper for Step to return variable value.\nSimplified example\n```python\nStep('http_request_example',Call('http.get', {'url': 'https://www.gcpweekly.com/'}, 'request_result'))\nStep('return_response', Return('request_result'))\n```\nIn the first step is defined variable `request_result` and in the second step is returned with `Return` object\n\n```yaml\nmain:\n  steps:\n  - http_request_example:\n      call: http.get\n      args:\n        url: https://www.gcpweekly.com/\n      result: request_result\n  - return_response:\n      return: ${request_result}\n```\n\n### Subworkflows\nSubworkflows are defined with class Subworkflow. They are almost the same as Workflows, i.e. they consist of a name, \nlist of Steps and params. Difference with Workflow class is in output rendering\n\n```python\nw = Workflow()\n# define subworkflow with name and input parameter. We'll add 2 steps\nsub = SubWorkflow('make_get_req', params='[url]')\n# The first step is to make request based on url input parameter\nsub.add_step(Step('http_get', Http('get', '${url}', result='req_out')))\n# Second step is to log response\nsub.add_step(Step('log_result', SysLog(json='${req_out}', severity='INFO')))\n# add subworkflow to Workflow\nw.add_subworkflow(sub)\n\n# add two calls to subworkflow\nw.add_step(Step('http_get1', Call(sub, {'url': 'https://www.gcpweekly.com'})))\nw.add_step(Step('http_get2', Call(sub, {'url': 'https://www.the-swamp.info'})))\n```\noutput in yaml:\n\n```yaml\nmain:\n  steps:\n  - http_get1:\n      call: make_get_req\n      args:\n        url: https://www.gcpweekly.com\n  - http_get2:\n      call: make_get_req\n      args:\n        url: https://www.the-swamp.info\nmake_get_req:\n  params: [url]\n  steps:\n  - http_get:\n      call: http.get\n      args:\n        url: ${url}\n      result: req_out\n  - log_result:\n      call: sys.log\n      args:\n        json: ${req_out}\n        severity: INFO\n```\n\nyou can add existing workflow as subworkflow. You need to rename workflow's name (if it's 'main') \nsee example in `api_datausa_to_bq.py`\n\n## Connectors \nConnectors module is implementation of Workflow connectors. At the moment very few are implemented, but it should be \nquite straightforward to implement the rest.  \nIt all boils to return Call operation.\n\nOne thing is that in some cases input parameters in functions are expected to be passed as variable names and in other cases values.\nIn some cases where \"path\" is constructed, like in PubSub we pass variable names\n```python\nname = f'${{\"projects/\" + {project} + \"/topics/\" + \"{topic_name}\" }}'\n```\nhere `project_name` and `topic_name` are passed as a variable names, since whole \"name\" is expression. For example:\n```python\nw.add_step(Step('assign_step', Assign({\n    'ps_topic': 'mytopic',\n    'gcp_project': '${sys.get_env(\"GOOGLE_CLOUD_PROJECT_ID\")}'\n})))\nw.add_step(Step('create_topic', PubSub.create_topic('ps_topic', 'gcp_project', 'ps_output')))\n```\n\non the other hand, for example in BigQuery, input variables are expected to be values, so it's common that variables are \nwrapped as expressions when calling a function\n\n```python\nStep('create_bq_table_req',\n    BigQueryTables.insert('${bq_dataset}', '${gcp_project}', bq_table_reference,\n                          result='bq_response'))\n```\nAlso for now, I'm leaving up to users to correctly construct (in some cases, quite complex) input body.\nSee for example [bq_create_table.py](./samples/bq_create_table.py) or [xkcd.py](./samples/xkcd.py)\n\n## Deployment\n\nWorkflow YAML code can be generated by calling Worflow method `to_yaml()` and paste content to Cloud Console :)\n```python\nprint(w.to_yaml())  \n```\nor generate content into the file with method `to_yaml_file()`\n```python\nw.to_yaml_file('my_workflow.yaml')\n```\n\n### Building on existing blocks\n\nHaving basic Workflows constructs, allows us not to create simpler and reusable components.  \nFor example Http is created based on Call (see in [workflows.core.py](./workflows/core.py))\nand simplify usage of previous example to this:\n```python\nHttp('get', 'https://cloud.google.com/workflows')\n```\nOther example is `workflows.connectors` module. It's completely build on top of `Call`\n\n## Future / Thoughts / TODO\n\n## Missing features\nSome functionalities/features are not implemented, namely functions like for example `sys.get_env`. In such cases, we're\nleft to use hard coded string:\n```yaml\nAssign(variables=[    \n    {'gcp_project': '${sys.get_env(\"GOOGLE_CLOUD_PROJECT_ID\")}'}\n])\n```\nHopefully in some of the next releases, it will be implemented. The issue is related to how yaml renders Python objects.\n\n\n### Variables\nMy initial idea was to provide possibility of variable definition and then reference it throughout the workflow code. \nThis has advantage of potentially reducing human/typo errors. \n\nsimple case when basically variable name is passed (pseudo code)\n```python\nw = Workflow()\napi_response = Variable('api_res')\nw.add_step(Step('fetch_data', Http('get', 'https://www.gcpweekly.com/', result=api_response)))\nw.add_step(Step('return_output', Return(api_response)))\n```\n\nit would get a bit trickier if variable would have complex fields.\nPseudo code\n```python\nw = Workflow()\n\n#(inspired by Django, SQLAlchemy, maybe some other structure would be simpler and sufficient)\nclass ApiResponse(Variable):\n    body = Field()\n    code = Field()\n    headers = Field()\n    \nw.add_step(Step('fetch_data', Http('get', 'https://www.gcpweekly.com/', result=ApiResponse)))\n# we could now reference just 'body'\nw.add_step(Step('return_output', Return(ApiResponse.body)))\n```\nbut I'm not sure if this would be a right direction, I'm trying to find some better ways than using strings on the \nother hand I wouldn't like to make it too much complex.\n\n### Connectors\nIn connections with variables, one possible simplification would be to have defined\nvariables as input/outputs of GCP Connectors, where those objects can be pretty complex, so that could simplify.  \nI think those definitions exists (proto messages?) and could be somehow generated into Python objects, but this is a bit\ndistant future.  \n\n\nVersions.\nThere can be multiple version for product, at the moment I'm handling the latest, although it would be fine if all \nversions would be supported\n\n### Rest\nAdding Tests  \nComplete rest of stuff from core/standard features  \nReturn should return list of variables  \nEnumerations (Auth, Http methods...)  \nWhen calling subworkflows, check if args are matching expected input parameters that subworkflow expects  \nEasier deployment via Workflows API  \nGenerally add verifications so human errors are prevented as much as possible.  \nAbility to generate JSON (theoretically it should be possible)  \n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/zdenulo/cloud-workflows",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cloud-workflows",
    "package_url": "https://pypi.org/project/cloud-workflows/",
    "platform": "",
    "project_url": "https://pypi.org/project/cloud-workflows/",
    "project_urls": {
      "Homepage": "https://github.com/zdenulo/cloud-workflows"
    },
    "release_url": "https://pypi.org/project/cloud-workflows/0.0.1/",
    "requires_dist": [
      "pyyaml",
      "yamlable"
    ],
    "requires_python": "",
    "summary": "Unoffical SDK for Cloud Workflows",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12224914,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "67d2e03de9a6c5f03075e8a998b4973e6e932dedf8cf7780cca2484e3e21cac0",
          "md5": "80bad68a22b0305a040a325eac43bbb6",
          "sha256": "54234fe26c214a7fa9a3445acb3de92b5e7c7d2e8a211b7515e2d74248d86fec"
        },
        "downloads": -1,
        "filename": "cloud_workflows-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "80bad68a22b0305a040a325eac43bbb6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16055,
        "upload_time": "2021-12-06T16:32:27",
        "upload_time_iso_8601": "2021-12-06T16:32:27.796296Z",
        "url": "https://files.pythonhosted.org/packages/67/d2/e03de9a6c5f03075e8a998b4973e6e932dedf8cf7780cca2484e3e21cac0/cloud_workflows-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "96d37167e1fa99ac657991152d22b2af7804ef2b88de975d6a090801cfbd6f27",
          "md5": "f32ac5775e4463dc9ac5d480d8ea3107",
          "sha256": "3c0ab94abb43719ffc56915bfed11983bdb68673ee67f634920c0722ea3744f8"
        },
        "downloads": -1,
        "filename": "cloud-workflows-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f32ac5775e4463dc9ac5d480d8ea3107",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13668,
        "upload_time": "2021-12-06T16:32:29",
        "upload_time_iso_8601": "2021-12-06T16:32:29.639886Z",
        "url": "https://files.pythonhosted.org/packages/96/d3/7167e1fa99ac657991152d22b2af7804ef2b88de975d6a090801cfbd6f27/cloud-workflows-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "67d2e03de9a6c5f03075e8a998b4973e6e932dedf8cf7780cca2484e3e21cac0",
        "md5": "80bad68a22b0305a040a325eac43bbb6",
        "sha256": "54234fe26c214a7fa9a3445acb3de92b5e7c7d2e8a211b7515e2d74248d86fec"
      },
      "downloads": -1,
      "filename": "cloud_workflows-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "80bad68a22b0305a040a325eac43bbb6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 16055,
      "upload_time": "2021-12-06T16:32:27",
      "upload_time_iso_8601": "2021-12-06T16:32:27.796296Z",
      "url": "https://files.pythonhosted.org/packages/67/d2/e03de9a6c5f03075e8a998b4973e6e932dedf8cf7780cca2484e3e21cac0/cloud_workflows-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "96d37167e1fa99ac657991152d22b2af7804ef2b88de975d6a090801cfbd6f27",
        "md5": "f32ac5775e4463dc9ac5d480d8ea3107",
        "sha256": "3c0ab94abb43719ffc56915bfed11983bdb68673ee67f634920c0722ea3744f8"
      },
      "downloads": -1,
      "filename": "cloud-workflows-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "f32ac5775e4463dc9ac5d480d8ea3107",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 13668,
      "upload_time": "2021-12-06T16:32:29",
      "upload_time_iso_8601": "2021-12-06T16:32:29.639886Z",
      "url": "https://files.pythonhosted.org/packages/96/d3/7167e1fa99ac657991152d22b2af7804ef2b88de975d6a090801cfbd6f27/cloud-workflows-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}