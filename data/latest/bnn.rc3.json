{
  "info": {
    "author": "Adrian Bulat",
    "author_email": "adrian@adrianbulat.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# Binary Neural Networks (BNN)\n\n[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause) [![PyPI](https://img.shields.io/pypi/v/bnn.svg?style=flat)](https://pypi.org/project/bnn/) [![Test BNN Python package](https://github.com/1adrianb/binary-networks-pytorch/actions/workflows/python-package.yml/badge.svg)](https://github.com/1adrianb/binary-networks-pytorch/actions/workflows/python-package.yml)\n\nBNN is a Pytorch based library that facilitates the binarization (i.e. 1 bit quantization) of neural networks.\n\n## Installation\n\n### Requirements\n\n* Python 3.7+\n* PyTorch (>=1.8)\n\nThe easiest way to install the package is using pip or conda. Alternatively you can install the package from source.\n\n| **Using pip**                | **Using conda**                            |\n|------------------------------|--------------------------------------------|\n| `pip install bnn`            | `conda install -c 1adrianb bnn`            |\n\n## Why network binarization?\n\nNetwork binarization is the most extreme case of quantization restricting the input features and/or weights to two states only {-1,1}. Such hardware friendly representation can reduce the size of a float32 layer by **x32** times via bitpacking. Similarly, on modern x64 CPUs the operations can be executed up to **x64** faster via SIMD. Note that in order to take advantage at runtime of such speed-ups a hardware-friendly implementation is required which the current repo doesn't include currently.\n\n## Quick start\n\nIn order to facilitate common chaining operation that typically occur when binarizing neural networks we provide an easy mechanism to achieve this via a set of yaml configuration files (herein called recipes). An example of such file can be found in the recipes folder.\n\nNote that the examples provided bellow are simply intended to showcase the API are not necessarily the optimal configurations. For a more detailed behaviour of the available functions please check the corresponding documentation and research papers. The examples folder provides a full working example.\n\n### **1. Explicit usage**\n\nSimilarly with the pytorch quantization module we can define a binarization configuration  that will contains the binarization strategies(modules) used. Once defined, the `prepare_binary_model` function will propagate them to all nodes and then swap the modules with the fake binarized ones.\nAlternatively, the user can define manually, at network creation time, the bconfig for each layer and then call then `convert` function to swap the modules appropriately.\n\n```python\nimport torch\nimport torchvision.models as models\n\nfrom bnn import BConfig, prepare_binary_model\n# Import a few examples of quantizers\nfrom bnn.ops import BasicInputBinarizer, BasicScaleBinarizer, XNORWeightBinarizer\n\n# Create your desire model (note the default R18 may be suboptimal)\n# additional binarization friendly models are available in bnn.models\nmodel = models.resnet18(pretrained=False)\n\n# Define the binarization configuration and assign it to the model\nbconfig = BConfig(\n    activation_pre_process = BasicInputBinarizer,\n    activation_post_process = BasicScaleBinarizer,\n    # optionally, one can pass certain custom variables\n    weight_pre_process = XNORWeightBinarizer.with_args(center_weights=True)\n)\n# Convert the model appropiately, propagating the changes from parent node to leafs\n# The custom_config_layers_name syntax will perform a match based on the layer name, setting a custom quantization function.\nbmodel = prepare_binary_model(model, bconfig, custom_config_layers_name=['conv1' : BConfig()])\n\n# You can also ignore certain layers using the ignore_layers_name. \n# To pass regex expression, frame them between $ symbols, i.e.: $expression$.\n\n```\n\n### **2. Using binarization recepies**\n\n```python\nimport torch\nimport torchvision.models as models\n\n# Import the recepies consumer enginer\nfrom bnn.executor.engine import BinaryChef\n\n# Create your desire model (note the default R18 may be suboptimal)\nmodel = models.resnet18(pretrained=False)\nchef = BinaryChef('../recepies/xnor-net.yaml')\n\n# Repeat the training procedure using the steps define in the config file\nfor _ in range(len(chef)):\n    # Convert the model according to the recepie\n    model = chef.next(model)\n\n    ### Run here your training logich for N epochs\n```\n\n### **3. Implementing a custom weight binarizer**\n\nImplementing custom operations is a straightforward process. You can simply define your new classpython register class to a given module:\n\n```python\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CustomOutputBinarizer(nn.Module):\n    def __init__(self):\n        super(CustomOutputBinarizer, self).__init__()\n        \n    def forward(self, x_after, x_before):\n        # scale binarizer takes a list of input containg [conv_output and conv_input]\n        return F.normalize(x_after, p=2) # operate on the conv_output\n\nclass CustomInputBinarizer(nn.Module):\n    def __init__(self):\n        super(CustomInputBinarizer, self).__init__()\n        \n    def forward(self, x):\n        # dummy example of using sign instead of tanh\n        return torch.tanh(x) # operate on the conv_output\n\n# apply the custom functions into the binarization model\nbconfig = BConfig(\n    activation_pre_process = CustomInputBinarizer,\n    activation_post_process = CustomOutputBinarizer,\n    weight_pre_process = nn.Identity # this will keep the weights real\n)\n\n```\n\n### **4. Using adapted architecures**\n\nWhile existing of the shelves modules can be used directly, binarizing them may prove problematic.\nThe `bnn.models` implement a few popular choices:\n\n  1. Hierarchical Block - *Hierarchical binary CNNs for landmark localization with limited resources, A. Bulat, G. Tzimiropoulos, IEEE TPAMI 2020 (<https://arxiv.org/abs/1808.04803>).*\n  2. Residual layers with pre-activation - *XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, M. Rastegari, V. Ordonez, J. Redmond, A. Farhadi, ECCV 2016 (<https://arxiv.org/abs/1603.05279>).*\n  3. PReLU/Leaky ReLU instead of ReLU - *Improved training of binary networks for human pose estimation and image recognition, A. Bulat, G. Tzimiropoulos, J. Kossaifi, M. Pantic, arXiv 2019 (<https://arxiv.org/abs/1904.05868>).*\n  4. Efficient ResNet stem - *daBNN: A Super Fast Inference Framework for Binary Neural Networks on ARM devices, J. Zhang, Y. Pan, T. Yao, H. Zhao, T. Mei, ACMMM 2019 (<https://arxiv.org/abs/1908.05858>).*\n  5. BATS NAS - *BATS: Binary ArchitecTure Search, A. Bulat, B. Martinez, G. Tzimiropoulos, ECCV 2020 (<https://arxiv.org/abs/2003.01711>)*\n\nNote that they are implemented based on the descriptions provided in the original paper\n\n### **5. Counting FLOPs and BOPs (binary operations)**\n\nThis aspect makes usage of our _pthflops_ package. For instalation instructions please visit [https://github.com/1adrianb/pytorch-estimate-flops](https://github.com/1adrianb/pytorch-estimate-flops).\n\n```python\nfrom pthflops import count_ops\n\ndevice = 'cuda:0'\ninp = torch.rand(1,3,224,224).to(device)\n\nall_ops, all_data = count_ops(model, inp)\n\nflops, bops = 0, 0\nfor op_name, ops_count in all_data.items():\n    if 'Conv2d' in op_name and 'onnx::' not in op_name:\n        bops += ops_count\n    else:\n        flops += ops_count\n\nprint('Total number of FLOPs: {}', flops)\nprint('Total number of BOPs: {}', bops)\n\n```\n\n## Contributing\n\nAll contributions are highly welcomed. Feel free to self-assign yourself to existing issues, or open a new pull request if you would like to add a features. For new features, opening a issue for having a prior discussion is probably the best course of action.\n\n## Citation\n\nThis code was developed during my PhD done at University of Nottingham and is released in support of my thesis.\nIf you found this package helpfull, please cite:\n\n```lang-latex\n@inproceedings{bulat2017binarized,\n  title={Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources},\n  author={Bulat, Adrian and Tzimiropoulos, Georgios},\n  booktitle={Proceedings of the IEEE International Conference on Computer Vision},\n  pages={3706--3714},\n  year={2017}\n}\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/1adrianb/binary-networks-pytorch",
    "keywords": "artificial intelligence,convolutional neural network,binary networks,quantization,pytorch",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bnn",
    "package_url": "https://pypi.org/project/bnn/",
    "platform": "",
    "project_url": "https://pypi.org/project/bnn/",
    "project_urls": {
      "Homepage": "https://github.com/1adrianb/binary-networks-pytorch"
    },
    "release_url": "https://pypi.org/project/bnn/0.1.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Binarize deep convolutional neural networks using python and pytorch",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10866313,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e9be8f9a4dc614fcfc2ab67373c43ee35975dc1bb7ebc872b3ae59454161c5b4",
          "md5": "770939c9a9de9e0f5eae77044b7a7ae5",
          "sha256": "3b450f90a0e25cabad147dbf75ce4e902a75517c4f33df4ea4a503e3396eb99a"
        },
        "downloads": -1,
        "filename": "bnn-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "770939c9a9de9e0f5eae77044b7a7ae5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23260,
        "upload_time": "2021-04-26T19:48:53",
        "upload_time_iso_8601": "2021-04-26T19:48:53.310503Z",
        "url": "https://files.pythonhosted.org/packages/e9/be/8f9a4dc614fcfc2ab67373c43ee35975dc1bb7ebc872b3ae59454161c5b4/bnn-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "872db51136e14260e862b8d2d484ea5cd963790f14bd5557602d0e0697a31578",
          "md5": "ffc39e0ad4ab2f6b5027a2253109b7e9",
          "sha256": "70d37e76c9ff5fe168670b942900bc5b4fa9f1e55e0c0d4f15f8f5bb85e9e9a2"
        },
        "downloads": -1,
        "filename": "bnn-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ffc39e0ad4ab2f6b5027a2253109b7e9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23393,
        "upload_time": "2021-04-26T20:23:29",
        "upload_time_iso_8601": "2021-04-26T20:23:29.820808Z",
        "url": "https://files.pythonhosted.org/packages/87/2d/b51136e14260e862b8d2d484ea5cd963790f14bd5557602d0e0697a31578/bnn-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ff6b926cc81b86e845245558343a6a7e0c5345111cfb874a97d85e5b954238d5",
          "md5": "631fb6028e2b386136164cb32ed9126f",
          "sha256": "2a0794cfe2489297a4a36411229fc2a9486acef3c583efce9f9b32bf0b27d5a2"
        },
        "downloads": -1,
        "filename": "bnn-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "631fb6028e2b386136164cb32ed9126f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23750,
        "upload_time": "2021-07-09T15:47:59",
        "upload_time_iso_8601": "2021-07-09T15:47:59.956014Z",
        "url": "https://files.pythonhosted.org/packages/ff/6b/926cc81b86e845245558343a6a7e0c5345111cfb874a97d85e5b954238d5/bnn-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ff6b926cc81b86e845245558343a6a7e0c5345111cfb874a97d85e5b954238d5",
        "md5": "631fb6028e2b386136164cb32ed9126f",
        "sha256": "2a0794cfe2489297a4a36411229fc2a9486acef3c583efce9f9b32bf0b27d5a2"
      },
      "downloads": -1,
      "filename": "bnn-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "631fb6028e2b386136164cb32ed9126f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 23750,
      "upload_time": "2021-07-09T15:47:59",
      "upload_time_iso_8601": "2021-07-09T15:47:59.956014Z",
      "url": "https://files.pythonhosted.org/packages/ff/6b/926cc81b86e845245558343a6a7e0c5345111cfb874a97d85e5b954238d5/bnn-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}