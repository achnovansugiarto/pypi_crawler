{
  "info": {
    "author": "Maarten Grootendorst, Yao Su",
    "author_email": "maartengrootendorst@gmail.com, 1092702101@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "[![PyPI - Python](https://img.shields.io/badge/python-3.6%20|%203.7%20|%203.8-blue.svg)](https://pypi.org/project/keybert/)\n[![PyPI - License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/MaartenGr/keybert/blob/master/LICENSE)\n\n# ZhKeyBERT\n\n[中文文档](https://github.com/deepdialog/ZhKeyBERT/blob/master/README-zh.md)\n\n\nBased on [KeyBERT](https://github.com/MaartenGr/KeyBERT), enhance the keyword\nextraction model for Chinese.\n\nCorresponding medium post can be found [here](https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea).\n\n<a name=\"toc\"/></a>\n## Table of Contents  \n<!--ts-->\n   1. [About the Project](#about)  \n   2. [Getting Started](#gettingstarted)    \n        2.1. [Installation](#installation)    \n        2.2. [Basic Usage](#usage)     \n        2.3. [Maximal Marginal Relevance](#maximal)  \n        2.4. [Embedding Models](#embeddings)\n<!--te-->\n\n\n<a name=\"about\"/></a>\n## 1. About the Project\n[Back to ToC](#toc)  \n\nAlthough there are already many methods available for keyword generation \n(e.g., \n[Rake](https://github.com/aneesha/RAKE), \n[YAKE!](https://github.com/LIAAD/yake), TF-IDF, etc.) \nI wanted to create a very basic, but powerful method for extracting keywords and keyphrases. \nThis is where **KeyBERT** comes in! Which uses BERT-embeddings and simple cosine similarity\nto find the sub-phrases in a document that are the most similar to the document itself.\n\nFirst, document embeddings are extracted with BERT to get a document-level representation. \nThen, word embeddings are extracted for N-gram words/phrases. Finally, we use cosine similarity \nto find the words/phrases that are the most similar to the document. The most similar words could \nthen be identified as the words that best describe the entire document.  \n\nKeyBERT is by no means unique and is created as a quick and easy method\nfor creating keywords and keyphrases. Although there are many great \npapers and solutions out there that use BERT-embeddings \n(e.g., \n[1](https://github.com/pranav-ust/BERT-keyphrase-extraction),\n[2](https://github.com/ibatra/BERT-Keyword-Extractor),\n[3](https://www.preprints.org/manuscript/201908.0073/download/final_file),\n), I could not find a BERT-based solution that did not have to be trained from scratch and\ncould be used for beginners (**correct me if I'm wrong!**).\nThus, the goal was a `pip install keybert` and at most 3 lines of code in usage.   \n\n<a name=\"gettingstarted\"/></a>\n## 2. Getting Started\n[Back to ToC](#toc)  \n\n<a name=\"installation\"/></a>\n###  2.1. Installation\n\n```\ngit clone https://github.com/deepdialog/ZhKeyBERT\ncd ZhKeyBERT\npython setup.py install --user\n```\n\n<a name=\"usage\"/></a>\n###  2.2. Usage\n\nThe most minimal example can be seen below for the extraction of keywords:\n```python\nfrom zhkeybert import KeyBERT, extract_kws_zh\n\ndocs = \"\"\"时值10月25日抗美援朝纪念日，《长津湖》片方发布了“纪念中国人民志愿军抗美援朝出国作战71周年特别短片”，再次向伟大的志愿军致敬！\n电影《长津湖》全情全景地还原了71年前抗美援朝战场上那场史诗战役，志愿军奋不顾身的英勇精神令观众感叹：“岁月峥嵘英雄不灭，丹心铁骨军魂永存！”影片上映以来票房屡创新高，目前突破53亿元，暂列中国影史票房总榜第三名。\n值得一提的是，这部影片的很多主创或有军人的血脉，或有当兵的经历，或者家人是军人。提起这些他们也充满自豪，影片总监制黄建新称：“当兵以后会有一种特别能坚持的劲儿。”饰演雷公的胡军透露：“我父亲曾经参加过抗美援朝，还得了一个三等功。”影片历史顾问王树增表示：“我当了五十多年的兵，我的老部队就是上甘岭上下来的，那些老兵都是我的偶像。”\n“身先士卒卫华夏家国，血战无畏护山河无恙。”片中饰演七连连长伍千里的吴京感叹：“要永远记住这些先烈们，他们给我们带来今天的和平。感谢他们的付出，才让我们有今天的幸福生活。”饰演新兵伍万里的易烊千玺表示：“战争的残酷、碾压式的伤害，其实我们现在的年轻人几乎很难能体会到，希望大家看完电影后能明白，是那些先辈们的牺牲奉献，换来了我们的现在。”\n影片对战争群像的恢弘呈现，对个体命运的深切关怀，令许多观众无法控制自己的眼泪，观众称：“当看到影片中的惊险战斗场面，看到英雄们壮怀激烈的拼杀，为国捐躯的英勇无畏和无悔付出，我明白了为什么说今天的幸福生活来之不易。”（记者 王金跃）\"\"\"\nkw_model = KeyBERT(model='paraphrase-multilingual-MiniLM-L12-v2')\nextract_kws_zh(docs, kw_model)\n```\n\nComparison\n```python\n>>> extract_kws_zh(docs, kw_model)\n\n[('纪念中国人民志愿军抗美援朝', 0.7034),\n ('电影长津湖', 0.6285),\n ('周年特别短片', 0.5668),\n ('纪念中国人民志愿军', 0.6894),\n ('作战71周年', 0.5637)]\n>>> import jieba; kw_model.extract_keywords(' '.join(jieba.cut(docs)), keyphrase_ngram_range=(1, 3), \n                                            use_mmr=True, diversity=0.25)\n\n[('抗美援朝 纪念日 长津湖', 0.796),\n ('纪念 中国人民志愿军 抗美援朝', 0.7577),\n ('作战 71 周年', 0.6126),\n ('25 抗美援朝 纪念日', 0.635),\n ('致敬 电影 长津湖', 0.6514)]\n```\n\nYou can set `ngram_range`, whose default value is `(1, 3)`,\nto set the length of the resulting keywords/keyphrases:\n\n```python\n>>> extract_kws_zh(docs, kw_model, ngram_range=(1, 1))\n[('中国人民志愿军', 0.6094),\n ('长津湖', 0.505),\n ('周年', 0.4504),\n ('影片', 0.447),\n ('英雄', 0.4297)]\n```\n\n```python\n>>> extract_kws_zh(docs, kw_model, ngram_range=(1, 2))\n[('纪念中国人民志愿军', 0.6894),\n ('电影长津湖', 0.6285),\n ('年前抗美援朝', 0.476),\n ('中国人民志愿军抗美援朝', 0.6349),\n ('中国影史', 0.5534)]\n``` \n\n**NOTE**: For a full overview of all possible transformer models see [sentence-transformer](https://www.sbert.net/docs/pretrained_models.html).\nI would advise `\"paraphrase-multilingual-MiniLM-L12-v2\"` Chinese documents for efficiency\nand acceptable accuracy.\n\n<a name=\"maximal\"/></a>\n###  2.3. Maximal Marginal Relevance\n\nIt's recommended to use Maximal Margin Relevance (MMR) for diversity by\nsetting the optional parameter `use_mmr`, which is `True` in default.  \nTo diversify the results, we can use MMR to create\nkeywords / keyphrases which is also based on cosine similarity. The results \nwith **high diversity**:\n\n```python\n>>> extract_kws_zh(docs, kw_model, use_mmr = True, diversity=0.7)\n[('纪念中国人民志愿军抗美援朝', 0.7034),\n ('观众无法控制自己', 0.1212),\n ('山河无恙', 0.2233),\n ('影片上映以来', 0.5427),\n ('53亿元', 0.3287)]\n``` \n\nThe results with **low diversity**:  \n\n```python\n>>> extract_kws_zh(docs, kw_model, use_mmr = True, diversity=0.2)\n[('纪念中国人民志愿军抗美援朝', 0.7034),\n ('电影长津湖', 0.6285),\n ('纪念中国人民志愿军', 0.6894),\n ('周年特别短片', 0.5668),\n ('作战71周年', 0.5637)]\n``` \n\nAnd the default and recommended `diversity` is `0.25`.\n\n<a name=\"embeddings\"/></a>\n###  2.4. Embedding Models\nKeyBERT supports many embedding models that can be used to embed the documents and words:\n\n* Sentence-Transformers\n* Flair\n* Spacy\n* Gensim\n* USE\n\nClick [here](https://maartengr.github.io/KeyBERT/guides/embeddings.html) for a full overview of all supported embedding models.\n\n**Sentence-Transformers**  \nYou can select any model from `sentence-transformers` [here](https://www.sbert.net/docs/pretrained_models.html) \nand pass it through KeyBERT with `model`:\n\n```python\nfrom zhkeybert import KeyBERT\nkw_model = KeyBERT(model='all-MiniLM-L6-v2')\n```\n\nOr select a SentenceTransformer model with your own parameters:\n\n```python\nfrom zhkeybert import KeyBERT\nfrom sentence_transformers import SentenceTransformer\n\nsentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nkw_model = KeyBERT(model=sentence_model)\n```\n\nFor Chinese keywords extraction, you should choose multilingual models\nlike `paraphrase-multilingual-mpnet-base-v2` and `paraphrase-multilingual-MiniLM-L12-v2`. \n\n**MUSE**  \nMultilingual Universal Sentence Encoder([MUSE](https://arxiv.org/abs/1907.04307))\n\n```python\nfrom zhkeybert import KeyBERT\nimport tensorflow_hub import hub\n\nmodule_url = 'https://hub.tensorflow.google.cn/google/universal-sentence-encoder-multilingual-large/3'\n\nmodel = hub.load(module_url)\nkw_model = KeyBERT(model=model) ## slow but acceptable performance\n```\n\n## Citation\nTo cite KeyBERT in your work, please use the following bibtex reference:\n\n```bibtex\n@misc{grootendorst2020keybert,\n  author       = {Maarten Grootendorst},\n  title        = {KeyBERT: Minimal keyword extraction with BERT.},\n  year         = 2020,\n  publisher    = {Zenodo},\n  version      = {v0.3.0},\n  doi          = {10.5281/zenodo.4461265},\n  url          = {https://doi.org/10.5281/zenodo.4461265}\n}\n```\n\n## References\nBelow, you can find several resources that were used for the creation of KeyBERT \nbut most importantly, these are amazing resources for creating impressive keyword extraction models: \n\n**Papers**:  \n* Sharma, P., & Li, Y. (2019). [Self-Supervised Contextual Keyword and Keyphrase Retrieval with Self-Labelling.](https://www.preprints.org/manuscript/201908.0073/download/final_file)\n\n**Github Repos**:  \n* https://github.com/thunlp/BERT-KPE\n* https://github.com/ibatra/BERT-Keyword-Extractor\n* https://github.com/pranav-ust/BERT-keyphrase-extraction\n* https://github.com/swisscom/ai-research-keyphrase-extraction\n\n**MMR**:  \nThe selection of keywords/keyphrases was modeled after:\n* https://github.com/swisscom/ai-research-keyphrase-extraction\n\n**NOTE**: If you find a paper or github repo that has an easy-to-use implementation\nof BERT-embeddings for keyword/keyphrase extraction, let me know! I'll make sure to\nadd a reference to this repo. \n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/deepdialog/ZhKeyBERT",
    "keywords": "nlp bert keyword extraction embeddings for Chinese",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "zhkeybert",
    "package_url": "https://pypi.org/project/zhkeybert/",
    "platform": "",
    "project_url": "https://pypi.org/project/zhkeybert/",
    "project_urls": {
      "Homepage": "https://github.com/deepdialog/ZhKeyBERT"
    },
    "release_url": "https://pypi.org/project/zhkeybert/0.1.2/",
    "requires_dist": [
      "jieba",
      "sentence-transformers (>=0.3.8)",
      "scikit-learn (>=0.22.2)",
      "numpy (>=1.18.5)",
      "rich (>=10.4.0)",
      "mkdocs (>=1.1) ; extra == 'dev'",
      "mkdocs-material (>=4.6.3) ; extra == 'dev'",
      "mkdocstrings (>=0.8.0) ; extra == 'dev'",
      "pytest (>=5.4.3) ; extra == 'dev'",
      "pytest-cov (>=2.6.1) ; extra == 'dev'",
      "mkdocs (>=1.1) ; extra == 'docs'",
      "mkdocs-material (>=4.6.3) ; extra == 'docs'",
      "mkdocstrings (>=0.8.0) ; extra == 'docs'",
      "transformers (==3.5.1) ; extra == 'flair'",
      "torch (<1.7.1,>=1.4.0) ; extra == 'flair'",
      "flair (==0.7) ; extra == 'flair'",
      "pytest (>=5.4.3) ; extra == 'test'",
      "pytest-cov (>=2.6.1) ; extra == 'test'"
    ],
    "requires_python": ">=3.6",
    "summary": "Based on KeyBERT performs Chinese documents keyword extraction with state-of-the-art transformer models.",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12046362,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "306f3f699fef137ed270a92b5dc4eed5f436ce61d567bed70af10ff2423d0db7",
          "md5": "e450847a58c84a629f561e84d98bbb3d",
          "sha256": "98804491d1c98f6a6ab58d4a1c6c723d55742d42a1e65c6baed2021d162868d6"
        },
        "downloads": -1,
        "filename": "zhkeybert-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e450847a58c84a629f561e84d98bbb3d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 23122,
        "upload_time": "2021-11-17T07:38:08",
        "upload_time_iso_8601": "2021-11-17T07:38:08.790833Z",
        "url": "https://files.pythonhosted.org/packages/30/6f/3f699fef137ed270a92b5dc4eed5f436ce61d567bed70af10ff2423d0db7/zhkeybert-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "02044db7df2476940068e3c899e7ac9180db4928513dcfae7d8932f02d0cc4ed",
          "md5": "98f843310a02c3bc2b3d14a7bff7b6c2",
          "sha256": "6275939ad6a32b76f3e2b15b6514d557a3d7dbb0ee13e7990e63c09fb29d4c0c"
        },
        "downloads": -1,
        "filename": "zhkeybert-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "98f843310a02c3bc2b3d14a7bff7b6c2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 23101,
        "upload_time": "2021-11-17T09:04:10",
        "upload_time_iso_8601": "2021-11-17T09:04:10.267388Z",
        "url": "https://files.pythonhosted.org/packages/02/04/4db7df2476940068e3c899e7ac9180db4928513dcfae7d8932f02d0cc4ed/zhkeybert-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1e0a7f2e81bd0b8615490506ba2985e87d026d0749e8b454f74ec9188906661b",
          "md5": "8f68b0a3f9f12c1b0474855ccac99937",
          "sha256": "01763ddb5c6429f23c48d75343cc874b67b181c7f8e7dac553be013061ace945"
        },
        "downloads": -1,
        "filename": "zhkeybert-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8f68b0a3f9f12c1b0474855ccac99937",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 23111,
        "upload_time": "2021-11-17T09:11:02",
        "upload_time_iso_8601": "2021-11-17T09:11:02.390812Z",
        "url": "https://files.pythonhosted.org/packages/1e/0a/7f2e81bd0b8615490506ba2985e87d026d0749e8b454f74ec9188906661b/zhkeybert-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1e0a7f2e81bd0b8615490506ba2985e87d026d0749e8b454f74ec9188906661b",
        "md5": "8f68b0a3f9f12c1b0474855ccac99937",
        "sha256": "01763ddb5c6429f23c48d75343cc874b67b181c7f8e7dac553be013061ace945"
      },
      "downloads": -1,
      "filename": "zhkeybert-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8f68b0a3f9f12c1b0474855ccac99937",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 23111,
      "upload_time": "2021-11-17T09:11:02",
      "upload_time_iso_8601": "2021-11-17T09:11:02.390812Z",
      "url": "https://files.pythonhosted.org/packages/1e/0a/7f2e81bd0b8615490506ba2985e87d026d0749e8b454f74ec9188906661b/zhkeybert-0.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}