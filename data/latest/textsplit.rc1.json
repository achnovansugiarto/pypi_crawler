{
  "info": {
    "author": "Christoph Schock",
    "author_email": "chschock@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# Introduction\nThis library contains simple functionality to tackle the problem of segmenting\ndocuments into coherent parts. Imagine you don't have a good paragraph\nannotation in your documents, as it is often the case for scraped pdfs or html\ndocuments. For NLP tasks you want to split them at points where the topic\nchanges. Good results have been achieved using topic representations, but they\ninvolve a further step of topic modeling which is quite domain dependent. This\napproach uses only word embeddings which are assumed to be less domain specific.\nSee [https://arxiv.org/pdf/1503.05543.pdf] for an overview and an approach very\nsimilar to the one presented here.\n\n\nThe algorithm uses word embeddings to find a segmentation where the splits are\nchosen such that the segments are coherent. This coherence can be described as\naccumulated weighted cosine similarity of the words of a segment to the mean\nvector of that segment.  More formally segments are chosen as to maximize the\nquantity |v|, where v is a segment vector and |.| denotes the l2-norm. The\naccumulated weighted cosine similarity turns up by a simple transformation:\n|v| = 1/|v| <v, v> = <v, v/|v|> = \\sum_i <w_i, v/|v|> = \\sum_i |w_i| <w_i/|w_i|, v/|v|>,\nwhere v = \\sum_i w_i is the definition of the segment vector from word vectors\nw_i. The expansion gives a good intuition of what we try to achieve. As we\nusually compare word embeddings with cosine similarity, the last scalar product\n<w_i/|w_i|, v/|v|> is just the cosine similarity of a word w_i to the segment\nvector v. The weighting with the length of w_i suppresses frequent noise words,\nthat typically have a shorter length.\n\nThis leads to the interpretation that coherence corresponds to segment vector\nlength, in the sense that two segment vectors of same length contain the same\namount of information. This interpretation is of course only capturing\ninformation that we are given as input by means of the word embeddings, but it\nserves as an abstraction.\n\n# Formalization\n\nTo optimize for segment vector length |v|, we look for a sequence of split\npositions such that the sum of l2-norms of the segment vectors formed by summing\nthe words between the splits is maximal. Given this objective without\nconstraints, the optimal solution is to split the document between every two\nsubsequent words (triangle inequality). We have to impose some limit on the\ngranularity of the segmentation to get useful results. This is done by a penalty\nfor every split made, that counts against the vector norms, i.e. is subtracted\nfrom the sum of vector norms.\n\nLet Seg := {(0 = t_0 < t_i < ... < t_n = L) | s_i natural number} where L is a\ndocuments length. A segment [a, b[ comprises the words at positions a, a+1, ...,\nb-1. Let l(j, k) := |\\sum_i=j^{k-1} w_i| denote the vector of segment [i, j[. We\noptimize the function f mapping elements of Seg to the real numbers with\nf: (t_0, ..., t_n) \\mapsto \\sum_{i=0}^{n-1} (l(t_{i-1}, t_i) + l(t_i, t_{i+1}) - penalty).\n\n# Algorithms\n\nThere are two variants, a greedy that is fast and a dynamic programming approach\nthat computes the optimal segmentation. Both depend on a penalty hyperparameter,\nthat defined the granularity of the split.\n\n## Greedy\nSplit the text iteratively at the position where the gain is highest until this\ngain would be below a given penalty threshold. The gain is the sum of norms of\nthe left and right segments minus the norm of the segment that is to be split.\n\n## Optimal (Dynamic Programming)\nIteratively construct a data structure storing the results of optimally\nsplitting a prefix of the document. This results in a matrix storing a score\nfor making a segment from position i to j, given a optimal segmentation up to i.\n\n# Tools\n\n## Penalty hyperparameter choice\nThe greedy implementation does not need the penalty parameter, but can also be\nrun by limiting the number of segments. This is leveraged by the `get_penalty`\nfunction to approximately determine a penalty parameter for a desired average\nsegment length computed over a set of documents.\n\n## Measure accuracy of segmentation against reference\nTo measure the accuracy of an algorithm against a given reference segmentation\n`P_k` is a commonly used metric described e.g. in above paper.\n\n## Apply segmentation definition to document\nThe function `get_segments` simply applies a segmentation determined by one of\nthe algorithms to e.g. the sentences of a text used when generating the\nsegmentation.\n\n# Usage\n\n## Input\nThe algorithms are fed a matrix `docmat` containing vectors representing the\ncontent of a text. These vectors are supposed to have cosine similarity as a\nnatural similarity measure and length roughly corresponding to the content\nlength of a text particle. Particles could be words in which case word2vec\nembeddings are a good choice as vectors. The width of `docmat` is the embedding\ndimension and the height the number of particles.\n\n## Split along sentence borders\nIf you want to split text into paragraphs, you most likely already have a good\nidea of what potential sentence borders are. It makes sense not to give the word\nvectors as input but sentence vectors formed by e.g. the sum of word vectors, as\nit is usual practice.\n\n\n# Getting Started\nIn the Jupyter notebook HowTo.ipynb you find code that demonstrates the use of\nthe module. It downloads a corpus to trains word2vec vectors on and an example\ntext for segmentation. You achieve better results if you compute word vectors on\na larger corpus.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/chschock/textsplit",
    "keywords": "nlp text segmentation paragraph embeddings",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "textsplit",
    "package_url": "https://pypi.org/project/textsplit/",
    "platform": "",
    "project_url": "https://pypi.org/project/textsplit/",
    "project_urls": {
      "Homepage": "https://github.com/chschock/textsplit"
    },
    "release_url": "https://pypi.org/project/textsplit/0.5/",
    "requires_dist": [
      "nose (>=1.3.7)",
      "numpy (>=1.13.1)"
    ],
    "requires_python": "",
    "summary": "Segment documents into coherent parts using wordembeddings.",
    "version": "0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7366600,
  "releases": {
    "0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f8f846e5dfcaac6974d6b13f91d91f9194ffa4eea08129684afb25fac18974af",
          "md5": "b63eace9170c1f14666fa0411f948103",
          "sha256": "83303a1a9b655343c8b97b0bb10be6d9af8ca467b3285c9fca7fe93ce1b67872"
        },
        "downloads": -1,
        "filename": "textsplit-0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b63eace9170c1f14666fa0411f948103",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 9626,
        "upload_time": "2020-05-31T21:42:34",
        "upload_time_iso_8601": "2020-05-31T21:42:34.385056Z",
        "url": "https://files.pythonhosted.org/packages/f8/f8/46e5dfcaac6974d6b13f91d91f9194ffa4eea08129684afb25fac18974af/textsplit-0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b2f2312d57cbca4299353b816b055a8c6b42576a60e79881b6374fe9b9cefda7",
          "md5": "04f66f961de8b4a2518d21668f5f15d1",
          "sha256": "12b32a4a63c00b5173d10450e6a65be5f5825ff53e5724cd2e96df1dd0d28243"
        },
        "downloads": -1,
        "filename": "textsplit-0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "04f66f961de8b4a2518d21668f5f15d1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 8648,
        "upload_time": "2020-05-31T21:42:36",
        "upload_time_iso_8601": "2020-05-31T21:42:36.379357Z",
        "url": "https://files.pythonhosted.org/packages/b2/f2/312d57cbca4299353b816b055a8c6b42576a60e79881b6374fe9b9cefda7/textsplit-0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f8f846e5dfcaac6974d6b13f91d91f9194ffa4eea08129684afb25fac18974af",
        "md5": "b63eace9170c1f14666fa0411f948103",
        "sha256": "83303a1a9b655343c8b97b0bb10be6d9af8ca467b3285c9fca7fe93ce1b67872"
      },
      "downloads": -1,
      "filename": "textsplit-0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b63eace9170c1f14666fa0411f948103",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 9626,
      "upload_time": "2020-05-31T21:42:34",
      "upload_time_iso_8601": "2020-05-31T21:42:34.385056Z",
      "url": "https://files.pythonhosted.org/packages/f8/f8/46e5dfcaac6974d6b13f91d91f9194ffa4eea08129684afb25fac18974af/textsplit-0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b2f2312d57cbca4299353b816b055a8c6b42576a60e79881b6374fe9b9cefda7",
        "md5": "04f66f961de8b4a2518d21668f5f15d1",
        "sha256": "12b32a4a63c00b5173d10450e6a65be5f5825ff53e5724cd2e96df1dd0d28243"
      },
      "downloads": -1,
      "filename": "textsplit-0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "04f66f961de8b4a2518d21668f5f15d1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 8648,
      "upload_time": "2020-05-31T21:42:36",
      "upload_time_iso_8601": "2020-05-31T21:42:36.379357Z",
      "url": "https://files.pythonhosted.org/packages/b2/f2/312d57cbca4299353b816b055a8c6b42576a60e79881b6374fe9b9cefda7/textsplit-0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}