{
  "info": {
    "author": "Dadmatech AI Company",
    "author_email": "info@dadmatech.ir",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<!-- <h1 align=\"center\">\n  <img src=\"images/dadmatech.jpeg\"  width=\"150\"  />\n   Dadmatools\n</h1> -->\n\n\n<h2 align=\"center\">DadmaTools: A Python NLP Library for Persian</h2>\n\n<div align=\"center\">\n  <a href=\"https://pypi.org/project/dadmatools/\"><img src=\"https://img.shields.io/pypi/v/dadmatools.svg\"></a>\n  <a href=\"\"><img src=\"https://img.shields.io/badge/license-Apache%202-blue.svg\"></a>\n  <a href='https://dadmatools.readthedocs.io/en/latest/'><img src='https://readthedocs.org/projects/danlp-alexandra/badge/?version=latest' alt='Documentation Status' /></a>\n</div>\n<div align=\"center\">\n  <h5>\n      Named Entity Recognition\n    <span> | </span>\n      Part of Speech Tagging\n    <span> | </span>\n      Dependency Parsing\n  </h5>\n  <h5>\n      Constituency Parsing\n    <span> | </span>\n      Chunking\n  </h5>\n  <h5>\n      Tokenizer\n    <span> | </span>\n      Lemmatizer\n  </h5>\n  <h5>\n  </h5>\n</div>\n\n\n# **DadmaTools**\nDadmaTools is a repository for Natural Language Processing resources for the Persian Language. The aim is to make it easier and more applicable to practitioners in the industry to use Persian NLP, and hence this project is licensed to allow commercial use. The project features code examples on how to use the models in popular NLP frameworks such as spaCy and Transformers, as well as Deep Learning frameworks such as PyTorch. Furthermore, DadmaTools support common Persian embedding and Persian datasets.\nfor more details about how to use this tool read the instruction below.\n\n## NLP Models\n\nNatural Language Processing is an active area of research, and it consists of many different tasks. \nThe DadmaTools repository provides an overview of Persian models for some of the most basic NLP tasks (and is continuously evolving). \n\nHere is the list of NLP tasks we currently cover in the repository. These NLP tasks are defined as pipelines. Therefore, a pipeline list must be created and passed through the model. This will allow the user to choose the only task needed without loading others. \nEach task has its abbreviation as follows:\n-  Named Entity Recognition: ```ner```\n-  Part of speech tagging: ```pos```\n-  Dependency parsing: ```dep```\n-  Constituency parsing: ```cons```\n-  Chunking: ```chunk```\n-  Lemmatizing: ```lem```\n-  Tokenizing: ```tok```\n-  Normalizing\n\n**Note** that the normalizer can be used outside of the pipeline as there are several configs (the default config is in the pipeline with the name of def-norm).\n**Note** that if no pipeline is passed to the model, the tokenizer will be loaded as default.\n\n### Use Case\n\n<!-- These NLP tasks are defined as pipelines. Therefore, a pipeline list must be created and passed through the model. This will allow the user to choose the only task needed without loading others. \nEach task has its abbreviation as following:\n-  ```ner```: Named entity recognition\n-  ```pos```: Part of speech tagging\n-  ```dep```: Dependency parsing\n-  ```cons```: Constituency parsing\n-  ```chunk```: Chunking\n-  ```lem```: Lemmatizing\n-  ```tok```: Tokenizing\n\nNote that the normalizer can be used outside of the pipeline as there are several configs.\nNote that if no pipeline is passed to the model the tokenizer will be load as default. -->\n\n### Normalizer\ncleaning text and unify characters.\n\nNote: None means no action! \n```python\nfrom dadmatools.models.normalizer import Normalizer\n\nnormalizer = Normalizer(\n    full_cleaning=False,\n    unify_chars=True,\n    refine_punc_spacing=True,\n    remove_extra_space=True,\n    remove_puncs=False,\n    remove_html=False,\n    remove_stop_word=False,\n    replace_email_with=\"<EMAIL>\",\n    replace_number_with=None,\n    replace_url_with=\"\",\n    replace_mobile_number_with=None,\n    replace_emoji_with=None,\n    replace_home_number_with=None\n)\n\ntext = \"\"\"\n<p>\nدادماتولز اولین نسخش سال ۱۴۰۰ منتشر شده. \nامیدواریم که این تولز بتونه کار با متن رو براتون شیرین‌تر و راحت‌تر کنه\nلطفا با ایمیل dadmatools@dadmatech.ir با ما در ارتباط باشید\nآدرس گیت‌هاب هم که خب معرف حضور مبارک هست:\n https://github.com/Dadmatech/DadmaTools\n</p>\n\"\"\"\nnormalized_text = normalizer.normalize(text)\n#<p> دادماتولز اولین نسخش سال 1400 منتشر شده. امیدواریم که این تولز بتونه کار با متن رو براتون شیرین‌تر و راحت‌تر کنه لطفا با ایمیل <EMAIL> با ما در ارتباط باشید آدرس گیت‌هاب هم که خب معرف حضور مبارک هست: </p>\n\n#full cleaning\nnormalizer = Normalizer(full_cleaning=True)\nnormalized_text = normalizer.normalize(text)\n#دادماتولز نسخش سال منتشر تولز بتونه کار متن براتون شیرین‌تر راحت‌تر کنه ایمیل ارتباط آدرس گیت‌هاب معرف حضور مبارک\n\n```\n\n### Pipeline - Tokenizer, Lemmatizer, POS Tagger, Dependancy Parser, Constituency Parser\n```python\nimport dadmatools.pipeline.language as language\n\n# here lemmatizer and pos tagger will be loaded\n# as tokenizer is the default tool, it will be loaded as well even without calling\npips = 'tok,lem,pos,dep,chunk,cons' \nnlp = language.Pipeline(pips)\n\n# you can see the pipeline with this code\nprint(nlp.analyze_pipes(pretty=True))\n\n# doc is an SpaCy object\ndoc = nlp('از قصهٔ کودکیشان که می‌گفت، گاهی حرص می‌خورد!')\n```\n[```doc```](https://spacy.io/api/doc) object has different extensions. First, there are ```sentences``` in ```doc``` which is the list of the list of [```Token```](https://spacy.io/api/token). Each [```Token```](https://spacy.io/api/token) also has its own extensions. Note that we defined our own extension as well in DadmaTools. If any pipeline related to the specific extensions is not called, that extension will have no value.\n\nTo better see the results which you can use this code:\n\n```python\n\ndictionary = language.to_json(pips, doc)\nprint(dictionary)\n```\n\n```python\n[[{'id': 1, 'text': 'از', 'lemma': 'از', 'pos': 'ADP', 'rel': 'case', 'root': 2}, {'id': 2, 'text': 'قصهٔ', 'lemma': 'قصه', 'pos': 'NOUN', 'rel': 'obl', 'root': 10}, {'id': 3, 'text': 'کودکی', 'lemma': 'کودکی', 'pos': 'NOUN', 'rel': 'nmod', 'root': 2}, {'id': 4, 'text': 'شان', 'lemma': 'آنها', 'pos': 'PRON', 'rel': 'nmod', 'root': 3}, {'id': 5, 'text': 'که', 'lemma': 'که', 'pos': 'SCONJ', 'rel': 'mark', 'root': 6}, {'id': 6, 'text': 'می\\u200cگفت', 'lemma': 'گفت#گو', 'pos': 'VERB', 'rel': 'acl', 'root': 2}, {'id': 7, 'text': '،', 'lemma': '،', 'pos': 'PUNCT', 'rel': 'punct', 'root': 6}, {'id': 8, 'text': 'گاهی', 'lemma': 'گاه', 'pos': 'NOUN', 'rel': 'obl', 'root': 10}, {'id': 9, 'text': 'حرص', 'lemma': 'حرص', 'pos': 'NOUN', 'rel': 'compound:lvc', 'root': 10}, {'id': 10, 'text': 'می\\u200cخورد', 'lemma': 'خورد#خور', 'pos': 'VERB', 'rel': 'root', 'root': 0}, {'id': 11, 'text': '!', 'lemma': '!', 'pos': 'PUNCT', 'rel': 'punct', 'root': 10}]]\n\n```\n\n```python\nsentences = doc._.sentences\nfor sentence in sentences:\n    text = sentence.text\n    for token in sentences:\n        token_text = token.text\n        lemma = token.lemma_ ## this has value only if lem is called\n        pos_tag = token.pos_ ## this has value only if pos is called\n        dep = token.dep_ ## this has value only if dep is called\n        dep_arc = token._.dep_arc ## this has value only if dep is called\nsent_constituency = doc._.constituency ## this has value only if cons is called\nsent_chunks = doc._.chunks ## this has value only if cons is called\nners = doc._.ners ## this has value only if ner is called\n```\n\n**Note** that ```_.constituency``` and ```_.chunks``` are the object of [SuPar](https://parser.yzhang.site/en/latest/) class.\n\n### Loading Persian NLP Datasets\nWe provide an easy-to-use way to load some popular Persian NLP datasets\n\nHere is the list of supported datasets.\n\n   |    Dataset             | Task \n|       :----------------:               |  :----------------:   \n   |    PersianNER           |   Named Entity Recognition   | \n   |       ARMAN             |   Named Entity Recognition\n   |       Peyma             | Named Entity Recognition\n  |       FarsTail           | Textual Entailment\n |        FaSpell           | Spell Checking\n  |      PersianNews        | Text Classification\n  |       PerUDT            | Universal Dependency\n  |      PnSummary          | Text Summarization\n  |    SnappfoodSentiment   | Sentiment Classification\n  |           TEP           | Text Translation(eng-fa)\n| WikipediaCorpus               | Corpus\n| PersianTweets           | Corpus\n\n\nall datasets are iterator and can be used like below:\n```python\nfrom dadmatools.datasets import FarsTail\nfrom dadmatools.datasets import SnappfoodSentiment\nfrom dadmatools.datasets import Peyma\nfrom dadmatools.datasets import PerUDT\nfrom dadmatools.datasets import PersianTweets\nfrom dadmatools.datasets import PnSummary\n\n\nfarstail = FarsTail()\n#len of dataset\nprint(len(farstail.train))\n\n#like a generator\nprint(next(farstail.train))\n\n#dataset details\npn_summary = PnSummary()\nprint('PnSummary dataset information: ', pn_summary.info)\n\n#loop over dataset\nsnpfood_sa = SnappfoodSentiment()\nfor i, item in enumerate(snpfood_sa.test):\n    print(item['comment'], item['label'])\n\n#get first tokens' lemma of all dev items\nperudt = PerUDT()\nfor token_list in perudt.dev:\n    print(token_list[0]['lemma'])\n\n#get NER tag of first Peyma's data\npeyma = Peyma()\nprint(next(peyma.data)[0]['tag'])\n\n#corpus \ntweets = PersianTweets()\nprint('tweets count : ', len(tweets.data))\nprint('sample tweet: ', next(tweets.data))\n```\nget dataset info:\n```python\n\nfrom dadmatools.datasets import get_all_datasets_info\n\nget_all_datasets_info().keys()\n#dict_keys(['Persian-NEWS', 'fa-wiki', 'faspell', 'PnSummary', 'TEP', 'PerUDT', 'FarsTail', 'Peyma', 'snappfoodSentiment', 'Persian-NER', 'Arman', 'PerSent'])\n\n#specify task\nget_all_datasets_info(tasks=['NER', 'Sentiment-Analysis'])\n```\nthe output will be:\n\n```json\n{\"ARMAN\": {\"description\": \"ARMAN dataset holds 7,682 sentences with 250,015 sentences tagged over six different classes.\\n\\nOrganization\\nLocation\\nFacility\\nEvent\\nProduct\\nPerson\",\n  \"filenames\": [\"train_fold1.txt\",\n   \"train_fold2.txt\",\n   \"train_fold3.txt\",\n   \"test_fold1.txt\",\n   \"test_fold2.txt\",\n   \"test_fold3.txt\"],\n  \"name\": \"ARMAN\",\n  \"size\": {\"test\": 7680, \"train\": 15361},\n  \"splits\": [\"train\", \"test\"],\n  \"task\": \"NER\",\n  \"version\": \"1.0.0\"},\n \"PersianNer\": {\"description\": \"source: https://github.com/Text-Mining/Persian-NER\",\n  \"filenames\": [\"Persian-NER-part1.txt\",\n   \"Persian-NER-part2.txt\",\n   \"Persian-NER-part3.txt\",\n   \"Persian-NER-part4.txt\",\n   \"Persian-NER-part5.txt\"],\n  \"name\": \"PersianNer\",\n  \"size\": 976599,\n  \"splits\": [],\n  \"task\": \"NER\",\n  \"version\": \"1.0.0\"},\n \"Peyma\": {\"description\": \"source: http://nsurl.org/2019-2/tasks/task-7-named-entity-recognition-ner-for-farsi/\",\n  \"filenames\": [\"peyma/600K\", \"peyma/300K\"],\n  \"name\": \"Peyma\",\n  \"size\": 10016,\n  \"splits\": [],\n  \"task\": \"NER\",\n  \"version\": \"1.0.0\"},\n \"snappfoodSentiment\": {\"description\": \"source: https://huggingface.co/HooshvareLab/bert-fa-base-uncased-sentiment-snappfood\",\n  \"filenames\": [\"snappfood/train.csv\",\n   \"snappfood/test.csv\",\n   \"snappfood/dev.csv\"],\n  \"name\": \"snappfoodSentiment\",\n  \"size\": {\"dev\": 6274, \"test\": 6972, \"train\": 56516},\n  \"splits\": [\"train\", \"test\", \"dev\"],\n  \"task\": \"Sentiment-Analysis\",\n  \"version\": \"1.0.0\"}}\n```\n\n\n### Loading Persian Word Embeddings\ndownload, load and use some pre-trained Persian word embeddings.\n\ndadmatools supports all glove, fasttext, and word2vec formats.\n```python\nfrom dadmatools.embeddings import get_embedding, get_all_embeddings_info, get_embedding_info\nfrom pprint import pprint\n\npprint(get_all_embeddings_info())\n\n#get embedding information of specific embedding\nembedding_info = get_embedding_info('glove-wiki')\n\n#### load embedding ####\nword_embedding = get_embedding('glove-wiki')\n\n#get vector of the word\nprint(word_embedding['سلام'])\n\n#vocab\nvocab = word_embedding.get_vocab()\n\n### some useful functions ###\nprint(word_embedding.top_nearest(\"زمستان\", 10))\nprint(word_embedding.similarity('کتب', 'کتاب'))\nprint(word_embedding.embedding_text('امروز هوای خوبی بود'))\n```\nThe following word embeddings are currently supported: \n\n| Name | Embedding Algorithm | Corpus | \n| :-------------: | :-------------:  | :-------------:  | \n| [`glove-wiki`](https://github.com/Text-Mining/Persian-Wikipedia-Corpus/tree/master/models/glove)  | glove | Wikipedia  |\n| [`fasttext-commoncrawl-bin`](https://fasttext.cc/docs/en/crawl-vectors.html) | fasttext | CommonCrawl |\n| [`fasttext-commoncrawl-vec`](https://fasttext.cc/docs/en/crawl-vectors.html) | fasttext | CommonCrawl |\n| [`word2vec-conll`](http://vectors.nlpl.eu/) | word2vec | Persian CoNLL17 corpus  |\n\n## Evaluation\nWe have compared our pos tagging, dependancy parsing, and lemmatization models to `stanza` and `hazm`.\n\n<table>\n  <tr align='center'>\n    <td colspan=\"4\"><b>PerDT (F1 score)</b></td>\n  </tr>\n  <tr align='center'>\n    <td><b>Toolkit</b></td>\n    <td><b>POS Tagger (UPOS)</b></td>\n    <td><b>Dependancy Parser (UAS/LAS)</b></td>\n    <td><b>Lemmatizer</b></td>\n  </tr>\n  <tr align='center'>\n    <td>DadmaTools</td>\n    <td><b>97.52%</b></td>\n    <td><b>95.36%</b>  /  <b>92.54%</b> </td>\n    <td><b>99.14%</b> </td>\n  </tr>\n  <tr align='center'>\n    <td>stanza</td>\n    <td>97.35%</td>\n    <td>93.34%  /  91.05% </td>\n    <td>98.97% </td>\n  </tr>\n  <tr align='center'>\n    <td>hazm</td>\n    <td>-</td>\n    <td>- </td>\n    <td>89.01% </td>\n  </tr>\n\n\n  <tr align='center'>\n    <td colspan=\"4\"><b>Seraji (F1 score)</b></td>\n  </tr>\n  <tr align='center'>\n    <td><b>Toolkit</b></td>\n    <td><b>POS Tagger (UPOS)</b></td>\n    <td><b>Dependancy Parser (UAS/LAS)</b></td>\n    <td><b>Lemmatizer</b></td>\n  </tr>\n  <tr align='center'>\n    <td>DadmaTools</td>\n    <td><b>97.83%</b></td>\n    <td><b>92.5%</b>  /  <b>89.23%</b> </td>\n    <td> - </td>\n  </tr>\n  <tr align='center'>\n    <td>stanza</td>\n    <td>97.43%</td>\n    <td>87.20% /  83.89% </td>\n    <td> - </td>\n  </tr>\n  <tr align='center'>\n    <td>hazm</td>\n    <td>-</td>\n    <td>- </td>\n    <td>86.93% </td>\n  </tr>\n</table>\n\n\n<table>\n  <tr align='center'>\n    <td colspan=\"2\"><b>Tehran university tree bank (F1 score)</b></td>\n  </tr>\n  <tr align='center'>\n    <td><b>Toolkit</b></td>\n    <td><b>Constituency Parser</b></td>\n  </tr>\n  <tr align='center'>\n    <td>DadmaTools (without preprocess))</td>\n    <td><b>82.88%</b></td>\n  </tr>\n  <tr align='center'>\n    <td>Stanford (with some preprocess on POS tags)</td>\n    <td>80.28</td>\n  </tr>\n</table>\n\n## Installation\n\nTo get started using DadmaTools in your python project, simply install via the pip package. Note that installing the default pip package \nwill not install all NLP libraries because we want you to have the freedom to limit the dependency on what you use. Instead, we provide you with an installation option if you want to install all the required dependencies. \n\n### Install with pip\n\nTo get started using DadmaTools, simply install the project with pip:\n\n```bash\npip install dadmatools \n```\n\nNote that the default installation of DadmaTools **does** install other NLP libraries such as SpaCy and supar.\n\nYou can check the `requirements.txt` file to see what version the packages has been tested with.\n\n### Install from github\nAlternatively you can install the latest version from github using:\n```bash\npip install git+https://github.com/Dadmatech/dadmatools.git\n```\n\n## How to use (Colab)\nYou can see the codes and the output here.\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1re_7tr-U6XOmzptkb-s-_lK2H9Kb0Y6l?usp=sharing)\n\n## Cite\nWill be added in future.\n<!-- \nIf you want to cite this project, please use the following BibTeX entry: \n\n```\n@inproceedings{\n}\n``` -->\n\n<!-- Read the paper here.  -->\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Dadmatech/DadmaTools",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dadmatools-light",
    "package_url": "https://pypi.org/project/dadmatools-light/",
    "platform": null,
    "project_url": "https://pypi.org/project/dadmatools-light/",
    "project_urls": {
      "Homepage": "https://github.com/Dadmatech/DadmaTools"
    },
    "release_url": "https://pypi.org/project/dadmatools-light/1.2.0/",
    "requires_dist": [
      "bpemb (>=0.3.3)",
      "spacy (>=3.0.0)",
      "sklearn (>=0.0)",
      "torch (>=1.7.1)",
      "transformers (>=4.9.1pyconll>=3.1.0)",
      "html2text",
      "conllu",
      "gdown (>=4.3.1)",
      "gensim",
      "py7zr (>=0.17.2)"
    ],
    "requires_python": "",
    "summary": "DadmaTools is a Persian NLP toolkit",
    "version": "1.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14754808,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b8af9a7abd111507c694a5a2c9edd83ccdd2ca818298547a6bcbdac8671d69ff",
          "md5": "2aa4c6a9497b96251d43e673dc01db13",
          "sha256": "ab5fd1624d758f7264bf36e40a090ff97b090aef17f831aff54b07e8b3415a1c"
        },
        "downloads": -1,
        "filename": "dadmatools_light-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2aa4c6a9497b96251d43e673dc01db13",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 216264,
        "upload_time": "2022-08-14T08:24:27",
        "upload_time_iso_8601": "2022-08-14T08:24:27.337066Z",
        "url": "https://files.pythonhosted.org/packages/b8/af/9a7abd111507c694a5a2c9edd83ccdd2ca818298547a6bcbdac8671d69ff/dadmatools_light-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8fb9356529eb85c7c188524351e466838031a317aaa6a6605f27b9471c03dea7",
          "md5": "2c8bfece99ae1e432153beef4fcace02",
          "sha256": "2aa5d359988c42ee924b5c0b47ee28ac8c8bc1d62c65253c651ed3b437e1785e"
        },
        "downloads": -1,
        "filename": "dadmatools_light-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2c8bfece99ae1e432153beef4fcace02",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 216290,
        "upload_time": "2022-08-14T08:40:00",
        "upload_time_iso_8601": "2022-08-14T08:40:00.076350Z",
        "url": "https://files.pythonhosted.org/packages/8f/b9/356529eb85c7c188524351e466838031a317aaa6a6605f27b9471c03dea7/dadmatools_light-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7b5c38394b37151313a2be979ab3362b8636c623f0a91c5c5faff7b487378eb1",
          "md5": "b7fdaa7bbe55364efffdf837ce7ba7ab",
          "sha256": "0a4009b2dfafc18b371ef8511f8cf9d8729f9f21d0d3146363add9633521e3c6"
        },
        "downloads": -1,
        "filename": "dadmatools_light-1.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b7fdaa7bbe55364efffdf837ce7ba7ab",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 224428,
        "upload_time": "2022-08-14T09:16:05",
        "upload_time_iso_8601": "2022-08-14T09:16:05.346525Z",
        "url": "https://files.pythonhosted.org/packages/7b/5c/38394b37151313a2be979ab3362b8636c623f0a91c5c5faff7b487378eb1/dadmatools_light-1.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7b5c38394b37151313a2be979ab3362b8636c623f0a91c5c5faff7b487378eb1",
        "md5": "b7fdaa7bbe55364efffdf837ce7ba7ab",
        "sha256": "0a4009b2dfafc18b371ef8511f8cf9d8729f9f21d0d3146363add9633521e3c6"
      },
      "downloads": -1,
      "filename": "dadmatools_light-1.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b7fdaa7bbe55364efffdf837ce7ba7ab",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 224428,
      "upload_time": "2022-08-14T09:16:05",
      "upload_time_iso_8601": "2022-08-14T09:16:05.346525Z",
      "url": "https://files.pythonhosted.org/packages/7b/5c/38394b37151313a2be979ab3362b8636c623f0a91c5c5faff7b487378eb1/dadmatools_light-1.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}