{
  "info": {
    "author": "Simon Le Mouellic",
    "author_email": "simon.lemouellic@outlook.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "\n![alt text](https://github.com/SimonLMC/cloudflow/blob/main/image/cloudflow_logo.svg?raw=true)\n\n# ðŸ’¡ What is __Cloudflow__ ? \n\n__Cloudflow is a Python package that combines two powerful tools for machine learning model management: Cloudpickle and MLflow.__\n\n- Cloudpickle is a serialization library that allows Python objects to be serialized and deserialized across different Python processes or even different Python versions. This is especially useful for machine learning models, which can be complex and difficult to share between different environments.\n\n- MLflow, on the other hand, is an open source platform for the complete machine learning lifecycle that helps to manage experiments, package code into reproducible runs, and share and deploy models. \n\nBy integrating Cloudpickle with MLflow, __CloudFlow__ provides an easy and efficient way to serialize and save complex machine learning models, track your experiments and their results, and share your models with others in a reproducible and scalable way.\n\nWhether you are working on a single project or collaborating with a team, Cloudflow can help you manage your machine learning models more effectively and streamline your machine learning workflow.\n\n- One of the key benefits of Cloudflow is that it makes it easy to __save and manage complex combinations models__. \nWith traditional approaches, it can be challenging to __combine multiple models into a single object that can be easily serialized and saved__. However, with Cloudflow, you can easily combine multiple models, __along with their associated pre-processing and post-processing steps__, into a single model object that can be easily serialized and saved. This makes it simple to manage complex combinations of models, which can be very useful when you are working on more advanced machine learning problems.\n\n- Another powerful feature of Cloudflow is its ability to __hide your code__. When you use Cloudflow to serialize a machine learning models and associated functions, you exclude the original Python code as the resulting object is serialized. This can be particularly useful when you want to share a model but you don't want to reveal the underlying code. By hiding the code, you can protect your intellectual property and ensure that your models remain secure. This feature is especially important when working with sensitive data, where the privacy and security of the models is paramount.\n\n- [Installation](#installation)\n- [Getting started](#getting-started)\n- [API](#api)\n\n## ðŸ¦¾ Installation\n\n> You need Python 3.6 or above.\nFrom the terminal (or Anaconda prompt in Windows), enter:\n\n```bash\npip install cloudflow\n```\n\n## ðŸš€ Getting started\n\n- [Saving](#Saving)\n- [Loading](#Loading)\n\n## ðŸ’¾ Saving\n\n__All code from the saving part of this section can be found in demo/MLFLOW_save folder__\n\nFirst, let's train/import some models.\nTo simplify the demonstration, we will use models from the HuggingFace library. However, any other type of model can be used.\nBy using pre-trained models from HuggingFace, the demo code can focus on demonstrating the functionality of the cloudflow package, rather than on the complexities of model training and selection. This approach also makes it easier to reproduce the demo code, since the pre-trained models can be easily accessed and used by anyone who has access to the HuggingFace library.\n\n```python\nfrom transformers import pipeline\n\nsummarization_model  = pipeline(\"summarization\")        # Summarize a text\nsentiment_model      = pipeline(\"sentiment-analysis\")   # Sentiment analysis\ntranslation_model    = pipeline('translation_en_to_fr') # Traduction from english to french\nimage_classification = pipeline(\"image-classification\") # Image Classification\n\n```\nThis block of code imports the required packages pipeline from the Huggingfaces library. \nIt also imports four pipelines for different NLP/Image classifcation tasks - summarization, sentiment analysis, \ntranslation, and image classification - are initialized using the pipeline() method.\n\n\nThen, lets create a function that will combine all those models.\nWe will also use differents functions from differents modules to simulate a complexe project.\n\n```python\nimport download_file\n\ndef predict_to_save(sentiment_model, summarization_model, translation_model, image_classification, input_type, data=None, min_length=0, max_length=150):\n    \"\"\"\n    This function combines different machine learning models for sentiment analysis, summarization, translation, and image classification.\n\n    Parameters:\n    -----------\n    sentiment_model : object\n        The pipeline object for sentiment analysis.\n    summarization_model : object\n        The pipeline object for summarization.\n    translation_model : object\n        The pipeline object for translation.\n    image_classification : object\n        The pipeline object for image classification.\n    input_type : str\n        The type of input data, either \"sentiment\", \"translation\", \"image\", or \"summarization\".\n    data : str, optional\n        The input data to be used for prediction. Required only for \"summarization\" input type, and optional for \"image\".\n    min_length : int, optional\n        The minimum length of the summary (used only for \"summarization\" input type).\n    max_length : int, optional\n        The maximum length of the summary (used only for \"summarization\" input type).\n\n    Returns:\n    --------\n    result : object\n        The predicted result for the given input data and type.\n\n    Raises:\n    -------\n    ValueError\n        If an invalid input type is selected.\n    Exception\n        If there is an error during prediction.\n\n    Examples:\n    ---------\n    >>> predict_to_save(sentiment_model, summarization_model, translation_model, image_classification, input_type = \"sentiment\", \"I am happy today.\")\n    \"Positive\"\n    >>> predict_to_save(sentiment_model, summarization_model, translation_model, image_classification, input_type = \"translation\", \"Hello, how are you?\")\n    \"Bonjour, comment allez-vous?\"\n    >>> predict_to_save(sentiment_model, summarization_model, translation_model, image_classification, input_type = \"image\", \"https://example.com/image.jpg\")\n    {\"category\": \"dog\", \"confidence\": 0.95}\n    >>> predict_to_save(sentiment_model, summarization_model, translation_model, image_classification, input_type = \"summarization\", \"https://example.com/story.txt\")\n    {\"summary\": \"A summary of the story.\", \"input_text\": \"The full text of the story.\"}\n    \"\"\"\n\n    if input_type not in [\"sentiment\", \"translation\", \"image\", \"summarization\"]:\n        raise ValueError(\"Invalid input type selected\")\n\n    try:\n        if input_type == \"sentiment\":\n            return sentiment_model(data)\n        elif input_type == \"translation\":\n            return translation_model(data)\n        elif input_type == \"image\":\n            image = download_file.download_image(data)\n            return image_classification(image)\n        elif input_type == \"summarization\":\n            if data is None:\n                data = download_file.download_story()\n            summary = summarization_model(data, min_length=min_length, max_length=max_length)[0]\n            result = {\"summary\": summary, \"input_text\": data}\n            return result\n    except Exception as e:\n        raise Exception(f\"Error during prediction: {str(e)}\")\n\n```\n\nThis code defines a function called predict_to_save, which select model to predict with from different types of pre-trained models based on the input data type. The function takes in five required parameters, sentiment_model, summarization_model, translation_model, image_classification, and input_type, as well as three optional parameters, data, min_length, and max_length.\n\nThe purpose of the function is to make predictions using one or more of the provided pre-trained models based on the input_type. \n- If input_type is \"sentiment\", sentiment_model will be used to predict the sentiment of the input data. \n- If input_type is \"translation\", translation_model will be used to translate the input data. \n- If input_type is \"image\", image_classification will be used to classify the input image. \n- If input_type is \"summarization\", summarization_model will be used to summarize the input text.\n- If the input type is \"summarization\" and no data is provided, the function uses a story downloaded by the download_story function defined in the download_file module. \n\nThe demo code built to showcase the capabilities of Cloudflow to work with complex project structures, such as the one shown below:\n\n\n    â”Œâ”€â”€ sub_folder/             # Sub-folder\n        â”œâ”€â”€ sub_sub_folder/     # Sub-folder within the sub-folder\n            â”œâ”€â”€ is_sub.py       # Python file for printing a message to the console \n    â”œâ”€â”€ download_file.py        # Python file for downloading Image for image classif\n    â”œâ”€â”€ get_print.py            # Python file for printing a message to the console\n    â”œâ”€â”€ intermediate_module.py  # Python file for an intermediate module that connects different modules\n    â”œâ”€â”€ utils.py                # Python file for utility functions\n    â””â”€â”€ SAVE_MLflow.ipynb       # Main notebook file for demonstration\n\n\nAs you can see, the project folder has multiple files and folders, including subfolders with further subfolders. This project structure is more complex than a basic Python script, but the Cloudflow package can still be used to manage the workflow. The demo code showcases how to integrate the Cloudflow package with this type of project structure and provides examples of how to use the package to manage the workflow, track the results, and save the models.\n\n\nfinaly, we save our project:\n\n```python\n#import the necessary packages.\nimport mlflow\nimport cloudflow\n\n#specify the folder path where to save the model and the name of the experiment.\ntracking_uri =\"/saving_path\"\nexperiment_id = \"experiment_name\" \n\n# prepare the environment for logging the model and the results.\ncloudflow.prepare_env(tracking_uri,experiment_id)\n\n#start an MLflow run for the specified experiment and assign it to the run object.\nwith mlflow.start_run(experiment_id = experiment_id) as run:\n\n    print(\"RUN ID : \", run.info.run_id)\n\n    # log a metric in the current MLflow run, with the name 'test_metrics' and the value 0.99.\n    mlflow.log_metric('test_metrics', 0.99)\n\n    # create a cloudflow_model object with the specified debugging level (\"INFO\", \"DEBUG\", \"ERROR\", \"WARNING\").\n    model = cloudflow.cloudflow_model()\n\n    # save the cloudflow_model object to the specified file path and experiment, with the specified run_id. \n    # The predict_function is the function to use for making predictions, \n    # and the models dictionary contains the models to include in the saved model.        \n    model.save(tracking_uri    = tracking_uri,\n               experiment_id = experiment_id,\n               run_id = run.info.run_id, \n               predict_function = predict_to_save, \n               models = {\"summarization_model\"  : summarization_model, \n                         \"image_classification\" : image_classification,\n                         \"translation_model\"    : translation_model,\n                         \"sentiment_model\"      : sentiment_model})\n```\n\nThe code demonstrates just how easy it is to use the cloudflow packages to log and save machine learning models. By importing the necessary packages, specifying the file path and experiment name, and preparing the environment for logging, this script quickly sets up a system for tracking the results of a machine learning experiment. With just a few lines of code, a metric can be logged in the current MLflow run, and a cloudflow_model object can be created and saved to the specified file path and experiment.\n\n## ðŸ˜Ž Loading\n\n__All code from the Loading part of this section can be found in demo/MLFLOW_load folder__\n\nNow, let's load our project:\n\nA saved model can be loaded in just a few lines of code. \n\nThe load_model() method of the cloudflow_model object is used to load the saved model from the specified tracking URI, experiment ID, and run ID. Once the model is loaded, it can be used for making predictions by calling its predict function. \n\n```python\nimport cloudflow\n\nloaded_model = cloudflow.cloudflow_model()\n\nloaded_model.load_model(tracking_uri  = \"/saving_path\"\n                        experiment_id = \"experiment_name\" ,\n                        run_id        = \"# your run id\") #from the run.info.run_id\n\n```\n\nFinaly, we can use the saved predict_function\n\n```python\nloaded_model.predict(input_type = \"image\",\n                     data       = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\")\n\n```\nmore examples can be found in the MLFLOW_load notebook.\n\n\n## ðŸ“– API\n\nYou can choose your logging level when you initialize the cloudflow model\n```python\ncloudflow_model = cloudflow.cloudflow_model(log_level = \"DEBUG\") #(\"INFO\", \"DEBUG\", \"ERROR\", \"WARNING\")\n```\n\n\n### cloudflow_model.save(tracking_uri, experiment_id, run_id, predict_function, models)\n\nSave a predict function as an cloudflow model using cloudpickle.\n#### Parameters\n\n- __tracking_uri__ (str): The path to the folder where the MLflow experiment will be saved.\n- __experiment_id__ (str): The name of the MLflow experiment in which the model will be saved.\n- __run_id__ (str): The ID of the run (model ID).\n- __predict_function__ (callable): The user-defined predict function to save.\n- __models__ (dict): A dictionary of all models used in the predict function. The dictionary should have the following format:\n```python\n    { \"model_name_1\": model_1,\n      \"model_name_2\": model_2,\n    ... }\n```\nwhere each key is a string representing the name of the model (it must be the exact name used in the predict function), and each value is an object representing the corresponding model.\n\n\n\n### cloudflow_model.load_model(tracking_uri,experiment_id ,run_id)\n\nLoad a desired Cloudflow model.\n\n#### Parameters\n\n- __tracking_uri__ (str): The path to the folder where the MLflow experiment is saved.\n- __experiment_id__ (str): The name of the MLflow experiment where the model is saved.\n- __run_id__ (str): The ID of the run (model ID) to load.\n#### Returns\n- __MLflow_model__(callable): The loaded MLflow model.\n\n\n### cloudflow_model.predict(self, **params)\n\nExecute the user-defined saved predict function of the loaded model.\n\n#### Parameters\n\n- __params__ :\n    - (Any): params of the saved predict_function\n#### Returns\n- (Any): The expected output of the saved predict function.\n\n\n### prepare_env(tracking_uri, experiment_id)\n\nThis function prepares the environment for an MLflow experiment by setting the tracking URI and creating a new experiment with the specified experiment ID if it doesn't already exist.\n\n#### Parameters\n- __tracking_uri__ (str): The URI for tracking the MLflow experiment.\n- __experiment_id__ (str): The ID of the experiment to create or use.\n#### Returns\n- None\n\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/SimonLMC/cloudflow/",
    "keywords": "",
    "license": "mit",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cloudflow",
    "package_url": "https://pypi.org/project/cloudflow/",
    "platform": "any",
    "project_url": "https://pypi.org/project/cloudflow/",
    "project_urls": {
      "Documentation": "https://github.com/SimonLMC/cloudflow/",
      "Homepage": "https://github.com/SimonLMC/cloudflow/"
    },
    "release_url": "https://pypi.org/project/cloudflow/1.0.5/",
    "requires_dist": [
      "mlflow (>=2.0.1)",
      "cloudpickle (>=2.0.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "Cloudflow: A platform to streamline your end-to-end Machine Learning pipeline management",
    "version": "1.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17102952,
  "releases": {
    "1.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5b9f10d33549e631d2afdce539a9bc864911fa050f4f48f69ee2d597ffadd9a3",
          "md5": "6f2b9ac19c8e0fb2246aa34177c533a4",
          "sha256": "e8f50da337e9eb2f5a7724df9aa17d7d66cb155b19b9be58e9a7c8614af62865"
        },
        "downloads": -1,
        "filename": "cloudflow-1.0.5-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6f2b9ac19c8e0fb2246aa34177c533a4",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.6",
        "size": 13928,
        "upload_time": "2023-02-28T23:57:47",
        "upload_time_iso_8601": "2023-02-28T23:57:47.164889Z",
        "url": "https://files.pythonhosted.org/packages/5b/9f/10d33549e631d2afdce539a9bc864911fa050f4f48f69ee2d597ffadd9a3/cloudflow-1.0.5-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "41eabbcf351a20f41d95684c3197dcf5dcb18862a2ded015e843ed970924a698",
          "md5": "9fb188b22d3ec847709292ddb22a646c",
          "sha256": "ac5a274401a81e1fb3609bfc3a60e1a5dd3d3ad9691183d8a5762b9b13867b34"
        },
        "downloads": -1,
        "filename": "cloudflow-1.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "9fb188b22d3ec847709292ddb22a646c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 18331,
        "upload_time": "2023-02-28T23:57:48",
        "upload_time_iso_8601": "2023-02-28T23:57:48.862192Z",
        "url": "https://files.pythonhosted.org/packages/41/ea/bbcf351a20f41d95684c3197dcf5dcb18862a2ded015e843ed970924a698/cloudflow-1.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5b9f10d33549e631d2afdce539a9bc864911fa050f4f48f69ee2d597ffadd9a3",
        "md5": "6f2b9ac19c8e0fb2246aa34177c533a4",
        "sha256": "e8f50da337e9eb2f5a7724df9aa17d7d66cb155b19b9be58e9a7c8614af62865"
      },
      "downloads": -1,
      "filename": "cloudflow-1.0.5-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "6f2b9ac19c8e0fb2246aa34177c533a4",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": ">=3.6",
      "size": 13928,
      "upload_time": "2023-02-28T23:57:47",
      "upload_time_iso_8601": "2023-02-28T23:57:47.164889Z",
      "url": "https://files.pythonhosted.org/packages/5b/9f/10d33549e631d2afdce539a9bc864911fa050f4f48f69ee2d597ffadd9a3/cloudflow-1.0.5-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "41eabbcf351a20f41d95684c3197dcf5dcb18862a2ded015e843ed970924a698",
        "md5": "9fb188b22d3ec847709292ddb22a646c",
        "sha256": "ac5a274401a81e1fb3609bfc3a60e1a5dd3d3ad9691183d8a5762b9b13867b34"
      },
      "downloads": -1,
      "filename": "cloudflow-1.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "9fb188b22d3ec847709292ddb22a646c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 18331,
      "upload_time": "2023-02-28T23:57:48",
      "upload_time_iso_8601": "2023-02-28T23:57:48.862192Z",
      "url": "https://files.pythonhosted.org/packages/41/ea/bbcf351a20f41d95684c3197dcf5dcb18862a2ded015e843ed970924a698/cloudflow-1.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}