{
  "info": {
    "author": "Dr. Eren Unlu",
    "author_email": "eren.unlu@datategy.net",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "![--](docs/assets/dacoto_docs_01.png)\n\n##### ❏ ❏ ❏  ❏ ❏ ❏  ❏ ❏ ❏  ❏ ❏ ❏ \n\n```python\npip install dacota\n```\n##### ❏ ❏ ❏  ❏ ❏ ❏  ❏ ❏ ❏  ❏ ❏ ❏ \n\n# **DACOTA**\n\n##### ❏ ❏ ❏  ❏ ❏ ❏  ❏ ❏ ❏  ❏ ❏ ❏ \n\n### **Overview**\n\n“Dacota”, Datategy Cohort Targeting, (pronounced /- dəˈkoʊtə/ ) is a python library developed by Datategy SAS’s data science team making automatic cohort discovery a breeze.  We have identified the crucial need and urgency among the community to provide a tool which identifies cohorts related to a target variable (subgroups of the dataset of various sizes with similar characteristics) with solid statistical guarantees. \n\nUsing Dacota, by choosing your ultimate task, whether regression or classification and your desired parameters related to cohortization and optimization process, with a single click you can identify the subgroups with statistical confidence.\n\n\n### **Cohort Centric Data Science and ML**\n\nEver since Andrew Ng coined the term “Data Centric AI”, the community has increased its efforts to focus more on data in terms of analysis, feature engineering and quality augmentation rather than ML models. Though this observation is totally correct especially in terms of tabular tasks, there exists an important gap, especially for certain types of data and context : The need for a cohort centric approach. \n\nWhen considering “imbalance” in data science and ML, we directly refer to the imbalance of target variable’s distribution, either for regression or classification. From a perspective, this stems from the fact that we generally want to optimize an “overall metric”, averaged over whole test dataset such as “mean squared error” or “mean classification accuracy”. However, most of the time, in real life use cases, overall metric is far from reflecting the actual objectives. This is due to the “imbalance in predictors” rather than on target variable. In other words, due to the cohortized nature of the dataset. \n\nThese cohort specific concerns are prevalent especially in sensitive ML use cases, such as healthcare, security, finance etc. For instance, a dataset for developing a medical treatment may contain mostly young individuals, where the ML model optimized to maximize mean accuracy would discard the performance among elders, a cohort of minority. Of course, in this specific case the real life objective is to retain a certain degree of reliability for any important subgroup, rather than the overall metric. \n\nThough there exists many post hoc set of ML model performance assessment techniques, the cohort awareness should exists since the beginning of the development cycle, in exploratory data analysis.  Therefore, as Datategy data science team, we wanted to declare a manifest of “Cohort Centric AI”. \n\nThe library is at its extreme infancy, probably requiring tons of improvements in computation and also many functional extensions. We just wanted to spark an initiative around our cohort centric manifest and encourage community to improve the library. In our seed version, considering practically infinite set of options when it comes to automatic dataset cohortization, we aimed an optimal trade-off of usability, complexity,  customizability, speed, statistical reliability and accuracy. \n\n### **How it works ?**\n\nDacota uses genetic algorithm to identify diverse set of cohorts inside a dataset for a particular task related to a target variable.  In our devised framework, we assume each predictor is categorical. For continuous predictors, the algorithm automatically discretizes them in quantiles with number of bins set by the user. Given this constraint, a cohort can be encoded as a binary string like “10110…”, each bit representing a category of an input variable. Obviously, a cohort for a single feature cannot be all ‘0s’, whilst it can be all ‘1s’ (indicating this cohort for this particular feature has no statistical constraint) and any other combination. \n\nThe idea is to identify cohorts which are divergent from the whole dataset with predetermined minimum statistical confidence. For this purpose, in case of regression, we perform permuted t-test with unequal variance. Fitness of a cohort in this case represents the absolute  distance of cohort’s target variable mean from the whole dataset. For classification, we perform a permuted version of Jensen-Shannon distance measurement of categorical distribution of target variable. To incorporate the effect of cohort size, for each permutation, we sample a random control group of the same cohort size, record the JS distances and perform a t-test among sampled distances.   \n\nUnder this context, genetic algorithm is the natural choice of optimization as it provides maximum diversity in produced results and the notions of mutation and crossover fits well to the concept of cohortization.\n\nThe user can choose the minimum and maximum size of his/her cohort definition and minimum statistical confidence among other variables related to genetic algorithm optimization.\n\n### **Usage**\n\n```python\n## regression\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom dacota import process_cohortization, get_cohort\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.datasets import load_boston\n\n\ndict_data = load_boston()\ndf = pd.DataFrame(data = dict_data[\"data\"], columns=dict_data[\"feature_names\"])\ndf[\"target\"] = dict_data[\"target\"]\ndel(dict_data)\n\ncategorical_cols = [ \"CHAS\", 'target']\ncontinous_cols = ['ZN', 'INDUS', 'TAX',  'CRIM', 'AGE', 'NOX', 'DIS', 'PTRATIO', 'LSTAT', ]\ntarget_col = \"RAD\"\ntask = \"classification\"\nall_columns = list(set.union(set(categorical_cols),set(continous_cols),set([target_col])))\n\n\ndf = df[all_columns]\ndf_cohorts, df_generations = process_cohortization(df,\n                target_col,\n                categorical_cols, \n                continous_cols,\n                task,\n                minimum_confidence_for_a_cohort = 0.98, \n                minimum_population_for_a_cohort = 5,\n                maximum_population_for_a_cohort = 50,\n                no_permutations = 100,\n                no_initial_population_size = 2000,\n                no_generations= 20,\n                no_random_additions_per_generation = 1000,\n                no_mutations_per_generation = 1000,\n                no_crossovers_per_generation = 1000,\n                top_quantile_to_survive_per_generation = 0.5,\n                no_bins_discretization_continous = 5,\n                treat_nans_as_category = True,\n                target_nan_strategy = None,\n                )\n\nsolution_form = df_cohorts.iloc[0].solution_form\nget_cohort(df, solution_form, continous_cols)\n```\n\n```python\n## classification\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom dacota import process_cohortization, get_cohort\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.datasets import load_boston\n\ndict_data = load_boston()\ndf = pd.DataFrame(data = dict_data[\"data\"], columns=dict_data[\"feature_names\"])\ndf[\"target\"] = dict_data[\"target\"]\ndel(dict_data)\n\ncategorical_cols = [ \"CHAS\", 'target']\ncontinous_cols = ['ZN', 'INDUS', 'TAX',  'CRIM', 'AGE', 'NOX', 'DIS', 'PTRATIO', 'LSTAT', ]\ntarget_col = \"RAD\"\ntask = \"classification\"\nall_columns = list(set.union(set(categorical_cols),set(continous_cols),set([target_col])))\n\n\ndf = df[all_columns]\ndf_cohorts, df_generations = process_cohortization(df,\n                target_col,\n                categorical_cols, \n                continous_cols,\n                task,\n                minimum_confidence_for_a_cohort = 0.98, \n                minimum_population_for_a_cohort = 5,\n                maximum_population_for_a_cohort = 50,\n                no_permutations = 100,\n                no_initial_population_size = 2000,\n                no_generations= 20,\n                no_random_additions_per_generation = 1000,\n                no_mutations_per_generation = 1000,\n                no_crossovers_per_generation = 1000,\n                top_quantile_to_survive_per_generation = 0.5,\n                no_bins_discretization_continous = 5,\n                treat_nans_as_category = True,\n                target_nan_strategy = None,\n                )\n\nsolution_form = df_cohorts.iloc[0].solution_form\nget_cohort(df, solution_form, continous_cols)\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/datascientistunlu/dacota",
    "keywords": "Cohort,Cohortization,Data Analysis,Python 3,EDA",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dacota",
    "package_url": "https://pypi.org/project/dacota/",
    "platform": null,
    "project_url": "https://pypi.org/project/dacota/",
    "project_urls": {
      "Homepage": "https://github.com/datascientistunlu/dacota"
    },
    "release_url": "https://pypi.org/project/dacota/0.1.2/",
    "requires_dist": null,
    "requires_python": ">=3",
    "summary": "Datategy Cohort Targeting : One-click discovery of diverse cohorts in your dataset with statistical guarantees",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17095743,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7a2ee54340c103b9d8a36edcf52983c2ca3ce839a55169dd477340a09922d46e",
          "md5": "34fca4de47bd665f05f86bd82c403bfc",
          "sha256": "b095a733a239230e0f7ce2e60962dbb8d66b840d8615510c042f238659e466ae"
        },
        "downloads": -1,
        "filename": "dacota-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "34fca4de47bd665f05f86bd82c403bfc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 10738,
        "upload_time": "2023-02-24T23:50:38",
        "upload_time_iso_8601": "2023-02-24T23:50:38.444848Z",
        "url": "https://files.pythonhosted.org/packages/7a/2e/e54340c103b9d8a36edcf52983c2ca3ce839a55169dd477340a09922d46e/dacota-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "560205beb411bac744c403079e8afa373e1e6dea7869c8627ae3ab710f9f4e9b",
          "md5": "e905ce5166f9f95102763ce46f42407b",
          "sha256": "13dcbbfec2700b21ab5bee1cf613516795903e2324b739294ea258a70bdc2ae9"
        },
        "downloads": -1,
        "filename": "dacota-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e905ce5166f9f95102763ce46f42407b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 11992,
        "upload_time": "2023-02-24T23:50:40",
        "upload_time_iso_8601": "2023-02-24T23:50:40.715666Z",
        "url": "https://files.pythonhosted.org/packages/56/02/05beb411bac744c403079e8afa373e1e6dea7869c8627ae3ab710f9f4e9b/dacota-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "236d14ffc0c4d12f2a2c3652c937fefb3041097fe92c62b4a85204cbcb65c083",
          "md5": "65dce68f13e7e41687458a1fc201aa97",
          "sha256": "ade10b93ab9636a889b47b39fe3a958c1dff52671812a0a74e56943b2c423638"
        },
        "downloads": -1,
        "filename": "dacota-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "65dce68f13e7e41687458a1fc201aa97",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 11897,
        "upload_time": "2023-02-28T13:29:29",
        "upload_time_iso_8601": "2023-02-28T13:29:29.770416Z",
        "url": "https://files.pythonhosted.org/packages/23/6d/14ffc0c4d12f2a2c3652c937fefb3041097fe92c62b4a85204cbcb65c083/dacota-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "236d14ffc0c4d12f2a2c3652c937fefb3041097fe92c62b4a85204cbcb65c083",
        "md5": "65dce68f13e7e41687458a1fc201aa97",
        "sha256": "ade10b93ab9636a889b47b39fe3a958c1dff52671812a0a74e56943b2c423638"
      },
      "downloads": -1,
      "filename": "dacota-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "65dce68f13e7e41687458a1fc201aa97",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3",
      "size": 11897,
      "upload_time": "2023-02-28T13:29:29",
      "upload_time_iso_8601": "2023-02-28T13:29:29.770416Z",
      "url": "https://files.pythonhosted.org/packages/23/6d/14ffc0c4d12f2a2c3652c937fefb3041097fe92c62b4a85204cbcb65c083/dacota-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}