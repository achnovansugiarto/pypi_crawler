{
  "info": {
    "author": "Terry Chan",
    "author_email": "napoler2008@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# <p align=center>`Longformer-chinese`</p>\nAll work is based on `Longformer`(https://github.com/allenai/longformer)\n\n`Longformer-chinese` 提供了：基于BERT的中文预训练模型、在分类任务上的实现\n\n### WHAT'S DIFFERENT\n\n`Longformer-chinese` 基于BERT框架进行修改，在embedding层会与原版的稍有区别。加载时使用longformer.longformer：\n\n```\nfrom longformer.longformer import *\nconfig = LongformerConfig.from_pretrained('schen/longformer-chinese-base-4096')\nmodel = Longformer.from_pretrained('schen/longformer-chinese-base-4096', config=config)\n```\n\n\n\n\n#### 使用嵌入\n\n\n\n```python\nfrom longformer.longformer import Longformer, LongformerConfig,LongformerEmbedding\n\n# model=LongformerEmbedding('schen/longformer-chinese-base-4096',attention_mode='n2')\n\nmodel=LongformerEmbedding('schen/longformer-chinese-base-4096')\n\n# inputs = model.tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\",padding=\"max_length\",truncation=True,max_length=model.tokenizer.model_max_length)\ninputs = model.tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\",padding=\"max_length\",truncation=True,max_length=40)\n\noutputs = model(**inputs)\nprint(\"outputs\",outputs)\n\nprint(\"outputs\",outputs.keys())\n\nprint(\"outputs\",outputs['last_hidden_state'].size())\n\n```\n\n\n\n\n\n\n\n\n\n\n 使用`schen/longformer-chinese-base-4096`会自动从transformers下载预训练模型，也可以自行下载后替换成所在目录：\n https://huggingface.co/schen/longformer-chinese-base-4096\n\n### How to use\n\n1. Download pretrained model\n  * [`longformer-base-4096`](https://ai2-s2-research.s3-us-west-2.amazonaws.com/longformer/longformer-base-4096.tar.gz)\n  * [`longformer-large-4096`](https://ai2-s2-research.s3-us-west-2.amazonaws.com/longformer/longformer-large-4096.tar.gz)\n\n2. Install environment and code\n\n    ```bash\n    conda create --name longformer python=3.7\n    conda activate longformer\n    conda install cudatoolkit=10.0\n    pip install git+https://github.com/allenai/longformer.git\n    ```\n\n3. Run the model\n\n    ```python\n    import torch\n    from longformer.longformer import Longformer, LongformerConfig\n    from longformer.sliding_chunks import pad_to_window_size\n    from transformers import RobertaTokenizer\n\n    config = LongformerConfig.from_pretrained('longformer-base-4096/') \n    # choose the attention mode 'n2', 'tvm' or 'sliding_chunks'\n    # 'n2': for regular n2 attantion\n    # 'tvm': a custom CUDA kernel implementation of our sliding window attention\n    # 'sliding_chunks': a PyTorch implementation of our sliding window attention\n    config.attention_mode = 'sliding_chunks'\n\n    model = Longformer.from_pretrained('longformer-base-4096/', config=config)\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    tokenizer.model_max_length = model.config.max_position_embeddings\n\n    SAMPLE_TEXT = ' '.join(['Hello world! '] * 1000)  # long input document\n \n    input_ids = torch.tensor(tokenizer.encode(SAMPLE_TEXT)).unsqueeze(0)  # batch of size 1\n\n    # TVM code doesn't work on CPU. Uncomment this if `config.attention_mode = 'tvm'`\n    # model = model.cuda(); input_ids = input_ids.cuda()\n\n    # Attention mask values -- 0: no attention, 1: local attention, 2: global attention\n    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device) # initialize to local attention\n    attention_mask[:, [1, 4, 21,]] =  2  # Set global attention based on the task. For example,\n                                         # classification: the <s> token\n                                         # QA: question tokens\n\n    # padding seqlen to the nearest multiple of 512. Needed for the 'sliding_chunks' attention\n    input_ids, attention_mask = pad_to_window_size(\n            input_ids, attention_mask, config.attention_window[0], tokenizer.pad_token_id)\n\n    output = model(input_ids, attention_mask=attention_mask)[0]\n    ```\n\n### Model pretraining\n\n[This notebook](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) demonstrates our procedure for training Longformer starting from the RoBERTa checkpoint. The same procedure can be followed to get a long-version of other existing pretrained models. \n\n### TriviaQA\n\n* Training scripts: `scripts/triviaqa.py`\n* Pretrained large model: [`here`](https://ai2-s2-research.s3-us-west-2.amazonaws.com/longformer/triviaqa-longformer-large.tar.gz) (replicates leaderboard results)\n* Instructions: `scripts/cheatsheet.txt`\n\n\n### CUDA kernel\n\nOur custom CUDA kernel is implemented in TVM.  For now, the kernel only works on GPUs and Linux. We tested it on Ubuntu, Python 3.7, CUDA10, PyTorch >= 1.2.0. If it doesn't work for your environment, please create a new issue.\n\n**Compiling the kernel**: We already include the compiled binaries of the CUDA kernel, so most users won't need to compile it, but if you are intersted, check `scripts/cheatsheet.txt` for instructions.\n\n\n### Known issues\n\nPlease check the repo [issues](https://github.com/allenai/longformer/issues) for a list of known issues that we are planning to address soon. If your issue is not discussed, please create a new one. \n\n\n### Citing\n\nIf you use `Longformer` in your research, please cite [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150).\n```\n@article{Beltagy2020Longformer,\n  title={Longformer: The Long-Document Transformer},\n  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},\n  journal={arXiv:2004.05150},\n  year={2020},\n}\n```\n\n`Longformer` is an open-source project developed by [the Allen Institute for Artificial Intelligence (AI2)](http://www.allenai.org).\nAI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/napoler/longformer-chinese",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "longformer-chinese",
    "package_url": "https://pypi.org/project/longformer-chinese/",
    "platform": "",
    "project_url": "https://pypi.org/project/longformer-chinese/",
    "project_urls": {
      "Homepage": "https://github.com/napoler/longformer-chinese"
    },
    "release_url": "https://pypi.org/project/longformer-chinese/0.0.0.8/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Terry toolkit longformer chinese",
    "version": "0.0.0.8",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10874285,
  "releases": {
    "0.0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f58f4bbf0fc13825dfde825527bdd3c9a00350dc10f7d12bbdedf5967a553502",
          "md5": "7a0861edc38e03a7aa4bf6033cefca2b",
          "sha256": "5e77efeb7e08a3413fc9fc7bcbc362901faecc1f10c7687fce1d2b1989144192"
        },
        "downloads": -1,
        "filename": "longformer _chinese-0.0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7a0861edc38e03a7aa4bf6033cefca2b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16897,
        "upload_time": "2021-07-10T05:16:21",
        "upload_time_iso_8601": "2021-07-10T05:16:21.940334Z",
        "url": "https://files.pythonhosted.org/packages/f5/8f/4bbf0fc13825dfde825527bdd3c9a00350dc10f7d12bbdedf5967a553502/longformer%20_chinese-0.0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "99e0824fc8d13f318aab93ba1571cdeb6fa5ef84c0c7d7f8a09ca27afe55d874",
          "md5": "3cdb11a92cbb64eb6205ed58e340eae5",
          "sha256": "e8abdaa2d3ebc91ad60a920822f06a14041f968ad54d5f867d7ff3fd6bdbac77"
        },
        "downloads": -1,
        "filename": "longformer_chinese-0.0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3cdb11a92cbb64eb6205ed58e340eae5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16907,
        "upload_time": "2021-07-10T05:20:27",
        "upload_time_iso_8601": "2021-07-10T05:20:27.922787Z",
        "url": "https://files.pythonhosted.org/packages/99/e0/824fc8d13f318aab93ba1571cdeb6fa5ef84c0c7d7f8a09ca27afe55d874/longformer_chinese-0.0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b634e8f273a0c83497bf2b833479ef2878128f6c5cfa0677b75f5b86a2bf82fb",
          "md5": "a967e19ba48eed522172b73ffc8509da",
          "sha256": "5ce38e6e7bca6ae185211a2d78eaa113cf89a07f4de50e5ff5611be215a81914"
        },
        "downloads": -1,
        "filename": "longformer_chinese-0.0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "a967e19ba48eed522172b73ffc8509da",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16969,
        "upload_time": "2021-07-10T05:24:44",
        "upload_time_iso_8601": "2021-07-10T05:24:44.612514Z",
        "url": "https://files.pythonhosted.org/packages/b6/34/e8f273a0c83497bf2b833479ef2878128f6c5cfa0677b75f5b86a2bf82fb/longformer_chinese-0.0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f13a2ff4a5729c36d4e015e85df0d3868df7627b65ecd7656b43663102d039f5",
          "md5": "42d61498cecb2215524ce8e5e9a7d06e",
          "sha256": "810c6e781fc511749ae52ee49c3a2a54dfb7b622f9c0d45fb05787a2b4748f23"
        },
        "downloads": -1,
        "filename": "longformer_chinese-0.0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "42d61498cecb2215524ce8e5e9a7d06e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16990,
        "upload_time": "2021-07-10T05:27:44",
        "upload_time_iso_8601": "2021-07-10T05:27:44.041155Z",
        "url": "https://files.pythonhosted.org/packages/f1/3a/2ff4a5729c36d4e015e85df0d3868df7627b65ecd7656b43663102d039f5/longformer_chinese-0.0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d426d9fdea3572e486552bd2227614f4dca9425802266701681993636580d21c",
          "md5": "1bd46d97db7f97cfa792023fadee62c5",
          "sha256": "fab70ce766a18acb5ab25d4567cfda00d2b4417ba0e63b07929cad291555ddb2"
        },
        "downloads": -1,
        "filename": "longformer_chinese-0.0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "1bd46d97db7f97cfa792023fadee62c5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18078,
        "upload_time": "2021-07-10T07:38:07",
        "upload_time_iso_8601": "2021-07-10T07:38:07.539367Z",
        "url": "https://files.pythonhosted.org/packages/d4/26/d9fdea3572e486552bd2227614f4dca9425802266701681993636580d21c/longformer_chinese-0.0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "daa053314e0fba81b73b9ff1582e17cfd45d9b61f71405a01f20736d56c3f645",
          "md5": "c81047e260e381e020d22ae6707adac0",
          "sha256": "f4f7470981a1853b9fb529b178cbc731cedf7a90aa2ebbd00caa4a91562152c2"
        },
        "downloads": -1,
        "filename": "longformer_chinese-0.0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "c81047e260e381e020d22ae6707adac0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18057,
        "upload_time": "2021-07-10T07:38:09",
        "upload_time_iso_8601": "2021-07-10T07:38:09.000345Z",
        "url": "https://files.pythonhosted.org/packages/da/a0/53314e0fba81b73b9ff1582e17cfd45d9b61f71405a01f20736d56c3f645/longformer_chinese-0.0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9d04db8787dbf6ada9e83bb23b00d7f5e070b3deb7c535550d6240ed78ee830e",
          "md5": "768fea311f3d484b0321e12a233ef646",
          "sha256": "a0a788574664c3023b5b6dc0561744a700875a5a9bf0fbc7cf31e6fefcf6f97b"
        },
        "downloads": -1,
        "filename": "longformer_chinese-0.0.0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "768fea311f3d484b0321e12a233ef646",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18332,
        "upload_time": "2021-07-10T14:58:34",
        "upload_time_iso_8601": "2021-07-10T14:58:34.622785Z",
        "url": "https://files.pythonhosted.org/packages/9d/04/db8787dbf6ada9e83bb23b00d7f5e070b3deb7c535550d6240ed78ee830e/longformer_chinese-0.0.0.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9d04db8787dbf6ada9e83bb23b00d7f5e070b3deb7c535550d6240ed78ee830e",
        "md5": "768fea311f3d484b0321e12a233ef646",
        "sha256": "a0a788574664c3023b5b6dc0561744a700875a5a9bf0fbc7cf31e6fefcf6f97b"
      },
      "downloads": -1,
      "filename": "longformer_chinese-0.0.0.8.tar.gz",
      "has_sig": false,
      "md5_digest": "768fea311f3d484b0321e12a233ef646",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 18332,
      "upload_time": "2021-07-10T14:58:34",
      "upload_time_iso_8601": "2021-07-10T14:58:34.622785Z",
      "url": "https://files.pythonhosted.org/packages/9d/04/db8787dbf6ada9e83bb23b00d7f5e070b3deb7c535550d6240ed78ee830e/longformer_chinese-0.0.0.8.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}