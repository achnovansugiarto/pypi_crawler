{
  "info": {
    "author": "Graeme Holliday",
    "author_email": "gholliday2@hotmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "\n# PyTorch Vision Transformers with Distillation\nBased on the paper \"[Training data-efficient image transformers & distillation through attention](https://arxiv.org/pdf/2012.12877.pdf)\".\n\nThis repository will allow you to use distillation techniques with vision transformers in PyTorch. Most importantly, you can use pretrained models for the teacher, the student, or even both! My motivation was to use transfer learning to decrease the amount of resources it takes to train a vision transformer.\n\n### Quickstart\n\nInstall with `pip install distillable_vision_transformer` and load a pretrained transformer with:\n\n```python\nfrom distillable_vision_transformer import DistillableVisionTransformer\nmodel = DistillableVisionTransformer.from_pretrained('ViT-B_16')\n```\n\n### Installation\n\nInstall via pip:\n\n```\npip install distillable_vision_transformer\n```\n\nOr install from source:\n\n```\ngit clone https://github.com/Graeme22/DistillableVisionTransformer.git\ncd DistillableVisionTransformer\npip install -e .\n```\n\n### Usage\n\nLoad a model architecture:\n\n```python\nfrom distillable_vision_transformer import DistillableVisionTransformer\nmodel = DistillableVisionTransformer.from_name('ViT-B_16')\n```\n\nLoad a pretrained model:\n\n```python\nfrom distillable_vision_transformer import DistillableVisionTransformer\nmodel = DistillableVisionTransformer.from_pretrained('ViT-B_16')\n```\n\nDefault hyper parameters:\n\n| Param\\Model       | ViT-B_16 | ViT-B_32 | ViT-L_16 | ViT-L_32 |\n| ----------------- | -------- | -------- | -------- | -------- |\n| image_size        | 384, 384 | 384, 384 | 384, 384 | 384, 384 |\n| patch_size        | 16       | 32       | 16       | 32       |\n| emb_dim           | 768      | 768      | 1024     | 1024     |\n| mlp_dim           | 3072     | 3072     | 4096     | 4096     |\n| num_heads         | 12       | 12       | 16       | 16       |\n| num_layers        | 12       | 12       | 24       | 24       |\n| num_classes       | 1000     | 1000     | 1000     | 1000     |\n| attn_dropout_rate | 0.0      | 0.0      | 0.0      | 0.0      |\n| dropout_rate      | 0.1      | 0.1      | 0.1      | 0.1      |\n\nIf you need to modify these hyperparameters, just overwrite them:\n\n```python\nmodel = DistillableVisionTransformer.from_name('ViT-B_16', patch_size=64, emb_dim=2048, ...)\n```\n\n### Training\n\nWrap the student (instance of `DistillableVisionTransformer`) and the teacher (any network that you want to use to train the student) with a `DistillationTrainer`:\n\n```python\nfrom distillable_vision_transformer import DistillableVisionTransformer, DistillationTrainer\n\nstudent = DistillableVisionTransformer.from_pretrained('ViT-B_16')\ntrainer = DistillationTrainer(teacher=teacher, student=student) # where teacher is some pretrained network, e.g. an EfficientNet\n```\n\nFor the loss function, it is recommended that you use the `DistilledLoss` class, which is a kind of hybrid between cross-entropy and KL-divergence loss.\nIt takes as arguments `teacher_logits`, `student_logits`, and `distill_logits`, which are obtained from the forward pass on `DistillationTrainer`, as well as the true labels `labels`.\n\n```python\nfrom distillable_vision_transformer import DistilledLoss\n\nloss_fn = DistilledLoss(alpha=0.5, temperature=3.0)\nloss = loss_fn(teacher_logits, student_logits, distill_logits, labels)\n```\n\n### Inference\n\nFor inference, we want to use the `DistillableVisionTransformer` instance, not its `DistillationTrainer` wrapper.\n\n```python\nimport torch\nfrom distillable_vision_transformer import DistillableVisionTransformer\n\nmodel = DistillableVisionTransformer.from_pretrained('ViT-B_16')\nmodel.eval()\n\ninputs = torch.rand(1, 3, *model.image_size)\n# we can discard the distillation tokens, as they are only needed to calculate loss\noutputs, _ = model(inputs)\n```\n\n### Contributing\n\nIf you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Graeme22/DistillableVisionTransformer",
    "keywords": "",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "distillable-vision-transformer",
    "package_url": "https://pypi.org/project/distillable-vision-transformer/",
    "platform": "",
    "project_url": "https://pypi.org/project/distillable-vision-transformer/",
    "project_urls": {
      "Homepage": "https://github.com/Graeme22/DistillableVisionTransformer"
    },
    "release_url": "https://pypi.org/project/distillable-vision-transformer/1.0.1/",
    "requires_dist": [
      "torch (>=1.5.0)",
      "numpy"
    ],
    "requires_python": ">=3.5.0",
    "summary": "A PyTorch vision transformer for distillation.",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9044306,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bef65f911dd89848ad70f90df33f3900424634d43cce368f0a9dd92460f33b98",
          "md5": "6fd6d3ce1fadcfd19a7ce6461337b9c8",
          "sha256": "802fba5be13833b9615471d9db1a0e8a8f289b7342e65c8f435356351c111376"
        },
        "downloads": -1,
        "filename": "distillable_vision_transformer-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6fd6d3ce1fadcfd19a7ce6461337b9c8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5.0",
        "size": 11590,
        "upload_time": "2021-01-03T05:50:58",
        "upload_time_iso_8601": "2021-01-03T05:50:58.242285Z",
        "url": "https://files.pythonhosted.org/packages/be/f6/5f911dd89848ad70f90df33f3900424634d43cce368f0a9dd92460f33b98/distillable_vision_transformer-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "079da8dee88335161b20c32fe8759d1245f23d644cd3386d3dd102e417639b1b",
          "md5": "05d484b99031d2351ea483419b490275",
          "sha256": "20663a57754c7656aef57d379de5afb668f39c0de12e1acb03cceda5a4bccdb4"
        },
        "downloads": -1,
        "filename": "distillable_vision_transformer-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "05d484b99031d2351ea483419b490275",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5.0",
        "size": 8012,
        "upload_time": "2021-01-03T05:51:00",
        "upload_time_iso_8601": "2021-01-03T05:51:00.002063Z",
        "url": "https://files.pythonhosted.org/packages/07/9d/a8dee88335161b20c32fe8759d1245f23d644cd3386d3dd102e417639b1b/distillable_vision_transformer-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eb79a5212a21a0c6cc9ec76e93ddffbb30440778bf07fead70fc018526269a6f",
          "md5": "4a8fc16ab26418c07eadde146ec34843",
          "sha256": "c853401eae3548f0f713680147537db4337515eb399def47ac5a7d4099bca6bb"
        },
        "downloads": -1,
        "filename": "distillable_vision_transformer-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4a8fc16ab26418c07eadde146ec34843",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5.0",
        "size": 12081,
        "upload_time": "2021-01-03T21:20:12",
        "upload_time_iso_8601": "2021-01-03T21:20:12.074611Z",
        "url": "https://files.pythonhosted.org/packages/eb/79/a5212a21a0c6cc9ec76e93ddffbb30440778bf07fead70fc018526269a6f/distillable_vision_transformer-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bbdba79003717000e0b32820bd1a1eb38dd72e9f76215b336d7b1609513d0afe",
          "md5": "b035ced1c8b599fa8fccc5ae0877f85a",
          "sha256": "a7cc2daa4f455ab3705080c432e5a88db012764bb3de59968c894c621ba5f572"
        },
        "downloads": -1,
        "filename": "distillable_vision_transformer-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b035ced1c8b599fa8fccc5ae0877f85a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5.0",
        "size": 8671,
        "upload_time": "2021-01-03T21:20:15",
        "upload_time_iso_8601": "2021-01-03T21:20:15.110001Z",
        "url": "https://files.pythonhosted.org/packages/bb/db/a79003717000e0b32820bd1a1eb38dd72e9f76215b336d7b1609513d0afe/distillable_vision_transformer-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eb79a5212a21a0c6cc9ec76e93ddffbb30440778bf07fead70fc018526269a6f",
        "md5": "4a8fc16ab26418c07eadde146ec34843",
        "sha256": "c853401eae3548f0f713680147537db4337515eb399def47ac5a7d4099bca6bb"
      },
      "downloads": -1,
      "filename": "distillable_vision_transformer-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4a8fc16ab26418c07eadde146ec34843",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5.0",
      "size": 12081,
      "upload_time": "2021-01-03T21:20:12",
      "upload_time_iso_8601": "2021-01-03T21:20:12.074611Z",
      "url": "https://files.pythonhosted.org/packages/eb/79/a5212a21a0c6cc9ec76e93ddffbb30440778bf07fead70fc018526269a6f/distillable_vision_transformer-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bbdba79003717000e0b32820bd1a1eb38dd72e9f76215b336d7b1609513d0afe",
        "md5": "b035ced1c8b599fa8fccc5ae0877f85a",
        "sha256": "a7cc2daa4f455ab3705080c432e5a88db012764bb3de59968c894c621ba5f572"
      },
      "downloads": -1,
      "filename": "distillable_vision_transformer-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "b035ced1c8b599fa8fccc5ae0877f85a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5.0",
      "size": 8671,
      "upload_time": "2021-01-03T21:20:15",
      "upload_time_iso_8601": "2021-01-03T21:20:15.110001Z",
      "url": "https://files.pythonhosted.org/packages/bb/db/a79003717000e0b32820bd1a1eb38dd72e9f76215b336d7b1609513d0afe/distillable_vision_transformer-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}