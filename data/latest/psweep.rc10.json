{
  "info": {
    "author": "Steve Schmerler",
    "author_email": "git@elcorto.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "![pypi](https://img.shields.io/pypi/v/psweep?color=blue)\n![GitHub Workflow Status](https://img.shields.io/github/workflow/status/elcorto/psweep/tests?label=tests)\n[![DOI](https://zenodo.org/badge/92956212.svg)](https://zenodo.org/badge/latestdoi/92956212)\n\n# About\n\nThis package helps you to set up and run parameter studies.\n\nMostly, you'll start with a script and a for-loop and ask \"why do I\nneed a package for that\"? Well, soon you'll want housekeeping tools\nand a database for your runs and results. This package exists because\nsooner or later, everyone doing parameter scans arrives at roughly the\nsame workflow and tools.\n\nThis package deals with commonly encountered boilerplate tasks:\n\n* write a database of parameters and results automatically\n* make a backup of the database and all results when you repeat or\n  extend the study\n* append new rows to the database when extending the study\n* simulate a parameter scan\n* `git` support to track progress of your work and recover from mistakes\n* **experimental**: support for managing batch runs, e.g. on remote HPC\n  systems, including `git` support\n\nOtherwise, the main goal is to not constrain your flexibility by\nbuilding a complicated framework -- we provide only very basic building\nblocks. All data structures are really simple (dicts), as are the\nprovided functions. The database is a normal pandas DataFrame.\n\n\n# Getting started\n\nA simple example: Loop over two parameters `a` and `b` in a nested loop (grid),\ncalculate and store the result of a calculation for each parameter combination.\n\n```py\n>>> import random\n>>> import psweep as ps\n\n\n>>> def func(pset):\n...    return {\"result\": random.random() * pset[\"a\"] * pset[\"b\"]}\n\n>>> a = ps.plist(\"a\", [1,2,3])\n>>> b = ps.plist(\"b\", [88,99])\n>>> params = ps.pgrid(a,b)\n>>> df = ps.run_local(func, params)\n```\n\n`pgrid` produces a list `params` of parameter sets (dicts `{'a': ..., 'b':\n...}`) to loop over:\n\n```\n[{'a': 1, 'b': 88},\n {'a': 1, 'b': 99},\n {'a': 2, 'b': 88},\n {'a': 2, 'b': 99},\n {'a': 3, 'b': 88},\n {'a': 3, 'b': 99}]\n```\n\nand a database of results (pandas DataFrame `df`, pickled file\n`calc/database.pk` by default):\n\n```py\n>>> import pandas as pd\n>>> pd.set_option(\"display.max_columns\", None)\n>>> print(df)\n\n   a   b                               _run_id  \\\n0  1  88  d3e44cd1-96a1-4825-b931-4e5113b433cb\n1  1  99  d3e44cd1-96a1-4825-b931-4e5113b433cb\n2  2  88  d3e44cd1-96a1-4825-b931-4e5113b433cb\n3  2  99  d3e44cd1-96a1-4825-b931-4e5113b433cb\n4  3  88  d3e44cd1-96a1-4825-b931-4e5113b433cb\n5  3  99  d3e44cd1-96a1-4825-b931-4e5113b433cb\n\n                               _pset_id _calc_dir  \\\n0  4fe605c3-39a8-4fd4-8076-8b5d4a676657      calc\n1  809f4d31-f777-4912-8741-c5a2ed7a3803      calc\n2  ba4c1446-b390-4d5a-ad30-662a353e84e0      calc\n3  80acd6f8-c416-4c4f-8b8d-1668c6b3490e      calc\n4  79329ab7-9442-499e-b43b-a90b1a101eba      calc\n5  84a789b0-a2e9-4360-850f-739415de8c1d      calc\n\n                      _time_utc                                _pset_hash  \\\n0 2022-08-16 07:21:41.182055473  2580bf27aca152e5427389214758e61ea0e544e0\n1 2022-08-16 07:21:41.184616089  f2f17559c39b416483251f097ac895945641ea3a\n2 2022-08-16 07:21:41.186779737  010552c86c69e723feafb1f2fdd5b7d7f7e46e32\n3 2022-08-16 07:21:41.188885450  b57c5feac0608a43a65518f01da5aaf20a493535\n4 2022-08-16 07:21:41.190981627  719b2a864450534f5b683a228de018bc71f4cf2d\n5 2022-08-16 07:21:41.193049431  54baeefd998f4d8a8c9524c50aa0d88407cabb46\n\n   _pset_seq  _run_seq     result  _pset_runtime\n0          0         0  43.838220       0.000004\n1          1         0  62.688537       0.000003\n2          2         0  17.665135       0.000003\n3          3         0  65.960342       0.000005\n4          4         0  21.357208       0.000002\n5          5         0  71.136104       0.000003\n```\n\nYou see the columns `a` and `b`, the column `result` (returned by\n`func`) and a number of reserved fields for book-keeping such as\n\n```\n_run_id\n_pset_id\n_run_seq\n_pset_seq\n_pset_hash\n_pset_runtime\n_calc_dir\n_time_utc\n```\n\nObserve that one call `ps.run_local(func, params)` creates one `_run_id` -- a\nUUID identifying this run, where by \"run\" we mean one loop over all parameter\ncombinations. Inside that, each call `func(pset)` creates a UUID `_pset_id` and\na new row in the DataFrame (the database). In addition we also add sequential\ninteger IDs `_run_seq` and `_pset_seq` for convenience, as well as an\nadditional hash `_pset_hash` over the input dict (`pset` in the example) to\n`func()`. `_pset_runtime` is the time of one `func()` call. `_pset_seq` is the\nsame as the integer index `df.index`.\n\n\n# Concepts\n\nThe basic data structure for a param study is a list of \"parameter sets\" or\nshort \"psets\", each of which is a dict.\n\n\n```py\nparams = [{\"a\": 1, \"b\": 88},  # pset 1\n          {\"a\": 1, \"b\": 99},  # pset 2\n          ...                 # ...\n         ]\n```\n\nEach pset contains values of parameters which are varied during the parameter\nstudy.\n\nYou need to define a callback function `func`, which takes exactly one\npset such as:\n\n```py\n{'a': 1, 'b': 88}\n```\n\nand runs the workload for that pset. `func` must return a\ndict, for example:\n\n```py\n{'result': 1.234}\n```\n\nor an updated 'pset':\n\n```py\n{'a': 1, 'b': 88, 'result': 1.234}\n```\n\nWe always merge (`dict.update()`) the result of `func` with the pset, which gives\nyou flexibility in what to return from `func`. In particular, you are free to\nalso return an empty dict if you record results in another way (see the\n`save_data_on_disk` example later).\n\nThe psets form the rows of a pandas `DataFrame`, which we use to store the\npset and the result from each `func(pset)`.\n\nThe idea is now to run `func` in a loop over all psets in `params`. You do this\nusing the `ps.run_local()` helper function. The function adds some special\ncolumns such as `_run_id` (once per `ps.run_local()` call) or `_pset_id` (once\nper pset). Using `ps.run_local(... poolsize=...)` runs `func` in parallel on\n`params` using `multiprocessing.Pool`.\n\n# Building parameter grids\n\nThis package offers some very simple helper functions which assist in creating\n`params`. Basically, we define the to-be-varied parameters and then use\nsomething like `itertools.product()` to loop over them to create `params`,\nwhich is passed to `ps.run_local()` to actually perform the loop over all\npsets.\n\n```py\n>>> from itertools import product\n>>> import psweep as ps\n>>> a=ps.plist(\"a\", [1, 2])\n>>> b=ps.plist(\"b\", [\"xx\", \"yy\"])\n>>> a\n[{'a': 1}, {'a': 2}]\n>>> b\n[{'b': 'xx'}, {'b': 'yy'}]\n\n>>> list(product(a,b))\n[({'a': 1}, {'b': 'xx'}),\n ({'a': 1}, {'b': 'yy'}),\n ({'a': 2}, {'b': 'xx'}),\n ({'a': 2}, {'b': 'yy'})]\n\n>>> ps.itr2params(product(a,b))\n[{'a': 1, 'b': 'xx'},\n {'a': 1, 'b': 'yy'},\n {'a': 2, 'b': 'xx'},\n {'a': 2, 'b': 'yy'}]\n```\n\nHere we used the helper function `itr2params()` which accepts an iterator\nthat represents the loops over params. It merges dicts to psets and also deals\nwith nesting when using `zip()` (see below).\n\nThe last pattern is so common that we have a short-cut function\n`pgrid()`, which basically does `itr2params(product(a,b))`.\n\n```py\n>>> ps.pgrid(a,b)\n[{'a': 1, 'b': 'xx'},\n {'a': 1, 'b': 'yy'},\n {'a': 2, 'b': 'xx'},\n {'a': 2, 'b': 'yy'}]\n```\n\n`pgrid()` accepts either a sequence or individual args (but please\ncheck the \"pgrid gotchas\" section below for some corner cases).\n\n```py\n>>> ps.pgrid([a,b])\n>>> ps.pgrid(a,b)\n```\n\nSo the logic of the param study is entirely contained in the creation of\n`params`. For instance, if parameters shall be varied together (say `a` and\n`b`), then use `zip`. The nesting from `zip()` is flattened in `itr2params()`\nand `pgrid()`.\n\n```py\n>>> ##ps.itr2params(zip(a, b))\n>>> ps.pgrid([zip(a, b)])\n[{'a': 1, 'b': 'xx'},\n {'a': 2, 'b': 'yy'}]\n```\n\nLet's add a third parameter to vary. Of course, in general, plists can have\ndifferent lengths.\n\n```py\n>>> c=ps.plist(\"c\", [88, None, \"Z\"])\n>>> ##ps.itr2params(product(zip(a, b), c))\n>>> ##ps.pgrid([zip(a, b), c])\n>>> ps.pgrid(zip(a, b), c)\n[{'a': 1, 'b': 'xx', 'c': 88},\n {'a': 1, 'b': 'xx', 'c': None},\n {'a': 1, 'b': 'xx', 'c': 'Z'},\n {'a': 2, 'b': 'yy', 'c': 88},\n {'a': 2, 'b': 'yy', 'c': None},\n {'a': 2, 'b': 'yy', 'c': 'Z'}]\n```\n\nIf you want to add a parameter which is constant, use a list of length one.\n\n```py\n>>> const=ps.plist(\"const\", [1.23])\n>>> ps.pgrid(zip(a, b), c, const)\n[{'a': 1, 'b': 'xx', 'c': 88,   'const': 1.23},\n {'a': 1, 'b': 'xx', 'c': None, 'const': 1.23},\n {'a': 1, 'b': 'xx', 'c': 'Z',  'const': 1.23},\n {'a': 2, 'b': 'yy', 'c': 88,   'const': 1.23},\n {'a': 2, 'b': 'yy', 'c': None, 'const': 1.23},\n {'a': 2, 'b': 'yy', 'c': 'Z',  'const': 1.23}]\n```\n\nSo, as you can see, the general idea is that we do all the loops\n*before* running any workload, i.e. we assemble the parameter grid to be\nsampled before the actual calculations. This has proven to be very\npractical as it helps detecting errors early.\n\nYou are, by the way, of course not restricted to use simple nested loops\nover parameters using `pgrid()` (which uses `itertools.product()`). You\nare totally free in how to create `params`, be it using other fancy\nstuff from `itertools` or explicit loops. Of course you can also define\na static `params` list\n\n```py\nparams = [\n    {'a': 1,    'b': 'xx', 'c': None},\n    {'a': 1,    'b': 'yy', 'c': 1.234},\n    {'a': None, 'b': 'xx', 'c': 'X'},\n    ...\n    ]\n```\n\nor read `params` in from an external source such as a database from a\nprevious study, etc.\n\nThe point is: you generate `params`, we run the study.\n\n## Gotchas\n\n### `pgrid`\n\ntl;dr: `pgrid(a,b,...)` is a convenience API. It can't handle all corner\ncases. If in doubt, use `pgrid([a,b,...])` (or even `itr2params(product(...))`\ndirectly).\n\nNote that for a single param we have\n\n```py\n>>> a=ps.plist(\"a\", [1,2])\n>>> a\n[{'a': 1}, {'a': 2}]\n>>> ps.pgrid([a])\n[{'a': 1}, {'a': 2}]\n```\n\ni.e. the loop from `itertools.product()` is over `[a]` which returns `a`\nitself. You can leave off `[...]` if you have at least two args, say `a` and\n`b` as in\n\n```py\n>>> pgrid([a,b])\n>>> pgrid(a,b)\n```\n\nFor a single arg calling `pgrid(a)` is wrong since then `itertools.product()`\nwill be called on the entries of `a` which is not what you want. In fact doing\nso will raise an error.\n\nAlso, in case\n\n```py\n>>> pgrid([zip(a,b)])\n```\n\nthe list `[zip(a,b)]` is what you want to loop over and `pgrid(zip(a,b))` will\nraise an error, just as in case `pgrid(a)` above.\n\nAnd as before, if you have more plists, then `[...]` is optional, e.g.\n\n```py\n>>> pgrid([zip(a,b), c])\n>>> pgrid(zip(a, b), c)\n```\n\n\n### `zip`\n\nWhen using `zip(a,b)`, make sure that `a` and `b` have the same length, else `zip`\nwill return an iterator whose length is `min(len(a), len(b))`.\n\n\n# The database\n\nBy default, `ps.run_local()` writes a database `calc/database.pk` (a pickled\nDataFrame) with the default `calc_dir='calc'`. You can turn that off using\n`save=False` if you want. If you run `ps.run_local()` again\n\n```py\n>>> ps.run_local(func, params)\n>>> ps.run_local(func, other_params)\n```\n\nit will read and append to that file. The same happens in an interactive\nsession when you pass in `df` again, in which case we don't read it from disk:\n\n```py\n# default is df=None -> create empty df\n# save=False: don't write db to disk, optional\n>>> df_run_0 = ps.run_local(func, params, save=False)\n>>> df_run_0_and_1 = ps.run_local(func, other_params, save=False, df=df_run_0)\n```\n\n\n# Special database fields and repeated runs\n\nSee `examples/*repeat.py`.\n\nIt is important to get the difference between the two special fields\n`_run_id` and `_pset_id`, the most important one being `_pset_id`.\n\nBoth are random UUIDs. They are used to uniquely identify things.\n\nOnce per `ps.run_local()` call, a `_run_id` and `_run_seq` is created. Which\nmeans that when you call `ps.run_local()` multiple times *using the same\ndatabase* as just shown, you will see multiple (in this case two) `_run_id`\nand `_run_seq` values.\n\n```\n                             _run_id                              _pset_id  _run_seq  _pset_seq\n8543fdad-4426-41cb-ab42-8a80b1bebbe2  08cb5f7c-8ce8-451f-846d-db5ac3bcc746         0          0\n8543fdad-4426-41cb-ab42-8a80b1bebbe2  18da3840-d00e-4bdd-b29c-68be2adb164e         0          1\n8543fdad-4426-41cb-ab42-8a80b1bebbe2  bcc47205-0919-4084-9f07-072eb56ed5fd         0          2\n969592bc-65e6-4315-9e6b-5d64b6eaa0b3  809064d6-c7aa-4e43-81ea-cebfd4f85a12         1          3\n969592bc-65e6-4315-9e6b-5d64b6eaa0b3  ef5f06d4-8906-4605-99cb-2a9550fdd8de         1          4\n969592bc-65e6-4315-9e6b-5d64b6eaa0b3  162a7b8c-3ab5-41bb-92cd-1e5d0db0842f         1          5\n```\n\nEach `ps.run_local()` call in turn calls `func(pset)` for each pset in `params`.\nEach `func` invocation creates a unique `_pset_id` and increment the integer\ncounter `_pset_seq`. Thus, we have a very simple, yet powerful one-to-one\nmapping and a way to refer to a specific pset.\n\nAn interesting special case (see `examples/vary_1_param_repeat_same.py`) is\nwhen you call `ps.run_local()` multiple times using *the exact same* `params`,\n\n```py\n>>> ps.run_local(func, params)\n>>> ps.run_local(func, params)\n```\n\nwhich is perfectly fine, e.g. in cases where you just want to sample more data\nfor the same psets in `params` over and over again. In this case, you will have\nas above two unique `_run_id`s but *two sets of the same* `_pset_hash`.\n\n```\n                             _run_id                              _pset_id  _run_seq  _pset_seq                                _pset_hash  a    result\n8543fdad-4426-41cb-ab42-8a80b1bebbe2  08cb5f7c-8ce8-451f-846d-db5ac3bcc746         0          0  e4ad4daad53a2eec0313386ada88211e50d693bd  1  0.381589\n8543fdad-4426-41cb-ab42-8a80b1bebbe2  18da3840-d00e-4bdd-b29c-68be2adb164e         0          1  7b7ee754248759adcee9e62a4c1477ed1a8bb1ab  2  1.935220\n8543fdad-4426-41cb-ab42-8a80b1bebbe2  bcc47205-0919-4084-9f07-072eb56ed5fd         0          2  9e0e6d8a99c72daf40337183358cbef91bba7311  3  2.187107\n969592bc-65e6-4315-9e6b-5d64b6eaa0b3  809064d6-c7aa-4e43-81ea-cebfd4f85a12         1          3  e4ad4daad53a2eec0313386ada88211e50d693bd  1  0.590200\n969592bc-65e6-4315-9e6b-5d64b6eaa0b3  ef5f06d4-8906-4605-99cb-2a9550fdd8de         1          4  7b7ee754248759adcee9e62a4c1477ed1a8bb1ab  2  1.322758\n969592bc-65e6-4315-9e6b-5d64b6eaa0b3  162a7b8c-3ab5-41bb-92cd-1e5d0db0842f         1          5  9e0e6d8a99c72daf40337183358cbef91bba7311  3  1.639455\n```\n\nThis is a very powerful tool to filter the database for calculations that used\nthe same pset, e.g. an exact repetition of one experiment. But since we use\nUUIDs for `_pset_id`, those calculations can still be distinguished.\n\n\n# Best practices\n\nThe following workflows and practices come from experience. They are, if\nyou will, the \"framework\" for how to do things. However, we decided to\nnot codify any of these ideas but to only provide tools to make them\nhappen easily, because you will probably have quite different\nrequirements and workflows.\n\nPlease also have a look at the `examples/` dir where we document these\nand more common workflows.\n\n\n## Save data on disk, use UUIDs\n\nAssume that you need to save results from a `func()` call not only in the\nreturned dict from `func` (or even not at all!) but on disk, for instance when\nyou call an external program which saves data on disk. Consider this toy\nexample (`examples/save_data_on_disk/10run.py`):\n\n```py\n#!/usr/bin/env python3\n\nimport os\nimport subprocess\nimport psweep as ps\n\n\ndef func(pset):\n    fn = os.path.join(pset[\"_calc_dir\"], pset[\"_pset_id\"], \"output.txt\")\n    cmd = (\n        f\"mkdir -p $(dirname {fn}); \"\n        f\"echo {pset['a']} {pset['a']*2} {pset['a']*4} > {fn}\"\n    )\n    subprocess.run(cmd, shell=True)\n    return {\"cmd\": cmd}\n\n\nif __name__ == \"__main__\":\n    params = ps.plist(\"a\", [1, 2, 3, 4])\n    df = ps.run_local(func, params)\n    print(df)\n```\n\nIn this case, you call an external program (here a dummy shell command) which\nsaves its output on disk. Note that we don't return any output from the\nexternal command in `func`'s `return` statement. We only update the database\nrow added for each call to `func` by returning a dict `{\"cmd\": cmd}` with the\nshell `cmd` we call in order to have that in the database.\n\nAlso note how we use the special fields `_pset_id` and `_calc_dir`, which are\nadded in `ps.run_local()` to `pset` *before* `func` is called.\n\nAfter the run, we have four dirs for each pset, each simply named with\n`_pset_id`:\n\n```\ncalc\n├── 63b5daae-1b37-47e9-a11c-463fb4934d14\n│   └── output.txt\n├── 657cb9f9-8720-4d4c-8ff1-d7ddc7897700\n│   └── output.txt\n├── d7849792-622d-4479-aec6-329ed8bedd9b\n│   └── output.txt\n├── de8ac159-b5d0-4df6-9e4b-22ebf78bf9b0\n│   └── output.txt\n└── database.pk\n```\n\nThis is a useful pattern. History has shown that in the end, most naming\nconventions start simple but turn out to be inflexible and hard to adapt\nlater on. I have seen people write scripts which create things like:\n\n    calc/param_a=1.2_param_b=66.77\n    calc/param_a=3.4_param_b=88.99\n\ni.e. encode the parameter values in path names, because they don't have\na database. Good luck parsing that. I don't say this cannot be done --\nsure it can (in fact the example above easy to parse). It is just not\nfun -- and there is no need to. What if you need to add a \"column\"\nfor parameter 'c' later? Impossible (well, painful at least). This\napproach makes sense for very quick throw-away test runs, but gets out\nof hand quickly.\n\nSince we have a database, we can simply drop all data in\n`calc/<_pset_id>` and be done with it. Each parameter set is identified\nby a UUID that will never change. This is the only kind of naming\nconvention which makes sense in the long run.\n\n\n## Post-processing\n\nAn example of a simple post-processing script that reads data from disk\n(`examples/save_data_on_disk/20eval.py`):\n\n```py\ndf = ps.df_read(\"calc/database.pk\")\n\n# Filter database\ndf = df[df.a > 0 & ~df.a.isna()]\n\narr = np.array(\n    [np.loadtxt(f\"calc/{pset_id}/output.txt\") for pset_id in df._pset_id.values]\n)\n\n# Add new column to database, print and write new eval database\ndf[\"mean\"] = arr.mean(axis=1)\n\ncols = [\"a\", \"mean\", \"_pset_id\"]\nps.df_print(df[cols])\n\nps.df_write(df, \"calc/database_eval.pk\")\n```\n\n\n## Iterative extension of a parameter study\n\nSee `examples/multiple_local_1d_scans/` and `examples/*repeat*`.\n\nYou can backup old calc dirs when repeating calls to `ps.run_local()` using the\n`backup` keyword.\n\n```py\ndf = ps.run_local(func, params, backup=True)\n```\n\nThis will save a copy of the old `calc_dir` to something like\n\n```\ncalc.bak_2021-03-19T23:20:33.621004Z_run_id_d309c2c6-e4ba-4ef4-934c-2a4b2df07941\n```\n\nThat way, you can track old states of the overall study, and recover from\nmistakes, e.g. by just\n\n```sh\n$ rm -r calc\n$ mv calc.bak_2021-03-19T2* calc\n```\n\nFor any non-trivial work, you won't use an interactive session. Instead, you\nwill have a driver script (say `input.py`, or a jupyter notebook, or ...) which\ndefines `params` and starts `ps.run_local()`. Also in a common workflow, you\nwon't define `params` and run a study once. Instead you will first have an idea\nabout which parameter values to scan. You will start with a coarse grid of\nparameters and then inspect the results and identify regions where you need\nmore data (e.g. more dense sampling). Then you will modify `params` and run the\nstudy again. You will modify `input.py` multiple times, as you refine your\nstudy.\n\n\n## Use git\n\nInstead or in addition to using\n\n```py\n>>> ps.run_local(..., backup=True)\n```\n\nwe recommend a `git`-based workflow to at least track changes to `input.py`\n(instead of manually creating backups such as `input.py.0`, `input.py.1`, ...). You\ncan manually `git commit` at any point of course, or use\n\n```py\n>>> ps.run_local(..., git=True)\n```\n\nThis will commit any changes made to e.g. `input.py` itself and create a commit\nmessage containing the current `_run_id` such as\n\n```\npsweep: batch_with_git: run_id=68f5d9b7-efa6-4ed8-9384-4ffccab6f7c5\n```\n\nWe **strongly recommend** to create a `.gitignore` such as\n\n```\n# ignore backups\ncalc.bak*\n\n# ignore simulate runs\ncalc.simulate\n\n# ignore the whole calc/ dir, track only scripts\n##calc/\n\n# or just ignore potentially big files coming from a simulation you run\ncalc/*/*.bin\n```\n\n### How to handle large files when using git\n\nThe first option is to `.gitignore` them. Another is to use\n[`git-lfs`][git-lfs] (see the section on that later). That way you track their\nchanges but only store the most recent version. Or you leave those files on\nanother local or remote storage and store only the path to them (and maybe a\nhash) in the database. It's up to you.\n\nAgain, we don't enforce a specific workflow but instead just provide basic\nbuilding blocks.\n\n## Simulate / Dry-Run: look before you leap\n\nSee `examples/vary_1_param_simulate.py`.\n\nWhen you fiddle with finding the next good `params` and even when using\n`backup` and/or `git`, appending to the old database might be\na hassle if you find that you made a mistake when setting up `params`.\nYou need to abort the current run, copy the backup over or use `git` to go\nback.\n\nInstead, while you tinker with `params`, use another\n`calc_dir`, e.g.\n\n```sh\n# only needed so that we can copy the old database over\n$ mkdir -p calc.simulate\n$ cp calc/database.pk calc.simulate/\n```\n\n```py\ndf = ps.run_local(func, params, calc_dir='calc.simulate')\n```\n\nBut what's even better: keep everything as it is and just set\n`simulate=True`, which performs exactly the two steps above.\n\n```py\ndf = ps.run_local(func, params, simulate=True)\n```\n\nIt will copy only the database, not all the (possible large) data in `calc/` to\n`calc.simulate/` and run the study there. Additionally , it *will not call* call\n`func()` to run any workload. So you still append to your old database as in a\nreal run, but in a safe separate dir which you can delete later.\n\n\n## Advanced: Give runs names for easy post-processing\n\nSee `examples/vary_1_param_study_column.py`.\n\nPost-processing is not the scope of the package. The database is a\npandas DataFrame and that's it. You can query it and use your full pandas\nNinja skills here, e.g. \"give me all psets where parameter 'a' was\nbetween 10 and 100, while 'b' was constant, ...\". You get the idea.\n\nTo ease post-processing, it can be useful practice to add a constant parameter\nnamed \"study\" or \"scan\" to label a certain range of runs. If you, for instance,\nhave 5 runs (meaning 5 calls to `ps.run_local()`) where you scan values for\nparameter 'a' while keeping parameters 'b' and 'c' constant, you'll have 5\n`_run_id` values. When querying the database later, you could limit by\n`_run_id` if you know the values:\n\n```py\n>>> df_filtered = df[(df._run_id=='afa03dab-071e-472d-a396-37096580bfee') |\n                     (df._run_id=='e813db52-7fb9-4777-a4c8-2ce0dddc283c') |\n                     ...\n                     ]\n```\n\nThis doesn't look like fun. It shows that the UUIDs (`_run_id` and\n`_pset_id`) are rarely meant to be used directly, but rather to\nprogrammatically link psets and runs to other data (as shown above in the\n\"Save data on disk\" example). You can also use the integer IDs `_run_seq` and\n`_pset_seq` instead. But still, you need to know to which parameter values they\ncorrespond to.\n\nWhen possible, you could limit by the constant values of the other parameters:\n\n```py\n>>> df_filtered = df[(df.b==10) & (df.c=='foo')]\n```\n\nMuch better! This is what most post-processing scripts will do. In fact, we\nhave a shortcut function\n\n```py\n>>> conds = [df.b==10, df.c=='foo']\n>>> df_filtered = ps.df_filter_conds(df, conds)\n```\n\nwhich is useful in post-processing scripts where `conds` is\ncreated programmatically.\n\nBut when you have a column \"study\" which has the value `'a'` all the\ntime, it is just\n\n```py\n>>> df = df[df.study=='a']\n```\n\nYou can do more powerful things with this approach. For instance, say\nyou vary parameters 'a' and 'b', then you could name the \"study\"\nfield 'scan=a:b' and encode which parameters (thus column names) you\nhave varied. Later in the post-processing\n\n```py\n>>> study = 'scan=a:b'\n# cols = ['a', 'b']\n>>> cols = study.split('=')[1].split(':')\n>>> values = df[cols].values\n```\n\nSo in this case, a naming convention *is* useful in order to bypass\npossibly complex database queries. But it is still flexible -- you can\nchange the \"study\" column at any time, or delete it again.\n\nPro tip: You can manipulate the database at any later point and add the\n\"study\" column after all runs have been done.\n\nSuper Pro tip: Make a backup of the database first!\n\n\n# Remote cluster batch runs\n\nWe have experimental support for managing calculations on remote systems such\nas HPC clusters with a batch system like SLURM. It is basically a modernized\nand stripped-down version of\n[`pwtools.batch`](https://elcorto.github.io/pwtools/written/background/param_study.html).\nNote that we don't use any method like DRMAA to automatically dispatch jobs to\nclusters. We just write out a shell script to submit jobs, simple. Our design\nrevolves around maximal user control of each step of the workflow.\n\nThe central function to use is `ps.prep_batch()`. See `examples/batch_with_git`\nfor a full example.\n\nThe workflow is based on **template files**. In the templates, we use (for now)\nthe standard library's `string.Template`, where each `$foo` is replaced by a\nvalue contained in a pset, so `$param_a`, `$param_b`, as well as `$_pset_id`\nand so forth.\n\nWe piggy-back on the `run_local()` workflow from above to use all it's power\nand flexibility to, instead of running jobs with it, just **create batch\nscripts using template files**.\n\nYou can use the proposed workflow below directly the on remote machine (need to\ninstall `psweep` there) or run it locally and use a copy-to-cluster workflow.\nSince we actually don't start jobs or talk to the batch system, you have full\ncontrol over every part of the workflow. We just automate the boring stuff.\n\n## Workflow summary\n\n* define `params` to be varied as shown above (probably in a script, say\n  `input.py`)\n* in that script, call `ps.prep_batch(params)`, which does\n  * use `templates/calc/*`: scripts that you want to run in each batch job\n  * use `templates/machines/<mycluster>/jobscript`: batch job script\n  * read `templates/machines/<mycluster>/info.yaml`: machine-specific info\n    (e.g. command to submit the `jobscript`)\n  * define `func()` that will create a dir named `calc/<_pset_id>` for each\n    batch job, **replace placeholders** such as `$param_a` from psets\n    (including special ones such as `$_pset_id`)\n  * call `run_local(func, params)`\n  * create a script `calc/run_<mycluster>.sh` to submit all jobs\n\nThus, we replace running jobs directly (i.e. what `ps.run_local()` would do)\nwith:\n\n* use `prep_batch(params, ...)` instead of `run_local(params, ...)`\n* if running locally\n  * use `scp` or `rsync` or the helper script `bin/psweep-push <mycluster>` (uses\n    `rsync`) to copy `calc/` to a cluster\n  * ssh to cluster\n* execute `calc/run_<mycluster>.sh`, wait ...\n* if running locally\n  * use `scp` or `rsync` or the helper script use `bin/psweep-pull <mycluster>`\n    (uses `rsync`) to copy results back\n\nNow suppose that each of our batch jobs produces an output file, then we have\nthe same post-processing setup as in `save_data_on_disk`, namely\n\n```\ncalc\n├── 63b5daae-1b37-47e9-a11c-463fb4934d14\n│   └── output.txt\n├── 657cb9f9-8720-4d4c-8ff1-d7ddc7897700\n│   └── output.txt\n├── d7849792-622d-4479-aec6-329ed8bedd9b\n│   └── output.txt\n├── de8ac159-b5d0-4df6-9e4b-22ebf78bf9b0\n│   └── output.txt\n└── database.pk\n```\n\nPost-processing is (almost) as before:\n\n* analyze results, run post-processing script(s) on `calc/database.pk`, read in\n  `output.txt` for each `_pset_id`\n* when extending the study (modify `params`, call `input.py` again which calls\n  `prep_batch(params)`), we use the same features shown above\n  * append to database\n  * create new unique `calc/<_pset_id>` without overwriting anything\n  * **additionally**: write a new `calc/run_<mycluster>.sh` with old submit\n    commands still in there, but commented out\n\n\n## Templates layout and written files\n\nAn example template dir, based on ``examples/batch_with_git``:\n\n```\ntemplates\n├── calc\n│   └── run.py\n└── machines\n    ├── cluster\n    │   ├── info.yaml\n    │   └── jobscript\n    └── local\n        ├── info.yaml\n        └── jobscript\n```\n\n### calc templates\n\nEach file in `templates/calc` such as `run.py` will be treated as\ntemplate, goes thru the file template machinery and ends up in\n`calc/<_pset_id>/`.\n\n### machine templates\n\nThe example above has machine templates for 2 machines, \"local\" and a\nremote machine named \"cluster\". `psweep` will generate `run_<machine>.sh`\nfor both. Also you must provide a file `info.yaml` to store\nmachine-specific info. ATM this is only `subcmd`, e.g.\n\n```yaml\n# templates/machines/cluster/info.yaml\n---\nsubcmd: sbatch\n```\n\nAll other SLURM stuff can go into `templates/machines/cluster/jobscript`, e.g.\n\n```sh\n#SBATCH --time 00:20:00\n#SBATCH -o out.job\n#SBATCH -J foo_${_pset_seq}_${_pset_id}\n#SBATCH -p bar\n#SBATCH -A baz\n\n# Because we use Python's string.Template, we need to escape the dollar char\n# with two.\necho \"hostname=$$(hostname)\"\n\nmodule purge\n\nmodule load bzzrrr/1.2.3\nmodule load python\n\npython3 run.py\n```\n\nFor the \"local\" machine we'd just use `sh` (or `bash` or ...) as \"submit\ncommand\".\n\n```yaml\n\n# templates/machines/local/info.yaml\n---\nsubcmd: sh\n```\n\n\nThe files written are:\n\n```\nrun_cluster.sh                              # submit script for each machine\nrun_local.sh\n\ncalc/3c4efcb7-e37e-4ffe-800d-b05db171b41b   # one dir per pset\n├── jobscript_cluster                       # jobscript for each machine\n├── jobscript_local\n└── run.py                                  # from templates/calc/run.py\ncalc/11967c0d-7ce6-404f-aae6-2b0ea74beefa\n├── jobscript_cluster\n├── jobscript_local\n└── run.py\ncalc/65b7b453-96ec-4694-a0c4-4f71c53d982a\n...\n```\n\nIn each `run_<machine>.sh` we use the `subcmd` from `info.yaml`.\n\n```sh\n#!/bin/sh\nhere=$(pwd)\ncd 3c4efcb7-e37e-4ffe-800d-b05db171b41b; sbatch jobscript_cluster; cd $here  # run_seq=0 pset_seq=0\ncd 11967c0d-7ce6-404f-aae6-2b0ea74beefa; sbatch jobscript_cluster; cd $here  # run_seq=0 pset_seq=1\n...\n```\n\n## git support\n\nUse `prep_batch(..., git=True)` to have some basic git support such as\nautomatic commits in each call. It just uses `run_local(..., git=True)` when\ncreating batch scripts, so all best practices for that apply here as well. In\nparticular, make sure to create `.gitignore` first, else we'll track `calc/` as\nwell, which you may safely do when data in `calc` is small. Else use `git-lfs`,\nfor example.\n\n# Install\n\n```sh\n$ pip install psweep\n```\n\nDev install of this repo:\n\n```sh\n$ pip install -e .\n```\n\nSee also <https://github.com/elcorto/samplepkg>.\n\n# Tests\n\n```sh\n$ pytest\n```\n\n# Special topics\n\n## How to migrate a normal `git` repo to `git-lfs`\n\nFirst we'll quickly mention how to set up LFS in a *new* repo. In this case we\njust need to configure `git lfs` to track certain files. We'll use dirs\n`_pics/` and `calc/` as examples.\n\n```sh\n$ git lfs track \"_pics/**\" \"calc/**\"\n```\n\nwhere `**` means recursive. This will write the config to `.gitattributes`.\n\n```sh\n$ cat .gitattributes\n_pics/** filter=lfs diff=lfs merge=lfs -text\ncalc/** filter=lfs diff=lfs merge=lfs -text\n```\n\nPlease refer to the [git lfs docs][git-lfs] for more info.\n\nNote: LFS can be tricky to get right the first time around. We actually\nrecommend to fork the upstream repo, call that remote something like\n`lfsremote` and experiment with that before force-pushing LFS content to\n`origin`. Anyhow, let's continue.\n\nNow we like to migrate an existing git repo to LFS. Here we don't need to call\n`git lfs track` because we'll use `git lfs migrate import` to convert the repo.\nWe will use the `-I/--include=` option to specify which files we would like to\nconvert to LFS. Those patterns will end up in `.gitattributes` and the file\nwill even be created of not present already.\n\nWe found that only one `-I/--include=` at a time works, but we can separate\npatterns by \",\" to include multiple ones.\n\n```sh\n$ git lfs migrate import -I '_pics/**,calc/**' --include-ref=master\n\n$ cat .gitattributes\n_pics/** filter=lfs diff=lfs merge=lfs -text\ncalc/** filter=lfs diff=lfs merge=lfs -text\n```\n\nNow after the migrate, all LFS files in the working tree (files on disk) have\nbeen converted from their real content to text stub files.\n\n```sh\n$ cat _pics/foo.png\nversion https://git-lfs.github.com/spec/v1\noid sha256:de0a80ff0fa13a3e8cf8662c073ce76bfc986b64b3c079072202ecff411188ba\nsize 28339\n```\n\nThe following will not change that.\n\n```sh\n$ git push lfsremote master -f\n$ git lfs fetch lfsremote --all\n```\n\nTheir real content is however still contained in the `.git` dir. A simple\n\n```sh\n$ git lfs checkout .\n\n$ cat _pics/foo.png\n<<binary foo>>\n```\n\nwill bring the content back to the working dir.\n\n# Scope and related projects\n\nThis project aims to be easy to set up and use with as few dependencies and new\nconcepts to learn as possible. We strive to use standard Python data structures\n(dicts) and functionality (itertools) as well as widely available third party\npackages (pandas). Users should be able to get going quickly without having to\nset up and learn a complex framework. Perhaps most importantly, this project is\ncompletely agnostic to the field of study, e.g. any problem that can be\nformulated as \"let's vary X and analyze the results\".\n\nUnsurprisingly, there is a huge pile of similar tools. This project is super\nsmall and as such of course lacks a lot of features that other packages offer.\nWe just attempt to scratch some particular itches which we haven't found to\nbe covered in that combination by other tools, namely\n\n* simulate runs\n* backups\n* git support\n* simple local database (no db server to set up, no Mongo, etc)\n* interactive (Python REPL) and script-driven runs\n* local runs, also in parallel\n* tooling for remote runs (template-based workflow)\n* minimal naming conventions, rely on UUIDs\n* no yaml-ish DSLs, just Python please, thank you :)\n* no CLIs, just Python please, thank you :)\n* no config files, just Python please, thank you :)\n* not application specific (e.g. machine learning)\n\nHere is a list of related projects which offer some of the mechanisms\nimplemented here.\n\n* https://materialsproject.github.io/fireworks/\n* https://luigi.readthedocs.io\n* https://snakemake.readthedocs.io\n* https://github.com/eviatarbach/parasweep\n* https://github.com/SmokinCaterpillar/pypet\n* https://github.com/pharmbio/sciluigi\n* https://github.com/open-research/sumatra\n* https://www.nist.gov/programs-projects/simulation-management-tools\n* https://github.com/IDSIA/sacred\n* https://www.wandb.com/\n* https://github.com/maiot-io/zenml\n* https://github.com/LLNL/maestrowf\n* https://mlflow.org/\n* https://metaflow.org/\n* https://www.nextflow.io/\n* https://dvc.org/\n\n[git-lfs]: https://git-lfs.github.com\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/elcorto/psweep",
    "keywords": "parameter study sweep scan database pandas",
    "license": "BSD 3-Clause",
    "maintainer": "",
    "maintainer_email": "",
    "name": "psweep",
    "package_url": "https://pypi.org/project/psweep/",
    "platform": null,
    "project_url": "https://pypi.org/project/psweep/",
    "project_urls": {
      "Homepage": "https://github.com/elcorto/psweep"
    },
    "release_url": "https://pypi.org/project/psweep/0.9.0/",
    "requires_dist": [
      "docopt",
      "joblib",
      "pandas (>=0.19.2)",
      "pyyaml",
      "tabulate (>=0.8.2)"
    ],
    "requires_python": ">=3.8",
    "summary": "loop like a pro, make parameter studies fun: set up and run a parameter study/sweep/scan, save a database",
    "version": "0.9.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15083512,
  "releases": {
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f39ff83be8f219ea6e663e6c8d8d61a2934c80401c8c78a717563662b121fa36",
          "md5": "c7bf5b2dd91e4ce6e0b630d64d67f4d6",
          "sha256": "c35de839b64ef732fea88542d378645f7a03770a8666eee782499bc3c03c29b5"
        },
        "downloads": -1,
        "filename": "psweep-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c7bf5b2dd91e4ce6e0b630d64d67f4d6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5509,
        "upload_time": "2018-08-26T20:00:48",
        "upload_time_iso_8601": "2018-08-26T20:00:48.896107Z",
        "url": "https://files.pythonhosted.org/packages/f3/9f/f83be8f219ea6e663e6c8d8d61a2934c80401c8c78a717563662b121fa36/psweep-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f33cade75985db5c8bb467ecfee9a37dd4df817af7afc951176e1ac38d072535",
          "md5": "aa820b31a164285872f5c32bd1d071b1",
          "sha256": "8cf3be1baf2099ec597b22ca22dd14730948996707d59cbfeec2956e6e42895c"
        },
        "downloads": -1,
        "filename": "psweep-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "aa820b31a164285872f5c32bd1d071b1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7076,
        "upload_time": "2018-08-26T20:00:50",
        "upload_time_iso_8601": "2018-08-26T20:00:50.600746Z",
        "url": "https://files.pythonhosted.org/packages/f3/3c/ade75985db5c8bb467ecfee9a37dd4df817af7afc951176e1ac38d072535/psweep-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9d592ce9b6fa8eaa82153573255e3832ce4d92c0df8d3f854da04f28a2c372e6",
          "md5": "39102bbdaabd67342013d535277d9c4d",
          "sha256": "155944d4bb80278acd960e4651b1a716195a775457854c030a95f616417a2356"
        },
        "downloads": -1,
        "filename": "psweep-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "39102bbdaabd67342013d535277d9c4d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 13350,
        "upload_time": "2018-10-06T18:07:01",
        "upload_time_iso_8601": "2018-10-06T18:07:01.377632Z",
        "url": "https://files.pythonhosted.org/packages/9d/59/2ce9b6fa8eaa82153573255e3832ce4d92c0df8d3f854da04f28a2c372e6/psweep-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "119a0d264fc01434c3aab13f4e0f18ecacf071faec1ae645b3c2f44fdeb9b4e0",
          "md5": "6a730c9d8cfc6290c8bffaf9d7ca3236",
          "sha256": "d78a87eaf610b1bff4fc2b1fc1e15fb33b2704610a10a8f1bf0e6bcd6fe5502f"
        },
        "downloads": -1,
        "filename": "psweep-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "6a730c9d8cfc6290c8bffaf9d7ca3236",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16575,
        "upload_time": "2018-10-06T18:07:03",
        "upload_time_iso_8601": "2018-10-06T18:07:03.221734Z",
        "url": "https://files.pythonhosted.org/packages/11/9a/0d264fc01434c3aab13f4e0f18ecacf071faec1ae645b3c2f44fdeb9b4e0/psweep-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ca14b6e9aa752f2eed25b86e97ae875ef533d5b01c056ee692c7612352dd4a05",
          "md5": "d7d0e43c7b75ca7bf3ea452db17a7bf7",
          "sha256": "b9483d61782a6a93aa26237655f715587053901f0975a97d3a302a87d5662c98"
        },
        "downloads": -1,
        "filename": "psweep-0.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d7d0e43c7b75ca7bf3ea452db17a7bf7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 14313,
        "upload_time": "2019-08-27T10:54:42",
        "upload_time_iso_8601": "2019-08-27T10:54:42.959227Z",
        "url": "https://files.pythonhosted.org/packages/ca/14/b6e9aa752f2eed25b86e97ae875ef533d5b01c056ee692c7612352dd4a05/psweep-0.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f418442eda3223317fe8cf4f1eb9222f4a3b29d4b531215a68e5f200d2dda37",
          "md5": "367a6a8191aaccbae638fe07d7b5ae36",
          "sha256": "49d3009d940be351aab9fffe309427fe260d47ad8f75dcc20b4b5ed56ad2221f"
        },
        "downloads": -1,
        "filename": "psweep-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "367a6a8191aaccbae638fe07d7b5ae36",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 21132,
        "upload_time": "2019-08-27T10:54:45",
        "upload_time_iso_8601": "2019-08-27T10:54:45.876541Z",
        "url": "https://files.pythonhosted.org/packages/5f/41/8442eda3223317fe8cf4f1eb9222f4a3b29d4b531215a68e5f200d2dda37/psweep-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "42f1586b3bd96058788b232adbafa049dcafbf4d818635c630d7f846176e8a30",
          "md5": "18476ad3c3ac04a00acef5a5faf69adf",
          "sha256": "fbb944f9f63db561f97ee40a95250ae398fb4d0696cb1aaee5318d67ab7fa2d9"
        },
        "downloads": -1,
        "filename": "psweep-0.4.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "18476ad3c3ac04a00acef5a5faf69adf",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 14910,
        "upload_time": "2020-05-22T20:34:17",
        "upload_time_iso_8601": "2020-05-22T20:34:17.254455Z",
        "url": "https://files.pythonhosted.org/packages/42/f1/586b3bd96058788b232adbafa049dcafbf4d818635c630d7f846176e8a30/psweep-0.4.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "122a89a930bd9a67bd681736410ae56e8c63c01f9df3d8eebcdb3a1b68ce5ee9",
          "md5": "4cb932316aba4436552a979a410a98a4",
          "sha256": "057250bbbebbd3f2de6d6a59dfbd564ea9ae46bd8dd8b6b24a7d0292a34d76ca"
        },
        "downloads": -1,
        "filename": "psweep-0.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "4cb932316aba4436552a979a410a98a4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22308,
        "upload_time": "2020-05-22T20:34:19",
        "upload_time_iso_8601": "2020-05-22T20:34:19.090571Z",
        "url": "https://files.pythonhosted.org/packages/12/2a/89a930bd9a67bd681736410ae56e8c63c01f9df3d8eebcdb3a1b68ce5ee9/psweep-0.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1e189215b3425e42b384e53fe4b2dc6718a4a711b0ba2511a17f019e747f930b",
          "md5": "16f7b31cc157121e0542411eeb15725c",
          "sha256": "a9ac40f33f267ecaa603adba616660e4c783188cdfff7cc53882c3fce6149cc1"
        },
        "downloads": -1,
        "filename": "psweep-0.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "16f7b31cc157121e0542411eeb15725c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 20501,
        "upload_time": "2021-03-21T20:07:36",
        "upload_time_iso_8601": "2021-03-21T20:07:36.745790Z",
        "url": "https://files.pythonhosted.org/packages/1e/18/9215b3425e42b384e53fe4b2dc6718a4a711b0ba2511a17f019e747f930b/psweep-0.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1fd7f9d778b4a1bc24c5433828576124b89563a5532130db8bd1bbb46daa82b4",
          "md5": "5c6113979f48525ff434d649e121267e",
          "sha256": "d12b184b3ac1b119b0befdfe99194b1f13d7d81f0a6e4f514fc4047405ad4c59"
        },
        "downloads": -1,
        "filename": "psweep-0.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "5c6113979f48525ff434d649e121267e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 31972,
        "upload_time": "2021-03-21T20:07:38",
        "upload_time_iso_8601": "2021-03-21T20:07:38.717770Z",
        "url": "https://files.pythonhosted.org/packages/1f/d7/f9d778b4a1bc24c5433828576124b89563a5532130db8bd1bbb46daa82b4/psweep-0.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1fdf632725d36d3d557df7c993186d6b31799e4add65496d4af23e46bed3f760",
          "md5": "d5e0c9d776ec497bd27b7ead9d3ccdc1",
          "sha256": "8c9b424de95e839388b5a8b439f667e3d2d94202f033c50ab84f369e035b5fd0"
        },
        "downloads": -1,
        "filename": "psweep-0.5.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d5e0c9d776ec497bd27b7ead9d3ccdc1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 20821,
        "upload_time": "2021-10-28T18:55:59",
        "upload_time_iso_8601": "2021-10-28T18:55:59.934889Z",
        "url": "https://files.pythonhosted.org/packages/1f/df/632725d36d3d557df7c993186d6b31799e4add65496d4af23e46bed3f760/psweep-0.5.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c5a2416e93e9a74739321796b7e5f15cc01be4f68524593444ad8ec08bb91ddf",
          "md5": "c192456500e0fb0a81272e0d76ebf9ef",
          "sha256": "66347d2dfa7f61e1119b9dd26bad703295e5175fc15582ad131166a3305bb6bc"
        },
        "downloads": -1,
        "filename": "psweep-0.5.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c192456500e0fb0a81272e0d76ebf9ef",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 29073,
        "upload_time": "2021-10-28T18:56:01",
        "upload_time_iso_8601": "2021-10-28T18:56:01.748508Z",
        "url": "https://files.pythonhosted.org/packages/c5/a2/416e93e9a74739321796b7e5f15cc01be4f68524593444ad8ec08bb91ddf/psweep-0.5.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d915bf7770d524ffbd06dd9e60110e0858b183f29c354eca0f6a6758af55010",
          "md5": "78fb7e42ede927fec34419b8b6adec55",
          "sha256": "c0f9803099a4dcff3a192a36ec3df6c4d0ef644666de5e689d6c5c965ab627c2"
        },
        "downloads": -1,
        "filename": "psweep-0.6.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "78fb7e42ede927fec34419b8b6adec55",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 27938,
        "upload_time": "2022-04-07T21:40:45",
        "upload_time_iso_8601": "2022-04-07T21:40:45.866405Z",
        "url": "https://files.pythonhosted.org/packages/5d/91/5bf7770d524ffbd06dd9e60110e0858b183f29c354eca0f6a6758af55010/psweep-0.6.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "279f99a0bf364bfb76ea2e360a26c44afd96d8a94dd4ab6a2fd7e8daaf0918f9",
          "md5": "e6b8677139120d911be2422522168b37",
          "sha256": "107d0c49343f10ad22eeac537db12794b4241c63f1dd1de6a61845cfe03631b7"
        },
        "downloads": -1,
        "filename": "psweep-0.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "e6b8677139120d911be2422522168b37",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 51788,
        "upload_time": "2022-04-07T21:40:48",
        "upload_time_iso_8601": "2022-04-07T21:40:48.472447Z",
        "url": "https://files.pythonhosted.org/packages/27/9f/99a0bf364bfb76ea2e360a26c44afd96d8a94dd4ab6a2fd7e8daaf0918f9/psweep-0.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.7.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cf2222af42704cc34722f06438cf1f0c34bfa6eb6c5762815982f14370ca994b",
          "md5": "0da7a9de3dce6c766c556475b9023f8a",
          "sha256": "aee2e6c86e6b185dc801c5ecc78b13eadf8510065ea1ee182de4529be7fde87f"
        },
        "downloads": -1,
        "filename": "psweep-0.7.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0da7a9de3dce6c766c556475b9023f8a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 29024,
        "upload_time": "2022-05-25T11:51:50",
        "upload_time_iso_8601": "2022-05-25T11:51:50.459044Z",
        "url": "https://files.pythonhosted.org/packages/cf/22/22af42704cc34722f06438cf1f0c34bfa6eb6c5762815982f14370ca994b/psweep-0.7.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b002a7a5882763389a84eee9747cdd6f3a0eb420e254d627d0cfe8823bdb8e15",
          "md5": "87a029e0729569f87ddf2691cd5973a6",
          "sha256": "ab189f45eb5eeb628bd953ce039896d6aef6c42de50ec05b026a40dcc944b153"
        },
        "downloads": -1,
        "filename": "psweep-0.7.0.tar.gz",
        "has_sig": false,
        "md5_digest": "87a029e0729569f87ddf2691cd5973a6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 52828,
        "upload_time": "2022-05-25T11:51:53",
        "upload_time_iso_8601": "2022-05-25T11:51:53.241834Z",
        "url": "https://files.pythonhosted.org/packages/b0/02/a7a5882763389a84eee9747cdd6f3a0eb420e254d627d0cfe8823bdb8e15/psweep-0.7.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.8.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ecc26f4a41bc37220b493d9b6469381329c09cd63295296c9632a4d88d85b9f",
          "md5": "9f2f4766bcfea0c2fc43ef077b2236bb",
          "sha256": "eac7feab7a667e0aae10d2ac943d1d4989858d99afd03eacb36a73e6b0de1dbd"
        },
        "downloads": -1,
        "filename": "psweep-0.8.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9f2f4766bcfea0c2fc43ef077b2236bb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 29056,
        "upload_time": "2022-08-23T20:12:09",
        "upload_time_iso_8601": "2022-08-23T20:12:09.919659Z",
        "url": "https://files.pythonhosted.org/packages/5e/cc/26f4a41bc37220b493d9b6469381329c09cd63295296c9632a4d88d85b9f/psweep-0.8.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "78744ffa039bd02af75a1989d4ae385733c75283cdfc18d4e8a9f7d0a04eb1e5",
          "md5": "b42c43005a6561ad8b9dc4a1a5e73b73",
          "sha256": "ef8de903d823bbfac46e85439039e4de7ca4a45770b70c8ca2a805c2155e6a27"
        },
        "downloads": -1,
        "filename": "psweep-0.8.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b42c43005a6561ad8b9dc4a1a5e73b73",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 52873,
        "upload_time": "2022-08-23T20:12:12",
        "upload_time_iso_8601": "2022-08-23T20:12:12.437914Z",
        "url": "https://files.pythonhosted.org/packages/78/74/4ffa039bd02af75a1989d4ae385733c75283cdfc18d4e8a9f7d0a04eb1e5/psweep-0.8.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.9.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b843839363db16dd5acef7d9cd98e6b6b87769497a1cabef778458e9dc05894f",
          "md5": "2bc09f505ff25d2976f1ba032ea99b24",
          "sha256": "65ac5d6a96ff7a0e9fd633a47716239ef9f3e1e7f72d7399b11cf3903ca85d5b"
        },
        "downloads": -1,
        "filename": "psweep-0.9.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2bc09f505ff25d2976f1ba032ea99b24",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 29616,
        "upload_time": "2022-09-13T20:44:58",
        "upload_time_iso_8601": "2022-09-13T20:44:58.910292Z",
        "url": "https://files.pythonhosted.org/packages/b8/43/839363db16dd5acef7d9cd98e6b6b87769497a1cabef778458e9dc05894f/psweep-0.9.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8568d48a84112ea44e6a05d842735a46c6c70e7b9c91411a2e3eb80bc6ede10c",
          "md5": "1aee7aae6b5fdcd2566cef271a960913",
          "sha256": "12186d4726c834d715592e4ab9fe7f7b0adcf88b88dad5c6de9a1f21cb610fce"
        },
        "downloads": -1,
        "filename": "psweep-0.9.0.tar.gz",
        "has_sig": false,
        "md5_digest": "1aee7aae6b5fdcd2566cef271a960913",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 53477,
        "upload_time": "2022-09-13T20:45:01",
        "upload_time_iso_8601": "2022-09-13T20:45:01.095554Z",
        "url": "https://files.pythonhosted.org/packages/85/68/d48a84112ea44e6a05d842735a46c6c70e7b9c91411a2e3eb80bc6ede10c/psweep-0.9.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b843839363db16dd5acef7d9cd98e6b6b87769497a1cabef778458e9dc05894f",
        "md5": "2bc09f505ff25d2976f1ba032ea99b24",
        "sha256": "65ac5d6a96ff7a0e9fd633a47716239ef9f3e1e7f72d7399b11cf3903ca85d5b"
      },
      "downloads": -1,
      "filename": "psweep-0.9.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "2bc09f505ff25d2976f1ba032ea99b24",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 29616,
      "upload_time": "2022-09-13T20:44:58",
      "upload_time_iso_8601": "2022-09-13T20:44:58.910292Z",
      "url": "https://files.pythonhosted.org/packages/b8/43/839363db16dd5acef7d9cd98e6b6b87769497a1cabef778458e9dc05894f/psweep-0.9.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8568d48a84112ea44e6a05d842735a46c6c70e7b9c91411a2e3eb80bc6ede10c",
        "md5": "1aee7aae6b5fdcd2566cef271a960913",
        "sha256": "12186d4726c834d715592e4ab9fe7f7b0adcf88b88dad5c6de9a1f21cb610fce"
      },
      "downloads": -1,
      "filename": "psweep-0.9.0.tar.gz",
      "has_sig": false,
      "md5_digest": "1aee7aae6b5fdcd2566cef271a960913",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 53477,
      "upload_time": "2022-09-13T20:45:01",
      "upload_time_iso_8601": "2022-09-13T20:45:01.095554Z",
      "url": "https://files.pythonhosted.org/packages/85/68/d48a84112ea44e6a05d842735a46c6c70e7b9c91411a2e3eb80bc6ede10c/psweep-0.9.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}