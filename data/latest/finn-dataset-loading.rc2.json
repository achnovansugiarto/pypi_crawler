{
  "info": {
    "author": "Fergal Cotter",
    "author_email": "fbc23@cam.ac.uk",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Topic :: Software Development :: Libraries",
      "Topic :: Utilities"
    ],
    "description": "Dataset Loading\n===============\n\n**This is a fork of dataset_loading, used as a dependency for the finn-examples repo.**\n\nThis repo is aimed at being a centralized resource for loading in commonly used\nimage datasets like CIFAR, PASCAL VOC, MNIST, ImageNet and others.\n\nSome of these datasets will fit easily on disk (CIFAR and MNIST), but many of\nthe others won't. This means we have to set up threads to load them as we need\nthem into memory. Tensorflow provides some ability to do this, but after\nseveral attempts at using these resources, we found them far too opaque and\ndifficult to use. This package does essentially the same thing as what\ntensorflow does, but using python's threading, multiprocessing and queue\npackages. \n\n\n\nThreads vs Processes\n--------------------\nInitially this package would only use Python's threading package to parallelize\ntasks. It quickly became apparent that this caps the benefits of\nparallelization, as all of these threads will only take up to 1 processor core.\nIn reality, we want to be able to take up more processors for data loading to\nreduce bottlenecks. It is still untested, but we are adding in multiprocess\nsupport for the heavy lifting tasks (in particular, loading and preprocessing\nimages into `The ImageQueue`_).\n\nDataset Specific Usage\n----------------------\nFor instructions on how to call the functions to get images in for common\ndatasets, see their help pages. These functions wrap around the `General Usage`_\nfunctions and are provided for convenience. If your application doesn't quite\nfit into these functions, or if you have a new dataset, have a look at `General\nUsage`_, as it was designed to make queueing for any dataset type as easy as\npossible.\n\n- `MNIST usage instructions`__\n- `CIFAR10/CIFAR100 usage instructions`__\n\n__ http://dataset-loading.readthedocs.io/en/latest/mnist.html \n__ http://dataset-loading.readthedocs.io/en/latest/cifar.html \n\nGeneral Usage\n-------------\nFor the bigger datasets, we need 2 queues and several threads (perhaps on\nmultiple processors) to load images in parallel.\n\n1. A File Queue to store a list of file names.\n   Sequencing can be done by shuffling the file names before inserting into the\n   queue. \n\n   - One thread should be enough to manage this queue.\n\n2. An Image Queue to load images into.\n\n   - Several threads will likely be needed to read file names from the file\n     queue, load from disk, and put into the Image Queue. We may get away with\n     running these all in one Python process, but may need to use more.\n\n\nThe FileQueue\n~~~~~~~~~~~~~\nA FileQueue_ is used to store a list of file names (e.g.  jpegs).  This is also\nthe location of sequencing (there is an option to shuffle the entries in this\nqueue when adding) and where we set the limits on the number of epochs processed\n(if we wish to). For example, this would set up a file queue for 50 epochs: \n\n.. code:: python\n\n    import dataset_loading as dl\n    IM_DIR = /path/to/images\n    files = os.listdir(IM_DIR)\n    files = [f for f in files if os.path.splitext(f)[1] == '.jpeg']\n    file_queue = dl.FileQueue()\n    file_queue.load_epochs(files, max_epochs=50)\n    ...\n    ...\n    file_queue.join()\n\nThe `load_epochs` method will also start a single thread to manage the queue and\nrefill it if it's getting low (shuffling along as it goes).\n\nIf you know what the labels are, you should also feed them to the File Queue\nalongside the file names in a list of (file, label) tuples. E.g.:\n\n.. code:: python\n\n    # Assume <labels> is a list of all of the labels and <files> is a \n    # list of the files.\n    file_queue = dl.FileQueue()\n    file_queue.load_epochs(list(zip(files, labels)), max_epochs=float('inf'))\n\nNote that when you are done with the queue, you should call the queue's\n`join` method, which will make sure the queue is empty and the loader\nthread exits.\n\nThe ImageQueue\n~~~~~~~~~~~~~~\nAn ImageQueue_ to hold a set amount of images (not the entire batch, but enough\nto keep the main program happily fed). This class has a method we call for\nstarting image reader threads (again, you can choose how many of these you need\nto meet your main's demand). Following the above code, you add an image\nqueue like so:\n\n.. code:: python\n\n    img_queue = dl.ImgQueue(maxsize=1000)\n    img_queue.start_loaders(file_queue, num_threads=3, img_dir=IM_DIR)\n    # Wait for the image queue to fill up\n    sleep(2)\n    data, labels = img_queue.get_batch(batch_size=100)\n    ...\n    ...\n    img_queue.join()\n\nThe ImgQueue.start_loaders_ method will start `num_threads` threads, each of\nwhich read from the file_queue, load from disk and feed into the image queue.\n\nIf you want the loaders to pre-process images before putting them into the image\nqueue, you can provide a callable to ImgQueue.start_loaders_ to do this (see its\ndocstring for more info). For example:\n\n.. code:: python\n\n    img_queue = dl.ImgQueue()\n    def preprocess(x):\n        x = x.astype(np.float32)\n        x = x - np.mean(x)\n        x = x/max(1, np.std(x))\n        return x\n    img_queue.start_loaders(file_queue, num_threads=3, transform=preprocess)\n\nThe ImgQueue.get_batch_ method has two extra options (`block` and `timeout`),\ninstructing it how to handle cases when the image queue doesn't have a full\nbatch worth of images (should we return with whatever's there, or wait for the\nloaders to catch up?). See its docstring for more info.\n\nFor synchronization with epochs, the ImageQueue has an attribute `last_batch`\nthat will be set to true when an epoch's worth of images have been pulled from\nthe ImageQueue. \n\n.. code:: python\n\n    data, labels = img_queue.get_batch(batch_size=100)\n    last_batch = img_queue.last_batch\n    if last_batch:\n        # Print summary info...\n\nYou can monitor the queue size and fetch times for the ImgQueue too (to check\nwhether you need to tweak some settings). This works by printing out info to\na tensorboard summary file (currently only supported way of doing it). \nAll you need to do is create a `tf.summary.FileWriter` (you can use the same one\nthe rest of your main program is using), and call the ImgQueue.add_logging_\nmethod. This will add the data as a to your tensorboard file.\n\n.. code:: python\n\n    img_queue = dl.ImgQueue()\n    def preprocess(x):\n        x = x.astype(np.float32)\n        x = x - np.mean(x)\n        x = x/max(1, np.std(x))\n        return x\n    img_queue.start_loaders(file_queue, num_threads=3, transform=preprocess)\n    file_writer = tf.summary.FileWriter('./log', tf.get_default_graph())\n    # Write period is the sample period in numbers of batches for dumping data\n    img_queue.add_logging(file_writer, write_period=10)\n\nNote that when you are done with the queue, you should call the queue's\n`join` method, which will make sure the queue is empty and the loader\nthread exits.\n\nSmall Datasets\n~~~~~~~~~~~~~~\nIf you have a special case where the dataset is small, and so can fit into\nmemory (like CIFAR or MNIST), then you won't need the same complexity to get\nbatches of data and labels. However, it may still be beneficial to use the\nImgQueue class for two reasons:\n\n- Keeps the same programmatic interface regardless of the dataset\n- May still want to parallelize things if you want to do preprocessing of images\n  before putting them in the queue.\n\nFor this, use ImgQueue.take_dataset_ instead of ImgQueue.start_loaders_.\nThis method also has options like whether to shuffle the samples or not (will\nshuffle by default), and can take a callable function to apply to the images\nbefore putting them in the queue. The default number of threads to create is 1,\nbut this can be increased with the `num_threads` parameter.\n\nNote: **to avoid duplicating things in memory, the ImgQueue will not copy the\ndata/labels**. This means that once your main program calls the `take_dataset`\nmethod, it shouldn't modify the arrays.\n\nE.g.\n\n.. code:: python\n\n    import dataset_loading as dl\n    import dataset_loading.cifar as dlcifar\n    train_d, train_l, test_d, test_l, val_d, val_l = \\\n        dlcifar.load_cifar_data('/path/to/data')\n    img_queue = dl.ImgQueue()\n    img_queue.take_dataset(train_d, train_l)\n    data, labels = img_queue.get_batch(100)\n    # Or say we want to use more parallel threads and morph the image\n    def preprocess(x):\n        x = x.astype(np.float32)\n        x = x - np.mean(x)\n        x = x/max(1, np.std(x))\n        return x\n    img_queue = dl.ImgQueue()\n    img_queue.take_dataset(train_d, train_l, num_threads=3, \n                           transform=preprocess)\n    data, labels = img_queue.get_batch(100)\n\n\nInstallation\n------------\nDirect install from github (useful if you use pip freeze). To get the master\nbranch, try::\n\n    $ pip install -e git+https://github.com/fbcotter/dataset_loading#egg=dataset_loading\n\nor for a specific tag (e.g. 0.0.1), try::\n\n    $ pip install -e git+https://github.com/fbcotter/dataset_loading.git@0.0.1#egg=dataset_loading\n\nDownload and pip install from Git::\n\n    $ git clone https://github.com/fbcotter/dataset_loading\n    $ cd dataset_loading\n    $ pip install -r requirements.txt\n    $ pip install -e .\n\nIt is recommended to download and install (with the editable flag), as it is\nlikely you'll want to tweak things/add functions more quickly than we can handle\npull requests.\n\nFurther documentation\n---------------------\n\nThere is `more documentation`__\navailable online and you can build your own copy via the Sphinx documentation\nsystem::\n\n    $ python setup.py build_sphinx\n\nCompiled documentation may be found in ``build/docs/html/`` (index.html will be\nthe homepage)\n\n__ http://dataset-loading.readthedocs.io\n.. _FileQueue: http://dataset-loading.readthedocs.io/en/latest/filequeue.html#filequeue\n.. _ImageQueue: http://dataset-loading.readthedocs.io/en/latest/imagequeue.html#imagequeue\n.. _ImgQueue.get_batch: http://dataset-loading.readthedocs.io/en/latest/functions.html#dataset_loading.core.ImgQueue.get_batch\n.. _ImgQueue.start_loaders: http://dataset-loading.readthedocs.io/en/latest/functions.html#dataset_loading.core.ImgQueue.start_loaders\n.. _ImgQueue.take_dataset: http://dataset-loading.readthedocs.io/en/latest/functions.html#dataset_loading.core.ImgQueue.take_dataset\n.. _ImgQueue.add_logging: http://dataset-loading.readthedocs.io/en/latest/functions.html#dataset_loading.core.ImgQueue.add_logging\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/fbcotter/dataset_loading.git",
    "keywords": "image datasets,cifar,pascal,mnist,imagenet",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "finn-dataset-loading",
    "package_url": "https://pypi.org/project/finn-dataset-loading/",
    "platform": "",
    "project_url": "https://pypi.org/project/finn-dataset-loading/",
    "project_urls": {
      "Homepage": "https://github.com/fbcotter/dataset_loading.git"
    },
    "release_url": "https://pypi.org/project/finn-dataset-loading/0.0.5/",
    "requires_dist": [
      "Pillow",
      "scipy"
    ],
    "requires_python": "",
    "summary": "Loading image datasets using queues.",
    "version": "0.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16834547,
  "releases": {
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "10f856e186df7c01770d63bc509d8c759e47447fa6ebc27323f555ea110f92f2",
          "md5": "bcfa8c1130b52d6f83ecb2ee4884843e",
          "sha256": "c346f12dd41628e7b1b314aa6ac8b428f5984d9387c37f05a2070ac65684f9bd"
        },
        "downloads": -1,
        "filename": "finn_dataset_loading-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bcfa8c1130b52d6f83ecb2ee4884843e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 29294,
        "upload_time": "2020-12-17T21:25:12",
        "upload_time_iso_8601": "2020-12-17T21:25:12.653640Z",
        "url": "https://files.pythonhosted.org/packages/10/f8/56e186df7c01770d63bc509d8c759e47447fa6ebc27323f555ea110f92f2/finn_dataset_loading-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ea74565cd9906c40e765020dcb109642c71fa12cd597eb7df2510128c502ed56",
          "md5": "8e4b80e30d4f91393666b1d376593499",
          "sha256": "b6c804abd3ce631d6b39475c160a571a2e083cdcdb0d0ad976deb87a09c407ee"
        },
        "downloads": -1,
        "filename": "finn_dataset_loading-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "8e4b80e30d4f91393666b1d376593499",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23056,
        "upload_time": "2020-12-17T21:25:14",
        "upload_time_iso_8601": "2020-12-17T21:25:14.198782Z",
        "url": "https://files.pythonhosted.org/packages/ea/74/565cd9906c40e765020dcb109642c71fa12cd597eb7df2510128c502ed56/finn_dataset_loading-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f51054299b440e77a4165e7ef094904ffa840f4aa52a64f3cf3c24c6de010f93",
          "md5": "fae45f4ac612c178074eb8422135a7e1",
          "sha256": "6d8d7cf0fc1abb7319f43fae1816a4c0ffb289b222e2b77c89fd5b6bac0de390"
        },
        "downloads": -1,
        "filename": "finn_dataset_loading-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fae45f4ac612c178074eb8422135a7e1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 29338,
        "upload_time": "2021-06-01T17:35:10",
        "upload_time_iso_8601": "2021-06-01T17:35:10.761710Z",
        "url": "https://files.pythonhosted.org/packages/f5/10/54299b440e77a4165e7ef094904ffa840f4aa52a64f3cf3c24c6de010f93/finn_dataset_loading-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2c261680849f0146ea9c273a1d0bf47d5a23980dd9877469afd7bd12971ba08d",
          "md5": "6ed86653e63406df6ef24cc7e399078e",
          "sha256": "3eb50c67a171bb060085f61e5724918a6cbe42172139744f48abd46fd4246457"
        },
        "downloads": -1,
        "filename": "finn_dataset_loading-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "6ed86653e63406df6ef24cc7e399078e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 29796,
        "upload_time": "2021-06-01T17:35:11",
        "upload_time_iso_8601": "2021-06-01T17:35:11.984997Z",
        "url": "https://files.pythonhosted.org/packages/2c/26/1680849f0146ea9c273a1d0bf47d5a23980dd9877469afd7bd12971ba08d/finn_dataset_loading-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f51054299b440e77a4165e7ef094904ffa840f4aa52a64f3cf3c24c6de010f93",
        "md5": "fae45f4ac612c178074eb8422135a7e1",
        "sha256": "6d8d7cf0fc1abb7319f43fae1816a4c0ffb289b222e2b77c89fd5b6bac0de390"
      },
      "downloads": -1,
      "filename": "finn_dataset_loading-0.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fae45f4ac612c178074eb8422135a7e1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 29338,
      "upload_time": "2021-06-01T17:35:10",
      "upload_time_iso_8601": "2021-06-01T17:35:10.761710Z",
      "url": "https://files.pythonhosted.org/packages/f5/10/54299b440e77a4165e7ef094904ffa840f4aa52a64f3cf3c24c6de010f93/finn_dataset_loading-0.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2c261680849f0146ea9c273a1d0bf47d5a23980dd9877469afd7bd12971ba08d",
        "md5": "6ed86653e63406df6ef24cc7e399078e",
        "sha256": "3eb50c67a171bb060085f61e5724918a6cbe42172139744f48abd46fd4246457"
      },
      "downloads": -1,
      "filename": "finn_dataset_loading-0.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "6ed86653e63406df6ef24cc7e399078e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 29796,
      "upload_time": "2021-06-01T17:35:11",
      "upload_time_iso_8601": "2021-06-01T17:35:11.984997Z",
      "url": "https://files.pythonhosted.org/packages/2c/26/1680849f0146ea9c273a1d0bf47d5a23980dd9877469afd7bd12971ba08d/finn_dataset_loading-0.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}