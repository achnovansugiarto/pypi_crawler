{
  "info": {
    "author": "Joeran Bosma, Anindo Saha and Matin Hosseinzadeh",
    "author_email": "Joeran.Bosma@radboudumc.nl",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# Evaluation Utilities for 3D Detection and Diagnosis in Medical Imaging\n![Tests](https://github.com/DIAGNijmegen/picai_eval/actions/workflows/tests.yml/badge.svg)\n\nThis repository contains standardized functions to evaluate 3D detection and diagnosis performance in medical imaging —with its evaluation strategy being geared towards clinically significant prostate cancer (csPCa) detection in MRI. It is used for the official evaluation pipeline of the [PI-CAI challenge](https://pi-cai.grand-challenge.org/).\n\n## Supported Evaluation Metrics\n- **Average Precision (AP)**\n- **Area Under the Receiver Operating Characteristic curve (AUROC)**\n- **Overall AI Ranking Metric of the [PI-CAI challenge](https://pi-cai.grand-challenge.org/): `(AUROC + AP) / 2`**\n- **Precision-Recall (PR) curve**\n- **Receiver Operating Characteristic (ROC) curve**\n- **Free-Response Receiver Operating Characteristic (FROC) curve**\n\n## Additional Supported Functionalities\n- **Subset Analysis**: By providing a list of case identifiers, performance can be evaluated for only that specific subset.\n- **Case-Wise Sample Weighting**: Sample weighting can help facilitate [inverse probability weighting](https://www.bmj.com/content/352/bmj.i189). Note, when this feature is used in conjunction with lesion-level evaluation, the same weight is applied to all lesion candidates of the same case. Lesion-wise sample weighting is currently not supported.\n- **Statistical Tests**: Permutation tests and bootstrapping techniques to facilitate AI vs AI/radiologists comparisons in the [PI-CAI challenge](https://pi-cai.grand-challenge.org/).\n\n## Installation\n`picai_eval` is pip-installable:\n\n`pip install picai_eval`\n\n## Evaluation Pipeline\n![Detection pipeline overview](detection-pipeline.png)\n_Figure: Detection/diagnosis evaluation pipeline of the [PI-CAI challenge](https://pi-cai.grand-challenge.org/). (top) Lesion-level csPCa detection (modeled by 'AI'): For a given patient case, using the bpMRI exam, predict a 3D detection map of non-overlapping, non-connected csPCa lesions (with the same dimensions and resolution as the T2W image). For each predicted lesion, all voxels must comprise a single floating point value between 0-1, representing that lesion’s likelihood of harboring csPCa. (bottom) Patient-level csPCa diagnosis (modeled by 'f(x)'): For a given patient case, using the predicted csPCa lesion detection map, compute a single floating point value between 0-1, representing that patient’s overall likelihood of harboring csPCa. For instance, f(x) can simply be a function that takes the maximum of the csPCa lesion detection map, or it can be a more complex heuristic (defined by the AI developer)._\n\n\n## Usage\n\n### Expected Predictions and Annotations\nOur evaluation pipeline expects **detection maps** and **annotations** in the following format:\n- **Detection Maps**: 3D volumes with non-connected, non-overlapping lesion detections. Each lesion detection is a connected component (in 3D) with the same _confidence or likelihood score_ (floating point) per voxel. Each detection map may contain an arbitrary number of such lesion detections.\n\n- **Annotations**: 3D volumes of the same shape as their corresponding detection maps, with non-connected, non-overlapping ground-truth lesions. Each ground-truth lesion is a connected component (in 3D) with the integer value 1 per voxel. Background voxels are represented by the integer value 0.\n\nNote, we define a _connected component_ as all non-zero voxels with _squared connectivity_ equal to three. This means that in a 3×3×3 neighbourhood all voxels are connected to the centre voxel. See [26-Connectivity](https://en.wikipedia.org/wiki/Pixel_connectivity) for an illustration.\n\n#\n\n### Evaluate Detection Maps with Python\nTo run evaluation scripts from Python, import the `evaluate` function and provide detection maps (`y_det`) and annotations (`y_true`):\n\n```python\nfrom picai_eval import evaluate\n\nsubject_list = [\n    \"case-0\",\n    \"case-1\",\n    \"case-2\",\n]\n\nmetrics = evaluate(\n    y_det=y_det,\n    y_true=y_true,\n    subject_list=subject_list,  # optional\n)\n```\n\n- `y_det`: Iterable of all detection maps to evaluate. Each detection map is a 3D volume with non-connected, non-overlapping lesion detections. Each lesion detection is a connected component (in 3D) with the same _confidence or likelihood score_ per voxel. Each detection map may contain an arbitrary number of such lesion detections. Alternatively, `y_det` may contain filenames of detection maps ending in `.nii.gz`/`.mha`/`.mhd`/`.npy`/`.npz`, which will be loaded on-the-fly.\n\n- `y_true`: Iterable of all ground-truth annotations. Each annotation should be a 3D volume of the same shape as its corresponding detection map, with non-connected, non-overlapping ground-truth lesions. `1` is used to encode ground-truth lesions, and `0` is to encode the background. Alternatively, `y_true` may contain filenames of binary annotations ending in `.nii.gz`/`.mha`/`.mhd`/`.npy`/`.npz`, which will be loaded on-the-fly. \n\nDefault parameters will perform evaluation as per the specifications of the [PI-CAI challenge](https://pi-cai.grand-challenge.org/). Optionally, the specifications for evaluation can be adapted using the following parameters:\n\n- `sample_weight`: Case-level sample weight. When this feature is used in conjunction with lesion-level evaluation, the same weight is applied to all lesion candidates of the same case. Default: equal weight for all cases.\n\n- `subject_list`: List of sample identifiers, to give recognizable names to the evaluation results.\n\n- `min_overlap`: Defines the threshold of the hit criterion, i.e. the minimal required Intersection over Union (IoU) or Dice similarity coefficient (DSC) between predicted lesion candidates and ground-truth lesions, for predicted lesions to be counted as true positive detections. Default: 0.1.\n\n- `overlap_func`: Function used to calculate the basis of the hit criterion, i.e. the object overlap between predicted lesion candidates and ground-truth lesions. This can be set as 'IoU' to use Intersection over Union, or 'DSC' to use Dice similarity coefficient. Alternatively, any other function can also be provided with the signature `func(detection_map, annotation) -> overlap [0, 1]`. Default: 'IoU'.\n\n- `case_confidence_func`: Function used to derive case-level prediction or confidence, from lesion-level detections or confidences (as denoted by 'f(x)' in ['Evaluation Pipeline'](#evaluation-pipeline). Default: 'max' (which simply takes the maximum of the detection map, as the case-level prediction).\n\n- `multiple_lesion_candidates_selection_criteria`: Used to account for [split-merge scenarios](https://www.nature.com/articles/s41598-020-64803-w/figures/1). When multiple lesion candidates have sufficient overlap with the ground-truth lesion, this condition determines which lesion candidate is selected as the true positive, and which lesion candidates are discarded or counted as false positives. Default: 'overlap' (which selects the lesion candidate with the highest degree of overlap).\n\n- `allow_unmatched_candidates_with_minimal_overlap`: Used to account for [split-merge scenarios](https://www.nature.com/articles/s41598-020-64803-w/figures/1). When multiple lesion candidates have sufficient overlap with the ground-truth lesion, this condition determines whether non-selected lesion candidates are discarded or count as false positives. Default: True (i.e. non-selected lesion candidates are not counted as false positives).\n\n- `num_parallel_calls`: Number of CPU threads used to process evaluation. Default: 3.\n\n#\n\n### Evaluate all Detection Maps stored in a specific folder\nTo evaluate numerous detection maps stored on disk, prepare input folders in the following format:\n\n```\npath/to/detection_maps/\n├── [case-0]_detection_map.nii.gz\n├── [case-1]_detection_map.nii.gz\n├── [case-2]_detection_map.nii.gz\n...\n\npath/to/annotations/\n├── [case-0]_label.nii.gz\n├── [case-1]_label.nii.gz\n├── [case-2]_label.nii.gz\n```\n\nSee [here](https://github.com/DIAGNijmegen/picai_eval/tree/public-release-prep/tests/test-maps) for an example. If the folders containing all detection maps and annotations are different, then the `_detection_map` and `_label` suffixes are optional. Allowed file extensions are: `.npz` (as used in the [nnU-Net](https://github.com/MIC-DKFZ/nnUNet) framework), `.npy`, `.nii.gz`, `.nii`, `.mha` and `.mhd`. First file matching one of these extensions (in the order stated in the previous sentence) is selected.\n\n**Using Python:**  \nEvaluates all cases specified in `subject_list`. Function `evaluate_folder` also accepts all parameters described [above](#evaluate-individual-detection-maps-with-python).\n\n```python\nfrom picai_eval import evaluate_folder\n\nsubject_list = [\n    \"case-0\",\n    \"case-1\",\n    \"case-2\",\n]\n\nmetrics = evaluate_folder(\n    y_det_dir=\"path/to/detection_maps\",\n    y_true_dir=\"path/to/annotations\",\n    subject_list=subject_list,          # optional\n)\n```\n\n**Using the command line:**  \nEvaluates all cases found in `path/to/detection_maps` against the annotations in `path/to/annotations`, and store the metrics in `path/to/detection_maps/metrics.json`. Optionally, the `--labels` parameter may be omitted, which will then default to the `--input` folder. To specify the output location of the metrics, use `--output /path/to/metrics.json`.\n\n```bash\npython -m picai_eval --input path/to/detection_maps --labels path/to/annotations\n```\n\n#\n\n### Evaluate Softmax Volumes (instead of Detection Maps)\nTo evaluate softmax predictions (instead of detection maps), a function to extract lesion candidates from the softmax volume must be provided. For instance, the dynamic lesion extraction method from the [`report_guided_annotation`](https://github.com/DIAGNijmegen/Report-Guided-Annotation) module can be used for this (see [mechanism](https://github.com/DIAGNijmegen/Report-Guided-Annotation#mechanism) for a depiction of its working principle).\n\n**Evaluating Softmax Volumes using Python:**\n```python\nfrom picai_eval import evaluate\nfrom report_guided_annotation import extract_lesion_candidates\n\nmetrics = evaluate(\n    y_det=y_pred,\n    y_true=y_true,\n    subject_list=subject_list,  # may be omitted\n    y_det_postprocess_func=lambda pred: extract_lesion_candidates(pred)[0],\n)\n```\n\nFor the structure of the inputs and additional parameters, see [Evaluate Detection Maps with Python](#evaluate-detection-maps-with-python).\n\n**Evaluating all Softmax Volumes stored in a specific folder:**\n```python\nfrom picai_eval import evaluate_folder\nfrom report_guided_annotation import extract_lesion_candidates\n\nmetrics = evaluate_folder(\n    y_det_dir=in_dir_softmax,\n    y_true_dir=in_dir_annot,\n    y_det_postprocess_func=lambda pred: extract_lesion_candidates(pred)[0],\n)\n```\n\n#\n\n### Accessing Metrics after Evaluation\nTo access metrics after evaluation, we recommend using the `Metrics` class:\n\n```python\nmetrics = ...  # from evaluate, evaluate_folder, or Metrics(\"/path/to/metrics.json\")\n\n# aggregate metrics\nAP = metrics.AP\nauroc = metrics.auroc\npicai_score = metrics.score\n\n# Precision-Recall (PR) curve\nprecision = metrics.precision\nrecall = metrics.recall\n\n# Receiver Operating Characteristic (ROC) curve\ntpr = metrics.case_TPR\nfpr = metrics.case_FPR\n\n# Free-Response Receiver Operating Characteristic (FROC) curve\nsensitivity = metrics.lesion_TPR\nfp_per_case = metrics.lesion_FPR\n```\n\nFor example, these can be used to plot performance curves:\n\n```python\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay\n\n# plot recision-Recall (PR) curve\ndisp = PrecisionRecallDisplay(precision=precision, recall=recall, average_precision=AP)\ndisp.plot()\nplt.show()\n\n# plot Receiver Operating Characteristic (ROC) curve\ndisp = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auroc)\ndisp.plot()\nplt.show()\n\n# plot Free-Response Receiver Operating Characteristic (FROC) curve\nf, ax = plt.subplots()\ndisp = RocCurveDisplay(fpr=fp_per_case, tpr=sensitivity)\ndisp.plot(ax=ax)\nax.set_xlim(0.001, 5.0); ax.set_xscale('log')\nax.set_xlabel(\"False positives per case\"); ax.set_ylabel(\"Sensitivity\")\nplt.show()\n```\n\nTo perform subset analysis, a list of _subject IDs_ can be provided. To view all available subject IDs, run `print(metrics.subject_list)`.\n\n```python\nsubject_list = [..., ...]  # list of case identifiers\nmetrics = Metrics(\"path/to/metrics.json\", subject_list=subject_list)\nprint(metrics)  # prints performance for specified subset\n```\n\nOr, with existing metrics:\n\n```python\nmetrics = ...  # from evaluate, evaluate_folder, or Metrics(\"/path/to/metrics.json\")\nmetrics.subject_list = subject_list\nprint(metrics)  # prints performance for specified subset\n```\n\nAll performance metrics for a subset, can be accessed in the same manner as for the full set.\n\n#\n\n### Storing and Reading Metrics\nMetrics can be easily saved and loaded to/from disk, to facilitate evaluation of multiple models, and subsequent (statistical) analyses. To read metrics, simply provide the path to the saved `.json` file:\n\n```python\nfrom picai_eval import Metrics\n\nmetrics = Metrics(\"path/to/metrics.json\")\n```\n\nTo save metrics, provide the path to save a corresponding `.json` file:\n\n```python\nmetrics.save(\"path/to/metrics.json\")\n# metrics.save_full(\"path/to/metrics.json\")     # also store derived curves\n# metrics.save_minimal(\"path/to/metrics.json\")  # only store minimal information to reload Metrics instance\n```\n\nCommand line interface described in ['Evaluate All Detection Maps stored in a Specific Folder'](#evaluate-all-detection-maps-stored-in-a-specific-folder)) will automatically save metrics to disk. Its output path can be controlled with the `--output` parameter.\n\n<br>\n\n## Statistical Tests\nThe [PI-CAI challenge](https://pi-cai.grand-challenge.org/) features 'AI vs AI', 'AI vs Radiologists from Clinical Routine' and 'AI vs Radiologists from Reader Study' comparisons. Each of these comparisons come with a statistical test. For 'AI vs AI', a permuations test with the overall ranking metric is performend. Readers cannot be assigned a ranking metric without introducing bias, so for 'AI vs Radiologists from Reader Study' and 'AI vs Radiologists from Clinical Routine', we compare performance at matched operating points. See each section below for more details.\n\nFor the following tests, we assume that each AI algorithm is trained on the same training dataset and evaluated on the same testing dataset, multiple times (5-10x), and all of these independently trained instances are used in each statistical test. By doing so, we account for the performance variance resulting from the stochastic optimization of machine/deep learning models (due to which, the same AI architecture, trained on the same data, for the same number of training steps, typically can exhibit different performance each time). Our goal is to avoid basing any conclusions off of one arbitrary training run (which may prove “lucky” or “unlucky” for a given AI algorithm), and to promote reproducibility. Thus, we statistically evaluate the overall AI algorithm, and not just a single trained instance of that algorithm.\n\n**Note**: Extended tests to verify whether a given statistical test is well-calibrated (i.e. it does not over-/under-estimate the p-value), will be incorporated in the future.\n\n#\n\n### AI vs AI\n**Comparison**: Between a given pair of AI algorithms, with multiple independently trained instances per AI algorithm.\n\n**Statistical Question**: What is the probability that one AI algorithm outperforms another, while accounting for the performance variance stemming from each AI algorithm’s training method?\n\n**Statistical Test**: Permutation tests (as applied in [Ruamviboonsuk et al., 2022](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00017-6/fulltext), [McKinney et al., 2020](https://www.nature.com/articles/s41586-019-1799-6) and [Bulten et al., 2022](https://www.nature.com/articles/s41591-021-01620-2)). In each replication, performance metrics (ranking score, AP or AUROC) are shuffled across methods (different AI algorithms) and their instances (independently trained samples of each method).\n\nPermutation test can be used as follows:\n```python\nfrom picai_eval.statistical_helper import perform_permutation_test\n\nscores_algorithm_a = [0.96, 0.91, 0.90, 0.85, 0.81, 0.80]\nscores_algorithm_b = [0.92, 0.94, 0.95, 0.81, 0.82, 0.86]\n\n# perform permutation tests\np = perform_permutation_test(\n    scores_alternative=scores_algorithm_a,\n    scores_baseline=scores_algorithm_b,\n)\n\n# p-value should be 0.7218614718614719\n```\n\nThis will calculate the p-value for the null hypothesis _Performance(baseline algorithm) > Performance(alternative algorithm)_ (given the provided or observed performance metrics). Note, the scores shown above (0.92, 0.94, etc.) are **performance metrics** (e.g. AUROC, AP), not model predictions (i.e. likelihood score predicted per case). While using individual predictions provides more, correlated samples (numerous individual predictions are associated with the same, single trained instance of an AI model) for the permutation test, using overall performance metrics provides relatively fewer, but independent samples (only a single overall performance metric is associated with the same, single trained instance of an AI model). Hence, we opt for the latter and use multiple training runs to facilitate the same. Performance metrics can be obtained from the evaluation pipeline, as follows:\n\n```python\nfrom picai_eval import Metrics\n\nscores_algorithm = [\n    Metrics(path).score\n    for path in [\n        \"/path/to/algorithm/metrics-restart-1.json\",\n        \"/path/to/algorithm/metrics-restart-2.json\",\n        \"/path/to/algorithm/metrics-restart-3.json\",\n        ...\n    ]\n]\n```\n#\n\n### AI vs Radiologists from Clinical Routine\n**Comparison**: Between multiple independently trained instances of a given AI algorithm, and the historical reads made by radiologists during clinical routine.\n\n**Statistical Question**: What is the probability that a given trained AI algorithm outperforms radiologists from clinical routine, while accounting for the performance variance stemming from different cases and the AI algorithm’s training method? \n\n**Statistical Test**: Paired bootstrapping (as applied in [Ruamviboonsuk et al., 2022](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00017-6/fulltext), [McKinney et al., 2020](https://www.nature.com/articles/s41586-019-1799-6), [Rodriguez-Ruiz et al., 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6748773/)), using predictions from a given operating point. Here, the operating point is that of radiologists ([PI-RADS](https://www.europeanurology.com/article/S0302-2838(19)30180-0/fulltext) ≥ 3 or [PI-RADS](https://www.europeanurology.com/article/S0302-2838(19)30180-0/fulltext) ≥ 4) from clinical routine. Trained AI algorithms are thresholded to match the radiologist's sensitivity/specificity (for patient diagnosis) or recall/precision (for lesion detection). In each of 1M replications, ∼U(0,N) cases are sampled with replacement, and used to calculate the _test statistic_. Iterations that sample only one class are rejected. Here, the test statistic is the rank of historical reads made by radiologists, with respect to the predictions made by trained AI algorithms, where the rank is determined by the conjugate performance metric.\n\n**Note**: In contrast to the [permutation test](#ai-vs-ai), bootstrapping approximates the statistical question. As a result, the p-value from bootstrapping can be miscalibrated (i.e. giving p-values that are higher or lower than they should be). The permutation test does not have this issue, but cannot be applied in this scenario, because we have only a single radiologist prediction per case.\n\nMatched bootstrapping test can be used as follows:\n```python\nimport numpy as np\nfrom picai_eval.statistical_helper import perform_matched_boostrapping\n\n# predictions: 3 restarts (rows) of 4 cases (columns)\ny_pred_ai = [\n    [0.92, 0.23, 0.12, 0.95],\n    [0.42, 0.81, 0.13, 0.86],\n    [0.26, 0.15, 0.14, 0.67]\n]\ny_pred_reader = np.array([5, 4, 2, 3]) >= 3\ny_true = [1, 1, 0, 0]\n\n# perform matched bootstrapping\np = perform_matched_boostrapping(\n    y_true=y_true,\n    y_pred_ai=y_pred_ai,\n    y_pred_reader=y_pred_reader,\n    match='sensitivity',\n    iterations=int(1e4),\n)\n\n# Probability for Performance(AI) > Performance(Reader): p = 0.3 (approximately)\n```\n\nNote, the numbers shown above (0.92, 0.23, ..., 0.95 and 5, 4, ..., 2 etc.) are **predictions** (i.e. likelihood score predicted per case), not performance metrics (e.g. AUROC, AP). All radiologist predictions must be binarized (e.g. thresholded at [PI-RADS](https://www.europeanurology.com/article/S0302-2838(19)30180-0/fulltext) ≥ 3 or [PI-RADS](https://www.europeanurology.com/article/S0302-2838(19)30180-0/fulltext) ≥ 4), while all predictions for the algorithm must be likelihood scores between 0 and 1 inclusive. Predictions be obtained from the evaluation pipeline, as follows:\n```python\nfrom picai_eval import Metrics\n\ny_pred_ai = [\n    Metrics(path).case_pred\n    for path in [\n        \"/path/to/algorithm/metrics-restart-1.json\",\n        \"/path/to/algorithm/metrics-restart-2.json\",\n        \"/path/to/algorithm/metrics-restart-3.json\",\n        ...\n    ]\n]\n```\n# \n\n### AI vs Radiologists from Reader Study\n**Comparison**: Between multiple independently trained instances of a given AI algorithm, and a given panel of radiologists or readers.\n\n**Statistical Question**: What is the probability that a given AI algorithm outperforms the typical reader from a given panel of radiologists, while accounting for the performance variance stemming from different readers, and the AI algorithm’s training method?\n\n**Statistical Test**: Permutation test (as applied in [Ruamviboonsuk et al., 2022](https://www.thelancet.com/journals/landig/article/PIIS2589-7500(22)00017-6/fulltext), [McKinney et al., 2020](https://www.nature.com/articles/s41586-019-1799-6) and [Bulten et al., 2022](https://www.nature.com/articles/s41591-021-01620-2)). Permutation tests are used to statistically compare lesion-level detection and patient-level diagnosis performance at PI-RADS operating points. Here, in each of the replications, performance metrics (reader performance w.r.t. AI performance at reader’s operating point) are shuffled across methods (AI, radiologists) and their instances (independently trained samples of AI algorithm, different readers).\n\n```python\nimport numpy as np\nfrom picai_eval.statistical_helper import perform_matched_permutation_test\n\n# predictions: 3 restarts (rows) of 4 cases (columns)\ny_pred_ai = [\n    [0.92, 0.23, 0.12, 0.95],\n    [0.82, 0.81, 0.13, 0.42],\n    [0.26, 0.90, 0.14, 0.67]\n]\ny_pred_readers = np.array([\n    [5, 4, 2, 2],\n    [4, 5, 1, 2],\n    [5, 2, 3, 2]\n]) >= 3\ny_true = [1, 1, 0, 0]\n\np = perform_matched_permutation_test(\n    y_true=y_true,\n    y_pred_ai=y_pred_ai,\n    y_pred_readers=y_pred_readers,\n    match=\"sensitivity\",\n    iterations=int(1e4),\n)\n\n# Probability for Performance(Panel of readers) > Performance(AI): p = 0.8 (approximately)\n```\n\nNote, the numbers shown above (0.92, 0.23, ..., 0.95 and 5, 4, ..., 2 etc.) are **predictions** (i.e. likelihood score predicted per case), not performance metrics (e.g. AUROC, AP). All radiologist predictions must be binarized (e.g. thresholded at [PI-RADS](https://www.europeanurology.com/article/S0302-2838(19)30180-0/fulltext) ≥ 3 or [PI-RADS](https://www.europeanurology.com/article/S0302-2838(19)30180-0/fulltext) ≥ 4), while all predictions for the algorithm must be likelihood scores between 0 and 1 inclusive. Predictions be obtained from the evaluation pipeline, as follows:\n\n```python\nfrom picai_eval import Metrics\n\ny_pred_ai = [\n    Metrics(path).case_pred\n    for path in [\n        \"/path/to/algorithm/metrics-restart-1.json\",\n        \"/path/to/algorithm/metrics-restart-2.json\",\n        \"/path/to/algorithm/metrics-restart-3.json\",\n        ...\n    ]\n]\n```\n\n## Reference\nIf you are using this codebase or some part of it, please cite the following article:\n\n● [A. Saha, J. J. Twilt, J. S. Bosma, B. van Ginneken, D. Yakar, M. Elschot, J. Veltman, J. J. Fütterer, M. de Rooij, H. Huisman, \"Artificial Intelligence and Radiologists at Prostate Cancer Detection in MRI: The PI-CAI Challenge (Study Protocol)\", DOI: 10.5281/zenodo.6667655](https://zenodo.org/record/6667655)\n\n**BibTeX:**\n```\n@ARTICLE{PICAI_BIAS,\n    author = {Anindo Saha, Jasper J. Twilt, Joeran S. Bosma, Bram van Ginneken, Derya Yakar, Mattijs Elschot, Jeroen Veltman, Jurgen Fütterer, Maarten de Rooij, Henkjan Huisman},\n    title  = {{Artificial Intelligence and Radiologists at Prostate Cancer Detection in MRI: The PI-CAI Challenge (Study Protocol)}}, \n    year   = {2022},\n    doi    = {10.5281/zenodo.6667655}\n}\n```\n\n## Managed By\nDiagnostic Image Analysis Group,\nRadboud University Medical Center,\nNijmegen, The Netherlands\n\n## Contact Information\n- Joeran Bosma: Joeran.Bosma@radboudumc.nl\n- Anindo Saha: Anindya.Shaha@radboudumc.nl\n- Henkjan Huisman: Henkjan.Huisman@radboudumc.nl\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/DIAGNijmegen/picai_eval",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "picai-eval",
    "package_url": "https://pypi.org/project/picai-eval/",
    "platform": "unix",
    "project_url": "https://pypi.org/project/picai-eval/",
    "project_urls": {
      "Bug Tracker": "https://github.com/DIAGNijmegen/picai_eval/issues",
      "Homepage": "https://github.com/DIAGNijmegen/picai_eval"
    },
    "release_url": "https://pypi.org/project/picai-eval/1.4.4/",
    "requires_dist": [
      "numpy",
      "tqdm",
      "SimpleITK",
      "scipy",
      "scikit-learn (>=0.20.3)",
      "mlxtend",
      "report-guided-annotation (>=0.2.4)",
      "pytest (>=6.0) ; extra == 'testing'",
      "pytest-cov (>=2.0) ; extra == 'testing'",
      "mypy (>=0.910) ; extra == 'testing'",
      "flake8 (>=3.9) ; extra == 'testing'",
      "tox (>=3.24) ; extra == 'testing'"
    ],
    "requires_python": ">=3.6",
    "summary": "PICAI Evaluation",
    "version": "1.4.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15713702,
  "releases": {
    "1.4.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "84c41b84a0b2a85d5e9fb1622ec675e5833e93fc04f630fa13aea2a11855df9b",
          "md5": "15492e9d6971525b75319bcf797f5ee1",
          "sha256": "f6f90788075464f3d16891f7f924272dfcb04b2b4208e523c788915e5846991c"
        },
        "downloads": -1,
        "filename": "picai_eval-1.4.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "15492e9d6971525b75319bcf797f5ee1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 40386,
        "upload_time": "2022-07-02T18:58:42",
        "upload_time_iso_8601": "2022-07-02T18:58:42.539839Z",
        "url": "https://files.pythonhosted.org/packages/84/c4/1b84a0b2a85d5e9fb1622ec675e5833e93fc04f630fa13aea2a11855df9b/picai_eval-1.4.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "368099cb35d8a8bbfa697a38e748fa485696a336a37f594bb50a72d40526195e",
          "md5": "1845b5e1d2b6b8224aa22e58e6323769",
          "sha256": "399ab509a6c749a27e18549babb1a8bba8f34b1a072580a603bc55b18ba66d00"
        },
        "downloads": -1,
        "filename": "picai_eval-1.4.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1845b5e1d2b6b8224aa22e58e6323769",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 42837,
        "upload_time": "2022-07-02T18:58:45",
        "upload_time_iso_8601": "2022-07-02T18:58:45.879957Z",
        "url": "https://files.pythonhosted.org/packages/36/80/99cb35d8a8bbfa697a38e748fa485696a336a37f594bb50a72d40526195e/picai_eval-1.4.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.4.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c3ada560aca61e06d4dee3f00c9bcdea163c88824f84e408e956402ab6e25d0e",
          "md5": "557ccde5188d0d30f33b27b98ea6d84d",
          "sha256": "a716f8949e19f1a960e6c9182f760c4b0f0fa30ff3e3f409e0282f3f3dc797f6"
        },
        "downloads": -1,
        "filename": "picai_eval-1.4.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "557ccde5188d0d30f33b27b98ea6d84d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 40418,
        "upload_time": "2022-09-28T09:45:27",
        "upload_time_iso_8601": "2022-09-28T09:45:27.756643Z",
        "url": "https://files.pythonhosted.org/packages/c3/ad/a560aca61e06d4dee3f00c9bcdea163c88824f84e408e956402ab6e25d0e/picai_eval-1.4.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "511b042bce51c655c881b236721f6de06bba12c4338be025809388f1d8bc06d6",
          "md5": "146b7bd6dbd571ab131a48d0ea8a09f5",
          "sha256": "0c29e385ce06e381af57e264d373105792e9d96af95e6f295c5026f3a72828d9"
        },
        "downloads": -1,
        "filename": "picai_eval-1.4.3.tar.gz",
        "has_sig": false,
        "md5_digest": "146b7bd6dbd571ab131a48d0ea8a09f5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 42973,
        "upload_time": "2022-09-28T09:45:29",
        "upload_time_iso_8601": "2022-09-28T09:45:29.648047Z",
        "url": "https://files.pythonhosted.org/packages/51/1b/042bce51c655c881b236721f6de06bba12c4338be025809388f1d8bc06d6/picai_eval-1.4.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.4.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "063197068b7ed6710dc0841d6427bd193ce4f73afc22e57c1bca314139d7337c",
          "md5": "38853c2782771f27640a11ed6e74f4dc",
          "sha256": "9e7eab0301b6a4b653daca4ae9c30eae536997cd8d22739f82c65cb3027da3ba"
        },
        "downloads": -1,
        "filename": "picai_eval-1.4.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "38853c2782771f27640a11ed6e74f4dc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 40793,
        "upload_time": "2022-11-09T15:50:23",
        "upload_time_iso_8601": "2022-11-09T15:50:23.958195Z",
        "url": "https://files.pythonhosted.org/packages/06/31/97068b7ed6710dc0841d6427bd193ce4f73afc22e57c1bca314139d7337c/picai_eval-1.4.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fa6481ce42c6ab84e13326023d6b3bc1ebef6f16e1a21699247cad1a73080820",
          "md5": "4bd739483934a8061ce3eec103214e77",
          "sha256": "803862fa0e86b49bb074f5f98ff139f66a45ff5110ce997bd565dc59afaa9713"
        },
        "downloads": -1,
        "filename": "picai_eval-1.4.4.tar.gz",
        "has_sig": false,
        "md5_digest": "4bd739483934a8061ce3eec103214e77",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 43363,
        "upload_time": "2022-11-09T15:50:26",
        "upload_time_iso_8601": "2022-11-09T15:50:26.198992Z",
        "url": "https://files.pythonhosted.org/packages/fa/64/81ce42c6ab84e13326023d6b3bc1ebef6f16e1a21699247cad1a73080820/picai_eval-1.4.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "063197068b7ed6710dc0841d6427bd193ce4f73afc22e57c1bca314139d7337c",
        "md5": "38853c2782771f27640a11ed6e74f4dc",
        "sha256": "9e7eab0301b6a4b653daca4ae9c30eae536997cd8d22739f82c65cb3027da3ba"
      },
      "downloads": -1,
      "filename": "picai_eval-1.4.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "38853c2782771f27640a11ed6e74f4dc",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 40793,
      "upload_time": "2022-11-09T15:50:23",
      "upload_time_iso_8601": "2022-11-09T15:50:23.958195Z",
      "url": "https://files.pythonhosted.org/packages/06/31/97068b7ed6710dc0841d6427bd193ce4f73afc22e57c1bca314139d7337c/picai_eval-1.4.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fa6481ce42c6ab84e13326023d6b3bc1ebef6f16e1a21699247cad1a73080820",
        "md5": "4bd739483934a8061ce3eec103214e77",
        "sha256": "803862fa0e86b49bb074f5f98ff139f66a45ff5110ce997bd565dc59afaa9713"
      },
      "downloads": -1,
      "filename": "picai_eval-1.4.4.tar.gz",
      "has_sig": false,
      "md5_digest": "4bd739483934a8061ce3eec103214e77",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 43363,
      "upload_time": "2022-11-09T15:50:26",
      "upload_time_iso_8601": "2022-11-09T15:50:26.198992Z",
      "url": "https://files.pythonhosted.org/packages/fa/64/81ce42c6ab84e13326023d6b3bc1ebef6f16e1a21699247cad1a73080820/picai_eval-1.4.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}