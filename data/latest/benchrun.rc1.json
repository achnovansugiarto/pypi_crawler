{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# Python {benchrun}\n\n{benchrun} is a Python package to run macrobenchmarks, deliberately designed to work\nwell with the larger conbench ecosystem.\n\n## Installation\n\n{benchrun} is not [yet] on a package archive like PyPI; you can install from GitHub\nwith\n\n```shell\npip install benchrun@git+https://github.com/conbench/conbench.git@main#subdirectory=benchrun/python\n```\n\n## Writing benchmarks\n\n### `Iteration`\n\nThe code to run for a benchmark is contained in a class inheriting from the abstract\n`Iteration` class. At a minimum, users must override the `name` attribute and `run()`\nmethod (the code to time), but may also override `setup()`, `before_each()`,\n`after_each()` and `teardown()` methods, where `*_each()` runs before/after each\niteration, and `setup()` and `teardown()` run once before/after all iterations. A\nsimple implementation might look like\n\n```python\nimport time\n\nfrom benchrun import Iteration\n\nclass MyIteration(Iteration):\n    name = \"my-iteration\"\n\n    def before_each(self, case: dict) -> None:\n        # use the `env` dict attribute to pass data between stages\n        self.env = {\"success\": False}\n\n    def run(self, case: dict) -> dict:\n        # code to time goes here\n        time.sleep(case[\"sleep_seconds\"])\n        self.env[\"success\"] = True\n\n    def after_each(self, case: dict) -> None:\n        assert run_results[\"success\"]\n        self.env = {}\n```\n\n### `CaseList`\n\nAn `Iteration`'s methods are parameterized with `case`, a dict where keys are\nparameters for the benchmark, and the values are scalar arguments. Cases are managed\nwith an instance of `CaseList`, a class which takes a `params` dict, which is like a\ncase dict with the difference that the arguments are lists of valid arguments, not\nscalars. `CaseList` will populate a `case_list` attribute which contains the grid of\nspecified cases to be run:\n\n```python\nfrom benchrun import CaseList\n\ncase_list = CaseList(params={\"x\": [1, 2], \"y\": [\"a\", \"b\", \"c\"]})\ncase_list.case_list\n#> [{'x': 1, 'y': 'a'},\n#>  {'x': 1, 'y': 'b'},\n#>  {'x': 1, 'y': 'c'},\n#>  {'x': 2, 'y': 'a'},\n#>  {'x': 2, 'y': 'b'},\n#>  {'x': 2, 'y': 'c'}]\n```\n\n`CaseList` contains an overridable `filter_cases()` method that can be used to remove\ninvalid combinations of parameters, e.g. if an `x` of `2` with a `y` of `b` is not\nviable:\n\n```python\nclass MyCaseList(CaseList):\n    def filter_cases(self, case_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        filtered_case_list = []\n        for case in case_list:\n            if not (case[\"x\"] == 2 and case[\"y\"] == \"b\"):\n                filtered_case_list.append(case)\n\n        return filtered_case_list\n\nmy_case_list = MyCaseList(params={\"x\": [1, 2], \"y\": [\"a\", \"b\", \"c\"]})\nmy_case_list.case_list\n#> [{'x': 1, 'y': 'a'},\n#>  {'x': 1, 'y': 'b'},\n#>  {'x': 1, 'y': 'c'},\n#>  {'x': 2, 'y': 'a'},\n#>  {'x': 2, 'y': 'c'}]\n```\n\nIf there are so many restrictions that it is simpler to specify which cases are\nviable than which are not, the `case_list` parameter of `filter_cases()` can be\ncompletely ignored and a manually-generated list can be returned.\n\n### `Benchmark`\n\nA `Benchmark` in {benchrun} consists of an `Iteration` instance, a `CaseList`\ninstance, and potentially a bit more metadata about how to run it like whether to\ndrop disk caches beforehand.\n\n```python\nmy_benchmark = Benchmark(iteration=my_iteration, case_list=my_case_list)\n```\n\nThis class has a `run()` method to run all cases, or `run_case()` to run a single\ncase.\n\n### `BenchmarkList`\n\nA `BenchmarkList` is a lightweight class to tie together all the instances of\n`Benchmark` that should be run together (e.g. all the benchmarks for a package).\n\n```python\nfrom benchrun import BenchmarkList\n\nmy_benchmark_list = BenchmarkList(benchmarks = [my_benchmark])\n```\n\nThe class has a `__call__()` method that will run all benchmarks in its list,\ntaking care that they all use the same `run_id` so they will all appear together\non conbench.\n\n## Running benchmarks and sending results to conbench\n\n`BenchmarkList` is designed to work seamlessly with\n{[benchadapt](https://github.com/conbench/conbench/tree/main/benchadapt/python)}'s\n`CallableAdapter` class:\n\n```python\nfrom benchadapt.adapters import CallableAdapter\n\nmy_adapter = CallableAdapter(callable=my_benchmark_list)\n```\n\nLike all adapters, it then has a `run()` method to run all the benchmarks it\ncontains (handling generic metadata appropriately for you), a `post_results()`\nmethod that will send the results to a conbench server, and a `__call__()` method\nthat will do both. These are the methods that should be called in whatever CI or\nautomated build system will be used for running benchmarks.\n\n## Setting more metadata\n\n{benchrun} and {benchadapt} make an effort to handle as much metadata for you as\npossible (e.g. things like machine info), but you will still need to specify some\nmetadata yourself, e.g. build flags used in compilation or things like `run_reason`\n(often something like `commit` or `merge`). To see what actually gets sent to\nconbench, see the documentation for `benchadapt.BenchmarkResult`.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/conbench/conbench/tree/main/benchrun",
    "keywords": "",
    "license": "",
    "maintainer": "Voltron Data",
    "maintainer_email": "conbench@voltrondata.com",
    "name": "benchrun",
    "package_url": "https://pypi.org/project/benchrun/",
    "platform": null,
    "project_url": "https://pypi.org/project/benchrun/",
    "project_urls": {
      "Homepage": "https://github.com/conbench/conbench/tree/main/benchrun"
    },
    "release_url": "https://pypi.org/project/benchrun/2023.2.8/",
    "requires_dist": [
      "benchadapt"
    ],
    "requires_python": ">=3.8",
    "summary": "Tools for Running Benchmarks",
    "version": "2023.2.8",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16757208,
  "releases": {
    "2023.2.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ac0079505552cbe109948f055d71783b08c530d59b168257791da7e52c555200",
          "md5": "bdb24e1a87922415b870ddca38f48855",
          "sha256": "1b41ca16b4685823151724bda8e3879e2388e4046f79888b131300e7ae2c7f54"
        },
        "downloads": -1,
        "filename": "benchrun-2023.2.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bdb24e1a87922415b870ddca38f48855",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 16403,
        "upload_time": "2023-02-08T21:33:01",
        "upload_time_iso_8601": "2023-02-08T21:33:01.755248Z",
        "url": "https://files.pythonhosted.org/packages/ac/00/79505552cbe109948f055d71783b08c530d59b168257791da7e52c555200/benchrun-2023.2.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e5002170ba8b96306aefba956a1c6c0b0911c47d9f7e01893b78645f5460e8b6",
          "md5": "b2228567c2eb87a1f56b9eca03801cbe",
          "sha256": "53c617065af2148c3f8902e632cbe3fb4309c1a1bb32e4632641335f4c6cbc57"
        },
        "downloads": -1,
        "filename": "benchrun-2023.2.8.tar.gz",
        "has_sig": false,
        "md5_digest": "b2228567c2eb87a1f56b9eca03801cbe",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 15224,
        "upload_time": "2023-02-08T21:33:04",
        "upload_time_iso_8601": "2023-02-08T21:33:04.164264Z",
        "url": "https://files.pythonhosted.org/packages/e5/00/2170ba8b96306aefba956a1c6c0b0911c47d9f7e01893b78645f5460e8b6/benchrun-2023.2.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ac0079505552cbe109948f055d71783b08c530d59b168257791da7e52c555200",
        "md5": "bdb24e1a87922415b870ddca38f48855",
        "sha256": "1b41ca16b4685823151724bda8e3879e2388e4046f79888b131300e7ae2c7f54"
      },
      "downloads": -1,
      "filename": "benchrun-2023.2.8-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "bdb24e1a87922415b870ddca38f48855",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 16403,
      "upload_time": "2023-02-08T21:33:01",
      "upload_time_iso_8601": "2023-02-08T21:33:01.755248Z",
      "url": "https://files.pythonhosted.org/packages/ac/00/79505552cbe109948f055d71783b08c530d59b168257791da7e52c555200/benchrun-2023.2.8-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e5002170ba8b96306aefba956a1c6c0b0911c47d9f7e01893b78645f5460e8b6",
        "md5": "b2228567c2eb87a1f56b9eca03801cbe",
        "sha256": "53c617065af2148c3f8902e632cbe3fb4309c1a1bb32e4632641335f4c6cbc57"
      },
      "downloads": -1,
      "filename": "benchrun-2023.2.8.tar.gz",
      "has_sig": false,
      "md5_digest": "b2228567c2eb87a1f56b9eca03801cbe",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 15224,
      "upload_time": "2023-02-08T21:33:04",
      "upload_time_iso_8601": "2023-02-08T21:33:04.164264Z",
      "url": "https://files.pythonhosted.org/packages/e5/00/2170ba8b96306aefba956a1c6c0b0911c47d9f7e01893b78645f5460e8b6/benchrun-2023.2.8.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}