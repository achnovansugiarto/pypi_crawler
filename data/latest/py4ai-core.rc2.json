{
  "info": {
    "author": "",
    "author_email": "Nicola Donelli <nicoladonelli87@gmail.com>, Enrico Deusebio <edeusebio85@gmail.com>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "py4ai core\n====\n\n[![PyPI](https://img.shields.io/pypi/v/py4ai-core.svg)](https://pypi.python.org/pypi/py4ai-core)\n[![Python version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://pypi.python.org/pypi/py4ai-core)\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://py4ai.github.io/py4ai-core/)\n![Python package](https://github.com/NicolaDonelli/py4ai-core/workflows/CI%20-%20Build%20and%20Test/badge.svg)\n\n--------------------------------------------------------------------------------\n\n\nA Python library defining data structures optimized for machine learning pipelines \n\n\n## What is it ?\n**py4ai-core** is a Python package with modular design that provides powerful abstractions to build data \ningestion pipelines and run end to end machine learning pipelines. \nThe library offers lightweight object-oriented interface to MongoDB as well as Pandas based data structures. \nThe aim of the library is to provide extensive support for developing machine learning based applications \nwith a focus on practicing clean code and modular design. \n\n## Features\nSome cool features that we are proud to mention are: \n\n### Data layers \n1. Archiver: Offers an object-oriented design to perform ETL on Mongodb collections as well as Pandas DataFrames.\n2. DAO: Data Access Object to allow archivers to serialize domain objects into the proper persistence layer support \nobject (e.g. in the case of MongoDB, a DAO serializes a domain object into a MongoDB document) and to parse objects\nretrieved from the given persistence layer in the correct representation in our framework (e.g. a text will be parsed in \na Document while tabular data will be parsed in a pandas DataFrame).\n3. Database: Object representing a relational database\n4. Table: Object representing a table of a relational database\n\n### Data Model\nOffers the following data structures: \n1. Document : Data structure specifically designed to work with NLP applications that parses a json-like document \ninto a couple of uuid and dictionary of information.\n2. Sample : Data structure representing an observation (a.k.a. sample) as used in machine learning applications\n3. MultiFeatureSample : Data structure representing an observation defined by a nested list of arrays.\n4. Dataset : Data structure designed to be used specifically for machine learning applications representing a collection \nof samples.\n\n## Installation\nFrom pypi server\n```\npip install py4ai-core\n```\n\nFrom source\n```\ngit clone https://github.com/NicolaDonelli/py4ai-core\ncd py4ai-core\nmake install\n```\n\n## Tests \n```\nmake tests\n```\n\n## Checks \nTo run predefined checks (unit-tests, linting checks, formatting checks and static typing checks):\n```\nmake checks\n```\n\n## Examples \n\n### Data Layers\n\nThe Data Layer abstractions are designed to decouple the business layers from \nthe detail of the persistence layer implementation. The basic abstraction that will \nmake this possible is the `Repository`.\n\nAs an example, imagine to have a domain business object `Entity`\n\n```python\nfrom pydantic import BaseModel\n\nclass Entity(BaseModel):\n    my_id: int\n    my_data: str\n```\n\nTo start with, imagine we want to use csv files store on disk as a persistence \nlayer. To do so, we will use the `CsvRepository` that uses pandas DataFrames stored\nin memory and written to the disk as csv. Thus, we need to write the business logic\nto serialize the `Entity` into a row of the pandas DataFrame, i.e. a pandas Series:\n\n```python\nimport pandas as pd\nfrom py4ai.core.data.layer.common.serialiazer import DataSerializer\n\nclass EntitySerializer(DataSerializer[int, int, Entity, pd.Series]):\n    def to_object(self, entity: Entity) -> pd.Series:\n        return pd.Series(entity.dict())\n\n    def to_entity(self, document: pd.Series) -> Entity:\n        return Entity(**document)\n\n    def to_object_key(self, key: int) -> int:\n        return key\n\n    def get_key(self, entity: Entity) -> int:\n        return entity.my_id\n```\n\nWe can now instantiate the repository class that has all the methods for \nreading and writing objects from the persistence layer. \n\n```python\nfrom py4ai.core.data.layer.pandas.repository import CsvRepository\n\nrepo = CsvRepository(filename, EntitySerializer())\n\nentity = Entity(my_id=1234, my_data=\"Important data\")\n\n# This will create the entity in the persistence layer\nawait repo.create(entity)\n\n# Retrieving the entity\nretrieved = repo.retrieve(key=1234)\n\n# Retrieving all entities\nall_entities = repo.list()\n\n```\n\nImagine now that, given the data increase in size, we now would like to change the \npersistence layer with a proper backend into something more structured and scalable, such \nas a NoSQL document-based data platform, such as MongoDB. We only need to create \na new business logic to serialize/deserialize our class into a json (represented\nin python by a dictionary):\n\n```python\nfrom bson import ObjectId\nfrom py4ai.core.data.layer.mongo.serializer import create_mongo_id\nfrom py4ai.core.data.layer.common.serialiazer import DataSerializer\n\nclass MongoDataSerializer(DataSerializer[int, ObjectId, Entity, dict]):\n    def get_key(self, entity: Entity) -> int:\n        return entity.my_id\n\n    def to_object(self, entity: Entity) -> dict:\n        doc = entity.dict()\n        doc[\"_id\"] = self.to_object_key(self.get_key(entity))\n        return doc\n\n    def to_entity(self, document: dict) -> Entity:\n        return Entity(**document)\n\n    def to_object_key(self, key: int) -> ObjectId:\n        # This converts a string into an hash compatible with MongoDB format\n        return create_mongo_id(str(key))\n```\n\nA new repository based on the MongoDB persistence layer can now be created using\n\n```python\nfrom py4ai.core.data.layer.mongo.repository import MongoRepository\n\nrepo = MongoRepository(collection, MongoDataSerializer())\n```\n\nThis repository is compatible with the previous and can be used in place of the \nprevious one, having the same signatures. \n\n#### Abstracting Data Querying \n\nThe `Repository` abstraction also allow to retrieve data based on certain query/filters:\n\n```python\nentities = repo.retrieve_by_criteria(criteria)\n```\n\nHowever, the format of the query also depends on the type of the persistence layer and \nmore specifically on how the data are organized. Therefore, in order to abstract\nand decouple the notion of the underlying persistence layer, we need to define a general \nclass containing the possible queries for a certain database:\n\n```python\nfrom typing import Generic\nfrom abc import ABC, abstractmethod\n\nfrom py4ai.core.data.layer.common.criteria import SearchCriteria\n\nclass EntityCriteriaFactory(ABC, Generic[Q]):\n    @abstractmethod\n    def by_id(self, id: int) -> SearchCriteria[Q]:\n        ...\n```\n\nWhen considering a particular persistence layer, the querying business logic \nneeds to be specified\n\n```python\nfrom py4ai.core.data.layer.mongo.criteria import MongoSearchCriteria\n\nclass MongoCriteriaFactory(EntityCriteriaFactory[Dict[str, Any]]):\n    \n    def by_id(self, my_id: int) -> MongoSearchCriteria:\n        return MongoSearchCriteria({\"my_id\": my_id})\n\ncriteria = MongoCriteriaFactory()\n\nentities = repo.retrieve_by_criteria(criteria.by_id(1234))\n```\n\nNote that `SearchCriteria` can be also joined using logical operators:\n\n```python\n\nentities = repo.retrieve_by_criteria(\n    criteria.by_id(1234) & criteria.by_id(1235) \n)\n\nentities = repo.retrieve_by_criteria(\n    criteria.by_id(1234) | criteria.by_id(1235) \n)\n\n```\n\n\n\n\n\n### Data Model\n\nCreating a PandasDataset object\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom py4ai.core.data.model.ml import PandasDataset\n\ndataset = PandasDataset(features=pd.concat([pd.Series([1, np.nan, 2, 3], name=\"feat1\"),\n                                            pd.Series([1, 2, 3, 4], name=\"feat2\")], axis=1),\n                        labels=pd.Series([0, 0, 0, 1], name=\"Label\"))\n\n# access features as a pandas dataframe \nprint(dataset.features.head())\n# access labels as pandas dataframe \nprint(dataset.labels.head())\n# access features as a python dictionary \ndataset.getFeaturesAs('dict')\n# access features as numpy array \ndataset.getFeaturesAs('array')\n\n# indexing operations \n# access features and labels at the given index as a pandas dataframe  \nprint(dataset.loc([2]).features.head())\nprint(dataset.loc([2]).labels.head())\n```\n\nCreating a PandasTimeIndexedDataset object\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom py4ai.core.data.model.ml import PandasTimeIndexedDataset\n\ndateStr = [str(x) for x in pd.date_range('2010-01-01', '2010-01-04')]\ndataset = PandasTimeIndexedDataset(\n    features=pd.concat([\n        pd.Series([1, np.nan, 2, 3], index=dateStr, name=\"feat1\"),\n        pd.Series([1, 2, 3, 4], index=dateStr, name=\"feat2\")\n    ], axis=1))\n```\n\n## How to contribute ? \n\nWe are very much willing to welcome any kind of contribution whether it is bug report, bug fixes, contributions to the \nexisting codebase or improving the documentation. \n\n### Where to start ? \nPlease look at the [Github issues tab](https://github.com/NicolaDonelli/py4ai-core/issues) to start working on open \nissues \n\n### Contributing to py4ai-core \nPlease make sure the general guidelines for contributing to the code base are respected\n1. [Fork](https://docs.github.com/en/get-started/quickstart/contributing-to-projects) the py4ai-core repository. \n2. Create/choose an issue to work on in the [Github issues page](https://github.com/NicolaDonelli/py4ai-core/issues). \n3. [Create a new branch](https://docs.github.com/en/get-started/quickstart/github-flow) to work on the issue. \n4. Commit your changes and run the tests to make sure the changes do not break any test. \n5. Open a Pull Request on Github referencing the issue.\n6. Once the PR is approved, the maintainers will merge it on the main branch.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "py4ai-core",
    "package_url": "https://pypi.org/project/py4ai-core/",
    "platform": null,
    "project_url": "https://pypi.org/project/py4ai-core/",
    "project_urls": {
      "Homepage": "https://github.com/NicolaDonelli/py4ai-core"
    },
    "release_url": "https://pypi.org/project/py4ai-core/1.0.0/",
    "requires_dist": [
      "cfg-load (>=0.8)",
      "deprecated (>=1.2.12)",
      "pandas (>=1.0)",
      "pydantic (>=1.9)",
      "scipy (>=1.7.3)",
      "setuptools (>=61.2)",
      "tomli (>=2.0.1)",
      "typing-extensions (>=4.0)",
      "bandit[toml] (>=1.7.4) ; extra == 'dev'",
      "black (>=21.12b0) ; extra == 'dev'",
      "darglint (>=1.8.1) ; extra == 'dev'",
      "flake8 (>=4.0.1) ; extra == 'dev'",
      "flake8-docstrings (>=1.6) ; extra == 'dev'",
      "flake8-rst-docstrings (>=0.2.5) ; extra == 'dev'",
      "Flake8-pyproject (>=1.1.0) ; extra == 'dev'",
      "greenlet (>=1.1.3) ; extra == 'dev'",
      "isort (>=5.10.1) ; extra == 'dev'",
      "m2r2 (<0.3.3,>=0.3.2) ; extra == 'dev'",
      "mypy (>=0.910) ; extra == 'dev'",
      "mypy-extensions (>=0.4.3) ; extra == 'dev'",
      "pip-tools (<7.0.0,>=6.6.2) ; extra == 'dev'",
      "pytest (>=6.2) ; extra == 'dev'",
      "pytest-cov (>=3.0) ; extra == 'dev'",
      "recommonmark (>=0.7) ; extra == 'dev'",
      "Sphinx (<6.0.0,>4.1) ; extra == 'dev'",
      "sphinx-rtd-theme (>=0.5) ; extra == 'dev'",
      "twine (>=3.8) ; extra == 'dev'",
      "versioneer (>=0.21) ; extra == 'dev'"
    ],
    "requires_python": ">=3.8",
    "summary": "A Python library of data structures optimized for machine learning tasks",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16258612,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "725b50f33ddadc660401b312d8d7d6542fcf0131cca61eab5f7dece9bf5769f9",
          "md5": "a670b34fb4fabd865348b80a04b44896",
          "sha256": "e487f9ba57ac2cd7b197287c495afd034e596be7cd5c7adb66a97e7d5a2d4d38"
        },
        "downloads": -1,
        "filename": "py4ai_core-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a670b34fb4fabd865348b80a04b44896",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.7",
        "size": 59777,
        "upload_time": "2022-10-07T16:29:28",
        "upload_time_iso_8601": "2022-10-07T16:29:28.875492Z",
        "url": "https://files.pythonhosted.org/packages/72/5b/50f33ddadc660401b312d8d7d6542fcf0131cca61eab5f7dece9bf5769f9/py4ai_core-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4f93e73ab01e77ed07f1a7ec84c6135e81dbf93a02103b8546855a59230f1416",
          "md5": "5e3a2e825b28e96c28031c6c8eda679c",
          "sha256": "4054af38a74005b3c5b125f06d55c9fe2c112517d9dade1ea3c08b609ac57c98"
        },
        "downloads": -1,
        "filename": "py4ai-core-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5e3a2e825b28e96c28031c6c8eda679c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.7",
        "size": 69067,
        "upload_time": "2022-10-07T16:29:30",
        "upload_time_iso_8601": "2022-10-07T16:29:30.507112Z",
        "url": "https://files.pythonhosted.org/packages/4f/93/e73ab01e77ed07f1a7ec84c6135e81dbf93a02103b8546855a59230f1416/py4ai-core-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8c6871c88db259a6a9b22e9c27a8f6cf7311f8b2bf847d2e7acb5c18c0d67f8b",
          "md5": "a9e61355881f2f28f23a6d8c10d6fdd9",
          "sha256": "80cfdb2a88901f8771b13fea64aa696726e352e589b7258fd042e7f3eae3a4c3"
        },
        "downloads": -1,
        "filename": "py4ai_core-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a9e61355881f2f28f23a6d8c10d6fdd9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 23384,
        "upload_time": "2022-12-30T14:37:05",
        "upload_time_iso_8601": "2022-12-30T14:37:05.722668Z",
        "url": "https://files.pythonhosted.org/packages/8c/68/71c88db259a6a9b22e9c27a8f6cf7311f8b2bf847d2e7acb5c18c0d67f8b/py4ai_core-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cd7ac2361ab9195c4c8c9a0f3bcd1151ebef4bbf6e5ffa6cf5f76565f0da7dfc",
          "md5": "08bcc9946c085d722ddf843bde35411b",
          "sha256": "13266824e4af70d8beb88a19283adf1d208ce707e9c0fe3ac848f8b006b4984e"
        },
        "downloads": -1,
        "filename": "py4ai-core-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "08bcc9946c085d722ddf843bde35411b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 41672,
        "upload_time": "2022-12-30T14:37:07",
        "upload_time_iso_8601": "2022-12-30T14:37:07.565001Z",
        "url": "https://files.pythonhosted.org/packages/cd/7a/c2361ab9195c4c8c9a0f3bcd1151ebef4bbf6e5ffa6cf5f76565f0da7dfc/py4ai-core-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8c6871c88db259a6a9b22e9c27a8f6cf7311f8b2bf847d2e7acb5c18c0d67f8b",
        "md5": "a9e61355881f2f28f23a6d8c10d6fdd9",
        "sha256": "80cfdb2a88901f8771b13fea64aa696726e352e589b7258fd042e7f3eae3a4c3"
      },
      "downloads": -1,
      "filename": "py4ai_core-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a9e61355881f2f28f23a6d8c10d6fdd9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 23384,
      "upload_time": "2022-12-30T14:37:05",
      "upload_time_iso_8601": "2022-12-30T14:37:05.722668Z",
      "url": "https://files.pythonhosted.org/packages/8c/68/71c88db259a6a9b22e9c27a8f6cf7311f8b2bf847d2e7acb5c18c0d67f8b/py4ai_core-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cd7ac2361ab9195c4c8c9a0f3bcd1151ebef4bbf6e5ffa6cf5f76565f0da7dfc",
        "md5": "08bcc9946c085d722ddf843bde35411b",
        "sha256": "13266824e4af70d8beb88a19283adf1d208ce707e9c0fe3ac848f8b006b4984e"
      },
      "downloads": -1,
      "filename": "py4ai-core-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "08bcc9946c085d722ddf843bde35411b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 41672,
      "upload_time": "2022-12-30T14:37:07",
      "upload_time_iso_8601": "2022-12-30T14:37:07.565001Z",
      "url": "https://files.pythonhosted.org/packages/cd/7a/c2361ab9195c4c8c9a0f3bcd1151ebef4bbf6e5ffa6cf5f76565f0da7dfc/py4ai-core-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}