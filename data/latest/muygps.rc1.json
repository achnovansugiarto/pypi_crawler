{
  "info": {
    "author": "Benjamin W. Priest",
    "author_email": "priest2@llnl.gov",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: MacOS",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Mathematics",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "[![pipeline status](https://lc.llnl.gov/gitlab/muygps/MuyGPyS/badges/main/pipeline.svg)](https://lc.llnl.gov/gitlab/muygps/MuyGPyS/-/commits/main)\n[![Documentation Status](https://readthedocs.org/projects/muygpys/badge/?version=latest)](https://muygpys.readthedocs.io/en/latest/?badge=latest)\n\n# Fast implementation of the MuyGPs Gaussian process hyperparameter estimation algorithm\n\n\nMuyGPs is a GP estimation method that affords fast hyperparameter optimization by way of performing leave-one-out cross-validation.\nMuyGPs achieves best-in-class speed and scalability by limiting inference to the information contained in k nearest neighborhoods for prediction locations for both hyperparameter optimization and tuning.\nThis feature affords the optimization of hyperparameters by way of leave-one-out cross-validation, as opposed to the more expensive loglikelihood evaluations requires by similar sparse methods. \n\n\n## Installation\n\n\nPip installation instructions:\n```\n$ pip install muygpys\n```\n\nTo install from source, follow these instructions:\n```\n$ git clone git@github.com:LLNL/MuyGPyS.git\n$ pip install -e MuyGPyS\n```\n\n\n## Building Docs\n\nAutomatically-generated documentation can be found at [readthedocs.io](https://muygpys.readthedocs.io/en/latest/?).\n\nDoc building instructions:\n```\n$ cd /path/to/this/repo/docs\n$ pip install -r requirements.txt\n$ sphinx-build -b html docs docs/_build/html\n```\nThen open the file `docs/_build/html/index.html` in your browser of choice.\n\n\n## The Basics\n\n\n### Data format\n\n\n`MuyGPyS` expects that each train or test observation corresponds to a row index in feature and response matrices.\nIn our examples we assume that train data is bundled into a `(train_count, feature_count)` feature matrix `train_features` and a `(train_count, response_count)` response matrix `train_responses`. \nIn classification examples we will instead refer to a `(train_count, class_count)` label matrix `train_labels` whose rows are one-hot encodings.\nOur examples will assume that the data is accessible via imaginary getter functions. \n\n\n### Constructing Nearest Neighbor Lookups\n\n\n`MuyGPyS.neighbors.NN_Wrapper` is an api for tasking several KNN libraries with the construction of lookup indexes that empower fast training and inference.\nThe wrapper constructor expects the training features, the number of nearest neighbors, and a method string specifying which algorithm to use, as well as any additional kwargs used by the methods.\nCurrently supported implementations include [exact KNN using sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) (\"exact\") and [approximate KNN using hnsw](https://github.com/nmslib/hnswlib) (\"hnsw\").\n\nConstruct exact and approximate  KNN data example with k = 10\n```\n>>> from MuyGPyS.neighors import NN_Wrapper \n>>> train_features = load_train_features()  # imaginary getter\n>>> nn_count = 10\n>>> exact_nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"exact\", algorithm=\"ball_tree\")\n>>> approx_nbrs_lookup = NN_Wrapper(train_features, nn_count, nn_method=\"hnsw\", space=\"l2\", M=16)\n```\n\nThese lookup data structures are then usable to find nearest neighbors of queries in the training data.\n\n\n### Sampling Batches of Data\n\n\nMuyGPyS includes convenience functions for sampling batches of data from existing datasets.\nThese batches are returned in the form of row indices, both of the sampled data as well as their nearest neighbors.\nAlso included is the ability to sample \"balanced\" batches, where the data is partitioned by class and we attempt to sample as close to an equal number of items from each class as is possible. \n\nSampling random and balanced (for classification) batches of 100 elements:\n```\n>>> from MuyGPyS.optimize.batch import sample_batch, get_balanced_batch\n>>> train_labels = load_train_labels()  # imaginary getter\n>>> batch_count = 200\n>>> train_count, _ = train_features.shape\n>>> batch_indices, batch_nn_indices = sample_batch(\n...         exact_nbrs_lookup, batch_count, train_count\n... )\n>>> train_lookup = np.argmax(train[\"output\"], axis=1)\n>>> balanced_indices, balanced_nn_indices = get_balanced_batch(\n...         exact_nbrs_lookup, train_lookup, batch_count\n... ) # Classification only!\n```\n\nThese `indices` and `nn_indices` arrays are the basic operating blocks of `MuyGPyS` linear algebraic inference.\nThe elements of `indices.shape == (batch_count,)` lists all of the row indices into `train`'s matrices corresponding to the sampled data.\nThe rows of `nn_indices.shape == (batch_count, nn_count)` list the row indices into `train`'s matrices corresponding to the nearest neighbors of the sampled data.\nWhile the user need not use MuyGPyS sampling tools to construct these data, they will need to construct similar indices into their data in order to use MuyGPyS.\n\n\n### Setting and Optimizing Hyperparameters\n\n\nOne initializes a MuyGPS object by indicating the kernel, as well as optionally specifying hyperparameters.\n\nCreating a Matern kernel:\n```\n>>> from MuyGPyS.gp.muygps import MuyGPS\n>>> k_kwargs = {\n...         \"kern\": \"rbf\",\n...         \"metric\": \"F2\",\n...         \"eps\": {\"val\": 1e-5},\n...         \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...         \"length_scale\": {\"val\": 7.2},\n... }\n>>> muygps = MuyGPS(**k_kwarg)\n```\n\nHyperparameters can be initialized or reset using dictionary arguments containing the optional `\"val\"` and `\"bounds\"` keys.\n`\"val\"` sets the hyperparameter to the given value, and `\"bounds\"` determines the upper and lower bounds to be used for optimization.\nIf `\"bounds\"` is set, `\"val\"` can also take the arguments `\"sample\"` and `\"log_sample\"` to generate a uniform or log uniform sample, respectively.\nIf `\"bounds\"` is set to `\"fixed\"`, the hyperparameter will remain fixed during any optimization.\nThis is the default behavior for all hyperparameters if `\"bounds\"` is unset by the user.\n\nOne sets hyperparameters such as `eps`, `sigma_sq`, as well as kernel-specific hyperparameters, e.g. `nu` and  `length_scale` for the Matern kernel, at initialization as above.\n\nMuyGPyS depends upon linear operations on specially-constructed tensors in order to efficiently estimate GP realizations.\nConstructing these tensors depends upon the nearest neighbor index matrices that we described above.\nWe can construct a distance tensor coalescing all of the square pairwise distance matrices of the nearest neighbors of a batch of points.\nThis snippet constructs a Euclidean distance tensor.\n```\n>>> from MuyGPyS.gp.distance import pairwise_distances\n>>> pairwise_dists = pairwise_distances(\n...         train_features, batch_nn_indices, metric=\"l2\"\n... )\n```\n\nWe can similarly construct a matrix coalescing all of the distance vectors between the same batch of points and their nearest neighbors.\n```\n>>> from MuyGPyS.gp.distance import crosswise_distances\n>>> crosswise_dists = crosswise_distances(\n...         train_features,\n...         train_features,\n...         batch_indices,\n...         batch_nn_indices,\n...         metric='l2',\n... )\n```\n\nWe can easily realize kernel tensors using a `MuyGPS` object's kernel functor:\n```\n>>> K = muygps.kernel(pairwise_dists)\n>>> Kcross = muygps.kernel(crosswise_dists)\n```\n\nWe supply a convenient leave-one-out cross-validation utility that internally realizes kernel tensors in this manner.\n```\n>>> from MuyGPyS.optimize.chassis.scipy_optimize_from_tensors\n>>> scipy_optimize_from_tensors(\n...         muygps,\n...         batch_indices,\n...         batch_nn_indices,\n...         crosswise_dists,\n...         pairwise_dists,\n...         train_labels,\n...         loss_method=\"mse\",\n...         verbose=True,\n... )\nparameters to be optimized: ['nu']\nbounds: [[0.1 1. ]]\nsampled x0: [0.8858425]\noptimizer results:\n      fun: 0.4797763813693626\n hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n      jac: array([-3.06976666e-06])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n     nfev: 16\n      nit: 5\n     njev: 8\n   status: 0\n  success: True\n        x: array([0.39963594])\n```\n\nIf you do not need to keep the distance tensors around for reference, you can use a related function:\n```\n>>> from MuyGPyS.optimize.chassis.scipy_optimize_from_indices\n>>> scipy_optimize_from_indices(\n...         muygps,\n...         batch_indices,\n...         batch_nn_indices,\n...         train_features,\n...\t    test_features,\n...\t    train_labels,\n...         loss_method=\"mse\",\n...         verbose=False,\n... )\n```\n\n\n### One-line model creation\n\n\nThe library also provides one-line APIs for creating MuyGPs models intended for regression and classification.\nThe functions are `MuyGPyS.examples.regress.make_regressor` and `MuyGPyS.examples.classify.make_classifier`, respectively.\nThese functions provide convenient mechanisms for specifying and optimizing models if you have no need to later reference their intermediate data structures (such as the training batches or their distance tensors).\nThey return only the trained `MuyGPyS.gp.muygps.MuyGPS` model and the `MuyGPyS.neighbors.NN_Wrapper` neighbors lookup data structure.\n\nAn example regressor.\nIn order to automatically train `sigma_sq`, set `k_kwargs[\"sigma_sq\"] = \"learn\"`. \n```\n>>> from MuyGPyS.examples.regress import make_regressor\n>>> train_features, train_responses = load_train()  # imaginary train getter\n>>> test_features, test_responses = load_test()  # imaginary test getter\n>>> nn_kwargs = {\"nn_method\": \"exact\", \"algorithm\": \"ball_tree\"}\n>>> k_kwargs = {\n...         \"kern\": \"rbf\",\n...         \"metric\": \"F2\",\n...         \"eps\": {\"val\": 1e-5},\n...         \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...         \"length_scale\": {\"val\": 7.2},\n...         \"sigma_sq\": \"learn\",\n... }\n>>> muygps, nbrs_lookup = make_regressor(\n...         train_features,\n...         train_responses,\n...         nn_count=40,\n...         batch_size=500,\n...         loss_method=\"mse\",\n...         k_kwargs=k_kwargs,\n...         nn_kwargs=nn_kwargs,\n...         verbose=False,\n... )    \n```\n\nAn example surrogate classifier.\n```\n>>> from MuyGPyS.examples.classify import make_classifier\n>>> train_features, train_labels = load_train()  # imaginary train getter\n>>> nn_kwargs = {\"nn_method\": \"exact\", \"algorithm\": \"ball_tree\"}\n>>> k_kwargs = {\n...         \"kern\": \"rbf\",\n...         \"metric\": \"F2\",\n...         \"eps\": {\"val\": 1e-5},\n...         \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...         \"length_scale\": {\"val\": 7.2},\n... }\n>>> muygps, nbrs_lookup = make_classifier(\n...         train_features,\n...         train_labels,\n...         nn_count=40,\n...         batch_size=500,\n...         loss_method=\"log\",\n...         k_kwargs=k_kwargs,\n...         nn_kwargs=nn_kwargs, \n...         verbose=False,\n... )    \n```\n\n\n### Multivariate Models\n\n\nMuyGPyS also supports multivariate models via the `MuyGPyS.gp.muygps.MultivariateMuyGPS` class, which maintains a separte kernel function for each response dimension.\nThis class is similar in interface to `MuyGPyS.gp.muygps.MuyGPS`, but requires a list of hyperparameter dicts at initialization.\nSee the following example:\n```\n>>> from MuyGPyS.gp.muygps import MultivariateMuyGPS as MMuyGPS\n>>> k_args = [\n... \t    {\n...                 \"eps\": {\"val\": 1e-5},\n...                 \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...                 \"length_scale\": {\"val\": 7.2},\n...\t    },\n... \t    {\n...                 \"eps\": {\"val\": 1e-5},\n...                 \"nu\": {\"val\": 0.67, \"bounds\": (0.1, 2.5)},\n...                 \"length_scale\": {\"val\": 7.2},\n...\t    },\n... ]\n>>> mmuygps = MMuyGPS(\"matern\", **k_args)\n```\n\nTraining is similar, and depends upon the same neighbors index datastructures as the singular models.\nIn order to train, one need only loop over the models contained within the multivariate object.\n```\n>>> from MuyGPyS.optimize.chassis.scipy_optimize_from_indices\n>>> for i, model in mmuygps.models:\n>>>         scipy_optimize_from_indices(\n...                 model,\n...                 batch_indices,\n...                 batch_nn_indices,\n...                 train_features,\n...\t            test_features,\n...\t            train_responses[:, i].reshape(train_count, 1),\n...                 loss_method=\"mse\",\n...                 verbose=False,\n...         )\n```\n\nWe also support one-line make functions for regression and classification:\n```\n>>> from MuyGPyS.examples.regress import make_multivariate_regressor\n>>> train_features, train_responses = load_train()  # imaginary train getter\n>>> nn_kwargs = {\"nn_method\": \"exact\", \"algorithm\": \"ball_tree\"}\n>>> k_args = [\n... \t    {\n...                 \"eps\": {\"val\": 1e-5},\n...                 \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...                 \"length_scale\": {\"val\": 7.2},\n...\t    },\n... \t    {\n...                 \"eps\": {\"val\": 1e-5},\n...                 \"nu\": {\"val\": 0.67, \"bounds\": (0.1, 2.5)},\n...                 \"length_scale\": {\"val\": 7.2},\n...\t    },\n... ]\n>>> muygps, nbrs_lookup = make_multivariate_regressor(\n...         train_features,\n...         train_responses,\n...         nn_count=40,\n...         batch_size=500,\n...         loss_method=\"mse\",\n...\t    kern=\"matern\",\n...         k_args=k_args,\n...         nn_kwargs=nn_kwargs,\n...         verbose=False,\n... )    \n```\n\n\n\n### Inference\n\n\nWith set hyperparameters, we are able to use the `muygps` object to predict the response of test data.\nSeveral workflows are supported.\nSee below a simple regression workflow, using the data structures built up in this example:\n```\n>>> indices = np.arange(test_count)\n>>> nn_indices = train_nbrs_lookup.get_nns(test[\"input\"])\n>>> pairwise_dists = pairwise_distances(\n...         train_features, batch_nn_indices, metric=\"l2\"\n... )\n>>> crosswise_dists = crosswise_distances(\n...         test_features,\n...         train_features,\n...         indices,\n...         nn_indices,\n...         metric='l2',\n... )\n>>> K = muygps.kernel(pairwise_dists)\n>>> Kcross = muygps.kernel(crosswise_dists)\n>>> predictions = muygps.regress(K, Kcross, train_responses[nn_indices, :])\n```\n\nAgain if you do not want to reuse your tensors, you can run the more compact:\n```\n>>> indices = np.arange(test_count)\n>>> nn_indices = train_nbrs_lookup.get_nns(test[\"input\"])\n>>> muygps.regress_from_indices(\n...         indices,\n...\t    nn_indices,\n...\t    test_features,\n...\t    train_features,\n...\t    train_responses,\n... )\n```\n\nMultivariate models support the same functions.\n\nMore complex workflows are of course available.\nSee the `MuyGPyS.examples` high-level API functions for examples.\n\n\n## API Examples\n\n\nListed below are several examples using the high-level APIs located in `MuyGPyS.examples.classify` and `MuyGPyS.examples.regress`.\nNote that one need not go through these APIs to use `MuyGPyS`, but they condense many basic workflows into a single function call.\nIn all of these examples, note that if all of the hyperparameters are fixed in `k_kwargs` (i.e. you supply no optimization bounds), the API will perform no optimization and will instead simply predict on the data problem using the provided kernel.\nWhile these examples all use a single model, one can modify those with multivariate responses to use multivariate models by supplying the additional keyword argument `kern=kernel_name`, for `kernel_name in ['rbf', 'matern']` and providing a list of hyperparameter dicts to the keyword argument `k_kwargs` as above.\n\n## Regression\n\n\nThe following example performs GP regression on the [Heaton spatial statistics case study dataset](https://github.com/finnlindgren/heatoncomparison).\nIn the example, `load_heaton` is a unspecified function that reads in the dataset in the specified dict format.\nIn practice, a user can use any conforming dataset.\nIf one wants to predict on a univariate response as in this example, one must ensure the data is stored as a matrix rather than as a vector, i.e. that `train['output'].shape = (train_count, 1)`.\nThe regression API adds a `sigma_sq` scale parameter for the variance.\nOne can set `sigma_sq` using the `hyper_dict` kwarg like other hyperparameters.\nThe API expects that `sigma_sq` is a `numpy.ndarray` with a value associated with each dimension of the response, i.e. that `train['output'].shape[1] == len(sigma_sq)`.\n\nRegress on Heaton data with no variance\n```\n>>> import numpy as np\n>>> from MuyGPyS.examples.regress import do_regress\n>>> from MuyGPyS.optimize.objective import mse_fn\n>>> train_features, train_responses = load_heaton_train()  # imaginary train getter\n>>> test_features, test_responses = load_heaton_test()  # imaginary test getter\n>>> nn_kwargs = {\"nn_method\": \"exact\", \"algorithm\": \"ball_tree\"}\n>>> k_kwargs = {\n...         \"kern\": \"rbf\",\n...         \"metric\": \"F2\",\n...         \"eps\": {\"val\": 1e-5},\n...         \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...         \"length_scale\": {\"val\": 7.2},\n... }\n>>> muygps, nbrs_lookup, predictions = do_regress(\n...         test_features,\n...         train_features,\n...         train_responses,\n...         nn_count=30,\n...         batch_size=200,\n...         loss_method=\"mse\",\n...         variance_mode=None,\n...         k_kwargs=k_kwargs,\n...         nn_kwargs=nn_kwargs,\n...         verbose=True,\n... )\nparameters to be optimized: ['nu']\nbounds: [[0.1 1. ]]\nsampled x0: [0.8858425]\noptimizer results:\n      fun: 0.4797763813693626\n hess_inv: <1x1 LbfgsInvHessProduct with dtype=float64>\n      jac: array([-3.06976666e-06])\n  message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n     nfev: 16\n      nit: 5\n     njev: 8\n   status: 0\n  success: True\n        x: array([0.39963594])\nNN lookup creation time: 0.04974837500000007s\nbatch sampling time: 0.017116840000000022s\ntensor creation time: 0.14439213699999998s\nhyper opt time: 2.742181974s\nsigma_sq opt time: 5.359999999399179e-07s\nprediction time breakdown:\n        nn time:0.1069446140000001s\n        agree time:1.9999999967268423e-07s\n        pred time:10.363597161000001s\nfinds hyperparameters:\n        nu : 0.8858424985399979\n>>> print(f\"mse : {mse_fn(predictions, test_responses)}\")\nobtains mse: 2.345136495565052\n```\n\nIf one requires the (individual, independent) posterior variances for each of the predictions, one can pass `variance_mode=\"diagonal\"`.\nThis mode assumes that each output dimension uses the same model, and so will output an additional vector `variance` with a scalar posterior variance associated with each test point.\nThe API also returns a (possibly trained) `MuyGPyS.gp.MuyGPS` or `MuyGPyS.gp.MultivariateMuyGPS` instance, whose `sigma_sq` member reports an array of multiplicative scaling parameters associated with the variance of each dimension.\nIn order to tune `sigma-sq` using the `do_regress` API, pass `k_kwargs[\"sigma_sq\"] = \"learn\"`.\nObtaining the tuned posterior variance implies multiplying the returned variance by the scaling parameter along each dimension.\n\n\nRegress on Heaton data while estimating diagonal variance\n```\n>>> import numpy as np\n>>> from MuyGPyS.examples.regress import do_regress\n>>> from MuyGPyS.optimize.objective import mse_fn\n>>> train_features, train_responses = load_heaton_train()  # imaginary train getter\n>>> test_features, test_responses = load_heaton_test()  # imaginary test getter\n>>> nn_kwargs = {\"nn_method\": \"exact\", \"algorithm\": \"ball_tree\"}\n>>> k_kwargs = {\n...         \"kern\": \"rbf\",\n...         \"metric\": \"F2\",\n...         \"eps\": {\"val\": 1e-5},\n...         \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...         \"length_scale\": {\"val\": 7.2},\n...\t    \"sigma_sq\": \"learn\",\n... }\n>>> muygps, nbrs_lookup, predictions, variance = do_regress(\n...         test_features,\n...         train_features,\n...         train_responses,\n...         nn_count=30,\n...         batch_size=200,\n...         loss_method=\"mse\",\n...         variance_mode=\"diagonal\",\n...         k_kwargs=k_kwargs,\n...         nn_kwargs=nn_kwargs,\n...         verbose=False,\n... )\n>>> print(f\"mse : {mse_fn(predictions, test_responses)}\")\nobtains mse: 2.345136495565052\n>>> print(f\"diagonal posterior variance: {variance * muygps.sigma_sq()}\")\ndiagonal posterior variance: [0.52199482 0.45934382 0.81381388 ... 0.64982631 0.45958342 0.68602048]\n```\n\nIndependent diagonal variance for each test item is the only form of posterior variance supported for a single model, and independent diagonal variance for each test item along each response dimension is the only form of posterior variacne supported for a multivariate model. \nComputing the full posterior covariance between the dimensions of multivariate output is not currently supported, but is planned for a future release.\nComputing the full posterior covariance between all inputs is not and will not be supported for scalability reasons. \n\n\n\n## Classification\n\n\nWhat follows is an example workflow performing two-class classification with uncertainty quantification.\nSpecific outputs uses a star-galaxy image dataset, where stars are labeled `[-1, +1]` and galaxies are labeled `[+1, -1]`.\nLoading logic is encapsulated in the imaginary `load_stargal` function.\nThe workflow suffices for any conforming 2-class dataset.\n\nWhat follows is example code surrounding the invocation of `MuyGPyS.examples.classify.do_classify_uq`.\nThis function returns GP predictions `surrogate_predictions` and a list of index masks `masks`.  \n\nRun star-gal with UQ example instructions:\n```\n>>> import numpy as np\n>>> from MuyGPyS.examples.two_class_classify_uq import do_classify_uq, do_uq, example_lambdas\n>>> from MuyGPyS.optimize.objective import mse_fn\n>>> train_features, train_labels = load_stargal_train()  # imaginary train getter\n>>> test_features, test_labels = load_stargal_test()  # imaginary test getter\n>>> nn_kwargs = {\"nn_method\": \"exact\", \"algorithm\": \"ball_tree\"}\n>>> k_kwargs = {\n...         \"kern\": \"rbf\",\n...         \"metric\": \"F2\",\n...         \"eps\": {\"val\": 1e-5},\n...         \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...         \"length_scale\": {\"val\": 7.2},\n... }\n>>> muygps, nbrs_lookup, surrogate_predictions, masks = do_classify_uq(\n...         test_features,\n...         train_features,\n...         train_labels,\n...         nn_count=30,\n...         opt_batch_size=200,\n...\t    uq_batch_size=500,\n...         loss_method=\"log\",\n...         variance_mode=None,\n...\t    uq_objectives=example_lambdas,\n...         k_kwargs=k_kwargs,\n...         nn_kwargs=nn_kwargs,\n...         verbose=False,\n... )\n>>> accuracy, uq = do_uq(surrogate_predictions, test_labels, masks)\n>>> print(f\"obtained accuracy: {accuracy}\")\nobtained accuracy: 0.973...\n>>> print(f\"mask uq : \\n{uq}\")\nmask uq : \n[[8.21000000e+02 8.53836784e-01 9.87144569e-01]\n [8.59000000e+02 8.55646100e-01 9.87528717e-01]\n [1.03500000e+03 8.66666667e-01 9.88845510e-01]\n [1.03500000e+03 8.66666667e-01 9.88845510e-01]\n [5.80000000e+01 6.72413793e-01 9.77972239e-01]]\n```\n\n`uq_objectives` expects a list of functions of `alpha`, `beta`, `correct_count`, and `incorrect_count`, where `alpha` and `beta` are the number of type I and type II errors, respectively.\n`MuyGPyS.examples.classify.example_lambdas` lists some options, but you can supply your own.\n\nIf uncertainty quantification is not desired, or the classifcation problem in question involves more than two classes, see instead an example workflow like that in `MuyGPyS.examples.classify.do_classify`.\n\nRun MNIST without UQ example instructions:\n```\n>>> import numpy as np\n>>> from MuyGPyS.examples.classify import do_classify\n>>> from MuyGPyS.optimize.objective import mse_fn\n>>> train_features, train_labels = load_stargal_train()  # imaginary train getter\n>>> test_features, test_labels = load_stargal_test()  # imaginary test getter\n>>> nn_kwargs = {\"nn_method\": \"exact\", \"algorithm\": \"ball_tree\"}\n>>> k_kwargs = {\n...         \"kern\": \"rbf\",\n...         \"metric\": \"F2\",\n...         \"eps\": {\"val\": 1e-5},\n...         \"nu\": {\"val\": 0.38, \"bounds\": (0.1, 2.5)},\n...         \"length_scale\": {\"val\": 7.2},\n... }\n>>> muygps, nbrs_lookup, surrogate_predictions = do_classify(\n...         test_features,\n...         train_features,\n...         train_labels,\n...         nn_count=30,\n...         batch_size=200,\n...         loss_method=\"log\",\n...         variance_mode=None,\n...         k_kwargs=k_kwargs,\n...         nn_kwargs=nn_kwargs,\n...         verbose=False,\n... )\n>>> predicted_labels = np.argmax(surrogate_predictions, axis=1)\n>>> true_labels = np.argmax(test_labels, axis=1)\n>>> accuracy = np.mean(predicted_labels == true_labels)\n>>> print(f\"obtained accuracy: {accuracy}\")\n0.97634\n```\n\n\n# About\n\n## Authors\n\n* Benjamin W. Priest (priest2 at llnl dot gov)\n* Amanada L. Muyskens (muyskens1 at llnl dot gov)\n\n## Papers\n\nMuyGPyS has been used the in the following papers (newest first):\n\n1. [Gaussian Process Classification fo Galaxy Blend Identification in LSST](https://arxiv.org/abs/2107.09246)\n2. [Star-Galaxy Image Separation with Computationally Efficient Gaussian Process Classification](https://arxiv.org/abs/2105.01106)\n3. [Star-Galaxy Separation via Gaussian Processes with Model Reduction](https://arxiv.org/abs/2010.06094)\n\n## Citation\n\nIf you use MuyGPyS in a research paper, please reference our article:\n\n```\n@article{muygps2021,\n  title={MuyGPs: Scalable Gaussian Process Hyperparameter Estimation Using Local Cross-Validation},\n  author={Muyskens, Amanda and Priest, Benjamin W. and Goumiri, Im{\\`e}ne and Schneider, Michael},\n  journal={arXiv preprint arXiv:2104.14581},\n  year={2021}\n}\n\n```\n\n## License\n\nMuyGPyS is distributed under the terms of the MIT license.\nAll new contributions must be made under the MIT license.\n\nSee [LICENSE-MIT](LICENSE-MIT), [NOTICE](NOTICE), and [COPYRIGHT](COPYRIGHT) for details.\n\nSPDX-License-Identifier: MIT\n\n## Release\n\nLLNL-CODE-824804\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://pypi.org/project/muygpys",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/LLNL/MuyGPyS",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "muygps",
    "package_url": "https://pypi.org/project/muygps/",
    "platform": "",
    "project_url": "https://pypi.org/project/muygps/",
    "project_urls": {
      "Bug Tracker": "https://github.com/LLNL/MuyGPyS/issues",
      "Documentation": "https://muygpys.readthedocs.io",
      "Download": "https://pypi.org/project/muygpys",
      "Homepage": "https://github.com/LLNL/MuyGPyS",
      "Source Code": "https://github.com/LLNL/MuyGPyS"
    },
    "release_url": "https://pypi.org/project/muygps/0.3.0/",
    "requires_dist": [
      "numpy (>=1.18.5)",
      "scipy (==1.4.1)",
      "scikit-learn (>=0.23.2)",
      "absl-py (>=0.13.0)",
      "pybind11 (>=2.5.0)",
      "hnswlib (>=0.5.2)"
    ],
    "requires_python": ">=3.6",
    "summary": "Scalable Approximate Gaussian Process using Local Kriging",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11073908,
  "releases": {
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63e81fc65b963f7613644685d61486fe65986a1374b3f021295af4127707e414",
          "md5": "e761645a27d0485f23004d1336608692",
          "sha256": "3741021afdfde8b92b05c3e6dafc702d983b1da967bca94a23d53c57c06f1808"
        },
        "downloads": -1,
        "filename": "muygps-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e761645a27d0485f23004d1336608692",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 54307,
        "upload_time": "2021-08-02T22:25:14",
        "upload_time_iso_8601": "2021-08-02T22:25:14.475501Z",
        "url": "https://files.pythonhosted.org/packages/63/e8/1fc65b963f7613644685d61486fe65986a1374b3f021295af4127707e414/muygps-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "406a337d3573fb093cb68f361b50cf032ae01b2d00703bb47c526b1c9403c212",
          "md5": "ec996b7311fe07186fd13662479d62ae",
          "sha256": "029e203c35def736c4a541a305fb4d963f36ac812b0205f002d3f889c361d2e6"
        },
        "downloads": -1,
        "filename": "muygps-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ec996b7311fe07186fd13662479d62ae",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 43575,
        "upload_time": "2021-08-02T22:25:16",
        "upload_time_iso_8601": "2021-08-02T22:25:16.364067Z",
        "url": "https://files.pythonhosted.org/packages/40/6a/337d3573fb093cb68f361b50cf032ae01b2d00703bb47c526b1c9403c212/muygps-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "63e81fc65b963f7613644685d61486fe65986a1374b3f021295af4127707e414",
        "md5": "e761645a27d0485f23004d1336608692",
        "sha256": "3741021afdfde8b92b05c3e6dafc702d983b1da967bca94a23d53c57c06f1808"
      },
      "downloads": -1,
      "filename": "muygps-0.3.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e761645a27d0485f23004d1336608692",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 54307,
      "upload_time": "2021-08-02T22:25:14",
      "upload_time_iso_8601": "2021-08-02T22:25:14.475501Z",
      "url": "https://files.pythonhosted.org/packages/63/e8/1fc65b963f7613644685d61486fe65986a1374b3f021295af4127707e414/muygps-0.3.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "406a337d3573fb093cb68f361b50cf032ae01b2d00703bb47c526b1c9403c212",
        "md5": "ec996b7311fe07186fd13662479d62ae",
        "sha256": "029e203c35def736c4a541a305fb4d963f36ac812b0205f002d3f889c361d2e6"
      },
      "downloads": -1,
      "filename": "muygps-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ec996b7311fe07186fd13662479d62ae",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 43575,
      "upload_time": "2021-08-02T22:25:16",
      "upload_time_iso_8601": "2021-08-02T22:25:16.364067Z",
      "url": "https://files.pythonhosted.org/packages/40/6a/337d3573fb093cb68f361b50cf032ae01b2d00703bb47c526b1c9403c212/muygps-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}