{
  "info": {
    "author": "Yonti Levin",
    "author_email": "therealyontilevin@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python"
    ],
    "description": "go to github for more info",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/yontilevin/hebrew_tokenizer",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hebrew-tokenizer",
    "package_url": "https://pypi.org/project/hebrew-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/hebrew-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/yontilevin/hebrew_tokenizer"
    },
    "release_url": "https://pypi.org/project/hebrew-tokenizer/2.3.0/",
    "requires_dist": null,
    "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
    "summary": "A very simple python tokenizer for Hebrew text",
    "version": "2.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12014839,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9abb5b4bd03848f6c5dd29873fb775ea8464753adaac5ecb15fd7007f044e2f7",
          "md5": "04b65b8b63f71709a7531982ec8068b3",
          "sha256": "75c1efa972e60ac75ec9f71c23c9854baf0d34256c699c06b1115b1e2467eed7"
        },
        "downloads": -1,
        "filename": "hebrew_tokenizer-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "04b65b8b63f71709a7531982ec8068b3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 3886,
        "upload_time": "2019-03-07T07:38:03",
        "upload_time_iso_8601": "2019-03-07T07:38:03.967886Z",
        "url": "https://files.pythonhosted.org/packages/9a/bb/5b4bd03848f6c5dd29873fb775ea8464753adaac5ecb15fd7007f044e2f7/hebrew_tokenizer-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "86d90ca8b9877f8b1e40c34047863251f3bff93dbd768946fdb90029484dd124",
          "md5": "21bba75482930af83d34acd66a9d03b1",
          "sha256": "1eae458337bc55fa70bfb10fb74dbc2be9e1009c8121c01f3f18c0d993471e37"
        },
        "downloads": -1,
        "filename": "hebrew_tokenizer-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "21bba75482930af83d34acd66a9d03b1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 3827,
        "upload_time": "2020-01-06T19:44:05",
        "upload_time_iso_8601": "2020-01-06T19:44:05.019691Z",
        "url": "https://files.pythonhosted.org/packages/86/d9/0ca8b9877f8b1e40c34047863251f3bff93dbd768946fdb90029484dd124/hebrew_tokenizer-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "93e62a24755f1361a503821b13808adfefac5d2753c0614b96b94b54dd53f2de",
          "md5": "62a9d603ece7891d88f4f5ce9e619245",
          "sha256": "829191abb65455b0197842dfb14ebf6560e294b5f5974083367faf70284612e4"
        },
        "downloads": -1,
        "filename": "hebrew_tokenizer-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "62a9d603ece7891d88f4f5ce9e619245",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 4129,
        "upload_time": "2020-01-19T08:17:14",
        "upload_time_iso_8601": "2020-01-19T08:17:14.048609Z",
        "url": "https://files.pythonhosted.org/packages/93/e6/2a24755f1361a503821b13808adfefac5d2753c0614b96b94b54dd53f2de/hebrew_tokenizer-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "738ec0566f6a25cfdabaeae1dd77aa470e178607e2185808f9db6298f4192970",
          "md5": "bbc566e7dd83483060fd67f39adeefc9",
          "sha256": "2b64ca7d2e6e7d6c477d2f9a17bc1430c72dc265c473824c3ad5ce80b34da712"
        },
        "downloads": -1,
        "filename": "hebrew_tokenizer-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bbc566e7dd83483060fd67f39adeefc9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 7725,
        "upload_time": "2020-06-11T14:41:48",
        "upload_time_iso_8601": "2020-06-11T14:41:48.444517Z",
        "url": "https://files.pythonhosted.org/packages/73/8e/c0566f6a25cfdabaeae1dd77aa470e178607e2185808f9db6298f4192970/hebrew_tokenizer-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fe74efd9c94145d5c553ee6b736b022490a941b3ede5a401cb8e1d28388af3fc",
          "md5": "9695eb923e76be0acd90ef6b4e9381f5",
          "sha256": "519bddd502f41ba2eb2e15c14aaea2b88236366cdd847ae37804e2344335ca22"
        },
        "downloads": -1,
        "filename": "hebrew_tokenizer-2.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "9695eb923e76be0acd90ef6b4e9381f5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 7108,
        "upload_time": "2020-10-23T19:23:18",
        "upload_time_iso_8601": "2020-10-23T19:23:18.167923Z",
        "url": "https://files.pythonhosted.org/packages/fe/74/efd9c94145d5c553ee6b736b022490a941b3ede5a401cb8e1d28388af3fc/hebrew_tokenizer-2.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "637227c4de2b535044a0cc97bcdffea20d5290e3b8531db7e7a575ea7f440386",
          "md5": "2fd865481dffa3be5f0fe0d21ded5fe6",
          "sha256": "f0a88dccc1945414c6fcbe4937b391f66408dcd140614ffd2db3ebf724b1573a"
        },
        "downloads": -1,
        "filename": "hebrew_tokenizer-2.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2fd865481dffa3be5f0fe0d21ded5fe6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 11380,
        "upload_time": "2021-04-28T06:29:20",
        "upload_time_iso_8601": "2021-04-28T06:29:20.846452Z",
        "url": "https://files.pythonhosted.org/packages/63/72/27c4de2b535044a0cc97bcdffea20d5290e3b8531db7e7a575ea7f440386/hebrew_tokenizer-2.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ae5402dec97447aa653341818e80b427ec6ec9f267aa3c6409fec0ac906b730",
          "md5": "37ade3d75f5ab566ef7c308194950db3",
          "sha256": "a03030212e867870796612e575fdb8ab042948f8fa9fbd55246410b6e7743ff3"
        },
        "downloads": -1,
        "filename": "hebrew_tokenizer-2.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "37ade3d75f5ab566ef7c308194950db3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 11570,
        "upload_time": "2021-10-23T13:02:28",
        "upload_time_iso_8601": "2021-10-23T13:02:28.063370Z",
        "url": "https://files.pythonhosted.org/packages/5a/e5/402dec97447aa653341818e80b427ec6ec9f267aa3c6409fec0ac906b730/hebrew_tokenizer-2.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8735a56b3f07eacd06cfded8ec18d9ebe3d6a4c63d31d056f076d860930b1a70",
          "md5": "44f464d9aac2042004bab8525e848a60",
          "sha256": "ce96a160baa001785f52eeb095dfb9d219cf6d70654d3a8ac3aec5bedf969ace"
        },
        "downloads": -1,
        "filename": "hebrew_tokenizer-2.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "44f464d9aac2042004bab8525e848a60",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
        "size": 13135,
        "upload_time": "2021-11-13T21:21:50",
        "upload_time_iso_8601": "2021-11-13T21:21:50.682514Z",
        "url": "https://files.pythonhosted.org/packages/87/35/a56b3f07eacd06cfded8ec18d9ebe3d6a4c63d31d056f076d860930b1a70/hebrew_tokenizer-2.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8735a56b3f07eacd06cfded8ec18d9ebe3d6a4c63d31d056f076d860930b1a70",
        "md5": "44f464d9aac2042004bab8525e848a60",
        "sha256": "ce96a160baa001785f52eeb095dfb9d219cf6d70654d3a8ac3aec5bedf969ace"
      },
      "downloads": -1,
      "filename": "hebrew_tokenizer-2.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "44f464d9aac2042004bab8525e848a60",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=2.6, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*",
      "size": 13135,
      "upload_time": "2021-11-13T21:21:50",
      "upload_time_iso_8601": "2021-11-13T21:21:50.682514Z",
      "url": "https://files.pythonhosted.org/packages/87/35/a56b3f07eacd06cfded8ec18d9ebe3d6a4c63d31d056f076d860930b1a70/hebrew_tokenizer-2.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}