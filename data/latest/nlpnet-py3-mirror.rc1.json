{
  "info": {
    "author": "Erick Fonseca,Bruno Medina",
    "author_email": "bruno@neurologic.com.br",
    "bugtrack_url": null,
    "classifiers": [],
    "description": ".. image:: https://badges.gitter.im/Join%20Chat.svg\n   :alt: Join the chat at https://gitter.im/erickrf/nlpnet\n   :target: https://gitter.im/erickrf/nlpnet?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n   \nGitter is chat room for developers.\n\n===============================================================\n``nlpnet`` --- Natural Language Processing with neural networks\n===============================================================\n\n``nlpnet`` is a Python library for Natural Language Processing tasks based on neural networks. \nCurrently, it performs part-of-speech tagging, semantic role labeling and dependency parsing. \nMost of the architecture is language independent, but some functions were specially tailored for working\nwith Portuguese. This system was inspired by SENNA_.\n\n.. _SENNA: http://ronan.collobert.com/senna/\n\n**Important:** in order to use the trained models for Portuguese NLP, you will need to download the data from http://nilc.icmc.usp.br/nlpnet/models.html.\n\nDependencies\n------------\n\n``nlpnet`` requires NLTK_ and numpy_. Additionally, it needs to download some data from NLTK. After installing it, call\n\n    >>> nltk.download()\n\ngo to the `Models` tab and select the Punkt tokenizer. It is used in order to split the text into sentences.\n\nCython_ is used to generate C extensions and run faster. You probably won't need it, since the generated ``.c`` file is already provided with `nlpnet`, but you will need a C compiler. On Linux and Mac systems this shouldn't be a problem, but may be on Windows, because  setuptools_ requires the Microsoft C Compiler by default. If you don't have it already, it is usually easier to install MinGW_ instead and follow the instructions `here <http://docs.cython.org/src/tutorial/appendix.html>`_.\n\n.. _NLTK: http://www.nltk.org\n.. _numpy: http://www.numpy.org\n.. _Cython: http://cython.org\n.. _MinGW: http://www.mingw.org\n.. _setuptools: http://pythonhosted.org/setuptools/\n\nBasic usage\n-----------\n\n``nlpnet`` can be used both as a Python library or by its standalone scripts. Both usages are explained below.\n\nLibrary usage\n~~~~~~~~~~~~~\n\nYou can use ``nlpnet`` as a library in Python code as follows:\n\n.. code-block:: python\n\n    >>> import nlpnet\n    >>> tagger = nlpnet.POSTagger('/path/to/pos-model/', language='pt')\n    >>> tagger.tag('O rato roeu a roupa do rei de Roma.')\n    [[(u'O', u'ART'), (u'rato', u'N'), (u'roeu', u'V'), (u'a', u'ART'), (u'roupa', u'N'), (u'do', u'PREP+ART'), (u'rei', u'N'), (u'de', u'PREP'), (u'Roma', u'NPROP'), (u'.', 'PU')]]\n\nIn the example above, the ``POSTagger`` constructor receives as the first argument the directory where its trained model is located. The second argument is the two letter language code (currently, onle ``pt`` and ``en`` are supported). This only has impact in tokenization.\n\nCalling an annotation tool is pretty straightforward. The provided ones are ``POSTagger``, ``SRLTagger`` and ``DependencyParser``, all of them having a method ``tag`` which receives strings with text to be tagged (in ``DependencyParser``, there is an alias to the method ``parse``, which sounds more adequate). The tagger splits the text into sentences and then tokenizes each one (hence the return of the POSTagger is a list of lists).\n\nThe output of the SRLTagger is slightly more complicated:\n\n    >>> tagger = nlpnet.SRLTagger()\n    >>> tagger.tag(u'O rato roeu a roupa do rei de Roma.')\n    [<nlpnet.taggers.SRLAnnotatedSentence at 0x84020f0>]\n\nInstead of a list of tuples, sentences are represented by instances of ``SRLAnnotatedSentence``. This class serves basically as a data holder, and has two attributes:\n\n    >>> sent = tagger.tag(u'O rato roeu a roupa do rei de Roma.')[0]\n    >>> sent.tokens\n    [u'O', u'rato', u'roeu', u'a', u'roupa', u'do', u'rei', u'de', u'Roma', u'.']\n    >>> sent.arg_structures\n    [(u'roeu',\n      {u'A0': [u'O', u'rato'],\n       u'A1': [u'a', u'roupa', u'do', u'rei', u'de', u'Roma'],\n       u'V': [u'roeu']})]\n\nThe ``arg_structures`` is a list containing all predicate-argument structures in the sentence. The only one in this example is for the verb `roeu`. It is represented by a tuple with the predicate and a dictionary mapping semantic role labels to the tokens that constitute the argument.\n\nNote that the verb appears as the first member of the tuple and also as the content of label 'V' (which stands for verb). This is because some predicates are multiwords. In these cases, the \"main\" predicate word (usually the verb itself) appears in ``arg_structures[0]``, and all the words appear under the key 'V'.\n\nHere's an example with the DependencyParser:\n\n    >>> parser = nlpnet.DependencyParser('dependency', language='en')\n    >>> parsed_text = parser.parse('The book is on the table.')\n    >>> parsed_text\n    [<nlpnet.taggers.ParsedSentence at 0x10e067f0>]\n    >>> sent = parsed_text[0]\n    >>> print(sent.to_conll())\n    1       The     _       DT      DT      _       2       NMOD\n    2       book    _       NN      NN      _       3       SBJ\n    3       is      _       VBZ     VBZ     _       0       ROOT\n    4       on      _       IN      IN      _       3       LOC-PRD\n    5       the     _       DT      DT      _       6       NMOD\n    6       table   _       NN      NN      _       4       PMOD\n    7       .       _       .       .       _       3       P\n\nThe ``to_conll()`` method of ParsedSentence objects prints them in the `CoNLL`_ notation. The tokens, labels and head indices are accessible through member variables:\n\n    >>> sent.tokens\n    [u'The', u'book', u'is', u'on', u'the', u'table', u'.']\n    >>> sent.heads\n    array([ 1,  2, -1,  2,  5,  3,  2])\n    >>> sent.labels\n    [u'NMOD', u'SBJ', u'ROOT', u'LOC-PRD', u'NMOD', u'PMOD', u'P']\n    \nThe ``heads`` member variable is a numpy array. The i-th position in the array contains the index of the head of the i-th token, except for the root token, which has a head of -1. Notice that these indices are 0-based, while the ones shown in the ``to_conll()`` function are 1-based.\n\n.. _`CoNLL`: http://ilk.uvt.nl/conll/#dataformat\n\nStandalone scripts\n~~~~~~~~~~~~~~~~~~\n\n``nlpnet`` also provides scripts for tagging text, training new models and testing them. They are copied to the `scripts` subdirectory of your Python installation, which can be included in the system PATH variable. You can call them from command line and give some text input.\n\n.. code-block:: bash\n\n    $ nlpnet-tag.py pos --data /path/to/nlpnet-data/ --lang pt\n    O rato roeu a roupa do rei de Roma.\n    O_ART rato_N roeu_V a_ART roupa_N do_PREP+ART rei_N de_PREP Roma_NPROP ._PU\n\nIf ``--data`` is not given, the script will search for the trained models in the current directory. ``--lang`` defaults to ``en``. If you have text already tokenized, you may use the ``-t`` option; it assumes tokens are separated by whitespaces.\n    \nWith semantic role labeling:\n\n.. code-block:: bash\n\n    $ nlpnet-tag.py srl /path/to/nlpnet-data/\n    O rato roeu a roupa do rei de Roma.\n    O rato roeu a roupa do rei de Roma .\n    roeu\n        A1: a roupa do rei de Roma\n        A0: O rato\n        V: roeu\n\nThe first line was typed by the user, and the second one is the result of tokenization.\n\nAnd dependency parsing:\n\n.. code-block:: bash\n\n    $ nlpnet-tag.py dependency --data dependency --lang en\n    The book is on the table.\n    1       The     _       DT      DT      _       2       NMOD\n    2       book    _       NN      NN      _       3       SBJ\n    3       is      _       VBZ     VBZ     _       0       ROOT\n    4       on      _       IN      IN      _       3       LOC-PRD\n    5       the     _       DT      DT      _       6       NMOD\n    6       table   _       NN      NN      _       4       PMOD\n    7       .       _       .       .       _       3       P\n\nTo learn more about training and testing new models, and other functionalities, refer to the documentation at http://nilc.icmc.usp.br/nlpnet\n\nReferences\n----------\n\nThe following references describe the design of nlpnet, as well as experiments carried out. Some improvements to the code have been implemented since their publication.\n\n* Fonseca, Erick and Aluísio, Sandra M. **A Deep Architecture for Non-Projective Dependency Parsing**. Proceedings of the NAACL-HLT Workshop on Vector Space Modeling for NLP. 2015\n\n* Fonseca, Erick and Rosa, João Luís G. **A Two-Step Convolutional Neural Network Approach for Semantic Role Labeling**. Proceedings of the International Joint Conference on Neural Networks. 2013.\n\n* Fonseca, Erick, Rosa, João Luís G. and Aluísio, Sandra M. **Evaluating word embeddings and a revised corpus for part-of-speech tagging in Portuguese**. Journal of The Brazilian Computer Society. 2015.",
    "description_content_type": null,
    "docs_url": null,
    "download_url": "UNKNOWN",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://nilc.icmc.usp.br/nlpnet",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "nlpnet-py3-mirror",
    "package_url": "https://pypi.org/project/nlpnet-py3-mirror/",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/nlpnet-py3-mirror/",
    "project_urls": {
      "Download": "UNKNOWN",
      "Homepage": "http://nilc.icmc.usp.br/nlpnet"
    },
    "release_url": "https://pypi.org/project/nlpnet-py3-mirror/1.2.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Neural networks for NLP tasks",
    "version": "1.2.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 1940053,
  "releases": {
    "1.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c75f9e72a7172062011374f7e5f60a287d877ccf54b0cd1299cc74b6c233f53b",
          "md5": "e47118bfbeb73c576539b98c2ba14f9f",
          "sha256": "0a3e2964838197eafdc76f1a16d03a85680457d90dcb6235dd3e385b2751a3a4"
        },
        "downloads": -1,
        "filename": "nlpnet-py3-mirror-1.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "e47118bfbeb73c576539b98c2ba14f9f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 336041,
        "upload_time": "2016-02-04T17:15:32",
        "upload_time_iso_8601": "2016-02-04T17:15:32.600879Z",
        "url": "https://files.pythonhosted.org/packages/c7/5f/9e72a7172062011374f7e5f60a287d877ccf54b0cd1299cc74b6c233f53b/nlpnet-py3-mirror-1.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c75f9e72a7172062011374f7e5f60a287d877ccf54b0cd1299cc74b6c233f53b",
        "md5": "e47118bfbeb73c576539b98c2ba14f9f",
        "sha256": "0a3e2964838197eafdc76f1a16d03a85680457d90dcb6235dd3e385b2751a3a4"
      },
      "downloads": -1,
      "filename": "nlpnet-py3-mirror-1.2.2.tar.gz",
      "has_sig": false,
      "md5_digest": "e47118bfbeb73c576539b98c2ba14f9f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 336041,
      "upload_time": "2016-02-04T17:15:32",
      "upload_time_iso_8601": "2016-02-04T17:15:32.600879Z",
      "url": "https://files.pythonhosted.org/packages/c7/5f/9e72a7172062011374f7e5f60a287d877ccf54b0cd1299cc74b6c233f53b/nlpnet-py3-mirror-1.2.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}