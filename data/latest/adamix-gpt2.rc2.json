{
  "info": {
    "author": "Sahaj Agarwal",
    "author_email": "sahagar@microsoft.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Adapting GPT-2 using LoRA, Adapters and Adamix\n\nThis folder contains the implementation of LoRA, Adamix Adapter and Adamix LoRA in GPT-2 using the modiifed Python package `lora` and steps to replicate the results in our recent paper\n\n<p>\n<img src=\"https://github.com/microsoft/AdaMix/raw/main/NLG/figures/Results.png\" width=\"800\" >\n</p>\n\nThis repo reproduces our experiments on GPT-2 Medium.\n\n## Repository Overview\n\nOur implementation is based on the fine-tuning code for GPT-2 in [Hugging Face](https://huggingface.co/).\nThere are several directories in this repo:\n* [src/](src) contains the source code used for data processing, training, and decoding.\n* [eval/](eval) contains the code for task-specific evaluation scripts.\n* [data/](data) contains the raw data we used in our experiments.\n* [vocab/](vocab) contains the GPT-2 vocabulary files.\n\n## Getting Started\n\n 1. You can start with the following docker image: `nvcr.io/nvidia/pytorch:20.03-py3` on a GPU-capable machine, but any generic PyTorch image should work.\n ```\n docker run -it nvcr.io/nvidia/pytorch:20.03-py3\n ```\n\n 2. Clone the repo and install dependencies using the provided setup script in a virtual environment (remove sudo wherever necessary if running in docker container):\n ```\n bash setup.sh\n ```\n\n#### Now we are ready to replicate the results in our paper.\n\n## Replicating Our Results on GPT-2 Medium\n#### (see our paper for hyperparameters for GPT-2 Medium) \n\n1. Train GPT-2 Medium on E2E NLG Challenge dataset\n\n- LoRA\n\n    ```\n    python -m torch.distributed.launch --nproc_per_node=1 src/gpt2_ft.py \\\n    --train_data ./data/e2e/train.jsonl \\\n    --valid_data ./data/e2e/valid.jsonl \\\n    --train_batch_size 8 \\\n    --grad_acc 1 \\\n    --valid_batch_size 4 \\\n    --seq_len 512 \\\n    --model_card gpt2.md \\\n    --init_checkpoint ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin \\\n    --platform local \\\n    --clip 0.0 \\\n    --lr 0.0002 \\\n    --weight_decay 0.01 \\\n    --correct_bias \\\n    --adam_beta2 0.999 \\\n    --scheduler linear \\\n    --warmup_step 500 \\\n    --max_epoch 5 \\\n    --save_interval 1000 \\\n    --lora_dim 4 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.1 \\\n    --label_smooth 0.1 \\\n    --work_dir ./trained_models/GPT2_M/e2e/lora_only \\\n    --random_seed 110 \\\n    --lora_only 1\n    ```\n\n- Adapter with Adamix\n    ```\n    python -m torch.distributed.launch --nproc_per_node=1 src/gpt2_ft.py \\\n    --train_data ./data/e2e/train.jsonl \\\n    --valid_data ./data/e2e/valid.jsonl \\\n    --train_batch_size 8 \\\n    --grad_acc 1 \\\n    --valid_batch_size 4 \\\n    --seq_len 512 \\\n    --model_card gpt2.md \\\n    --init_checkpoint ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin \\\n    --platform local \\\n    --clip 0.0 \\\n    --lr 0.0002 \\\n    --weight_decay 0.01 \\\n    --correct_bias \\\n    --adam_beta2 0.999 \\\n    --scheduler linear \\\n    --warmup_step 2000 \\\n    --max_epoch 20 \\\n    --eval_interval 5000 \\\n    --save_interval 5000 \\\n    --lora_dim 4 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.1 \\\n    --label_smooth 0.1 \\\n    --work_dir ./trained_models/GPT2_M/e2e/adapter_adamix \\\n    --random_seed 110 \\\n    --adamix_only 1 \\\n    --n_experts 8 \\\n    --share_A 0 \\\n    --share_B 1\n    ```\n\n- LoRA with Adamix\n    ```\n    python -m torch.distributed.launch --nproc_per_node=1 src/gpt2_ft.py \\\n    --train_data ./data/e2e/train.jsonl \\\n    --valid_data ./data/e2e/valid.jsonl \\\n    --train_batch_size 8 \\\n    --grad_acc 1 \\\n    --valid_batch_size 4 \\\n    --seq_len 512 \\\n    --model_card gpt2.md \\\n    --init_checkpoint ./pretrained_checkpoints/gpt2-medium-pytorch_model.bin \\\n    --platform local \\\n    --clip 0.0 \\\n    --lr 0.0002 \\\n    --weight_decay 0.01 \\\n    --correct_bias \\\n    --adam_beta2 0.999 \\\n    --scheduler linear \\\n    --warmup_step 2000 \\\n    --max_epoch 20 \\\n    --eval_interval 5000 \\\n    --save_interval 5000 \\\n    --lora_dim 4 \\\n    --lora_alpha 32 \\\n    --lora_dropout 0.1 \\\n    --label_smooth 0.1 \\\n    --work_dir ./trained_models/GPT2_M/e2e/lora_adamix \\\n    --random_seed 110 \\\n    --n_experts 8 \\\n    --share_A 0 \\\n    --share_B 1\n    ```\n\n2. Generate outputs from the trained model using beam search (LoRA with Adamix):\n    ```\n    python -m torch.distributed.launch --nproc_per_node=1 src/gpt2_beam.py \\\n    --data ./data/e2e/test.jsonl \\\n    --batch_size 1 \\\n    --seq_len 128 \\\n    --eval_len 64 \\\n    --model_card gpt2.md \\\n    --init_checkpoint ./trained_models/GPT2_M/e2e/lora_adamix/model.final.pt \\\n    --platform local \\\n    --lora_dim 4 \\\n    --lora_alpha 32 \\\n    --beam 10 \\\n    --length_penalty 0.8 \\\n    --no_repeat_ngram_size 4 \\\n    --repetition_penalty 1.0 \\\n    --eos_token_id 628 \\\n    --work_dir ./trained_models/GPT2_M/e2e/lora_adamix \\\n    --output_file predict.jsonl \\\n    --n_experts 8 \\\n    --share_A 0 \\\n    --share_B 1\n    ```\n\n3. Decode outputs from step (2)\n    ```\n    python src/gpt2_decode.py \\\n    --vocab ./vocab \\\n    --sample_file ./trained_models/GPT2_M/e2e/lora_adamix/predict.jsonl \\\n    --input_file ./data/e2e/test_formatted.jsonl \\\n    --output_ref_file e2e_ref.txt \\\n    --output_pred_file e2e_pred.txt\n    ```\n\n4. Run evaluation on E2E test set\n    ```\n    python eval/e2e/measure_scores.py e2e_ref.txt e2e_pred.txt -p\n    ```\n\n### Replicating Our Result on WebNLG\n\n1. Follow steps 1 and 2 from E2E pipeline by replacing references to E2E with webnlg (see our paper for hyperparameters)\n\n2. Decode outputs from beam search (step 2 above)\n    ```\n    python src/gpt2_decode.py \\\n        --vocab ./vocab \\\n        --sample_file ./trained_models/GPT2_M/webnlg/lora_adamix/predict.jsonl \\\n        --input_file ./data/webnlg_challenge_2017/test_formatted.jsonl \\\n        --ref_type webnlg \\\n        --ref_num 6 \\\n        --output_ref_file eval/GenerationEval/data/references_webnlg \\\n        --output_pred_file eval/GenerationEval/data/hypothesis_webnlg \\\n        --tokenize --lower\n    ```\n\n3. Run evaluation on WebNLG test set\n    ```\n    cd ./eval/GenerationEval/\n    python eval.py \\\n        -R data/references_webnlg/reference \\\n        -H data/hypothesis_webnlg \\\n        -nr 6 \\\n        -m bleu,meteor,ter \n    cd ../..\n    ```\n\n### Replicating Our Result on DART\n\n1. Follow steps 1 and 2 from E2E pipeline by replacing references to E2E with dart (see our paper for hyperparameters)\n\n2. Decode outputs from beam search (step 2 above)\n    ```\n    python src/gpt2_decode.py \\\n            --vocab ./vocab \\\n            --sample_file ./trained_models/GPT2_M/dart/lora_adamix/predict.jsonl \\\n            --input_file ./data/dart/test_formatted.jsonl \\\n            --ref_type dart \\\n            --ref_num 6 \\\n            --output_ref_file eval/GenerationEval/data/references_dart \\\n            --output_pred_file eval/GenerationEval/data/hypothesis_dart \\\n            --tokenize --lower\n    ```\n\n3. Run evaluation on Dart test set\n    ```\n    cd ./eval/GenerationEval/\n    python eval.py \\\n        -R data/references_dart/reference \\\n        -H data/hypothesis_dart \\\n        -nr 6 \\\n        -m bleu,meteor,ter \n    cd ../..\n    ```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/microsoft/AdaMix",
    "keywords": "python,adamix,peft",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "adamix-gpt2",
    "package_url": "https://pypi.org/project/adamix-gpt2/",
    "platform": null,
    "project_url": "https://pypi.org/project/adamix-gpt2/",
    "project_urls": {
      "Homepage": "https://github.com/microsoft/AdaMix"
    },
    "release_url": "https://pypi.org/project/adamix-gpt2/0.0.2/",
    "requires_dist": [
      "transformers (==3.3.1)",
      "spacy",
      "tqdm",
      "tensorboard",
      "progress",
      "loralib (==0.1.1)"
    ],
    "requires_python": ">=3.6",
    "summary": "PyTorch implementation of low-rank adaptation (LoRA) and Adamix, a parameter-efficient approach to adapt a large pre-trained deep learning model which obtains performance better than full fine-tuning.",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14968492,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7f7f240da6f7929f765db6f0142c0ebd6dc3493877890652708a1b14ac047768",
          "md5": "d3988b70e552e659be8e10fe00f3fce7",
          "sha256": "694eb83399c74529aa34b6103b2e51c1fbc5e0ee89257a33d97eac7300c40421"
        },
        "downloads": -1,
        "filename": "adamix_gpt2-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d3988b70e552e659be8e10fe00f3fce7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 37359,
        "upload_time": "2022-08-31T21:36:45",
        "upload_time_iso_8601": "2022-08-31T21:36:45.399935Z",
        "url": "https://files.pythonhosted.org/packages/7f/7f/240da6f7929f765db6f0142c0ebd6dc3493877890652708a1b14ac047768/adamix_gpt2-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9901ec94715301ad1454b275c555b2358b339d9ed7e745925b0f26cec01e7e5a",
          "md5": "39eedda2435cf76a30f92f389c7823c3",
          "sha256": "b3d10e2eed7cf229195556bbcb46880bb2f67b1a7a9af880b5f0d79d50ad2b2b"
        },
        "downloads": -1,
        "filename": "adamix_gpt2-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "39eedda2435cf76a30f92f389c7823c3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 31436,
        "upload_time": "2022-08-31T21:36:46",
        "upload_time_iso_8601": "2022-08-31T21:36:46.827265Z",
        "url": "https://files.pythonhosted.org/packages/99/01/ec94715301ad1454b275c555b2358b339d9ed7e745925b0f26cec01e7e5a/adamix_gpt2-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e7f2f29456a339e6f1bd2d57b2afb808e3cd28a79b26d1d7badeeeeb69e5e4fe",
          "md5": "0c462b0f65f4801603da9d4f3fff53e7",
          "sha256": "9fa4b6a30be905d5b0fd4dc733d78a5cddc7d112a579610a61befbf14eafb4f4"
        },
        "downloads": -1,
        "filename": "adamix_gpt2-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0c462b0f65f4801603da9d4f3fff53e7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 37369,
        "upload_time": "2022-09-02T00:08:45",
        "upload_time_iso_8601": "2022-09-02T00:08:45.017432Z",
        "url": "https://files.pythonhosted.org/packages/e7/f2/f29456a339e6f1bd2d57b2afb808e3cd28a79b26d1d7badeeeeb69e5e4fe/adamix_gpt2-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ab5c625b20fc436c8e978bf0f215078814bb94cb40981a7a99c5fcf5930d07ea",
          "md5": "ab60efd5a06831ba82f937a9e84d227b",
          "sha256": "b7d4db5ee496b3ba60dd951d03e1b9cba4cac175152202994df1b349baab9ab5"
        },
        "downloads": -1,
        "filename": "adamix_gpt2-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "ab60efd5a06831ba82f937a9e84d227b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 31455,
        "upload_time": "2022-09-02T00:08:46",
        "upload_time_iso_8601": "2022-09-02T00:08:46.753896Z",
        "url": "https://files.pythonhosted.org/packages/ab/5c/625b20fc436c8e978bf0f215078814bb94cb40981a7a99c5fcf5930d07ea/adamix_gpt2-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e7f2f29456a339e6f1bd2d57b2afb808e3cd28a79b26d1d7badeeeeb69e5e4fe",
        "md5": "0c462b0f65f4801603da9d4f3fff53e7",
        "sha256": "9fa4b6a30be905d5b0fd4dc733d78a5cddc7d112a579610a61befbf14eafb4f4"
      },
      "downloads": -1,
      "filename": "adamix_gpt2-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0c462b0f65f4801603da9d4f3fff53e7",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 37369,
      "upload_time": "2022-09-02T00:08:45",
      "upload_time_iso_8601": "2022-09-02T00:08:45.017432Z",
      "url": "https://files.pythonhosted.org/packages/e7/f2/f29456a339e6f1bd2d57b2afb808e3cd28a79b26d1d7badeeeeb69e5e4fe/adamix_gpt2-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ab5c625b20fc436c8e978bf0f215078814bb94cb40981a7a99c5fcf5930d07ea",
        "md5": "ab60efd5a06831ba82f937a9e84d227b",
        "sha256": "b7d4db5ee496b3ba60dd951d03e1b9cba4cac175152202994df1b349baab9ab5"
      },
      "downloads": -1,
      "filename": "adamix_gpt2-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "ab60efd5a06831ba82f937a9e84d227b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 31455,
      "upload_time": "2022-09-02T00:08:46",
      "upload_time_iso_8601": "2022-09-02T00:08:46.753896Z",
      "url": "https://files.pythonhosted.org/packages/ab/5c/625b20fc436c8e978bf0f215078814bb94cb40981a7a99c5fcf5930d07ea/adamix_gpt2-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}