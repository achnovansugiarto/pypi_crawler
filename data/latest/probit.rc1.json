{
  "info": {
    "author": "Benjamin Boys, Toby Boyne, Ieronymos Maxoutis",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# [probit](http://github.com/bb515/probit)\n[![CI](https://github.com/bb515/diffusionjax/actions/workflows/CI.yml/badge.svg)](https://github.com/bb515/diffusionjax/actions/workflows/CI.yml)\n[![Coverage Status](https://coveralls.io/repos/github/bb515/diffusionjax/badge.svg?branch=master)](https://coveralls.io/github/bb515/diffusionjax?branch=master)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nprobit is a simple and accessible Gaussian process package in Jax.\n\nprobit uses [MLKernels](https://github.com/wesselb/mlkernels) for the GP prior, see the available [means](https://github.com/wesselb/mlkernels#available-means) and [kernels](https://github.com/wesselb/mlkernels#available-kernels) with [compositional design](https://github.com/wesselb/mlkernels#compositional-design).\n\nContents:\n\n- [Installation](#installation)\n- [Examples](#examples)\n    - [Regression and hyperparameter optimization](#regression-and-hyperparameter-optimization)\n    - [Ordinal regression and hyperparameter optimization](#ordinal-regression-and-hyperparameter-optimization)\n- [Doesn't haves](#doesnt-haves)\n- [References](#references)\n\nTLDR:\n```python\n>>> from probit.approximators import LaplaceGP as GP\n>>> from probit.utilities import log_gaussian_likelihood\n>>> from mlkernels import EQ\n>>>\n>>> def prior(prior_parameters):\n>>>     lengthscale, signal_variance = prior_parameters\n>>>     # Here you can define the kernel that defines the Gaussian process\n>>>     return signal_variance * EQ().stretch(lengthscale).periodic(0.5)\n>>>\n>>> gaussian_process = GP(data=(X, y), prior=prior, log_likelihood=log_gaussian_likelihood)\n>>> likelihood_parameters = 1.0\n>>> prior_parameters = (1.0, 1.0)\n>>> parameters = (likelihood_parameters, prior_parameters)\n>>> weight, precision = gaussian_process.approximate_posterior(parameters)\n>>> predictive_mean, predictive_variance = gaussian_process.predict(\n>>>     X_test,\n>>>     parameters, weight, precision)\n```\n\n\n## Installation\n`pip install probit` or for developers,\n- Clone the repository `git clone git@github.com:bb515/probit.git`\n- Install using pip `pip install -e .` from the root directory of the repository (see the `setup.py` for the requirements that this command installs)\n\n## Examples\nYou can find examples of how to use the package under:`examples/`.\n\n### Regression and hyperparameter optimization\nRun the regression example by typing `python example/regression.py`.\n```python\n>>> def prior(prior_parameters):\n>>>     lengthscale, signal_variance = prior_parameters\n>>>     # Here you can define the kernel that defines the Gaussian process\n>>>     return signal_variance * EQ().stretch(lengthscale).periodic(0.5)\n>>>\n>>> # Generate data\n>>> key = random.PRNGKey(0)\n>>> noise_std = 0.2\n>>> (X, y, X_show, f_show, N_show) = generate_data(\n>>>     key, N_train=20,\n>>>     kernel=prior((1.0, 1.0)), noise_std=noise_std,\n>>>     N_show=1000)\n>>>\n>>> gaussian_process = GP(data=(X, y), prior=prior, log_likelihood=log_gaussian_likelihood)\n>>> evidence = gaussian_process.objective()\n>>>\n>>> vs = Vars(jnp.float32)\n>>>\n>>> def model(vs):\n>>>     p = vs.struct\n>>>     return (p.lengthscale.positive(), p.signal_variance.positive()), (p.noise_std.positive(),)\n>>>\n>>> def objective(vs):\n>>>     return evidence(model(vs))\n>>>\n>>> # Approximate posterior\n>>> parameters = model(vs)\n>>> weight, precision = gaussian_process.approximate_posterior(parameters)\n>>> mean, variance = gaussian_process.predict(\n>>>     X_show, parameters, weight, precision)\n>>> noise_variance = vs.struct.noise_std()**2\n>>> obs_variance = variance + noise_variance\n>>> plot((X, y), (X_show, f_show), mean, variance, fname=\"readme_regression_before.png\")\n```\n![Prediction](readme_regression_before.png)\n```python\n>>> print(\"Before optimization, \\nparams={}\".format(parameters))\n```\nBefore optimization, \nparams=((Array(0.10536897, dtype=float32), Array(0.2787192, dtype=float32)), (Array(0.6866876, dtype=float32),))\n```python\n>>> minimise_l_bfgs_b(objective, vs)\n>>> parameters = model(vs)\n>>> print(\"After optimization, \\nparams={}\".format(parameters))\n```\nAfter optimization, \nparams=((Array(1.354531, dtype=float32), Array(0.48594338, dtype=float32)), (Array(0.1484054, dtype=float32),))\n```python\n>>> # Approximate posterior\n>>> weight, precision = gaussian_process.approximate_posterior(parameters)\n>>> mean, variance = gaussian_process.predict(\n>>>     X_show, parameters, weight, precision)\n>>> noise_variance = vs.struct.noise_std()**2\n>>> obs_variance = variance + noise_variance\n>>> plot((X, y), (X_show, f_show), mean, obs_variance, fname=\"readme_regression_after.png\")\n```\n![Prediction](readme_regression_after.png)\n\n### Ordinal regression and hyperparameter optimization\nRun the ordinal regression example by typing `python example/classification.py`.\n\n```python\n>>> # Generate data\n>>> J = 3  # use a value of J=2 for GP binary classification\n>>> key = random.PRNGKey(1)\n>>> noise_variance = 0.4\n>>> signal_variance = 1.0\n>>> lengthscale = 1.0\n>>> kernel = signal_variance * Matern12().stretch(lengthscale)\n>>> (N_show, X, g_true, y, cutpoints,\n>>> X_test, y_test,\n>>> X_show, f_show) = generate_data(key,\n>>>     N_train_per_class=10, N_test_per_class=100,\n>>>     J=J, kernel=kernel, noise_variance=noise_variance,\n>>>     N_show=1000, jitter=1e-6)\n>>>\n>>> # Initiate a misspecified model, using a kernel\n>>> # other than the one used to generate data\n>>> def prior(prior_parameters):\n>>>     # Here you can define the kernel that defines the Gaussian process\n>>>     return signal_variance * EQ().stretch(prior_parameters)\n>>>\n>>> classifier = Approximator(data=(X, y), prior=prior,\n>>>     log_likelihood=log_probit_likelihood,\n>>>     tolerance=1e-5  # tolerance for the jaxopt fixed-point resolution\n>>>     )\n>>> negative_evidence_lower_bound = classifier.objective()\n>>>\n>>> vs = Vars(jnp.float32)\n>>>\n>>> def model(vs):\n>>>     p = vs.struct\n>>>     noise_std = jnp.sqrt(noise_variance)\n>>>     return (p.lengthscale.positive(1.2)), (noise_std, cutpoints)\n>>>\n>>> def objective(vs):\n>>>     return negative_evidence_lower_bound(model(vs))\n>>>\n>>> # Approximate posterior\n>>> parameters = model(vs)\n>>> weight, precision = classifier.approximate_posterior(parameters)\n>>> mean, variance = classifier.predict(\n>>>     X_show,\n>>>     parameters,\n>>>     weight, precision)\n>>> obs_variance = variance + noise_variance\n>>> predictive_distributions = probit_predictive_distributions(\n>>>     parameters[1],\n>>>     mean, variance)\n>>> plot(X_show, predictive_distributions, mean,\n>>>     obs_variance, X_show, f_show, X, y, g_true,\n>>>     J, colors, fname=\"readme_classification_before\")\n```\n![Prediction](readme_classification_before_contour.png)\n![Prediction](readme_classification_before_mean_variance.png)\n\n```python\n>>>\n>>> # Evaluate model\n>>> mean, variance = classifier.predict(\n>>>     X_test,\n>>>     parameters,\n>>>     weight, precision)\n>>> predictive_distributions = probit_predictive_distributions(\n>>>     parameters[1],\n>>>     mean, variance)\n>>> print(\"\\nEvaluation of model:\")\n>>> calculate_metrics(y_test, predictive_distributions)\n>>> print(\"Before optimization, \\nparameters={}\".format(parameters))\n```\nEvaluation of model:\\\n116 sum incorrect\\\n184 sum correct\\\nmean_absolute_error=0.41\\\nlog_pred_probability=-140986.54\\\nmean_zero_one_error=0.39\n\nBefore optimization, \nparameters=(Array(1.2, dtype=float32), (Array(0.63245553, dtype=float64, weak_type=True), Array([       -inf, -0.54599167,  0.50296235,         inf], dtype=float64)))\n```python\n>>>\n>>> minimise_l_bfgs_b(objective, vs)\n>>> parameters = model(vs)\n>>> print(\"After optimization, \\nparameters={}\".format(model(vs)))\n```\nAfter optimization, \nparameters=(Array(0.07389855, dtype=float32), (Array(0.63245553, dtype=float64, weak_type=True), Array([       -inf, -0.54599167,  0.50296235,         inf], dtype=float64)))\n```python\n>>>\n>>> # Approximate posterior\n>>> parameters = model(vs)\n>>> weight, precision = classifier.approximate_posterior(parameters)\n>>> mean, variance = classifier.predict(\n>>>     X_show,\n>>>     parameters,\n>>>     weight, precision)\n>>> predictive_distributions = probit_predictive_distributions(\n>>>     parameters[1],\n>>>     mean, variance)\n>>> plot(X_show, predictive_distributions, mean,\n>>>     obs_variance, X_show, f_show, X, y, g_true,\n>>>     J, colors, fname=\"readme_classification_after\")\n```\n![Prediction](readme_classification_after_contour.png)\n![Prediction](readme_classification_after_mean_variance.png)\n```python\n>>> # Evaluate model\n>>> mean, variance = classifier.predict(\n>>>     X_test,\n>>>     parameters,\n>>>     weight, precision)\n>>> obs_variance = variance + noise_variance\n>>> predictive_distributions = probit_predictive_distributions(\n>>>     parameters[1],\n>>>     mean, variance)\n>>> print(\"\\nEvaluation of model:\")\n>>> calculate_metrics(y_test, predictive_distributions)\n```\nEvaluation of model:\\\n106 sum incorrect\\\n194 sum correct\\\nmean_absolute_error=0.36\\\nlog_pred_probability=-161267.49\\\nmean_zero_one_error=0.35\n```python\n>>> nelbo = lambda x : negative_evidence_lower_bound(((x), (jnp.sqrt(noise_variance), cutpoints)))\n>>> fg = vmap(value_and_grad(nelbo))\n>>>\n>>> domain = ((-2, 2), None)\n>>> resolution = (50, None)\n>>> x = jnp.logspace(\n>>>     domain[0][0], domain[0][1], resolution[0])\n>>> xlabel = r\"lengthscale, $\\ell$\"\n>>> xscale = \"log\"\n>>> phis = jnp.log(x)\n>>>\n>>> fgs = fg(x)\n>>> fs = fgs[0]\n>>> gs = fgs[1]\n>>> plot_obj(vs.struct.lengthscale(), lengthscale, x, fs, gs, domain, xlabel, xscale)\n```\n![Prediction](readme_objective.png)\n![Prediction](readme_grad.png)\n\n## Doesn't haves\n- [Variational Gaussian Process](https://gpflow.readthedocs.io/en/v1.5.1-docs/notebooks/theory/vgp_notes.html) or [Sparse Variational Gaussian Process](https://gpflow.readthedocs.io/en/v1.5.1-docs/notebooks/theory/SGPR_notes.html).\n\n## References\nAlgorithms in this package were ported from pre-existing code. In particular, the code was ported from the following papers and repositories:\n\nLaplace approximation http://www.gatsby.ucl.ac.uk/~chuwei/ordinalregression.html\\\n@article{Chu2005,\\\nauthor = {Chu, Wei and Ghahramani, Zoubin},\\\nyear = {2005},\\\nmonth = {07},\\\npages = {1019-1041},\\\ntitle = {Gaussian Processes for Ordinal Regression.},\\\nvolume = {6},\\\njournal = {Journal of Machine Learning Research},\\\nhowpublished = {\\url{http://www.gatsby.ucl.ac.uk/~chuwei/ordinalregression.html}}}\n\nVariational inference via factorizing assumption and free form minimization\\\n@article{Girolami2005,\\\n  author=\"M. Girolami and S. Rogers\",\\\n  journal=\"Neural Computation\",\\\n  title=\"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors\",\\\n  year=\"2006\",\\\n  volume=\"18\",\\\n  number=\"8\",\\\n  pages=\"1790-1817\"}\\\n and\\\n @Misc{King2005,\\\n  title = \t {Variational Inference in <span>G</span>aussian Processes via Probabilistic Point Assimilation},\\\n  author = \t {King, Nathaniel J. and Lawrence, Neil D.},\\\n  year = \t {2005},\\\n  number = {CS-05-06},\\\n  url = \t {http://inverseprobability.com/publications/king-ppa05.html}}\n\nAn [implicit functions tutorial](http://implicit-layers-tutorial.org/implicit_functions/) was used to define the fixed-point layer.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/bb515/probit",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "probit",
    "package_url": "https://pypi.org/project/probit/",
    "platform": null,
    "project_url": "https://pypi.org/project/probit/",
    "project_urls": {
      "Homepage": "https://github.com/bb515/probit"
    },
    "release_url": "https://pypi.org/project/probit/0.0.0/",
    "requires_dist": null,
    "requires_python": ">=3.8",
    "summary": "probit is a simple and accessible Gaussian process implementation in Jax.",
    "version": "0.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16545281,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "41b2f133c75578c2f27045f793e4f02cae440b4656211a32e66017e0bc8299ec",
          "md5": "cbf3ecb20673dacaaec07514cc0b0a46",
          "sha256": "bd60881970d0b2cc867e299f4ebc0cecd10869a0a1d64accab02d717fca421e6"
        },
        "downloads": -1,
        "filename": "probit-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "cbf3ecb20673dacaaec07514cc0b0a46",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 423391,
        "upload_time": "2023-01-24T11:23:22",
        "upload_time_iso_8601": "2023-01-24T11:23:22.942686Z",
        "url": "https://files.pythonhosted.org/packages/41/b2/f133c75578c2f27045f793e4f02cae440b4656211a32e66017e0bc8299ec/probit-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "41b2f133c75578c2f27045f793e4f02cae440b4656211a32e66017e0bc8299ec",
        "md5": "cbf3ecb20673dacaaec07514cc0b0a46",
        "sha256": "bd60881970d0b2cc867e299f4ebc0cecd10869a0a1d64accab02d717fca421e6"
      },
      "downloads": -1,
      "filename": "probit-0.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "cbf3ecb20673dacaaec07514cc0b0a46",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 423391,
      "upload_time": "2023-01-24T11:23:22",
      "upload_time_iso_8601": "2023-01-24T11:23:22.942686Z",
      "url": "https://files.pythonhosted.org/packages/41/b2/f133c75578c2f27045f793e4f02cae440b4656211a32e66017e0bc8299ec/probit-0.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}