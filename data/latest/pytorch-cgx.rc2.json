{
  "info": {
    "author": "Ilia Markov",
    "author_email": "ilia.markov@ist.ac.at",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# CGX\n\nCGX is a pytorch extension adding a backend for pytorch distributed supporting allreduce of quantized buffers.\nIt supports quantizations of float16, float32 to 1-8 bits.\n\nCGX is based on MPI torch.distributed backend. The extension essentially only replaces allreduce primitive.\n\n## Quick Start\n\n### Prerequisites\nCGX, as a pytorch extension, requires `pytorch>=1.10.0`.\n\nFor faster build we recommend to have `ninja` installed (`pip install ninja`).\n\nThe compression is only supported for GPU-based buffers so either CUDA or ROCm is required.\nIf CUDA or ROCm are installed not in the standard paths, set `[CUDA|ROCM]_HOME` or `[CUDA|ROCM]_PATH` accordingly. \n\nAs long as it is based on MPI, it requires OpenMPI with GPU support installed (other MPI implementations were not tested).\nAlso, the library supports NCCL based communications, so it requires NVIDIA NCCL library.\n### Install\n```bash\nexport MPI_HOME=/path/to/mpi\nexport NCCL_HOME=/path/to/nccl\npip install pytorch-cgx\n```\n\n### Build from source\nSet `MPI_HOME` environment variable to mpi home. In case of AMD GPU, set `CGX_CUDA` to 0.\nSet `NCCL_HOME` environment variable to NCCL home, or `NCCL_INCLUDE` and `NCCL_LIB`.\nSet `QSGD_DETERMENISTIC=0` if you want to have stochastic version QSGD.\n\n```bash\ngit clone https://github.com/IST-DASLab/torch_cgx\ncd torch_cgx\nexport MPI_HOME=/path/to/mpi\nexport NCCL_HOME=/path/to/nccl\npython setup.py install\n```\n\n### Usage\nThe only changes to the training script using pytorch distributed required\n are importing the built extension and specifying `cgx` as `torch.distributed.init_process_group` backend parameter.\n \nExample:\n``` python\nimport torch\nimport torch.distributed as dist\nimport torch_cgx\n\ndist.init_process_group('cgx', init_method='env://', rank=args.local_rank)\n```\nAlso, it order to perform layerwise compression and being able to filter small sensitive to gradient compression\nlayers (typically these are batch norm layers and biases) the `cgx` needs to have information about the model.\nFor that users need to register the communication hook. The minimal size of the layers can be \ncontrolled with `layer_min_size` parameter.\n\n``` python\nmodel = torch.nn.parallel.DistributedDataParallel(...)\nfrom cgx_utils import cgx_hook, CGXState\nstate = CGXState(torch.distributed.group.WORLD, layer_min_size=1024,\n                  compression_params={\"bits\": args.quantization_bits,\n                                      \"bucket_size\": args.quantization_bucket_size})\nmodel.register_comm_hook(state, cgx_hook)\n``` \n\nAs long as the extension is based on MPI backend, it requires MPI-compliant launcher (`torch.distributed.launch` won't work):\n`$ mpirun -np 2 python train.py`\n\nAlso, if your training script was run previously with `torch.distributed.launch` utility, due to MPI launcher you need to set an environment variables (see cifar_train.py in examples)\n```\nif \"OMPI_COMM_WORLD_SIZE\" in os.environ:\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '4040'\n    os.environ[\"WORLD_SIZE\"] = os.environ[\"OMPI_COMM_WORLD_SIZE\"]\n    os.environ[\"RANK\"] = os.environ[\"OMPI_COMM_WORLD_RANK\"]\n```\n\n## Tuning\nCGX can be tuned with the following environment variables:\n\n- `CGX_COMPRESSION_QUANTIZATION_BITS` - number of bits each value of buffer is quantized to (from 1 to 8). Default is 32 which means no quantization is applied. This variable must be used if the `cgx_hook` communication hook is not registered.\n- `CGX_COMPRESSION_BUCKET_SIZE` - size of subarray into which buffer is split before quantization. Default is 512.\n- `CGX_COMPRESSION_SKIP_INCOMPLETE_BUCKETS` - boolean variable (0 or 1). After the splitting buffer into buckets, some values of buffer may remain. The variable tells quantization algorithm to compress or not to compress the remaining values. Default 0.\n- `CGX_COMPRESSION_MINIMAL_SIZE` - minimal size of buffer (number of elements) to compress. Default is 0 but in fact minimal size is forced to be not less than 16.\n- `CGX_FUSION_BUFFER_SIZE_MB`. CGX is leveraging [Tensor Fusion](https://github.com/horovod/horovod#tensor-fusion), a performance feature introduced in Horovod. This feature batches small allreduce operations. This decreases a latency in Data Parallel training. The environment variable controls the size of maximal buffer (in MB) that is communicated within one iteration of allreduce algorithm. Default is 64. The variable must be set **before** loading the module.\n- `CGX_INNER_COMMUNICATOR_TYPE`. Specifies what library to use as communication backend for intra node communication (MPI, SHM, NCCL).\n- `CGX_CROSS_COMMUNICATOR_TYPE`. Specifies what library to use as communication backend for inter node communication (MPI, NCCL).\n- `CGX_INTRA_BROADCAST`. Parameter for multinode training. When enabled, inter-node communication is performed by only one gpu per node.\n\n## Examples\n\nBasic examples are provided under the [example](examples) folder.\n\n## Notes\n - As Compression method, basic max-min uniform quantization function is used. In order to use max-min with random rounding like in QSGD, compile the library with QSGD_DETERMINISTIC=0\n - Reduction algorithm: Scatter-Reduce-AllGather.\n - Part of the source code is based on [Horovod](https://github.com/horovod/horovod) and [NCCL](https://github.com/NVIDIA/nccl) sources.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/IST-DASLab/torch_cgx/archive/refs/tags/v0.1.1.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/IST-DASLab/torch_cgx/",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pytorch-cgx",
    "package_url": "https://pypi.org/project/pytorch-cgx/",
    "platform": null,
    "project_url": "https://pypi.org/project/pytorch-cgx/",
    "project_urls": {
      "Download": "https://github.com/IST-DASLab/torch_cgx/archive/refs/tags/v0.1.1.tar.gz",
      "Homepage": "https://github.com/IST-DASLab/torch_cgx/"
    },
    "release_url": "https://pypi.org/project/pytorch-cgx/0.1.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "pytorch extension adding a backend supporting allreduce of quantized buffers.",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15856623,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "071512ce99145b5a042515a325b0b760ad81418319740f09c69c9bf9d474a53d",
          "md5": "7b3ece3dc596779d0235e522566327b2",
          "sha256": "d3509095f36972eae11469381b13cfb5be310ecde490e1385f461fdafbb0c65b"
        },
        "downloads": -1,
        "filename": "pytorch_cgx-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7b3ece3dc596779d0235e522566327b2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 60200,
        "upload_time": "2022-11-17T16:49:20",
        "upload_time_iso_8601": "2022-11-17T16:49:20.163562Z",
        "url": "https://files.pythonhosted.org/packages/07/15/12ce99145b5a042515a325b0b760ad81418319740f09c69c9bf9d474a53d/pytorch_cgx-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b1b5f382b9b98018240b9877a3532fea08f64bcae42ce17fb36d63eaff1cdee9",
          "md5": "2872fb53c5133608c297d7189a7b8b84",
          "sha256": "9d3cb5cf185482effea894c59ab6d1df7ad2f2c96dd2e9ad219faed76d3044c7"
        },
        "downloads": -1,
        "filename": "pytorch_cgx-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "2872fb53c5133608c297d7189a7b8b84",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 60387,
        "upload_time": "2022-11-22T16:59:37",
        "upload_time_iso_8601": "2022-11-22T16:59:37.816918Z",
        "url": "https://files.pythonhosted.org/packages/b1/b5/f382b9b98018240b9877a3532fea08f64bcae42ce17fb36d63eaff1cdee9/pytorch_cgx-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b1b5f382b9b98018240b9877a3532fea08f64bcae42ce17fb36d63eaff1cdee9",
        "md5": "2872fb53c5133608c297d7189a7b8b84",
        "sha256": "9d3cb5cf185482effea894c59ab6d1df7ad2f2c96dd2e9ad219faed76d3044c7"
      },
      "downloads": -1,
      "filename": "pytorch_cgx-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "2872fb53c5133608c297d7189a7b8b84",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 60387,
      "upload_time": "2022-11-22T16:59:37",
      "upload_time_iso_8601": "2022-11-22T16:59:37.816918Z",
      "url": "https://files.pythonhosted.org/packages/b1/b5/f382b9b98018240b9877a3532fea08f64bcae42ce17fb36d63eaff1cdee9/pytorch_cgx-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}