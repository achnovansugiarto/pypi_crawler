{
  "info": {
    "author": "Raven Protocol",
    "author_email": "kailash@ravenprotocol.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# RavML\nRavenverse's Machine Learning Library\n\nRavML is the machine learning library based on RavOp. It contains implementations of various machine learning algorithms like ordinary least squares, linear regression, logistic regression, KNN, Kmeans, Mini batch Kmeans, Decision Tree classifier, and Naive Bayes classifier\n\n\n[Github](https://github.com/ravenprotocol/ravml.git)\n\n## Installation\n\nCreate a virtual environment\n    \n    virtualenv ravml -p python3\n    \nActivate the virtual environment\n    \n    source ravml/bin/activate\n\nInstall Dependencies\n\n    pip install git+https://github.com/ravenprotocol/ravml.git\n\nInstall Ravml\n\n    python3 setup.py install\n \n> **Note :** Use `python3 setup --help` for argument details\n\n---\n## RavML Supported Algorithms\n\n### K-Nearest Neighbours\nK-Nearest Neighbors (KNN) is a simple machine learning technique for regression and classification problems. KNN algorithms take data and apply similarity metrics to classify fresh data points (e.g. distance function). A majority vote of its neighbours is used to classify it. The information is assigned to the class with the most neighbours. As the number of nearest neighbours grows, so does the value of k, and so does the accuracy.\n\n```python\nfrom ravml.classifier import KNNClassifier\n\nknn = KNNClassifier()\nknn.fit(X_train, y_train, n_neighbours=5, n_classes=3)\nprint(knn.predict(X_test))\nprint(knn.score(y_test=y_test))\n```\nYou can view a sample implementation of KNN on ravml [here](https://github.com/ravenprotocol/ravml/blob/main/examples/knn_classifier.py)\n\n\n### Naive Bayes Classifier\nIt's a classification method based on Bayes' Theorem and the assumption of predictor independence. A Naive Bayes classifier, in simple terms, assumes that the existence of one feature in a class is unrelated to the presence of any other feature. The Naive Bayes model is simple to construct and is especially good for huge data sets. Naive Bayes is renowned to outperform even the most advanced classification systems due to its simplicity.\n\n```python\nfrom ravml.linear_model.naive_bayes import NaiveBayesClassifier\n\nmodel = NaiveBayesClassifier()\n\nmodel.fit(X_train, y_train)\ny_preds = model.predict(X_test)\n\ncalc_preds = []\nfor y_pred in y_preds:\n    keys = list(y_pred.keys())\n    calc_pred = {key: y_pred[key]() for key in keys}\n    calc_preds.append(calc_pred)\n\nMAPs = []\nfor pred in calc_preds:\n    MAP = max(pred, key= pred.get)\n    MAPs.append(MAP)\n\nprint(\"NaiveBayesClassifier accuracy: {0:.3f}\".format(model.accuracy(y_test, MAPs)))\n```\nYou can view a sample implementation of Naive Bayes on ravml [here](https://github.com/ravenprotocol/ravml/blob/main/examples/naive_bayes_classifier.py)\n\n### Support Vector Machine\nSVM (Support Vector Machine) is a supervised machine learning technique that can be used to solve classification and regression problems. It is, however, mostly employed to solve categorization difficulties. Each data item is plotted as a point in n-dimensional space (where n is the number of features you have), with the value of each feature being the value of a certain coordinate in the SVM algorithm. Then we accomplish classification by locating the hyper-plane that clearly distinguishes the two classes.\n### K-Means\n\nClustering is a type of unsupervised learning wherein data points are grouped into different sets based on their degree of similarity. The k-means clustering algorithm assigns data points to categories, or clusters, by finding the mean distance between data points. It then iterates through this technique in order to perform more accurate classifications over time. Since you must first start by classifying your data into k categories, it is essential that you understand your data well enough to do this\n\n```python\nfrom ravml.cluster import KMeans\n\nk = KMeans()\nk.fit(X_train, 3, iter=30)\n```\n\nYou can view a sample implementation of K-means on ravml [here](https://github.com/ravenprotocol/ravml/blob/main/examples/kmeans.py)\n\n### Linear Regression\n\nLinear Regression is a supervised machine learning technique with a continuous and constant slope projected output. Rather than aiming to classify data into categories (e.g. cat, dog), it is used to predict values within a continuous range (e.g. sales, price). There are two main types: Simple and Multivariable Regression.\n\n```python\nfrom ravml.linear.linear_regression import LinearRegression\n\nmodel = LinearRegression(x,y,theta)\n\nmodel.compute_cost()  # initial cost with coefficients at zero\n\noptimal_theta = model.gradient_descent(alpha=0.01, iterations=20)\n\nmodel.plot_graph(optimal_theta)\n```\n\nYou can view the implementation of Linear Regression [*here*](https://github.com/ravenprotocol/ravml/blob/main/ravml/linear/linear_regression.py).\n\n### Logistic Regression\n\nThe Supervised Learning methodology of logistic regression is used to predict the categorical dependent variable using a set of independent factors. A categorical dependent variable's output is predicted using logistic regression. As a result, the result must be a discrete or categorical value. Logistic Regression is much similar to Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems. \n\n```python\nfrom ravml.linear.logistic_regression import LogisticRegression\nmodel = LogisticRegression(lr=0.1, num_iter=30)\n\nmodel.fit(X, y)\n\npreds = model.predict(X)\n\nprint((preds == y).mean())\nprint(model.theta())\n\nmodel.plot_loss()\n\nmodel.visualize(X,y)\n```\n\nYou can view the implementation of Logistic Regression on IRIS flower dataset [*here*](https://github.com/ravenprotocol/ravml/tree/main/examples/logistic_regression.py).\n\n### Multi-Layer Perceptron\n\nThe multi-layer perceptron (MLP) is a feed-forward neural network supplement. It has three layers: an input layer, an output layer, and a hidden layer. The input signal to be processed is received by the input layer. The output layer is responsible for tasks such as prediction and categorization. The true computational engine of the MLP is an arbitrary number of hidden layers inserted between the input and output layers. In an MLP, data flows from input to output layer in the forward direction, similar to a feed-forward network. The backpropagation learning algorithm is used to train the neurons in the MLP. MLPs can tackle issues that aren't linearly separable and are designed to approximate any continuous function. Pattern categorization, recognition, prediction, and approximation are some of MLP's most common applications.\n\n```python\nfrom ravml.linear.perceptron import Perceptron\n\nmodel = Perceptron(input_dims=4, hidden_dims=10, output_dims=3)\n\nmodel.fit(X_train, y_train, alpha = 0.01, epoch = 3)\n\npr = model.predict(X_test[1])\nprint('Prediction : ',pr)\n\nmodel.plot_metrics()      # plots accuracy and loss\n```\n\nYou can view the implementation of MLP on IRIS Flower Dataset [*here*](https://github.com/ravenprotocol/ravml/blob/main/ravml/linear/mlp_iris.py).\n\n### Decision Trees\n\nDecision Tree is a supervised learning technique that may be used to solve both classification and regression problems, however, it is most commonly employed to solve classification issues. Internal nodes represent dataset attributes, branches represent decision rules, and each leaf node provides the conclusion in this tree-structured classifier. The Decision Node and the Leaf Node are the two nodes of a Decision tree. Leaf nodes are the output of those decisions and do not contain any more branches, whereas Decision nodes are used to make any decision and have several branches. The decisions or tests are made based on the characteristics of the given dataset.\n\n```python\nfrom ravml.tree import DecisionTreeClassifier\n\nobj = DecisionTreeClassifier(max_depth=3)\nobj.fit(X_train[:30], y_train[:30])\npr = obj.predict(X_test)\n\nprint(f1_score(y_test, pr, average='weighted'))\n```\n\nYou can view the implementation of Decision Tree [*here*](https://github.com/ravenprotocol/ravml/blob/main/ravml/tree/decision_tree.py).",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ravenprotocol/ravdl",
    "keywords": "Ravml,machine learning library,algorithms",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ravml",
    "package_url": "https://pypi.org/project/ravml/",
    "platform": null,
    "project_url": "https://pypi.org/project/ravml/",
    "project_urls": {
      "Homepage": "https://github.com/ravenprotocol/ravdl"
    },
    "release_url": "https://pypi.org/project/ravml/0.5/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "",
    "version": "0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14627209,
  "releases": {
    "0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "acade250d186faad1d2b4cebef70a6940aa6113974fe20b9f4d0eb4749f79506",
          "md5": "d70564877a9b75b42878ac7f4cbb38f4",
          "sha256": "4beb2b9b88912f6f39d67f3d637eb556c7139478b5d4145247ba9d507645880b"
        },
        "downloads": -1,
        "filename": "ravml-0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "d70564877a9b75b42878ac7f4cbb38f4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13499,
        "upload_time": "2022-08-02T13:17:07",
        "upload_time_iso_8601": "2022-08-02T13:17:07.371459Z",
        "url": "https://files.pythonhosted.org/packages/ac/ad/e250d186faad1d2b4cebef70a6940aa6113974fe20b9f4d0eb4749f79506/ravml-0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "acade250d186faad1d2b4cebef70a6940aa6113974fe20b9f4d0eb4749f79506",
        "md5": "d70564877a9b75b42878ac7f4cbb38f4",
        "sha256": "4beb2b9b88912f6f39d67f3d637eb556c7139478b5d4145247ba9d507645880b"
      },
      "downloads": -1,
      "filename": "ravml-0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "d70564877a9b75b42878ac7f4cbb38f4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 13499,
      "upload_time": "2022-08-02T13:17:07",
      "upload_time_iso_8601": "2022-08-02T13:17:07.371459Z",
      "url": "https://files.pythonhosted.org/packages/ac/ad/e250d186faad1d2b4cebef70a6940aa6113974fe20b9f4d0eb4749f79506/ravml-0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}