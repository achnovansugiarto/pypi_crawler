{
  "info": {
    "author": "ffreemt",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# Phrase Tokenizer\n[![pytest](https://github.com/ffreemt/phrase-tokenizer/actions/workflows/on-push.yml/badge.svg)](https://github.com/ffreemt/phrase-tokenizer/actions)[![python](https://img.shields.io/static/v1?label=python+&message=3.7%2B&color=blue)](https://www.python.org/downloads/)[![Codacy Badge](https://app.codacy.com/project/badge/Grade/d7e1c1f44dbb423099a929aadd7db2fd)](https://www.codacy.com/gh/ffreemt/phrase-tokenizer/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=ffreemt/phrase-tokenizer&amp;utm_campaign=Badge_Grade)[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)[![docstyle: google](https://img.shields.io/badge/docstyle-google-green.svg)](https://google.github.io/styleguide/pyguide.html)[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![PyPI version](https://badge.fury.io/py/phrase-tokenizer.svg)](https://badge.fury.io/py/phrase-tokenizer)\n\nTokenize an English sentence to phrases via benepar.\n\n## Installation\n\n```bash\npip install phrase-tokenizer\n# pip install phrase-tokenizer -U to update\n# or to install the latest from github:\n# pip git+https://github.com/ffreemt/phrase-tokenizer.git\n```\n\nOr clone the repo `https://github.com/ffreemt/phrase-tokenizer.git`:\n\n```bash\ngit clone https://github.com/ffreemt/phrase-tokenizer.git\ncd phrase-tokenizer\npip install logzero benepar tensorflow\n```\nOr use `poetry`, e.g.\n```bash\ngit clone https://github.com/ffreemt/phrase-tokenizer.git\ncd phrase-tokenizer\npoetry install\n```\n\n## Usage\n\n```python\nfrom phrase_tokenizer import phrase_tok\n\nres = phrase_tok(\"Short cuts make long delays.\")\nprint(res)\n# ['Short cuts', 'make long delays']\n\n# verbose=True turns on verbose to see the tokenizing process\nres = phrase_tok(\"Short cuts make long delays\", verbose=True)\n# ',..Short.cuts,.make..long.delays..'\n```\n\nConsult the source code for details.\n\n## For Developers\n\n```bash\ngit clone https://github.com/ffreemt/phrase-tokenizer.git\ncd phrase-tokenizer\npip install -r requirements-dev.txt\n```\n\nIn `ipython`, ``plot_tree`` is able to draw a nice tree to aid the development, e.g.,\n\n```python\nfrom phrase_tokenizer.phrase_tok import plot_tree\n\nplot_tree(\"Short cuts make long delays.\")\n```\n![img](https://github.com/ffreemt/phrase-tokenizer/blob/master/img/short_cuts.png?raw=true)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ffreemt/phrase-tokenizer",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "phrase-tokenizer",
    "package_url": "https://pypi.org/project/phrase-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/phrase-tokenizer/",
    "project_urls": {
      "Bug Tracker": "https://github.com/ffreemt/phrase-tokenizer/issues",
      "Homepage": "https://github.com/ffreemt/phrase-tokenizer",
      "Repository": "https://github.com/ffreemt/phrase-tokenizer"
    },
    "release_url": "https://pypi.org/project/phrase-tokenizer/0.1.3/",
    "requires_dist": [
      "benepar (>=0.2.0,<0.3.0)",
      "logzero (>=1.6.3,<2.0.0)",
      "nltk (>=3.2.5,<4.0.0)",
      "svgling (>=0.3.0,<0.4.0)"
    ],
    "requires_python": ">=3.7,<4.0",
    "summary": "Tokenize an English sentence to phrases",
    "version": "0.1.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12717981,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4a47ab4d46ebc58f480e5a51da0ae0763837a1c4934d7154dbf93b0d3dc3cef8",
          "md5": "5d238d18320061b7da148352a7297400",
          "sha256": "6f167ee31e3c806059a31ded46aee0391cbdcf1bb2a64997ccff140d4def9555"
        },
        "downloads": -1,
        "filename": "phrase_tokenizer-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5d238d18320061b7da148352a7297400",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.1,<4.0.0",
        "size": 2734,
        "upload_time": "2021-01-09T09:30:29",
        "upload_time_iso_8601": "2021-01-09T09:30:29.944224Z",
        "url": "https://files.pythonhosted.org/packages/4a/47/ab4d46ebc58f480e5a51da0ae0763837a1c4934d7154dbf93b0d3dc3cef8/phrase_tokenizer-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b3bdae4ebd570c6fff056776a7d6a9498799dcdbf9526ad964a8833e82f85340",
          "md5": "d2a16ed7ac0f8a50a1e9eb28eddefb60",
          "sha256": "bc2b14f451cc159da7e50e7186bff3aa559f0bea1c8a54a3bcb0ceb431a8f82a"
        },
        "downloads": -1,
        "filename": "phrase-tokenizer-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "d2a16ed7ac0f8a50a1e9eb28eddefb60",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.1,<4.0.0",
        "size": 2473,
        "upload_time": "2021-01-09T09:30:28",
        "upload_time_iso_8601": "2021-01-09T09:30:28.558733Z",
        "url": "https://files.pythonhosted.org/packages/b3/bd/ae4ebd570c6fff056776a7d6a9498799dcdbf9526ad964a8833e82f85340/phrase-tokenizer-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aa72d90ec4a6550c84a688edbfff332080bbebad1f0fafa4761b9b55440de692",
          "md5": "dddf451c0abeae863c4d044e250df3ed",
          "sha256": "65c3cdbd93484bacd8da0f04e28453671132f6590d6c52e85dd4690cb16fd067"
        },
        "downloads": -1,
        "filename": "phrase_tokenizer-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dddf451c0abeae863c4d044e250df3ed",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.1,<4.0.0",
        "size": 4216,
        "upload_time": "2021-01-09T09:51:02",
        "upload_time_iso_8601": "2021-01-09T09:51:02.864365Z",
        "url": "https://files.pythonhosted.org/packages/aa/72/d90ec4a6550c84a688edbfff332080bbebad1f0fafa4761b9b55440de692/phrase_tokenizer-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "10577d23469ab11bf861918816c7a052261f9798936644ddf60f3111f577cc89",
          "md5": "4ca35cffcaefb1eb42366b57b16dfd66",
          "sha256": "d5e52d6c3181529bba39faf055223b07b1b3e8a263614f3d734eed9ca90472ab"
        },
        "downloads": -1,
        "filename": "phrase-tokenizer-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4ca35cffcaefb1eb42366b57b16dfd66",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.1,<4.0.0",
        "size": 4073,
        "upload_time": "2021-01-09T09:51:01",
        "upload_time_iso_8601": "2021-01-09T09:51:01.471528Z",
        "url": "https://files.pythonhosted.org/packages/10/57/7d23469ab11bf861918816c7a052261f9798936644ddf60f3111f577cc89/phrase-tokenizer-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cd98db2f60ae4376119676e7639f40f20cae3194eea51ccb6c30abdfdb4d4861",
          "md5": "d73b33616a63f0898f2ac00f5efcfbca",
          "sha256": "1cf561b8397b6012196ae9b8d7ebf6c9701945f78f8ff6948c7627c43f058a01"
        },
        "downloads": -1,
        "filename": "phrase_tokenizer-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d73b33616a63f0898f2ac00f5efcfbca",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.1,<4.0.0",
        "size": 4321,
        "upload_time": "2021-01-09T11:49:31",
        "upload_time_iso_8601": "2021-01-09T11:49:31.088563Z",
        "url": "https://files.pythonhosted.org/packages/cd/98/db2f60ae4376119676e7639f40f20cae3194eea51ccb6c30abdfdb4d4861/phrase_tokenizer-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0e1ecda9caeb8e7c1348ab9c6e417c4ce3773b21c79e2793ec5cb0a237362ee6",
          "md5": "6f269be1ce1b2774d78ed3653130cb00",
          "sha256": "bd67a8446b1242b468b5948a9269fd98520e18bff408614defddc53cb40fdb13"
        },
        "downloads": -1,
        "filename": "phrase-tokenizer-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "6f269be1ce1b2774d78ed3653130cb00",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.1,<4.0.0",
        "size": 4182,
        "upload_time": "2021-01-09T11:49:29",
        "upload_time_iso_8601": "2021-01-09T11:49:29.737838Z",
        "url": "https://files.pythonhosted.org/packages/0e/1e/cda9caeb8e7c1348ab9c6e417c4ce3773b21c79e2793ec5cb0a237362ee6/phrase-tokenizer-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "814bb470ff9c53414d052a24c6a2afb378b24720f42ad2d9bb04ae531847a90f",
          "md5": "bda0d55d33e06a4c625ff499b79a3c27",
          "sha256": "5f5c0dc455d34c2d5beb35086d36f9f7d60bc2d34cc17807b7b74712ed18762b"
        },
        "downloads": -1,
        "filename": "phrase_tokenizer-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bda0d55d33e06a4c625ff499b79a3c27",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<4.0",
        "size": 4858,
        "upload_time": "2022-01-28T04:32:48",
        "upload_time_iso_8601": "2022-01-28T04:32:48.420022Z",
        "url": "https://files.pythonhosted.org/packages/81/4b/b470ff9c53414d052a24c6a2afb378b24720f42ad2d9bb04ae531847a90f/phrase_tokenizer-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d36a19fe47fa5bd8811f6e940c0636acd0c8d8a2ded593f8528c28c35e94e143",
          "md5": "d59192e422f7f1f1192baa99a034e972",
          "sha256": "3da2e6557661a9248d7782ef73eae55afec51ca3c7ac1bab16b7b73beb6ed048"
        },
        "downloads": -1,
        "filename": "phrase-tokenizer-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "d59192e422f7f1f1192baa99a034e972",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<4.0",
        "size": 4764,
        "upload_time": "2022-01-28T04:32:46",
        "upload_time_iso_8601": "2022-01-28T04:32:46.821108Z",
        "url": "https://files.pythonhosted.org/packages/d3/6a/19fe47fa5bd8811f6e940c0636acd0c8d8a2ded593f8528c28c35e94e143/phrase-tokenizer-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "814bb470ff9c53414d052a24c6a2afb378b24720f42ad2d9bb04ae531847a90f",
        "md5": "bda0d55d33e06a4c625ff499b79a3c27",
        "sha256": "5f5c0dc455d34c2d5beb35086d36f9f7d60bc2d34cc17807b7b74712ed18762b"
      },
      "downloads": -1,
      "filename": "phrase_tokenizer-0.1.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "bda0d55d33e06a4c625ff499b79a3c27",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7,<4.0",
      "size": 4858,
      "upload_time": "2022-01-28T04:32:48",
      "upload_time_iso_8601": "2022-01-28T04:32:48.420022Z",
      "url": "https://files.pythonhosted.org/packages/81/4b/b470ff9c53414d052a24c6a2afb378b24720f42ad2d9bb04ae531847a90f/phrase_tokenizer-0.1.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d36a19fe47fa5bd8811f6e940c0636acd0c8d8a2ded593f8528c28c35e94e143",
        "md5": "d59192e422f7f1f1192baa99a034e972",
        "sha256": "3da2e6557661a9248d7782ef73eae55afec51ca3c7ac1bab16b7b73beb6ed048"
      },
      "downloads": -1,
      "filename": "phrase-tokenizer-0.1.3.tar.gz",
      "has_sig": false,
      "md5_digest": "d59192e422f7f1f1192baa99a034e972",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7,<4.0",
      "size": 4764,
      "upload_time": "2022-01-28T04:32:46",
      "upload_time_iso_8601": "2022-01-28T04:32:46.821108Z",
      "url": "https://files.pythonhosted.org/packages/d3/6a/19fe47fa5bd8811f6e940c0636acd0c8d8a2ded593f8528c28c35e94e143/phrase-tokenizer-0.1.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}