{
  "info": {
    "author": "Lucas Phillips",
    "author_email": "Luchcubs23@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Education",
      "License :: OSI Approved :: MIT License",
      "Operating System :: Microsoft :: Windows :: Windows 10",
      "Programming Language :: Python :: 3"
    ],
    "description": "SpiderWebCraler is a library to automatically complete web-scraping related tasks such as:\nfinding element(s) by class or ID, finding elements by XPATH, finding tables, paragraphs, headers, footers, headings, images, etc., finding images' source, finding and returning images!\n\n\n03/11/2021\n\nVersion 1.0.1 \n=-=-=-=-=-=-=\n\n-Created SpiderWebCrawler\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://upload.pypi.org/legacy/",
    "keywords": "webscraper",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "SpiderWebCrawler",
    "package_url": "https://pypi.org/project/SpiderWebCrawler/",
    "platform": "",
    "project_url": "https://pypi.org/project/SpiderWebCrawler/",
    "project_urls": {
      "Homepage": "https://upload.pypi.org/legacy/"
    },
    "release_url": "https://pypi.org/project/SpiderWebCrawler/0.0.1/",
    "requires_dist": [
      "beautifulsoup4 (==4.9.3)",
      "bs4 (==0.0.1)",
      "certifi (==2020.12.5)",
      "chardet (==4.0.0)",
      "idna (==2.10)",
      "numpy (==1.20.1)",
      "pandas (==1.2.3)",
      "Pillow (==8.1.2)",
      "python-dateutil (==2.8.1)",
      "pytz (==2021.1)",
      "requests (==2.25.1)",
      "six (==1.15.0)",
      "soupsieve (==2.2)",
      "urllib3 (==1.26.3)"
    ],
    "requires_python": "",
    "summary": "Automatically Scrape Websites",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9758251,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a0fc53cf2076aec5f8b327d82f4f2244682c3579b6ddacbcbd2a39e2ab96e851",
          "md5": "87bf4fc9d7221e15ca81918edae5d9e1",
          "sha256": "06c6cd58b952ec423f198de7c71274e6e260465383d3e66f198ae2aea75b45a7"
        },
        "downloads": -1,
        "filename": "SpiderWebCrawler-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "87bf4fc9d7221e15ca81918edae5d9e1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 4382,
        "upload_time": "2021-03-13T02:42:20",
        "upload_time_iso_8601": "2021-03-13T02:42:20.630304Z",
        "url": "https://files.pythonhosted.org/packages/a0/fc/53cf2076aec5f8b327d82f4f2244682c3579b6ddacbcbd2a39e2ab96e851/SpiderWebCrawler-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dd4838929a299059bbd120a964c4ac656313bc95b8394d53e798a5e26b1e5451",
          "md5": "f3a627a0f3ad968c10b7155f05b16157",
          "sha256": "05cd7bb2ad4337ce22b2137821fb69651168c4862eb0bfff8d5fd8058dcccd7c"
        },
        "downloads": -1,
        "filename": "SpiderWebCrawler-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f3a627a0f3ad968c10b7155f05b16157",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 1550595,
        "upload_time": "2021-03-13T03:10:40",
        "upload_time_iso_8601": "2021-03-13T03:10:40.674631Z",
        "url": "https://files.pythonhosted.org/packages/dd/48/38929a299059bbd120a964c4ac656313bc95b8394d53e798a5e26b1e5451/SpiderWebCrawler-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a0fc53cf2076aec5f8b327d82f4f2244682c3579b6ddacbcbd2a39e2ab96e851",
        "md5": "87bf4fc9d7221e15ca81918edae5d9e1",
        "sha256": "06c6cd58b952ec423f198de7c71274e6e260465383d3e66f198ae2aea75b45a7"
      },
      "downloads": -1,
      "filename": "SpiderWebCrawler-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "87bf4fc9d7221e15ca81918edae5d9e1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 4382,
      "upload_time": "2021-03-13T02:42:20",
      "upload_time_iso_8601": "2021-03-13T02:42:20.630304Z",
      "url": "https://files.pythonhosted.org/packages/a0/fc/53cf2076aec5f8b327d82f4f2244682c3579b6ddacbcbd2a39e2ab96e851/SpiderWebCrawler-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dd4838929a299059bbd120a964c4ac656313bc95b8394d53e798a5e26b1e5451",
        "md5": "f3a627a0f3ad968c10b7155f05b16157",
        "sha256": "05cd7bb2ad4337ce22b2137821fb69651168c4862eb0bfff8d5fd8058dcccd7c"
      },
      "downloads": -1,
      "filename": "SpiderWebCrawler-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "f3a627a0f3ad968c10b7155f05b16157",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 1550595,
      "upload_time": "2021-03-13T03:10:40",
      "upload_time_iso_8601": "2021-03-13T03:10:40.674631Z",
      "url": "https://files.pythonhosted.org/packages/dd/48/38929a299059bbd120a964c4ac656313bc95b8394d53e798a5e26b1e5451/SpiderWebCrawler-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}