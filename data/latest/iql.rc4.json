{
  "info": {
    "author": "Paul Timmins",
    "author_email": "paul@iqmo.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# IQMO Query Language (IQL)\r\n\r\nIQL extends SQL with pluggable Python extensions allowing you to use external data seamlessly within SQL. It's intended as a light weight layer to allow insertion of code, all fully within SQL. IQL provides extensions for a variety of data sources: \r\n\r\nIQL works out of the box, allowing you to execute SQL statements over your dataframes and external data sources with no configuration. It is able to do this because of DuckDB, IQL's default database, but other databases may also be connected/used. DuckDB is a highly performant in-memory OLAP database: it's great for complex analytical work. \r\n\r\n- SEC EDGAR\r\n    ```\r\n    iql.execute(f\"select * from edgar(type='submissions', cik='{CIK}')\")\r\n    ```\r\n- Bloomberg Query Language (BQL)\r\n    ```\r\n    iql.execute(f\"\"\"select * from bql(\"{bqlquery}\") as bql1 join mytable on mytable.id=bql1.id\"\"\")\r\n    ```\r\n- Kaggle Data Files\r\n    ```\r\n    iql.execute(f\"\"\"select * from kaggle(\"user/dataset/datafile\")\"\"\")\r\n    ```\r\n- FRED Economic Data\r\n    ```\r\n    iql.execute(f\"\"\"select * from fred(type=\"observations\", seriesid=\"{seriesid}\", startdate=\"{date_str}\") as freddata\"\"\")\r\n    ```\r\n- Pandas operations\r\n    ```\r\n    SELECT * FROM pandas(table=xyz, pivot=('col1', 'col2', 'values'))\r\n    ```\r\n\r\n## Why?\r\nIQL allows many analytics tasks to be condensed into simpler queries, and allows seamless interplay between Pandas (or Polars) dataframes and database operations. \r\n\r\nSQL (with IQL) condenses a lot of tedious and repetitive Pandas code and is friendlier to non-developers who don't need to learn the intricacies of the Pandas API.\r\n\r\nWe use IQL for early stage exploration, cleaning and staging. We also use IQL for later stage reporting and data presentation. In between, we still use Pandas/Polars for intermediate ML and modelling. \r\n\r\n### Native Formats\r\nOur goal is to support the native formats of the systems:\r\n- The native SQL of the database engine is supported: only the IQL SubQuery's are used. \r\n- The native SubQuery syntax is preserved. Bloomberg BQL queries run without modification. REST API calls can be called via URLs directly. etc.\r\n\r\n### Simplicity and Performance\r\nMultiple transformations, aggregations and operations can be expressed in a single statement. This reduces the amount of code we need to write, eliminates long chains of Pandas operations, and often leads to significant performance increases. \r\n\r\nIQL's default database is a higly efficient in-memory OLAP database called DuckDB. [DuckDB Performance vs Pandas](https://duckdb.org/2021/05/14/sql-on-pandas.html)\r\n\r\n### All in One Place and Extensible\r\n\r\nYou can query REST APIs as if they were database tables. You can add custom business logic to control how certain files are retrieved, cached or pre-processed.\r\n\r\nIQL is portable across DB environments. DuckDB is shipped by default, but can be replaced with PyArrow, SnowFlake or other databases.\r\n\r\n### How Does It Work\r\nHow does it work? IQL iterates over SQL statements (if multiple statements are used), and extracts IQL SubQuerys (ie: fred(...)). Each SubQuery is executed and stored (as a DataFrame or local file). The SQL query is modified to reference the results of the SubQuery instead of the SubQuery itself, and the database engine runs the modified SQL query. \r\n\r\nFor example, given the following query:\r\n```  \r\nSELECT * \r\n  FROM fred('query1') as q1\r\n  JOIN fred('query2') as q2\r\n      ON q1.id=q2.id\r\n```\r\nIn pseudocode (this is logically but not literally what happens):\r\n```\r\n  # pseudocode\r\n  df_q1 = iql.execute(\"fred('query1\")\")\r\n  df_q2 = iql.execute(\"fred('query2\")\")\r\n\r\n  db.execute(\"SELECT * FROM df_q1 JOIN df_q2 on df_q1.id = df_q2.id\")\r\n```\r\n\r\n## Disclaimers\r\n\r\nREAD the license. We do not WARRANTY this for anything. This software is provided AS IS. TEST before you use. Use at your OWN risk. ETC.\r\n\r\nTHIS PROJECT IS NOT AFFILIATED WITH, SUPPORTED BY, ENDORSED BY OR CONNECTED TO ANY OF THE COMPANIES, PRODUCTS OR SERVICES BELOW.\r\n\r\n\r\n# Extensions:\r\n\r\n- [Bloomberg BQL](BLOOMBERG_BQL_README.md)\r\n- [Kaggle Datasets](https://www.kaggle.com/datasets)\r\n  - Requires kaggle module\r\n    pip install kaggle\r\n- [FRED Economic Data](https://fred.stlouisfed.org/docs/api/fred/)\r\n- [Amazon S3](https://aws.amazon.com/s3/): S3 API extension will download Parquet and CSV files locally, and then access them via DuckDB's Parquet and CSV support.\r\n  - Requires boto3\r\n    pip install boto3\r\n  - Alternatively, use DuckDBs [HTTPFS](https://duckdb.org/docs/extensions/httpfs.html) to access S3.\r\n  - The IQL AWS extension provides a caching, authentication and logic entrypoint\r\n  - The DuckDB extension provides support for processing multiple Parquet files, such as with Hive partitions.\r\n  - More information on [DuckDB and Parquet](https://duckdb.org/docs/data/parquet)\r\n- [Pandas](https://pandas.pydata.org/): Allows Pandas operations to be executed within the SQL statement. Not all Pandas operations are available.\r\n\r\nSee the examples/ folder for complete examples.\r\n\r\n## Syntax\r\n\r\nIQL extensions are executed as functional subqueries. Each extension is registered with a unique name.\r\n\r\nKaggle:\r\n\r\n```\r\nSELECT \\*\r\nFROM\r\nkaggle('username/datasetname/filename')\r\n```\r\n\r\nor (FRED and Kaggle)\r\n\r\n```\r\nSELECT \\*\r\nFROM\r\nkaggle(\"....\") q1\r\nJOIN\r\nfred(\"....\") q2\r\nON\r\nq1.something = q2.something\r\n```\r\n\r\nor (Bloomberg BQL)\r\n\r\n```\r\nSELECT \\*\r\nFROM\r\nbql(\"get (...) for (...)\") q1\r\nJOIN\r\nbql(\"get (...) for (...)\") q2\r\nON\r\nq1.id = q2.id\r\n```\r\n\r\nSee the example notebooks for more interesting examples.\r\n\r\n## Dependencies\r\n\r\n- Extensions may have specific module dependencies, such as the Kaggle Extensions requiring the Kaggle API.\r\n- Extensions are loaded dynamically on first use, so dependencies are not required at install time\r\n- duckdb is required, although not needed if a different db_connector is used.\r\n\r\n## SQL Syntax\r\n\r\nIQL does not modify the SQL other than replacing the underlying SQL.\r\n\r\nSee the database specific documentation for more information: [DuckDB SQL Statements](https://duckdb.org/docs/sql/introduction)\r\n\r\n## Quoting Strings\r\n\r\nStrings must be properly quoted and/or escaped, according to normal Python rules. The SubQuery requires a quoted string, be careful to use different quote types for the entire SQL string and the SubQuery string.\r\n\r\nIt's not that complicated, but it's easy to screw up with multiple levels of quoting.\r\n\r\nTriple quotes are convenient, since SQL queries tend to be long and multi-line. Note the three levels of quotes: triple \"\"\", single \" and single '.\r\n\r\n```\r\nimport iql\r\n\r\nbql_str = \"get (...) for ('XYZ')\"\r\nsql_str = f\"\"\"\r\n    -- This uses a Python f-string, which allows us to use the {bql_str} variable\r\n    SELECT *\r\n    FROM\r\n        -- bql() is an IQL extension. Note the quotes around the BQL statement.\r\n        -- if the BQL statement contains double quotes,\r\n        bql(\"{bql_str}\")\r\n    \"\"\"\r\n\r\niql.execute(sql_str)\r\n```\r\n\r\nOr, make sure to escape properly, such as here:\r\n\r\n```\r\nimport iql # Use \\\" to escape the double quotes within double quotes\r\niql.execute(\"SELECT \\* FROM bql('get (...) for (\\\"XYZ\\\")')\")\r\n```\r\n\r\nSometimes it's just easier to break a query into smaller strings.\r\n\r\n```\r\nimport iql\r\nbql_str = \"get (...) for ('XYZ')\"\r\niql.execute(f\"\"\"\r\n    SELECT *\r\n    FROM\r\n    bql(\"{BQL}\")\r\n  \"\"\")\r\n```\r\n\r\n# Getting Started - Kaggle\r\n\r\n## Authentication - KAGGLE_KEY and KAGGLE_USERNAME\r\n\r\nLogin to Kaggle and visit the account page to download the configuration JSON. Extract the KEY and USERNAME from the configuration JSON.\r\n\r\nSet KAGGLE_KEY and KAGGLE_USERNAME to the appropriate values, or set it via:\r\n\r\n```\r\nfrom iql.extensions import kaggle_extension\r\nkaggle_extension.set_kaggle_credentials(kaggle_username='your username', kaggle_key='kaggle API key')\r\n```\r\n\r\n## Usage\r\n\r\nSubQuery syntax is:\r\n\r\n```\r\nkaggle(\"{user}/{dataset}/{filename}\")\r\n```\r\n\r\nExample:\r\n\r\n```\r\nimport iql\r\niql.execute('SELECT \\* FROM kaggle(\"{user}/{dataset}/{filename}\")')\r\n```\r\n\r\nCurrently, only CSV and XLSX datasets are supported. See examples/kaggle_examples.ipynb notebook for more complete examples.\r\n\r\n## Comments\r\n\r\nThe Kaggle extension will download the file once and reuse it as long as it's in the local directory. This will persist across kernel restarts. You can override this behavior by passing the refreshcache=True flag:\r\n\r\n```\r\niql.execute('SELECT * FROM kaggle(\"{user}/{dataset}/{filename}\", refreshcache=True)')\r\n```\r\n\r\nThe Kaggle extension will also load the datafile to an in-memory DataFrame. In our testing, the extension was slower for the first read than DuckDB's read_csv_auto, but subsequent reuse was subsequently much faster. The extension could be modified to download the file and use DuckDB's read_csv_auto feature instead of creating a DataFrame (similar to the AWS S3 Extension). In our testing, our in-memory approach was faster for analytical workloads with ample memory and frequent reuse.\r\n\r\n# Pandas Extension\r\n\r\nThe pandas options are available in every extension, but sometimes its better to run after the data has been first populated in an earlier query.\r\n\r\nThe syntax is:\r\n\r\n```\r\niql.execute(\"\"\"SELECT \\* FROM pandas(table=xyz, pivot=('col1', 'col2', 'values'))\"\"\"\r\n```\r\n\r\nThese operations may also be used in each of the extensions:\r\n\r\n- fillna_pre='string': Before pivoting, replaces only in a single column: DataFrame[\"value\"].fillna(val)\r\n- dropna_pre=True | str | list[str]: Before pivoting, If True, DataFrame.dropna(). Else, DataFrame.dropna(subset=[value])\r\n- pivot=(index,columns,values): DataFrame.pivot(index=index, columns=columns, values=values)\r\n- fillna=val: DataFrame.fillna(val)\r\n- dropna=True | str | list[str]: If True, DataFrame.dropna(). Else, DataFrame.dropna(subset=[value])\r\n\r\nNote: While still in development, [DuckDB's Pivot and Unpivot](https://github.com/duckdb/duckdb/pull/6387) may change how we handle pivoting.\r\n\r\n# Operations available to all IQL SubQueries:\r\n\r\n## Parameter Passing\r\n\r\nThere are two ways to dynamically pass parameters:\r\n\r\n- Fixed List: A list of fixed parameters may be passed. In this example, the paramlist is evaluated first, and any occurence of $SERIESID in the entire fred() option is replaced. One query is run for each value, and the results are UNIONed.\r\n\r\n```\r\nquery2 = f\"\"\"select * from fred(type=\"series\", seriesid=\"$SERIESID\", paramlist=(\"$SERIESID\", [\"UNRATE\", \"EXUSEU\"])) as q1\"\"\"\r\n```\r\n\r\n- Dynamic Lists: A list of values from a previous run query may also be passed. In this example, a list of values is created in the first query, then for each value, a FRED function is called and the results are UNIONed\r\n\r\n```\r\n# Using a parameter list to retrieve multiple series\r\n# SQL uses single quotes for string literals / constants. Unlike Python, SQL treats single and double quotes differently.\r\n\r\nquery2 = f\"\"\"\r\n    CREATE TEMP TABLE series_results as values('UNRATE'), ('EXUSEU');\r\n    SELECT * FROM fred(type=\"series\", seriesid=\"$SERIESID\", paramquery=(\"$SERIESID\", \"select * from series_results\")) as q1\r\n    \"\"\"\r\ndf2 = iql.execute(query2)\r\ndisplay(df2)\r\n```\r\n\r\nLimitations: Only one paramquery or paramlist may be used per SubQuery.\r\n\r\n# Getting Started - FRED Economic Data\r\n\r\nThe FRED extension is a lightweight wrapper around [FRED's REST API](https://fred.stlouisfed.org/docs/api/fred/)\r\n\r\nWe opted to not build yet another parameterized / Pythonic API for FRED. Instead, we use the native REST URLs as the query parameters.\r\n\r\nInstead, just find a REST endpoint want, construct a query string, and query it natively.\r\n\r\n## Get a FRED FRED_API_KEY\r\n\r\nVisit [FRED](https://fred.stlouisfed.org/) and create an account and get your API key.\r\n\r\n## Set the API Key\r\n\r\n```\r\nfrom iql.extensions import fred_extension\r\nfred_extension.FRED_API_KEY = abcdef\r\n```\r\n\r\n## Usage: Types\r\n\r\nReleases, Series and Observations\r\n\r\n```\r\nquery1 = \"\"\"select \\* from fred(type=\"releases\")\"\"\"\r\n\r\nseriesid = \"UNRATE\"\r\nquery2 = f\"\"\"select * from fred(type=\"series\", seriesid=\"{seriesid}\") as q1\"\"\"\r\n\r\nseriesid = \"UNRATE\"\r\ndate_str = \"2023-01-01\"\r\nquery3 = f\"\"\"select * from fred(type=\"observations\", seriesid=\"{seriesid}\", startdate=\"{date_str}\") as q1\"\"\"\r\n```\r\n\r\n## Usage: Raw URL\r\n\r\n```\r\n\"\"\"Get information about a FRED series\"\"\"\r\nseriesid = \"UNRATE\"\r\nquery = f\"SELECT * FROM fred('https://api.stlouisfed.org/fred/series?series_id={seriesid}') as q1\"\r\ndf = iql.execute(query)\r\ndisplay(df)\r\n```\r\n\r\nSee the exammples/fred_examples.ipynb notebook for examples.\r\n\r\n# IQL extension for Bloomberg BQL\r\n\r\nSee [IQL Extension for Bloomberg BQL Readme](BLOOMBERG_BQL_README.md) for more information.\r\n\r\n## Amazon S3 Extension\r\n\r\nThe Amazon S3 extension downloads Parquet and CSV files directly from S3 to a local file, then uses DuckDBs native support to access the local Parquet and CSV files.\r\n\r\nWe implemented this to add our own authentication and caching logic to reduce frequent downloads of the same data.\r\n\r\n## Troubleshooting: If you see an initialization failure, verify that BQL is available and working.\r\n\r\n```\r\nimport bql\r\nbq = bql.Service()\r\nbq.execute(\"get(name) for('IBM US Equity')\")\r\n```\r\n\r\nIf this fails, you are probably not running in BQuant.\r\n\r\n# AWS S3 Extension\r\n\r\nIQL provides an S3 Extension to provide an entrypoint for control of S3 data retrieval and local caching. This extension is intended to be extended to support environment-specific authentication and cache control requirments.\r\n\r\nUsage:\r\n\r\n```\r\nSELECT * FROM s3('s3://bucket/prefix/key/objname.parquet') as data1\r\n```\r\n\r\nWhat happens:\r\n\r\n- objname.parquet is to the iql.iqmo.DEFAULT_EXT_DIRECTORY, or the default dir if no directory is set.\r\n- s3(...) is replaced with a reference to the local Parquet file, allowing the database engine to read the Parquet file natively\r\n\r\n## S3 File Caching\r\n\r\nS3 files are downloaded locally and reused while the cache reference has not expired.\r\n\r\n## AWS Authentication\r\n\r\n- Default behavior: The AWS S3 extension simply calls - boto3.resource(\"s3\"), assuming the environment is already set properly\r\n- Option 1: Create and store boto3 S3 resources to iql.extensions.aws_s3_extension::BOTO_S3_RESOURCES for each bucket\r\n\r\n```\r\nfrom iql.extensions import aws_s3_extension\r\nurl_prefix = \"s3://something/something\"\r\n.... AWS CREDENTIAL STUFF ....\r\ns3res = boto3.resource(\"s3\")\r\naws_s3_extension.BOTO3_S3_RESOURCES[url_prefix] = s3res\r\n```\r\n\r\n- Option 2: Replace iql.extensions.aws_s3_extension::get_boto3_resource_for_request(), which takes a single URL parameter and returns a boto3 resource object\r\n\r\n# Caching\r\n\r\nSubQueries are often expensive and often reused while developing and refining queries. If enabled, IQL will cache the SubQuery results in memory and/or in a persistent file cache.\r\n\r\nCaching can be controlled via activate_cache:\r\n\r\n```\r\ndef activate_cache(duration_seconds: Optional[int], cache_directory: Optional[str]):\r\n```\r\n\r\nduration_seconds:\r\n\r\n```\r\n-1 (Default): Infinite (for life of kernel)\r\nNone: Disabled\r\nint: Maximum life of the cached value from the time of creation.\r\n```\r\n\r\ncache_directory:\r\n\r\n```\r\nNone (Default): Don't use a persistent file cache\r\npath: Location for the external file cache\r\n```\r\n\r\n## Caching Waterfall\r\n\r\n- nocache directive\r\n- cache directive\r\n- extension cache period\r\n- global \\_CACHE_PERIOD\r\n\r\n## Caching Limitations\r\n\r\nCached values are not pruned on expiration. The in memory dictionary is cleared on kernel restart. The file cache will grow unbounded until cleared:\r\n\r\n```\r\nfrom iql import q_cache # clear the in memory and file cache\r\nxq_cache.clear_caches()\r\n```\r\n\r\n## Caching Hints\r\n\r\n### cache=seconds\r\n\r\nOverrides the caching period. -1 is infinite caching.\r\n\r\n```\r\niql.execute(\"\"\"SELECT * FROM keyword(\"...\", cache=60) as q1\"\"\")\r\n```\r\n\r\n### nocache=True\r\n\r\nDoes not use any cached results\r\n\r\n```\r\niql.execute(\"\"\"SELECT * FROM keyword(\"...\", nocache=True) as q1\"\"\")\r\n```\r\n\r\n## Caching Best Practices\r\n\r\nCaching is important to get right but depend on the data and use cache. Caching can significantly improve performance during design and development which frequently reuse unchanging data. For live data that changes frequently, an appropriate cache interval should be used to avoid reuse of stale data.\r\n\r\nIf your kernels are short-lived, this is largely a non-issue, since the cache is reset automatically, unless a file cache is activated.\r\n\r\n# Database\r\n\r\n## Valid Queries\r\n\r\nIn DuckDB, IQL was developed against DuckDB's statements, including SELECTs with CTE's (WITH clauses). IQL is seamless: it only modifies the extension SubQueries, and otherwise passes the results to the database.\r\n\r\nAny valid DuckDB query should be supported.\r\n\r\nWhen troubleshooting, check the usual suspects first:\r\n\r\n- Make sure parentheses and quotes are balanced\r\n  Most query errors are from forgetting a closing quote and/or parenthesis.\r\n- SQL uses single quotes for string literals (constants):\r\n\r\n```\r\n  select 'abc' # 'abc' is a string literal\r\n```\r\n\r\nis not the same as\r\n\r\n```\r\nselect \"abc\" # \"abc\" is a column name\r\n```\r\n\r\n- Valid SQL syntax: Complex SQL queries can be cumbersome. Consider breaking a complex query into several individual steps, at least to refine the logic. This can have a negative performance impact as it defeats any database query optimization, but in practical terms, it is often beneficial.\r\n\r\n## Database Lifecycle\r\n\r\n### Default: In-Memory Database for each iql.execute()\r\n\r\nBy default, a series of iql.execute() calls will create and close an in-memory DuckDB connection for each request. \r\n\r\n### Option 1: Keep Database Open\r\n\r\nUse the iql default connection setting (in-memory only), but leave the connection open:\r\n\r\n```\r\ncon = iql.IQL.get_dbconnector().get_connection()\r\ntry:\r\n  iql.execute(\"CREATE TABLE abc as SELECT * FROM (values(1),(2),(3))\", con=con)\r\n  df=iql.execute(\"SELECT * FROM abc\", con=con)\r\n  display(df)\r\nfinally:\r\n  con.close()\r\n```\r\n\r\nSQL statements separated by semicolons. The entire set will be run sequentially against a single database, so side effects will be maintained.\r\n\r\n### Option 2: Create Database Externally\r\n\r\nWith this method, you can use a file-based persistent database along with other connectivity options.\r\n\r\nOr, create a DuckDB Connection [duckdb.connect()](https://duckdb.org/docs/api/python/overview), such as for a file-based persistent database.\r\n\r\n```\r\ndf=iql.execute(\"SELECT * FROM abc\", con=con)\r\n```\r\n\r\n# FAQ\r\n\r\n## Why DuckDB as the default?\r\n\r\nWe chose [DuckDB](https://duckdb.org/) as the default database module for a few reasons:\r\n\r\n- DuckDB is awesome and [fast](https://duckdb.org/2021/05/14/sql-on-pandas.html), with vectorized columnar operations.\r\n- It runs with no setup\r\n- It runs fully locally and has support for a variety of data sources\r\n- DuckDB's SQL language is standard\r\n- DuckDB natively supports Pandas\r\n\r\n## Why not a DuckDB Extensions?\r\n\r\nWe could have written this as a set of DuckDB extensions, but we didn't. Why?\r\n\r\n- Portability: DuckDB is great, but it's not the only game in town. PyArrow and SnowFlake are important.\r\n- Native Python is easy to develop, easy to debug, and convenient to modify and extend.\r\n- Performance: In our workflows, there was little performance to be gained. Runtime was dominated by external data transfer.\r\n\r\n## Databases other than DuckDB:\r\n\r\nAny database can be supported by implementing a database module. The key step that's dependent on the database engine is registering (or loading) the SubQuery dataframes to the database engine prior to executing the queries.\r\n\r\nModules could be added to support other databases:\r\n\r\n- [SQLDF](https://pypi.org/project/sqldf/) and [PandaSQL](https://pypi.org/project/pandasql/): Local-only databases that can connect to in-memory Pandas dataframes\r\n- PyArrow: SubQuery dataframes would be loaded via [pyarrow.Table.from_pandas()](https://arrow.apache.org/docs/python/pandas.html)\r\n- SnowFlake: During registration step, the Pandas dataframes need to be loaded via the [SnowFlake Pandas Connector](https://docs.snowflake.com/en/user-guide/python-connector-pandas)\r\n- Other Pandas-centric engines, such as SQLDF and PandaSQL\r\n\r\n## What about Polars?\r\n\r\nSince DuckDB supports Polars, IQL extensions could be modified to use Polars DataFrames since DuckDB supports Polars. This would be a relatively simple change, made in each extension to create a Polars DataFrame instead of a Pandas DataFrame. This could be made extensible, so the default DataFrame implementation is user selectable.\r\n\r\n## Design Principles\r\n\r\n- Extensibility: Extensions and Database Connectors can be easily modified, replaced, or extended.\r\n- KISS: Keep it simple. Don't add complexity.\r\n  - REST APIs, such as FRED: Use the complete URL, rather than building yet-another-Python-API\r\n  - Bloomberg BQL: Use native BQL queries without modification\r\n- Minimal dependencies: Extensions are loaded on-demand. Unused dependencies are not required.\r\n\r\n# Footnotes\r\n\r\n## Useful DuckDB Features\r\n\r\n### CTEs\r\n\r\n```\r\nimport iql\r\ndf = iql.execute(\"\"\"\r\n  WITH c AS keyword(\"...\"),\r\n      idx AS keyword(\"...\")\r\n    SELECT c.*, idx.*\r\n    FROM c\r\n    JOIN idx\r\n      ON c.idx=idx.id\"\"\")\r\ndisplay(df)\r\n```\r\n\r\n### Accessing Global DataFrames:\r\n\r\n```\r\nimport iql\r\nimport pandas as pd\r\n\r\nfun = pd.DataFrame([{'id': 'Someone', 'fun_level': 'High'}])\r\niql.execute(\"\"\"SELECT * FROM fun\"\"\")\r\n```\r\n\r\n### Copy (query) to 'file'\r\n\r\n```\r\nimport iql\r\niql.execute(\"\"\"COPY (query) TO 'somefile.parquet'\"\"\")\r\n```\r\n\r\n## Copy to Paruet\r\n\r\n- Copy to parquet:\r\n  https://duckdb.org/docs/guides/import/parquet_export.html#:~:text=To%20export%20the%20data%20from,exported%20to%20a%20Parquet%20file.\r\n\r\n# Futures\r\n\r\n## SQL ReWrite\r\nInstead of modifying the SQL in a single step, we could introduce an intermediate statement that has the same logical flow as the code today. This would make it easier to debug, allowing the user to view and debug each step. \r\n```\r\nSELECT * FROM fred() a JOIN fred() b on a.id=b.id\r\n```\r\ncould be transformed first into:\r\n```\r\na=fred();\r\nb=fred();\r\nSELECT * FROM a JOIN b\r\n```\r\n\r\nOne decision needed here is how to express the first two statements: would we use a CREATE TEMP TABLE or COPY TO to store the SubQuery results, or do we introduce something like CREATE DF.  \r\n\r\n## Simplifying Parsing\r\n\r\nWe didn't implement a grammar, because each grammar is very platform dependent. Each database has its own product-specific grammar. \r\n\r\nThe current IQL implementation first parses the SQL to extract the named functions, using the sqlparse library, then extracts the IQL subquerys by their named keywords. The SubQueries are then parsed via an AST to extract the parameters and values. Any parsing introduces risks and fragility:  \r\n- It's possible that sqlparse will fail to parse certain database specific language features. We haven't encountered this yet, but it's something we're thinking about\r\n- It's also possible that our extraction will fail to recognize proper subqueries, due to how sqlparse extracts the tokens. The code here is not as robust as we'd like, and more testing is needed.\r\n\r\nThere's a few ways to improve this:\r\n- Direct string extraction: identify subquery() blocks and extract them directly as strings, rather than parsing the entire SQL file. This would have to properly account for commenting, quoting, and nesting.\r\n- DuckDB (or whatever platform) extensions: use a lightweight extension to allow the database to externally call the IQL layer, rather than having IQL act as an intermediate step.\r\n\r\n## Caching\r\nThe in-memory cache will grow unbounded within each kernel session. The expiration is only used to invalid data, but expired results are not evicted from memory if not accessed. \r\n\r\nWe'll replace the in-memory cache with something more robust, probably another cache implementation such as [cachetools](https://github.com/tkem/cachetools). Alternatively, IQL could maintain a cache per \"session\", rather than globally.\r\n\r\nIf your kernels are long-lived, clear the cache at appropriate intervals or points in your workflow:\r\n\r\n```\r\niql.clear_caches()\r\n```\r\n\r\n## Parquet Files or PyArrow vs DataFrame\r\n\r\nIn-Memory Extensions could serialize data to other formats, instead of to DataFrames. \r\n\r\nNote: This isn't required for file-based extensions like AWS S3. \r\n\r\n## Projection and Filter Pushdowns\r\n\r\nSubQueries are executed in their entirety _first_ and then queried by the database.\r\n\r\nThis could be modified to add support for projection and filter pushdowns, similar to what was done in the [DuckDB - Querying Parquet](https://duckdb.org/2021/06/25/querying-parquet.html).\r\n\r\nThe main concern here is both complexity and dependency on database specifics. Currently, paramquery's provide a similar (but more limited) benefit with little added complexity.\r\n\r\n## Limitations\r\n\r\nSubQueries are executed in their entirety, so a SubQuery cannot be parameterized based on results from another SubQuery without using an intermediate table: execute the first SubQuery, store the results in a table, then execute the second SubQuery with a paramquery.\r\n\r\n# Footer\r\n\r\nCopyright (C) 2023, IQMO Corporation [info@iqmo.com]\r\nAll Rights Reserved\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://www.iqmo.com",
    "keywords": "IQMO Query Language IQMOQL BQL BQUANT FRED EDGAR KAGGLE",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "iql",
    "package_url": "https://pypi.org/project/iql/",
    "platform": null,
    "project_url": "https://pypi.org/project/iql/",
    "project_urls": {
      "Homepage": "https://www.iqmo.com"
    },
    "release_url": "https://pypi.org/project/iql/0.1.6/",
    "requires_dist": [
      "duckdb (>=0.7.0)",
      "pandas (>=1.3.4)",
      "sqlparse (>=0.4.3)"
    ],
    "requires_python": "",
    "summary": "SQL Overlay Language with support for Bloomberg BQL, FRED, Edgar, Kaggle and other financial data sources within SQL",
    "version": "0.1.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17229859,
  "releases": {
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "95bd9e4082b4b895663d23c768ba787cd1f2be1d0323dc7f5df20e9d71564a03",
          "md5": "eb0f09eeb069b9bfe3712f5b52d7e532",
          "sha256": "bf559062d21a251684cbf7d91f3f4dd095dd25480fe804895df24e89623effb4"
        },
        "downloads": -1,
        "filename": "iql-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eb0f09eeb069b9bfe3712f5b52d7e532",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 71037,
        "upload_time": "2023-03-08T17:56:48",
        "upload_time_iso_8601": "2023-03-08T17:56:48.325851Z",
        "url": "https://files.pythonhosted.org/packages/95/bd/9e4082b4b895663d23c768ba787cd1f2be1d0323dc7f5df20e9d71564a03/iql-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "00754d23d46f2e95ea43b8b7caff4d34aaedf226b7d0618f0b3bfa4f019ec2a8",
          "md5": "9c49ae8899f2d7ba789a6c81195dc666",
          "sha256": "5d2832a35f1f4713eacdb41ddfab57907fb9e1b5b91212ddf0b1b6c7adbe4b18"
        },
        "downloads": -1,
        "filename": "iql-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9c49ae8899f2d7ba789a6c81195dc666",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 71031,
        "upload_time": "2023-03-08T19:35:47",
        "upload_time_iso_8601": "2023-03-08T19:35:47.981171Z",
        "url": "https://files.pythonhosted.org/packages/00/75/4d23d46f2e95ea43b8b7caff4d34aaedf226b7d0618f0b3bfa4f019ec2a8/iql-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c673ce62ffaa353d15125043bc0275ee767ef5e6f26dd4bebc8327b4dfe4e54e",
          "md5": "3f85cdcb25d5ed4cd585397257732832",
          "sha256": "75445be7a272009f2bb37131496c2585bc8ad41f0fbdd6bb1165630fae99eaa3"
        },
        "downloads": -1,
        "filename": "iql-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3f85cdcb25d5ed4cd585397257732832",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 73341,
        "upload_time": "2023-03-09T14:04:58",
        "upload_time_iso_8601": "2023-03-09T14:04:58.599536Z",
        "url": "https://files.pythonhosted.org/packages/c6/73/ce62ffaa353d15125043bc0275ee767ef5e6f26dd4bebc8327b4dfe4e54e/iql-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "36b2ff116e2dffb1e862ad4eda89b8d35e928542eb46a5704b904f476d3a6166",
          "md5": "1e6bf12fecb8c97aca035ef21ba7043c",
          "sha256": "fc6a3825df0e082fd405dfafa6c8d4a2398c5f2e656744370be682d9b0d26dec"
        },
        "downloads": -1,
        "filename": "iql-0.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1e6bf12fecb8c97aca035ef21ba7043c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 75243,
        "upload_time": "2023-03-09T21:56:34",
        "upload_time_iso_8601": "2023-03-09T21:56:34.014207Z",
        "url": "https://files.pythonhosted.org/packages/36/b2/ff116e2dffb1e862ad4eda89b8d35e928542eb46a5704b904f476d3a6166/iql-0.1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "36b2ff116e2dffb1e862ad4eda89b8d35e928542eb46a5704b904f476d3a6166",
        "md5": "1e6bf12fecb8c97aca035ef21ba7043c",
        "sha256": "fc6a3825df0e082fd405dfafa6c8d4a2398c5f2e656744370be682d9b0d26dec"
      },
      "downloads": -1,
      "filename": "iql-0.1.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1e6bf12fecb8c97aca035ef21ba7043c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 75243,
      "upload_time": "2023-03-09T21:56:34",
      "upload_time_iso_8601": "2023-03-09T21:56:34.014207Z",
      "url": "https://files.pythonhosted.org/packages/36/b2/ff116e2dffb1e862ad4eda89b8d35e928542eb46a5704b904f476d3a6166/iql-0.1.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}