{
  "info": {
    "author": "Arsam Aryandoust",
    "author_email": "arsama@ethz.ch",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "The name altility stands for 'actively learning utility', and was first developed\nto help electric utilities in the process of placing new smart meters in space and \ncollecting their data at different times. This package, however, can now be used\nand be further developed for any type of spatio-temporal prediction task.\n\n\n### Installation:\n```\npip install altility\n```\n\n\n### Docker:\nFor using altility within an Ubuntu docker container\n```\ndocker run -it aryandoustarsam/altility\n```\n\nFor using altility with Jupyter notebook inside a docker container\n```\ndocker run -it -p 3333:1111 -v ~/path_to_data/data:/data aryandoustarsam/altility:jupyter\n[inside running container]: jupyter notebook --ip 0.0.0.0 --port 1111 --no-browser --allow-root\n[in local machine browser]: localhost:3333 \n[in local machine browser, type token shown in terminal]\n```\n\n\n### Usage guide:\nAt the core of the altility package stands the class **altility.ADL_model**. It \nbundles properties and methods of the active deep learning (ADL) model that we \nwant to train. Bellow is a list of all parameters it takes when initialized, methods \nit contains and results it can generate.\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Parameters </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>name (='adl_model')</b>: <br />  string\n    </td>\n    <td>\n      The name of active deep learning (ADL) model.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>path_to_results (='results')</b>: <br />  string\n    </td>\n    <td>\n      The path to where resulting plots and values are supposed to be stored.\n    </td>\n  </tr> \n    \n</table>\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Methods </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>initialize(y, x_t=None, x_s=None, x_st=None, **kwargs):</b>\n    </td>\n    <td>\n      Initializes prediction model.\n    </td>\n  </tr> \n  \n  <tr> \n    <td>\n      <b>collect(x_t_cand=None, x_s_cand=None, x_st_cand=None, **kwargs):</b>\n    </td>\n    <td>\n      Collect candidate data with embedding uncertainty active learning.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>train(y_picked, x_t_picked=None, x_s_picked=None, x_st_picked=None, **kwargs):</b>\n    </td>\n    <td>\n      Train model with queried labels of chosen candidate data points.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>predict(y_pred=None, x_t_cand=None, x_s_cand=None, x_st_cand=None, **kwargs):</b>\n    </td>\n    <td>\n      Predict labels for unqueried candidate data points. If you are testing model,\n      and have labels available, you can pass these and see the difference between\n      true and predicted labels of unqueried candidate data points.\n    </td>\n  </tr>\n    \n</table>\n\n\n\n### Methods:\n\nA complete lits of key word arguments or parameters that can be passed to\n**ADL_model.initialize()**\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Parameters </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>y (required)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of labels.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_t (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of time-variant features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_s (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of space-variant features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_st (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of space-time-variant features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>encoder_layers (=1)</b>: <br /> int\n    </td>\n    <td>\n      Choose how many neural network layers you want to use for encoding features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>network_layers (=1)</b>: <br /> int\n    </td>\n    <td>\n      Choose how many layers you want to use after encoders. This is your network \n      depth.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>encoding_nodes_x_t (=100)</b>: <br /> int\n    </td>\n    <td>\n      Choose the dimension of the encoding outcome of temporal features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>encoding_nodes_x_s (=100)</b>: <br /> int\n    </td>\n    <td>\n      Choose the dimension of the encoding outcome of spatial features.\n    </td>\n  </tr>\n    \n  <tr> \n    <td>\n      <b>encoding_nodes_x_st (=100)</b>: <br /> int\n    </td>\n    <td>\n      Choose the dimension of the encoding outcome of spatio-temporal features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>encoding_nodes_joint (=100)</b>: <br /> int\n    </td>\n    <td>\n      Choose the dimension of the encoding outcome of entire concatenated \n      feature vector.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>nodes_per_layer_dense (=1000)</b>: <br /> int\n    </td>\n    <td>\n      Choose how many nodes per dense layer you want to use. This determines the \n      width of your network.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>filters_per_layer_cnn (=16)</b>: <br /> int\n    </td>\n    <td>\n      Choose how many filtes per convolutional layer you want to use.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>states_per_layer_rnn (=200)</b>: <br /> int\n    </td>\n    <td>\n      Choose how many states per recurrent layer you want to use.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>activation_encoding (='relu')</b>: <br /> string\n    </td>\n    <td>\n      Choose which activation function to use on last encoding layer. Choose from \n      None, 'relu', 'tanh', 'selu', 'elu', 'exponential'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>activation_dense (='relu')</b>: <br /> string\n    </td>\n    <td>\n      Choose which activation function to use in each dense layer. Choose from\n      None, 'relu', 'tanh', 'selu', 'elu', 'exponential'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>activation_cnn (='relu')</b>: <br /> string\n    </td>\n    <td>\n      Choose which activation function to use in each convolutional layer. Choose\n      from None, 'relu', 'tanh', 'selu', 'elu', 'exponential'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>activation_rnn (='tanh')</b>: <br /> string\n    </td>\n    <td>\n      Choose which activation function to use in each recurrent layer. Choose\n      from None, 'relu', 'tanh', 'selu', 'elu', 'exponential'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>layer_type_x_st (='CNN')</b>: <br /> string\n    </td>\n    <td>\n      Choose which layers to use for X_st inputs. Choose one from 'ANN', 'CNN', \n      'LSTM'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>initialization_method (='glorot_normal')</b>: <br /> string\n    </td>\n    <td>\n      Choose how to initiliaze weights for Conv1D, Conv2D and Dense layers. \n      Choose from 'glorot_normal'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>initialization_method_rnn (='orthogonal')</b>: <br /> string\n    </td>\n    <td>\n      Choose how to initiliaze weights for LSTM layers. Choose from 'orthogonal'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>regularizer (='l1_l2')</b>: <br /> string\n    </td>\n    <td>\n      Choose how to regularize weights. Choose from None, 'l1', 'l2', 'l1_l2'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>batch_normalization (=False)</b>: <br /> bool\n    </td>\n    <td>\n      Choose whether or not to use batch normalization on each layer in your NN.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>train_split (=0.7)</b>: <br /> float \n    </td>\n    <td>\n      Choose on the splitting ratio between training and validation datasets. \n      Choose a value between 0 and 1.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>split_intervals (=0.05)</b>: <br /> float \n    </td>\n    <td>\n     Decide in which frequency to do train-validation split. 1 equals one datapoint \n     per bin, 0.5 equals two datapoints per bin.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>random_seed (=None)</b>: <br /> float \n    </td>\n    <td>\n     Provide a seed for reproducibility of your experiments. This is then used\n     when initializing weights of deep learning model, when choosing random\n     data sequences during training and anywhere, where stochastic processes play\n     a role.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>epochs (=30)</b>: <br /> int \n    </td>\n    <td>\n     Choose for how many epochs you want to train your model.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>patience (=10)</b>: <br /> int \n    </td>\n    <td>\n     Choose how many epochs to have patience on not increasing validation loss \n     during training before early stopping.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>batch_size (=16)</b>: <br /> int \n    </td>\n    <td>\n     Choose how large your data batch size should be during training. Choose a \n     value to the power of 2.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>monitor (='val_loss')</b>: <br /> string \n    </td>\n    <td>\n     Choose which value to monitor for early stopping. Choose from 'val_loss' and\n     'train_loss'.\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>silent (=True)</b>: <br /> bool\n    <td>\n      Decide whether or not to print out progress.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>plot (=False)</b>: <br /> bool\n    <td>\n      Decide whether or not to visualize process.\n    </td>\n    </td>\n  </tr>\n  \n</table>\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Results </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>models</b>: <br /> list of Tensorflow models\n    </td>\n    <td>\n      List of computational graphs that compound our active deep learning embedding\n      network.\n    </td>\n  </tr>\n    \n</table>\n\n\nA complete lits of key word arguments or parameters that can be passed to\n**ADL_model.collect()**\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Parameters </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_t_cand (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of time-variant features for candidate data points.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_s_cand (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of space-variant features for candidate data points.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_st_cand (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of space-time-variant features for candidate data points.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>budget (=0.5)</b>: <br /> float\n    </td>\n    <td>\n      Choose which share of candidate data pool we want to select. This is our\n      data budget for new querying new data points. Choose a value between 0 and\n      1.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>method (='embedding_uncertainty')</b>: <br /> string\n    </td>\n    <td>\n      Choose which active learning method to use. Currently, only queries with\n      embedding uncertainty are supported.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>method_variant (='max_uncertainty')</b>: <br /> string\n    </td>\n    <td>\n      Choose which variant of the active learning method to use. Choose from\n      'max_uncertainty', 'min_uncertainty', 'avg_uncertainty' and 'rnd_uncertainty'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>method_distance (='laplacian_kernel')</b>: <br /> string\n    </td>\n    <td>\n      Choose which distance metric to use for calculating embedding uncertainty\n      to cluster centers. Choose from 'rbf_kernel', 'laplacian_kernel' and \n      'cosine_similarity'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>method_cluster (='KMeans')</b>: <br /> string\n    </td>\n    <td>\n      Choose which clusting method to use for clusting embedded candidate data\n      points. Choose from 'rbf_kernel', 'laplacian_kernel' and 'cosine_similarity'.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>subsample (=None)</b>: <br /> int\n    </td>\n    <td>\n      Choose None or a subsample size of uniformly chosen candidates.\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>silent (=True)</b>: <br /> bool\n    <td>\n      Decide whether or not to print out progress.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>plot (=False)</b>: <br /> bool\n    <td>\n      Decide whether or not to visualize process.\n    </td>\n    </td>\n  </tr>\n  \n</table>\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Results </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>batch_index_list</b>: <br /> list of integers\n    </td>\n    <td>\n      List of indices for most informative data points suggested to collect.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>inf_score_list</b>: <br /> list of floats\n    </td>\n    <td>\n      List of information scores for most informative data points suggested to \n      collect.\n    </td>\n  </tr>\n    \n</table>\n\n\nA complete lits of key word arguments or parameters that can be passed to\n**ADL_model.train()**\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Parameters </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>y_picked (required)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of labels.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_t_picked (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of time-variant features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_s_picked (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of space-variant features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_st_picked (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of space-time-variant features.\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>silent (=True)</b>: <br /> bool\n    <td>\n      Decide whether or not to print out progress.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>plot (=False)</b>: <br /> bool\n    <td>\n      Decide whether or not to visualize process.\n    </td>\n    </td>\n  </tr>\n  \n</table>\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Results </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>models</b>: <br /> list of Tensorflow models\n    </td>\n    <td>\n      List of computational graphs that compound our active deep learning embedding\n      network further trained on the passed dataset of picked candidate data.\n    </td>\n  </tr>\n    \n</table>\n\n\nA complete lits of key word arguments or parameters that can be passed to\n**ADL_model.predict()**\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Parameters </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>y_pred (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of labels.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_t_pred (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of time-variant features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_s_pred (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of space-variant features.\n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>x_st_pred (=None)</b>: <br /> numpy array\n    </td>\n    <td>\n      Array or matrix of space-time-variant features.\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>silent (=True)</b>: <br /> bool\n    <td>\n      Decide whether or not to print out progress.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>plot (=False)</b>: <br /> bool\n    <td>\n      Decide whether or not to visualize process.\n    </td>\n    </td>\n  </tr>\n  \n</table>\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Results </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>predictions</b>: <br /> list of floats\n    </td>\n    <td>\n      List of predictions made for passed features. \n    </td>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>testing_loss</b>: <br /> float\n    </td>\n    <td>\n      Testing loss score calculated from true vs. predicted labels. Only calculated\n      if true labels <b>'y_pred'</b> are provided.\n    </td>\n  </tr>\n    \n</table>\n\n\n### Datasets:\nThe package can be tested on datasets that are either publicly available, or which\nwe make public for making spatio-temporal predictions. A first dataset consists of \nelectric load that we provide in our Github repository. To prepare the data\nfor usage with altility, use the **prep_load_forecasting_data()** function provided\nin **load_forecasting.py** with the following parameter and return values:\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Parameters </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>path_to_data (='data/public/electric load forecasting/')</b>: <br />  string\n    </td>\n    <td>\n      The path to where data is stored. This is 'data/public/electric load forecasting/'\n      in our original repository.\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>dataset_name (='profiles_100')</b>: <br /> string\n    <td>\n      Choose between 'profiles_100' and 'profiles_400'. These are two distinct\n      datasets containing load profiles from either 100 or 400 industrial, commercial,\n      and residential buildings of different sizes, shapes, consumption and occupancy\n      patterns in Switzerland.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>label_type (='feature_scaled')</b>: <br /> string\n    <td>\n      Decide which labels to consider. Choose from 'random_scaled' and 'feature_scaled'.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>spatial_features (='histogram')</b>: <br /> string\n    <td>\n      Decide how to treat aerial imagery. Choose one from 'average' and 'histogram'.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>meteo_types </b>: <br /> list\n    <td>\n      Decide which meteo data types to consider. Choose from 'air_density', \n      'cloud_cover', 'precipitation', 'radiation_surface', 'radiation_toa',\n      'snow_mass', 'snowfall', 'temperature' and 'wind_speed'. The default is a \n      list of all meteorological conditions.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>timestamp_data </b>: <br /> list\n    <td>\n      Decide which time stamp information to consider. Choose from: '15min',\n      'hour', 'day', 'month' and 'year'.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>time_encoding (='ORD')</b>: <br /> string\n    <td>\n      Decide how to encode time stamp data. Choose one of 'ORD', 'ORD-1D' or 'OHE'\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>histo_bins (=100)</b>: <br /> int\n    <td>\n      Set the number of histogram bins that you want to use. Applied if parameter\n      spatial_features = 'histogram'.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>grey_scale (=False)</b>: <br /> bool\n    <td>\n      Decide whether you want to consider underlying RGB images in grey-scale.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>profiles_per_year (=1)</b>: <br /> float\n    <td>\n      Decide how many building-year profiles you want to consider for each year. \n      Choose a share between 0 and 1. A value of 1 corresponds to about 100 profiles \n      for the profiles_100 and 400 profiles for the profiles_400 dataset.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>points_per_profile (=0.003)</b>: <br /> float\n    <td>\n      Decide how many data points per building-year profile you want to consider. \n      Choose a share between 0 and 1. A value of 0.01 corresponds to approximately \n      350 points per profile.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>history_window_meteo (=24)</b>: <br /> int\n    <td>\n      Choose past time window for the meteo data. Resolution is hourly.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>prediction_window (=96)</b>: <br /> int\n    <td>\n      Decide how many time steps to predict consumption into the future. Resolution \n      is 15 min. A values of 96 corresponds to 24h.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>test_split (=0.7)</b>: <br /> float\n    <td>\n      Decides how many buildings and how much of the time period to separate for\n      testing.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>normalization (=True)</b>: <br /> bool\n    <td>\n      Decide whether or not to normalize features.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>standardization (=True)</b>: <br /> bool\n    <td>\n      Decide whether to standardize features to zero mean and unit variance.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>silent (=True)</b>: <br /> bool\n    <td>\n      Decide whether or not to print out progress of data processing.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>plot (=False)</b>: <br /> bool\n    <td>\n      Decide whether or not to visualize examples of processed data.\n    </td>\n    </td>\n  </tr>\n    \n</table>\n\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Returns </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>datasets</b>: <br /> dict\n    </td>\n    <td>\n      A dictionary containing available and candidate data, that are stored with\n      the keys <b>'avail_data'</b> and <b>'cand_data'</b>. These are dictionaries \n      themselves, and store variables under keys <b>'x_t'</b>, <b>'x_s'</b>, \n      <b>'x_st'</b> and <b>'y'</b>. These stand for only time-variant features \n      <b>'x_t'</b>, only space-variant features <b>'x_s'</b>, space- and \n      time-variant features <b>'x_st'</b> and labels <b>'y'</b>.\n    </td>\n  </tr>\n    \n</table>\n\n\nA second dataset consists of travel time data provided by the Uber movement project.\n**Note:** This data is licensed under Creative Commons, Attribution Non-Commercial\n(https://creativecommons.org/licenses/by-nc/3.0/us/). This is different from the\nMIT license we provide for our package here. To prepare the data for usage with \naltility, use the **prep_travel_forecasting_data()** function provided in \n**travel_forecasting.py** with the following parameters and return values.\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Parameters </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>path_to_data (='data/public/travel time forecasting/')</b>: <br /> string\n    </td>\n    <td>\n      The path to where data is stored. This is 'data/public/travel time forecasting/'\n      in our original repository.\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>dataset_name (='Uber movement')</b>: <br /> string\n    <td>\n      This is currently the only dataset source we provide for travel time data.\n      An alternative source is the Google Maps API.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>city_name (='Amsterdam')</b>: <br /> string\n    <td>\n      Choose a city for which you want to predict missing travel time data between\n      their single city zones. All available cities can be seen under the path\n      'data/public/travel time forecasting/Uber movement/'.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>test_split (=0.7)</b>: <br /> float\n    <td>\n      Decides how many data to separate for creating the candidate data pool.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>time_encoding (='ORD')</b>: <br /> string\n    <td>\n      Decide how to encode time stamp data. Choose one of 'ORD' for ordinal encoding\n      or 'OHE' for one-hot encoding.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>normalization (=True)</b>: <br /> bool\n    <td>\n      Decide whether or not to normalize features.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>standardization (=True)</b>: <br /> bool\n    <td>\n      Decide whether to standardize features to zero mean and unit variance.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>silent (=True)</b>: <br /> bool\n    <td>\n      Decide whether or not to print out progress of data processing.\n    </td>\n    </td>\n  </tr>\n  \n  <tr>\n    <td>\n      <b>plot (=False)</b>: <br /> bool\n    <td>\n      Decide whether or not to visualize examples of processed data.\n    </td>\n    </td>\n  </tr>\n    \n</table>\n\n\n<table>\n\n  <tr>\n    <th scope='row' colspan='2'> Returns </th>\n  </tr>\n  \n  <tr> \n    <td>\n      <b>datasets</b>: <br /> dict\n    </td>\n    <td>\n      A dictionary containing available and candidate data, that are stored with\n      the keys <b>'avail_data'</b> and <b>'cand_data'</b>. These are dictionaries \n      themselves, and store variables under keys <b>'x_t'</b>, <b>'x_s'</b> and \n      <b>'y'</b>. These stand for only time-variant features <b>'x_t'</b>, only \n      space-variant features <b>'x_s'</b> and labels <b>'y'</b>.\n    </td>\n  </tr>\n    \n</table>\n\n\n\n\n### Examples:\n\nAn example for forecasting electric consumption of single buildings.\n```\nimport altility.adl_model as adl_model\nimport altility.datasets.load_forecasting as load_forecasting\n\n### Import and prepare load forecasting data\ndatasets = load_forecasting.prep_load_forecasting_data(\n    silent=False,\n    plot=True\n)\n\n\n### Get features and labels for available data\ny = datasets['avail_data']['y']\nx_t = datasets['avail_data']['x_t']\nx_s = datasets['avail_data']['x_s']\nx_st = datasets['avail_data']['x_st']\n\n\n### Get features and labels for candidate data from spatio-temporal test set\ny_cand = datasets['cand_data']['y']\nx_t_cand = datasets['cand_data']['x_t']\nx_s_cand = datasets['cand_data']['x_s']\nx_st_cand = datasets['cand_data']['x_st']\n\n\n### Create a class instance\nADL_model = adl_model.ADL_model('Electrific f_nn')\n\n\n### Initialize model by creating and training it\nADL_model.initialize(\n    y,\n    x_t,\n    x_s,\n    x_st,\n    silent=True,\n    plot=True\n)\n\n\n### Collect candidate data\nADL_model.collect(\n    x_t_cand,\n    x_s_cand,\n    x_st_cand,\n    silent=True,\n    plot=False\n)\n\n\n### Create one array for picked and one for unpicked data to be predicted\npicked_array = np.zeros([len(y_cand),], dtype=bool)\npicked_array[ADL_model.batch_index_list] = True\npred_array = np.invert(picked_array)\n\n\n### Extract selected data from candidate data pool for training\ny_picked = y_cand[picked_array]\nx_t_picked = x_t_cand[picked_array]\nx_s_picked = x_s_cand[picked_array]\nx_st_picked = x_st_cand[picked_array]\n\n\n### Train model with picked data\nADL_model.train(\n    y_picked,\n    x_t_picked,\n    x_s_picked,\n    x_st_picked,\n    silent=False,\n    plot=True\n)\n\n\n### Extract not selected data from candidate data pool for testing/predicting\ny_pred = y_cand[pred_array]\nx_t_pred = x_t_cand[pred_array]\nx_s_pred = x_s_cand[pred_array]\nx_st_pred = x_st_cand[pred_array] \n\n\n### Predict on remaining data\nADL_model.predict(\n    y_pred,\n    x_t_pred,\n    x_s_pred,\n    x_st_pred,\n    silent=False,\n    plot=True\n)\n```\n\nAn example for forecasting travel times between single city zones.\n```\nimport altility.adl_model as adl_model\nimport altility.datasets.travel_forecasting as travel_forecasting\n\n\n### Import and prepare travel forecasting data\ndatasets = travel_forecasting.prep_travel_forecasting_data(\n    silent=False,\n    plot=True\n)\n\n\n### Get features and labels for available data\nn_points=1000\ny = datasets['avail_data']['y'][:n_points]\nx_t = datasets['avail_data']['x_t'][:n_points]\nx_s = datasets['avail_data']['x_s'][:n_points]\n\n\n### Get features and labels for candidate data from spatio-temporal test set\ny_cand = datasets['cand_data']['y'][:n_points]\nx_t_cand = datasets['cand_data']['x_t'][:n_points]\nx_s_cand = datasets['cand_data']['x_s'][:n_points]\n\n\n### Create a class instance\nADL_model = adl_model.ADL_model('Spacetimetravelic f_nn')\n\n\n### Initialize model by creating and training it\nADL_model.initialize(\n    y,\n    x_t=x_t,\n    x_s=x_s,\n    silent=True,\n    plot=True\n)\n\n\n### Show us if we created all models\nfor model_name, model in ADL_model.models.items():\n    print(model_name)\n\n\n### Collect candidate data\nADL_model.collect(\n    x_t_cand,\n    x_s_cand,\n    silent=False,\n    plot=True\n)\n\n\n### Create one array for picked and one for unpicked data to be predicted\npicked_array = np.zeros([len(y_cand),], dtype=bool)\npicked_array[ADL_model.batch_index_list] = True\npred_array = np.invert(picked_array)\n\n\n### Extract selected data from candidate data pool for training\ny_picked = y_cand[picked_array]\nx_t_picked = x_t_cand[picked_array]\nx_s_picked = x_s_cand[picked_array]\n\n\n### Train model with picked data\nADL_model.train(\n    y_picked,\n    x_t_picked,\n    x_s_picked,\n    silent=False,\n    plot=True\n)\n\n\n### Extract not selected data from candidate data pool for testing/predicting\ny_pred = y_cand[pred_array]\nx_t_pred = x_t_cand[pred_array]\nx_s_pred = x_s_cand[pred_array] \n\n\n### Predict on remaining data\nADL_model.predict(\n    y_pred,\n    x_t_pred,\n    x_s_pred,\n    silent=False,\n    plot=True\n)\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "altility",
    "package_url": "https://pypi.org/project/altility/",
    "platform": "",
    "project_url": "https://pypi.org/project/altility/",
    "project_urls": {
      "Code repository": "https://github.com/ArsamAryandoust/altility"
    },
    "release_url": "https://pypi.org/project/altility/0.0.3/",
    "requires_dist": [
      "numpy (>=1.20.0)",
      "pandas (>=1.3.0)",
      "matplotlib (>=3.4.0)",
      "tensorflow (>=2.7.0)",
      "sklearn (>=1.0.2)"
    ],
    "requires_python": "",
    "summary": "Enhanced spatio-temporal predictions using less data with active deep learning",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12550005,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "759b93f57dcdec0a88dc084121b87de0c29bf7c3aee6616931ac27c176607b9c",
          "md5": "536e0543f8695aaa90838fb21f30cc05",
          "sha256": "89f266bd25a011cb7a0338dfecc56a5a892c21e8ac541256a37901f23e9d7139"
        },
        "downloads": -1,
        "filename": "altility-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "536e0543f8695aaa90838fb21f30cc05",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 1437,
        "upload_time": "2021-11-23T07:24:20",
        "upload_time_iso_8601": "2021-11-23T07:24:20.333902Z",
        "url": "https://files.pythonhosted.org/packages/75/9b/93f57dcdec0a88dc084121b87de0c29bf7c3aee6616931ac27c176607b9c/altility-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6067c7c93b189ed8f3c81bb2b971746cf4639fde568dd855b970ea916c935de2",
          "md5": "a0409abc997d49c530aa68a1b76ce5a3",
          "sha256": "ee73c22616a8782367e625d94b1eebedd04eed7ab1f61716007f89cc91f4cecb"
        },
        "downloads": -1,
        "filename": "altility-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a0409abc997d49c530aa68a1b76ce5a3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 1383,
        "upload_time": "2021-11-23T07:24:21",
        "upload_time_iso_8601": "2021-11-23T07:24:21.582944Z",
        "url": "https://files.pythonhosted.org/packages/60/67/c7c93b189ed8f3c81bb2b971746cf4639fde568dd855b970ea916c935de2/altility-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a514e638494e191ccb342c2e70decf03d42412c52567a4cfa42a22e9becf457e",
          "md5": "76c84307584f1c3b32df5a282ead0109",
          "sha256": "45817e0ee95959f20a60eb71d7b725d42950dd878ba8cd7d4b423626d6909f97"
        },
        "downloads": -1,
        "filename": "altility-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "76c84307584f1c3b32df5a282ead0109",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 8091,
        "upload_time": "2022-01-08T22:12:54",
        "upload_time_iso_8601": "2022-01-08T22:12:54.364869Z",
        "url": "https://files.pythonhosted.org/packages/a5/14/e638494e191ccb342c2e70decf03d42412c52567a4cfa42a22e9becf457e/altility-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "30e94d2dee6137002b4452a13ac7221946102eea3b90d34aabe2fbac5a49e957",
          "md5": "a9694836da5570ad2bc4964cc38dbfd2",
          "sha256": "04228dbda95d69dc2806aa8b7cae163cd8f6740691baf3fb6ace60bdcc600a3b"
        },
        "downloads": -1,
        "filename": "altility-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "a9694836da5570ad2bc4964cc38dbfd2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 12825,
        "upload_time": "2022-01-08T22:12:55",
        "upload_time_iso_8601": "2022-01-08T22:12:55.925435Z",
        "url": "https://files.pythonhosted.org/packages/30/e9/4d2dee6137002b4452a13ac7221946102eea3b90d34aabe2fbac5a49e957/altility-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e7bda98e01c3f45371140f91d54a06bbc4739dafb0c8fcffa1ce0fd56f88d1a",
          "md5": "ff316906ec34cfca00547088966d68c7",
          "sha256": "47973656148d5321cf65868f9e1eb08b8f0b3defc34565f3d110f17b0d308284"
        },
        "downloads": -1,
        "filename": "altility-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ff316906ec34cfca00547088966d68c7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 26088,
        "upload_time": "2022-01-12T11:19:41",
        "upload_time_iso_8601": "2022-01-12T11:19:41.937673Z",
        "url": "https://files.pythonhosted.org/packages/5e/7b/da98e01c3f45371140f91d54a06bbc4739dafb0c8fcffa1ce0fd56f88d1a/altility-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b4331c06c4bd52e728d670af56e0c46f75812325be905a7a804cee619418c589",
          "md5": "082e75c32d798201e4e145b1d1e9b20d",
          "sha256": "5449e3619d14266ed9c696ab3d401c3ce8880711c0a7252bce7dde4c56599564"
        },
        "downloads": -1,
        "filename": "altility-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "082e75c32d798201e4e145b1d1e9b20d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30219,
        "upload_time": "2022-01-12T11:19:43",
        "upload_time_iso_8601": "2022-01-12T11:19:43.424215Z",
        "url": "https://files.pythonhosted.org/packages/b4/33/1c06c4bd52e728d670af56e0c46f75812325be905a7a804cee619418c589/altility-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5e7bda98e01c3f45371140f91d54a06bbc4739dafb0c8fcffa1ce0fd56f88d1a",
        "md5": "ff316906ec34cfca00547088966d68c7",
        "sha256": "47973656148d5321cf65868f9e1eb08b8f0b3defc34565f3d110f17b0d308284"
      },
      "downloads": -1,
      "filename": "altility-0.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ff316906ec34cfca00547088966d68c7",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 26088,
      "upload_time": "2022-01-12T11:19:41",
      "upload_time_iso_8601": "2022-01-12T11:19:41.937673Z",
      "url": "https://files.pythonhosted.org/packages/5e/7b/da98e01c3f45371140f91d54a06bbc4739dafb0c8fcffa1ce0fd56f88d1a/altility-0.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b4331c06c4bd52e728d670af56e0c46f75812325be905a7a804cee619418c589",
        "md5": "082e75c32d798201e4e145b1d1e9b20d",
        "sha256": "5449e3619d14266ed9c696ab3d401c3ce8880711c0a7252bce7dde4c56599564"
      },
      "downloads": -1,
      "filename": "altility-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "082e75c32d798201e4e145b1d1e9b20d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 30219,
      "upload_time": "2022-01-12T11:19:43",
      "upload_time_iso_8601": "2022-01-12T11:19:43.424215Z",
      "url": "https://files.pythonhosted.org/packages/b4/33/1c06c4bd52e728d670af56e0c46f75812325be905a7a804cee619418c589/altility-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}