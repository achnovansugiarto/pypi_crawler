{
  "info": {
    "author": "David Batista, Matthew Upson",
    "author_email": "david.batista@gmail.com,matthew.a.upson@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "[![Build Status](https://travis-ci.org/ivyleavedtoadflax/nervaluate.svg?branch=master)](https://travis-ci.org/ivyleavedtoadflax/nervaluate) [![codecov](https://codecov.io/gh/ivyleavedtoadflax/nervaluate/branch/master/graph/badge.svg)](https://codecov.io/gh/ivyleavedtoadflax/nervaluate)\n![GitHub](https://img.shields.io/github/license/ivyleavedtoadflax/nervaluate)\n![PyPI](https://img.shields.io/pypi/v/nervaluate)\n\n# nervaluate\n\nnervaluate is a python module for evaluating Named Entity Recognition (NER) models as defined in the SemEval 2013 - 9.1 task.\n\nThe evaluation metrics output by nervaluate go beyond a simple token/tag based schema, and consider diferent scenarios based on wether all the tokens that belong to a named entity were classified or not, and also whether the correct entity type was assigned.\n\nThis full problem is described in detail in the [original blog](http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/) post by [David Batista](https://github.com/davidsbatista), and extends the code in the [original repository](https://github.com/davidsbatista/NER-Evaluation) which accompanied the blog post.\n\nThe code draws heavily on:\n\n* Segura-bedmar, I., & Mart, P. (2013). 2013 SemEval-2013 Task 9 Extraction of Drug-Drug Interactions from. Semeval, 2(DDIExtraction), 341–350. [link](https://www.aclweb.org/anthology/S13-2056)\n* https://www.cs.york.ac.uk/semeval-2013/task9/data/uploads/semeval_2013-task-9_1-evaluation-metrics.pdf\n\n## The problem \n\n### Token level evaluation for NER is too simplistic\n\nWhen running machine learning models for NER, it is common to report metrics at the individual token level. This may not be the best approach, as a named entity can be made up of multiple tokens, so a full-entity accuracy would be desireable.\n\nWhen comparing the golden standard annotations with the output of a NER system different scenarios might occur:\n\n__I. Surface string and entity type match__\n\n|Token|Gold|Prediction|\n|---|---|---|\n|in|O|O|\n|New|B-LOC|B-LOC|\n|York|I-LOC|I-LOC|\n|.|O|O|\n\n__II. System hypothesized an incorrect entity__\n\n|Token|Gold|Prediction|\n|---|---|---|\n|an|O|O|\n|Awful|O|B-ORG|\n|Headache|O|I-ORG|\n|in|O|O|\n\n__III. System misses an entity__\n\n|Token|Gold|Prediction|\n|---|---|---|\n|in|O|O|\n|Palo|B-LOC|O|\n|Alto|I-LOC|O|\n|,|O|O|\n\nBased on these three scenarios we have a simple classification evaluation that can be measured in terms of false positives, true positives, false negatives and false positives, and subsequently compute precision, recall and f1-score for each named-entity type.\n\nHowever this simple schema ignores the possibility of partial matches or other scenarios when the NER system gets the named-entity surface string correct but the type wrong, and we might also want to evaluate these scenarios again at a full-entity level.\n\nFor example:\n\n__IV. System assigns the wrong entity type__\n\n|Token|Gold|Prediction|\n|---|---|---|\n|I|O|O|\n|live|O|O|\n|in|O|O|\n|Palo|B-LOC|B-ORG|\n|Alto|I-LOC|I-ORG|\n|,|O|O|\n\n__V. System gets the boundaries of the surface string wrong__\n\n|Token|Gold|Prediction|\n|---|---|---|\n|Unless|O|B-PER|\n|Karl|B-PER|I-PER|\n|Smith|I-PER|I-PER|\n|resigns|O|O|\n\n__VI. System gets the boundaries and entity type wrong__\n\n|Token|Gold|Prediction|\n|---|---|---|\n|Unless|O|B-ORG|\n|Karl|B-PER|I-ORG|\n|Smith|I-PER|I-ORG|\n|resigns|O|O|\n\nHow can we incorporate these described scenarios into evaluation metrics? See the [original blog](http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/) for a great explanation, a summary is included here:\n\nWe can use the following five metrics to consider difference categories of errors:\n\n|Error type|Explanation|\n|---|---|\n|Correct (COR)|both are the same|\n|Incorrect (INC)|the output of a system and the golden annotation don’t match|\n|Partial (PAR)|system and the golden annotation are somewhat “similar” but not the same|\n|Missing (MIS)|a golden annotation is not captured by a system|\n|Spurius (SPU)|system produces a response which doesn’t exit in the golden annotation|\n\nThese five metrics can be measured in four different ways:\n\n|Evaluation schema|Explanation|\n|---|---|\n|Strict|exact boundary surface string match and entity type|\n|Exact|exact boundary match over the surface string, regardless of the type|\n|Partial|partial boundary match over the surface string, regardless of the type|\n|Type|some overlap between the system tagged entity and the gold annotation is required|\n\nThese five errors and four evaluation schema interact in the following ways:\n\n|Scenario|Gold entity|Gold string|Pred entity|Pred string|Type|Partial|Exact|Strict|\n|---|---|---|---|---|---|---|---|---|\n|III|BRAND|tikosyn| | |MIS|MIS|MIS|MIS|\n|II| | |BRAND|healthy|SPU|SPU|SPU|SPU|\n|V|DRUG|warfarin|DRUG|of warfarin|COR|PAR|INC|INC|\n|IV|DRUG|propranolol|BRAND|propranolol|INC|COR|COR|INC|\n|I|DRUG|phenytoin|DRUG|phenytoin|COR|COR|COR|COR|\n|VI|GROUP|contraceptives|DRUG|oral contraceptives|INC|PAR|INC|INC|\n\nThen precision/recall/f1-score are calculated for each different evaluation schema. In order to achieve data, two more quantities need to be calculated:\n\n```\nPOSSIBLE (POS) = COR + INC + PAR + MIS = TP + FN\nACTUAL (ACT) = COR + INC + PAR + SPU = TP + FP\n```\n\nThen we can compute precision/recall/f1-score, where roughly describing precision is the percentage of correct named-entities found by the NER system, and recall is the percentage of the named-entities in the golden annotations that are retrieved by the NER system. This is computed in two different ways depending wether we want an exact match (i.e., strict and exact ) or a partial match (i.e., partial and type) scenario:\n\n__Exact Match (i.e., strict and exact )__\n```\nPrecision = (COR / ACT) = TP / (TP + FP)\nRecall = (COR / POS) = TP / (TP+FN)\n```\n__Partial Match (i.e., partial and type)__\n```\nPrecision = (COR + 0.5 × PAR) / ACT = TP / (TP + FP)\nRecall = (COR + 0.5 × PAR)/POS = COR / ACT = TP / (TP + FP)\n```\n\n__Putting all together:__\n\n|Measure|Type|Partial|Exact|Strict|\n|---|---|---|---|---|\n|Correct|3|3|3|2|\n|Incorrect|2|0|2|3|\n|Partial|0|2|0|0|\n|Missed|1|1|1|1|\n|Spurius|1|1|1|1|\n|Precision|0.5|0.66|0.5|0.33|\n|Recall|0.5|0.66|0.5|0.33|\n|F1|0.5|0.66|0.5|0.33|\n\n\n## Notes:\n\nIn scenarios IV and VI the entity type of the `true` and `pred` does not match, in both cases we only scored against the true entity, not the predicted one. You can argue that the predicted entity could also be scored as spurious, but according to the definition of `spurius`:\n\n* Spurius (SPU) : system produces a response which doesn’t exist in the golden annotation;\n\nIn this case there exists an annotation, but with a different entity type, so we assume it's only incorrect.\n\n## Installation\n\nTo install the package:\n\n```\npip install nervaluate\n```\n\nTo create a virtual environment for development:\n\n```\nmake virtualenv\n\n# Then to activate the virtualenv:\n\nsource /build/virtualenv/bin/activate\n```\n\nAlternatively you can use your own virtualenv manager and simply `make reqs` to install requirements.\n\nTo run tests:\n\n```\n# Will run tox\n\nmake test\n```\n\n## Example:\n\nThe main `Evaluator` class will accept a number of formats:\n\n* [prodi.gy](https://prodi.gy) style lists of spans.\n* Nested lists containing NER labels.\n* CoNLL style tab delimited strings.\n\n### Prodigy spans\n\n```\ntrue = [\n    [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n    [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n     {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n]\n\npred = [\n    [{\"label\": \"PER\", \"start\": 2, \"end\": 4}],\n    [{\"label\": \"LOC\", \"start\": 1, \"end\": 2},\n     {\"label\": \"LOC\", \"start\": 3, \"end\": 4}]\n]\n\nfrom nervaluate import Evaluator\n\nevaluator = Evaluator(true, pred, tags=['LOC', 'PER'])\n\n# Returns overall metrics and metrics for each tag\n\nresults, results_per_tag = evaluator.evaluate()\n\nprint(results)\n```\n\n```\n{\n    'ent_type':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'partial':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'strict':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    },\n    'exact':{\n        'correct':3,\n        'incorrect':0,\n        'partial':0,\n        'missed':0,\n        'spurious':0,\n        'possible':3,\n        'actual':3,\n        'precision':1.0,\n        'recall':1.0\n    }\n}\n```\n\n```\nprint(results_by_tag)\n```\n\n```\n{\n    'LOC':{\n        'ent_type':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'partial':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'strict':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'exact':{\n            'correct':2,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':2,\n            'actual':2,\n            'precision':1.0,\n            'recall':1.0\n        }\n    },\n    'PER':{\n        'ent_type':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'partial':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'strict':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        },\n        'exact':{\n            'correct':1,\n            'incorrect':0,\n            'partial':0,\n            'missed':0,\n            'spurious':0,\n            'possible':1,\n            'actual':1,\n            'precision':1.0,\n            'recall':1.0\n        }\n    }\n}\n```\n\n### Nested lists\n\n```\ntrue = [\n    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n    ['O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O'],\n]\n\npred = [\n    ['O', 'O', 'B-PER', 'I-PER', 'O'],\n    ['O', 'B-LOC', 'I-LOC', 'B-LOC', 'I-LOC', 'O'],\n]\n\nevaluator = Evaluator(true, pred, tags=['LOC', 'PER'], loader=\"list\")\n\nresults, results_by_tag = evaluator.evaluate()\n```\n\n### CoNLL style tab delimited\n\n```\n\ntrue = \"word\\tO\\nword\\tO\\B-PER\\nword\\tI-PER\\n\"\n\npred = \"word\\tO\\nword\\tO\\B-PER\\nword\\tI-PER\\n\"\n\nevaluator = Evaluator(true, pred, tags=['PER'], loader=\"conll\")\n\nresults, results_by_tag = evaluator.evaluate()\n\n```\n\n## Extending the package to accept more formats\n\nAdditional formats can easily be added to the module by creating a converstion function in `nervaluate/utils.py`, for example `conll_to_spans()`. This function must return the spans in the prodigy style dicts shown in the prodigy example above.\n\nThe new function can then be added to the list of loaders in `nervaluate/nervaluate.py`, and can then be selection with the `loader` argument when instantiating the `Evaluator` class.\n\nA list of formats we intend to include is included in https://github.com/ivyleavedtoadflax/nervaluate/issues/3.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://pypi.org/project/nervaluate",
    "keywords": "",
    "license": "GNU GPL v3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "nervaluate",
    "package_url": "https://pypi.org/project/nervaluate/",
    "platform": "",
    "project_url": "https://pypi.org/project/nervaluate/",
    "project_urls": {
      "Homepage": "https://pypi.org/project/nervaluate"
    },
    "release_url": "https://pypi.org/project/nervaluate/0.1.8/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "NER evaluation done right",
    "version": "0.1.8",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8432355,
  "releases": {
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9188f310a1b16da368ffba86717f83b2fe102e1f67bd139f23c071c79f05c37b",
          "md5": "db77e30601c0adc0b9fdefa2ab7b715c",
          "sha256": "71e4ddb2e1ca4c785ea1c797e180fa911d2654db68e24b4576c650d43388d360"
        },
        "downloads": -1,
        "filename": "nervaluate-0.1.3.linux-x86_64.tar.gz",
        "has_sig": false,
        "md5_digest": "db77e30601c0adc0b9fdefa2ab7b715c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9529,
        "upload_time": "2019-12-06T21:14:04",
        "upload_time_iso_8601": "2019-12-06T21:14:04.321335Z",
        "url": "https://files.pythonhosted.org/packages/91/88/f310a1b16da368ffba86717f83b2fe102e1f67bd139f23c071c79f05c37b/nervaluate-0.1.3.linux-x86_64.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b0bb351cbcf6533d6ce9498ebcfabdb95a8b5869fe18a961b98806b6fea5166f",
          "md5": "4b2ced0907ef4c0290ec842e7d5519f6",
          "sha256": "ce2eb1831770ed4979b2fb840b9875787aec98f6ad4f5ab469a76236ab0b29e9"
        },
        "downloads": -1,
        "filename": "nervaluate-0.1.4.linux-x86_64.tar.gz",
        "has_sig": false,
        "md5_digest": "4b2ced0907ef4c0290ec842e7d5519f6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10998,
        "upload_time": "2019-12-06T21:27:17",
        "upload_time_iso_8601": "2019-12-06T21:27:17.253241Z",
        "url": "https://files.pythonhosted.org/packages/b0/bb/351cbcf6533d6ce9498ebcfabdb95a8b5869fe18a961b98806b6fea5166f/nervaluate-0.1.4.linux-x86_64.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2530e329227b16914405deab65dee84491c49e9e767f7c948f2a1cc8f9cf5f90",
          "md5": "b9ac68a9e1c079799b4a99aeb6cb5d30",
          "sha256": "2a0f4075769472d9c0603277e28214cde6918df35aba47997fd6296b4568ad88"
        },
        "downloads": -1,
        "filename": "nervaluate-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b9ac68a9e1c079799b4a99aeb6cb5d30",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21205,
        "upload_time": "2019-12-06T21:46:05",
        "upload_time_iso_8601": "2019-12-06T21:46:05.276035Z",
        "url": "https://files.pythonhosted.org/packages/25/30/e329227b16914405deab65dee84491c49e9e767f7c948f2a1cc8f9cf5f90/nervaluate-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b0c3936e68696d5af52445c10626eca9fd50738cd6828203fe17d7255e79bf80",
          "md5": "15f68c43bc683a210ac8a407182cdb03",
          "sha256": "95227f7f441e94be08936f6d6d43a5321fe08797d6c47f55f6890b16da7f65c1"
        },
        "downloads": -1,
        "filename": "nervaluate-0.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "15f68c43bc683a210ac8a407182cdb03",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 22464,
        "upload_time": "2019-12-07T18:59:50",
        "upload_time_iso_8601": "2019-12-07T18:59:50.891869Z",
        "url": "https://files.pythonhosted.org/packages/b0/c3/936e68696d5af52445c10626eca9fd50738cd6828203fe17d7255e79bf80/nervaluate-0.1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3366ee4edaa89f9ee78406207785dd7ffb25c1dac75c6bdcf4808939fe203526",
          "md5": "fde6ca775a1899d8617c751c5067d46d",
          "sha256": "6108535160ce52a3549259befc82e52916648befc11aec0581b8c485c72069d3"
        },
        "downloads": -1,
        "filename": "nervaluate-0.1.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fde6ca775a1899d8617c751c5067d46d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 22563,
        "upload_time": "2019-12-07T19:15:35",
        "upload_time_iso_8601": "2019-12-07T19:15:35.053003Z",
        "url": "https://files.pythonhosted.org/packages/33/66/ee4edaa89f9ee78406207785dd7ffb25c1dac75c6bdcf4808939fe203526/nervaluate-0.1.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9593ca9b0721982816a6a9ec09a44b5472fd8026782cbac0da5876dd3e43fc37",
          "md5": "d98ae0986d93012d7fda2d7c349c1868",
          "sha256": "fa99d78cfc03425f60c690cc0627da9e5ae788bd7dd65b76728e1da97590de14"
        },
        "downloads": -1,
        "filename": "nervaluate-0.1.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d98ae0986d93012d7fda2d7c349c1868",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 24448,
        "upload_time": "2020-10-16T23:52:17",
        "upload_time_iso_8601": "2020-10-16T23:52:17.986640Z",
        "url": "https://files.pythonhosted.org/packages/95/93/ca9b0721982816a6a9ec09a44b5472fd8026782cbac0da5876dd3e43fc37/nervaluate-0.1.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9593ca9b0721982816a6a9ec09a44b5472fd8026782cbac0da5876dd3e43fc37",
        "md5": "d98ae0986d93012d7fda2d7c349c1868",
        "sha256": "fa99d78cfc03425f60c690cc0627da9e5ae788bd7dd65b76728e1da97590de14"
      },
      "downloads": -1,
      "filename": "nervaluate-0.1.8-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d98ae0986d93012d7fda2d7c349c1868",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 24448,
      "upload_time": "2020-10-16T23:52:17",
      "upload_time_iso_8601": "2020-10-16T23:52:17.986640Z",
      "url": "https://files.pythonhosted.org/packages/95/93/ca9b0721982816a6a9ec09a44b5472fd8026782cbac0da5876dd3e43fc37/nervaluate-0.1.8-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}