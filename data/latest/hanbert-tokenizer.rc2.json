{
  "info": {
    "author": "TwoBlockAI",
    "author_email": "indexxlim@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/tbai2019/HanBARTT",
    "keywords": "hanbert,tokenizer,moran",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hanbert-tokenizer",
    "package_url": "https://pypi.org/project/hanbert-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/hanbert-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/tbai2019/HanBARTT"
    },
    "release_url": "https://pypi.org/project/hanbert-tokenizer/0.1.7/",
    "requires_dist": [
      "torch (>=1.7.1)",
      "transformers (>=4.3.3)"
    ],
    "requires_python": ">=3",
    "summary": "Hanbert Tokenizer",
    "version": "0.1.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12130421,
  "releases": {
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "adcb5003548ae8344851a2124ad1d222ed1a9c8a0f4d4c1e199a2a7c33df77bd",
          "md5": "c91b6da5d805ba5202669905ae99e40d",
          "sha256": "742e3226875b2f3d0b543b8e56f51c2675a8116684eb30b445d5396d4b9f8fc2"
        },
        "downloads": -1,
        "filename": "hanbert_tokenizer-0.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c91b6da5d805ba5202669905ae99e40d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 427577,
        "upload_time": "2021-11-26T02:36:20",
        "upload_time_iso_8601": "2021-11-26T02:36:20.499500Z",
        "url": "https://files.pythonhosted.org/packages/ad/cb/5003548ae8344851a2124ad1d222ed1a9c8a0f4d4c1e199a2a7c33df77bd/hanbert_tokenizer-0.1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7b5223f690bdff9eab6cab227ad3040e7f51cf191fc44354a0a58f0cec32fcc4",
          "md5": "213c80831a5dd7c11e83e05334c19a98",
          "sha256": "13e466df3d6b8525d0ea023a4941f7641bccf89657faccc813f3235c3f8ed42a"
        },
        "downloads": -1,
        "filename": "hanbert_tokenizer-0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "213c80831a5dd7c11e83e05334c19a98",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 427502,
        "upload_time": "2021-11-26T02:36:23",
        "upload_time_iso_8601": "2021-11-26T02:36:23.084090Z",
        "url": "https://files.pythonhosted.org/packages/7b/52/23f690bdff9eab6cab227ad3040e7f51cf191fc44354a0a58f0cec32fcc4/hanbert_tokenizer-0.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "00f7c5e63b222400d6430ff14dec629c6a0b64a16aba67041bf8e9cf2a0f003d",
          "md5": "efeeb6b2e66e8f0c52bca3c786202b21",
          "sha256": "3dcfe5de4f6ff22c5d088b87eccfd329d5e8b16f3c2d3da509761221be57c775"
        },
        "downloads": -1,
        "filename": "hanbert_tokenizer-0.1.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "efeeb6b2e66e8f0c52bca3c786202b21",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 428156,
        "upload_time": "2021-11-26T07:37:15",
        "upload_time_iso_8601": "2021-11-26T07:37:15.828267Z",
        "url": "https://files.pythonhosted.org/packages/00/f7/c5e63b222400d6430ff14dec629c6a0b64a16aba67041bf8e9cf2a0f003d/hanbert_tokenizer-0.1.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d07ece9fda7738fb2bd40ef9a066c9e6960fcb61b1375c8cbcab44dbd523fb51",
          "md5": "5f6cfbb9772cbca5dc027e9c18838b9d",
          "sha256": "53ccafb72e3681233debbf7463a5a69512256453b612ec0403ec4e72462b2043"
        },
        "downloads": -1,
        "filename": "hanbert_tokenizer-0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "5f6cfbb9772cbca5dc027e9c18838b9d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 427853,
        "upload_time": "2021-11-26T07:37:18",
        "upload_time_iso_8601": "2021-11-26T07:37:18.064212Z",
        "url": "https://files.pythonhosted.org/packages/d0/7e/ce9fda7738fb2bd40ef9a066c9e6960fcb61b1375c8cbcab44dbd523fb51/hanbert_tokenizer-0.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "00f7c5e63b222400d6430ff14dec629c6a0b64a16aba67041bf8e9cf2a0f003d",
        "md5": "efeeb6b2e66e8f0c52bca3c786202b21",
        "sha256": "3dcfe5de4f6ff22c5d088b87eccfd329d5e8b16f3c2d3da509761221be57c775"
      },
      "downloads": -1,
      "filename": "hanbert_tokenizer-0.1.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "efeeb6b2e66e8f0c52bca3c786202b21",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3",
      "size": 428156,
      "upload_time": "2021-11-26T07:37:15",
      "upload_time_iso_8601": "2021-11-26T07:37:15.828267Z",
      "url": "https://files.pythonhosted.org/packages/00/f7/c5e63b222400d6430ff14dec629c6a0b64a16aba67041bf8e9cf2a0f003d/hanbert_tokenizer-0.1.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d07ece9fda7738fb2bd40ef9a066c9e6960fcb61b1375c8cbcab44dbd523fb51",
        "md5": "5f6cfbb9772cbca5dc027e9c18838b9d",
        "sha256": "53ccafb72e3681233debbf7463a5a69512256453b612ec0403ec4e72462b2043"
      },
      "downloads": -1,
      "filename": "hanbert_tokenizer-0.1.7.tar.gz",
      "has_sig": false,
      "md5_digest": "5f6cfbb9772cbca5dc027e9c18838b9d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3",
      "size": 427853,
      "upload_time": "2021-11-26T07:37:18",
      "upload_time_iso_8601": "2021-11-26T07:37:18.064212Z",
      "url": "https://files.pythonhosted.org/packages/d0/7e/ce9fda7738fb2bd40ef9a066c9e6960fcb61b1375c8cbcab44dbd523fb51/hanbert_tokenizer-0.1.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}