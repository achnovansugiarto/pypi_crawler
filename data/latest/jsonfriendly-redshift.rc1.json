{
  "info": {
    "author": "Amrish Mishra",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Information Technology",
      "Intended Audience :: System Administrators",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "#### Description\nA high level Python wrapper using pandas.\nIt is meant to provide a point-in-time json data handling for redshift load Job.\n\n#### Installation\nSupports Python 3.6+\n```bash\npip install jsonfriendly-redshift\n```\n##### Features\n<li> Automatic schema creation, if schema doesn't exist\n<li> Automatic table creation, if table doesn't exist\n<li> Automatic column addition to existing table, if column doesn't exist\n<li> Dict utils helper functions to create flat json\n<li> Dict utils helper functions to rename keys as per mapping provided and format datetime string to standard ISO formt.\n<li> Utils helper functions to create New Line Delimited Json required for s3 load job with json\n\n##### Refer Below Example For More Details\n    import os\n    import json\n    import pandas as pd\n    import psycopg2 as pg\n    from jsonfriendly_redshift.handler import RedShiftHandler\n    from jsonfriendly_redshift.dict_utils import flatten_dict_for_redshift, flatten_and_fix_timestamp, fix_keys\n    from jsonfriendly_redshift.utils import generate_json_for_copy_query\n\n    # you can use environment variable or direct assignment\n    db_name = os.environ[\"DB_NAME\"]\n    db_password = os.environ[\"DB_PASSWORD\"]\n    db_user = os.environ[\"DB_USER\"]\n    db_port = os.environ[\"DB_PORT\"]\n    db_schema = os.environ[\"DB_SCHEMA\"]\n    db_host = os.environ[\"DB_HOST\"]\n    db_iam_arn = os.environ[\"REDSHIFT_IAM\"]\n    write_bucket = os.environ[\"WRITE_BUCKET\"]\n\n    # pg connection object\n    connection = pg.connect(host=db_host,\n                            user=db_user,\n                            password=db_password,\n                            database=db_name,\n                            port=db_port)\n\n\n    def lambda_handler(event, context):\n        # s3 file retrieval logic goes here\n        # Create a file object using the bucket and object key.\n        # replace file_obj with yours\n        file_obj = {}\n\n        s3_data = file_obj['Body'].read().decode(\"utf-8\")\n        s3_data = json.loads(s3_data)\n\n        final_data = {}\n\n        # example for s3_data data of type list(dict, dict, dict)\n\n        for dict_data in s3_data:\n            ################\n            # # fix_keys # #\n            ################\n            # replaces all the special char in key name of dictionary and replaces it\n            # using re.sub(r\"[^A-Za-z0-9_]+\", \"\", key_name) and it does it recursively\n            processed_data = fix_keys(dict_data)\n\n            #################################\n            # # flatten_and_fix_timestamp # #\n            #################################\n            # makes a flat dictionary object, excluding key with array, they are converted in a json string\n            # for eg: converts {\"first\" {\"second\": \"abc\", \"third\": [{..},{..}]}}\n            # to {\"first_second\": \"abc\", \"first_third\": \"[{..},{..}]\"} first_third = json.dumps object\n            # and if it encounters datetime string which can be parsed from python date utils it\n            # converts it to iso formatted datetime string\n            # Optional if mapping dict is provided, it maps the flat key name to the name provided\n            # eg: mapper = {\"first_second\": \"my_key_name\"} => {\"my_key_name\": data}\n            # usage flatten_and_fix_timestamp(dict_obj, mapper)\n            mapper_dict = {}  # provide your mappings for long column name\n            processed_data = flatten_and_fix_timestamp(dict_data, mapper_dict)\n\n            #################################\n            # # flatten_dict_for_redshift # #\n            #################################\n            # its not recommended it does the same thing as flatten_and_fix_timestamp\n            # but it doesn't takes any mapper\n            # for safe side it strips the key name with last 127 chars\n            # as redshift supports only 127 bytes of length for column name\n            # and it may grow in size due to flattening of dict\n            processed_data = flatten_dict_for_redshift(dict_data)\n\n            data_df = pd.read_json(json.dumps(s3_data))\n            # test data\n            table_names = [\"foo\", \"foo1\", \"foo2\"]\n            for table in table_names:\n                ###################################\n                # # initialize redshift handler # #\n                ###################################\n                rs_handler = RedShiftHandler(df=data_df, schema=db_schema,\n                                             table=table, conn=connection)\n\n                # handles the schema creation, table creation and column creation automatically\n                # by checking the metadata of table already present, if not, it creates it with\n                # most suitable column data types\n\n                ##########################################################################################\n                # # Highly suggested to use good amount of data for initial load job (500 rows minimum)# #\n                ##########################################################################################\n\n                handle_successful, handle_error = rs_handler.handle_schema()\n                if handle_successful and not handle_error:\n                    # you can create json format supported by redshift for load operation by using\n                    list_of_rows = [{}, {}, {}]  # your row data\n                    new_line_delimited_json = generate_json_for_copy_query(list_of_rows)\n                    # save this json to s3 bucket and provide the bucket_name and object key along \n                    # with IAM Role to execute_json_copy_query method of rs_handler\n                    ##################################################################################\n                    # # you logic for saving data to s3 goes here, if you want to use copy command # #\n                    ##################################################################################\n                    # or else you can write your own insert query  and execute it using \n                    # rs_handler.execute_query(query_string) goes here\n                    # s3_object_path is where you have sored you file object\n                    s3_object_path = \"bucket_name/your_object_key\"  # you dont need to provide s3:// initials\n                    success, error = rs_handler.execute_json_copy_query(s3_object_path, db_iam_arn)\n                        if error:\n                            # Do somthing to manage\n\n#### Troubleshooting\nIf you are facing any issue with psycopg2 installation i would suggest installing it using conda<br>\n```bash\nconda install psycopg2\n```\nPR's are welcome.\n#### Contact\nEmail: mishraamrish.asm@gmail.com\n#### Questions or suggestions?\nSimple, just send me an email or open an issue :)\n#### License\n[MIT License](https://github.com/mishraamrish/jsonfriendly_redshift/blob/master/LICENSE)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/mishraamrish/jsonfriendly_redshift",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "jsonfriendly-redshift",
    "package_url": "https://pypi.org/project/jsonfriendly-redshift/",
    "platform": "",
    "project_url": "https://pypi.org/project/jsonfriendly-redshift/",
    "project_urls": {
      "Homepage": "https://github.com/mishraamrish/jsonfriendly_redshift"
    },
    "release_url": "https://pypi.org/project/jsonfriendly-redshift/1.0.1/",
    "requires_dist": [
      "pandas (==1.1.0)",
      "numpy (==1.19.1)",
      "psycopg2"
    ],
    "requires_python": "",
    "summary": "A high level Python wrapper using pandas. It is meant to provide a point-in-time json data handling for redshift load Job.",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7848154,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "79e5fd3574e1cc4305483f6c4628340b4a205b5c12c247860253c8fa859f2e5e",
          "md5": "c58a5b7e739fc4d12cf336b9fe0754c6",
          "sha256": "c89afb01d9bbc494d8754cdb03b56d5d5b382d2a3215dcdebb4a0ddee5c1b378"
        },
        "downloads": -1,
        "filename": "jsonfriendly_redshift-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c58a5b7e739fc4d12cf336b9fe0754c6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 9913,
        "upload_time": "2020-07-30T19:19:01",
        "upload_time_iso_8601": "2020-07-30T19:19:01.199164Z",
        "url": "https://files.pythonhosted.org/packages/79/e5/fd3574e1cc4305483f6c4628340b4a205b5c12c247860253c8fa859f2e5e/jsonfriendly_redshift-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6205b90b052f3920d2162e4e1d7bce7f3b302dbcbf0a6d8019a78c798a9418e6",
          "md5": "fee59f73bc1fe1ca858b97373aab465d",
          "sha256": "bac4a0b52160694f76f1e8c64544c80d6786bb381aa59d8e913e2bfdab49cd1e"
        },
        "downloads": -1,
        "filename": "jsonfriendly_redshift-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "fee59f73bc1fe1ca858b97373aab465d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9334,
        "upload_time": "2020-07-30T19:19:03",
        "upload_time_iso_8601": "2020-07-30T19:19:03.006055Z",
        "url": "https://files.pythonhosted.org/packages/62/05/b90b052f3920d2162e4e1d7bce7f3b302dbcbf0a6d8019a78c798a9418e6/jsonfriendly_redshift-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "79e5fd3574e1cc4305483f6c4628340b4a205b5c12c247860253c8fa859f2e5e",
        "md5": "c58a5b7e739fc4d12cf336b9fe0754c6",
        "sha256": "c89afb01d9bbc494d8754cdb03b56d5d5b382d2a3215dcdebb4a0ddee5c1b378"
      },
      "downloads": -1,
      "filename": "jsonfriendly_redshift-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c58a5b7e739fc4d12cf336b9fe0754c6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 9913,
      "upload_time": "2020-07-30T19:19:01",
      "upload_time_iso_8601": "2020-07-30T19:19:01.199164Z",
      "url": "https://files.pythonhosted.org/packages/79/e5/fd3574e1cc4305483f6c4628340b4a205b5c12c247860253c8fa859f2e5e/jsonfriendly_redshift-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6205b90b052f3920d2162e4e1d7bce7f3b302dbcbf0a6d8019a78c798a9418e6",
        "md5": "fee59f73bc1fe1ca858b97373aab465d",
        "sha256": "bac4a0b52160694f76f1e8c64544c80d6786bb381aa59d8e913e2bfdab49cd1e"
      },
      "downloads": -1,
      "filename": "jsonfriendly_redshift-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "fee59f73bc1fe1ca858b97373aab465d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 9334,
      "upload_time": "2020-07-30T19:19:03",
      "upload_time_iso_8601": "2020-07-30T19:19:03.006055Z",
      "url": "https://files.pythonhosted.org/packages/62/05/b90b052f3920d2162e4e1d7bce7f3b302dbcbf0a6d8019a78c798a9418e6/jsonfriendly_redshift-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}