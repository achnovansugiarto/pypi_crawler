{
  "info": {
    "author": "Thomas FEL, Julien Colin",
    "author_email": "thomas_fel@brown.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3"
    ],
    "description": "\n# üîÆ What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods\n\n_Julien Colin*, Thomas Fel*, R√©mi Cad√®ne, Thomas Serre_\n\n\n<p align=\"center\">\n  <a href=\"https://arxiv.org/abs/2112.04417\"><strong>Read the official paper ¬ª</strong></a>\n  <br>\n  <br>\n  <a href=\"https://serre-lab.github.io/Meta-predictor/\">Documentation</a>\n  ¬∑\n  <a href=\"https://github.com/serre-lab/Meta-predictor\">Github</a>\n  .\n  <a href=\"https://serre-lab.github.io/Meta-predictor/tutorials\">Tutorials (coming soon)</a>\n</p>\n\n## Paper summary\n\n<img src=\"docs/assets/intro.jpg\" width=\"100%\" align=\"center\">\n\nA multitude of explainability methods and associated fidelity performance metrics\nhave been proposed to help better understand how modern AI systems make decisions. However, much of the current work has remained theoretical ‚Äì without\nmuch consideration for the human end-user. In particular, it is not yet known\n**(1) how useful current explainability methods are in practice** for more real-world\nscenarios and\n**(2) how well associated performance metrics accurately predict how much knowledge individual explanations contribute to a human end-user trying to understand the inner-workings of the system**.\n\nTo fill this gap, we conducted\npsychophysics experiments at scale ($n = 1,150$) to evaluate the usefulness of attribution methods in three real-world scenarios:\n_identifying bias in an AI system_,\n_characterizing the visual strategy it uses for tasks that are too difficult_ for an untrained non-expert human observer as well as _understanding its failure cases_.\n\nOur results demonstrate that the degree to which individual attribution methods help human participants better understand an AI system varied\nwidely across these scenarios. This suggests a critical need for the field to move\npast quantitative improvements of current attribution methods towards the development of complementary approaches that provide qualitatively different sources of\ninformation to human end-users.\n\n### Human-centered Framework\n\n<img src=\"docs/assets/framework.jpg\" width=\"100%\" align=\"center\">\n\nIn this work, we propose to measure the usefulness of attribution methods, i.e., do they help users to understand how a model works beyond simply observing how it classifies images.\n\nMore precisely, we evaluate how much attribution maps help training users at getting better at predicting a models‚Äô decisions on unseen images.\nThe utility score are computed from the relative improvement of users in accuracy with vs. without the attribution maps in training.\n\n### 3 Datasets, 3  Use cases\n\n<img src=\"docs/assets/dataset.jpg\" width=\"100%\" align=\"center\">\n\nWe evaluate the usefulness of representative attribution methods on 3 important use cases for eXplainable AI in vision:\n\n  - (1) Bias detection\n  - (2) Identify novel strategies discovered by an expert AI system\n  - (3) Understand failure cases of an AI system\n\n\n### Results\n\n<img src=\"docs/assets/utility_score.jpg\" width=\"100%\" align=\"center\">\n\nWe find mixed results: current attribution methods are helpful in simple use cases but none of the methods were helpful in diagnosing more challenging failure cases of the system.\nThis result highlights a fundamental challenge for XAI,  that, we argue, cannot be overcome by attribution methods alone, but that will require qualitatively different sources of information for human end-users.\n\n### Going beyond attribution method ?\n\n<img src=\"docs/assets/understand_why.jpg\" width=\"100%\" align=\"center\">\n\nWe explored several possibilities for why attribution methods are not as useful in more complex scenarios. Making attribution methods (1) more faithful or (2) less visually complex does not seem like  a promising avenue for future research.  We argue instead for methods that will contribute novel ‚Äúwhat‚Äù information to the user beyond the ‚Äúwhere‚Äù information contributed by attribution methods.\n\n\n## üóûÔ∏è Citation\n\nIf you use or build on our work as part of your workflow in a scientific publication, please consider citing the [official paper](https://arxiv.org/abs/2211.04533):\n\n```\n@article{colin2022metapredictor,\n  title={What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods},\n  author={Colin, Julien and Fel, Thomas and Cadene, Remi and Serre, Thomas},\n  journal={Advances in Neural Information Processing Systems (NeurIPS)},\n  year={2022}\n}\n```\n\n## Tutorials\n\nComing soon: a tutorial to evaluate your own explainability method.\n\n\n## üìù License\n\nThe package is released under <a href=\"https://choosealicense.com/licenses/mit\"> MIT license</a>.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "Metapredictor",
    "package_url": "https://pypi.org/project/Metapredictor/",
    "platform": null,
    "project_url": "https://pypi.org/project/Metapredictor/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/Metapredictor/0.0.0/",
    "requires_dist": [
      "tensorflow",
      "pytorch",
      "xplique",
      "numpy",
      "scikit-image",
      "matplotlib",
      "scipy",
      "opencv-python",
      "mkdocs ; extra == 'docs'",
      "mkdocs-material ; extra == 'docs'",
      "numkdoc ; extra == 'docs'",
      "pytest ; extra == 'tests'",
      "pylint ; extra == 'tests'"
    ],
    "requires_python": ">=3.6",
    "summary": "A Human-Centered Evaluation Framework for Explainability Methods",
    "version": "0.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15840662,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d337c5b91fd19098908d663a0aeb90875916b286edef330020bd8109e9463831",
          "md5": "241e84edb8380001f87c71a8e4bc0c2c",
          "sha256": "4f216031d6633270bd0d0be88751a9059721a1e4166d034a6fc53b6d05e2514c"
        },
        "downloads": -1,
        "filename": "Metapredictor-0.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "241e84edb8380001f87c71a8e4bc0c2c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 5308,
        "upload_time": "2022-11-21T13:56:36",
        "upload_time_iso_8601": "2022-11-21T13:56:36.305152Z",
        "url": "https://files.pythonhosted.org/packages/d3/37/c5b91fd19098908d663a0aeb90875916b286edef330020bd8109e9463831/Metapredictor-0.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ae3a50b4250d322e96056dc2e11946ef3c385122aeab34717379760207dd72b1",
          "md5": "bf8a265fafb04aef93ab06b9f125d52a",
          "sha256": "50d47970fd9ccdd461e616e613596e260d8ad22a59bb4247f6df0ac6940f1a19"
        },
        "downloads": -1,
        "filename": "Metapredictor-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bf8a265fafb04aef93ab06b9f125d52a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 5080,
        "upload_time": "2022-11-21T13:56:37",
        "upload_time_iso_8601": "2022-11-21T13:56:37.867878Z",
        "url": "https://files.pythonhosted.org/packages/ae/3a/50b4250d322e96056dc2e11946ef3c385122aeab34717379760207dd72b1/Metapredictor-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d337c5b91fd19098908d663a0aeb90875916b286edef330020bd8109e9463831",
        "md5": "241e84edb8380001f87c71a8e4bc0c2c",
        "sha256": "4f216031d6633270bd0d0be88751a9059721a1e4166d034a6fc53b6d05e2514c"
      },
      "downloads": -1,
      "filename": "Metapredictor-0.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "241e84edb8380001f87c71a8e4bc0c2c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 5308,
      "upload_time": "2022-11-21T13:56:36",
      "upload_time_iso_8601": "2022-11-21T13:56:36.305152Z",
      "url": "https://files.pythonhosted.org/packages/d3/37/c5b91fd19098908d663a0aeb90875916b286edef330020bd8109e9463831/Metapredictor-0.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ae3a50b4250d322e96056dc2e11946ef3c385122aeab34717379760207dd72b1",
        "md5": "bf8a265fafb04aef93ab06b9f125d52a",
        "sha256": "50d47970fd9ccdd461e616e613596e260d8ad22a59bb4247f6df0ac6940f1a19"
      },
      "downloads": -1,
      "filename": "Metapredictor-0.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "bf8a265fafb04aef93ab06b9f125d52a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 5080,
      "upload_time": "2022-11-21T13:56:37",
      "upload_time_iso_8601": "2022-11-21T13:56:37.867878Z",
      "url": "https://files.pythonhosted.org/packages/ae/3a/50b4250d322e96056dc2e11946ef3c385122aeab34717379760207dd72b1/Metapredictor-0.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}