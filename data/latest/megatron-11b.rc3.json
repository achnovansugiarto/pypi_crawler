{
  "info": {
    "author": "Hyunwoong Ko",
    "author_email": "gusdnd852@naver.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.2",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries"
    ],
    "description": "# Megatron 11B\n- Porting of Megatron LM 11B model published on facebook on Huggingface Transformers.\n- This repo contains the model's code, checkpoints and parallelization examples.\n  <br><br>\n\n## Installation\n```console\npip install megatron-11b\n```\n<br>\n\n## Usage\n### 1. Tokenizer\n- The usage of tokenizer is the same as other tokenizers of the existing Huggingface.\n- BOS and EOS token are automatically attached, so if you want to use it as a prompt, please exclude EOS token (using `[:-1]`)\n```python\nfrom megatron_11b import MegatronTokenizer\n\ntokenizer = MegatronTokenizer.from_pretrained(\"hyunwoongko/megatron-11B\")\ntokens = tokenizer.encode(\"Kevin is\")\n# [0, 21910, 16] ---> include EOS\ntokens = tokenizer.encode(\"Kevin is\")[:, :-1]\n# [0, 21910, 16, 2] ---> exclude EOS\n```\n<br>\n\n### 2. Model\n- We currently support the CausalLM model and the SequenceClassification model.\n- The usage of model is also the same as other models of the existing Huggingface.\n\n```python\nfrom megatron_11b import MegatronForCausalLM, MegatronForSequenceClassification\n\nmodel_clm = MegatronForCausalLM.from_pretrained(\"hyunwoongko/megatron-11B\")\nmodel_clf = MegatronForSequenceClassification.from_pretrained(\"hyunwoongko/megatron-11B\")\n```\n<br>\n\n\n### 3. Generation\n```python\nfrom megatron_11b import MegatronForCausalLM, MegatronTokenizer\n\ntokenizer = MegatronTokenizer.from_pretrained(\"hyunwoongko/megatron-11B\")\nmodel = MegatronForCausalLM.from_pretrained(\"hyunwoongko/megatron-11B\").half().cuda()\n\ninputs = \"Kevin is\"\ninputs = tokenizer.encode(inputs, return_tensors=\"pt\").cuda()[:, :-1]  # exclude EOS\n\noutput = model.generate(inputs, num_beams=5, no_repeat_ngram_size=4, repetition_penalty=1.2)\nprint(tokenizer.batch_decode(output))\n```\n- output of generation.\n```\n<s>Kevin is a great guy.</s>\n```\n<br>\n\n### 4. Model Parallelism\n- Currently, I'm preparing an opensource called `Parallelformers` that can parallelize all models of Huggingface Transformers. \n- I plan to support model parallelization through this library. (maybe I can release it next month)\n- The relevant code can be found via `MegatronPolicy` object below.\n```python\nfrom parallelformers.polices.base import Policy, Layer\nfrom parallelformers.utils.dist_utils import AllReduceLinear\nfrom megatron_11b.modeling_megatron import MegatronDecoderLayer\n\n\nclass MegatronPolicy(Policy):\n\n    @staticmethod\n    def replace_arguments(config, world_size):\n        return {\n            # 1. reduce hidden size\n            \"self_attn.embed_dim\": config.d_model // world_size,\n\n            # 2. reduce number of heads\n            \"self_attn.num_heads\": config.encoder_attention_heads // world_size,\n        }\n\n    @staticmethod\n    def attn_qkv():\n        return [\n            Layer(\n                weight=\"self_attn.q_proj.weight\",\n                bias=\"self_attn.q_proj.bias\",\n            ),\n            Layer(\n                weight=\"self_attn.k_proj.weight\",\n                bias=\"self_attn.k_proj.bias\",\n            ),\n            Layer(\n                weight=\"self_attn.v_proj.weight\",\n                bias=\"self_attn.v_proj.bias\",\n            ),\n        ]\n\n    @staticmethod\n    def attn_out():\n        return [\n            Layer(\n                weight=\"self_attn.out_proj.weight\",\n                bias=\"self_attn.out_proj.bias\",\n                replace=AllReduceLinear,\n            ),\n        ]\n\n    @staticmethod\n    def mlp_in():\n        return [\n            Layer(\n                weight=\"fc1.weight\",\n                bias=\"fc1.bias\",\n            ),\n        ]\n\n    @staticmethod\n    def mlp_out():\n        return [\n            Layer(\n                weight=\"fc2.weight\",\n                bias=\"fc2.bias\",\n                replace=AllReduceLinear,\n            ),\n        ]\n\n    @staticmethod\n    def original_layer_class():\n        return MegatronDecoderLayer\n```\n<br><br>\n\n\n## References\n- https://github.com/pytorch/fairseq/tree/master/examples/megatron_11b\n- https://github.com/huggingface/transformers/pull/10301\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/hyunwoongko/megatron-lm",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "megatron-11b",
    "package_url": "https://pypi.org/project/megatron-11b/",
    "platform": "",
    "project_url": "https://pypi.org/project/megatron-11b/",
    "project_urls": {
      "Homepage": "https://github.com/hyunwoongko/megatron-lm"
    },
    "release_url": "https://pypi.org/project/megatron-11b/1.2/",
    "requires_dist": [
      "transformers",
      "torch"
    ],
    "requires_python": ">=3",
    "summary": "Megatron LM 11B on Huggingface Transformers",
    "version": "1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10848636,
  "releases": {
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "092f5f93e519f471f9a0083a196b66c9ecdc69f347f1e8b6514fb2f4a68ecad0",
          "md5": "6a8229c58d7eff375f03f51536d4325a",
          "sha256": "b9685ff4642cd88230d20d80b6046ac2d2b13fb239d32e94f597348c23a1280e"
        },
        "downloads": -1,
        "filename": "megatron_11b-1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6a8229c58d7eff375f03f51536d4325a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 22853,
        "upload_time": "2021-07-05T03:49:38",
        "upload_time_iso_8601": "2021-07-05T03:49:38.025155Z",
        "url": "https://files.pythonhosted.org/packages/09/2f/5f93e519f471f9a0083a196b66c9ecdc69f347f1e8b6514fb2f4a68ecad0/megatron_11b-1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e9d597e0af735f9887265b714c7244b3efde661ff4c2fb5870b55c1cfc343efd",
          "md5": "fe2b0601919c2488bbce3d58f1a4e25e",
          "sha256": "7f9f2f9ac01394be0fdf1b1a86128fcfd096fd6595ddbe6f198207b73c3da5f8"
        },
        "downloads": -1,
        "filename": "megatron_11b-1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fe2b0601919c2488bbce3d58f1a4e25e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 23333,
        "upload_time": "2021-07-07T15:14:03",
        "upload_time_iso_8601": "2021-07-07T15:14:03.083431Z",
        "url": "https://files.pythonhosted.org/packages/e9/d5/97e0af735f9887265b714c7244b3efde661ff4c2fb5870b55c1cfc343efd/megatron_11b-1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2e8666437c148d50f9b7017b3f7e91f74d20b6d878af1e794aaedf4efcf440e1",
          "md5": "1ee846b1c73481b1d85d1c8052e85d17",
          "sha256": "22dee0f78ea7a54bb90be06ddf973b0dbd80db5035782fa35216aad3d0e716fa"
        },
        "downloads": -1,
        "filename": "megatron_11b-1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1ee846b1c73481b1d85d1c8052e85d17",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 23332,
        "upload_time": "2021-07-07T19:33:27",
        "upload_time_iso_8601": "2021-07-07T19:33:27.230393Z",
        "url": "https://files.pythonhosted.org/packages/2e/86/66437c148d50f9b7017b3f7e91f74d20b6d878af1e794aaedf4efcf440e1/megatron_11b-1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2e8666437c148d50f9b7017b3f7e91f74d20b6d878af1e794aaedf4efcf440e1",
        "md5": "1ee846b1c73481b1d85d1c8052e85d17",
        "sha256": "22dee0f78ea7a54bb90be06ddf973b0dbd80db5035782fa35216aad3d0e716fa"
      },
      "downloads": -1,
      "filename": "megatron_11b-1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1ee846b1c73481b1d85d1c8052e85d17",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3",
      "size": 23332,
      "upload_time": "2021-07-07T19:33:27",
      "upload_time_iso_8601": "2021-07-07T19:33:27.230393Z",
      "url": "https://files.pythonhosted.org/packages/2e/86/66437c148d50f9b7017b3f7e91f74d20b6d878af1e794aaedf4efcf440e1/megatron_11b-1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}