{
  "info": {
    "author": "John H Williamson",
    "author_email": "johnhw@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Differentiable parallel approximate sorting networks\nA Python implementation of a fully-differentiable *approximate* sorting function for power-of-2 length vectors. Uses Numpy, PyTorch or Tensorflow (or autograd, JAX or cupy), but trivial to use in other backends. Works on GPU. \n\n<img src=\"imgs/sorting_example.png\">\n\n[![Build Status](https://travis-ci.com/johnhw/differentiable_sorting.svg?branch=master)](https://travis-ci.com/johnhw/differentiable_sorting)\n\n\n        \n```python\n    from differentiable_sorting import bitonic_matrices, diff_sort, diff_argsort\n\n    # sort 8 element vectors\n    sort_matrices = bitonic_matrices(8)\n    x = [5.0, -1.0, 9.5, 13.2, 16.2, 20.5, 42.0, 18.0]\n\n    print(diff_sort(sort_matrices, x)) \n    >>> [-1.007  4.996  9.439 13.212 15.948 18.21  20.602 42.   ]\n\n    # we can relax towards averaging by plugging in another\n    # softmax function to the network    \n    from differentiable_sorting import softmax_smooth\n    \n    print(diff_sort(sort_matrices, x, lambda a,b: softmax_smooth(a,b, 0.0))) # as above\n    >>> [-1.007  4.996  9.439 13.212 15.948 18.21  20.602 42.   ]\n\n    print(diff_sort(sort_matrices, x, lambda a,b: softmax_smooth(a,b, 0.05))) # smoothed\n    >>> [ 1.242  5.333  9.607 12.446 16.845 18.995 20.932 37.999]\n        \n    print(diff_sort(sort_matrices, x, lambda a,b: softmax_smooth(a,b, 1.0))) # relax completely to mean\n    >>> [15.425 15.425 15.425 15.425 15.425 15.425 15.425 15.425]\n\n    print(np.mean(x))\n    >>> 15.425\n\n    ###### Ranking\n    # We can rank as well\n    x = [1, 2, 3, 4, 8, 7, 6, 4]\n\n    print(diff_argsort(sort_matrices, x))\n    >>> [0. 1. 2. 3. 7. 6. 5. 3.]\n\n    # smoothed ranking function\n    print(diff_argsort(sort_matrices, x, sigma=0.25))\n    >>> [0.13 1.09 2.   3.11 6.99 6.   5.   3.11]\n\n    # using autograd to differentiate smooth argsort\n    from autograd import jacobian\n    jac_rank = jacobian(diff_argsort, argnum=1)\n    print(jac_rank(sort_matrices, np.array(x), 0.25))\n\n    >>>     [[ 2.162 -1.059 -0.523 -0.287 -0.01  -0.018 -0.056 -0.21 ]\n            [-0.066  0.562 -0.186 -0.155 -0.005 -0.011 -0.035 -0.105]\n            [-0.012 -0.013  0.041 -0.005 -0.    -0.001 -0.002 -0.008]\n            [-0.012 -0.025 -0.108  0.564 -0.05  -0.086 -0.141 -0.14 ]\n            [-0.001 -0.001 -0.003 -0.005  0.104 -0.058 -0.028 -0.008]\n            [-0.    -0.001 -0.002 -0.004 -0.001  0.028 -0.012 -0.007]\n            [-0.    -0.    -0.001 -0.002 -0.016 -0.018  0.038 -0.001]\n            [-0.012 -0.025 -0.108 -0.209 -0.05  -0.086 -0.141  0.633]]\n```\n\nCaveats:\n* May not be very efficient (!), requiring approximately `log_2(n)^2` matrix multiplies of size `n x n`. These are just permutations, so can also be efficiently implemented via indexing, requiring `2n` indexing operations per layer.\n* Dynamic range is limited, especially with `float32`. Very large or very small values will cause trouble. Values distributed between 1 and ~300 work reasonably with `float64` (and similarly for negative values). Values of magnitude 0.0-1.0 are troublesome. Inputs should be pre-normalised (e.g. batch-norm followed by a constant scale by 100)\n\n* The networks are *theoretically* differentiable, but gradients may be very small for larger networks.\n* This appears to be the idea presented in **NeuroSort** by Grover et al. **Grover, A., Wang, E., Zweig, A., & Ermon, S. (2019). Stochastic optimization of sorting networks via continuous relaxations. arXiv preprint arXiv:1903.08850.**\n\n## Vector sorting\nWe can also sort vector valued entries using a particular \"key\" function, assuming this function is also differentiable (e.g. sort a matrix by a particular column) using `vector_sort`:\n\n```python\n    vector_sort(\n        bitonic_matrices(4),\n        np.array([[1, 5], \n                  [30, 30], \n                  [6, 9], \n                  [80, -2]]),\n        lambda x: (x @ [1,0]), # sort by column 1    \n        alpha=1.0\n    )\n\n    >>> array( [[80.  , -2.  ],\n                [30.  , 30.  ],\n                [ 5.97,  8.97],\n                [ 1.03,  5.03]])\n\n    vector_sort(\n        bitonic_matrices(4),\n        np.array([[1, 5], \n                  [30, 30], \n                  [6, 9], \n                  [80, -2]]),\n        lambda x: (x @ [0,1]), # sort by column 2    \n    )\n\n    >>>  array([[30.  , 30.  ],\n                [ 5.91,  8.93],\n                [ 1.16,  5.07],\n                [79.93, -1.99]])\n```\n\n\n## Bitonic sorting\n\n[Bitonic sorts](https://en.wikipedia.org/wiki/Bitonic_sorter) create sorting networks with a sequence of fixed conditional swapping operations executed on an `n` element vector in parallel where, `n=2^k`. \n\n<img src=\"imgs/BitonicSort1.svg.png\">\n\n*[Image: from Wikipedia, by user Bitonic, CC0](https://en.wikipedia.org/wiki/Bitonic_sorter#/media/File:BitonicSort1.svg)*\n\nThe sorting network for `n=2^k` elements has `k(k-1)/2` \"layers\" where parallel compare-and-swap operations are used to rearrange a `n` element vector into sorted order. The sequence of operations is independent of the data.\n\n\n### Differentiable compare-and-swap\n\nIf we define the `softmax(a,b)` function (not the traditional \"softmax\" used for classification, but rather log-sum-exp!) as the [continuous approximation to the `max(a,b)` function](https://en.wikipedia.org/wiki/Smooth_maximum), \n\n`softmax(a,b) = log(exp(a) + exp(b))`. \n\nthen \n\n`softmin(a,b) = a + b - softmax(a,b)`.  \n\n[Alternatively `softmin(a,b) = -log(exp(-a) + exp(-b))` but this adds numerical instability]\n\nNotice that we now have a differentiable compare-and-swap operation: \n\n`softcswap(a,b) = (softmin(a,b), softmax(a,b))`\n\nWe can alternatively use:\n\n```python\nsmoothmax(a,b, alpha) = (a * exp(a * alpha) +  b * exp(b * alpha)) /\n                         (exp(a * alpha) + exp(b * alpha))\n```\n\nwhich has a configurable `alpha` term, allowing interpolation between a hard maximum (alpha -> infinity) and mean averaging (alpha -> 0).\n\n<img src=\"imgs/Logsumexp_curve.png\" width=\"40%\">\n\n*Softmax/logsumexp/quasimax across the range [-5, 5] for various alpha.*\n\n<img src=\"imgs/Smoothmax_curve.png\" width=\"40%\">\n\n*Smoothmax across the range [-5, 5] for various alpha.*\n\n---\n\n\n## Differentiable sorting\n\nFor each layer in the sorting network, we can split all of the pairwise comparison-and-swaps into left-hand and right-hand sides. We can any write function that selects the relevant elements of the vector as a multiply with a binary matrix.\n\nFor each layer, we can derive two binary matrices `left` and `right` which select the elements to be compared for the left and right hands respectively. This will result in the comparison between two `n/2` length vectors. We can also derive two matrices `l_inv` and `r_inv` which put the results of the compare-and-swap operation back into the right positions in the original vector.\n\nThe entire sorting network can then be written in terms of matrix multiplies and the `softcswap(a, b)` operation.\n\n```python\n        def softmax(a, b):\n            return np.log(np.exp(a) + np.exp(b))\n\n        def softmin(a, b):\n            return a + b - softmax(a, b)\n\n        def softcswap(a, b):\n            return softmin(a, b), softmax(a, b)\n\n        def diff_bisort(matrices, x):   \n            for l, r, l_inv, r_inv in matrices:\n                a, b = softcswap(l @ x, r @ x)\n                x = l_inv @ a + r_inv @ b\n            return x\n```\n\nThe rest of the code is simply computing the `l, r, l_inv, r_inv` matrices, which are fixed for a given `n`. \n\n\nIf we're willing to include a split and join operation, we can reduce this to a single `n x n` multiply for each layer (plus one extra multiply at the start) by fusing consecutive permutations:\n\n```python\n    def diff_sort_weave(fused, x, softmax=np.maximum):\n        \"\"\" Given a set of bitonic sort matrices generated by \n            bitonic_woven_matrices(n), sort a sequence x of length n. \"\"\"\n        split = len(x) // 2\n        x = fused[0] @ x # initial permutation\n        for mat in fused[1:]:\n            a, b = x[:split], x[split:]\n            mx, mn = softcswap(a, b)\n            x = mat @ np.concatenate([mn, mx])\n        return x\n```\n\n\n\n---\n\n## Libraries\n\nThe base code works with NumPy. If you want to use [autograd](https://github.com/HIPS/autograd) [jax](https://github.com/google/jax) or [cupy](https://cupy.chainer.org/) then install the [autoray](https://pypi.org/project/autoray/) package.\n\n    pip install autoray\n\nThe code should then automatically work with whatever backend you are using if you pass arrays from the right backend.\n\n### PyTorch and Tensorflow\nPyTorch is supported:\n```python \nimport torch\nfrom differentiable_sorting.torch import bitonic_matrices, diff_sort\nfrom torch.autograd import Variable\n\nx = [5.0, -1.0, 9.5, 13.2, 16.2, 20.5, 42.0, 18.0]\nmatrices = bitonic_matrices(8)\ntorch_input = Variable(torch.from_numpy(np.array(x)).float(), requires_grad=True)\nresult = diff_sort(matrices, torch_input)\nprint(result)\n\n>>> tensor([-1.0075,  4.9958,  9.4394, 13.2117, 15.9480, 18.2103, 20.6023, 42.0000],\n       grad_fn=<AddBackward0>)\n\nprint(torch.autograd.grad(result[0], torch_input)[0])\n\n>>> tensor([7.3447e-03, 9.9257e-01, 8.1266e-05, 3.9275e-06, 3.4427e-08, 1.4447e-09,\n        0.0000e+00, 9.5952e-09])\n```\n\n\nTensorflow is also supported:\n\n```python \nimport tensorflow as tf\nfrom differentiable_sorting.tensorflow import bitonic_matrices, diff_sort, diff_argsort\n\ntf_input = tf.reshape(tf.convert_to_tensor([5.0, -1.0, 9.5, 13.2, 16.2, 20.5, 42.0, 18.0], dtype=tf.float64), (-1,1))\ntf_output = tf.reshape(diff_sort(tf_matrices, tf_input), (-1,))\nwith tf.Session() as s:    \n    print(s.run((tf_output)))    \n\n>>> [-1.007  4.996  9.439 13.212 15.948 18.21  20.602 42.   ]    \n```\n---\n\nThis implementation was inspired by [this tweet](https://twitter.com/francoisfleuret/status/1139580698694733825) by @francoisfleuret:\n> FranÃ§ois Fleuret @francoisfleuret Jun 14\n>\n>Discussion with Ronan Collober reminded me that (max, +) is a semi-ring, and made me realize that the same is true for (softmax, +) where\n>\n>softmax(a, b) = log(exp(a) + exp(b))\n>\n>All this coolness at the heart of his paper \n>\n>https://arxiv [dot] org/abs/1902.06022 \n\n---\n\n\n## Error analysis\nThe plot below shows the relative RMS (RMS error divided by the maximum range of the input vector) between the softmax sorted array and the ground truth sorted array, for vectors of length `n=2` through `n=512`, with test values distributed randomly uniformly in ranges from [0, 2^-5] to [0, 2^10]. The main factor affecting precision is the numerical range. Small values will be corrupted, but values > ~300 will overflow (in `float64`). The dotted line is shown at 10% relative error.\n\n<img src=\"imgs/error_analysis_float64.png\" width=\"75%\">\n\n**Softmax** error curve.\n\n<img src=\"imgs/error_analysis_float64_smoothmax.png\" width=\"75%\">\n\n**Smoothmax** error curve.\n\n\n\n\n---\n\nThere is also a function to pretty-print bitonic networks:\n\n```python\n        pretty_bitonic_network(8)\n```\n\n        0  1  2  3  4  5  6  7 \n        â•­â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â”‚  â•°â”€â”€â•®  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â•­â”€â”€â•¯  â”‚  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â•°â”€â”€â•® \n        â•­â”€â”€â”€â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â•­â”€â”€â”€â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â•°â”€â”€â”€â”€â”€â•®  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â”‚  â•°â”€â”€â”€â”€â”€â•® \n        â•­â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â”‚  â•­â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â•°â”€â”€â•®  â”‚  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â•°â”€â”€â•® \n        â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â”‚  â”‚  â”‚ \n        â”‚  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â”‚  â”‚ \n        â”‚  â”‚  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯  â”‚ \n        â”‚  â”‚  â”‚  â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ \n        â•­â”€â”€â”€â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â•­â”€â”€â”€â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â•­â”€â”€â”€â”€â”€â•¯  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â”‚  â•­â”€â”€â”€â”€â”€â•¯ \n        â•­â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â”‚  â•­â”€â”€â•¯  â”‚  â”‚  â”‚  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â•­â”€â”€â•¯  â”‚  â”‚ \n        â”‚  â”‚  â”‚  â”‚  â”‚  â”‚  â•­â”€â”€â•¯",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/johnhw/differentiable_sorting/tarball/0.1",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/johnhw/differentiable_sorting",
    "keywords": "sorting,differentiable,autograd,sort,bitonic",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "differentiable-sorting",
    "package_url": "https://pypi.org/project/differentiable-sorting/",
    "platform": "",
    "project_url": "https://pypi.org/project/differentiable-sorting/",
    "project_urls": {
      "Download": "https://github.com/johnhw/differentiable_sorting/tarball/0.1",
      "Homepage": "https://github.com/johnhw/differentiable_sorting"
    },
    "release_url": "https://pypi.org/project/differentiable-sorting/0.0.3/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Differentiable approximate sorting/argsorting with bitonic networks",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7663611,
  "releases": {
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "643dac486cac99e6ef7f760392b513f72e6764aba848a63d79cefc12b360e359",
          "md5": "72e5c2b746e6660fbc728cc85894ff5e",
          "sha256": "de85c778764c4ac90fcd08de7ba51d71b3b85f6eebe81049118c0f917ac399d6"
        },
        "downloads": -1,
        "filename": "differentiable_sorting-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "72e5c2b746e6660fbc728cc85894ff5e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10958,
        "upload_time": "2020-07-09T09:14:46",
        "upload_time_iso_8601": "2020-07-09T09:14:46.013097Z",
        "url": "https://files.pythonhosted.org/packages/64/3d/ac486cac99e6ef7f760392b513f72e6764aba848a63d79cefc12b360e359/differentiable_sorting-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "17726f56631d210ff6f7532d998fd8c525319d8a6bcacc8406c39eab6f9c1fe3",
          "md5": "41bb57abab7ead36e469d28134051471",
          "sha256": "d04842cf51b3fd247ca5792d3c9e8bb277f5d0d1e79f4e7bd27c514f84603c36"
        },
        "downloads": -1,
        "filename": "differentiable_sorting-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "41bb57abab7ead36e469d28134051471",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14485,
        "upload_time": "2020-07-09T09:20:56",
        "upload_time_iso_8601": "2020-07-09T09:20:56.329450Z",
        "url": "https://files.pythonhosted.org/packages/17/72/6f56631d210ff6f7532d998fd8c525319d8a6bcacc8406c39eab6f9c1fe3/differentiable_sorting-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "17726f56631d210ff6f7532d998fd8c525319d8a6bcacc8406c39eab6f9c1fe3",
        "md5": "41bb57abab7ead36e469d28134051471",
        "sha256": "d04842cf51b3fd247ca5792d3c9e8bb277f5d0d1e79f4e7bd27c514f84603c36"
      },
      "downloads": -1,
      "filename": "differentiable_sorting-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "41bb57abab7ead36e469d28134051471",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 14485,
      "upload_time": "2020-07-09T09:20:56",
      "upload_time_iso_8601": "2020-07-09T09:20:56.329450Z",
      "url": "https://files.pythonhosted.org/packages/17/72/6f56631d210ff6f7532d998fd8c525319d8a6bcacc8406c39eab6f9c1fe3/differentiable_sorting-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}