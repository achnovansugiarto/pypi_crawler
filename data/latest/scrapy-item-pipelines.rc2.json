{
  "info": {
    "author": "Kasun Herath",
    "author_email": "kasunh01@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Framework :: Scrapy",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: Implementation :: CPython"
    ],
    "description": "# scrapy-item-pipelines\n\nVarious scrapy item pipelines\n\n## Installation\n\npip install scrapy-item-pipelines\n\n## SaveToKafkaPipeline\n\nItem pipeline to push items to kafka. Items will be converted into JSON format and pushed to a defined kafka topic.\n\n\n### Settings\n\n```\nSL_SCRAPY_ITEM_PIPELINES_SETTINGS = {\n    \"push_to_kafka_hosts\": \"localhost:9092\"  # Kafka broker hosts. Separated with a comma.\n    \"push_to_kafka_default_topic\": \"\"  # kafka default topic.\n}\n```\n\n\n### Usage\n\nIf items should be pushed to different kafka topics per item, the topic can be defined in the item class.\nAlso if a data key should be pushed to kafka we can define the item field value to use by defining it\nin the item class. If no `kafka_data_key` is defined no data key will be pushed.\n\n```\nclass DemoItem(scrapy.Item):\n    kafka_topic = \"topic-to-push-items\"\n    kafka_data_key = \"another_unique_field\"\n\n    field_name = scrapy.Field()\n    another_unique_field = scrapy.Field()\n```\n\nAfter configuring add `scrapy_item_pipelines.streaming.PushToKafkaPipeline` to the ITEM_PIPELINES setting.\n\n```\nITEM_PIPELINES = {\n    ...\n    ...\n    \"scrapy_item_pipelines.streaming.PushToKafkaPipeline\": 999,\n}\n```\n\n## FilterDuplicatesPipeline\n\nItem pipeline to filter out duplicate items calculated using defined keys in the item.\n\n### Usage\n\nDefine an attribute called unique_key in the item. If the unique key is a single field\nunique_key can be defined as a string or if the unique key is a multi field key unique_key\nshould be a tuple of strings. If no unique_key is defined filtering will be done using `id` field.\nIf you want to skip duplicate filtering for an item define unique_key as None.\n\nThe pipeline will include a stats called `duplicate_item_count` which is the number\nof duplicate items dropped.\n\n```\nclass DemoItem(scrapy.Item):\n    field1 = scrapy.Field()\n    field2 = scrapy.Field()\n\n    unique_key = None  # duplicates won't be filtered.\n\n\nclass DemoItem(scrapy.Item):\n    # No unique_key is defined. Filtering will be done using `id` field.\n    field1 = scrapy.Field()\n    field2 = scrapy.Field()\n    id = scrapy.Field()\n\n\nclass DemoItem(scrapy.Item):\n    field1 = scrapy.Field()\n    field2 = scrapy.Field()\n\n    unique_key = \"field1\"  # Duplicates will be filtered using field1.\n\n\nclass DemoItem(scrapy.Item):\n    field1 = scrapy.Field()\n    field2 = scrapy.Field()\n\n    unique_key = (\"field1\", \"field2\")  # Duplicates will be filtered using both field1 and field2\n```\n\nAdd `scrapy_item_pipelines.misc.FilterDuplicatesPipeline` to the ITEM_PIPELINES setting.\n\n\n```\nITEM_PIPELINES = {\n    ...\n    ...\n    \"scrapy_item_pipelines.misc.FilterDuplicatesPipeline\": 500,\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/singletonlabs/scrapy-item-pipelines",
    "keywords": "scrapy,pipelines",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-item-pipelines",
    "package_url": "https://pypi.org/project/scrapy-item-pipelines/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-item-pipelines/",
    "project_urls": {
      "Homepage": "https://github.com/singletonlabs/scrapy-item-pipelines"
    },
    "release_url": "https://pypi.org/project/scrapy-item-pipelines/0.2/",
    "requires_dist": [
      "confluent-kafka",
      "flake8 ; extra == \"dev\""
    ],
    "requires_python": ">=3.6",
    "summary": "Collection of reusable scrapy item pipelines",
    "version": "0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10693569,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "625e5c4d592c1d9e4d1bdbb54dcf61901bc6c88f4a9ddb30c14dfb503b5c5fd2",
          "md5": "e1944f8745bad3827d7b67ae8442d4d0",
          "sha256": "206c414ac6f804a0e71e52d61287668083dfe44371e927a5190850f6460843d6"
        },
        "downloads": -1,
        "filename": "scrapy_item_pipelines-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e1944f8745bad3827d7b67ae8442d4d0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 3920,
        "upload_time": "2021-06-16T08:09:03",
        "upload_time_iso_8601": "2021-06-16T08:09:03.712453Z",
        "url": "https://files.pythonhosted.org/packages/62/5e/5c4d592c1d9e4d1bdbb54dcf61901bc6c88f4a9ddb30c14dfb503b5c5fd2/scrapy_item_pipelines-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "80dbe7f38bbf85b67778dc315d1b2c20aaa4425ed6a4953b922ef804eab1f846",
          "md5": "7d33e63c4fc041ff2cfa90e0068742ed",
          "sha256": "66893b29511c998578f7d025a14d0c1de68621fae0c1bc7c8c0e3d4d2600a4ec"
        },
        "downloads": -1,
        "filename": "scrapy-item-pipelines-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7d33e63c4fc041ff2cfa90e0068742ed",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 4446,
        "upload_time": "2021-06-16T08:09:19",
        "upload_time_iso_8601": "2021-06-16T08:09:19.603520Z",
        "url": "https://files.pythonhosted.org/packages/80/db/e7f38bbf85b67778dc315d1b2c20aaa4425ed6a4953b922ef804eab1f846/scrapy-item-pipelines-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d23a28f79377dc83ff367898d69645b19ee5c60f25452cc01b99435767f20c01",
          "md5": "70d3edca16580e220129817331478c13",
          "sha256": "72fb9d42e4bb83450aadc8d370e2c33a957543b0807db460ef65002122174dac"
        },
        "downloads": -1,
        "filename": "scrapy_item_pipelines-0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "70d3edca16580e220129817331478c13",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 5045,
        "upload_time": "2021-06-20T08:33:21",
        "upload_time_iso_8601": "2021-06-20T08:33:21.792372Z",
        "url": "https://files.pythonhosted.org/packages/d2/3a/28f79377dc83ff367898d69645b19ee5c60f25452cc01b99435767f20c01/scrapy_item_pipelines-0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "25ec99d8721a694949cc79cbb6fc69694be58b5d5832c2a46c4a9da504a8f4de",
          "md5": "8c2456aff66c00af4f2a3672c41a4430",
          "sha256": "bdbcb1a64aa1190eab33da41f493f18dc289f7d37e7a2680c7659f9816fee0c9"
        },
        "downloads": -1,
        "filename": "scrapy-item-pipelines-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "8c2456aff66c00af4f2a3672c41a4430",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 5286,
        "upload_time": "2021-06-20T08:33:32",
        "upload_time_iso_8601": "2021-06-20T08:33:32.692104Z",
        "url": "https://files.pythonhosted.org/packages/25/ec/99d8721a694949cc79cbb6fc69694be58b5d5832c2a46c4a9da504a8f4de/scrapy-item-pipelines-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d23a28f79377dc83ff367898d69645b19ee5c60f25452cc01b99435767f20c01",
        "md5": "70d3edca16580e220129817331478c13",
        "sha256": "72fb9d42e4bb83450aadc8d370e2c33a957543b0807db460ef65002122174dac"
      },
      "downloads": -1,
      "filename": "scrapy_item_pipelines-0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "70d3edca16580e220129817331478c13",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 5045,
      "upload_time": "2021-06-20T08:33:21",
      "upload_time_iso_8601": "2021-06-20T08:33:21.792372Z",
      "url": "https://files.pythonhosted.org/packages/d2/3a/28f79377dc83ff367898d69645b19ee5c60f25452cc01b99435767f20c01/scrapy_item_pipelines-0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "25ec99d8721a694949cc79cbb6fc69694be58b5d5832c2a46c4a9da504a8f4de",
        "md5": "8c2456aff66c00af4f2a3672c41a4430",
        "sha256": "bdbcb1a64aa1190eab33da41f493f18dc289f7d37e7a2680c7659f9816fee0c9"
      },
      "downloads": -1,
      "filename": "scrapy-item-pipelines-0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "8c2456aff66c00af4f2a3672c41a4430",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 5286,
      "upload_time": "2021-06-20T08:33:32",
      "upload_time_iso_8601": "2021-06-20T08:33:32.692104Z",
      "url": "https://files.pythonhosted.org/packages/25/ec/99d8721a694949cc79cbb6fc69694be58b5d5832c2a46c4a9da504a8f4de/scrapy-item-pipelines-0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}