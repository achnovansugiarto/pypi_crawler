{
  "info": {
    "author": "Alexey Tochin",
    "author_email": "alexey.tochin@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# tf-seq2seq-losses\nTensorflow implementations for\n[Connectionist Temporal Classification](file:///home/alexey/Downloads/Connectionist_temporal_classification_Labelling_un.pdf)\n(CTC) loss in TensorFlow.\n\n## Installation\nTested with Python 3.7. \n```bash\n$ pip install tf-seq2seq-losses\n```\n\n## Why\n### 1. Faster\nOfficial CTC loss implementation \n[`tf.nn.ctc_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss)\nis unreasonably slow. \nThe proposed implementation is approximately 30 times faster as it follows form the benchmark:\n\n|       name       |      forward time      |  gradient calculation time  |                 \n|:----------------:|:----------------------:|:---------------------------:|\n|  tf.nn.ctc_loss  |      13.2 ± 0.02       |          10.4 ± 3.          |\n| classic_ctc_loss |     0.138 ± 0.006      |         0.28 ± 0.01         |\n| simple_ctc_loss  |     0.0531 ± 0.003     |        0.119 ± 0.004        |\n\n(Tested on single GPU: GeForce GTX 970,  Driver Version: 460.91.03, CUDA Version: 11.2). See \n[`benchmark.py`](tests/performance_test.py)\nfor the experimental setup. To reproduce this benchmark use\n```bash\n$ pytest -o log_cli=true --log-level=INFO tests/benchmark.py\n```\nfrom the project root directory.\nHere `classic_ctc_loss` is the standard version of CTC loss\nthat corresponds to the decoding with repeated tokens collapse like \n> a_bb_ccc_c   ->   abcc\n\n(equivalent to `tensorflow.nn.ctc_loss`).\nThe loss function `simple_ctc_loss` is a simplified version corresponding to the trivial decoding rule, for example,\n\n> a_bb_ccc_c   ->   abbcccc\n\n(simple blank removing).\n\n### 2. Supports second order derivative\n```python\n    import tensorflow as tf\n    from tf_seq2seq_losses import classic_ctc_loss \n\n    batch_size = 2\n    num_tokens = 3\n    logit_length = 5\n    labels = tf.constant([[1, 2, 2, 1], [1, 2, 1, 0]], dtype=tf.int32)\n    label_length = tf.constant([4, 3], dtype=tf.int32)\n    logits = tf.zeros(shape=[batch_size, logit_length, num_tokens], dtype=tf.float32)\n    logit_length = tf.constant([5, 4], dtype=tf.int32)\n\n    with tf.GradientTape(persistent=True) as tape1: \n        tape1.watch([logits])\n        with tf.GradientTape() as tape2:\n            tape2.watch([logits])\n            loss = tf.reduce_sum(classic_ctc_loss(\n                labels=labels,\n                logits=logits,\n                label_length=label_length,\n                logit_length=logit_length,\n                blank_index=0,\n            ))\n        gradient = tape2.gradient(loss, sources=logits)\n    hessian = tape1.batch_jacobian(gradient, source=logits, experimental_use_pfor=False)\n    # shape = [2, 5, 3, 5, 3]\n```\nAs well as for the first order gradient the TensorFlow autogradient is not used here.\nInstead, an approach similar to \n[this one](https://www.tensorflow.org/api_docs/python/tf/nn/ctc_loss)\nis used with complexity \n<img src=\"https://render.githubusercontent.com/render/math?math=O(l^4)\">\nwhere \n<img src=\"https://render.githubusercontent.com/render/math?math=l\">\nis the sequence length (the complexity for the gradient is \n<img src=\"https://render.githubusercontent.com/render/math?math=O(l^2)\">\n). \n\n### 3. Numerically stable \n1. Proposed implementation is more numerically stable. For example, it calculates resonable output for\n`logits` of order `1e+10` and even for `-tf.inf`.\n2. If logit length is too short to predict label output the probability of expected prediction is zero.\nThus, the loss output is `tf.inf` for this sample but not `707.13184` for `tf.nn.ctc_loss`.\n\n\n### 4. No C++ compilation\nThis is a pure Python/TensorFlow implementation. We do not have to build or compile any C++/CUDA stuff.\n\n\n## Usage\nThe interface is identical to `tensorflow.nn.ctc_loss` with `logits_time_major=False`.\n```python\nimport tensorflow as tf\nfrom tf_seq2seq_losses import classic_ctc_loss\n\nbatch_size = 1\nnum_tokens = 3 # = 2 tokens + blank\nlogit_length = 5\nloss = classic_ctc_loss(\n    labels=tf.constant([[1, 2, 2, 1]], dtype=tf.int32),\n    logits=tf.zeros(shape=[batch_size, logit_length, num_tokens], dtype=tf.float32),\n    label_length=tf.constant([4], dtype=tf.int32),\n    logit_length=tf.constant([logit_length], dtype=tf.int32),\n    blank_index=0,\n)\n```\n\n## Under the roof\nTensorFlow operations sich as `tf.while_loop` and `tf.TensorArray`. \nThe bottleneck is the iterations over the logit length in order to calculate\nα and β\n(see, for example, the original \n[paper](file:///home/alexey/Downloads/Connectionist_temporal_classification_Labelling_un.pdf)\n). Expected gradient GPU calculation time is linear over logit length. \n\n## Known Probelems\n1. Warning:\n> AutoGraph could not transform <function classic_ctc_loss at ...> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n\nobserved for TensorFlow version 2.4.1\nhas no effect for performance. It is caused by `Union` in type annotations. \n\n2. `tf.jacobian` and `tf.batch_jacobian` for second derivative of `classic_ctc_loss` \nwith `experimental_use_pfor=False` in `tf.GradientTape` causes an unexpected `UnimplementedError`.\nfor TensorFlow version 2.4.1. This can be avoided by `experimental_use_pfor=True` \nor by using `ClassicCtcLossData.hessain` directly without `tf.GradientTape`. \n\n## Future plans\n1. Add decoding (inference)\n2. Add rnn-t loss.\n3. Add m-wer loss.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/alexeytochin/tf-seq2seq-losses",
    "keywords": "tensorflow,loss,loss function,ctc,connectionist temporal classification,seq2se,seq 2 seq,seq to seq,asr,automatic speach recognition,sequence recognitionhessian,second derivative",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tf-seq2seq-losses",
    "package_url": "https://pypi.org/project/tf-seq2seq-losses/",
    "platform": "",
    "project_url": "https://pypi.org/project/tf-seq2seq-losses/",
    "project_urls": {
      "Homepage": "https://github.com/alexeytochin/tf-seq2seq-losses"
    },
    "release_url": "https://pypi.org/project/tf-seq2seq-losses/0.2.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Tensorflow implementations for seq2seq Machine Learning model loss functions",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12830460,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cb97a624caab0fe3e7bc71ae42dbd8337933b8350433b597de5105c058f6c8e2",
          "md5": "231fd155b483face3381a92c3f47360e",
          "sha256": "de36504a0e159e3898745621845a59895466424cabf97999e3561d84c0a828bb"
        },
        "downloads": -1,
        "filename": "tf-seq2seq-losses-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "231fd155b483face3381a92c3f47360e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18965,
        "upload_time": "2021-11-28T19:30:32",
        "upload_time_iso_8601": "2021-11-28T19:30:32.212283Z",
        "url": "https://files.pythonhosted.org/packages/cb/97/a624caab0fe3e7bc71ae42dbd8337933b8350433b597de5105c058f6c8e2/tf-seq2seq-losses-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f13fceac542bf9fb011a471e4a8c402c314f7d5b4daa2b308d09ca0cb129242",
          "md5": "64148cec020e025ba3108836d2df9030",
          "sha256": "b0c29cc5292e2ba3b4802bb51a8818fea0202e8252bd158e0085921652be8d0d"
        },
        "downloads": -1,
        "filename": "tf-seq2seq-losses-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "64148cec020e025ba3108836d2df9030",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 17974,
        "upload_time": "2021-11-28T19:38:39",
        "upload_time_iso_8601": "2021-11-28T19:38:39.463610Z",
        "url": "https://files.pythonhosted.org/packages/5f/13/fceac542bf9fb011a471e4a8c402c314f7d5b4daa2b308d09ca0cb129242/tf-seq2seq-losses-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bebb5e6efaa7c99f2d34f71f99701455c052a1c4b970946a149ecb556b4a2217",
          "md5": "d28a1c1da7a07d9a87c3497200d0b3be",
          "sha256": "c77c4f96e0bfb5b6d8418bce36b297decdd0a45893c10d81b3e274b6ca179173"
        },
        "downloads": -1,
        "filename": "tf-seq2seq-losses-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "d28a1c1da7a07d9a87c3497200d0b3be",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18210,
        "upload_time": "2021-11-30T18:41:27",
        "upload_time_iso_8601": "2021-11-30T18:41:27.768891Z",
        "url": "https://files.pythonhosted.org/packages/be/bb/5e6efaa7c99f2d34f71f99701455c052a1c4b970946a149ecb556b4a2217/tf-seq2seq-losses-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9971dfa8271b526a7c3381c3b33a3520573d90b3a5853c74d8ea5827fd2db2f2",
          "md5": "aa1929c9336bded03996401ea22fd892",
          "sha256": "66cacc99454fd71c0568b1654d17a35ed7155141b8b860ec7fc3ea022379afaa"
        },
        "downloads": -1,
        "filename": "tf-seq2seq-losses-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "aa1929c9336bded03996401ea22fd892",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18213,
        "upload_time": "2021-12-01T10:37:37",
        "upload_time_iso_8601": "2021-12-01T10:37:37.584900Z",
        "url": "https://files.pythonhosted.org/packages/99/71/dfa8271b526a7c3381c3b33a3520573d90b3a5853c74d8ea5827fd2db2f2/tf-seq2seq-losses-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ae46317e552d2497b4cde96056ce43cc77d97a77189a416c6493e02c48d8108a",
          "md5": "905e1f3441a84bc1900eed84ee8a06d1",
          "sha256": "22ead059173bcfe5f22e9d1c0f2faa163bd99614af09387cd04c65449f0e3330"
        },
        "downloads": -1,
        "filename": "tf-seq2seq-losses-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "905e1f3441a84bc1900eed84ee8a06d1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22885,
        "upload_time": "2022-02-08T20:54:09",
        "upload_time_iso_8601": "2022-02-08T20:54:09.329608Z",
        "url": "https://files.pythonhosted.org/packages/ae/46/317e552d2497b4cde96056ce43cc77d97a77189a416c6493e02c48d8108a/tf-seq2seq-losses-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ae46317e552d2497b4cde96056ce43cc77d97a77189a416c6493e02c48d8108a",
        "md5": "905e1f3441a84bc1900eed84ee8a06d1",
        "sha256": "22ead059173bcfe5f22e9d1c0f2faa163bd99614af09387cd04c65449f0e3330"
      },
      "downloads": -1,
      "filename": "tf-seq2seq-losses-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "905e1f3441a84bc1900eed84ee8a06d1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 22885,
      "upload_time": "2022-02-08T20:54:09",
      "upload_time_iso_8601": "2022-02-08T20:54:09.329608Z",
      "url": "https://files.pythonhosted.org/packages/ae/46/317e552d2497b4cde96056ce43cc77d97a77189a416c6493e02c48d8108a/tf-seq2seq-losses-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}