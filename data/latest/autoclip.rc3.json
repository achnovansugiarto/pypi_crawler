{
  "info": {
    "author": "HesitantlyHuman",
    "author_email": "tannersims@hesitantlyhuman.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# AutoClip\nPytorch and tensorflow implementations (and variations) of the AutoClip gradient smoothing procedure from [Seetharaman et al](https://arxiv.org/abs/2007.14469).\n\n> Prem Seetharaman, Gordon Wichern, Bryan Pardo, Jonathan Le Roux. \"AutoClip: Adaptive Gradient Clipping for Source Separation Networks.\" 2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2020.\n\n## About\n\nWhile training your model, AutoClip keeps a running history of all of your model's gradient magnitudes. Using these, the gradient clipper can adaptively clamp outlier gradient values before they reach the optimizer of your choice.\n\nWhile AutoClip is great as a preventative measure against exploding gradients, it also speeds up training time, and encourages the optimizer to find more optimal models. At an intuitive level, AutoClip compensates for the stochastic nature of training over batches, regularizing training effects.\n\n## Installation\n\nAutoClip is listed on pypi. To install AutoClip simply run the following command\n```\npip install autoclip\n```\nand the `autoclip` package will be installed in your currently active environment.\n\n## Torch API\n\nBelow are some examples how to use `autoclip`'s torch API.\n\n### Clippers as Optimizer Wrappers\nUsing the optimizer wrapping pattern is the recommended way to use AutoClip, and `autoclip`'s torch API supports wrapping arbitrary pytorch optimizers. The wrapping pattern allows you to avoid changing your training code when you want to use an AutoClip clipper. This is especially useful if you do not own the training code for whatever reason. (Say for example you are using someone else's Trainer class, as is often the case with frameworks like `huggingface`.)\n\nThe following is an example of how to integrate AutoClip into your model training using this pattern:\n```python\nimport torch\nfrom autoclip.torch import QuantileClip\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 2)\n)\n\noptimizer = torch.optim.AdamW(model.parameters())\noptimizer = QuantileClip.as_optimizer(optimizer=optimizer, quantile=0.9, history_length=1000)\n```\nNow you can use the optimizer just like you would have before adding the clipper, and the clipping will be applied automatically.\n\n### Raw AutoClip Clippers\nYou can still use the clipper manually if you would like. If this is the case, then you would create your clipper like this:\n```python\nimport torch\nfrom autoclip.torch import QuantileClip\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 2)\n)\n\nclipper = QuantileClip(model.parameters(), quantile=0.9, history_length=1000)\n```\nThen, to clip the model's gradients, simply run the clipper's `.step()` function during your training loop. Note that you should call the clipper's `step` before you call your optimizer's `step`. Calling it after would mean that your clipping will have no effect, since the model will have already been updated using the unclipped gradients. For example:\n```python\nfor batch_num, batch in enumerate(training_dataset):\n    model_prediction = model(batch['data'])\n    loss = loss_function(model_prediction, batch['targets'])\n    loss.backward()\n    clipper.step() # clipper comes before optimizer\n    optimizer.step()\n```\n\n### Global vs Local Clipping\n`autoclip`'s torch clippers support two clipping modes. The first is `global_clipping`, which is the original AutoClip as described in Seetherman et al. The second is local or parameter-wise clipping. In this mode a history is kept for every parameter, and each is clipped according to its own history. By default, the `autoclip` clippers will use the parameter-wise clipping.\nTo use the global mode, simply pass the appropriate flag:\n```python\nclipper = QuantileClip(\n    model.parameters(),\n    quantile=0.9,\n    history_length=1000,\n    global_clipping=True\n)\n```\n\n### Checkpointing\nThe torch clippers also support checkpointing through `state_dict()` and `load_state_dict()`, just like torch models and optimizers. For example, if you want to checkpoint a clipper to `clipper.pth`:\n```python\nclipper = QuantileClip(model.parameters())\ntorch.save(clipper.state_dict(), 'clipper.pth')\n\n# Then later\nclipper = QuantileClip(model.parameters())\nclipper.load_state_dict(torch.load('clipper.pth'))\n```\nKeep in mind that just like a torch optimizer this will error if you give the clipper differently sized model parameters.\n\nWhile it is generally recommended to use `state_dict`s instead (see the [pytorch documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-entire-model) on this subject for more info), you may also use `torch.save` and `torch.load` directly to pickle the entire clipper object.\n\n## Tensorflow\n`autoclip`'s tensorflow API does not currently have feature parity with the torch API (If you want to change this, feel free to [contribute](https://github.com/HesitantlyHuman/autoclip/issues/2)).\nAs it is, the tensorflow API currently only supports the original AutoClip algorithm, and does not support checkpointing. Below is a short example:\n```python\nimport tensorflow as tf\nfrom autoclip.tf import QuantileClip\n\nmodel = tf.keras.models.Sequential(\n    [\n        tf.keras.layers.Dense(50),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Dense(10),\n        tf.keras.layers.ReLU(),\n        tf.keras.layers.Dense(\n            2,\n            activation=tf.keras.activations.tanh\n        ),\n    ]\n)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(\n        learning_rate=0.001,\n        gradient_transformers=[\n            QuantileClip(\n                quantile=0.9,\n                history_length=1000\n            )\n        ]\n    ),\n    loss=\"mean_absolute_error\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(train_data, train_targets)\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/HesitantlyHuman/autoclip",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "autoclip",
    "package_url": "https://pypi.org/project/autoclip/",
    "platform": null,
    "project_url": "https://pypi.org/project/autoclip/",
    "project_urls": {
      "Homepage": "https://github.com/HesitantlyHuman/autoclip"
    },
    "release_url": "https://pypi.org/project/autoclip/0.2.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Smart gradient clippers",
    "version": "0.2.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15248449,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ff4fc5c080ce6190399b31ee0fbeb46782e4dfa43bcad7672d2223f919ec8bd2",
          "md5": "0650914908caf46e547862db39b2042f",
          "sha256": "74ae38cc3aa6b2691f63b1ec62bb3575840e22daaad6c3cb284a3e190820e219"
        },
        "downloads": -1,
        "filename": "autoclip-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0650914908caf46e547862db39b2042f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 9482,
        "upload_time": "2022-07-09T21:12:38",
        "upload_time_iso_8601": "2022-07-09T21:12:38.542457Z",
        "url": "https://files.pythonhosted.org/packages/ff/4f/c5c080ce6190399b31ee0fbeb46782e4dfa43bcad7672d2223f919ec8bd2/autoclip-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "394f161401af655bc8f764ec0aa70eecb11630cc6738b066a6f194a0260f8500",
          "md5": "bb62141860e04d3b09b192b8cad59f65",
          "sha256": "5426d71d0feebc0c834325fd41a4a7783aa9d57e175906905622ed6ac6bbc001"
        },
        "downloads": -1,
        "filename": "autoclip-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bb62141860e04d3b09b192b8cad59f65",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9308,
        "upload_time": "2022-07-09T21:12:40",
        "upload_time_iso_8601": "2022-07-09T21:12:40.954883Z",
        "url": "https://files.pythonhosted.org/packages/39/4f/161401af655bc8f764ec0aa70eecb11630cc6738b066a6f194a0260f8500/autoclip-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fae1a2bf975f3d28534934d3aa6f392b161971943de132a3f84c51fa75d5e708",
          "md5": "784faa65bd67e9d3c74db49da034b22d",
          "sha256": "a4949fbd5f56402c8ec00881a4b6fe5f775d8d6b01556111d82f8765e909d243"
        },
        "downloads": -1,
        "filename": "autoclip-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "784faa65bd67e9d3c74db49da034b22d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10823,
        "upload_time": "2022-07-16T05:46:46",
        "upload_time_iso_8601": "2022-07-16T05:46:46.516446Z",
        "url": "https://files.pythonhosted.org/packages/fa/e1/a2bf975f3d28534934d3aa6f392b161971943de132a3f84c51fa75d5e708/autoclip-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "21d777dd42b4dabe962e935cdcf3b7de3b8ac92e71bce83eef5c15d11e89f6a8",
          "md5": "7aa14b56a625d19b726a746721e7895e",
          "sha256": "ab653babb37db1c7188a8841d6f5e7cf23f14f15d7aaaf298c5eb41218d87ecb"
        },
        "downloads": -1,
        "filename": "autoclip-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7aa14b56a625d19b726a746721e7895e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10615,
        "upload_time": "2022-07-16T05:46:50",
        "upload_time_iso_8601": "2022-07-16T05:46:50.173126Z",
        "url": "https://files.pythonhosted.org/packages/21/d7/77dd42b4dabe962e935cdcf3b7de3b8ac92e71bce83eef5c15d11e89f6a8/autoclip-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "85117c85521c95f04fbcefbd11bf1fbd84bb74cff8b60ecd722a1e2d09d1e71b",
          "md5": "0dec9fcca7c1adf05fcb38e12b6d32ec",
          "sha256": "33682d198d0190bd6fcee1e9ad71d776ad2c6c3552b8c60a55b65d8ce6fbe4ce"
        },
        "downloads": -1,
        "filename": "autoclip-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0dec9fcca7c1adf05fcb38e12b6d32ec",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 11193,
        "upload_time": "2022-09-29T02:15:22",
        "upload_time_iso_8601": "2022-09-29T02:15:22.769501Z",
        "url": "https://files.pythonhosted.org/packages/85/11/7c85521c95f04fbcefbd11bf1fbd84bb74cff8b60ecd722a1e2d09d1e71b/autoclip-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b2c455cad7944883a4813b0f8048a5146beef2bf0f28cca660fd46173fe84900",
          "md5": "97aa491ecb0ff02f5c18e4851155b76d",
          "sha256": "85cd478ff54bb41658757916b59f352912744a8078075ed1278367d047d27de3"
        },
        "downloads": -1,
        "filename": "autoclip-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "97aa491ecb0ff02f5c18e4851155b76d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11138,
        "upload_time": "2022-09-29T02:15:26",
        "upload_time_iso_8601": "2022-09-29T02:15:26.705558Z",
        "url": "https://files.pythonhosted.org/packages/b2/c4/55cad7944883a4813b0f8048a5146beef2bf0f28cca660fd46173fe84900/autoclip-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "85117c85521c95f04fbcefbd11bf1fbd84bb74cff8b60ecd722a1e2d09d1e71b",
        "md5": "0dec9fcca7c1adf05fcb38e12b6d32ec",
        "sha256": "33682d198d0190bd6fcee1e9ad71d776ad2c6c3552b8c60a55b65d8ce6fbe4ce"
      },
      "downloads": -1,
      "filename": "autoclip-0.2.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0dec9fcca7c1adf05fcb38e12b6d32ec",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 11193,
      "upload_time": "2022-09-29T02:15:22",
      "upload_time_iso_8601": "2022-09-29T02:15:22.769501Z",
      "url": "https://files.pythonhosted.org/packages/85/11/7c85521c95f04fbcefbd11bf1fbd84bb74cff8b60ecd722a1e2d09d1e71b/autoclip-0.2.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b2c455cad7944883a4813b0f8048a5146beef2bf0f28cca660fd46173fe84900",
        "md5": "97aa491ecb0ff02f5c18e4851155b76d",
        "sha256": "85cd478ff54bb41658757916b59f352912744a8078075ed1278367d047d27de3"
      },
      "downloads": -1,
      "filename": "autoclip-0.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "97aa491ecb0ff02f5c18e4851155b76d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 11138,
      "upload_time": "2022-09-29T02:15:26",
      "upload_time_iso_8601": "2022-09-29T02:15:26.705558Z",
      "url": "https://files.pythonhosted.org/packages/b2/c4/55cad7944883a4813b0f8048a5146beef2bf0f28cca660fd46173fe84900/autoclip-0.2.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}