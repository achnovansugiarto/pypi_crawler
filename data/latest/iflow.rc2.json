{
  "info": {
    "author": "Christina Gao,             Joshua Isaacson,             Claudius Krause",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3"
    ],
    "description": "<!--# i-flow-->\n\n<!--![i-flow logo](i-flow.gif)-->\n<div align=\"center\">\n\n<img src=\"i-flow_crop.gif\"  alt=\"i-flow-logo\" width=\"500\" height=\"200\">\n</div>\n\n[![pipeline status](https://gitlab.com/i-flow/i-flow/badges/master/pipeline.svg)](https://gitlab.com/i-flow/i-flow/commits/master)\n[![coverage report](https://gitlab.com/i-flow/i-flow/badges/master/coverage.svg)](https://gitlab.com/i-flow/i-flow/commits/master)\n\nThis project implements normalizing flows using [TensorFlow](https://tensorflow.org) for the purpose of multi-dimensional integration and sampling.\n\n## Citation\nIf you use **i-flow** for your research, please cite:  \n- *\"i-flow: High-dimensional Integration and Sampling with Normalizing Flows\"*,  \nBy Christina Gao, Joshua Isaacson, Claudius Krause.  \nMach. Learn.: Sci. Technol. [DOI](https://doi.org/10.1088/2632-2153/abab62) <br>\n[arXiv:2001.05486](https://arxiv.org/abs/2001.05486). \n\n## Table of Contents\n\n[[_TOC_]]\n\n## Importance Sampling\n\nIn high dimensional integration, Monte-Carlo (MC) techniques are required in order to obtain convergence of the integral in\na efficient manner (See the discussion of [curse of\ndimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) for details). The na&iuml;ve MC approach would\nbe to uniformly sample points over the integration domain ($`\\Omega`$) encompassing a volume $`V`$. The integral $`I`$ of a function $`f(x)`$ can then be estimated from\nevaluating the function at $`N`$ random points and taking the mean, i.e.\n```math\nI = \\int_\\Omega f(x)\\ dx \\approx \\frac{V}{N}\\sum_{i=1}^{N} f(x_i) \\equiv V \\langle f \\rangle_x,\n```\nand the uncertainty is determined by the standard deviation of the mean, i.e.\n```math\n\\sigma_f \\approx V \\sqrt{\\frac{\\langle f^2\\rangle_x - \\langle f\\rangle_x^2}{N-1}}.\n```\nTo improve upon the uncertainty of the estimated integral, a technique known as **importance sampling** was\ndeveloped. The goal of importance sampling is the minimize the standard deviation by sampling points from a\ndistribution that is closer to the function $`f(x)`$ than the uniform distribution. Given a probability\ndistribution function $`g(x)`$\nwith cumulative distribution function $`G(x)`$, it is possible to transform the integral using the change of variable\nformula, giving:\n```math\nI = \\int_\\Omega \\frac{f(x)}{g(x)}\\ dG(x) \\approx \\frac{V}{N}\\sum_{i=1}^N\n\\frac{f(\\tilde{x}_i)}{g(\\tilde{x}_i)} \\equiv V \\langle f/g\\rangle_G,\n```\nwhere $`\\tilde{x}_i`$ is distributed according to $`g(x)`$. The standard deviation can be calculated in a similar method.\nFrom this, it can easily be seen that if $`g(x) \\rightarrow f(x)/I`$, then the uncertainty from this method would\nproduce an integral estimate with vanishing uncertainty. However, this would require to know the analytic solution\nto the integral!\n\nTherefore, the goal of importance sampling is to find a distribution $`g(x)`$ as close to $`f(x)`$ as possible that is\nalso easy to sample from.\n\n## Normalizing Flows\n\nA Normalizing Flow is a bijective transformation from a (usually simple) base distribution to a \n(usually more complicated) target distribution, i.e. a change of variables transformation. \nIt is therefore able to describe arbitrary, high-dimensional probability disctributions of continuous\nvariables. The transformation is invertible and can therefore be used in two directions: In the one \ndirection, we can infer the probability of a given sample point. In the other direction, we can sample \nrandom numbers according to the target distribution. Usually, Normalizing Flows are implemented usitilizing\nNeural Networks, see [[1]](#references) for a review. Different architectures exist that are either fast\nin inference and slow at sampling or the other way around. Since our application for numerical integration with \nimportance sampling requires both directions to be fast, we use the architecture based on Coupling Layers that\nwas proposed in [[2]](#references). <br>\n\nIn a Coupling Layer, the $`N`$-dimensional is split in to two sets $`\\vec{x}_a=\\{ x_1, \\ldots, x_n\\}`$ and\n$`\\vec{x}_b=\\{x_{n+1}, \\ldots, x_N\\}`$, where $`1 \\leq n < N`$. One set remains untransformed, but is used as\ninput into a neural network. The other set is transformed based on a set of parameters determined from the output of\nthe neural network. In other words,\n```math\nx'_a = x_a, \\quad\\quad\\quad\\quad\\quad\\quad\\ \\  A\\in[1,n]\\quad\\quad\\  \\\\\nx'_b = C(x_b; m(\\vec{x}_a)),\\quad\\quad B\\in[n+1,N]\n```\nwith the inverse map given by: \n```math\nx_a = x'_a,\\quad\\quad\nx_b = C^{-1}(x'_b; m(\\vec{x'}_a)) = C^{-1}(x'_b; m(\\vec{x}_a)).\n```\nThis leads to an analytic Jacobian independent on the gradient of the neural network $`m(\\vec{x}_a)`$, and\ncan be calculated as:\n```math\n\\left\\lvert\\frac{\\partial c(\\vec{x})}{\\partial\\vec{x}}\\right\\rvert^{-1}\n= \\left\\lvert\n\\begin{pmatrix}\n\\vec{1} & 0 \\\\\n\\frac{\\partial C}{\\partial m}\\frac{\\partial m}{\\partial \\vec{x}_a} & \\frac{\\partial C}{\\partial \\vec{x}_b}\n\\end{pmatrix}\n\\right\\rvert^{-1}\n= \\left\\lvert \\frac{\\partial C(\\vec{x}_b; m(\\vec{x}_a))}{\\partial \\vec{x}_b} \\right\\rvert^{-1}.\n```\n\nBelow is a graphical representation of single coupling layer, plus a permutation on the indices before being passed\ninto a subsequent layer.\n<div align=\"center\">\n<img src=\"CL.png\" width=\"500\">\n</div>\n\n\n### Coupling Layers\nCurrently, there are three well tested and validated coupling layers implemented in the **i-flow** package. These are:\n- [Linear Spline](#linear-spline)\n- [Quadratic Spline](#quadratic-spline)\n- [Rational Quadratic Spline](#rational-quadratic-spline)\n\n#### Linear Spline\n\nThe linear spline is based on a piecewise linear function defining the CDF [[3]](#references).\nGiven $`N`$ dimensions and $`K`$ bins with a width of $`w`$ the PDF is defined as\n$`q_i(t) = \\{Q_{ij} | i \\in [1,N], (j-1)w \\leq t < jw, j \\in [1, K]\\}`$.\nThe CDF can be calculated by integrating the PDF, giving:\n```math\nC(x_i^B; Q) = \\alpha Q_{ib} + \\sum_{k=1}^{b-1} Q_{ik},\n```\nwhere $`b`$ is the bin in which $`x_i^B`$ occurs, and $`\\alpha=\\frac{x_i^B-(b-1)w}{w}`$.\nThe Jacobian for this calculation is straight forward to calculate and is:\n```math\n\\left\\lvert \\frac{\\partial C}{\\partial x_B} \\right\\rvert = \\prod_i Q_{ib}\n```\n\n#### Quadratic Spline\n\nThe quadratic spline is also based on a piecewise quadratic function defining the\nCDF [[2]](#references). Given $`N`$ dimensions, $`K`$ bins with widths $`W_{ik}`$, and $`K+1`$ vertices\nwith heights $`V_{ik}`$, the PDF is defined as:\n```math\nq_i(t) = \\begin{cases}\n      \\frac{V_{i2}-V_{i1}}{W_{i1}}t+V_{i1} & t < W_{i1} \\\\\n      \\frac{V_{i3}-V_{i2}}{W_{i2}}\\left(t-W_{i1}\\right)+V_{i2} & W_{i1} \\leq t < W_{i1}+W_{i2} \\\\\n      \\quad\\vdots & \\\\\n      \\frac{V_{i(K+1)}-V_{iK}}{W_{iK}}\\left(t-\\sum_{k=1}^{K-1}W_{ik}\\right)+V_{iK} & \\sum_{k=1}^{K-1} W_{ik} \\leq t < 1 \\\\\n      \\end{cases}\n```\nIntegrating the above to obtain the CDF gives:\n```math\nC(x_i^B; W, V) = \\frac{\\alpha^2}{2}\\left(V_{ib+1}-V_{ib}\\right) W_{ib} + V_{ib}W_{ib}\\alpha + \\sum_{k=1}^{b-1}\\frac{V_{ik+1} + V_{ik}}{2} W_{ik},\n```\nwith $`b`$ the solution to $`\\sum_{k=1}^{b-1} W_{ik} \\leq x_i^B < \\sum_{k=1}^b W_{ik}`$,\nand $`\\alpha = \\frac{x_i^B-\\sum_{k=1}^{b-1}W_{ik}}{W_{ib}}`$ is the relative position of $`x_i^B`$ in bin $`b`$.\n\n#### Rational Quadratic Spline\n\nThe rational quadratic spline was originally defined in [[4]](#references).\nGiven $`N`$ dimensions, $`K+1`$ knot points $`\\{(x^{(k)}, y^{(k)}) \\}_{k=0}^K`$ that are monotonically increasing, with\n$`( x^{(0)}, y^{(0)} ) = (0, 0)`$ and $`( x^{(K)}, y^{(K)} ) = (1, 1)`$, and \n$`K+1`$ non-negative derivatives $`\\{ d^{(k)} \\}_{k=0}^K`$, the CDF can be calculated using the algorithm from\n[[5]](#references), and is reproduced below.\n\nFirst define bin widths $`w^{(k)} = x^{(k+1)} - x^{(k)}`$ and slopes $`s^{k}=\\frac{y^{(k+1)}-y^{(k)}}{w^{(k)}}`$.\nGiven the point $`x`$, the bin in which $`x`$ falls is $`k`$, and the fractional distance $`x`$ is between the two\ncorresponding knots is $`\\xi=\\frac{x-x^{(k)}}{w^{(k)}}`$. From this, the CDF can be calculated from:\n```math\ng(x) = y^{(k)} + \\frac{\\left(y^{(k+1)} - y^{(k)}\\right)\\left[s^{(k)}\\xi^2+d^{(k)}\\xi\\left(1-\\xi\\right)\\right]}{s^{(k)}+\\left[d^{(k+1)}+d^{(k)}-2s^{(k)}\\right]\\xi\\left(1-\\xi\\right)}.\n```\n\n#### Futher documentation\nAdditional documentation can be found at: [doc](https://i-flow.gitlab.io/i-flow/)\n\n## Installation\n\nThe **i-flow** program can be installed using pip.\n\n```bash\npip install iflow\n```\n\n## Requirements\n\nIn order to run the **i-flow** program, there are a few additional packages that are required. The packages are:\n\n1. [TensorFlow](https://tensorflow.org) (version 2.0 or newer)\n2. [TensorFlow-Probability](https://www.tensorflow.org/probability) (version 0.8 or newer)\n3. [Numpy](https://numpy.org) \n\nIn order to run the tests provided in the paper and the [iflow_test.py](https://gitlab.com/i-flow/i-flow/-/blob/23be9317eaa0e404bf7f2932803bf7849787c0fb/iflow_test.py) file, a few additional packages\nare required. These are:\n\n1. [Vegas](https://vegas.readthedocs.io/en/latest/) (to compare to the vegas algorithm)\n1. [matplotlib](https://matplotlib.org) (for plotting the results)\n2. [absl-py](https://abseil.io/docs) (for command line parsing)\n\n## Usage \n\nRunning the script [iflow_test.py](https://gitlab.com/i-flow/i-flow/-/blob/23be9317eaa0e404bf7f2932803bf7849787c0fb/iflow_test.py) will produce the results of our paper. It also illustrates how to use **i-flow**.\n\n|     Line       | Explanation |\n| :------------: | :---------- |\n| `0 -- 20` | Import required packages, see [Requirements](#requirements). | \n| `21 -- 24` | Define shortcuts and set precision to float64. |\n| `25 -- 44` | Define command line flags to control program. |\n| `45 -- 325` | The class `TestFunctions` contains the functions to be integrated. This should be replaced by the desired function for integration. |\n| `326 -- 353` | `build()` defines the Neural Network that is used in each Coupling Layer. This can be adapted to the problem at hand, for example by adding more layers or making the layers wider. |\n| `354 -- 379` | `binary_masks()` creates the masking (i.e. which dimensions are transformed and which are passed through a coupling layer) based on the discussion of section III A of our paper. |\n| `380 -- 412` | `build_iflow()` builds the **i-flow** Integrator object. The most important parts are explained below. |\n| `391` | Defines which masking is used. Can be replaced by a custom list. |\n| `394 -- 397` | Defines the main bijector. `PiecewiseRationalQuadratic` can be replaced by any other coupling function defined in [couplings.py](iflow/integration/couplings.py). |\n| `408` | Defines which optimizer is used. |\n| `410` | Defines which loss function is used. `'exponential'` can be replaced by any other loss function defined in [divergences.py](iflow/integration/divergences.py). |\n| `414 -- 442` | `train_iflow()` trains **i-flow** for a given number of epochs. |\n| `443 -- 472` | `train_iflow_target()` trains **i-flow** until a target precision is reached. |\n| `473 -- 497` | `sample_iflow()` samples random numbers according to the trained distribution. Not used in the current setup. |\n| `498 -- 528` | Defines helper functions used in the presentation of the results. |\n| `529 -- 925` | Runs the setup defined with the command line flags. |\n\nRunning\n```bash\npython iflow_test.py -f=Gauss -d=2 -p=5000 -tp=1e-4 -t\n```\ngives the result of our first test case, apart from small fluctuations due to a different random seed. \n\n## References\n1. George Papamakarios, Eric Nalisnick,  Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan.<br>\n  Normalizing Flows for Probabilistic Modeling and Inference. arXiv:1912.02762 [link](https://arxiv.org/abs/1912.02762)  \n2. Laurent Dinh, David Krueger, and Yoshua Bengio. <br>\nNICE: non-linear independent components estimation. In *3rd International Conference on Learning Representations*, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings, 2015. arXiv:1410.8516 [link](http://arxiv.org/abs/1410.8516)  \n3. Thomas M&uuml;ller, Brian Mcwilliams, Fabrice Rousselle, Markus Gross, and Jan Nov&aacute;k.<br>\n  Neural Importance Sampling. *ACM Trans. Graph.,* 38(5), October 2019. [link](https://doi.org/10.1145/3341156)\n4. Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios.<br>\n  Neural Spline Flows. *Advances in Neural Information Processing Systems,* 7511--7522, 2019. [link](http://arxiv.org/abs/1906.04032)\n5. J. A. Gregory and R. Delbourgo.<br>\n  Piecewise Rational Quadratic Interpolation to Monotonic Data.</br>\n  *IMA Journal of Numerical Analysis*, 2(2):123-130, 04 1982. [link](https://doi.org/10.1093/imanum/2.2.123)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.com/isaacson/nicephasespace",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "iflow",
    "package_url": "https://pypi.org/project/iflow/",
    "platform": "",
    "project_url": "https://pypi.org/project/iflow/",
    "project_urls": {
      "Homepage": "https://gitlab.com/isaacson/nicephasespace"
    },
    "release_url": "https://pypi.org/project/iflow/1.0.1/",
    "requires_dist": [
      "numpy",
      "tensorflow (>=2.0)",
      "tensorflow-probability (>=0.8.0)",
      "pytest ; extra == 'test'",
      "coverage ; extra == 'test'",
      "pytest-cov ; extra == 'test'"
    ],
    "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, <4",
    "summary": "Monte-Carlo Integration using Neural Networks",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7980730,
  "releases": {
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "adb6975658a2f69f697f875dc5daa2796acf12fc074f1620ccfa565597da76b2",
          "md5": "dae8c4a04bed4359ff546459952513d4",
          "sha256": "ca3cc2897baaf8370a88cc826162f9c80738a86e3a86df941ef9049409c1e217"
        },
        "downloads": -1,
        "filename": "iflow-1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dae8c4a04bed4359ff546459952513d4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, <4",
        "size": 43610,
        "upload_time": "2020-08-12T19:30:39",
        "upload_time_iso_8601": "2020-08-12T19:30:39.562870Z",
        "url": "https://files.pythonhosted.org/packages/ad/b6/975658a2f69f697f875dc5daa2796acf12fc074f1620ccfa565597da76b2/iflow-1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "41b60c9df8999434cd6ad949c28cabfad9196a0b98c2b720945c1ed5950fde10",
          "md5": "5d104d7297173fe4568b86b1fc99b1fe",
          "sha256": "e1cc4927d2a47c33e58dcf61d44f5322be1fb5950fc247b5414bc55ed35d2b80"
        },
        "downloads": -1,
        "filename": "iflow-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "5d104d7297173fe4568b86b1fc99b1fe",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, <4",
        "size": 1128550,
        "upload_time": "2020-08-12T19:30:43",
        "upload_time_iso_8601": "2020-08-12T19:30:43.074802Z",
        "url": "https://files.pythonhosted.org/packages/41/b6/0c9df8999434cd6ad949c28cabfad9196a0b98c2b720945c1ed5950fde10/iflow-1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ed7905653a5a8ed60865a8e1e20b65d30964f5fe1454969fe9d54e4f94de9680",
          "md5": "4586a2de09ffb40c3417623c3363e805",
          "sha256": "204283f95329a7ccf73ad731158f67d2ba8f8e4533bc9d09c2e505aa9e93b2af"
        },
        "downloads": -1,
        "filename": "iflow-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4586a2de09ffb40c3417623c3363e805",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, <4",
        "size": 44521,
        "upload_time": "2020-08-17T16:52:39",
        "upload_time_iso_8601": "2020-08-17T16:52:39.299616Z",
        "url": "https://files.pythonhosted.org/packages/ed/79/05653a5a8ed60865a8e1e20b65d30964f5fe1454969fe9d54e4f94de9680/iflow-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ed7905653a5a8ed60865a8e1e20b65d30964f5fe1454969fe9d54e4f94de9680",
        "md5": "4586a2de09ffb40c3417623c3363e805",
        "sha256": "204283f95329a7ccf73ad731158f67d2ba8f8e4533bc9d09c2e505aa9e93b2af"
      },
      "downloads": -1,
      "filename": "iflow-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4586a2de09ffb40c3417623c3363e805",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, <4",
      "size": 44521,
      "upload_time": "2020-08-17T16:52:39",
      "upload_time_iso_8601": "2020-08-17T16:52:39.299616Z",
      "url": "https://files.pythonhosted.org/packages/ed/79/05653a5a8ed60865a8e1e20b65d30964f5fe1454969fe9d54e4f94de9680/iflow-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}