{
  "info": {
    "author": "Data and Analytics Research",
    "author_email": "analytics.dar@take.net",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# TakeBlipSentimentAnalysis\n_Data & Analytics Research_\n\n## Overview\nSentiment analysis is the process of detecting the sentiment of a sentence, the \nsentiment could be negative, positive or neutral.\n\nThis implementation uses a LSTM implementation to solve the task. The \nimplementation is using PyTorch framework and Gensim FastText as input embedding.\n\nTo train the model it is necessary a csv file with the labeled dataset, and an \nembedding file. For prediction, the files needed are the embedding model, the \ntrained model and the vocabulary of the labels (output of the train).\n\nThis implementation presents the possibility to predict the sentiment for a single\nsentence and for a batch of sentences (by file or dictionary).\n\nThe LSTM architecture utilized in this implementation has 4 layers:\n\n1) Embedding layer: a layer with the embedding representation of each word.\n\n2) LSTM layer: receives as input the embedding representation of each word in \na sentence. For each word generate a output with size pre-defined.\n\n3) The linear output layer: receives as input the last word hidden output of \nthe LSTM and applies a linear function to get a vector of the size of the \npossible labels.\n\n4) Softmax layer: receives the output of the linear layer and apply softmax\n    operation to get the probability of each label.\n\nFor the bidirectional LSTM the linear output layer receives the hidden\noutput from the first and the last word.\n\n## Training \nTo train your own Sentiment Analysis model you will need a csv file with the \nfollowing structure:\n\n\tMessage\t\t                              Sentiment\n    achei pessimo o atendimento\t              Negative\n    otimo trabalho\t\t                      Positive\n    bom dia                                       Neutral\n    ...,\t\t\t\t\t\t...\n\nA few steps should be followed to train the model.\n\n1) Import the main functions\n2) Set the variables\n3) Generate the vocabularies\n4) Initialize the model\n5) Training\n\nAn example with the steps\n### Import main functions\n\n```\nimport torch\nimport os\nimport pickle\n\nfrom TakeSentimentAnalysis import model, utils\nfrom TakeSentimentAnalysis.train import LSTMTrainer\n```\n\n### Set the variables\n\n**File variables**\n```\ninput_path = '*.csv'\nsentence_column = 'Message'\nlabel_column = 'Sentiment'\nencoding = 'utf-8'\nseparator = '|'\nuse_pre_processing = True\nsave_dir = 'path_to_save_folder'\nwordembed_path = '*.kv'\n```\n\nThe file variables are:\n* input-path: Path to input file that contains sequences of sentences.\n* sentence_column: String with the name of the column of sentences to be read \nfrom input file.\n* label_column: String with the name of the column of labels to be read from \ninput file.\n* encoding: Input file encoding.\n* separator: Input file column separator.\n* use_pre_processing: Whether to pre process input data\n* save-dir: Directory to save outputs.\n* wordembed-path: Path to pre-trained word embeddings.\n\n**Validation variables**\n```\nval = True\nval-path = '*.csv'\nval-period = 1\n```\n\n* val: Whether to perform validation.\n* val-path: Validation file path. Must follow the same structure of the input \nfile.\n* val-period: Period to wait until a validation is performed.\n\n**Model variables**\n```\nword-dim = 300\nlstm-dim = 300\nlstm-layers = 1\ndropout-prob = 0.05\nbidirectional = False\nepochs = 5\nbatch-size = 32\nshuffle = False\nlearning-rate = 0.001\nlearning-rate-decay = 0.1\nmax-patience = 2\nmax-decay-num = 2\npatience-threshold = 0.98\n```\n\n* word-dim: Dimension of word embeddings.\n* lstm-dim: Dimensions of lstm cells. This determines the hidden state and cell\nstate sizes.\n* lstm-layers: Number of layers of the lstm cells.\n* dropout-prob: Probability in dropout layers.\n* bidirectional: Whether lstm cells are bidirectional. \n* epochs: Number of training epochs.\n* batch-size: Mini-batch size to train the model.\n* shuffle: Whether to shuffle the dataset.\n* learning-rate: Learning rate to train the model.\n* learning-rate-decay: Learning rate decay after the model not improve.\n* max-patience: Number maximum of epochs accept with decreasing loss in \nvalidation, before reduce the learning rate.\n* max-decay-num: Number maximum of times that the learning can be reduced.\n* patience-threshold: Threshold of the loss in validation to be considered \nthat the model didn't improve.\n\n### Generate the vocabularies \n\nGenerate the sentences vocabulary. This steps is necessary to generate a index \nto each word in the sentences (on train and validation datasets) to retrieve \ninformation after PyTorch operations. \n\n```\n    pad_string = '<pad>'\n    unk_string = '<unk>'\n    sentence_vocab = vocab.create_vocabulary(\n        input_path=input_path,\n        column_name=sentence_column,\n        pad_string=pad_string,\n        unk_string=unk_string,\n        encoding=encoding,\n        separator=separator,\n        use_pre_processing=use_pre_processing)\n\n    if val:\n        sentences = vocab.read_sentences(\n            path=val_path,\n            column=sentence_column,\n            encoding=encoding,\n            separator=separator,\n            use_pre_processing=use_pre_processing)\n        vocab.populate_vocab(sentences, sentence_vocab)\n```\n\nGenerating the labels vocabulary. To generate a index of each label, this \nobject is necessary to predict, so must be saved.\n\n```\n    label_vocab = vocab.create_vocabulary(\n        input_path=input_path,\n        column_name=label_column,\n        pad_string=pad_string,\n        unk_string=unk_string,\n        encoding=encoding,\n        separator=separator,\n        is_label=True)\n    vocab_label_path = os.path.join(save_dir, \n                                    'vocab-label.pkl')\n    pickle.dump(label_vocab, open(vocab_label_path, 'wb'))\n```\n\n### Initialize the model\n\nInitialize the LSTM model.\n\n```\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    lstm_model = model.LSTM(\n        vocab_size=len(sentence_vocab),\n        word_dim=word_dim,\n        n_labels=len(label_vocab),\n        hidden_dim=lstm_dim,\n        layers=lstm_layers,\n        dropout_prob=dropout_prob,\n        device=device,\n        bidirectional=bidirectional\n    ).to(device)\n\n    lstm_model.reset_parameters()\n\n```\n\nFill the embedding layer with the representation of each word in the vocabulary.\n```\n    wordembed_path = wordembed_path\n    fasttext = utils.load_fasttext_embeddings(wordembed_path, pad_string)\n    lstm_model.embeddings[0].weight.data = torch.from_numpy(\n        fasttext[sentence_vocab.i2f.values()])\n    lstm_model.embeddings[0].weight.requires_grad = False\n```\n\n### Training \n```\n    trainer = LSTMTrainer(\n        lstm_model=lstm_model,\n        epochs=epochs,\n        input_vocab=sentence_vocab,\n        input_path=input_path,\n        label_vocab=label_vocab,\n        save_dir=save_dir,\n        val=val,\n        val_period=val_period,\n        pad_string=pad_string,\n        unk_string=unk_string,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        label_column=label_column,\n        encoding=encoding,\n        separator=separator,\n        use_pre_processing=use_pre_processing,\n        learning_rate=learning_rate,\n        learning_rate_decay=learning_rate_decay,\n        max_patience=max_patience,\n        max_decay_num=max_decay_num,\n        patience_threshold=patience_threshold,\n        val_path=val_path)\n    trainer.train()\n\n```\n\n## Prediction\nThe prediction can be made for a single sentence or for a batch of sentences.\n\nIn both cases a few steps should be followed.\n\n1) Import the main functions\n2) Set the variables\n3) Initialize the model\n4) Predict\n\n### Import main functions\n```\nimport sys\nimport os\nimport torch\n\nfrom TakeSentimentAnalysis import utils\nfrom TakeSentimentAnalysis.predict import SentimentPredict\n```\n\n### Set the variables\n\n```\nmodel_path = '*.pkl'\nlabel_vocab = '*.pkl'\nsave_dir = '*.csv'\nencoding = 'utf-8'\nseparator = '|'\n```\n\n* model_path: Path to trained model.\n* label_vocab: Path to input file that contains the label vocab.\n* save_dir: Directory to save predict.\n* encoding: Input file encoding.\n* separator: Input file column separator.\n\n### Initialize the model\n```\nsys.path.insert(0, os.path.dirname(model_path))\nlstm_model = torch.load(model_path)\n\npad_string = '<pad>'\nunk_string = '<unk>'\n\nembedding = utils.load_fasttext_embeddings(wordembed_path, \n                                           pad_string)\n\nSentimentPredicter = SentimentPredict(model=lstm_model,\n                                      label_path=label_vocab,\n                                      embedding=embedding,\n                                      save_dir=save_dir,\n                                      encoding=encoding,\n                                      separator=separator)    \n```\n\n### Single Prediction\nTo predict a single sentence\n\n```\nSentimentPredicter.predict_line(line=sentence)\n```\n\n### Batch Prediction\nTo predict in a batch a few more variables are need:\n\n* batch_size: Mini-batch size.\n* shuffle: Whether to shuffle the dataset.\n* use_pre_processing: Whether to pre-processing the input data.\n\nTo predict a batch using dictionary:\n````\nSentimentPredicter.predict_batch(\n        filepath='',\n        sentence_column='',\n        pad_string=pad_string,\n        unk_string=unk_string,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        use_pre_processing=use_pre_processing,\n        sentences=[{'id': 1, 'sentence': sentence_1},\n                   {'id': 2, 'sentence': sentence_2}]))\n````\n\nTo predict a batch using a csv file:\n```\nSentimentPredicter.predict_batch(\n            filepath=input_path,\n            sentence_column=sentence_column,\n            pad_string=pad_string,\n            unk_string=unk_string,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            use_pre_processing=use_pre_processing)\n```\n\n* input_path: Path to the input file containing the sentences to be predicted.\n*  sentence_column: String with the name of the column of sentences to be \nread from input file.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "sentiment analysis",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "TakeBlipSentimentAnalysis",
    "package_url": "https://pypi.org/project/TakeBlipSentimentAnalysis/",
    "platform": null,
    "project_url": "https://pypi.org/project/TakeBlipSentimentAnalysis/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/TakeBlipSentimentAnalysis/0.0.1b0/",
    "requires_dist": [
      "torch (==1.9.0)",
      "gensim (==3.8.3)",
      "take-text-preprocess (==0.0.5)",
      "numpy (==1.19.1)"
    ],
    "requires_python": "",
    "summary": "Sentiment Analysis Package",
    "version": "0.0.1b0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13721415,
  "releases": {
    "0.0.1b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "78227f4c7cf960bc580ef38dea3771201c9c9e193bc0bee43425d52937b5e075",
          "md5": "ac3f80f4eba28b0e3f0c422cf51797d8",
          "sha256": "a1d21d12610d52261e4db5027e10ffa8b6ebb1360dd3ef553e2fd32a7245ffb1"
        },
        "downloads": -1,
        "filename": "TakeBlipSentimentAnalysis-0.0.1b0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ac3f80f4eba28b0e3f0c422cf51797d8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21665,
        "upload_time": "2022-05-05T13:51:13",
        "upload_time_iso_8601": "2022-05-05T13:51:13.705953Z",
        "url": "https://files.pythonhosted.org/packages/78/22/7f4c7cf960bc580ef38dea3771201c9c9e193bc0bee43425d52937b5e075/TakeBlipSentimentAnalysis-0.0.1b0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e82a6c2279f69d29d835049b5319cc247ae08ee30f2f02e12478a136f41950c3",
          "md5": "e3ab005f54545d62522c2611e27d671f",
          "sha256": "2b1c508bd16891b13f537a0cf02e5d25ece421faea4bf68f8434c2debf3a55d7"
        },
        "downloads": -1,
        "filename": "TakeBlipSentimentAnalysis-0.0.1b0.tar.gz",
        "has_sig": false,
        "md5_digest": "e3ab005f54545d62522c2611e27d671f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18625,
        "upload_time": "2022-05-05T13:51:15",
        "upload_time_iso_8601": "2022-05-05T13:51:15.598822Z",
        "url": "https://files.pythonhosted.org/packages/e8/2a/6c2279f69d29d835049b5319cc247ae08ee30f2f02e12478a136f41950c3/TakeBlipSentimentAnalysis-0.0.1b0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "78227f4c7cf960bc580ef38dea3771201c9c9e193bc0bee43425d52937b5e075",
        "md5": "ac3f80f4eba28b0e3f0c422cf51797d8",
        "sha256": "a1d21d12610d52261e4db5027e10ffa8b6ebb1360dd3ef553e2fd32a7245ffb1"
      },
      "downloads": -1,
      "filename": "TakeBlipSentimentAnalysis-0.0.1b0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ac3f80f4eba28b0e3f0c422cf51797d8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 21665,
      "upload_time": "2022-05-05T13:51:13",
      "upload_time_iso_8601": "2022-05-05T13:51:13.705953Z",
      "url": "https://files.pythonhosted.org/packages/78/22/7f4c7cf960bc580ef38dea3771201c9c9e193bc0bee43425d52937b5e075/TakeBlipSentimentAnalysis-0.0.1b0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e82a6c2279f69d29d835049b5319cc247ae08ee30f2f02e12478a136f41950c3",
        "md5": "e3ab005f54545d62522c2611e27d671f",
        "sha256": "2b1c508bd16891b13f537a0cf02e5d25ece421faea4bf68f8434c2debf3a55d7"
      },
      "downloads": -1,
      "filename": "TakeBlipSentimentAnalysis-0.0.1b0.tar.gz",
      "has_sig": false,
      "md5_digest": "e3ab005f54545d62522c2611e27d671f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 18625,
      "upload_time": "2022-05-05T13:51:15",
      "upload_time_iso_8601": "2022-05-05T13:51:15.598822Z",
      "url": "https://files.pythonhosted.org/packages/e8/2a/6c2279f69d29d835049b5319cc247ae08ee30f2f02e12478a136f41950c3/TakeBlipSentimentAnalysis-0.0.1b0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}