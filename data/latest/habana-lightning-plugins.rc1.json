{
  "info": {
    "author": "Habana Labs Ltd., an Intel Company",
    "author_email": "support@habana.ai",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: Other/Proprietary License",
      "Operating System :: Unix",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Scientific/Engineering :: Mathematics",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "\n<h1 style=\"text-align: center;\">Habana Lightning Plugins</h1>\n\nHabana Lightning plugins is a suite of plugins that aid/accelerate model training using Lightning framework for HPU.\nThe plugins acts as an extension to the lightning framework to support HPU specific features.\n\nCurrently, the following plugins are available:\n\n* HPUDataModule\n* HPUProfiler\n\n## Installation\n\nTo install Habana lightning plugins run the following command:\n```bash\npython -um pip install habana-lightning-plugins\n```\n\n## HPUDataModule\n\n``HPUDataModule`` is an extension to the ``LightningDataModule`` class which uses Habana's dataloader to load and pre-process the input data.\nUsing HPUDataModule offloads the data preprocessing overhead to the HPU and in turn increases the performance of training. The wrapper also\naids in switching between hardware and software preprocessor based on the specific Gaudi device used.\n\nVisit [Habana Dataloader](https://docs.habana.ai/en/latest/PyTorch/Habana_Media_Loader_PT/Media_Loader_PT.html) for more information related to Habana Dataloader.\n\n### Usage\nThe following shows an example of how to use the ``HPUDataModule``:\n\n 1. Import Habana Datamodule:\n\n```python\n    from habana_lightning_plugins.datamodule import HPUDataModule\n```\n\n 2. Create and initialize HPUDataModule object with the dataset and the configuration required to preprocess the data:\n\n```python\n    train_dir = \"./path/to/train/data\"\n    val_dir = \"./path/to/val/data\"\n\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    train_transforms = [\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize,\n    ]\n    val_transforms = [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        normalize,\n    ]\n\n    data_module = HPUDataModule(\n        train_dir,\n        val_dir,\n        train_transforms=train_transforms,\n        val_transforms=val_transforms,\n        num_workers=8,\n        batch_size=32,\n        shuffle=False,\n        pin_memory=True,\n        drop_last=True,\n    )\n```\n\n3. Create an object of Lightning trainer and model:\n```python\n    trainer = pl.Trainer(devices=1, accelerator=\"hpu\", max_epochs=1, max_steps=2)\n    model = RN50Module()  # Or any other model to be defined by user\n```\n\n4. Pass the datamodule object as an argument to trainer to execute train/val/test loops:\n```python\n    trainer.fit(model, datamodule=data_module)\n    trainer.validate(model, datamodule=data_module)\n```\n### Examples\n - A sample script can be found at ``examples/hpu_datamodule_sample.py``.\n ```python\n python examples/hpu_datamodule_sample.py --data-path <path to Imagenet dataset - ILSVRC2012>\n```\nA reference model using HPUDataModule can be found in the [ResNet50 Model Reference](https://github.com/HabanaAI/Model-References/tree/1.8.0/PyTorch/computer_vision/classification)\n\n\n\n### Limitations\n \n - HPUDataModule supports the ``Imagenet`` dataset only.\n - HPUDataModule supports only 8 parallel data loader workers.\n\n\n## HPUProfiler\n\nHPUProfiler is a lightning implementation of PyTorch profiler for HPU devices. It aids in obtaining profiling summary of PyTorch functions. \nIt subclasses PyTorch Lightning's [PyTorch profiler](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.profilers.PyTorchProfiler.html#pytorch_lightning.profilers.PyTorchProfiler).\n\n\n\n### Default Profiling\nFor auto profiling, create a HPUProfiler instance and pass it to trainer.\nAt the end of ``profiler.fit()``, it will generate a json trace for the run.\nIn case ``accelerator = \"hpu\"`` is not used with HPUProfiler, then it will dump only CPU traces, similar to PyTorchProfiler.\n\n``` python\n# Import profiler\nfrom habana_lightning_plugins.profiler import HPUProfiler\n\n# Create profiler object\nprofiler = HPUProfiler()\naccelerator = \"hpu\"\n\n# Pass profiler to the trainer\n    trainer = Trainer(\n        profiler=profiler,\n        accelerator=accelerator,\n    )\n```\n\n### Distributed Profiling\n\nTo profile a distributed model, use the HPUProfiler with the filename argument which will save a report per rank:\n\n``` python\nfrom habana_lightning_plugins.profiler import HPUProfiler\n\nprofiler = HPUProfiler(filename=\"perf-logs\")\ntrainer = Trainer(profiler=profiler, accelerator=\"hpu\")\n\n```\n### Custom Profiling\nTo [profile custom actions of interest](https://pytorch-lightning.readthedocs.io/en/stable/tuning/profiler_expert.html#profile-custom-actions-of-interest), reference a profiler in the LightningModule:\n\n``` python\nfrom habana_lightning_plugins.profiler import HPUProfiler\n\n# Reference profiler in LightningModule\nclass MyModel(LightningModule):\n    def __init__(self, profiler=None):\n        self.profiler = profiler\n\n# To profile in any part of your code, use the self.profiler.profile() function\n    def custom_processing_step_basic(self, data):\n        with self.profiler.profile(\"my_custom_action\"):\n            ...\n        return data\n\n# Alternatively, use self.profiler.start(\"my_custom_action\")\n# and self.profiler.stop(\"my_custom_action\") functions\n# to enclose the part of code to be profiled.\n    def custom_processing_step_granular(self, data):\n        self.profiler.start(\"my_custom_action\") \n            ...\n        self.profiler.stop(\"my_custom_action\")\n        return data\n\n# Pass profiler instance to LightningModule\nprofiler = HPUProfiler()\nmodel = MyModel(profiler)\ntrainer = Trainer(profiler=profiler, accelerator=\"hpu\")\n```\nFor more details on profiler, refer to [PyTorchProfiler](https://pytorch-lightning.readthedocs.io/en/stable/tuning/profiler_intermediate.html)\n\n### Visualize Profiled Operations\nProfiler will dump traces in json format. The traces can be\nvisualized in 2 ways:\n\n#### Using PyTorch TensorBoard Profiler \n\nFor further instructions see, https://github.com/pytorch/kineto/tree/master/tb_plugin.\n\n``` python\n# Install tensorbaord\npython -um pip install tensorboard torch-tb-profiler\n\n# Start the TensorBoard server (default at port 6006):\ntensorboard --logdir ./tensorboard --port 6006\n\n# Now open the following url on your browser\nhttp://localhost:6006/#profile\n```\n\n#### Using Chrome\n\n    1. Open Chrome and copy/paste this URL: `chrome://tracing/`.\n    2. Once tracing opens, click on `Load` at the top-right and load one of the generated traces.\n\n### Limitations\n\n- When using the HPUProfiler, wall clock time will not be representative of the true wall clock time. This is due to forcing profiled operations to be measured synchronously, when many HPU ops happen asynchronously. It is recommended to use this Profiler to find bottlenecks/breakdowns, however for end to end wall clock time use the SimpleProfiler.\n\n- HPUProfiler.summary() is not supported\n\n- Passing profiler name as string \"hpu\" to the trainer is not supported.\n\n## Contributing\n\nPull requests are welcome. For major changes, please open an issue first\nto discuss what you would like to change.\n\nPlease make sure to update tests as appropriate.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://habana.ai/",
    "keywords": "",
    "license": "See LICENSE file",
    "maintainer": "",
    "maintainer_email": "",
    "name": "habana-lightning-plugins",
    "package_url": "https://pypi.org/project/habana-lightning-plugins/",
    "platform": null,
    "project_url": "https://pypi.org/project/habana-lightning-plugins/",
    "project_urls": {
      "Homepage": "https://habana.ai/"
    },
    "release_url": "https://pypi.org/project/habana-lightning-plugins/1.9.0.580.1/",
    "requires_dist": [
      "pytorch-lightning",
      "torch",
      "torchvision",
      "habana-torch-dataloader",
      "habana-torch-plugin"
    ],
    "requires_python": "",
    "summary": "Habana's lightning-specific optimized plugins",
    "version": "1.9.0.580.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17474066,
  "releases": {
    "1.9.0.580.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6215068f09ba6a8be365c233c65a2bb81e8cf4432aca08e3d7f31754947308a8",
          "md5": "517a3157e9089ad06cf38148a6c21bf8",
          "sha256": "897454e2f3aa6a4aa1065735471283442a9cf21ae9cadd434e6129d8d6b5ebd4"
        },
        "downloads": -1,
        "filename": "habana_lightning_plugins-1.9.0.580.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "517a3157e9089ad06cf38148a6c21bf8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 17288,
        "upload_time": "2023-03-28T06:57:35",
        "upload_time_iso_8601": "2023-03-28T06:57:35.478330Z",
        "url": "https://files.pythonhosted.org/packages/62/15/068f09ba6a8be365c233c65a2bb81e8cf4432aca08e3d7f31754947308a8/habana_lightning_plugins-1.9.0.580.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6215068f09ba6a8be365c233c65a2bb81e8cf4432aca08e3d7f31754947308a8",
        "md5": "517a3157e9089ad06cf38148a6c21bf8",
        "sha256": "897454e2f3aa6a4aa1065735471283442a9cf21ae9cadd434e6129d8d6b5ebd4"
      },
      "downloads": -1,
      "filename": "habana_lightning_plugins-1.9.0.580.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "517a3157e9089ad06cf38148a6c21bf8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 17288,
      "upload_time": "2023-03-28T06:57:35",
      "upload_time_iso_8601": "2023-03-28T06:57:35.478330Z",
      "url": "https://files.pythonhosted.org/packages/62/15/068f09ba6a8be365c233c65a2bb81e8cf4432aca08e3d7f31754947308a8/habana_lightning_plugins-1.9.0.580.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}