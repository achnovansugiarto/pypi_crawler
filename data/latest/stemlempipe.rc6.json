{
  "info": {
    "author": "Demetry Pascal",
    "author_email": "qtckpuhdsa@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "[![PyPI\nversion](https://badge.fury.io/py/StemLemPipe.svg)](https://pypi.org/project/StemLemPipe/)\n[![GitHub issues](https://img.shields.io/github/issues/Naereen/StrapDown.js.svg)](https://github.com/PasaOpasen/Stem-Lem-Pipeline/issues) \n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/PasaOpasen/Stem-Lem-Pipeline/pulls)\n\n[![Downloads](https://pepy.tech/badge/StemLemPipe)](https://pepy.tech/project/StemLemPipe)\n[![Downloads](https://pepy.tech/badge/StemLemPipe/month)](https://pepy.tech/project/StemLemPipe)\n[![Downloads](https://pepy.tech/badge/StemLemPipe/week)](https://pepy.tech/project/StemLemPipe)\n\n# Stemmer-lemmatizer pipeline\n\nRussian-based (but not only) text transformer used several stemming and lemmatization backends and text preparation features\n\n```\npip install StemLemPipe\n```\n\n- [Stemmer-lemmatizer pipeline](#stemmer-lemmatizer-pipeline)\n  - [The purpose of this package](#the-purpose-of-this-package)\n  - [Small example](#small-example)\n  - [Big example](#big-example)\n  - [Steps description](#steps-description)\n    - [Splitting text to sentences](#splitting-text-to-sentences)\n    - [Text preparations like removing some symbols and setting lower case](#text-preparations-like-removing-some-symbols-and-setting-lower-case)\n    - [Splitting each sentence by some symbols and stop words](#splitting-each-sentence-by-some-symbols-and-stop-words)\n    - [Lemmatization and/or stemming](#lemmatization-andor-stemming)\n    - [N-grams](#n-grams)\n    - [Union to set](#union-to-set)\n  - [Pipeline](#pipeline)\n  - [Another tools](#another-tools)\n    - [Metrics](#metrics)\n    - [Little functions](#little-functions)\n\n\n## The purpose of this package\n\nThe main purpose of `StemLemPipe` is the conversion texts to python sets of useful strings which are 1-, 2-, 3-, ..., n-grams of lemmatized and/or stemmed words of these texts splitted by:\n* sentences\n* parts of sentences\n* stop words\n\nHave a look to the concept of `StemLemPipe` pipeline:\n\n![](images/diagram.png)\n\n## Small example\n```python\nfrom StemLemPipe import phrases2lower, phrases_without_excess_symbols, phrases_transform, text2sentences, split_by_words, sentence_split, create_stemmer_lemmer, words_to_ngrams_list, sum_phrases, wordlist2set, stopwords, StemLemPipeline\n\n\ntext_example = \"\"\"Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\"\"\"\n\n#text_example = \"This is some example # @ noised text. It shows all transformations, but it's small because of consequences\"\n\ndef print2(obj):\n    print(obj)\n    print()\n\n\n# create stemmer-lemmatizer pipeline function\nstem_lem = create_stemmer_lemmer(lemmatizer_backend='wordnet', stemmer_backend='snowball', language = 'en')\n\n\n# convert all text to list of sentences\nsentences = text2sentences(text_example)\nprint2(sentences)\n\n# ['Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item', 'Lemmatization is similar to stemming but it brings context to the words', 'So it links words with similar meaning to one word']\n\n\n# transform each phrase to lower case\nclean_sentences = phrases2lower(sentences)\nprint2(clean_sentences)\n# ['lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item', 'lemmatization is similar to stemming but it brings context to the words', 'so it links words with similar meaning to one word']\n\n# split each sentence to list of phrases between separators and stop words\nphrases = [sentence_split(sentence , separators=',;', stop_words = stopwords('en')) for sentence in clean_sentences]\nprint2(phrases)\n# [['lemmatization', 'process', 'grouping together', 'different inflected forms', 'word', 'can', 'analysed', 'single item'], ['lemmatization', 'similar', 'stemming', 'brings context', 'words'], ['links words', 'similar meaning', 'one word']]\n\n\n# remove excess symbols from phrases\nchar_phrases = phrases_without_excess_symbols(phrases, include_alpha= True, include_numbers= True)\nprint2(char_phrases)\n# [['lemmatization', 'process', 'grouping together', 'different inflected forms', 'word', 'can', 'analysed', 'single item'], ['lemmatization', 'similar', 'stemming', 'brings context', 'words'], ['links words', 'similar meaning', 'one word']]\n\n# stem and lemmatize all words in all phrases\nstemmed_phrases = phrases_transform(char_phrases, func = stem_lem)\nprint2(stemmed_phrases)\n# [['lemmatizatio', 'proce', 'group togeth', 'differ inflect form', 'wor', 'ca', 'analys', 'singl ite'], ['lemmatizatio', 'simila', 'stemmin', 'bring contex', 'wor'], ['link word', 'similar meanin', 'one wor']]\n\n# convert each phrase to list of n-grams\nn_grams = phrases_transform(stemmed_phrases, func = lambda w: words_to_ngrams_list(w.split(), n_min = 1, n_max = 2))\nprint2(n_grams)\n# [[['lemmatizatio'], ['proce'], ['group', 'togeth', 'group togeth'], ['differ', 'inflect', 'form', 'differ inflect', 'inflect form'], ['wor'], ['ca'], ['analys'], ['singl', 'ite', 'singl ite']], [['lemmatizatio'], ['simila'], ['stemmin'], ['bring', 'contex', 'bring contex'], ['wor']], [['link', 'word', 'link word'], ['similar', 'meanin', 'similar meanin'], ['one', 'wor', 'one wor']]]\n\n# convert list of list of list to just list\ntotal = sum_phrases(n_grams)\nprint2(total)\n# ['lemmatizatio', 'proce', 'group', 'togeth', 'group togeth', 'differ', 'inflect', 'form', 'differ inflect', 'inflect form', 'wor', 'ca', 'analys', 'singl', 'ite', 'singl ite', 'lemmatizatio', 'simila', 'stemmin', 'bring', 'contex', 'bring contex', 'wor', 'link', 'word', 'link word', 'similar', 'meanin', 'similar meanin', 'one', 'wor', 'one wor']\n\n# convert all objects to set\ntotal_set = wordlist2set(total, save_order=False)\nprint2(total_set)\n# {'wor', 'ite singl', 'group', 'stemmin', 'meanin similar', 'form inflect', 'differ inflect', 'lemmatizatio', 'analys', 'one', 'ite', 'group togeth', 'ca', 'word', 'meanin', 'singl', 'inflect', 'similar', 'form', 'bring', 'contex', 'link', 'bring contex', 'link word', 'togeth', 'one wor', 'differ', 'proce', 'simila'}\n\n\n# all these steps are equal to pipeline\n\npipe = StemLemPipeline([\n\n    text2sentences,phrases2lower,\n    lambda sentences: list(map(lambda s: sentence_split(s , separators=',;', stop_words = stopwords('en')), sentences)),\n\n    lambda p: phrases_without_excess_symbols(p, include_alpha= True, include_numbers= True),\n    lambda p: phrases_transform(p, stem_lem),\n    lambda p: phrases_transform(p, func = lambda w: words_to_ngrams_list(w.split(), n_min = 1, n_max = 2)),\n    sum_phrases,\n    lambda p: wordlist2set(p, save_order=False)\n])\n\npipe(text_example)\n# {'wor', 'ite singl', 'group', 'stemmin', 'meanin similar', 'form inflect', 'differ inflect', 'lemmatizatio', 'analys', 'one', 'ite', 'group togeth', 'ca', 'word', 'meanin', 'singl', 'inflect', 'similar', 'form', 'bring', 'contex', 'link', 'bring contex', 'link word', 'togeth', 'one wor', 'differ', 'proce', 'simila'}\n\nprint(total_set == pipe(text_example)) # true\n```\n\n## Big example\n\n```python\nfrom StemLemPipe import phrases2lower, phrases_without_excess_symbols, phrases_transform, text2sentences, split_by_words, sentence_split, create_stemmer_lemmer, words_to_ngrams_list, sum_phrases, wordlist2set, stopwords, StemLemPipeline\n\n\ntext_example = \"\"\"Если в жопе шило, я могу достать.\nЯ имею опыт, лучше \"да\" сказать.\nВлагалище твое в тонус приведу.\nПиши адрес, я приду.\n\nМой член, словно банан, кривой\nИ в матку вплоть до дна зайдет,\nИ полчаса стоять смогет,\nКак на границе часовой.\n\nТы будешь просто разрываться,\nКогда начнем с тобой ебаться,\nИ в счастье будешь же казниться,\nЧто раньше не решилась соблазниться.\n\nЯ медленно сниму штаны с тебя,\nИ поцелуями сломлю твое сомненье,\nОт похоти войдешь ты в опьяненье,\nПока не кончишь от меня.\n\nЯ смазку принесу с собой,\nСначала ты почувствуешь прохладу,\nНо буду жарить я своей большой трубой,\nПока не получу усладу.\n\nТы напиши, и я приду,\nЕсли не буду очень занят,\nПощекочу твою пизду\nСнутри и долго, ибо не устанет.\n\nЯ покусаю твою попу\nИ буду долго трахать в жопу,\nПока ментов не вызовут на вой\nТой девки, что кайфует подо мной.\n\nПотом сниму презерватив,\nМирамистином всё полью,\nМинут пятнадцать перерыв —\nИ снова я на час в бою.\n\nИ шея будет вся в укусах и засосах,\nИ будет всё болеть, включая твою грудь,\nНо через пару дней, тоскуя без вопросов,\nЗахочешь эту ночь вернуть.\"\"\"\n\ndef print2(obj):\n    print(obj)\n    print()\n\n\n# create stemmer-lemmatizer pipeline function\nstem_lem = create_stemmer_lemmer(lemmatizer_backend='pymorphy', stemmer_backend='snowball')\n\n# convert all text to list of sentences\nsentences = text2sentences(text_example)\nprint2(sentences)\n# ['Если в жопе шило, я могу достать', 'Я имею опыт, лучше \"да\" сказать', 'Влагалище твое в тонус приведу', 'Пиши адрес, я приду', 'Мой член, словно банан, кривой И в матку вплоть до дна зайдет, И полчаса стоять смогет, Как на границе часовой', 'Ты будешь просто разрываться, Когда начнем с тобой ебаться, И в счастье будешь же казниться, Что раньше не решилась соблазниться', 'Я медленно сниму штаны с тебя, И поцелуями сломлю твое сомненье, От похоти войдешь ты в опьяненье, Пока не кончишь от меня', 'Я смазку принесу с собой, Сначала ты почувствуешь прохладу, Но буду жарить я своей большой трубой, Пока не получу усладу', 'Ты напиши, и я приду, Если не буду очень занят, Пощекочу твою пизду Снутри и долго, ибо не устанет', 'Я покусаю твою попу И буду долго трахать в жопу, Пока ментов не вызовут на вой Той девки, что кайфует подо мной', 'Потом сниму презерватив, Мирамистином всё полью, Минут пятнадцать перерыв — И снова я на час в бою', 'И шея будет вся в укусах и засосах, И будет всё болеть, включая твою грудь, Но через пару дней, тоскуя без вопросов, Захочешь эту ночь вернуть']\n\n# transform each phrase to lower case\nclean_sentences = phrases2lower(sentences)\nprint2(clean_sentences)\n# ['если в жопе шило, я могу достать', 'я имею опыт, лучше \"да\" сказать', 'влагалище твое в тонус приведу', 'пиши адрес, я приду', 'мой член, словно банан, кривой и в матку вплоть до дна зайдет, и полчаса стоять смогет, как на границе часовой', 'ты будешь просто разрываться, когда начнем с тобой ебаться, и в счастье будешь же казниться, что раньше не решилась соблазниться', 'я медленно сниму штаны с тебя, и поцелуями сломлю твое сомненье, от похоти войдешь ты в опьяненье, пока не кончишь от меня', 'я смазку принесу с собой, сначала ты почувствуешь прохладу, но буду жарить я своей большой трубой, пока не получу усладу', 'ты напиши, и я приду, если не буду очень занят, пощекочу твою пизду снутри и долго, ибо не устанет', 'я покусаю твою попу и буду долго трахать в жопу, пока ментов не вызовут на вой той девки, что кайфует подо мной', 'потом сниму презерватив, мирамистином всё полью, минут пятнадцать перерыв — и снова я на час в бою', 'и шея будет вся в укусах и засосах, и будет всё болеть, включая твою грудь, но через пару дней, тоскуя без вопросов, захочешь эту ночь вернуть']\n\n# split each sentence to list of phrases between separators and stop words\nphrases = [sentence_split(sentence , separators=',;', stop_words = stopwords('ru')) for sentence in clean_sentences]\nprint2(phrases)\n# [['жопе шило', 'могу достать'], ['имею опыт', '\"да\"'], ['влагалище твое', 'тонус приведу'], ['пиши адрес', 'приду'], ['член', 'словно банан', 'кривой', 'матку вплоть', 'дна зайдет', 'полчаса стоять смогет', 'границе часовой'], ['разрываться', 'начнем', 'ебаться', 'счастье', 'казниться', 'решилась соблазниться'], ['медленно сниму штаны', 'поцелуями сломлю твое сомненье', 'похоти войдешь', 'опьяненье', 'кончишь'], ['смазку принесу', 'почувствуешь прохладу', 'жарить', 'большой трубой', 'получу усладу'], ['напиши', 'приду', 'пощекочу твою пизду снутри', 'ибо', 'устанет'], ['покусаю твою попу', 'трахать', 'жопу', 'ментов', 'вызовут', 'вой той девки', 'кайфует подо'], ['сниму презерватив', 'мирамистином', 'полью', 'минут', 'перерыв —', 'час', 'бою'], ['шея', 'укусах', 'засосах', 'болеть', 'включая твою грудь', 'пару дней', 'тоскуя', 'вопросов', 'захочешь', 'ночь вернуть']]\n\n# remove excess symbols from phrases\nchar_phrases = phrases_without_excess_symbols(phrases, include_alpha= True, include_numbers= True)\nprint2(char_phrases)\n# [['жопе шило', 'могу достать'], ['имею опыт', 'да'], ['влагалище твое', 'тонус приведу'], ['пиши адрес', 'приду'], ['член', 'словно банан', 'кривой', 'матку вплоть', 'дна зайдет', 'полчаса стоять смогет', 'границе часовой'], ['разрываться', 'начнем', 'ебаться', 'счастье', 'казниться', 'решилась соблазниться'], ['медленно сниму штаны', 'поцелуями сломлю твое сомненье', 'похоти войдешь', 'опьяненье', 'кончишь'], ['смазку принесу', 'почувствуешь прохладу', 'жарить', 'большой трубой', 'получу усладу'], ['напиши', 'приду', 'пощекочу твою пизду снутри', 'ибо', 'устанет'], ['покусаю твою попу', 'трахать', 'жопу', 'ментов', 'вызовут', 'вой той девки', 'кайфует подо'], ['сниму презерватив', 'мирамистином', 'полью', 'минут', 'перерыв ', 'час', 'бою'], ['шея', 'укусах', 'засосах', 'болеть', 'включая твою грудь', 'пару дней', 'тоскуя', 'вопросов', 'захочешь', 'ночь вернуть']]\n\n# stem and lemmatize all words in all phrases\nstemmed_phrases = phrases_transform(char_phrases, func = stem_lem)\nprint2(stemmed_phrases)\n# [['жоп шил', 'моч доста'], ['имет оп', 'да'], ['влагалищ тво', 'тонус привест'], ['писа адрес', 'прийт'], ['член', 'словн бана', 'крив', 'матк вплот', 'дно зайт', 'полчас стоя смогет', 'границ часов'], ['разрыва', 'нача', 'еба', 'счаст', 'казн', 'реш соблазн'], ['медлен снят штан', 'поцел слом тво сомнен', 'похот войт', 'опьянен', 'конч'], ['смазк принест', 'почувствова прохлад', 'жар', 'больш труб', 'получ услад'], ['написа', 'прийт', 'пощекота тво пизд снутерет', 'иб', 'уста'], ['покуса тво поп', 'траха', 'жоп', 'мент', 'вызва', 'во тот девк', 'кайфова под'], ['снят презерват', 'мирамистин', 'пол', 'минут', 'перер', 'час', 'бо'], ['ше', 'укус', 'засос', 'болет', 'включ тво груд', 'пар ден', 'тоскова', 'вопрос', 'захотет', 'ноч вернут']]\n\n# convert each phrase to list of n-grams\nn_grams = phrases_transform(stemmed_phrases, func = lambda w: words_to_ngrams_list(w.split(), n_min = 1, n_max = 2))\nprint2(n_grams)\n# [[['жоп', 'шил', 'жоп шил'], ['моч', 'доста', 'моч доста']], [['имет', 'оп', 'имет оп'], ['да']], [['влагалищ', 'тво', 'влагалищ тво'], ['тонус', 'привест', 'тонус привест']], [['писа', 'адрес', 'писа адрес'], ['прийт']], [['член'], ['словн', 'бана', 'словн бана'], ['крив'], ['матк', 'вплот', 'матк вплот'], ['дно', 'зайт', 'дно зайт'], ['полчас', 'стоя', 'смогет', 'полчас стоя', 'стоя смогет'], ['границ', 'часов', 'границ часов']], [['разрыва'], ['нача'], ['еба'], ['счаст'], ['казн'], ['реш', 'соблазн', 'реш соблазн']], [['медлен', 'снят', 'штан', 'медлен снят', 'снят штан'], ['поцел', 'слом', 'тво', 'сомнен', 'поцел слом', 'слом тво', 'тво сомнен'], ['похот', 'войт', 'похот войт'], ['опьянен'], ['конч']], [['смазк', 'принест', 'смазк принест'], ['почувствова', 'прохлад', 'почувствова прохлад'], ['жар'], ['больш', 'труб', 'больш труб'], ['получ', 'услад', 'получ услад']], [['написа'], ['прийт'], ['пощекота', 'тво', 'пизд', 'снутерет', 'пощекота тво', 'тво пизд', 'пизд снутерет'], ['иб'], ['уста']], [['покуса', 'тво', 'поп', 'покуса тво', 'тво поп'], ['траха'], ['жоп'], ['мент'], ['вызва'], ['во', 'тот', 'девк', 'во тот', 'тот девк'], ['кайфова', 'под', 'кайфова под']], [['снят', 'презерват', 'снят презерват'], ['мирамистин'], ['пол'], ['минут'], ['перер'], ['час'], ['бо']], [['ше'], ['укус'], ['засос'], ['болет'], ['включ', 'тво', 'груд', 'включ тво', 'тво груд'], ['пар', 'ден', 'пар ден'], ['тоскова'], ['вопрос'], ['захотет'], ['ноч', 'вернут', 'ноч вернут']]]\n\n# convert list of list of list to just list\ntotal = sum_phrases(n_grams)\nprint2(total)\n# ['жоп', 'шил', 'жоп шил', 'моч', 'доста', 'моч доста', 'имет', 'оп', 'имет оп', 'да', 'влагалищ', 'тво', 'влагалищ тво', 'тонус', 'привест', 'тонус привест', 'писа', 'адрес', 'писа адрес', 'прийт', 'член', 'словн', 'бана', 'словн бана', 'крив', 'матк', 'вплот', 'матк вплот', 'дно', 'зайт', 'дно зайт', 'полчас', 'стоя', 'смогет', 'полчас стоя', 'стоя смогет', 'границ', 'часов', 'границ часов', 'разрыва', 'нача', 'еба', 'счаст', 'казн', 'реш', 'соблазн', 'реш соблазн', 'медлен', 'снят', 'штан', 'медлен снят', 'снят штан', 'поцел', 'слом', 'тво', 'сомнен', 'поцел слом', 'слом тво', 'тво сомнен', 'похот', 'войт', 'похот войт', 'опьянен', 'конч', 'смазк', 'принест', 'смазк принест', 'почувствова', 'прохлад', 'почувствова прохлад', 'жар', 'больш', 'труб', 'больш труб', 'получ', 'услад', 'получ услад', 'написа', 'прийт', 'пощекота', 'тво', 'пизд', 'снутерет', 'пощекота тво', 'тво пизд', 'пизд снутерет', 'иб', 'уста', 'покуса', 'тво', 'поп', 'покуса тво', 'тво поп', 'траха', 'жоп', 'мент', 'вызва', 'во', 'тот', 'девк', 'во тот', 'тот девк', 'кайфова', 'под', 'кайфова под', 'снят', 'презерват', 'снят презерват', 'мирамистин', 'пол', 'минут', 'перер', 'час', 'бо', 'ше', 'укус', 'засос', 'болет', 'включ', 'тво', 'груд', 'включ тво', 'тво груд', 'пар', 'ден', 'пар ден', 'тоскова', 'вопрос', 'захотет', 'ноч', 'вернут', 'ноч вернут']\n\n# convert all objects to set\ntotal_set = wordlist2set(total, save_order=False)\nprint2(total_set)\n# {'траха', 'влагалищ', 'больш труб', 'адрес', 'зайт', 'влагалищ тво', 'снят штан', 'вопрос', 'счаст', 'слом', 'груд тво', 'поцел слом', 'дно', 'опьянен', 'жоп', 'иб', 'труб', 'болет', 'тот', 'мирамистин', 'моч', 'поцел', 'доста моч', 'прохлад', 'мент', 'пощекота тво', 'укус', 'ден', 'кайфова', 'уста', 'войт похот', 'да', 'девк тот', 'во тот', 'вернут ноч', 'груд', 'границ часов', 'услад', 'засос', 'имет оп', 'презерват', 'стоя', 'принест', 'сомнен тво', 'пизд тво', 'покуса тво', 'разрыва', 'перер', 'оп', 'сомнен', 'соблазн', 'еба', 'крив', 'тонус', 'полчас стоя', 'жар', 'захотет', 'тоскова', 'смогет', 'вплот', 'писа', 'бо', 'пощекота', 'адрес писа', 'почувствова прохлад', 'медлен', 'снят', 'вызва', 'кайфова под', 'ноч', 'получ услад', 'во', 'прийт', 'пар', 'член', 'минут', 'похот', 'медлен снят', 'казн', 'написа', 'штан', 'включ тво', 'реш', 'войт', 'снутерет', 'покуса', 'ше', 'пол', 'девк', 'смогет стоя', 'час', 'пизд снутерет', 'конч', 'почувствова', 'жоп шил', 'доста', 'ден пар', 'включ', 'часов', 'привест тонус', 'слом тво', 'дно зайт', 'пизд', 'смазк', 'вернут', 'словн', 'больш', 'презерват снят', 'бана словн', 'имет', 'тво', 'вплот матк', 'принест смазк', 'шил', 'полчас', 'поп тво', 'под', 'поп', 'бана', 'матк', 'нача', 'реш соблазн', 'границ', 'получ', 'привест'}\n\n\n# all these steps are equal to pipeline\n\npipe = StemLemPipeline([\n\n    text2sentences,phrases2lower,\n    lambda sentences: list(map(lambda s: sentence_split(s , separators=',;', stop_words = stopwords('ru')), sentences)),\n\n    lambda p: phrases_without_excess_symbols(p, include_alpha= True, include_numbers= True),\n    lambda p: phrases_transform(p, stem_lem),\n    lambda p: phrases_transform(p, func = lambda w: words_to_ngrams_list(w.split(), n_min = 1, n_max = 2)),\n    sum_phrases,\n    lambda p: wordlist2set(p, save_order=False)\n])\n\npipe(text_example)\n\n\nprint(total_set == pipe(text_example)) # True\n```\n\n## Steps description\n\nIs it so needed to make many splits, lemmatization and stemming?.. Why we cannot use just regular expressions? I think we always should do more to **remove unnecessary information and engineer better information** from text data. For example:\n* It's obvious that words *'see'* and *'saw'* are same words. We ought to use lemmatization to save this relations for ML model.\n* For some words it will be better to use stemming: *'dog'*, *'dogs'*, *'dog\\`s'*, *'dogs\\`'* are same words. It's better to have 1 word instead of 4\n* Simplest way to save relations between words is using n-grams but it's not good to make so many 2-grams and 3-grams because of [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality); so, **we shouldn't make n-grams which have no sense**! But how to prevent it? I think next rules are right almost always:\n  * In text like *'I think it is good idea. Let me know'* there is no sense of creating n-grams *'idea let'* or *'idea let me'* because the probability of importance between last words in sentence and first words in next sentence is small. So we should split text by sentences and create n-grams for each sentence, not for all text\n  * There is same idea for words beside stop words or symbols like `, ; ? !`. And we should split sentences by these objects too before creating n-grams\n\nBeneath there are some instructions how to do these logic using `StemLemPipe`\n\n### Splitting text to sentences\n\nThere are several functions for splitting text to list of strings using different logic:\n* `text2sentences(txt, equal_to_space = [\"\\n\"])`\n* `split_by_words(sentence, words)`\n* `sentence_split(sentence, separators = \",;!?\", stop_words = None)`\n\nSee [example of using](tests/splitting_text.py)\n\nUse `text2sectences` to split text to list of sentences.\n\n\n### Text preparations like removing some symbols and setting lower case\n\nAfter previous preparations we have list of sentences where each sentence is string or list of string. To apply some functions to strings in this construction there is `phrases_transform(phrases, func, progress_bar = False)` function where `func` is the function applying to each string. Also there are some wrappers of `phrases_transform` for certain tasks:\n* `phrases2lower(phrases)`\n* `phrases_without_excess_symbols(phrases, include_alpha = True, include_numbers = False, include_also = None)`\n\nSee [example of using](tests/phrases_prep.py)\n\n### Splitting each sentence by some symbols and stop words\n\nFor this u can use `sentence_split` or `split_by_words` functions from [previous block](#splitting-text-to-sentences).\n\nUse `stopwords(language = 'ru')` function to get stop words, but u can use your own stop words too!\n\n### Lemmatization and/or stemming\n\nTo apply lemmatization of stemming for phrases u should create some lemmatization function and use if with `phrases_transform`. Create lemmatization function by one of ways:\n* `create_lemmatizer(backend = 'pymorphy', language = 'ru')`\n* `create_stemmer(backend = 'snowball', language = 'ru')`\n* `create_stemmer_lemmer(lemmatizer_backend = 'pymorphy', stemmer_backend = 'snowball', language = 'ru')` -- pipeline with lemmatizer and stemmer\n* create any another wrapper for function `text -> text`\n\n**Available stemmers**:\n\n| language        | backend           |\n| ------------- |:-------------:| \n| `'ru'`      | `'snowball'` |\n| `'en'`      | `'snowball'`      |\n\n**Available lemmatizers**:\n\n| language        | backend           |\n| ------------- |:-------------:| \n| `'ru'`      | `'pymorphy'`, `'mystem'` |\n| `'en'`      | `'wordnet'`      |\n\nIt's not hard to add new lemmatizers but just let me know. Also have a look at [source file](StemLemPipe/stemlem_operators.py)\n\n### N-grams\n\nMethods for getting n-grams *from array of words*:\n* `get_ngrams(arr, n=2)`\n* `words_to_ngrams_list(words, n_min = 1, n_max = 2)`\n\nHow to use:\n```python\nfrom StemLemPipe import get_ngrams, words_to_ngrams_list\n\ntext = \"word1 word2 word3 ... word10\"\n\n# returns generator\ngen = get_ngrams(text.split(), n = 3)\n# just list of lists\nprint(list(gen))\n# [['word1', 'word2', 'word3'], ['word2', 'word3', '...'], ['word3', '...', 'word10']]\n\n\n# words in n-gram are combined, it's list of strings\nprint(words_to_ngrams_list(text.split(), n_min = 1, n_max = 3))\n# ['word1', 'word2', 'word3', '...', 'word10', 'word1 word2', 'word2 word3', 'word3 ...', '... word10', 'word1 word2 word3', 'word2 word3 ...', 'word3 ... word10']\n```\n\n### Union to set\n\nWe can convert those lists of lists of lists... to list of strings using `sum_phrases` function. Also we can convert these list of strings to set using `wordlist2set(input_list, save_order = False)` function there `save_order = True` means that n-grams like `word1 word2` and `word2 word1` are not some (same otherwise).\n\n## Pipeline\n\nCreate `StemLemPipeline` object for using certain functions one after the other for new texts.\n\nCreate pipeline using code:\n```python\nfrom StemLemPipe import StemLemPipeline\n\npipe = StemLemPipeline([func1, func2, ...])\n```\n\nFor using pipeline just call:\n```python\nresult = pipe('some text for preparations')\n```\n\n\n## Another tools\n\n### Metrics\n\nIt supports next metrics:\n* `Levenstein.usual(str1, str2)`\n* `Levenstein.deep(s1, s2, remove_desc = True)`\n\n\n### Little functions\n\nFor text preparation it can be highly useful to use next functions:\n* `remove_words(text, words)` -- just removes next words from text without splitting to phrases (unlike `sentence_split`)\n* `remove_hook_words(text, hook_words)` --  removes hook words from text with one next word. For `text = \"a b c d e f\"` and `hook_words = ['b', 'e']` returns `\"a d\"` (without b, e and next words) ([example](/tests/hook_words.py))\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/PasaOpasen/Stem-Lem-Pipeline",
    "keywords": "text,nlp,nltk,ngrams,transformation,words,stemming,lemmatization",
    "license": "",
    "maintainer": "['Demetry Pascal']",
    "maintainer_email": "",
    "name": "StemLemPipe",
    "package_url": "https://pypi.org/project/StemLemPipe/",
    "platform": "",
    "project_url": "https://pypi.org/project/StemLemPipe/",
    "project_urls": {
      "Homepage": "https://github.com/PasaOpasen/Stem-Lem-Pipeline"
    },
    "release_url": "https://pypi.org/project/StemLemPipe/0.1.5/",
    "requires_dist": [
      "nltk",
      "pymorphy2",
      "pymystem3",
      "stop-words",
      "tqdm"
    ],
    "requires_python": "",
    "summary": "simple text transformer used several stemming and lemmatization backends",
    "version": "0.1.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9474162,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4ce5ae5ee5e6a737bcfa2dc95d45be6d8effb69b1131993d5fb040735a3365be",
          "md5": "219a4606fd173e63281752503777a3c9",
          "sha256": "f8bf0fcb1114f93e99641eaa97029a745406e559039a3412e02856ba272cf2ee"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "219a4606fd173e63281752503777a3c9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 13728,
        "upload_time": "2021-01-18T13:58:58",
        "upload_time_iso_8601": "2021-01-18T13:58:58.652228Z",
        "url": "https://files.pythonhosted.org/packages/4c/e5/ae5ee5e6a737bcfa2dc95d45be6d8effb69b1131993d5fb040735a3365be/StemLemPipe-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "241edf76e0c5483929af5ec7418445e2c96f3035129f9b37bf1b74ffba40b328",
          "md5": "be3269611f90130c1e58da36e2b65068",
          "sha256": "b97f91e1fb6c0909f022fe7cac189de8abaf83164c9536d442a57fde84f7d819"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "be3269611f90130c1e58da36e2b65068",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22149,
        "upload_time": "2021-01-18T13:59:00",
        "upload_time_iso_8601": "2021-01-18T13:59:00.222444Z",
        "url": "https://files.pythonhosted.org/packages/24/1e/df76e0c5483929af5ec7418445e2c96f3035129f9b37bf1b74ffba40b328/StemLemPipe-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "90ce72155ae9e17df56e1e023bb7c0c1a0bc697191e804768fd933ed25f59045",
          "md5": "b714ca604a9eaa1ae684bd34d603afcd",
          "sha256": "fa47094087ef7d90d87ceb45c53762bae19e7a90990a3a21376aa4fce298d0c4"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b714ca604a9eaa1ae684bd34d603afcd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 13934,
        "upload_time": "2021-01-19T08:32:07",
        "upload_time_iso_8601": "2021-01-19T08:32:07.840695Z",
        "url": "https://files.pythonhosted.org/packages/90/ce/72155ae9e17df56e1e023bb7c0c1a0bc697191e804768fd933ed25f59045/StemLemPipe-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c6fb077c364c1fcad945d95545b7f3e9e892907952e9d4c102bf2b88f042905b",
          "md5": "fb22548f8d0d6be8f378c1f0be776d39",
          "sha256": "030adc970268aa279f5a0ef7dbf0cbe1921eb9a96507e848581299f25ec01526"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "fb22548f8d0d6be8f378c1f0be776d39",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23403,
        "upload_time": "2021-01-19T08:32:09",
        "upload_time_iso_8601": "2021-01-19T08:32:09.386392Z",
        "url": "https://files.pythonhosted.org/packages/c6/fb/077c364c1fcad945d95545b7f3e9e892907952e9d4c102bf2b88f042905b/StemLemPipe-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "afca2ff7f5edc7b8d3798f3f5b40f149114ec7679c8feb2236f7488a8ab311e1",
          "md5": "2eeae9ebcc6a676dcc41c90bb5f93e15",
          "sha256": "97fc91d0ce4ab4118d845d4ac77df32b55112d3241b37b7184329c91fdecd99e"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2eeae9ebcc6a676dcc41c90bb5f93e15",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 14728,
        "upload_time": "2021-01-24T14:10:09",
        "upload_time_iso_8601": "2021-01-24T14:10:09.988837Z",
        "url": "https://files.pythonhosted.org/packages/af/ca/2ff7f5edc7b8d3798f3f5b40f149114ec7679c8feb2236f7488a8ab311e1/StemLemPipe-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f7db7f1700b314f6f4ffb84ba69f4a0fd952dd54bb71c27b19d104201929b4c1",
          "md5": "bbbc2052b8852833a754228426c8f56b",
          "sha256": "3b50cc4d0715509fc317cfc697db0ed89841412e10c9c22418ee35231d67b1c3"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "bbbc2052b8852833a754228426c8f56b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24148,
        "upload_time": "2021-01-24T14:10:11",
        "upload_time_iso_8601": "2021-01-24T14:10:11.900941Z",
        "url": "https://files.pythonhosted.org/packages/f7/db/7f1700b314f6f4ffb84ba69f4a0fd952dd54bb71c27b19d104201929b4c1/StemLemPipe-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "279fc81d0e66c34f83ae8b0fc7de120b37eb3fc73bdfd465f50e6c0de08d316d",
          "md5": "c2683bd3cb4a10953514c28469708b93",
          "sha256": "d6ca24e55bb7c2e6bec19d9778b7f37c1e06ae178bbc92a22198154b06e8b883"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c2683bd3cb4a10953514c28469708b93",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 15030,
        "upload_time": "2021-01-25T17:05:35",
        "upload_time_iso_8601": "2021-01-25T17:05:35.582759Z",
        "url": "https://files.pythonhosted.org/packages/27/9f/c81d0e66c34f83ae8b0fc7de120b37eb3fc73bdfd465f50e6c0de08d316d/StemLemPipe-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3605b94fa6099b03c0bd56ae2397d21b6a5963dda853968a15ce829b427ed749",
          "md5": "9570e55194232fe137b1d090fd73319f",
          "sha256": "92d083a90b91aa6551593875485132aa36bc12226234da0e7282b4a2eecccb5e"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "9570e55194232fe137b1d090fd73319f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24665,
        "upload_time": "2021-01-25T17:05:37",
        "upload_time_iso_8601": "2021-01-25T17:05:37.282326Z",
        "url": "https://files.pythonhosted.org/packages/36/05/b94fa6099b03c0bd56ae2397d21b6a5963dda853968a15ce829b427ed749/StemLemPipe-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7ce0065a7869ef26145d3173d5bde035d3041155fd7fb2b27c0c3bcd83c67789",
          "md5": "d421c642d142c25583eebb16d8d7b6aa",
          "sha256": "6b90d223ea5957ba9f837214e164a109d586ae1bc24040473ddeb47d8c0f47be"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d421c642d142c25583eebb16d8d7b6aa",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 15498,
        "upload_time": "2021-01-26T17:22:56",
        "upload_time_iso_8601": "2021-01-26T17:22:56.789502Z",
        "url": "https://files.pythonhosted.org/packages/7c/e0/065a7869ef26145d3173d5bde035d3041155fd7fb2b27c0c3bcd83c67789/StemLemPipe-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4ed67b30db96f15b412ddf8ebf6ba64b69b89f216c4a16eb5a491deda15a5227",
          "md5": "125d560f0ea4f9c19fd5df8056a3e73c",
          "sha256": "439761e0e83978e93f378307d525cbe9ab5884d9beddaa31bcf6afc67f41eb38"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "125d560f0ea4f9c19fd5df8056a3e73c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 25134,
        "upload_time": "2021-01-26T17:22:58",
        "upload_time_iso_8601": "2021-01-26T17:22:58.332321Z",
        "url": "https://files.pythonhosted.org/packages/4e/d6/7b30db96f15b412ddf8ebf6ba64b69b89f216c4a16eb5a491deda15a5227/StemLemPipe-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "44dfc32ce1445357b625ed3b32f79bbfe37b5600bb8b8d0993bc5ad8ed7f0dac",
          "md5": "e9b1458c8ddee0c217fad5a1fcbfc56c",
          "sha256": "91dc257f6a57725ebc373b91c6138531b00049cfec2f73d94ce93156582b2268"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e9b1458c8ddee0c217fad5a1fcbfc56c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 15659,
        "upload_time": "2021-02-20T08:36:32",
        "upload_time_iso_8601": "2021-02-20T08:36:32.305070Z",
        "url": "https://files.pythonhosted.org/packages/44/df/c32ce1445357b625ed3b32f79bbfe37b5600bb8b8d0993bc5ad8ed7f0dac/StemLemPipe-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e44de737e8b712553a88c70ce09d454256a3c2aff4b72b2d3e2000f1ddb835d8",
          "md5": "1ffdeaef4913cc88c992c9fe20f1b6c6",
          "sha256": "6d954f61d158f8b130f22a94358caa2b48c37747fe99d8f7b13b4401a1457bc9"
        },
        "downloads": -1,
        "filename": "StemLemPipe-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "1ffdeaef4913cc88c992c9fe20f1b6c6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 25336,
        "upload_time": "2021-02-20T08:36:33",
        "upload_time_iso_8601": "2021-02-20T08:36:33.890782Z",
        "url": "https://files.pythonhosted.org/packages/e4/4d/e737e8b712553a88c70ce09d454256a3c2aff4b72b2d3e2000f1ddb835d8/StemLemPipe-0.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "44dfc32ce1445357b625ed3b32f79bbfe37b5600bb8b8d0993bc5ad8ed7f0dac",
        "md5": "e9b1458c8ddee0c217fad5a1fcbfc56c",
        "sha256": "91dc257f6a57725ebc373b91c6138531b00049cfec2f73d94ce93156582b2268"
      },
      "downloads": -1,
      "filename": "StemLemPipe-0.1.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e9b1458c8ddee0c217fad5a1fcbfc56c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 15659,
      "upload_time": "2021-02-20T08:36:32",
      "upload_time_iso_8601": "2021-02-20T08:36:32.305070Z",
      "url": "https://files.pythonhosted.org/packages/44/df/c32ce1445357b625ed3b32f79bbfe37b5600bb8b8d0993bc5ad8ed7f0dac/StemLemPipe-0.1.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e44de737e8b712553a88c70ce09d454256a3c2aff4b72b2d3e2000f1ddb835d8",
        "md5": "1ffdeaef4913cc88c992c9fe20f1b6c6",
        "sha256": "6d954f61d158f8b130f22a94358caa2b48c37747fe99d8f7b13b4401a1457bc9"
      },
      "downloads": -1,
      "filename": "StemLemPipe-0.1.5.tar.gz",
      "has_sig": false,
      "md5_digest": "1ffdeaef4913cc88c992c9fe20f1b6c6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 25336,
      "upload_time": "2021-02-20T08:36:33",
      "upload_time_iso_8601": "2021-02-20T08:36:33.890782Z",
      "url": "https://files.pythonhosted.org/packages/e4/4d/e737e8b712553a88c70ce09d454256a3c2aff4b72b2d3e2000f1ddb835d8/StemLemPipe-0.1.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}