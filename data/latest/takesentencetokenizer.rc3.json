{
  "info": {
    "author": "Karina Tiemi Kato",
    "author_email": "karinat@take.net",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# TakeSentenceTokenizer\n\nTakeSentenceTokenizer is a tool for pre processing and tokenizing sentences. \nThe package is used to:\n\t- convert the first word of the sentence to lowercase\n\t- convert from uppercase to lowercase\n\t- convert word to lowercase after punctuation\n\t- replace words for placeholders: laugh, date, time, ddd, measures (10kg, 20m, 5gb, etc), code, phone number, cnpj, cpf, email, money, url, number (ordinal and cardinal)\n\t- replace abbreviations\n\t- replace common typos\n\t- split punctuations\n\t- remove emoji\n\t- remove characters that are not letters or punctuation\n\t- add missing accentuation\n\t- tokenize the sentence\n\n## Installation\n\nUse the package manager [pip](https://pip.pypa.io/en/stable/) to install TakeSentenceTokenizer\n\n```bash\npip install TakeSentenceTokenizer\n```\n\n## Usage\n\nExample 1: full processing not keeping registry of removed punctuation\n\nCode:\n```python\nfrom SentenceTokenizer import SentenceTokenizer\nsentence = 'P/ saber disso eh c/ vc ou consigo ver pelo site www.dÃºvidas.com.br/minha-dÃºvida ??'\ntokenizer = SentenceTokenizer()\nprocessed_sentence = tokenizer.process_message(sentence)\nprint(processed_sentence)\n```\n\nOutput:\n```python\n'para saber disso Ã© com vocÃª ou consigo ver pelo site URL ? ?'\n```\n\n\nExample 2: full processing keeping registry of removed punctuation\n```python\nfrom SentenceTokenizer import SentenceTokenizer\nsentence = 'como assim $@???'\ntokenizer = SentenceTokenizer(keep_registry_punctuation = True)\nprocessed_sentence = tokenizer.process_message(sentence)\nprint(processed_sentence)\nprint(tokenizer.removal_registry_lst)\n```\n\nOutput:\n```python\ncomo assim ? ? ?\n[['como assim $@ ? ? ?', {'punctuation': '$', 'position': 11}, {'punctuation': '@', 'position': 12}, {'punctuation': ' ', 'position': 13}]]\n```\n\n## Author\nTake Data&Analytics Research\n\n## License\n[MIT](https://choosealicense.com/licenses/mit/)\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "Tokenization",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "TakeSentenceTokenizer",
    "package_url": "https://pypi.org/project/TakeSentenceTokenizer/",
    "platform": null,
    "project_url": "https://pypi.org/project/TakeSentenceTokenizer/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/TakeSentenceTokenizer/1.0.2/",
    "requires_dist": [
      "emoji (==1.7.0)"
    ],
    "requires_python": "",
    "summary": "TakeSentenceTokenizer is a tool for tokenizing and pre processing messages",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14777986,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "33f1f7a02f78c7ed12a8fd1f00ab04631d1b1faf643dfec76ab578ccf3fe3b4d",
          "md5": "e1eed1bd73cfebe18d97c7ad3defa2f6",
          "sha256": "c710130bb674ac86fe0317a7e98a9e54a70067603fa04c97f4d4b363c8d0d667"
        },
        "downloads": -1,
        "filename": "TakeSentenceTokenizer-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e1eed1bd73cfebe18d97c7ad3defa2f6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 407942,
        "upload_time": "2020-03-10T16:50:45",
        "upload_time_iso_8601": "2020-03-10T16:50:45.105581Z",
        "url": "https://files.pythonhosted.org/packages/33/f1/f7a02f78c7ed12a8fd1f00ab04631d1b1faf643dfec76ab578ccf3fe3b4d/TakeSentenceTokenizer-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ec6db7ba692035842bc0d2340e0ba9a07febb00b79115aed29f1e80a3d82da7",
          "md5": "90e31e202f1d9df93e5b6bccabff5115",
          "sha256": "27539df5752be6e9a1460970eb74f68ab2d9663ae9967932dde64b4077c39c48"
        },
        "downloads": -1,
        "filename": "TakeSentenceTokenizer-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "90e31e202f1d9df93e5b6bccabff5115",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 412012,
        "upload_time": "2020-08-12T21:41:11",
        "upload_time_iso_8601": "2020-08-12T21:41:11.528511Z",
        "url": "https://files.pythonhosted.org/packages/5e/c6/db7ba692035842bc0d2340e0ba9a07febb00b79115aed29f1e80a3d82da7/TakeSentenceTokenizer-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1827cb7bce94e3c770519436374c426df946b9bd39c3c1d03a39f6514bd7ef0a",
          "md5": "b216d95683ff4887aa13b611b387093f",
          "sha256": "0f1713b37d15de79246aae9c20f8f65986fd07111655c532865cdd7b399946a4"
        },
        "downloads": -1,
        "filename": "TakeSentenceTokenizer-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b216d95683ff4887aa13b611b387093f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7305,
        "upload_time": "2020-08-12T21:41:13",
        "upload_time_iso_8601": "2020-08-12T21:41:13.141208Z",
        "url": "https://files.pythonhosted.org/packages/18/27/cb7bce94e3c770519436374c426df946b9bd39c3c1d03a39f6514bd7ef0a/TakeSentenceTokenizer-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ec0294d828621011be09aee4736151dd1e6a1f5e819ffe5a045de1618f481027",
          "md5": "5735c9974532de2b8467125f894cfcf9",
          "sha256": "f9f533ef3a880c1ac01859f19bf0de44105e96a4615a8f8d67f5fb4a50465532"
        },
        "downloads": -1,
        "filename": "TakeSentenceTokenizer-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5735c9974532de2b8467125f894cfcf9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 408226,
        "upload_time": "2022-08-16T15:03:34",
        "upload_time_iso_8601": "2022-08-16T15:03:34.182473Z",
        "url": "https://files.pythonhosted.org/packages/ec/02/94d828621011be09aee4736151dd1e6a1f5e819ffe5a045de1618f481027/TakeSentenceTokenizer-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c04aca551bbff747a4c414382a1a390eb4b429dbc9ad587009e2b25fdaa7da95",
          "md5": "675a0a1de585e9b40e790578655d6159",
          "sha256": "1fd6be085c4ce022fa5ab9620f85a85ac3c10ba7b14a3302f44a08e7c0dbf947"
        },
        "downloads": -1,
        "filename": "TakeSentenceTokenizer-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "675a0a1de585e9b40e790578655d6159",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7498,
        "upload_time": "2022-08-16T15:03:36",
        "upload_time_iso_8601": "2022-08-16T15:03:36.519875Z",
        "url": "https://files.pythonhosted.org/packages/c0/4a/ca551bbff747a4c414382a1a390eb4b429dbc9ad587009e2b25fdaa7da95/TakeSentenceTokenizer-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ec0294d828621011be09aee4736151dd1e6a1f5e819ffe5a045de1618f481027",
        "md5": "5735c9974532de2b8467125f894cfcf9",
        "sha256": "f9f533ef3a880c1ac01859f19bf0de44105e96a4615a8f8d67f5fb4a50465532"
      },
      "downloads": -1,
      "filename": "TakeSentenceTokenizer-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "5735c9974532de2b8467125f894cfcf9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 408226,
      "upload_time": "2022-08-16T15:03:34",
      "upload_time_iso_8601": "2022-08-16T15:03:34.182473Z",
      "url": "https://files.pythonhosted.org/packages/ec/02/94d828621011be09aee4736151dd1e6a1f5e819ffe5a045de1618f481027/TakeSentenceTokenizer-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c04aca551bbff747a4c414382a1a390eb4b429dbc9ad587009e2b25fdaa7da95",
        "md5": "675a0a1de585e9b40e790578655d6159",
        "sha256": "1fd6be085c4ce022fa5ab9620f85a85ac3c10ba7b14a3302f44a08e7c0dbf947"
      },
      "downloads": -1,
      "filename": "TakeSentenceTokenizer-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "675a0a1de585e9b40e790578655d6159",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 7498,
      "upload_time": "2022-08-16T15:03:36",
      "upload_time_iso_8601": "2022-08-16T15:03:36.519875Z",
      "url": "https://files.pythonhosted.org/packages/c0/4a/ca551bbff747a4c414382a1a390eb4b429dbc9ad587009e2b25fdaa7da95/TakeSentenceTokenizer-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}