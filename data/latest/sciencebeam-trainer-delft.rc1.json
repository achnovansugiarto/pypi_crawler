{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# ScienceBeam Trainer DeLFT\n\nWork in-progress..\n\nA thin(ish) wrapper around [DeLFT](https://github.com/kermitt2/delft) to enable training in the cloud.\n\nSome of the main features:\n\n- resources (model, data etc.) can be loaded from remote sources, currently:\n  - HTTP (`https://`, `http://`)\n  - Google Storage (`gs://`)\n- resources can be saved to remote buckets, currently:\n  - Google Storage (`gs://`)\n- on-demand embedding download\n- Docker container(s)\n- Support for Wapiti models\n\n## Prerequisites\n\n- Python 3\n\nWhen using [pyenv](https://github.com/pyenv/pyenv),\nyou may need `libsqlite3-dev` and have Python installed with the `--enable-shared` flag.\n\nFor example:\n\n```bash\napt-get install libsqlite3-dev\n```\n\n```bash\nPYTHON_CONFIGURE_OPTS=\"--enable-shared\" pyenv install --force 3.7.9\n```\n\n## Example Notebooks\n\n- [train-header.ipynb](notebooks/train-header.ipynb) ([open in colab](https://colab.research.google.com/github/elifesciences/sciencebeam-trainer-delft/blob/develop/notebooks/train-header.ipynb))\n\n## GROBID Docker Image with DeLFT\n\nThe Docker image `elifesciences/sciencebeam-trainer-delft-grobid_unstable`\ncan be used in-place of the main GROBID image.\nIt includes DeLFT (currently with CPU support only).\n\nThere are several ways to change the configuration or override models.\n\n### Override Models using Docker Image\n\nThe `OVERRIDE_MODELS` or `OVERRIDE_MODEL_*` environment variables allow models to be overriden. Both environment variables are equivallent. `OVERRIDE_MODELS` is meant for overriding multiple models via a single environment variable (separated by `|`), whereas `OVERRIDE_MODEL_*` can be used to specify each model separately.\n\n```bash\ndocker run --rm \\\n    --env \"OVERRIDE_MODELS=segmentation=/path/to/segmentation-model|header=/path/to/header-model\" \\\n    elifesciences/sciencebeam-trainer-delft-grobid_unstable\n```\n\nor:\n\n```bash\ndocker run --rm \\\n    --env \"OVERRIDE_MODEL_1=segmentation=/path/to/segmentation-model\" \\\n    --env \"OVERRIDE_MODEL_2=header=/path/to/header-model\" \\\n    elifesciences/sciencebeam-trainer-delft-grobid_unstable\n```\n\ne.g.:\n\n```bash\ndocker run --rm \\\n    --env \"OVERRIDE_MODEL_1=header=https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/delft-grobid-header-biorxiv-no-word-embedding-2020-05-05.tar.gz\" \\\n    elifesciences/sciencebeam-trainer-delft-grobid_unstable\n```\n\nThis functionality is mainly intended for loading models from a compressed file or bucket, such as Google Storage or S3 (you may also need to mount the relevant credentials).\n\n## GROBID Trainer CLI\n\nThe GROBID Trainer CLI is the equivallent to [DeLFT's grobidTagger](https://github.com/kermitt2/delft/blob/master/grobidTagger.py). That is the main interface to interact with this project.\n\nTo get a list of all of the available parameters:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer --help\n```\n\n### Using Docker Image\n\n```bash\ndocker run --rm elifesciences/sciencebeam-trainer-delft_unstable \\\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer --help\n```\n\n### Train Sub Command\n\nTraining a model comes with many parameters. The following is an example to run the training without recommending parameters.\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train \\\n    --batch-size=\"10\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\nAn example command using more configurable parameters:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train \\\n    --batch-size=\"10\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --char-embedding-size=\"11\" \\\n    --char-lstm-units=\"12\" \\\n    --char-input-mask-zero \\\n    --char-input-dropout=\"0.3\" \\\n    --char-lstm-dropout=\"0.3\" \\\n    --max-char-length=\"13\" \\\n    --word-lstm-units=\"14\" \\\n    --dropout=\"0.1\" \\\n    --recurrent-dropout=\"0.2\" \\\n    --max-epoch=\"50\"\n```\n\n### Train Eval Sub Command\n\nThe `train_eval` sub command is combining the `train` and `eval` command. It is reserving a slice of the input for the evaluation.\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"10\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\nIf you rather want to provide separate evaluation data:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"10\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --eval-input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --eval-limit=\"100\" \\\n    --eval-max-sequence-length=\"100\" \\\n    --eval-input-window-stride=\"90\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\nYou can also train without using word embedding:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"10\" \\\n    --no-embedding \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\n### Train with layout features\n\nLayout features are additional features provided with each token, e.g. whether it's the start of the line.\n\nThe model needs to support using such features. The following models do:\n\n- `BidLSTM_CRF_FEATURES`\n- `CustomBidLSTM_CRF`\n- `CustomBidLSTM_CRF_FEATURES`\n\nThe features are generally provided. Some of the features are not suitable as input features because there are too many of them (e.g. a variation of the token itself). The features should be specified via `--features-indices`. The `input_info` sub command can help identify useful feature ranges (based on the count of unique values).\n\nExample commands:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"10\" \\\n    --no-embedding \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --architecture=\"BidLSTM_CRF_FEATURES\" \\\n    --use-features \\\n    --features-indices=\"9-30\" \\\n    --features-embedding-size=\"5\" \\\n    --features-lstm-units=\"7\" \\\n    --early-stopping-patience=\"10\" \\\n    --max-epoch=\"50\"\n```\n\nor\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"10\" \\\n    --no-embedding \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --architecture=\"CustomBidLSTM_CRF_FEATURES\" \\\n    --use-features \\\n    --features-indices=\"9-30\" \\\n    --features-embedding-size=\"5\" \\\n    --features-lstm-units=\"7\" \\\n    --early-stopping-patience=\"10\" \\\n    --max-epoch=\"50\"\n```\n\nor\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"10\" \\\n    --no-embedding \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --architecture=\"CustomBidLSTM_CRF\" \\\n    --use-features \\\n    --features-indices=\"9-30\" \\\n    --features-embedding-size=\"0\" \\\n    --features-lstm-units=\"0\" \\\n    --early-stopping-patience=\"10\" \\\n    --max-epoch=\"50\"\n```\n\nBy default features are assumed to be categorical.\nBut features may also be [continuous](https://en.wikipedia.org/wiki/Continuous_or_discrete_variable).\nThose values can be specified via the `--continuous-features-indices` parameter.\nIn that case they will automatically be part of the `features` and do not need to specified separately.\nContinuous features will get [min-max scaled](https://en.wikipedia.org/wiki/Feature_scaling).\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    segmentation \\\n    train_eval \\\n    --batch-size=\"10\" \\\n    --no-embedding \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-segmentation.train.gz \\\n    --limit=\"100\" \\\n    --architecture=\"CustomBidLSTM_CRF\" \\\n    --use-features \\\n    --features-indices=\"6-11\" \\\n    --continuous-features-indices=\"22,23,26\" \\\n    --features-embedding-size=\"0\" \\\n    --features-lstm-units=\"0\" \\\n    --early-stopping-patience=\"10\" \\\n    --max-epoch=\"50\"\n```\n\n### Training with additional text features\n\nLayout features may also contain additional token or text features.\n\nFor example the default GROBID *segmentation* model uses one data row for the whole line. With the first token being the main token, and the second token of the line being the the first feature (index `0`).\n\nTrain with additional token features:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    segmentation \\\n    train_eval \\\n    --batch-size=\"10\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --additional-token-feature-indices=\"0\" \\\n    --max-char-length=\"60\" \\\n    --max-sequence-length=\"100\" \\\n    --input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-segmentation.train.gz\" \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\nAdditionally, a ScienceBeam modifcation of the GROBID *segmentation* model also contains a text feature containing the whole line (further details below).\n\nTrain with text features (using three tokens for word embeddings):\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    segmentation \\\n    train_eval \\\n    --batch-size=\"10\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --text-feature-indices=\"32\" \\\n    --concatenated-embeddings-token-count=\"3\" \\\n    --max-char-length=\"60\" \\\n    --max-sequence-length=\"100\" \\\n    --input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/2020-07-30-biorxiv-1927-delft-segmentation-with-text-feature-32.train.gz\" \\\n    --eval-input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/2020-07-30-biorxiv-961-delft-segmentation-with-text-feature-32.validation.gz\" \\\n    --limit=\"100\" \\\n    --eval-limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\nIn the [referenced training data](https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/2020-07-30-biorxiv-1927-delft-segmentation-with-text-feature-32.train.gz), the last feature (`32`) represents the whole line (using non-breaking spaces instead of spaces). To use the model with GROBID, that [feature would need to be enabled](https://github.com/elifesciences/grobid/pull/25).\n\nThe same text feature also allows us to explore, whether the model would perform better,\nif each token within the text feature was a separate token (data row).\nIn that case one would specify `--unroll-text-feature-index` with the token index of the text feature\nthat should get re-tokenized and \"unrolled\". The features and labels will get copied.\nAnother feature will get added with the *line status* (`LINESTART`, `LINEIN`, `LINEEND`) - feature index `33` in the example below.\nWhere the label has a beginning prefix (`B-`), it will get converted to an inside prefix (`I-`) for the remaining tokens\n(see [IOB format](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))).\nAt the prediction time, the model will receive the \"unrolled\" data, wheras the original data will get returned,\nwith the majority label for that line (majority without prefix, a beginning prefix will be used if the label has changed).\n\nExample:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    segmentation \\\n    train_eval \\\n    --batch-size=\"10\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --unroll-text-feature-index=\"32\" \\\n    --use-features \\\n    --feature-indices=\"6-11,33\" \\\n    --max-char-length=\"60\" \\\n    --max-sequence-length=\"100\" \\\n    --input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/2020-07-30-biorxiv-1927-delft-segmentation-with-text-feature-32.train.gz\" \\\n    --eval-input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/2020-07-30-biorxiv-961-delft-segmentation-with-text-feature-32.validation.gz\" \\\n    --limit=\"100\" \\\n    --eval-batch-size=\"1\" \\\n    --eval-limit=\"10\" \\\n    --eval-max-sequence-length=\"100\" \\\n    --early-stopping-patience=\"10\" \\\n    --max-epoch=\"50\"\n```\n\nTo inspect the unrolled predictions further, it is also possible to use the `tag` sub command using\n`--tag-transformed`.\nThat flag will only make a difference for models already trained using the aforementioned\n`--unroll-text-feature-index` parameter.\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    tag \\\n    --tag-transformed \\\n    --batch-size=\"16\" \\\n    --input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/2020-07-30-biorxiv-961-delft-segmentation-with-text-feature-32.validation.gz\" \\\n    --model-path=\"data/models/sequenceLabelling/grobid-segmentation\" \\\n    --limit=\"2\" \\\n    --tag-output-format=\"data_unidiff\" \\\n    --tag-output-path=\"/tmp/test.diff\"\n```\n\n### Resume training\n\nSometimes it can be useful to continue training a model.\nFor example an exception was thrown after epoch 42, you could continue training from the last checkpoint.\nOr you want to fine-tune an existing model by training it on new data.\nNote: the model configuration will be loaded from the checkpoint\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train \\\n    --resume-train-model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header/\" \\\n    --initial-epoch=\"10\" \\\n    --batch-size=\"10\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --resume-train-model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header/\" \\\n    --initial-epoch=\"10\" \\\n    --batch-size=\"10\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --eval-input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --eval-limit=\"100\" \\\n    --eval-batch-size=\"5\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\n### Auto-resume training\n\nAs detailed in the previous section \"Resume training\",\nthere are situations where resuming training can be useful.\nIn particular, when the training process itself is automatically restarted,\nthen it is usually preferable to resume training rather than start it from\nthe beginning. By adding the `--auto-resume` flag, the training will be resume from the\nthe last saved checkpoint. Not surprisingly, saving checkpoints need to be enabled as well.\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train \\\n    --auto-resume \\\n    --batch-size=\"10\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --checkpoint=\"./data/checkpoints/header-model\" \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\n### Transfer learning (experimental)\n\nA limited form of transfer learning is also possible by copying selected layers from a previously trained model. e.g.:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --transfer-source-model-path=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/2020-10-04-delft-grobid-header-biorxiv-no-word-embedding.tar.gz\" \\\n    --transfer-copy-layers=\"char_embeddings=char_embeddings|char_lstm=char_lstm|word_lstm=word_lstm|word_lstm_dense=word_lstm_dense\" \\\n    --transfer-copy-preprocessor-fields=\"vocab_char,feature_preprocessor\" \\\n    --transfer-freeze-layers=\"char_embeddings,char_lstm,word_lstm\" \\\n    --batch-size=\"16\" \\\n    --architecture=\"CustomBidLSTM_CRF\" \\\n    --no-embedding \\\n    --input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/grobid-0.6.1/delft-grobid-0.6.1-header.train.gz\" \\\n    --limit=\"1000\" \\\n    --eval-input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/grobid-0.6.1/delft-grobid-0.6.1-header.test.gz\" \\\n    --eval-limit=\"100\" \\\n    --max-sequence-length=\"1000\" \\\n    --eval-batch-size=\"5\" \\\n    --early-stopping-patience=\"3\" \\\n    --word-lstm-units=\"200\" \\\n    --use-features \\\n    --feature-indices=\"9-25\" \\\n    --max-epoch=\"50\"\n```\n\nOr transfer character weights from a different GROBID model:\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    affiliation-address \\\n    train_eval \\\n    --transfer-source-model-path=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/2020-10-04-delft-grobid-header-biorxiv-no-word-embedding.tar.gz\" \\\n    --transfer-copy-layers=\"char_embeddings=char_embeddings|char_lstm=char_lstm\" \\\n    --transfer-copy-preprocessor-fields=\"vocab_char\" \\\n    --transfer-freeze-layers=\"char_embeddings,char_lstm\" \\\n    --batch-size=\"32\" \\\n    --architecture=\"CustomBidLSTM_CRF\" \\\n    --no-embedding \\\n    --input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/grobid-0.6.1/delft-grobid-0.6.1-affiliation-address.train.gz\" \\\n    --limit=\"1000\" \\\n    --eval-input=\"https://github.com/elifesciences/sciencebeam-datasets/releases/download/grobid-0.6.1/delft-grobid-0.6.1-affiliation-address.test.gz\" \\\n    --eval-limit=\"100\" \\\n    --max-sequence-length=\"100\" \\\n    --eval-batch-size=\"5\" \\\n    --early-stopping-patience=\"5\" \\\n    --word-lstm-units=\"20\" \\\n    --max-epoch=\"50\"\n```\n\n### Training very long sequences\n\nSome training sequences can be very long and may exceed the available memory. This is in particular an issue when training the sequences.\n\nSome approches to deal with the issue.\n\n#### Truncate the sequences to a maximum length\n\nBy passing in the `--max-sequence-length`, sequences are being truncated.\nIn that case the model will not be trained on any data beyond the max sequence length.\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"16\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"100\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\n#### Training using [truncated BPTT](https://en.wikipedia.org/wiki/Backpropagation_through_time#Pseudocode) (Backpropagation through time)\n\nThis requires the LSTMs to be *stateful* (the state from the previous batch is passed on to the next). The `--stateful` flag should be passed in, and the `--input-window-stride` should be the same as `--max-sequence-length`\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"16\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"100\" \\\n    --input-window-stride=\"100\" \\\n    --stateful \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\nUnfortunately the current implementation is very slow and training time might increase significantly.\n\n#### Training using window slices\n\nThe alternative to the above is to not use *stateful* LSTMs but still pass in the input data using sliding windows.\nTo do that, do not pass `--stateful`. But use `--input-window-stride` which is equal or less to `--max-sequence-length`.\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header train_eval \\\n    --batch-size=\"16\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"100\" \\\n    --input-window-stride=\"50\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"100\" \\\n    --early-stopping-patience=\"3\" \\\n    --max-epoch=\"50\"\n```\n\nThis will not allow the LSTM to capture long term dependencies beyond the max sequence length but it will allow it to have seen all of the data, in chunks. Therefore max sequence length should be large enough, which depends on the model.\n\n### Eval Sub Command\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    eval \\\n    --batch-size=\"16\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header/\" \\\n    --limit=\"10\" \\\n    --quiet\n```\n\nThe evaluation format can be changed to `json` using the `--eval-output-format`.\nIt can also be saved using `--eval-output-path`.\n\n### Tag Sub Command\n\nThe `tag` sub command supports multiple output formats (`--tag-output-path`):\n\n- `json`: more detailed tagging output\n- `data`: data output with features but label being replaced by predicted label\n- `text`: not really a tag output as it just outputs the input text\n- `xml`: uses predicted labels as XML elements\n- `xml_diff`: same as `xml` but it is showing a diff between expected and predicted results\n\nThe output will be written to the path specified via `--tag-output-path` if present. Otherwise it will be written to *stdout*.\n\n#### XML Output Example\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    tag \\\n    --batch-size=\"16\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header/\" \\\n    --limit=\"1\" \\\n    --tag-output-format=\"xml\" \\\n    --quiet\n```\n\nWith the result:\n\n```xml\n<xml>\n  <p>\n    <title>Markov Chain Algorithms for Planar Lattice Structures</title>\n    <author>Michael Luby y Dana Randall z Alistair Sinclair</author>\n    <abstract>Abstract Consider the following Markov chain , whose states are all domino tilings of a 2n &#x6EF59; 2n chessboard : starting from some arbitrary tiling , pick a 2 &#x6EF59; 2 window uniformly at random . If the four squares appearing in this window are covered by two parallel dominoes , rotate the dominoes in place . Repeat many times . This process is used in practice to generate a tiling , and is a tool in the study of the combinatorics of tilings and the behavior of dimer systems in statistical physics . Analogous Markov chains are used to randomly generate other structures on various two - dimensional lattices . This paper presents techniques which prove for the &#x6EF59;rst time that , in many interesting cases , a small number of random moves suuce to obtain a uniform distribution .</abstract>\n  </p>\n</xml>\n```\n\n#### XML Diff Output Example\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    tag \\\n    --batch-size=\"16\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header/\" \\\n    --limit=\"2\" \\\n    --tag-output-format=\"xml_diff\" \\\n    --quiet\n```\n\nWith the result (the second document contains differences):\n\n```xml\n  <xml>\n    <p>\n      <title>Markov Chain Algorithms for Planar Lattice Structures</title>\n      <author>Michael Luby y Dana Randall z Alistair Sinclair</author>\n      <abstract>Abstract Consider the following Markov chain , whose states are all domino tilings of a 2n 񮽙 2n chessboard : starting from some arbitrary tiling , pick a 2 񮽙 2 window uniformly at random . If the four squares appearing in this window are covered by two parallel dominoes , rotate the dominoes in place . Repeat many times . This process is used in practice to generate a tiling , and is a tool in the study of the combinatorics of tilings and the behavior of dimer systems in statistical physics . Analogous Markov chains are used to randomly generate other structures on various two - dimensional lattices . This paper presents techniques which prove for the 񮽙rst time that , in many interesting cases , a small number of random moves suuce to obtain a uniform distribution .</abstract>\n    </p>\n  \n  \n    <p>\n      <title>Translucent Sums : A Foundation for Higher - Order Module Systems</title>\n      <author>Mark Lillibridge</author>\n      <date>May , 1997</date>\n-     <pubnum>- - 95 -</pubnum>\n+     <pubnum>- - 95 - of</pubnum>\n?                     +++\n-     <affiliation>of</affiliation>\n    </p>\n  </xml>\n```\n\n#### DATA Output Example\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    tag \\\n    --batch-size=\"16\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header/\" \\\n    --limit=\"1\" \\\n    --tag-output-format=\"data\" \\\n    --quiet \\\n    | head -5\n```\n\nWith the result:\n\n```text\nMarkov markov M Ma Mar Mark v ov kov rkov BLOCKSTART LINESTART NEWFONT HIGHERFONT 0 0 0 INITCAP NODIGIT 0 0 0 0 0 0 0 0 0 0 NOPUNCT 0 0 B-<title>\nChain chain C Ch Cha Chai n in ain hain BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 INITCAP NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<title>\nAlgorithms algorithms A Al Alg Algo s ms hms thms BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 INITCAP NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<title>\nfor for f fo for for r or for for BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 NOCAPS NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<title>\nPlanar planar P Pl Pla Plan r ar nar anar BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 INITCAP NODIGIT 0 0 0 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<title>\n```\n\n#### DATA Unidiff Output Example\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    tag \\\n    --batch-size=\"16\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header/\" \\\n    --limit=\"2\" \\\n    --tag-output-format=\"data_unidiff\" \\\n    --tag-output-path=\"/tmp/test.diff\"\n```\n\nThe output can be viewed using a specialised tool (such as [Kompare](https://en.wikipedia.org/wiki/Kompare)).\n\nExample [unidiff](https://en.wikipedia.org/wiki/Diff#Unified_format) result:\n\n```diff\n--- header_document_000002.expected\n+++ header_document_000002.actual\n@@ -1,21 +1,21 @@\n Translucent translucent T Tr Tra Tran t nt ent cent BLOCKSTART LINESTART NEWFONT HIGHERFONT 1 0 0 INITCAP NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 B-<title>\n Sums sums S Su Sum Sums s ms ums Sums BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 1 0 0 INITCAP NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<title>\n : : : : : : : : : : BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 1 0 0 ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 0 PUNCT 0 0 I-<title>\n A a A A A A A A A A BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 1 0 0 ALLCAP NODIGIT 1 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<title>\n Foundation foundation F Fo Fou Foun n on ion tion BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 1 0 0 INITCAP NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<title>\n for for f fo for for r or for for BLOCKIN LINEEND SAMEFONT SAMEFONTSIZE 1 0 0 NOCAPS NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<title>\n...\n - - - - - - - - - - BLOCKIN LINEEND SAMEFONT SAMEFONTSIZE 0 0 0 ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 1 HYPHEN 0 0 I-<pubnum>\n - - - - - - - - - - BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 1 HYPHEN 0 0 I-<pubnum>\n 95 95 9 95 95 95 5 95 95 95 BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 NOCAPS ALLDIGIT 0 0 0 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<pubnum>\n - - - - - - - - - - BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 ALLCAP NODIGIT 1 0 0 0 0 0 0 0 0 1 HYPHEN 0 0 I-<pubnum>\n-of of o of of of f of of of BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 NOCAPS NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<affiliation>\n+of of o of of of f of of of BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 NOCAPS NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0 I-<pubnum>\n```\n\n#### Text Output Example\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    tag \\\n    --batch-size=\"16\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header/\" \\\n    --limit=\"1\" \\\n    --tag-output-format=\"text\" \\\n    --quiet\n```\n\nWith the result:\n\n```text\nMarkov Chain Algorithms for Planar Lattice Structures Michael Luby y Dana Randall z Alistair Sinclair Abstract Consider the following Markov chain , whose states are all domino tilings of a 2n 񮽙 2n chessboard : starting from some arbitrary tiling , pick a 2 񮽙 2 window uniformly at random . If the four squares appearing in this window are covered by two parallel dominoes , rotate the dominoes in place . Repeat many times . This process is used in practice to generate a tiling , and is a tool in the study of the combinatorics of tilings and the behavior of dimer systems in statistical physics . Analogous Markov chains are used to randomly generate other structures on various two - dimensional lattices . This paper presents techniques which prove for the 񮽙rst time that , in many interesting cases , a small number of random moves suuce to obtain a uniform distribution .\n```\n\n### Wapiti Sub Commands\n\nThe Wapiti sub commands allow to use a similar process for training, evaluating and tagging Wapiti models, as the sub commands for the other DL model(s) above.\n\nCurrently you would need to either install [Wapiti](https://wapiti.limsi.fr/) and make the `wapiti` command available in the path, or use the `--wapiti-install-source` switch to download and install a version from source.\n\n#### Wapiti Train Sub Command\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header wapiti_train \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --wapiti-template=https://raw.githubusercontent.com/kermitt2/grobid/0.5.6/grobid-trainer/resources/dataset/header/crfpp-templates/header.template \\\n    --wapiti-install-source=https://github.com/kermitt2/Wapiti/archive/5f9a52351fddf21916008daa4becd41d56e7f608.tar.gz \\\n    --output=\"data/models\" \\\n    --limit=\"100\" \\\n    --max-epoch=\"10\"\n```\n\n#### Wapiti Train Eval Sub Command\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    header wapiti_train_eval \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --eval-input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --wapiti-template=https://raw.githubusercontent.com/kermitt2/grobid/0.5.6/grobid-trainer/resources/dataset/header/crfpp-templates/header.template \\\n    --output=\"data/models\" \\\n    --limit=\"100\" \\\n    --max-epoch=\"10\"\n```\n\n#### Wapiti Eval Sub Command\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    wapiti_eval \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header\" \\\n    --quiet\n```\n\n#### Wapiti Tag Sub Command\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    wapiti_tag \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.test.gz \\\n    --model-path=\"https://github.com/kermitt2/grobid/raw/0.5.6/grobid-home/models/header\" \\\n    --limit=\"1\" \\\n    --tag-output-format=\"xml_diff\" \\\n    --quiet\n```\n\n### Input Info Sub Command\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    input_info \\\n    --quiet \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz\n```\n\nResult:\n\n```text\nnumber of input sequences: 2538\nsequence lengths: {'q.00': 1, 'q.25': 61.0, 'q.50': 178.0, 'q.75': 300.75, 'q1.0': 6606}\ntoken lengths: {'q.00': 1, 'q.25': 1.0, 'q.50': 3.0, 'q.75': 7.0, 'q1.0': 142}\nnumber of features: 31\ninconsistent feature length counts: {31: 536893, 30: 12855}\nexamples with feature length=31:\ndie D Di Die Die e ie Die Die BLOCKSTART LINESTART NEWFONT HIGHERFONT 0 0 0 INITCAP NODIGIT 0 0 1 0 0 0 0 0 0 0 NOPUNCT 0 0\nabscheidung A Ab Abs Absc g ng ung dung BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 INITCAP NODIGIT 0 0 0 0 0 0 0 0 0 0 NOPUNCT 0 0\nströmender s st str strö r er der nder BLOCKIN LINEIN SAMEFONT SAMEFONTSIZE 0 0 0 NOCAPS NODIGIT 0 0 0 0 0 0 0 0 0 0 NOPUNCT 0 0\nexamples with feature length=30:\ngudina G Gu Gud Gudi a na ina dina BLOCKSTART LINESTART LINEINDENT NEWFONT HIGHERFONT 0 0 0 INITCAP NODIGIT 0 0 0 0 0 0 0 0 NOPUNCT 0 0\net e et et et t et et et BLOCKIN LINEIN LINEINDENT NEWFONT SAMEFONTSIZE 0 0 0 NOCAPS NODIGIT 0 0 0 0 0 0 0 0 NOPUNCT 0 0\nal a al al al l al al al BLOCKIN LINEIN LINEINDENT SAMEFONT SAMEFONTSIZE 0 0 0 NOCAPS NODIGIT 0 1 0 0 0 0 0 0 NOPUNCT 0 0\nfeature value lengths: {0: {'q.00': 1, 'q.25': 1.0, 'q.50': 3.0, 'q.75': 7.0, 'q1.0': 142}, 1: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 2: {'q.00': 1, 'q.25': 1.0, 'q.50': 2.0, 'q.75': 2.0, 'q1.0': 2}, 3: {'q.00': 1, 'q.25': 1.0, 'q.50': 3.0, 'q.75': 3.0, 'q1.0': 3}, 4: {'q.00': 1, 'q.25': 1.0, 'q.50': 3.0, 'q.75': 4.0, 'q1.0': 4}, 5: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 6: {'q.00': 1, 'q.25': 1.0, 'q.50': 2.0, 'q.75': 2.0, 'q1.0': 2}, 7: {'q.00': 1, 'q.25': 1.0, 'q.50': 3.0, 'q.75': 3.0, 'q1.0': 3}, 8: {'q.00': 1, 'q.25': 1.0, 'q.50': 3.0, 'q.75': 4.0, 'q1.0': 4}, 9: {'q.00': 7, 'q.25': 7.0, 'q.50': 7.0, 'q.75': 7.0, 'q1.0': 10}, 10: {'q.00': 6, 'q.25': 6.0, 'q.50': 6.0, 'q.75': 6.0, 'q1.0': 9}, 11: {'q.00': 7, 'q.25': 8.0, 'q.50': 8.0, 'q.75': 8.0, 'q1.0': 8}, 12: {'q.00': 9, 'q.25': 12.0, 'q.50': 12.0, 'q.75': 12.0, 'q1.0': 12}, 13: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 14: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 15: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 16: {'q.00': 6, 'q.25': 6.0, 'q.50': 6.0, 'q.75': 6.0, 'q1.0': 7}, 17: {'q.00': 7, 'q.25': 7.0, 'q.50': 7.0, 'q.75': 7.0, 'q1.0': 14}, 18: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 19: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 20: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 21: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 22: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 23: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 24: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 25: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 26: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 27: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 28: {'q.00': 3, 'q.25': 7.0, 'q.50': 7.0, 'q.75': 7.0, 'q1.0': 11}, 29: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}, 30: {'q.00': 1, 'q.25': 1.0, 'q.50': 1.0, 'q.75': 1.0, 'q1.0': 1}}\nfeature counts: {0: 1000, 1: 247, 2: 1000, 3: 1000, 4: 1000, 5: 265, 6: 1000, 7: 1000, 8: 1000, 9: 3, 10: 3, 11: 2, 12: 3, 13: 2, 14: 2, 15: 2, 16: 3, 17: 3, 18: 2, 19: 2, 20: 2, 21: 1, 22: 1, 23: 2, 24: 2, 25: 1, 26: 2, 27: 2, 28: 8, 29: 1, 30: 1}\nsuggested feature indices: 9-30\nlabel counts: {'B-<title>': 2363, 'I-<title>': 24481, 'B-<author>': 2241, 'I-<author>': 25830, 'B-<reference>': 414, 'I-<reference>': 10121, 'B-<submission>': 409, 'I-<submission>': 3729, 'B-<abstract>': 1528, 'I-<abstract>': 269983, 'B-<affiliation>': 2782, 'I-<affiliation>': 23886, 'B-<address>': 2330, 'I-<address>': 13963, 'B-<date>': 658, 'I-<date>': 2204, 'B-<grant>': 105, 'I-<grant>': 4509, 'B-<email>': 891, 'I-<email>': 7796, 'B-<keyword>': 424, 'I-<keyword>': 7804, 'B-<entitle>': 24, 'I-<entitle>': 421, 'B-<pubnum>': 421, 'I-<pubnum>': 3755, 'B-<note>': 1823, 'I-<note>': 26033, 'B-<copyright>': 281, 'I-<copyright>': 5152, 'B-<date-submission>': 29, 'I-<date-submission>': 166, 'B-<intro>': 439, 'I-<intro>': 96944, 'B-<web>': 187, 'I-<web>': 3162, 'B-<phone>': 71, 'I-<phone>': 710, 'B-<dedication>': 22, 'I-<dedication>': 243, 'B-<degree>': 59, 'I-<degree>': 1355}\n```\n\n### Other CLI Parameters\n\n#### `--log-file`\n\nSpecifying a log file (can also be gzipped by adding the `.gz` extension), will save the logging output to the file. This is mainly intended for cloud usage. Locally you could also use `tee` for that.\n\nIf the specified file is a remote file, then it will be uploaded when the program finishes (no streaming logs).\n\n#### `--notification-url`\n\nFor a long running training process (`train` and `train_eval` or `wapiti_train` and `wapiti_train_eval`), it is possible to get notified via a Webhook URL\n(e.g. [Slack](https://api.slack.com/messaging/webhooks) or [Mattermost](https://docs.mattermost.com/developer/webhooks-incoming.html)).\nIn that case, a message will be sent when the training completes or in case of an error (although not all error may be caught).\n\n### Environment Variables\n\nEnvironment variables can be useful when not directly interacting with the CLI, e.g. via GROBID.\n\nThe following environment variables can be specified:\n\n| Name | Default | Description\n| ---- | ------- | -----------\n| `SCIENCEBEAM_DELFT_MAX_SEQUENCE_LENGTH` | *None* | The maximum sequence length to use, e.g. when tagging.\n| `SCIENCEBEAM_DELFT_INPUT_WINDOW_STRIDE` | *None* | The window stride to use (if any). If the model is stateless, this could be set to the maximum sequence length. Otherwise this could be a set to a value below the maximum sequence length. The difference will be the overlapping window. If no window stride was specified, the sequence will be truncated at the maximum sequence length.\n| `SCIENCEBEAM_DELFT_BATCH_SIZE` | `10` | The batch size to use\n| `SCIENCEBEAM_DELFT_STATEFUL` | *None* (*False*) | Whether to enable stateful mode. This may only work with a batch size of `1`. Note: the stateful mode is currently very slow.\n\n## Training in Google's AI Platform\n\nYou can train a model using Google's [AI Platform](https://cloud.google.com/ai-platform/). e.g.\n\n```bash\ngcloud beta ai-platform jobs submit training \\\n    --job-dir \"gs://your-job-bucket/path\" \\\n    --scale-tier=custom \\\n    --master-machine-type=n1-highmem-8 \\\n    --master-accelerator=count=1,type=NVIDIA_TESLA_K80 \\\n    --region=europe-west1 \\\n    --stream-logs \\\n    --module-name sciencebeam_trainer_delft.sequence_labelling.grobid_trainer \\\n    --package-path sciencebeam_trainer_delft \\\n    -- \\\n    header train_eval \\\n    --batch-size=\"16\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"500\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"10000\" \\\n    --early-stopping-patience=\"10\" \\\n    --max-epoch=\"50\"\n```\n\nOr using the project's wrapper script which provides some default values:\n\n```bash\n./gcloud-ai-platform-submit.sh \\\n    --job-prefix \"my_job_prefix\" \\\n    --job-dir \"gs://your-job-bucket/path\" \\\n    --scale-tier=custom \\\n    --master-machine-type=n1-highmem-8 \\\n    --master-accelerator=count=1,type=NVIDIA_TESLA_K80 \\\n    --region=europe-west1 \\\n    -- \\\n    header train_eval \\\n    --batch-size=\"16\" \\\n    --embedding=\"https://github.com/elifesciences/sciencebeam-models/releases/download/v0.0.1/glove.6B.50d.txt.xz\" \\\n    --max-sequence-length=\"500\" \\\n    --input=https://github.com/elifesciences/sciencebeam-datasets/releases/download/v0.0.1/delft-grobid-0.5.6-header.train.gz \\\n    --limit=\"10000\" \\\n    --early-stopping-patience=\"10\" \\\n    --max-epoch=\"50\"\n```\n\n(Alternatively you can train for free using Google Colab, see Example Notebooks above)\n\n## Text Classification\n\n### Train Text Classification\n\n```bash\npython -m sciencebeam_trainer_delft.text_classification \\\n    train \\\n    --model-path=\"data/models/textClassification/toxic\" \\\n    --train-input-limit=100 \\\n    --train-input=\"https://github.com/kermitt2/delft/raw/v0.2.3/data/textClassification/toxic/train.csv\"\n```\n\n### Eval Text Classification\n\n```bash\npython -m sciencebeam_trainer_delft.text_classification \\\n    eval \\\n    --model-path=\"data/models/textClassification/toxic\" \\\n    --eval-input-limit=100 \\\n    --eval-input=\"https://github.com/kermitt2/delft/raw/v0.2.3/data/textClassification/toxic/test.csv\" \\\n    --eval-label-input=\"https://github.com/kermitt2/delft/raw/v0.2.3/data/textClassification/toxic/test_labels.csv\"\n```\n\n### Predict Text Classification\n\n```bash\npython -m sciencebeam_trainer_delft.text_classification \\\n    predict \\\n    --model-path=\"data/models/textClassification/toxic\" \\\n    --predict-input-limit=100 \\\n    --predict-input=\"https://github.com/kermitt2/delft/raw/v0.2.3/data/textClassification/toxic/test.csv\" \\\n    --predict-output=\"./data/toxic_test_predictions.tsv\"\n```\n\n### Train Eval Text Classification\n\n```bash\npython -m sciencebeam_trainer_delft.text_classification \\\n    train_eval \\\n    --model-path=\"data/models/textClassification/toxic\" \\\n    --train-input-limit=100 \\\n    --train-input=\"https://github.com/kermitt2/delft/raw/v0.2.3/data/textClassification/toxic/train.csv\" \\\n    --eval-input-limit=100 \\\n    --eval-input=\"https://github.com/kermitt2/delft/raw/v0.2.3/data/textClassification/toxic/test.csv\" \\\n    --eval-label-input=\"https://github.com/kermitt2/delft/raw/v0.2.3/data/textClassification/toxic/test_labels.csv\"\n```\n\n## Checkpoints CLI\n\nThe checkpoints CLI tool is there to give you a summary of the saved checkpoints. Checkpoints are optionally saved during training, they allow you to resume model training or further evaluate performance at the individual checkpoints. Usually training will stop after the f1 score hasn't improved for a number of epochs. The last checkpoint may not be the best.\n\nThe checkpoints tool will sort by the f1 score and show the *n* (`limit`) top checkpoints.\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.tools.checkpoints --help\n```\n\n### Checkpoints Text Output\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.tools.checkpoints \\\n    --checkpoint=\"/path/to/checkpoints\" \\\n    --limit=3 \\\n    --output-format=text\n```\n\n```text\nbest checkpoints:\n00039: 0.5877923107411811 (/path/to/checkpoints/epoch-00039) (last)\n\n00036: 0.5899450117831894 (/path/to/checkpoints/epoch-00036)\n\n00034: 0.591387179996031 (/path/to/checkpoints/epoch-00034) (best)\n```\n\n### Checkpoints JSON Output\n\n```bash\npython -m sciencebeam_trainer_delft.sequence_labelling.tools.checkpoints \\\n    --checkpoint=\"/path/to/checkpoints\" \\\n    --limit=3 \\\n    --output-format=json\n```\n\n```json\n[\n  {\n    \"loss\": 40.520591011530236,\n    \"f1\": 0.5877923107411811,\n    \"optimizer\": {\n      \"type\": \"keras.optimizers.Adam\",\n      \"lr\": 0.0010000000474974513\n    },\n    \"epoch\": 39,\n    \"path\": \"/path/to/checkpoints/epoch-00039\",\n    \"is_last\": true,\n    \"is_best\": false\n  },\n  {\n    \"loss\": 44.48661111276361,\n    \"f1\": 0.5899450117831894,\n    \"optimizer\": {\n      \"type\": \"keras.optimizers.Adam\",\n      \"lr\": 0.0010000000474974513\n    },\n    \"epoch\": 36,\n    \"path\": \"/path/to/checkpoints/epoch-00036\",\n    \"is_last\": false,\n    \"is_best\": false\n  },\n  {\n    \"loss\": 47.80826501711393,\n    \"f1\": 0.591387179996031,\n    \"optimizer\": {\n      \"type\": \"keras.optimizers.Adam\",\n      \"lr\": 0.0010000000474974513\n    },\n    \"epoch\": 34,\n    \"path\": \"/path/to/checkpoints/epoch-00034\",\n    \"is_last\": false,\n    \"is_best\": true\n  }\n]\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/elifesciences/sciencebeam-trainer-delft",
    "keywords": "sciencebeam delft",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "sciencebeam-trainer-delft",
    "package_url": "https://pypi.org/project/sciencebeam-trainer-delft/",
    "platform": "",
    "project_url": "https://pypi.org/project/sciencebeam-trainer-delft/",
    "project_urls": {
      "Homepage": "https://github.com/elifesciences/sciencebeam-trainer-delft"
    },
    "release_url": "https://pypi.org/project/sciencebeam-trainer-delft/0.0.31/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "ScienceBeam Trainer DeLFT",
    "version": "0.0.31",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13203606,
  "releases": {
    "0.0.31": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ab2d3ffb539a6e1e7ccef12019d6b8e2975edc3d473d00bf50a229ce41c30549",
          "md5": "9c3ebe38490b7525c64d003aabf726aa",
          "sha256": "3f10b56dbad8d0b08b464268521e02c4754192a87a7d682ccead21f635e94776"
        },
        "downloads": -1,
        "filename": "sciencebeam_trainer_delft-0.0.31.tar.gz",
        "has_sig": false,
        "md5_digest": "9c3ebe38490b7525c64d003aabf726aa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 134570,
        "upload_time": "2021-11-10T18:04:47",
        "upload_time_iso_8601": "2021-11-10T18:04:47.974552Z",
        "url": "https://files.pythonhosted.org/packages/ab/2d/3ffb539a6e1e7ccef12019d6b8e2975edc3d473d00bf50a229ce41c30549/sciencebeam_trainer_delft-0.0.31.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ab2d3ffb539a6e1e7ccef12019d6b8e2975edc3d473d00bf50a229ce41c30549",
        "md5": "9c3ebe38490b7525c64d003aabf726aa",
        "sha256": "3f10b56dbad8d0b08b464268521e02c4754192a87a7d682ccead21f635e94776"
      },
      "downloads": -1,
      "filename": "sciencebeam_trainer_delft-0.0.31.tar.gz",
      "has_sig": false,
      "md5_digest": "9c3ebe38490b7525c64d003aabf726aa",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 134570,
      "upload_time": "2021-11-10T18:04:47",
      "upload_time_iso_8601": "2021-11-10T18:04:47.974552Z",
      "url": "https://files.pythonhosted.org/packages/ab/2d/3ffb539a6e1e7ccef12019d6b8e2975edc3d473d00bf50a229ce41c30549/sciencebeam_trainer_delft-0.0.31.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}