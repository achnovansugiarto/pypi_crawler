{
  "info": {
    "author": "Vincent Vankrunkelsven",
    "author_email": "vincent@datacamp.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# Viewflow\n\nViewflow is a framework built on the top of Airflow that enables data scientists to create materialized views. It allows data scientists to focus on the logic of the view creation in their preferred tool (e.g., SQL, Python).\n\nViewflow automatically creates Airflow DAGs and tasks based on SQL or Python files. \n\nOne of the major features of Viewflow is its ability to manage tasks' dependencies, i.e., views used to create another view. Viewflow can automatically extract from the code (SQL query or Python script) the internal and external dependencies. An internal dependency is a view that belongs to the same DAG as a view being created. An external dependency is a view that belongs to a different DAG. The benefits of automatic dependency management are twofold: First, data scientists don't have to manually list dependencies, usually an error-prone process. Second, it makes sure that no view is built on stale data (because all dependent views will be updated beforehand).\n\nCurrently, Viewflow supports SQL and Python views and PostgreSQL/Redshift as a destination. We will continue improving Viewflow by adding new view types (e.g., R, Jupyter Notebooks, ...) and destination (e.g., Snowflake, BigQuery, ...).\n\nDo you want more context on why we built and released Viewflow? Check out our announcement blog post: [*Data Scientists, donâ€™t worry about data engineering: Viewflow has your back.*](https://medium.com/datacamp-engineering/viewflow-fe07353fa068)!\n\n## Viewflow demo\n\nWe created a demo that shows how Viewflow works. The demo creates two DAGs: `viewflow-demo-1` and `viewflow-demo-2`. These DAGs create a total of four views in a local Postgres database. Check out the view files in [demo/dags/](./demo/dags/). Some of the following commands are different based on which Airflow version you're using. For new users, Airflow 2 is the best option. However, you can also run the demo using the older Airflow 1.10 version by using the indicated commands.\n\n### Run the demo \nWe use `docker-compose` to instantiate an Apache Airflow instance and a Postgres database. The Airflow container and the Postgres container are defined in the `docker-compose-airflow<version>.yml` files. The first time you want to run the demo, you will first have to build the Apache Airflow docker image that embeds Viewflow:\n\n```sh\ndocker-compose -f docker-compose-airflow2.yml build     # Airflow 2\ndocker-compose -f docker-compose-airflow1.10.yml build  # Airflow 1.10\n```\n\nThen run the docker containers:\n```sh\ndocker-compose -f docker-compose-airflow2.yml up     # Airflow 2\ndocker-compose -f docker-compose-airflow1.10.yml up  # Airflow 1.10\n```\n\nGo to your local Apache Airflow instance on [http://localhost:8080](http://localhost:8080). There are two DAGs called `viewflow-demo-1` and `viewflow-demo-2`. Notice how Viewflow automatically generated these DAGs based on the example queries in the subfolders of [demo/dags/](./demo/dags/)!\n\n<img src=\"./img/viewflow-demo-1.png\" width=\"800\">\n\n<img src=\"./img/viewflow-demo-2.png\" width=\"800\">\n\nBy default, the DAGs are disabled. Turn the DAGs on by clicking on the button `Off`. This will trigger the DAGs.\n\n### Query the views\n\nOnce the DAGs have run and all tasks completed, you can query the views created by Viewflow in the local Postgres database created by Docker. You can use any Postgres client (note that Postgres is running locally on port `5433`):\n\n```sh\npsql -h localhost -p 5433 -U airflow -d airflow\n```\n\nUse `airflow` when `psql` asks you for the user password.\n\nThere is a schema named `viewflow_raw` and a schema named `viewflow_demo`. The first one contains three tables: `users`, `courses`, and `user_course`. They are considered as the raw data. The second schema, `viewflow_demo`, is the schema in which the views created by Viewflow are stored. \n\n```sql\n\\dn\n\n+---------------+---------+\n| Name          | Owner   |\n|---------------+---------|\n| public        | airflow |\n| viewflow_demo | airflow |\n| viewflow_raw  | airflow |\n+---------------+---------+\n```\n\nViewflow created four views: `user_xp` (SQL), `user_enriched` (SQL), `course_enriched` (SQL) and `top_3_user_xp` (Python)\n\n```sql\n\\dt viewflow_demo.\n\n+---------------+-----------------+--------+---------+\n| Schema        | Name            | Type   | Owner   |\n|---------------+-----------------+--------+---------|\n| viewflow_demo | course_enriched | table  | airflow |\n| viewflow_demo | top_3_user_xp   | table  | airflow |\n| viewflow_demo | user_enriched   | table  | airflow |\n| viewflow_demo | user_xp         | table  | airflow |\n+---------------+-----------------+--------+---------+\n```\n\nYou can query these tables to see their data:\n```sql\nselect * from viewflow_demo.user_xp;\n\n+-----------+------+-----------------------+\n| user_id   | xp   | __view_generated_at   |\n|-----------+------+-----------------------|\n| 1         | 750  | 2021-03-17            |\n| 2         | 200  | 2021-03-17            |\n| 3         | 550  | 2021-03-17            |\n| 4         | 500  | 2021-03-17            |\n| 5         | 650  | 2021-03-17            |\n| 6         | 430  | 2021-03-17            |\n| 7         | 300  | 2021-03-17            |\n| 8         | 280  | 2021-03-17            |\n| 9         | 100  | 2021-03-17            |\n| 10        | 350  | 2021-03-17            |\n+-----------+------+-----------------------+\n```\n\nYou can also access the tables' comment (both table and columns):\n\n```sql\nselect obj_description('viewflow_demo.user_enriched'::regclass) as view_description;\n\n+---------------------------------------------+\n| view_description                            |\n|---------------------------------------------|\n| A table with enriched information of users  |\n+---------------------------------------------+\n```\n\n```sql\nselect\n   column_name,\n   col_description((table_schema||'.'||table_name)::regclass::oid, ordinal_position) as column_comment\n from\n   information_schema.columns\n where\n   table_schema = 'viewflow_demo'\n and\n   table_name = 'user_enriched';\n\n+--------------------------+-----------------------------------------------+\n| column_name              | column_comment                                |\n|--------------------------+-----------------------------------------------|\n| user_id                  | The user id                                   |\n| xp                       | The user amount of XP                         |\n| last_course_completed_at | When was the last course completed by a user  |\n| last_course_completed    | Name of the latest completed course by a user |\n| number_courses_completed | Number of completed courses by a user         |\n| __view_generated_at      | <null>                                        |\n+--------------------------+-----------------------------------------------+\n```\n\nAnd that's it! Congrats on running the demo :rocket:  If you want to play more with Viewflow, follow the installation instructions below.\n\n## Installation instructions\n\n:envelope: *If you have any issue with the installation, configuration, or creation of your DAGs, do not hesitate to [contact us](./MAINTAINERS.md)!*\n\n\nThe current installation process requires you to install Viewflow from the GitHub repository:\n\n```sh\nRUN pip install git+https://github.com/datacamp/viewflow.git \n```\n\n## Create a new DAG \nViewflow creates the DAGs automatically based on configuration files.\n\nHere are the steps to create a DAG for the first time.\n\n### Create the Viewflow main script\nIn your Airflow DAG directory (usually `$AIRFLOW_HOME/dags`), create a python script called `viewflow-dags.py` that contains the following Python code:\n\n```python\nfrom viewflow import create_dags\n\nDAG = create_dags(\"./dags\", globals(), \"<views_schema_name>\")\n```\n\nThis script is executed by Airflow. It calls the main Viewflow function that creates your DAGs. The first parameter is the directory in which your dag folders are located. The third parameter is the schema name in your data warehouse, where your views will be materialized.\n\n### Create an Airflow connection to your destination\nViewflow needs to know where to write the views. It uses an Airflow connection that is referred to in the view files by specifying a `connection_id`. Currently, Viewflow supports Postgres (or Redshift) data warehouses. Please look at the [Airflow documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/connection.html) to create a Postgres connection.\n\nE.g. the demo's connection is managed using environmemt variables declared in [demo/.env](./demo/.env). This file is the `env_file` specified in the `docker-compose-airflow<version>.yml` files and it allows the scheduler and webserver containers to connect to the Postgres server.\n\n### Create your DAG directories\n\nIn Viewflow, the DAGs are created based on a configuration file and on the SQL and Python files in the same directory. \n\nIn `$AIRFLOW_HOME/dags/`, create a directory called `my-first-viewflow-dag`. In this directory, create a `config.yml` file that contains the following yml fields:\n\n```yml\ndefault_args:\n    owner: <owner>@dag.com\n    retries: 1\nschedule_interval: 0 6 * * *\nstart_date: \"2021-01-01\"\n```\nAdapt the values of each element to what suits you. The `default_args` element contains the Airflow [default DAG  parameters](https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html#default-arguments).\n\nThe `schedule_interval` and `start_date` elements are the Viewflow counterparts of Airflow's `schedule_interval` and `start_date`. \n\nYou can now add your SQL and Python files in this directory (see sections below). This will create a new DAG in Airflow called `my-first-viewflow-dag` that will be triggered every day at 6 AM UTC as of January 1, 2021. All failed tasks will be retried once.\n\n### View metadata\n\nViewflow expects some metadata that must be included in the SQL and Python files (examples follow). Here are the fields that should be included in a `yml` format:\n\n* **owner**: The owner of the view (i.e., who is view responsible). The owner appears in Airflow and allows users to know who they should talk to if they have some questions about the view.\n* **description**: What is the view about. Viewflow uses this field as a view comment in the database. The description can be retrieved in SQL (see Section [*Query the views*](https://github.com/datacamp/viewflow#query-the-views)).\n* **fields (list)**: Description of each column of the view. Viewflow uses these fields as column comments in the database. The column descriptions can be retrieved in SQL (see Section [*Query the views*](https://github.com/datacamp/viewflow#query-the-views)).\n* **schema**: The name of the schema in which Viewflow creates the view. It's also used by Viewflow to create the dependencies.\n* **connection_id**: Airflow connection name used to connect to the database (See Section [*Create an Airflow connection to your destination*](https://github.com/datacamp/viewflow#create-an-airflow-connection-to-your-destination)).\n\n### SQL views\n\nA SQL view is created by a SQL file. This SQL file must contain the SQL query (as a `SELECT` statement) of your view and the view metadata. Here's an example:\n\n```sql\n/* \n---\nowner: name-of-the-view-owner\ndescription: A description of your view. It's used as the view's description in the database\nfields:\n  email: Description of your column -- used as the view column's description in the database\nschema: schema_name_in_your_destination (e.g. viewflow_demo)\nconnection_id: airflow_destination_connection\n--- \n*/\n\nSELECT DISTINCT email FROM viewflow_raw.users\n```\n\n### Python views\n\n*Please note that the implementation of the Python view should be considered as beta. It is a newer implementation of the Python view that we use at DataCamp.*\n\nA Python view is created based on a Python script. This script must contain at least one function with the view's description metadata in its docstring, which returns a Pandas dataframe.\n\nHere's an example of a Python view:\n\n```python\nimport pandas as pd\n\ndef python_view(db_engine):\n    \"\"\"\n    ---\n    owner: name-of-the-view-owner\n    description: A description of your view. It's used as the view's description in the database\n    fields:\n        email: Description of your column -- used as the view column's description in the database\n    schema: schema_name_in_your_destination (e.g. viewflow_demo)\n    connection_id: airflow_destination_connection\n    ---\n    \"\"\"\n    df = pd.read_sql_table(\"users\", db_engine, schema=\"viewflow_raw\")\n    return df[[\"email\"]]\n```\n\nPlease note that Viewflow expects the Python function that creates the view to have the parameter `db_engine` (used to connect to the database). You don't have to set `db_engine` anywhere. Viewflow takes care of setting this variable.\n\n# Contributing to Viewflow\n\nWe welcome all sorts of contributions, be it new features, bug fixes or documentation, we encourage you to create a new PR. To create a new PR or to report new bugs, please read how to [contribute to Viewflow](CONTRIBUTION.md). \n\nIn the remainder of this section, we show you how to prepare your environment to contribute to Viewflow.\n## Install Poetry\n\nSee https://python-poetry.org/docs/#osx-linux-bashonwindows-install-instructions for comprehensive documentation.\n\n`curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python`\n\n## Install the dependencies\n\n\nYou can automatically install the required dependencies by running\n\n```bash\npoetry install\n```\n\nBy default, this will install Airflow 2 and its corresponding dependencies. If you want to use Airflow 1.10, copy the [Airflow@1.10/pyproject.toml](./Airflow@1.10/pyproject.toml) file to the main directory.\n\n## Prepare your environment to run the tests\n\n### Postgres\nUse docker compose to set up a PostgreSQL database locally (password: `passw0rd`):\n\n```bash\ndocker-compose -f docker-compose-test.yml up\n```\n\nIf you get a message saying that port 5432 is in use, it means you have a different PostgreSQL server running on your machine. If you used homebrew to install it, you could use `brew services stop postgresql` to stop the other server.\n\nImport the fixtures into the local database:\n\n```bash\npsql -U user -W -h localhost -f tests/fixtures/load_postgres.sql -d viewflow\n```\n\n### Run Pytest\n\nBefore you can continue, you will need to set up an Airflow SQLite database.\n\n\n```bash\npoetry run airflow db init  # Airflow 2\npoetry run airflow initdb   # Airflow 1.10\n```\n\nIf running into problems, this [link](https://airflow.apache.org/docs/apache-airflow/stable/installation.html#troubleshooting) can be helpful. In particular, it's possible you get a `Symbol not found: _Py_GetArgcArgv` error. This is easily fixed by creating a Python virtual environment as demonstrated in the [link](https://airflow.apache.org/docs/apache-airflow/stable/installation.html#troubleshooting), activating this virtual environment and then running `poetry install` again.\n\nNote for Airflow 1.10.12: if you get an `ImportError`, it can be helpful to refer to this [post](https://stackoverflow.com/questions/64891058/issue-on-airflow-initdb).\n\nAfter setting up the database, run\n\n```bash\npoetry run pytest\n```\n\n\nIn case the database connection is set up incorrectly, run\n\n```bash\npoetry run airflow db reset  # Airflow 2\npoetry run airflow resetdb   # Airflow 1.10\n```\n\n## Viewflow architecture\n\nWe built Viewflow around three main components: the *parser*, the *adapter*, and the *dependency extractor*.\n\nThe *parser* transforms a source file (e.g., SQL, Rmd, Python) that contains the view's metadata (e.g., view's owner, view's descriptions, and column's descriptions) and the view's code into a specific Viewflow data structure. The data structure is used by the other components in the Viewflow architecture: the adapter and the dependency creator. \n\nThe *adapter* is the translation layer of Viewflow's views to their corresponding Airflow counterpart. It uses the data structure objects created by the parser to create an Airflow task object (i.e., an Airflow operator).\n\nFinally, the *dependency extractor* uses the parser's data structure objects to set the internal and external dependencies to the Airflow task object created by the adapter.\n\nThis architecture allows us to add new source file types in the future easily (e.g., Python notebook, R markdown).\n\n# Acknowledgments \n\nToday's version of Viewflow is the result of a joint effort of ex and current DataCampers. We would like to thank in particular the following persons who significantly contributed to Viewflow:\n- [David Robinson](https://github.com/dgrtwo)\n- [Anthony Baker](https://github.com/bakera81)\n- [Michael Chow](https://github.com/machow)\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "viewflow",
    "package_url": "https://pypi.org/project/viewflow/",
    "platform": "",
    "project_url": "https://pypi.org/project/viewflow/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/viewflow/0.1.0/",
    "requires_dist": [
      "apache-airflow (==2.1.1)",
      "apache-airflow-providers-postgres (>=2.0.0,<3.0.0)",
      "psycopg2 (>=2.8.5,<3.0.0)",
      "numpy (>=1.19.1,<2.0.0)",
      "pandas (>=1.1)",
      "boto3 (>=1.14.34,<2.0.0)",
      "fsspec (>=0.8.0,<0.9.0)",
      "ratelimit (>=2.2.1,<3.0.0)",
      "networkx (>=2.5,<3.0)",
      "sqlalchemy (<1.4.0)"
    ],
    "requires_python": ">=3.7,<4.0",
    "summary": "Viewflow is an Airflow-based framework that allows data scientists to create data models without writing Airflow code.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10985983,
  "releases": {
    "0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2429ed0379f298d7600d2db64dcd617daaef7fbf07d7ffb5fdc0b4c9f198369f",
          "md5": "f1162da962987396670949f67b069a0a",
          "sha256": "59dacae3d4457abb7721f6c5dc2619f471433b4c8be991805aabb663e08dc257"
        },
        "downloads": -1,
        "filename": "viewflow-0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f1162da962987396670949f67b069a0a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 886,
        "upload_time": "2019-05-09T23:25:44",
        "upload_time_iso_8601": "2019-05-09T23:25:44.904103Z",
        "url": "https://files.pythonhosted.org/packages/24/29/ed0379f298d7600d2db64dcd617daaef7fbf07d7ffb5fdc0b4c9f198369f/viewflow-0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "09444a7f1319135aea23abb7e92a57b9c31321d3fee7412e9f2f93c521eff840",
          "md5": "7edc761d5adc085cb3a28423411c0046",
          "sha256": "514dec1a4ac31c6f64ed68e91b80692e7b9b40c899c799606c45f92316bab639"
        },
        "downloads": -1,
        "filename": "viewflow-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7edc761d5adc085cb3a28423411c0046",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<4.0",
        "size": 17484,
        "upload_time": "2021-07-23T12:01:48",
        "upload_time_iso_8601": "2021-07-23T12:01:48.586416Z",
        "url": "https://files.pythonhosted.org/packages/09/44/4a7f1319135aea23abb7e92a57b9c31321d3fee7412e9f2f93c521eff840/viewflow-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fa6263736e40e9728924e7c8a7e0a51c713b3c794e49dff554be02e134fb2eb0",
          "md5": "17712972162c7dd8ed347c7d8bf93233",
          "sha256": "f27ff3422dc25ee3d177632e5167277b88f214264210d24f35d1de37c699c97d"
        },
        "downloads": -1,
        "filename": "viewflow-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "17712972162c7dd8ed347c7d8bf93233",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<4.0",
        "size": 21004,
        "upload_time": "2021-07-23T12:01:50",
        "upload_time_iso_8601": "2021-07-23T12:01:50.610834Z",
        "url": "https://files.pythonhosted.org/packages/fa/62/63736e40e9728924e7c8a7e0a51c713b3c794e49dff554be02e134fb2eb0/viewflow-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "09444a7f1319135aea23abb7e92a57b9c31321d3fee7412e9f2f93c521eff840",
        "md5": "7edc761d5adc085cb3a28423411c0046",
        "sha256": "514dec1a4ac31c6f64ed68e91b80692e7b9b40c899c799606c45f92316bab639"
      },
      "downloads": -1,
      "filename": "viewflow-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "7edc761d5adc085cb3a28423411c0046",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7,<4.0",
      "size": 17484,
      "upload_time": "2021-07-23T12:01:48",
      "upload_time_iso_8601": "2021-07-23T12:01:48.586416Z",
      "url": "https://files.pythonhosted.org/packages/09/44/4a7f1319135aea23abb7e92a57b9c31321d3fee7412e9f2f93c521eff840/viewflow-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fa6263736e40e9728924e7c8a7e0a51c713b3c794e49dff554be02e134fb2eb0",
        "md5": "17712972162c7dd8ed347c7d8bf93233",
        "sha256": "f27ff3422dc25ee3d177632e5167277b88f214264210d24f35d1de37c699c97d"
      },
      "downloads": -1,
      "filename": "viewflow-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "17712972162c7dd8ed347c7d8bf93233",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7,<4.0",
      "size": 21004,
      "upload_time": "2021-07-23T12:01:50",
      "upload_time_iso_8601": "2021-07-23T12:01:50.610834Z",
      "url": "https://files.pythonhosted.org/packages/fa/62/63736e40e9728924e7c8a7e0a51c713b3c794e49dff554be02e134fb2eb0/viewflow-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}