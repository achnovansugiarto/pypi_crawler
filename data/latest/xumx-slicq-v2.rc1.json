{
  "info": {
    "author": "Sevag Hanssian",
    "author_email": "sevagh@pm.me",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "<!--\n# META:TODO\n\n1. upload pip wheel\n1. git tag with \"1.0.0\" if pip wheel is good\n-->\n\n# xumx-sliCQ-V2\n\n<!--[![PyPI Wheel](https://img.shields.io/pypi/v/openunmix.svg)](https://pypi.python.org/pypi/openunmix)\n[![arXiv](https://img.shields.io/badge/arXiv-2112.05509-b31b1b.svg)](https://arxiv.org/abs/2112.05509)-->\n\nxumx-sliCQ-V2 is a PyTorch neural network for music demixing, trained only on [MUSDB18-HQ](https://zenodo.org/record/3338373).\n\nIt demixes a musical mixture into stems (vocals/drums/bass/other) by masking the magnitude spectrogram. The code is based on [Open-Unmix (UMX)](https://github.com/sigsep/open-unmix-pytorch) with some key differences:\n1. Spectral transform: sliced Constant-Q Transform (sliCQT) with the Bark scale vs. STFT\n1. Neural network architecture: convolutional denoising autoencoder (CDAE) vs. dense + Bi-LSTM\n1. All targets are trained together with combined loss functions like [CrossNet-Open-Unmix (X-UMX)](https://github.com/sony/ai-research-code/blob/master/x-umx/x-umx.md)\n1. Differentiable Wiener filtering is vendored under `xumx_slicq_v2.norbert` from [this PyTorch fork](https://github.com/yoyololicon/norbert) of the [sigsep/norbert library](https://github.com/sigsep/norbert)\n\n**xumx-sliCQ-V2 scores a total SDR of 4.4 dB with 60 MB\\* of pretrained weights for all targets** on the MUSDB18-HQ test set.\n\n**The realtime model scores 4.07 dB and is light and fast!** It takes an average of 2 seconds to demix a song with a GPU and 11 seconds with a CPU using PyTorch.<sup>†</sup> The provided ONNX model optimizes the performance further, taking 7 seconds on the CPU.\n\nBoth variants beat the 3.6 dB score of the original [xumx-sliCQ](https://github.com/sevagh/xumx-sliCQ) (28 MB) with the improvements [described here](#improvements-over-xumx-slicq). It also brings the performance closer to the 4.64 dB and 5.54 dB scored by UMX and X-UMX (137 MB) respectively.<sup>‡</sup>\n\n<!--\nCite xumx-sliCQ-V2:\n```\n(TODO latex citation block here)\nwrite arxiv paper\n```-->\n\n<sub>\n\n\\*: Pretrained weights for xumx-sliCQ-V2 are stored in this repo with Git LFS: [offline](./pretrained_model), [realtime](./pretrained_model_realtime)\n\n</sub>\n\n<sub>\n\n†: See the [inference section below](#run) for measurements\n\n</sub>\n\n<sub>\n\n‡: UMX and X-UMX were independently re-evaluated as part of xumx-sliCQ: [1](https://github.com/sevagh/xumx_slicq_extra/blob/main/old-latex/mdx-submissions21/paper.md#results), [2](https://github.com/sevagh/xumx_slicq_extra)\n\n</sub>\n\n### Roadmap\n\n* Submit paper to arXiv\n* Submission to [Cadenza Challenge](http://cadenzachallenge.org/)\n\n## Key concepts\n\nBark-scale sliCQT to better represent musical signals with a nonuniform time-frequency resolution compared to the fixed resolution STFT:\n\n![slicq-spectral](.github/slicq_spectral.png)\n\nConvolutional network applied to ragged sliCQT (time-frequency blocks with different frame rates):\n\n![slicq-diagram](.github/slicq_diagram.png)\n\n## Usage\n\n### Prerequisites\n\nYou need Python + pip >= 3.8 or Docker. To use your GPU, you need the NVIDIA CUDA Toolkit and an NVIDIA CUDA-capable GPU. For Docker + GPU, you also need the [nvidia-docker 2.0 runtime](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker). For your own training, tuning, and evaluation, you need the [MUSDB18-HQ dataset](https://zenodo.org/record/3338373).\n\n### Install\n\nIf you want to use Docker (**recommended**), git clone the source code and build the container:\n```\n$ git clone https://github.com/sevagh/xumx-sliCQ-V2\n$ cd ./xumx-sliCQ-V2\n$ docker build -t \"xumx-slicq-v2\" .\n```\n\nThe container is based on the [NVIDIA PyTorch NGC Container](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch) to include features and optimizations for to NVIDIA GPUs, such as automatic [TF32 for Ampere+](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/), [bfloat16 support for Ampere+](https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html), and more.\n\nTo dynamically update the source code in the container while you develop new features, you can volume mount the local checkout of xumx-sliCQ-V2 to `:/xumx-sliCQ-V2`. If not, the container will use a frozen copy of the source code when you built the image.\n\nI only provide Docker instructions; pip instructions are coming soon!\n<!--```\n# basic inference\n$ pip install 'xumx_slicq_v2 @ git+ssh://git@github.com/sevagh/xumx-sliCQ-V2'\n\n# training, tuning, development, etc.\n$ pip install 'xumx_slicq_v2[devel] @ git+ssh://git@github.com/sevagh/xumx-sliCQ-V2'\n```-->\n\n<details>\n<summary>List of all scripts</summary>\n\n| Script | Description | Device |\n|:-|:-|:-|\n| For end users | |\n| xumx_slicq_v2.inference | Demix mixed songs | CPU **or** CUDA GPU |\n| For developers | |\n| xumx_slicq_v2.evaluation | Evaluate pretrained networks | CPU |\n| xumx_slicq_v2.training | Train the network | CUDA GPU |\n| xumx_slicq_v2.optuna | Optuna hyperparam tuning | CUDA GPU |\n| xumx_slicq_v2.slicqfinder | Random sliCQT param search | CPU **or** CUDA GPU |\n| xumx_slicq_v2.visualization | Generate spectrograms | CPU |\n| xumx_slicq_v2.export_onnx | Generate ONNX model for optimized inference | CPU |\n\nIf you installed the package with pip, run them like `python -m xumx_slicq_v2.$script_name`.\n\n</details>\n\n### Run\n\n**Pip**: coming soon! I need to add code to download the pretrained weights from GitHub once I make the repo public\n\n**Docker**: run inference to generate outputs on a folder containing mixed song wav files:\n\n```\n$ docker run --rm -it \\\n    -v /path/to/input/songs/:/input \\\n    -v /path/to/demixed/outputs:/output \\\n    python -m xumx_slicq_v2.inference --help\n\n#add below lines for gpu support\n#--gpus=all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n#python -m xumx_slicq_v2.inference --cuda\n```\n\nInference time for the realtime variant is **2x faster** than the offline model, and **3x faster with ONNX**, measuring the average time taken to demix the 50 MUSDB18-HQ test tracks:\n| Model | Device | Inference time (s, avg per track) |\n|:-|:-|:-|\n| Realtime | GPU | 2.08 |\n| Realtime (ONNXRuntime) | CPU | 6.9 |\n| Realtime | CPU | 11.35 |\n| Offline | CPU | 23.17 |\n\n<details>\n<summary>Optimizing inference</summary>\n\nThe offline model has to trade off speed and memory usage from the embedded Wiener-EM step, so I only use it for offline CPU inference. The embedded Wiener-EM filtering step from the Norbert library also introduces additional complexity (complex numbers, etc.) for ONNX exporting.\n\nThe ONNX optimizations could be taken further with more effort and/or modifying the xumx-sliCQ-V2 code:\n* Improving the CUDA performance\n* Enhancing CUDA with the TensorRT provider\n* Enhancing CPU performance with the OpenVino provider\n\n</details>\n\n<details>\n<summary>Training</summary>\n\n```\n$ docker run --rm -it \\\n    --gpus=all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n    -v /path/to/MUSDB18-HQ/dataset:/MUSDB18-HQ \\\n    -v /path/to/save/trained/model:/model \\\n    -p 6006:6006 \\\n    xumx-slicq-v2 \\\n    python -m xumx_slicq_v2.training --help\n```\n\nThe Tensorboard training web dashboard is launched by the training script: <http://127.0.0.1:6006/>.\n\n**To persist the model**, you can volume mount a host volume to `:/model` (as in the command above). Killing and relaunching the container with a persisted model will continue the training process. If not, the trained model will disappear when the container is killed.\n\nThe lowest lost achieved (complex cross-target MSE + mask sum MSE loss) was 0.0405 at epoch 198. The average epoch time was around 170 seconds, or just under 3 minutes, with a batch size of 64 (and 8 cpu workers for the dataloader).\n\nThe lowest lost achieved for the realtime model was 0.0437 at epoch 161. The average epoch time was around 110 seconds, or just under 2 minutes, with a batch size of 64 (and 8 cpu workers for the dataloader).\n\n</details>\n\n<details>\n<summary>Hyperparameter tuning</summary>\n\n```\n$ docker run --rm -it \\\n    --gpus=all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n    -v /path/to/MUSDB18-HQ/dataset:/MUSDB18-HQ \\\n    -p 6006:6006 \\\n    xumx-slicq-v2 \\\n    python -m xumx_slicq_v2.optuna --help\n```\n\nThe Optuna tuning script runs on a cut-down training and validation dataset, and minimizes the SDR score achieved by the model within 10 epochs per trial. It runs for 100 trials and was used to discover improved hyperparameters for xumx-sliCQ-V2 ([read more here](#improvements-over-xumx-slicq)).\n\nThe Optuna tuning web dashboard is launched by the tuning script: <http://127.0.0.1:6006/>.\n\n</details>\n\n<details>\n<summary>Evaluation</summary>\n\n```\n$ docker run --rm -it \\\n    -v /path/to/MUSDB18-HQ/dataset:/MUSDB18-HQ \\\n    xumx-slicq-v2 \\\n    python -m xumx_slicq_v2.evaluation --help\n```\n\nBy default, the pretrained model will be evaluated. **Pass different models to evaluate** as a path inside the container relative to the source code dir:\n```\n$ docker run --rm -it \\\n    -v /path/to/MUSDB18-HQ/dataset:/MUSDB18-HQ \\\n    -v /path/to//xumx-sliCQ-V2/source/code:/xumx-sliCQ-V2/ \\\n    xumx-slicq-v2 \\\n    python -m xumx_slicq_v2.evaluation \\\n    --model-path='/xumx-sliCQ-V2/model-to-evaluate'\n```\n\nThis takes ~2-3 hours to run on all 50 test tracks of MUSDB18-HQ on my CPU (5950X + 64GB RAM). It will output the BSS scores of each track, and at the end, output the median score across all frames and tracks:\n```\nloading separator\nscale=bark, fbins=262, fmin=32.90, fmax=22050.00, sllen=18060, trlen=4516\n  0%|                                                                              | 0/50 [00:00<?, ?it/s]track: AM Contra - Heart Peripheral\ngetting audio\napplying separation\nn chunks: 4\n...\n<output truncated>\n...\nvocals          ==> SDR:   4.791  SIR:   7.794  ISR:   8.579  SAR:   4.500\ndrums           ==> SDR:   4.846  SIR:   8.062  ISR:   8.649  SAR:   4.953\nbass            ==> SDR:   4.690  SIR:   8.778  ISR:   5.558  SAR:   4.193\nother           ==> SDR:   3.273  SIR:   2.532  ISR:   8.065  SAR:   4.422\n```\nTo get the total SDR, simply sum the four target SDRs and divide by 4:\n```\nSDR_tot = (SDR_vocals + SDR_drums + SDR_bass + SDR_other)/4.0\n```\n\n</details>\n\n## Theory\n\n### Motivation\n\nThe sliced Constant-Q Transform (sliCQT) is a realtime implementation of the Nonstationary Gabor Transform (NSGT), which is a generalized nonuniform time-frequency transform with perfect inverse. Nonuniform time-frequency transforms are better suited to representing sounds with time-varying frequencies, such as music and speech. The STFT is limited due to its use of fixed windows and the time-frequency uncertainty principle of Gabor.\n\nThe NSGT can be used to implement a Constant-Q Transform (logarithmic scale), but it can use any type of frequency scale. In xumx-sliCQ and xumx-sliCQ-V2, the same Bark scale is used (262 Bark frequency bins from 32.9-22050 Hz).\n\n<details>\n<summary>Past work</summary>\n\nIn 2021, I worked on xumx-sliCQ (V1), [the first variant](https://github.com/sevagh/xumx-sliCQ), to submit to the MDX 21 ([Music Demixing Challenge 2021](https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021) on AICrowd), and got my paper published to [the MDX 21 workshop](https://mdx-workshop.github.io/proceedings/hanssian.pdf) at ISMIR 2021 (and [arXiv](https://arxiv.org/abs/2112.05509)). The time-frequency uncertainty principle aligned with my desired thesis topic at the Music Technology Master's program at McGill.\n\nIn 2023, I chose to revisit the code of xumx-sliCQ for submission to the [First Cadenza Challenge (CAD1)](http://cadenzachallenge.org/), which is a music demixing challenge with the additional context of hearing loss and accessibility. Nonuniform time-frequency transforms, like the sliCQT, are related to the nolinear human auditory system, and I had specific auditory motivations for choosing the Bark scale for the sliCQT in xumx-sliCQ.\n\n</details>\n\n### Improvements over xumx-sliCQ\n\n<details>\n<summary>Performance tuning</summary>\n\nFirst, I improved a lot of sloppy non-neural network code. The embedded [nsgt library](./xumx_slicq_v2/nsgt), which provides the sliCQT (and originates from <https://github.com/sevagh/nsgt>, and before that, the source <https://github.com/grrrr/nsgt>), had a lot of calls to NumPy after my initial conversion to PyTorch, leading to unnecessary host-device communication throughout an epoch trained on my GPU.\n\nNext, I focused on making my epochs faster. The faster I can train it, the more I can work on xumx-sliCQ-V2 within a given time frame. To get the most out of the PyTorch code and my NVIDIA Ampere GPU (3090), I used two resources:\n* Using the NVIDIA PyTorch Docker container (`nvcr.io/nvidia/pytorch:22.12-py3`) as the base for my training container to take advantage of implicit speedups provided by NVIDIA (e.g. automatically-enabled [TF32](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/_))\n* Modifying my PyTorch code according to the [performance tuning guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n\nThe code changes were the following:\n* In the model code:\n    1. `bias=False` for every conv layer that was followed by a batch norm:\n        ```\n        encoder.extend([\n            Conv2d(\n                hidden_size_1,\n                hidden_size_2,\n                (freq_filter, time_filter_2),\n                bias=False,\n            ),\n            BatchNorm2d(hidden_size_2),\n            ReLU(),\n        ])\n        ```\n* In the training code:\n    1. Set the model `.to(memory_format=torch.channels_last)`\n    1. Enable cuDNN benchmarking\n        ```\n        torch.backends.cudnn.benchmark = True\n        ```\n    1. Forcing some additional more TF32-related settings:\n        ```\n        torch.backends.cudnn.allow_tf32 = True\n        ```\n    1. Using AMP (Automatic Mixed Precision) with bfloat16 (on CUDA and CPU) (greatly reduces memory during training, allowing a larger batch size):\n        ```\n        with torch.autocast(\"cuda\", dtype=torch.bfloat16),\n                torch.autocast(\"cpu\", dtype=torch.bfloat16):\n        ```\n\nAn epoch takes ~170s (train + validation) on my RTX 3090 with 24GB of GPU memory with `--batch-size=64 --nb-workers=8`. xumx-sliCQ by contrast took 350s per epoch with a batch size of 32 on an RTX 3080 Ti (which had 12GB GPU memory, half of my 3090). However, the old code used PyTorch 1.10, so the upgrade of V2 to 1.13 may also be contributing to improved performance.\n\n</details>\n\n<details>\n<summary>Using the full frequency bandwidth</summary>\n\nIn xumx-sliCQ, I didn't use frequency bins above 16,000 Hz in the neural network; the demixing was only done on the frequency bins lower than that limit, copying the `umx` pretrained model of UMX. UMX's other pretrained model, `umxhq`, uses the full spectral bandwidth. In xumx-sliCQ-V2, I removed the bandwidth parameter to pass all the frequency bins of the sliCQT through the neural network.\n\n</details>\n\n<details>\n<summary>Removing dilations from the convolution layers</summary>\n\nIn the CDAE of xumx-sliCQ, I used a dilation of 2 in the time axis to arbitrarily increase the receptive field without paying attention to music demixing quality (because dilations sound cool).\n\nIn xumx-sliCQ-V2, I didn't use any dilations since I had no reason to.\n\n</details>\n\n<details>\n<summary>Removing the inverse sliCQT and time-domain SDR loss</summary>\n\nIn xumx-sliCQ, I applied the mixed-domain SDR and MSE loss of X-UMX. However, due to the large computational graph introduced by the inverse sliCQT operation, I was disabling its gradient:\n```\nX = slicqt(x)\nXmag = torch.abs(X)\nYmag_est = unmix(Xmag)\nYcomplex_est = mix_phase(torch.angle(X), Ymag_est)\n\nwith torch.no_grad():\n     y_est = islicqt(Ycomplex_est)\n```\n\nWithout this, the epoch time goes from 1-5 minutes to 25+ minutes, making training unfeasible. However, by disabling the gradient, the SDR loss can't influence the network performance. In practice, I found that the MSE was an acceptable correlate to SDR performance, and dropped the isliCQT and SDR loss calculation.\n\n</details>\n\n<details>\n<summary>Replacing the overlap-add with pure convolutional layers</summary>\n\nA quirk of the sliCQT is that rather than the familiar 2 dimensions of time and frequency, it has 3 dimensions: slice, time-per-slice, and frequency. Adjacent slices have a 50% overlap with one another and must be summed to get the true spectrogram in a destructive operation (50% of the time coefficients are lost, with no inverse).\n\nIn xumx-sliCQ, an extra transpose convolutional layer with stride 2 is used to grow the time coefficients back to the original size after the 4-layer CDAE, to undo the destruction of the overlap-add.\n\nIn xumx-sliCQ-V2, the first convolutional layer takes the overlap into account by setting the kernel and stride to the window and hop size of the destructive overlap-add. The result is that the input is downsampled in a way that is recovered by the final transpose convolution layer in the 4-layer CDAE, eliminating the need for an extra upsampling layer.\n\nDiagram (shown for one time-frequency block):\n![slicq-overlap-improved](.github/slicq_overlap_improved.png)\n\nBy this point, I had a model that scored **4.1 dB** with 28 MB of weights using magnitude MSE loss.\n\n</details>\n\n<details>\n<summary>Differentiable Wiener-EM and complex MSE</summary>\n\nBorrowing from [Danna-Sep](https://github.com/yoyololicon/danna-sep), one of the [top performers in the MDX 21 challenge](https://github.com/yoyololicon/music-demixing-challenge-ismir-2021-entry), the differentiable Wiener-EM step is used inside the neural network during training, such that the output of xumx-sliCQ-V2 is a complex sliCQT, and the complex MSE loss function is used instead of the magnitude MSE loss. Wiener-EM is applied separately in each frequency block as shown in the [architecture diagram at the top of the README](#key-concepts).\n\nThis got the score to **4.24 dB** with 28 MB of weights trained with complex MSE loss (0.0395).\n\nIn xumx-sliCQ, Wiener-EM was only applied in the STFT domain as a post-processing step. The network was trained using magnitude MSE loss. The waveform estimate of xumx-sliCQ combined the estimate of the target magnitude with the phase of the mix (noisy phase or mix phase).\n\n</details>\n\n<details>\n<summary>Discovering hyperparameters with Optuna</summary>\n\nUsing the included [Optuna tuning script](./xumx_slicq_v2/tuning.py), new hyperparameters that gave the highest SDR after cut-down training/validation epochs were:\n* Changing the hidden sizes (channels) of the 2-layer CDAE from 25,55 to 50,51 (increased the model size from ~28-30MB to 60MB)\n* Changing the size of the time filter in the 2nd layer from 3 to 4\n\nNote that:\n* The time kernel and stride of the first layer uses the window and hop size related to the overlap-add procedure, so it's not a tunable hyperparameter\n* The ragged nature of the sliCQT makes it tricky to modify frequency kernel sizes (since the time-frequency bins can vary in their frequency bins, from 1 single frequency up to 86), so I kept those fixed from xumx-sliCQ\n* The sliCQT params could be considered a hyperparameter, but the shape of the sliCQT modifies the network architecture, so for simplicity I kept it the same as xumx-sliCQ (262 bins, Bark scale, 32.9-22050 Hz)\n\nThis got the score to **4.35 dB** with 60 MB of weights trained with complex MSE loss of 0.0390.\n\n</details>\n\n<details>\n<summary>Mask sum MSE loss</summary>\n\nIn spectrogram masking approaches to music demixing, commonly a ReLU or Sigmoid activation function is applied as the final activation layer to produce a non-negative mask for the mix magnitude spectrogram. In xumx-sliCQ, I used a Sigmoid activation in the final layer (UMX uses a ReLU). The final mask is multiplied with the input mixture:\n```\nmix = x.clone()\n\n# x is a mask\nx = cdae(x)\n\n# apply the mask, i.e. multiplicative skip connection\nx = x*mix\n```\n\nSince the mask for each target is between [0, 1], and the targets must add up to the mix, then the masks must add up to exactly 1:\n```\ndrum_mask*mix + vocals_mask*mix + other_mask*mix + bass_mask*mix = mix\ndrum_mask + vocals_mask + other_mask + bass_mask = 1.0\n```\n\nIn xumx-sliCQ-V2, I added a second loss term called the mask sum loss, which is the MSE between the sum of the four target masks and a matrix of 1s. This needs a small code change where both the complex slicqt (after Wiener-EM) and the sigmoid masks are returned in the training loop.\n\nThis got the score to **4.4 dB** with 60 MB of weights trained with complex MSE loss + mask sum loss of 0.0405.\n\n</details>\n\n<details>\n<summary>Realtime variant</summary>\n\nFor a future realtime demixing project, I decided to create a realtime variant of xumx-sliCQ-V2. To support realtime inputs:\n* I added left padding of the first convolution layer, such that the intermediate representations throughout the autoencoder are only derived from causal inputs\n* I replaced the resource-intensive Wiener-EM target maximization with the naive mix-phase approach, which is computationally much lighter (simply combine the target magnitude slicqt with the phase of the mix)\n\n</details>\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/sevagh/xumx-sliCQ-V2",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "xumx-slicq-v2",
    "package_url": "https://pypi.org/project/xumx-slicq-v2/",
    "platform": null,
    "project_url": "https://pypi.org/project/xumx-slicq-v2/",
    "project_urls": {
      "Homepage": "https://github.com/sevagh/xumx-sliCQ-V2"
    },
    "release_url": "https://pypi.org/project/xumx-slicq-v2/1.0.0a0/",
    "requires_dist": [
      "torch (>=1.13.1)",
      "torchaudio (>=0.13.1)",
      "numpy",
      "tqdm",
      "requests",
      "scipy",
      "tensorboard ; extra == 'devel'",
      "torchinfo ; extra == 'devel'",
      "scikit-learn ; extra == 'devel'",
      "auraloss ; extra == 'devel'",
      "optuna ; extra == 'devel'",
      "optuna-dashboard ; extra == 'devel'",
      "matplotlib ; extra == 'devel'",
      "musdb (==0.3.1) ; extra == 'devel'",
      "museval (==0.3.1) ; extra == 'devel'",
      "musdb (==0.3.1) ; extra == 'musdb'",
      "museval (==0.3.1) ; extra == 'musdb'",
      "onnx ; extra == 'onnxruntime-cpu'",
      "onnxruntime ; extra == 'onnxruntime-cpu'",
      "onnx ; extra == 'onnxruntime-cuda'",
      "onnxruntime-gpu ; extra == 'onnxruntime-cuda'",
      "tensorrt ; extra == 'onnxruntime-cuda'"
    ],
    "requires_python": ">=3.8",
    "summary": "V2 of my original sliCQT adaptation of Open-Unmix",
    "version": "1.0.0a0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17339538,
  "releases": {
    "1.0.0a0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cd815f02081a2e46b996498a18fd4e797ad62008b5c34658c531af9aaa6ac056",
          "md5": "12e9340940ec9dd64509ea913b255763",
          "sha256": "08857896beb4c83822dac8c9ffc8324ecb7dd7b98605f8bd33119da82df76fdc"
        },
        "downloads": -1,
        "filename": "xumx_slicq_v2-1.0.0a0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "12e9340940ec9dd64509ea913b255763",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 62067,
        "upload_time": "2023-03-17T21:22:52",
        "upload_time_iso_8601": "2023-03-17T21:22:52.096540Z",
        "url": "https://files.pythonhosted.org/packages/cd/81/5f02081a2e46b996498a18fd4e797ad62008b5c34658c531af9aaa6ac056/xumx_slicq_v2-1.0.0a0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cd815f02081a2e46b996498a18fd4e797ad62008b5c34658c531af9aaa6ac056",
        "md5": "12e9340940ec9dd64509ea913b255763",
        "sha256": "08857896beb4c83822dac8c9ffc8324ecb7dd7b98605f8bd33119da82df76fdc"
      },
      "downloads": -1,
      "filename": "xumx_slicq_v2-1.0.0a0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "12e9340940ec9dd64509ea913b255763",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 62067,
      "upload_time": "2023-03-17T21:22:52",
      "upload_time_iso_8601": "2023-03-17T21:22:52.096540Z",
      "url": "https://files.pythonhosted.org/packages/cd/81/5f02081a2e46b996498a18fd4e797ad62008b5c34658c531af9aaa6ac056/xumx_slicq_v2-1.0.0a0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}