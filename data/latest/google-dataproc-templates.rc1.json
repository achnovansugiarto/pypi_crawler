{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "![Build Status](https://dataproctemplatesci.com/buildStatus/icon?job=dataproc-templates-build%2Fbuild-job-python&&subject=python-build)\n\n# Dataproc Templates (Python - PySpark)\n* [BigQueryToGCS](/python/dataproc_templates/bigquery#bigquery-to-gcs) (blogpost [link](https://medium.com/google-cloud/moving-data-from-bigquery-to-gcs-using-gcp-dataproc-serverless-and-pyspark-f6481b86bcd1))\n* [CassandraToBigquery](/python/dataproc_templates/cassandra#cassandra-to-bigquery)\n* [CassandraToGCS](/python/dataproc_templates/cassandra#cassandra-to-gcs)\n* [GCSToBigQuery](/python/dataproc_templates/gcs#gcs-to-bigquery) (blogpost [link](https://medium.com/@ppaglilla/getting-started-with-dataproc-serverless-pyspark-templates-e32278a6a06e))\n* [GCSToBigTable](/python/dataproc_templates/gcs#gcs-to-bigtable) (blogpost [link](https://medium.com/google-cloud/pyspark-load-data-from-gcs-to-bigtable-using-gcp-dataproc-serverless-c373430fe157))\n* [GCSToGCS](/python/dataproc_templates/gcs#gcs-to-gcs---sql-transformation)(blogpost [link](https://medium.com/@ankuljain/migrate-gcs-to-gcs-using-dataproc-serverless-3b7b0f6ad6b9))\n* [GCSToJDBC](/python/dataproc_templates/gcs#gcs-to-jdbc) (blogpost [link](https://medium.com/google-cloud/import-data-from-gcs-to-jdbc-databases-using-dataproc-serverless-c7154b242430))\n* [GCSToMongo](/python/dataproc_templates/gcs#gcs-to-mongodb) (blogpost [link](https://medium.com/google-cloud/importing-data-from-gcs-to-mongodb-using-dataproc-serverless-fed58904633a))\n* [HbaseToGCS](/python/dataproc_templates/hbase#hbase-to-gcs)\n* [HiveToBigQuery](/python/dataproc_templates/hive#hive-to-bigquery) (blogpost [link](https://medium.com/google-cloud/processing-data-from-hive-to-bigquery-using-pyspark-and-dataproc-serverless-217c7cb9e4f8))\n* [HiveToGCS](/python/dataproc_templates/hive#hive-to-gcs)(blogpost [link](https://medium.com/@surjitsh/processing-large-data-tables-from-hive-to-gcs-using-pyspark-and-dataproc-serverless-35d3d16daaf))\n* [JDBCToBigQuery](/python/dataproc_templates/jdbc#3-jdbc-to-bigquery) (blogpost [link](https://medium.com/@sjlva/python-fast-export-large-database-tables-using-gcp-serverless-dataproc-bfe77a132485))\n* [JDBCToGCS](/python/dataproc_templates/jdbc#2-jdbc-to-gcs) (blogpost [link](https://medium.com/google-cloud/importing-data-from-databases-into-gcs-via-jdbc-using-dataproc-serverless-f330cb0160f0))\n* [JDBCToJDBC](/python/dataproc_templates/jdbc#1-jdbc-to-jdbc) (blogpost [link](https://medium.com/google-cloud/migrating-data-from-one-databases-into-another-via-jdbc-using-dataproc-serverless-c5336c409b18))\n* [KafkaToGCS](/python/dataproc_templates/kafka/#kafka-to-gcs)\n* [KafkaToBigQuery](/python/dataproc_templates/kafka/#kafka-to-bigquery)\n* [MongoToGCS](/python/dataproc_templates/mongo#mongo-to-gcs)(blogpost [link](https://medium.com/google-cloud/exporting-data-from-mongodb-to-gcs-buckets-using-dataproc-serverless-64830fb15b51))\n* [RedshiftToGCS](/python/dataproc_templates/redshift#redshift-to-gcs)(blogpost [link](https://medium.com/google-cloud/exporting-data-from-redshift-to-gcs-using-gcp-dataproc-serverless-and-pyspark-9ab78de11405))\n* [S3ToBigQuery](/python/dataproc_templates/s3#amazon-s3-to-bigquery)\n* [SnowflakeToGCS](/python/dataproc_templates/snowflake#1-snowflake-to-gcs)(blogpost [link](https://medium.com/@varunikagupta96/exporting-data-from-snowflake-to-gcs-using-pyspark-on-dataproc-serverless-363d3bed551b))\n* [TextToBigQuery](/python/dataproc_templates/gcs#text-to-bigquery)\n\n\nDataproc Templates (Python - PySpark) submit jobs to Dataproc Serverless using [batches submit pyspark](https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/pyspark).\n\n## Run using PyPi package\n\nIn this README, you see instructions on how to submit Dataproc Serverless template jobs.  \nCurrently, 3 options are described:\n- Using bin/start.sh\n- Using gcloud CLI\n- Using Vertex AI\n\nThose 3 options require you to clone this repo and start running the templates.  \nThe [Dataproc Templates PyPi package](https://pypi.org/project/google-dataproc-templates) is a **4th option** to run templates from a PySpark environment directly (Dataproc or local/another).  \nExample:  \n\n```\n!pip3 install --user google-dataproc-templates==0.0.3\n\nfrom dataproc_templates.bigquery.bigquery_to_gcs import BigQueryToGCSTemplate\nfrom pyspark.sql import SparkSession\n\nargs = dict()\nargs[\"bigquery.gcs.input.table\"] = \"<bq_dataset>.<bq_table>\"\nargs[\"bigquery.gcs.input.location\"] = \"<location>\"\nargs[\"bigquery.gcs.output.format\"] = \"<format>\"\nargs[\"bigquery.gcs.output.mode\"] = \"<mode>\"\nargs[\"bigquery.gcs.output.location\"] = \"gs://<bucket_name/path>\"\n\nspark = SparkSession.builder \\\n        .appName(\"BIGQUERYTOGCS\") \\\n        .enableHiveSupport() \\\n        .getOrCreate()\n\ntemplate = BigQueryToGCSTemplate()\ntemplate.run(spark, args)\n```\n\n**Pro Tip**: [Start a Dataproc Serverless Spark sessions](https://cloud.google.com/vertex-ai/docs/workbench/managed/serverless-spark#start_a_spark_session) in a Vertex AI managed notebook, and leverage a serverless Spark session, in which your job will run using Dataproc Serverless, instead of your local PySpark environment.\n\nWhile this provides an easy way to get started, remember that the bin/start.sh already provides an easy way for you to, for example, specify required .jar dependencies. Using the PyPi package, you need to configure your PySpark sessions in accordance with the requirements of your specific template. You would need to, for example, specify the spark.driver.extraClassPath configuration:\n\n```\nspark = SparkSession.builder \\\n        ... \\\n        .config('spark.driver.extraClassPath', '<template_required_dependency>.jar')\n        ... \\\n        .getOrCreate()\n```\n\n## Setting up the local environment\n\nIt is recommended to use a [virtual environment](https://docs.python.org/3/library/venv.html) when setting up the local environment. This setup is not required for submitting templates, only for running and developing locally.\n\n``` bash\n# Create a virtual environment, activate it and install requirements\nmkdir venv\npython -m venv venv/\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n## Running unit tests\n\nUnit tests are developed using [`pytest`](https://docs.pytest.org/en/7.1.x/).\n\nTo run all unit tests, simply run pytest:\n\n``` bash\npytest\n```\n\nTo generate a coverage report, run the tests using coverage\n\n``` bash\ncoverage run \\\n  --source=dataproc_templates \\\n  --module pytest \\\n  --verbose \\\n  test\n\ncoverage report --show-missing\n```\n\n## Submitting templates to Dataproc Serverless\n\nA shell script is provided to:\n- Build the python package\n- Set Dataproc parameters based on environment variables\n- Submit the desired template to Dataproc with the provided template parameters\n\n<hr>\n\nWhen submitting, there are 3 types of properties/parameters for the user to provide.  \n- **Spark properties**: Refer to this [documentation](https://cloud.google.com/dataproc-serverless/docs/concepts/properties) to see the available spark properties.\n- **Each template's specific parameters**: refer to each template's README.\n- **Common arguments**: --template_name and --log_level\n  - The **--log_level** parameter is optional, it defaults to INFO.\n    - Possible choices are the Spark log levels: [\"ALL\", \"DEBUG\", \"ERROR\", \"FATAL\", \"INFO\", \"OFF\", \"TRACE\", \"WARN\"].\n\n\n\n\n\n<hr>\n\n**bin/start.sh usage**:\n\n```\n# Set required environment variables\nexport GCP_PROJECT=<project_id>\nexport REGION=<region>\nexport GCS_STAGING_LOCATION=<gs://path>\n\n# Set optional environment variables\nexport SUBNET=<subnet>\nexport JARS=\"gs://additional/dependency.jar\"\nexport HISTORY_SERVER_CLUSTER=projects/{projectId}/regions/{regionId}/clusters/{clusterId}\nexport METASTORE_SERVICE=projects/{projectId}/locations/{regionId}/services/{serviceId}\n\n# Submit to Dataproc passing template parameters\n./bin/start.sh [--properties=<spark.something.key>=<value>] \\\n               -- --template=TEMPLATENAME \\\n                  --log_level=INFO \\\n                  --my.property=\"<value>\" \\\n                  --my.other.property=\"<value>\"\n                  (etc...)\n```\n\n**gcloud CLI usage**:\n\nIt is also possible to submit jobs using the `gcloud` CLI directly. That can be achieved by:\n\n1. Building the `dataproc_templates` package into an `.egg`\n\n``` bash\nPACKAGE_EGG_FILE=dist/dataproc_templates_distribution.egg\npython setup.py bdist_egg --output=${PACKAGE_EGG_FILE}\n```\n\n2. Submitting the job\n  * The `main.py` file should be the main python script\n  * The `.egg` file for the package must be bundled using the `--py-files` flag\n\n```\ngcloud dataproc batches submit pyspark \\\n      --region=<region> \\\n      --project=<project_id> \\\n      --jars=\"<required_jar_dependencies>\" \\\n      --deps-bucket=<gs://path> \\\n      --subnet=<subnet> \\\n      --py-files=${PACKAGE_EGG_FILE} \\\n      [--properties=<spark.something.key>=<value>] \\\n      main.py \\\n      -- --template=TEMPLATENAME \\\n         --log_level=INFO \\\n         --<my.property>=\"<value>\" \\\n         --<my.other.property>=\"<value>\"\n         (etc...)\n```\n\n**Vertex AI usage**:\n\nFollow [Dataproc Templates (Jupyter Notebooks) README](../notebooks/README.md) to submit Dataproc Templates from a Vertex AI notebook.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/GoogleCloudPlatform/dataproc-templates",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "google-dataproc-templates",
    "package_url": "https://pypi.org/project/google-dataproc-templates/",
    "platform": "Posix; MacOS X; Windows",
    "project_url": "https://pypi.org/project/google-dataproc-templates/",
    "project_urls": {
      "Homepage": "https://github.com/GoogleCloudPlatform/dataproc-templates"
    },
    "release_url": "https://pypi.org/project/google-dataproc-templates/0.1.0b0/",
    "requires_dist": [
      "pyspark (>=3.2.0)",
      "google-cloud-bigquery (>=3.4.0)"
    ],
    "requires_python": ">=3.7",
    "summary": "Google Dataproc templates written in Python",
    "version": "0.1.0b0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17480917,
  "releases": {
    "0.1.0b0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8852f7c5f9e0e794ffba9fc1b51409c922c3a11c2e90c679b87064ea1556e6f7",
          "md5": "55cd68ebf1d6d4ba485525e175dd3b86",
          "sha256": "8d0812d6feed3f474ac5c34b5b2e13532aa78873868fdbb0d5db784072f2177c"
        },
        "downloads": -1,
        "filename": "google_dataproc_templates-0.1.0b0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "55cd68ebf1d6d4ba485525e175dd3b86",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.7",
        "size": 90079,
        "upload_time": "2023-03-17T13:18:46",
        "upload_time_iso_8601": "2023-03-17T13:18:46.201354Z",
        "url": "https://files.pythonhosted.org/packages/88/52/f7c5f9e0e794ffba9fc1b51409c922c3a11c2e90c679b87064ea1556e6f7/google_dataproc_templates-0.1.0b0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f0f1c1fcaaf81436dced62f3a364d54f825e76ea2274518a0cb53f7fecff6827",
          "md5": "aac559fe641ee36f9bcd2547fa4e0323",
          "sha256": "d2381a27d49e1991fcce025d13aa5d74724cf88e4bc8cf10f733866953fab4cb"
        },
        "downloads": -1,
        "filename": "google-dataproc-templates-0.1.0b0.tar.gz",
        "has_sig": false,
        "md5_digest": "aac559fe641ee36f9bcd2547fa4e0323",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 38883,
        "upload_time": "2023-03-17T13:18:48",
        "upload_time_iso_8601": "2023-03-17T13:18:48.868698Z",
        "url": "https://files.pythonhosted.org/packages/f0/f1/c1fcaaf81436dced62f3a364d54f825e76ea2274518a0cb53f7fecff6827/google-dataproc-templates-0.1.0b0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8852f7c5f9e0e794ffba9fc1b51409c922c3a11c2e90c679b87064ea1556e6f7",
        "md5": "55cd68ebf1d6d4ba485525e175dd3b86",
        "sha256": "8d0812d6feed3f474ac5c34b5b2e13532aa78873868fdbb0d5db784072f2177c"
      },
      "downloads": -1,
      "filename": "google_dataproc_templates-0.1.0b0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "55cd68ebf1d6d4ba485525e175dd3b86",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": ">=3.7",
      "size": 90079,
      "upload_time": "2023-03-17T13:18:46",
      "upload_time_iso_8601": "2023-03-17T13:18:46.201354Z",
      "url": "https://files.pythonhosted.org/packages/88/52/f7c5f9e0e794ffba9fc1b51409c922c3a11c2e90c679b87064ea1556e6f7/google_dataproc_templates-0.1.0b0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f0f1c1fcaaf81436dced62f3a364d54f825e76ea2274518a0cb53f7fecff6827",
        "md5": "aac559fe641ee36f9bcd2547fa4e0323",
        "sha256": "d2381a27d49e1991fcce025d13aa5d74724cf88e4bc8cf10f733866953fab4cb"
      },
      "downloads": -1,
      "filename": "google-dataproc-templates-0.1.0b0.tar.gz",
      "has_sig": false,
      "md5_digest": "aac559fe641ee36f9bcd2547fa4e0323",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 38883,
      "upload_time": "2023-03-17T13:18:48",
      "upload_time_iso_8601": "2023-03-17T13:18:48.868698Z",
      "url": "https://files.pythonhosted.org/packages/f0/f1/c1fcaaf81436dced62f3a364d54f825e76ea2274518a0cb53f7fecff6827/google-dataproc-templates-0.1.0b0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}