{
  "info": {
    "author": "",
    "author_email": "Jinfeng Li <jinfeng@nvidia.com>, Bobby Wang <bobwang@nvidia.com>, Erik Ordentlich <eordentlich@nvidia.com>, Lee Yang <leey@nvidia.com>",
    "bugtrack_url": null,
    "classifiers": [
      "Environment :: GPU :: NVIDIA CUDA :: 11",
      "Environment :: GPU :: NVIDIA CUDA :: 11.5",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# Spark Rapids ML (Python)\n\nThis PySpark-compatible API leverages the RAPIDS cuML python API to provide GPU-accelerated implementations of many common ML algorithms.  These implementations are adapted to use PySpark for distributed training and inference.\n\n## Installation\n\nFor simplicity, the following instructions just use Spark local mode, assuming a server with at least one GPU.\n\nFirst, install RAPIDS cuML per [these instructions](https://rapids.ai/start.html).\n```bash\nconda create -n rapids-23.02 \\\n    -c rapidsai -c nvidia -c conda-forge \\\n    cuml=23.02 python=3.8 cudatoolkit=11.5\n```\n\n**Note**: while testing, we recommend using conda or docker to simplify installation and isolate your environment while experimenting.  Once you have a working environment, you can then try installing directly, if necessary.\n\n**Note**: you can select the latest version compatible with your environment from [rapids.ai](https://rapids.ai/start.html#get-rapids).\n\nOnce you have the conda environment, activate it and install the required packages.\n```bash\nconda activate rapids-23.02\n\ngit clone --branch spark-cuml https://github.com/NVIDIA/spark-rapids-ml.git\ncd spark-rapids-ml/python\npip install -r requirements.txt\npip install -e .\n```\n\n## Examples\n\nThese examples demonstrate the API using toy datasets.  However, GPUs are more effective when using larger datasets that require extensive compute.  So once you are confident in your environment setup, use a more representative dataset for your specific use case to gauge how GPUs can improve performance.\n\n### PySpark shell\n\n#### Linear Regression\n```python\n## pyspark --master local[*]\n# from pyspark.ml.regression import LinearRegression\nfrom spark_rapids_ml.regression import LinearRegression\nfrom pyspark.ml.linalg import Vectors\n\ndf = spark.createDataFrame([\n     (1.0, 2.0, Vectors.dense(1.0, 0.0)),\n     (0.0, 2.0, Vectors.dense(0.0, 1.0))], [\"label\", \"weight\", \"features\"])\n\n# number of partitions should match number of GPUs in Spark cluster\ndf = df.repartition(1)\n\nlr = LinearRegression(regParam=0.0, solver=\"normal\")\nlr.setMaxIter(5)\nlr.setRegParam(0.0)\nlr.setFeaturesCol(\"features\")\nlr.setLabelCol(\"label\")\n\nmodel = lr.fit(df)\n\nmodel.coefficients\n# DenseVector([0.5, -0.5])\n```\n\n#### K-Means\n```python\n## pyspark --master local[*]\n# from pyspark.ml.clustering import KMeans\nfrom spark_rapids_ml.clustering import KMeans\nfrom pyspark.ml.linalg import Vectors\ndata = [(Vectors.dense([0.0, 0.0]), 2.0), (Vectors.dense([1.0, 1.0]), 2.0),\n        (Vectors.dense([9.0, 8.0]), 2.0), (Vectors.dense([8.0, 9.0]), 2.0)]\ndf = spark.createDataFrame(data, [\"features\", \"weighCol\"])\n\n# number of partitions should match number of GPUs in Spark cluster\ndf = df.repartition(1)\n\nkmeans = KMeans(k=2)\nkmeans.setSeed(1)\nkmeans.setMaxIter(20)\nkmeans.setFeaturesCol(\"features\")\n\nmodel = kmeans.fit(df)\n\ncenters = model.clusterCenters()\nprint(centers)\n# [array([0.5, 0.5]), array([8.5, 8.5])]\n\nmodel.setPredictionCol(\"newPrediction\")\ntransformed = model.transform(df)\ntransformed.show()\n# +--------+----------+-------------+\n# |weighCol|  features|newPrediction|\n# +--------+----------+-------------+\n# |     2.0|[0.0, 0.0]|            1|\n# |     2.0|[1.0, 1.0]|            1|\n# |     2.0|[9.0, 8.0]|            0|\n# |     2.0|[8.0, 9.0]|            0|\n# +--------+----------+-------------+\nrows[0].newPrediction == rows[1].newPrediction\n# True\nrows[2].newPrediction == rows[3].newPrediction\n# True\n```\n\n#### PCA\n```python\n## pyspark --master local[*]\n# from pyspark.ml.feature import PCA\nfrom spark_rapids_ml.feature import PCA\n\ndata = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\ndf = spark.createDataFrame(data,[\"features\"])\n\n# number of partitions should match number of GPUs in Spark cluster\ndf = df.repartition(1)\n\npca = PCA(k=2, inputCol=\"features\")\npca.setOutputCol(\"pca_features\")\n\nmodel = pca.fit(df)\n\nmodel.setOutputCol(\"output\")\nmodel.transform(df).collect()[0].output\n# [-1.6485728230896184, -4.013282697765595]\nmodel.explainedVariance\n# DenseVector([0.7944, 0.2056])\nmodel.pc\n# DenseMatrix(5, 2, [0.4486, -0.133, 0.1252, -0.2165, 0.8477, -0.2842, -0.0562, 0.7636, -0.5653, -0.1156], False)\n```\n\n### Jupyter Notebooks\nTo run the example notebooks locally, see [these instructions](../notebooks/README.md).\n\nTo run the example notebooks in Databricks (assuming you already have a Databricks account), follow [these instructions](../notebooks/databricks/README.md).\n\n## API Compatibility\n\nWhile the Spark Rapids ML API attempts to mirror the PySpark ML API to minimize end-user code changes, the underlying implementations are entirely different, so there are some differences.\n- **Unsupported ML Params** - some PySpark ML algorithms have ML Params which do not map directly to their respective cuML implementations.  For these cases, the ML Param default values will be ignored, and if explicitly set by end-user code:\n    - a warning will be printed (for non-critical cases that should have minimal impact, e.g. `initSteps`).\n    - an exception will be raised (for critical cases that can greatly affect results, e.g. `weightCol`).\n- **Unsupported methods** - some PySpark ML methods may not map to the underlying cuML implementations, or may not be meaningful for GPUs.  In these cases, an error will be raised if the method is invoked.\n- **cuML parameters** - there may be additional cuML-specific parameters which might be useful for optimizing GPU performance.  These can be supplied to the various class constructors, but they are _not_ exposed via getters and setters to avoid any confusion with the PySpark ML Params.  If needed, they can be observed via the `cuml_params` attribute.\n- **Algorithmic Results** - again, since the GPU implementations are entirely different from their PySpark ML CPU counterparts, there may be slight differences in the produced results.  This can be due to various reasons, including different optimizations, randomized initializations, or algorithm design.  While these differences should be relatively minor, they should still be reviewed in the context of your specific use case to see if they are within acceptable limits.\n\n**Example**\n```python\n# from pyspark.ml.clustering import KMeans\nfrom spark_rapids_ml.clustering import KMeans\nform pyspark.ml.linalg import Vectors\n\ndata = [(Vectors.dense([0.0, 0.0]), 2.0), (Vectors.dense([1.0, 1.0]), 2.0),\n        (Vectors.dense([9.0, 8.0]), 2.0), (Vectors.dense([8.0, 9.0]), 2.0)]\ndf = spark.createDataFrame(data, [\"features\", \"weighCol\"]).repartition(1)\n\n# `k` is a Spark ML Param, `max_samples_per_batch` is a cuML parameter\nkmeans = KMeans(k=3, max_samples_per_batch=16384)\nkmeans.setK(2)  # setter is available for `k`, but not for `max_samples_per_batch`\nkmeans.setInitSteps(10)  # non-critical unsupported param, prints a warning\n# kmeans.setWeightCol(\"weight\")  # critical unsupported param, raises an error\n\n# show cuML-specific parameters\nprint(kmeans.cuml_params)\n# {'n_clusters': 2, 'max_iter': 20, 'tol': 0.0001, 'verbose': False, 'random_state': 1909113551, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 16384}\n\nmodel = kmeans.fit(df)\n\nsample = df.head().features  # single example\n# unsupported method, raises an error, since not optimal use-case for GPUs\n# model.predict(sample)\n\ncenters = model.clusterCenters()\nprint(centers)  # slightly different results\n# [[8.5, 8.5], [0.5, 0.5]]\n# PySpark: [array([0.5, 0.5]), array([8.5, 8.5])]\n```\n\n## API Documentation\n\n- [Spark Rapids ML](https://nvidia.github.io/spark-rapids-ml/)\n- [PySpark ML](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ml.html)\n- [RAPIDS cuML](https://docs.rapids.ai/api/cuml/stable/api.html)\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "spark-rapids-ml",
    "package_url": "https://pypi.org/project/spark-rapids-ml/",
    "platform": null,
    "project_url": "https://pypi.org/project/spark-rapids-ml/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/spark-rapids-ml/23.2.0/",
    "requires_dist": null,
    "requires_python": ">=3.8",
    "summary": "Apache Spark integration with RAPIDS and cuML",
    "version": "23.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17522150,
  "releases": {
    "23.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "24fbffe1e995cd75e4aa247765caa783fa61fc9b4ef0ecb420b00e013d02a406",
          "md5": "1bf9f1804d793a041b47cf3da9207501",
          "sha256": "6c55b1e543f47d7bbe077bfa7aaa7e74d5aa876d6cf5a441d64583ca93857249"
        },
        "downloads": -1,
        "filename": "spark_rapids_ml-23.2.0-11_ab575bc-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1bf9f1804d793a041b47cf3da9207501",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 48340,
        "upload_time": "2023-03-31T06:25:56",
        "upload_time_iso_8601": "2023-03-31T06:25:56.317562Z",
        "url": "https://files.pythonhosted.org/packages/24/fb/ffe1e995cd75e4aa247765caa783fa61fc9b4ef0ecb420b00e013d02a406/spark_rapids_ml-23.2.0-11_ab575bc-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "24fbffe1e995cd75e4aa247765caa783fa61fc9b4ef0ecb420b00e013d02a406",
        "md5": "1bf9f1804d793a041b47cf3da9207501",
        "sha256": "6c55b1e543f47d7bbe077bfa7aaa7e74d5aa876d6cf5a441d64583ca93857249"
      },
      "downloads": -1,
      "filename": "spark_rapids_ml-23.2.0-11_ab575bc-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1bf9f1804d793a041b47cf3da9207501",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 48340,
      "upload_time": "2023-03-31T06:25:56",
      "upload_time_iso_8601": "2023-03-31T06:25:56.317562Z",
      "url": "https://files.pythonhosted.org/packages/24/fb/ffe1e995cd75e4aa247765caa783fa61fc9b4ef0ecb420b00e013d02a406/spark_rapids_ml-23.2.0-11_ab575bc-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}