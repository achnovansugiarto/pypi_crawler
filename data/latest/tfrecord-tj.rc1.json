{
  "info": {
    "author": "Tao Jiang",
    "author_email": "tjiang.work@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# TFRecord reader and writer\n\nThis library allows reading and writing tfrecord files efficiently in python. The library also provides an IterableDataset reader of tfrecord files for PyTorch. Currently uncompressed and compressed gzip TFRecords are supported.\n\n## Installation\n\n```pip3 install tfrecord```\n\n## Usage\n\nIt's recommended to create an index file for each TFRecord file. Index file must be provided when using multiple workers, otherwise the loader may return duplicate records.\n```\npython3 -m tfrecord.tools.tfrecord2idx <tfrecord path> <index path>\n```\n\n## Reading & Writing tf.train.Example\n\n### Reading tf.Example records in PyTorch\nUse TFRecordDataset to read TFRecord files in PyTorch.\n\n```python\nimport torch\nfrom tfrecord_tj.torch.dataset import TFRecordDataset\n\ntfrecord_path = \"/tmp/data.tfrecord_tj\"\nindex_path = None\ndescription = {\"image\": \"byte\", \"label\": \"float\"}\ndataset = TFRecordDataset(tfrecord_path, index_path, description)\nloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n\ndata = next(iter(loader))\nprint(data)\n```\n\nUse MultiTFRecordDataset to read multiple TFRecord files. This class samples from given tfrecord files with given probability.\n\n```python\nimport torch\nfrom tfrecord_tj.torch.dataset import MultiTFRecordDataset\n\ntfrecord_pattern = \"/tmp/{}.tfrecord_tj\"\nindex_pattern = \"/tmp/{}.index\"\nsplits = {\n    \"dataset1\": 0.8,\n    \"dataset2\": 0.2,\n}\ndescription = {\"image\": \"byte\", \"label\": \"int\"}\ndataset = MultiTFRecordDataset(tfrecord_pattern, index_pattern, splits, description)\nloader = torch.utils.data.DataLoader(dataset, batch_size=32)\n\ndata = next(iter(loader))\nprint(data)\n```\n\n### Infinite and finite PyTorch dataset\n\nBy default, `MultiTFRecordDataset` is infinite, meaning that it samples the data forever. You can make it finite by providing the appropriate flag\n```\ndataset = MultiTFRecordDataset(..., infinite=False)\n```\n\n### Shuffling the data\n\nBoth TFRecordDataset and MultiTFRecordDataset automatically shuffle the data when you provide a queue size.\n```\ndataset = TFRecordDataset(..., shuffle_queue_size=1024)\n```\n\n### Transforming input data\n\nYou can optionally pass a function as `transform` argument to perform post processing of features before returning. \nThis can for example be used to decode images or normalize colors to a certain range or pad variable length sequence.\n\n```python\nimport tfrecord_tj\nimport cv2\n\n\ndef decode_image(features):\n    # get BGR image from bytes\n    features[\"image\"] = cv2.imdecode(features[\"image\"], -1)\n    return features\n\n\ndescription = {\n    \"image\": \"bytes\",\n}\n\ndataset = tfrecord_tj.torch.TFRecordDataset(\"/tmp/data.tfrecord_tj\",\n                                            index_path=None,\n                                            description=description,\n                                            transform=decode_image)\n\ndata = next(iter(dataset))\nprint(data)\n```\n\n### Writing tf.Example records in Python\n\n```python\nimport tfrecord_tj\n\nwriter = tfrecord_tj.TFRecordWriter(\"/tmp/data.tfrecord_tj\")\nwriter.write({\n    \"image\": (image_bytes, \"byte\"),\n    \"label\": (label, \"float\"),\n    \"index\": (index, \"int\")\n})\nwriter.close()\n```\n\n### Reading tf.Example records in Python\n\n```python\nimport tfrecord_tj\n\nloader = tfrecord_tj.tfrecord_loader(\"/tmp/data.tfrecord_tj\", None, {\n    \"image\": \"byte\",\n    \"label\": \"float\",\n    \"index\": \"int\"\n})\nfor record in loader:\n    print(record[\"label\"])\n```\n\n## Reading & Writing tf.train.SequenceExample\n\nSequenceExamples can be read and written using the same methods shown above with an extra argument\n(`sequence_description` for reading and `sequence_datum` for writing) which cause the respective\nread/write functions to treat the data as a SequenceExample.\n\n### Writing SequenceExamples to file\n\n```python\nimport tfrecord_tj\n\nwriter = tfrecord_tj.TFRecordWriter(\"/tmp/data.tfrecord_tj\")\nwriter.write({'length': (3, 'int'), 'label': (1, 'int')},\n             {'tokens': ([[0, 0, 1], [0, 1, 0], [1, 0, 0]], 'int'), 'seq_labels': ([0, 1, 1], 'int')})\nwriter.write({'length': (3, 'int'), 'label': (1, 'int')},\n             {'tokens': ([[0, 0, 1], [1, 0, 0]], 'int'), 'seq_labels': ([0, 1], 'int')})\nwriter.close()\n```\n\n### Reading SequenceExamples in python\n\nReading from a SequenceExample yeilds a tuple containing two elements.\n\n```python\nimport tfrecord_tj\n\ncontext_description = {\"length\": \"int\", \"label\": \"int\"}\nsequence_description = {\"tokens\": \"int\", \"seq_labels\": \"int\"}\nloader = tfrecord_tj.tfrecord_loader(\"/tmp/data.tfrecord_tj\", None,\n                                     context_description,\n                                     sequence_description=sequence_description)\n\nfor context, sequence_feats in loader:\n    print(context[\"label\"])\n    print(sequence_feats[\"seq_labels\"])\n```\n\n### Read SequenceExamples in PyTorch\n\nAs described in the section on `Transforming Input`, one can pass a function as the `transform` argument to\nperform post processing of features. This should be used especially for the sequence features as these are\nvariable length sequence and need to be padded out before being batched.\n\n```python\nimport torch\nimport numpy as np\nfrom tfrecord_tj.torch.dataset import TFRecordDataset\n\nPAD_WIDTH = 5\n\n\ndef pad_sequence_feats(data):\n    context, features = data\n    for k, v in features.items():\n        features[k] = np.pad(v, ((0, PAD_WIDTH - len(v)), (0, 0)), 'constant')\n    return (context, features)\n\n\ncontext_description = {\"length\": \"int\", \"label\": \"int\"}\nsequence_description = {\"tokens\": \"int \", \"seq_labels\": \"int\"}\ndataset = TFRecordDataset(\"/tmp/data.tfrecord_tj\",\n                          index_path=None,\n                          description=context_description,\n                          transform=pad_sequence_feats,\n                          sequence_description=sequence_description)\nloader = torch.utils.data.DataLoader(dataset, batch_size=32)\ndata = next(iter(loader))\nprint(data)\n```\n\nAlternatively, you could choose to implement a custom `collate_fn` in order to assemble the batch,\nfor example, to perform dynamic padding.\n\n```python\nimport torch\nimport numpy as np\nfrom tfrecord_tj.torch.dataset import TFRecordDataset\n\n\ndef collate_fn(batch):\n    from torch.utils.data._utils import collate\n    from torch.nn.utils import rnn\n    context, feats = zip(*batch)\n    feats_ = {k: [torch.Tensor(d[k]) for d in feats] for k in feats[0]}\n    return (collate.default_collate(context),\n            {k: rnn.pad_sequence(f, True) for (k, f) in feats_.items()})\n\n\ncontext_description = {\"length\": \"int\", \"label\": \"int\"}\nsequence_description = {\"tokens\": \"int \", \"seq_labels\": \"int\"}\ndataset = TFRecordDataset(\"/tmp/data.tfrecord_tj\",\n                          index_path=None,\n                          description=context_description,\n                          transform=pad_sequence_feats,\n                          sequence_description=sequence_description)\nloader = torch.utils.data.DataLoader(dataset, batch_size=32, collate_fn=collate_fn)\ndata = next(iter(loader))\nprint(data)\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/seamanj/tfrecord_tj.git",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tfrecord-tj",
    "package_url": "https://pypi.org/project/tfrecord-tj/",
    "platform": "",
    "project_url": "https://pypi.org/project/tfrecord-tj/",
    "project_urls": {
      "Homepage": "https://github.com/seamanj/tfrecord_tj.git"
    },
    "release_url": "https://pypi.org/project/tfrecord-tj/0.0.3/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "TFRecord reader",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13079218,
  "releases": {
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "750c2edd38790eccc18b43b90981f4cce25fdf5375ee1e4d867862093ffdd802",
          "md5": "9acf375eb5d72d574b4ddd73731fa79e",
          "sha256": "e57d0b43091cfe45522917df3a1f2afc867941acae5928ac3fa305cd28802d5f"
        },
        "downloads": -1,
        "filename": "tfrecord_tj-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "9acf375eb5d72d574b4ddd73731fa79e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 15709,
        "upload_time": "2022-03-04T12:45:30",
        "upload_time_iso_8601": "2022-03-04T12:45:30.070173Z",
        "url": "https://files.pythonhosted.org/packages/75/0c/2edd38790eccc18b43b90981f4cce25fdf5375ee1e4d867862093ffdd802/tfrecord_tj-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "750c2edd38790eccc18b43b90981f4cce25fdf5375ee1e4d867862093ffdd802",
        "md5": "9acf375eb5d72d574b4ddd73731fa79e",
        "sha256": "e57d0b43091cfe45522917df3a1f2afc867941acae5928ac3fa305cd28802d5f"
      },
      "downloads": -1,
      "filename": "tfrecord_tj-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "9acf375eb5d72d574b4ddd73731fa79e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 15709,
      "upload_time": "2022-03-04T12:45:30",
      "upload_time_iso_8601": "2022-03-04T12:45:30.070173Z",
      "url": "https://files.pythonhosted.org/packages/75/0c/2edd38790eccc18b43b90981f4cce25fdf5375ee1e4d867862093ffdd802/tfrecord_tj-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}