{
  "info": {
    "author": "Abhishek Suresh",
    "author_email": "abhishek.sures@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# langidentification: Language identification using fastText for Romanized South Asian and Arabic Scripts\n\nThis package is a language detector built using fastText with the objective of identifying languages written in \nnon-native romanized scripts. There are two models supported in this package: `original` and `augmented`. \n\nThe `original` model was trained on freely \navailable data found on the [Tatoeba website](https://tatoeba.org/en/downloads). It is trained for 105 languages \nfor which at least 100 sample sentences were present in the data.\n\nThe `augmented` model was trained on all the data used for the `original` model with the addition of three more datasets:\n* The [Dakshina Dataset (romanized sentences)](https://github.com/google-research-datasets/dakshina), with Wikipedia \n  sentence data representing 12 South Asian languages: Bengali (`bn`), Gujarati (`gu`), Kannada (`kn`), \n  Malayalam (`ml`), Sinhala (`si`), Tamil (`ta`), Telugu (`te`), Hindi (`hi`), Marathi (`mr`), Punjabi (`pa`), \n  Urdu (`ur`) and Sindhi (`sd`)\n* The [Tunisian Arabizi Dialectal Dataset](https://aclanthology.org/2021.wanlp-1.25.pdf), representing Tunisian \n  Arabic, which displays frequent use of French loanwords\n* The [Arabizi Identification in Twitter Data dataset](https://aclanthology.org/P16-3008.pdf), representing Egyptian \n  and Lebanese Arabic, which have some use of English and French loanwords\n\nAlthough built using fastText, these models **are not comparable** to the official \n[fastText models](https://fasttext.cc/docs/en/language-identification.html), which are built on more data than the \nTatoeba dataset. These models are however capable of identifying romanized South Asian language and Arabic (`ar`) text \nwith the suffix `-rom`.\n\n## Model Performance\n\n|Model|Precision|Recall|F1 score|\n|---|---|---|---|\n|**original**|0.79|0.65|0.65|\n|**augmented**|0.75|0.66|0.66|\n\n## Installation\n\n```\npip install langidentification\n```\n\n## Usage\nValid `model_type`s are `original` and `augmented`. The `augmented` model is capable of making romanized language \npredictions.\n\nBoth models are hosted on [GitHub releases](https://github.com/absu5530/langidentification/releases) and are downloaded \nat runtime if not found.\n\n```\n>>> from langidentification import LangIdentification\n>>> model = LangIdentification(model_type='augmented')\nINFO:root:Model langdetect_augmented.ftz was not found. Checking for models directory and creating if not available...\n\nINFO:root:Downloading langdetect_augmented.ftz...\n\n100% [......................................................................] 276567331 / 276567331INFO:root:Loading model langdetect_augmented.ftz...\n\n>>> model.predict_lang('ithu velai seyyuma?')\n(('__label__ta-rom',), array([0.87046105]))\n```\n\n## What is fastText and how were these models built?\n\nfastText is the implementation of a text embedding method presented in \n[Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606) by Bojanowski et al. \n\nThis method is an enhancement of the Word2vec skipgram model which attempts to learn a vector representation for every \nword by training it to predict other words that appear in the context of the given word. The objective of the skipgram \nmodel is to find a set of parameters, i.e. word vectors, such that the likelihood of observing a certain context word \ngiven a certain target word is maximized.\n\nfastText builds on the skipgram model by representing words as a sum of the vector representations of their constituent \ncharacter n-grams and training with the goal of independently predicting the presence of context words. \"Positive\" \nexamples, being the actual context words, and \"negative\" examples, being other words randomly drawn from the vocabulary, \nare used to train on.\n\nFor example, in the sentence `She is a pretty cat.`, if the target word is `pretty`, over the course of training, the \nvector representation for `pretty` should evolve to be closer to the representations for the words `a` and `cat` and \nfurther from other randomly sampled words, such as `computer` or `dirt`. As the word vector for `pretty` is adjusted \nduring this process, so are the constituent character n-gram vectors, e.g. for `pre`, `etty` and `rett`.\n\nThe intuition here is that training subword representations helps to more accurately capture nuances in word morphology.\n\nThe character n-grams used for the models in this package are between two and four characters long. The vector \nrepresentations learned are in 50 dimensions.\n\nThe models were quantized to make them smaller and faster at inference time. \n\n## What is quantization?\n\nQuantization ([relevant SciPy modules](https://docs.scipy.org/doc/scipy/reference/cluster.vq.html)) is a process by which the embedding vectors (n vectors x \nd dimensions) are split into m distinct subvectors (n vectors x d/m dimensions), and then a k-means clustering algorithm \nis run on each of those subvectors. In doing so, a set of centroids (k clusters x d/m dimensions) is identified for each \nsubvector.\n\nThis set of centroids acts as the encoding with which to transform the original word vectors (n vectors x d dimensions) \ninto quantized vectors (n vectors x m dimensions). For each one of m subvectors, each of the original word vectors \nis classified with the cluster number or centroid index of the centroid closest to it. In this way, each word gets \nassigned m values, producing a final matrix of n vectors x m dimensions.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/absu5530/langidentification",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "langidentification",
    "package_url": "https://pypi.org/project/langidentification/",
    "platform": "",
    "project_url": "https://pypi.org/project/langidentification/",
    "project_urls": {
      "Homepage": "https://github.com/absu5530/langidentification"
    },
    "release_url": "https://pypi.org/project/langidentification/1.2/",
    "requires_dist": [
      "fasttext (>=0.9.2)",
      "wget (>=3.2)"
    ],
    "requires_python": "",
    "summary": "A package for language identification based on fastText, including romanised South Asian languages and Arabic",
    "version": "1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11168494,
  "releases": {
    "1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f6b565b2523ddf3ad7796e581b31ca0bc1cf632be9ac4f9c35d60c5012580659",
          "md5": "d73098b5527c4d78710834c289bf3d8a",
          "sha256": "3e4095dd7ab032dd0c7f0695e5e1fb69b2649c75955c13c6f7acd838236513a6"
        },
        "downloads": -1,
        "filename": "langidentification-1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d73098b5527c4d78710834c289bf3d8a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5134,
        "upload_time": "2021-08-13T05:45:56",
        "upload_time_iso_8601": "2021-08-13T05:45:56.120293Z",
        "url": "https://files.pythonhosted.org/packages/f6/b5/65b2523ddf3ad7796e581b31ca0bc1cf632be9ac4f9c35d60c5012580659/langidentification-1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "048ba80d5625ed0db3c8a7ba50fff4255e3f1df8e848e9eaaa0a1622df46b4a1",
          "md5": "d6d1d34f6b2d70743481520c10417fd6",
          "sha256": "d1a6f10bb6cfb6deb074ba362db39e2c2f46be7edaaf132fe78371fe91f7fbe1"
        },
        "downloads": -1,
        "filename": "langidentification-1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "d6d1d34f6b2d70743481520c10417fd6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4642,
        "upload_time": "2021-08-13T05:45:57",
        "upload_time_iso_8601": "2021-08-13T05:45:57.987004Z",
        "url": "https://files.pythonhosted.org/packages/04/8b/a80d5625ed0db3c8a7ba50fff4255e3f1df8e848e9eaaa0a1622df46b4a1/langidentification-1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f6b565b2523ddf3ad7796e581b31ca0bc1cf632be9ac4f9c35d60c5012580659",
        "md5": "d73098b5527c4d78710834c289bf3d8a",
        "sha256": "3e4095dd7ab032dd0c7f0695e5e1fb69b2649c75955c13c6f7acd838236513a6"
      },
      "downloads": -1,
      "filename": "langidentification-1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d73098b5527c4d78710834c289bf3d8a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 5134,
      "upload_time": "2021-08-13T05:45:56",
      "upload_time_iso_8601": "2021-08-13T05:45:56.120293Z",
      "url": "https://files.pythonhosted.org/packages/f6/b5/65b2523ddf3ad7796e581b31ca0bc1cf632be9ac4f9c35d60c5012580659/langidentification-1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "048ba80d5625ed0db3c8a7ba50fff4255e3f1df8e848e9eaaa0a1622df46b4a1",
        "md5": "d6d1d34f6b2d70743481520c10417fd6",
        "sha256": "d1a6f10bb6cfb6deb074ba362db39e2c2f46be7edaaf132fe78371fe91f7fbe1"
      },
      "downloads": -1,
      "filename": "langidentification-1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "d6d1d34f6b2d70743481520c10417fd6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 4642,
      "upload_time": "2021-08-13T05:45:57",
      "upload_time_iso_8601": "2021-08-13T05:45:57.987004Z",
      "url": "https://files.pythonhosted.org/packages/04/8b/a80d5625ed0db3c8a7ba50fff4255e3f1df8e848e9eaaa0a1622df46b4a1/langidentification-1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}