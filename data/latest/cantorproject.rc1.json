{
  "info": {
    "author": "V Abhijith Rao",
    "author_email": "varsg007@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# The CantorProject\n\nPartial differential equations (PDEs) and Ordinary Differential Equations (ODEs) are extensively used in \nthe mathematical modelling of various problems in physics, engineering, finance and healthcare. In practical situations, \nthese equations typically lack analytical solutions or are simply too difficult to solve and are hence solved \nnumerically. In current practice, most numerical approaches to solve PDEs like finite element method (FEM), \nfinite difference method (FDM) and finite volume method (FVM) are mesh based. A typical implementation of a \nmesh based approach involves three steps: \n1. Grid generation, \n2. Discretization of governing equation and \n3. Solution of the discretized equations with some iterative method.\n\nHowever, there are limitations to these approaches. Some of the limitations of these methods are as follows:\n1. They cannot be used to solve PDEs in complex computational domains because grid generation (step 1) itself becomes infeasible.\n2. The process of discretization (step 2) creates a discrepancy between the mathematical nature of actual ODE/PDE\nand its approximate difference equation. Sometimes this can lead to quite serious problems.\n\nOne of the options to fix these issues is to use neural networks. In this approach, the data set consists of some randomly\nselected points in the domain and on the boundary, called the collocation points. The governing equations and the boundary conditions are fitted\nusing neural network. There are two main motivations for this approach. First, being universal approximators, neural\nnetworks can potentially represent any ODE/PDE. So this avoids the discretization step and thus discretization based physics\nerrors too. Second, it is meshfree and therefore complex geometries can be easily handled. Initial work in this\ndirection can be credited to Lagaris et al. Firstly, they solved the initial boundary value problem using neural\nnetworks and later extended their work to handle irregular boundaries. Since then, a lot of work has been done in\nthis field. In particular, we refer to the physics-informed neural networks (PINN) approach by\nRaissi and Karniadakis and Raissi et. al. This approach has produced promising results for a series of\nbenchmark nonlinear problems.\n\nIn this project we have used the approach introduced by Maziar Raissi, Paris Perdikaris and George Em Karniadakis in their paper, \nPhysics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations (see citations at the bottom). \n\n### Contributions\nThe approach mentioned here is heavily based on the code I found [here](https://github.com/okada39/pinn_wave). I mostly added GPU\nutilization as well as first order optimizers for faster convergence. I also came up with a cool name for the package:)\n\n## Table of Content\n\n* [Library](#library)\n* [Usage](#usage)\n  * [Building the Gradient Layer](#building-the-gradient-layer)\n  * [Preparing the Neural Net](#preparing-the-nn)\n  * [Dataset Preparation](#dataset-preparation)\n  * [Training using First Order Optimizers](#training-using-first-order-optimizers)\n  * [Training using Quasi-Newtonian Methods](#training-using-quasi-newtonian-methods)\n  * [Inference](#inference)\n* [Conclusion](#conclusion)\n* [Citations](#citations)\n\n## Library\nThe cantorProject aims to bring a set of tools that can be used by researchers and students alike to solve differential equations \nusing neural networks. The library currently uses Tensorflow 2.4, along with Scipy-Optimize and Tensorflow Proability for its functioning. The library provides, \na simple class for constructing a neural network and then two other classes for optimizing the same, the choice of which is left to the user. In the tutorial \nthat follows, we will demostrate how to utilize the tools provided by the library to solve an extremely simple ODE of the form, y'' + y' + y = 0, with the Cauchy \ncondiotions being, y(0) = 1, and y'(0) = 5. The method illustrated, could be thus extended to solve much more complex equations, the solution to the harmonic \noscillator being one of them, for which we have we have demostrated the solution in a Jupyter notebook, attached within the GitHub repo. \n\n## Usage\nWe will now leverage the tools offered by the library to solve an extremely simple ODE, for more examples related to the same, check out the jupyter notebooks, within the repo. \n\n### Building the Gradient Layer\n\nWe first demostrate the code that accomplishes the task, followed by an explaination of the code. \n\n```sh\nclass GradientLayer(tf.keras.layers.Layer):\n    \"\"\"\n    Custom layer to compute 1st and 2nd derivatives of the neural net.\n    Attributes:\n        model: keras network model.\n    \"\"\"\n    def __init__(self, model, **kwargs):\n        \"\"\"\n        Args:\n            model: keras network model.\n        \"\"\"\n        self.model = model\n        super().__init__(**kwargs)\n\n    def call(self, x):\n        \"\"\"\n        Computing 1st and 2nd derivatives of the neural net.\n        Args:\n            tx: input variables (x).\n        Returns:\n            u: network output.\n            du_dx: 1st derivative of x.\n            d2u_dx2: 2nd derivative of x.\n        \"\"\"\n        with tf.GradientTape() as g:\n            g.watch(x)\n            with tf.GradientTape() as gg:\n                gg.watch(x)\n                u = self.model(x)\n            du_dtx = gg.batch_jacobian(u, x)\n            du_dx = du_dtx[..., 0]\n        d2u_dtx2 = g.batch_jacobian(du_dtx, x)\n        d2u_dx2 = d2u_dtx2[..., 0, 0]\n\n        return u, du_dx, d2u_dx2\n ```\n\nThe code snippet as presented above creates a Gradient layer class, that helps us in calculating the gradient of the neural network, which is then\nused by the loss function in order to convert the differential equation to an optimization problem. This gradient layer, as you can tell, returns \nthree variables:\n1. The output of the network \n2. The first derivative of the network with respect to the input and\n3. It's second derivative with respect to the same. \n\nTo modify the Gradient Layer class to accomodate for say PDEs we need only modify the call method, to return the diffrent partial derivatives involved in the process. \nFor examples of the same, please refer to the attached notebooks. \n\n### Preparing the Neural Net\nWe now build the Neural Network, that we utilize to approximate the function that simulates the differential equation. \n\n```sh\nimport cantorProject as cp\nnetwork = cp.Network.build()\nnetwork.summary()\n```\nThis code snippet construct an extremely simple neural network with 8 layers and 20 neurons in each. It uses the hyperbolic tangent activation in each of those layers,\nby deafult. To modify the regular behaviour, we can pass in arguments as follows: \n\n```sh\nneural_net = [30, 30, 30]\nnetwork = cp.Network.build(layers=neural_net, activation='selu')\nnetwork.summary()\n```\nThe code snippet presented above produces a neural net with 3 layers and 30 neurons in each, with activation being the selu function. \n\nHowever, this only accomplishes half the task. For training, we require a wrapping of the above class, that can utilize the earlier created Gradient layer class \nto evaluate the differential equation, and naturally help us simulate the same. \n\n```sh\nclass PINN:\n    \"\"\"\n    Build a wrapping for the above initialized neural net.\n    Attributes:\n        network: keras network model with input (x) and output u(x).\n        grads: gradient layer.\n    \"\"\"\n\n    def __init__(self, network):\n        \"\"\"\n        Args:\n            network: keras network model with input (x) and output u(x).\n        \"\"\"\n\n        self.network = network\n        self.grads = GradientLayer(self.network)\n\n    def build(self):\n        \"\"\"\n        Build a PINN model class as described by the PINN paper.\n        Returns:\n            PINN model for the projectile motion with\n                input: [ (x) relative to equation,\n                         (x=0) relative to initial condition ],\n                output: [ u(x) relative to equation,\n                          u(x=0) relative to initial condition,\n                          du_dt(x=0) relative to initial derivative of x  ]\n        \"\"\"\n\n        # equation input: (x)\n        tx_eqn = tf.keras.layers.Input(shape=(1,))\n        # initial condition input: (x)\n        tx_ini = tf.keras.layers.Input(shape=(1,))\n\n        # compute gradients\n        u, du_dx,  d2u_dx2 = self.grads(tx_eqn)\n\n        # equation output being zero\n        u_eqn = d2u_dx2 + du_dx + u \n        # initial condition output\n        u_ini, du_dx_ini, d2u_dx2  = self.grads(tx_ini)\n\n        # build the PINN model\n        return tf.keras.models.Model(\n            inputs=[tx_eqn, tx_ini],\n            outputs=[u_eqn, u_ini, du_dx_ini])\n ```\nIf you look closely at the code, you will realize that the PINN model, takes two inputs, one being the collocation points and the second being the boundary conditions. \nWe calculate the derivatives and find the u_eqn which becomes our optimization problem. We follow it up with finding the intitial condition outputs which becomes part\nof our output. From the problem we are trying to solve, we see that, tx_eqn could be any number (basically x) and forms the collocation points of our function, tx_ini is \nbasically all just x=0, representing our Cauchy conditions. The outputs, u_eqn is always zero, since that's our optimization problem, u_ini is 1, as mentioned in the \nproblem posed, whereas du_dx_ini is 5. \n\n```sh\npinn = PINN(network).build()\n```\n\n### Dataset Preparation\nThis is as described at the end of the previous section. We simply accumlate the values of all points we are trying to simulate, and combine them to form\nthe dataset. \n\n```sh\n# create training input\ntx_eqn = np.random.rand(num_train_samples, 1)\ntx_eqn[..., 0] = 2*tx_eqn[..., 0] - 1              # x = -1 ~ +1\ntx_ini = np.random.rand(num_train_samples, 1)\ntx_ini[..., 0] = 0*tx_ini[..., 0]                  # x = 0\nx_train = [tx_eqn, tx_ini]\n```\n```sh\nu_zero = np.zeros((num_train_samples, 1))          # y = 0\nu_ini = np.ones((num_train_samples, 1))            # y(0) = 1\ndu_dt_ini = 5 * np.ones((num_train_samples, 1))    # y'(0) = 5\ny_train = [u_zero, u_ini, du_dt_ini]\n```\nNOTE: To replicate the results achieved by the author in the simulation use, num_train_samples=10000 and num_test_samples=1000. \n\n### Training using First Order Optimizers\nFirst order optimizers such as RMSProp and Adam are widely used in ML, specifically in the field of deep learning to train large neural nets. However, one\nmain drawback of theirs is that they utilize only the first order derivative of the function to optimize the weights and biases. This leads to poor accuracy scores, hence, \nthe use of first order derivatives is heavily discouraged. However, owing to great speed of convergence, we utilize first order optimizers to initially train the network before following it up with second order derivative based optimizers. \n\n### Training using Quasi-Newtonian Methods\nThe library currently supports only the L-BFGS methods, both as packaged using Scipy-Optimize and Tensorflow-Proabability. The following code is used to create the trainer\nobject that is then utilized to train the neural network. We demonstrate both the L-BFGS implementations: \n\n```sh\n# Scipy-Optimize implementation of L-BFGS\nsci_trainer = cp.sci_Trainer(pinn, x_train, y_train)\nsci_trainer.train()\n```\n\n```sh\n# Tensorflow-Proabability implementation of L-BFGS\ntfp_trainer = cp.tfp_Trainer(pinn, x_train, y_train)\nresult = tfp_trainer.train()\ncp.set_weights(tfp_trainer, pinn, result.position)\n```\nWith this we have our neural net ready for predictions.\n\nAs with creating the neural net, the trainer also offers many modifications that can help us in custimizing the same. Let's see a code sample: \n\n```sh\n# sci_Trainer arguments\nsci_trainer = cp.sci_Trainer(model, x_train, y_train, first_order_trainer='rmsprop', batch_size=128, first_order_epochs=10,\n                 factr=10, m=50, maxls=50, maxiter=15000)\n```\n\n1. model: The model to be trained, PINN in our case\n2. x_train, y_train: The dataset to be used for optimization\n3. first_order_trainer: The first order optimizer to be used, set first_order_epochs to zero, to avoid using the same\n4. batch_size: batch size of the first order optimizer\n5. first_order_epochs: Number of epochs for the first order optimizer \n6. factr: The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps, where eps is the machine precision, which is automatically generated by the code. Typical values for factr are: 1e12 for low accuracy; 1e7 for moderate accuracy; 10.0 for extremely high accuracy.\n7. m: The maximum number of variable metric corrections used to define the limited memory matrix\n8. maxls: Maximum number of line search steps (per iteration)\n9. maxiter: Maximum number of iterations, for L-BFGS\n\n```sh\n# tfp_Trainer arguments\ntfp_trainer = cp.tfp_Trainer(pinn, x_train, y_train, model, first_order_trainer='rmsprop', batch_size=128, \n                 first_order_epochs=10, bfgs_iter=100)\n ```\n\n1. model: The model to be trained, PINN in our case\n2. x_train, y_train: The dataset to be used for optimization\n3. first_order_trainer: The first order optimizer to be used, set first_order_epochs to zero, to avoid using the same\n4. batch_size: batch size of the first order optimizer\n5. first_order_epochs: Number of epochs for the first order optimizer\n6. maxiter: Maximum number of iterations, for L-BFGS\n\n\n### Inference\nLet's now verify our results by plotting a graph of our predictions and that of the actual results to get a side by side comparison of the two. \n\n```sh\ndef actual_sol(x):\n    return np.exp(-x/2) * (np.cos(((3**(0.5))*x)/2) + (11/(3**(0.5)) * np.sin(((3**(0.5))*x)/2)))\n```\n\n![Graph](Graph.JPG)\n\nThis doesn't look too bad now does it!!!\n\n## Conclusion\nAlthough this library only consists of very few tools, as mentioned in the introduction, this approach to numerically solving differential equations has numerous advantages. \nWith that being said, the current state of the art research methods in discrete numerical analysis is far superior to that of the neural net based approaches, part of the \nreason being because ML has always been about vanilla applications that offer no meaningful contribution to the field of computation. Hopefully, with the advent of \nsuch creative endeavours we would finally witness the true nature of ML, not as a simple money making tool for huge organizations but rather a reliable source of many wonders. \n\n## Citations \n\n    @article{raissi2019physics,\n      title={Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},\n      author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George E},\n      journal={Journal of Computational Physics},\n      volume={378},\n      pages={686--707},\n      year={2019},\n      publisher={Elsevier}\n    }\n\n    @article{raissi2017physicsI,\n      title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n      author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n      journal={arXiv preprint arXiv:1711.10561},\n      year={2017}\n    }\n\n    @article{raissi2017physicsII,\n      title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n      author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n      journal={arXiv preprint arXiv:1711.10566},\n      year={2017}\n    }\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/VANRao-Stack/cantorProject",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cantorProject",
    "package_url": "https://pypi.org/project/cantorProject/",
    "platform": "",
    "project_url": "https://pypi.org/project/cantorProject/",
    "project_urls": {
      "Homepage": "https://github.com/VANRao-Stack/cantorProject"
    },
    "release_url": "https://pypi.org/project/cantorProject/0.0.3/",
    "requires_dist": [
      "tensorflow (>=2.4.0)",
      "tensorflow-probability (>=0.12.0)",
      "numpy (>=1.19.4)",
      "scipy (>=1.4.1)"
    ],
    "requires_python": ">=3.6",
    "summary": "A set of tools to numerically solve differential equations using Neural Networks.",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9708584,
  "releases": {
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a449e6224e7b7347e75fc479dc8ff52e20b42e1f9d8a72322853097ba615016f",
          "md5": "c9b957de310adcbb1648c8bfd43e7731",
          "sha256": "cb80350949a6e32202173bbf19a77f2622e83d2c134afb59785d1d98ff7ecbff"
        },
        "downloads": -1,
        "filename": "cantorProject-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c9b957de310adcbb1648c8bfd43e7731",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 11717,
        "upload_time": "2021-03-08T23:01:40",
        "upload_time_iso_8601": "2021-03-08T23:01:40.778037Z",
        "url": "https://files.pythonhosted.org/packages/a4/49/e6224e7b7347e75fc479dc8ff52e20b42e1f9d8a72322853097ba615016f/cantorProject-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aba887648c67d257978aaf36f81d2c457534ebf80471758eccbe3088861aeee0",
          "md5": "02965e0ffdad7cd15788e71c1468ffc3",
          "sha256": "815f1094b890ca1115b786e4b81707bf11ec2ba08bff02b03e51496559662777"
        },
        "downloads": -1,
        "filename": "cantorProject-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "02965e0ffdad7cd15788e71c1468ffc3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 15948,
        "upload_time": "2021-03-08T23:01:42",
        "upload_time_iso_8601": "2021-03-08T23:01:42.402780Z",
        "url": "https://files.pythonhosted.org/packages/ab/a8/87648c67d257978aaf36f81d2c457534ebf80471758eccbe3088861aeee0/cantorProject-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a449e6224e7b7347e75fc479dc8ff52e20b42e1f9d8a72322853097ba615016f",
        "md5": "c9b957de310adcbb1648c8bfd43e7731",
        "sha256": "cb80350949a6e32202173bbf19a77f2622e83d2c134afb59785d1d98ff7ecbff"
      },
      "downloads": -1,
      "filename": "cantorProject-0.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c9b957de310adcbb1648c8bfd43e7731",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 11717,
      "upload_time": "2021-03-08T23:01:40",
      "upload_time_iso_8601": "2021-03-08T23:01:40.778037Z",
      "url": "https://files.pythonhosted.org/packages/a4/49/e6224e7b7347e75fc479dc8ff52e20b42e1f9d8a72322853097ba615016f/cantorProject-0.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "aba887648c67d257978aaf36f81d2c457534ebf80471758eccbe3088861aeee0",
        "md5": "02965e0ffdad7cd15788e71c1468ffc3",
        "sha256": "815f1094b890ca1115b786e4b81707bf11ec2ba08bff02b03e51496559662777"
      },
      "downloads": -1,
      "filename": "cantorProject-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "02965e0ffdad7cd15788e71c1468ffc3",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 15948,
      "upload_time": "2021-03-08T23:01:42",
      "upload_time_iso_8601": "2021-03-08T23:01:42.402780Z",
      "url": "https://files.pythonhosted.org/packages/ab/a8/87648c67d257978aaf36f81d2c457534ebf80471758eccbe3088861aeee0/cantorProject-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}