{
  "info": {
    "author": "Elves M. Rodrigues",
    "author_email": "elvesmateusrodrigues@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: Unix",
      "Programming Language :: Python :: 3"
    ],
    "description": "# antiblock-scrapy\n\nMecanismos antibloqueios para o Scrapy.\n\n## Funcionalidades\n\nScrapy já vem com vários recursos antibloqueios que só precisam ser configurados, bem como muitos outros implementados por terceiros (alguns deles listados abaixo). \n\nOs recursos implementados foram:\n\n- Rotacionador de user-agents\n- Rotacionar de IPs via proxy Tor\n\n## Instalação\n\nManeira mais fácil:\n\n```bash\npip install antiblock-scrapy\n```\n\n## Instalação e configuração do Tor\n\nÉ necessário configurar o **Tor**. Primeiramente, instale-o:\n\n```bash\nsudo apt-get install tor\n```\n\nPare sua execução para realizar configurações:\n\n```bash\nsudo service tor stop\n```\n\nAbra seu arquivo de configuração como root, disponível em */etc/tor/torrc*, por exemplo, usando o nano:\n\n```bash\nsudo nano /etc/tor/torrc\n```\nColoque as linhas abaixo e salve:\n\n```\nControlPort 9051\nCookieAuthentication 0\n```\n\nReinicie o Tor:\n\n```bash\nsudo service tor start\n```\n\n\nÉ possível verificar o IP de sua máquina e comparar com o do Tor da seguinte forma:\n- Para ver seu IP:\n    ```bash\n    curl http://icanhazip.com/\n    ```\n- Para ver o IP do TOR:\n    ```bash\n    torify curl http://icanhazip.com/   \n    ```\n\nProxy do Tor não são suportados pelo Scrapy. Para contornar esse problema, é necessário o uso de um intermediário, nesse caso o **[Privoxy](https://www.privoxy.org/)**. \n\n> O servidor proxy do Tor se encontra, por padrão, no endereço 127.0.0.1:9050\n\n### Instalação e configuração do **Privoxy**:\n- Instalar: \n    ```bash\n    sudo apt install privoxy\n    ```\n- Pare sua execução:\n    ```bash\n    sudo service privoxy stop\n    ```\n- Configurá-lo para usar Tor, abra seu arquivo de configuração:\n    ```bash\n    sudo nano /etc/privoxy/config\n    ```\n- Adicione as seguintes linhas:\n    ```bash\n    forward-socks5t / 127.0.0.1:9050 .\n    ``` \n- Inicie-o: \n    ```\n    service privoxy start\n    ```\n\n> Por padrão, privoxy executará no endereço 127.0.0.1:8118 \n\nTeste: \n```bash\ntorify curl http://icanhazip.com/\n```\n```bash\ncurl -x 127.0.0.1:8118 http://icanhazip.com/\n```\nO IP mostrado nos dois passos acima deve ser o mesmo.\n\n## Rotação de IPs via Tor\n\n- Configure o middleware no arquivo de configuração de seu projeto (**settings.py**):\n    ```python\n    DOWNLOADER_MIDDLEWARES = {\n        ...,\n        'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 110,\n        'antiblock_scrapy.middlewares.TorProxyMiddleware': 100\n    }\n    ```\n\n- Habilite o uso da extensão:  \n    ```python\n    TOR_IPROTATOR_ENABLED = True\n    TOR_IPROTATOR_CHANGE_AFTER = #número de requisições feitas em um mesmo endereço IP\n    ```\nPor padrão, um IP poderá ser reutilizado após 10 usos de outros. Esse valor pode ser alterado pela variável TOR_IPROTATOR_ALLOW_REUSE_IP_AFTER, como abaixo:\n\n```python\nTOR_IPROTATOR_ALLOW_REUSE_IP_AFTER = #\n```\n\nUm número grande demais pode tornar mais lento recuperar um novo IP para uso ou nem encontrar. Se o valor for 0, não haverá registro de IPs usados.\n\n## Rotação de IPs via lista de proxies\n\nRotacionar IPs via Tor pode tornar o processo de coleta lento. Para isso existem ferramentas de terceiros que rotacionam uma lista de proxies dada, possivelmente deixando a coleta mais rápida (em comparação ao Tor), como:\n\n- https://github.com/xiaowangwindow/scrapy-rotated-proxy\n- https://github.com/TeamHG-Memex/scrapy-rotating-proxies\n\n## Rotação de user-agents\n\n- No arquivo de configuração de seu projeto Scrapy, adicione as seguintes linhas (**settings.py**):\n    ```python\n    DOWNLOADER_MIDDLEWARES = {\n        ...,\n        'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n        'antiblock_scrapy.middlewares.RotateUserAgentMiddleware': 500,\n    }\n    ```\n- Defina a lista de user-agents, ative o módulo e defina um uso mínimo e máximo de cada user-agent (o uso de um user-agent será aleatório entre esses números) (**settings.py**):\n    ```python\n    USER_AGENTS = ['user-agent-1', 'user-agent-2',...,'user-agent-n']\n    ROTATE_USER_AGENT_ENABLED = True\n    MIN_USER_AGENT_USAGE = #uso mínimo de user-agent\n    MAX_USER_AGENT_USAGE = #uso máximo de user-agent\n    ```\n\n- É possível conferir o user-agent usado no site: https://www.whatismybrowser.com/detect/what-is-my-user-agent \n\n## Atrasos entre requisições\n\nEssa funcionalidade já é disponível por padrão ao Scrapy por meio de [DOWNLOAD_DELAY](https://docs.scrapy.org/en/latest/topics/settings.html#download-delay).\n\nPor padrão:\n- O valor de DOWNLOAD_DELAY é de 0.25 segundos\n- O tempo entre requisições não é fixo, um valor entre 0.5 * DOWNLOAD_DELAY e 1.5 * DOWNLOAD_DELAY é escolhido entre cada requisição\n\nPara alterar o atraso entre requisição, faça (em **settings.py**):\n\n```python\nDOWNLOAD_DELAY = # tem em segundos entre requisições\n```\n\nPara forçar que o tempo entre requisições seja fixo, ao contrário do padrão aleatório, faça (em **settings.py**):\n```python\nRANDOMIZE_DOWNLOAD_DELAY = False\n```\n### AutoThrottle\n\nUma opção mais avançada de colocar atrasos entre requisições é o [AutoThrottle](https://docs.scrapy.org/en/latest/topics/autothrottle.html#autothrottle-extension). Ela alterará a velocidade entre requisições de acordo com a latência de resposta do servidor e a capacidade de processamento da engine de maneira automática.  \n\nPor padrão, essa configuração está desativada. Mas pode ser ativada por meio do seguinte comando (em **settings.py**):\n\n```python\nAUTOTHROTTLE_ENABLED = True\n```\n\nÉ necessário definir um delay inicial que será ajustado ao longo das requisições automaticamente. Defina-o por meio do comando abaixo, o default de 5.0 segundos (em **settings.py**):\n\n```python\nAUTOTHROTTLE_START_DELAY = #delay inicial  \n```\n\nDefina também um delay máximo, o default de 60.0 segundos (em **settings.py**):\n\n```python\nAUTOTHROTTLE_MAX_DELAY = #delay máximo\n```\n\nO atraso das próximas requisições será ajustado para um valor respeitando DOWNLOAD_DELAY e AUTOTHROTTLE_MAX_DELAY, levando em conta a média de requisições paralelas enviadas ao servidor que, por padrão, é 1.0. Esse valor pode ser ajustado pelo comando abaixo (em **settings.py**): \n\n```python\nAUTOTHROTTLE_TARGET_CONCURRENCY = #média de requisições concorrentes\n```\n\nMais detalhes podem ser encontrados [aqui](https://docs.scrapy.org/en/latest/topics/autothrottle.html#throttling-algorithm).\n\n## Gerenciamento de cookies\n\nScrapy já possui mecanismos de gerencialmente de cookies e detalhes podem ser encontrados [aqui](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.cookies).\n\nPor exemplo, caso possua os cookies de acesso de um site que é necessário login, uma das possíveis abordagens é criar um Spider como o abaixo:\n\n```python\n\nimport scrapy\n\n# Cookies de login di site\nCOOKIES = [{},..., {}]\n\nclass LoginSpider(scrapy.Spider):\n    name = 'foo'\n\n    def start_requests(self):\n        urls = ['site-com-login-1', 'site-com-login-2', ..., 'site-com-login-n']\n        for url in urls:\n            yield scrapy.Request(url='site-que-precisa-login', cookies=COOKIES, callback=self.parse)\n\n    def parse(self, response):\n        ...\n```\n\nOutras formas de lidar como cookies como, por exemplo, cada requisição com seu próprio cookie, podem ser feitas usando **cookiejar**, mais detalhes [aqui](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.cookies). \n\nBibliotecas de terceiros permitem **persistência de cookies** e outros recursos, como [scrapy-cookies](https://scrapy-cookies.readthedocs.io/en/latest/intro/overview.html).  \n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/elvesrodrigues/antiblock-scrapy",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "antiblock-scrapy",
    "package_url": "https://pypi.org/project/antiblock-scrapy/",
    "platform": "",
    "project_url": "https://pypi.org/project/antiblock-scrapy/",
    "project_urls": {
      "Homepage": "https://github.com/elvesrodrigues/antiblock-scrapy"
    },
    "release_url": "https://pypi.org/project/antiblock-scrapy/0.0.1/",
    "requires_dist": [
      "pysocks (==1.7.1)",
      "requests (==2.23.0)",
      "stem (==1.8.0)"
    ],
    "requires_python": ">=3.0",
    "summary": "Mecanismos antibloqueios para Scrapy",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7454618,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a8e7061b3e9045231353a13c56a4841e2e6a09312d059b25182055e1ff7581e5",
          "md5": "c0bbbc1798e32be764034884d3c26a96",
          "sha256": "810e963d29e653c2ac1e2677ede09524117c64f9cb79d39e88f98082b4c9908f"
        },
        "downloads": -1,
        "filename": "antiblock_scrapy-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c0bbbc1798e32be764034884d3c26a96",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.0",
        "size": 7655,
        "upload_time": "2020-06-11T17:24:02",
        "upload_time_iso_8601": "2020-06-11T17:24:02.981748Z",
        "url": "https://files.pythonhosted.org/packages/a8/e7/061b3e9045231353a13c56a4841e2e6a09312d059b25182055e1ff7581e5/antiblock_scrapy-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b4e170857e9d88861ec80ba84aaeb3c388507a5965fe5cc497542a594baf91bb",
          "md5": "099e9eb08b741a3a2aea3e3dc62f9192",
          "sha256": "f8ea29f36478a5d22b387c99b548099bd2e8bcdbeea582b57a087304efa23a1f"
        },
        "downloads": -1,
        "filename": "antiblock-scrapy-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "099e9eb08b741a3a2aea3e3dc62f9192",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.0",
        "size": 6482,
        "upload_time": "2020-06-11T17:24:05",
        "upload_time_iso_8601": "2020-06-11T17:24:05.875178Z",
        "url": "https://files.pythonhosted.org/packages/b4/e1/70857e9d88861ec80ba84aaeb3c388507a5965fe5cc497542a594baf91bb/antiblock-scrapy-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a8e7061b3e9045231353a13c56a4841e2e6a09312d059b25182055e1ff7581e5",
        "md5": "c0bbbc1798e32be764034884d3c26a96",
        "sha256": "810e963d29e653c2ac1e2677ede09524117c64f9cb79d39e88f98082b4c9908f"
      },
      "downloads": -1,
      "filename": "antiblock_scrapy-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c0bbbc1798e32be764034884d3c26a96",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.0",
      "size": 7655,
      "upload_time": "2020-06-11T17:24:02",
      "upload_time_iso_8601": "2020-06-11T17:24:02.981748Z",
      "url": "https://files.pythonhosted.org/packages/a8/e7/061b3e9045231353a13c56a4841e2e6a09312d059b25182055e1ff7581e5/antiblock_scrapy-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b4e170857e9d88861ec80ba84aaeb3c388507a5965fe5cc497542a594baf91bb",
        "md5": "099e9eb08b741a3a2aea3e3dc62f9192",
        "sha256": "f8ea29f36478a5d22b387c99b548099bd2e8bcdbeea582b57a087304efa23a1f"
      },
      "downloads": -1,
      "filename": "antiblock-scrapy-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "099e9eb08b741a3a2aea3e3dc62f9192",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.0",
      "size": 6482,
      "upload_time": "2020-06-11T17:24:05",
      "upload_time_iso_8601": "2020-06-11T17:24:05.875178Z",
      "url": "https://files.pythonhosted.org/packages/b4/e1/70857e9d88861ec80ba84aaeb3c388507a5965fe5cc497542a594baf91bb/antiblock-scrapy-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}