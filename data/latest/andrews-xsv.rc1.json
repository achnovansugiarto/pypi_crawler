{
  "info": {
    "author": "Andrew Gallant",
    "author_email": "jamslam@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3"
    ],
    "description": "xsv is a command line program for indexing, slicing, analyzing, splitting\nand joining CSV files. Commands should be simple, fast and composable:\n\n1. Simple tasks should be easy.\n2. Performance trade offs should be exposed in the CLI interface.\n3. Composition should not come at the expense of performance.\n\nThis README contains information on how to\n[install `xsv`](https://github.com/BurntSushi/xsv#installation), in addition to\na quick tour of several commands.\n\n[![Linux build status](https://api.travis-ci.org/BurntSushi/xsv.svg)](https://travis-ci.org/BurntSushi/xsv)\n[![Windows build status](https://ci.appveyor.com/api/projects/status/github/BurntSushi/xsv?svg=true)](https://ci.appveyor.com/project/BurntSushi/xsv)\n[![](https://meritbadge.herokuapp.com/xsv)](https://crates.io/crates/xsv)\n\nDual-licensed under MIT or the [UNLICENSE](https://unlicense.org).\n\n\n### Available commands\n\n* **cat** - Concatenate CSV files by row or by column.\n* **count** - Count the rows in a CSV file. (Instantaneous with an index.)\n* **fixlengths** - Force a CSV file to have same-length records by either\n  padding or truncating them.\n* **flatten** - A flattened view of CSV records. Useful for viewing one record\n  at a time. e.g., `xsv slice -i 5 data.csv | xsv flatten`.\n* **fmt** - Reformat CSV data with different delimiters, record terminators\n  or quoting rules. (Supports ASCII delimited data.)\n* **frequency** - Build frequency tables of each column in CSV data. (Uses\n  parallelism to go faster if an index is present.)\n* **headers** - Show the headers of CSV data. Or show the intersection of all\n  headers between many CSV files.\n* **index** - Create an index for a CSV file. This is very quick and provides\n  constant time indexing into the CSV file.\n* **input** - Read CSV data with exotic quoting/escaping rules.\n* **join** - Inner, outer and cross joins. Uses a simple hash index to make it\n  fast.\n* **partition** - Partition CSV data based on a column value.\n* **sample** - Randomly draw rows from CSV data using reservoir sampling (i.e.,\n  use memory proportional to the size of the sample).\n* **reverse** - Reverse order of rows in CSV data.\n* **search** - Run a regex over CSV data. Applies the regex to each field\n  individually and shows only matching rows.\n* **select** - Select or re-order columns from CSV data.\n* **slice** - Slice rows from any part of a CSV file. When an index is present,\n  this only has to parse the rows in the slice (instead of all rows leading up\n  to the start of the slice).\n* **sort** - Sort CSV data.\n* **split** - Split one CSV file into many CSV files of N chunks.\n* **stats** - Show basic types and statistics of each column in the CSV file.\n  (i.e., mean, standard deviation, median, range, etc.)\n* **table** - Show aligned output of any CSV data using\n  [elastic tabstops](https://github.com/BurntSushi/tabwriter).\n\n\n### A whirlwind tour\n\nLet's say you're playing with some of the data from the\n[Data Science Toolkit](https://github.com/petewarden/dstkdata), which contains\nseveral CSV files. Maybe you're interested in the population counts of each\ncity in the world. So grab the data and start examining it:\n\n```bash\n$ curl -LO https://burntsushi.net/stuff/worldcitiespop.csv\n$ xsv headers worldcitiespop.csv\n1   Country\n2   City\n3   AccentCity\n4   Region\n5   Population\n6   Latitude\n7   Longitude\n```\n\nThe next thing you might want to do is get an overview of the kind of data that\nappears in each column. The `stats` command will do this for you:\n\n```bash\n$ xsv stats worldcitiespop.csv --everything | xsv table\nfield       type     min            max            min_length  max_length  mean          stddev         median     mode         cardinality\nCountry     Unicode  ad             zw             2           2                                                   cn           234\nCity        Unicode   bab el ahmar  Þykkvibaer     1           91                                                  san jose     2351892\nAccentCity  Unicode   Bâb el Ahmar  ïn Bou Chella  1           91                                                  San Antonio  2375760\nRegion      Unicode  00             Z9             0           2                                        13         04           397\nPopulation  Integer  7              31480498       0           8           47719.570634  302885.559204  10779                   28754\nLatitude    Float    -54.933333     82.483333      1           12          27.188166     21.952614      32.497222  51.15        1038349\nLongitude   Float    -179.983333    180            1           14          37.08886      63.22301       35.28      23.8         1167162\n```\n\nThe `xsv table` command takes any CSV data and formats it into aligned columns\nusing [elastic tabstops](https://github.com/BurntSushi/tabwriter). You'll\nnotice that it even gets alignment right with respect to Unicode characters.\n\nSo, this command takes about 12 seconds to run on my machine, but we can speed\nit up by creating an index and re-running the command:\n\n```bash\n$ xsv index worldcitiespop.csv\n$ xsv stats worldcitiespop.csv --everything | xsv table\n...\n```\n\nWhich cuts it down to about 8 seconds on my machine. (And creating the index\ntakes less than 2 seconds.)\n\nNotably, the same type of \"statistics\" command in another\n[CSV command line toolkit](https://csvkit.readthedocs.io/)\ntakes about 2 minutes to produce similar statistics on the same data set.\n\nCreating an index gives us more than just faster statistics gathering. It also\nmakes slice operations extremely fast because *only the sliced portion* has to\nbe parsed. For example, let's say you wanted to grab the last 10 records:\n\n```bash\n$ xsv count worldcitiespop.csv\n3173958\n$ xsv slice worldcitiespop.csv -s 3173948 | xsv table\nCountry  City               AccentCity         Region  Population  Latitude     Longitude\nzw       zibalonkwe         Zibalonkwe         06                  -19.8333333  27.4666667\nzw       zibunkululu        Zibunkululu        06                  -19.6666667  27.6166667\nzw       ziga               Ziga               06                  -19.2166667  27.4833333\nzw       zikamanas village  Zikamanas Village  00                  -18.2166667  27.95\nzw       zimbabwe           Zimbabwe           07                  -20.2666667  30.9166667\nzw       zimre park         Zimre Park         04                  -17.8661111  31.2136111\nzw       ziyakamanas        Ziyakamanas        00                  -18.2166667  27.95\nzw       zizalisari         Zizalisari         04                  -17.7588889  31.0105556\nzw       zuzumba            Zuzumba            06                  -20.0333333  27.9333333\nzw       zvishavane         Zvishavane         07      79876       -20.3333333  30.0333333\n```\n\nThese commands are *instantaneous* because they run in time and memory\nproportional to the size of the slice (which means they will scale to\narbitrarily large CSV data).\n\nSwitching gears a little bit, you might not always want to see every column in\nthe CSV data. In this case, maybe we only care about the country, city and\npopulation. So let's take a look at 10 random rows:\n\n```bash\n$ xsv select Country,AccentCity,Population worldcitiespop.csv \\\n  | xsv sample 10 \\\n  | xsv table\nCountry  AccentCity       Population\ncn       Guankoushang\nza       Klipdrift\nma       Ouled Hammou\nfr       Les Gravues\nla       Ban Phadèng\nde       Lüdenscheid      80045\nqa       Umm ash Shubrum\nbd       Panditgoan\nus       Appleton\nua       Lukashenkivske\n```\n\nWhoops! It seems some cities don't have population counts. How pervasive is\nthat?\n\n```bash\n$ xsv frequency worldcitiespop.csv --limit 5\nfield,value,count\nCountry,cn,238985\nCountry,ru,215938\nCountry,id,176546\nCountry,us,141989\nCountry,ir,123872\nCity,san jose,328\nCity,san antonio,320\nCity,santa rosa,296\nCity,santa cruz,282\nCity,san juan,255\nAccentCity,San Antonio,317\nAccentCity,Santa Rosa,296\nAccentCity,Santa Cruz,281\nAccentCity,San Juan,254\nAccentCity,San Miguel,254\nRegion,04,159916\nRegion,02,142158\nRegion,07,126867\nRegion,03,122161\nRegion,05,118441\nPopulation,(NULL),3125978\nPopulation,2310,12\nPopulation,3097,11\nPopulation,983,11\nPopulation,2684,11\nLatitude,51.15,777\nLatitude,51.083333,772\nLatitude,50.933333,769\nLatitude,51.116667,769\nLatitude,51.133333,767\nLongitude,23.8,484\nLongitude,23.2,477\nLongitude,23.05,476\nLongitude,25.3,474\nLongitude,23.1,459\n```\n\n(The `xsv frequency` command builds a frequency table for each column in the\nCSV data. This one only took 5 seconds.)\n\nSo it seems that most cities do not have a population count associated with\nthem at all. No matter—we can adjust our previous command so that it only\nshows rows with a population count:\n\n```bash\n$ xsv search -s Population '[0-9]' worldcitiespop.csv \\\n  | xsv select Country,AccentCity,Population \\\n  | xsv sample 10 \\\n  | xsv table\nCountry  AccentCity       Population\nes       Barañáin         22264\nes       Puerto Real      36946\nat       Moosburg         4602\nhu       Hejobaba         1949\nru       Polyarnyye Zori  15092\ngr       Kandíla          1245\nis       Ólafsvík         992\nhu       Decs             4210\nbg       Sliven           94252\ngb       Leatherhead      43544\n```\n\nErk. Which country is `at`? No clue, but the Data Science Toolkit has a CSV\nfile called `countrynames.csv`. Let's grab it and do a join so we can see which\ncountries these are:\n\n```bash\ncurl -LO https://gist.githubusercontent.com/anonymous/063cb470e56e64e98cf1/raw/98e2589b801f6ca3ff900b01a87fbb7452eb35c7/countrynames.csv\n$ xsv headers countrynames.csv\n1   Abbrev\n2   Country\n$ xsv join --no-case  Country sample.csv Abbrev countrynames.csv | xsv table\nCountry  AccentCity       Population  Abbrev  Country\nes       Barañáin         22264       ES      Spain\nes       Puerto Real      36946       ES      Spain\nat       Moosburg         4602        AT      Austria\nhu       Hejobaba         1949        HU      Hungary\nru       Polyarnyye Zori  15092       RU      Russian Federation | Russia\ngr       Kandíla          1245        GR      Greece\nis       Ólafsvík         992         IS      Iceland\nhu       Decs             4210        HU      Hungary\nbg       Sliven           94252       BG      Bulgaria\ngb       Leatherhead      43544       GB      Great Britain | UK | England | Scotland | Wales | Northern Ireland | United Kingdom\n```\n\nWhoops, now we have two columns called `Country` and an `Abbrev` column that we\nno longer need. This is easy to fix by re-ordering columns with the `xsv\nselect` command:\n\n```bash\n$ xsv join --no-case  Country sample.csv Abbrev countrynames.csv \\\n  | xsv select 'Country[1],AccentCity,Population' \\\n  | xsv table\nCountry                                                                              AccentCity       Population\nSpain                                                                                Barañáin         22264\nSpain                                                                                Puerto Real      36946\nAustria                                                                              Moosburg         4602\nHungary                                                                              Hejobaba         1949\nRussian Federation | Russia                                                          Polyarnyye Zori  15092\nGreece                                                                               Kandíla          1245\nIceland                                                                              Ólafsvík         992\nHungary                                                                              Decs             4210\nBulgaria                                                                             Sliven           94252\nGreat Britain | UK | England | Scotland | Wales | Northern Ireland | United Kingdom  Leatherhead      43544\n```\n\nPerhaps we can do this with the original CSV data? Indeed we can—because\njoins in `xsv` are fast.\n\n```bash\n$ xsv join --no-case Abbrev countrynames.csv Country worldcitiespop.csv \\\n  | xsv select '!Abbrev,Country[1]' \\\n  > worldcitiespop_countrynames.csv\n$ xsv sample 10 worldcitiespop_countrynames.csv | xsv table\nCountry                      City                   AccentCity             Region  Population  Latitude    Longitude\nSri Lanka                    miriswatte             Miriswatte             36                  7.2333333   79.9\nRomania                      livezile               Livezile               26      1985        44.512222   22.863333\nIndonesia                    tawainalu              Tawainalu              22                  -4.0225     121.9273\nRussian Federation | Russia  otar                   Otar                   45                  56.975278   48.305278\nFrance                       le breuil-bois robert  le Breuil-Bois Robert  A8                  48.945567   1.717026\nFrance                       lissac                 Lissac                 B1                  45.103094   1.464927\nAlbania                      lumalasi               Lumalasi               46                  40.6586111  20.7363889\nChina                        motzushih              Motzushih              11                  27.65       111.966667\nRussian Federation | Russia  svakino                Svakino                69                  55.60211    34.559785\nRomania                      tirgu pancesti         Tirgu Pancesti         38                  46.216667   27.1\n```\n\nThe `!Abbrev,Country[1]` syntax means, \"remove the `Abbrev` column and remove\nthe second occurrence of the `Country` column.\" Since we joined with\n`countrynames.csv` first, the first `Country` name (fully expanded) is now\nincluded in the CSV data.\n\nThis `xsv join` command takes about 7 seconds on my machine. The performance\ncomes from constructing a very simple hash index of one of the CSV data files\ngiven. The `join` command does an inner join by default, but it also has left,\nright and full outer join support too.\n\n\n### Installation\n\nBinaries for Windows, Linux and macOS are available [from Github](https://github.com/BurntSushi/xsv/releases/latest).\n\nIf you're a **macOS Homebrew** user, then you can install xsv\nfrom homebrew-core:\n\n```\n$ brew install xsv\n```\n\nIf you're a **macOS MacPorts** user, then you can install xsv\nfrom the [official ports](https://www.macports.org/ports.php?by=name&substr=xsv):\n\n```\n$ sudo port install xsv\n```\n\nIf you're a **Nix/NixOS** user, you can install xsv from nixpkgs:\n\n```\n$ nix-env -i xsv\n```\n\nAlternatively, you can compile from source by\n[installing Cargo](https://crates.io/install)\n([Rust's](https://www.rust-lang.org/) package manager)\nand installing `xsv` using Cargo:\n\n```bash\ncargo install xsv\n```\n\nCompiling from this repository also works similarly:\n\n```bash\ngit clone git://github.com/BurntSushi/xsv\ncd xsv\ncargo build --release\n```\n\nCompilation will probably take a few minutes depending on your machine. The\nbinary will end up in `./target/release/xsv`.\n\n\n### Benchmarks\n\nI've compiled some [very rough\nbenchmarks](https://github.com/BurntSushi/xsv/blob/master/BENCHMARKS.md) of\nvarious `xsv` commands.\n\n\n### Motivation\n\nHere are several valid criticisms of this project:\n\n1. You shouldn't be working with CSV data because CSV is a terrible format.\n2. If your data is gigabytes in size, then CSV is the wrong storage type.\n3. Various SQL databases provide all of the operations available in `xsv` with\n   more sophisticated indexing support. And the performance is a zillion times\n   better.\n\nI'm sure there are more criticisms, but the impetus for this project was a 40GB\nCSV file that was handed to me. I was tasked with figuring out the shape of the\ndata inside of it and coming up with a way to integrate it into our existing\nsystem. It was then that I realized that every single CSV tool I knew about was\nwoefully inadequate. They were just too slow or didn't provide enough\nflexibility. (Another project I had comprised of a few dozen CSV files. They\nwere smaller than 40GB, but they were each supposed to represent the same kind\nof data. But they all had different column and unintuitive column names. Useful\nCSV inspection tools were critical here—and they had to be reasonably fast.)\n\nThe key ingredients for helping me with my task were indexing, random sampling,\nsearching, slicing and selecting columns. All of these things made dealing with\n40GB of CSV data a bit more manageable (or dozens of CSV files).\n\nGetting handed a large CSV file *once* was enough to launch me on this quest.\nFrom conversations I've had with others, CSV data files this large don't seem\nto be a rare event. Therefore, I believe there is room for a tool that has a\nhope of dealing with data that large.\n\n\n### Naming collision\n\nThis project is unrelated to another similar project with the same name:\nhttps://mj.ucw.cz/sw/xsv/\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/sumitkanoje/xsv",
    "keywords": "",
    "license": "",
    "maintainer": "Sumitkumar Kanoje",
    "maintainer_email": "sumitkanoje@gmail.com",
    "name": "andrews-xsv",
    "package_url": "https://pypi.org/project/andrews-xsv/",
    "platform": null,
    "project_url": "https://pypi.org/project/andrews-xsv/",
    "project_urls": {
      "Homepage": "https://github.com/sumitkanoje/xsv"
    },
    "release_url": "https://pypi.org/project/andrews-xsv/0.13.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A fast CSV command line toolkit written in Rust.",
    "version": "0.13.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14419241,
  "releases": {
    "0.13.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b3b2481d3e88cc8c7df5e161ce3d15fb55e4968804fce132787872bdfa44753c",
          "md5": "4a6a5271a0caf77bfc783b17161d5d53",
          "sha256": "963f2999301c4dd5662b5f02f8799d74413b4cdf6ff527df69faf553415f96bf"
        },
        "downloads": -1,
        "filename": "andrews_xsv-0.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
        "has_sig": false,
        "md5_digest": "4a6a5271a0caf77bfc783b17161d5d53",
        "packagetype": "bdist_wheel",
        "python_version": "cp37",
        "requires_python": null,
        "size": 4784525,
        "upload_time": "2022-07-13T06:41:51",
        "upload_time_iso_8601": "2022-07-13T06:41:51.052255Z",
        "url": "https://files.pythonhosted.org/packages/b3/b2/481d3e88cc8c7df5e161ce3d15fb55e4968804fce132787872bdfa44753c/andrews_xsv-0.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7a540abc0eeb432a1add88e4c0f61580a84d3584fb669b1197551a4559d5ebb9",
          "md5": "75fdf1202a348ba1b4210c6b382ded7b",
          "sha256": "c11b272e791b462d5f659cee1908deb45c0f391db286a4786b4e32dd5c64ebce"
        },
        "downloads": -1,
        "filename": "andrews_xsv-0.13.0-cp38-cp38-macosx_10_9_x86_64.whl",
        "has_sig": false,
        "md5_digest": "75fdf1202a348ba1b4210c6b382ded7b",
        "packagetype": "bdist_wheel",
        "python_version": "cp38",
        "requires_python": null,
        "size": 1955009,
        "upload_time": "2022-07-13T06:41:56",
        "upload_time_iso_8601": "2022-07-13T06:41:56.000269Z",
        "url": "https://files.pythonhosted.org/packages/7a/54/0abc0eeb432a1add88e4c0f61580a84d3584fb669b1197551a4559d5ebb9/andrews_xsv-0.13.0-cp38-cp38-macosx_10_9_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d1e9fb3a36da4857561d0bcd33b7ffd0df7d3c8937c0a0f8c13ad070b13343c",
          "md5": "6595cdd58786dee5831dc043c9e432f2",
          "sha256": "8ab0e72571c3d90f79cf1dd9ad33f6b86a8486a5f9c83ddbe1c6e580bb15db3e"
        },
        "downloads": -1,
        "filename": "andrews_xsv-0.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
        "has_sig": false,
        "md5_digest": "6595cdd58786dee5831dc043c9e432f2",
        "packagetype": "bdist_wheel",
        "python_version": "cp38",
        "requires_python": null,
        "size": 4784524,
        "upload_time": "2022-07-13T06:42:03",
        "upload_time_iso_8601": "2022-07-13T06:42:03.476455Z",
        "url": "https://files.pythonhosted.org/packages/5d/1e/9fb3a36da4857561d0bcd33b7ffd0df7d3c8937c0a0f8c13ad070b13343c/andrews_xsv-0.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b3b2481d3e88cc8c7df5e161ce3d15fb55e4968804fce132787872bdfa44753c",
        "md5": "4a6a5271a0caf77bfc783b17161d5d53",
        "sha256": "963f2999301c4dd5662b5f02f8799d74413b4cdf6ff527df69faf553415f96bf"
      },
      "downloads": -1,
      "filename": "andrews_xsv-0.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
      "has_sig": false,
      "md5_digest": "4a6a5271a0caf77bfc783b17161d5d53",
      "packagetype": "bdist_wheel",
      "python_version": "cp37",
      "requires_python": null,
      "size": 4784525,
      "upload_time": "2022-07-13T06:41:51",
      "upload_time_iso_8601": "2022-07-13T06:41:51.052255Z",
      "url": "https://files.pythonhosted.org/packages/b3/b2/481d3e88cc8c7df5e161ce3d15fb55e4968804fce132787872bdfa44753c/andrews_xsv-0.13.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7a540abc0eeb432a1add88e4c0f61580a84d3584fb669b1197551a4559d5ebb9",
        "md5": "75fdf1202a348ba1b4210c6b382ded7b",
        "sha256": "c11b272e791b462d5f659cee1908deb45c0f391db286a4786b4e32dd5c64ebce"
      },
      "downloads": -1,
      "filename": "andrews_xsv-0.13.0-cp38-cp38-macosx_10_9_x86_64.whl",
      "has_sig": false,
      "md5_digest": "75fdf1202a348ba1b4210c6b382ded7b",
      "packagetype": "bdist_wheel",
      "python_version": "cp38",
      "requires_python": null,
      "size": 1955009,
      "upload_time": "2022-07-13T06:41:56",
      "upload_time_iso_8601": "2022-07-13T06:41:56.000269Z",
      "url": "https://files.pythonhosted.org/packages/7a/54/0abc0eeb432a1add88e4c0f61580a84d3584fb669b1197551a4559d5ebb9/andrews_xsv-0.13.0-cp38-cp38-macosx_10_9_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5d1e9fb3a36da4857561d0bcd33b7ffd0df7d3c8937c0a0f8c13ad070b13343c",
        "md5": "6595cdd58786dee5831dc043c9e432f2",
        "sha256": "8ab0e72571c3d90f79cf1dd9ad33f6b86a8486a5f9c83ddbe1c6e580bb15db3e"
      },
      "downloads": -1,
      "filename": "andrews_xsv-0.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
      "has_sig": false,
      "md5_digest": "6595cdd58786dee5831dc043c9e432f2",
      "packagetype": "bdist_wheel",
      "python_version": "cp38",
      "requires_python": null,
      "size": 4784524,
      "upload_time": "2022-07-13T06:42:03",
      "upload_time_iso_8601": "2022-07-13T06:42:03.476455Z",
      "url": "https://files.pythonhosted.org/packages/5d/1e/9fb3a36da4857561d0bcd33b7ffd0df7d3c8937c0a0f8c13ad070b13343c/andrews_xsv-0.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}