{
  "info": {
    "author": "",
    "author_email": "Gabriel de Jesus <gabriel.dejesus@timornews.tl>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "### Tetun Tokenizer\n\nTetun tokenizer is a Python package for tokenizing a string (or text) into tokens. There are several tokenization techniques we built alongside this package as follows:\n1. `TetunStandardTokenizer()`: tokenize the input string by `word`, `punctuations`, and `special characters` delimiters.\n2. `TetunWhiteSpaceTokenizer()`: tokenize the input string by `whitespace` delimiter.\n3. `TetunBlankLineTokenizer()`: tokenize the input string by `blank lines` delimiter.\n4. `TetunSimpleTokenizer()`: tokenize the input string by extracting `only string and number` and ignore punctuations and special characters.\n5. `TetunWordTokenizer()`: tokenize the input string by extracting `only string` and ignore numbers, punctuations, and special characters.\n\n\n### Installation\n\nTo install the Tetun tokenizer, run the following command:\n\n```\npython3 -m pip install tetun-tokenizer\n```\n\nor simply run:\n\n```\npip install tetun-tokenizer\n```\n\nIt also supports `conda` and commands similar to `pipenv`.\n\n\n### Usage\n\nTo use the Tetun tokenizer, `from` the `tokenizer` module on the `tetuntokenizer` package, `import` a tokenizer class. Instantiate the imported class and then call a `tokenize` function as follows:\n\n1. Using  `TetunStandardTokenizer()` to tokenize a string.\n\n```python\nfrom tetuntokenizer.tokenizer import TetunStandardTokenizer\n\ntetun_tokenizer = TetunStandardTokenizer()\n\nstring_text = \"Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.\"\noutput = tetun_tokenizer.tokenize(string_text)\nprint(output)\n```\n\nThe output will be:\n\n```\n[\"Ha'u\", 'mak', 'ita-nia', 'maluk', \"di'ak\", '.', \"Ha'u\", 'iha', '$', '0.25', 'atu', 'fó', 'ba', 'ita', '.']\n```\n\n2. Using `TetunWhiteSpaceTokenizer()` to tokenize a string.\n\n```python\nfrom tetuntokenizer.tokenizer import TetunWhiteSpaceTokenizer\n\ntetun_tokenizer = TetunWhiteSpaceTokenizer()\n\nstring_text = \"Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.\"\noutput = tetun_tokenizer.tokenize(string_text)\nprint(output)\n```\n\nThe output will be:\n\n```\n[\"Ha'u\", 'mak', 'ita-nia', 'maluk', \"di'ak.\", \"Ha'u\", 'iha', '$0.25', 'atu', 'fó', 'ba', 'ita.']\n```\n\n3. Using `TetunBlankLineTokenizer()` to tokenize a string.\n\n```python\nfrom tetuntokenizer.tokenizer import TetunBlankLineTokenizer\n\ntetun_tokenizer = TetunBlankLineTokenizer()\n\nstring = \"\"\"\n        Ha'u mak ita-nia maluk di'ak.\n        Ha'u iha $0.25 atu fó ba ita.\n        \"\"\"\noutput = tetun_tokenizer.tokenize(string_text)\nprint(output)\n```\n\nThe output will be:\n\n```\n[\"\\n            Ha'u mak ita-nia maluk di'ak.\\n            Ha'u iha $0.25 atu fó ba ita.\\n            \"]\n```\n\n4. Using `TetunSimpleTokenizer()` to tokenize a string.\n\n```python\nfrom tetuntokenizer.tokenizer import TetunSimpleTokenizer\n\ntetun_tokenizer = TetunSimpleTokenizer()\n\nstring_text = \"Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.\"\noutput = tetun_tokenizer.tokenize(string_text)\nprint(output)\n```\n\nThe output will be:\n\n```\n[\"Ha'u\", 'mak', 'ita-nia', 'maluk', \"di'ak\", \"Ha'u\", 'iha', '0.25', 'atu', 'fó', 'ba', 'ita']\n```\n\n5. Using `TetunWordTokenizer()` to tokenize a string.\n\n```python\nfrom tetuntokenizer.tokenizer import TetunWordTokenizer\n\ntetun_tokenizer = TetunWordTokenizer()\n\nstring_text = \"Ha'u mak ita-nia maluk di'ak. Ha'u iha $0.25 atu fó ba ita.\"\noutput = tetun_tokenizer.tokenize(string_text)\nprint(output)\n```\n\nThe output will be:\n\n```\n[\"Ha'u\", 'mak', 'ita-nia', 'maluk', \"di'ak\", \"Ha'u\", 'iha', 'atu', 'fó', 'ba', 'ita']\n```\n\nTo print the resulting string to the console, with each element on a new line, you can use `for` loop or simply use `join` as follows:\n\n```\nprint('\\n'.join(output))\n```\n\nThe output will be:\n\n```\nHa'u\nmak\nita-nia\nmaluk\ndi'ak\nHa'u\niha\natu\nfó\nba\nita\n```\n\nYou can also use the tokenizer to tokenize a text from a file. Here is an example:\n\n```python\n# Assume that we use Path instead of a string for the file path\nfrom pathlib import Path\nfrom tetuntokenizer.tokenizer import TetunSimpleTokenizer\n\n\nfile_path = Path(\"myfile/example.txt\")\n\ntry:\n    with file_path.open('r', encoding='utf-8') as f:\n    contents = [line.strip() for line in f]\nexcept FileNotFoundError:\n    print(f\"File not found at: {file_path}\")\n\n# You can also lowercase the contents before tokenizing them.\nlowercase_contents = contents.lower()\n\ntetun_tokenizer = TetunSimpleTokenizer()\n\noutput = '\\n'.join(tetun_tokenizer.tokenize(str(lowercase_contents)))\nprint(output)\n\n```\n\nThe output will be:\n\n```\nha'u\norgullu\ndezenvolve\nha'u-nia\nlian\ntetun \n...\n```\n\nThere are a few more ways to read file contents that you can use to achieve the same output.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tetun-tokenizer",
    "package_url": "https://pypi.org/project/tetun-tokenizer/",
    "platform": null,
    "project_url": "https://pypi.org/project/tetun-tokenizer/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/tetun-tokenizer/1.0.5/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Tetun tokenizer package",
    "version": "1.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17471754,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0baa59c9607bbde471099322c7d820d8d506911d53582a780104f27d50c646d9",
          "md5": "d3e8682f519819ac0cbe836a43b8af94",
          "sha256": "c8e2926412b4a7ba5332cb3240bd369ceff89a88ec56f0c5f7df7f4a8b606386"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d3e8682f519819ac0cbe836a43b8af94",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15536,
        "upload_time": "2023-03-27T19:27:40",
        "upload_time_iso_8601": "2023-03-27T19:27:40.043878Z",
        "url": "https://files.pythonhosted.org/packages/0b/aa/59c9607bbde471099322c7d820d8d506911d53582a780104f27d50c646d9/tetun_tokenizer-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b822ff1bcec935a2d25e215277e760b37bccee7bd86d6136e1ea8ee6f24e2074",
          "md5": "0f4b75332f08aea6073580c66bff5310",
          "sha256": "d101b43a004746f5b64a21d03a23e499a066c66a81a6a610f10d8e66c391375b"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0f4b75332f08aea6073580c66bff5310",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15236,
        "upload_time": "2023-03-27T19:27:42",
        "upload_time_iso_8601": "2023-03-27T19:27:42.410240Z",
        "url": "https://files.pythonhosted.org/packages/b8/22/ff1bcec935a2d25e215277e760b37bccee7bd86d6136e1ea8ee6f24e2074/tetun_tokenizer-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "af222063f61c1776298720aa621acd3c5c53a00b0a8027b19e59646d4826e6c2",
          "md5": "bae98be62cf961feb6ae9c9190e65f65",
          "sha256": "6dba5724ee2203cdbcb7f17dddc7f7b1bb0e55b916200ae32ff492cf7a7ef2d0"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bae98be62cf961feb6ae9c9190e65f65",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15648,
        "upload_time": "2023-03-27T20:25:58",
        "upload_time_iso_8601": "2023-03-27T20:25:58.753776Z",
        "url": "https://files.pythonhosted.org/packages/af/22/2063f61c1776298720aa621acd3c5c53a00b0a8027b19e59646d4826e6c2/tetun_tokenizer-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7df3850a6766bf31d25e56880c1bfdd92d5f8c07304073e555e9a42efde23e68",
          "md5": "2f8e14199929c07e7405ddff74332234",
          "sha256": "5667691284ef78e0e836e8ca5d08a0e413ce5dfc5729376508e5b926513a7e67"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "2f8e14199929c07e7405ddff74332234",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15308,
        "upload_time": "2023-03-27T20:26:01",
        "upload_time_iso_8601": "2023-03-27T20:26:01.853593Z",
        "url": "https://files.pythonhosted.org/packages/7d/f3/850a6766bf31d25e56880c1bfdd92d5f8c07304073e555e9a42efde23e68/tetun_tokenizer-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e7278994952d4c6d6e56f389ba8d614cda9c92d8b2596317b05873106f212681",
          "md5": "24aa4948dc9b3f29da9155930ba86285",
          "sha256": "ae23eb33a91ca4f5dd0ee31f672ef53b41ac2ac6897a5478403c3efb8739a7ea"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "24aa4948dc9b3f29da9155930ba86285",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15643,
        "upload_time": "2023-03-27T20:29:14",
        "upload_time_iso_8601": "2023-03-27T20:29:14.155893Z",
        "url": "https://files.pythonhosted.org/packages/e7/27/8994952d4c6d6e56f389ba8d614cda9c92d8b2596317b05873106f212681/tetun_tokenizer-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cf363f215fa76a8a35bb5236dcabd9ac4165a43fb69acb9d0c8bf1bd787ad098",
          "md5": "a5992ee90d41a0f7ba52e6b18bd8f3d0",
          "sha256": "666d05a45b1ff4c1e02fe0b3b3658ba44ac09682c3aa29f3def68c0b08f3df8e"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "a5992ee90d41a0f7ba52e6b18bd8f3d0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15301,
        "upload_time": "2023-03-27T20:29:18",
        "upload_time_iso_8601": "2023-03-27T20:29:18.232629Z",
        "url": "https://files.pythonhosted.org/packages/cf/36/3f215fa76a8a35bb5236dcabd9ac4165a43fb69acb9d0c8bf1bd787ad098/tetun_tokenizer-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8fcbd867ca3ae1b3485b7b413fdaf5479fbf6a668f2d3f69bca50af2f6b464d0",
          "md5": "f1c3dd32de19a92bb5c7ab84b965f278",
          "sha256": "4f8ca737d1c0882adc339acd1a9df9b5e679f8edcc887e43953fde989b1cfa54"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f1c3dd32de19a92bb5c7ab84b965f278",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15641,
        "upload_time": "2023-03-27T20:46:59",
        "upload_time_iso_8601": "2023-03-27T20:46:59.936576Z",
        "url": "https://files.pythonhosted.org/packages/8f/cb/d867ca3ae1b3485b7b413fdaf5479fbf6a668f2d3f69bca50af2f6b464d0/tetun_tokenizer-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ab2660adb87ec75462e66d07d6aa611505de06be7e9ff7edffb7e34af199c38",
          "md5": "c13fbefbead9151060c844a0f3daa894",
          "sha256": "f45d078169708fca4e62190591e8d01ec27489f7327446a75c2aa2f9e74a2eda"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "c13fbefbead9151060c844a0f3daa894",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15301,
        "upload_time": "2023-03-27T20:47:05",
        "upload_time_iso_8601": "2023-03-27T20:47:05.895467Z",
        "url": "https://files.pythonhosted.org/packages/5a/b2/660adb87ec75462e66d07d6aa611505de06be7e9ff7edffb7e34af199c38/tetun_tokenizer-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5095a7fab01842286b5c927319206dc9b218858851129b593aba01df1077d679",
          "md5": "79485abb30519c93635c1e62dc5adc46",
          "sha256": "2b8cc16fe3cb5c43106c3d3b9773c40da3c27963600f2e487afe5d3994cc211b"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "79485abb30519c93635c1e62dc5adc46",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15647,
        "upload_time": "2023-03-27T21:02:17",
        "upload_time_iso_8601": "2023-03-27T21:02:17.954506Z",
        "url": "https://files.pythonhosted.org/packages/50/95/a7fab01842286b5c927319206dc9b218858851129b593aba01df1077d679/tetun_tokenizer-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "35b5aa747e3cd5135c942743e8086023e70d974bac1efea63f2df8d62000d1cb",
          "md5": "f35f98294a615aede8a9d82d31c134b8",
          "sha256": "0fdd144cc18ad8f345819a252036b8cfc2349628c89edbdcd27d514ba3312052"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "f35f98294a615aede8a9d82d31c134b8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15307,
        "upload_time": "2023-03-27T21:02:23",
        "upload_time_iso_8601": "2023-03-27T21:02:23.694336Z",
        "url": "https://files.pythonhosted.org/packages/35/b5/aa747e3cd5135c942743e8086023e70d974bac1efea63f2df8d62000d1cb/tetun_tokenizer-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1a153973ef5a6aa646c11cb0da4365420aa03c1ddc7ef1f47dca88c51e5412c6",
          "md5": "1a54e355c934bd5d5ab8abdca5c345ff",
          "sha256": "84cb850829d1619fe2979383996c5eb44a3c213bb4e278783560258ab6736d98"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1a54e355c934bd5d5ab8abdca5c345ff",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15640,
        "upload_time": "2023-03-27T21:05:05",
        "upload_time_iso_8601": "2023-03-27T21:05:05.291716Z",
        "url": "https://files.pythonhosted.org/packages/1a/15/3973ef5a6aa646c11cb0da4365420aa03c1ddc7ef1f47dca88c51e5412c6/tetun_tokenizer-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fcd367ffcfb58042d68660902f67526d238c6c86870a04193b2c147aeb83c225",
          "md5": "d126b7139684cb9ba99e788cd91538a6",
          "sha256": "8df9f4c1eddcbf8ca37a55386a3393529c207d0fda6c442ddd94f4ed81a0fe86"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "d126b7139684cb9ba99e788cd91538a6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15302,
        "upload_time": "2023-03-27T21:05:13",
        "upload_time_iso_8601": "2023-03-27T21:05:13.981755Z",
        "url": "https://files.pythonhosted.org/packages/fc/d3/67ffcfb58042d68660902f67526d238c6c86870a04193b2c147aeb83c225/tetun_tokenizer-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e3809191c5b479cad33b222ce88cbce9f19e1d24f21c167c9a009b31e33db850",
          "md5": "d74cbb5e97f5ef0224196fc52f267b36",
          "sha256": "04d2ba06e8ee2a171bb0e94dabff769fcab239b587c9ab7b47cf43e47b8c2e39"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d74cbb5e97f5ef0224196fc52f267b36",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15678,
        "upload_time": "2023-03-27T21:52:47",
        "upload_time_iso_8601": "2023-03-27T21:52:47.074691Z",
        "url": "https://files.pythonhosted.org/packages/e3/80/9191c5b479cad33b222ce88cbce9f19e1d24f21c167c9a009b31e33db850/tetun_tokenizer-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2b4e181f7f8de9ed3e8c489ce4510c1edf644ce380082276753f27d883ea0707",
          "md5": "3d415155020208d6562a2e07dbb17a39",
          "sha256": "fe05d61666774cde999005e3db5a2f091edb888a797973303a56aac9ca62f599"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3d415155020208d6562a2e07dbb17a39",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15347,
        "upload_time": "2023-03-27T21:52:56",
        "upload_time_iso_8601": "2023-03-27T21:52:56.375489Z",
        "url": "https://files.pythonhosted.org/packages/2b/4e/181f7f8de9ed3e8c489ce4510c1edf644ce380082276753f27d883ea0707/tetun_tokenizer-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "35f4a92e2df2a170f50c3c32dea7ab7a2059f45cc2c0579870443319474fa6fd",
          "md5": "9be7125720c7b4bc04d20d8933e5c347",
          "sha256": "27defda2a61bfa221923c089a78d9cd0424cde3b6aaea0bb740baa887954f2d3"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9be7125720c7b4bc04d20d8933e5c347",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15872,
        "upload_time": "2023-03-27T22:20:16",
        "upload_time_iso_8601": "2023-03-27T22:20:16.124717Z",
        "url": "https://files.pythonhosted.org/packages/35/f4/a92e2df2a170f50c3c32dea7ab7a2059f45cc2c0579870443319474fa6fd/tetun_tokenizer-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f77cd24d9cb329293e5c40de8088e04feb66025d56f6050851b2b8e547f63a78",
          "md5": "50658d67f0ada4959d559a34306089db",
          "sha256": "aba8e5b711dc69b28f1607011177bca6ffaf242f847195009f0c2afcb8567ce3"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "50658d67f0ada4959d559a34306089db",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15519,
        "upload_time": "2023-03-27T22:20:24",
        "upload_time_iso_8601": "2023-03-27T22:20:24.508201Z",
        "url": "https://files.pythonhosted.org/packages/f7/7c/d24d9cb329293e5c40de8088e04feb66025d56f6050851b2b8e547f63a78/tetun_tokenizer-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "76c6826a5ddfc5ef9e054e887113c90aba09df014ee71d60d83c68ef175a4f45",
          "md5": "145c0453976b897dc3b59c954242052e",
          "sha256": "3d163fecd66d6a0dc3dd7c0effe993fc8f9fcc7ffcce71ea1bf9587e91f3c206"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "145c0453976b897dc3b59c954242052e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15883,
        "upload_time": "2023-03-27T22:34:28",
        "upload_time_iso_8601": "2023-03-27T22:34:28.363317Z",
        "url": "https://files.pythonhosted.org/packages/76/c6/826a5ddfc5ef9e054e887113c90aba09df014ee71d60d83c68ef175a4f45/tetun_tokenizer-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ebe05251e3eaebc6202467c6caec75e74cbbc9999bebdbc3a70048bded5b307c",
          "md5": "5a6d38186d299a235d740e2209a2b490",
          "sha256": "5ea87bd44d550e928bd1ffa88d707994a96449f4e549bce1ac18f79c81046d94"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "5a6d38186d299a235d740e2209a2b490",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15529,
        "upload_time": "2023-03-27T22:34:30",
        "upload_time_iso_8601": "2023-03-27T22:34:30.752065Z",
        "url": "https://files.pythonhosted.org/packages/eb/e0/5251e3eaebc6202467c6caec75e74cbbc9999bebdbc3a70048bded5b307c/tetun_tokenizer-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7051934851de6a590bb923ac4eb25ea1b3cab745916d9189589bb58ef225f807",
          "md5": "e2ec2abace8c45787cd7cee4d0c0470b",
          "sha256": "bb254446b88753f65f0dbf0324708486acead8e68b7151690ce7b9b8b59bf992"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e2ec2abace8c45787cd7cee4d0c0470b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15883,
        "upload_time": "2023-03-28T00:46:16",
        "upload_time_iso_8601": "2023-03-28T00:46:16.069993Z",
        "url": "https://files.pythonhosted.org/packages/70/51/934851de6a590bb923ac4eb25ea1b3cab745916d9189589bb58ef225f807/tetun_tokenizer-1.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "98bf6706b16ccabe8fc4e7fa151e4e461e92029a3f966d8516756c7e7d02fed0",
          "md5": "33a70497439410a6d3c1a9ecc159e93f",
          "sha256": "8b0f797ac2c9004477c0c3bd75e8013516d47769b42f08c8f595b35e7699ffbc"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "33a70497439410a6d3c1a9ecc159e93f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15528,
        "upload_time": "2023-03-28T00:46:19",
        "upload_time_iso_8601": "2023-03-28T00:46:19.650577Z",
        "url": "https://files.pythonhosted.org/packages/98/bf/6706b16ccabe8fc4e7fa151e4e461e92029a3f966d8516756c7e7d02fed0/tetun_tokenizer-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0690ed86a480c9878ff496bfd2132883b9c7309a2ec31e77b2f31acdd53ba95a",
          "md5": "41dc0a97d0c8d1d6a417ee9960f1a507",
          "sha256": "b7d11627c1e4b3cbb1f85d12f0798c21204fda38c2a7750802002dfd0c876a58"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "41dc0a97d0c8d1d6a417ee9960f1a507",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15875,
        "upload_time": "2023-03-28T01:32:58",
        "upload_time_iso_8601": "2023-03-28T01:32:58.602513Z",
        "url": "https://files.pythonhosted.org/packages/06/90/ed86a480c9878ff496bfd2132883b9c7309a2ec31e77b2f31acdd53ba95a/tetun_tokenizer-1.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "90990183be95be0af94994ff6aad3da52f7f1db529407536efaf564278fa76c8",
          "md5": "4f52928f60d9d8fb42d70953b61c1cf0",
          "sha256": "4462b73b5507cf4d5dbf24c1f2a8f76821558be5c3a2b509f137053493d0c0c7"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "4f52928f60d9d8fb42d70953b61c1cf0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15528,
        "upload_time": "2023-03-28T01:33:05",
        "upload_time_iso_8601": "2023-03-28T01:33:05.523705Z",
        "url": "https://files.pythonhosted.org/packages/90/99/0183be95be0af94994ff6aad3da52f7f1db529407536efaf564278fa76c8/tetun_tokenizer-1.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b2a802136c91535594f6a0d521bf97033f207d645858008e3812c0c187eec5d5",
          "md5": "4653dafa2c9ed72cd0cb9d9528ced0fa",
          "sha256": "1b8afad6eb13c9db7d91e96312ca04b981e5189cca33c82649068549276d90f3"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4653dafa2c9ed72cd0cb9d9528ced0fa",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 15879,
        "upload_time": "2023-03-28T01:36:30",
        "upload_time_iso_8601": "2023-03-28T01:36:30.296322Z",
        "url": "https://files.pythonhosted.org/packages/b2/a8/02136c91535594f6a0d521bf97033f207d645858008e3812c0c187eec5d5/tetun_tokenizer-1.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cdb9466a9ff0fdafb154eaa3bbea23829d7255f40842f0179147ab0a17844f8d",
          "md5": "1d6184e59870c7dd0d908c3537f02e90",
          "sha256": "bb169a4219ba4ac67fc300abe3ecf65f4b1347752a71ab896decc2ec48c3327d"
        },
        "downloads": -1,
        "filename": "tetun_tokenizer-1.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "1d6184e59870c7dd0d908c3537f02e90",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 15531,
        "upload_time": "2023-03-28T01:36:33",
        "upload_time_iso_8601": "2023-03-28T01:36:33.532780Z",
        "url": "https://files.pythonhosted.org/packages/cd/b9/466a9ff0fdafb154eaa3bbea23829d7255f40842f0179147ab0a17844f8d/tetun_tokenizer-1.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b2a802136c91535594f6a0d521bf97033f207d645858008e3812c0c187eec5d5",
        "md5": "4653dafa2c9ed72cd0cb9d9528ced0fa",
        "sha256": "1b8afad6eb13c9db7d91e96312ca04b981e5189cca33c82649068549276d90f3"
      },
      "downloads": -1,
      "filename": "tetun_tokenizer-1.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4653dafa2c9ed72cd0cb9d9528ced0fa",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 15879,
      "upload_time": "2023-03-28T01:36:30",
      "upload_time_iso_8601": "2023-03-28T01:36:30.296322Z",
      "url": "https://files.pythonhosted.org/packages/b2/a8/02136c91535594f6a0d521bf97033f207d645858008e3812c0c187eec5d5/tetun_tokenizer-1.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cdb9466a9ff0fdafb154eaa3bbea23829d7255f40842f0179147ab0a17844f8d",
        "md5": "1d6184e59870c7dd0d908c3537f02e90",
        "sha256": "bb169a4219ba4ac67fc300abe3ecf65f4b1347752a71ab896decc2ec48c3327d"
      },
      "downloads": -1,
      "filename": "tetun_tokenizer-1.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "1d6184e59870c7dd0d908c3537f02e90",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 15531,
      "upload_time": "2023-03-28T01:36:33",
      "upload_time_iso_8601": "2023-03-28T01:36:33.532780Z",
      "url": "https://files.pythonhosted.org/packages/cd/b9/466a9ff0fdafb154eaa3bbea23829d7255f40842f0179147ab0a17844f8d/tetun_tokenizer-1.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}