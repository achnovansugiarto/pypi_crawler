{
  "info": {
    "author": "Phil Wang",
    "author_email": "lucidrains@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/lucidrains/molecule-attention-transformer",
    "keywords": "artificial intelligence,attention mechanism,molecules",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "molecule-attention-transformer",
    "package_url": "https://pypi.org/project/molecule-attention-transformer/",
    "platform": "",
    "project_url": "https://pypi.org/project/molecule-attention-transformer/",
    "project_urls": {
      "Homepage": "https://github.com/lucidrains/molecule-attention-transformer"
    },
    "release_url": "https://pypi.org/project/molecule-attention-transformer/0.0.4/",
    "requires_dist": [
      "torch (>=1.6)",
      "einops (>=0.3)"
    ],
    "requires_python": "",
    "summary": "Molecule Attention Transformer - Pytorch",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8796876,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a538b46f691570c4ff2094d31bdd0e705bc3bd92f4e9fb81fb14cccd29ada3c0",
          "md5": "7abb528e87852a022d89bdb8d7e47ca5",
          "sha256": "fc13637b3fcd20708d9390434912c154b36f48e7f516738dbb1d852090c62fa4"
        },
        "downloads": -1,
        "filename": "molecule_attention_transformer-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7abb528e87852a022d89bdb8d7e47ca5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 4193,
        "upload_time": "2020-12-01T20:44:56",
        "upload_time_iso_8601": "2020-12-01T20:44:56.818433Z",
        "url": "https://files.pythonhosted.org/packages/a5/38/b46f691570c4ff2094d31bdd0e705bc3bd92f4e9fb81fb14cccd29ada3c0/molecule_attention_transformer-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6900e87365abdc5ee456b2009d6f32f0959f4002685f704df7b6e13f6a3511de",
          "md5": "576417fd73227efb4c7da6bdc1bdbb8c",
          "sha256": "88d6209af53e53dd2d956e75d8f18bb80f6ecb3d071fb264156cbadb9e2ef6f0"
        },
        "downloads": -1,
        "filename": "molecule-attention-transformer-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "576417fd73227efb4c7da6bdc1bdbb8c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 3394,
        "upload_time": "2020-12-01T20:44:58",
        "upload_time_iso_8601": "2020-12-01T20:44:58.055394Z",
        "url": "https://files.pythonhosted.org/packages/69/00/e87365abdc5ee456b2009d6f32f0959f4002685f704df7b6e13f6a3511de/molecule-attention-transformer-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bdbc3d040d2deaa4a7a29288627ae824f6642dc55bf10a76c86e96baf109b9c2",
          "md5": "eb7a0e9bcaefd2b7f581edc3b56c2fea",
          "sha256": "49009d16b2402a294051fd9bd67212891c05d2d4a3036967435015d6506ca671"
        },
        "downloads": -1,
        "filename": "molecule_attention_transformer-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eb7a0e9bcaefd2b7f581edc3b56c2fea",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 4217,
        "upload_time": "2020-12-01T20:52:26",
        "upload_time_iso_8601": "2020-12-01T20:52:26.408814Z",
        "url": "https://files.pythonhosted.org/packages/bd/bc/3d040d2deaa4a7a29288627ae824f6642dc55bf10a76c86e96baf109b9c2/molecule_attention_transformer-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "29a3d7b4a72da3ecb24115e0eba622d868044db2b715f1fbe2d98be0a835bf3d",
          "md5": "9a9bc2e50b8429a011ce8fd7638739c2",
          "sha256": "556311dbe9bd0e4db5de47f60f1e56ef435c17a9f8d6225371f29934fb689474"
        },
        "downloads": -1,
        "filename": "molecule-attention-transformer-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "9a9bc2e50b8429a011ce8fd7638739c2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 3418,
        "upload_time": "2020-12-01T20:52:27",
        "upload_time_iso_8601": "2020-12-01T20:52:27.270676Z",
        "url": "https://files.pythonhosted.org/packages/29/a3/d7b4a72da3ecb24115e0eba622d868044db2b715f1fbe2d98be0a835bf3d/molecule-attention-transformer-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "68dc856bd35922d74a0b35f742003d4c8b3ce9c3c9f6828299ea5cdaf365f930",
          "md5": "5aa4433b4de5e9f8fc1e6c557d673bc6",
          "sha256": "b5ae4224d21fd12425ee43397c7784b24c813ba4e19105faf6d8c64b3bb0f080"
        },
        "downloads": -1,
        "filename": "molecule_attention_transformer-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5aa4433b4de5e9f8fc1e6c557d673bc6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 4253,
        "upload_time": "2020-12-01T20:55:08",
        "upload_time_iso_8601": "2020-12-01T20:55:08.729547Z",
        "url": "https://files.pythonhosted.org/packages/68/dc/856bd35922d74a0b35f742003d4c8b3ce9c3c9f6828299ea5cdaf365f930/molecule_attention_transformer-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "855cd7fab17372dbdb1ae3235405624bcdea867388a5761d797e6c1d76c03031",
          "md5": "1b25f7ee82247e17419f0fa365db2093",
          "sha256": "c4af8ff3ca4162a9861c67f5b70f0fe73e401044bdb31195b857050bf7ba9563"
        },
        "downloads": -1,
        "filename": "molecule-attention-transformer-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "1b25f7ee82247e17419f0fa365db2093",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 3466,
        "upload_time": "2020-12-01T20:55:10",
        "upload_time_iso_8601": "2020-12-01T20:55:10.485911Z",
        "url": "https://files.pythonhosted.org/packages/85/5c/d7fab17372dbdb1ae3235405624bcdea867388a5761d797e6c1d76c03031/molecule-attention-transformer-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1904c459485e5e61d9284a0b7d97a398c33a68325d398cf2ebacaf0577ef49a7",
          "md5": "1eaaac8cbc404ce5137e5691e43096c5",
          "sha256": "4d15a45e9824da9fc22aed4c15a31d681650220bb3f1a208169f7ac1df640a0c"
        },
        "downloads": -1,
        "filename": "molecule_attention_transformer-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1eaaac8cbc404ce5137e5691e43096c5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 4389,
        "upload_time": "2020-12-02T04:15:03",
        "upload_time_iso_8601": "2020-12-02T04:15:03.468709Z",
        "url": "https://files.pythonhosted.org/packages/19/04/c459485e5e61d9284a0b7d97a398c33a68325d398cf2ebacaf0577ef49a7/molecule_attention_transformer-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a004e4fec5d875b647089678fcfaa88589e0687c2b11fdd9299dd528bdfb12e4",
          "md5": "d19f023105b630ec687b2f2d4cd46020",
          "sha256": "3bfac3f212b192d12d12b076be929c264da0447fa0d2080155a780ec3039f37b"
        },
        "downloads": -1,
        "filename": "molecule-attention-transformer-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "d19f023105b630ec687b2f2d4cd46020",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 3808,
        "upload_time": "2020-12-02T04:15:04",
        "upload_time_iso_8601": "2020-12-02T04:15:04.647884Z",
        "url": "https://files.pythonhosted.org/packages/a0/04/e4fec5d875b647089678fcfaa88589e0687c2b11fdd9299dd528bdfb12e4/molecule-attention-transformer-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1904c459485e5e61d9284a0b7d97a398c33a68325d398cf2ebacaf0577ef49a7",
        "md5": "1eaaac8cbc404ce5137e5691e43096c5",
        "sha256": "4d15a45e9824da9fc22aed4c15a31d681650220bb3f1a208169f7ac1df640a0c"
      },
      "downloads": -1,
      "filename": "molecule_attention_transformer-0.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1eaaac8cbc404ce5137e5691e43096c5",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 4389,
      "upload_time": "2020-12-02T04:15:03",
      "upload_time_iso_8601": "2020-12-02T04:15:03.468709Z",
      "url": "https://files.pythonhosted.org/packages/19/04/c459485e5e61d9284a0b7d97a398c33a68325d398cf2ebacaf0577ef49a7/molecule_attention_transformer-0.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a004e4fec5d875b647089678fcfaa88589e0687c2b11fdd9299dd528bdfb12e4",
        "md5": "d19f023105b630ec687b2f2d4cd46020",
        "sha256": "3bfac3f212b192d12d12b076be929c264da0447fa0d2080155a780ec3039f37b"
      },
      "downloads": -1,
      "filename": "molecule-attention-transformer-0.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "d19f023105b630ec687b2f2d4cd46020",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 3808,
      "upload_time": "2020-12-02T04:15:04",
      "upload_time_iso_8601": "2020-12-02T04:15:04.647884Z",
      "url": "https://files.pythonhosted.org/packages/a0/04/e4fec5d875b647089678fcfaa88589e0687c2b11fdd9299dd528bdfb12e4/molecule-attention-transformer-0.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}