{
  "info": {
    "author": "Charles Condevaux",
    "author_email": "charles.condevaux@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# LSG Attention: Extrapolation of pretrained Transformers to long sequences\n\nArXiv [paper](https://arxiv.org/abs/2210.15497) \\\nAccepted as a conference paper in PAKDD 2023.\n\n\nRequires `transformers >= 4.23.1`\n\nOptional package to convert models:\n```bash\npip install lsg-converter\n```\n\n* [Compatible models](#compatible-models)\n* [Efficiency](#efficiency)\n* [Conversion](#convert-checkpoint-to-lsg)\n* [Usage](#model-usage)\n* [Block-Local-Attention](#block-local-attention)\n* [LSG-Attention](#lsg-attention)\n* [Experiments](#experiments)\n\n# Compatible models\n\nThis script converts HuggingFace checkpoints to its LSG (Local-Sparse-Global) variant to handle long sequences. Available model types:\n\n* ALBERT [`\"albert\"`]\n* BART [`\"bart\"`] (encoder attention modified only)\n* BARThez [`\"barthez\"`] (encoder attention modified only)\n* BERT [`\"bert\"`]\n* CamemBERT [`\"camembert\"`]\n* DistilBERT [`\"distilbert\"`]\n* Electra [`\"electra\"`]\n* mBART [`\"mbart\"`] (not tested extensively, encoder attention modified only)\n* Pegasus [`\"pegasus\"`] (not tested extensively, encoder attention modified only)\n* RoBERTa [`\"roberta\"`]\n* XLM-RoBERTa [`\"xlm-roberta\"`]\n\nSome converted checkpoints are available [here](https://huggingface.co/ccdv).\n\n![attn](img/attn.png)\n\n# Efficiency\nMemory and training speed for a binary classification task with 4x4096 tokens batches and Adam (RTX 8000).\n\n| Models                     | Seconds per step | Memory w/ attn dropout  | Memory w/o attn dropout |\n|----------------------------|------------------|-------------------------|-------------------------|\n| Longformer-base            | 3.22 s/step      | 34.38 Gb                | 32.83 Gb                |\n| BigBird-RoBERTa-base       | 2.85 s/step      | 38.13 Gb                | 38.13 Gb (no effect)    |\n| LSG-RoBERTa 256/0          | 1.40 s/step      | 32.92 Gb                | 24.80 Gb                |\n| LSG-RoBERTa 128/128 (norm) | 1.51 s/step      | 33.80 Gb                | 27.52 Gb                |\n| LSG-RoBERTa 32/32 (norm)   | 1.20 s/step      | 24.53 Gb                | 22.53 Gb                |\n\n\n\n# Convert checkpoint to LSG \n\nModels can be converted with or without the `lsg-converter` package.\n\n## With package\n\nBERT example with the `lsg-converter` package:\n\n```python\nfrom lsg_converter import LSGConverter\n\nconverter = LSGConverter(max_sequence_length=4096)\n\n# Example 1\nmodel, tokenizer = converter.convert_from_pretrained(\"bert-base-uncased\", num_global_tokens=7)\nprint(type(model))\n# <class 'lsg_converter.bert.modeling_lsg_bert.LSGBertForMaskedLM'>\n\n# Example 2\nmodel, tokenizer = converter.convert_from_pretrained(\"bert-base-uncased\", architecture=\"BertForSequenceClassification\", use_auth_token=True)\nprint(type(model))\n# <class 'lsg_converter.bert.modeling_lsg_bert.LSGBertForSequenceClassification'>\n```\n\n## Without package\n\nUse `convert_checkpoint.py` to convert a model (check model_type from config.json). \\\nThe architecture of the model is inferred from the config file, but you can specify a different one if the config is incorrect (which can happen for BART models), see  `python convert_checkpoint.py --help`. \\\nTo test the converted model, add `--run_test` (experimental).\n\nRoBERTa example (from `RobertaForMaskedLM` to `RobertaForSequenceClassification`) without package:\n```bash\ngit clone https://github.com/ccdv-ai/convert_checkpoint_to_lsg.git\ncd convert_checkpoint_to_lsg\n\nexport MODEL_TO_CONVERT=roberta-base\nexport MODEL_NAME=lsg-roberta-base\nexport MAX_LENGTH=4096\n\npython convert_checkpoint.py \\\n    --initial_model $MODEL_TO_CONVERT \\\n    --model_name $MODEL_NAME \\\n    --model_kwargs \"{'mask_first_token': true, 'sparsity_type': 'lsh', 'block_size': 32}\" \\\n    --architecture RobertaForSequenceClassification \\\n    --max_sequence_length $MAX_LENGTH\n```\n\n# Model Usage\n\nWorks with the AutoClass.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\n# Load created model\nMODEL_NAME = \"lsg-roberta-base\"\nSENTENCE = \"This is a test sentence.\"\n\nmodel = AutoModelForMaskedLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ninputs = tokenizer(SENTENCE, return_tensors=\"pt\")\nmodel(**inputs)\n```\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# Load created model\nMODEL_NAME = \"lsg-roberta-base\"\nSENTENCE = \"This is a test sentence.\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ninputs = tokenizer(SENTENCE, return_tensors=\"pt\")\nmodel(**inputs)\n```\n\n# Block-Local-Attention\n\nFor those who only want a vanilla Block-Local-Attention module, see `/block_local_attention`:\n\n```python\nfrom block_local_attention import *\n\n# batch, num_heads, sequence length, hidden_size\nn, h, t, d = 2, 4, 58, 32  \n\nQ, K, V = torch.randn(n, h, t, d), torch.randn(n, h, t, d), torch.randn(n, h, t, d)\nattention_mask = torch.zeros(n, 1, 1, t).float()\n\nattn = BlockLocalSelfAttention(block_size=16, compute_global_attention=True, is_causal=False, attention_dropout_prob=0.1)\n\n# expect (n, h, t, d) inputs,\n# attention_mask is (n, 1, 1, t) or (n, 1, t, t) for causal\n# attention_mask is 0 for no mask, -inf for mask (similar to most HuggingFace models)\noutputs = attn(Q, K, V, attention_mask)\n\nprint(outputs.shape)\n> torch.Size([2, 4, 58, 32])\n```\n\n# LSG Attention\n\nLSG relies on 3 components: block local attention, sparse attention and prepended global attention.\n\n## Parameters\nYou can change various parameters like : \n* local block size (block_size=128)\n* sparse block size (sparse_block_size=128)\n* sparsity factor (sparsity_factor=2)\n* mask_first_token (mask first token since it is redundant with the first global token)\n* the number of global tokens (num_global_tokens=1)\n* see config.json file\n\n## Sparse selection type\nThere are 5 different sparse selection patterns. The best type is task dependent. \\\nIf `sparse_block_size=0` or `sparsity_type=\"none\"`, only local attention is considered. \\\nNote that for sequences with length < 2*block_size, the type has no effect.\n* `sparsity_type=\"norm\"`, select highest norm tokens\n    * Works best for a small sparsity_factor (2 to 4)\n    * Additional parameters:\n        * None\n* `sparsity_type=\"pooling\"`, use average pooling to merge tokens\n    * Works best for a small sparsity_factor (2 to 4)\n    * Additional parameters:\n        * None\n* `sparsity_type=\"lsh\"`, use the LSH algorithm to cluster similar tokens\n    * Works best for a large sparsity_factor (4+)\n    * LSH relies on random projections, thus inference may differ slightly with different seeds\n    * Additional parameters:\n        * lsg_num_pre_rounds=1, pre merge tokens n times before computing centroids\n* `sparsity_type=\"stride\"`, use a striding mecanism per head\n    * Each head will use different tokens strided by sparsify_factor\n    * Not recommended if sparsify_factor > num_heads\n* `sparsity_type=\"block_stride\"`, use a striding mecanism per head\n    * Each head will use block of tokens strided by sparsify_factor\n    * Not recommended if sparsify_factor > num_heads\n\n# Experiments\n\nSee `experiments/`\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ccdv-ai/convert_checkpoint_to_lsg",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lsg-converter",
    "package_url": "https://pypi.org/project/lsg-converter/",
    "platform": null,
    "project_url": "https://pypi.org/project/lsg-converter/",
    "project_urls": {
      "Homepage": "https://github.com/ccdv-ai/convert_checkpoint_to_lsg"
    },
    "release_url": "https://pypi.org/project/lsg-converter/0.0.5/",
    "requires_dist": [
      "transformers (>=4.23.1)",
      "torch (>=1.7)"
    ],
    "requires_python": ">=3.7",
    "summary": "To convert HuggingFace models to their LSG variant",
    "version": "0.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17126142,
  "releases": {
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d5edf8d545c4d3eb6c6bfb0f63fcd65f46107e708bb0ded4919bf1dec8b84e7d",
          "md5": "9eb9310c4b94085391ae3b5d480b65cf",
          "sha256": "51d5a1c0129112c43ced88aa821845359ada3e05771a0c71331dfce61ca41afa"
        },
        "downloads": -1,
        "filename": "lsg_converter-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9eb9310c4b94085391ae3b5d480b65cf",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 118167,
        "upload_time": "2022-11-01T11:52:36",
        "upload_time_iso_8601": "2022-11-01T11:52:36.187810Z",
        "url": "https://files.pythonhosted.org/packages/d5/ed/f8d545c4d3eb6c6bfb0f63fcd65f46107e708bb0ded4919bf1dec8b84e7d/lsg_converter-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "af2f59888feef49d54dde37bfa13430d1326415194de39692fc964ab69d52621",
          "md5": "2065c1aede6ef387dd899109dee13a80",
          "sha256": "ff98e0732519758e4aceb8a0da1904703700a1439a3cf2bb6aaf89e5e489cd75"
        },
        "downloads": -1,
        "filename": "lsg-converter-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "2065c1aede6ef387dd899109dee13a80",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 107703,
        "upload_time": "2022-11-01T11:52:38",
        "upload_time_iso_8601": "2022-11-01T11:52:38.772953Z",
        "url": "https://files.pythonhosted.org/packages/af/2f/59888feef49d54dde37bfa13430d1326415194de39692fc964ab69d52621/lsg-converter-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "77d28c0f002d656d6e671ee8a4ac2fb1f830fe3c50924e121d1c9ead82f5ed01",
          "md5": "6b876a0ca9d0a752853e63f9cc4273e5",
          "sha256": "b13698f9a553dbef856cf71603949c25c192d88762e16531345f29b7e8f5e164"
        },
        "downloads": -1,
        "filename": "lsg_converter-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6b876a0ca9d0a752853e63f9cc4273e5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 118192,
        "upload_time": "2022-11-20T14:30:04",
        "upload_time_iso_8601": "2022-11-20T14:30:04.397494Z",
        "url": "https://files.pythonhosted.org/packages/77/d2/8c0f002d656d6e671ee8a4ac2fb1f830fe3c50924e121d1c9ead82f5ed01/lsg_converter-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8defcd70b4b649c5f7a0d3d36dbdd01e0aea21adc0a9b3aa31888b3d5bae0d82",
          "md5": "4a231de48b5ca6b68397a253837249bf",
          "sha256": "de6f30aefa6092b5d68bbff9897001e16b8b7919b822fbe95411233548f7225c"
        },
        "downloads": -1,
        "filename": "lsg-converter-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "4a231de48b5ca6b68397a253837249bf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 107758,
        "upload_time": "2022-11-20T14:30:05",
        "upload_time_iso_8601": "2022-11-20T14:30:05.970017Z",
        "url": "https://files.pythonhosted.org/packages/8d/ef/cd70b4b649c5f7a0d3d36dbdd01e0aea21adc0a9b3aa31888b3d5bae0d82/lsg-converter-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "571e09f5d30c55f721841e765fbd7bbfa595751a20dee98ae75b09e427b95193",
          "md5": "a1bca1f39aadcc5e6497354d06f23e93",
          "sha256": "9efa7ae90b2b9ae131afb3ddfbdf1eff00df3e65b503729b59566b2a085bbe1b"
        },
        "downloads": -1,
        "filename": "lsg_converter-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a1bca1f39aadcc5e6497354d06f23e93",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 118240,
        "upload_time": "2022-11-23T15:05:58",
        "upload_time_iso_8601": "2022-11-23T15:05:58.598327Z",
        "url": "https://files.pythonhosted.org/packages/57/1e/09f5d30c55f721841e765fbd7bbfa595751a20dee98ae75b09e427b95193/lsg_converter-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b761bc452e1aef695d6f4ea5ccbfcffa77bd488f70340afe7534b001478c1029",
          "md5": "3c8acf91bfcc73addde08f97ce609f2b",
          "sha256": "76cfb6c9bb6342c9afd587b5b2bd42cce6c683fc0c3050d5c487a758ec9ee70d"
        },
        "downloads": -1,
        "filename": "lsg-converter-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "3c8acf91bfcc73addde08f97ce609f2b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 107808,
        "upload_time": "2022-11-23T15:05:59",
        "upload_time_iso_8601": "2022-11-23T15:05:59.990353Z",
        "url": "https://files.pythonhosted.org/packages/b7/61/bc452e1aef695d6f4ea5ccbfcffa77bd488f70340afe7534b001478c1029/lsg-converter-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c89cd1f3fde29152cb2193b699dee9b9358e2990e675727cf92098d5c7d7ac1c",
          "md5": "031059ee74aab6da0ec9cb2d0e3c5a8c",
          "sha256": "63d221195a8967beb55467ea647446aa8c307efb955940b9a368f34dc87423ad"
        },
        "downloads": -1,
        "filename": "lsg_converter-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "031059ee74aab6da0ec9cb2d0e3c5a8c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 118255,
        "upload_time": "2023-03-02T11:28:54",
        "upload_time_iso_8601": "2023-03-02T11:28:54.192940Z",
        "url": "https://files.pythonhosted.org/packages/c8/9c/d1f3fde29152cb2193b699dee9b9358e2990e675727cf92098d5c7d7ac1c/lsg_converter-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2c37a498a8cfa948770eb127d71775435ea0783f6784e71d710b49107401bf54",
          "md5": "a51d957983c0777f490a718e27f71ec6",
          "sha256": "984534f1ce61519788204662cf9c09d9b59d82932f26e6480921baadce9fb208"
        },
        "downloads": -1,
        "filename": "lsg-converter-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "a51d957983c0777f490a718e27f71ec6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 107851,
        "upload_time": "2023-03-02T11:28:55",
        "upload_time_iso_8601": "2023-03-02T11:28:55.700621Z",
        "url": "https://files.pythonhosted.org/packages/2c/37/a498a8cfa948770eb127d71775435ea0783f6784e71d710b49107401bf54/lsg-converter-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c89cd1f3fde29152cb2193b699dee9b9358e2990e675727cf92098d5c7d7ac1c",
        "md5": "031059ee74aab6da0ec9cb2d0e3c5a8c",
        "sha256": "63d221195a8967beb55467ea647446aa8c307efb955940b9a368f34dc87423ad"
      },
      "downloads": -1,
      "filename": "lsg_converter-0.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "031059ee74aab6da0ec9cb2d0e3c5a8c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 118255,
      "upload_time": "2023-03-02T11:28:54",
      "upload_time_iso_8601": "2023-03-02T11:28:54.192940Z",
      "url": "https://files.pythonhosted.org/packages/c8/9c/d1f3fde29152cb2193b699dee9b9358e2990e675727cf92098d5c7d7ac1c/lsg_converter-0.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2c37a498a8cfa948770eb127d71775435ea0783f6784e71d710b49107401bf54",
        "md5": "a51d957983c0777f490a718e27f71ec6",
        "sha256": "984534f1ce61519788204662cf9c09d9b59d82932f26e6480921baadce9fb208"
      },
      "downloads": -1,
      "filename": "lsg-converter-0.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "a51d957983c0777f490a718e27f71ec6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 107851,
      "upload_time": "2023-03-02T11:28:55",
      "upload_time_iso_8601": "2023-03-02T11:28:55.700621Z",
      "url": "https://files.pythonhosted.org/packages/2c/37/a498a8cfa948770eb127d71775435ea0783f6784e71d710b49107401bf54/lsg-converter-0.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}