{
  "info": {
    "author": "The HuggingFace Team ft. Mage Team",
    "author_email": "greg@mage.space",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<p align=\"center\">\n    <br>\n    <img src=\"https://github.com/huggingface/diffusers/raw/main/docs/source/imgs/diffusers_library.jpg\" width=\"400\"/>\n    <br>\n<p>\n<p align=\"center\">\n    <a href=\"https://github.com/huggingface/diffusers/blob/main/LICENSE\">\n        <img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/datasets.svg?color=blue\">\n    </a>\n    <a href=\"https://github.com/huggingface/diffusers/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/diffusers.svg\">\n    </a>\n    <a href=\"CODE_OF_CONDUCT.md\">\n        <img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg\">\n    </a>\n</p>\n\nü§ó Diffusers provides pretrained diffusion models across multiple modalities, such as vision and audio, and serves\nas a modular toolbox for inference and training of diffusion models.\n\nMore precisely, ü§ó Diffusers offers:\n\n- State-of-the-art diffusion pipelines that can be run in inference with just a couple of lines of code (see [src/diffusers/pipelines](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines)). Check [this overview](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/README.md#pipelines-summary) to see all supported pipelines and their corresponding official papers.\n- Various noise schedulers that can be used interchangeably for the preferred speed vs. quality trade-off in inference (see [src/diffusers/schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers)).\n- Multiple types of models, such as UNet, can be used as building blocks in an end-to-end diffusion system (see [src/diffusers/models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models)).\n- Training examples to show how to train the most popular diffusion model tasks (see [examples](https://github.com/huggingface/diffusers/tree/main/examples), *e.g.* [unconditional-image-generation](https://github.com/huggingface/diffusers/tree/main/examples/unconditional_image_generation)).\n\n## Installation\n\n### For PyTorch\n\n**With `pip`** (official package)\n    \n```bash\npip install --upgrade diffusers[torch]\n```\n\n**With `conda`** (maintained by the community)\n\n```sh\nconda install -c conda-forge diffusers\n```\n\n### For Flax\n\n**With `pip`**\n\n```bash\npip install --upgrade diffusers[flax]\n```\n\n**Apple Silicon (M1/M2) support**\n\nPlease, refer to [the documentation](https://huggingface.co/docs/diffusers/optimization/mps).\n\n## Contributing\n\nWe ‚ù§Ô∏è  contributions from the open-source community! \nIf you want to contribute to this library, please check out our [Contribution guide](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md).\nYou can look out for [issues](https://github.com/huggingface/diffusers/issues) you'd like to tackle to contribute to the library.\n- See [Good first issues](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) for general opportunities to contribute\n- See [New model/pipeline](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+pipeline%2Fmodel%22) to contribute exciting new diffusion models / diffusion pipelines\n- See [New scheduler](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+scheduler%22)\n\nAlso, say üëã in our public Discord channel <a href=\"https://discord.gg/G7tWnz98XR\"><img alt=\"Join us on Discord\" src=\"https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white\"></a>. We discuss the hottest trends about diffusion models, help each other with contributions, personal projects or\njust hang out ‚òï.\n\n## Quickstart\n\nIn order to get started, we recommend taking a look at two notebooks:\n\n- The [Getting started with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb) notebook, which showcases an end-to-end example of usage for diffusion models, schedulers and pipelines.\n  Take a look at this notebook to learn how to use the pipeline abstraction, which takes care of everything (model, scheduler, noise handling) for you, and also to understand each independent building block in the library.\n- The [Training a diffusers model](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb) notebook summarizes diffusion models training methods. This notebook takes a step-by-step approach to training your\n  diffusion models on an image dataset, with explanatory graphics. \n  \n## Stable Diffusion is fully compatible with `diffusers`!  \n\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), [LAION](https://laion.ai/) and [RunwayML](https://runwayml.com/). It's trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 4GB VRAM.\nSee the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information.\n\n\n### Text-to-Image generation with Stable Diffusion\n\nFirst let's install\n\n```bash\npip install --upgrade diffusers transformers accelerate\n```\n\nWe recommend using the model in [half-precision (`fp16`)](https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/) as it gives almost always the same results as full\nprecision while being roughly twice as fast and requiring half the amount of GPU RAM.\n\n```python\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n```\n\n#### Running the model locally\n\nYou can also simply download the model folder and pass the path to the local folder to the `StableDiffusionPipeline`.\n\n```\ngit lfs install\ngit clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n```\n\nAssuming the folder is stored locally under `./stable-diffusion-v1-5`, you can run stable diffusion\nas follows:\n\n```python\npipe = StableDiffusionPipeline.from_pretrained(\"./stable-diffusion-v1-5\")\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n```\n\nIf you are limited by GPU memory, you might want to consider chunking the attention computation in addition \nto using `fp16`.\nThe following snippet should result in less than 4GB VRAM.\n\n```python\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\npipe.enable_attention_slicing()\nimage = pipe(prompt).images[0]  \n```\n\nIf you wish to use a different scheduler (e.g.: DDIM, LMS, PNDM/PLMS), you can instantiate\nit before the pipeline and pass it to `from_pretrained`.\n    \n```python\nfrom diffusers import LMSDiscreteScheduler\n\npipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\n\nIf you want to run Stable Diffusion on CPU or you want to have maximum precision on GPU, \nplease run the model in the default *full-precision* setting:\n\n```python\nfrom diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n\n# disable the following line if you run on CPU\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\n\n### JAX/Flax\n\nDiffusers offers a JAX / Flax implementation of Stable Diffusion for very fast inference. JAX shines specially on TPU hardware because each TPU server has 8 accelerators working in parallel, but it runs great on GPUs too.\n\nRunning the pipeline with the default PNDMScheduler:\n\n```python\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", revision=\"flax\", dtype=jax.numpy.bfloat16\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n```\n\n**Note**:\nIf you are limited by TPU memory, please make sure to load the `FlaxStableDiffusionPipeline` in `bfloat16` precision instead of the default `float32` precision as done above. You can do so by telling diffusers to load the weights from \"bf16\" branch.\n\n```python\nimport jax\nimport numpy as np\nfrom flax.jax_utils import replicate\nfrom flax.training.common_utils import shard\n\nfrom diffusers import FlaxStableDiffusionPipeline\n\npipeline, params = FlaxStableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", revision=\"bf16\", dtype=jax.numpy.bfloat16\n)\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\n\nprng_seed = jax.random.PRNGKey(0)\nnum_inference_steps = 50\n\nnum_samples = jax.device_count()\nprompt = num_samples * [prompt]\nprompt_ids = pipeline.prepare_inputs(prompt)\n\n# shard inputs and rng\nparams = replicate(params)\nprng_seed = jax.random.split(prng_seed, jax.device_count())\nprompt_ids = shard(prompt_ids)\n\nimages = pipeline(prompt_ids, params, prng_seed, num_inference_steps, jit=True).images\nimages = pipeline.numpy_to_pil(np.asarray(images.reshape((num_samples,) + images.shape[-3:])))\n```\n\n### Image-to-Image text-guided generation with Stable Diffusion\n\nThe `StableDiffusionImg2ImgPipeline` lets you pass a text prompt and an initial image to condition the generation of new images.\n\n```python\nimport requests\nimport torch\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\n# load the pipeline\ndevice = \"cuda\"\nmodel_id_or_path = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n\n# or download via git clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n# and pass `model_id_or_path=\"./stable-diffusion-v1-5\"`.\npipe = pipe.to(device)\n\n# let's download an initial image\nurl = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image = init_image.resize((768, 512))\n\nprompt = \"A fantasy landscape, trending on artstation\"\n\nimages = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5).images\n\nimages[0].save(\"fantasy_landscape.png\")\n```\nYou can also run this example on colab [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/image_2_image_using_diffusers.ipynb)\n\n### In-painting using Stable Diffusion\n\nThe `StableDiffusionInpaintPipeline` lets you edit specific parts of an image by providing a mask and a text prompt.\n\n```python\nimport PIL\nimport requests\nimport torch\nfrom io import BytesIO\n\nfrom diffusers import StableDiffusionInpaintPipeline\n\ndef download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = download_image(img_url).resize((512, 512))\nmask_image = download_image(mask_url).resize((512, 512))\n\npipe = StableDiffusionInpaintPipeline.from_pretrained(\"runwayml/stable-diffusion-inpainting\", torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\nimage = pipe(prompt=prompt, image=init_image, mask_image=mask_image).images[0]\n```\n\n### Tweak prompts reusing seeds and latents\n\nYou can generate your own latents to reproduce results, or tweak your prompt on a specific result you liked.\nPlease have a look at [Reusing seeds for deterministic generation](https://huggingface.co/docs/diffusers/main/en/using-diffusers/reusing_seeds).\n\n## Fine-Tuning Stable Diffusion\n\nFine-tuning techniques make it possible to adapt Stable Diffusion to your own dataset, or add new subjects to it. These are some of the techniques supported in `diffusers`:\n\nTextual Inversion is a technique for capturing novel concepts from a small number of example images in a way that can later be used to control text-to-image pipelines. It does so by learning new 'words' in the embedding space of the pipeline's text encoder. These special words can then be used within text prompts to achieve very fine-grained control of the resulting images. \n\n- Textual Inversion. Capture novel concepts from a small set of sample images, and associate them with new \"words\" in the embedding space of the text encoder. Please, refer to [our training examples](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) or [documentation](https://huggingface.co/docs/diffusers/training/text_inversion) to try for yourself.\n\n- Dreambooth. Another technique to capture new concepts in Stable Diffusion. This method fine-tunes the UNet (and, optionally, also the text encoder) of the pipeline to achieve impressive results. Please, refer to [our training example](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) and [training report](https://huggingface.co/blog/dreambooth) for additional details and training recommendations.\n\n- Full Stable Diffusion fine-tuning. If you have a more sizable dataset with a specific look or style, you can fine-tune Stable Diffusion so that it outputs images following those examples. This was the approach taken to create [a Pok√©mon Stable Diffusion model](https://huggingface.co/justinpinkney/pokemon-stable-diffusion) (by Justing Pinkney / Lambda Labs), [a Japanese specific version of Stable Diffusion](https://huggingface.co/spaces/rinna/japanese-stable-diffusion) (by [Rinna Co.](https://github.com/rinnakk/japanese-stable-diffusion/) and others. You can start at [our text-to-image fine-tuning example](https://github.com/huggingface/diffusers/tree/main/examples/text_to_image) and go from there.\n\n\n## Stable Diffusion Community Pipelines\n\nThe release of Stable Diffusion as an open source model has fostered a lot of interesting ideas and experimentation. \nOur [Community Examples folder](https://github.com/huggingface/diffusers/tree/main/examples/community) contains many ideas worth exploring, like interpolating to create animated videos, using CLIP Guidance for additional prompt fidelity, term weighting, and much more! [Take a look](https://huggingface.co/docs/diffusers/using-diffusers/custom_pipeline_overview) and [contribute your own](https://huggingface.co/docs/diffusers/using-diffusers/contribute_pipeline).\n\n## Other Examples\n\nThere are many ways to try running Diffusers! Here we outline code-focused tools (primarily using `DiffusionPipeline`s and Google Colab) and interactive web-tools.\n\n### Running Code\n\nIf you want to run the code yourself üíª, you can try out:\n- [Text-to-Image Latent Diffusion](https://huggingface.co/CompVis/ldm-text2im-large-256)\n```python\n# !pip install diffusers[\"torch\"] transformers\nfrom diffusers import DiffusionPipeline\n\ndevice = \"cuda\"\nmodel_id = \"CompVis/ldm-text2im-large-256\"\n\n# load model and scheduler\nldm = DiffusionPipeline.from_pretrained(model_id)\nldm = ldm.to(device)\n\n# run pipeline in inference (sample random noise and denoise)\nprompt = \"A painting of a squirrel eating a burger\"\nimage = ldm([prompt], num_inference_steps=50, eta=0.3, guidance_scale=6).images[0]\n\n# save image\nimage.save(\"squirrel.png\")\n```\n- [Unconditional Diffusion with discrete scheduler](https://huggingface.co/google/ddpm-celebahq-256)\n```python\n# !pip install diffusers[\"torch\"]\nfrom diffusers import DDPMPipeline, DDIMPipeline, PNDMPipeline\n\nmodel_id = \"google/ddpm-celebahq-256\"\ndevice = \"cuda\"\n\n# load model and scheduler\nddpm = DDPMPipeline.from_pretrained(model_id)  # you can replace DDPMPipeline with DDIMPipeline or PNDMPipeline for faster inference\nddpm.to(device)\n\n# run pipeline in inference (sample random noise and denoise)\nimage = ddpm().images[0]\n\n# save image\nimage.save(\"ddpm_generated_image.png\")\n```\n- [Unconditional Latent Diffusion](https://huggingface.co/CompVis/ldm-celebahq-256)\n- [Unconditional Diffusion with continuous scheduler](https://huggingface.co/google/ncsnpp-ffhq-1024)\n\n**Other Image Notebooks**:\n* [image-to-image generation with Stable Diffusion](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/image_2_image_using_diffusers.ipynb) ![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg),\n* [tweak images via repeated Stable Diffusion seeds](https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb) ![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg),\n\n**Diffusers for Other Modalities**:\n* [Molecule conformation generation](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/geodiff_molecule_conformation.ipynb) ![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg),\n* [Model-based reinforcement learning](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/reinforcement_learning_with_diffusers.ipynb) ![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg),\n\n### Web Demos\nIf you just want to play around with some web demos, you can try out the following üöÄ Spaces:\n| Model                          \t| Hugging Face Spaces                                                                                                                                               \t|\n|--------------------------------\t|-------------------------------------------------------------------------------------------------------------------------------------------------------------------\t|\n| Text-to-Image Latent Diffusion \t| [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/CompVis/text2img-latent-diffusion) \t|\n| Faces generator                \t| [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/CompVis/celeba-latent-diffusion)    \t|\n| DDPM with different schedulers \t| [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/fusing/celeba-diffusion)           \t|\n| Conditional generation from sketch  \t| [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/huggingface/diffuse-the-rest)           \t|\n| Composable diffusion | [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Shuang59/Composable-Diffusion)           \t|\n\n## Definitions\n\n**Models**: Neural network that models $p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$ (see image below) and is trained end-to-end to *denoise* a noisy input to an image.\n*Examples*: UNet, Conditioned UNet, 3D UNet, Transformer UNet\n\n<p align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/10695622/174349667-04e9e485-793b-429a-affe-096e8199ad5b.png\" width=\"800\"/>\n    <br>\n    <em> Figure from DDPM paper (https://arxiv.org/abs/2006.11239). </em>\n<p>\n    \n**Schedulers**: Algorithm class for both **inference** and **training**.\nThe class provides functionality to compute previous image according to alpha, beta schedule as well as predict noise for training. Also known as **Samplers**.\n*Examples*: [DDPM](https://arxiv.org/abs/2006.11239), [DDIM](https://arxiv.org/abs/2010.02502), [PNDM](https://arxiv.org/abs/2202.09778), [DEIS](https://arxiv.org/abs/2204.13902)\n\n<p align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/10695622/174349706-53d58acc-a4d1-4cda-b3e8-432d9dc7ad38.png\" width=\"800\"/>\n    <br>\n    <em> Sampling and training algorithms. Figure from DDPM paper (https://arxiv.org/abs/2006.11239). </em>\n<p>\n    \n\n**Diffusion Pipeline**: End-to-end pipeline that includes multiple diffusion models, possible text encoders, ...\n*Examples*: Glide, Latent-Diffusion, Imagen, DALL-E 2\n\n<p align=\"center\">\n    <img src=\"https://user-images.githubusercontent.com/10695622/174348898-481bd7c2-5457-4830-89bc-f0907756f64c.jpeg\" width=\"550\"/>\n    <br>\n    <em> Figure from ImageGen (https://imagen.research.google/). </em>\n<p>\n    \n## Philosophy\n\n- Readability and clarity is preferred over highly optimized code. A strong importance is put on providing readable, intuitive and elementary code design. *E.g.*, the provided [schedulers](https://github.com/huggingface/diffusers/tree/main/src/diffusers/schedulers) are separated from the provided [models](https://github.com/huggingface/diffusers/tree/main/src/diffusers/models) and provide well-commented code that can be read alongside the original paper.\n- Diffusers is **modality independent** and focuses on providing pretrained models and tools to build systems that generate **continuous outputs**, *e.g.* vision and audio.\n- Diffusion models and schedulers are provided as concise, elementary building blocks. In contrast, diffusion pipelines are a collection of end-to-end diffusion systems that can be used out-of-the-box, should stay as close as possible to their original implementation and can include components of another library, such as text-encoders. Examples for diffusion pipelines are [Glide](https://github.com/openai/glide-text2im) and [Latent Diffusion](https://github.com/CompVis/latent-diffusion).\n\n## In the works\n\nFor the first release, ü§ó Diffusers focuses on text-to-image diffusion techniques. However, diffusers can be used for much more than that! Over the upcoming releases, we'll be focusing on:\n\n- Diffusers for audio\n- Diffusers for reinforcement learning (initial work happening in https://github.com/huggingface/diffusers/pull/105).\n- Diffusers for video generation\n- Diffusers for molecule generation (initial work happening in https://github.com/huggingface/diffusers/pull/54)\n\nA few pipeline components are already being worked on, namely:\n\n- BDDMPipeline for spectrogram-to-sound vocoding\n- GLIDEPipeline to support OpenAI's GLIDE model\n- Grad-TTS for text to audio generation / conditional audio generation\n\nWe want diffusers to be a toolbox useful for diffusers models in general; if you find yourself limited in any way by the current API, or would like to see additional models, schedulers, or techniques, please open a [GitHub issue](https://github.com/huggingface/diffusers/issues) mentioning what you would like to see.\n\n## Credits\n\nThis library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:\n\n- @CompVis' latent diffusion models library, available [here](https://github.com/CompVis/latent-diffusion)\n- @hojonathanho original DDPM implementation, available [here](https://github.com/hojonathanho/diffusion) as well as the extremely useful translation into PyTorch by @pesser, available [here](https://github.com/pesser/pytorch_diffusion)\n- @ermongroup's DDIM implementation, available [here](https://github.com/ermongroup/ddim).\n- @yang-song's Score-VE and Score-VP implementations, available [here](https://github.com/yang-song/score_sde_pytorch)\n\nWe also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights.\n\n## Citation\n\n```bibtex\n@misc{von-platen-etal-2022-diffusers,\n  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},\n  title = {Diffusers: State-of-the-art diffusion models},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/huggingface/diffusers}}\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ghunkins/diffusers-unchained",
    "keywords": "deep learning",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "diffusers-unchained",
    "package_url": "https://pypi.org/project/diffusers-unchained/",
    "platform": null,
    "project_url": "https://pypi.org/project/diffusers-unchained/",
    "project_urls": {
      "Homepage": "https://github.com/ghunkins/diffusers-unchained"
    },
    "release_url": "https://pypi.org/project/diffusers-unchained/0.11.1/",
    "requires_dist": [
      "importlib-metadata",
      "filelock",
      "huggingface-hub (>=0.10.0)",
      "numpy",
      "regex (!=2019.12.17)",
      "requests",
      "Pillow",
      "black (==22.8) ; extra == 'dev'",
      "isort (>=5.5.4) ; extra == 'dev'",
      "flake8 (>=3.8.3) ; extra == 'dev'",
      "hf-doc-builder (>=0.3.0) ; extra == 'dev'",
      "datasets ; extra == 'dev'",
      "k-diffusion ; extra == 'dev'",
      "librosa ; extra == 'dev'",
      "parameterized ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-timeout ; extra == 'dev'",
      "pytest-xdist ; extra == 'dev'",
      "safetensors ; extra == 'dev'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'dev'",
      "scipy ; extra == 'dev'",
      "torchvision ; extra == 'dev'",
      "transformers (>=4.25.1) ; extra == 'dev'",
      "accelerate (>=0.11.0) ; extra == 'dev'",
      "tensorboard ; extra == 'dev'",
      "modelcards (>=0.1.4) ; extra == 'dev'",
      "torch (>=1.4) ; extra == 'dev'",
      "jax (!=0.3.2,>=0.2.8) ; extra == 'dev'",
      "jaxlib (>=0.1.65) ; extra == 'dev'",
      "flax (>=0.4.1) ; extra == 'dev'",
      "hf-doc-builder (>=0.3.0) ; extra == 'docs'",
      "jax (!=0.3.2,>=0.2.8) ; extra == 'flax'",
      "jaxlib (>=0.1.65) ; extra == 'flax'",
      "flax (>=0.4.1) ; extra == 'flax'",
      "black (==22.8) ; extra == 'quality'",
      "isort (>=5.5.4) ; extra == 'quality'",
      "flake8 (>=3.8.3) ; extra == 'quality'",
      "hf-doc-builder (>=0.3.0) ; extra == 'quality'",
      "datasets ; extra == 'test'",
      "k-diffusion ; extra == 'test'",
      "librosa ; extra == 'test'",
      "parameterized ; extra == 'test'",
      "pytest ; extra == 'test'",
      "pytest-timeout ; extra == 'test'",
      "pytest-xdist ; extra == 'test'",
      "safetensors ; extra == 'test'",
      "sentencepiece (!=0.1.92,>=0.1.91) ; extra == 'test'",
      "scipy ; extra == 'test'",
      "torchvision ; extra == 'test'",
      "transformers (>=4.25.1) ; extra == 'test'",
      "torch (>=1.4) ; extra == 'torch'",
      "accelerate (>=0.11.0) ; extra == 'torch'",
      "accelerate (>=0.11.0) ; extra == 'training'",
      "datasets ; extra == 'training'",
      "tensorboard ; extra == 'training'",
      "modelcards (>=0.1.4) ; extra == 'training'"
    ],
    "requires_python": ">=3.7.0",
    "summary": "Diffusers",
    "version": "0.11.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16178091,
  "releases": {
    "0.10.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9f4b0315b0a32c9504498717447febf96d2e9a3b633decdd92c212fd88c4828a",
          "md5": "15d6895842e150f9c0dc7dc7c7f6fabb",
          "sha256": "5e7341aae1fc782693ce34d4b3fc7f7593a13808fc5fbe1f1b068daeca09600b"
        },
        "downloads": -1,
        "filename": "diffusers_unchained-0.10.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "15d6895842e150f9c0dc7dc7c7f6fabb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 503144,
        "upload_time": "2022-12-18T22:47:44",
        "upload_time_iso_8601": "2022-12-18T22:47:44.343175Z",
        "url": "https://files.pythonhosted.org/packages/9f/4b/0315b0a32c9504498717447febf96d2e9a3b633decdd92c212fd88c4828a/diffusers_unchained-0.10.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "215fcbb7c534e670de029e547135ef6c4fae8d5f2258f58dc3dbe965a8f98767",
          "md5": "cf2b0847d6a7e74ef0f32267090a7773",
          "sha256": "bef3b83be5caaa1e423a2109e950cf877f35b9ec3fcd4239f4ed92b94e99cbc3"
        },
        "downloads": -1,
        "filename": "diffusers-unchained-0.10.2.tar.gz",
        "has_sig": false,
        "md5_digest": "cf2b0847d6a7e74ef0f32267090a7773",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 302883,
        "upload_time": "2022-12-18T22:47:46",
        "upload_time_iso_8601": "2022-12-18T22:47:46.371763Z",
        "url": "https://files.pythonhosted.org/packages/21/5f/cbb7c534e670de029e547135ef6c4fae8d5f2258f58dc3dbe965a8f98767/diffusers-unchained-0.10.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.11.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5eb7e82b5808fdc15f7e740b0e5634577153a40667c1c0d0b77fe715261f535a",
          "md5": "0299fb58520ca75ed42b0ec6f36b44bb",
          "sha256": "17965c8619f11490c25451f10d7a6259eb0e9a52dd0e9392722917c9ad3aa6c7"
        },
        "downloads": -1,
        "filename": "diffusers_unchained-0.11.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0299fb58520ca75ed42b0ec6f36b44bb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 524940,
        "upload_time": "2022-12-21T22:20:34",
        "upload_time_iso_8601": "2022-12-21T22:20:34.633400Z",
        "url": "https://files.pythonhosted.org/packages/5e/b7/e82b5808fdc15f7e740b0e5634577153a40667c1c0d0b77fe715261f535a/diffusers_unchained-0.11.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9c70cb1be2c80d6c94dd9f66be053b93b1928a1d7c3ff396420bb23681a10c87",
          "md5": "195aabdd7bd87d58fbc21f261dbfd099",
          "sha256": "7372696fbb0db203b3e2cb7524fdead87b8696c91008bacf74a8f9a9345e4362"
        },
        "downloads": -1,
        "filename": "diffusers-unchained-0.11.1.tar.gz",
        "has_sig": false,
        "md5_digest": "195aabdd7bd87d58fbc21f261dbfd099",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 324130,
        "upload_time": "2022-12-21T22:20:36",
        "upload_time_iso_8601": "2022-12-21T22:20:36.660840Z",
        "url": "https://files.pythonhosted.org/packages/9c/70/cb1be2c80d6c94dd9f66be053b93b1928a1d7c3ff396420bb23681a10c87/diffusers-unchained-0.11.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a8741d3bda06e75b21ad21ca6e20b700d7fece204f05fcace0b72681cfe20c09",
          "md5": "fde9ed92951c21db8566e38000ea18e3",
          "sha256": "63e34d248780e89696b9733f1ffa37b7ff283896fa3eacd61261945d96f6f068"
        },
        "downloads": -1,
        "filename": "diffusers-unchained-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "fde9ed92951c21db8566e38000ea18e3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 133817,
        "upload_time": "2022-09-21T18:57:41",
        "upload_time_iso_8601": "2022-09-21T18:57:41.214388Z",
        "url": "https://files.pythonhosted.org/packages/a8/74/1d3bda06e75b21ad21ca6e20b700d7fece204f05fcace0b72681cfe20c09/diffusers-unchained-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6905d3135148e8b686e795cb3093dc8bc29f2d0f3a22f7a0063267848ca8406c",
          "md5": "d0e0894c49bbdfba1226ed59e2bac3ea",
          "sha256": "bb103b04b43ddcf51cd11ce1ab6428d66207445f8b0db9c6fbdb31dbebcfdcff"
        },
        "downloads": -1,
        "filename": "diffusers_unchained-0.4.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d0e0894c49bbdfba1226ed59e2bac3ea",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 229698,
        "upload_time": "2022-10-09T17:24:56",
        "upload_time_iso_8601": "2022-10-09T17:24:56.390787Z",
        "url": "https://files.pythonhosted.org/packages/69/05/d3135148e8b686e795cb3093dc8bc29f2d0f3a22f7a0063267848ca8406c/diffusers_unchained-0.4.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "276683690845bd823db235a07cfb0c7a8cd702d8ce08cf554de281d3e8f63c0a",
          "md5": "1e5610f89df1516b27d76af27f4f7257",
          "sha256": "70fdadb78f4945e1f4f54d10433970dfa2036d70e38a2ee60f3a98b95942108f"
        },
        "downloads": -1,
        "filename": "diffusers-unchained-0.4.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1e5610f89df1516b27d76af27f4f7257",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 144029,
        "upload_time": "2022-10-09T17:24:58",
        "upload_time_iso_8601": "2022-10-09T17:24:58.691375Z",
        "url": "https://files.pythonhosted.org/packages/27/66/83690845bd823db235a07cfb0c7a8cd702d8ce08cf554de281d3e8f63c0a/diffusers-unchained-0.4.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.7.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f96517a7a984f59ae2436f0dd680fe7d12be071a7c23c231529aa34d52755c76",
          "md5": "0e62c5fadd594c97e122c73ca862073c",
          "sha256": "afd4fd93e43e57ca5d742ecbe3977cfa15e9b673e94e2964566e71f8d2ae200a"
        },
        "downloads": -1,
        "filename": "diffusers_unchained-0.7.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0e62c5fadd594c97e122c73ca862073c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 305190,
        "upload_time": "2022-11-04T00:37:46",
        "upload_time_iso_8601": "2022-11-04T00:37:46.413360Z",
        "url": "https://files.pythonhosted.org/packages/f9/65/17a7a984f59ae2436f0dd680fe7d12be071a7c23c231529aa34d52755c76/diffusers_unchained-0.7.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fe3f9ab67e9e6afce794a49a518b550624969543931454d928617d721b917555",
          "md5": "a83cb51830b3865d9315023edff41f1f",
          "sha256": "b4b6852d37315f5a103ef243ac047685e71f3b1ec67ae3e52e785ceaf04f8a27"
        },
        "downloads": -1,
        "filename": "diffusers-unchained-0.7.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a83cb51830b3865d9315023edff41f1f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 184511,
        "upload_time": "2022-11-04T00:37:48",
        "upload_time_iso_8601": "2022-11-04T00:37:48.165495Z",
        "url": "https://files.pythonhosted.org/packages/fe/3f/9ab67e9e6afce794a49a518b550624969543931454d928617d721b917555/diffusers-unchained-0.7.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.9.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "28fb4d3cc7bf910bc6d99ad5d1f313ebd4bbfd06f7a5adc13da4476e28cc73fc",
          "md5": "17a1ef9b1e8f8e702a8869a9651e70a1",
          "sha256": "c7bd2f0f1e8929b52537b8844626bca06eb5e1a622ba787a4ba91a8ed913a7b7"
        },
        "downloads": -1,
        "filename": "diffusers_unchained-0.9.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "17a1ef9b1e8f8e702a8869a9651e70a1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 453201,
        "upload_time": "2022-12-02T19:22:14",
        "upload_time_iso_8601": "2022-12-02T19:22:14.482701Z",
        "url": "https://files.pythonhosted.org/packages/28/fb/4d3cc7bf910bc6d99ad5d1f313ebd4bbfd06f7a5adc13da4476e28cc73fc/diffusers_unchained-0.9.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aac55a14548eb2ae7d9856daf81c8eed7f3c3b4a8c350996280115b037e0882c",
          "md5": "67ebf3cb2d0c985bccb2a2c784d6b8e1",
          "sha256": "fa394889b622851429d73251c2dc01ac69bb1e3ca4f76a5b64d40e02a496f06b"
        },
        "downloads": -1,
        "filename": "diffusers-unchained-0.9.0.tar.gz",
        "has_sig": false,
        "md5_digest": "67ebf3cb2d0c985bccb2a2c784d6b8e1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 284247,
        "upload_time": "2022-12-02T19:22:16",
        "upload_time_iso_8601": "2022-12-02T19:22:16.742078Z",
        "url": "https://files.pythonhosted.org/packages/aa/c5/5a14548eb2ae7d9856daf81c8eed7f3c3b4a8c350996280115b037e0882c/diffusers-unchained-0.9.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5eb7e82b5808fdc15f7e740b0e5634577153a40667c1c0d0b77fe715261f535a",
        "md5": "0299fb58520ca75ed42b0ec6f36b44bb",
        "sha256": "17965c8619f11490c25451f10d7a6259eb0e9a52dd0e9392722917c9ad3aa6c7"
      },
      "downloads": -1,
      "filename": "diffusers_unchained-0.11.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0299fb58520ca75ed42b0ec6f36b44bb",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7.0",
      "size": 524940,
      "upload_time": "2022-12-21T22:20:34",
      "upload_time_iso_8601": "2022-12-21T22:20:34.633400Z",
      "url": "https://files.pythonhosted.org/packages/5e/b7/e82b5808fdc15f7e740b0e5634577153a40667c1c0d0b77fe715261f535a/diffusers_unchained-0.11.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9c70cb1be2c80d6c94dd9f66be053b93b1928a1d7c3ff396420bb23681a10c87",
        "md5": "195aabdd7bd87d58fbc21f261dbfd099",
        "sha256": "7372696fbb0db203b3e2cb7524fdead87b8696c91008bacf74a8f9a9345e4362"
      },
      "downloads": -1,
      "filename": "diffusers-unchained-0.11.1.tar.gz",
      "has_sig": false,
      "md5_digest": "195aabdd7bd87d58fbc21f261dbfd099",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.0",
      "size": 324130,
      "upload_time": "2022-12-21T22:20:36",
      "upload_time_iso_8601": "2022-12-21T22:20:36.660840Z",
      "url": "https://files.pythonhosted.org/packages/9c/70/cb1be2c80d6c94dd9f66be053b93b1928a1d7c3ff396420bb23681a10c87/diffusers-unchained-0.11.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}