{
  "info": {
    "author": "Hlib Babii",
    "author_email": "hlibbabii@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Environment :: Console",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Natural Language :: English",
      "Operating System :: MacOS :: MacOS X",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: Software Development :: Pre-processors"
    ],
    "description": "<!--\nSPDX-FileCopyrightText: 2020 Hlib Babii <hlibbabii@gmail.com>\n\nSPDX-License-Identifier: Apache-2.0\n-->\n\n# Codeprep\n\n[![Build Status](https://travis-ci.org/giganticode/codeprep.svg?branch=master)](https://travis-ci.org/giganticode/codeprep)\n\n**This is a tool for preprocessing source code corpora according to a specified vocabulary modeling choice.**\n\nSupported modeling choices are: \n* Splitting algorithm (no identifier splitting, camel-case splitting, snake-case splitting, BPE (byte-pair-encoding), \nnumber-splitting, ronin: http://joss.theoj.org/papers/10.21105/joss.00653); \n* Number of merges if using BPE; \n* Ignoring/preserving string literals; \n* Ignoring/preserving comments; \n* Preserving case/lowercasing;\n* Preserving/ignoring newlines and tabs.\n* applying/not applying stemming after basic splitting \n\n# Getting started\n\nMake sure you have python >= 3.6 installed in your system; pip, setuptools and wheel are up to date.\n```bash\npython --version\npython -m pip install --upgrade pip setuptools wheel\n```\n\nInstall **codeprep** lib:\n```bash\npip install giganticode-codeprep\n```\n\nIn order to run the **ronin** algorithm, you will have to additionally install Spiral module (https://github.com/casics/spiral/):\n```bash\npip install git+https://github.com/casics/spiral.git\n```\n\nThe tool can be used **as a python library** as well as a standalone module runnable with a **CLI**. \nYou can pass the path to the dataset or the text itself to be preprocessed. When using Python API for the former option \nyou need to import methods from `codeprep.api.text` module, for the latter - from `codeprep.api.corpus`.\nBelow you can see the general patterns of usage.\n\n\nPython API\n```python\n>>> import codeprep.api.text as cp\n>>> cp.<commmand>('Some code to be split')\n```\n\n```python\n>>> import codeprep.api.corpus as cp\n>>> cp.<commmand>('/path/to/the/dataset')\n```\n\nCLI\n```bash\ncodeprep <commmand> \"Some code to be split\"\n```\n\n```bash\ncodeprep <commmand> --path /path/to/the/dataset\n```\n\nHereafter we will demonstrate the usage as a python library. The CLI is analogous to the python API. You can find the documentation about how to use it [here](codeprep/cli/spec.py). \n\n## Usage examples\n\n### Basic splitting \nTokenization + CamelCase- and snake_case- splitting:\n\n```python\n>>> import codeprep.api.text as cp\n>>> input_code = '''void test_WordUeberraschungPrinter() {\n...     if (eps >= 0.345e+4) { // FIXME\n...         printWord(\"     ...     Überraschung\");\n...     }\n... }'''\n>>> cp.basic(input_code)\n['void', '<w>', 'test', '_', 'Word', 'Ueberraschung', 'Printer', '</w>', '(', ')', '{', '\\n', \n'\\t', 'if', '(', 'eps', '>', '=', '0', '.', '<w>', '345', 'e', '</w>', '+', '4', ')', '{', '/', '/', 'FIXME', '\\n', \n'\\t', '\\t', '<w>', 'print', 'Word', '</w>', '(', '\"', '\\t', '.', '.', '.', '\\t', 'Überraschung', '\"', ')', ';', '\\n', \n'\\t', '}', '\\n', \n'}']\n```\n\n### Tokenize but don't split identifiers\n\n```python\n>>> import codeprep.api.text as cp\n>>> input_code = '''void test_WordUeberraschungPrinter() {\n...     if (eps >= 0.345e+4) { // FIXME\n...         printWord(\"     ...     Überraschung\");\n...     }\n... }'''\n>>> cp.nosplit(input_code)\n['void', 'test_WordUeberraschungPrinter', '(', ')', '{', '\\n', \n'\\t', 'if', '(', 'eps', '>', '=', '0', '.', '345e', '+', '4', ')', '{', '/', '/', 'FIXME', '\\n', \n'\\t', '\\t', 'printWord', '(', '\"', '\\t', '.', '.', '.', '\\t', 'Überraschung', '\"', ')', ';', '\\n', \n'\\t', '}', '\\n', \n'}']\n```\n\n### BPE (Byte-Pair encoding)\n\nThe following code does **camelCase-** and **snake_case-** splitting and applies **bpe with 10k merges** on top:\n\n```python\n>>> import codeprep.api.text as cp\n>>> input_code = '''void test_WordUeberraschungPrinter() {\n...     if (eps >= 0.345e+4) { // FIXME\n...         printWord(\"     ...     Überraschung\");\n...     }\n... }'''\n>>> cp.bpe(input_code, bpe_codes_id='10k')\n['v', 'oid</t>', 'test_', 'Word', 'U', 'eb', 'err', 'as', 'ch', 'un', 'g', 'Print', 'er</t>', '(</t>', ')</t>', '{</t>', '\\n', \n'\\t', 'i', 'f</t>', '(</t>', 'e', 'ps</t>', '></t>', '=</t>', '0</t>', '.</t>', '34', '5', 'e</t>', '+</t>', '4</t>', ')</t>', '{</t>', '/</t>', '/</t>', 'FIX', 'M', 'E</t>',  '\\n', \n'\\t', '\\t', 'print', 'Word</t>', '(</t>', '\"</t>', '\\t', '.</t>', '.</t>', '.</t>', '\\t', 'Ü', 'b', 'err', 'as', 'ch', 'un', 'g</t>', '\"</t>', ')</t>', ';</t>', '\\n', \n'\\t', '}</t>', '\\n', \n'}</t>']\n```\n\n**codeprep** by default does BPE using bpe codes leaned on [the Github Java Corpus](http://groups.inf.ed.ac.uk/cup/javaGithub/). The argument `bpe_codes_id='10k'` tells the **codeprep** tool to use 10,000 bpe merges. \nOther possible values are `1k` and `5k` (1,000 and 5,000 merges respectively). Please refer to section [Learning custom BPE codes](#Learning-custom-BPE-codes) to train custom bpe codes.\n\n**For other commands and options like `chars`, `--split-numbers`, `--ronin`, `--stem`, please refer to the [docs](codeprep/cli/spec.py)**.\n\n## Calculate vocabulary \nSet `calc_vocab` param to `True` when calling a preprocessing method to calculate the vocabulary of the preprocessed corpus, e.g.:\n```python\n>>> import codeprep.api.corpus as cp\n>>> cp.basic('/path/to/train/on', calc_vocab=True)\n...\nVocab is available at /path/to/vocab\n```\n\n## Learning custom BPE codes\nIf you don't want to use, pre-trained BPE codes, it's possible to train custom ones. For example, to train 10,000 merges on the corpus located at the path `/path/to/train/on`, the following command should be run (only CLI):\n\n```bash\ncodeprep learn-bpe 10000 -p /path/to/train/on --id custom-bpe-codes \n```\n\nNow it is possible to do bpe splitting by running the bpe command with the number of merges from 0 to 10,000 (for example with 3500 merges):\n\n```bash\ncodeprep bpe custom-bpe-codes-3500 -p /path/to/preprocess \n```\n\nBefore bpe codes are trained, the [basic preprocessing](#basic-splitting) is done, which can also be tuned with arguments described in section [Tweaking preprocessing](#tweaking-preprocessing).\n\n\n## Additional options\n### Tweaking preprocessing\nYou can pass the following parameters with a `True` value (default values for all of them are False), to tweak the way the imput is preprocessed:\n\n * `no_str` - replace strings with <string> placeholders.\n * `no_com` - replace comments with <comment> placeholders.\n * `no_spaces` - remove newlines and tabs.\n * `no_unicode` - replace words containing non-ascii characters with <non-en> placeholders.\n * `no_case` - lowercase words and encode information about case in <Cap> <CAP> tokens.\n```python\n>>> import codeprep.api.text as cp\n>>> input_code = '''void test_WordUeberraschungPrinter() {\n...     if (eps >= 0.345e+4) { // FIXME\n...         printWord(\"     ...     Überraschung\");\n...     }\n... }'''\n>>> cp.basic(input_code, no_spaces=True, no_unicode=True, no_case=True, no_com=True, no_str=True)\n['void', '<w>', 'test', '_', '<Cap>', 'word', '<Cap>', 'ueberraschung', '<Cap>', 'printer', '</w>', '(', ')', '{', \n'if', '(', 'eps', '>', '=', '0', '.', '<w>', '345', 'e', '</w>', '+', '4', ')', '{', '/', '/', '<CAPS>', 'fixme', \n'<w>', 'print', '<Cap>', 'word', '</w>', '(', '\"', '.', '.', '.', '<Cap>', '<non-en>', '\"', ')', ';', \n'}', \n'}']\n```\n\nSimilar params can be specified as switches `--no-str`, `--no-com`, `--no-spaces`, `--no-unicode`, `--no-case` in CLI commands.\n\n### Specifying the language\nUnless explicitely specified, **codeprep** will assume the language is java. To make sure the input is preprocessed as intended, it is always **highly recommended** to specify it:\n```python\nimport codeprep.api.text as cp\n>>> cp.bpe(\"volatile\", '1k')\n['volatile']\n>>> cp.bpe(\"volatile\", '1k', extension=\"py\")\n['v', 'ol', 'a', 'ti', 'le</t>']\n# Since 'volatile' is a keyword in java, it is represented as one token unlike in python \n# where it is pretty rare when used as an identifier and therefore represented as multiple subtokens.\n```\n\nWhen preprocessing a corpus, `codeprep` identifies the language based on the file extension. If you want only files with (a) certain extension(s) to be preprocessed, you can specify --ext param \n```bash\ncodeprep basic --path /path/to/be/preprocessed --ext \"java\"\n\n# or if you want to pre-process multiple types of files: \ncodeprep basic --path /path/to/be/preprocessed --ext \"java|c|py|js\"\n```\n### Miscellaneous\nYou can specify the path to where the preprocessed corpus will be written:\n```bash\ncodeprep basic --path /path/to/preprocess --output-path /path/to/output\n```\n\nTo print logs with log level DEBUG and higher to stdout:\n```bash\ncodeprep basic --path /path/to/preprocess --verbose\n```\n\n## Getting Help\nTo get help on commands and options:\n\n```bash\ncodeprep --help\n```\n\n\n# Advanced\n\n### Caching\n\nWhen preprocessing a dataset, **codeprep** first parses source code and converts it into internal representation, \nwhich is after that converted to a preprocessed dataset depending on provided parameters. The intermediate \nrepresentation is cached, so that when the same dataset is pre-processed again with different parameters,\n**codeprep** (providing no changes have been made to the dataset) would use the cache rather than parsing \nthe source code again.\n\nTo store the cache, **codeprep** uses a directory speecified by `$XDG_CACHE_HOME/codeprep/<codeprep_version>` variable if its value is set, \n`$HOME/.cache/codeprep/<codeprep_version>` otherwise.\n\nRemoving the cache will not change the final result, however, will result in slower pre-processing.\n\n# Releases\n\n## 1.0.0\n- DOI assigned\n\n## 1.0.0-alpha.12\n- Bugfixes and minor improvements\n\n## 1.0.0-alpha.11 (NOT backward compatible with 1.0.0-alpha.10)\n\n- Include token types in the metadata\n- Expand on token type hierarchy\n- Make possible to return full token index in the iterator\n\n## 1.0.0-alpha.10 (NOT backward compatible with 1.0.0-alpha.9)\n\n- Add boundaries of comments to pre-processing metadata\n- Add Windows and OSx support\n- Switch from unittest to pytest+doctest\n- Bugfixes related to literal presentation of tokens on the disk\n- Bugfixes related to adding </t> to mark the end of a full token\n\n## 1.0.0-alpha.9 (NOT backward compatible with 1.0.0-alpha.7)\n\n- Add `get_corpus_size()` method to `PreprocessedCorpus` class\n- Do BPE splitting without splitting by convention first\n- Use </t> to mark the last sub-token of a token\n- Replacing non-ascii sequences with a special char\n- Follow symlinks when reading a dataset\n- make possible to preserve case when doing stemming\n- Bugfixes\n\n## 1.0.0-alpha.7 (NOT backward compatible with 1.0.0-alpha.6)\n\n- Store version in `codeprep.__version__`\n- implement `--full-strings` and `--max-str-length` options\n- replace `ronin` method/command wit`--ronin` option and apply ronin algorithm on word level instead of full identifier level\n- if `split_numbers` option is set to `True`, split numbers not only in code but also in strings and comments\n- change placeholder values to more human-readable\n- improve logging displaying\n- Bugfixes\n\n## 1.0.0-alpha.6\n\nInitial PyPI release\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://github.com/giganticode/codeprep",
    "keywords": "big large data source code corpus machine learning pre-processing nlp",
    "license": "Apache-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "giganticode-codeprep",
    "package_url": "https://pypi.org/project/giganticode-codeprep/",
    "platform": "",
    "project_url": "https://pypi.org/project/giganticode-codeprep/",
    "project_urls": {
      "Homepage": "http://github.com/giganticode/codeprep"
    },
    "release_url": "https://pypi.org/project/giganticode-codeprep/1.0.0/",
    "requires_dist": [
      "appdirs (==1.4.3)",
      "dill (==0.3.1.1)",
      "docopt (==0.6.2)",
      "docopt-subcommands (==3.0.0)",
      "jsons (==1.0.0)",
      "nltk (==3.4.5)",
      "Pygments (==2.5.2)",
      "PyYAML (==5.1.2)",
      "regex (==2019.11.1)",
      "tqdm (==4.39.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "A toolkit for pre-processing large source code corpora",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6514457,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eac9e8815d8897cc7f6f9e30bdfbe46fc12f10eadb0840a2bee69252b374fdb6",
          "md5": "720940e75deb4c8c84a4fe08ffa9cc4a",
          "sha256": "13469bbf29dc563d939b6b4a91dcc4ef75c01e1f8643266f20a187956523b2e1"
        },
        "downloads": -1,
        "filename": "giganticode_codeprep-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "720940e75deb4c8c84a4fe08ffa9cc4a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 252193,
        "upload_time": "2020-01-24T15:57:27",
        "upload_time_iso_8601": "2020-01-24T15:57:27.294250Z",
        "url": "https://files.pythonhosted.org/packages/ea/c9/e8815d8897cc7f6f9e30bdfbe46fc12f10eadb0840a2bee69252b374fdb6/giganticode_codeprep-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1091e755f305e74f358a05719a15e5fdf193a32afe131e59b82c4e40a1826aa8",
          "md5": "ec230e520c306dc88ade270d199e5831",
          "sha256": "e699d35f7b8734c99bfff729135388c90deb0a9399f756cb8ae5d2d959b5134b"
        },
        "downloads": -1,
        "filename": "giganticode-codeprep-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ec230e520c306dc88ade270d199e5831",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 227874,
        "upload_time": "2020-01-24T15:57:31",
        "upload_time_iso_8601": "2020-01-24T15:57:31.132293Z",
        "url": "https://files.pythonhosted.org/packages/10/91/e755f305e74f358a05719a15e5fdf193a32afe131e59b82c4e40a1826aa8/giganticode-codeprep-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eac9e8815d8897cc7f6f9e30bdfbe46fc12f10eadb0840a2bee69252b374fdb6",
        "md5": "720940e75deb4c8c84a4fe08ffa9cc4a",
        "sha256": "13469bbf29dc563d939b6b4a91dcc4ef75c01e1f8643266f20a187956523b2e1"
      },
      "downloads": -1,
      "filename": "giganticode_codeprep-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "720940e75deb4c8c84a4fe08ffa9cc4a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 252193,
      "upload_time": "2020-01-24T15:57:27",
      "upload_time_iso_8601": "2020-01-24T15:57:27.294250Z",
      "url": "https://files.pythonhosted.org/packages/ea/c9/e8815d8897cc7f6f9e30bdfbe46fc12f10eadb0840a2bee69252b374fdb6/giganticode_codeprep-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1091e755f305e74f358a05719a15e5fdf193a32afe131e59b82c4e40a1826aa8",
        "md5": "ec230e520c306dc88ade270d199e5831",
        "sha256": "e699d35f7b8734c99bfff729135388c90deb0a9399f756cb8ae5d2d959b5134b"
      },
      "downloads": -1,
      "filename": "giganticode-codeprep-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ec230e520c306dc88ade270d199e5831",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 227874,
      "upload_time": "2020-01-24T15:57:31",
      "upload_time_iso_8601": "2020-01-24T15:57:31.132293Z",
      "url": "https://files.pythonhosted.org/packages/10/91/e755f305e74f358a05719a15e5fdf193a32afe131e59b82c4e40a1826aa8/giganticode-codeprep-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}