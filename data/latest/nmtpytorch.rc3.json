{
  "info": {
    "author": "Ozan Caglayan",
    "author_email": "ozan.caglayan@univ-lemans.fr",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: POSIX",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering"
    ],
    "description": "![nmtpytorch](https://github.com/lium-lst/nmtpytorch/blob/master/docs/logo.png?raw=true \"nmtpytorch\")\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n\nClick [here](https://join.slack.com/t/nmtpy/shared_invite/enQtNDQ5NTY2ODU1MzI5LTBkMzZjNjYyZDZiYjk0NjNmYzRmMzY2MWVjZWYyMDRhNTFiOTljY2UyYWVjODUyZWQ1YWExMWEzZmU2MmFkZmY) to join to our Slack channel for questions.\n\n`nmtpytorch` allows training of various end-to-end neural architectures including\nbut not limited to neural machine translation, image captioning and automatic\nspeech recognition systems. The initial codebase was in `Theano` and was\ninspired from the famous [dl4mt-tutorial](https://github.com/nyu-dl/dl4mt-tutorial)\ncodebase.\n\n`nmtpytorch` is mainly developed by the **Language and Speech Team** of **Le Mans University** but\nreceives valuable contributions from the [Grounded Sequence-to-sequence Transduction Team](https://github.com/srvk/jsalt-2018-grounded-s2s)\nof *Frederick Jelinek Memorial Summer Workshop 2018*:\n\nLoic Barrault, Ozan Caglayan, Amanda Duarte, Desmond Elliott, Spandana Gella, Nils Holzenberger,\nChirag Lala, Jasmine (Sun Jae) Lee, Jindřich Libovický, Pranava Madhyastha,\nFlorian Metze, Karl Mulligan, Alissa Ostapenko, Shruti Palaskar, Ramon Sanabria, Lucia Specia and Josiah Wang.\n\nIf you use **nmtpytorch**, you may want to cite the following [paper](https://ufal.mff.cuni.cz/pbml/109/art-caglayan-et-al.pdf):\n```\n@article{nmtpy2017,\n  author    = {Ozan Caglayan and\n               Mercedes Garc\\'{i}a-Mart\\'{i}nez and\n               Adrien Bardet and\n               Walid Aransa and\n               Fethi Bougares and\n               Lo\\\"{i}c Barrault},\n  title     = {NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems},\n  journal   = {Prague Bull. Math. Linguistics},\n  volume    = {109},\n  pages     = {15--28},\n  year      = {2017},\n  url       = {https://ufal.mff.cuni.cz/pbml/109/art-caglayan-et-al.pdf},\n  doi       = {10.1515/pralin-2017-0035},\n  timestamp = {Tue, 12 Sep 2017 10:01:08 +0100}\n}\n```\n\n## Installation\n\n`nmtpytorch` currently requires `python>=3.6` and `torch>=0.4.1`.\nWe are not planning to support Python 2.x.\n\n**IMPORTANT:** After installing `nmtpytorch`, you **need** to run `nmtpy-install-extra`\nto download METEOR related files into your `${HOME}/.nmtpy` folder.\nThis step is only required once.\n\n### pip\n\nYou can install `nmtpytorch` from `PyPI` using `pip` (or `pip3` depending on your\noperating system and environment):\n\n```\n$ pip install nmtpytorch\n```\n\nThis will automatically fetch and install the dependencies as well. For the `torch`\ndependency it will specifically install the `torch 1.0.0` package from `PyPI` that\nships `CUDA 9.0` within.\n\n### conda\n\nWe provide an `environment.yml` file in the repository that you can use to create\na ready-to-use anaconda environment for `nmtpytorch`:\n\n```\n$ conda update --all\n$ git clone https://github.com/lium-lst/nmtpytorch.git\n$ conda env create -f nmtpytorch/environment.yml\n```\n\n### Development Mode\n\nFor continuous development and testing, it is sufficient to run `python setup.py develop`\nin the root folder of your GIT checkout. From now on, all modifications to the source\ntree are directly taken into account without requiring reinstallation.\n\n## Documentation\n\nWe currently only provide some preliminary documentation in our [wiki](https://github.com/lium-lst/nmtpytorch/wiki).\n\n## Release Notes\n\nThis release supports Pytorch >= 0.4.1 including the recent 1.0 release. The relevant\n`setup.py` and `environment.yml` files will default to `1.0.0` installation.\n\n### v4.0.0 (18/12/2018)\n - **Critical**: `NumpyDataset` now returns tensors of shape `HxW, N, C` for 3D/4D convolutional features, `1, N, C` for 2D feature files. Models should be adjusted to adapt to this new shaping.\n - An `order_file` per split (`ord: path/to/txt file with integer per line`) can be given from the configurations to change the feature order of numpy tensors to flexibly revert, shuffle, tile, etc. them.\n - Better dimension checking to ensure that everything is OK.\n - Added `LabelDataset` for single label input/outputs with associated `Vocabulary` for integer mapping.\n - Added `handle_oom=(True|False)` argument for `[train]` section to recover from **GPU out-of-memory (OOM)** errors during training. This is disabled by default, you need to enable it from the experiment configuration file. Note that it is still possible to get an OOM during validation perplexity computation. If you hit that, reduce the `eval_batch_size` parameter.\n - Added `de-hyphen` post-processing filter to stitch back the aggressive hyphen splitting of Moses during early-stopping evaluations.\n - Added optional projection layer and layer normalization to `TextEncoder`.\n - Added `enc_lnorm, sched_sampling` options to `NMT` to enable layer normalization for encoder and use **scheduled sampling** at a given probability.\n - `ConditionalDecoder` can now be initialized with max-pooled encoder states or the last state as well.\n - You can now experiment with different decoders for `NMT` by changing the `dec_variant` option.\n - Collect all attention weights in `self.history` dictionary of the decoders.\n - Added **n-best** output to `nmtpy translate` with the argument `-N`.\n - Changed the way `-S` works for `nmtpy translate`. Now you need to give the split name with `-s` all the time but `-S` is used to override the input data sources defined for that split in the configuration file.\n - Removed decoder-initialized multimodal NMT `MNMTDecInit`. Same functionality exists within the `NMT` model by using the model option `dec_init=feats`.\n - **New model MultimodalNMT:** that supports encoder initialization, decoder initialization, both, concatenation of embeddings with visual features, prepending and appending. This model covers almost all the models from [LIUM-CVC's WMT17 multimodal systems](https://arxiv.org/abs/1707.04481) except the multiplicative interaction variants such as `trgmul`.\n - **New model MultimodalASR:** encoder-decoder initialized ASR model. See the [paper](https://arxiv.org/abs/1811.03865)\n - **New Model AttentiveCaptioning:** Similar but not an exact reproduction of show-attend-and-tell, it uses feature files instead of raw images.\n - **New model AttentiveMNMTFeaturesFA:** [LIUM-CVC's WMT18 multimodal system](https://arxiv.org/abs/1809.00151) i.e. filtered attention\n - **New (experimental) model NLI:** A simple LSTM-based NLI baseline for [SNLI](https://nlp.stanford.edu/projects/snli/) dataset:\n    - `direction` should be defined as `direction: pre:Text, hyp:Text -> lb:Label`\n    - `pre, hyp` and `lb` keys point to plain text files with one sentence per line. A vocabulary should be constructed even for the labels to fit the nmtpy architecture.\n    - `acc` should be added to `eval_metrics` to compute accuracy.\n\n### v3.0.0 (05/10/2018)\nMajor release that brings support for **Pytorch 0.4** and drops support for **0.3**.\n\nTraining and testing on **CPUs** are now supported thanks to easier device\nsemantics of Pytorch 0.4: just give `-d cpu` to `nmtpy` to switch to CPU mode.\nNOTE: Training on CPUs is only logical for debugging, otherwise it's very slow.\n  - NOTE: `device_id` is no longer a configuration option. It should be removed\n  from your old configurations.\n  - Multi-GPU is not supported. Always restrict to single GPU using\n    `CUDA_VISIBLE_DEVICES` environment variable.\n\nYou can now override the config options used to train a model during\ninference: Example: `nmtpy translate (...) -x model.att_temp:0.9`\n\n`nmtpy train` now detects invalid/old `[train]` options and refuses to\ntrain the model.\n\n**New sampler:** `ApproximateBucketBatchSampler`\nSimilar to the default `BucketBatchSampler` but more efficient for sparsely\ndistributed sequence-lengths as in speech recognition. It bins similar-length\nitems to buckets. It no longer guarantees that the batches are completely\nmade of same-length sequences so **care has to be taken in the encoders**\nto support packing/padding/masking. `TextEncoder` already does this automatically\nwhile speech encoder `BiLSTMp` does not care.\n\n**EXPERIMENTAL**: You can decode an ASR system using the approximate sampler\nalthough the model does not take care of the padded positions (a warning\nwill be printed at each batch).\nThe loss is 0.2% WER for a specific dataset that we tried. So although the computations\nin the encoder becomes noisy and not totally correct, the model can handle\nthis noise quite robustly:\n\n`$ nmtpy translate -s val -o hyp -x model.sampler_type:approximate best_asr.ckpt`\n\nThis type of batching cuts ASR decoding time almost by a factor of 2-3.\n\n#### Other changes\n  - Vocabularies generated by `nmtpy-build-vocab` now contains frequency\n    information as well. The code is backward-compatible with old vocab files.\n  - `Batch` objects should now be explicitly moved to the allocated device\n    using `.device()` method. See `mainloop.py` and `test_performance()` from\n    the `NMT` model.\n  - Training no longer shows the cached GPU allocation from `nvidia-smi` output\n    as it was in the end a hacky thing to call `nvidia-smi` periodically. We\n    plan to use `torch.cuda.*` to get an estimate on memory consumption.\n  - NOTE: Multi-process data loading is temporarily disabled as it was\n    crashing from time to time so `num_workers > 0` does not have an effect\n    in this release.\n  - `Attention` is separated into `DotAttention` and `MLPAttention` and a\n    convenience function `get_attention()` is provided to select between them\n    during model construction.\n  - `get_activation_fn()` should be used to select between non-linearities\n    dynamically instead of doing `getattr(nn.functional, activ)`. The latter\n    will not work for `tanh` and `sigmoid` in the next Pytorch releases.\n  - Simplification: `ASR` model is now derived from `NMT`.\n\n\n### v2.0.0 (26/09/2018)\n  - Ability to install through `pip`.\n  - Advanced layers are now organized into subfolders.\n  - New basic layers: Convolution over sequence, MaxMargin.\n  - New attention layers: Co-attention, multi-head attention, hierarchical attention.\n  - New encoders: Arbitrary sequence-of-vectors encoder, BiLSTMp speech feature encoder.\n  - New decoders: Multi-source decoder, switching decoder, vector decoder.\n  - New datasets: Kaldi dataset (.ark/.scp reader), Shelve dataset, Numpy sequence dataset.\n  - Added learning rate annealing: See `lr_decay*` options in `config.py`.\n  - Removed subword-nmt and METEOR files from repository. We now depend on\n    the PIP package for subword-nmt. For METEOR, `nmtpy-install-extra` should\n    be launched after installation.\n  - More multi-task and multi-input/output `translate` and `training` regimes.\n  - New early-stopping metrics: Character and word error rate (cer,wer) and ROUGE (rouge).\n  - Curriculum learning option for the `BucketBatchSampler`, i.e. length-ordered batches.\n  - New models:\n     - ASR: Listen-attend-and-spell like automatic speech recognition\n     - Multitask*: Experimental multi-tasking & scheduling between many inputs/outputs.\n\n### v1.4.0 (09/05/2018)\n  - Add `environment.yml` for easy installation using `conda`. You can now\n  create a ready-to-use `conda` environment by just calling `conda env create -f environment.yml`.\n  - Make `NumpyDataset` memory efficient by keeping `float16` arrays as they are\n  until batch creation time.\n  - Rename `Multi30kRawDataset` to `Multi30kDataset` which now supports both\n  raw image files and pre-extracted visual features file stored as `.npy`.\n  - Add CNN feature extraction script under `scripts/`.\n  - Add doubly stochastic attention to `ShowAttendAndTell` and multimodal NMT.\n  - New model `MNMTDecinit` to initialize decoder with auxiliary features.\n  - New model `AMNMTFeatures` which is the attentive MMT but with features file\n  instead of end-to-end feature extraction which was memory hungry.\n\n### v1.3.2 (02/05/2018)\n\n  - Updates to `ShowAttendAndTell` model.\n\n### v1.3.1 (01/05/2018)\n\n  - Removed old `Multi30kDataset`.\n  - Sort batches by source sequence length instead of target.\n  - Fix `ShowAttendAndTell` model. It should now work.\n\n### v1.3 (30/04/2018)\n\n - Added `Multi30kRawDataset` for training end-to-end systems from raw images as input.\n - Added `NumpyDataset` to read `.npy/.npz` tensor files as input features.\n - You can now pass `-S` to `nmtpy train` to produce shorter experiment files with not all the hyperparameters in file name.\n - New post-processing filter option `de-spm` for Google SentencePiece (SPM) processed files.\n - `sacrebleu` is now a dependency as it is now accepted as an early-stopping metric.\n It only makes sense to use it with SPM processed files since they are detokenized\n once post-processed.\n - Added `sklearn` as a dependency for some metrics.\n - Added `momentum` and `nesterov` parameters to `[train]` section for SGD.\n - `ImageEncoder` layer is improved in many ways. Please see the code for further details.\n - Added unmerged upstream [PR](https://github.com/pytorch/pytorch/pull/5297/files) for `ModuleDict()` support.\n - `METEOR` will now fallback to English if language can not be detected from file suffixes.\n - `-f` now produces a separate numpy file for token frequencies when building vocabulary files with `nmtpy-build-vocab`.\n - Added new command `nmtpy test` for non beam-search inference modes.\n - Removed `nmtpy resume` command and added `pretrained_file` option for `[train]` to initialize model weights from a checkpoint.\n - Added `freeze_layers` option for `[train]` to give comma-separated list of layer name prefixes to freeze.\n - Improved seeding: seed is now printed in order to reproduce the results.\n - Added IPython notebook for attention visualization.\n - **Layers**\n   - New shallow `SimpleGRUDecoder` layer.\n   - `TextEncoder`: Ability to set `maxnorm` and `gradscale` of embeddings and work with or without sorted-length batches.\n   - `ConditionalDecoder`: Make it work with GRU/LSTM, allow setting `maxnorm/gradscale` for embeddings.\n   - `ConditionalMMDecoder`: Same as above.\n - **nmtpy translate**\n   - `--avoid-double` and `--avoid-unk` removed for now.\n   - Added Google's length penalty normalization switch `--lp-alpha`.\n   - Added ensembling which is enabled automatically if you give more than 1 model checkpoints.\n - New machine learning metric wrappers in `utils/ml_metrics.py`:\n   - Label-ranking average precision `lrap`\n   - Coverage error\n   - Mean reciprocal rank\n\n### v1.2 (20/02/2018)\n\n - You can now use `$HOME` and `$USER` in your configuration files.\n - Fixed an overflow error that would cause NMT with more than 255 tokens to fail.\n - METEOR worker process is now correctly killed after validations.\n - Many runs of an experiment are now suffixed with a unique random string instead of incremental integers to avoid race conditions in cluster setups.\n - Replaced `utils.nn.get_network_topology()` with a new `Topology` [class](nmtpytorch/utils/topology.py) that will parse the `direction` string of the model in a more smart way.\n - If `CUDA_VISIBLE_DEVICES` is set, the `GPUManager` will always honor it.\n - Dropped creation of temporary/advisory lock files under `/tmp` for GPU reservation.\n - Time measurements during training are now structered into batch overhead, training and evaluation timings.\n - **Datasets**\n   - Added `TextDataset` for standalone text file reading.\n   - Added `OneHotDataset`, a variant of `TextDataset` where the sequences are not prefixed/suffixed with `<bos>` and `<eos>` respectively.\n   - Added experimental `MultiParallelDataset` that merges an arbitrary number of parallel datasets together.\n - **nmtpy translate**\n   - `.nodbl` and `.nounk` suffixes are now added to output files for `--avoid-double` and `--avoid-unk` arguments respectively.\n   - A model-agnostic enough `beam_search()` is now separated out into its own file `nmtpytorch/search.py`.\n   - `max_len` default is increased to 200.\n\n### v1.1 (25/01/2018)\n\n - New experimental `Multi30kDataset` and `ImageFolderDataset` classes\n - `torchvision` dependency added for CNN support\n - `nmtpy-coco-metrics` now computes one METEOR without `norm=True`\n - Mainloop mechanism is completely refactored with **backward-incompatible**\n   configuration option changes for `[train]` section:\n    - `patience_delta` option is removed\n    - Added `eval_batch_size` to define batch size for GPU beam-search during training\n    - `eval_freq` default is now `3000` which means per `3000` minibatches\n    - `eval_metrics` now defaults to `loss`. As before, you can provide a list\n      of metrics like `bleu,meteor,loss` to compute all of them and early-stop\n      based on the first\n    - Added `eval_zero (default: False)` which tells to evaluate the model\n      once on dev set right before the training starts. Useful for sanity\n      checking if you fine-tune a model initialized with pre-trained weights\n    - Removed `save_best_n`: we no longer save the best `N` models on dev set\n      w.r.t. early-stopping metric\n    - Added `save_best_metrics (default: True)` which will save best models\n      on dev set w.r.t each metric provided in `eval_metrics`. This kind of\n      remedies the removal of `save_best_n`\n    - `checkpoint_freq` now to defaults to `5000` which means per `5000`\n      minibatches.\n    - Added `n_checkpoints (default: 5)` to define the number of last\n      checkpoints that will be kept if `checkpoint_freq > 0` i.e. checkpointing enabled\n  - Added `ExtendedInterpolation` support to configuration files:\n    - You can now define intermediate variables in `.conf` files to avoid\n      typing same paths again and again. A variable can be referenced\n      from within its **section** using `tensorboard_dir: ${save_path}/tb` notation\n      Cross-section references are also possible: `${data:root}` will be replaced\n      by the value of the `root` variable defined in the `[data]` section.\n  - Added `-p/--pretrained` to `nmtpy train` to initialize the weights of\n    the model using another checkpoint `.ckpt`.\n  - Improved input/output handling for `nmtpy translate`:\n    - `-s` accepts a comma-separated test sets **defined** in the configuration\n      file of the experiment to translate them at once. Example: `-s val,newstest2016,newstest2017`\n    - The mutually exclusive counterpart of `-s` is `-S` which receives a\n      single input file of source sentences.\n    - For both cases, an output prefix **should now be** provided with `-o`.\n      In the case of multiple test sets, the output prefix will be appended\n      the name of the test set and the beam size. If you just provide a single file with `-S`\n      the final output name will only reflect the beam size information.\n - Two new arguments for `nmtpy-build-vocab`:\n    - `-f`: Stores frequency counts as well inside the final `json` vocabulary\n    - `-x`: Does not add special markers `<eos>,<bos>,<unk>,<pad>` into the vocabulary\n\n#### Layers/Architectures\n\n - Added `Fusion()` layer to `concat,sum,mul` an arbitrary number of inputs\n - Added *experimental* `ImageEncoder()` layer to seamlessly plug a VGG or ResNet\n   CNN using `torchvision` pretrained models\n - `Attention` layer arguments improved. You can now select the bottleneck\n   dimensionality for MLP attention with `att_bottleneck`. The `dot`\n   attention is **still not tested** and probably broken.\n\nNew layers/architectures:\n\n - Added **AttentiveMNMT** which implements modality-specific multimodal attention\n   from the paper [Multimodal Attention for Neural Machine Translation](https://arxiv.org/abs/1609.03976)\n - Added **ShowAttendAndTell** [model](http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf)\n\nChanges in **NMT**:\n\n  - `dec_init` defaults to `mean_ctx`, i.e. the decoder will be initialized\n    with the mean context computed from the source encoder\n  - `enc_lnorm` which was just a placeholder is now removed since we do not\n    provided layer-normalization for now\n  - Beam Search is completely moved to GPU\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/lium-lst/nmtpytorch",
    "keywords": "nmt neural-mt translation sequence-to-sequence deep-learning pytorch",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "nmtpytorch",
    "package_url": "https://pypi.org/project/nmtpytorch/",
    "platform": "",
    "project_url": "https://pypi.org/project/nmtpytorch/",
    "project_urls": {
      "Homepage": "https://github.com/lium-lst/nmtpytorch",
      "Wiki": "https://github.com/lium-lst/nmtpytorch/wiki"
    },
    "release_url": "https://pypi.org/project/nmtpytorch/4.0.0/",
    "requires_dist": [
      "numpy",
      "scipy",
      "scikit-learn",
      "tqdm",
      "pillow",
      "torch (==1.0.0)",
      "torchvision (==0.2.1)",
      "sacrebleu (>=1.2.9)",
      "tensorboardX (==1.1)",
      "editdistance (==0.4)",
      "subword-nmt (==0.3.5)"
    ],
    "requires_python": "~=3.6",
    "summary": "Sequence-to-Sequence Framework in Pytorch",
    "version": "4.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4613001,
  "releases": {
    "2.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0aa387931507b1173f577610fafa18ccfd06a64460f08c168fa7a7788a7d9e47",
          "md5": "9ec961b956cde9a0126b7e302cd53693",
          "sha256": "149042651a163b0fdf9d76e1ae14f954d29e8142377819aac7733f60f9931900"
        },
        "downloads": -1,
        "filename": "nmtpytorch-2.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9ec961b956cde9a0126b7e302cd53693",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.6",
        "size": 144868,
        "upload_time": "2018-09-26T14:57:44",
        "upload_time_iso_8601": "2018-09-26T14:57:44.413414Z",
        "url": "https://files.pythonhosted.org/packages/0a/a3/87931507b1173f577610fafa18ccfd06a64460f08c168fa7a7788a7d9e47/nmtpytorch-2.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "33dedfc94c038779a1a826c50035cb9ff38b4ab80288f549c011e1e9b8198908",
          "md5": "e4a7e396d0a4bc759b7aef5881f9ae20",
          "sha256": "3ea5448cc3d2c2f2d3704f1c37b632eabc54836ca2683aaf86d09bd3336d00aa"
        },
        "downloads": -1,
        "filename": "nmtpytorch-2.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "e4a7e396d0a4bc759b7aef5881f9ae20",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.6",
        "size": 99203,
        "upload_time": "2018-09-26T14:57:46",
        "upload_time_iso_8601": "2018-09-26T14:57:46.346940Z",
        "url": "https://files.pythonhosted.org/packages/33/de/dfc94c038779a1a826c50035cb9ff38b4ab80288f549c011e1e9b8198908/nmtpytorch-2.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "3.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9109a8f36bec3f2c5efbb90841c158bf06da3f66487a2e811822a0ee6859b947",
          "md5": "33e8139cb93677f6a72db84ce4951356",
          "sha256": "339bca6bc20e3df2ba3ec89f6d6b2ce257df9ddb5a4d207ea21cd008f87da061"
        },
        "downloads": -1,
        "filename": "nmtpytorch-3.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "33e8139cb93677f6a72db84ce4951356",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.6",
        "size": 149430,
        "upload_time": "2018-10-05T10:42:58",
        "upload_time_iso_8601": "2018-10-05T10:42:58.967109Z",
        "url": "https://files.pythonhosted.org/packages/91/09/a8f36bec3f2c5efbb90841c158bf06da3f66487a2e811822a0ee6859b947/nmtpytorch-3.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f51cb8bb93ae1c22f5a9520fc9567f2f31d3e0e4cff4c180b853fa48cd6a3146",
          "md5": "f7295c10a12bee017f06092cd2d3bd58",
          "sha256": "4a65d265bce156e9e0d83f93da41cc75110eb067b94d8ccfac1189b783899b1c"
        },
        "downloads": -1,
        "filename": "nmtpytorch-3.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f7295c10a12bee017f06092cd2d3bd58",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.6",
        "size": 102111,
        "upload_time": "2018-10-05T10:43:00",
        "upload_time_iso_8601": "2018-10-05T10:43:00.977054Z",
        "url": "https://files.pythonhosted.org/packages/f5/1c/b8bb93ae1c22f5a9520fc9567f2f31d3e0e4cff4c180b853fa48cd6a3146/nmtpytorch-3.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "4.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5068e882fd7a0bd06e1ee18889769fb1ed100f3f24d8cbebee9f82c1c1bc957a",
          "md5": "520dd8c820a33215b3f94cc33d5d5540",
          "sha256": "47147d91e03de1e004aae54c1693406e1e535dc553ac0465a9fafe3d566e22e2"
        },
        "downloads": -1,
        "filename": "nmtpytorch-4.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "520dd8c820a33215b3f94cc33d5d5540",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.6",
        "size": 168615,
        "upload_time": "2018-12-18T16:55:05",
        "upload_time_iso_8601": "2018-12-18T16:55:05.486792Z",
        "url": "https://files.pythonhosted.org/packages/50/68/e882fd7a0bd06e1ee18889769fb1ed100f3f24d8cbebee9f82c1c1bc957a/nmtpytorch-4.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ba598927c44727f163123f4f2be8bb6463135c87e6c867807af0d9237dcfcdf0",
          "md5": "dc02b46c1546696146f9425fbd2ec595",
          "sha256": "8aa762afadac5aa73e0faeebeeafabb337bce987aee5b8ce538c7749fa2cc0cd"
        },
        "downloads": -1,
        "filename": "nmtpytorch-4.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "dc02b46c1546696146f9425fbd2ec595",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.6",
        "size": 115575,
        "upload_time": "2018-12-18T16:55:08",
        "upload_time_iso_8601": "2018-12-18T16:55:08.141054Z",
        "url": "https://files.pythonhosted.org/packages/ba/59/8927c44727f163123f4f2be8bb6463135c87e6c867807af0d9237dcfcdf0/nmtpytorch-4.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5068e882fd7a0bd06e1ee18889769fb1ed100f3f24d8cbebee9f82c1c1bc957a",
        "md5": "520dd8c820a33215b3f94cc33d5d5540",
        "sha256": "47147d91e03de1e004aae54c1693406e1e535dc553ac0465a9fafe3d566e22e2"
      },
      "downloads": -1,
      "filename": "nmtpytorch-4.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "520dd8c820a33215b3f94cc33d5d5540",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "~=3.6",
      "size": 168615,
      "upload_time": "2018-12-18T16:55:05",
      "upload_time_iso_8601": "2018-12-18T16:55:05.486792Z",
      "url": "https://files.pythonhosted.org/packages/50/68/e882fd7a0bd06e1ee18889769fb1ed100f3f24d8cbebee9f82c1c1bc957a/nmtpytorch-4.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ba598927c44727f163123f4f2be8bb6463135c87e6c867807af0d9237dcfcdf0",
        "md5": "dc02b46c1546696146f9425fbd2ec595",
        "sha256": "8aa762afadac5aa73e0faeebeeafabb337bce987aee5b8ce538c7749fa2cc0cd"
      },
      "downloads": -1,
      "filename": "nmtpytorch-4.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "dc02b46c1546696146f9425fbd2ec595",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "~=3.6",
      "size": 115575,
      "upload_time": "2018-12-18T16:55:08",
      "upload_time_iso_8601": "2018-12-18T16:55:08.141054Z",
      "url": "https://files.pythonhosted.org/packages/ba/59/8927c44727f163123f4f2be8bb6463135c87e6c867807af0d9237dcfcdf0/nmtpytorch-4.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}