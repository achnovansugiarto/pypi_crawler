{
  "info": {
    "author": "Cristi Constantin",
    "author_email": "cristi.constantin@speedpost.net",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Environment :: Console",
      "Framework :: Scrapy",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: Software Development"
    ],
    "description": "# Scrapy-link-filter\n\n  [![Python ver][python-image]][python-url]\n  [![Build Status][build-image]][build-url]\n  [![Code coverage][cover-image]][cover-url]\n  [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\nSpider Middleware that allows a [Scrapy Spider](https://scrapy.readthedocs.io/en/latest/topics/spiders.html) to filter requests.\nThere is similar functionality in the [CrawlSpider](https://scrapy.readthedocs.io/en/latest/topics/spiders.html#crawlspider) already using Rules and in the [RobotsTxtMiddleware](https://scrapy.readthedocs.io/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.robotstxt), but there are twists.\nThis middleware allows defining rules dinamically per request, or as spider arguments instead of project settings.\n\n\n## Install\n\nThis project requires [Python 3.6+](https://www.python.org/) and [pip](https://pip.pypa.io/). Using a [virtual environment](https://virtualenv.pypa.io/) is strongly encouraged.\n\n```sh\n$ pip install git+https://github.com/croqaz/scrapy-link-filter\n```\n\n\n## Usage\n\nFor the middleware to be enabled as a Spider Middleware, it must be added in the project `settings.py`:\n\n```\nSPIDER_MIDDLEWARES = {\n    # maybe other Spider Middlewares ...\n    # can go after DepthMiddleware: 900\n    'scrapy_link_filter.middleware.LinkFilterMiddleware': 950,\n}\n```\n\nOr, it can be enabled as a Downloader Middleware, in the project `settings.py`:\n\n```\nDOWNLOADER_MIDDLEWARES = {\n    # maybe other Downloader Middlewares ...\n    # can go before RobotsTxtMiddleware: 100\n    'scrapy_link_filter.middleware.LinkFilterMiddleware': 50,\n}\n```\n\nThe rules must be defined either in the spider instance, in a `spider.extract_rules` dict, or per request, in `request.meta['extract_rules']`.\nInternally, the extract_rules dict is converted into a [LinkExtractor](https://docs.scrapy.org/en/latest/topics/link-extractors.html), which is used to match the requests.\n\n**Note** that the URL matching is case-sensitive by default, which works in most cases. To enable case-insensitive matching, you can specify a \"(?i)\" inline flag in the beggining of each \"allow\", or \"deny\" rule that needs to be case-insensitive.\n\n\nExample of a specific allow filter, on a spider instance:\n\n```py\nfrom scrapy.spiders import Spider\n\nclass MySpider(Spider):\n    extract_rules = {\"allow_domains\": \"example.com\", \"allow\": \"/en/items/\"}\n```\n\nOr a specific deny filter, inside a request meta:\n\n```py\nrequest.meta['extract_rules'] = {\n    \"deny_domains\": [\"whatever.com\", \"ignore.me\"],\n    \"deny\": [\"/privacy-policy/?$\", \"/about-?(us)?$\"]\n}\n```\n\nThe possible fields are:\n* `allow_domains` and `deny_domains` - one, or more domains to specifically limit to, or specifically reject\n* `allow` and `deny` - one, or more sub-strings, or patterns to specifically allow, or reject\n\nAll fields can be defined as string, list, set, or tuple.\n\n-----\n\n## License\n\n[BSD3](LICENSE) Â© Cristi Constantin.\n\n\n[build-image]: https://github.com/croqaz/scrapy-link-filter/workflows/Python/badge.svg\n[build-url]: https://github.com/croqaz/scrapy-link-filter/actions\n[cover-image]: https://codecov.io/gh/croqaz/scrapy-link-filter/branch/master/graph/badge.svg\n[cover-url]: https://codecov.io/gh/croqaz/scrapy-link-filter\n[python-image]: https://img.shields.io/badge/Python-3.6-blue.svg\n[python-url]: https://python.org\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/croqaz/scrapy-link-filter",
    "keywords": "scrapy link filter",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-link-filter",
    "package_url": "https://pypi.org/project/scrapy-link-filter/",
    "platform": "Any",
    "project_url": "https://pypi.org/project/scrapy-link-filter/",
    "project_urls": {
      "Homepage": "https://github.com/croqaz/scrapy-link-filter"
    },
    "release_url": "https://pypi.org/project/scrapy-link-filter/0.2.0/",
    "requires_dist": null,
    "requires_python": ">=3.6.0",
    "summary": "Scrapy Middleware that allows a Scrapy Spider to filter requests.",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6285826,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3eb534c59d25a104dd87c90fd3f22f53f1bbbba080a06f3b0a230370c4a7606a",
          "md5": "a9d250b08095b6e707f8dad5fa301719",
          "sha256": "25093db548efc86823b6eb27db973c67b9877e0892713ceb676723d636be0b63"
        },
        "downloads": -1,
        "filename": "scrapy_link_filter-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a9d250b08095b6e707f8dad5fa301719",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 4931,
        "upload_time": "2019-12-12T13:16:41",
        "upload_time_iso_8601": "2019-12-12T13:16:41.940176Z",
        "url": "https://files.pythonhosted.org/packages/3e/b5/34c59d25a104dd87c90fd3f22f53f1bbbba080a06f3b0a230370c4a7606a/scrapy_link_filter-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "036c6e45ddd2686db395babbde8daaa5518bd13d6ff7335c2b8657808b102cae",
          "md5": "40db0993a46ff47482395c967187600a",
          "sha256": "eb562565d5ce4241d751f594998183516524b7a1f34d4dd4c3bf78ac090e3ddd"
        },
        "downloads": -1,
        "filename": "scrapy-link-filter-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "40db0993a46ff47482395c967187600a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 4464,
        "upload_time": "2019-12-12T13:16:44",
        "upload_time_iso_8601": "2019-12-12T13:16:44.734449Z",
        "url": "https://files.pythonhosted.org/packages/03/6c/6e45ddd2686db395babbde8daaa5518bd13d6ff7335c2b8657808b102cae/scrapy-link-filter-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a8d549b7e0fcd23809a513a59d9224812355d2b8424b672b3963e7d8155e4ba9",
          "md5": "d8319c5b016dd2c6f0a4f3e81e233b9a",
          "sha256": "f1f6c25569a765945a331daac3929731cd1f4e07ad129705ee53d0b6e65f9d36"
        },
        "downloads": -1,
        "filename": "scrapy_link_filter-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d8319c5b016dd2c6f0a4f3e81e233b9a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 6214,
        "upload_time": "2019-12-12T13:17:11",
        "upload_time_iso_8601": "2019-12-12T13:17:11.647523Z",
        "url": "https://files.pythonhosted.org/packages/a8/d5/49b7e0fcd23809a513a59d9224812355d2b8424b672b3963e7d8155e4ba9/scrapy_link_filter-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3d1c175ecef969380c969abbd90f1063b8375fe68131f8a2c0c5995beb0b0a84",
          "md5": "c46a8775512a159c7e5898e48c779229",
          "sha256": "64bf701cbbbc9f51dad47094c1effbf9ca47b6d6e9f54c0f64cedefcbbbc72e4"
        },
        "downloads": -1,
        "filename": "scrapy-link-filter-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c46a8775512a159c7e5898e48c779229",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 5449,
        "upload_time": "2019-12-12T13:17:14",
        "upload_time_iso_8601": "2019-12-12T13:17:14.034780Z",
        "url": "https://files.pythonhosted.org/packages/3d/1c/175ecef969380c969abbd90f1063b8375fe68131f8a2c0c5995beb0b0a84/scrapy-link-filter-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a8d549b7e0fcd23809a513a59d9224812355d2b8424b672b3963e7d8155e4ba9",
        "md5": "d8319c5b016dd2c6f0a4f3e81e233b9a",
        "sha256": "f1f6c25569a765945a331daac3929731cd1f4e07ad129705ee53d0b6e65f9d36"
      },
      "downloads": -1,
      "filename": "scrapy_link_filter-0.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d8319c5b016dd2c6f0a4f3e81e233b9a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 6214,
      "upload_time": "2019-12-12T13:17:11",
      "upload_time_iso_8601": "2019-12-12T13:17:11.647523Z",
      "url": "https://files.pythonhosted.org/packages/a8/d5/49b7e0fcd23809a513a59d9224812355d2b8424b672b3963e7d8155e4ba9/scrapy_link_filter-0.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3d1c175ecef969380c969abbd90f1063b8375fe68131f8a2c0c5995beb0b0a84",
        "md5": "c46a8775512a159c7e5898e48c779229",
        "sha256": "64bf701cbbbc9f51dad47094c1effbf9ca47b6d6e9f54c0f64cedefcbbbc72e4"
      },
      "downloads": -1,
      "filename": "scrapy-link-filter-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "c46a8775512a159c7e5898e48c779229",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 5449,
      "upload_time": "2019-12-12T13:17:14",
      "upload_time_iso_8601": "2019-12-12T13:17:14.034780Z",
      "url": "https://files.pythonhosted.org/packages/3d/1c/175ecef969380c969abbd90f1063b8375fe68131f8a2c0c5995beb0b0a84/scrapy-link-filter-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}