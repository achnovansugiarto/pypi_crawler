{
  "info": {
    "author": "Hashmap, Inc",
    "author_email": "accelerators@hashmapinc.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<!---\n# Modifications Â© 2020 Hashmap, Inc\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n-->\n# dataframez\n\nExtension to pandas to allow for simple cross-cloud-platform interactions with data, use of data versioning tools, and much more. The idea is to make it very simple for pandas users to interact with named data sources. \n\nA named data source is a source where the name can be used to retrieve the data without giving additional access criteria - such as would be necessary when accessing data through a database connection, connection to a cloud resource, and so on. This can be bothersome for a data scientist. Who wants to track where their data resides!!!\n\nIn modern environment a data catalog is often used to track data assets. But interacting with these catalogs is also bothersome. The use of a named asset abstracts the interface with such catalogs by providing all the necessary interactions with this 'catalog' to identify and retrieve teh data. Cataloging in this sense can also mean a data versioning utility. In gereal, however, this means that the catalog interactions of dataframez can work across many catalogs in tandem; that is, with enterprise catalog and data scientist catalogs at the same time. \n\n##About\ndataframez is pandas wrapper designed to provide an abstraction between a data catalog, or catalog for short, and the users standard interaction with pandas.\n\n**Intent**\n\nThe purpose for the catalog is two fold.\n1. To abstract away the need to know where data is being stored and to simplify reading without having to necessarily know what kind of data you are reading.\n2. Enable enterprise governance controls to mandate where data is stored, what kind of data persistence is allowed. They will also work with IT to make sure the correct interface is available if it does not currently exist.\n\n**Configuration**\n\nConfiguration will identify what kind of catalog is being used (name of catalog class - preferably lowercase). It will also identify for each type of persistence where the data will be persisted (or other appropriate abstraction) and whether a specific kind of persistence is allowed, or not.\n\nThe configuration file is a YAML file with the following format\n```yaml\nversion: VERSION_NUMBER\n\nconfigurations:\n  catalog:\n    type: CATALOG_TYPE\n    conf: SOME_CONFIGURATION\n  writers:\n    csv:\n      type: csv\n        conf:\n          enabled: BOOLEAN\n          OTHER_CONF: values\n    parquet:\n      type: parquet\n        conf:\n          enabled: BOOLEAN\n          OTHER_CONF: values\n#etc...    \n```\nThe values that are in all CAPS are to be filled in with appropriate values. At this time there is only one configuration version.\n\n**Example Configuration** \n\n```yaml\nversion: 1\n\nconfigurations:\n  catalog:\n    type: local\n    conf:\n      location: $HOME/.dataframez\n      name: default_catalog.dfz\n  writers:\n    csv:\n      type: csv\n      conf:\n        path: $HOME/.dataframez\n        allowed: true\n    parquet:\n      type: parquet\n      conf:\n        allowed: false\n```\n\n##API\nThe intent has been to keep the API as simple as possible by minimally extending the pandas API and supporting, for the most part, the same functionality in terms of saving data outputs as is done in pandas.\n\n### Reading from a Catalog\n__pandas.from_catalog(name: str, version: int, **kwargs) -> pandas.DataFrame__\nThis method extends the read capabilities of pandas to read from a 'cataloged' asset. \n\n###Extended Write Capabilities\n\nThe write capabilities - to cataloged entrypoints - of pandas has been extended by providing capabilities in the pandas name space extension 'dataframez'. In this namespace standard pandas write methods are added - with the addition of an asset registration name in place of common persistence identifies like a path. In some cases default parameters are changed to make the seemless integration of read & write smooth.\n\nIn addition to the norm - additional methods have been added for specialized data source interactions.\n\nAlso, in order to discover cataloged resources, you can call the list_assets() method to retrieve a list of all asset names.\n#### Supported Methods\n* pandas.DataFrame.dataframez.to_csv\n* pandas.DataFrame.dataframez.to_parquet\n* pandas.DataFrame.dataframez.to_pickle\n\n*NOTE: Through all of the write methods it should be noted that entry_name is used both in the name of the source and as the name of the entry in the catalog.*\n#### to_csv(entry_name: str, **kwargs)\nThis will write the data out to a persistence storage as CSV format while logging the asset to a catalog with entry_name. kwargs represents the standard write parameters in pandas which can be used here in the same. \n\n*Make note that the default value of index_col has been changed to 0 to make sure that the write & read defaults are as seamless as possible.*\n\n#### to_parquet(entry_name: str, **kwargs)\nThis will write the data out to a persistence storage as CSV format while logging the asset to a catalog with entry_name. kwargs represents the standard write parameters in pandas which can be used here in the same.\n\n#### to_pickle(entry_name: str, **kwargs)\nThis will write the data out to a persistence storage as CSV format while logging the asset to a catalog with entry_name. kwargs represents the standard write parameters in pandas which can be used here in the same.\n\n## Examples\n\n__Reading and Writing__\n```python\nimport pandas as pd \nimport dataframez\n\ndf_to_write = pd.DataFrame.from_dict({'a': [1, 2, 3], 'b': [2, 3, 5]})\n\ndf_to_write.dataframez.to_parquet(entry_name='my_asset')\n\ndf_read_from_catalog = pd.from_catalog(entry_name='test_data_parquet')\n```\n\n__Getting list of Assets__\n```python\nimport pandas as pd\nimport dataframez\n\nasset_list = pd.list_assets()\n```\n\n## Future Features\n1. Extended support of read/write IO types\n2. Extension to Dask\n3. Extension to pySpark",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/hashmapinc/dataframez",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dataframez",
    "package_url": "https://pypi.org/project/dataframez/",
    "platform": "",
    "project_url": "https://pypi.org/project/dataframez/",
    "project_urls": {
      "Homepage": "https://github.com/hashmapinc/dataframez"
    },
    "release_url": "https://pypi.org/project/dataframez/0.0.2.2/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "DO NOT USE - This is a sample program",
    "version": "0.0.2.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8603949,
  "releases": {
    "0.0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5a0ae12c17b91bd8f95f448a16260c74d04f4e5d0ac5a9ac25b41daf3ec27dac",
          "md5": "142da993554c47888583abf39cc8e1fd",
          "sha256": "506ff8d8fbf4f4074d873767d533430f2608b744fa9596ff601fb2042a0b3f7b"
        },
        "downloads": -1,
        "filename": "dataframez-0.0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "142da993554c47888583abf39cc8e1fd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 11736,
        "upload_time": "2020-11-05T02:05:44",
        "upload_time_iso_8601": "2020-11-05T02:05:44.204275Z",
        "url": "https://files.pythonhosted.org/packages/5a/0a/e12c17b91bd8f95f448a16260c74d04f4e5d0ac5a9ac25b41daf3ec27dac/dataframez-0.0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aff38105e5e126e097f9e340484b1b28386236f68220d451684908c4f77931ff",
          "md5": "725717c6e6d5b02f5b8123577c3abe77",
          "sha256": "433571605592e53a8d4b701565489cb633fed065f26dbe3fdcc656b7c92579d4"
        },
        "downloads": -1,
        "filename": "dataframez-0.0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "725717c6e6d5b02f5b8123577c3abe77",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 11747,
        "upload_time": "2020-11-05T03:45:32",
        "upload_time_iso_8601": "2020-11-05T03:45:32.933482Z",
        "url": "https://files.pythonhosted.org/packages/af/f3/8105e5e126e097f9e340484b1b28386236f68220d451684908c4f77931ff/dataframez-0.0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4ca0bfd506a0e248ce52421f1c79b0c551f1a42daf056addc1a98b3abc88f506",
          "md5": "d1562f4143347bd0167d4a22d94058b3",
          "sha256": "128e7b6c7d5db72cdccf6deecf68da6e74c21c5bbbce2e11f8c49453945b4869"
        },
        "downloads": -1,
        "filename": "dataframez-0.0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "d1562f4143347bd0167d4a22d94058b3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 11818,
        "upload_time": "2020-11-07T15:09:49",
        "upload_time_iso_8601": "2020-11-07T15:09:49.094636Z",
        "url": "https://files.pythonhosted.org/packages/4c/a0/bfd506a0e248ce52421f1c79b0c551f1a42daf056addc1a98b3abc88f506/dataframez-0.0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4ca0bfd506a0e248ce52421f1c79b0c551f1a42daf056addc1a98b3abc88f506",
        "md5": "d1562f4143347bd0167d4a22d94058b3",
        "sha256": "128e7b6c7d5db72cdccf6deecf68da6e74c21c5bbbce2e11f8c49453945b4869"
      },
      "downloads": -1,
      "filename": "dataframez-0.0.2.2.tar.gz",
      "has_sig": false,
      "md5_digest": "d1562f4143347bd0167d4a22d94058b3",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 11818,
      "upload_time": "2020-11-07T15:09:49",
      "upload_time_iso_8601": "2020-11-07T15:09:49.094636Z",
      "url": "https://files.pythonhosted.org/packages/4c/a0/bfd506a0e248ce52421f1c79b0c551f1a42daf056addc1a98b3abc88f506/dataframez-0.0.2.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}