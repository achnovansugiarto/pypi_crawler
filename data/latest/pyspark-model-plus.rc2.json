{
  "info": {
    "author": "Rajarshi Bhadra",
    "author_email": "bhadrarajarshi9@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# pyspark-model-plus\n[![PyPI version](https://img.shields.io/pypi/v/pyspark-model-plus.svg)](https://img.shields.io/pypi/v/pyspark-model-plus)\n\nThis package has been written keeping in mind some functions that we commonly use in scikit-learn but are not currently available in \nspark machine learning library. Capabilities the package is currently adding are\n\n* Multi Class LogLoss Evaluator\n* Stratified Cross Validation\n* Impute multiple columns by column mean (faster)\n\n## About the functions\n\n**MulticlassLogLossEvaluator**\n\n[Spark documentation](https://spark.apache.org/docs/1.6.0/mllib-evaluation-metrics.html) mentions currently there is no existing function available in default spark mllib to perform logloss evaluation for categorical variables. The corresponding function that enables us to perform this in scikit-learn is [log_loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html).This function is an attempt to add that functionality so that it can be used with standard ML pipelines. The core idea for the algorthm has been made on the basis of this [post](http://www.kaggle.com/c/emc-data-science/forums/t/2149/is-anyone-noticing-difference-betwen-validation-and-leaderboard-error/12209#post12209).\n\n**StratifiedCrossValidator**  \n\nStratified sampling is important for hyper parameter tuning during CV fold training as it enables us keep the final tuning parameters robust against sampling bias speciaally whe the data is unabalanced. [Spark documentation](https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation) indicates that we currently cannot do that. As a result many approaches has been proposed to include this in pyspark. For example [spark_stratifier](https://github.com/interviewstreet/spark-stratifier) implements this functionality but with two major drawbacks\n\n* The algorithm is dependent on joins\n* It only works for binary classification problems(as of now)\n\nThis function tries to address both the issues by making the function independant of joins and also making the approach general such that startified cross validation can be done on multiclass classification problems as well\n\n**CustomMeanImputer**  \n\n[Spark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=impute#pyspark.ml.feature.Imputer) shows that a imputer class exists. However for large data sets using for loop in this function makes it slow. This function tried to address that usse by tryinjg to do impute by mean simulateneously using agg and python distionary\n\n## Requirements\n\nThe package currently requires only [`numpy`](https://github.com/numpy/numpy) and [`pyspark`](https://github.com/apache/spark/tree/master/python/pyspark).\n\n## Installation\n```\n$ pip install pyspark-model-plus\n```\n## How to use\n\nHere is an example on how to use the function using the [iris data](https://archive.ics.uci.edu/ml/datasets/iris).\nLet us first try to split the data using scikit learn's train test split functionality\n\n```py\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfull_iris = pd.read_csv(\"iris.csv\")\ntrain,test = train_test_split(full_iris,stratify = full_iris[\"Species\"],test_size = .2)\ntrain.append(train[train[\"Species\"] == \"setosa\"]).\\\n      append(train[train[\"Species\"] == \"setosa\"]).to_csv(\"iris_train.csv\", index = False)\ntest.to_csv(\"iris_test.csv\", index = False)\n```\n\n**Importing Packages**\n\n```py\nfrom pyspark_model_plus.evaluation import MulticlassLogLossEvaluator\nfrom pyspark_model_plus.training import StratifiedCrossValidator\nfrom pyspark_model_plus.transform import CustomMeanImputer\n```\n\n\n**Importing to pyspark**\n\n```py\ndf_train = spark.read.csv(\"iris_train.csv\", inferSchema=True, header=True)\ndf_test = spark.read.csv(\"iris_test.csv\", inferSchema=True, header=True)\n```\n\n**Creating pipeline to prepare training data**\n\n```py\nstages = []\nindexer = StringIndexer(inputCol=\"Species\", outputCol=\"labelIndex\")\nstages += [indexer]\nimputer = CustomMeanImputer(cols_to_impute = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])\nstages += [imputer]\nassembler = VectorAssembler(\n    inputCols=[\"Sepal_Length\", \"Sepal_Width\", \"Petal_Length\", \"Petal_Width\"],\n    outputCol=\"features\")\nstages += [assembler]\n\npipeline = Pipeline(stages=stages)\npipelineData = pipeline.fit(df_train)\ntraining_data = pipelineData.transform(df_train)\n\n```\n\n**Training RandomForest on unbalanced data using Stratified Crossvalidation**\n\n```py\nmodel = RandomForestClassifier(labelCol=\"labelIndex\",\n                               featuresCol=\"features\",\n                               probabilityCol=\"probability\",\n                               predictionCol=\"prediction\")\nparamGrid = (ParamGridBuilder().addGrid(model.numTrees, [250, 300]).build())\ncv = StratifiedCrossValidator(\n    labelCol = \"labelIndex\",\n    estimator=model,\n    estimatorParamMaps=paramGrid,\n    evaluator=MulticlassLogLossEvaluator(labelCol=\"labelIndex\"),\n    numFolds=3,\n    stratify_summary = True\n)\n\n# stratifiedCV = StratifiedCrossValidator(cv)\ncvModel = cv.fit(training_data)\n```\nAs the training progresses you will see the progress being printed\n\n```py\nInitiating Training for fold 1\nInitiating Training for fold 2\nInitiating Training for fold 3\nOut[19]: [0.06820200113875617, 0.06960025759185091]\n```\n\nAdditionally if you want to see how the startified crossvalidator is splitting the data for each fold, you can run with the `stratify_summary=True` and see the report as below\n\n```py\n+----------+------+------+------+\n|labelIndex|fold_1|fold_2|fold_3|\n+----------+------+------+------+\n|       0.0|    40|    40|    40|\n|       1.0|    13|    14|    13|\n|       2.0|    13|    14|    13|\n+----------+------+------+------+\n```\n# Evaluate and compare with scikit learn\n\n### With MulticlassLogLossEvaluator\n\n```py\ntest_data = pipelineData.transform(df_test)\npredictions = cvModel.transform(test_data)\nevaluator = MulticlassLogLossEvaluator(labelCol=\"labelIndex\")\naccuracy = evaluator.evaluate(predictions)\naccuracy\n\nOut[23]: 0.07676493894621013\n```\n\n### With scikit-learn\n\n```py\nfrom sklearn.metrics import log_loss\npredictions_pandas = predictions.toPandas()\nlog_loss(predictions_pandas[\"labelIndex\"].tolist(),predictions_pandas[\"probability\"].tolist())\n\nOut[24]: 0.0767649389462101\n```\n\n# Contributing\n\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](https://github.com/RajarshiBhadra/pyspark-model-plus/issues)\n\nIf you want to write some code and contribute to this project, go ahead and start a pull request. I hope this tool is useful for the community and I would love to hear about how this helps solve your problems!\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://https://github.com/RajarshiBhadra/pyspark-model-plus",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pyspark-model-plus",
    "package_url": "https://pypi.org/project/pyspark-model-plus/",
    "platform": "",
    "project_url": "https://pypi.org/project/pyspark-model-plus/",
    "project_urls": {
      "Homepage": "https://https://github.com/RajarshiBhadra/pyspark-model-plus"
    },
    "release_url": "https://pypi.org/project/pyspark-model-plus/1.0.2/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Enhancements to commonly used pyspark functions for building models",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9215811,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "46e12e53bd21cef65a47a7e211e1d55a9e78780a365b192f79d79cbbc3cacd44",
          "md5": "48980b8233a072ffce9555da94c7156a",
          "sha256": "ec9b96f4892ec818d0238e9b2f9ba16690606d533feda308e784fd4d488921fb"
        },
        "downloads": -1,
        "filename": "pyspark_model_plus-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "48980b8233a072ffce9555da94c7156a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 11088,
        "upload_time": "2021-01-24T08:27:04",
        "upload_time_iso_8601": "2021-01-24T08:27:04.402962Z",
        "url": "https://files.pythonhosted.org/packages/46/e1/2e53bd21cef65a47a7e211e1d55a9e78780a365b192f79d79cbbc3cacd44/pyspark_model_plus-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3bbdaef819a465a964e3a19efa7c7c9078a7bf05eacca8584028a81bbf27a2cf",
          "md5": "0cfd510132aee2fad21e158b34c6db72",
          "sha256": "9283c2f2fce8927ba3059e76a415c97920a70c1173215cb8d3b5ef30b1720386"
        },
        "downloads": -1,
        "filename": "pyspark-model-plus-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0cfd510132aee2fad21e158b34c6db72",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 6212,
        "upload_time": "2021-01-24T08:27:05",
        "upload_time_iso_8601": "2021-01-24T08:27:05.527926Z",
        "url": "https://files.pythonhosted.org/packages/3b/bd/aef819a465a964e3a19efa7c7c9078a7bf05eacca8584028a81bbf27a2cf/pyspark-model-plus-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2e1e69c065ba7e6ce0b936d4574333c4cb11754942195c2f46cf011144a3f0d0",
          "md5": "af839ac234a9ae7874584433e2b990cf",
          "sha256": "25ef70534dcb8d740f204518803c3ccae9827211a8bcf4386c4b848b31a43c16"
        },
        "downloads": -1,
        "filename": "pyspark_model_plus-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "af839ac234a9ae7874584433e2b990cf",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 11060,
        "upload_time": "2021-01-24T13:20:42",
        "upload_time_iso_8601": "2021-01-24T13:20:42.454803Z",
        "url": "https://files.pythonhosted.org/packages/2e/1e/69c065ba7e6ce0b936d4574333c4cb11754942195c2f46cf011144a3f0d0/pyspark_model_plus-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bac3afa5f81f90c7c292e2a0bbbd1fb8fc3feb1f3f2c9e212271a41cbb115c48",
          "md5": "d2ed3eb49562e095cb6a4e89d32f431f",
          "sha256": "ca4a4f47981a58bee85e5d232e52ea3000ad7a84d24a2dfb712a010cff059964"
        },
        "downloads": -1,
        "filename": "pyspark-model-plus-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "d2ed3eb49562e095cb6a4e89d32f431f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 6181,
        "upload_time": "2021-01-24T13:20:43",
        "upload_time_iso_8601": "2021-01-24T13:20:43.382281Z",
        "url": "https://files.pythonhosted.org/packages/ba/c3/afa5f81f90c7c292e2a0bbbd1fb8fc3feb1f3f2c9e212271a41cbb115c48/pyspark-model-plus-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2e1e69c065ba7e6ce0b936d4574333c4cb11754942195c2f46cf011144a3f0d0",
        "md5": "af839ac234a9ae7874584433e2b990cf",
        "sha256": "25ef70534dcb8d740f204518803c3ccae9827211a8bcf4386c4b848b31a43c16"
      },
      "downloads": -1,
      "filename": "pyspark_model_plus-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "af839ac234a9ae7874584433e2b990cf",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 11060,
      "upload_time": "2021-01-24T13:20:42",
      "upload_time_iso_8601": "2021-01-24T13:20:42.454803Z",
      "url": "https://files.pythonhosted.org/packages/2e/1e/69c065ba7e6ce0b936d4574333c4cb11754942195c2f46cf011144a3f0d0/pyspark_model_plus-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bac3afa5f81f90c7c292e2a0bbbd1fb8fc3feb1f3f2c9e212271a41cbb115c48",
        "md5": "d2ed3eb49562e095cb6a4e89d32f431f",
        "sha256": "ca4a4f47981a58bee85e5d232e52ea3000ad7a84d24a2dfb712a010cff059964"
      },
      "downloads": -1,
      "filename": "pyspark-model-plus-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "d2ed3eb49562e095cb6a4e89d32f431f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 6181,
      "upload_time": "2021-01-24T13:20:43",
      "upload_time_iso_8601": "2021-01-24T13:20:43.382281Z",
      "url": "https://files.pythonhosted.org/packages/ba/c3/afa5f81f90c7c292e2a0bbbd1fb8fc3feb1f3f2c9e212271a41cbb115c48/pyspark-model-plus-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}