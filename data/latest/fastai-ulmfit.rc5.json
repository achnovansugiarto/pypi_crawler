{
  "info": {
    "author": "Florian Leuerer",
    "author_email": "florian@meansqua.red",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# fast.ai ULMFiT with SentencePiece from pretraining to deployment\n\n\n\n**Motivation:**\nWhy even bother with a non-BERT / Transformer language model? Short answer: you can train a state of the art text classifier with ULMFiT with limited data and affordable hardware. The whole process (preparing the Wikipedia dump, pretrain the language model, fine tune the language model and training the classifier) takes about 5 hours on my workstation with a RTX 3090. The training of the model with FP16 requires less than 8 GB VRAM - so you can train the model on affordable GPUs.\n\nI also saw this paper on the roadmap for fast.ai 2.3 [Single Headed Attention RNN: Stop Thinking With Your Head](https://arxiv.org/abs/1911.11423) which could improve the performance further. \n\nThis Repo is based on: \n- https://github.com/fastai/fastai\n- [ULMFiT Paper](https://arxiv.org/abs/1801.06146)\n- the fast.ai NLP-course repository: https://github.com/fastai/course-nlp\n\n## Pretrained models\n\n| Language | (local) | code  | Perplexity | Vocab Size | Tokenizer | Download (.tgz files) |\n|---|---|---|---|---|---|---|\n| German | Deutsch  | de  | 16.1 | 15k | SP | https://bit.ly/ulmfit-dewiki |\n| Dutch | Nederlands | nl  | 20.5  | 15k | SP | https://bit.ly/ulmfit-nlwiki |\n| Russian | Русский | ru  | 29.8  | 15k | SP | https://bit.ly/ulmfit-ruwiki |\n| Portuguese | Português | pt  | 17.3 | 15k | SP | https://bit.ly/ulmfit-ptwiki |\n| Vietnamese | Tiếng Việt | vi  | 18.8 | 15k | SP | https://bit.ly/ulmfit-viwiki |\n| Japanese | 日本語 | ja  | 42.6 | 15k | SP | https://bit.ly/ulmfit-jawiki |\n| Italian | Italiano | it  | 23.7 | 15k | SP |https://bit.ly/ulmfit-itwiki |\n| Spanish | Español | es  | 21.9 | 15k | SP |https://bit.ly/ulmfit-eswiki |\n| Korean | 한국어 | ko  | 39.6 | 15k | SP |https://bit.ly/ulmfit-kowiki |\n| Thai | ไทย | th  | 56.4 | 15k | SP |https://bit.ly/ulmfit-thwiki |\n| Hebrew | עברית | he  | 46.3 | 15k | SP |https://bit.ly/ulmfit-hewiki |\n| Arabic | العربية | ar  | 50.0 | 15k | SP |https://bit.ly/ulmfit-arwiki |\n| Mongolian | Монгол | mn | | | | see: [Github: RobertRitz](https://github.com/robertritz/NLP/tree/main/02_mongolian_language_model) |\n\n\n**Download with wget**\n````\n# to preserve the filenames (.tgz!) when downloading with wget use --content-disposition\nwget --content-disposition https://bit.ly/ulmfit-dewiki \n````\n\n### Usage of pretrained models - library fastai_ulmfit.pretrained\n\nI've written a small library around this repo, to easily use the pretrained models. You don't have to bother with model, vocab and tokenizer files and paths - the following functions will take care of that. \n\nTutorial:  [fastai_ulmfit_pretrained_usage.ipynb](https://github.com/floleuerer/fastai_ulmfit/blob/main/fastai_ulmfit_pretrained_usage.ipynb) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/floleuerer/fastai_ulmfit/blob/main/fastai_ulmfit_pretrained_usage.ipynb)\n\n\n**Installation**\n````\npip install fastai-ulmfit\n````\n\n**Usage**\n\n```\n# import\nfrom fastai_ulmfit.pretrained import *\n\nurl = 'http://bit.ly/ulmfit-dewiki'\n\n# get tokenizer - if pretrained=True, the SentencePiece Model used for language model pretraining will be used. Default: False \ntok = tokenizer_from_pretrained(url, pretrained=False)\n\n# get language model learner for fine-tuning\nlearn = language_model_from_pretrained(dls, url=url, drop_mult=0.5).to_fp16()\n\n# save fine-tuned model for classification\npath = learn.save_lm('tmp/test_lm')\n\n# get text classifier learner from fine-tuned model\nlearn = text_classifier_from_lm(dls, path=path, metrics=[accuracy]).to_fp16()\n````\n\n### Extract Sentence Embeddings\n\n```\nfrom fastai_ulmfit.embeddings import SentenceEmbeddingCallback\n\nse = SentenceEmbeddingCallback(pool_mode='concat')\n_ = learn.get_preds(cbs=[se])\n\nfeat = se.feat\npca = PCA(n_components=2)\npca.fit(feat['vec'])\ncoords = pca.transform(feat['vec'])\n```\n\n## Model pretraining \n\n### Setup \n\n#### Python environment\n\n```\nfastai-2.2.7\nfastcore-1.3.19\nsentencepiece-0.1.95\nfastinference-0.0.36\n```\n\n**Install packages**\n`pip install -r requirements.txt`\n\nThe trained language models are compatible with other fastai versions!\n\n#### Docker\n\nThe Wikipedia-dump preprocessing requires docker https://docs.docker.com/get-docker/.\n\n### Project structure\n\n````\n.\n├── we                         Docker image for the preperation of the Wikipedia-dump / wikiextractor\n└── data          \n    └── {language-code}wiki         \n        ├── dump                    downloaded Wikipedia dump\n        │   └── extract             extracted wikipedia-articles using wikiextractor\n        ├── docs \n        │   ├── all                 all extracted Wikipedia articles as single txt-files\n        │   ├── sampled             sampled Wikipedia articles for language model pretraining\n        │   └── sampled_tok         cached tokenized sampled articles - created by fastai / sentencepiece\n        └── model \n            ├── lm                  language model trained in step 2\n            │   ├── fwd             forward model\n            │   ├── bwd             backwards model\n            │   └── spm             SentencePiece model\n            │\n            ├── ft                  fine tuned model trained in step 3\n            │   ├── fwd             forward model\n            │   ├── bwd             backwards model\n            │   └── spm             SentencePiece model\n            │\n            └── class               classifier trained in step 4\n                ├── fwd             forward learner\n                └── bwd             backwards learner\n````\n\n### 1. Prepare Wikipedia-dump for pretraining\n\nULMFiT can be peretrained on relativly small datasets - 100 million tokens are sufficient to get state-of-the art classification results (compared to Transformer models as BERT, which need huge amounts of training data). The easiest way is to pretrain a language model on Wikipedia.\n\nThe code for the preperation steps is heavily inspired by / copied from the **fast.ai NLP-course**: https://github.com/fastai/course-nlp/blob/master/nlputils.py\n\nI built a docker container and script, that automates the following steps:\n1) Download Wikipedia XML-dump\n2) Extract the text from the dump\n3) Sample 160.000 documents with a minimum length of 1800 characters (results in 100m-120m tokens) both parameters can be changed - see the usage below\n\nThe whole process will take some time depending on the download speed and your hardware. For the 'dewiki' the preperation took about 45 min.\n\nRun the following commands in the current directory\n```\n# build the wikiextractor docker file\ndocker build -t wikiextractor ./we\n\n# run the docker container for a specific language\n# docker run -v $(pwd)/data:/data -it wikiextractor -l <language-code> \n# for German language-code de run:\ndocker run -v $(pwd)/data:/data -it wikiextractor -l de\n...\nsucessfully prepared dewiki - /data/dewiki/docs/sampled, number of docs 160000/160000 with 110699119 words / tokens!\n\n# To change the number of sampled documents or the minimum length see\nusage: preprocess.py [-h] -l LANG [-n NUMBER_DOCS] [-m MIN_DOC_LENGTH] [--mirror MIRROR] [--cleanup]\n\n# To cleanup indermediate files (wikiextractor and all splitted documents) run the following command. \n# The Wikipedia-XML-Dump and the sampled docs will not be deleted!\ndocker run -v $(pwd)/data:/data -it wikiextractor -l <language-code> --cleanup\n```\n\n### 2. Language model pretraining on Wikipedia Dump\n\nNotebook: `2_ulmfit_lm_pretraining.ipynb`\n\nTo get the best result, you can train two seperate language models - a forward and a backward model. You'll have to run the complete notebook twice and set the `backwards` parameter accordingly. The models will be saved in seperate folders (fwd / bwd). The same applies to fine-tuning and training of the classifier.\n\n#### Parameters \n\nChange the following parameters according to your needs:\n```\nlang = 'de' # language of the Wikipedia-Dump\nbackwards = False # Train backwards model? Default: False for forward model\nbs=128 # batch size\nvocab_sz = 15000 # vocab size - 15k / 30k work fine with sentence piece\nnum_workers=18 # num_workers for the dataloaders\nstep = 'lm' # language model - don't change\n```\n#### Training Logs + config\n\n`model.json` contains the **parameters** the language model was trained with and the **statistics** (looses and metrics) of the last epoch \n```json\n{\n    \"lang\": \"de\",\n    \"step\": \"lm\",\n    \"backwards\": false,\n    \"batch_size\": 128,\n    \"vocab_size\": 15000,\n    \"lr\": 0.01,\n    \"num_epochs\": 10,\n    \"drop_mult\": 0.5,\n    \"stats\": {\n        \"train_loss\": 2.894167184829712,\n        \"valid_loss\": 2.7784812450408936,\n        \"accuracy\": 0.46221256256103516,\n        \"perplexity\": 16.094558715820312\n    }\n}\n```\n\n`history.csv` log of the training metrics (epochs, losses, accuracy, perplexity)\n```\nepoch,train_loss,valid_loss,accuracy,perplexity,time\n0,3.375441551208496,3.369227886199951,0.3934227228164673,29.05608367919922,23:00\n...\n9,2.894167184829712,2.7784812450408936,0.46221256256103516,16.094558715820312,22:44\n```\n\n### 3. Language model fine-tuning on unlabled data\n\nNotebook: `3_ulmfit_lm_finetuning.ipynb`\n\nTo improve the performance on the downstream-task, the language model should be fine-tuned. We are using a Twitter dataset (GermEval2018/2019), so we fine-tune the LM on unlabled tweets.\n\nTo use the notebook on your own dataset, create a `.csv`-file containing your (unlabled) data in the `text` column.\n\nFiles required from the Language Model (previous step):\n- Model (*model.pth)\n- Vocab (*vocab.pkl)\n\nI am not reusing the SentencePiece-Model from the language model! This could lead to slightly different tokenization but fast.ai (-> language_model_learner()) and the fine-tuning takes care of adding and training unknown tokens! This approch gave slightly better results than reusing the SP-Model from the language model.\n\n\n\n### 4. Train the classifier\n\nNotebook: `4_ulmfit_train_classifier.ipynb`\n\nThe (fine-tuned) language model now can be used to train a classifier on a (small) labled dataset.\n\nTo use the notebook on your own dataset, create a `.csv`-file containing your texts in the `text` and labels in the `label` column.\n\nFiles required from the fine-tuned LM (previous step):\n- Encoder (*encoder.pth)\n- Vocab (*vocab.pkl)\n- SentencePiece-Model (spm/spm.model)\n\n### 5. Use the classifier for predictions / inference on new data\n\nNotebook: `5_ulmfit_inference.ipynb`\n\n## Evaluation\n\n### German pretrained model\nResults with an ensemble of forward + backward model (see the inference notebook). Neither the fine-tuning of the LM, nor the training of the classifier was optimized - so there is still room for improvement.\n\nOfficial results: https://ids-pub.bsz-bw.de/frontdoor/deliver/index/docId/9319/file/Struss_etal._Overview_of_GermEval_task_2_2019.pdf\n\n#### Task 1 Coarse Classification \n\nClasses: OTHER, OFFENSE\n\nAccuracy: 79,68 \nF1: 75,96 (best BERT 76,95)\n\n#### Task 2 Fine Classification \n\nClasses: OTHER, PROFANITY, INSULT, ABUSE\n\nAccuracy: 74,56 %\nF1: 52,54 (best BERT 53.59)\n\n### Dutch model\n\nCompared result with: https://arxiv.org/pdf/1912.09582.pdf  \nDataset https://github.com/benjaminvdb/DBRD\n\nAccuracy 93,97 % (best BERT 93,0 %)\t\n\n### Japanese model\nCopared results with: \n- https://github.com/laboroai/Laboro-BERT-Japanese\n- https://github.com/yoheikikuta/bert-japanese  \n\nLivedoor news corpus   \nAccuracy 97,1% (best BERT ~98 %)\n\n### Korean model\n\nCompared with: https://github.com/namdori61/BERT-Korean-Classification\nDataset: https://github.com/e9t/nsmc\nAccuracy 89,6 % (best BERT 90,1 %)\n\n## Deployment as REST-API\n\nsee https://github.com/floleuerer/fastai-docker-deploy\n\n.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/floleuerer/fastai_ulmfit/tree/main/",
    "keywords": "fast.ai ulmfit NLP",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "fastai-ulmfit",
    "package_url": "https://pypi.org/project/fastai-ulmfit/",
    "platform": "",
    "project_url": "https://pypi.org/project/fastai-ulmfit/",
    "project_urls": {
      "Homepage": "https://github.com/floleuerer/fastai_ulmfit/tree/main/"
    },
    "release_url": "https://pypi.org/project/fastai-ulmfit/0.0.5/",
    "requires_dist": [
      "fastai (>=2.2.7)",
      "sentencepiece (>=0.1.95)"
    ],
    "requires_python": ">=3.6",
    "summary": "Easy to use pretrained-models for fast.ai ULMFiT",
    "version": "0.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11178482,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "65b13f56057a13bc1e0fe6a32f278094c1af63fbee6ba4a7786054914b3f771c",
          "md5": "8c8d5a50fc7bcc174fcb5ca857630545",
          "sha256": "510e8ec0b236ca7b8a6757bc266a6a80cd45ae3ccddfacf7f18afc397e8126eb"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8c8d5a50fc7bcc174fcb5ca857630545",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 8237,
        "upload_time": "2021-03-23T22:41:57",
        "upload_time_iso_8601": "2021-03-23T22:41:57.847884Z",
        "url": "https://files.pythonhosted.org/packages/65/b1/3f56057a13bc1e0fe6a32f278094c1af63fbee6ba4a7786054914b3f771c/fastai_ulmfit-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9466f44ef1b94aaaca33b9f542045b3bbc5bab33064f344096dfdd5ca8691399",
          "md5": "b480f4672717abe3697e35e3d97855e6",
          "sha256": "786396a90ce3522d3b60219c200f5cf20edd954c39cbd233f539e0cd52755d15"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b480f4672717abe3697e35e3d97855e6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 10997,
        "upload_time": "2021-03-23T22:41:59",
        "upload_time_iso_8601": "2021-03-23T22:41:59.139440Z",
        "url": "https://files.pythonhosted.org/packages/94/66/f44ef1b94aaaca33b9f542045b3bbc5bab33064f344096dfdd5ca8691399/fastai_ulmfit-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0b7524cc0e916966a0da2f90f7e420bbc7a61b99a07beb6d3eb73bda03f392b9",
          "md5": "d184d4ef1dbc403070715363c002b244",
          "sha256": "b5dd47e1ea68c86329d7ba0da90741f106d1359e0f09917c86837fb33dbdbe30"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d184d4ef1dbc403070715363c002b244",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 8440,
        "upload_time": "2021-03-23T22:49:01",
        "upload_time_iso_8601": "2021-03-23T22:49:01.834998Z",
        "url": "https://files.pythonhosted.org/packages/0b/75/24cc0e916966a0da2f90f7e420bbc7a61b99a07beb6d3eb73bda03f392b9/fastai_ulmfit-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08bc0e0a83732bbc14178412d2fe07e5365340453d3175587bde5195e61e6de0",
          "md5": "1c85929b7060dc243dc7818a42ef9509",
          "sha256": "233aad4528f99dfeaafa9397ed9a736eb732ac539ee40fc531bbc5f30cfd42ec"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1c85929b7060dc243dc7818a42ef9509",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 10566,
        "upload_time": "2021-03-23T22:49:02",
        "upload_time_iso_8601": "2021-03-23T22:49:02.794772Z",
        "url": "https://files.pythonhosted.org/packages/08/bc/0e0a83732bbc14178412d2fe07e5365340453d3175587bde5195e61e6de0/fastai_ulmfit-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bf1ff602e16e20f53791379bc62b502c3643f75d47655dae9d677fc74bb3dd09",
          "md5": "87455e2c9034f730e7d0b41cf76ae9d1",
          "sha256": "70d1ee3746762aa8af3bce1da5d51516c1c65f11a54c719e8e9a43d81e4d7adc"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "87455e2c9034f730e7d0b41cf76ae9d1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 8833,
        "upload_time": "2021-03-26T19:43:56",
        "upload_time_iso_8601": "2021-03-26T19:43:56.146228Z",
        "url": "https://files.pythonhosted.org/packages/bf/1f/f602e16e20f53791379bc62b502c3643f75d47655dae9d677fc74bb3dd09/fastai_ulmfit-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c1b37e59df8d9f6720f695e2e774e5c1797b81d53521b40e1d740ff359489757",
          "md5": "f595083295d663028d469787dd2e1f82",
          "sha256": "0359349934e12c09defce2f23e036b0a3c0693fae855a76ad582855337f5627d"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "f595083295d663028d469787dd2e1f82",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 11157,
        "upload_time": "2021-03-26T19:43:57",
        "upload_time_iso_8601": "2021-03-26T19:43:57.125843Z",
        "url": "https://files.pythonhosted.org/packages/c1/b3/7e59df8d9f6720f695e2e774e5c1797b81d53521b40e1d740ff359489757/fastai_ulmfit-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "73e45389c4d53a59356f5f2ac503138fe7572a310d154eacad091b5a761b79b3",
          "md5": "43cff0a10f37a15d06fb8bf16ae3094c",
          "sha256": "6ec9b9528f7c26cfc0cd067936288f42abb8d3f3bc911e4e891ea63eafa4df33"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "43cff0a10f37a15d06fb8bf16ae3094c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9043,
        "upload_time": "2021-03-27T22:24:40",
        "upload_time_iso_8601": "2021-03-27T22:24:40.036657Z",
        "url": "https://files.pythonhosted.org/packages/73/e4/5389c4d53a59356f5f2ac503138fe7572a310d154eacad091b5a761b79b3/fastai_ulmfit-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fc50a3243f94e56464f5c967f9dadd5ed54d188c6a297638ccf3875009d80d63",
          "md5": "e097ee860b63e855d65ce40635914f28",
          "sha256": "f351b71440b3e4464360ad9fc6ada1feb0c8b2d675fa1e069aa98a35e2c3255f"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "e097ee860b63e855d65ce40635914f28",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 11395,
        "upload_time": "2021-03-27T22:24:41",
        "upload_time_iso_8601": "2021-03-27T22:24:41.267728Z",
        "url": "https://files.pythonhosted.org/packages/fc/50/a3243f94e56464f5c967f9dadd5ed54d188c6a297638ccf3875009d80d63/fastai_ulmfit-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "538af636fb805e66ae15b6e755d9700d43a8c962f6034d746052701901fc2fce",
          "md5": "379fe5cb9bf5db9494e6bd4323974957",
          "sha256": "d9429159313c740047c4240dc405ed39d8ae5e81d173e0424587e6049cc15a2c"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "379fe5cb9bf5db9494e6bd4323974957",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 10620,
        "upload_time": "2021-08-14T14:39:41",
        "upload_time_iso_8601": "2021-08-14T14:39:41.022624Z",
        "url": "https://files.pythonhosted.org/packages/53/8a/f636fb805e66ae15b6e755d9700d43a8c962f6034d746052701901fc2fce/fastai_ulmfit-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "76ddeba759ab9951b46d72e2cdd31f01be9787e3bb9e15c717c895e7bab89de6",
          "md5": "1329053c3bfb0e1b7e1ecd8641fa9405",
          "sha256": "7e151194a6feeab2388c3d28152358e834a0bfea00d6e8bb70ddcd2e0b3d1348"
        },
        "downloads": -1,
        "filename": "fastai_ulmfit-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "1329053c3bfb0e1b7e1ecd8641fa9405",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 16105,
        "upload_time": "2021-08-14T14:39:42",
        "upload_time_iso_8601": "2021-08-14T14:39:42.136131Z",
        "url": "https://files.pythonhosted.org/packages/76/dd/eba759ab9951b46d72e2cdd31f01be9787e3bb9e15c717c895e7bab89de6/fastai_ulmfit-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "538af636fb805e66ae15b6e755d9700d43a8c962f6034d746052701901fc2fce",
        "md5": "379fe5cb9bf5db9494e6bd4323974957",
        "sha256": "d9429159313c740047c4240dc405ed39d8ae5e81d173e0424587e6049cc15a2c"
      },
      "downloads": -1,
      "filename": "fastai_ulmfit-0.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "379fe5cb9bf5db9494e6bd4323974957",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 10620,
      "upload_time": "2021-08-14T14:39:41",
      "upload_time_iso_8601": "2021-08-14T14:39:41.022624Z",
      "url": "https://files.pythonhosted.org/packages/53/8a/f636fb805e66ae15b6e755d9700d43a8c962f6034d746052701901fc2fce/fastai_ulmfit-0.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "76ddeba759ab9951b46d72e2cdd31f01be9787e3bb9e15c717c895e7bab89de6",
        "md5": "1329053c3bfb0e1b7e1ecd8641fa9405",
        "sha256": "7e151194a6feeab2388c3d28152358e834a0bfea00d6e8bb70ddcd2e0b3d1348"
      },
      "downloads": -1,
      "filename": "fastai_ulmfit-0.0.5.tar.gz",
      "has_sig": false,
      "md5_digest": "1329053c3bfb0e1b7e1ecd8641fa9405",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 16105,
      "upload_time": "2021-08-14T14:39:42",
      "upload_time_iso_8601": "2021-08-14T14:39:42.136131Z",
      "url": "https://files.pythonhosted.org/packages/76/dd/eba759ab9951b46d72e2cdd31f01be9787e3bb9e15c717c895e7bab89de6/fastai_ulmfit-0.0.5.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}