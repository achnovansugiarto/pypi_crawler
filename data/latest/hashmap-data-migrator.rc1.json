{
  "info": {
    "author": "Hashmap, Inc",
    "author_email": "accelerators@hashmapinc.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<!---\nCopyright Â© 2020 Hashmap, Inc\n\nLicensed under the Apache License, Version 2.0 the \\(\"License\"\\);\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n--->\n# Hashmap Data Migrator\n\nTable of Contents\n\n* [About](#about)\n* [Using hdm](#using-hashmap-data-migrator)\n* [State Management](#state-management)\n* [Pipeline YAML](#pipeline-yaml)\n* [Connection Profile YAML](#connection-profile-yaml)\n* [Logging](#logging)\n* [User Documentation](#user-documentation)\n    * [Sources](#sources)\n        * [NetezzaSource](#netezzasource)\n        * [NetezzaExternalTableSource](#netezzaexternaltablesource)\n        * [FSSource](#fssource)\n        * [FSChunkSource](#fschunksource)\n        * [AzureBlobSource](#azureblobsource)\n        * [DummySource](#dummysource) \n    * [Sinks](#sinks)\n        * [FSSink](#fssink)\n        * [AzureBlobSink](#azureblobsink)\n        * [SnowflakeAzureCopySink](#snowflakeazurecopysink)\n        * [DummySink](#dummysink)\n   * [StateManagement](#state-management-1)\n        * [AzureSQLServerStateManager](#azuresqlserverstatemanager)\n        * [SQLServerStateManager](#sqlserverstatemanager)\n        * [MySQLStateManager](#mysqlstatemanager)\n        * [SqLiteStateManager](#sqlitestatemanager)\n        * [StateManager- Base class](#statemanager)\n   * [Data Access Object (DAO)](#data-access-object)\n        * [NetezzaJDBC](#netezzajdbc)\n        * [NetezzaODBC](#netezzaodbc)\n        * [SnowflakeAzureCopy](#snowflakeazurecopy)\n        * [MySQL](#mysql)\n        * [SqLiteState](#sqlitestate)\n        * [SQLServer](#sqlserver)\n        * [AzureSQLServer](#azuresqlserver)\n   * [Catalog](#catalog)\n        * [NetezzaCrawler](#netezzacrawler)\n        * [NetezzaToSnowflakeMapper](#netezzatosnowflakemapper)\n        * [SnowflakeDDLWriter](#ddlwriter)\n   * [Orchestrator](#orchestrator)\n        * [DeclaredOrchestrator](#declaredorchestrator)\n        * [BatchOrchestrator](#batchorchestrator)\n        * [AutoOrchestrator](#autoorchestrator)\n    * [Utils](#utils)\n        * [Project Configuration](#project-configuration)\n* [Repository Cloning](#repository-cloning)\n* [Miscellaneous](#miscellaneous)\n\n## About\n\nHashmap Data Migrator, or hdm, is a collection of composable data transport modules designed to incrementally move data from sources systems to cloud data systems.\n\nData is moved through pairs of sources and sinks. Ideally, this movement is meant to use a dataset as an intermediary. This allows one to create pipelines that follow modern practices while at the same time solving many issues that may arise if a pipeline is kept in memory in its entirety. While there is an additional IO overhead in many cases, this also allows workloads to be distributed arbitrarily and for portions of the pipelines to be ran on disparate systems - sharing only a state management data store (when full lineage is desired). In fact, in many situations, especially those of interest, data necessarily must be stored in files, broken into smaller chunks and transported across the wire to a cloud provider from an on-premises system.\n\n## Using Hashmap Data Migrator\n\nTo use the Hashmap Data Migrator (hdm) you must first\n\n1. Identify all locations where you would like for your solution to be deployed. It is a distributed application and can be partially deployed in many locations.\n1. If it does not already exist in the deployment environment, create a hidden directory in the 'user' root called .hashmap_data_migrator.\n1. Within the directory created in step 2 above, you must create a [connection profile YAML](Connection Profile YAML). This will hold the necessary connection information to connect Netezza, BigQuery and other data sources. Out of the box, at this time, there is no key management solution integrated. This is on the feature roadmap.\n1. Install hashmap_data_migrator and all of its dependencies. This is a pypi package and can be installed as\n```bash\npip install hashmap-data-migrator\n```\n1. Have a database available to use for state management.\n\nPipelines are defined declaratively in YAML files. These YAML files identify\n\n* The orchestrator (internal hdm concept) used to orchestrate the execution of your pipeline. The options are:\n\n  * declared_orchestrator - for manual or fully specified execution\n  * batch_orchestrator - for when orchestration is defined in a fully specified batch\n  * auto_batch_orchestrator - for when the execution is across all tables in specified combinations of databases and schemas\n\nIt is formatted in the YAML as such:\n```yaml\norchestrator:\n  name: Manual Orchestration\n  type: declared_orchestrator\n  conf: null\n```\n\nNext, and this should be consistent across all of the pipelines, the State Manager is specified. This is the glue the couples the otherwise independent portions of the pipeline together.\n\nIt is formatted in the YAML as such:\n```yaml\nstate_manager:\n  name: state_manager\n  type: SQLiteStateManager\n  conf:\n    connection: state_manager\n```\n\nNext is the portion of the YAML that specifies the different steps in the data movement. These will be specified in two separate sections:\n\n* declared_data_links - these are fully specified portions of a pipeline. In this each pair of source & sink is called a stage. See the example below that is targeted at offloading data from Netezza and storing it on a filesystem.\n\n```yaml\ndeclared_data_links:\n  stages:\n    - source:\n        name: Netezza Source Connection\n        type: NetezzaSource\n        conf:\n          ...\n      sink:\n        name: File System Sink Connection\n        type: FSSink\n        conf:\n          ...\n```\n* template_data_links - these are partially defined source/sink pairs. Instead of being called stages they are called templates. A template has an additional field called a batch_definition. A batch definition will define how the template source is used - source is ALWAYS the template. See an example below for creating a pipeline that is pulling multiple tables at once. A similar example would be found for auto batch orchestration.\n```yaml\ntemplate_data_links:\n  templates:\n    - batch_definition:\n        - source_name: netezza_source\n          field: table_name\n          values:\n            - database.schema.table_1\n            - database.schema.table_1\n          threads: 5\n      source:\n        name: netezza_source\n        type: NetezzaSource\n        conf:\n          ...\n          table_name: <<template>>\n      sink:\n        name: fs_chunk_stg\n        type: FSSink\n        conf:\n          ...\n```\n*__NOTE:__* *Any asset (source or sink) specified must either exist or be creatable through the connector. Any and all credentials must exist in the hdm_profiles.yml as well.*\n\n*__NOTE:__* *The pipeline can be split into separate files and executed distributively.*\n\nNow, before we move on to more detailed documentation there remains one last function - cataloging operations. Before you run your code, when you are migrating data from one database to another you must\n1. Catalog the existing assets\n2. Map the assets in the source system to the target system\n\nNow that the environment is specified, pipeline defined, and so on, all that remains is to run the code. The code is executed from bash (or at the terminal) through\n\n```\npython -m hashmap_data_migrator {manifest} -l {log settings} -e {env}\n\nor\n\nhashmap_data_migrator {manifest} -l {log settings} -e {env}\n```\n\nThe parameters are:\n\n* manifest - path of manifest to run\n* log_settings - log settings path , default value =\"log_settings.yml\"\n* env - environment to take connection information , default value =\"prod\"\n\n\n## State Management\n\nThe management of the state of data movement. Useful for handling failures, storing history (audit trails) and much more. State of all data transport is stored in a database - which database is up to you - there is an extensible API with many out of the box implementations. This state management data is used to manage the control flow of a pipeline from end-to-end across a distributed deployments.\n\nAs of now MySQL, SQLLite, SQL Server and Azure SQL Server database tables can be used for state management.\n\nThe state management table will track the following values:\n\n* state_id: unique identifier\n* run_id: unique identifier for a run\n* job_id: unique identifier for a stage (source and sink pair).\n* correlation_id_in: Correlation id linking to a preceding pipeline\n* correlation_id_out: Correlation id on persistence\n* action: Action performed - sourcing pre-pull | sourcing post-pull| sinking pre-pull | sinking post-pull\n* status: Status of the transformation - success | failure | in_progress\n* source_name: Name of source\n* source_type: Type of source\n* sink_name: Name of sink\n* sink_type: Type of sink\n* source_entity: Asset being transported\n* source_filter: Any filtering applied\n* sink_entity: Asset being dumped \n* sink_filter: Any filtering applied \n* first_record_pulled: First record pulled in the run.Relevant to database only.\n* last_record_pulled: Last record pulled in the run.Relevant to database only. \n* git_sha: Correlates code execution to the late\n* sourcing_start_time: When sourcing started\n* sourcing_end_time: When sourcing ended \n* sinking_start_time: When sinking started \n* sinking_end_time: When sourcing ended \n* updated_on: When this entry was last updated\n* row_count: Number of distinct rows extracted\n* created_on: When this entry was created\n* manifest_name: Name of the pipeline YAML file\n\n## Pipeline YAML\n\nWhat the user should be focused on until a front-end gets built. This is merely a configuration file,\ngiven parameters by the user it will flow these data points to the relevant classes and move the data from source to sink.\nExample YAML can be found under manifests folder.\n\n```yaml\nversion: 1\nstate_manager_type:\ndata_links:\n  type: builder\n  mode: manual\n  stages:\n    - source:\n        name: source_name_1\n        type: NetezzaSource\n        conf:\n      sink:\n        name: sink_name_1\n        type: FSSink\n        conf:\n    - source:\n        name: source_name_2\n        type: FSSource\n        conf:\n      sink:\n        name: sink_name_2\n        type: s3Sink\n        conf:\n    - source:\n        name: source_name_3\n        type: s3Source\n        conf:\n      sink:\n        name: sink_name_3\n        type: SnowflakeCopySink\n        conf:\n```\n## Connection Profile YAML\n\nThis files stores the connection information to the source, stage , sink, database for state management.\nIts stored in local FS and its path is set in environment variable \"HOME\".\nThe below yml file format is based on netezza to snowflake data transport using azure blob staging:\n```yaml\ndev:\n  netezza_jdbc:  * Note:Add this section if using JDBC driver\n    host: <host>\n    port: <port>\n    database: <database_name>\n    user: <user_name>\n    password: <password>\n    driver:\n      name: <driver_name>\n      path: <driver_path>\n  netezza_odbc:  * Note:Add this section if using ODBC driver\n    host: <host>\n    port: <port>\n    database: <database_name>\n    user: <user_name>\n    password: <password>\n    driver: <driver_name>\n  snowflake_admin_schema:\n    authenticator: snowflake\n    account: <account>\n    role: <role>\n    warehouse: <warehouse_name>\n    database: <database_name>\n    schema: <schema_name>\n    user: <user_name>\n    password: <password>\n  azure:\n    url: <blob_url>\n    azure_account_url: <blob_url starting with azure://...>\n    sas: <sas_key>\n    container_name: <blob_container_name>\n  state_manager:\n    host: <host>\n    port: <port>\n    database: <database_name>\n    user: <user_name>\n    password: <password>\n    driver: ODBC Driver 17 for SQL Server <*Note:only for azure sql server>\n```\n\n## Logging\nThe application logging is configurable in log_settings.yml. The log files are created at the root.\n```yaml\nversion: 1\nformatters:\n  simple:\n    format: '%(asctime)s - %(name)s - %(levelname)s  - %(message)s'\n  json:\n    format: '%(asctime)s %(name)s %(levelname)s %(message)s'\n    class: pythonjsonlogger.jsonlogger.JsonFormatter\nhandlers:\n  console:\n    class : logging.StreamHandler\n    formatter: simple\n    level   : INFO\n    stream  : ext://sys.stdout\n  file:\n    class : logging.handlers.RotatingFileHandler\n    formatter: json\n    level: <logging level>\n    filename: <log file name>\n    maxBytes: <maximum bytes per log file>\n    backupCount: <backup log file count>\nloggers:\n  hdm:\n    level: <logging level>\n    handlers: [file, console]\n    propagate: True\n  hdm.core.source.netezza_source:\n    level: <logging level>\n    handlers: [file, console]\n    propagate: True\n  hdm.core.source.fs_source:\n    level: <logging level>\n    handlers: [file, console]\n    propagate: <logging level>\n  hdm.core.source.snowflake_external_stage_source:\n    level: <logging level>\n    handlers: [file, console]\n    propagate: True\n```\n\n### User Documentation\n\n#### Sources\n\n##### NetezzaSource\nNetezza storage source.\n\n*base class*\n```\nRDBMSSource\n```\n*configuration*\n* env - section name in hdm profile yml file for connection information\n* table_name - table name\n* watermark \n    - column: watermark column name\n    - offset: offset value to compare to\n* checksum \n    - function: checksum function. supported checksum methods are: hash, hash4, hash8\n    - column: checksum column name\n```\n    - source:\n        name: netezza_source\n        type: NetezzaSource\n        conf:\n          env: netezza_jdbc\n          table_name: ADMIN.TEST1\n          watermark:\n              column: T1\n              offset: 2\n          checksum:\n              function:\n              column:\n```\n\n*consume API*\n\ninput:\n```\nenv: section name in hdm profile yml file for connection information | required\ntable_name: table name | required\nwatermark: watermark information\nchecksum: checksum information | default random\n```\n\noutput:\n```\ndata_frame: pandas.DataFrame\nsource_type: 'database'\nrecord_count: pandas.DataFrame shape\ntable_name: table name\n```\n##### NetezzaExternalTableSource\n\nNetezza External Table storage source.\n\n*base class*\n```\nSource\n```\n\n*configuration*\n* env - section name in hdm profile yml file for connection information\n* table_name - table name\n* directory - staging directory\n* watermark \n    - column: watermark column name\n    - offset: offset value to compare to\n* checksum \n    - function: checksum function. supported checksum methods are: hash, hash4, hash8\n    - column: checksum column name\n```\n    - source:\n        name: netezza_source\n        type: NetezzaExternalTableSource\n        conf:\n          env: netezza_jdbc\n          table_name: ADMIN.TEST1\n          watermark:\n              column: T1\n              offset: 2\n          checksum:\n              function:\n              column:\n          directory: $HDM_DATA_STAGING\n```\n\n*consume API*\n\ninput:\n```\nenv: section name in hdm profile yml file for connection information | required\ntable_name: table name | required\ndirectory: staging directory | required\nwatermark: watermark information\nchecksum: checksum information | default random\n```\n\noutput:\n```\ndata_frame: pandas.DataFrame | required\nsource_type: 'database'\nrecord_count: pandas.DataFrame shape | required\n```\n##### FSSource\n\nFile system storage source.\n\n*base class*\n```\nSource\n```\n\n*configuration*\n* directory - staging directory\n```\n    - source:\n        name: fs_stg\n        type: FSSource\n        conf:\n          directory: $HDM_DATA_STAGING\n```\n*consume API*\n\ninput:\n```\ndirectory: staging directory | required\n```\n\noutput:\n```\ndata_frame: pandas.DataFrame | required\nfile_name: file name\nrecord_count: pandas.DataFrame shape | required\ntable_name: extracted table name from blob file path\n```\n##### FSChunkSource\n\nFile system chunking storage source.\n\n*base class*\n```\nSource\n```\n\n*configuration*\n* directory - staging directory\n* chunk - file chunk size\n```\n    - source:\n        name: fs_chunk_stg\n        type: FSChunkSource\n        conf:\n          directory: $HDM_DATA_STAGING\n          chunk: 200\n```\n*consume API*\n\ninput:\n```\ndirectory: staging directory | required\nchunk: file chunk size\n```\n\noutput:\n```\ndata_frame: pandas.DataFrame | required\nfile_name: file name\nparent_file_name: file name of the large parent file | required\nrecord_count: pandas.DataFrame shape | required\ntable_name: extracted table name from file path\nsource_filter: source filter to query state management record for updates when the source_entity is same (the parent file name)| required\n```\n##### AzureBlobSource\n\nBLOB data storage source.\n\n*base class*\n```\nSource\n```\n\n*configuration*\n* env - section name in hdm profile yml file for connection information\n* container - blob container name\n```\n    - source:\n        name: azure_source\n        type: AzureBlobSource\n        conf:\n          env: azure\n          container: data\n```\n\n*consume API*\n\ninput:\n```\nenv: section name in hdm profile yml file for connection information | required\ncontainer: container name | required\nfile_format: file format | default csv\n```\noutput:\n```\ndata_frame: pandas.DataFrame | required\nfile_name: file name\nrecord_count: pandas.DataFrame shape | required\ntable_name: extracted table name from blob file path\n```\n\n##### DummySource\n\nDummy storage source. Use when a source is not needed.\n\n*base class*\n```\nSource\n```\n*configuration*\n* dummy: None\n```\n    - source:\n        name: source_name\n        type: DummySource\n        conf:\n          dummy: None\n```\n\n*consume API*\n\ninput:\n```\ndummy: placeholder | required\n```\n\noutput:\n```\nNone\n```\n#### Sinks\n##### FSSink\n\nFile system storage sink\n\nbase class:\n```\nSink\n```\n\n*configuration*\n* directory - staging directory\n```\n      sink:\n        name: sink_name\n        type: FSSink\n        conf:\n          directory: $HDM_DATA_STAGING\n```\n*consume API*\n\ninput:\n```\ndirectory: staging directory | required\n```\n\noutput:\n```\nrecord_count: pandas.DataFrame shape | required\n```\n##### AzureBlobSink\n\nAzure blob storage sink\n\n*base class*\n```\nSink\n```\n*configuration*\n* env - section name in hdm profile yml file for connection information\n* container - staging directory\n\n```\n      sink:\n        name: azure_sink\n        type: AzureBlobSink\n        conf:\n          env: azure\n          container: data\n```\n*consume API*\n\ninput:\n```\nenv: section name in hdm profile yml file for connection information | required\ncontainer: container name | required\n```\n\noutput:\n```\nrecord_count: pandas.DataFrame shape | required\n```\n##### SnowflakeAzureCopySink\n\nSnowflake Azure storage stage sink\n\n*base class*\n```\nSnowflakeCopySink\n```\n*configuration*\n* env - section name in hdm profile yml file for connection information\n* stage_name - staging directory\n* file_format - file format\n* stage_directory - azure blob container name\n\n```\n      sink:\n        name: sflk_copy_into_sink\n        type: SnowflakeAzureCopySink\n        conf:\n          stage_name: TMP_KNERRIR\n          file_format: csv\n```\n*consume API*\n\ninput:\n```\nenv: section name in hdm profile yml file for connection information | required\nstage_name: snowflake storage stage name| required\nfile_format: file format | required\nstage_directory: container name | required\n```\n\noutput:\n```\nrecord_count: pandas.DataFrame shape | required\n```\n##### DummySink\n\nDummy sink. Use when a sink is not needed.\n\n*base class*\n```\nSink\n```\n*configuration*\n* dummy: None\n```\n      sink:\n        name: sink_name\n        type: DummySink\n        conf:\n          dummy: None\n```\n*consume API*\n\ninput:\n```\ndummy: placeholder | required\n```\noutput:\n```\nNone\n```\n#### State Management\n\n##### AzureSQLServerStateManager\n\nAzure SQL Server State Management\n\n*base class*\n```\nStateManager\n```\n\n*configuration*\n* connection - state_manager\n\n```\nstate_manager:\n  name: state_manager\n  type: AzureSQLServerStateManager\n  conf:\n    connection: state_manager\n```\n*consume API*\n\ninput:\n```\nconnection: state_manager  | required\ndao: 'azuresqlserver'  | preset value in code | required\nformat_date: False   | preset value in code | required\n```\n\n##### SQLServerStateManager\n\nSQL Server State Management\n\n*base class*\n```\nStateManager\n```\n\n*configuration*\n```\nstate_manager:\n  name: state_manager\n  type: SQLServerStateManager\n  conf:\n    connection: state_manager\n```\n*consume API*\n\ninput:\n```\nconnection: state_manager  | required\ndao: 'azuresqlserver'  | preset value in code | required\nformat_date: False   | preset value in code | required\n```\n\n##### MySQLStateManager\n\nMYSQL State Management\n\n*base class*\n```\nStateManager\n```\n\n*configuration*\n* connection - state_manager\n\n```\nstate_manager:\n  name: state_manager\n  type: MySQLStateManager\n  conf:\n    connection: state_manager\n```\n*consume API*\n\ninput:\n```\nconnection: state_manager  | required\ndao: 'mysql'  | preset value in code | required\nformat_date: True   | preset value in code | required\n```\n\n##### SqLiteStateManager\n\nSQLLite State Management\n\n*base class*\n```\nStateManager\n```\n*configuration*\n* connection - state_manager\n\n```\nstate_manager:\n  name: state_manager\n  type: SqLiteStateManager\n  conf:\n    connection: state_manager\n```\n*consume API*\n\ninput:\n```\nconnection: state_manager  | required\ndao: 'sqlite'  | preset value in code | required\nformat_date: True   | preset value in code | required\n```\n\n##### StateManager\n State Management base class\n\nMethods:\n\n```\nname: insert_state\n      insert new state\nparams: source_entity, source_filter,action,state_id,\n       status, correlation_id_in, correlation_id_out,\n       sink_entity, sink_filter,sourcing_start_time, \n       sourcing_end_time, sinking_start_time, sinking_end_time,\n       record_count, first_record_pulled, last_record_pulled\n\nreturn value: dictionary of state_id,job_id,correlation_id_in,\n               correlation_id_out,source_entity,source_filter,\n               sourcing_start_time,sourcing_end_time,sinking_start_time,\n               sinking_end_time, first_record_pulled,last_record_pulled,\n               record_count,run_id,manifest_name\n```\n```\nname: update_state\n      update a state\nparams:source_entity, source_filter,action,state_id,\n           status, correlation_id_in, correlation_id_out,\n           sink_entity, sink_filter,sourcing_start_time, \n           sourcing_end_time, sinking_start_time, sinking_end_time,\n           record_count, first_record_pulled, last_record_pulled\nreturn value :dictionary of state_id,job_id,correlation_id_in,\n                   correlation_id_out,source_entity,source_filter,\n                   sourcing_start_time,sourcing_end_time,sinking_start_time,\n                   sinking_end_time, first_record_pulled,last_record_pulled,\n                   record_count,run_id,manifest_name\n```\n```\nname: get_current_state\n      get current state\nparams: job_id | required, entity, entity_filter\nreturn value : dictionary of state_id,job_id,correlation_id_in,\n                   correlation_id_out,source_entity,source_filter,\n                   sourcing_start_time,sourcing_end_time,sinking_start_time,\n                   sinking_end_time, first_record_pulled,last_record_pulled,\n                   record_count,run_id,manifest_name\n```\n```\nname: get_last_record\n      gets last_record_pulled value\nparams: entity\nreturn value : last_record_pulled\n```\n```\nname: get_processing_history\n      get processing history\nparams: none\nreturn value : list of sink_entity for a source_name\n```\n#### Data Access Object\n\n##### NetezzaJDBC\n\nconnect to netezza using JDBC driver\n\n*base class*\n```\nnetezza\n```\n\ninput:\n```\nconnection:  section name in hdm profile yml file for connection information | required\n```\noutput:\n```\nconnection - connection to netezza\n```\n##### NetezzaODBC\n\nconnect to netezza using ODBC driver\n\nbase class:\n```\nnetezza\n```\ninput:\n```\nconnection:  section name in hdm profile yml file for connection information | required\n```\noutput:\n```\nconnection - connection to netezza\n```\n##### AzureBlob\n\nconnect to azure storage account\n\n*base class*\n```\nObjectStoreDAO\n```\ninput:\n```\nconnection:  section name in hdm profile yml file for connection information | required\n```\noutput:\n```\nconnection - connection to netezza\n```\n##### SnowflakeAzureCopy\n\nconnect to snowflake\ncreate or replace snowflake azure storage stage\n\n*base class*\n```\nSnowflakeCopy\n```\ninput:\n```\nconnection:  section name in hdm profile yml file for connection information | required\nstage_directory: name of azure blob container used for staging files.\nstage_name: name of snowflake azure storage stage\n```\noutput:\n```\nconnection - connection to snowflake\n```\n##### MySQL\n\nconnect to MySQL db\n\n*base class*\n```\nDBDAO\n```\n\ninput:\n```\nconnection:  section name in hdm profile yml file for connection information | required\n```\noutput:\n```\nconnection - connection to MySQL\nengine - connection engine\n```\n##### SQLlite\n\nconnect to SQLlite db\n\n*base class*\n```\nDBDAO\n```\n\ninput:\n```\nconnection:  section name in hdm profile yml file for connection information | required\n```\noutput:\n```\nconnection - connection to SQLlite\nengine - connection engine\n```\n##### SQLServer\n\nconnect to SQL Server db\n\n*base class*\n```\n DBDAO\n```\n\ninput:\n```\nconnection:  section name in hdm profile yml file for connection information | required\nengine - connection engine\n```\noutput:\n```\nconnection - connection to SQL Server\nengine - connection engine\n```\n##### AzureSQLServer\n\nconnect to Azure SQL Server db\n\n*base class*\n```\n DBDAO\n```\ninput:\n```\nconnection:  section name in hdm profile yml file for connection information | required\n```\noutput:\n```\nconnection - connection to Azure SQL Server\nengine - connection engine\n```\n#### catalog\nGet databases, schemas and tables from Netezza DW and create the same in Snowflake DW.\nRun the below:\n```\npython -m CloneNetezzaDDL\n```\n##### NetezzaCrawler\nGet database, schema and table information in Netezza DW.\n\ninput:\n```\nconnection_name:  section name in hdm profile yml file for connection information | required\n```\noutput:\n```\ndatabases: list of databases | required\nschemas: list of schemas| required\ntables: list of tables | required\n```\n##### NetezzaToSnowflakeMapper\nExecute database, schema and table ddls\n\ninput:\n```\ndatabases: list of databases | required\nschemas: list of schemas| required\ntables: list of tables | required\n```\noutput:\n```\ndatabase_sql: database ddl | required\nschema_sql: schema ddl | required\ntable_sql: table ddl\n```\n##### SnowflakeDDLWriter\nExecute database, schema and table ddls\n\ninput:\n```\nenv: section name in hdm profile yml file for connection information | required\ndatabase_sql: database ddl | required\nschema_sql: schema ddl | required\ntable_sql: table ddl\n```\noutput:\n```\nnone\n```\n#### Orchestration\n\n##### DeclaredOrchestrator\n\nThis is an orchestrator which build DataLinks and will run them as defined - they must be fully defined.\n\n*base class*\n```\norchestrator\n```\n*configuration*\n\n```\norchestrator:\n  name: Manual Orchestration\n  type: DeclaredOrchestrator\n  conf: null\n```\n*consume API*\n\ninput:\n```\nnone\n```\noutput:\n```\ndata_links: list of data links  | required\n```\n##### BatchOrchestrator\n\nThis is an orchestrator which build DataLinks and will run them as defined or templated.\n\n*base class*\n```\norchestrator\n```\n*configuration*\n\n```\norchestrator:\n  name: Batch Orchestration\n  type: BatchOrchestrator\n  conf: null\n```\n*consume API*\n\ninput:\n```\nnone\n```\noutput:\n```\ndata_links: list of data links  | required\n```\n##### AutoOrchestrator\n\nThis is an orchestrator which build DataLinks and will run them as defined or templated.\nThe template will have information about schema and database of the source system.\n\nNote: This is work in progress ...\n\n*base class*\n```\norchestrator\n```\n*configuration*\n\n```\norchestrator:\n  name: Auto Batch Orchestration\n  type: AutoOrchestrator\n  conf: null\n```\n*consume API*\n\ninput:\n```\nnone\n```\noutput:\n```\ndata_links: list of data links  | required\n```\n\n#### Utils\n\n##### Project Configuration\nThe following can be configured in utils/project_config.py\n* profile_path - folder and name of profile YML file\n* file_prefix - prefix to be used for staged files\n* state_manager_table_name - state management table name\n* archive_folder - archive folder name (used in FSChunkSource)\n* connection_max_attempts - maximum number of try  (used in DAOs)\n* connection_timeout - connection timeout value  (used in DAOs)\n* query_limit - numbers rows returned by a query\n\n## Repository Cloning\nPlease refer to [clone readme](../../Desktop/hashmap_data_migrator/clone/clone-readme.md)\n\n## Miscellaneous\n* The database, schema, table name are part of the folder structure where files are dumped - in any file staging location.\n* The sink_name of a stage is part of the folder structure where files are dumped - in any file staging location.\n* Any file created locally is stored in {HDM_DATA_STAGING}\\{sink_name}\\{table_name}\\\n* Any file created in cloud staging is stored in folder structure {sink_name}\\{table_name}\\\n* The source_name and sink_name <b>MUST MATCH between stages (combination of source and sink)</b> for the migrator to be able to pick up files for processing.\nE.g. Below you can see that sink_name for stage \"FS to AzureBlob\" is <b>azure_sink</b>. so, the source_name for stage \"cloud storage create staging and run copy\"\nis also <b>azure_sink</b>\n```\n# FS to AzureBlob\n    - source:\n        name: fs_chunk_stg\n        type: FSSource\n        conf:\n          directory: $HDM_DATA_STAGING\n      sink:\n        name: *azure_sink*\n        type: AzureBlobSink\n        conf:\n          env: azure\n          container: data\n\n#cloud storage create staging and run copy\n    - source:\n        name: *azure_sink*\n        type: AzureBlobSource\n        conf:\n          env: azure\n          container: data\n      sink:\n        name: sflk_copy_into_sink\n        type: SnowflakeAzureCopySink\n        conf:\n          env: snowflake_knerrir_schema\n          stage_name: TMP_KNERRIR\n          file_format: csv\n          stage_directory: data\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.com/hashmapinc/oss/hashmap_data_migrator",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hashmap-data-migrator",
    "package_url": "https://pypi.org/project/hashmap-data-migrator/",
    "platform": "",
    "project_url": "https://pypi.org/project/hashmap-data-migrator/",
    "project_urls": {
      "Homepage": "https://gitlab.com/hashmapinc/oss/hashmap_data_migrator"
    },
    "release_url": "https://pypi.org/project/hashmap-data-migrator/0.1.0.3/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Early version of library - do not use",
    "version": "0.1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9105790,
  "releases": {
    "0.1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a0ec1459ee8d912999ac01fafc4ac002d70e34a8ff893280a7930ed7b1ca7d82",
          "md5": "5ebdb4bdf7daa6ad0a5dc320a8d946b9",
          "sha256": "d8eabb5cc68bec65865ce1e1ae605e64406cf895cabae4106027f89d799657d2"
        },
        "downloads": -1,
        "filename": "hashmap-data-migrator-0.1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "5ebdb4bdf7daa6ad0a5dc320a8d946b9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 60520,
        "upload_time": "2021-01-11T15:02:34",
        "upload_time_iso_8601": "2021-01-11T15:02:34.918794Z",
        "url": "https://files.pythonhosted.org/packages/a0/ec/1459ee8d912999ac01fafc4ac002d70e34a8ff893280a7930ed7b1ca7d82/hashmap-data-migrator-0.1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a0ec1459ee8d912999ac01fafc4ac002d70e34a8ff893280a7930ed7b1ca7d82",
        "md5": "5ebdb4bdf7daa6ad0a5dc320a8d946b9",
        "sha256": "d8eabb5cc68bec65865ce1e1ae605e64406cf895cabae4106027f89d799657d2"
      },
      "downloads": -1,
      "filename": "hashmap-data-migrator-0.1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "5ebdb4bdf7daa6ad0a5dc320a8d946b9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 60520,
      "upload_time": "2021-01-11T15:02:34",
      "upload_time_iso_8601": "2021-01-11T15:02:34.918794Z",
      "url": "https://files.pythonhosted.org/packages/a0/ec/1459ee8d912999ac01fafc4ac002d70e34a8ff893280a7930ed7b1ca7d82/hashmap-data-migrator-0.1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}