{
  "info": {
    "author": "Don Chesworth",
    "author_email": "donald.chesworth@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: BSD License",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "![example workflow](https://github.com/donchesworth/pytorch-quik/actions/workflows/github-ci.yml/badge.svg)\n[![](https://img.shields.io/pypi/v/pytorch-quik.svg)](https://pypi.org/pypi/name/)\n[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](https://opensource.org/licenses/BSD-3-Clause)\n[![Docker Repository on Quay](https://quay.io/repository/donchesworth/rapids-dask-pytorch/status \"Docker Repository on Quay\")](https://quay.io/repository/donchesworth/rapids-dask-pytorch)\n[![code style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![codecov](https://codecov.io/gh/donchesworth/pytorch-quik/branch/main/graph/badge.svg?token=U92M8C8AFM)](https://codecov.io/gh/donchesworth/pytorch-quik)\n\n# pytorch-quik   <img src=\"pytorch-quik.png\" align=\"right\" width=\"120\" />\n\n## For the quick-er distribution of neural nets in PyTorch\n\nAs I was building out the same set of code for a recommender system, a BERT sentiment model, and a co-worker was about to build a classification model, I decided to standardize the code into this package. It's lightweight, because I didn't want to hide the standard steps to keep the user from learning Neural Networks, but I also didn't want to maintain the code in multiple places.\n\n## Updates: \n### v0.3.0 (10/25/2021)\n- Integrated ray tune for hyperparameter optimization using `use_ray_tune = True`\n- Added needed unit tests to bring code coverage to 85%\n- Updated README\n### v0.2.0 (07/22/2021)\n- Integrated MLflow into `QuikTrek` using `QuikMlflow` and `use_mlflow = True`\n- Added utilities to create all required files for a transformers TorchServe model archive\n- Added utilities to build a dataset and query a TorchServe API\n### v0.1.0 (06/03/2021)\n- Refactored to provide classes `QuikTrek`, `QuikTraveler`, `QuikData`\n- Slight change from 0.0.1, where now you need `trek = pq.travel.QuikTrek(args, gpu); tr = pq.travel.QuikTraveler(trek)`\n- Added documentation\n\n### Installation\n\n``` bash\npip install pytorch-quik\n```\n\n# Usage\n\n## Intro, QuikTrek, QuikTraveler\n\nIn its simplest form, you'll want to:\n\n- create a QuikTrek (an object that stores overall attributes and various travelers across your neural network)\n- create a QuikTraveler (an object that \"travels\" forward and backward on your neural network)\n- add QuikData\n- add a criterion, an optimizer\n- train\n\n``` python\nimport pytorch_quik as pq\nfrom torch import nn, optim\n\ntrek = pq.travel.QuikTrek()\ntr = pq.travel.QuikTraveler(trek)\ntr.add_data(pq.io.load_torch_object(\"train\", tr.args))\n\n# from pytorch.org/tutorials/beginner/pytorch_with_examples.html\nmodel = nn.Sequential(nn.Linear(3, 1), nn.Flatten(0, 1))\n\ntr.add_model(model)\ntr.set_criterion(nn.MSELoss)\ntr.set_optimizer(optim.Adam)\n\nfor epoch in range(tr.epochs):\n    tr.model.train()\n    for batch in tr.data.data_loader:\n        users, items, labels = [tens.to(tr.world.device) for tens in batch]\n        outputs = tr.model.forward(users, items)\n        loss = tr.criterion(outputs, labels)\n        tr.backward(loss)\n        tr.criterion.step()\n    pq.io.save_state_dict(tr.model, tr.args, epoch)\n````\n\n## QuikData\n\nA little more about how the data pull works. I usually run my project from my repo, and from the command line. Either way, `QuikData` expects your tensors, models, and state_dicts to be in your current path, and in a `data` subfolder. It will also expect to use your traveler's arguments to determine the date for the filename. You can set it using argparse like I do, or throw it in to your script like this:\n``` python\ntr = pq.travel.QuikTraveler()\ntr.args.data_date = '20210101'\ntr.add_data(pq.io.load_torch_object(\"train\", tr.args))\n```\nThe file will also have the words tensor, model, or state_dict in it, train/valid/test if it's a tensor, and e* if it's a state_dict, where e* stands for which epoch the state_dict was saved in. Therefore, the file structure using your repo as the base directory may look like this (I also save my original state_dict to make sure my model is actually training):\n- data/train_tensor_20210101.pt\n- data/valid_tensor_20210101.pt\n- data/test_tensor_20210101.pt\n- data/model_20210101.pt\n- data/state_dict_eorig_20210101.pt\n- data/state_dict_e0_20210101.pt\n- data/state_dict_e1_20210101.pt\n\n## Model and Functions\n\n### Model State\n\nSometimes your training and validation losses will converge sooner than expected, and you'll want to test an epoch before the final one. this is possible, because the `pq.io.save_state_dict` function will save the weights and biases at the end of the epoch to disk.\n\n### Set_* Functions\nSetting the criterion, optimizer, and scheduler just takes a callback, and can use both general defaults and specific ones. For instance, I have an OptKwargs class that can receive parameters via argparse that most optimizers have (lr, weight_decay, eps, and betas), but then you can also feed in specific parameters like `amsgrad=False` if you are using Adam at instantiation like this: `tr.set_optimizer(Adam, amsgrad=False)`. For simplicity I didn't use a scheduler above, but you could include something like `tr.set_scheduler(OneCycleLR)`, and then after your backward, include a `tr.scheduler.step()`.\n\n## Distributed Data Parallel (DDP)\n\n### Intro to DDP\n\nThis is really why pytorch-quik is quick-er for me. Adding in DDP can be tough, and I tried to do so and allow you to switch back and forth when necessary. I run my code on an OpenShift cluster, and sometimes can't get on my multi-GPU setup. This allows me to just use a different set of args and just deal with slower code, not broken code!\n\nI would suggest spending time setting up argparse so that you can have your own default arguments for batch size, learning rate, etc, but if you don't want to, you deal with my defaults. These assume you have a GPU on 1 node, which is the simplest benefit from pytorch-quik:\n\n``` python\nfrom argparse import ArgumentParser\nparser = pq.args.add_learn_args(ArgumentParser())\nargs = parser.parse_args()\ngpu = 0\ntr = pq.travel.QuikTraveler(args, gpu)\ntr.run_prep()\n```\n\nNotice the addition of providing the QuikTraveler your args, as well as telling it to run on your GPU 0. If you were truly distributing this across GPUs, you'd have to spawn QuikTravelers on each GPU, but more on that later. Also, the tr.run_prep() will start your DDP process_group.\n\n### Automated Mixed Precision (AMP)\n\nTangentally related is AMP, and if your model.forward() is already set up with mixed precision, this should work for you also. Just add `args.mixed_precision = True` before creating your traveler, and add `with tr.amp.caster` before and within your forward like so (you will have to change your `with autocast():` to be a `with myparam` where myparam is what we're sending in here:\n``` python\nwith tr.amp.caster:\n  outputs = tr.model.forward(users, items, tr.amp.caster)\n  loss = tr.criterion(outputs, labels)\n```\nHere is my .forward:\n``` python\ndef forward(self, users, items, caster):\n  with caster:\n```\n\n### Logging\n\nThis isn't true logging (though there is some), but this is a good place to talk about the progress bar and metrics. If your training/validation is distributed, your loss will eventually be aggregated with ring-all reduce on each GPU, so it doesn't matter on which one you calculate loss. So, I make your GPU 0 your \"logger\" GPU, and it will be the one with `tr.world.is_logger = True`. When you see this used, it's just telling the progress bar to be drawn, or the metrics to be calculated.\n\n#### tqdm Progress Bar\n\nOne benefit of this is the [tqdm](https://pypi.org/project/tqdm/) progress bar. I didn't bother with the notebook one so that it can work distributed across GPUs, with IPython, or in a Jupyter Notebook. It should look something like this:\n\n```\nepoch: 1/2:  22%|████████████▏                                          | 1020/4591 [00:20<01:13, 48.69it/s]\n```\n\nI take care of running it only on your first GPU for you, and calculate your steps based on how your bunches distribute across samples. So, your initialization looks like this: \n\n``` python\ntrtq = pq.ddp.tq_bar(tr.data.steps, epoch, tr.epochs)\n```\n\nand then your update and close are as they would normally be with bar. I also suggest you `del` your bar because it can have trouble drawing the next one if you don't.\n\n#### Metrics\n\nOne cool addition that I pulled from fast.ai (thanks fast.ai!) Is the way that they pull training loss and validation loss as they are training, and show them at the end of each epoch so you can see your progress. It works everywhere because it's just a pandas df, and it passes between training and validation because it's a class attribute. (Here we're only using training, so there's zeros). After the backward, you just need a `tr.metrics.add_loss(loss)` to accumulate it, and then after your loop a `tr.metrics.write()` to build the DataFrame. Then you can access it whenever you want at `tr.metrics.results`:\n\n``` python\nprint(tr.metrics.results)\n   epoch    train_loss  valid_loss   time\n0      0        0.6435         0.0  05:03\n1      1        0.5281         0.0  06:20\n```\n\n## Attributes\n\nPytorch-quik class instance attributes come in as arguments, and are (mostly) stored in dataclasses.\n\n### Arguments\n\nThere are tons of arguments to set when you're training your model. These are split into various types:\n\n- DDP args: (node rank, nodes, gpus, number of workers)\n- learning args: (epochs, mixed precision, batch size, learning rate, weight decay, betas, epc, and find_unused_paramters)\n- ray tune args: (use_ray_tune, number of samples)\n- MLflow args: (use_mlflow, experiment, user, tracking URI, endpoint URL)\n\nThere are four ways to set arguments with pytorch-quik:\n\n- Use the defaults: `pq.arg.add_learn_args(parser)`\n- Provide them at the command line: `--learning_rate 1.5e-6`\n- Defaulted within your script: `pq.arg.add_learn_args(parser, {\"learning_rate\": 1.5e-6)`\n- And of course, fix them afterward (but check their name in the Namespace: `args.lr = 1.5e-6`\n\nPutting all these things together, a simple setup would be:\n``` python\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser = pq.arg.add_ddp_args(parser)\nparser = pq.arg.add_learn_args(parser)\nparser = pq.arg.add_mlflow_args(parser)\nparser = pq.arg.add_ray_tune_args(parser)\nargs = parser.parse_args()\n```\n\n### Dataclasses\n\nSimilar to arguments, I've organized the attributes within a `QuikTrek` as three dataclasses and `QuikMlflow` as one dataclass containing keyword arguments (kwargs):\n- `DlKwargs`: Data Loader kwargs such as batch size, pin memory, and number of workers\n- `OptKwargs`: Optimizer kwargs such as learning rate, weight decay, and betas (these attempt to use universal names like `lr`)\n- `World`: This is playing off the distributed kwarg of `world_size`, and is really anything about the world, such as nodes, gpus, init group, and ray tune\n- `MlfKwargs`: MLflow kwargs such as tracking URI, endpoint URL and is_parent\n\n## Usage Summary\n\nPutting this all together, moving the argument creation into a `main`, and moving the rest into a function (let's call it `train`), which takes just two parameters, `args` and `gpu`. This would be my full example using all the above:\n\n``` python\nimport pytorch_quik as pq\nfrom torch import nn, optim\nfrom argparse import ArgumentParser\n\ndef train(args, gpu):\n    trek = pq.travel.QuikTrek(args, gpu)\n    tr = pq.travel.QuikTraveler(trek)\n    tr.run_prep()\n\n    tr.args.data_date = '20210101'\n    tr.add_data(pq.io.load_torch_object(\"train\", tr.args))\n\n    # from pytorch.org/tutorials/beginner/pytorch_with_examples.html\n    model = nn.Sequential(nn.Linear(3, 1), nn.Flatten(0, 1))\n\n    tr.add_model(model)\n    tr.set_criterion(nn.MSELoss)\n    tr.set_optimizer(optim.Adam)\n    tr.set_scheduler(OneCycleLR, steps_per_epoch=tr.data.steps)\n    if tr.world.is_logger:\n        tr.io.save_state_dict(tr.model, tr.args, \"orig\")\n\n    for epoch in range(tr.epochs):\n        if tr.world.is_logger:\n            trbar = pq.ddp.tq_bar(tr.data.steps, epoch, tr.epochs)\n        tr.model.train()\n        for batch in tr.data.data_loader:\n            users, items, labels = [tens.to(tr.world.device) for tens in batch]\n            with tr.amp.caster:\n                outputs = tr.model.forward(users, items, tr.amp.caster)\n                loss = tr.criterion(outputs, labels)\n            tr.backward(loss)\n            tr.scheduler.step()\n            tr.metrics.add_loss(loss)\n            if tr.world.is_logger:\n                trbar.update()\n        if tr.world.is_ddp:\n            dist.barrier()\n        if tr.world.is_logger:\n            trbar.close()\n            del trbar\n            pq.io.save_state_dict(tr.model, tr.args, epoch)\n    tr.metrics.write()\n    if tr.world.is_logger:\n        print(tr.metrics.results)\n    if tr.world.is_ddp:\n        pq.ddp.cleanup()\n\n\ndef main():\n    parser = pq.args.add_learn_args(ArgumentParser())\n    args = parser.parse_args()\n    gpu = 0\n    train(args, gpu)\n\n\nif __name__ == \"__main__\":\n    main()\n\n````\n\n## Additional Usage Information\n\n### Spawning across GPUs... or not!\n\nThis is why I love pytorch-quik.  then you can use my `traverse` function that will make sure your GPUs are available to PyTorch, and if so, send your training across each GPU! If it decides you can't, it send use that same exact code to the CPU run it on there. This is what they would all look like (I'll abbreviate train):\n\n``` python\nimport pytorch_quik as pq\nfrom torch import nn, optim\nfrom argparse import ArgumentParser\n\ndef train(args, gpu):\n...\n\ndef main():\n    parser = pq.args.add_learn_args(ArgumentParser())\n    args = parser.parse_args()\n    args.gpus = 4\n    pq.ddp.traverse(train, args)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### Running from command line\n\nNow, you can package your repo, not even touch the code, and run it on four GPUs like `python main.py --gpus 4`, on one gpu when there are four like `python main.py --gpus 1`, or throw it on a cluster, let the cluster decide where to run it, and when you run `python main.py`, pytorch-quik will _determine if you have GPUs and whether it should distribute or not!_\n\nCool, huh.\n\nLet me know if you have any questions, and I'll keep adding to this documentation!\n\n# Integrations\n\n## MLflow integration\n\n#### Instantiation\n\nMLflow for model tracking has been integrated with `QuikTrek`. When instantiated, if `use_mlflow=True`, then a `QuikFlow` is instantiated, as well as a run, which involves:\n- checking if `args.experiment` exists, and if not, creates it\n- creates a run under the experiment\n- adds appropriate run tags\n- adds data loader, optimizer, and world [dataclasses](#dataclasses) as run parameters\n\n#### Tracking\n\nNext, as training occurs, if the `QuikTraveler` `add_loss` function is used, the \"train_loss\" metric will be added to MLFlow after _every step_, and the \"valid_loss\" will be added after every epoch. It's best to use step, since this will align the points on the MLflow graph based on training process, not clock time.\n\n#### Saving artifacts and accuracy metrics\n\nIn addition, artifacts are saved when the following `QuikTraveler`:\n- `save_state_dict` now saves each epoch's state dict as an artifact in the run\n- `record_results`: records the classification report results as metrics, and saves the `png` of the confusion matrix as an artifact\n\n## Ray Tune integration\n\nAs it's difficult to manage the execution (and tracking) of hundreds of training runs, pytorch-quik attempted to use both [Optuna](https://optuna.org/) and [Ray Tune](https://docs.ray.io/en/ray-0.4.0/tune.html). Both were successful integrations, but optuna lacked the ability to work with DDP. If there are multiple GPUs on your platform, then ray tune will execute a run on each GPU. When `use_ray_tune=True`, the following options are available:\n- Creating a parent (with `is_parent=True`) in MLflow, to contain all runs in a collapsible parent run\n- Using `get_tune_config` to pull the hyperparameter options from a yaml file, such as:\n    ``` yaml\n    bs:\n        choice:\n            categories: [8, 12, 16, 24]\n    weight_decay:\n        choice:\n            categories: [0, 0.001]\n    ```\n- using `run_ddp_tune` to have all test permutations be distributed across GPUs\n\n## Huggingface integration\n\nAs many of my models are transformers, the following options are available (only yet tested on BERT and RoBERTa models:\n- `get_bert_info`: using `arg.bert_type`, will pull model information for a sequence classification model\n- `get_pretrained_model`: using `arg.bert_type`, will pull the model itself\n- `get_encodings`: using `arg.bert_type`, will pull the model's tokenizer and encode text\n- `save_bert_model`: pull a state dict, and add it to the model before using transformer's `save_pretrained`\n- `save_tokenizer`: save the tokenizer, and _also update the tokenizer.json_ to include the correct pathing for serving\n\n\n## Torch Serve integration\n\n### TS Functionality\n\nFinally, in order to productionalize a pytorch model, [TorchServe](https://github.com/pytorch/serve) is an excellent choice. There is quite a bit of overhead to get it up and running, so I've provided some helpful functions. The main ones to note are:\n- `build_extra_files`: There are many files needed to build a model archive. This function will build:\n    - `setup_config.json`\n    - `index_to_name.json`\n    - `sample_text.json`\n    - A default transformer handler if you don't have one (`transformer_handler_pq`)\n- `create_mar`: First checks to see if you have all the required files, then uses the command line function to create your torch model archive.\n\nI should note the benefit of using my customized handler, is that it will provide the predictions like Huggingface's example, but will _also provide the logits_ as required by my stakeholder.\n\n### TS Usage\n\nIn order to create your torch model archive (mar) file, there are only a few steps. When I do so, I pull my [state dict from MLflow](#saving-artifacts-and-accuracy-metrics), which makes it even more modular. If you create a function called `parse_my_args()` including all the `pq.args` in the [Arguments snippet](#arguments), and `INDEX_LABELS` is an `OrderedDict` of your labels, here's an example where you (1) pull the state dict, (2) download the model, (3) save the model and files to disk, and (4) create the mar:\n\n``` python\n        args = parse_my_args() # this would be all pq.arg functions you'd need\n        mlf = pq.mlflow.QuikMlflow(args)\n        serve_path = pq.io.id_str(\"\", args).parent.joinpath(\"serve\")\n        filter_str = \"params.lr = '1.6e-06'\" # some filter to differentiate the specific run for your state dict\n        args.data_date = mlf.get_state_dict(args.test_epoch, filter_string=filter_str, serve_path=serve_path)\n        model = pq.bert.get_pretrained_model(labels=list(INDEX_LABELS.keys()), bert_type=args.bert_type)\n        pq.bert.save_bert_model(model, args, args.test_epoch, serve_path)\n        pq.serve.build_extra_files(args, INDEX_LABELS, serve_path)\n        pq.serve.create_mar(args, serve_path)\n```\n\n### Torch Serve API Usage\n\nNow that you have a torch serve mar (let's call it `my_model.mar`, and have started it (hopefully in a container!) by running `torchserve --start --models \"my_tc=my_model.mar\" --ncs`, then you can use pytorch-quik's `batch_inference` function. The benefits of this function are:\n- it takes an array of records, vs. sending one input at a time to the API!\n- formats the text as the handler expects (json with an \"inference\" header and a \"data\" key for each record)\n- batches the API calls so as to not overload the API (I usually use a `batch_size` of 20-35 records)\n- opens a session with the API, and retries whenever you receive a 507 error (which is common for me)\n- normalizes the response and drops it into a pandas dataframe.\n\nUsage is pretty easy. If your test set is from sklearn train test split, then this should work (`INDEX_LABELS` is an `OrderedDict` of your labels):\n``` python\ndf = pq.api.batch_inference(Xte[:, 0], INDEX_LABELS, batch_size=20)\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/donchesworth/pytorch-quik",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pytorch-quik",
    "package_url": "https://pypi.org/project/pytorch-quik/",
    "platform": null,
    "project_url": "https://pypi.org/project/pytorch-quik/",
    "project_urls": {
      "Homepage": "https://github.com/donchesworth/pytorch-quik"
    },
    "release_url": "https://pypi.org/project/pytorch-quik/0.3.3/",
    "requires_dist": [
      "torch (>=1.7.0)",
      "torchvision",
      "pandas (>=1.1)",
      "transformers",
      "sentencepiece",
      "imbalanced-learn",
      "pytest",
      "pytest-cov",
      "pytest-mpl",
      "pytest-lazy-fixture",
      "pytest-dotenv",
      "codecov",
      "numpy (>=1.21.0)",
      "bump2version",
      "mlflow",
      "boto3",
      "ray[tune]",
      "torchserve",
      "torch-model-archiver",
      "ruamel.yaml"
    ],
    "requires_python": "",
    "summary": "functions to make working in pytorch quik-er",
    "version": "0.3.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13798226,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "21e630cfbe9c200577af8c8ec0a8c73212ab279f88782354b4025331b0c03cfe",
          "md5": "7ff24e7ae237092d22e212ec0dcd4eef",
          "sha256": "2d7ac21a3239601b0e468863328747a35df9f7d37354a261cd3719fdb36db272"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7ff24e7ae237092d22e212ec0dcd4eef",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 7080,
        "upload_time": "2021-04-05T14:01:35",
        "upload_time_iso_8601": "2021-04-05T14:01:35.378902Z",
        "url": "https://files.pythonhosted.org/packages/21/e6/30cfbe9c200577af8c8ec0a8c73212ab279f88782354b4025331b0c03cfe/pytorch_quik-0.0.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5aa8b3a46b84c13ec2af5e394ff3663237df7f9fd3e97b910685fc1a59d7db0f",
          "md5": "afa2eeb4b337ebdded510f479728bb15",
          "sha256": "25dae1dbd741211ec7b6114299ba8d07a1f653fc941eb6d5e64033e2bd047172"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "afa2eeb4b337ebdded510f479728bb15",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6513,
        "upload_time": "2021-04-05T14:01:36",
        "upload_time_iso_8601": "2021-04-05T14:01:36.551589Z",
        "url": "https://files.pythonhosted.org/packages/5a/a8/b3a46b84c13ec2af5e394ff3663237df7f9fd3e97b910685fc1a59d7db0f/pytorch-quik-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "82d72a74b4ac4eecca30a6d57921cb0e28a272369a08e716e045295e1f9fa4ac",
          "md5": "d204782963a22aeae076cccc56cf988d",
          "sha256": "4f555c187dc8b93fc225b52274458fb5aa7c1ddbaff20bfe191d38520cee1f64"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.0.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d204782963a22aeae076cccc56cf988d",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 7611,
        "upload_time": "2021-04-08T16:00:34",
        "upload_time_iso_8601": "2021-04-08T16:00:34.055100Z",
        "url": "https://files.pythonhosted.org/packages/82/d7/2a74b4ac4eecca30a6d57921cb0e28a272369a08e716e045295e1f9fa4ac/pytorch_quik-0.0.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3f4af1e384b1d867bd8df0a7f907f95e7464b180f4f58af3212173e27e3a8ac3",
          "md5": "c93a16e5332acfdfcba5dcfa319ff365",
          "sha256": "f9e2fc70c9b8995e8a0fb29a52fa685eb7c7c35ad97a8b5750685039a714e931"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "c93a16e5332acfdfcba5dcfa319ff365",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6991,
        "upload_time": "2021-04-08T16:00:35",
        "upload_time_iso_8601": "2021-04-08T16:00:35.026528Z",
        "url": "https://files.pythonhosted.org/packages/3f/4a/f1e384b1d867bd8df0a7f907f95e7464b180f4f58af3212173e27e3a8ac3/pytorch-quik-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "498ce7d47bc3a6ee0abedaed9faab721129e1264b8b2653bf7c0211bef9aa40e",
          "md5": "4f574320d20155b6407df7a5e4512e4e",
          "sha256": "9b09a0468f107fdbd3ed19c5fe6dda31a12bef690376c9901f428e16f4943731"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.0.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4f574320d20155b6407df7a5e4512e4e",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 16314,
        "upload_time": "2021-05-06T19:05:08",
        "upload_time_iso_8601": "2021-05-06T19:05:08.071708Z",
        "url": "https://files.pythonhosted.org/packages/49/8c/e7d47bc3a6ee0abedaed9faab721129e1264b8b2653bf7c0211bef9aa40e/pytorch_quik-0.0.3-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "90a4ec1cb0812472f25ac9e35284c9f5d29ca1429924ad2bb1ab1fec8079e878",
          "md5": "58fc25d71207590e3dcb9e07f58ae3f5",
          "sha256": "545b11581d4ff5a5fb0584947d8e9be94eb49d754079d7c8f9494c81fff2eef2"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "58fc25d71207590e3dcb9e07f58ae3f5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13577,
        "upload_time": "2021-05-06T19:05:10",
        "upload_time_iso_8601": "2021-05-06T19:05:10.243178Z",
        "url": "https://files.pythonhosted.org/packages/90/a4/ec1cb0812472f25ac9e35284c9f5d29ca1429924ad2bb1ab1fec8079e878/pytorch-quik-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "83a8ec10514d11dcaf37978496dd1dea6831e9db7743b7c1adcdd247a241d789",
          "md5": "bc1235e2630ccd33c483fb85329ed078",
          "sha256": "6749c30f1d7505c9c87036213d760820ec19ca564f58aa7c379d3e85a5849ec7"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.0.4-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bc1235e2630ccd33c483fb85329ed078",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 17529,
        "upload_time": "2021-05-21T13:47:17",
        "upload_time_iso_8601": "2021-05-21T13:47:17.768721Z",
        "url": "https://files.pythonhosted.org/packages/83/a8/ec10514d11dcaf37978496dd1dea6831e9db7743b7c1adcdd247a241d789/pytorch_quik-0.0.4-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3c892ff15b994983739863a15f424fb1d3f165149162fb7eff537126e1fd78c8",
          "md5": "c7200a7ecae9cc3b10b2959fffeab353",
          "sha256": "d4efd6739035cafb97d4e80c0586e511743e21ef58526fb7a0e4c868d0ceaea8"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "c7200a7ecae9cc3b10b2959fffeab353",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14729,
        "upload_time": "2021-05-21T13:47:18",
        "upload_time_iso_8601": "2021-05-21T13:47:18.975547Z",
        "url": "https://files.pythonhosted.org/packages/3c/89/2ff15b994983739863a15f424fb1d3f165149162fb7eff537126e1fd78c8/pytorch-quik-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "048d269196028eef968d5740c9933c68d2099943a11fd1e3d696319d71955a08",
          "md5": "de2dd588040f163a71ada78ade0d322c",
          "sha256": "862f0ba19dbec044bab11b81d220943dab57a14216c311a2857755f7a8bd7fa2"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "de2dd588040f163a71ada78ade0d322c",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 23133,
        "upload_time": "2021-06-03T18:24:28",
        "upload_time_iso_8601": "2021-06-03T18:24:28.200767Z",
        "url": "https://files.pythonhosted.org/packages/04/8d/269196028eef968d5740c9933c68d2099943a11fd1e3d696319d71955a08/pytorch_quik-0.1.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6e4859515322733b65c73959b1f63814d45b8cba2bba8dcabfeb0cc283bd3be5",
          "md5": "c727c93639679cbeefe0b1077e7ab9b9",
          "sha256": "f7a5106f3a8ff6489db658fd711e56cce6ff2546434f4890802764b60a4c02ad"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c727c93639679cbeefe0b1077e7ab9b9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23849,
        "upload_time": "2021-06-03T18:24:29",
        "upload_time_iso_8601": "2021-06-03T18:24:29.486808Z",
        "url": "https://files.pythonhosted.org/packages/6e/48/59515322733b65c73959b1f63814d45b8cba2bba8dcabfeb0cc283bd3be5/pytorch-quik-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e108b4280c25937ea4526b7e85bb7a3f6af0a52e8eaaed46b78635d3a1b8b5c9",
          "md5": "d3cad63fe2db3d9d1c878e3f2d09debe",
          "sha256": "fed9df502671ceeb13a4210934b3e7cfd1ba727f0bc418b5f3329a7585d3ee4b"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.2.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d3cad63fe2db3d9d1c878e3f2d09debe",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 29461,
        "upload_time": "2021-07-22T21:31:53",
        "upload_time_iso_8601": "2021-07-22T21:31:53.255329Z",
        "url": "https://files.pythonhosted.org/packages/e1/08/b4280c25937ea4526b7e85bb7a3f6af0a52e8eaaed46b78635d3a1b8b5c9/pytorch_quik-0.2.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f5719b483c5bbe9e2f87dfa52f11c6cf1134317cc42aa7f4424a7c5c927a1ca4",
          "md5": "34fdea7c9ba167a850985ec8701b17d5",
          "sha256": "9267b83e82ad772a5290e190e3a88b71a131c2cf075fca91c3ade66a87a625aa"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "34fdea7c9ba167a850985ec8701b17d5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 29173,
        "upload_time": "2021-07-22T21:31:55",
        "upload_time_iso_8601": "2021-07-22T21:31:55.013399Z",
        "url": "https://files.pythonhosted.org/packages/f5/71/9b483c5bbe9e2f87dfa52f11c6cf1134317cc42aa7f4424a7c5c927a1ca4/pytorch-quik-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "50bed74d8a4ba5205cff8c8b00c38d35f6bfb2e30d27218d63287be90dc2368f",
          "md5": "32a3f8e7c1e9953f1bfdebc362ee71b9",
          "sha256": "3e6453f809bd668e33cdbc5dc3179f06fb6a500bbfb13f337248895dd1aab517"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.3.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "32a3f8e7c1e9953f1bfdebc362ee71b9",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 46156,
        "upload_time": "2021-10-26T22:23:25",
        "upload_time_iso_8601": "2021-10-26T22:23:25.205033Z",
        "url": "https://files.pythonhosted.org/packages/50/be/d74d8a4ba5205cff8c8b00c38d35f6bfb2e30d27218d63287be90dc2368f/pytorch_quik-0.3.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a27e5acae2a533c9622911f4db8ffaccbba0d9d9b8a863cf25ba9c945e865bf4",
          "md5": "f7b573fe2b24e6bc909a16fe1dff115b",
          "sha256": "bb31b36bc18520a8a5ee3a268df9c9b5151b5582f993a2f1d901ad04de36e695"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f7b573fe2b24e6bc909a16fe1dff115b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 45469,
        "upload_time": "2021-10-26T22:23:26",
        "upload_time_iso_8601": "2021-10-26T22:23:26.390037Z",
        "url": "https://files.pythonhosted.org/packages/a2/7e/5acae2a533c9622911f4db8ffaccbba0d9d9b8a863cf25ba9c945e865bf4/pytorch-quik-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1a4f99ffa5d267a8e8073737b6e5b94ce937d46eab75df935a9424c172554f7d",
          "md5": "2350f4cff135f3498ff8ca8ec0e7db84",
          "sha256": "334906c7bc6145414135eae9f2f4cb0eea10d8be9e0650d90ce71b3a9ee8afd6"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.3.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2350f4cff135f3498ff8ca8ec0e7db84",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 46173,
        "upload_time": "2021-11-04T21:54:28",
        "upload_time_iso_8601": "2021-11-04T21:54:28.934779Z",
        "url": "https://files.pythonhosted.org/packages/1a/4f/99ffa5d267a8e8073737b6e5b94ce937d46eab75df935a9424c172554f7d/pytorch_quik-0.3.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8b3a418a5a2f27dfc360c8a10a5fdb320e9260755ff1734a16344a146f0cbba7",
          "md5": "9ac5d6b62bc412f3d310a4eb0319b781",
          "sha256": "fd69b0e82b4e80a4f1ce52563675d25b3c9d2888a8b9805b26aa6c9f403c9031"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9ac5d6b62bc412f3d310a4eb0319b781",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 45493,
        "upload_time": "2021-11-04T21:54:30",
        "upload_time_iso_8601": "2021-11-04T21:54:30.681597Z",
        "url": "https://files.pythonhosted.org/packages/8b/3a/418a5a2f27dfc360c8a10a5fdb320e9260755ff1734a16344a146f0cbba7/pytorch-quik-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4957006ce3a9fa39d0e98f10a1a08f1dacb7e5767e2b9a4835ca8408bc90d783",
          "md5": "3bf1e562a279fe22cab741c226eecc75",
          "sha256": "1991214b7912f4a323a30f6d24eaf210a0aac124bdd333483d80ce6a543602d4"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.3.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3bf1e562a279fe22cab741c226eecc75",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 45590,
        "upload_time": "2022-05-11T23:45:34",
        "upload_time_iso_8601": "2022-05-11T23:45:34.719694Z",
        "url": "https://files.pythonhosted.org/packages/49/57/006ce3a9fa39d0e98f10a1a08f1dacb7e5767e2b9a4835ca8408bc90d783/pytorch_quik-0.3.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6af4ba0e345403dd927984c30898b34fab7355ecbb7562181c6cc0aa2a0dcb9a",
          "md5": "1875bfb6b929e79c3ea6d0b4a3b0aa98",
          "sha256": "a58197d848d7a17949abec5c11cf0039db42a8621e8c8d5454156c603c96ddef"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.3.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1875bfb6b929e79c3ea6d0b4a3b0aa98",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 45084,
        "upload_time": "2022-05-11T23:45:36",
        "upload_time_iso_8601": "2022-05-11T23:45:36.184763Z",
        "url": "https://files.pythonhosted.org/packages/6a/f4/ba0e345403dd927984c30898b34fab7355ecbb7562181c6cc0aa2a0dcb9a/pytorch-quik-0.3.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "83552eb687c86f0b50ae6ed11cb2d4019789760497c9910de31a8c896c5645ea",
          "md5": "3471a39aac4fcffcf617b0ca8de28a14",
          "sha256": "633685985db93ac7f5628698e9fc4c37dd2d5f03e0d3db6f94a6654c71ade44f"
        },
        "downloads": -1,
        "filename": "pytorch_quik-0.3.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3471a39aac4fcffcf617b0ca8de28a14",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 45582,
        "upload_time": "2022-05-12T20:37:23",
        "upload_time_iso_8601": "2022-05-12T20:37:23.511371Z",
        "url": "https://files.pythonhosted.org/packages/83/55/2eb687c86f0b50ae6ed11cb2d4019789760497c9910de31a8c896c5645ea/pytorch_quik-0.3.3-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4835e9a464d1a314b9be77055bf7cb688d95cd9e41065c227b6ac36bb7760b57",
          "md5": "4e56a9e4c46bb755c21c6dc7fbef70a3",
          "sha256": "bfff7dd6fade2339cd5481d9ee139027ad7f25498d494fecee5cc171bd867253"
        },
        "downloads": -1,
        "filename": "pytorch-quik-0.3.3.tar.gz",
        "has_sig": false,
        "md5_digest": "4e56a9e4c46bb755c21c6dc7fbef70a3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 45087,
        "upload_time": "2022-05-12T20:37:25",
        "upload_time_iso_8601": "2022-05-12T20:37:25.582220Z",
        "url": "https://files.pythonhosted.org/packages/48/35/e9a464d1a314b9be77055bf7cb688d95cd9e41065c227b6ac36bb7760b57/pytorch-quik-0.3.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "83552eb687c86f0b50ae6ed11cb2d4019789760497c9910de31a8c896c5645ea",
        "md5": "3471a39aac4fcffcf617b0ca8de28a14",
        "sha256": "633685985db93ac7f5628698e9fc4c37dd2d5f03e0d3db6f94a6654c71ade44f"
      },
      "downloads": -1,
      "filename": "pytorch_quik-0.3.3-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3471a39aac4fcffcf617b0ca8de28a14",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 45582,
      "upload_time": "2022-05-12T20:37:23",
      "upload_time_iso_8601": "2022-05-12T20:37:23.511371Z",
      "url": "https://files.pythonhosted.org/packages/83/55/2eb687c86f0b50ae6ed11cb2d4019789760497c9910de31a8c896c5645ea/pytorch_quik-0.3.3-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4835e9a464d1a314b9be77055bf7cb688d95cd9e41065c227b6ac36bb7760b57",
        "md5": "4e56a9e4c46bb755c21c6dc7fbef70a3",
        "sha256": "bfff7dd6fade2339cd5481d9ee139027ad7f25498d494fecee5cc171bd867253"
      },
      "downloads": -1,
      "filename": "pytorch-quik-0.3.3.tar.gz",
      "has_sig": false,
      "md5_digest": "4e56a9e4c46bb755c21c6dc7fbef70a3",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 45087,
      "upload_time": "2022-05-12T20:37:25",
      "upload_time_iso_8601": "2022-05-12T20:37:25.582220Z",
      "url": "https://files.pythonhosted.org/packages/48/35/e9a464d1a314b9be77055bf7cb688d95cd9e41065c227b6ac36bb7760b57/pytorch-quik-0.3.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}