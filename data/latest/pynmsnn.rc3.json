{
  "info": {
    "author": "Ajay Arunachalam",
    "author_email": "ajay.arunachalam08@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "pynmsnn: NeuroMorphic Predictive Model with Spiking Neural Networks (SNN) in Python using Pytorch\n=================================================================================================\n\n**pynmsnn**\n\nGitHub: https://github.com/ajayarunachalam/pynmsnn\n\nAbout pynmsnn\n=============\n\n`pynmsnn` is an open source, low-code library in python to build neuromorphic predictive models (Classification & Regression problems) using [Spiking Neural Networks (SNNs)] (https://en.wikipedia.org/wiki/Spiking_neural_network) at ease. It allows you to go from preparing your data to deploying your spiking model within minutes. SNNs are neural networks that mimics the biological brain. In the case of SNNs, the neurons accumulate the input activation until a threshold is reached, and when this threshold is reached, the neuron empties itself from it's activation and fire. Once empty, it should indeed take some [refractory period](https://en.wikipedia.org/wiki/Refractory_period_(physiology)) until it fires again, as it happen in the brain. \n\npynmsnn is `simple`, `easy to use` and `low-code`.  It enables users to perform end-to-end Proof-Of-Concept (POC) experiments quickly and efficiently.\n\nWho should use pynmsnn?\n=======================\n\nPYNMSNN is an open source library ideal for:-\n\n- Citizen Data Scientists who prefer a low code solution.\n- Experienced Data Scientists who want to increase model accuracy and improve productivity.\n- Data Science Professionals and Consultants involved in building proof-of-concept (poc) projects.\n- Researchers for quick poc testing.\n- Students and Teachers.\n- ML Enthusiasts.\n\n\nNeuromorphic Design with SNN\n============================\n\nNeuromorphic design finds new life in machine learning. Neuromorphic architecture has had little practical success in building machines that can tackle standard tests such as logistic regression or image recognition. But lately it has been combined with the best of machine learning, and the networks of spiking neurons, bringing new hope for neuromorphic breakthroughs. Neuromorphic computing is an umbrella term given to a variety of efforts to build computation that resembles some aspect of the way the brain is formed. The brain is incredibly efficient, and one of the things that makes it efficient is because it uses spikes. If we can get a model of a spiking neurons as part of deep nets, it can be a big boost to inference, the task of making predictions, even on the energy-constrained edge computing devices such as mobile phones. \n\n\nSNN vs ANN\n----------\n\nIn SNNs, there is a time axis and the neural network sees data througout time, and activation functions are spikes that are raised past a certain pre-activation threshold. Pre-activation values constantly fades if neurons aren't excited enough. Think of it like a time-distributed ReLU with spikes or no spikes at certain time steps.\n\nSpiking Neural Networks (SNNs) are neural networks that are closer to what happens in the brain compared to what people usually code when doing Machine Learning and Deep Learning. In the case of SNNs, the neurons accumulate the input activation until a threshold is reached, and when this threshold is reached, the neuron empties itself from it's activation and fire. Once empty, it should indeed take some [refractory period](https://en.wikipedia.org/wiki/Refractory_period_(physiology)) until it fires again, as it happen in the brain.\n\nThis implies adding a **time axis** to Artificial Neural Networks (ANNs), where signal is accumulated throughout time in a pre-activation phase, then once a threshold is reached, signal is raised to the neurons above as a firing activation. At every moment, such as when the threshold isn't reached yet, the signal's pre-activation value fades.\n\nFor more detail information about SNNs you are recommended to watch [this interesting SNN animation](https://www.youtube.com/embed/3JQ3hYko51Y?start=120) which will quickly get you a feel of what it is (especially notice how neurons gets activated only gradually over time like a storm that fire together rather than statically from their inputs).\n\nWhy Neuromorphic Predictive Model?\n----------------------------------\n\nThe concept is as given in the figure. \n\n.. image:: figures/images/why_NMSNN.png\n\n\nHow does it work?\n-----------------\n\nThe neuron's firing method is through the following steps, where the argument `x` is an input:\n\nBefore anything, we need an initialize (or to empty) the state for each neuron upon starting predictions.\n\n.. code:: python\n\n    self.prev_inner = torch.zeros([batch_size, self.n_hidden]).to(self.device)\n    self.prev_outer = torch.zeros([batch_size, self.n_hidden]).to(self.device)\n\nThen, a weight matrix multiplies the input x, which is the input dataset. It is to be noted that x was modified to be flickering randomly through time (depending on the intensity of the original input x multiplied by a random uniform noise mask), or else x is already the output of a lower deep spiking layer:\n\n.. code:: python\n\n    input_excitation = self.fully_connected(x)\n\nWe then add the result to a decayed version of the information inside the neuron that we already had at the previous time step / time tick (Î”t time elapsed). The `decay_multiplier` serves the purpose of slowly fading the inner activation such that we don't accumulate stimulis for too long to be able to have the neurons to rest. The `decay_multiplier` could have a value of 0.9 for example. Decay as such is also called exponential decay and yields an effect of [Exponential moving average](https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average) over time on the most recent values seen, which also affects the gradients upon backpropagating. So, by repeatedly multplying by 0.9 the inner activation through time, it decays and neurons unexcite themselves before firing. In this sense, it's now really true that \"neurons that fire together wire together\": when a pre-synaptic input is received closer to the moment of giving an output, that recentmost value will not have had the time to be decayed/faded. This way, the gradient of recent neurons that participated in exciting the current neuron that fired will be strong. Learning will be able to take place by gradient descent according to the decay's weighting's. So, in the opposite case, a stimuli that happened too long ago will suffer from vanishing gradients since it has been exponentially decayed down. So, it won't be useful in the learning process of backprop, which is what we want and respects the \"neurons that fire together, wire together\" idiom of Hebbian learning.\n\n\n.. code:: python\n\n    inner_excitation = input_excitation + self.prev_inner * self.decay_multiplier\n\n\nNow, we compute the activation of the neurons to find their output value. We have a threshold to reach before having the neuron activating. The ReLU function might not be the most appropriate here, but just for a working prototype ReLU was used:\n\n.. code:: python\n\n    outer_excitation = F.relu(inner_excitation - self.threshold)\n\n\nNow, the magic happens. If the neuron fires, the activation of the neuron is *subtracted to its inner state* to reset each neuron. First, this has the effect of resetting them to a resting position such that they won't be firing constantly upon being activated once. Second, resetting them as such will clip the gradient through time to isolate each firing events from each other. SNNs as such are truly inspired from the brain, as the natural neurons also have a refractory period: neurons, once they fire, will need to wait a bit before firing again even if fully excited by lower neurons they have as inputs. So here, I even subtracted a second penalty named `penalty_threshold` after each resetting `threshold`. Disclaimer:- I wasn't sure whether the negative part in the biological refractory period was on the outputs of the neurons or inside the neurons (e.g.: axon v.s. body?), so here I've simply put it inside. \nLet's see how I subtract this just when the neuron fires to have it to have a refractory period:\n\n.. code:: python\n\n    do_penalize_gate = (outer_excitation > 0).float()\n    inner_excitation = inner_excitation - (self.penalty_threshold + outer_excitation) * do_penalize_gate\n\n\nFinally, I return the previous output, simulating a small firing delay, which may not be useful in all the case, but which may be interesting to have if the SNN that is designed was ever to have Recurrent connections which would require time offsets in the connections from top layers near the outputs back into bottom layers near the input:\n\n.. code:: python\n\n    delayed_return_state = self.prev_inner\n    delayed_return_output = self.prev_outer\n    self.prev_inner = inner_excitation\n    self.prev_outer = outer_excitation\n    return delayed_return_state, delayed_return_output\n\n\nPast that, to do the classification, the values of the classification output spiking neurons are averaged over the time axis so as to have one number per class to plug into the softmax cross entropy loss for classification as we know it and we backpropagate. This means the present SNN PyTorch class is reusable within any other feedforward neural network, as it repeats intputs over time with random noisy masks, and averages outputs over time.  \n\nBasically, the neurons' activation must decay through time, and fire only when getting past a certain threshold. \n\nRequirements\n============\n\n-  **Python 3.6.x**\n-  torch[>=1.4.0]\n-  NumPy[>=1.9.0]\n-  SciPy[>=0.14.0]\n-  Scikit-learn[>=0.16]\n-  Pandas[>=0.23.0]\n-  Xgboost[>=1.4.1]\n-  Matplotlib\n-  Seaborn[0.9.0]\n-  Plot-metric\n-  regressormetricgraphplot\n-  tqdm\n\n\nQuickly Setup package with automation scripts\n=============================================\n\n.. code:: bash\n\n    sudo bash setup.sh\n\nInstallation\n------------\nUsing pip:\n\n.. code:: sh\n\n    pip install pynmsnn\n\n.. code:: bash\n\n    $ git clone https://github.com/ajayarunachalam/pynmsnn\n    $ cd pynmsnn\n    $ python setup.py install\n\n\nUsing notebook:\n\n.. code:: sh\n\n    !pip install pynmsnn\n\n\nGetting started\n===============\n\n-  **DEMO:**\n\nExample Binary Classification: Synthetic dataset\n------------------------------------------------\n\n.. code:: python\n\n    __author__ = 'Ajay Arunachalam'\n    __version__ = '0.0.1'\n    __date__ = '19.7.2021'\n\n\timport numpy as np\n\timport pandas as pd\n\timport seaborn as sns\n\timport matplotlib.pyplot as plt\n\t%matplotlib inline\n\timport torch\n\timport torch.nn as nn\n\timport torch.optim as optim\n\tfrom torch.utils.data import Dataset, DataLoader\n\tfrom sklearn.preprocessing import StandardScaler, MinMaxScaler    \n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.metrics import confusion_matrix, classification_report\n\tfrom sklearn.datasets import make_classification\n\tfrom pyNM.cf_matrix import make_confusion_matrix\n\tfrom pyNM.spiking_binary_classifier import *\n\tfrom plot_metric.functions import MultiClassClassification, BinaryClassification\n\n\t#fixing random state\n\trandom_state=1234\n\n\t# Generate 2 class dataset\n\tX, Y = make_classification(n_samples=10000, n_classes=2, weights=[1,1], random_state=1)\n\t# split into train/test sets\n\t#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n\n\t# Load dataset (we just selected 4 classes of digits)\n\t#X, Y = load_digits(n_class=4, return_X_y=True)\n\n\tprint(f'Predictors: {X}')\n\n\tprint(f'Outcome: {Y}')\n\n\tprint(f'Distribution of target:')\n\tprint(pd.value_counts(Y))\n\n\t# Add noisy features to make the problem more harder\n\trandom_state = np.random.RandomState(123)\n\tn_samples, n_features = X.shape\n\tX = np.c_[X, random_state.randn(n_samples, 1000 * n_features)]\n\n\t## Spliting data into train and test sets.\n\tX, X_test, y, y_test = train_test_split(X, Y, test_size=0.4,\n\t                                        random_state=123)\n\n\t## Spliting train data into training and validation sets.\n\tX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2,\n\t                                                      random_state=1)\n\n\tprint('Data shape:')\n\tprint('X_train: %s, X_valid: %s, X_test: %s \\n' %(X_train.shape, X_valid.shape,\n\t                                                  X_test.shape))\n\n\t# Scale data to have mean '0' and variance '1' \n\t# which is importance for convergence of the neural network\n\tscaler = StandardScaler()\n\tX_train = scaler.fit_transform(X_train)\n\tX_valid = scaler.transform(X_valid)\n\tX_test = scaler.transform(X_test)\n\n\tX_train, y_train = np.array(X_train), np.array(y_train)\n\n\tX_valid, y_valid = np.array(X_valid), np.array(y_valid)\n\n\tX_test, y_test = np.array(X_test), np.array(y_test)\n\n\tEPOCHS = 50\n\tBATCH_SIZE = 64\n\tLEARNING_RATE = 0.001\n\n\t## train data\n\tclass trainData(Dataset):\n\n\t    def __init__(self, X_data, y_data):\n\t        self.X_data = X_data\n\t        self.y_data = y_data\n\n\t    def __getitem__(self, index):\n\t        return self.X_data[index], self.y_data[index]\n\n\t    def __len__ (self):\n\t        return len(self.X_data)\n\n\n\ttrain_data = trainData(torch.FloatTensor(X_train), \n\t                       torch.FloatTensor(y_train))\n\n\t## validation data\n\n\tclass valData(Dataset):\n\t    def __init__(self, X_data, y_data):\n\t        self.X_data = X_data\n\t        self.y_data = y_data\n\n\t    def __getitem__(self, index):\n\t        return self.X_data[index], self.y_data[index]\n\n\t    def __len__ (self):\n\t        return len(self.X_data)\n\n\tval_data = valData(torch.FloatTensor(X_valid), \n\t                       torch.FloatTensor(y_valid))\n\n\t## test data    \n\tclass testData(Dataset):\n\n\t    def __init__(self, X_data):\n\t        self.X_data = X_data\n\n\t    def __getitem__(self, index):\n\t        return self.X_data[index]\n\n\t    def __len__ (self):\n\t        return len(self.X_data)\n\n\n\ttest_data = testData(torch.FloatTensor(X_test))\n\n\ttrain_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n\tval_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=True)\n\ttest_loader = DataLoader(dataset=test_data, batch_size=1)\n\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tprint(device)\n\n\tmodel = SpikingNeuralNetwork(device, X_train.shape[1], n_time_steps=500, begin_eval=0)\n\tmodel.to(device)\n\tprint(model)\n\tcriterion = nn.BCEWithLogitsLoss()\n\toptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n\tdef binary_acc(y_pred, y_test):\n\n    \ty_pred_tag = torch.round(torch.sigmoid(y_pred))\n\n    \tcorrect_results_sum = (y_pred_tag == y_test).sum().float()\n    \tacc = correct_results_sum/y_test.shape[0]\n    \tacc = torch.round(acc * 100)\n\n    \treturn acc\n\n\tmodel.train()\n\tfor e in range(1,EPOCHS+1):\n\t    epoch_loss = 0\n\t    epoch_acc = 0\n\t    for X_batch, y_batch in train_loader:\n\t        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\t        optimizer.zero_grad()\n\t        y_pred = model(X_batch)\n\t        loss = criterion(y_pred, y_batch.unsqueeze(1))\n\t        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n\n\t        loss.backward()\n\t        optimizer.step()\n\n\t        epoch_loss += loss.item()\n\t        epoch_acc += acc.item()\n\n\t    print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n\n\ty_pred_list = []\n\tmodel.eval()\n\twith torch.no_grad():\n\t    for X_batch in test_loader:\n\t        X_batch = X_batch.to(device)\n\t        y_test_pred = model(X_batch)\n\t        y_test_pred = torch.sigmoid(y_test_pred)\n\t        y_pred_tag = torch.round(y_test_pred)\n\t        y_pred_list.append(y_pred_tag.cpu().numpy())\n\n\ty_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n\n\t#Get the confusion matrix\n\tcf_matrix = confusion_matrix(y_test, y_pred_list)\n\tprint(cf_matrix)\n\tmake_confusion_matrix(cf_matrix, figsize=(8,6), cbar=False, title='CF Matrix')\n\n\tprint(classification_report(y_test, y_pred_list))\n\n\t# report\n\t# Visualisation of plots\n\tbc = BinaryClassification(y_test, y_pred_list, labels=[0, 1])\n\t# Figures\n\tplt.figure(figsize=(15,10))\n\tplt.subplot2grid(shape=(2,6), loc=(0,0), colspan=2)\n\tbc.plot_roc_curve()\n\tplt.subplot2grid((2,6), (0,2), colspan=2)\n\tbc.plot_precision_recall_curve()\n\tplt.subplot2grid((2,6), (0,4), colspan=2)\n\tbc.plot_class_distribution()\n\tplt.subplot2grid((2,6), (1,1), colspan=2)\n\tbc.plot_confusion_matrix()\n\tplt.subplot2grid((2,6), (1,3), colspan=2)\n\tbc.plot_confusion_matrix(normalize=True)\n\n\t# Save figure\n\tplt.savefig('./example_binary_classification.png')\n\n\t# Display Figure\n\tplt.show()\n\tplt.close()\n\n\t# Full report of the classification\n\tbc.print_report()\n\n\t# Example custom param using dictionnary\n\tparam_pr_plot = {\n\t    'c_pr_curve':'blue',\n\t    'c_mean_prec':'cyan',\n\t    'c_thresh_lines':'red',\n\t    'c_f1_iso':'green',\n\t    'beta': 2,\n\t}\n\n\tplt.figure(figsize=(6,6))\n\tbc.plot_precision_recall_curve(**param_pr_plot)\n\n\t# Save figure\n\tplt.savefig('./example_binary_class_PRCurve_custom.png')\n\n\t# Display Figure\n\tplt.show()\n\tplt.close()\n\nExample MultiClass Classification: IRIS dataset\n------------------------------------------------\n\n.. code:: python\n\n    __author__ = 'Ajay Arunachalam'\n    __version__ = '0.0.1'\n    __date__ = '19.7.2021'\n\n    from pyNM.spiking_multiclass_classifier import *\n    import torch\n    import torch.nn.functional as F\n    import torch.nn as nn\n    from torch.autograd import Variable\n    from torch.utils.data import Dataset, DataLoader\n    import torch.optim as optim\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.decomposition import PCA\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler    \n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score, precision_score, recall_score\n    from sklearn.metrics import mean_squared_error, r2_score\n    from sklearn.linear_model import LinearRegression\n    from sklearn.ensemble import RandomForestRegressor\n    from xgboost import XGBRegressor\n    from tqdm.notebook import tqdm\n    from sklearn.datasets import load_iris\n    from sklearn.metrics import roc_curve, auc\n    from sklearn.metrics import confusion_matrix\n    from pyNM.cf_matrix import make_confusion_matrix\n    from plot_metric.functions import MultiClassClassification\n    import seaborn as sns\n    from random import *\n    get_ipython().run_line_magic('matplotlib', 'inline')\n\n    def run_classifier():\n        torch.multiprocessing.freeze_support()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        print(device)\n\n        # load iris dataset\n        iris_df = pd.read_csv('../data/iris_data.csv')\n        print(iris_df.shape)\n        print(iris_df.head())\n\n        # transforming target/class to numeric\n\n        #iris_df.loc[iris_df.species=='Iris-setosa','species'] = 0\n        #iris_df.loc[iris_df.species=='Iris-versicolor','species'] = 1\n        #iris_df.loc[iris_df.species=='Iris-virginica','species'] = 2\n\n        #checking class distribution\n        iris_df['target'].value_counts().plot.bar(legend='Class Distribution')\n\n        # final dataset\n\n        X = iris_df.iloc[:,0:4]\n        y = iris_df.iloc[:,4]\n        #print(y)\n\n        # Scale data to have mean '0' and variance '1' \n        # which is importance for convergence of the neural network\n        scaler = StandardScaler()\n        X_scaled = scaler.fit_transform(X)\n\n        # Split the data set into training and testing\n        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y.values.astype(float), test_size=0.2, random_state=2)\n\n        #####################\n        spiking_model = SpikingNeuralNetwork(device, X_train.shape[1], num_class=3, n_time_steps=64, begin_eval=0)\n        #####################\n        optimizer = torch.optim.Adam(spiking_model.parameters(), lr=0.001)\n        #optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n        loss_fn   = nn.CrossEntropyLoss()\n        print(spiking_model)\n\n        # Train the model\n        EPOCHS  = 100\n        X_train = Variable(torch.from_numpy(X_train)).float()\n        y_train = Variable(torch.from_numpy(y_train)).long()\n        X_test  = Variable(torch.from_numpy(X_test)).float()\n        y_test  = Variable(torch.from_numpy(y_test)).long()\n\n        loss_list     = np.zeros((EPOCHS,))\n        accuracy_list = np.zeros((EPOCHS,))\n\n        for epoch in tqdm(range(EPOCHS)): \n            y_pred = spiking_model(X_train) #model\n            #print(y_pred)\n            loss = loss_fn(y_pred, y_train)\n            loss_list[epoch] = loss.item()\n\n            # Zero gradients\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                y_pred = spiking_model(X_test) #model\n                correct = (torch.argmax(y_pred, dim=1) == y_test).type(torch.FloatTensor)\n                accuracy_list[epoch] = correct.mean()\n\n\n            if epoch % 10 == 0:\n                print('number of epoch', epoch, 'loss', loss.item()) \n                print('number of epoch', epoch, 'accuracy', correct[0])\n\n        # Plot Accuracy and Loss from Training\n        fig, (ax1, ax2) = plt.subplots(2, figsize=(12, 6), sharex=True)\n\n        ax1.plot(accuracy_list)\n        ax1.set_ylabel(\"validation accuracy\")\n        ax2.plot(loss_list)\n        ax2.set_ylabel(\"validation loss\")\n        ax2.set_xlabel(\"epochs\");\n\n\n        # Show ROC Curve\n        plt.figure(figsize=(10, 10))\n        plt.plot([0, 1], [0, 1], 'k--')\n\n        # One hot encoding\n        enc = OneHotEncoder()\n        Y_onehot = enc.fit_transform(y_test[:, np.newaxis]).toarray()\n\n        with torch.no_grad():\n            y_pred = spiking_model(X_test).numpy() #model\n            #pred = torch.argmax(y_pred).type(torch.FloatTensor)\n            fpr, tpr, threshold = roc_curve(Y_onehot.ravel(), y_pred.ravel())\n\n        plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc(fpr, tpr)))\n        plt.xlabel('False positive rate')\n        plt.ylabel('True positive rate')\n        plt.title('ROC curve')\n        plt.legend();\n\n        print(y_test)\n        print(np.argmax(y_pred, axis=1))\n        y_pred_ = np.argmax(y_pred, axis=1)\n\n        #Get the confusion matrix\n        cf_matrix = confusion_matrix(y_test, y_pred_)\n        print(cf_matrix)\n        make_confusion_matrix(cf_matrix, figsize=(8,6), cbar=False, title='IRIS CF Matrix')\n\n        # report\n        # Visualisation of plots\n        mc = MultiClassClassification(y_test, y_pred, labels=[0, 1, 2])\n        plt.figure(figsize=(13,4))\n        plt.subplot(131)\n        mc.plot_roc()\n        plt.subplot(132)\n        mc.plot_confusion_matrix()\n        plt.subplot(133)\n        mc.plot_confusion_matrix(normalize=True)\n\n        plt.savefig('../figures/images/plot_multi_classification.png')\n        plt.show()\n\n        mc.print_report()\n\n    if (__name__ == '__main__'):\n        run_classifier()\n\n\nExample MultiClass Classification: MNIST dataset\n------------------------------------------------\n\n.. code:: python\n\n    import os\n    import matplotlib.pyplot as plt\n    import torchvision.datasets\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    import torchvision.transforms as transforms\n    from torch.autograd import Variable\n    import numpy as np\n    import pandas as pd\n\n\n    def train(model, device, train_set_loader, optimizer, epoch, logging_interval=100):   \n        model.train()\n        for batch_idx, (data, target) in enumerate(train_set_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            optimizer.step()\n\n            if batch_idx % logging_interval == 0:\n                pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n                correct = pred.eq(target.view_as(pred)).float().mean().item()\n                print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f} Accuracy: {:.2f}%'.format(\n                    epoch, batch_idx * len(data), len(train_set_loader.dataset),\n                    100. * batch_idx / len(train_set_loader), loss.item(),\n                    100. * correct))\n\n    def train_many_epochs(model): \n        epoch = 1\n        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n        train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n        test(model, device, test_set_loader)\n\n        epoch = 2\n        optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.5)\n        train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n        test(model, device, test_set_loader)\n\n        epoch = 3\n        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n        train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n        test(model, device, test_set_loader)\n\n    def test(model, device, test_set_loader):\n        model.eval()\n        test_loss = 0\n        correct = 0\n\n        with torch.no_grad():\n            for data, target in test_set_loader:\n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                # Note: with `reduce=True`, I'm not sure what would happen with a final batch size \n                # that would be smaller than regular previous batch sizes. For now it works.\n                test_loss += F.nll_loss(output, target, reduce=True).item() # sum up batch loss\n                pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n                correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_set_loader.dataset)\n        print(\"\")\n        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n            test_loss, \n            correct, len(test_set_loader.dataset),\n            100. * correct / len(test_set_loader.dataset)))\n        print(\"\")\n\n    def download_mnist(data_path):\n        if not os.path.exists(data_path):\n            os.mkdir(data_path)\n        transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n        training_set = torchvision.datasets.MNIST(data_path, train=True, transform=transformation, download=True)\n        testing_set = torchvision.datasets.MNIST(data_path, train=False, transform=transformation, download=True)\n        return training_set, testing_set\n\n    batch_size = 1000\n    DATA_PATH = './data' #set your data path here\n\n    training_set, testing_set = download_mnist(DATA_PATH)\n    train_set_loader = torch.utils.data.DataLoader(\n        dataset=training_set,\n        batch_size=batch_size,\n        shuffle=True)\n    test_set_loader = torch.utils.data.DataLoader(\n        dataset=testing_set,\n        batch_size=batch_size,\n        shuffle=False)\n\n    # Use GPU wherever possible!\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    print(device)\n\n    class SpikingNeuronLayer(nn.Module):\n\n        def __init__(self, device, n_inputs=28*28, n_hidden=100, decay_multiplier=0.9, threshold=2.0, penalty_threshold=2.5):\n            super(SpikingNeuronLayer, self).__init__()\n            self.device = device\n            self.n_inputs = n_inputs\n            self.n_hidden = n_hidden\n            self.decay_multiplier = decay_multiplier\n            self.threshold = threshold\n            self.penalty_threshold = penalty_threshold\n\n            self.fc = nn.Linear(n_inputs, n_hidden)\n\n            self.init_parameters()\n            self.reset_state()\n            self.to(self.device)\n\n        def init_parameters(self):\n            for param in self.parameters():\n                if param.dim() >= 2:\n                    nn.init.xavier_uniform_(param)\n\n        def reset_state(self):\n            self.prev_inner = torch.zeros([self.n_hidden]).to(self.device)\n            self.prev_outer = torch.zeros([self.n_hidden]).to(self.device)\n\n        def forward(self, x):\n            \"\"\"\n            Call the neuron at every time step.\n\n            x: activated_neurons_below\n\n            return: a tuple of (state, output) for each time step. Each item in the tuple\n            are then themselves of shape (batch_size, n_hidden) and are PyTorch objects, such \n            that the whole returned would be of shape (2, batch_size, n_hidden) if casted.\n            \"\"\"\n            if self.prev_inner.dim() == 1:\n                # Adding batch_size dimension directly after doing a `self.reset_state()`:\n                batch_size = x.shape[0]\n                self.prev_inner = torch.stack(batch_size * [self.prev_inner])\n                self.prev_outer = torch.stack(batch_size * [self.prev_outer])\n\n            # 1. Weight matrix multiplies the input x\n            input_excitation = self.fc(x)\n\n            # 2. We add the result to a decayed version of the information we already had.\n            inner_excitation = input_excitation + self.prev_inner * self.decay_multiplier\n\n            # 3. We compute the activation of the neuron to find its output value, \n            #    but before the activation, there is also a negative bias that refrain thing from firing too much.\n            outer_excitation = F.relu(inner_excitation - self.threshold)\n\n            # 4. If the neuron fires, the activation of the neuron is subtracted to its inner state \n            #    (and with an extra penalty for increase refractory time), \n            #    because it discharges naturally so it shouldn't fire twice. \n            do_penalize_gate = (outer_excitation > 0).float()\n            # TODO: remove following /2?\n            inner_excitation = inner_excitation - (self.penalty_threshold/self.threshold * inner_excitation) * do_penalize_gate\n\n            # 5. The outer excitation has a negative part after the positive part. \n            outer_excitation = outer_excitation #+ torch.abs(self.prev_outer) * self.decay_multiplier / 2.0\n\n            # 6. Setting internal values before returning. \n            #    And the returning value is the one of the previous time step to delay \n            #    activation of 1 time step of \"processing\" time. For logits, we don't take activation.\n            delayed_return_state = self.prev_inner\n            delayed_return_output = self.prev_outer\n            self.prev_inner = inner_excitation\n            self.prev_outer = outer_excitation\n            return delayed_return_state, delayed_return_output\n\n\n    class InputDataToSpikingPerceptronLayer(nn.Module):\n\n        def __init__(self, device):\n            super(InputDataToSpikingPerceptronLayer, self).__init__()\n            self.device = device\n\n            self.reset_state()\n            self.to(self.device)\n\n        def reset_state(self):\n            #     self.prev_state = torch.zeros([self.n_hidden]).to(self.device)\n            pass\n\n        def forward(self, x, is_2D=True):\n            x = x.view(x.size(0), -1)  # Flatten 2D image to 1D for FC\n            random_activation_perceptron = torch.rand(x.shape).to(self.device)\n            return random_activation_perceptron * x\n\n\n    class OutputDataToSpikingPerceptronLayer(nn.Module):\n\n        def __init__(self, average_output=True):\n            \"\"\"\n            average_output: might be needed if this is used within a regular neural net as a layer.\n            Otherwise, sum may be numerically more stable for gradients with setting average_output=False.\n            \"\"\"\n            super(OutputDataToSpikingPerceptronLayer, self).__init__()\n            if average_output:\n                self.reducer = lambda x, dim: x.sum(dim=dim)\n            else:\n                self.reducer = lambda x, dim: x.mean(dim=dim)\n\n        def forward(self, x):\n            if type(x) == list:\n                x = torch.stack(x)\n            return self.reducer(x, 0)\n\n\n    class SpikingNeuralNetwork(nn.Module):\n\n        def __init__(self, device, n_time_steps, begin_eval):\n            super(SpikingNeuralNetwork, self).__init__()\n            assert (0 <= begin_eval and begin_eval < n_time_steps)\n            self.device = device\n            self.n_time_steps = n_time_steps\n            self.begin_eval = begin_eval\n\n            self.input_conversion = InputDataToSpikingPerceptronLayer(device)\n\n            self.layer1 = SpikingNeuronLayer(\n                device, n_inputs=28*28, n_hidden=100,\n                decay_multiplier=0.9, threshold=1.0, penalty_threshold=1.5\n            )\n\n            self.layer2 = SpikingNeuronLayer(\n                device, n_inputs=100, n_hidden=10,\n                decay_multiplier=0.9, threshold=1.0, penalty_threshold=1.5\n            )\n\n            self.output_conversion = OutputDataToSpikingPerceptronLayer(average_output=False)  # Sum on outputs.\n\n            self.to(self.device)\n\n        def forward_through_time(self, x):\n            \"\"\"\n            This acts as a layer. Its input is non-time-related, and its output too.\n            So the time iterations happens inside, and the returned layer is thus\n            passed through global average pooling on the time axis before the return \n            such as to be able to mix this pipeline with regular backprop layers such\n            as the input data and the output data.\n            \"\"\"\n            self.input_conversion.reset_state()\n            self.layer1.reset_state()\n            self.layer2.reset_state()\n\n            out = []\n\n            all_layer1_states = []\n            all_layer1_outputs = []\n            all_layer2_states = []\n            all_layer2_outputs = []\n            for _ in range(self.n_time_steps):\n                xi = self.input_conversion(x)\n\n                # For layer 1, we take the regular output.\n                layer1_state, layer1_output = self.layer1(xi)\n\n                # We take inner state of layer 2 because it's pre-activation and thus acts as out logits.\n                layer2_state, layer2_output = self.layer2(layer1_output)\n\n                all_layer1_states.append(layer1_state)\n                all_layer1_outputs.append(layer1_output)\n                all_layer2_states.append(layer2_state)\n                all_layer2_outputs.append(layer2_output)\n                out.append(layer2_state)\n\n            out = self.output_conversion(out[self.begin_eval:])\n            return out, [[all_layer1_states, all_layer1_outputs], [all_layer2_states, all_layer2_outputs]]\n\n        def forward(self, x):\n            out, _ = self.forward_through_time(x)\n            return F.log_softmax(out, dim=-1)\n\n        def visualize_all_neurons(self, x):\n            assert x.shape[0] == 1 and len(x.shape) == 4, (\n                \"Pass only 1 example to SpikingNeuralNetwork.visualize(x) with outer dimension shape of 1.\")\n            _, layers_state = self.forward_through_time(x)\n\n            for i, (all_layer_states, all_layer_outputs) in enumerate(layers_state):\n                layer_state  =  torch.stack(all_layer_states).data.cpu().numpy().squeeze().transpose()\n                layer_output = torch.stack(all_layer_outputs).data.cpu().numpy().squeeze().transpose()\n\n                self.plot_layer(layer_state, title=\"Inner state values of neurons for layer {}\".format(i))\n                self.plot_layer(layer_output, title=\"Output spikes (activation) values of neurons for layer {}\".format(i))\n\n        def visualize_neuron(self, x, layer_idx, neuron_idx):\n            assert x.shape[0] == 1 and len(x.shape) == 4, (\n                \"Pass only 1 example to SpikingNeuralNetwork.visualize(x) with outer dimension shape of 1.\")\n            _, layers_state = self.forward_through_time(x)\n\n            all_layer_states, all_layer_outputs = layers_state[layer_idx]\n            layer_state  =  torch.stack(all_layer_states).data.cpu().numpy().squeeze().transpose()\n            layer_output = torch.stack(all_layer_outputs).data.cpu().numpy().squeeze().transpose()\n\n            self.plot_neuron(layer_state[neuron_idx], title=\"Inner state values neuron {} of layer {}\".format(neuron_idx, layer_idx))\n            self.plot_neuron(layer_output[neuron_idx], title=\"Output spikes (activation) values of neuron {} of layer {}\".format(neuron_idx, layer_idx))\n\n        def plot_layer(self, layer_values, title):\n            \"\"\"\n            plot the layer\n            \"\"\"\n            width = max(16, layer_values.shape[0] / 8)\n            height = max(4, layer_values.shape[1] / 8)\n            plt.figure(figsize=(width, height))\n            plt.imshow(\n                layer_values,\n                interpolation=\"nearest\",\n                cmap=plt.cm.rainbow\n            )\n            plt.title(title)\n            plt.colorbar()\n            plt.xlabel(\"Time\")\n            plt.ylabel(\"Neurons of layer\")\n            plt.show()\n\n        def plot_neuron(self, neuron_through_time, title):\n            width = max(16, len(neuron_through_time) / 8)\n            height = 4\n            plt.figure(figsize=(width, height))\n            plt.title(title)\n            plt.plot(neuron_through_time)\n            plt.xlabel(\"Time\")\n            plt.ylabel(\"Neuron's activation\")\n            plt.show()\n\n    class NonSpikingNeuralNetwork(nn.Module):\n\n        def __init__(self):\n            super(NonSpikingNeuralNetwork, self).__init__()\n            self.layer1 = nn.Linear(28*28, 100)\n            self.layer2 = nn.Linear(100, 10)\n\n        def forward(self, x, is_2D=True):\n            x = x.view(x.size(0), -1)  # Flatten 2D image to 1D for FC\n            x = F.relu(self.layer1(x))\n            x = self.layer2(x)\n            return F.log_softmax(x, dim=-1)\n\n    '''\n    Training a Spiking Neural Network (SNN)\n    '''\n    spiking_model = SpikingNet(device, n_time_steps=128, begin_eval=0)\n    train_many_epochs(spiking_model)\n\n    '''\n    Training a Feedforward Neural Network (for comparison) - Non-Spiking Neural Network\n\n    It has the same number of layers and neurons, and also uses ReLU activation, but it's not an SNN, this one is a regular one as defined in the code above with the class NonSpikingNeuralNetwork.\n    '''\n\n    non_spiking_model = NonSpikingNeuralNetwork().to(device)\n    train_many_epochs(non_spiking_model)\n\n    '''\n    Let's see how the neurons spiked\n    '''\n    data, target = test_set_loader.__iter__().__next__()\n\n    # taking 1st testing example:\n    x = torch.stack([data[0]]) \n    y = target.data.numpy()[0]\n    plt.figure(figsize=(12,12))\n    plt.imshow(x.data.cpu().numpy()[0,0])\n    plt.title(\"Input image x of label y={}:\".format(y))\n    plt.show()\n\n    # plotting neuron's activations:\n    spiking_model.visualize_all_neurons(x)\n    print(\"A hidden neuron that looks excited:\")\n    spiking_model.visualize_neuron(x, layer_idx=0, neuron_idx=0)\n    print(\"The output neuron of the label:\")\n    spiking_model.visualize_neuron(x, layer_idx=1, neuron_idx=y)\n\nFull Demo\n=========\n## Important Links\n------------------\n- Find the notebook for the Spiking Neural Network Multiclass classifier predictive model demo here : https://github.com/ajayarunachalam/pynmsnn/blob/main/pyNM/spiking-multiclass-classifier-model.ipynb\n\n- Find the notebook for the Non-Spiking Neural Network Multiclass classifier predictive model demo here : https://github.com/ajayarunachalam/pynmsnn/blob/main/pyNM/nonspiking-multiclass-classifier-model.ipynb\n\n- Find the notebook for the Spiking Neural Network Binary class classifier predictive model demo here : https://github.com/ajayarunachalam/pynmsnn/blob/main/pyNM/spiking-binary-classifier-model.ipynb\n\n- Find the notebook for the Non-Spiking Neural Network Binary class classifier predictive model demo here : https://github.com/ajayarunachalam/pynmsnn/blob/main/pyNM/nonspiking-binary-classifier-model.ipynb\n\n- Find the notebook for the Spiking Neural Network Regressor predictive model demo here : https://github.com/ajayarunachalam/pynmsnn/blob/main/pyNM/spiking-regressor-model.ipynb\n\n- Find the notebook for the Non-Spiking Neural Network Regressor predictive model demo here : https://github.com/ajayarunachalam/pynmsnn/blob/main/pyNM/nonspiking-regressor-model.ipynb\n\n\nLicense\n=======\nCopyright 2021-2022 Ajay Arunachalam <ajay.arunachalam08@gmail.com>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Â© 2021 GitHub, Inc.\n\n\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ajayarunachalam/pynmsnn/",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pynmsnn",
    "package_url": "https://pypi.org/project/pynmsnn/",
    "platform": "",
    "project_url": "https://pypi.org/project/pynmsnn/",
    "project_urls": {
      "Homepage": "https://github.com/ajayarunachalam/pynmsnn/"
    },
    "release_url": "https://pypi.org/project/pynmsnn/0.0.3/",
    "requires_dist": [
      "pandas",
      "numpy",
      "torch",
      "seaborn",
      "xgboost",
      "scipy",
      "tqdm",
      "scikit-learn",
      "matplotlib",
      "plot-metric",
      "regressormetricgraphplot"
    ],
    "requires_python": "",
    "summary": "NeuroMorphic Predictive Model with Spiking Neural Networks (SNN) using Pytorch",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10954486,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1f31f3e2d859849685bec11011de1de830c95e6df243d87e6ab10964fbae9382",
          "md5": "dd0089db26b577e05669ab2bf5f7282c",
          "sha256": "1b6244dd2ea64731ac88344bdb31c2633bbe98ad20eb2a4fce2b841cde02d7ce"
        },
        "downloads": -1,
        "filename": "pynmsnn-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dd0089db26b577e05669ab2bf5f7282c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 24844,
        "upload_time": "2021-07-20T06:27:54",
        "upload_time_iso_8601": "2021-07-20T06:27:54.048917Z",
        "url": "https://files.pythonhosted.org/packages/1f/31/f3e2d859849685bec11011de1de830c95e6df243d87e6ab10964fbae9382/pynmsnn-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e60c6dfab6bd834c3b8cec0acb8fee4cbef473302f489730dd5673c7552a3af1",
          "md5": "4d56ea3251caf003c2b37fe6664b235b",
          "sha256": "8b0bce49012f269ce262dbfd5be95620a580dbd16db6bb203372e648205507aa"
        },
        "downloads": -1,
        "filename": "pynmsnn-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4d56ea3251caf003c2b37fe6664b235b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 35621,
        "upload_time": "2021-07-20T06:27:56",
        "upload_time_iso_8601": "2021-07-20T06:27:56.813402Z",
        "url": "https://files.pythonhosted.org/packages/e6/0c/6dfab6bd834c3b8cec0acb8fee4cbef473302f489730dd5673c7552a3af1/pynmsnn-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0c8990b67f63c488f228d119a0701186f9460dbf0e170b28a1ad74f6da67a17a",
          "md5": "91db064fa1494fba326438c434005e35",
          "sha256": "2fc55f511316ca637e01c2386c4b6c5ea0c09bd097d315692a8eb81634b2fb33"
        },
        "downloads": -1,
        "filename": "pynmsnn-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "91db064fa1494fba326438c434005e35",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 24851,
        "upload_time": "2021-07-20T06:36:58",
        "upload_time_iso_8601": "2021-07-20T06:36:58.854458Z",
        "url": "https://files.pythonhosted.org/packages/0c/89/90b67f63c488f228d119a0701186f9460dbf0e170b28a1ad74f6da67a17a/pynmsnn-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ed7cf1d5332b0074ed4ee0d3f256de99be6ff78b15455bad0e3202570663f755",
          "md5": "88f52ae303a52ca8ceff714d941d185b",
          "sha256": "cd46c3030a05e9f23584d6bc3427b11e4f043154a85de23516f8ec1cef6fb8a9"
        },
        "downloads": -1,
        "filename": "pynmsnn-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "88f52ae303a52ca8ceff714d941d185b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 35628,
        "upload_time": "2021-07-20T06:37:01",
        "upload_time_iso_8601": "2021-07-20T06:37:01.370998Z",
        "url": "https://files.pythonhosted.org/packages/ed/7c/f1d5332b0074ed4ee0d3f256de99be6ff78b15455bad0e3202570663f755/pynmsnn-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ef106f39cd75456c1ff7b7c03ca0e5ad368905518b60a862c2ad8b382de8b786",
          "md5": "7d6886c242fab1b62f9609487b8b55a4",
          "sha256": "1ed552827e3349072dd366cb297dc1ab43188e5b98b382fd7994da33c3d83ca0"
        },
        "downloads": -1,
        "filename": "pynmsnn-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7d6886c242fab1b62f9609487b8b55a4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 26527,
        "upload_time": "2021-07-20T08:24:12",
        "upload_time_iso_8601": "2021-07-20T08:24:12.209985Z",
        "url": "https://files.pythonhosted.org/packages/ef/10/6f39cd75456c1ff7b7c03ca0e5ad368905518b60a862c2ad8b382de8b786/pynmsnn-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c062b114638f46cd59e6dcc57d081f8db13ac402343dcf8dab0695d147059c1f",
          "md5": "f833cbc2fea7819e6adb81969d46fd3a",
          "sha256": "2978f2b77a57b5cbb1a1f0cb1714db20b69dae7964f4b19b60fc6464766767e7"
        },
        "downloads": -1,
        "filename": "pynmsnn-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "f833cbc2fea7819e6adb81969d46fd3a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 40910,
        "upload_time": "2021-07-20T08:24:14",
        "upload_time_iso_8601": "2021-07-20T08:24:14.740097Z",
        "url": "https://files.pythonhosted.org/packages/c0/62/b114638f46cd59e6dcc57d081f8db13ac402343dcf8dab0695d147059c1f/pynmsnn-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ef106f39cd75456c1ff7b7c03ca0e5ad368905518b60a862c2ad8b382de8b786",
        "md5": "7d6886c242fab1b62f9609487b8b55a4",
        "sha256": "1ed552827e3349072dd366cb297dc1ab43188e5b98b382fd7994da33c3d83ca0"
      },
      "downloads": -1,
      "filename": "pynmsnn-0.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "7d6886c242fab1b62f9609487b8b55a4",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 26527,
      "upload_time": "2021-07-20T08:24:12",
      "upload_time_iso_8601": "2021-07-20T08:24:12.209985Z",
      "url": "https://files.pythonhosted.org/packages/ef/10/6f39cd75456c1ff7b7c03ca0e5ad368905518b60a862c2ad8b382de8b786/pynmsnn-0.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c062b114638f46cd59e6dcc57d081f8db13ac402343dcf8dab0695d147059c1f",
        "md5": "f833cbc2fea7819e6adb81969d46fd3a",
        "sha256": "2978f2b77a57b5cbb1a1f0cb1714db20b69dae7964f4b19b60fc6464766767e7"
      },
      "downloads": -1,
      "filename": "pynmsnn-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "f833cbc2fea7819e6adb81969d46fd3a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 40910,
      "upload_time": "2021-07-20T08:24:14",
      "upload_time_iso_8601": "2021-07-20T08:24:14.740097Z",
      "url": "https://files.pythonhosted.org/packages/c0/62/b114638f46cd59e6dcc57d081f8db13ac402343dcf8dab0695d147059c1f/pynmsnn-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}