{
  "info": {
    "author": "Microsft AdsBrain Team",
    "author_email": "fastseq@microsoft.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<h1 align=\"Center\"> <p> FastSeq </p> </h1>\n\n# Introduction\n\nFastSeq provides efficient implementations of the popular sequence models with high performance for text generation, summarization, and translation tasks. It can automatically optimize the performance of the pupular NLP toolkits (e.g. [FairSeq](https://github.com/pytorch/fairseq)) by simply `import fastseq`.\n\n# Supported Models\n\n## Supported models in [fairseq](https://github.com/pytorch/fairseq)\n\n- [x] [ProphetNet](https://github.com/microsoft/ProphetNet)\n- [x] [BART](https://arxiv.org/pdf/1910.13461.pdf)\n- [x] [Scaling Neural Machine Translation (Ott et al., 2018)](https://github.com/pytorch/fairseq/blob/master/examples/scaling_nmt/README.md)\n- [x] [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](https://github.com/pytorch/fairseq/blob/master/examples/translation_moe/README.md)\n- [x] [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](https://github.com/pytorch/fairseq/blob/master/examples/pay_less_attention_paper/README.md)\n\n\n## Supported models in [HuggingFace-Transformers](https://github.com/huggingface/transformers)\n\n- [x] [BART](https://huggingface.co/transformers/model_doc/bart.html)\n- [x] [T5](https://huggingface.co/transformers/model_doc/t5.html)\n- [ ] [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)\n- [ ] [UniLM-V1](https://github.com/microsoft/unilm)\n- [ ] [UniLM-V2](https://github.com/microsoft/unilm)\n- [ ] [ProphetNet](https://github.com/microsoft/ProphetNet)\n\n# Benchmarks\n\n## ProphetNet\n\n- CNN daily mail val data, NVIDIA-V100-16GB\n\n  |       BatchSize      |       32      |        64       |      128       |\n  |:--------------------:|:-------------:|:---------------:|:--------------:|\n  |      prophetnet      | 2.7 samples/s |  3.1 samples/s  |      OOM       |\n  | prophetnet + fastseq | 5.5 samples/s |  8.4 samples/s  | 10.3 samples/s |\n\nwith setting:\n\n```bash\n$ fastseq-generate-for-fairseq \\\n      cnn_dm_bert.1k/len-1024.bin \\\n      --path prophetnet/model.pt \\\n      --fp16 \\\n      --task translation_prophetnet \\\n      --batch-size BATCH_SIZE \\\n      --beam 4 \\\n      --num-workers 4 \\\n      --min-len 55 \\\n      --max-len-b 140 \\\n      --no-repeat-ngram-size 3 \\\n      --lenpen 2.0 \\\n      --remove-bpe \\\n      --gen-subset valid \\\n```\n\n## BART from Fairseq\n\n- CNN daily mail val data, NVIDIA-V100-16GB\n\n  |     BatchSize    |       32      |        64       |      128       |\n  |:----------------:|:-------------:|:---------------:|:--------------:|\n  | fairseq-0.9.0    | 2.7 samples/s |       OOM       |      OOM       |\n  | above + fastseq  | 9.0 samples/s | 12.5 samples/s  | 14.5 samples/s |\n\nwith setting:\n\n```bash\n$ fastseq-generate-for-fairseq \\\n      cnn_dm.1k/len-1024.bin \\\n      --path bart.large.cnn/model.pt \\\n      --fp16 \\\n      --task translation \\\n      --batch-size BATCH_SIZE \\\n      --gen-subset valid \\\n      --truncate-source  \\\n      --bpe gpt2 \\\n      --beam 4 \\\n      --num-workers 4 \\\n      --min-len 55 \\\n      --max-len-b 140 \\\n      --no-repeat-ngram-size 3 \\\n      --lenpen 2.0\n```\n\nTo get the baseline fairseq's speed number, replace `fastseq-generate-for-fairseq` by `fairseq-generate`.\n\n## BART from Transformers\n\n- CNN daily mail val data, NVIDIA-V100-16GB\n\n  |      BatchSize      |       32      |       64       |       128      |\n  |:-------------------:|:-------------:|:--------------:|:--------------:|\n  | transformers-3.0.2  | 3.4 samples/s |      OOM       |      OOM       |\n  |  above + fastseq    | 5.2 samples/s | 6.2 samples/s  | 6.4 samples/s  |\n  | transformers-2.11.0 | 2.5 samples/s |      OOM       |      OOM       |\n  |  above + fastseq    | 4.4 samples/s | 5.3 samples/s  | >5.3 samples/s |\n\n(numbers for 2.11.0 needs to be updated based on docker env.)\n\nwith setting:\n\n```bash\n$ fastseq-generate-for-transformers \\\n    facebook/bart-large-cnn \\\n    cnn_dm.1k/val.source \\\n    out.summary \\\n    --reference_path cnn_dm/val.target \\\n    --device cuda \\\n    --bs 128 \\\n    --fp16 \\\n    --score_path out.score \\\n    --task summarization\n```\n\nTo get the baseline transformers' speed number, we can either add option `--without_fastseq_opt` or use [tool](https://github.com/huggingface/transformers/tree/master/examples/seq2seq) provided in Transformers GitHub repository.\n\n## WMT from Fairseq\n- [WMT16 En-De](https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt) model\n\n  |     BatchSize    |      256       |      512       |      1024      |\n  |:----------------:|:--------------:|:--------------:|:--------------:|\n  | fairseq-0.9.0    |  84 samples/s  |      OOM       |      OOM       |\n  | above + fastseq  | 129 samples/s  |  131 samples/s |  135 samples/s |\n\n\nwith setting:\n\n```bash\n$ fastseq-generate-for-fairseq \\\n      wmt14.en-fr.joined-dict.newstest2014/ \\\n      --path wmt14.en-fr.joined-dict.transformer/model.pt \\\n      --beam 4 \\\n      --lenpen 0.6 \\\n      --remove-bpe \\\n      --batch-size 32\n```\n\nTo get the fairseq's speed number, replace `fastseq-generate-for-fairseq` by `fairseq-generate`.\n\n# Installation\n\n## Requirements\n\n- Python version >= 3.6\n- [torch](http://pytorch.org/) >= 1.4.0\n- [fairseq](https://github.com/pytorch/fairseq) >= 0.9.0\n- [transformers](https://github.com/huggingface/transformers) >= 3.0.2\n- [requets](https://pypi.org/project/requests/) >= 2.24.0\n- [absl-py](https://pypi.org/project/absl-py/) >= 0.9.0\n- [rouge-score](https://pypi.org/project/rouge-score/)\n\nIf you use fairseq or transformers, you only need to install one of them. If you use both, you need to install both.\n\n## Python package\n\n`fastseq` Python package can be directly installed with pip using\n\n```bash\n$ pip install fastseq\n```\n\n## Install from the source\n\n```bash\n$ git clone https://github.com/microsoft/fastseq\n$ cd fastseq\n$ pip install --editable ./\n```\n\n# Usage\n\n## Example\n\nOnly one line of code change is needed to use the optimizations provided by `FastSeq`.\n\n```Python\n# import fastseq at the beginning of your program\nimport fastseq\nimport torch\n\n# Download bart.large.cnn\nbart = torch.hub.load('pytorch/fairseq', 'bart.large.cnn')\n\nbart.cuda()  # use GPU\nbart.eval()  # disable dropout for evaluation\nbart.half()\n\nslines = ['FastSeq provides efficient implementations of the popular sequence models. Please visit https://github.com/microsoft/fastseq for more details.']\n\nhypotheses = bart.sample(\n    slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\n\nprint(hypotheses)\n```\n\n## Command line tool for fairseq models\nExample\n\n```bash\n$ fastseq-generate-for-fairseq \\\n    cnn_dnn/bin \\\n    --path bart.large.cnn/model.pt \\\n    --fp16 \\\n    --task translation \\\n    --batch-size 128 \\\n    --gen-subset valid \\\n    --truncate-source  \\\n    --bpe gpt2 \\\n    --beam 4 \\\n    --num-workers 4 \\\n    --min-len 55 \\\n    --max-len-b 140 \\\n    --no-repeat-ngram-size 3 \\\n    --lenpen 2.0\n```\n\n## Command line tool for transformers models\nExample\n\n```bash\n$ fastseq-generate-for-transformers \\\n    facebook/bart-large-cnn \\\n    cnn_dm/val.source \\\n    out.summary \\\n    --reference_path cnn_dm/val.target \\\n    --device cuda \\\n    --bs 128 \\\n    --fp16 \\\n    --score_path out.score \\\n    --task summarization\n```\n\n## Run tests\n\n```bash\n# run a single test.\n$ python tests/optimizer/fairseq/test_fairseq_optimizer.py\n\n# run benchmark.\n$ python tests/optimizer/fairseq/benchmark_fairseq_optimizer.py\n\n# run all the tests.\n$ python -m unittest discover -s tests/ -p '*.py'\n\n# run all the benchmarks.\n$ cd benchmarks && bash run_all_benchmarks.sh\n```\n\n## Build\n\n```bash\n# build package\n$ python setup.py sdist bdist_wheel\n```\n\n# Code Style\n\n## Python coding style\n\nChanges to Python code should conform to [PEP 8](https://www.python.org/dev/peps/pep-0008/). `yapf` can be used to help format the python code, and use `pylint` to check your Python changes.\n\n```bash\n# format the code by yapf\n$ yapf --style pep8 -i -r PYTHON_FILE/PACKAGE\n\n# run pylint check\n$ pylint --rcfile=.pylintrc  PYTHON_FILE/PACKAGE\n```\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/microsoft/fastseq",
    "keywords": "NLP NLG deep learning transformer sequence pytorch tensorflow BERT GPT GPT-2 Microsoft",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "fastseq",
    "package_url": "https://pypi.org/project/fastseq/",
    "platform": "",
    "project_url": "https://pypi.org/project/fastseq/",
    "project_urls": {
      "Homepage": "https://github.com/microsoft/fastseq"
    },
    "release_url": "https://pypi.org/project/fastseq/0.0.3/",
    "requires_dist": [
      "absl-py",
      "numpy",
      "requests",
      "packaging",
      "fairseq (>=0.9.0) ; extra == 'fairseq'",
      "torch (>=1.4.0) ; extra == 'torch'",
      "transformers (>=3.0.2) ; extra == 'transformers'"
    ],
    "requires_python": ">=3.6.0",
    "summary": "Efficient implementations of sequence models with fast performance",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8254123,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f596b17a83ebe1515d0c2c5dc9ad07db9f9af9a5aef5b818b4ed566193282361",
          "md5": "34a0729bdb881a67d04aa49a3ae0eed0",
          "sha256": "2b6684fe0ca4758bc6a7df7dbc86df8fc4fab923f12cf181725dc7d9dd3f0ace"
        },
        "downloads": -1,
        "filename": "fastseq-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "34a0729bdb881a67d04aa49a3ae0eed0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 30911,
        "upload_time": "2020-07-27T22:52:51",
        "upload_time_iso_8601": "2020-07-27T22:52:51.313293Z",
        "url": "https://files.pythonhosted.org/packages/f5/96/b17a83ebe1515d0c2c5dc9ad07db9f9af9a5aef5b818b4ed566193282361/fastseq-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7a20e3fb80c413ea19840dd090c073e47cb51a54923b871d32d8b08e57555544",
          "md5": "32548de8a0b46c76e1f65c68c5cf1617",
          "sha256": "858d3da4c57af3ddd3c2bf0f0806b9e584b4a532b85f547574e395f79d9e7bb1"
        },
        "downloads": -1,
        "filename": "fastseq-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "32548de8a0b46c76e1f65c68c5cf1617",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 25157,
        "upload_time": "2020-07-27T22:52:53",
        "upload_time_iso_8601": "2020-07-27T22:52:53.592566Z",
        "url": "https://files.pythonhosted.org/packages/7a/20/e3fb80c413ea19840dd090c073e47cb51a54923b871d32d8b08e57555544/fastseq-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "39e0be799676a88f1eb38794f9cfcb9d27d5afd3a45483fd4741b7109dce2fe2",
          "md5": "6acdd0f4b7a43550bfc2b5c20f7fe20e",
          "sha256": "00860c771a9db52309cce3d8bfc5f737c7aa215a99745160ad1e21c0a70a1152"
        },
        "downloads": -1,
        "filename": "fastseq-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6acdd0f4b7a43550bfc2b5c20f7fe20e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 31725,
        "upload_time": "2020-07-28T21:40:34",
        "upload_time_iso_8601": "2020-07-28T21:40:34.492135Z",
        "url": "https://files.pythonhosted.org/packages/39/e0/be799676a88f1eb38794f9cfcb9d27d5afd3a45483fd4741b7109dce2fe2/fastseq-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a5daf1ef494e16f3c7b898dcc9e9c4b9442b33ed8fd5618642a2c964b4ed98a5",
          "md5": "cf7bf5c2c061da42b1e1c9f2fcc6e867",
          "sha256": "3a5bfecf0991a8b1c39aa5461dc4ac563c098c61b256916164c9e4be01fbb253"
        },
        "downloads": -1,
        "filename": "fastseq-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "cf7bf5c2c061da42b1e1c9f2fcc6e867",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 26995,
        "upload_time": "2020-07-28T21:40:36",
        "upload_time_iso_8601": "2020-07-28T21:40:36.054781Z",
        "url": "https://files.pythonhosted.org/packages/a5/da/f1ef494e16f3c7b898dcc9e9c4b9442b33ed8fd5618642a2c964b4ed98a5/fastseq-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4e02f2e05af51e1fdf197c778cd3dac3f957f0391f09ded3f871ff774a386abe",
          "md5": "fd290c073fc85a2d90995e4f8fabadf7",
          "sha256": "0ad5ba8945a651c60b1921b083c9028d2a6eb58038fd097d836e2f7f6768776c"
        },
        "downloads": -1,
        "filename": "fastseq-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fd290c073fc85a2d90995e4f8fabadf7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 75321,
        "upload_time": "2020-09-23T16:15:51",
        "upload_time_iso_8601": "2020-09-23T16:15:51.861949Z",
        "url": "https://files.pythonhosted.org/packages/4e/02/f2e05af51e1fdf197c778cd3dac3f957f0391f09ded3f871ff774a386abe/fastseq-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3b54af32fac50f6c3f85fd67b455c98facc6f2f5fe404466a95a7939b7cb5b28",
          "md5": "c9a2366f5a94fdec0eb5d9111a09c8af",
          "sha256": "e1d0c9e97e01750a0d23ed46d866e0b6174a6a3e34fbbbd3de1d25465b4d362c"
        },
        "downloads": -1,
        "filename": "fastseq-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "c9a2366f5a94fdec0eb5d9111a09c8af",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 64385,
        "upload_time": "2020-09-23T16:15:53",
        "upload_time_iso_8601": "2020-09-23T16:15:53.534777Z",
        "url": "https://files.pythonhosted.org/packages/3b/54/af32fac50f6c3f85fd67b455c98facc6f2f5fe404466a95a7939b7cb5b28/fastseq-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4e02f2e05af51e1fdf197c778cd3dac3f957f0391f09ded3f871ff774a386abe",
        "md5": "fd290c073fc85a2d90995e4f8fabadf7",
        "sha256": "0ad5ba8945a651c60b1921b083c9028d2a6eb58038fd097d836e2f7f6768776c"
      },
      "downloads": -1,
      "filename": "fastseq-0.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fd290c073fc85a2d90995e4f8fabadf7",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 75321,
      "upload_time": "2020-09-23T16:15:51",
      "upload_time_iso_8601": "2020-09-23T16:15:51.861949Z",
      "url": "https://files.pythonhosted.org/packages/4e/02/f2e05af51e1fdf197c778cd3dac3f957f0391f09ded3f871ff774a386abe/fastseq-0.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3b54af32fac50f6c3f85fd67b455c98facc6f2f5fe404466a95a7939b7cb5b28",
        "md5": "c9a2366f5a94fdec0eb5d9111a09c8af",
        "sha256": "e1d0c9e97e01750a0d23ed46d866e0b6174a6a3e34fbbbd3de1d25465b4d362c"
      },
      "downloads": -1,
      "filename": "fastseq-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "c9a2366f5a94fdec0eb5d9111a09c8af",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 64385,
      "upload_time": "2020-09-23T16:15:53",
      "upload_time_iso_8601": "2020-09-23T16:15:53.534777Z",
      "url": "https://files.pythonhosted.org/packages/3b/54/af32fac50f6c3f85fd67b455c98facc6f2f5fe404466a95a7939b7cb5b28/fastseq-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}