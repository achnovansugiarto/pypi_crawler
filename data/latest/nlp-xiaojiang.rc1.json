{
  "info": {
    "author": "yongzhuo",
    "author_email": "1903865025@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "# nlp_xiaojiang\n\n\n# AugmentText\n    - 回译（效果比较好）\n    - EDA（同义词替换、插入、交换和删除）（效果还行）\n    - HMM-marko（质量较差）\n    - syntax（依存句法、句法、语法书）（简单句还可）\n    - seq2seq（深度学习同义句生成，效果不理想，seq2seq代码大都是 [https://github.com/qhduan/just_another_seq2seq] 的，效果不理想）\n\n# ChatBot\n    - 检索式ChatBot\n        - 像ES那样直接检索(如使用fuzzywuzzy)，只能字面匹配\n        - 构造句向量，检索问答库，能够检索有同义词的句子\n    - 生成式ChatBot（todo）\n        - seq2seq\n        - GAN\n\n# FeatureProject\n    - bert句向量、文本相似度\n        - bert/extract_keras_bert_feature.py:提取bert句向量特征\n        - bert/tet_bert_keras_sim.py:测试bert句向量cosin相似度\n    - normalization_util指的是数据归一化\n        - 0-1归一化处理\n        - 均值归一化\n        - sig归一化处理\n    - sim feature（ML）\n        - distance_text_or_vec:各种计算文本、向量距离等\n        - distance_vec_TS_SS：TS_SS计算词向量距离\n        - cut_td_idf：将小黄鸡语料和gossip结合\n        - sentence_sim_feature：计算两个文本的相似度或者距离，例如qq（问题和问题），或者qa（问题和答案）\n\n# run(可以在win10下,pycharm下运行)\n  - 1.创建tf-idf文件等（运行2需要先跑1）:\n                                       ```\n                                       python cut_td_idf.py\n                                       ```\n  - 2.计算两个句子间的各种相似度，先计算一个预定义的，然后可输入自定义的（先跑1）:\n                                       ```\n                                       python sentence_sim_feature.py\n                                       ```\n  - 3.chatbot_1跑起来(fuzzy检索-没)（独立）：\n                                       ```\n                                       python chatbot_fuzzy.py\n                                       ```\n  - 4.chatbot_2跑起来(句向量检索-词)（独立）：\n                                       ```\n                                       python chatbot_sentence_vec_by_word.py\n                                       ```\n  - 5.chatbot_3跑起来(句向量检索-字)（独立）：\n                                       ```\n                                       python chatbot_sentence_vec_by_char.py\n                                       ```\n  - 6.数据增强（eda)：                     python enhance_eda.py\n  - 7.数据增强（marko）:                   python enhance_marko.py\n  - 8.数据增强（translate_account）:       python translate_tencent_secret.py\n  - 9.数据增强（translate_tools）:         python translate_translate.py\n  - 10.数据增强（translate_web）:          python translate_google.py\n  - 11.数据增强（augment_seq2seq）:        先跑 python extract_char_webank.py生成数据，\n                                          再跑 python train_char_anti.py\n                                          然后跑 python predict_char_anti.py\n  - 12.特征计算(bert)（提取特征、计算相似度）:\n                      ```\n                      run extract_keras_bert_feature.py\n                      run tet_bert_keras_sim.py\n                      ```\n\n# Data\n    - chinese_L-12_H-768_A-12（谷歌预训练好的模型）\n       github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket\n       解压后就可以啦\n    - chinese_vector\n        github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket\n        - 截取的部分word2vec训练词向量（自己需要下载全效果才会好）\n        - w2v_model_wiki_char.vec、w2v_model_wiki_word.vec都只有部分\n    - corpus\n        github项目中只是上传部分数据，需要的前往链接: https://pan.baidu.com/s/1I3vydhmFEQ9nuPG2fDou8Q 提取码: rket\n        - 小黄鸡和gossip问答预料（数据没清洗）,chicken_and_gossip.txt\n        - 微众银行和支付宝文本相似度竞赛数据， sim_webank.csv\n    - sentence_vec_encode_char\n        - 1.txt（字向量生成的前100000句向量）\n    - sentence_vec_encode_word\n        - 1.txt（词向量生成的前100000句向量）\n    - tf_idf（chicken_and_gossip.txt生成的tf-idf）\n\n# requestments.txt\n    - python_Levenshtei\n        - 调用Levenshtein，我的python是3.6，\n        - 打开其源文件: https://www.lfd.uci.edu/~gohlke/pythonlibs/\n        - 查找python_Levenshtein-0.12.0-cp36-cp36m-win_amd64.whl下载即可\n    - pyemd\n        - pyemd-0.5.1-cp36-cp36m-win_amd64.whl\n    - pyhanlp\n        - 下好依赖JPype1-0.6.3-cp36-cp36m-win_amd64.whl\n\n# 参考/感谢\n* eda_chinese：[https://github.com/zhanlaoban/eda_nlp_for_Chinese](https://github.com/zhanlaoban/eda_nlp_for_Chinese)\n* 主谓宾提取器：[https://github.com/hankcs/MainPartExtractor](https://github.com/hankcs/MainPartExtractor)\n* HMM生成句子：[https://github.com/takeToDreamLand/SentenceGenerate_byMarkov](https://github.com/takeToDreamLand/SentenceGenerate_byMarkov)\n* 同义词等：[https://github.com/fighting41love/funNLP/tree/master/data/](https://github.com/fighting41love/funNLP/tree/master/data/)\n* 小牛翻译：[http://www.niutrans.com/index.html](http://www.niutrans.com/index.html)\n\n# 其他资料\n* NLP数据增强汇总:[https://github.com/quincyliang/nlp-data-augmentation](https://github.com/quincyliang/nlp-data-augmentation)\n* 知乎NLP数据增强话题:[https://www.zhihu.com/question/305256736/answer/550873100](https://www.zhihu.com/question/305256736/answer/550873100)\n* chatbot_seq2seq_seqGan（比较好用）：[https://github.com/qhduan/just_another_seq2seq](https://github.com/qhduan/just_another_seq2seq)\n* 自己动手做聊天机器人教程: [https://github.com/warmheartli/ChatBotCourse](https://github.com/warmheartli/ChatBotCourse)\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/yongzhuo/nlp_xiaojiang",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "nlp-xiaojiang",
    "package_url": "https://pypi.org/project/nlp-xiaojiang/",
    "platform": "",
    "project_url": "https://pypi.org/project/nlp-xiaojiang/",
    "project_urls": {
      "Homepage": "https://github.com/yongzhuo/nlp_xiaojiang"
    },
    "release_url": "https://pypi.org/project/nlp-xiaojiang/0.0.1/",
    "requires_dist": [
      "scikit-learn (>=0.19.1)",
      "fuzzywuzzy (>=0.17.0)",
      "openpyxl (>=2.6.2)",
      "xpinyin (>=0.5.6)",
      "gensim (>=3.7.1)",
      "jieba (>=0.39)",
      "xlrd (>=1.2.0)",
      "tensorflow (>=1.8.0)",
      "keras-bert (>=0.41.0)",
      "Keras (>=2.2.0)",
      "pandas (>=0.23.0)",
      "h5py (>=2.7.1)",
      "numpy (>=1.16.1)",
      "pyemd (==0.5.1)",
      "pathlib",
      "translate",
      "PyExecJS",
      "stanfordcorenlp"
    ],
    "requires_python": "",
    "summary": "nlp of augment、chatbot、classification and featureproject of chinese text",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 5487824,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "626c6d1e026b8c2ff2bf336d0886af5570f73ee0d74c862a640153f76636da05",
          "md5": "3ca8b0323bdc9da449d54051d17c34a2",
          "sha256": "76034429322c14b469558ac80d2bfebb50797ea0e8014c46be36e7673a56a316"
        },
        "downloads": -1,
        "filename": "nlp_xiaojiang-0.0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3ca8b0323bdc9da449d54051d17c34a2",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 217157,
        "upload_time": "2019-07-04T18:04:44",
        "upload_time_iso_8601": "2019-07-04T18:04:44.253322Z",
        "url": "https://files.pythonhosted.org/packages/62/6c/6d1e026b8c2ff2bf336d0886af5570f73ee0d74c862a640153f76636da05/nlp_xiaojiang-0.0.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "626c6d1e026b8c2ff2bf336d0886af5570f73ee0d74c862a640153f76636da05",
        "md5": "3ca8b0323bdc9da449d54051d17c34a2",
        "sha256": "76034429322c14b469558ac80d2bfebb50797ea0e8014c46be36e7673a56a316"
      },
      "downloads": -1,
      "filename": "nlp_xiaojiang-0.0.1-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3ca8b0323bdc9da449d54051d17c34a2",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 217157,
      "upload_time": "2019-07-04T18:04:44",
      "upload_time_iso_8601": "2019-07-04T18:04:44.253322Z",
      "url": "https://files.pythonhosted.org/packages/62/6c/6d1e026b8c2ff2bf336d0886af5570f73ee0d74c862a640153f76636da05/nlp_xiaojiang-0.0.1-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}