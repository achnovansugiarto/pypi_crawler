{
  "info": {
    "author": "Ulf Hermjakob",
    "author_email": "ulf@isi.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3 :: Only",
      "Topic :: Text Processing",
      "Topic :: Text Processing :: Filters",
      "Topic :: Text Processing :: General",
      "Topic :: Text Processing :: Linguistic",
      "Topic :: Utilities"
    ],
    "description": "# wildebeest\n\nThe wildebeest scripts investigate, repair and normalize text for a wide range of issues at the character level.\n\n**wb-ana** (or wb_analysis.py)\n\nThis script searches a tokenized text for a range of potential problems, \nsuch as UTF-8 encoding violations, control characters, zero-with characters, \nletters/numbers/punctuation/letter-modifiers from various scripts \n(e.g. Latin and Cyrillic), tokens with letters from different scripts, \nXML tokens, tokens with certain punctuation of interest, orphan letter modifiers, \nnon-canonical character combinations.\n\n**wb-norm** (or wb_normalize.py)\n\nThis script automatically corrects some of the issues raised by wb-ana.\nThe script can repair common encoding errors, normalize characters into their UTF8-canonical form, map digits and some\npunctuation to ASCII, delete many non-printable characters and perform other repair, normalization and cleaning steps.\nA few steps are specific to Pashto, Farsi, or Devanagari (Hindi etc.).\nNormalization steps can be activated *√† la carte*.\n\n## Installation\n\n<details>\n<summary>Click here for installation info</summary>\n\n```bash\n# Install from PyPi:\npip install wildebeest-nlp\n\n# Alternatively, pip-install from GitHub master branch:\npip install git+https://github.com/uhermjakob/wildebeest.git\n\n# Alternatively, clone GitHub, which might be useful for editing/development:\ngit clone https://github.com/uhermjakob/wildebeest.git\n# or git clone git://github.com/uhermjakob/wildebeest.git\ncd wildebeest\npip install --editable .   # run it from dir having setup.py\n```\n\nA pip-install will provide commands `wb-norm` and `wb-ana` as well as their alternate forms `wb_normalize.py` and `wb_analysis.py`.\n\nAfter a regular `git clone` (without pip-install), in order to be able to call the Python scripts `wb_normalize.py` and `wb_analysis.py`, make sure that:\n1. `wb_normalize.py` and `wb_analysis.py` are executable (i.e. 'x' mode bits are set)\n2. your $PYTHONPATH includes the directory in which this README file resides in (\"outer wildebeest\") and\n3. your $PATH includes the directory that includes `wb_normalize.py` and `wb_analysis.py` (\"inner wildebeest\")\n\n</details>\n  \n## wb-norm (or wb_normalize.py)\n\nThe script repairs common encoding errors, normalizes characters into their canonical form,\ndeletes many non-printable characters and performs other repair, normalization and cleaning steps.\nThe script can be parameterized to include or exclude specific normalization steps (e.g. whether\nor not to map non-ASCII digits and punctuation to ASCII).\nA few steps are specific to Pashto, Farsi, or Devanagari (Hindi etc.).\n\n### Usage &nbsp; (click below for details)\n<details>\n<summary>CLI to normalize a file: <code>wb-norm</code> or <code>wb_normalize.py</code></summary>\n\n```\nusage: wb-norm [-h] [-i INPUT-FILENAME] [-o OUTPUT-FILENAME] [--lc LANGUAGE-CODE] [--skip NORM-STEPS]\n               [--add NORM-STEPS] [--all] [--all-except NORM-STEPS] [--only NORM-STEPS] [-v] [--version]\n# or wb_normalize.py [-h] ...\n\nNormalizes and cleans a given text\n\noptions:\n  -h, --help            show this help message and exit\n  -i INPUT-FILENAME, --input INPUT-FILENAME\n                        (default: STDIN)\n  -o OUTPUT-FILENAME, --output OUTPUT-FILENAME\n                        (default: STDOUT)\n  --lc LANGUAGE-CODE    ISO 639-3, e.g. 'fas' for Persian\n  --skip NORM-STEPS     perform all default normalization/cleaning steps except those specified in comma-separated list\n                        (default normalization/cleaning steps: repair-encoding-errors,del-surrogate,del-ctrl-char,\n                        del-tatweel,core-compat,pres-form,hangul,repair-combining,combining-compose,combining-decompose,\n                        repair-xml,repair-url-escapes)\n  --add NORM-STEPS      perform all default normalization/cleaning steps plus those specified in comma-separated list \n                        (non-default normalization/cleaning steps: del-zero-width,del-arabic-diacr,del-hebrew-diacr,\n                        ligatures,signs-and-symbols,cjk,width,font,small,vertical,enclosure,punct,punct-dash,punct-arabic,\n                        punct-cjk,punct-greek,punct-misc-f,space,digit,arabic-char,farsi-char,pashto-char,georgian-char,\n                        look-alike,repair-token)\n  --all                 perform all normalization/cleaning steps, i.e. repair-encoding-errors,del-surrogate,\n                        del-zero-width,del-ctrl-char,del-tatweel,del-arabic-diacr,del-hebrew-diacr,core-compat,pres-form,\n                        ligatures,signs-and-symbols,cjk,width,font,small,vertical,enclosure,hangul,repair-combining,\n                        combining-compose,combining-decompose,punct,punct-dash,punct-arabic,punct-cjk,punct-greek,\n                        punct-misc-f,space,digit,arabic-char,farsi-char,pashto-char,georgian-char,look-alike,repair-xml,\n                        repair-url-escapes,repair-token\n  --all-except NORM-STEPS\n                        perform all normalization/cleaning steps except those specified in comma-separated list\n  --only NORM-STEPS     perform only normalization/cleaning steps specified in comma-separated list\n  -v, --verbose         write change log etc. to STDERR\n  --version             show program's version number and exit\n```\nExamples:\n```\nwb-norm -h  # for full usage info\nwb-norm --version\ncd `pip show wildebeest-nlp | grep ^Location | cut -d ' ' -f 2`  # go to directory where wildebeest-nlp is installed\ncd wildebeest/test/data\nwb-norm --lc fas -i wildebeest-test.txt -o wildebeest-test-norm.txt\nwb-norm --lc fas --verbose --skip del-ctrl-char,del-tatweel < wildebeest-test.txt > wildebeest-test-norm-custom.txt\nwb-norm --all < wildebeest-test.txt > wildebeest-test-norm-all.txt\nwb-norm --all-except del-arabic-diacr,del-hebrew-diacr < wildebeest-test.txt\nwb-norm --only del-arabic-diacr,del-hebrew-diacr < wildebeest-test.txt\nwb-norm --add del-arabic-diacr,del-hebrew-diacr --skip del-ctrl-char,del-tatweel < wildebeest-test.txt\n```\n<details>\n<summary>Same for alternate script name wb_normalize.py</summary>\n\n```\nwb_normalize.py -h  # for full usage info\nwb_normalize.py --version\ncd `pip show wildebeest-nlp | grep ^Location | cut -d ' ' -f 2`\ncd wildebeest/test/data\nwb_normalize.py --lc fas -i wildebeest-test.txt -o wildebeest-test-norm.txt\nwb_normalize.py --lc fas --verbose --skip del-ctrl-char,del-tatweel < wildebeest-test.txt > wildebeest-test-norm-custom.txt\nwb_normalize.py --all < wildebeest-test.txt > wildebeest-test-norm-all.txt\nwb_normalize.py --all-except del-arabic-diacr,del-hebrew-diacr < wildebeest-test.txt\nwb_normalize.py --only del-arabic-diacr,del-hebrew-diacr < wildebeest-test.txt\nwb_normalize.py --add del-arabic-diacr,del-hebrew-diacr --skip del-ctrl-char,del-tatweel < wildebeest-test.txt\n```\n</details>\n\nNote: For robustness regarding input files that do not fully conform to UTF8, please use -i (rather than STDIN), as it includes UTF8-encoding error handling.\n</details>\n\n<details>\n<summary>norm_clean_string (Python function call to normalize a string)</summary>\n\nNote: When working on a clone (as opposed to a pip-install), please make sure that your $PYTHONPATH includes the directory in which this README file resides.\n```python \nfrom wildebeest.wb_normalize import Wildebeest\nwb = Wildebeest()\nht = wb.build_norm_step_dict(base='ALL')  # base values: 'NONE', 'DEFAULT', 'ALL' (normalization steps)\n# ht = wb.build_norm_step_dict()  # defaults: base = 'DEFAULT', skip = None, add = None\n# ht = wb.build_norm_step_dict(base='NONE', add=['digit', 'enclosure'])  # normalize only digits (to ASCII) and enclosures\n# ht = wb.build_norm_step_dict(base='DEFAULT', skip=['del-tatweel'], add=['digit', 'space'])\n# ht = wb.build_norm_step_dict(base='ALL', skip=['punct-dash', 'enclosure', 'del-arabic-diacr'])\nwb.load_look_alike_file()           # optional\nprint(wb.norm_clean_string('üÑê‚Ä¶25km√Ç¬≤', ht, lang_code='eng'))\nprint(wb.norm_clean_string('‡≥ß‡≥Ø‡≥®‡≥©', ht, lang_code='kan'))\n``` \n</details>\n\n### Normalization Steps\n\nThe script can perform a wide variety of normalization steps.\n\n* 12 normalization steps are performed by default, including basic character repair and UTF8 encoding normalization. The default is generally suitable for applications that largely need to preserve the original text.\n* Another 25 normalization steps are available through options `--add (list of steps)`, `--all`, `--all-except (list of steps)`. The `--all` and `--all-excpet` settings are suitable for many NLP applications.\n* Default normalization steps can be disabled by option `--skip (list of steps)`.\n* Option `--only (list of steps)` applies only the normalization steps listed (without default normalization steps unless explicitly listed).\n* Option `--all-except (list of steps)` is equivalent to `--all --skip (list of steps)`\n\n<details>\n<summary>List of normalization steps included by default</summary>\n\n* `repair-encoding-errors` The script generally expects input encoded in UTF8. However, it will recognize and repair some common text encoding errors:\n  -  (Some) text is still encoded in Windows1252 or Latin1. Any byte that is not part of a well-formed UTF8 character will be interpreted as a Windows1252 character (and mapped to UTF8). This includes printable Latin1 characters as a subset.\n  - Text in Windows1252 was incorrectly converted to UTF8 by a Latin1-to-UTF8 converter. This maps Windows1252 characters \\x80-\\x9F to \\u0080-\\uu009F, which is the Unicode block of C1 control characters. These C1 control characters are extremely rare, and so our script will interpret such C1 control characters as ill-converted Windows1252 characters, as do many major software applications such as Google Chrome, Microsoft Outlook, Github (text files) and PyCharm (where they are often displayed in a slightly different form).\n  -  Text in Windows1252 or Latin1 was converted twice, using some combination of Latin1-to-UTF8 converter and Windows1252-to-UTF converter; or a file already in UTF8 was incorrectly subjected to another conversion. Sample *wildebeest* repair:\n    - Input: Don√¢¬Ä¬ôt tell your √¢¬Ä¬úfianc√É¬©√¢¬Ä¬ù √¢¬Ä¬î Sch√É¬∂ne Gr√É¬º√É¬üe aus M√É¬§hren√¢¬Ä¬¶ √¢¬Ä¬ì Ma s√Ö¬ìur trouve √É¬ßa √Ç¬´b√É¬™te√Ç¬ª. √Ç¬°Co√É¬±o! √¢¬Ç¬¨50 √¢¬Ä¬¢ 25km√Ç¬≤ √¢¬Ä¬¢ √Ç¬Ω√Ç¬µm\n    - Output: Don‚Äôt tell your ‚Äúfianc√©‚Äù ‚Äî Sch√∂ne Gr√º√üe aus M√§hren‚Ä¶ ‚Äì Ma s≈ìur trouve √ßa ¬´b√™te¬ª. ¬°Co√±o! ‚Ç¨50 ‚Ä¢ 25km¬≤ ‚Ä¢ ¬Ω¬µm\n* `del-surrogate` deletes surrogate characters (representing non-UTF8 characters in input), alternative/backup to windows-1252\n* `del-ctrl-char` deletes control characters (expect tab and linefeed), some variation selectors\n* `del-tatweel` deletes Arabic tatweel (a text alignment character that increases the distance between Arabic letters)\n* `core-compat` normalizes Hangul Compatibility characters to Unicode standard Hangul characters\n* `pres-form` e.g. maps from presentation form (isolated, initial, medial, final) to standard form\n* `hangul` combine Hangul jamos onto Hangul syllables\n* `repair-combining` e.g. order of nukta/vowel-sign\n* `combining-compose` e.g. applies combining-modifiers to preceding character, e.g. oÃà (o +  Ãà) -> √∂\n* `combining-decompose` e.g. for some Indian characters, splits off Nukta\n* `repair-xml` e.g. repairs multi-escaped tokens such as &amp;quot; or &amp;amp;#x200C;\n* `repair-url-escapes` e.g. repairs multi-escaped url substrings such as Jo%25C3%25ABlle_Aubron\n</details>\n\n<details>\n<summary>List of additional normalization steps included by --all option</summary>\n\n* `del-zero-width` deletes zero-width characters, byte order mark, directional marks, join marks\n* `arabic-char` to Arabic canonical forms, e.g. maps Farsi kaf/yeh to Arabic versions\n* `farsi-char` to Farsi canonical forms, e.g. maps Arabic yeh, kaf to Farsi versions\n* `pashto-char` to Pashto canonical forms, e.g. maps Arabic kaf to Farsi version\n* `georgian-char` to Georgian canonical forms, e.g. to standard script, map archaic characters\n* `ligatures` e.g. decomposes non-Arabic ligatures (e.g. ƒ≥, Ô¨É, «Ñ, Ô¨ì)\n* `signs-and-symbols` e.g. maps symbols (e.g. kappa symbol) and signs (e.g. micro sign ¬µ)\n* `cjk` e.g. CJK square composites (e.g. „ãÄ„èæ)\n* `width` e.g. maps fullwidth and halfwidth characters to ASCII, e.g. Ôº° to A\n* `font` maps font-variations characters such as ‚ÑÇ, ‚Ñπ, ùíú to regular characters\n* `small` maps small versions of characters to normal versions, such as small ampersand Ôπ† to regular &\n* `vertical` maps vertical versions of punctuation characters with normal horizontal version, such as vertical em-dash Ô∏± to horizontal em-dash ‚Äî\n* `enclosure` decomposes circled, squared and parenthesized characters, e.g. üÑê to (A)\n* `del-arabic-diacr` e.g. deletes optional Arabic diacritics such as fatha, damma, kasra\n* `del-hebrew-diacr` e.g. deletes Hebrew points\n* `digit` e.g. maps decimal-system digits of 54 scripts to ASCII digits\n* `punct` e.g. maps ellipsis ‚Ä¶ to periods ... and two-dot-lead ‚Ä• to ..; a few math symbols ‚à≠; ‚íõ üÑÜ \n* `punct-dash` e.g. maps various dashes, hyphens, minus signs to ASCII hyphen-minus\n* `punct-arabic` e.g. Arabic exclamation mark etc. to ASCII equivalent\n* `punct-cjk` e.g. Chinese Ideographic Full Stop etc. to ASCII equivalent\n* `punct-greek` e.g. Greek question mark etc. to ASCII equivalent\n* `punct-misc-f` e.g. Tibetan punctuation to ASCII equivalent\n* `space` e.g. maps non-zero spaces to normal space\n* `look-alike` normalizes Latin/Cyrillic/Greek look-alike characters, e.g. Latin character A to Greek Œë (capital alpha) in otherwise Greek word\n* `repair-token` e.g. splits +/-/*/digits off Arabic words; maps not-sign inside Arabic to token-separating hyphen\n</details>\n\n## wb-ana (or wb_analysis.py)\n\nScript searches a tokenized text for a range of potential problems,\nsuch as UTF-8 encoding violations, control characters, zero-with characters,\nletters/numbers/punctuation/letter-modifiers from various scripts,\ntokens with letters from different scripts, XML tokens, tokens with certain\npunctuation of interest, orphan letter modifiers, non-canonical character\ncombinations.\n\n### Usage\n\n<details>\n<summary>CLI to analyze a file: <code>wb-ana</code> or <code>wb_analysis.py</code> </summary>\n\n```\nusage: wb-ana  [-h] [-i INPUT-FILENAME] [--batch BATCH] [-s] [-o OUTPUT-FILENAME] [-j JSON-OUTPUT-FILENAME] [--file_id FILE_ID]\n               [--lc LANGUAGE-CODE] [-v] [-pb] [-n MAX_CASES] [-x MAX_EXAMPLES] [-r REF-FILENAME] [--version]\n# or wb_analysis.py  [-h] ... \n  \nAnalyzes a given text for a wide range of anomalies\n\noptions:\n  -h, --help            show this help message and exit\n  -i INPUT-FILENAME, --input INPUT-FILENAME\n                        (default: STDIN)\n  --batch BATCH_DIR     Directory with batch of input files (BATCH_DIR/*.txt)\n  -s, --summary         single summary line per file\n  -o OUTPUT-FILENAME, --output OUTPUT-FILENAME\n                        (default: STDOUT)\n  -j JSON-OUTPUT-FILENAME, --json JSON-OUTPUT-FILENAME\n                        (default: None)\n  --file_id FILE_ID\n  --lc LANGUAGE-CODE    ISO 639-3, e.g. 'fas' for Persian\n  -v, --verbose         write change log etc. to STDERR\n  -pb, --progress_bar   Show progress bar\n  -n MAX_CASES, --max_cases MAX_CASES\n                        max number of cases per group\n  -x MAX_EXAMPLES, --max_examples MAX_EXAMPLES\n                        max number of examples per line\n  -r REF-FILENAME, --ref_id_file REF-FILENAME\n                        (optional file with sentence reference IDs)\n  --version             show program's version number and exit\n```\n\nExamples:\n```\nwb-ana --help\necho 'H–µllŒø!' | wb-ana                  # 'H–µllŒø!' mischievously includes a Cyrillic and a Greek character\necho 'H–µllŒø!' | wb-norm --all | wb-ana  # different result\ncd `pip show wildebeest-nlp | grep ^Location | cut -d ' ' -f 2`  # go to directory where wildebeest-nlp is installed\ncd wildebeest/test/data\nwb-ana -i hello.txt\nwb-ana -i wildebeest-test.txt -o wildebeest-test-out\nwb-ana --batch phrasebook -s -o phrasebook-dir-out\nwb-ana -i phrasebook/deu.txt -r phrasebook/eng.txt -o phrasebook-deu-out\nwb-ana -i wildebeest-test-invalid-utf8.txt\n```\n\n<details>\n<summary>Same for alternate script name wb_analysis.py</summary>\n\n```\nwb_analysis.py --help\necho 'H–µllŒø!' | wb_analysis.py\necho 'H–µllŒø!' | wb_normalize.py --all | wb_analysis.py\ncd `pip show wildebeest-nlp | grep ^Location | cut -d ' ' -f 2`\ncd wildebeest/test/data\nwb_analysis.py -i hello.txt\nwb_analysis.py -i wildebeest-test.txt -o wildebeest-test-out\nwb_analysis.py --batch phrasebook -s -o phrasebook-dir-out\nwb_analysis.py -i phrasebook/deu.txt -r phrasebook/eng.txt -o phrasebook-deu-out\nwb_analysis.py -i wildebeest-test-invalid-utf8.txt\n```\n</details>\n</details>\n\n<details>\n<summary>wildebeest.wb_analysis.process (Python function call to analyze a string, a list of strings, or a file)</summary>\n\nNote: When working on a clone (as opposed to a pip-install), please make sure that your $PYTHONPATH includes the directory in which this README file resides.\n```python \nimport pprint\nimport sys\nimport wildebeest.wb_analysis as wb_ana\nwb = wb_ana.process(string=\"H–µllŒø!\")   # \"H–µllŒø!\" mischievously includes a Cyrillic and a Greek character\nwb.pretty_print(sys.stdout)            # custom pretty-print with OVERVIEW and DETAIL sections to STDOUT\npprint.pprint(wb.analysis)             # generic pretty-print\n```\n  \n```python \nimport wildebeest.wb_analysis as wb_ana\nwb = wb_ana.process(strings=[\"H–µllŒø!\", \"Tsch√º√ü\"])\nprint(wb.analysis)  # print analysis object (nested dictionary)\n```\n\nAssuming an input file `corpus.txt`, e.g. built by:\n```bash\nprintf 'H–µllŒø!\\nTsch√º√ü\\n' > corpus.txt\n```\n  \n```python \nimport wildebeest.wb_analysis as wb_ana\nwb = wb_ana.process(in_file='corpus.txt')\nprint(wb.analysis)\n```\n  \n```python \nimport wildebeest.wb_analysis as wb_ana\nwith open(f'out.txt', 'w') as out, open('out.json', 'w') as json:\n    wb_ana.process(in_file='corpus.txt', pp_output=out, json_output=json)\n```  \n</details>\n\n### wb-analysis.pl\n\nOld Perl script searches a tokenized text for a range of potential problems,\nsuch as UTF-8 encoding violations, control characters, non-ASCII punctuation,\ncharacters from a variety of language groups, very long tokens, unsplit 's,\nunsplit punctuation, script mixing; split URLs, email addresses, filenames,\nXML tokens.\n\nReports the number of instances in each category and give examples.\nCurrently available: wildebeest_analysis.pl (Perl) v2.6 (April 28, 2021)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/uhermjakob/wildebeest",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/uhermjakob/wildebeest",
    "keywords": "machine translation,datasets,NLP,natural language processing,computational linguistics",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "wildebeest-nlp",
    "package_url": "https://pypi.org/project/wildebeest-nlp/",
    "platform": "any",
    "project_url": "https://pypi.org/project/wildebeest-nlp/",
    "project_urls": {
      "Download": "https://github.com/uhermjakob/wildebeest",
      "Homepage": "https://github.com/uhermjakob/wildebeest"
    },
    "release_url": "https://pypi.org/project/wildebeest-nlp/0.9.2/",
    "requires_dist": [
      "regex (>=2021.8.3)",
      "tqdm (>=4.40)",
      "unicodeblock (>=0.3.1)",
      "wheel (>=0.38.4)"
    ],
    "requires_python": ">=3.8",
    "summary": "The wildebeest scripts investigate, repair and normalize a wide range of text file problems at the character level, e.g. encoding errors, normalization of characters into their canonical form, mapping digits and some punctuation to ASCII, deletion of some non-printable characters.",
    "version": "0.9.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15828932,
  "releases": {
    "0.9.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ba7447844ceeff7a267899b0889c908eb5a35cf1e918b70cfb8a13774a6491ab",
          "md5": "59a7bb7c00c8732810b7d7c729b0f92e",
          "sha256": "090754039aee379bc71295512cc642a44758499ce9556c4b2fec4ddd124d2f13"
        },
        "downloads": -1,
        "filename": "wildebeest_nlp-0.9.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "59a7bb7c00c8732810b7d7c729b0f92e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 283468,
        "upload_time": "2022-11-20T05:02:35",
        "upload_time_iso_8601": "2022-11-20T05:02:35.366096Z",
        "url": "https://files.pythonhosted.org/packages/ba/74/47844ceeff7a267899b0889c908eb5a35cf1e918b70cfb8a13774a6491ab/wildebeest_nlp-0.9.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "81c5037d4437c3ba7f9e560398c5913ace501959800cbd8d2b6a6f3c3740e52c",
          "md5": "2eba4f320cf11cb737d98815ebe7a43b",
          "sha256": "3c09c1b15999cd98e3db2f53b6c76949593efe2f0f33b997de479f3955835642"
        },
        "downloads": -1,
        "filename": "wildebeest-nlp-0.9.2.tar.gz",
        "has_sig": false,
        "md5_digest": "2eba4f320cf11cb737d98815ebe7a43b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 273658,
        "upload_time": "2022-11-20T05:02:37",
        "upload_time_iso_8601": "2022-11-20T05:02:37.504819Z",
        "url": "https://files.pythonhosted.org/packages/81/c5/037d4437c3ba7f9e560398c5913ace501959800cbd8d2b6a6f3c3740e52c/wildebeest-nlp-0.9.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ba7447844ceeff7a267899b0889c908eb5a35cf1e918b70cfb8a13774a6491ab",
        "md5": "59a7bb7c00c8732810b7d7c729b0f92e",
        "sha256": "090754039aee379bc71295512cc642a44758499ce9556c4b2fec4ddd124d2f13"
      },
      "downloads": -1,
      "filename": "wildebeest_nlp-0.9.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "59a7bb7c00c8732810b7d7c729b0f92e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 283468,
      "upload_time": "2022-11-20T05:02:35",
      "upload_time_iso_8601": "2022-11-20T05:02:35.366096Z",
      "url": "https://files.pythonhosted.org/packages/ba/74/47844ceeff7a267899b0889c908eb5a35cf1e918b70cfb8a13774a6491ab/wildebeest_nlp-0.9.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "81c5037d4437c3ba7f9e560398c5913ace501959800cbd8d2b6a6f3c3740e52c",
        "md5": "2eba4f320cf11cb737d98815ebe7a43b",
        "sha256": "3c09c1b15999cd98e3db2f53b6c76949593efe2f0f33b997de479f3955835642"
      },
      "downloads": -1,
      "filename": "wildebeest-nlp-0.9.2.tar.gz",
      "has_sig": false,
      "md5_digest": "2eba4f320cf11cb737d98815ebe7a43b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 273658,
      "upload_time": "2022-11-20T05:02:37",
      "upload_time_iso_8601": "2022-11-20T05:02:37.504819Z",
      "url": "https://files.pythonhosted.org/packages/81/c5/037d4437c3ba7f9e560398c5913ace501959800cbd8d2b6a6f3c3740e52c/wildebeest-nlp-0.9.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}