{
  "info": {
    "author": "Restart Partners",
    "author_email": "lucas@restart.us",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# COVID-19 Restart the World\n\nGetting the world up and running again isn't going to be easy. This project is\na start at making that easier. Feel free to take pieces and contribute. While\nwe wait for an effective treatment or a vaccine, we need all these pieces to\nsave lives from the infection and get the economy back to work.\n\nIt has four major components:\n\n1. Modeling the Need. Model the entire world of the COVID-19 response from the\n   epidemeological models, to the consumer confidence model and finally to the\nsupply chain models that estimate how much Personal Protective Equiptment (PPE),\nTest kits and other equipment is needed to make it work.\n2. Providing the Material. This so confusing that having a set of templates that\n   you can embed into any website to provide the latest training, the condensed\nrecommendation is critical. And then on the backend a marketplace that is easy\nto setup so you can buy or download what you need in one step.\n3. Changing Norms. No amount of equipment works without changing how people\n   work, live and play. This is a set of behavioral models and content that\nworks against the different segments that need to be protected. From children,\nto the elderly to specific groups that are disportionately effect, getting the\nright message at the right time is key.\n4. Community. Getting the entire network of public, private and non-profit\n   organizations working together.\n\n## Table of Contents\n\n1. [Conceptual Diagram](#conceptual-diagram)\n2. [Project Management](#project-management)\n3. [Directory Layout](#directory-layout)\n4. [Versions and Releases](#versions-and-releases)\n    - [Release Points](#release-points)\n    - [Release Notes](#release-notes)\n    - [Excel Bug Notes](#excel-bug-notes)\n5. [The Various Documents](#the-various-documents)\n6. [Data Sources](#data-sources)\n7. [Installation Guidelines](#mac-installation-guidelines)\n    - [Using Git LFS and XLTrail](#using-git-lfs-and-xltrail)\n    - [Gitpod.io](#gitpod-io)\n8. [Other Repos](#other-repos)\n9. [Release Schedule](#release-schedule)\n10. [GitHub, XLTrail, and Git LFS](#github-xltrail-and-git-lfs)\n11. [Notes on Using Excel](#notes-on-using-excel)\n\n## Conceptual diagram\n\nWe are building a system that calculates from several classes a system that\nlooks like:\n\n![Conceptual](doc/conceptual.dot.jpg)\n\n## Project Management\n\nThe overall project is managed with [Github\nProjects](https://github.com/restartus/covid-projection/projects/1). The process\nworks this way:\n\n1. We assign items to people for the weekly sprint. Check the board to see what\n   issues you own\n2. The syntax of each issue is in brackets Estimated time in hours to complete\n   Item name (actual used), so for instance `[3] Insert new Class (2)` which\n   means it will\ntake 3 hours to complete and you've used 2.\n3. When estimating times, we are using the Fibonacci series as a rough guide so\n   assign hours as when estimating as `0.5, 1, 2, 3, 5, 8, 13, 21` for how many\nhours something will take.\n4. We don't use that for for much now but it is a good way to see how accurate\n   we are. You should try to turn on your 30 minute announcment on and see how\nlog it takes.\n\n## Directory Layout\n\nThe directory layout has a few major areas:\n\n- [data](data). This is where all the raw data is kept. Right now, this uses Git\n  LFS so that we have version control and this works since the data sets are\nrelatively small at at most a few GB. You do need git lfs installed to read\nthis.\n- [bin](bin). This is where the developer tools live. They are mainly a subset\n  ported from @richtong [richtong](https://github.com/richtong/src) . The\n  most important is install.sh which should install the development environment\n  for Mac (for sure), Linux (hopefully) and Windows is in development. Our\n  standard dev environment is Mac, so let @richtong know if you want to become\n  a maintainer for the other builds.\n- [lib](lib). Used by bin, this gives a standard development environment with\n  some standard variables like SOURCE_DIR you can use everywhere\n- [model](model). This is where the new V2 model lives\n- [nb](nb). This is for experiments and is our poor man's Jupyter Hub for\n  notebooks\n- We do have some Excel sheets at the top, there is technical debt to fix the\n  Github actions to pull data from below, but they are named files that you copy\nin Athena sheets\n\n## Installation and Usage\n\nTo install with pip:\n\n```\npip install restart\n```\n\nSimple example of analysis for state of California:\n\n```python3\nfrom restart import RestartModel\n\nrestart = RestartModel(config_dir='restart', population='oes', state='California')\n\nmodel = restart.model  # this contains all the data now\n```\n\n## Versions and Releases\n\nThe main release scheme is to alternate between adding features (the v1, v3,...)\nand then solving technical debt issues and solidifying things, you can think of\nthese a v1, v1.x, v2, v2.x, etc\n\nOur first v1 models (codenamed Athena) are in the [excel](excel) these are\nExcel spreadsheets and they have now stabilized with a series of 1.x releases.\nAll versions are kept there.\n\nOur next generation or v2 models (codenamed Balsa) are the conversion to Python\nand implement the Surge models once again and then will add additional\nextensions. Most of this work lives in the Jupyter and src subdirectories.\n\nOur v2.x models (codenamed Concrete) will be a technical catchup release where we\nput in the CD/CI features\n\nAs with all semvar compliant systems, major versions v1, v2,... maintain the\nsame interface, that is they produce the same output and are called the same\n\n## Release Points\n\nThe system release two spreadsheets right now as of v1.x at\n[releases](https://github.com/restartus/restart/releases). These are right taken\nfrom the files at the root and renamed appropriately. So when you want to do\n\n- covid-who-surge-washington.xlsx. This is the copied latest file that is the\n- large model for State of Washington including SOC\n- covid-who-surge-single.xlsx. This is the\n  template is for a new client. It is not reentrent, so for each new client,\n  make a copy\n- covid-who-surge-single-cook.xlsx. Thsi is the first release that uses the\n  single for Cook county restaurants\n\n## Release Notes\n\n- v1.4.5 Removes the data table from main washington model and creates a county\n  only model\n- v1.4.4 First cut of the stockpile model\n- v1.4.2. This has fixes for disinfection calculation and introduction to a\n  single sheet models\n- v1.3.1. Fixes the the washington surge and has the new york city surge\n\n## Excel Bug Notes\n\nIf you put a data table inside the system, you will get a external content\nerror. To fix this, you should go to the Data tab and look at connections. This\nis the place to remove external connections\n[External](https://answers.microsoft.com/en-us/msoffice/forum/all/excel-for-mac-external-data-connections-have-been/03d3efa9-d540-4b00-8bc8-a06ddb7c4ea1)\n\n## The Various Documents\n\n- [README.md](README.md) You are reading this, the basic introduction\n- [INTRODUCTION.md](INTRODUCTION.md). The model and how it works at a high level\n- [RESEARCH.md](RESEARCH.md). Various call reports on new ideas\n\n## Data sources\n\n### Apple Mobility\n\nA regularly published CSV on mobility data\n[Apple](https://www.apple.com/covid19/mobility)\n\n### Google Mobility\n\nA regular data source from [Google](https://www.google.com/covid19/mobility/)\n\n## The PowerBI cube\n\nThe supporting documents needed are mainly in PowerBI.\n- [OCC Based Employment](https://azure.microsoft.com/email/?destination=https%3A%2F%2Fapp.powerbi.com%2FMobileRedirect.html%3Faction%3DOpenReport%26reportObjectId%3De9e58394-451a-429b-aed1-20ef6e317dc4%26ctid%3D1e355c04-e0a4-42ed-8e2d-7351591f0ef1%26groupObjectId%3Df2f0cf78-3695-4dd6-a6fd-cf2063d3195c%26OpenAppFromWindowsPCAndTablet%3Dfalse%26emailSource%3DReportInvitation&p=bT0xN2RlMjVkYy04ODg4LTQwYmYtOTJmYy1iNDEwODVlNDAzZDEmdT1hZW8mbD1Nb2JpbGVSZWRpcmVjdC5odG1s)\n\n## Installation Guidelines\n\n@richtong will shortly generate instructions for Windows machines, but here is\nan outline of steps: First you need to install [Homebrew](https://brew.sh/) so\nthat you can automatically install the rest of the stuff. This is done with\nTerminal and you have to run a machine\nincantation. Just copy and paste in the next line\n\n```shell\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\"\n```\n\nNow that you have done that, then run the following commands which installs the\nright pieces for you:\n\n```shell\nbrew install git git-lfs\n# You want this in a dedicated directory\nmkdir -p ~/ws\n```\n\nNow you need to create a logon at [GitHub](https://github.com) and then ask\n@richtong for rights to get into this repo then find a nice place for the\nforecast and run these commands to get all the latest spreadsheets directly into\nyour machine\n\n### Using Git LFS and XLTrail\n\nYou will need to install Git LFS as the models are quite large at 200MB and up\nwith and it will really clog your machine:\n\n```shell\ngit lfs install\n# to get whatever version you need, just use the version number\ngit checkout v1.0\n# to get the latest daily developmen release\n```\n\nAlso we support the use of release tags, the current versions are:\n\n- v1.0. This version went to the State of Washington for their surge mask\n  forecasting and is over at the Office of Financial Management. This contains\nall the data in a single sheet and runs on Excel or Google Sheets.\n- rich-dev. This is the development fork, please use with caution.\n\n```shell\n# assuming you put this into a directory called ws\ncd ~/ws\ngit clone https://github.com/restartpartners/covid-forecast\n# You will be asked to enter your user name and password\ncd covid-forecast\n# this says get big files like Excel spreadsheets\ngit lfs init\n# pull means get the latest from the cloud\ngit pull\n```\n\nAnd that is all there is to it, rather than asking for one at a time. You can\nthen edit the sheets directly as often as you like.\n\n```shell\n# This gives you your own private copy so no one can mess with your stuff\n# so if the branch can be any name but by convention it is typically your\n# name and then a dash and then what you are working on\ngit checkout -b _matt_-_utah_\n# This makes sure that the server knows about your private copy\n# origin means the cloud\ngit push --set-upstream _matt_-_utah_ origin\n# now make any changes that you want\n# when you are done, you just commit the changes and then push it to the cloud\ngit add -A\ngit commit -m \"I did some amazing things and other comments\"\ngit push\n# When you want others to see it, let @richtong know and he will merge it into\nthe base so others can see it\n```\n\n### Using Gitpod\n\nYou can start thing in Google Chrome or Firefox with an extension\n\n## Other repos\n\nThe best way to work on this is to see what others are doing. In\n[https://github.com/restartpartners](https://github.com/restartpartners])\nso here are the certain ways of doing it which are forked:\n\n- [NY Time](https://github.com/NYTimes/covide-19-data). This is the New York\n- Times data repository\n- [Datahub.io](https://github.com/datasets/covid-19). Time series data from datahub.io\n- [Imperial College of London](https://github.com/ImperialCollegeLondon/covid19model) their specfic\nmodel\n- [Reich](https://github.com/neherlab/covid19_scenarios). The summary and\n   visualization of all scenarios\n\n## Release Schedule\n\n### v1.4 Shipped\n\nAdd non-Washington cubes to the model. Create a separate sheet for\nnon-Washington that has the simplified model.\n\nEnable:\n\n1. Picking of different rows will be done by matching IDs rather than indexing\n2. For a given class, you can select a PPE row and then give it a weight. That\n3. The stretch goal. Patients will be added as a column so we can spread them\n   across the cubes\n\n### v1.3.2 - Surge model with non-Washington sheet -- In development\n\nUses the same basic form, but we do not assume Washington population data\n\n### v1.3.1 - Released - Washington Surge Model Corrected, NYC and Snohomish\n\nThis is the fixed model as the transition to Excel corrupted some cells and we\nlost formulas.\n\nThe enhancements are:\n\n1. AT the end of the model, any pivot table can be inserted into the model and\n   it will calculate based on that. It also slices a county appropriately based\non the Washington Cube\n2. The model now uses named ranges in Analysis 8 and 9 so just changing the\n   analysis is not just changing names rather than relinking absolute cell\nreferences\n3. Adds the NYC analysis as well at the bottom as well as Snohomish county and\n   it now uses a Pivottable and external data rather than copying it all into\nthe sheet, so this becomes more of analysis tool.\n4. Also adds a checksum in Analysis 8 and on to make sure the additions are\n   corect. Note that if you put in a Level but not an Essential, you will have\nissues. That is if no Essential is listed, it is not added to the total. That's\nan easy way to exclude groups by the way.\n5. This is on the way to generalization so if you want to change and to add new\n   analysis, copy down the latest Analysis and then change the formulas in the\nSUMIFS after you define new range names, the range names are where you replace\nthe `N`with the number of the analysis\n\nAnalysisNItems. All the items being managed\nAnalysisNLevels. The protection levels. These are numeric and index off the\nprotection level table. Soon they will be a tuple. The levels can be a fraction\nin which case it takes a percentage from the next level, so 4.2 means 20% from\nlevel 5 and the rest from level 4\nAnalysisNEssential. The degree of urgency to start. Arbitrarily, less than 4\nmeans a non-essential, (aka a non-bootstrap of the economy worker)\nAnalysisNLowLower. The lower bound of non-essential, >0\nAnalysisNLowUpper. The upper bound, usually <4\nAnalysisNHighLower. The lower bound of essentially, usually >=4\nAnalysisNHighUpper. The upper bound, usually <=999\n\nTo change a pivot, make sure you have lots of rows below, more than what the new\npivot needs, the go to Pivot Analysis/Change Data Source.\n\nRight now these are absolute paths, this still needs to get resolved how to make\nthis portable.\n\nThen you have to relink all of the data out of the pivot table. This takes some\ntime as you cannot just copy and paste, but have to do a hard equal to get the\nright extraction\n\nThe Pivot Table does not work with Hierarchical data, so in that case it is\nprobably better to either go to the owrk of chaning the lables so they are or to\njust copy the table in.\n\n## v1.0 - Released - Washington State Surge Model (deprecated)\n\nThis is the model that went to the Washington State Office of Financial\nManagement and we will send updates as needed. It has the following features\n(or bugs depending on how you look at it). Note that this model has a big bug,\nthe formulas were inadvertently deleted, so use 1.3.1 or later\n\n- Washington State only. No splits nor is this a country model\n- Five forecasts in one. NAICS-2, NAICS-6, SOC, Small Business only, Not\nemployed splits\n- Depends on WHO EFST v1.2 surge to estimate healthcare needs augmented with\n- DOH Tier 1-4 dates 14 April (not updated to latest) and some LNI rules for\nConstruction but not updated to latest phases\n- Estimates conserved use\n\n## Github, XLTrail, and Git LFS\n\nGithub and Excel spreadsheets are not really used much together, but as we are\ngoing to have both Excel spreadsheets and real code integrated, this seems like\na good place to put it.\n\nThere are a few changes to your workflow when you are using this tool that is\ndifferent from just storing Excel on your machine and emailing models around or\nhaving it in a shared dropbox:\n\n1. The versions are taken care of for you. This repo uses [XL\n   Trail](https://xltrail.com) to monitor all the spreadsheet changes. It\ngenerates a cell-by-cell comparison of what has actually changed.\n2. Github keeps track of every revision, so you can have a different set of\n   models and these get tagged so you can make sure you are getting the right\nmodel. This is independent of the filename of the model, so you can make sure\nyou are getting the right model at the right time.\n3. It stores every copy of the model in it so you can always roll back and\n   recover a model that is way cool..\n4. The final piece is Git LFS or Large File Storage, this makes it blazingly\n   fast to store even GB models (we do!) into the system\n\n## Notes on using Excel\n\n### Dealing with PivotTables\n\nHere is are the difficult parts on a Mac. A Pivot table cannot be moved with\ncopy and paste, instead, you need to go to the Ribbon and view analyze and there\nis an entry called `Move Pivottable` which lets you move it.\n\nWhen you select into a Pivot Table, you get a reference that is a cell number,\nyou get a named reference that looks like `PIVOTTABLE`. This works really well\nfor anything that is developed that has the same number of rows which is great.\n\n### Excel does not like Google Sheets\n\nThere is some sort of bug where Excel does not like certain forumulas in Google\nSheets so it deleted all the formulas that were using the Sumproduct. So this\nformula needed to be recreated on the Excel side and we should not use Google\nSheets as a result.\n\n## The resource formula\n\nThe key formula in the spreadsheet is the that takes the level of protection and\nmultiplies it against the row that is at the PPE level\n\nThe first formula only handled discrete rows like, so it indexes against a fixed\nprotection at G7:Q13 and then indexes into it with the D365 which is the\nprotection level. We add one because we index at 0. Then we calculate what the\ncolumn. Then multiply by the population\n\n```excel\n=@INDEX($G$7:$Q$13,$D365+1,COLUMN(H365)-COLUMN($G343)+1)*$E365\n```\n\n## Handling blending of rows\n\nIn many cases, a particular industry or job classification does not fit into any\none category, so we use sumproduct to figure this out. The key is to find the\nregion and then spread the data\n\nSo this first calculation gives you the two rows\n\n```excel\n=sumproduct(_two columns in the use matrix_, _the percentage split between the\ntwo_)\n```\n\nThe way that you collect the sum is by using the trick that modulo 1 gives you a\nfraction so mod(3.4, 1) is 0.4 :\n\nThis is where the spreadsheet broke because Google Sheets and Excel,\n\n```excel\n{MOD(protection,1),\n```\n\nNow this gives the weight average use and then you just multiply by the\npopulation and you are done\n\n```excel\n= sumproduct * population\n```\n\nOne problem with\n[Sumproduct](https://blog.udemy.com/excel-sumproduct/?utm_source=adwords&utm_medium=udemyads&utm_campaign=DSA_Catchall_la.EN_cc.US&utm_content=deal4584&utm_term=_._ag_95911180068_._ad_436653296108_._kw__._de_c_._dm__._pl__._ti_dsa-841699839063_._li_1027744_._pd__._&matchtype=b&gclid=EAIaIQobChMI-Leli9jD6QIV9Al9Ch3BXgn-EAAYASAAEgLSJfD_BwE) is that it does not like the vectors to be of\ndifferent shapes, so you can't do `sumproduct({1, 0}, {1 ; 0}), it needs both to\nbe row vectors.\n\nIn another note when used with a\n[Boolean](https://exceljet.net/excel-functions/excel-sumproduct-function), you\ncan use it if you do a double negative to coerce TRUE/FALSE into a number\n\n```excel\nsumproduct( --(A2:A6=\"TX\"), B2+B6)\n```\n\nwill only add numbers where the A column has the string \"TX\" in it. So you need to\n[transpose](https://support.office.com/en-us/article/transpose-function-ed039415-ed8a-4a81-93e9-4b6dfac76027) them first.\n\nSo the key formula looks like this where $J$7:$T$13 is the table and $D51 is the\nindex into it. Note that it automatically rounds down. Then the column\ncalculation makes sure you get the correct column starting from J, finally you\nwant the row below and then the trick is to transpose the next values.\n\nThis gets rid of the need to use the parentheses notation which might not be\nthat portable. This is just a simple function now. were E51 has 1-mod(d51) or\nthe amount for rounddown(e51,0) and mod(d51) is the fraction above.\n\n```excel\n=SUMPRODUCT(OFFSET($J$7:$T$13,$D51,COLUMN(J:J)-COLUMN($J:$J),2,1),TRANSPOSE($E51:$F51))*$G51\n```\n\n## Guards on the SUMIFS\n\nThe next complicated formula relies on ranges and does the summing. The main\ntrick here is that it uses SUMIFS as a conditional and you need to have a\nprotection level one greater at the end of each, so there is a mythical \"7\" or\nN+1. It made construction of the model very neat as a result.\n\n## Automatically deployment\n\n[xltrail](https://www.xltrail.com/blog/how-to-manage-and-release-excel-files-on-github-part2)\nhas a great explanation of how to make this work but see .github/workflow for\nadditional files.\n\nThe main trick here is the need to add a step to checkout the right Excel\nspreadsheet.\n\nTo make the deployment work, there is a named file, currently\ncovid-surge-who.xlsx which you need to copy the latest model into. Do not\nsymlink this as git lfs will get confused on the build\n\n## Automated Testing\n\nYou can use [XLWings](https://docs.xlwings.org/en/stable/installation.html) to\nrun an Excel application from Mac or PC. This uses a PIP package to control\nExcel.\n\nSince GitHub Actions allows runneers with Windows, you could theoretically start\na Windows machine, load Excel and run it with the Python to do testing. Man that\nseems complicated though.\n\nAnother approach might be to take models which are compatible with Google Sheets\nand push the model into Google Drive and drive it with Javascript\n\n## Mobility Modeling\n\nWe need a way to model economic behavior and mobility.\n\n## Recoded Python model Studies\n\nThis takes the study done with Jupyter notebooks and turns it into code as a\ndemo:\n\n## Model 0.0 feasibility of using Streamlit\nThis is the visual display demo using streamlit. Filled with dummy data\n\n- [streamlit](streamlit) experiments with using Streamlit for web display.\n- [model0/dashboard0.py](model0/dashboard0.py) is the spaghetti code that is the first implementation of the\n  model. Retain for testing purposes\n- [model](model) The current v2 model\n- [logging](logging). Experiments in building a robust logging system that works\n  across streamlit and command line\n- [altair](altair). Studies of using the Altair plotting interface to Vegas-lite\n- [yaml](yaml). Experiments in using YAML as model input\n- [namespace](namespace). Testing of multiple packages in the same namespace\n- [iterator](iterator). Learn how to Model to iterate against all Base classes\n    inside of it\n\n## Model that is the real python model first using streamlit as a demo v0.1\n\n- [model0/dashboard.py[(model0/dashboard.py). This is not yet complete but\n  implements the class model described in the readme\n- [README.ipynb](README.ipynb). This is the main read me that describes how the\n  module works. This is best read by starting [colab](https://colab.research.google.com)\nopening but it describes the equations and has the test code for the model.\n The main thing that it does is to make the variable names easy to understand.\n\n## The real code for the Python model for v2.x\n\nThe other files follow the standard Python scheme and is ready for docstring\ndocumentation\n- [src](src) the source for the python. it's on the floor right now.\n- [doc](doc) when we get documentation working we are using makedocs using\n  docstring as the production tool.\n\n## Note we are using [gravizo.com](https://gravizo.com) to render graphs with\ngraphviz and test. It actually supports DOT, PlnatUML and UML Graph as well as\nSVG so really useful for illustrations that are not just dumb graphics as\nexplained by @tlmak0 at https://github.com/tlmak0/gravizo. The way it works is\nthat you pass gravizo.com the URL of the README or whatever file, it will then\nparse it looking for Graphviz or other commands. It works because you set a\nmagic tag which must be unique in the text for it to find\n\nThe main tools you need here are the raw file link which you can get by looking\nat [github.com](https://help.data.world/hc/en-us/articles/115006300048-GitHub-how-to-find-the-sharable-download-URL-for-files-on-GitHub)\nand clicking on the `raw` button for a file and then put it\nthrough https://urlencoder.org to get the percent-encoding also call the URL\nencode.\n\nAlthough the gravizo.com site shows an easier way with a direct embed but this\n[no longer](https://gist.github.com/svenevs/ce05761128e240e27883e3372ccd4ecd)\nworks with github. Which is sad because the only way the indirect method works\nis for public repos since private repos require an authentication key.\n\nMost of the actual work is kept in a Jupyter Notebook\n[README.ipynb](README.ipynb) points to the latest one. You can launch from\n[colab](https://colab.research.google.com) to view it or you can see it statically\nrendered on [github](https://github.com)\n\n## Why no scientific notation\n\nThe most confusing part about this model are the many parameters. We use\nEinstein summations and the working model is in [README.ipynb](README.ipynb).\nNote that github markdown does not support Latex, so you have to use a\n[hack](https://gist.github.com/a-rodin/fef3f543412d6e1ec5b6cf55bf197d7b) to\ndisplay it properly by using an image call, so we just remove this from this\nreadme, otherwise it tracks the Jupyter Notebook but without the scientific\nnotation.\n\n## Class Structure\n\nThe main components of the v2 model are in a diagram\n\n![Alt text](https://g.gravizo.com/source/custom_mark?https%3A%2F%2Fraw.githubusercontent.com%2Frestartus%2Fcovid-projection%2Frich-demo%2Fmodel%2FREADME.md)\n<details>\n<summary></summary>\ncustom_mark\n  digraph \"Class Model\" {\n    node [shape=box]\n    subgraph Pop_class {\n      style=filled\n      P [label=\"Population, Essentiality\"]\n    }\n    D [label=Disease]\n    P -> D [label=\"Social Mobility\"]\n    D -> P [label=Patients]\n    E [label=Economy]\n    P -> E [label=\"Stage, Economic Activity\"]\n    E -> P [label=\"GDP, Employment\"]\n    subgraph Res {\n      R [label=Resource]\n      R -> P [label=Delivery]\n      P -> R [label=Demand]\n      I [label=Inventory]\n      R -> I [label=Fill]\n      I -> R [label=Use]\n    }\n    S [label=Supply]\n    R -> S [label=\"Sales Order\"]\n    S -> R [label=Fulfillment]\n  }\ncustom_mark\n</details>\n\n## The main classes (deprecated)\n\nYou should see the Jupyter notebook for complete documentation\n\nThen each major module can be subclassed from this and your can replace it. The\ncurrent list of modules are and in the notation, the class name is the major\npart of the variable. Note that is copied from Jupyter so you will see $Latex$\nformula for those of you who can read that, otherwise you can ignore it.\n\n## Model Class\n\nModel or `model`. This is the core framework. It holds the dimensions and\npointers to all the module component instances. It's main use is to \"dimension\"\nthe problem so that the other subclasses know the size of the problem. It also\nholds points to the entire \"graph\" of objects (similar to a Keras object in\nmachine learning. So the process is to add \"layers\" or model elements, then run\nthe model as needed with a \"solver\" that is used for specific purposes and later\nfor optimization of an objective function.\n\nThe objects in the world which has a single character $symbol$ or a 3-4\ncharacter `_short_name_` name and then some other facts\n\n## Population Class\n\nPopulation as or `pop`. This holds the population and details about it. It's\nmain output are two fold wtih a set of variables that are 'inside' Population\n\n- $P_{pd}$ or `Population.attr_pd[p, d] for p Populations with d details on\neach such as number of covid patients or number of runs per day\n- $P^R_{pn}$ or `Population.to_res_pn[p, n]`. This is a given populations use\nof all n resources and is the per capita, per day figure.\n- $P^T_{pn}$ or `Population.total_pn[p, n]`. This is the total population usage\n- $P^{LC}_{en}$ or `Population.level_cost_en[e, n]`. for every essentiality\nlevel, what is the cost for each resource n.\n- $P^{LT}_{en}$ or `Population.level_total_cost_en[e, n]`. The total cost for\n- every essential level for each resource\n$P^{D}_{ln}$ or `Population.demand_ln`. This is the conversion from essential\nlevels to items used where l is the number of levels and n is the number of\nresources. It is the core burn rate analysis\n- $P^{L}_{p,l}$ or `Population.to_level_pl[p, l]`. Converts a population into\nlevels of essentiality and use Finally there are various properties that these\nobjects can have. these are handles as superscripts in the formula notation or\nas the second word in the code as snake_case.\n\n- Burn rate $B$ or `burn` which is the use per day per person of a resource\n- Total units $U$ or `total` which is the total needed for an entire population\n- Cost $C$ or `cost`. The cost per unit\n- Total $T$ or `total_cost`. The total for an entire population\n  - Summary Level of Essentiality  `level`. The population summarized by summary\n    levels\n     that are used to restart the economy in phases and stop it the same way\n  - Demand $D$ or `demand`. This calculates the burn rates or usage of the\n    products. In the surge model these are done per person per day which\ngenerates an lxn matrix\n\n## Resource class\n\nResource  `res`. The resources needed, it takes demand from Population and\nreturns to the population what can actualy be supplied.\n\n- $R_{na}$ or `Resource.attr_na[n, a]` were Resource data for n items with a\n  attributes (like it's volume and cost), by convention, the first column has\n  a one in it for easy matrix multiplication\n- Supply $S$ or `supp`. The sources of supply and input of resources\n- Inventory $I$ or `inv`. What is currenty on hand\n\n## Economy Class\n\nEconomy $E$ or `econ`. This is a model of the economy that takes in the\nPopulation and the degree of economic activity and returns the GDP and\nemployment and other measures of work\n\n## Disease Class\n\nDisease $D$ or `disease`. This models the progression of the disease and takes\nin the population and social mobility and it returns the number of patients,\ndeaths and recovered patients.\n\nBut here are the major variables as a glossary and these usually have two forms,\nthe canonical dataframe and then an array form for tensors that are more than\ntwo dimensions. In general, we operate on the array form and display with the df\nform. The names change, but the first is for the surge model and the second for\nthe full range of data plus time series. And in colons are the class that\ncreates it\n\n-\n- Resource.attr_n_df (Resource.attr_na_df). Resource list main labels and it is all 1's in the surge model\n  then extends to the a attributes which are mainly things like volume.\n- Population.attr_p (Population.attr_pd_df). The populations we are studying.\n In this case, we are talking about d details including things like number of\n COVID patients in each population.\n- Demand.usage_res_ln_df (Demand.usc_res_dln_df). The Usage for each protection\nlevel for a resource per capita\n\nThen we have transformations that are typically a `to` attached:\n\n- Population.to_usage_pl. Converts p populations to the l usage level\n- Population.attr_p (Population.attr_pd). Population attributes with a 1 in the\n  first column always but then you can have others like covid patients\n- Population.resource_pn. Resource neede per capita\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/restartus/restart",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "restart-test",
    "package_url": "https://pypi.org/project/restart-test/",
    "platform": "",
    "project_url": "https://pypi.org/project/restart-test/",
    "project_urls": {
      "Homepage": "https://github.com/restartus/restart"
    },
    "release_url": "https://pypi.org/project/restart-test/2.5.0.7.1.0.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "COVID-19 decision model tool",
    "version": "2.5.0.7.1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8222932,
  "releases": {
    "2.5.0.7.1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bdc01cd6da476962d3993d985b859b3099feb2150d999c3ee4ad0be34d7f01e8",
          "md5": "0508d25cc739891387d63fddd7970d5d",
          "sha256": "162069a89fa42f5bf16d0c123fceadca3090925dcfd9074640d232db39cbfaa9"
        },
        "downloads": -1,
        "filename": "restart_test-2.5.0.7.1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0508d25cc739891387d63fddd7970d5d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 64305,
        "upload_time": "2020-09-19T07:45:56",
        "upload_time_iso_8601": "2020-09-19T07:45:56.510793Z",
        "url": "https://files.pythonhosted.org/packages/bd/c0/1cd6da476962d3993d985b859b3099feb2150d999c3ee4ad0be34d7f01e8/restart_test-2.5.0.7.1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e7798767d2154860f7f9fc185df56af772cf44ed77d01097a7fc8b078bb58972",
          "md5": "39ec0a14c8cc81304e6f04bb20ef62be",
          "sha256": "81b50f612c5e7dc13be023cb3fbcdfe95a2871801f251f4bc64a0aa14ec801f1"
        },
        "downloads": -1,
        "filename": "restart_test-2.5.0.7.1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "39ec0a14c8cc81304e6f04bb20ef62be",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 1627139,
        "upload_time": "2020-09-19T07:46:00",
        "upload_time_iso_8601": "2020-09-19T07:46:00.025008Z",
        "url": "https://files.pythonhosted.org/packages/e7/79/8767d2154860f7f9fc185df56af772cf44ed77d01097a7fc8b078bb58972/restart_test-2.5.0.7.1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bdc01cd6da476962d3993d985b859b3099feb2150d999c3ee4ad0be34d7f01e8",
        "md5": "0508d25cc739891387d63fddd7970d5d",
        "sha256": "162069a89fa42f5bf16d0c123fceadca3090925dcfd9074640d232db39cbfaa9"
      },
      "downloads": -1,
      "filename": "restart_test-2.5.0.7.1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0508d25cc739891387d63fddd7970d5d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 64305,
      "upload_time": "2020-09-19T07:45:56",
      "upload_time_iso_8601": "2020-09-19T07:45:56.510793Z",
      "url": "https://files.pythonhosted.org/packages/bd/c0/1cd6da476962d3993d985b859b3099feb2150d999c3ee4ad0be34d7f01e8/restart_test-2.5.0.7.1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e7798767d2154860f7f9fc185df56af772cf44ed77d01097a7fc8b078bb58972",
        "md5": "39ec0a14c8cc81304e6f04bb20ef62be",
        "sha256": "81b50f612c5e7dc13be023cb3fbcdfe95a2871801f251f4bc64a0aa14ec801f1"
      },
      "downloads": -1,
      "filename": "restart_test-2.5.0.7.1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "39ec0a14c8cc81304e6f04bb20ef62be",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 1627139,
      "upload_time": "2020-09-19T07:46:00",
      "upload_time_iso_8601": "2020-09-19T07:46:00.025008Z",
      "url": "https://files.pythonhosted.org/packages/e7/79/8767d2154860f7f9fc185df56af772cf44ed77d01097a7fc8b078bb58972/restart_test-2.5.0.7.1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}