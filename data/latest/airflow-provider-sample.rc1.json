{
  "info": {
    "author": "Pete DeJoy",
    "author_email": "pete@astronomer.io",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "<p align=\"center\">\n  <a href=\"https://www.airflow.apache.org\">\n    <img alt=\"Airflow\" src=\"https://cwiki.apache.org/confluence/download/attachments/145723561/airflow_transparent.png?api=v2\" width=\"60\" />\n  </a>\n</p>\n<h1 align=\"center\">\n  Airflow Sample Provider\n</h1>\n  <h3 align=\"center\">\n  Guidelines on building, deploying, and maintaining provider packages that will help Airflow users interface with external systems. Maintained with ❤️ by Astronomer.\n</h3>\n\n<br/>\n\nThis repository provides best practices for building, structuring, and deploying Airflow provider packages as independent python modules available on PyPI.\n\nProvider repositories must be public on Github and follow the structural and technical guidelines laid out in this Readme. Ensure that all of these requirements have been met before submitting a provider package for community review.\n\nHere, you'll find information on requirements and best practices for key aspects of your project:\n\n- File formatting\n- Development\n- Airflow integration\n- Documentation\n- Testing\n\n## Formatting Standards\n\nBefore writing and testing the functionality of your provider package, ensure that your project follows these formatting conventions.\n\n### Package name\n\nThe highest level directory in the provider package should be named in the following format:\n\n```\nairflow-provider-<provider-name>\n```\n\n### Repository structure\n\nAll provider packages must adhere to the following file structure:\n\n```bash\n├── LICENSE # A license is required, MIT or Apache is preferred.\n├── README.md\n├── sample_provider # Your package import directory. This will contain all Airflow modules and example DAGs.\n│   ├── __init__.py\n│   ├── example_dags\n│   │   ├── __init__.py\n│   │   └── sample-dag.py\n│   ├── hooks\n│   │   ├── __init__.py\n│   │   └── sample_hook.py\n│   ├── operators\n│   │   ├── __init__.py\n│   │   └── sample_operator.py\n│   └── sensors\n│       ├── __init__.py\n│       └── sample_sensor.py\n├── setup.py # A setup.py file to define dependencies and how the package is built and shipped. If you'd like to use setup.cfg, that is fine as well.\n└── tests # Unit tests for each module.\n    ├── __init__.py\n    ├── hooks\n    │   ├── __init__.py\n    │   └── sample_hook_test.py\n    ├── operators\n    │   ├── __init__.py\n    │   └── sample_operator_test.py\n    └── sensors\n        ├── __init__.py\n        └── sample_sensor_test.py\n```\n\n\n## Development Standards\n\nIf you followed the formatting guidelines above, you're now ready to start editing files to include standard package functionality.\n\n### Python Packaging Scripts\n\nYour `setup.py` file should contain all of the appropriate metadata and dependencies required to build your package. Use the [sample `setup.py` file](https://github.com/astronomer/airflow-provider-sample/blob/main/setup.py) in this repository as a starting point for your own project.\n\nIf some of the options for building your package are variables or user-defined, you can specify a `setup.cfg` file instead.\n\n### Managing Dependencies\n\nWhen building providers, these guidelines will help you avoid potential for dependency conflicts:\n\n- It is important that the providers do not include dependencies that conflict with the underlying dependencies for a particular Airflow version. All of the default dependencies included in the core Airflow project can be found in the Airflow [setup.py file](https://github.com/apache/airflow/blob/master/setup.py#L705).\n- Keep all dependencies relaxed at the upper bound. At the lower bound, specify minor versions (for example, `depx >=2.0.0, <3`).\n\n### Versioning\n\nUse standard semantic versioning for releasing your package. When cutting a new release, be sure to update all of the relevant metadata fields in your setup file.\n\n### Building Modules\n\nAll modules must follow a specific set of best practices to optimize their performance with Airflow:\n\n- **All classes should always be able to run without access to the internet.** The Airflow Scheduler parses DAGs on a regular schedule. Every time that parse happens, Airflow will execute whatever is contained in the `init` method of your class. If that `init` method contains network requests, such as calls to a third party API, there will be problems due to repeated network calls.\n- **Init methods should never call functions which return valid objects only at runtime**. This will cause a fatal import error when trying to import a module into a DAG. A common best practice for referencing connectors and variables within DAGs is to use [Jinja Templating](https://airflow.apache.org/docs/apache-airflow/stable/concepts.html#jinja-templating).\n- **All operator modules need an `execute` method.** This method defines the logic that the operator will implement.\n\nModules should also take advantage of native Airflow features that allow your provider to:\n\n- Register custom connection types, which improve the user experience when connecting to your tool.\n- Include `extra-links` that link your provider back to its page on the Astronomer Registry. This provides users easy access to documentation and example DAGs.\n\nRefer to the `Airflow Integration Standards` section for more information on how to build in these extra features.\n\n### Unit testing\n\nYour top-level `tests/` folder should include unit tests for all modules that exist in the repository. You can write tests in the framework of your choice, but the Astronomer team and Airflow community typically use [pytest](https://docs.pytest.org/en/stable/).\n\nYou can test this package by running: `python3 -m unittest` from the top-level of the directory.\n\n## Airflow Integration Standards\n\nAirflow exposes a number of plugins to interface from your provider package. We highly encourage provider maintainers to add these plugins because they significantly improve the user experience when connecting to a provider.\n\n### Defining an entrypoint\n\nTo enable custom connections, you first need to define an `apache_airflow_provider ` entrypoint in your `setup.py` or `setup.cfg` file:\n\n```    \nentry_points={\n  \"apache_airflow_provider\": [\n      \"provider_info=sample_provider.__init__:get_provider_info\"\n        ]\n    }\n```\n\nNext, you need to add a `get_provider_info` method to the `__init__` file in your top-level provider folder. This function needs to return certain metadata associated with your package in order for Airflow to use it at runtime:\n\n```python\ndef get_provider_info():\n    return {\n        \"package-name\": \"airflow-provider-sample'\",\n        \"name\": \"Sample Airflow Provider\", # Required\n        \"description\": \"A sample template for airflow providers.\", # Required\n        \"hook-class-names\": [\"sample_provider.hooks.sample_hook.SampleHook\"],\n        \"extra-links\": [\"sample_provider.operators.sample_operator.ExtraLink\"],\n        \"versions\": [\"0.0.1\"] # Required\n    }\n```\n\nOnce you define the entrypoint, you can use native Airflow features to expose custom connection types in the Airflow UI, as well as additional links to relevant documentation.\n\n### Adding Custom Connection Forms\n\nAirflow enables custom connection forms through discoverable hooks. The following is an example of a custom connection form for the Fivetran provider:\n\n<img src=\"https://user-images.githubusercontent.com/63181127/112921463-d07b2880-90d8-11eb-871b-fc4e1c6cade9.png\" width=\"600\" />\n\nAdd code to the hook class to initiate a discoverable hook and create a custom connection form. The following code defines a hook and a custom connection form:\n\n```python\nclass ExampleHook(BaseHook):\n    \"\"\"ExampleHook docstring...\"\"\"\n\n    conn_name_attr = 'example_conn_id'\n    default_conn_name = 'example_default'\n    conn_type = 'example'\n    hook_name = 'Example'\n\n    @staticmethod\n    def get_connection_form_widgets() -> Dict[str, Any]:\n        \"\"\"Returns connection widgets to add to connection form\"\"\"\n        from flask_appbuilder.fieldwidgets import BS3PasswordFieldWidget, BS3TextFieldWidget\n        from flask_babel import lazy_gettext\n        from wtforms import PasswordField, StringField, BooleanField\n\n        return {\n            \"extra__example__bool\": BooleanField(lazy_gettext('Example Boolean')),\n            \"extra__example__account\": StringField(\n                lazy_gettext('Account'), widget=BS3TextFieldWidget()\n            ),\n            \"extra__example__secret_key\": PasswordField(\n                lazy_gettext('Secret Key'), widget=BS3PasswordFieldWidget()\n            ),\n        }\n\n    @staticmethod\n    def get_ui_field_behaviour() -> Dict:\n        \"\"\"Returns custom field behaviour\"\"\"\n        import json\n\n        return {\n            \"hidden_fields\": ['port'],\n            \"relabeling\": {},\n            \"placeholders\": {\n                'extra': json.dumps(\n                    {\n                        \"example_parameter\": \"parameter\",\n                    },\n                    indent=1,\n                ),\n                'host': 'example hostname',\n                'schema': 'example schema',\n                'login': 'example username',\n                'password': 'example password',\n                'extra__example__account': 'example account name',\n                'extra__example__secret_key': 'example secret key',\n            },\n        }\n ```\n\nSome notes about using custom connections:\n\n- `get_connection_form_widgets()` creates extra fields using flask_appbuilder. Extra fields are defined in the following format:\n\n   ```\n   extra__<conn_type>__<field_name>\n   ```\n\n   A variety of field types can be created using this function, such as strings, passwords, booleans, and integers.\n\n- `get_ui_field_behaviour()` is a JSON schema describing the form field behavior. Fields can be hidden, relabeled, and given placeholder values.\n\n- To connect a form to Airflow, add the hook class name of a discoverable hook to `\"hook-class-names\"` in the `get_provider_info` method as mentioned in `Defining an entrypoint`.\n\n### Adding Custom Links\n\nOperators can add custom links that users can click to reach an external source when interacting with an operator in the Airflow UI. This link can be created dynamically based on the context of the operator. The following code example shows how to initiate an extra link within an operator:\n\n```python\nclass ExampleLink(BaseOperatorLink):\n    \"\"\"Link for ExmpleOperator\"\"\"\n\n    name = 'Example Link'\n\n    def get_link(self, operator, dttm):\n        \"\"\"Get link to registry page.\"\"\"\n\n        registry_link = \"https://{example}.com\"\n        return registry_link.format(example='example')\n\nclass ExampleOperator(BaseOperator):\n    \"\"\"ExampleOperator docstring...\"\"\"\n\n    operator_extra_links = (Example Link(),)\n```\n\nTo connect custom links to Airflow, add the operator class name to `\"extra-links\"` in the `get_provider_info` method mentioned above.\n\n## Documentation Standards\n\nCreating excellent documentation is essential for explaining the purpose of your provider package and how to use it.\n\n### Inline Module Documentation\n\nEvery Python module, including all hooks, operators, sensors, and transfers, should be documented inline via [sphinx-templated docstrings](https://pythonhosted.org/an_example_pypi_project/sphinx.html). These docstrings should be included at the top of each module file and contain three sections separated by blank lines:\n- A one-sentence description explaining what the module does.\n- A longer description explaining how the module works. This can include details such as code blocks or blockquotes. For more information Sphinx markdown directives, read the [Sphinx documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-code-block).\n- A declarative definition of parameters that you can pass to the module, templated per the example below.\n\nFor a full example of inline module documentation, see the [example operator in this repository](https://github.com/astronomer/airflow-provider-sample/blob/main/sample_provider/operators/sample_operator.py#L11).\n\n### README\n\nThe README for your provider package should give users an overview of what your provider package does. Specifically, it should include:\n\n- High-level documentation about the provider's service.\n- Steps for building a connection to the service from Airflow.\n- What modules exist within the package.\n- An exact set of dependencies and versions that your provider has been tested with.\n- Guidance for contributing to the provider package.\n\n## Functional Testing Standards\n\nTo build your repo into a python wheel that can be tested, follow the steps below:\n\n1. Clone the provider repo.\n2. `cd` into provider directory.\n3. Run `python3 -m pip install build`.\n4. Run `python3 -m build` to build the wheel.\n5. Find the .whl file in `/dist/*.whl`.\n6. Download the [Astro CLI](https://github.com/astronomer/astro-cli).\n7. Create a new project directory, cd into it, and run `astro dev init` to initialize a new astro project.\n8. Ensure the Dockerfile contains the Airflow 2.0 image:\n\n   ```\n   FROM quay.io/astronomer/ap-airflow:2.0.0-buster-onbuild\n   ```\n\n9. Copy the `.whl` file to the top level of your project directory.\n10. Install `.whl` in your containerized environment by adding the following to your Dockerfile:\n\n   ```\n   RUN pip install --user airflow_provider_<PROVIDER_NAME>-0.0.1-py3-none-any.whl\n   ```\n\n11. Copy your sample DAG to the `dags/` folder of your astro project directory.\n12. Run `astro dev start` to build the containers and run Airflow locally (you'll need Docker on your machine).\n13. When you're done, run `astro dev stop` to wind down the deployment. Run `astro dev kill` to kill the containers and remove the local Docker volume. You can also use `astro dev kill` to stop the environment before rebuilding with a new `.whl` file.\n\n> Note: If you are having trouble accessing the Airflow webserver locally, there could be a bug in your wheel setup. To debug, run `docker ps`, grab the container ID of the scheduler, and run `docker logs <scheduler-container-id>` to inspect the logs.\n\nOnce you have built and tested your provider package as a Python wheel, you're ready to [send us your repo](https://registry.astronomer.io/publish-provider) to be published on [The Astronomer Registry](https://registry.astronomer.io).",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://astronomer.io/",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "airflow-provider-sample",
    "package_url": "https://pypi.org/project/airflow-provider-sample/",
    "platform": "",
    "project_url": "https://pypi.org/project/airflow-provider-sample/",
    "project_urls": {
      "Homepage": "http://astronomer.io/"
    },
    "release_url": "https://pypi.org/project/airflow-provider-sample/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A sample provider package built by Astronomer.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11017794,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "32bcbaf69c80efb920e5eb9b363453d85f38307af1d15fcc35d50e33ce507e7b",
          "md5": "4afeb467679d5594f8d0a3d406fa02d4",
          "sha256": "c1dc3622fadb446719b1f88512c1db7bdb0b78aae73836c952a120d8917624ca"
        },
        "downloads": -1,
        "filename": "airflow_provider_sample-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4afeb467679d5594f8d0a3d406fa02d4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 13897,
        "upload_time": "2021-07-27T16:39:24",
        "upload_time_iso_8601": "2021-07-27T16:39:24.750879Z",
        "url": "https://files.pythonhosted.org/packages/32/bc/baf69c80efb920e5eb9b363453d85f38307af1d15fcc35d50e33ce507e7b/airflow_provider_sample-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "32bcbaf69c80efb920e5eb9b363453d85f38307af1d15fcc35d50e33ce507e7b",
        "md5": "4afeb467679d5594f8d0a3d406fa02d4",
        "sha256": "c1dc3622fadb446719b1f88512c1db7bdb0b78aae73836c952a120d8917624ca"
      },
      "downloads": -1,
      "filename": "airflow_provider_sample-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "4afeb467679d5594f8d0a3d406fa02d4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 13897,
      "upload_time": "2021-07-27T16:39:24",
      "upload_time_iso_8601": "2021-07-27T16:39:24.750879Z",
      "url": "https://files.pythonhosted.org/packages/32/bc/baf69c80efb920e5eb9b363453d85f38307af1d15fcc35d50e33ce507e7b/airflow_provider_sample-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}