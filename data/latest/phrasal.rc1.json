{
  "info": {
    "author": "Lucy Linder",
    "author_email": "lucy.derlin@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Phrasal\n\n## Forewords \n\n### What is it ?\n\nPhrasal is a library of tools to help gather meaningful, proper sentences from websites. \n\nWell, at least if used together. Each tool has a value of its own. \nFor example, the `Normalizer` (my favorite!) is very useful for NLP, when you have a crappy text corpus you need to clean.\nThe `MocySplitter` is a nice alternative to Moses when you need to cleverly split a stream of text into sentences, one per line. \nEtc.\n\n### Why was it developed ?\n\nI have been working on a project lately, called [SwissText](https://github.com/derlin/swisstext) that gathers Swiss German sentences from scraping the Internet (no kidding, see the LREC 2020 publication on [arXiv](https://arxiv.org/abs/1912.00159)).\nTo do so, I had to build upon existing tools and develop some of my own. \nWhile they were initially for Swiss German, I figured that it would maybe be useful in other contexts, hence this repo which is a stripped-down version of some of the SwissText modules.\n\n### How does it work ?\n\nThis repo contains implementations of four types of tools, which constitute together a pipeline:\n\n1. *converter*: extract (main) text from raw HTML;\n2. *normalizer*: normalize the raw text, including the encoding, quotes, spaces, etc.;\n3. *splitter*: split the text into chunks (potential sentences);\n4. *filterer*: filter chunks to keep only \"proper\" sentences.\n\nFor each step, I propose one or more implementations.\n\n## Tools available\n\n**HtmlConverters**\n\n* `phrasal.BsConverter` \\\nA converter built upon `BeautifulSoup` that exact text found on the HTML. \nText from code blocks, scripts or styles is ignored.\nIt deals cleverly with encodings and always delivers text in UTF-8.\n\n\n* `phrasal.JustextConverter` \\\na converter based on [`justext`](https://pypi.org/project/jusText/), that try to spot and remove boilerplate content.\nBy default, it only keeps \"good\" paragraph, that is text long enough to be a full sentences and with a low link density.\n\n**Normalizers**\n\n* `phrasal.Normalizer`, or simply `phrasal.normalize_text`\\\nNormalize some text (using a serie of homemade regexes), including: normalize spaces, replace combining diacritics by the accented letter codepoints and strip leftovers, normalize dashes and quotemarks, replace non-breakable spaces with regular ones, etc. \\\nIt can also try to fix encoding errors (see [`ftfy`](https://pypi.org/project/ftfy/)) and strip most unicode emoji symbols.\n\n**Splitters**\n\n* `phrasal.MosesSplitter`\\\nMoses' splitter [`split-sentences.perl`](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/ems/support/split-sentences.perl) completely rewritten in Python. It thus perfectly mimics the behavior, while being 5x faster than calling perl from Python (approach taken by [`MosesTokenizer`](https://pypi.org/project/mosestokenizer/) for example).\n* `phrasal.MocySplitter`\\\nAn improvement upon `MosesSplitter`, which: deals more efficiently with lowercase (people are lazy on the Web), try to preserve links, can split on `:` or `;` (optional), etc.\n\n**Filterers**\n\n* `phrasal.PatternSentenceFilter`\\\nA filterer based on a list of simple rules a proper sentence should respect, such as \"*at least five words*\", \"*no S P E L L E D* words\", etc. \\\nWhat is *awesome* ? The rules are expressed in a (homemade) YAML-based syntax and are highly customizable. If you don't like the behavior, have a look at `pattern_sentence_filter.yaml` and try writing your own set of rules !\n\n\n**link_utils**\n\nThe `phrasal.link_utils` module is a simple utility to process href links found on a page. It will resolve relative links\n(given a base URL), remove duplicates, strip anchors and exclude non-HTTP/HTTPs links.\n\nTo get the list of links from a URL (i.e. `href` found on the page main content), use `extract_links`:\n```python\nimport phrasal\n\nall_links = phrasal.extract_links('https://github.com/derlin/phrasal')\n```\n\n## How to use\n\nInstall the library using:\n```bash\n# regular install, one of:\npython setup.py install \npip install .\n\n# for development, one of:\npython setup.py develop\npip install -e .\npip install -e .[showcase] # for streamlit\n```\n\n### As a library\n\n```python\nimport phrasal\n```\nDone.\n\n### From the command line\n\nEach tool contains a command line interface with different arguments. Discover it by typing:\n```bash\npython -m phrasal --help\n```\n```bash\npython -m phrasal --help\nCall one of the tools from the command line. Usage: \n   classname [other arguments specific to classname]|[-h]\n\nAllowed classname arguments:\n - BsConverter\n - JustextConverter\n - PatternSentenceFilter\n - MocySplitter\n - MosesSplitter\n - Normalizer\n```\nHere are some examples:\n```bash\npython -m phrasal JustextConverter -u https://icosys.ch/swisscrawl\n=== from URL https://icosys.ch/swisscrawl\nAs part of the SwissTranslation project, SwissCrawl is a corpus of 500,000+ Swiss German (GSW)  [...]\n[...]\n```\n```bash\npython -m phrasal PatternSentenceFilter -i <(echo 'not-a-sentence\\nYEAH !!!\\nCet outil fonctionne très bien, je l’utilise tous les jours.')\nCet outil fonctionne très bien, je l’utilise tous les jours.\n```\n```bash\npython -m phrasal Normalizer -i raw_text.txt -o clean_text.txt\n```\n\n### I just need one tool...\n\nNo problem, each tool is more or less independent. \nYou may want to simplify the code a bit (e.g. remove the interface inheritance, transform classes into static scripts, I don't know), but I hope the source code is self-explaining. \n\n## Running tests\n\nTests are using `tox` and `pytest`. The easiest way to run them is:\n```bash\npip install tox tox-venv\ntox\n```\n\n## Running the showcase\n\nA showcase using [streamlit](https://www.streamlit.io/) is included. \nIt allows you to test the full pipeline straight from your browser and also play with the different tools and options\nfrom the **Live Customizer**. Once you found what works for you, you can simply copy-paste the code snippet generated.\n\nRun the showcase locally by doing:\n```bash\npip install streamlit\nstreamlit run src/showcase/lit.py\n```\n\n\n## License\n\nThis work is licensed under Apache 2.0, so you can basically do anything with it. \n\n*However*, I would **really enjoy** it if you **credit me** somehow, either by citing my name, send me an email to say hi (I get lonely sometime, may be nice to chat), leave a star on GitHub, or any other way you think may give me strength to keep doing open-source :blush:.\n\n## Related resources\n\n* [get-html](https://pypi.org/project/get-html/) to get raw or renderer HTML (used in this repo)\n* [SwissText](https://github.com/swisstext)\n* [SwissTranslation project page](https://icosys.ch/swisscrawl)\n* :octopus::octopus::octopus::octopus::octopus::octopus::octopus::octopus: (I just love octopuses)\n* [Personal website](https://derlin.ch)\n\n## TODO\n\n* add some usecases, such as finding links, cleaning a text file, etc. add language support information",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/derlin/phrasal",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "phrasal",
    "package_url": "https://pypi.org/project/phrasal/",
    "platform": "",
    "project_url": "https://pypi.org/project/phrasal/",
    "project_urls": {
      "Homepage": "https://github.com/derlin/phrasal"
    },
    "release_url": "https://pypi.org/project/phrasal/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "NLP tools to extract, normalize and filter sentences from text/HTML",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6768272,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08a33aa4821af24e5bc475ad9c6d783ca31b43be037f8ccd5fb1ab00589c9154",
          "md5": "0ff2475f011f00bb8dc26f73a4aaf57c",
          "sha256": "9220c66fff1fc1235f44b4bd0b0d29461ac98c1b4700ed571cb29ba358b481fe"
        },
        "downloads": -1,
        "filename": "phrasal-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0ff2475f011f00bb8dc26f73a4aaf57c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 27890,
        "upload_time": "2020-03-07T14:36:35",
        "upload_time_iso_8601": "2020-03-07T14:36:35.789729Z",
        "url": "https://files.pythonhosted.org/packages/08/a3/3aa4821af24e5bc475ad9c6d783ca31b43be037f8ccd5fb1ab00589c9154/phrasal-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "08a33aa4821af24e5bc475ad9c6d783ca31b43be037f8ccd5fb1ab00589c9154",
        "md5": "0ff2475f011f00bb8dc26f73a4aaf57c",
        "sha256": "9220c66fff1fc1235f44b4bd0b0d29461ac98c1b4700ed571cb29ba358b481fe"
      },
      "downloads": -1,
      "filename": "phrasal-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "0ff2475f011f00bb8dc26f73a4aaf57c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 27890,
      "upload_time": "2020-03-07T14:36:35",
      "upload_time_iso_8601": "2020-03-07T14:36:35.789729Z",
      "url": "https://files.pythonhosted.org/packages/08/a3/3aa4821af24e5bc475ad9c6d783ca31b43be037f8ccd5fb1ab00589c9154/phrasal-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}