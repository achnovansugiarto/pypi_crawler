{
  "info": {
    "author": "Ozan Özgür",
    "author_email": "ozan.zgur@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# mlpl\n\nA machine learning pipeline to speed up data science lifecycle.\n\nUsing this library, you can:\n- Test new experiments easily and keep track of their results.\n- Keep details of each preprocessing/FE step easily accessible in collapsibles.\n- Do hyperparameter search. (Bayesian search, quick linear search)\n- Create a pipeline that consists of useful steps and save/load it.\n- Automatically try different processing steps and use useful ones. (imputations, binning, one-hot encoding, ...)\n- Make your predictions more reliable by averaging results obtained from different CV splits and random seeds.\n\n#### Install:\n```\npip install mlpl\n```\n\n### Start a new pipeline\n***\nA pipeline consists of  a class and its config and log files.\nA pipeline will save your baseline model and the data after the baseline.\nFor trying new steps, it will load the data from its last state.\nIt will also automatically compare results of new steps to that of the baseline.\nAfter each useful step, final form of the dataset and hparams will be saved.\n\n**Hyperparameter search**\nPipeline will conduct bayesian/random search for the baseline.\nFor new steps after that, a simple hyperparameter search will take place.\n\n(The reason for that is you cannot conduct a bayesian search for each experiment.\nHowever, adding a new step will usually change the ideal hyperparameters, so\ndoing some hyperparameter search is required. Approach in this project is to conduct a bayesian search for the baseline, which will take a lot of time. For testing new steps, a custom simple hyperparameter search method will be used.)\n\n**create new pipeline:\n[(example: Titanic competition from Kaggle)](https://www.kaggle.com/c/titanic/overview \"(example: Titanic competition from Kaggle)\")**\n```python\nlabel_name = 'Survived'\ntrn_path = 'data/train.csv'\ntest_path = 'data/test.csv'\n\n# Pipeline class will keep track of your processed files, model metrics and experiments. \nlr_pipeline = pipe.Pipeline(label_name = label_name,\n                               overwrite = True,\n                               project_path = 'lr_pipeline',\n                               train_data_path = trn_path,\n                               test_data_path = test_path,\n                               minimize_metric = False,\n                               useful_limit = 0.001,\n                               line_search_iter = 1,\n                               n_random_seeds = 1,\n                               bayesian_search_iter= 50,\n                               bayesian_search_count = 1,\n                               final_bayesian_search_iter = 0,\n                               line_search_patience = 2,\n                               line_search_params = {'C': (1e-7, 1e3)})\n```\n\n\n### Hyperparameter search using hyperopt\n***\n- Specify hyperameter search space for each model.\n\nSearch is conducted on parameters in search space.\nFixed parameters are parameters that define the model.\n\nThis is an example for logistic regression.\n**fixed parameters:**\n```python\nfixed_params_lr = dict(score=accuracy_score,\n                       model=sklearn.linear_model.LogisticRegression,                       \n                       max_iter=5000,\n                       verbose = 0,\n                       n_jobs = 3,\n                       model_type = 'linear',\n                       folds=[KFold(n_splits= 5, shuffle = True, random_state = 42),\n                                  KFold(n_splits= 5, shuffle = True, random_state = 13),\n                                  KFold(n_splits= 5, shuffle = True, random_state = 100)])\n```\n**search space:**\n```python\nlr_search_space = dict(C = hp.loguniform('C', -7, 3),\n                       class_weight =  hp.choice('class_weight', ['balanced', None]),\n                       solver =  hp.choice('solver ', ['lbfgs', 'sag']))\n```\n**Averaging results over different splits**\n***\nBy  specifying multiple sklearn folds objects, average predictions over different splits.\n(Also available for random_state parameters for models.)\n\n```python\n    folds=[KFold(n_splits= 5, shuffle = True, random_state = 42),\n                 KFold(n_splits= 5, shuffle = True, random_state = 13),\n                 KFold(n_splits= 5, shuffle = True, random_state = 100)]\n```\n\n**Creating a baseline model**\n***\nA baseline step is a step with minimal processing. Preprocessing steps and feature engineering steps in the project will be tested against the metrics of baseline model.\n\n**create the baseline step:**\n```python\nlr_pipeline.set_baseline_step(model = pmodels.train_sklearn_pipeline,\n                                proc = pdefaults.default_sklearn_preprocess,\n                                search_model_params= lr_search_space,\n                                fixed_model_params = fixed_params_lr\n                               )\n```\n\n### Run baseline step\n***\n```python\nres = lr_pipeline.run_baseline(return_result = True)\n```\n- Output contains a javascript that hides details about the step in collapsible boxes.\n**output:**\n![baseline1](https://user-images.githubusercontent.com/40238324/69920598-f53a9e00-149a-11ea-9fed-45c60fb11b84.PNG)\n\n***\n\n![baseline2](https://user-images.githubusercontent.com/40238324/69920654-7c881180-149b-11ea-8168-3a07f9d28a04.PNG)\n\n***\n\n![baseline3](https://user-images.githubusercontent.com/40238324/69920657-89a50080-149b-11ea-9712-703a5c536042.PNG)\n\n### Create submission and save pipeline\n***\n**create submission:**\n```python\n# Convert test_preds to int from probabilities\n\n# Since this competition requires values to be 0 or 1,\n# We have to adjust a decision threshold. While selecting this threshold,\n# criteria is to make mean(label) equal to mean(predictions)\n# This step is not necessary in most projects\ntest_preds = (res['test_preds'] > 0.55).astype('int')\n\n# Prepare submission file\nto_sub = sub.copy()\nto_sub[label_name] = test_preds\nto_sub.to_csv('titanic_sub.csv', index = False)\ntest_preds.mean()\n\n# Baseline LB score: 0.76555\n```\n**save pipeline:**\n```python\nlr_pipeline.save_project()\n```\n\n### Experiments:\n***\nNew steps should be tried in a separate notebook. First, load the previously saved pipeline.\n```python\nlr_pipeline = pipe.Pipeline(project_path = 'lr_pipeline')\n```\nThen, create a function to create a kaggle submission for this competition.\nThis is not a part of the library.\n```python\n# Convert to (1,0) from probabilities\ndef make_submission(res, thresh):\n    test_preds = (res['test_preds'] > thresh).astype('int')\n\n    # Print mean to adjust threshold\n    print(test_preds.mean())\n\n    # Save submission\n    sub = pd.read_csv(r'data/gender_submission.csv')\n    to_sub = sub.copy()\n    to_sub[lr_pipeline.label_name] = test_preds\n    to_sub.to_csv('titanic_sub.csv', index = False)\n```\nThen we will try default steps for preprocessing and imputation.\n#### Default steps that will be tried:\nFor nominals, features with missing values are imputed in 3 different ways. These are:\n(Baseline model imputes with the most frequent value.)\n- Separate category (-9999)\n- Impute by dependent\n    (If missing values depend on another feature, this method will be useful.)\n\nOther default steps for nominals is to:\n- One-hot encode if specified\n- Group values with value_count < limit (default for limit is 10.)\n\nFor numeric steps, features with missing values are imputed in 3 different ways. These are:\n- Mean impute\n- Impute by dependent\n- Impute with fixed value (-9999)\n\nOther steps:\nBinning (if specified), One-hot encoding for binned features (if specified.)\nOne-hot encoding (if specified.)\n\n#### Standardization\nWhen we were creating a baseline step, we used argument\n```python\nmodel = pmodels.train_sklearn_pipeline\n```\npmodels.train_sklearn_pipeline standardizes all features if model_type = 'linear'.\n\n#### Sparse data\nCategoricals that were OHEd and numerics  that were binned and OHEd are kept in sparse form.\n\n#### Access current form of data\nYou can get the dataset from pipeline using,\n```python\ntrain, test = mypipeline.load_files()\n```\ntrain and test are DataTable instances, which are stored in pickles.\nDataTable is a class created to keep dataframes and sparse matrices together.\nWhen a column from a DataTable is OHEd, it is converted to a sparse matrix and added to the DataTable. Then, features can be accessed in the same way as in pandas. \n\n**try nominal steps:**\n```python\nsteps.try_default_nominal_steps(lr_pipeline,\n                                ohe = True,\n                                group_outliers = True,\n                                ohe_max_unique = 5000)\n```\n**try numeric steps:**\n```python\nsteps.try_default_numeric_steps(lr_pipeline,\n                                ohe = True,\n                                binning = True)\n```\n\n**Example output for nominals:**\n![steps1](https://user-images.githubusercontent.com/40238324/70061444-54142a80-15f5-11ea-9d64-7fdd80c8e5a6.PNG)\n***\n![steps2](https://user-images.githubusercontent.com/40238324/70061477-61c9b000-15f5-11ea-9fb9-9e7cbf256391.PNG)\n***\n![steps3](https://user-images.githubusercontent.com/40238324/70061511-71e18f80-15f5-11ea-825f-67c58b70a27c.PNG)\n\nSteps of each step can be also viewed.\n\n### Test after default steps\n***\n```python\n# When model is not specified, it is the baseline model\nlr_pipeline.add_model('lr')\nres = lr_pipeline.run_model('lr',\n                            hyperparam_search = False,\n                            return_pred = True,\n                            use_final_params = True)\n```\n```python\nmake_submission(res, 0.675)\n```\n### Try custom steps\n***\nIn order to try new steps, write your own function with the following arguments and outputs:\n```python\ndef my_step(feature_properties, train, test, label_name, MYARG1, MYARG2, ...):\n\t# first 4 features are obligatory, but you can add other arguments.\n\t# Arguments other than the first 4 must be provided to add_step function in\n\t# parameter proc_params as a dictionary\n\n\t# Preprocessing, FE, ... (Mutate train, test)\n\n\treturn cols_not_to_model, train, test\n```\n**another example:**\n```python\ndef my_step(feature_properties, train, test, label_name):\n\n\t# Add a new column to train, test\n\ttrain['mycol'] = train['a'] + train['b']\n\ttest['mycol'] = test['a'] + test['b']\n\n\t # Add absolute value of a\n\tfor df in [train, test]:\n\t\tdf['abs_a'] = df['a'].abs()\n\n\t# We don't want 'a' to be used in training. If it will be used in future, don't drop 'a'.\n\t# Instead, add it to cols_not_to_model. \n\t# If all columns will be used, place [ ] in cols_not_to_model.\n\tcols_not_to_model = ['a']\n\n\treturn cols_not_to_model, train, test\n```\n\n**example from titanic:**\n```python\n# Extract title from Name\n\ndef add_title(feature_properties, train, test, label_name):\n    # From: https://www.kaggle.com/kpacocha/top-5-titanic-machine-learning-from-disaster\n    def fe_title(df, col):\n        title_col = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n        title_col = np.where((title_col=='Capt') | (title_col=='Countess')\n                           | (title_col=='Don') | (title_col=='Dona')\n                           | (title_col=='Jonkheer') | (title_col=='Lady')\n                           | (title_col=='Sir') | (title_col=='Major')\n                           | (title_col=='Rev') | (title_col=='Col'),\n                           'Other',title_col)\n\n        title_col = pd.Series(title_col)\n        title_col = title_col.replace('Ms','Miss')\n        title_col = title_col.replace('Mlle','Miss')\n        title_col = title_col.replace('Mme','Mrs')\n        return title_col\n\n    # utils.utilize is a python decorator that transforms a function from:\n    # - takes dataframe, column name as input, returns pd.Series\n    # to:\n    # - takes multiple dataframes, can return a pd.Series, can add new column to\n    #   dataframes with a new name or replaces the original.\n    #   This behavior is controlled by 'mode' argument.\n    #  mode:\n    #  - 'add': add resulting column to the dataframe with a generated name\n    #  - 'replace': replace original column. \n    #  - 'return' : return pd.Series for each df.\n    #  \n    # utilize also has join_dfs argument (default=True)\n    # if join_dfs = True, operation is carried out after concatenating the column\n    # from dataframes.\n\n    # Process name, append result to train and test.\n    utils.utilize(mode = 'add')(fe_title)([train, test], 'Name')\n\n    # This is the name of the added column.\n    # Names are generated by utilize using this template:\n    #     '{function_name}_{col}'\n    #\n    # (This is if col is a single string. It can be a list)\n\n    new_name = 'fe_title_Name'\n\n    # Label encode new column and replace it.\n    utils.utilize(mode = 'replace')(prep.label_encode)([train, test], new_name)\n\n    # One hot encode new column\n    train, test = prep.one_hot_encode([train, test], col = new_name, sparse = True)\n    return [], train, test\n```\n#### Try a new step:\n```python\nres = lr_pipeline.add_step_apply_if_useful(proc = add_title)\n```\n**output: (details can be viewed by clicking on add_title)**\n![steps4](https://user-images.githubusercontent.com/40238324/70067928-1ff23700-1600-11ea-90e7-decad7b72cb9.PNG)\n\n**create a kaggle submission:**\n```python\nmake_submission(res, 0.7335)\n```\n\n#### Try mutually exclusive steps:\nSome steps are mutually excusive, which means that you will only apply one of them,\neven if more than one is useful. For example, different methods of imputations are mutually exclusive.\n\n<details>\n<summary>Code</summary>\n\n```python\ndef add_prefix(feature_properties, train, test, label_name, col_name):\n    def prefix(df, col):\n        def get_prefix(x):\n            x = str(x)\n            if len(x) == 1:\n                return x\n            else:\n                return x.split(' ')[0][0]\n        return df[col].apply(lambda x: get_prefix(x))\n\n    utils.utilize(mode = 'add')(prefix)([train, test], col_name)\n    new_name = f'prefix_{col_name}'\n    utils.utilize(mode = 'replace')(prep.label_encode)([train, test], new_name)\n    train, test = prep.one_hot_encode([train, test],\n                                      col = new_name,\n                                      mode = 'replace')\n    return [], train, test\n\ndef add_prefix_group_outliers(\n        feature_properties, train, test,\n        label_name, col_name, limit = 10):\n    @utils.utilize(mode = 'add')\n    def prefix(df, col):\n        def get_prefix(x):\n            x = str(x)\n            if len(x) == 1:\n                return x\n            else:\n                return x.split(' ')[0][0]\n        return df[col].apply(lambda x: get_prefix(x))\n\n    prefix([train, test], col_name)\n    new_name = f'prefix_{col_name}'\n    utils.utilize(mode = 'replace')(prep.label_encode)([train, test], new_name)\n    prep.group_outliers_replace([train, test], new_name, limit = limit)\n    train, test = prep.one_hot_encode([train, test],\n                                      col = new_name,\n                                      mode = 'add')\n\n    # Don't drop the original column, but don't use it in training\n    return [col_name], train, test\n\nlr_pipeline.add_step(proc = add_prefix,\n                     group = 'prefix_ticket',\n                     proc_params= {'col_name': 'Ticket'})\n\nlr_pipeline.add_step(proc = add_prefix_group_outliers,\n                     group = 'prefix_ticket',\n                     proc_params= {'col_name': 'Ticket'})\n\nres = lr_pipeline.group_apply_useful('prefix_ticket')\n```\n</details>\n\n**output:**\n![steps5](https://user-images.githubusercontent.com/40238324/70068419-043b6080-1601-11ea-969b-d6340d949ed5.PNG)\n\nNo need to generate a submission for this one, as nothing was changed in the data.\n\n### Train other models (or train from scratch using bayesian search)\n***\nTrain baseline model. (using hparams determined in line search)\n(Training baseline is necessary only if you will stack/blend)\n\n**train baseline:**\n```python\n# When model is not specified, it is the baseline model\nlr_pipeline.add_model('lr')\nres = lr_pipeline.run_model('lr',\n                            hyperparam_search = False,\n                            return_pred = True,\n                            use_final_params = True)\n```\n**output:**\n![models1](https://user-images.githubusercontent.com/40238324/70069132-55981f80-1602-11ea-8545-6e36bae1c840.PNG)\n***\n**train svm:**\nfixed_hparams and search_hparams can be used in other projects as they are.\n(I will add them to the library soon.)\n```python\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\nfixed_hparams = dict(model = SVC,\n                     probability = True,\n                     random_state = 42,\n                     score = accuracy_score,\n                     max_iter = 2000,\n                     folds=[KFold(n_splits= 5, shuffle = True, random_state = 42),\n                              KFold(n_splits= 5, shuffle = True, random_state = 13),\n                              KFold(n_splits= 5, shuffle = True, random_state = 100)\n                              ])\n\n\nsearch_hparams = dict(C = hp.loguniform('C', -3, 7),\n                      gamma = hp.loguniform('gamma', -3, 3),\n                      class_weight =  hp.choice('class_weight', ['balanced', None]),\n                      kernel = hp.choice('kernel', ['linear', 'rbf', 'poly'])\n                      )\n\nlr_pipeline.add_model('svc',\n                      model = pmodels.train_sklearn_pipeline,\n                      fixed_hparams = fixed_hparams,\n                      search_hparams = search_hparams)\n\nres = lr_pipeline.run_model('svc', return_pred = True, hyperparam_search = True)\n```\n**create submission:**\nYou should test each model before stacking/blending.\n```python\nmake_submission(res, 0.675)\n```\n***\n**train kneighbors:**\n```python\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfixed_hparams = dict(model = KNeighborsClassifier,\n                     folds = lr_pipeline.baseline_step['model_params']['folds'],\n                     score = accuracy_score)\n\n\nsearch_hparams = dict(n_neighbors  = hp.choice('n_neighbors', np.arange(4,25)),\n                      leaf_size = hp.choice('leaf_size', np.arange(15,50)))\n\nlr_pipeline.add_model('kn',\n                      model = pmodels.train_sklearn_pipeline,\n                      fixed_hparams = fixed_hparams,\n                      search_hparams = search_hparams)\n\nres = lr_pipeline.run_model('kn', hyperparam_search = True)\n```\n**output:**\n![steps6](https://user-images.githubusercontent.com/40238324/70069483-e53dce00-1602-11ea-82c1-634115cab3df.PNG)\n\n**create submission:**\n```python\nmake_submission(res, 0.675)\n```\n\n***\n### Blending\n(Stacking will be also available.)\n\n**blend predictions in a directory:**\n```python\nres = {}\nres['test_preds'] = putils.blend_from_csv(directory = lr_pipeline.test_preds_path)\n```\n**create submission:**\n```python\nmake_submission(res, 0.7)\n```\n\n**save project:**\n```python\nlr_pipeline.save_project()\n```\n\n### Note:\nIdeally, you should place each new experiment/step in a new notebook and save project after each useful step. In Titanic example, baseline is in its own notebook, but following steps are in a second one, to keep the example simpler.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ozanzgur/mlpl",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mlpl",
    "package_url": "https://pypi.org/project/mlpl/",
    "platform": "",
    "project_url": "https://pypi.org/project/mlpl/",
    "project_urls": {
      "Homepage": "https://github.com/ozanzgur/mlpl"
    },
    "release_url": "https://pypi.org/project/mlpl/0.1.1/",
    "requires_dist": [
      "dill (>=0.3.1.1)",
      "hyperopt (>=0.2.2)",
      "ipython (>=7.7.0)",
      "jupyter (>=1.0.0)",
      "pandas (>=0.25.1)",
      "scikit-learn (>=0.21.2)",
      "scipy (>=1.3.0)",
      "tqdm (>=4.36.1)",
      "lightgbm (>=2.2.3)",
      "json5 (>=0.8.5)"
    ],
    "requires_python": "",
    "summary": "A data science pipeline tool to speed up data science life cycle.",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6236356,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e88ce347e287b43f5905405ddc44f852f346c10d70aef26a58e727fdbe61325a",
          "md5": "a3990a669fe82806cf403b1db0959821",
          "sha256": "9aa52bcd57b25e12df01a06945394bbdb6a2f38928c6302a47972f7d59e0fbee"
        },
        "downloads": -1,
        "filename": "mlpl-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a3990a669fe82806cf403b1db0959821",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 150183,
        "upload_time": "2019-12-02T09:01:20",
        "upload_time_iso_8601": "2019-12-02T09:01:20.731382Z",
        "url": "https://files.pythonhosted.org/packages/e8/8c/e347e287b43f5905405ddc44f852f346c10d70aef26a58e727fdbe61325a/mlpl-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8bb40845a3c64e4d00889260589b8613795dcfed17490f9ca083869a475cb535",
          "md5": "3b7ad0c79171730d020065503bf39550",
          "sha256": "7816d8a22d2fa51c9705ae2de22a503b9695c70d96c0edbab17294e0a4836a34"
        },
        "downloads": -1,
        "filename": "mlpl-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3b7ad0c79171730d020065503bf39550",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 63923,
        "upload_time": "2019-12-02T09:01:23",
        "upload_time_iso_8601": "2019-12-02T09:01:23.988668Z",
        "url": "https://files.pythonhosted.org/packages/8b/b4/0845a3c64e4d00889260589b8613795dcfed17490f9ca083869a475cb535/mlpl-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3bee85e94f592932d0b20dd9c6eee29215fcb3ef4cc4e73097ac202aa7f4ed14",
          "md5": "6f41d1e755538c75038917c4403bdb1f",
          "sha256": "1ef18e3f32101beefc1fffd747b73f7d012c36e95560e22446fc4c3d938a0ffb"
        },
        "downloads": -1,
        "filename": "mlpl-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6f41d1e755538c75038917c4403bdb1f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 153878,
        "upload_time": "2019-12-03T17:52:38",
        "upload_time_iso_8601": "2019-12-03T17:52:38.995974Z",
        "url": "https://files.pythonhosted.org/packages/3b/ee/85e94f592932d0b20dd9c6eee29215fcb3ef4cc4e73097ac202aa7f4ed14/mlpl-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "24ec3f9ab57d9155c0dfea17308ae651fb0a352d994697efa23710c79944f67b",
          "md5": "6ad7e5760546eaf1ea4456a8fcec4e6a",
          "sha256": "5214b90bce42b985a606b7f5e50a580e542e25053bffa9ece8b12ceb5aab0d45"
        },
        "downloads": -1,
        "filename": "mlpl-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "6ad7e5760546eaf1ea4456a8fcec4e6a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 68873,
        "upload_time": "2019-12-03T17:52:41",
        "upload_time_iso_8601": "2019-12-03T17:52:41.794014Z",
        "url": "https://files.pythonhosted.org/packages/24/ec/3f9ab57d9155c0dfea17308ae651fb0a352d994697efa23710c79944f67b/mlpl-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3bee85e94f592932d0b20dd9c6eee29215fcb3ef4cc4e73097ac202aa7f4ed14",
        "md5": "6f41d1e755538c75038917c4403bdb1f",
        "sha256": "1ef18e3f32101beefc1fffd747b73f7d012c36e95560e22446fc4c3d938a0ffb"
      },
      "downloads": -1,
      "filename": "mlpl-0.1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "6f41d1e755538c75038917c4403bdb1f",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 153878,
      "upload_time": "2019-12-03T17:52:38",
      "upload_time_iso_8601": "2019-12-03T17:52:38.995974Z",
      "url": "https://files.pythonhosted.org/packages/3b/ee/85e94f592932d0b20dd9c6eee29215fcb3ef4cc4e73097ac202aa7f4ed14/mlpl-0.1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "24ec3f9ab57d9155c0dfea17308ae651fb0a352d994697efa23710c79944f67b",
        "md5": "6ad7e5760546eaf1ea4456a8fcec4e6a",
        "sha256": "5214b90bce42b985a606b7f5e50a580e542e25053bffa9ece8b12ceb5aab0d45"
      },
      "downloads": -1,
      "filename": "mlpl-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "6ad7e5760546eaf1ea4456a8fcec4e6a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 68873,
      "upload_time": "2019-12-03T17:52:41",
      "upload_time_iso_8601": "2019-12-03T17:52:41.794014Z",
      "url": "https://files.pythonhosted.org/packages/24/ec/3f9ab57d9155c0dfea17308ae651fb0a352d994697efa23710c79944f67b/mlpl-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}