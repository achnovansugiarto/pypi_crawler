{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: Console",
      "Environment :: MacOS X",
      "Intended Audience :: Developers",
      "Intended Audience :: Information Technology",
      "Intended Audience :: System Administrators",
      "License :: OSI Approved",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: POSIX :: Linux",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Software Development"
    ],
    "description": "# Introduction to Metadata Ingestion\n\n## Integration Options\n\nDataHub supports both **push-based** and **pull-based** metadata integration. \n\nPush-based integrations allow you to emit metadata directly from your data systems when metadata changes, while pull-based integrations allow you to \"crawl\" or \"ingest\" metadata from the data systems by connecting to them and extracting metadata in a batch or incremental-batch manner. Supporting both mechanisms means that you can integrate with all your systems in the most flexible way possible. \n\nExamples of push-based integrations include [Airflow](../docs/lineage/airflow.md), [Spark](../metadata-integration/java/spark-lineage/README.md), [Great Expectations](./integration_docs/great-expectations.md) and [Protobuf Schemas](../metadata-integration/java/datahub-protobuf/README.md). This allows you to get low-latency metadata integration from the \"active\" agents in your data ecosystem. Examples of pull-based integrations include BigQuery, Snowflake, Looker, Tableau and many others.\n\nThis document describes the pull-based metadata ingestion system that is built into DataHub for easy integration with a wide variety of sources in your data stack.\n\n## Getting Started\n\n### Prerequisites\n\nBefore running any metadata ingestion job, you should make sure that DataHub backend services are all running. You can either run ingestion via the [UI](../docs/ui-ingestion.md) or via the [CLI](../docs/cli.md). You can reference the CLI usage guide given there as you go through this page.\n\n## Core Concepts\n\n### Sources\n\nData systems that we are extracting metadata from are referred to as **Sources**. The `Sources` tab on the left in the sidebar shows you all the sources that are available for you to ingest metadata from. For example, we have sources for [BigQuery](https://datahubproject.io/docs/generated/ingestion/sources/bigquery), [Looker](https://datahubproject.io/docs/generated/ingestion/sources/looker), [Tableau](https://datahubproject.io/docs/generated/ingestion/sources/tableau) and many others.\n\n#### Metadata Ingestion Source Status\n\nWe apply a Support Status to each Metadata Source to help you understand the integration reliability at a glance.\n\n![Certified](https://img.shields.io/badge/support%20status-certified-brightgreen): Certified Sources are well-tested & widely-adopted by the DataHub Community. We expect the integration to be stable with few user-facing issues.\n\n![Incubating](https://img.shields.io/badge/support%20status-incubating-blue): Incubating Sources are ready for DataHub Community adoption but have not been tested for a wide variety of edge-cases. We eagerly solicit feedback from the Community to streghten the connector; minor version changes may arise in future releases.\n\n![Testing](https://img.shields.io/badge/support%20status-testing-lightgrey): Testing Sources are available for experiementation by DataHub Community members, but may change without notice. \n\n### Sinks\n\nSinks are destinations for metadata. When configuring ingestion for DataHub, you're likely to be sending the metadata to DataHub over either the [REST (datahub-sink)](./sink_docs/datahub.md#datahub-rest) or the [Kafka (datahub-kafka)](./sink_docs/datahub.md#datahub-kafka) sink. In some cases, the [File](./sink_docs/file.md) sink is also helpful to store a persistent offline copy of the metadata during debugging.\n\nThe default sink that most of the ingestion systems and guides assume is the `datahub-rest` sink, but you should be able to adapt all of them for the other sinks as well!\n\n### Recipes\n\nA recipe is the main configuration file that puts it all together. It tells our ingestion scripts where to pull data from (source) and where to put it (sink).\n\n:::tip\nName your recipe with **.dhub.yaml** extension like *myrecipe.dhub.yaml* to use vscode or intellij as a recipe editor with autocomplete\nand syntax validation.\n\nMake sure yaml plugin is installed for your editor:\n- For vscode install [Redhat's yaml plugin](https://marketplace.visualstudio.com/items?itemName=redhat.vscode-yaml)\n- For intellij install [official yaml plugin](https://plugins.jetbrains.com/plugin/13126-yaml\n)\n\n:::\n\nSince `acryl-datahub` version `>=0.8.33.2`, the default sink is assumed to be a DataHub REST endpoint:\n- Hosted at \"http://localhost:8080\" or the environment variable `${DATAHUB_GMS_URL}` if present\n- With an empty auth token or the environment variable `${DATAHUB_GMS_TOKEN}` if present. \n\nHere's a simple recipe that pulls metadata from MSSQL (source) and puts it into the default sink (datahub rest).\n\n```yaml\n# The simplest recipe that pulls metadata from MSSQL and puts it into DataHub\n# using the Rest API.\nsource:\n  type: mssql\n  config:\n    username: sa\n    password: ${MSSQL_PASSWORD}\n    database: DemoData\n\n# sink section omitted as we want to use the default datahub-rest sink\n```\n\nRunning this recipe is as simple as:\n```shell\ndatahub ingest -c recipe.dhub.yaml\n```\n\nor if you want to override the default endpoints, you can provide the environment variables as part of the command like below:\n```shell\nDATAHUB_GMS_URL=\"https://my-datahub-server:8080\" DATAHUB_GMS_TOKEN=\"my-datahub-token\" datahub ingest -c recipe.dhub.yaml\n```\n\nA number of recipes are included in the [examples/recipes](./examples/recipes) directory. For full info and context on each source and sink, see the pages described in the [table of plugins](../docs/cli.md#installing-plugins).\n\n> Note that one recipe file can only have 1 source and 1 sink. If you want multiple sources then you will need multiple recipe files.\n\n### Handling sensitive information in recipes\n\nWe automatically expand environment variables in the config (e.g. `${MSSQL_PASSWORD}`),\nsimilar to variable substitution in GNU bash or in docker-compose files. For details, see\nhttps://docs.docker.com/compose/compose-file/compose-file-v2/#variable-substitution. This environment variable substitution should be used to mask sensitive information in recipe files. As long as you can get env variables securely to the ingestion process there would not be any need to store sensitive information in recipes.\n\n### Basic Usage of CLI for ingestion\n\n```shell\npip install 'acryl-datahub[datahub-rest]'  # install the required plugin\ndatahub ingest -c ./examples/recipes/mssql_to_datahub.dhub.yml\n```\n\nThe `--dry-run` option of the `ingest` command performs all of the ingestion steps, except writing to the sink. This is useful to validate that the\ningestion recipe is producing the desired metadata events before ingesting them into datahub.\n\n```shell\n# Dry run\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yml --dry-run\n# Short-form\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yml -n\n```\n\nThe `--preview` option of the `ingest` command performs all of the ingestion steps, but limits the processing to only the first 10 workunits produced by the source.\nThis option helps with quick end-to-end smoke testing of the ingestion recipe.\n\n```shell\n# Preview\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yml --preview\n# Preview with dry-run\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yml -n --preview\n```\n\nBy default `--preview` creates 10 workunits. But if you wish to try producing more workunits you can use another option `--preview-workunits`\n\n```shell\n# Preview 20 workunits without sending anything to sink\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yml -n --preview --preview-workunits=20\n```\n\n#### Reporting\n\nBy default, the cli sends an ingestion report to DataHub, which allows you to see the result of all cli-based ingestion in the UI. This can be turned off with the `--no-default-report` flag.\n\n```shell\n# Running ingestion with reporting to DataHub turned off\ndatahub ingest -c ./examples/recipes/example_to_datahub_rest.dhub.yaml --no-default-report\n```\n\nThe reports include the recipe that was used for ingestion. This can be turned off by adding an additional section to the ingestion recipe.\n\n```yaml\n\nsource:\n   # source configs\n\nsink:\n   # sink configs\n\n# Add configuration for the datahub reporter\nreporting:\n  - type: datahub\n    config:\n      report_recipe: false\n```\n\n\n## Transformations\n\nIf you'd like to modify data before it reaches the ingestion sinks – for instance, adding additional owners or tags – you can use a transformer to write your own module and integrate it with DataHub. Transformers require extending the recipe with a new section to describe the transformers that you want to run.\n\nFor example, a pipeline that ingests metadata from MSSQL and applies a default \"important\" tag to all datasets is described below:\n```yaml\n# A recipe to ingest metadata from MSSQL and apply default tags to all tables\nsource:\n  type: mssql\n  config:\n    username: sa\n    password: ${MSSQL_PASSWORD}\n    database: DemoData\n\ntransformers: # an array of transformers applied sequentially\n  - type: simple_add_dataset_tags\n    config:\n      tag_urns:\n        - \"urn:li:tag:Important\"\n\n# default sink, no config needed\n```\n\nCheck out the [transformers guide](./docs/transformer/intro.md) to learn more about how you can create really flexible pipelines for processing metadata using Transformers!\n\n## Using as a library (SDK)\n\nIn some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. In this case, take a look at the [Python emitter](./as-a-library.md) and the [Java emitter](../metadata-integration/java/as-a-library.md) libraries which can be called from your own code. \n\n### Programmatic Pipeline\nIn some cases, you might want to configure and run a pipeline entirely from within your custom Python script. Here is an example of how to do it.\n - [programmatic_pipeline.py](./examples/library/programatic_pipeline.py) - a basic mysql to REST programmatic pipeline.\n\n## Developing\n\nSee the guides on [developing](./developing.md), [adding a source](./adding-source.md) and [using transformers](./docs/transformer/intro.md).",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://datahubproject.io/",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "acryl-datahub-tc",
    "package_url": "https://pypi.org/project/acryl-datahub-tc/",
    "platform": null,
    "project_url": "https://pypi.org/project/acryl-datahub-tc/",
    "project_urls": {
      "Changelog": "https://github.com/datahub-project/datahub/releases",
      "Documentation": "https://datahubproject.io/docs/",
      "Homepage": "https://datahubproject.io/",
      "Source": "https://github.com/datahub-project/datahub"
    },
    "release_url": "https://pypi.org/project/acryl-datahub-tc/0.10.0.0rc2/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "A CLI to work with DataHub metadata",
    "version": "0.10.0.0rc2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17368429,
  "releases": {
    "0.10.0.0rc1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c810c89a28bff50c50a42c2ea20802ee8fd8d59001fec5c06d006cd9334ae152",
          "md5": "ef43d43814904e8f9d2dd563dc045889",
          "sha256": "f8c13e610af8566ad53037d541cf0ee0582dd44a6ec315b071e0bcfe310e9c08"
        },
        "downloads": -1,
        "filename": "acryl-datahub-tc-0.10.0.0rc1.tar.gz",
        "has_sig": false,
        "md5_digest": "ef43d43814904e8f9d2dd563dc045889",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 896294,
        "upload_time": "2023-03-20T16:17:40",
        "upload_time_iso_8601": "2023-03-20T16:17:40.488769Z",
        "url": "https://files.pythonhosted.org/packages/c8/10/c89a28bff50c50a42c2ea20802ee8fd8d59001fec5c06d006cd9334ae152/acryl-datahub-tc-0.10.0.0rc1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.10.0.0rc2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5fb639fcb0b760e07195718a7991392b5e124c5e9a94324dd781e30a5e0f98f0",
          "md5": "1021fbead3d3bdf035c60bba69c40206",
          "sha256": "69a9a1c2333d97a173b5444b6ee0df6cf4274a30c6670af58a9e4119e27d1764"
        },
        "downloads": -1,
        "filename": "acryl-datahub-tc-0.10.0.0rc2.tar.gz",
        "has_sig": false,
        "md5_digest": "1021fbead3d3bdf035c60bba69c40206",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 896630,
        "upload_time": "2023-03-20T19:12:24",
        "upload_time_iso_8601": "2023-03-20T19:12:24.340916Z",
        "url": "https://files.pythonhosted.org/packages/5f/b6/39fcb0b760e07195718a7991392b5e124c5e9a94324dd781e30a5e0f98f0/acryl-datahub-tc-0.10.0.0rc2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.10.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fa88f9f80b836c265365a64df75230aa10bb02e18f5bc3fda6c792b9bc3314c5",
          "md5": "f076c27719e1d3aea7c9fb78f75dacf7",
          "sha256": "7f1880418bb74b583eea3fd2e71263a484076c181ccbdd12e1f75f6bf6432d26"
        },
        "downloads": -1,
        "filename": "acryl-datahub-tc-0.10.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f076c27719e1d3aea7c9fb78f75dacf7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 660672,
        "upload_time": "2023-03-16T14:41:57",
        "upload_time_iso_8601": "2023-03-16T14:41:57.440267Z",
        "url": "https://files.pythonhosted.org/packages/fa/88/f9f80b836c265365a64df75230aa10bb02e18f5bc3fda6c792b9bc3314c5/acryl-datahub-tc-0.10.0.1.tar.gz",
        "yanked": true,
        "yanked_reason": null
      }
    ],
    "0.10.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5616c4e9c1629b5ee98e400978bd8929e202ff004a214ff13e766397df30cef4",
          "md5": "1950539829be896b1d52f7f8cd72747c",
          "sha256": "0791f61e72c0d8b6b7fde0a2082d20584dd7356f4326c3670b884341d3666296"
        },
        "downloads": -1,
        "filename": "acryl-datahub-tc-0.10.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1950539829be896b1d52f7f8cd72747c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 689313,
        "upload_time": "2023-03-16T15:48:24",
        "upload_time_iso_8601": "2023-03-16T15:48:24.560744Z",
        "url": "https://files.pythonhosted.org/packages/56/16/c4e9c1629b5ee98e400978bd8929e202ff004a214ff13e766397df30cef4/acryl-datahub-tc-0.10.0.2.tar.gz",
        "yanked": true,
        "yanked_reason": "0.10.0.2"
      }
    ],
    "0.10.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bbe2222085218d399842111affe3e349d2ba5e0ff8f5208868eeb299836ef06a",
          "md5": "d2579580e1c459d2cf4b027b2e988bb1",
          "sha256": "8b17efd78274d936ed0bbeb64cc0fc87bdd385d8b130af1f621509b1a6d8136e"
        },
        "downloads": -1,
        "filename": "acryl-datahub-tc-0.10.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "d2579580e1c459d2cf4b027b2e988bb1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 896047,
        "upload_time": "2023-03-20T14:40:43",
        "upload_time_iso_8601": "2023-03-20T14:40:43.956276Z",
        "url": "https://files.pythonhosted.org/packages/bb/e2/222085218d399842111affe3e349d2ba5e0ff8f5208868eeb299836ef06a/acryl-datahub-tc-0.10.0.3.tar.gz",
        "yanked": true,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5fb639fcb0b760e07195718a7991392b5e124c5e9a94324dd781e30a5e0f98f0",
        "md5": "1021fbead3d3bdf035c60bba69c40206",
        "sha256": "69a9a1c2333d97a173b5444b6ee0df6cf4274a30c6670af58a9e4119e27d1764"
      },
      "downloads": -1,
      "filename": "acryl-datahub-tc-0.10.0.0rc2.tar.gz",
      "has_sig": false,
      "md5_digest": "1021fbead3d3bdf035c60bba69c40206",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 896630,
      "upload_time": "2023-03-20T19:12:24",
      "upload_time_iso_8601": "2023-03-20T19:12:24.340916Z",
      "url": "https://files.pythonhosted.org/packages/5f/b6/39fcb0b760e07195718a7991392b5e124c5e9a94324dd781e30a5e0f98f0/acryl-datahub-tc-0.10.0.0rc2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}