{
  "info": {
    "author": "XuMing",
    "author_email": "xuming624@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "[![PyPI version](https://badge.fury.io/py/text2vec.svg)](https://badge.fury.io/py/text2vec)\n[![Downloads](https://pepy.tech/badge/text2vec)](https://pepy.tech/project/text2vec)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![GitHub contributors](https://img.shields.io/github/contributors/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/graphs/contributors)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.5%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)\n\n# Text2vec\ntext2vec, Text to Vector.\n\n文本向量表征工具，把文本转化为向量矩阵，是文本进行计算机处理的第一步。\n\n**text2vec**实现了Word2Vec、RankBM25、BERT、Sentence-BERT、CoSENT等多种文本表征、文本相似度计算模型，并在文本语义匹配（相似度计算）任务上比较了各模型的效果。\n\n\n**Guide**\n- [Question](#Question)\n- [Solution](#Solution)\n- [Feature](#Feature)\n- [Evaluation](#Evaluation)\n- [Install](#install)\n- [Usage](#usage)\n- [Contact](#Contact)\n- [Reference](#reference)\n\n# Question\n文本向量表示咋做？文本匹配任务用哪个模型效果好？\n\n许多NLP任务的成功离不开训练优质有效的文本表示向量。特别是文本语义匹配（Semantic Textual Similarity，如paraphrase检测、QA的问题对匹配）、文本向量检索（Dense Text Retrieval）等任务。\n# Solution\n### 传统方法：基于特征的匹配\n\n- 基于TF-IDF、BM25、Jaccord、SimHash、LDA等算法抽取两个文本的词汇、主题等层面的特征，然后使用机器学习模型（LR, xgboost）训练分类模型\n- 优点：可解释性较好\n- 缺点：依赖人工寻找特征，泛化能力一般，而且由于特征数量的限制，模型的效果比较一般\n\n代表模型：\n- BM25\n\nBM25算法，通过候选句子的字段对qurey字段的覆盖程度来计算两者间的匹配得分，得分越高的候选项与query的匹配度更好，主要解决词汇层面的相似度问题。\n\n### 深度方法：基于表征的匹配\n- 基于表征的匹配方式，初始阶段对两个文本各自单独处理，通过深层的神经网络进行编码（encode），得到文本的表征（embedding），再对两个表征进行相似度计算的函数得到两个文本的相似度\n- 优点：基于BERT的模型通过有监督的Fine-tune在文本表征和文本匹配任务取得了不错的性能\n- 缺点：BERT自身导出的句向量（不经过Fine-tune，对所有词向量求平均）质量较低，甚至比不上Glove的结果，因而难以反映出两个句子的语义相似度\n\n> 主要原因是：\n> \n> 1.BERT对所有的句子都倾向于编码到一个较小的空间区域内，这使得大多数的句子对都具有较高的相似度分数，即使是那些语义上完全无关的句子对。\n> \n> 2.BERT句向量表示的聚集现象和句子中的高频词有关。具体来说，当通过平均词向量的方式计算句向量时，那些高频词的词向量将会主导句向量，使之难以体现其原本的语义。当计算句向量时去除若干高频词时，聚集现象可以在一定程度上得到缓解，但表征能力会下降。\n\n\n代表模型：\n\n- [DSSM(2013)](https://posenhuang.github.io/papers/cikm2013_DSSM_fullversion.pdf)\n- [CDSSM(2014)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/www2014_cdssm_p07.pdf)\n- [ARC I(2014)](https://arxiv.org/pdf/1503.03244.pdf)\n- [Siamese Network(2016)](https://www.aclweb.org/anthology/W16-1617.pdf)\n- [InferSent(2017)](https://arxiv.org/pdf/1705.02364.pdf)\n- [BERT(2018)](https://arxiv.org/pdf/1810.04805.pdf)\n- [Sentence-BERT(2019)](https://arxiv.org/abs/1908.10084)\n- [BERT-flow(2020)](https://arxiv.org/abs/2011.05864)\n- [SimCSE(2021)](https://arxiv.org/abs/2104.08821)\n- [ConSERT(2021)](https://aclanthology.org/2021.acl-long.393/)\n- [CoSENT(2022)](https://kexue.fm/archives/8847)\n\n由于2018年BERT模型在NLP界带来了翻天覆地的变化，此处不讨论和比较2018年之前的模型（如果有兴趣了解的同学，可以参考中科院开源的[MatchZoo](https://github.com/NTMC-Community/MatchZoo) 和[MatchZoo-py](https://github.com/NTMC-Community/MatchZoo-py)）。\n\n所以，本项目主要调研以下比原生BERT更优、适合文本匹配的向量表示模型：Sentence-BERT(2019)、BERT-flow(2020)、SimCSE(2021)、CoSENT(2022)。\n\n### 深度方法：基于交互的匹配\n\n- 基于交互的匹配方式，则认为在最后阶段才计算文本的相似度会过于依赖文本表征的质量，同时也会丢失基础的文本特征（比如词法、句法等），所以提出尽可能早的对文本特征进行交互，捕获更基础的特征，最后在高层基于这些基础匹配特征计算匹配分数\n- 优点：基于交互的匹配模型端到端处理，效果好\n- 缺点：这类模型（Cross-Encoder）的输入要求是两个句子，输出的是句子对的相似度值，模型不会产生句子向量表示（sentence embedding），我们也无法把单个句子输入给模型。因此，对于需要文本向量表示的任务来说，这类模型并不实用\n\n\n代表模型：\n\n- [ARC II(2014)](https://arxiv.org/pdf/1503.03244.pdf)\n- [MV-LSTM(2015)](https://arxiv.org/pdf/1511.08277.pdf)\n- [MatchPyramid(2016)](https://arxiv.org/pdf/1602.06359.pdf)\n- [DRMM(2016)](https://www.bigdatalab.ac.cn/~gjf/papers/2016/CIKM2016a_guo.pdf)\n- [Conv-KNRM(2018)](https://www.cs.cmu.edu/~zhuyund/papers/WSDM_2018_Dai.pdf)\n- [RE2(2019)](https://www.aclweb.org/anthology/P19-1465.pdf)\n- [Keyword-BERT(2020)](https://arxiv.org/ftp/arxiv/papers/2003/2003.11516.pdf)\n\nCross-Encoder适用于向量检索精排。\n\n# Feature\n### 文本向量表示模型\n- [Word2Vec](text2vec/word2vec.py)：通过腾讯AI Lab开源的大规模高质量中文[词向量数据（800万中文词轻量版）](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) (文件名：light_Tencent_AILab_ChineseEmbedding.bin 密码: tawe）实现词向量检索，本项目实现了句子（词向量求平均）的word2vec向量表示\n- [SBERT(Sentence-BERT)](text2vec/sentencebert_model.py)：权衡性能和效率的句向量表示模型，训练时通过有监督训练上层分类函数，文本匹配预测时直接句子向量做余弦，本项目基于PyTorch复现了Sentence-BERT模型的训练和预测\n- [CoSENT(Cosine Sentence)](text2vec/cosent_model.py)：CoSENT模型提出了一种排序的损失函数，使训练过程更贴近预测，模型收敛速度和效果比Sentence-BERT更好，本项目基于PyTorch实现了CoSENT模型的训练和预测\n\n# Evaluation\n\n### 文本匹配\n\n- 英文匹配数据集的评测结果：\n\n| Arch | Backbone | Model Name | English-STS-B | \n| :-- | :--- | :--- | :-: |\n| GloVe | glove | Avg_word_embeddings_glove_6B_300d | 61.77 |\n| BERT | bert-base-uncased | BERT-base-cls | 20.29 |\n| BERT | bert-base-uncased | BERT-base-first_last_avg | 59.04 |\n| BERT | bert-base-uncased | BERT-base-first_last_avg-whiten(NLI) | 63.65 |\n| SBERT | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-cls | 73.65 |\n| SBERT | sentence-transformers/bert-base-nli-mean-tokens | SBERT-base-nli-first_last_avg | 77.96 |\n| SBERT | xlm-roberta-base | paraphrase-multilingual-MiniLM-L12-v2 | 84.42 |\n| CoSENT | bert-base-uncased | CoSENT-base-first_last_avg | 69.93 |\n| CoSENT | sentence-transformers/bert-base-nli-mean-tokens | CoSENT-base-nli-first_last_avg | 79.68 |\n\n- 中文匹配数据集的评测结果：\n\n| Arch | Backbone | Model Name | ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | QPS |\n| :-- | :--- | :--- | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| CoSENT | hfl/chinese-macbert-base | CoSENT-macbert-base | 50.39 | **72.93** | **79.17** | **60.86** | **80.51** | **68.77**  | 3008 |\n| CoSENT | Langboat/mengzi-bert-base | CoSENT-mengzi-base | **50.52** | 72.27 | 78.69 | 12.89 | 80.15 | 58.90 | 2502 |\n| CoSENT | bert-base-chinese | CoSENT-bert-base | 49.74 | 72.38 | 78.69 | 60.00 | 80.14 | 68.19 | 2653 |\n| SBERT | bert-base-chinese | SBERT-bert-base | 46.36 | 70.36 | 78.72 | 46.86 | 66.41 | 61.74 | 3365 |\n| SBERT | hfl/chinese-macbert-base | SBERT-macbert-base | 47.28 | 68.63 | **79.42** | 55.59 | 64.82 | 63.15 | 2948 |\n| CoSENT | hfl/chinese-roberta-wwm-ext | CoSENT-roberta-ext | **50.81** | **71.45** | **79.31** | **61.56** | **81.13** | **68.85** | - |\n| SBERT | hfl/chinese-roberta-wwm-ext | SBERT-roberta-ext | 48.29 | 69.99 | 79.22 | 44.10 | 72.42 | 62.80 | - |\n\n- 本项目release模型的中文匹配评测结果：\n\n| Arch | Backbone | Model Name | ATEC | BQ | LCQMC | PAWSX | STS-B | Avg | QPS |\n| :-- | :--- | :---- | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n| Word2Vec | word2vec | w2v-light-tencent-chinese | 20.00 | 31.49 | 59.46 | 2.57 | 55.78 | 33.86 | 23769 |\n| SBERT | xlm-roberta-base | paraphrase-multilingual-MiniLM-L12-v2 | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 | 41.99 | 3138 |\n| CoSENT | hfl/chinese-macbert-base | text2vec-base-chinese | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 | **48.25** | 3008 |\n\n说明：\n- 结果值均使用spearman系数\n- 结果均只用该数据集的train训练，在test上评估得到的表现，没用外部数据\n- `CoSENT-macbert-base`模型达到同级别参数量SOTA效果，是用CoSENT方法训练，运行[examples/training_sup_text_matching_model.py](examples/training_sup_text_matching_model.py)代码可在各数据集复现结果\n- `text2vec-base-chinese`模型，是用CoSENT方法训练，基于MacBERT在中文STS-B数据训练得到，并在中文STS-B测试集评估达到SOTA，模型文件已经上传到huggingface的模型库[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)，中文语义匹配任务推荐使用\n- `SBERT-macbert-base`模型，是用SBERT方法训练，运行[examples/training_sup_text_matching_model.py](examples/training_sup_text_matching_model.py)代码复现结果\n- `paraphrase-multilingual-MiniLM-L12-v2`模型名称是`sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`，是用SBERT训练，是`paraphrase-MiniLM-L12-v2`模型的多语言版本，支持中文、英文等\n- `w2v-light-tencent-chinese`是腾讯词向量的Word2Vec模型，CPU加载使用，适用于中文字面匹配任务和缺少数据的冷启动情况\n- 各预训练模型均可以通过transformers调用，如MacBERT模型：`--model_name hfl/chinese-macbert-base`\n- 中文匹配数据集下载[链接见下方](#数据集)\n- 中文匹配任务实验表明，pooling最优是`first_last_avg`，即 SentenceModel 的`EncoderType.FIRST_LAST_AVG`，其与`EncoderType.MEAN`的方法在预测效果上差异很小\n- QPS的GPU测试环境是Tesla V100，显存32GB\n\n# Demo\n\nOfficial Demo: http://42.193.145.218/product/short_text_sim/\n\nHuggingFace Demo: https://huggingface.co/spaces/shibing624/text2vec\n\n![](docs/hf.png)\n\nrun example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:\n```shell\npython examples/gradio_demo.py\n```\n\n# Install\n```shell\npip install torch # conda install pytorch\npip install -U text2vec\n```\n\nor\n\n```shell\npip install torch # conda install pytorch\npip install -r requirements.txt\n\ngit clone https://github.com/shibing624/text2vec.git\ncd text2vec\npip install --no-deps .\n```\n\n### 数据集\n中文语义匹配数据集已经上传到huggingface datasets [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)\n\n数据集使用示例：\n```shell\npip install datasets\n```\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"shibing624/nli_zh\", \"STS-B\") # ATEC or BQ or LCQMC or PAWSX or STS-B\nprint(dataset)\nprint(dataset['test'][0])\n```\n\noutput:\n```shell\nDatasetDict({\n    train: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 5231\n    })\n    validation: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1458\n    })\n    test: Dataset({\n        features: ['sentence1', 'sentence2', 'label'],\n        num_rows: 1361\n    })\n})\n{'sentence1': '一个女孩在给她的头发做发型。', 'sentence2': '一个女孩在梳头。', 'label': 2}\n```\n\n常见中文语义匹配数据集，包含[ATEC](https://github.com/IceFlameWorm/NLP_Datasets/tree/master/ATEC)、[BQ](http://icrc.hitsz.edu.cn/info/1037/1162.htm)、\n[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html)、[PAWSX](https://arxiv.org/abs/1908.11828)、[STS-B](https://github.com/pluto-junzeng/CNSD)共5个任务。\n可以从数据集对应的链接自行下载，也可以从[百度网盘(提取码:qkt6)](https://pan.baidu.com/s/1d6jSiU1wHQAEMWJi7JJWCQ)下载。\n其中senteval_cn目录是评测数据集汇总，senteval_cn.zip是senteval目录的打包，两者下其一就好。\n\n# Usage\n\n## 文本向量表征\n\n基于`pretrained model`计算文本向量：\n\n```shell\n>>> from text2vec import SentenceModel\n>>> m = SentenceModel()\n>>> m.encode(\"如何更换花呗绑定银行卡\")\nEmbedding shape: (768,)\n```\n\nexample: [examples/computing_embeddings_demo.py](examples/computing_embeddings_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel, EncoderType\nfrom text2vec import Word2Vec\n\n\ndef compute_emb(model):\n    # Embed a list of sentences\n    sentences = [\n        '卡',\n        '银行卡',\n        '如何更换花呗绑定银行卡',\n        '花呗更改绑定银行卡',\n        'This framework generates embeddings for each input sentence',\n        'Sentences are passed as a list of string.',\n        'The quick brown fox jumps over the lazy dog.'\n    ]\n    sentence_embeddings = model.encode(sentences)\n    print(type(sentence_embeddings), sentence_embeddings.shape)\n\n    # The result is a list of sentence embeddings as numpy arrays\n    for sentence, embedding in zip(sentences, sentence_embeddings):\n        print(\"Sentence:\", sentence)\n        print(\"Embedding shape:\", embedding.shape)\n        print(\"Embedding head:\", embedding[:10])\n        print()\n\n\nif __name__ == \"__main__\":\n    # 中文句向量模型(CoSENT)，中文语义匹配任务推荐，支持fine-tune继续训练\n    t2v_model = SentenceModel(\"shibing624/text2vec-base-chinese\",\n                              encoder_type=EncoderType.FIRST_LAST_AVG)\n    compute_emb(t2v_model)\n\n    # 支持多语言的句向量模型（Sentence-BERT），英文语义匹配任务推荐，支持fine-tune继续训练\n    sbert_model = SentenceModel(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n                                encoder_type=EncoderType.MEAN)\n    compute_emb(sbert_model)\n\n    # 中文词向量模型(word2vec)，中文字面匹配任务和冷启动适用\n    w2v_model = Word2Vec(\"w2v-light-tencent-chinese\")\n    compute_emb(w2v_model)\n\n```\n\noutput:\n```\n<class 'numpy.ndarray'> (7, 768)\nSentence: 卡\nEmbedding shape: (768,)\n\nSentence: 银行卡\nEmbedding shape: (768,)\n ... \n```\n\n- 返回值`embeddings`是`numpy.ndarray`类型，shape为`(sentences_size, model_embedding_size)`\n- `shibing624/text2vec-base-chinese`模型是CoSENT方法在中文STS-B数据集训练得到的，模型已经上传到huggingface的\n模型库[shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)，\n是`text2vec.SentenceModel`指定的默认模型，可以通过上面示例调用，或者如下所示用[transformers库](https://github.com/huggingface/transformers)调用，\n模型自动下载到本机路径：`~/.cache/huggingface/transformers`\n- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`模型是Sentence-BERT的多语言句向量模型，\n适用于释义（paraphrase）识别，文本匹配，通过`text2vec.SentenceModel`和[sentence-transformers库]((https://github.com/UKPLab/sentence-transformers))都可以调用该模型\n- `w2v-light-tencent-chinese`是通过gensim加载的Word2Vec模型，使用腾讯词向量`Tencent_AILab_ChineseEmbedding.tar.gz`计算各字词的词向量，句子向量通过单词词\n向量取平均值得到，模型自动下载到本机路径：`~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n\n#### Usage (HuggingFace Transformers)\nWithout [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: \n\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\nexample: [examples/use_origin_transformers_demo.py](examples/use_origin_transformers_demo.py)\n\n```python\nimport os\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('shibing624/text2vec-base-chinese')\nmodel = AutoModel.from_pretrained('shibing624/text2vec-base-chinese')\nsentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### Usage (sentence-transformers)\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.\n\nInstall sentence-transformers:\n```shell\npip install -U sentence-transformers\n```\nThen load model and predict:\n```python\nfrom sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer(\"shibing624/text2vec-base-chinese\")\nsentences = ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']\n\nsentence_embeddings = m.encode(sentences)\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)\n```\n\n#### `Word2Vec`词向量\n\n提供两种`Word2Vec`词向量，任选一个：\n\n  - 轻量版腾讯词向量 [百度云盘-密码:tawe](https://pan.baidu.com/s/1La4U4XNFe8s5BJqxPQpeiQ) 或 [谷歌云盘](https://drive.google.com/u/0/uc?id=1iQo9tBb2NgFOBxx0fA16AZpSgc-bG_Rp&export=download)，二进制，运行程序，自动下载到 `~/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin`\n  - [腾讯词向量-官方全量](https://ai.tencent.com/ailab/nlp/zh/download.html), 6.78G放到： `~/.text2vec/datasets/Tencent_AILab_ChineseEmbedding.txt`，腾讯词向量主页：https://ai.tencent.com/ailab/nlp/zh/embedding.html 词向量下载地址：https://ai.tencent.com/ailab/nlp/zh/data/Tencent_AILab_ChineseEmbedding.tar.gz  更多查看[腾讯词向量介绍-wiki](https://github.com/shibing624/text2vec/wiki/%E8%85%BE%E8%AE%AF%E8%AF%8D%E5%90%91%E9%87%8F%E4%BB%8B%E7%BB%8D)\n\n\n\n## 下游任务\n### 1. 句子相似度计算\n\nexample: [examples/semantic_text_similarity_demo.py](examples/semantic_text_similarity_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import Similarity\n\n# Two lists of sentences\nsentences1 = ['如何更换花呗绑定银行卡',\n              'The cat sits outside',\n              'A man is playing guitar',\n              'The new movie is awesome']\n\nsentences2 = ['花呗更改绑定银行卡',\n              'The dog plays in the garden',\n              'A woman watches TV',\n              'The new movie is so great']\n\nsim_model = Similarity()\nfor i in range(len(sentences1)):\n    for j in range(len(sentences2)):\n        score = sim_model.get_score(sentences1[i], sentences2[j])\n        print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[j], score))\n```\n\noutput:\n```shell\n如何更换花呗绑定银行卡 \t\t 花呗更改绑定银行卡 \t\t Score: 0.9477\n如何更换花呗绑定银行卡 \t\t The dog plays in the garden \t\t Score: -0.1748\n如何更换花呗绑定银行卡 \t\t A woman watches TV \t\t Score: -0.0839\n如何更换花呗绑定银行卡 \t\t The new movie is so great \t\t Score: -0.0044\nThe cat sits outside \t\t 花呗更改绑定银行卡 \t\t Score: -0.0097\nThe cat sits outside \t\t The dog plays in the garden \t\t Score: 0.1908\nThe cat sits outside \t\t A woman watches TV \t\t Score: -0.0203\nThe cat sits outside \t\t The new movie is so great \t\t Score: 0.0302\nA man is playing guitar \t\t 花呗更改绑定银行卡 \t\t Score: -0.0010\nA man is playing guitar \t\t The dog plays in the garden \t\t Score: 0.1062\nA man is playing guitar \t\t A woman watches TV \t\t Score: 0.0055\nA man is playing guitar \t\t The new movie is so great \t\t Score: 0.0097\nThe new movie is awesome \t\t 花呗更改绑定银行卡 \t\t Score: 0.0302\nThe new movie is awesome \t\t The dog plays in the garden \t\t Score: -0.0160\nThe new movie is awesome \t\t A woman watches TV \t\t Score: 0.1321\nThe new movie is awesome \t\t The new movie is so great \t\t Score: 0.9591\n```\n\n> 句子余弦相似度值`score`范围是[-1, 1]，值越大越相似。\n\n### 2. 文本匹配搜索\n\n一般在文档候选集中找与query最相似的文本，常用于QA场景的问句相似匹配、文本相似检索等任务。\n\n\nexample: [examples/semantic_search_demo.py](examples/semantic_search_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom text2vec import SentenceModel, cos_sim, semantic_search\n\nembedder = SentenceModel()\n\n# Corpus with example sentences\ncorpus = [\n    '花呗更改绑定银行卡',\n    '我什么时候开通了花呗',\n    'A man is eating food.',\n    'A man is eating a piece of bread.',\n    'The girl is carrying a baby.',\n    'A man is riding a horse.',\n    'A woman is playing violin.',\n    'Two men pushed carts through the woods.',\n    'A man is riding a white horse on an enclosed ground.',\n    'A monkey is playing drums.',\n    'A cheetah is running behind its prey.'\n]\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = [\n    '如何更换花呗绑定银行卡',\n    'A man is eating pasta.',\n    'Someone in a gorilla costume is playing a set of drums.',\n    'A cheetah chases prey on across a field.']\n\nfor query in queries:\n    query_embedding = embedder.encode(query)\n    hits = semantic_search(query_embedding, corpus_embeddings, top_k=5)\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n    hits = hits[0]  # Get the hits for the first query\n    for hit in hits:\n        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n```\noutput:\n```shell\nQuery: 如何更换花呗绑定银行卡\nTop 5 most similar sentences in corpus:\n花呗更改绑定银行卡 (Score: 0.9477)\n我什么时候开通了花呗 (Score: 0.3635)\nA man is eating food. (Score: 0.0321)\nA man is riding a horse. (Score: 0.0228)\nTwo men pushed carts through the woods. (Score: 0.0090)\n\n======================\nQuery: A man is eating pasta.\nTop 5 most similar sentences in corpus:\nA man is eating food. (Score: 0.6734)\nA man is eating a piece of bread. (Score: 0.4269)\nA man is riding a horse. (Score: 0.2086)\nA man is riding a white horse on an enclosed ground. (Score: 0.1020)\nA cheetah is running behind its prey. (Score: 0.0566)\n\n======================\nQuery: Someone in a gorilla costume is playing a set of drums.\nTop 5 most similar sentences in corpus:\nA monkey is playing drums. (Score: 0.8167)\nA cheetah is running behind its prey. (Score: 0.2720)\nA woman is playing violin. (Score: 0.1721)\nA man is riding a horse. (Score: 0.1291)\nA man is riding a white horse on an enclosed ground. (Score: 0.1213)\n\n======================\nQuery: A cheetah chases prey on across a field.\nTop 5 most similar sentences in corpus:\nA cheetah is running behind its prey. (Score: 0.9147)\nA monkey is playing drums. (Score: 0.2655)\nA man is riding a horse. (Score: 0.1933)\nA man is riding a white horse on an enclosed ground. (Score: 0.1733)\nA man is eating food. (Score: 0.0329)\n```\n\n\n## 下游任务支持库\n**similarities库[推荐]**\n\n文本相似度计算和文本匹配搜索任务，推荐使用 [similarities库](https://github.com/shibing624/similarities) ，兼容本项目release的\nWord2vec、SBERT、Cosent类语义匹配模型，还支持字面维度相似度计算、匹配搜索算法，支持文本、图像。\n\n安装：\n```pip install -U similarities```\n\n句子相似度计算：\n```python\nfrom similarities import Similarity\n\nm = Similarity()\nr = m.similarity('如何更换花呗绑定银行卡', '花呗更改绑定银行卡')\nprint(f\"similarity score: {float(r)}\")  # similarity score: 0.855146050453186\n```\n\n# Models\n\n## CoSENT model\n\nCoSENT（Cosine Sentence）文本匹配模型，在Sentence-BERT上改进了CosineRankLoss的句向量方案\n\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/cosent_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/inference.png\" width=\"300\" />\n\n#### CoSENT 监督模型\n训练和预测，最简示例:\n\n```python\nfrom text2vec import CosentModel\nm = CosentModel(\"bert-base-chinese\")\nprint(m)\nm.train_model(use_hf_dataset=True, num_epochs=1, output_dir=\"./temp\")\nr = m.encode([\"我爱北京天安门\"])\nprint(r)\n```\n\n- 在中文STS-B数据集训练和评估`MacBERT+CoSENT`模型\n\nexample: [examples/training_sup_text_matching_model.py](examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-cosent\n```\n\n- 在蚂蚁金融匹配数据集ATEC上训练和评估`MacBERT+CoSENT`模型\n\n支持这些中文匹配数据集的使用：'ATEC', 'STS-B', 'BQ', 'LCQMC', 'PAWSX'，具体参考HuggingFace datasets [https://huggingface.co/datasets/shibing624/nli_zh](https://huggingface.co/datasets/shibing624/nli_zh)\n```shell\npython training_sup_text_matching_model.py --task_name ATEC --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/ATEC-cosent\n```\n\n- 在自有中文数据集上训练模型\n\nexample: [examples/training_sup_text_matching_model_selfdata.py](examples/training_sup_text_matching_model_selfdata.py)\n\n```shell\npython training_sup_text_matching_model_selfdata.py --do_train --do_predict\n```\n\n- 在英文STS-B数据集训练和评估`BERT+CoSENT`模型\n\nexample: [examples/training_sup_text_matching_model_en.py](examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased  --output_dir ./outputs/STS-B-en-cosent\n```\n\n#### CoSENT 无监督模型\n- 在英文NLI数据集训练`BERT+CoSENT`模型，在STS-B测试集评估效果\n\nexample: [examples/training_unsup_text_matching_model_en.py](examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch cosent --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-cosent\n```\n\n\n## Sentence-BERT model\n\nSentence-BERT文本匹配模型，表征式句向量表示方案\n\nNetwork structure:\n\nTraining:\n\n<img src=\"docs/sbert_train.png\" width=\"300\" />\n\n\nInference:\n\n<img src=\"docs/sbert_inference.png\" width=\"300\" />\n\n#### SentenceBERT 监督模型\n- 在中文STS-B数据集训练和评估`MacBERT+SBERT`模型\n\nexample: [examples/training_sup_text_matching_model.py](examples/training_sup_text_matching_model.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name hfl/chinese-macbert-base --output_dir ./outputs/STS-B-sbert\n```\n- 在英文STS-B数据集训练和评估`BERT+SBERT`模型\n\nexample: [examples/training_sup_text_matching_model_en.py](examples/training_sup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_sup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-sbert\n```\n\n#### SentenceBERT 无监督模型\n- 在英文NLI数据集训练`BERT+SBERT`模型，在STS-B测试集评估效果\n\nexample: [examples/training_unsup_text_matching_model_en.py](examples/training_unsup_text_matching_model_en.py)\n\n```shell\ncd examples\npython training_unsup_text_matching_model_en.py --model_arch sentencebert --do_train --do_predict --num_epochs 10 --model_name bert-base-uncased --output_dir ./outputs/STS-B-en-unsup-sbert\n```\n\n## BERT-Match model\nBERT文本匹配模型，原生BERT匹配网络结构，交互式句向量匹配模型\n\nNetwork structure:\n\nTraining and inference:\n\n<img src=\"docs/bert-fc-train.png\" width=\"300\" />\n\n训练脚本同上[examples/training_sup_text_matching_model.py](examples/training_sup_text_matching_model.py)。\n\n\n## 模型蒸馏（Model Distillation）\n\n由于text2vec训练的模型可以使用[sentence-transformers](https://github.com/UKPLab/sentence-transformers)库加载，此处复用其模型蒸馏方法[distillation](https://github.com/UKPLab/sentence-transformers/tree/master/examples/training/distillation)。\n\n1. 模型降维，参考[dimensionality_reduction.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/dimensionality_reduction.py)使用PCA对模型输出embedding降维，可减少milvus等向量检索数据库的存储压力，还能轻微提升模型效果。\n2. 模型蒸馏，参考[model_distillation.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/distillation/model_distillation.py)使用蒸馏方法，将Teacher大模型蒸馏到更少layers层数的student模型中，在权衡效果的情况下，可大幅提升模型预测速度。\n\n## 模型部署\n\n提供两种部署模型，搭建服务的方法： 1）基于Jina搭建gRPC服务【推荐】；2）基于FastAPI搭建原生Http服务。\n\n### Jina服务\n采用C/S模式搭建高性能服务，支持docker云原生，gRPC/HTTP/WebSocket，支持多个模型同时预测，GPU多卡处理。\n\n- 安装：\n```pip install jina```\n\n- 启动服务：\n\nexample: [examples/jina_server_demo.py](examples/jina_server_demo.py)\n```python\nfrom jina import Flow\n\nport = 50001\nf = Flow(port=port).add(\n    uses='jinahub://Text2vecEncoder',\n    uses_with={'model_name': 'shibing624/text2vec-base-chinese'}\n)\n\nwith f:\n    # backend server forever\n    f.block()\n```\n\n该模型预测方法（executor）已经上传到[JinaHub](https://hub.jina.ai/executor/eq45c9uq)，里面包括docker、k8s部署方法。\n\n- 调用服务：\n\n\n```python\nfrom jina import Client\nfrom docarray import Document, DocumentArray\n\nport = 50001\n\nc = Client(port=port)\n\ndata = ['如何更换花呗绑定银行卡',\n        '花呗更改绑定银行卡']\nprint(\"data:\", data)\nprint('data embs:')\nr = c.post('/', inputs=DocumentArray([Document(text='如何更换花呗绑定银行卡'), Document(text='花呗更改绑定银行卡')]))\nprint(r.embeddings)\n```\n\n批量调用方法见example: [examples/jina_client_demo.py](examples/jina_client_demo.py)\n\n\n### FastAPI服务\n\n- 安装：\n```pip install fastapi uvicorn```\n\n- 启动服务：\n\nexample: [examples/fastapi_server_demo.py](examples/fastapi_server_demo.py)\n```shell\ncd examples\npython fastapi_server_demo.py\n```\n\n- 调用服务：\n```shell\ncurl -X 'GET' \\\n  'http://0.0.0.0:8001/emb?q=hello' \\\n  -H 'accept: application/json'\n```\n\n# Contact\n\n- Issue(建议)：[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec.svg)](https://github.com/shibing624/text2vec/issues)\n- 邮件我：xuming: xuming624@qq.com\n- 微信我：\n加我*微信号：xuming624, 备注：姓名-公司-NLP* 进NLP交流群。\n\n<img src=\"docs/wechat.jpeg\" width=\"200\" />\n\n\n# Citation\n\n如果你在研究中使用了text2vec，请按如下格式引用：\n\nAPA:\n```latex\nXu, M. Text2vec: Text to vector toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/text2vec\n```\n\nBibTeX:\n```latex\n@software{Xu_Text2vec_Text_to,\nauthor = {Xu, Ming},\ntitle = {{Text2vec: Text to vector toolkit}},\nurl = {https://github.com/shibing624/text2vec},\nversion = {1.1.2}\n}\n```\n\n# License\n\n\n授权协议为 [The Apache License 2.0](LICENSE)，可免费用做商业用途。请在产品说明中附加text2vec的链接和授权协议。\n\n\n# Contribute\n项目代码还很粗糙，如果大家对代码有所改进，欢迎提交回本项目，在提交之前，注意以下两点：\n\n - 在`tests`添加相应的单元测试\n - 使用`python -m pytest -v`来运行所有单元测试，确保所有单测都是通过的\n\n之后即可提交PR。\n\n# Reference\n- [将句子表示为向量（上）：无监督句子表示学习（sentence embedding）](https://www.cnblogs.com/llhthinker/p/10335164.html)\n- [将句子表示为向量（下）：无监督句子表示学习（sentence embedding）](https://www.cnblogs.com/llhthinker/p/10341841.html)\n- [A Simple but Tough-to-Beat Baseline for Sentence Embeddings[Sanjeev Arora and Yingyu Liang and Tengyu Ma, 2017]](https://openreview.net/forum?id=SyK00v5xx)\n- [四种计算文本相似度的方法对比[Yves Peirsman]](https://zhuanlan.zhihu.com/p/37104535)\n- [Improvements to BM25 and Language Models Examined](http://www.cs.otago.ac.nz/homepages/andrew/papers/2014-2.pdf)\n- [CoSENT：比Sentence-BERT更有效的句向量方案](https://kexue.fm/archives/8847)\n- [谈谈文本匹配和多轮检索](https://zhuanlan.zhihu.com/p/111769969)\n- [Sentence-transformers](https://www.sbert.net/examples/applications/computing-embeddings/README.html)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/shibing624/text2vec",
    "keywords": "word embedding,text2vec,Chinese Text Similarity Calculation Tool,similarity,word2vec",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "text2vec",
    "package_url": "https://pypi.org/project/text2vec/",
    "platform": null,
    "project_url": "https://pypi.org/project/text2vec/",
    "project_urls": {
      "Homepage": "https://github.com/shibing624/text2vec"
    },
    "release_url": "https://pypi.org/project/text2vec/1.1.7/",
    "requires_dist": null,
    "requires_python": ">=3.6.0",
    "summary": "Text to vector Tool, encode text",
    "version": "1.1.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14506313,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "015004c8c6774bf0559ffa1df92f0727ae9a91e119c5e1df7515b86927f7cec8",
          "md5": "93c4f54b7b6a164960049fe0cfef4b96",
          "sha256": "07258f6ad2083e8e380ba19c6603492c0046d8307b3e7eeb59d20ecf1cb0a8f9"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.1-py3.6.egg",
        "has_sig": false,
        "md5_digest": "93c4f54b7b6a164960049fe0cfef4b96",
        "packagetype": "bdist_egg",
        "python_version": "3.6",
        "requires_python": null,
        "size": 129198,
        "upload_time": "2019-12-02T14:10:37",
        "upload_time_iso_8601": "2019-12-02T14:10:37.221810Z",
        "url": "https://files.pythonhosted.org/packages/01/50/04c8c6774bf0559ffa1df92f0727ae9a91e119c5e1df7515b86927f7cec8/text2vec-0.1.1-py3.6.egg",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9699c9b244f77d9afcd576790677dd73347a84ff40da5ca03737c0e39ea31dd1",
          "md5": "54f731bdb4bbcaa96ffa3e3b37a73b1d",
          "sha256": "d139d543c7cc201c53c32765d329c3c11e8daf55bdef8988b44475ac008bf458"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "54f731bdb4bbcaa96ffa3e3b37a73b1d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 47919,
        "upload_time": "2019-12-06T05:50:38",
        "upload_time_iso_8601": "2019-12-06T05:50:38.992294Z",
        "url": "https://files.pythonhosted.org/packages/96/99/c9b244f77d9afcd576790677dd73347a84ff40da5ca03737c0e39ea31dd1/text2vec-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1ab09f37ddc86cf0933036a86bd925c47851e43f0486884421a65aae6b128651",
          "md5": "1e5afa529a38a64bbee6093ab58a8344",
          "sha256": "839c407f4e5d78d3483845e9f1b846da1c496c80eabdebbd853910b9b68fadbc"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "1e5afa529a38a64bbee6093ab58a8344",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 52444,
        "upload_time": "2019-12-07T03:33:56",
        "upload_time_iso_8601": "2019-12-07T03:33:56.143864Z",
        "url": "https://files.pythonhosted.org/packages/1a/b0/9f37ddc86cf0933036a86bd925c47851e43f0486884421a65aae6b128651/text2vec-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "593598c4a7a8ad54cd978ba54d5159acd68d5317558df1d1efae000c4c52023e",
          "md5": "859b94311c573391b9c168a34f43273f",
          "sha256": "3a949380bc272b0a0ff26de69e3cf38d471f38d72042718f735108839a353314"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "859b94311c573391b9c168a34f43273f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 61298,
        "upload_time": "2020-03-02T06:39:19",
        "upload_time_iso_8601": "2020-03-02T06:39:19.722251Z",
        "url": "https://files.pythonhosted.org/packages/59/35/98c4a7a8ad54cd978ba54d5159acd68d5317558df1d1efae000c4c52023e/text2vec-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1fba19d2e41d54ffd2a93362ce8f43c3d69e9a070993b6dfaf5a46bff1a5c2f2",
          "md5": "f07145b27d006ddb51edfbbfb257bd90",
          "sha256": "cb511a8d174cf2fe97c54ebbbd13f96a98758220eb436e4eaeb71a03f96cabae"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "f07145b27d006ddb51edfbbfb257bd90",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 61299,
        "upload_time": "2020-06-21T09:14:39",
        "upload_time_iso_8601": "2020-06-21T09:14:39.560479Z",
        "url": "https://files.pythonhosted.org/packages/1f/ba/19d2e41d54ffd2a93362ce8f43c3d69e9a070993b6dfaf5a46bff1a5c2f2/text2vec-0.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e3a008cf70db6523bbea92a921d4cbc902b118bb729108aeba37292a5738016c",
          "md5": "50da757478892f6c0e8154db16e94042",
          "sha256": "199a2bea2381fd12a4d5dd40f565e668a7e2cd042239a097dab100c94f047022"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "50da757478892f6c0e8154db16e94042",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 61930,
        "upload_time": "2020-12-01T13:31:12",
        "upload_time_iso_8601": "2020-12-01T13:31:12.447911Z",
        "url": "https://files.pythonhosted.org/packages/e3/a0/08cf70db6523bbea92a921d4cbc902b118bb729108aeba37292a5738016c/text2vec-0.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "420cc8dc6a35896976a368915b8732e24ff287eafa1acf4ac1ee9f3766907a5b",
          "md5": "3b625f1d4400d9a4e3d7ce3e31d183b2",
          "sha256": "6358a3915d7973c9fb420075f40de4378f5b1ba6eb515252825290445c240efb"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "3b625f1d4400d9a4e3d7ce3e31d183b2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 61475,
        "upload_time": "2021-04-14T14:15:24",
        "upload_time_iso_8601": "2021-04-14T14:15:24.864634Z",
        "url": "https://files.pythonhosted.org/packages/42/0c/c8dc6a35896976a368915b8732e24ff287eafa1acf4ac1ee9f3766907a5b/text2vec-0.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dbe8444421be14fe249f99af1a418043bc75e165a1e7871d725fe225b3ebdbcd",
          "md5": "dc74eb8ecc8459b2055a86770ead4f68",
          "sha256": "412ec793e392d1c6b3915ca01caeb4217b514c244681a8f1f0b6f23ce62136c4"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.8.tar.gz",
        "has_sig": false,
        "md5_digest": "dc74eb8ecc8459b2055a86770ead4f68",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 61814,
        "upload_time": "2021-05-21T11:49:55",
        "upload_time_iso_8601": "2021-05-21T11:49:55.315855Z",
        "url": "https://files.pythonhosted.org/packages/db/e8/444421be14fe249f99af1a418043bc75e165a1e7871d725fe225b3ebdbcd/text2vec-0.1.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0352f153b18954712ccbfc7257a860e7fbb7426323ae486b7a6ec7129d7ed26f",
          "md5": "69de1c7c4c1af24d25d9ce7085f2b087",
          "sha256": "db2b3770f9595416f47293e890b1b42a8f753a00c15b6cb5bc33ed618338378e"
        },
        "downloads": -1,
        "filename": "text2vec-0.1.9.tar.gz",
        "has_sig": false,
        "md5_digest": "69de1c7c4c1af24d25d9ce7085f2b087",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 67316,
        "upload_time": "2021-05-21T11:56:23",
        "upload_time_iso_8601": "2021-05-21T11:56:23.118914Z",
        "url": "https://files.pythonhosted.org/packages/03/52/f153b18954712ccbfc7257a860e7fbb7426323ae486b7a6ec7129d7ed26f/text2vec-0.1.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "87c6348806fe5b51a885da96e201dab32480fc7dffbbb7b793fa797a31ec775f",
          "md5": "b6bc9033d81895846fc3198a145abdb4",
          "sha256": "141521b37a43a09ece5de7f33d5a60f705669b94b20142bdec9e4c138eda799d"
        },
        "downloads": -1,
        "filename": "text2vec-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b6bc9033d81895846fc3198a145abdb4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 35983,
        "upload_time": "2021-08-17T05:58:53",
        "upload_time_iso_8601": "2021-08-17T05:58:53.974268Z",
        "url": "https://files.pythonhosted.org/packages/87/c6/348806fe5b51a885da96e201dab32480fc7dffbbb7b793fa797a31ec775f/text2vec-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6a9330e944ecc1806db299595aba0777e71baffbe8061742ea76c8fb5c9f174a",
          "md5": "1ee7f429306580c006643bd6af9e5ce2",
          "sha256": "507a87d6f782bffdf32ed247254b1f46fd9b1c0598256cedb58dc6c8ecfca4fe"
        },
        "downloads": -1,
        "filename": "text2vec-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1ee7f429306580c006643bd6af9e5ce2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 36018,
        "upload_time": "2021-09-04T02:38:13",
        "upload_time_iso_8601": "2021-09-04T02:38:13.394782Z",
        "url": "https://files.pythonhosted.org/packages/6a/93/30e944ecc1806db299595aba0777e71baffbe8061742ea76c8fb5c9f174a/text2vec-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6572e1647610aa6968b5eff68ef50ead23677c074e52e5be4105aa72b6a2444a",
          "md5": "5f0b4644826fb5e18997e8a9ddc16e8c",
          "sha256": "da92b6d93249a3fbb4e02bc8b5cf57b663993f6529060d67b834b9abd38293ac"
        },
        "downloads": -1,
        "filename": "text2vec-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "5f0b4644826fb5e18997e8a9ddc16e8c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 37117,
        "upload_time": "2021-12-30T10:55:01",
        "upload_time_iso_8601": "2021-12-30T10:55:01.331645Z",
        "url": "https://files.pythonhosted.org/packages/65/72/e1647610aa6968b5eff68ef50ead23677c074e52e5be4105aa72b6a2444a/text2vec-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eef4ed5a1c59f10110d716d7af8c3e0d11701e4dd6b7b92d97ba180a9198e397",
          "md5": "26661faa8f7cfe51f0f17be99ce64321",
          "sha256": "f62947e7139ceaad1127a4ceffeb8829c7d3f842e0b5be395aadb6d03db33326"
        },
        "downloads": -1,
        "filename": "text2vec-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "26661faa8f7cfe51f0f17be99ce64321",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 38454,
        "upload_time": "2022-01-06T13:41:13",
        "upload_time_iso_8601": "2022-01-06T13:41:13.669477Z",
        "url": "https://files.pythonhosted.org/packages/ee/f4/ed5a1c59f10110d716d7af8c3e0d11701e4dd6b7b92d97ba180a9198e397/text2vec-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7073d5ecf84520dcae5cd61983716f8f42069f9d3fc32aad6031416229917807",
          "md5": "dcd6c3967b482aa2d410bc678e02ca23",
          "sha256": "d0d7a21fde57633f8cfc6b72f6eae56273e80daf0dd8d2a9b9341e56975f444d"
        },
        "downloads": -1,
        "filename": "text2vec-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "dcd6c3967b482aa2d410bc678e02ca23",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 38474,
        "upload_time": "2022-01-07T06:49:41",
        "upload_time_iso_8601": "2022-01-07T06:49:41.451540Z",
        "url": "https://files.pythonhosted.org/packages/70/73/d5ecf84520dcae5cd61983716f8f42069f9d3fc32aad6031416229917807/text2vec-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9e843d7bb944cbf6d10cf114052f81a03db56b5f946b4ba8f707f8f2f6c0ef32",
          "md5": "cd8ddbfaebdcd275c2c91922b50e0ac2",
          "sha256": "8533575cca6e12a4a17fd24123cbaf99799d0e62762eeb285a1c359d57a9cf88"
        },
        "downloads": -1,
        "filename": "text2vec-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "cd8ddbfaebdcd275c2c91922b50e0ac2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 51055,
        "upload_time": "2022-01-23T07:47:13",
        "upload_time_iso_8601": "2022-01-23T07:47:13.303779Z",
        "url": "https://files.pythonhosted.org/packages/9e/84/3d7bb944cbf6d10cf114052f81a03db56b5f946b4ba8f707f8f2f6c0ef32/text2vec-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "410186aa8e9ae52cd719c0d1710fed86491a455279f83d138f993c4271cd8c9c",
          "md5": "abeb1fd6d721508b1f1fc8a4c0c82ec8",
          "sha256": "b26ca4641a5474f09e53497fb49b090f78eabb3f2d31e10a24cbe85b32dbe25b"
        },
        "downloads": -1,
        "filename": "text2vec-1.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "abeb1fd6d721508b1f1fc8a4c0c82ec8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 809259,
        "upload_time": "2022-01-27T09:30:30",
        "upload_time_iso_8601": "2022-01-27T09:30:30.419666Z",
        "url": "https://files.pythonhosted.org/packages/41/01/86aa8e9ae52cd719c0d1710fed86491a455279f83d138f993c4271cd8c9c/text2vec-1.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4622072d054271fc6da32787f6b3dfbce7b5ee271e25c70fc5305679659e290d",
          "md5": "b3b764423e1611a555ac2cd4c4b62ac9",
          "sha256": "ef2cc70ccf107829a9c60f675ca9783d2a36b1c946c743028f230e2198f84097"
        },
        "downloads": -1,
        "filename": "text2vec-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b3b764423e1611a555ac2cd4c4b62ac9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 59932,
        "upload_time": "2022-02-26T06:30:00",
        "upload_time_iso_8601": "2022-02-26T06:30:00.402337Z",
        "url": "https://files.pythonhosted.org/packages/46/22/072d054271fc6da32787f6b3dfbce7b5ee271e25c70fc5305679659e290d/text2vec-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fd52cf5e8a8a89ed5ad4a5f151e3ab6e584b4373aae2895331067fb39073acf5",
          "md5": "83fb67ddebbfcd1e4405cc72895a83c4",
          "sha256": "58960760b12c038603573894804a5277aa64d0291c7032595dbd738bef1fc554"
        },
        "downloads": -1,
        "filename": "text2vec-1.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "83fb67ddebbfcd1e4405cc72895a83c4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 59105,
        "upload_time": "2022-02-26T11:45:37",
        "upload_time_iso_8601": "2022-02-26T11:45:37.273468Z",
        "url": "https://files.pythonhosted.org/packages/fd/52/cf5e8a8a89ed5ad4a5f151e3ab6e584b4373aae2895331067fb39073acf5/text2vec-1.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eeb5ffd432321a5a140d2fcde70d7f9aecd796418847a241ae1c42eec53a89b5",
          "md5": "b5fe961e20b8c363b73806b86e85d7e2",
          "sha256": "4bfda664d1506a8c87e240621ccecba539f62d5095fad7bbab0fc040e42f4d87"
        },
        "downloads": -1,
        "filename": "text2vec-1.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "b5fe961e20b8c363b73806b86e85d7e2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 60284,
        "upload_time": "2022-02-27T11:24:23",
        "upload_time_iso_8601": "2022-02-27T11:24:23.351536Z",
        "url": "https://files.pythonhosted.org/packages/ee/b5/ffd432321a5a140d2fcde70d7f9aecd796418847a241ae1c42eec53a89b5/text2vec-1.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c29f48f449527af639188426e19cbe43a115e620953b7d09b308a19e1225b8e7",
          "md5": "76eee96c21b5dd33bfc7b50d530b1790",
          "sha256": "c587fb00012c1c07edc16e293e239177df8abefebb10a184956c393e8d72dc30"
        },
        "downloads": -1,
        "filename": "text2vec-1.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "76eee96c21b5dd33bfc7b50d530b1790",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 65419,
        "upload_time": "2022-03-02T04:13:13",
        "upload_time_iso_8601": "2022-03-02T04:13:13.284128Z",
        "url": "https://files.pythonhosted.org/packages/c2/9f/48f449527af639188426e19cbe43a115e620953b7d09b308a19e1225b8e7/text2vec-1.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5141059b9d9bf039fc25fa5e46093d70196364d99ec36d5031e21e48372f40d4",
          "md5": "cdd420e02bb570cf0c877ad9aa69120f",
          "sha256": "4d41c0e7fe9c1a44eab3ba03071d17f17aa6f5eb5e6d4478ed4f43b1b1af12a3"
        },
        "downloads": -1,
        "filename": "text2vec-1.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "cdd420e02bb570cf0c877ad9aa69120f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 67654,
        "upload_time": "2022-03-11T08:22:54",
        "upload_time_iso_8601": "2022-03-11T08:22:54.871875Z",
        "url": "https://files.pythonhosted.org/packages/51/41/059b9d9bf039fc25fa5e46093d70196364d99ec36d5031e21e48372f40d4/text2vec-1.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "90f3991b47b81c23b50c4ee643f698ff40a9f57ce6831719c5e73b486b55e7db",
          "md5": "20384fd2f624a1aa386f291c2fef960d",
          "sha256": "8c66a9e52a6d82f41bf5247bebc6f004c9e65db72d98f416c2f40c6bef1f0208"
        },
        "downloads": -1,
        "filename": "text2vec-1.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "20384fd2f624a1aa386f291c2fef960d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 68274,
        "upload_time": "2022-03-12T06:02:15",
        "upload_time_iso_8601": "2022-03-12T06:02:15.900942Z",
        "url": "https://files.pythonhosted.org/packages/90/f3/991b47b81c23b50c4ee643f698ff40a9f57ce6831719c5e73b486b55e7db/text2vec-1.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "263e89d1a10cafcff488a76f64923251445a1d8f3255f957c6b66d02f2dba84f",
          "md5": "9d7bb0527658e7147ec2b28ff231d336",
          "sha256": "c6903044d6fd5a8346cb278545216b6d2aa18d3b6799a6cf36e644204f199bdd"
        },
        "downloads": -1,
        "filename": "text2vec-1.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "9d7bb0527658e7147ec2b28ff231d336",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 69271,
        "upload_time": "2022-05-17T02:42:00",
        "upload_time_iso_8601": "2022-05-17T02:42:00.533876Z",
        "url": "https://files.pythonhosted.org/packages/26/3e/89d1a10cafcff488a76f64923251445a1d8f3255f957c6b66d02f2dba84f/text2vec-1.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "51f814e4f02a2cc5fe9c172b719bfa5bb62477feb67eab177519cc5ff6f14642",
          "md5": "b42699041471b90af35bdac110032257",
          "sha256": "1812b7fa08cde3a50335fa0f8d3d3575905c2d5762016c1986c5764de36bf4b1"
        },
        "downloads": -1,
        "filename": "text2vec-1.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "b42699041471b90af35bdac110032257",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 71521,
        "upload_time": "2022-07-21T14:23:16",
        "upload_time_iso_8601": "2022-07-21T14:23:16.253718Z",
        "url": "https://files.pythonhosted.org/packages/51/f8/14e4f02a2cc5fe9c172b719bfa5bb62477feb67eab177519cc5ff6f14642/text2vec-1.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "51f814e4f02a2cc5fe9c172b719bfa5bb62477feb67eab177519cc5ff6f14642",
        "md5": "b42699041471b90af35bdac110032257",
        "sha256": "1812b7fa08cde3a50335fa0f8d3d3575905c2d5762016c1986c5764de36bf4b1"
      },
      "downloads": -1,
      "filename": "text2vec-1.1.7.tar.gz",
      "has_sig": false,
      "md5_digest": "b42699041471b90af35bdac110032257",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 71521,
      "upload_time": "2022-07-21T14:23:16",
      "upload_time_iso_8601": "2022-07-21T14:23:16.253718Z",
      "url": "https://files.pythonhosted.org/packages/51/f8/14e4f02a2cc5fe9c172b719bfa5bb62477feb67eab177519cc5ff6f14642/text2vec-1.1.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}