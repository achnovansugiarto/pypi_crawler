{
  "info": {
    "author": "Jérôme Dockès",
    "author_email": "jerome@dockes.org",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3"
    ],
    "description": "[![build](https://github.com/neuroquery/pubget/actions/workflows/testing.yml/badge.svg)](https://github.com/neuroquery/pubget/actions/workflows/testing.yml)\n[![codecov](https://codecov.io/gh/neuroquery/pubget/branch/main/graph/badge.svg?token=8KEBP2EN3A)](https://codecov.io/gh/neuroquery/pubget)\n[![pubget on GitHub](https://img.shields.io/static/v1?label=&message=pubget%20on%20GitHub&color=black&style=flat&logo=github)](https://github.com/neuroquery/pubget)\n\n\n`pubget` is a command-line tool for collecting data for large-scale\ncoordinate-based neuroimaging meta-analysis. It exposes some of the machinery\nthat was used to create the [neuroquery\ndataset](https://github.com/neuroquery/neuroquery_data), which powers\n[neuroquery.org](https://neuroquery.org).\n\n`pubget` downloads full-text articles from [PubMed\nCentral](https://www.ncbi.nlm.nih.gov/pmc/) and extracts their text and\nstereotactic coordinates. It also computes [TFIDF\nfeatures](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) for the extracted text.\n\nBesides the command-line interface, `pubget`'s functionality is also exposed\nthrough its [Python API](https://neuroquery.github.io/pubget/#python-api).\n\n# Installation\n\nYou can install `pubget` by running:\n```\npip install pubget\n```\n\nThis will install the `pubget` Python package, as well as the `pubget` command.\n\n# Quick Start\n\nOnce `pubget` is installed, we can download and process neuroimaging articles so\nthat we can later use them for meta-analysis.\n\n```\npubget run ./pubget_data -q \"fMRI[title]\"\n```\n\nSee `pubget run --help` for a description of this command. In particular, the\n`--n_jobs` option allows running some of the steps in parallel.\n\n# Usage\n\nThe creation of a dataset happens in four steps:\n- Downloading the articles in bulk from the\n  [PMC](https://www.ncbi.nlm.nih.gov/pmc/) API.\n- Extracting the articles from the bulk download\n- Extracting text, stereotactic coordinates and metadata from the articles, and\n  storing this information in CSV files.\n- Vectorizing the text: transforming it into vectors of\n  [TFIDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) features.\n\nEach of these steps stores its output in a separate directory. Normally, you\nwill run the whole procedure in one command by invoking `pubget run`. However,\nseparate commands are also provided to run each step separately. Below, we\ndescribe each step and its output. Use `pubget -h` to see a list of all available\ncommands and `pubget run -h` to see all the options of the main command.\n\nAll articles downloaded by `pubget` come from [PubMed\nCentral](https://www.ncbi.nlm.nih.gov/pmc/), and are therefore identified by\ntheir PubMed Central ID (`pmcid`). Note this is not the same as the PubMed ID\n(`pmid`). Not all articles in PMC have a `pmid`.\n\n## Step 1: Downloading articles from PMC\n\nThis step is executed by the `pubget download` command. Articles to download can\nbe selected in 2 different ways: by using a query to search the PMC database, or\nby providing an explicit list of article PMCIDs. To use a list of PMCIDs, we\nmust pass the path to a file containing the IDs as the `--pmcids_file`\nparameter. It must contain one ID per line, for example:\n\n```\n8217889\n7518235\n7500239\n7287136\n7395771\n7154153\n```\n\nNote these must be PubMedCentral IDs, *not* PubMed IDs. Moreover, Some articles\ncan be viewed on the PubMedCentral website, but are not in the Open Access\nsubset. The publisher of these articles forbids downloading their full text in\nXML form. Therefore, for such articles only the abstract and metadata will be\navailable. When we use a query instead of a PMCID list, only articles in the\nOpen Access subset are considered.\n\nIf we use a query instead, we do not use the `--pmcids_file` option, but either\n`--query` or `--query_file`. Everything else works in the same way, and the rest\nof this documentation relies on an example that uses a query.\n\nWe must first define our query, with which Pubmed Central will be searched for\narticles. It can be simple such as `fMRI`, or more specific such as\n`fMRI[Abstract] AND (2000[PubDate] : 2022[PubDate])`. You can build the\nquery using the [PMC advanced search\ninterface](https://www.ncbi.nlm.nih.gov/pmc/advanced). For more information see\n[the E-Utilities help](https://www.ncbi.nlm.nih.gov/books/NBK3837/).\nSome examples are provided in the `pubget` git repository, in `docs/example_queries`.\n\nThe query can be passed either as a string on the command-line with `-q` or\n`--query` or by passing the path of a text file containing the query with `-f`\nor `--query_file`.\n\nIf we have an Entrez API key (see details in the [E-utilities\ndocumentation](https://www.ncbi.nlm.nih.gov/books/NBK25497/)), we can provide it\nthrough the `NCBI_API_KEY` environment variable or through the `--api_key`\ncommand line argument (the latter has higher precedence).\n\nWe must also specify the directory in which all `pubget` data will be stored. It\ncan be provided either as a command-line argument (as in the examples below), or\nby exporting the `PUBGET_DATA_DIR` environment variable. Subdirectories will be\ncreated for each different query. In the following we suppose we are storing our\ndata in a directory called `pubget_data`.\n\nWe can thus download all articles with \"fMRI\" in their title published in 2019 by running:\n```\npubget download -q \"fMRI[Title] AND (2019[PubDate] : 2019[PubDate])\" pubget_data\n```\n\n---\n\n**Note:** writing the query in a file rather than passing it as an argument is\nmore convenient for complex queries, for example those that contain whitespace,\nnewlines or quotes. By storing it in a file we do not need to take care to quote\nor escape characters that would be interpreted by the shell. In this case we\nwould store our query in a file, say `query.txt`:\n\n```\nfMRI[Title] AND (2019[PubDate] : 2019[PubDate])\n```\n\nand run\n\n```\npubget download -f query.txt pubget_data\n```\n\n---\n\nAfter running this command, these are the contents of our data directory:\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articlesets\n      │   ├── articleset_00000.xml\n      │   └── info.json\n      └── query.txt\n```\n\n`pubget` has created a subdirectory for this query. If we run the download again\nfor the same query, the same subdirectory will be reused\n(`3c0556e22a59e7d200f00ac8219dfd6c` is the md5 checksum of the query). If we had\nused a PMCID list instead of a query, the subdirectory name would start with\n`pmcidList_` instead of `query_`.\n\nInside the query directory, the results of the bulk download are stored in the\n`articlesets` directory. The articles themselves are in XML files bundling up to\n500 articles called `articleset_*.xml`. Here there is only one because the\nsearch returned less than 500 articles.\n\nSome information about the download is stored in `info.json`. In particular,\n`is_complete` indicates if all articles matching the search have been\ndownloaded. If the download was interrupted, some batches failed to download, or\nthe number of results was limited by using the `--n_docs` parameter,\n`is_complete` will be `false` and the exit status of the program will\nbe 1. You may want to re-run the command before moving on to the next step if\nthe download is incomplete.\n\nIf we used a query it will be stored in `query.txt`, and if we used a list of\nPMCIDs, in `requested_pmcids.txt`, at the root of the query directory (ie at the\nsame level as `articlesets/`).\n\nIf we run the same query again, only missing batches will be downloaded. If we\nwant to force re-running the search and downloading the whole data we need to\nremove the `articlesets` directory.\n\n\n## Step 2: extracting articles from bulk download\n\nThis step is executed by the `pubget extract_articles` command.\n\nOnce our download is complete, we extract articles and store each of them in a\nseparate directory. To do so, we pass the `articlesets` directory created by the\n`pubget download` command in step 1:\n\n```\npubget extract_articles pubget_data/query_3c0556e22a59e7d200f00ac8219dfd6c/articlesets\n```\n\nThis creates an `articles` subdirectory in the query directory, containing the\narticles. To avoid having a large number of files in a single directory when\nthere are many articles, which can be problematic on some filesystems, the\narticles are spread over many subdirectories. The names of these subdirectories\nrange from `000` to `fff` and an article goes in the subdirectory that matches\nthe first 3 hexidecimal digits of the md5 hash of its `pmcid`.\n\nOur data directory now looks like this (with many articles ommitted for\nconciseness):\n\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articles\n      │   ├── 019\n      │   │   └── pmcid_6759467\n      │   │       ├── article.xml\n      │   │       └── tables\n      │   │           └── tables.xml\n      │   ├── 01f\n      │   │   └── pmcid_6781806\n      │   │       ├── article.xml\n      │   │       └── tables\n      │   │           ├── table_000.csv\n      │   │           ├── table_000_info.json\n      │   │           ├── table_001.csv\n      │   │           ├── table_001_info.json\n      │   │           └── tables.xml\n      │   ├── ...\n      │   └── info.json\n      └── articlesets\n```\n\nNote that the subdirectories such as `articles/01f` can contain one or more\narticles, even though the examples that appear here only contain one.\n\nEach article directory, such as `articles/01f/pmcid_6781806`, contains:\n- `article.xml`: the XML file containing the full article in its original\n  format.\n- a `tables` subdirectory, containing:\n  - `tables.xml`: all the article's tables, each provided in 2 formats: its\n    original version, and converted to XHTML using the\n    [DocBook](https://docbook.org/) stylesheets.\n  - For each table, a CSV file containing the extracted data and a JSON file\n    providing information such as the table label, id, caption, and\n    `n_header_rows`, the number of rows at the start of the CSV that should be\n    treated as part of the table header.\n\nIf the download and article extraction were successfully run and we run the same\nquery again, the article extraction is skipped. If we want to force re-running\nthe article extraction we need to remove the `articles` directory (or the\n`info.json` file it contains).\n\n\n## Step 3: extracting data from articles\n\nThis step is executed by the `pubget extract_data` command.\n\nIt creates another directory that contains CSV files, containing the text,\nmetadata and coordinates extracted from all the articles.\n\nIf we use the `--articles_with_coords_only` option, only articles in which\n`pubget` finds stereotactic coordinates are kept. The name of the resulting\ndirectory will reflect that choice.\n\nWe pass the path of the `articles` directory created by `pubget extract_articles`\nin the previous step to the `pubget extract_data` command:\n\n```\npubget extract_data --articles_with_coords_only pubget_data/query_3c0556e22a59e7d200f00ac8219dfd6c/articles/\n```\n\nOur data directory now contains (ommitting the contents of the previous steps):\n\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articles\n      ├── articlesets\n      └── subset_articlesWithCoords_extractedData\n          ├── authors.csv\n          ├── coordinates.csv\n          ├── coordinate_space.csv\n          ├── info.json\n          ├── links.csv\n          ├── metadata.csv\n          └── text.csv\n```\n\nIf we had not used `--articles_with_coords_only`, the new subdirectory would be\nnamed `subset_allArticles_extractedData` instead.\n\n- `metadata.csv` contains one row per article, with some metadata: `pmcid`\n  (PubMed Central ID), `pmid` (PubMed ID), `doi`, `title`, `journal`,\n  `publication_year` and `license`. Note some values may be missing (for example\n  not all articles have a `pmid` or `doi`).\n- `authors.csv` contains one row per article per author. Fields are `pmcid`,\n  `surname`, `given-names`.\n- `text.csv` contains one row per article. The first field is the `pmcid`, and\n  the other fields are `title`, `keywords`, `abstract`, and `body`, and contain\n  the text extracted from these parts of the article.\n- `links.csv` contains the external links found in the articles. The fields are\n  `pmcid`, `ext-link-type` (the type of link, for example \"uri\", \"doi\"), and\n  `href` (usually an URL).\n- `coordinates.csv` contains one row for each `(x, y, z)` stereotactic\n  coordinate found in any article. Its fields are the `pmcid` of the article,\n  the table label and id the coordinates came from, and `x`, `y`, `z`.\n- `coordinate_space.csv` has fields `pmcid` and `coordinate_space`. It contains\n  a guess about the stereotactic space coordinates are reported in, based on a\n  heuristic derived from [neurosynth](https://github.com/neurosynth/ACE).\n  Possible values for the space are the terms used by `neurosynth`: \"MNI\", \"TAL\"\n  (for Talairach space), and \"UNKNOWN\".\n\nThe different files can be joined on the `pmcid` field.\n\nIf all steps up to data extraction were successfully run and we run the same\nquery again, the data extraction is skipped. If we want to force re-running the\ndata extraction we need to remove the corresponding directory (or the\n`info.json` file it contains).\n\n## Optional step: extracting a new vocabulary\n\nThis step is executed by the `pubget extract_vocabulary` command.\nWhen running the full pipeline this step is optional: we must use\nthe `--extract_vocabulary` option for it to be executed.\n\nIt builds a vocabulary of all the words and 2-grams (groups of 2 \nwords) that appear in the downloaded text, and computes their document frequency\n(the proportion of documents in which a term appears).\n```\npubget extract_vocabulary pubget_data/query_3c0556e22a59e7d200f00ac8219dfd6c/subset_articlesWithCoords_extractedData\n```\n\nThe vocabulary is stored in a csv file in a new directory. There is no header\nand the 2 columns are the term and its document frequency.\n\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articles\n      ├── articlesets\n      ├── subset_articlesWithCoords_extractedData\n      └── subset_articlesWithCoords_extractedVocabulary\n          ├── info.json\n          └── vocabulary.csv\n```\n\nWhen running the whole pipeline (`pubget run`), if we use the\n`--extract_vocabulary` option and do not provide an explicit value for\n`--vocabulary_file`, the freshly-extracted vocabulary is used instead of the\ndefault `neuroquery` one for computing TFIDF features (see next step).\n\n\n## Optional step: vectorizing (computing TFIDF features)\n\nThis step is executed by the `pubget vectorize` command. When running the full\npipeline this step is optional: we must use the `--vectorize_text` option for it\nto be executed. However, if any of the subsequent steps that rely on TFIDF\nfeatures (NeuroQuery, NeuroSynth or NiMARE steps, see below) are requested, this\nstep is always run and `--vectorize_text` is ignored. This step is also run\nwhenever we use the `--vocabulary_file` option.\n\nSome large-scale meta-analysis methods such as\n[neurosynth](https://neurosynth.org/) and [neuroquery](https://neuroquery.org)\nrely on [TFIDF features](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to\nrepresent articles' text. The last step before we can apply these methods is\ntherefore to extract TFIDF features from the text we obtained in the previous\nstep.\n\nTFIDF features rely on a predefined vocabulary (set of terms or phrases). Each\ndimension of the feature vector corresponds to a term in the vocabulary and\nrepresents the importance of that term in the encoded text. This importance is\nan increasing function of the *term frequency* (the number of time the term\noccurs in the text divided by the length of the text) and a decreasing function\nof the *document frequency* (the total number of times the term occurs in the\nwhole corpus or dataset).\n\nTo extract the TFIDF features we must therefore choose a vocabulary.\n- By default, `pubget` will download and use the vocabulary used by\n[neuroquery.org](https://neuroquery.org).\n- If we use the `--extract_vocabulary` option, a new vocabulary is created from\n  the downloaded text and used for computing TFIDF features (see \"extracting a\n  new vocabulary\" below).\n- If we want to use a different vocabulary we can specify it with the\n`--vocabulary_file` option. This file will be parsed as a CSV file with no\nheader, whose first column contains the terms. Other columns are ignored.\n\nWe also pass to `pubget vectorize` the directory containing the text we want to\nvectorize, created by `pubget extract_data` in step 3 (here we are using the\ndefault vocabulary):\n\n```\npubget vectorize pubget_data/query_3c0556e22a59e7d200f00ac8219dfd6c/subset_articlesWithCoords_extractedData/\n```\n\nThis creates a new directory whose name reflects the data source (whether all\narticles are kept or only those with coordinates) and the chosen vocabulary\n(`e6f7a7e9c6ebc4fb81118ccabfee8bd7` is the md5 checksum of the contents of the\nvocabulary file, concatenated with those of the vocabulary mapping file, see\n\"vocabulary mapping\" below):\n\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articles\n      ├── articlesets\n      ├── subset_articlesWithCoords_extractedData\n      └── subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_vectorizedText\n          ├── abstract_counts.npz\n          ├── abstract_tfidf.npz\n          ├── body_counts.npz\n          ├── body_tfidf.npz\n          ├── feature_names.csv\n          ├── info.json\n          ├── keywords_counts.npz\n          ├── keywords_tfidf.npz\n          ├── merged_tfidf.npz\n          ├── pmcid.txt\n          ├── title_counts.npz\n          ├── title_tfidf.npz\n          ├── vocabulary.csv\n          └── vocabulary.csv_voc_mapping_identity.json\n```\n\nThe extracted features are stored in `.npz` files that can be read for example\nwith\n[`scipy.sparse.load_npz`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.load_npz.html).\n\nThese files contain matrices of shape `(n_docs, n_features)`, where `n_docs` is\nthe number of documents and `n_features` the number of terms in the vocabulary.\nThe `pmcid` corresponding to each row is found in `pmcid.txt`, and the term\ncorresponding to each column is found in the first column of\n`feature_names.csv`.\n\n`feature_names.csv` has no header; the first column contains terms and the\nsecond one contains their document frequency.\n\nFor each article part (\"title\", \"keywords\", \"abstract\" and \"body\"), we get the\n`counts` which hold the raw counts (the number of times each word occurs in that\nsection), and the `tfidf` which hold the TFIDF features (the counts divided by\narticle length and log document frequency). Moreover, `merged_tfidf` contains\nthe mean TFIDF computed across all article parts.\n\nIf all steps up to vectorization were successfully run and we run the same query\nagain, the vectorization is skipped. If we want to force re-running the\nvectorization we need to remove the corresponding directory (or the `info.json`\nfile it contains).\n\n### Vocabulary mapping: collapsing redundant words\n\nIt is possible to instruct the tokenizer (that extracts words from text) to\ncollapse some pairs of terms that have the same meaning but different spellings,\nsuch as \"brainstem\" and \"brain stem\".\n\nThis is done through a JSON file that contains a mapping of the form `{term:\nreplacement}`. For example if it contains `{\"brain stem\": \"brainstem\"}`, \"brain\nstem\" will be discarded from the vocabulary and every occurrence of \"brain stem\"\nwill be counted as an occurrence of \"brainstem\" instead. To be found by `pubget`,\nthis vocabulary mapping file must be in the same directory as the vocabulary\nfile, and its name must be the vocabulary file's name with\n`_voc_mapping_identity.json` appended: for example `vocabulary.csv`,\n`vocabulary.csv_voc_mapping_identity.json`.\n\nWhen a vocabulary mapping is provided, a shorter vocabulary is therefore created\nby removing redundant words. The TFIDF and word counts computed by `pubget`\ncorrespond to the shorter vocabulary, which is stored along with its document\nfrequencies in `feature_names.csv`.\n\n`vocabulary.csv` contains the document frequencies of the original (full,\nlonger) vocabulary. A `vocabulary.csv_voc_mapping_identity.json` file is always\ncreated by `pubget`, but if no vocabulary mapping was used, that file contains an\nempty mapping (`{}`) and `vocabulary.csv` and `feature_names.csv` are identical.\n\nThe vocabulary mapping is primarily used by the `neuroquery` package and its\ntokenization pipeline, and you can safely ignore this – just remember that the\nfile providing the terms corresponding to the TFIDF *features* is\n`feature_names.csv`.\n\n## Optional step: fitting a NeuroQuery encoding model\n\nThis step is executed by the `pubget fit_neuroquery` command. When running the\nfull pipeline it is optional: we must use the `--fit_neuroquery` option\nfor it to be executed.\n\nIn this step, once the TFIDF features and the coordinates have been extracted\nfrom downloaded articles, they are used to train a NeuroQuery encoding model --\nthe same type of model that is exposed at\n[neuroquery.org](https://neuroquery.org). Details about this model are provided\nin [the NeuroQuery paper](https://elifesciences.org/articles/53385) and the\ndocumentation for the [neuroquery\npackage](https://github.com/neuroquery/neuroquery).\n\nNote: for this model to give good results a large dataset is needed, ideally close to 10,000 articles (with coordinates).\n\nWe pass the `_vectorizedText` directory created by `pubget vectorize`:\n```\npubget fit_neuroquery pubget_data/query_3c0556e22a59e7d200f00ac8219dfd6c/subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_vectorizedText\n```\n\nThis creates a directory whose name ends with `_neuroqueryModel`:\n\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articles\n      ├── articlesets\n      ├── subset_articlesWithCoords_extractedData\n      ├── subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_neuroqueryModel\n      │   ├── app.py\n      │   ├── info.json\n      │   ├── neuroquery_model\n      │   │   ├── corpus_metadata.csv\n      │   │   ├── corpus_tfidf.npz\n      │   │   ├── mask_img.nii.gz\n      │   │   ├── regression\n      │   │   │   ├── coef.npy\n      │   │   │   ├── intercept.npy\n      │   │   │   ├── M.npy\n      │   │   │   ├── original_n_features.npy\n      │   │   │   ├── residual_var.npy\n      │   │   │   └── selected_features.npy\n      │   │   ├── smoothing\n      │   │   │   ├── smoothing_weight.npy\n      │   │   │   └── V.npy\n      │   │   ├── vocabulary.csv\n      │   │   └── vocabulary.csv_voc_mapping_identity.json\n      │   ├── README.md\n      │   └── requirements.txt\n      └── subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_vectorizedText\n```\n\nYou do not need to care about the contents of the `neuroquery_model` subdirectory, that is data used by the `neuroquery` package.\nJust know that it can be used to initialize a `neuroquery.NeuroQueryModel` with:\n```python\nfrom neuroquery import NeuroQueryModel\nmodel = NeuroQueryModel.from_data_dir(\"neuroquery_model\")\n```\nThe `neuroquery` documentation provides information and examples on how to use this model.\n\n### Visualizing the newly trained model in an interactive web page\n\nIt is easy to interact with the model through a small web (Flask) application.\nFrom inside the `[...]_neuroqueryModel` directory, just run `pip install -r requirements.txt` to install `flask`, `nilearn` and `neuroquery`.\nThen run `flask run` and point your web browser to `https://localhost:5000`: you can play with a local, simplified version of [neuroquery.org](https://neuroquery.org) built with the data we just downloaded.\n\n\n## Optional step: running a NeuroSynth meta-analysis\n\nThis step is executed by the `pubget fit_neurosynth` command. When running the\nfull pipeline it is optional: we must use the `--fit_neurosynth` option for it\nto be executed.\n\n\nIn this step, once the TFIDF features and the coordinates have been extracted\nfrom downloaded articles, they are used to run meta-analyses using NeuroSynth's\n\"association test\" method: a Chi-squared test of independence between voxel\nactivation and term occurrences. See [the NeuroSynth\npaper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3146590/) and\n[neurosynth.org](https://neurosynth.org), as well as the\n[neurosynth](https://github.com/neurosynth/neurosynth) and\n[NiMARE](https://nimare.readthedocs.io/) documentation pages for more\ninformation.\n\n\nWe pass the `_vectorizedText` directory created by `pubget vectorize`:\n```\npubget fit_neurosynth pubget_data/query_3c0556e22a59e7d200f00ac8219dfd6c/subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_vectorizedText\n```\n\nThis creates a directory whose name ends with `_neurosynthResults`:\n\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articles\n      ├── articlesets\n      ├── subset_articlesWithCoords_extractedData\n      ├── subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_neurosynthResults\n      │   ├── app.py\n      │   ├── info.json\n      │   ├── metadata.csv\n      │   ├── neurosynth_maps\n      │   │   ├── aberrant.nii.gz\n      │   │   ├── abilities.nii.gz\n      │   │   ├── ability.nii.gz\n      │   │   └── ...\n      │   ├── README.md\n      │   ├── requirements.txt\n      │   ├── terms.csv\n      │   └── tfidf.npz\n      └── subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_vectorizedText\n```\n\nThe meta-analytic maps for all the terms in the vocabulary can be found in the\n`neurosynth_maps` subdirectory.\n\n### Visualizing the meta-analytic maps in an interactive web page\n\nIt is easy to interact with the NeuroSynth maps through a small web (Flask)\napplication. From inside the `[...]_neurosynthResults` directory, just run `pip\ninstall -r requirements.txt` to install `flask` and other dependencies. Then run\n`flask run` and point your web browser to `https://localhost:5000`: you can\nsearch for a term and see the corresponding brain map and the documents that\nmention it.\n\n## Optional step: preparing articles for annotation with `labelbuddy`\n\nThis step is executed by the `pubget extract_labelbuddy_data` command.\nWhen running the full pipeline this step is optional: we must use\nthe `--labelbuddy` or `--labelbuddy_batch_size` option for it to be executed.\n\nIt prepares the articles whose data was extracted for annotation with\n[labelbuddy](https://jeromedockes.github.io/labelbuddy/).\n\nWe pass the `_extractedData` directory created by `pubget extract_data`:\n\n```\npubget extract_labelbuddy_data pubget_data/query_3c0556e22a59e7d200f00ac8219dfd6c/subset_articlesWithCoords_extractedData\n```\n\nThis creates a directory whose name ends with `labelbuddyData` containing the batches of documents in JSONL format (in this case there is a single batch):\n\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articles\n      ├── articlesets\n      ├── subset_articlesWithCoords_extractedData\n      ├── subset_articlesWithCoords_labelbuddyData\n      │   ├── batch_info.csv\n      │   ├── documents_00001.jsonl\n      │   └── info.json\n      └── subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_vectorizedText\n```\n\nThe documents can be imported into `labelbuddy` using the GUI or with:\n\n```\nlabelbuddy mydb.labelbuddy --import-docs documents_00001.jsonl\n```\n\nSee the [labelbuddy\ndocumentation](https://jeromedockes.github.io/labelbuddy/labelbuddy/current/documentation/)\nfor details.\n\nThe CSV file `batch_info.csv` provides the location of each article in the\n`.jsonl` files: its columns are `pmcid`, `file_name` (the name of the `.jsonl`\nfile containing that article) and `line` (the line number that contains that\narticle, first line is 0).\n\n## Optional step: creating a NiMARE dataset\n\nThis step is executed by the `pubget extract_nimare_data` command. When running\nthe full pipeline this step is optional: we must use the `--nimare` option for\nit to be executed.\n\nIt creates a [NiMARE](https://nimare.readthedocs.io/) dataset for the extracted\ndata in JSON format. See the NiMARE\n[documentation](https://nimare.readthedocs.io/en/latest/generated/nimare.dataset.Dataset.html#nimare.dataset.Dataset)\nfor details.\n\nWe pass the `_vectorizedText` directory created by `pubget vectorize`:\n\n```\npubget extract_nimare_data pubget_data/query_3c0556e22a59e7d200f00ac8219dfd6c/subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_vectorizedText\n```\n\nThe resulting directory contains a `nimare_dataset.json` file that can be used to initialize a `nimare.Dataset`. \n\n```\n· pubget_data\n  └── query_3c0556e22a59e7d200f00ac8219dfd6c\n      ├── articles\n      ├── articlesets\n      ├── subset_articlesWithCoords_extractedData\n      ├── subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_nimareDataset\n      │   ├── info.json\n      │   └── nimare_dataset.json\n      └── subset_articlesWithCoords-voc_e6f7a7e9c6ebc4fb81118ccabfee8bd7_vectorizedText\n```\n\nUsing this option requires installing NiMARE, which is not installed by default\nwith `pubget`. To use this option, install NiMARE separately with\n```\npip install nimare\n```\nor install `pubget` with\n```\npip install \"pubget[nimare]\"\n```\n\n## Full pipeline\n\nWe can run all steps in one command by using `pubget run`.\n\nThe full procedure described above could be run by executing:\n\n```\npubget run -q \"fMRI[Title] AND (2019[PubDate] : 2019[PubDate])\" \\\n    --articles_with_coords_only                               \\\n    pubget_data\n```\n\n(The output directory, `pubget_data`, could also be provided by exporting the\n`PUBGET_DATA_DIR` environment variable instead of passing it on the command line.)\n\nIf we also want to apply the optional steps:\n```\npubget run -q \"fMRI[Title] AND (2019[PubDate] : 2019[PubDate])\" \\\n    --articles_with_coords_only                               \\\n    --fit_neuroquery                                          \\\n    --labelbuddy                                              \\\n    --nimare                                                  \\\n    pubget_data\n```\n(remember that `--nimare` requires NiMARE to be installed).\n\nHere also, steps that had already been completed are skipped; we need to remove\nthe corresponding directories if we want to force running these steps again.\n\nSee `pubget run --help` for a description of all options.\n\n\n## Logging\n\nBy default `pubget` commands report their progress by writing to the standard\nstreams. In addition, they can write log files if we provide the `--log_dir`\ncommand-line argument, or if we define the `PUBGET_LOG_DIR` environment variable\n(the command-line argument has higher precedence). If this log directory is\nspecified, a new log file with a timestamp is created and all the output is\nwritten there as well.\n\n# Writing plugins\n\nIt is possible to write plugins and define [entry\npoints](https://setuptools.pypa.io/en/latest/userguide/entry_point.html) to add\nfunctionality that is automatically executed when `pubget` is run.\n\nThe name of the entry point should be `pubget.plugin_actions`. It must be\na function taking no arguments and returning a dictionary with keys\n`pipeline_steps` and `commands`. The corresponding values must be lists\nof processing step objects, that must implement the interface defined by\n`pubget.PipelineStep` and `pubget.Command` respectively (their types do not need to\ninherit from these classes).\n\nAll steps in `pipeline_steps` will be run when `pubget run` is used. All steps in\n`standalone_steps` will be added as additional pubget commands; for example if the\n`name` of a standalone step is `my_plugin`, the `pubget my_plugin` command will\nbecome available.\n\nAn example plugin that can be used as a template, and more details, are provided\nin the `pubget` git repository, in `docs/example_plugin`.\n\n# Contributing\n\nFeedback and contributions are welcome. Development happens at the\n[pubget GitHub repositiory](https://github.com/neuroquery/pubget).\nTo install the dependencies required for development, from the directory where you cloned `pubget`, run:\n```\npip install -e \".[dev]\"\n```\n\nThe tests can be run with `make test_all`, or `make test_coverage` to report\ntest coverage. The documentation can be rendered with `make doc`. `make\nrun_full_pipeline` runs the full `pubget` pipeline on a query returning a\nrealistic number of results (`fMRI[title]`).\n\n# Python API\n\n`pubget` is mostly intended for use as a command-line tool. However, it is also a\nPython package and its functionality can be used in Python programs. The Python\nAPI closely reflects the command-line programs described above.\n\nThe Python API is described on the `pubget` [website](https://neuroquery.github.io/pubget/#python-api).\n\nMIT License\n\nCopyright (c) 2022 Jérôme Dockès\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://neuroquery.github.io/pubget/",
    "keywords": "neuroimaging,meta-analysis,text-mining",
    "license": "MIT License",
    "maintainer": "Jérôme Dockès",
    "maintainer_email": "jerome@dockes.org",
    "name": "pubget",
    "package_url": "https://pypi.org/project/pubget/",
    "platform": null,
    "project_url": "https://pypi.org/project/pubget/",
    "project_urls": {
      "Homepage": "https://neuroquery.github.io/pubget/"
    },
    "release_url": "https://pypi.org/project/pubget/0.0.7/",
    "requires_dist": [
      "joblib (>=0.17)",
      "lxml",
      "importlib-metadata (>=3.6)",
      "neuroquery",
      "numpy (>=1.16)",
      "pandas (>=0.24)",
      "requests",
      "scikit-learn (>=0.21)",
      "scipy",
      "coverage ; extra == 'dev'",
      "flake8 ; extra == 'dev'",
      "mypy ; extra == 'dev'",
      "pdoc ; extra == 'dev'",
      "pylint ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "tox ; extra == 'dev'",
      "types-requests ; extra == 'dev'",
      "black ; extra == 'dev'",
      "isort ; extra == 'dev'",
      "nimare ; extra == 'nimare'"
    ],
    "requires_python": ">=3.7",
    "summary": "Download neuroimaging articles and extract text and stereotactic coordinates.",
    "version": "0.0.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16189867,
  "releases": {
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4f4c1e1ae89d3ba610f7637013a544c26090518d531c6370487d4bf563ce99e0",
          "md5": "f3e1de5e3a564a408d797b135bcd4048",
          "sha256": "0163f517d139707ef2a7d98a80378a1a9e6df5a3c896c83a6247397cd2ef8821"
        },
        "downloads": -1,
        "filename": "pubget-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f3e1de5e3a564a408d797b135bcd4048",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 24803822,
        "upload_time": "2022-10-06T22:20:16",
        "upload_time_iso_8601": "2022-10-06T22:20:16.492573Z",
        "url": "https://files.pythonhosted.org/packages/4f/4c/1e1ae89d3ba610f7637013a544c26090518d531c6370487d4bf563ce99e0/pubget-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2d3a532ef452cf94ed414b8b9f08882c94c7b7f90dd97878be8cb522669a1a3a",
          "md5": "fc837f2ddf2a1c42ae8cfc7838b53cbe",
          "sha256": "6365d2ee813e2de8045d40dc237a47d00fcf39ccd585fb3721ac2d02112f9fad"
        },
        "downloads": -1,
        "filename": "pubget-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "fc837f2ddf2a1c42ae8cfc7838b53cbe",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 23634369,
        "upload_time": "2022-10-06T22:20:24",
        "upload_time_iso_8601": "2022-10-06T22:20:24.937434Z",
        "url": "https://files.pythonhosted.org/packages/2d/3a/532ef452cf94ed414b8b9f08882c94c7b7f90dd97878be8cb522669a1a3a/pubget-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "91ab07fc4ad36d3c8a7f7b5f2b8b5bf0431f322d009aa5e727d42ac0bb8541cf",
          "md5": "5751c3a918d734cd62493412d2fa6f31",
          "sha256": "ff775d8688a2337f4efe1520b6f2ecf7379a5ac1a9df5b734d5ae3e1a22e812f"
        },
        "downloads": -1,
        "filename": "pubget-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5751c3a918d734cd62493412d2fa6f31",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 24804119,
        "upload_time": "2022-10-19T15:19:02",
        "upload_time_iso_8601": "2022-10-19T15:19:02.476257Z",
        "url": "https://files.pythonhosted.org/packages/91/ab/07fc4ad36d3c8a7f7b5f2b8b5bf0431f322d009aa5e727d42ac0bb8541cf/pubget-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bfd780860b9788dcf83bba8facb77a3080e1dd67c1738f6e43685463c2aaa511",
          "md5": "95f9484f268cae042f644ae625b5e5e2",
          "sha256": "723064d3a8254a342228c0d0794d84f034301aa94e0370c06ecb99db4b8ce646"
        },
        "downloads": -1,
        "filename": "pubget-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "95f9484f268cae042f644ae625b5e5e2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 23635038,
        "upload_time": "2022-10-19T15:19:10",
        "upload_time_iso_8601": "2022-10-19T15:19:10.671576Z",
        "url": "https://files.pythonhosted.org/packages/bf/d7/80860b9788dcf83bba8facb77a3080e1dd67c1738f6e43685463c2aaa511/pubget-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cc1ae1ff4e4db157286a0eda9ab238769108bc8f96301a8b9f82015e4d58c3e4",
          "md5": "20c6ac3c1e7154077779e4962564c068",
          "sha256": "2fdc67727dd10c2aaa02160946f272cebd18fa9ca15f25aceadb412edb29e73d"
        },
        "downloads": -1,
        "filename": "pubget-0.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "20c6ac3c1e7154077779e4962564c068",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 24803953,
        "upload_time": "2022-12-22T20:58:35",
        "upload_time_iso_8601": "2022-12-22T20:58:35.884129Z",
        "url": "https://files.pythonhosted.org/packages/cc/1a/e1ff4e4db157286a0eda9ab238769108bc8f96301a8b9f82015e4d58c3e4/pubget-0.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7ab5f24d8cd815d85a6cd0c0fe07304a1c133b72d8851f6fa5de4f8fbe6b45ae",
          "md5": "b3bb19077a7468740de4d76d07f19a77",
          "sha256": "f8f3c662937847729017235167880f0883e3cdc9f9989a17592afd41d399b93a"
        },
        "downloads": -1,
        "filename": "pubget-0.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "b3bb19077a7468740de4d76d07f19a77",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 23636349,
        "upload_time": "2022-12-22T20:58:48",
        "upload_time_iso_8601": "2022-12-22T20:58:48.625562Z",
        "url": "https://files.pythonhosted.org/packages/7a/b5/f24d8cd815d85a6cd0c0fe07304a1c133b72d8851f6fa5de4f8fbe6b45ae/pubget-0.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cc1ae1ff4e4db157286a0eda9ab238769108bc8f96301a8b9f82015e4d58c3e4",
        "md5": "20c6ac3c1e7154077779e4962564c068",
        "sha256": "2fdc67727dd10c2aaa02160946f272cebd18fa9ca15f25aceadb412edb29e73d"
      },
      "downloads": -1,
      "filename": "pubget-0.0.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "20c6ac3c1e7154077779e4962564c068",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 24803953,
      "upload_time": "2022-12-22T20:58:35",
      "upload_time_iso_8601": "2022-12-22T20:58:35.884129Z",
      "url": "https://files.pythonhosted.org/packages/cc/1a/e1ff4e4db157286a0eda9ab238769108bc8f96301a8b9f82015e4d58c3e4/pubget-0.0.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7ab5f24d8cd815d85a6cd0c0fe07304a1c133b72d8851f6fa5de4f8fbe6b45ae",
        "md5": "b3bb19077a7468740de4d76d07f19a77",
        "sha256": "f8f3c662937847729017235167880f0883e3cdc9f9989a17592afd41d399b93a"
      },
      "downloads": -1,
      "filename": "pubget-0.0.7.tar.gz",
      "has_sig": false,
      "md5_digest": "b3bb19077a7468740de4d76d07f19a77",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 23636349,
      "upload_time": "2022-12-22T20:58:48",
      "upload_time_iso_8601": "2022-12-22T20:58:48.625562Z",
      "url": "https://files.pythonhosted.org/packages/7a/b5/f24d8cd815d85a6cd0c0fe07304a1c133b72d8851f6fa5de4f8fbe6b45ae/pubget-0.0.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}