{
  "info": {
    "author": "Prudhvi GNV",
    "author_email": "prudhvi.gnv@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# Py-AutoML\n\n\n[![LICENCE.md](https://img.shields.io/github/license/PrudhviGNV/py-automl)](https://github.com/PrudhviGNV/py-automl/blob/master/LICENCE.md)\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/PrudhviGNV/py-automl)\n[![Website prudhvignv.github.io](https://img.shields.io/website-up-down-green-red/https/naereen.github.io.svg)](https://prudhvignv.github.io/)\n[![Ask Me Anything !](https://img.shields.io/badge/Ask%20me-anything-1abc9c.svg)](https://GitHub.com/PrudhviGNV)\n \n  \n[![PyPI version fury.io](https://badge.fury.io/py/py-automl.svg)](https://pypi.python.org/pypi/py-automl/)\n[![PyPI format](https://img.shields.io/pypi/format/ansicolortags.svg)](https://pypi.python.org/pypi/py-automl/)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/py-automl.svg)](https://pypi.python.org/pypi/py-automl/)\n[![PyPI status](https://img.shields.io/pypi/status/py-automl.svg)](https://pypi.python.org/pypi/py-automl/) \n[![Open Source Love svg2](https://badges.frapsoft.com/os/v2/open-source.svg?v=103)](https://github.com/PrudhviGNV/open-source-badges/)\n[![Awesome Badges](https://img.shields.io/badge/badges-awesome-green.svg)](https://github.com/PrudhviGNV/badges)\n\n\n\n\n\n\n\n  \n\n\n\n# Introduction\n\n## What is Py-AutoML?\nPy-AutoML is an open source `low-code` machine learning library in Python that aims to reduce the hypothesis to insights cycle time in a ML experiment. It mainly helps to do our pet projects quickly and efficiently. In comparison with the other open source machine learning libraries, Py-AutoML is an alternative low-code library that can be used to perform complex machine learning tasks with only few lines of code. Py-AutoML is essentially a Python wrapper around several machine learning libraries and frameworks such as `scikit-learn`, 'tensorflow','keras' and many more. \n\nThe design and simplicity of Py-AutoML is inspired by the  two principles KISS (keep it simple and sweet) and DRY (Don't Repeat Yourself) . We as engineers have to find a way  effective way to mitigate this gap and address data related challenges in business setting.\n\n\n# Modules\nPy-AutoML is a minimalistic library which not  simplifies the machine learning tasks and also makes our work easier.\n\nPy-AutoML consists of so many functionalities. such as \n-----------------\n\n   - #### model.py- implementing popular neural networks such as googlenet , vgg16, simple cnn ,basic cnn, lenet5, alexnet, lstm, mlp etc..\n   - #### checkpoint.py - consists of callbacks function which is used to store metrics \n   - #### utils.py - consists of some functionalities used to preprocess test images, spliting the data.\n   - #### preprocess.py - used to preprocess image dataset such as resize, reshape, convert to greyscale, normalisation etc..\n   - #### ml.py - allow us to implement and check metrics of popular classical machine learning models such as random forest, decision tree, svm , logistic regression and also displays metric reports of every model\n   - #### visualize.py - allow us to visualize neural networks in pictorial and graphs form.\n   \n   \n # ml.py -> Implemented algorithms\n\n------------\n- ### Logistic Regression\n- ### Support Vector Machine\n- ### Decision Tree Classifier\n- ### Random Forest Classifier\n- ### K-Nearest Neighbors\n--------------------------\n\n   \n # model.py -> Implemented popular neural network architectures\n\n------------\n- ### GoogleNet\n- ### VGG16\n- ### AlexNet\n- ### Lenet5\n- ### Inception\n- ### simple & basic cnn\n- ### basic_mlp & deep_mlp\n- ### lstm\nwith predefined configurations\n--------------------------\n# Getting started\n\n-----------------\n\n## Install the package\n```bash\npip install py-automl\n```\nNavigate to folder and install requirements: \n```bash\npip install -r requirements.txt\n\n```\n\n## Usage\nImporting the package\n```python\nimport pyAutoML\nfrom pyAutoML import *\nfrom pyAutoML.model import *\n# like that...\n```\nAssign the variables X and Y to the desired columns and assign the variable size to the desired test_size.  \n```python\nX = < df.features >\nY = < df.target >\nsize = < test_size >\n```\n## Encoding Categorical Data \nEncode target variable if non-numerical:\n```python\nfrom pyAutoML import *\nY = EncodeCategorical(Y)\n```\n## Running py-automl\n\nsignature is as follows :   ML(X, Y, size=0.25, *args)\n```python\nfrom pyAutoML.ml import ML,ml, EncodeCategorical\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\n\n\n\n\n##reading the Iris dataset into the code\ndf =  datasets.load_iris()\n\n##assigning the desired columns to X and Y  in preparation for running fastML\nX = df.data[:, :4]\nY = df.target\n\n##running the EncodeCategorical function from fastML to handle the process of categorial encoding of data\nY = EncodeCategorical(Y)\nsize = 0.33\n\nML(X, Y, size, SVC(), RandomForestClassifier(), DecisionTreeClassifier(), KNeighborsClassifier(), LogisticRegression(max_iter = 7000))\n\n```\n### output\n```python\n____________________________________________________\n.....................Py-AutoML......................\n____________________________________________________\nSVC ______________________________ \n\nAccuracy Score for SVC is \n0.98\n\n\nConfusion Matrix for SVC is \n[[16  0  0]\n [ 0 18  1]\n [ 0  0 15]]\n\n\nClassification Report for SVC is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        16\n           1       1.00      0.95      0.97        19\n           2       0.94      1.00      0.97        15\n\n    accuracy                           0.98        50\n   macro avg       0.98      0.98      0.98        50\nweighted avg       0.98      0.98      0.98        50\n\n\n\n____________________________________________________\nRandomForestClassifier ______________________________ \n\nAccuracy Score for RandomForestClassifier is \n0.96\n\n\nConfusion Matrix for RandomForestClassifier is \n[[16  0  0]\n [ 0 18  1]\n [ 0  1 14]]\n\n\nClassification Report for RandomForestClassifier is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        16\n           1       0.95      0.95      0.95        19\n           2       0.93      0.93      0.93        15\n\n    accuracy                           0.96        50\n   macro avg       0.96      0.96      0.96        50\nweighted avg       0.96      0.96      0.96        50\n\n\n\n____________________________________________________\nDecisionTreeClassifier ______________________________ \n\nAccuracy Score for DecisionTreeClassifier is \n0.98\n\n\nConfusion Matrix for DecisionTreeClassifier is \n[[16  0  0]\n [ 0 18  1]\n [ 0  0 15]]\n\n\nClassification Report for DecisionTreeClassifier is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        16\n           1       1.00      0.95      0.97        19\n           2       0.94      1.00      0.97        15\n\n    accuracy                           0.98        50\n   macro avg       0.98      0.98      0.98        50\nweighted avg       0.98      0.98      0.98        50\n\n\n\n____________________________________________________\nKNeighborsClassifier ______________________________ \n\nAccuracy Score for KNeighborsClassifier is \n0.98\n\n\nConfusion Matrix for KNeighborsClassifier is \n[[16  0  0]\n [ 0 18  1]\n [ 0  0 15]]\n\n\nClassification Report for KNeighborsClassifier is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        16\n           1       1.00      0.95      0.97        19\n           2       0.94      1.00      0.97        15\n\n    accuracy                           0.98        50\n   macro avg       0.98      0.98      0.98        50\nweighted avg       0.98      0.98      0.98        50\n\n\n\n____________________________________________________\nLogisticRegression ______________________________ \n\nAccuracy Score for LogisticRegression is \n0.98\n\n\nConfusion Matrix for LogisticRegression is \n[[16  0  0]\n [ 0 18  1]\n [ 0  0 15]]\n\n\nClassification Report for LogisticRegression is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        16\n           1       1.00      0.95      0.97        19\n           2       0.94      1.00      0.97        15\n\n    accuracy                           0.98        50\n   macro avg       0.98      0.98      0.98        50\nweighted avg       0.98      0.98      0.98        50\n\n\n\n                    Model Accuracy\n0                     SVC     0.98\n1  RandomForestClassifier     0.96\n2  DecisionTreeClassifier     0.98\n3    KNeighborsClassifier     0.98\n4      LogisticRegression     0.98\n```\n\n### you can also write as follows\n```python\nML(X,Y)\n```\n### output\n```python\n____________________________________________________\n.....................Py-AutoML......................\n____________________________________________________\nSVC ______________________________ \n\nAccuracy Score for SVC is \n0.9736842105263158\n\n\nConfusion Matrix for SVC is \n[[13  0  0]\n [ 0 15  1]\n [ 0  0  9]]\n\n\nClassification Report for SVC is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      0.94      0.97        16\n           2       0.90      1.00      0.95         9\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.98      0.97        38\nweighted avg       0.98      0.97      0.97        38\n\n\n\n____________________________________________________\nRandomForestClassifier ______________________________ \n\nAccuracy Score for RandomForestClassifier is \n0.9736842105263158\n\n\nConfusion Matrix for RandomForestClassifier is \n[[13  0  0]\n [ 0 15  1]\n [ 0  0  9]]\n\n\nClassification Report for RandomForestClassifier is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      0.94      0.97        16\n           2       0.90      1.00      0.95         9\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.98      0.97        38\nweighted avg       0.98      0.97      0.97        38\n\n\n\n____________________________________________________\nDecisionTreeClassifier ______________________________ \n\nAccuracy Score for DecisionTreeClassifier is \n0.9736842105263158\n\n\nConfusion Matrix for DecisionTreeClassifier is \n[[13  0  0]\n [ 0 15  1]\n [ 0  0  9]]\n\n\nClassification Report for DecisionTreeClassifier is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      0.94      0.97        16\n           2       0.90      1.00      0.95         9\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.98      0.97        38\nweighted avg       0.98      0.97      0.97        38\n\n\n____________________________________________________\nKNeighborsClassifier ______________________________ \n\nAccuracy Score for KNeighborsClassifier is \n0.9736842105263158\n\n\nConfusion Matrix for KNeighborsClassifier is \n[[13  0  0]\n [ 0 15  1]\n [ 0  0  9]]\n\n\nClassification Report for KNeighborsClassifier is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      0.94      0.97        16\n           2       0.90      1.00      0.95         9\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.98      0.97        38\nweighted avg       0.98      0.97      0.97        38\n\n\n\n____________________________________________________\nLogisticRegression ______________________________ \n\nAccuracy Score for LogisticRegression is \n0.9736842105263158\n\n\nConfusion Matrix for LogisticRegression is \n[[13  0  0]\n [ 0 15  1]\n [ 0  0  9]]\n\n\nClassification Report for LogisticRegression is \n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      0.94      0.97        16\n           2       0.90      1.00      0.95         9\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.98      0.97        38\nweighted avg       0.98      0.97      0.97        38\n\n\n\n                    Model            Accuracy\n0                     SVC  0.9736842105263158\n1  RandomForestClassifier  0.9736842105263158\n2  DecisionTreeClassifier  0.9736842105263158\n3    KNeighborsClassifier  0.9736842105263158\n4      LogisticRegression  0.9736842105263158\n```\n\n   \n ## Defining popular neural networks\n \n ### implementing alexNet may looks like this\n \n ```python\n  #Instantiation\n    AlexNet = Sequential()\n\n    #1st Convolutional Layer\n    AlexNet.add(Conv2D(filters=96, input_shape=input_shape, kernel_size=(11,11), strides=(4,4), padding='same'))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation('relu'))\n    AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n    #2nd Convolutional Layer\n    AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation('relu'))\n    AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n    #3rd Convolutional Layer\n    AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation('relu'))\n\n    #4th Convolutional Layer\n    AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation('relu'))\n\n    #5th Convolutional Layer\n    AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation('relu'))\n    AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n\n    #Passing it to a Fully Connected layer\n    AlexNet.add(Flatten())\n    # 1st Fully Connected Layer\n    AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation('relu'))\n    # Add Dropout to prevent overfitting\n    AlexNet.add(Dropout(0.4))\n\n    #2nd Fully Connected Layer\n    AlexNet.add(Dense(4096))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation('relu'))\n    #Add Dropout\n    AlexNet.add(Dropout(0.4))\n\n    #3rd Fully Connected Layer\n    AlexNet.add(Dense(1000))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation('relu'))\n    #Add Dropout\n    AlexNet.add(Dropout(0.4))\n\n    #Output Layer\n    AlexNet.add(Dense(10))\n    AlexNet.add(BatchNormalization())\n    AlexNet.add(Activation(classifier_function))\n\n    AlexNet.compile('adam', loss_function, metrics=['acc'])\n    return AlexNet\n```\nBut we implement this in a single line of code like below using this package.\n```python\nalexNet_model = model(input_shape= (30,30,4) , arch=\"alexNet\", classify=\"Mulit\" )\n```\nSimilarly we can also implement\n```python\nalexNet_model = model(\"alexNet\")\n\nlenet5_model = model(\"lenet5\")\n\ngoogleNet_model = model(\"googleNet\")\n\nvgg16_model = model(\"vgg16\")\n\n### etc...\n\n```\nFor more generalization , let's observe following code.\n```python\n# Lets take all models that are defined in the py_automl and which are implemented in a signle line of code\nmodels = [\"simple_cnn\", \"basic_cnn\", \"googleNet\", \"inception\",\"vgg16\",\"lenet5\",\"alexNet\", \"basic_mlp\",\"deep_mlp\",\"basic_lstm\",\"deep_lstm\" ]\n\nd= {}\n\nfor i in models:\n  d[i] = model(i)  # assigning all architectures to its model names using dictionary\n  \n```\n\n## Visualization \n### we can visualize neural networks architecture in different forms with ease.\nLet's observe the following code for better understanding\n```python\nimport keras\nfrom keras import layers\nmodel = keras.Sequential()\n\nmodel.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))\nmodel.add(layers.AveragePooling2D())\n\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\nmodel.add(layers.AveragePooling2D())\n\nmodel.add(layers.Flatten())\n\nmodel.add(layers.Dense(units=120, activation='relu'))\n\nmodel.add(layers.Dense(units=84, activation='relu'))\n\nmodel.add(layers.Dense(units=10, activation = 'softmax'))\n```\nnow let's visualise this\n```python \nnn_visualize(model)\n```\nBy default , it returns keras visualization object\n### output:\n![i1](https://user-images.githubusercontent.com/39909903/91040097-840bbf80-e5c2-11ea-8c3d-fad294b20722.png)\n\n\n```python\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy\n# fix random seed for reproducibility\nnumpy.random.seed(7)\n# load pima indians dataset\ndataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n# split into input (X) and output (Y) variables\nX = dataset[:,0:8]\nY = dataset[:,8]\n# create model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Fit the model\nmodel.fit(X, Y, epochs=150, batch_size=10)\n# evaluate the model\nscores = model.evaluate(X, Y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n\n\n\n#Neural network visualization \n\nnn_visualize(model,type = \"graphviz\")\n\n```\n### output\n![1_gTwmrLh1aYLzayMylHGIeg](https://user-images.githubusercontent.com/39909903/91041224-8242fb80-e5c4-11ea-8539-4c2c35f7bab5.jpeg)\n\n\nThis library is so developer friendly that even we declare type with starting letters.\n```python\nfrom pyAutoML.model import *\nmodel2 = model(arch=\"alexNet\")\n\nnn_visualize(model2,type=\"k\")\n\n```\n### output:\n![i3](https://user-images.githubusercontent.com/39909903/91040108-8837dd00-e5c2-11ea-87c4-a9951804d3c8.png)\n\n## This is a minimal documentation about the package. <br/>\nFor more information and understanding, see examples [HERE](https://github.com/PrudhviGNV/py-automl/edit/master/examples)\nand source code: [GITHUB](https://github.com/PrudhviGNV/py-automl)\n-------\n\n## Author: [Prudhvi GNV](prudhvignv.github.io)\n-------\n# Contact:\n\n[LinkedIn](https://linkedin.com/in/prudhvignv/) <br/>\n[Github](https://github.com/PrudhviGNV) <br/>\n[Instagram](https://instagram.com/prudhvi-gnv)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/PrudhviGNV/py-automl",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "py-automl",
    "package_url": "https://pypi.org/project/py-automl/",
    "platform": "",
    "project_url": "https://pypi.org/project/py-automl/",
    "project_urls": {
      "Homepage": "https://github.com/PrudhviGNV/py-automl"
    },
    "release_url": "https://pypi.org/project/py-automl/1.0.6/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "py-automl - An open source, low-code machine learning library in Python.",
    "version": "1.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8027709,
  "releases": {
    "1.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "25dff5585bd9119f9826d4d6515d164aeff759580925c7f49ca90ca7beaa9c29",
          "md5": "48e27527d340cd3053c7d886c9d9dd05",
          "sha256": "1e2627e550c2d955c39b5a143adefca0e18916c982e81148937a95d0966e7bcd"
        },
        "downloads": -1,
        "filename": "py-automl-1.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "48e27527d340cd3053c7d886c9d9dd05",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7615,
        "upload_time": "2020-08-24T13:52:54",
        "upload_time_iso_8601": "2020-08-24T13:52:54.222248Z",
        "url": "https://files.pythonhosted.org/packages/25/df/f5585bd9119f9826d4d6515d164aeff759580925c7f49ca90ca7beaa9c29/py-automl-1.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2451f2cfef7c6d158456678819df3af50d45ad22231945ea7b64adc9bf22f6ec",
          "md5": "ea8ca7a55e3ad01ef52db9b22be33f11",
          "sha256": "06b6c60ea190d45b1a8b9db50d5b673884530c7866a1f7044ba23c4e8818a226"
        },
        "downloads": -1,
        "filename": "py-automl-1.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "ea8ca7a55e3ad01ef52db9b22be33f11",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 15766,
        "upload_time": "2020-08-24T14:07:52",
        "upload_time_iso_8601": "2020-08-24T14:07:52.810981Z",
        "url": "https://files.pythonhosted.org/packages/24/51/f2cfef7c6d158456678819df3af50d45ad22231945ea7b64adc9bf22f6ec/py-automl-1.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2451f2cfef7c6d158456678819df3af50d45ad22231945ea7b64adc9bf22f6ec",
        "md5": "ea8ca7a55e3ad01ef52db9b22be33f11",
        "sha256": "06b6c60ea190d45b1a8b9db50d5b673884530c7866a1f7044ba23c4e8818a226"
      },
      "downloads": -1,
      "filename": "py-automl-1.0.6.tar.gz",
      "has_sig": false,
      "md5_digest": "ea8ca7a55e3ad01ef52db9b22be33f11",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 15766,
      "upload_time": "2020-08-24T14:07:52",
      "upload_time_iso_8601": "2020-08-24T14:07:52.810981Z",
      "url": "https://files.pythonhosted.org/packages/24/51/f2cfef7c6d158456678819df3af50d45ad22231945ea7b64adc9bf22f6ec/py-automl-1.0.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}