{
  "info": {
    "author": "Voi Technology AB",
    "author_email": "opensource@voiapp.io",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# dbt exposures crawler\n\n[![PyPI version](https://badge.fury.io/py/dbt-exposures-crawler.svg)](https://badge.fury.io/py/dbt-exposures-crawler)\n[![Tests](https://github.com/voi-oss/dbt-exposures-crawler/actions/workflows/run-tests.yaml/badge.svg)](https://github.com/voi-oss/dbt-exposures-crawler/actions/workflows/run-tests.yaml)\n[![Code checks](https://github.com/voi-oss/dbt-exposures-crawler/actions/workflows/run-code-checks.yaml/badge.svg)](https://github.com/voi-oss/dbt-exposures-crawler/actions/workflows/run-code-checks.yaml)\n[![codecov](https://codecov.io/gh/voi-oss/dbt-exposures-crawler/branch/main/graph/badge.svg?token=5JS1RLYRQF)](https://codecov.io/gh/voi-oss/dbt-exposures-crawler)\n[![Apache License 2.0](https://img.shields.io/github/license/voi-oss/dbt-exposures-crawler)](https://github.com/voi-oss/dbt-exposures-crawler/)\n\nAutomates the creation of dbt exposures from different sources. Currently, only Tableau workbooks using Snowflake SQL\nare supported as a source.\n\n> This project is in an ALPHA stage. Internal and external APIs might change between minor versions.\n\n> Please reach out if you try this at your own organization. Feedback is very appreciated, and we\n> would love to hear if you had any issues setting this up at your own.\n\n## Installation\n\nThis project requires Python 3.8+. We have  tested it internally with dbt 1.x, Tableau Server 2022.1 and Snowflake SQL\ndialect.\n\nYou can install the latest version of this package from PyPI by running the command below. Usage instructions can be\nfound further below in this document.\n\n```shell\n$ pip install dbt-exposures-crawler\n```\n\n## Motivation\n\n[dbt](https://www.getdbt.com/) is an open-source tool to manage data transformations in SQL. It automatically generates\na documentation portal from your project which includes a dependency lineage graph. It is possible to add external\ndownstream dependencies to this graph (such as a dashboard on a Business Intelligence tool) through a dbt feature called\n[exposures](https://docs.getdbt.com/docs/building-a-dbt-project/exposures), which are normally defined through `yaml`\nfiles.\n\nThis project automates the creation of exposures by implementing crawlers to parse the metadata of downstream tools.\nCurrently, only Tableau dashboards are supported, but we have plans to include Metabase as well.\n\nA few use cases on how having exposures can help:\n\n* analysts working on a model can use the exposures to perform impact analysis and see which reports might be impacted\n  by their changes;\n* report consumers can find their report on dbt and see which models are used and read their documentation;\n* report consumers can find which other reports are using the same models as their favorite reports.\n\n## How it works\n\nSummary:\n\n1. Retrieve dbt models and sources from `manifest.json`;\n2. Extract metadata (custom SQL and table references) from Tableau workbooks using their GraphQL API);\n3. Try to find occurrences from the dbt models and sources in the Tableau SQL;\n4. Use the Tableau REST API to retrieve additional information about the workbooks (author, project, etc);\n5. Create the dbt exposures (in-memory) and write it back to the `manifest.json`.\n\nMore in-depth explanation:\n\nFirst, you must provide the path to a dbt project [manifest](https://docs.getdbt.com/reference/artifacts/manifest-json).\nThe metadata and fully qualified names (database, schema and object name) are extracted from all dbt models and sources\nrepresented in the manifest. The combination of dbt models and dbt sources will from now on be referred as dbt nodes.\n\nNext, workbook metadata is extracted from Tableau using\ntheir [Metadata API](https://help.tableau.com/current/api/metadata_api/en-us/index.html), including workbooks that use\ncustom SQL queries and workbooks that don't (which are referred in this project as \"native SQL\" workbooks). Note that\nthis API is included in all Tableau licenses (i.e. it does not require the Data Management Add-on), but must\nbe [manually enabled](https://help.tableau.com/current/api/metadata_api/en-us/docs/meta_api_start.html#enable)\nif you host your own Tableau Server.\n\nThe SQL from the custom SQL workbooks and the table names from the native SQL workbooks are normalized through simple\nheuristics, such as removing quotes and converting the custom SQL to lowercase. Now that both normalized SQL and\nnormalized table names from Tableau, and the fully qualified names for the dbt nodes are available, the project tries to\nfind the occurrences of the latter in the former.\n\nThe result of the above is a mapping of workbooks and which dbt nodes they depend on. For every workbook (with mapped\ndependencies available), extra metadata that was not available in the Metadata API is then retrieved from Tableau by\nusing their [REST API](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api.htm), including when the\nworkbook was created, when it was last updated, to which folder it belongs on Tableau and information about the author\nof the workbook.\n\nAs a final step, the information above is written back in the provided `manifest.json` in the form of exposures. Note\nthat instead of generating `.yaml` files for each exposure, they are written directly on the `manifest.json`.\n\n## Example\n\nTo better understand how the project works, let's take as an example\nthe [jaffle_shop](https://github.com/fishtown-analytics/jaffle_shop) dbt sample project. It has, among other models,\na `customers` and an `orders` model.\n\nNow suppose that you company has 4 workbooks on Tableau:\n\n* Customers workbook: accesses the `customers` dbt model through custom SQL;\n* Company KPIs workbook: accesses both models through custom SQL;\n* Orders workbook: accesses the `orders` model without custom SQL;\n* Unrelated workbook: a workbook that does not use the dbt project but instead has a static data source.\n\nWhen running this project, you would get the following console output:\n\n<p align=\"center\">\n    <a href=\"https://github.com/voi-oss/dbt-exposures-crawler/blob/main/docs/cli_output.png\">\n        <img\n          src=\"https://github.com/voi-oss/dbt-exposures-crawler/blob/main/docs/cli_output.png?raw=true\"\n          alt=\"CLI example\"\n          width=\"600px\"\n        />\n    </a>\n</p>\n\nThe `manifest.json` that you provided would have 3 new exposures added to it, such as:\n\n<p align=\"center\">\n    <a href=\"https://github.com/voi-oss/dbt-exposures-crawler/blob/main/docs/modified_manifest.png\">\n        <img\n          src=\"https://github.com/voi-oss/dbt-exposures-crawler/blob/main/docs/modified_manifest.png?raw=true\"\n          alt=\"modified manifest\"\n          width=\"600px\"\n        />\n    </a>\n</p>\n\nThose exposures can then be visualized through your dbt documentation portal, either by finding which exposures are\ndownstream dependencies of a certain model:\n\n<p align=\"center\">\n    <a href=\"https://github.com/voi-oss/dbt-exposures-crawler/blob/main/docs/models.gif\">\n        <img\n          src=\"https://github.com/voi-oss/dbt-exposures-crawler/blob/main/docs/models.gif?raw=true\"\n          alt=\"models in the documentation portal\"\n          width=\"600px\"\n        />\n    </a>\n</p>\n\nOr by doing the inverse. Starting from an exposure, find which models are used on it:\n\n<p align=\"center\">\n    <a href=\"https://github.com/voi-oss/dbt-exposures-crawler/blob/main/docs/exposures.gif\">\n        <img\n          src=\"https://github.com/voi-oss/dbt-exposures-crawler/blob/main/docs/exposures.gif?raw=true\"\n          alt=\"exposures in the documentation portal\"\n          width=\"600px\"\n        />\n    </a>\n</p>\n\nThis example has been taken from the integration tests available in this project. You can read more in the `Testing`\nsection below.\n\n## Features, assumptions and limitations\n\n* Only custom SQL written on Tableau workbooks using fully qualified names (`DATABASE.SCHEMA.OBJECT`) will be detected;\n* For now, only Tableau workbooks (and not published data sources) are supported. Also, only Snowflake SQL is currently\n  supported;\n* Workbooks that are created under Tableau's [Personal spaces](https://help.tableau.com/current/pro/desktop/en-us/personal_space.htm) \nare ignored (since they usually not governed nor production-ready).\n\n## Usage\n\nInternally, we use this automation as part of our dbt docs release pipeline. We have a GitHub Action that does the\nfollowing:\n\n1. Clone our dbt repository;\n2. Install dbt and our dependencies;\n3. Run `dbt docs generate` (remember to run it against your production environment);\n4. Run this project (using the `manifest.json` generated from the previous command as input);\n5. Publish the generated documentation portal;\n\nTo run this project, we use:\n\n```shell\n$ python3 -m exposurescrawler.crawlers.tableau \\\n            --manifest-path=~path/to/dbt/target/manifest.json \\\n            --dbt-package-name=\"your_dbt_pakage_name\" \\\n            --tableau-ignore-projects Archive \\\n            --verbose\n```\n\nMake sure you check the `.env.example` file to see which environment variables must be defined.\n\n## Development\n\nClone the repository and install it in editable mode:\n\n```shell\n$ pip install -e .\n```\n\nBefore opening a pull request, make sure you run:\n\n* `make lint`: runs `mypy`, `black` and `flake8`;\n* `make test`: runs all tests\n\n## Architecture\n\nThe entry point for the crawlers should be on the `crawlers` module. For now, only Tableau is supported.\n\nThe `tableau` module contains all API clients (REST and GraphQL) and models.\n\nThe `dbt` module contains a model for representing a dbt exposure and utilities for parsing, interacting and saving dbt\nmanifests.\n\nFinally, the `utils` module has functions for logging and string parsing.\n\n## Testing\n\nFor the integration tests, we use a sample `manifest.json` as a fixture. It was manually generated from\nthe [jaffle_shop](https://github.com/fishtown-analytics/jaffle_shop), an official dbt sample project.\n\n```shell\n$ git clone https://github.com/fishtown-analytics/jaffle_shop\n$ cd jaffle_shop\n$ pipenv shell\n$ pip install dbt==0.19.1\n```\n\nAfter adding an entry on my dbt profile and then setting the default database on the project to `sample_dbt` on\nthe `dbt_project.yaml`:\n\n```shell\n$ dbt compile --target prod\n```\n\nThe generated `manifest.json` is then prettified and copied to the `tests/_fixtures` folder. I've also manually removed\nthe `macros` entries from the file just to make it easier to navigate through it in case of troubleshooting.\n\n```shell\n$ cat target/manifest.json | jq > $PROJECT_ROOT/tests/_fixtures/manifest.json\n```\n\n## Future ideas\n\n* Allow filters to be passed. E.g. only include Tableau workbooks with certain tags;\n* Add support to Tableau published data sources;\n* Include other BI tools to be crawled, such as Metabase.\n\n## Contributing\n\nWe are open and would love to have contributions, both in Pull Requests but also in ideas and feedback. Don't hesitate\nto create an Issue on this repository if you are trying this project in your organization or have anything to share.\n\n## Release\n\nThere is a GitHub Action that will trigger a release of this package on PyPI based on releases created on GitHub.\nSteps:\n\n* Loosely follow [semantic versioning](https://semver.org/)\n* Remember to pretend the tag name with `v`\n* Use the tag name as the release title on GitHub\n* Use the auto-generated release notes from GitHub\n* Append a link at the end of the release notes to the released version on PyPI\n\n## License\n\nThis project is licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/voi-oss/dbt-exposures-crawler",
    "keywords": "",
    "license": "Apache License, Version 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dbt-exposures-crawler",
    "package_url": "https://pypi.org/project/dbt-exposures-crawler/",
    "platform": null,
    "project_url": "https://pypi.org/project/dbt-exposures-crawler/",
    "project_urls": {
      "Homepage": "https://github.com/voi-oss/dbt-exposures-crawler"
    },
    "release_url": "https://pypi.org/project/dbt-exposures-crawler/0.1.4/",
    "requires_dist": [
      "click (~=8.0.1)",
      "python-slugify (~=4.0.1)",
      "tableauserverclient (~=0.10)"
    ],
    "requires_python": ">=3.8, <4",
    "summary": "Extracts information from different systems and convert them to dbt exposures",
    "version": "0.1.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16247158,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "96f39a68eb6b303c6cc17d0237f196ab5a0d084686a6e270c74acd68a10db5ba",
          "md5": "4f62436bbff7091d78c2fab27b63f9c8",
          "sha256": "73e1a4e81b91d60af5bf1f7b54a7e59c7bfe4fca0584c4b2de25af8a1154ca79"
        },
        "downloads": -1,
        "filename": "dbt_exposures_crawler-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4f62436bbff7091d78c2fab27b63f9c8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9, <4",
        "size": 18243,
        "upload_time": "2021-08-06T18:19:36",
        "upload_time_iso_8601": "2021-08-06T18:19:36.604079Z",
        "url": "https://files.pythonhosted.org/packages/96/f3/9a68eb6b303c6cc17d0237f196ab5a0d084686a6e270c74acd68a10db5ba/dbt_exposures_crawler-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aec9ff4b206c475536e2d2a04828ed3d8f5c502415788ea456f040bce1feda04",
          "md5": "534eefb5dbd0425595d56d9f6bfa9be0",
          "sha256": "676552895c7efdbb703a19d71ca0ae13680e59bd65c72d7f1b03029f2e4e35f3"
        },
        "downloads": -1,
        "filename": "dbt-exposures-crawler-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "534eefb5dbd0425595d56d9f6bfa9be0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9, <4",
        "size": 5950022,
        "upload_time": "2021-08-06T18:19:38",
        "upload_time_iso_8601": "2021-08-06T18:19:38.783411Z",
        "url": "https://files.pythonhosted.org/packages/ae/c9/ff4b206c475536e2d2a04828ed3d8f5c502415788ea456f040bce1feda04/dbt-exposures-crawler-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "720a01781675f54ef339c61ed7a19da246fcc21accd87f9721637d154420c324",
          "md5": "79aa2254366ca73bcaaa2b22ba8ff4ee",
          "sha256": "2dbd5532743bbea927a33a0b23a69d5690d7ae8a0e4b488f9ed4ff242b0d9f72"
        },
        "downloads": -1,
        "filename": "dbt_exposures_crawler-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "79aa2254366ca73bcaaa2b22ba8ff4ee",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9, <4",
        "size": 19192,
        "upload_time": "2021-08-06T19:43:51",
        "upload_time_iso_8601": "2021-08-06T19:43:51.396776Z",
        "url": "https://files.pythonhosted.org/packages/72/0a/01781675f54ef339c61ed7a19da246fcc21accd87f9721637d154420c324/dbt_exposures_crawler-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "69066f6d706da62629aa7063fff9e870405e10b7d8e4ade9647d853c9ba55ec3",
          "md5": "fca1b74fced7bef231cfee38f4c58e27",
          "sha256": "66b7c1501030cdfbf5aeeb8d7e4fc186ad724c55d2294aa692a12071990a22f6"
        },
        "downloads": -1,
        "filename": "dbt-exposures-crawler-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "fca1b74fced7bef231cfee38f4c58e27",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9, <4",
        "size": 5950153,
        "upload_time": "2021-08-06T19:43:53",
        "upload_time_iso_8601": "2021-08-06T19:43:53.567811Z",
        "url": "https://files.pythonhosted.org/packages/69/06/6f6d706da62629aa7063fff9e870405e10b7d8e4ade9647d853c9ba55ec3/dbt-exposures-crawler-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1bd3a80a786fdd24c7e590330b1be082ab806733244c9a9fc9a799666580279b",
          "md5": "d5f1e86f8c14e18abba3813b23f2a9ee",
          "sha256": "606375b5f7f9367e2c46542a3fbd90781b9ebbe077893db8b52c0622a49f9423"
        },
        "downloads": -1,
        "filename": "dbt_exposures_crawler-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d5f1e86f8c14e18abba3813b23f2a9ee",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8, <4",
        "size": 19426,
        "upload_time": "2021-08-10T12:50:20",
        "upload_time_iso_8601": "2021-08-10T12:50:20.742215Z",
        "url": "https://files.pythonhosted.org/packages/1b/d3/a80a786fdd24c7e590330b1be082ab806733244c9a9fc9a799666580279b/dbt_exposures_crawler-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "49ac0370079705869621cf96417069aa4c623ae561416d23c553be44629a98f1",
          "md5": "041a4879c42a074a2b8268b43ac77f7c",
          "sha256": "94133861e00a34dd67b064e309b42973684157aca8038de63fbced17d157ee21"
        },
        "downloads": -1,
        "filename": "dbt-exposures-crawler-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "041a4879c42a074a2b8268b43ac77f7c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8, <4",
        "size": 5950205,
        "upload_time": "2021-08-10T12:50:21",
        "upload_time_iso_8601": "2021-08-10T12:50:21.947828Z",
        "url": "https://files.pythonhosted.org/packages/49/ac/0370079705869621cf96417069aa4c623ae561416d23c553be44629a98f1/dbt-exposures-crawler-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3bc508ee4955307f0f377fbb13aa9101ee526d67ecdc2410842a168f7339a978",
          "md5": "9060ee5e5f378b9fcb7037a4456dbe2b",
          "sha256": "dcabbaf2d93c9e9a1c85eb85b4609c1d58975b0e00cdce582509d5774925ecc0"
        },
        "downloads": -1,
        "filename": "dbt_exposures_crawler-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9060ee5e5f378b9fcb7037a4456dbe2b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8, <4",
        "size": 19639,
        "upload_time": "2022-05-02T14:35:46",
        "upload_time_iso_8601": "2022-05-02T14:35:46.575192Z",
        "url": "https://files.pythonhosted.org/packages/3b/c5/08ee4955307f0f377fbb13aa9101ee526d67ecdc2410842a168f7339a978/dbt_exposures_crawler-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d1bc177ed52f9db702cd7cceea0c04225f5808cdbc9b2a966888cd6930d0f17d",
          "md5": "5d18e715fd6c4665082ea91020b30bd2",
          "sha256": "c717767ec63ec942647ccdbd54909ebabe5bfb8b70f768a096f49974df04d87f"
        },
        "downloads": -1,
        "filename": "dbt-exposures-crawler-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "5d18e715fd6c4665082ea91020b30bd2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8, <4",
        "size": 5950379,
        "upload_time": "2022-05-02T14:35:47",
        "upload_time_iso_8601": "2022-05-02T14:35:47.909002Z",
        "url": "https://files.pythonhosted.org/packages/d1/bc/177ed52f9db702cd7cceea0c04225f5808cdbc9b2a966888cd6930d0f17d/dbt-exposures-crawler-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7ff5df444f74cb0867aaf6ac457c1f8b57fc1434618fbe3ef9879a2ffcc53953",
          "md5": "1964dc6c47334706a64c932271e13b89",
          "sha256": "d5c4adff807e2c9911970bb997f3e20dac957970d6dc9abf84548f8de0672dcf"
        },
        "downloads": -1,
        "filename": "dbt_exposures_crawler-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1964dc6c47334706a64c932271e13b89",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8, <4",
        "size": 19824,
        "upload_time": "2022-12-29T09:26:45",
        "upload_time_iso_8601": "2022-12-29T09:26:45.074687Z",
        "url": "https://files.pythonhosted.org/packages/7f/f5/df444f74cb0867aaf6ac457c1f8b57fc1434618fbe3ef9879a2ffcc53953/dbt_exposures_crawler-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "245b9149959bf6426f3bd0a838660e62f28a285d809699047cdefeb61599a0dd",
          "md5": "94ccc122dda18920dc63a8d279e46db1",
          "sha256": "dbb2282a036b317c62ebea4389c521f31074d9bdfae8409d6d00bb32bd6ecdfd"
        },
        "downloads": -1,
        "filename": "dbt-exposures-crawler-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "94ccc122dda18920dc63a8d279e46db1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8, <4",
        "size": 5953239,
        "upload_time": "2022-12-29T09:26:46",
        "upload_time_iso_8601": "2022-12-29T09:26:46.730009Z",
        "url": "https://files.pythonhosted.org/packages/24/5b/9149959bf6426f3bd0a838660e62f28a285d809699047cdefeb61599a0dd/dbt-exposures-crawler-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7ff5df444f74cb0867aaf6ac457c1f8b57fc1434618fbe3ef9879a2ffcc53953",
        "md5": "1964dc6c47334706a64c932271e13b89",
        "sha256": "d5c4adff807e2c9911970bb997f3e20dac957970d6dc9abf84548f8de0672dcf"
      },
      "downloads": -1,
      "filename": "dbt_exposures_crawler-0.1.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1964dc6c47334706a64c932271e13b89",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8, <4",
      "size": 19824,
      "upload_time": "2022-12-29T09:26:45",
      "upload_time_iso_8601": "2022-12-29T09:26:45.074687Z",
      "url": "https://files.pythonhosted.org/packages/7f/f5/df444f74cb0867aaf6ac457c1f8b57fc1434618fbe3ef9879a2ffcc53953/dbt_exposures_crawler-0.1.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "245b9149959bf6426f3bd0a838660e62f28a285d809699047cdefeb61599a0dd",
        "md5": "94ccc122dda18920dc63a8d279e46db1",
        "sha256": "dbb2282a036b317c62ebea4389c521f31074d9bdfae8409d6d00bb32bd6ecdfd"
      },
      "downloads": -1,
      "filename": "dbt-exposures-crawler-0.1.4.tar.gz",
      "has_sig": false,
      "md5_digest": "94ccc122dda18920dc63a8d279e46db1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8, <4",
      "size": 5953239,
      "upload_time": "2022-12-29T09:26:46",
      "upload_time_iso_8601": "2022-12-29T09:26:46.730009Z",
      "url": "https://files.pythonhosted.org/packages/24/5b/9149959bf6426f3bd0a838660e62f28a285d809699047cdefeb61599a0dd/dbt-exposures-crawler-0.1.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}