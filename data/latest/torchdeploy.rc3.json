{
  "info": {
    "author": "MultiPy Devs",
    "author_email": "sahanp@fb.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "[![License](https://img.shields.io/badge/License-BSD%203--Clause-blue.svg)](LICENSE) ![Runtime Tests](https://github.com/pytorch/multipy/actions/workflows/runtime_tests.yaml/badge.svg)\n\n\n# `torch::deploy` (MultiPy)\n\n`torch::deploy` (MultiPy for non-PyTorch use cases) is a C++ library that enables you to run eager mode PyTorch models in production without any modifications to your model to support tracing. `torch::deploy` provides a way to run using multiple independent Python interpreters in a single process without a shared global interpreter lock (GIL). For more information on how `torch::deploy` works\ninternally, please see the related [arXiv paper](https://arxiv.org/pdf/2104.00254.pdf).\n\nTo learn how to use `torch::deploy` see [Installation](#installation) and [Examples](#examples).\n\nRequirements:\n\n* PyTorch 1.13+ or PyTorch nightly\n* Linux (ELF based)\n  * x86_64 (Beta)\n  * arm64/aarch64 (Prototype)\n\n> ℹ️ `torch::deploy` is ready for use in production environments, but is in Beta and may have some rough edges that we're continuously working on improving. We're always interested in hearing feedback and usecases that you might have. Feel free to reach out!\n\n## Installation\n\n### Building via Docker\n\nThe easiest way to build deploy and install the interpreter dependencies is to do so via docker.\n\n```shell\ngit clone --recurse-submodules https://github.com/pytorch/multipy.git\ncd multipy\nexport DOCKER_BUILDKIT=1\ndocker build -t multipy .\n```\n\nThe built artifacts are located in `multipy/runtime/build`.\n\nTo run the tests:\n\n```shell\ndocker run --rm multipy multipy/runtime/build/test_deploy\n```\n\n### Installing via `pip install`\n\nWe support installing both python modules and the runtime libs using `pip\ninstall`, with the caveat of having to manually install the C++ dependencies\nfirst. This serves as a single-command source build, essentially being a wrapper\naround `python setup.py develop`, once all the dependencies have been installed.\n\n\nTo start with, the multipy repo should be cloned first:\n\n```shell\ngit clone --recurse-submodules https://github.com/pytorch/multipy.git\ncd multipy\n\n# (optional) if using existing checkout\ngit submodule sync && git submodule update --init --recursive\n```\n\n#### Installing System Dependencies\n\nThe runtime system dependencies are specified in `build-requirements.txt`. To install them on Debian-based systems, one could run:\n\n```shell\nsudo apt update\nxargs sudo apt install -y -qq --no-install-recommends <build-requirements.txt\n```\n\n#### Python Environment Setup\n\nWe support both `conda` and `pyenv`+`virtualenv` to create isolated environments to build and run in. Since `multipy` requires a position-independent version of python to launch interpreters with, for `conda` environments we use the prebuilt `libpython-static=3.x` libraries from `conda-forge` to link with at build time, and for `virtualenv`/`pyenv` we compile python with `-fPIC` to create the linkable library.\n\n> **NOTE** We support Python versions 3.7 through 3.10 for `multipy`; note that for `conda` environments the `libpython-static` libraries are available for `3.8` onwards. With `virtualenv`/`pyenv` any version from 3.7 through 3.10 can be used, as the PIC library is built explicitly.\n\n<details>\n<summary>Click to expand</summary>\n\nExample commands for installing conda:\n```shell\ncurl -fsSL -v -o ~/miniconda.sh -O  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  && \\\nchmod +x ~/miniconda.sh && \\\n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh\n```\nVirtualenv / pyenv can be installed as follows:\n```shell\npip3 install virtualenv\ngit clone https://github.com/pyenv/pyenv.git ~/.pyenv\n```\n</details>\n\n\n#### Installing python, pytorch and related dependencies\n\nMultipy requires the latest version of pytorch to run models successfully, and we recommend fetching the latest _nightlies_ for pytorch and also cuda, if required.\n\n##### In a `conda` environment, we would do the following:\n```shell\nconda create -n newenv\nconda activate newenv\nconda install python=3.8\nconda install -c conda-forge libpython-static=3.8\n\n# cuda\nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch-nightly\n\n# cpu only\nconda install pytorch torchvision torchaudio cpuonly -c pytorch-nightly\n```\n\n##### For a `pyenv` / `virtualenv` setup, one could do:\n```shell\nexport CFLAGS=\"-fPIC -g\"\n~/.pyenv/bin/pyenv install --force 3.8.6\nvirtualenv -p ~/.pyenv/versions/3.8.6/bin/python3 ~/venvs/multipy\nsource ~/venvs/multipy/bin/activate\npip install -r dev-requirements.txt\n\n# cuda\npip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n\n# cpu only\npip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n\n```\n\n#### Running `pip install`\n\nOnce all the dependencies are successfully installed, most importantly including a PIC-library of python and the latest nightly of pytorch, we can run the following, in either `conda` or `virtualenv`, to install both the python modules and the runtime/interpreter libraries:\n```shell\n# from base multipy directory\npip install -e .\n```\nThe C++ binaries should be available in `/opt/dist`.\n\nAlternatively, one can install only the python modules without invoking `cmake` as follows:\n```shell\npip install  -e . --install-option=\"--cmakeoff\"\n```\n\n> **NOTE** As of 10/11/2022 the linking of prebuilt static fPIC versions of python downloaded from `conda-forge` can be problematic on certain systems (for example Centos 8), with linker errors like `libpython_multipy.a: error adding symbols: File format not recognized`. This seems to be an issue with `binutils`, and the steps in https://wiki.gentoo.org/wiki/Project:Toolchain/Binutils_2.32_upgrade_notes/elfutils_0.175:_unable_to_initialize_decompress_status_for_section_.debug_info can help. Alternatively, the user can go with the `virtualenv`/`pyenv` flow above.\n\n## Development\n\n### Manually building `multipy::runtime` from source\n\nBoth `docker` and `pip install` options above are wrappers around the `cmake`\nbuild of multipy's runtime. For development purposes it's often helpful to\ninvoke `cmake` separately.\n\nSee the install section for how to correctly setup the Python environment.\n\n```bash\n# checkout repo\ngit clone --recurse-submodules https://github.com/pytorch/multipy.git\ncd multipy\n\n# (optional) if using existing checkout\ngit submodule sync && git submodule update --init --recursive\n\ncd multipy\n# install python parts of `torch::deploy` in multipy/multipy/utils\npip install -e . --install-option=\"--cmakeoff\"\n\ncd multipy/runtime\n\n# configure runtime to build/\ncmake -S . -B build\n# if you need to override the ABI setting you can pass\ncmake -S . -B build -D_GLIBCXX_USE_CXX11_ABI=<0/1>\n\n# compile the files in build/\ncmake --build build --config Release -j\n```\n\n### Running unit tests for `multipy::runtime`\n\nWe first need to generate the neccessary examples. First make sure your python environment has [torch](https://pytorch.org). Afterwards, once `multipy::runtime` is built, run the following (executed automatically for `docker` and `pip` above):\n\n```\ncd multipy/multipy/runtime\npython example/generate_examples.py\ncd build\n./test_deploy\n```\n\n## Examples\n\nSee the [examples directory](./examples) for complete examples.\n\n### Packaging a model `for multipy::runtime`\n\n``multipy::runtime`` can load and run Python models that are packaged with\n``torch.package``. You can learn more about ``torch.package`` in the ``torch.package`` [documentation](https://pytorch.org/docs/stable/package.html#tutorials).\n\nFor now, let's create a simple model that we can load and run in ``multipy::runtime``.\n\n```python\nfrom torch.package import PackageExporter\nimport torchvision\n\n# Instantiate some model\nmodel = torchvision.models.resnet.resnet18()\n\n# Package and export it.\nwith PackageExporter(\"my_package.pt\") as e:\n    e.intern(\"torchvision.**\")\n    e.extern(\"numpy.**\")\n    e.extern(\"sys\")\n    e.extern(\"PIL.*\")\n    e.extern(\"typing_extensions\")\n    e.save_pickle(\"model\", \"model.pkl\", model)\n```\n\nNote that since \"numpy\", \"sys\", \"PIL\" were marked as \"extern\", `torch.package` will\nlook for these dependencies on the system that loads this package. They will not be packaged\nwith the model.\n\nNow, there should be a file named ``my_package.pt`` in your working directory.\n\n<br>\n\n### Load the model in C++\n```cpp\n#include <multipy/runtime/deploy.h>\n#include <multipy/runtime/path_environment.h>\n#include <torch/script.h>\n#include <torch/torch.h>\n\n#include <iostream>\n#include <memory>\n\nint main(int argc, const char* argv[]) {\n    if (argc != 2) {\n        std::cerr << \"usage: example-app <path-to-exported-script-module>\\n\";\n        return -1;\n    }\n\n    // Start an interpreter manager governing 4 embedded interpreters.\n    std::shared_ptr<multipy::runtime::Environment> env =\n        std::make_shared<multipy::runtime::PathEnvironment>(\n            std::getenv(\"PATH_TO_EXTERN_PYTHON_PACKAGES\") // Ensure to set this environment variable (e.g. /home/user/anaconda3/envs/multipy-example/lib/python3.8/site-packages)\n        );\n    multipy::runtime::InterpreterManager manager(4, env);\n\n    try {\n        // Load the model from the multipy.package.\n        multipy::runtime::Package package = manager.loadPackage(argv[1]);\n        multipy::runtime::ReplicatedObj model = package.loadPickle(\"model\", \"model.pkl\");\n    } catch (const c10::Error& e) {\n        std::cerr << \"error loading the model\\n\";\n        std::cerr << e.msg();\n        return -1;\n    }\n\n    std::cout << \"ok\\n\";\n}\n\n```\n\nThis small program introduces many of the core concepts of ``multipy::runtime``.\n\nAn ``InterpreterManager`` abstracts over a collection of independent Python\ninterpreters, allowing you to load balance across them when running your code.\n\n``PathEnvironment`` enables you to specify the location of Python\npackages on your system which are external, but necessary, for your model.\n\nUsing the ``InterpreterManager::loadPackage`` method, you can load a\n``multipy.package`` from disk and make it available to all interpreters.\n\n``Package::loadPickle`` allows you to retrieve specific Python objects\nfrom the package, like the ResNet model we saved earlier.\n\nFinally, the model itself is a ``ReplicatedObj``. This is an abstract handle to\nan object that is replicated across multiple interpreters. When you interact\nwith a ``ReplicatedObj`` (for example, by calling ``forward``), it will select\nan free interpreter to execute that interaction.\n\n<br>\n\n### Build and execute the C++ example\n\nAssuming the above C++ program was stored in a file called, `example-app.cpp`, a\nminimal `CMakeLists.txt` file would look like:\n\n```cmake\ncmake_minimum_required(VERSION 3.12 FATAL_ERROR)\nproject(multipy_tutorial)\n\nset(MULTIPY_PATH \"..\" CACHE PATH \"The repo where multipy is built or the PYTHONPATH\")\n\n# include the multipy utils to help link against\ninclude(${MULTIPY_PATH}/multipy/runtime/utils.cmake)\n\n# add headers from multipy\ninclude_directories(${MULTIPY_PATH})\n\n# link the multipy prebuilt binary\nadd_library(multipy_internal STATIC IMPORTED)\nset_target_properties(multipy_internal\n    PROPERTIES\n    IMPORTED_LOCATION\n    ${MULTIPY_PATH}/multipy/runtime/build/libtorch_deploy.a)\ncaffe2_interface_library(multipy_internal multipy)\n\nadd_executable(example-app example-app.cpp)\ntarget_link_libraries(example-app PUBLIC \"-Wl,--no-as-needed -rdynamic\" dl pthread util multipy c10 torch_cpu)\n```\n\nCurrently, it is necessary to build ``multipy::runtime`` as a static library.\nIn order to correctly link to a static library, the utility ``caffe2_interface_library``\nis used to appropriately set and unset ``--whole-archive`` flag.\n\nFurthermore, the ``-rdynamic`` flag is needed when linking to the executable\nto ensure that symbols are exported to the dynamic table, making them accessible\nto the deploy interpreters (which are dynamically loaded).\n\n**Updating LIBRARY_PATH and LD_LIBRARY_PATH**\n\nIn order to locate dependencies provided by PyTorch (e.g. `libshm`), we need to update the `LIBRARY_PATH` and `LD_LIBRARY_PATH` environment variables to include the path to PyTorch's C++ libraries. If you installed PyTorch using pip or conda, this path is usually in the site-packages. An example of this is provided below.\n\n```bash\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/home/user/anaconda3/envs/multipy-example/lib/python3.8/site-packages/torch/lib\"\nexport LIBRARY_PATH=\"$LIBRARY_PATH:/home/user/anaconda3/envs/multipy-example/lib/python3.8/site-packages/torch/lib\"\n```\n\nThe last step is configuring and building the project. Assuming that our code\ndirectory is laid out like this:\n\n```\nexample-app/\n    CMakeLists.txt\n    example-app.cpp\n```\n\n\nWe can now run the following commands to build the application from within the\n``example-app/`` folder:\n\n```bash\ncmake -S . -B build -DMULTIPY_PATH=\"/home/user/repos/multipy\" # the parent directory of multipy (i.e. the git repo)\ncmake --build build --config Release -j\n```\n\nNow we can run our app:\n\n```bash\n./example-app /path/to/my_package.pt\n```\n\n## Contributing\n\nWe welcome PRs! See the [CONTRIBUTING](CONTRIBUTING.md) file.\n\n## License\n\nMultiPy is BSD licensed, as found in the [LICENSE](LICENSE) file.\n\n## Legal\n\n[Terms of Use](https://opensource.facebook.com/legal/terms)\n[Privacy Policy](https://opensource.facebook.com/legal/privacy)\n\nCopyright (c) Meta Platforms, Inc. and affiliates.\nAll rights reserved.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/pytorch/multipy",
    "keywords": "pytorch,machine learning,inference",
    "license": "BSD-3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torchdeploy",
    "package_url": "https://pypi.org/project/torchdeploy/",
    "platform": null,
    "project_url": "https://pypi.org/project/torchdeploy/",
    "project_urls": {
      "Homepage": "https://github.com/pytorch/multipy"
    },
    "release_url": "https://pypi.org/project/torchdeploy/0.1.0/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "torch::deploy (multipy) is a C++ library that makes it easier to run eager PyTorch models in production by using independent python interpreters to avoid the GIL.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15571550,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fe51f773d84f100c9f544d3d659908facd0ef4ab6e2a8ddf42ec67f444754ded",
          "md5": "90767425304667c79c450f23e529a575",
          "sha256": "f22b7cff346e5fb29fd88c8057dec1241287471cf24e6ecd851b5eb2ad775fc9"
        },
        "downloads": -1,
        "filename": "torchdeploy-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "90767425304667c79c450f23e529a575",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 230504,
        "upload_time": "2022-10-28T20:33:26",
        "upload_time_iso_8601": "2022-10-28T20:33:26.479380Z",
        "url": "https://files.pythonhosted.org/packages/fe/51/f773d84f100c9f544d3d659908facd0ef4ab6e2a8ddf42ec67f444754ded/torchdeploy-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0.dev1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "faeca1bb629081c24da715c4c6f9c5b2d39e270ebdb1a068e498c13d078ad858",
          "md5": "3db25c422c8d0b42d63358e13289d148",
          "sha256": "ed4f01b3977ee4e5c2f95c6b424fbd719831023cf430637c23245ac23b96d57c"
        },
        "downloads": -1,
        "filename": "torchdeploy-0.1.0.dev1.tar.gz",
        "has_sig": false,
        "md5_digest": "3db25c422c8d0b42d63358e13289d148",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 230647,
        "upload_time": "2022-10-28T19:47:53",
        "upload_time_iso_8601": "2022-10-28T19:47:53.332415Z",
        "url": "https://files.pythonhosted.org/packages/fa/ec/a1bb629081c24da715c4c6f9c5b2d39e270ebdb1a068e498c13d078ad858/torchdeploy-0.1.0.dev1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0.dev2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a015757808ed16197e0705742ffad8d01f577bcb27971825d4f1aff64c6918c3",
          "md5": "ef0271c9f604425540c97ba9bf1660aa",
          "sha256": "3f3da3f273a481ccafff3cf810afde5e3c85d3dadfb3550dcff555901f8ecf3a"
        },
        "downloads": -1,
        "filename": "torchdeploy-0.1.0.dev2.tar.gz",
        "has_sig": false,
        "md5_digest": "ef0271c9f604425540c97ba9bf1660aa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 230718,
        "upload_time": "2022-10-28T19:48:47",
        "upload_time_iso_8601": "2022-10-28T19:48:47.844471Z",
        "url": "https://files.pythonhosted.org/packages/a0/15/757808ed16197e0705742ffad8d01f577bcb27971825d4f1aff64c6918c3/torchdeploy-0.1.0.dev2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fe51f773d84f100c9f544d3d659908facd0ef4ab6e2a8ddf42ec67f444754ded",
        "md5": "90767425304667c79c450f23e529a575",
        "sha256": "f22b7cff346e5fb29fd88c8057dec1241287471cf24e6ecd851b5eb2ad775fc9"
      },
      "downloads": -1,
      "filename": "torchdeploy-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "90767425304667c79c450f23e529a575",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 230504,
      "upload_time": "2022-10-28T20:33:26",
      "upload_time_iso_8601": "2022-10-28T20:33:26.479380Z",
      "url": "https://files.pythonhosted.org/packages/fe/51/f773d84f100c9f544d3d659908facd0ef4ab6e2a8ddf42ec67f444754ded/torchdeploy-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}