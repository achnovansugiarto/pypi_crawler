{
  "info": {
    "author": "driazati",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "This library is intended to make it easy to write small benchmarks and view the results.\n\n- [Usage](#usage)\n- [API](#api)\n  * [`Benchmark`](#benchmark)\n    + [`benchmark`](#benchmark)\n    + [`run`](#run)\n    + [`print_results`](#print-results)\n    + [`print_stats`](#print-stats)\n    + [`save_results`](#save-results)\n  * [`cleanup`](#cleanup)\n  * [`default_args`](#default-args)\n    + [`bench`](#bench)\n    + [`stats`](#stats)\n    + [`save`](#save)\n  * [`Timer`](#timer)\n  * [`Commit`](#commit)\n- [Developer Notes](#developer-notes)\n\n# Usage\n\nSee [examples/basic.py](examples/basic.py) for a full working example.\n\n```python\nfrom pyarkbench import Benchmark, Timer, default_args\n\nclass Basic(Benchmark):\n    def benchmark(self):\n        with Timer() as m1:\n            # Do some stuff\n            pass\n\n        with Timer() as m2:\n            # Do some other stuff\n            pass\n\n        return {\n            \"Metric 1 (ms)\": m1.ms_duration,\n            \"Metric 2 (ms)\": m2.ms_duration,\n        }\n\nif __name__ == '__main__':\n    # Initialize the benchmark and use the default command line args\n    bench = Basic(*default_args.bench())\n\n    # Run the benchmark (will run your code in `benchmark` many times, some to warm up and then some where the timer results are save)\n    results = bench.run()\n\n    # View the raw results\n    bench.print_results(results)\n\n    # See aggregate statistics about the results\n    bench.print_stats(results, stats=default_args.stats())\n\n    # Save the results to a JSON file named based on the benchmark class\n    bench.save_results(results, out_dir=default_args.save())\n```\n\n# API\n\n## `Benchmark`\n```python\nBenchmark(self, num_runs: int = 10, warmup_runs: int = 1, quiet: bool = False, commit: pybench.benchmarking_utils.Commit = None)\n```\n\nBenchmarks should extend this class and implement the `benchmark` method.\n\n### `benchmark`\n```python\nBenchmark.benchmark(self) -> Dict[str, float]\n```\n\nThis method must be implemented in your subclass and returns a dictionary\nof metric name to the time captured for that metric.\n\n### `run`\n```python\nBenchmark.run(self) -> Dict[str, Any]\n```\n\nThis is the entry point into your benchmark. It will first run `benchmark()`\n`self.warmup_runs` times without using the resulting timings, then it will\nrun `benchmark()` `self.num_runs` times and return the resulting timings.\n\n### `print_results`\n```python\nBenchmark.print_results(self, results)\n```\n\nPretty print the raw results by JSON dumping them.\n\n### `print_stats`\n```python\nBenchmark.print_stats(self, results, stats=('mean', 'median', 'variance'))\n```\n\nCollects and prints statistics over the results.\n\n### `save_results`\n```python\nBenchmark.save_results(self, results, out_dir, filename=None)\n```\n\nSave the results gathered from benchmarking and metadata about the commit\nto a JSON file named after the type of `self`.\n\n## `cleanup`\n```python\ncleanup()\n```\n\nChurn through a bunch of data, run the garbage collector, and sleep for a\nsecond to \"reset\" the Python interpreter.\n\n## `default_args`\n```python\ndefault_args(self, /, *args, **kwargs)\n```\n\nAdds a bunch of default command line arguments to make orchestrating\nbenchmark runs more convenient. To see all the options, call\n`default_args.init()` and run the script with the `--help` option.\n\n### `bench`\n```python\ndefault_args.bench()\n```\n\nDefault arguments to be passed to a `Benchmark` object\n\n### `stats`\n```python\ndefault_args.stats()\n```\n\nDefault arguments to be passed to the `Benchmark.print_stats` method\n\n### `save`\n```python\ndefault_args.save()\n```\n\nDefault arguments to be passed to the `Benchmark.save_results` method\n\n## `Timer`\n```python\nTimer(self, /, *args, **kwargs)\n```\n\nContext manager object that will time the execution of the statements it\nmanages.\n    `self.start` - start time\n    `self.end` - end time\n    `self.ms_duration` - end - start / 1000 / 1000\n\n## `Commit`\n```python\nCommit(self, time, pr, hash)\n```\n\nWrapper around a git commit\n\n# Developer Notes\n\nTo build this package locally, check it out and run\n\n```bash\npython setup.py develop\n```\n\nTo rebuild these docs, run\n\n```bash\npip install pydoc-markdown\npydocmd simple pybench.Benchmark+ pybench.cleanup pybench.default_args+ pybench.Timer pybench.Commit\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://github.com/driazati/pyarkbench",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pyarkbench",
    "package_url": "https://pypi.org/project/pyarkbench/",
    "platform": "",
    "project_url": "https://pypi.org/project/pyarkbench/",
    "project_urls": {
      "Homepage": "http://github.com/driazati/pyarkbench"
    },
    "release_url": "https://pypi.org/project/pyarkbench/1.0.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Benchmarking utilities",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6540550,
  "releases": {
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "be731c6bf971704825fcb51e8b1e214d24c27d14e090cf98c8a0f5a073cafe79",
          "md5": "63da8c712c41336409e85bb6d58b9600",
          "sha256": "2199142ee41716accb96b5c03f326453a3816fb8499c65a4cdb316bd36863c11"
        },
        "downloads": -1,
        "filename": "pyarkbench-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "63da8c712c41336409e85bb6d58b9600",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5662,
        "upload_time": "2020-01-29T22:59:44",
        "upload_time_iso_8601": "2020-01-29T22:59:44.109047Z",
        "url": "https://files.pythonhosted.org/packages/be/73/1c6bf971704825fcb51e8b1e214d24c27d14e090cf98c8a0f5a073cafe79/pyarkbench-1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1219370401ea4219ced3f4302e707d1a319a16b89a16e4fc574a1bcb22dbafd8",
          "md5": "a0e7c929f7642602acbb2021000ca37f",
          "sha256": "ea89fab273dbcdda4f6c807359c7d0d3cf7f7c666656e67636c19c254e89c22a"
        },
        "downloads": -1,
        "filename": "pyarkbench-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a0e7c929f7642602acbb2021000ca37f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6275,
        "upload_time": "2020-01-29T23:02:37",
        "upload_time_iso_8601": "2020-01-29T23:02:37.302729Z",
        "url": "https://files.pythonhosted.org/packages/12/19/370401ea4219ced3f4302e707d1a319a16b89a16e4fc574a1bcb22dbafd8/pyarkbench-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1219370401ea4219ced3f4302e707d1a319a16b89a16e4fc574a1bcb22dbafd8",
        "md5": "a0e7c929f7642602acbb2021000ca37f",
        "sha256": "ea89fab273dbcdda4f6c807359c7d0d3cf7f7c666656e67636c19c254e89c22a"
      },
      "downloads": -1,
      "filename": "pyarkbench-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "a0e7c929f7642602acbb2021000ca37f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 6275,
      "upload_time": "2020-01-29T23:02:37",
      "upload_time_iso_8601": "2020-01-29T23:02:37.302729Z",
      "url": "https://files.pythonhosted.org/packages/12/19/370401ea4219ced3f4302e707d1a319a16b89a16e4fc574a1bcb22dbafd8/pyarkbench-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}