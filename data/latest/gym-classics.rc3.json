{
  "info": {
    "author": "Brett Daley",
    "author_email": "brett.daley@ualberta.ca",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Gym Classics\n![pypi](https://img.shields.io/badge/pypi-0.0.2-blue)\n[![license](https://img.shields.io/badge/license-GPL%20v3.0-blue)](./LICENSE)\n![python](https://img.shields.io/badge/python-3.5%2B-green)\n\nGym Classics is a collection of well-known discrete MDPs from the reinforcement learning\nliterature implemented as OpenAI Gym environments.\nAPI support for dynamic programming is also provided.\n\nThe environments include tasks across a range of difficulties, from small random walks\nand gridworlds to challenging domains like racetracks and Jack's Car Rental.\nThese can be used as benchmarks for comparing the performance of various agents, or to\ntest and debug new learning methods.\n\n### Contents\n\n1. [Installation](#installation)\n\n1. [API Overview](#api-overview)\n\n1. [Example: Reinforcement Learning](#example-reinforcement-learning)\n\n1. [Example: Dynamic Programming](#example-dynamic-programming)\n\n1. [Environments Glossary](#environments-glossary)\n\n1. [References](#references)\n\n### Citing\n\nYou can cite this repository in published work using the following bibtex:\n\n```\n@misc{daley2021gym,\n  author={Daley, Brett},\n  title={Gym Classics},\n  year={2021},\n  publisher={GitHub},\n  journal={GitHub repository},\n  howpublished={\\url{https://github.com/brett-daley/gym-classics}},\n}\n```\n\n## Installation\n\nPrerequisites:\n- python 3.5+\n- gym\n- numpy\n- scipy (`JacksCarRental-v0` and `JacksCarRentalModified-v0` only)\n\n### Option 1: `pip`\n\n```\npip install gym-classics\n```\n\n### Option 2: `setuptools`\n\n```\ngit clone https://github.com/brett-daley/gym-classics.git\ncd gym-classics\npython setup.py install\n```\n\n---\n\n## API Overview\n\nOnce installed, the environments are automatically registered for `gym.make` by\nimporting the `gym_classics` package in your Python script.\nThe basic API is identical to that of OpenAI Gym.\nA minimal working example:\n\n```python\nimport gym\nimport gym_classics\n\nenv = gym.make('ClassicGridworld-v0')\nstate = env.reset()\nfor t in range(1, 100 + 1):\n    action = env.action_space.sample()  # Select a random action\n    next_state, reward, done, info = env.step(action)\n    print(\"t={}, state={}, action={}, reward={}, next_state={}, done={}\".format(\n        t, state, action, reward, next_state, done))\n    state = next_state if not done else env.reset()\nenv.close()\n```\n\nGym Classics also implements methods for querying a model of the environment.\nThe full interface of a Gym Classics environment therefore looks like this:\n\n```yaml\nclass Env:\n    # Standard Gym API:\n    - step(self, action)\n    - reset(self)\n    - render(self, mode='human')  # *currently not implemented by all environments*\n    - close(self)\n    - seed(self, seed=None)\n\n    # Extended Gym Classics API:\n    - states(self)                # returns a generator over all feasible states\n    - actions(self)               # returns a generator over all feasible actions\n    - model(self, state, action)  # returns all transitions from the given state-action pair\n```\n\nThe usage of `states`, `actions`, and `model` are discussed in\n[Example: Dynamic Programming](#example-dynamic-programming).\n\nState and action spaces for all environments are type `gym.spaces.Discrete`.\nThe size of these spaces can be queried as usual:\n`env.observation_space.n` and `env.action_space.n`.\nThis means that states and actions are represented as unique integers, which is useful\nfor advanced `numpy` indexing.\nNote that states and actions are enumerated in an arbitrary order for each environment.\n\n> **Tip:** Gym Classics environments also implement private methods called `_encode` and `_decode` which convert states between their integral and human-interpretable forms.\n> These should never be used by the agent, but can be useful for displaying results or debugging.\n> See the abstract [BaseEnv](gym_classics/envs/abstract/base_env.py) class for implementation details.\n\n## Example: Reinforcement Learning\n\nLet's test the classic Q-Learning algorithm [[4]](#references) on `ClassicGridworld-v0`.\n\n```python\nimport gym\nimport gym_classics\nimport numpy as np\n\n# Hyperparameters for Q-Learning\ndiscount = 0.9\nepsilon = 0.5\nlearning_rate = 0.025\n\n# Instantiate the environment\nenv = gym.make('ClassicGridworld-v0')\nstate = env.reset()\n\n# Set seeds for reproducibility\nnp.random.seed(0)\nenv.seed(0)\n\n# Our Q-function is a numpy array\nQ = np.zeros([env.observation_space.n, env.action_space.n])\n\n# Loop for 500k timesteps\nfor _ in range(500000):\n    # Select action from ε-greedy policy\n    if np.random.rand() < epsilon:\n        action = env.action_space.sample()\n    else:\n        action = np.argmax(Q[state])\n\n    # Step the environment\n    next_state, reward, done, _ = env.step(action)\n\n    # Q-Learning update:\n    # Q(s,a) <-- Q(s,a) + α * (r + γ max_a' Q(s',a') - Q(s,a))\n    target = reward - Q[state, action]\n    if not done:\n        target += discount * np.max(Q[next_state])\n    Q[state, action] += learning_rate * target\n\n    # Reset the environment if we're done\n    state = env.reset() if done else next_state\n\n# Now let's see what the value function looks like after training:\nV = np.max(Q, axis=1)\nprint(V)\n```\n\nOutput:\n\n```\n[ 0.5618515   0.75169693  1.          0.49147301  0.26363411 -1.\n  0.58655406  0.51379727  0.86959422  0.43567445  0.64966203]\n```\n\nThese values seem reasonable, but in the next section, we will certify their correctness\nby using dynamic programming.\n\n## Example: Dynamic Programming\n\nGym Classics extends the OpenAI Gym API by providing a lean interface for dynamic\nprogramming.\nGenerators are provided for the state and action spaces, enabling sweeps over the\nstate-action pairs:\n\n```python\nprint(sorted(env.states()))\nprint(sorted(env.actions()))\n```\n\nOutput:\n\n```\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n[0, 1, 2, 3]\n```\n\nWe can therefore see that `ClassicGridworld-v0` has 11 states and 4 actions.\nSince the state/action generators always return elements in the same arbitrary order,\nit is recommended to sort or shuffle them as needed.\n\nIt is also possible to poll the environment model at an arbitrary state-action pair.\nLet's inspect the model at state 0 and action 1:\n\n```python\nnext_states, rewards, dones, probabilities = env.model(state=0, action=1)\nprint(next_states)\nprint(rewards)\nprint(dones)\nprint(probabilities)\n```\n\nOutput:\n\n```\n[ 0  3 10]\n[0. 0. 0.]\n[0. 0. 0.]\n[0.8 0.1 0.1]\n```\n\nEach of the 4 return values are `numpy` arrays that represent the possible transitions.\nIn this case, there are 3 transitions from state 0 after taking action 1:\n\n1. Go to state 0, yield +0 reward, do not terminate episode. Probability: 80%.\n1. Go to state 3, yield +0 reward, do not terminate episode. Probability: 10%.\n1. Go to state 10, yield +0 reward, do not terminate episode. Probability: 10%.\n\nNote that these `numpy` arrays allow us to perform a value backup in a neat one-line\nsolution using advanced indexing!\n\n```python\nimport numpy as np\nV = np.zeros(env.observation_space.n)\nV[0] = np.sum(probabilities * (rewards + discount * (1.0 - dones) * V[next_states]))\n```\n\nIn practice, only advanced users will need to conduct backups manually like this.\nValue Iteration and other dynamic programming methods are already implemented in\n[dynamic_programming.py](gym_classics/dynamic_programming.py).\nLet's use Value Iteration to check that our Q-Learning implemention from\n[Example: Reinforcement Learning](#example-reinforcement-learning) is correct:\n\n```python\nimport gym\nfrom gym_classics.dynamic_programming import value_iteration\nimport numpy as np\n\n# Instantiate the environment\nenv = gym.make('ClassicGridworld-v0')\nstate = env.reset()\n\n# Set seeds for reproducibility\nnp.random.seed(0)\nenv.seed(0)\n\n# Compute the near-optimal values with Value Iteration\nV_star = value_iteration(env, discount=0.9, precision=1e-9)\n\n# Our Q-Learning values from earlier:\nV = [0.5618515,  0.75169693, 1.,         0.49147301, 0.26363411, -1.,\n     0.58655406, 0.51379727, 0.86959422, 0.43567445, 0.64966203]\n\n# Root Mean Square error:\nprint(\"RMS error: {}\".format(np.sqrt(np.square(V - V_star).mean())))\n\n# Maximum absolute difference:\nprint(\"Max abs diff: {}\".format(np.abs(V - V_star).max()))\n```\n\nOutput:\n\n```\nRMS error: 0.014976847878141084\nMax abs diff: 0.03832613967716292\n```\n\nBoth error metrics are very close to zero;\nwe can conclude that our Q-Learning implementation\nis working!\n\n## Environments Glossary\n\n| # | Env ID | Description |\n| :-: | :-: | --- |\n| 1 | `5Walk-v0` | A 5-state deterministic linear walk. Ideal for implementing random walk experiments.<br><br>**reference:** [[3]](#references) (page 125).<br><br>**state:** Discrete position {0, ..., 4} on the number line.<br><br>**actions:** Move left/right.<br><br>**rewards:** +1 for moving right in the extreme right state.<br><br>**termination:** Moving right in the extreme right state or moving left in the extreme left state. |\n| 2 | `19Walk-v0` | Same as `5Walk` but with 19 states and an additional -1 reward for moving left in the extreme left state.<br><br>**reference:** [[3]](#references) (page 145). |\n| 3 | `ClassicGridworld-v0` | A 4x3 pedagogical gridworld. The agent starts in the bottom-left cell. Actions are noisy; with a 10% chance each, a move action may be rotated by 90 degrees clockwise or counter-clockwise (the \"80-10-10 rule\"). Cell (1, 1) is blocked and cannot be occupied by the agent.<br><br>**reference:** [[1]](#references) (page 646).<br><br>**state**: Grid location.<br><br>**actions**: Move up/right/down/left.<br><br>**rewards**: +1 for taking any action in cell (3, 2). -1 for taking any action in cell (3, 1). *NOTE:* The original version adds a -0.04 penalty to all other transitions, but this implementation does not.<br><br>**termination**: Earning a nonzero reward. |\n| 4 | `CliffWalk-v0` | The Cliff Walking task, a 12x4 gridworld often used to contrast Sarsa with Q-Learning. The agent begins in the bottom-left cell and must navigate to the goal (bottom-right cell) without entering the region along the bottom (\"The Cliff\").<br><br>**reference:** [[3]](#references) (page 132, example 6.6).<br><br>**state**: Grid location.<br><br>**actions**: Move up/right/down/left.<br><br>**rewards**: -100 for entering The Cliff. -1 for all other transitions.<br><br>**termination**: Entering The Cliff or reaching the goal. |\n| 5 | `DynaMaze-v0` | A 9x6 deterministic gridworld with barriers to make navigation more challenging. The agent starts in cell (0, 3); the goal is the top-right cell.<br><br>**reference:** [[3]](#references) (page 164, example 8.1).<br><br>**state**: Grid location.<br><br>**actions**: Move up/right/down/left.<br><br>**rewards**: +1 for episode termination.<br><br>**termination**: Reaching the goal. |\n| 6 | `FourRooms-v0` | An 11x11 gridworld segmented into four rooms. The agent begins in the bottom-left cell; the goal is in the top-right cell. Actions are noisy; instead of the original transition probabilities, this implementation uses the 80-10-10 rule from `ClassicGridworld`.<br><br>**reference:** [[2]](#references) (page 192).<br><br>**state**: Grid location.<br><br>**actions**: Move up/right/down/left.<br><br>**rewards**: +1 for episode termination.<br><br>**termination**: Taking any action in the goal. |\n| 7 | `JacksCarRental-v0` | A challenging management problem where a rental company must balance the number of cars between two parking lots to maximize its profit. On each timestep, Poisson-distributed numbers of requests and returns come into each lot. (The lots have different statistics.) The agent may then move up to 5 cars between the lots for a proportional fee. The lots can never have more than 20 cars each, and a lot earns money for a request only if it has a car available.<br><br>**reference:** [[3]](#references) (page 81, example 4.2).<br><br>**state:** The number of cars at both lots.<br><br>**actions:** Move a number of cars {-5, ..., 5} for a total of 9 actions. Positive numbers represent moving cars from lot 1 to lot 2; negative numbers represent moving cars from lot 2 to lot 1.<br><br>**rewards:** +10 for each satisfied rental request. -2 for each car moved.<br><br>**termination:** 100 timesteps elapse. |\n| 8 | `JacksCarRentalModified-v0` | Same as `JacksCarRental` but with two modifications to the reward function. On each timestep:<br><br>1. One of Jack's employees can move a car from lot 1 to 2 for free.<br><br>2. Overnight parking incurs -4 reward per lot with more than 10 cars.<br><br>**reference:** [[3]](#references) (page 82, exercise 4.7). |\n| 9 | `Racetrack1-v0` | A gridworld-type racetrack where a racecar must traverse a right turn and reach the finish line as quickly as possible. The agent begins at a random location on the starting line and can only directly control the velocity of the racecar (not its position). Each velocity component can never be negative nor greater than 4. If the car goes out of bounds, it is reset to a random location on the starting line without terminating the episode. *NOTE:* While the original version forbids both velocity components from being zero simultaneously, no such restriction is enforced in this implementation.<br><br>**reference:** [[3]](#references) (page 112, figure 5.5, left).<br><br>**state:** Racecar position and velocity.<br><br>**actions:** Changes to the racecar's *velocity* (not position) vector, where the x- and y- components can be independently modified by {-1, 0, +1} on each timestep. This gives a total of 9 actions.<br><br>**rewards:** -1 on all transitions unless the finish line is reached.<br><br>**termination:** Reaching the finish line. |\n| 10 | `Racetrack2-v0` | Same as `Racetrack1` but with a different track layout.<br><br>**reference:** [[3]](#references) (page 112, figure 5.5, right). |\n| 11 | `SparseGridworld-v0` | A 10x8 featureless gridworld. The agent starts in cell (1, 3) and the goal is at cell (6, 3). To make it more challenging, the same 80-10-10 transition probabilities from `ClassicGridworld` are used. Great for testing various forms of credit assignment in the presence of noise.<br><br>**reference:** [[3]](#references) (page 147, figure 7.4).<br><br>**states:** Grid location.<br><br>**actions:** Move up/right/down/left.<br><br>**rewards:** +1 for episode termination.<br><br>**termination:** Reaching the goal. |\n| 12 | `WindyGridworld-v0` | A 10x7 deterministic gridworld where some columns are affected by an upward wind. The agent starts in cell (0, 3) and the goal is at cell (7, 3). If an agent executes an action from a cell with wind, the resulting position is given by the vector sum of the action's effect and the wind.<br><br>**reference:** [[3]](#references) (page 130, example 6.5).<br><br>**state:** Grid location.<br><br>**actions:** Move up/right/down/left.<br><br>**rewards:** -1 for all transitions unless the episode terminates.<br><br>**termination:** Reaching the goal. |\n| 13 | `WindyGridworldKings-v0` | Same as `WindyGridworld` but with diagonal \"King's\" moves permitted.<br><br>**reference:** [[3]](#references) (page 131, exercise 6.9).<br><br>**actions:** Move in the 4 cardinal directions and 4 intermediate directions. |\n| 14 | `WindyGridworldKingsNoOp-v0` | Same as `WindyGridworldKings` but with an extra \"no-op\" (do nothing) action.<br><br>**reference:** [[3]](#references) (page 131, exercise 6.9).<br><br>**actions:** Move in the 8 cardinal/intermediate directions or take a no-op action. |\n| 15 | `WindyGridworldKingsStochastic-v0` | Same as `WindyGridworldKings` but windy cells exhibit stochastic behavior: -1, +0, or +1 wind strength with probability 1/3 each.<br><br>**reference:** [[3]](#references) (page 131, exercise 6.10). |\n\n\n---\n\n## References\n\n1. [Russell & Norvig. Artificial Intelligence: A Modern Approach. 2009, 3rd Ed.](https://cs.calvin.edu/courses/cs/344/kvlinden/resources/AIMA-3rd-edition.pdf)\n\n1. [Sutton, Precup, & Singh. Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning. 1999.](https://people.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf)\n\n1. [Sutton & Barto. Reinforcement Learning: An Introduction. 2018, 2nd Ed.](http://incompleteideas.net/book/RLbook2020.pdf)\n\n1. [Watkins. Learning from Delayed Rewards. 1989.](http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf)\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/brett-daley/gym-classics",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "gym-classics",
    "package_url": "https://pypi.org/project/gym-classics/",
    "platform": null,
    "project_url": "https://pypi.org/project/gym-classics/",
    "project_urls": {
      "Homepage": "https://github.com/brett-daley/gym-classics"
    },
    "release_url": "https://pypi.org/project/gym-classics/0.0.2/",
    "requires_dist": null,
    "requires_python": ">=3.5",
    "summary": "Classic environments for reinforcement learning and dynamic programming, implemented in OpenAI Gym.",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17314760,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "051d3227beb857797751005858ebd0c957757e3b297e251afc2fc682d95684bb",
          "md5": "13dc481eb1945eddc130c989d5836d0e",
          "sha256": "1d3c767440bbcd59728c060aa4e160ac309db27a26d86688e18f8e4198024c0e"
        },
        "downloads": -1,
        "filename": "gym_classics-0.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "13dc481eb1945eddc130c989d5836d0e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 41232,
        "upload_time": "2021-01-26T05:58:27",
        "upload_time_iso_8601": "2021-01-26T05:58:27.106336Z",
        "url": "https://files.pythonhosted.org/packages/05/1d/3227beb857797751005858ebd0c957757e3b297e251afc2fc682d95684bb/gym_classics-0.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d7e673688ccba35a8beb826bbc1c11351365cb5f5b74e1e5f9af34748f27d796",
          "md5": "3e9770e7156230ac24b15fcb9ae30dc8",
          "sha256": "ff628f9ba66e74e3304b06a32c1e96c60697448a33da48a8d8bb69f35fe745e9"
        },
        "downloads": -1,
        "filename": "gym-classics-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3e9770e7156230ac24b15fcb9ae30dc8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 25703,
        "upload_time": "2021-01-26T05:58:28",
        "upload_time_iso_8601": "2021-01-26T05:58:28.458632Z",
        "url": "https://files.pythonhosted.org/packages/d7/e6/73688ccba35a8beb826bbc1c11351365cb5f5b74e1e5f9af34748f27d796/gym-classics-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4dfd84445b97fe38940c8aaabe97317fec5f8f5a665cd9b9ea19b444d2f57ec4",
          "md5": "88cd17d09363a67878c0e412909cca4e",
          "sha256": "3843360ff7e9746b613f1b6da437138d4906cbc79406785c24421caa6245bfcb"
        },
        "downloads": -1,
        "filename": "gym_classics-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "88cd17d09363a67878c0e412909cca4e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 41821,
        "upload_time": "2021-10-17T23:35:54",
        "upload_time_iso_8601": "2021-10-17T23:35:54.481055Z",
        "url": "https://files.pythonhosted.org/packages/4d/fd/84445b97fe38940c8aaabe97317fec5f8f5a665cd9b9ea19b444d2f57ec4/gym_classics-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f53ef5ecd0b89739118df1c4c02f37796b52e4d9a5e7c06c3a8406ae56a66cfb",
          "md5": "e62252d9df56e481ae0c19a6751f23e1",
          "sha256": "c9e390371f4d539f4cb0b8513f8430ce8fea0d54c76dab51ad8f71002d27eba3"
        },
        "downloads": -1,
        "filename": "gym-classics-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e62252d9df56e481ae0c19a6751f23e1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 37772,
        "upload_time": "2021-10-17T23:35:57",
        "upload_time_iso_8601": "2021-10-17T23:35:57.892486Z",
        "url": "https://files.pythonhosted.org/packages/f5/3e/f5ecd0b89739118df1c4c02f37796b52e4d9a5e7c06c3a8406ae56a66cfb/gym-classics-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "17d014540082c502f02041cb3f0a833d0dd8fc2d6e5d33f3c28e4da05f14d3fe",
          "md5": "a7687df98c8f13018959031fd325f4cc",
          "sha256": "7e6dc97e95e600667a6e2024f2a737c886d4a62a1d58e4f5434671e643698a1c"
        },
        "downloads": -1,
        "filename": "gym_classics-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a7687df98c8f13018959031fd325f4cc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 42129,
        "upload_time": "2023-03-16T06:52:52",
        "upload_time_iso_8601": "2023-03-16T06:52:52.179278Z",
        "url": "https://files.pythonhosted.org/packages/17/d0/14540082c502f02041cb3f0a833d0dd8fc2d6e5d33f3c28e4da05f14d3fe/gym_classics-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "07ba655d490134931ede1fe41b91ca129b86d87612e5f3205e83ea6e95dd2a48",
          "md5": "8e96ceba5540291fd61f7e0e8d07433f",
          "sha256": "5960a3e83a0394afa160b1414d7f92342268c6d40a037e69a871c169832a6281"
        },
        "downloads": -1,
        "filename": "gym-classics-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "8e96ceba5540291fd61f7e0e8d07433f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 38863,
        "upload_time": "2023-03-16T06:52:54",
        "upload_time_iso_8601": "2023-03-16T06:52:54.344577Z",
        "url": "https://files.pythonhosted.org/packages/07/ba/655d490134931ede1fe41b91ca129b86d87612e5f3205e83ea6e95dd2a48/gym-classics-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "17d014540082c502f02041cb3f0a833d0dd8fc2d6e5d33f3c28e4da05f14d3fe",
        "md5": "a7687df98c8f13018959031fd325f4cc",
        "sha256": "7e6dc97e95e600667a6e2024f2a737c886d4a62a1d58e4f5434671e643698a1c"
      },
      "downloads": -1,
      "filename": "gym_classics-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a7687df98c8f13018959031fd325f4cc",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5",
      "size": 42129,
      "upload_time": "2023-03-16T06:52:52",
      "upload_time_iso_8601": "2023-03-16T06:52:52.179278Z",
      "url": "https://files.pythonhosted.org/packages/17/d0/14540082c502f02041cb3f0a833d0dd8fc2d6e5d33f3c28e4da05f14d3fe/gym_classics-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "07ba655d490134931ede1fe41b91ca129b86d87612e5f3205e83ea6e95dd2a48",
        "md5": "8e96ceba5540291fd61f7e0e8d07433f",
        "sha256": "5960a3e83a0394afa160b1414d7f92342268c6d40a037e69a871c169832a6281"
      },
      "downloads": -1,
      "filename": "gym-classics-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "8e96ceba5540291fd61f7e0e8d07433f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 38863,
      "upload_time": "2023-03-16T06:52:54",
      "upload_time_iso_8601": "2023-03-16T06:52:54.344577Z",
      "url": "https://files.pythonhosted.org/packages/07/ba/655d490134931ede1fe41b91ca129b86d87612e5f3205e83ea6e95dd2a48/gym-classics-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}