{
  "info": {
    "author": "Tushar Sarkar , Nishant Rajadhyaksha",
    "author_email": "tushar.sarkar@somaiya.edu, n.rajadhyaksha@somaiya.edu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# TLA - Twitter Linguistic Analysis\n## Tool for linguistic analysis of communities \n\n\n[![](https://img.shields.io/badge/Made_with-PyTorch-res?style=for-the-badge&logo=pytorch)](https://pytorch.org/ \"PyTorch\")\n\n\nTLA is built using PyTorch, Transformers and several other State-of-the-Art machine learning\ntechniques and it aims to expedite and structure the cumbersome process of collecting, labeling, and analyzing data\nfrom Twitter for a corpus of languages while providing detailed labeled datasets\nfor all the languages. The analysis\nprovided by TLA will also go a long way in understanding the sentiments of\ndifferent linguistic communities and come up with new and innovative solutions\nfor their problems based on the analysis.\nList of languages our library provides support for are  listed as follows:<br>\n\n| Language | Code   | Language | Code |\n| ----------------  | ---------------- | ---------------- | ---------------- |\n| English |   en    | Hindi    |   hi  |\n| Swedish |   sv    | Thai     |   th  |\n| Dutch   |   nl   | Japanese |   ja  |\n | Turkish  |   tr  | Urdu     |  ur   |\n | Indonesian | id   |Portuguese | pt  |\n | French    | fr   | Chinese |  zn-ch |\n | Spanish  | es    | Persian |   fa   |\n | Romainain | ro  | Russian | ru |\n\n\n\n## Features\n\n- Provides 16 labeled Datasets for different languages for analysis.\n- Implements Bert based architecture to identify languages.\n- Provides Functionalities to Extract,process and label tweets from twitter.\n- Provides a Random Forest classifier to implement sentiment analysis on any string.\n\n---\n\n\n### Installation :\n```\npip install --upgrade https://github.com/tusharsarkar3/TLA.git\n```\n---\n## <div align=\"center\">Overview </div>\n\n<details>\n<summary>Extract data</summary>\n\n\n```\nfrom TLA.Data.get_data import store_data\nstore_data('en',False)\n```\nThis will extract and store the unlabeled data in a new directory inside data named \ndatasets.\n</details>\n\n<details>\n<summary>Label data</summary>\n\n\n```\nfrom TLA.Datasets.get_lang_data import language_data\ndf = language_data('en')\nprint(df)\n```\nThis will print the labeled data that we have already collected.\n</details>\n\n<details>\n<summary>Classify languages</summary>\n\n<details>\n<summary>Training </summary>\n\nTraining can be done in the following way:\n\n```\nfrom TLA.Lang_Classify.train import train_lang\ntrain_lang(path_to_dataset,epochs)\n```\n</details>\n\n<details>\n<summary>Prediction </summary>\n\nInference is done in the following way:\n\n```\nfrom TLA.Lang_Classify.predict import predict\nmodel = get_model(path_to_weights)\npreds = predict(dataframe_to_be_used,model)\n```\n</details>\n\n\n</details>\n\n\n<details>\n<summary>Analyse</summary>\n\n<details>\n<summary>Training </summary>\n\nTraining can be done in the following way:\n\n```\nfrom TLA.Analyse.train_rf import train_rf\ntrain_rf(path_to_dataset)\n```\nThis will store all the vectorizers and models in a seperate directory named\nsaved_rf and saved_vec and they are present inside Analysis directory.\nFurther instructions for training multiple languages is given in the next section which \nshows how to run the commands using CLI\n\n</details>\n\n<details>\n<summary>Final Analysis </summary>\n\nAnalysis is done in the following way:\n\n```\nfrom TLA.Analysis.analyse import analyse_data \nanalyse_data(path_to_weights)\n```\n\nThis will store the final analysis as .csv inside a new directory named\nanalysis.\n\n</details>\n\n\n</details>\n\n\n## <div align=\"center\">Overview with Git</div>\n<details> \n<summary>Installation another method</summary>\n\n```\ngit clone https://github.com/tusharsarkar3/TLA.git\n```\n</details>\n<details>\n<summary>Extract data</summary>\nNavigate to the required directory\n\n```\ncd Data\n```\n\nRun the following command:\n```\npython get_data.py --lang en --process True\n```\nLang flag is used to input the language of the dataset that is required and\nprocess flag shows where pre-processing should be done before returning the data.\nGive the following codes in the lang flag wrt the required language:\n\n\n\n <summary>Loading Dataset</summary>\n\nTo load a dataset run the following command in python.\n\n```\ndf= pd.read_csv(\"TLA/TLA/Datasets/get_data_en.csv\")\n\n```\nThe command will return a dataframe consisting of the data for the specific language requested.\n\nIn the phrase get_data_en, en can be sunstituted by the desired language code to load the dataframe for the specific language.\n\n  <summary>Pre-Processing</summary>\n\n To preprocess a given string run the following command.\n\n In your terminal use code \n\n ```\n cd Data\n ```\n\n then run the command in python\n\n ```\n from TLA.Data import Pre_Process_Tweets\n\n df=Pre_Process_Tweets.pre_process_tweet(df)\n ```\n\n Here the function pre_process_tweet takes an input as a dataframe of tweets and returns an output of a dataframe with the list of preprocessed words\n for a particular tweet next to the tweet in the dataframe.\n\n\n\n\n</details>\n\n\n\n\n<details>\n<summary>Analysis</summary>\n\n <summary> Training </summary>\n To train a random forest classifier for the purpose of sentiment analysis run the following command in your terminal.\n\n ```  \n cd Analysis\n ```\n then \n\n ```\n python train.rf --path \"path to your datafile\" --train_all_datasets False\n ```\n\n here the --path flag represents the path to the required dataset you want to train the Random Forest Classifier on\n the --train_all_datasets flag is a boolean which can be used to train the model on multiple datasets at once.\n\n The output is a file with the a .pkl file extention saved in the folder at location \"TLA\\Analysis\\saved_rf\\{}.pkl\"\n The output for vectorization of is stored in a .pkl file in the directory  \"TLA\\Analysis\\saved_vec\\{}.pkl\"\n\n <summary> Get Sentiment </summary>\n\n To get the sentiment of any string use the following code.\n\n In your terminal type\n\n ```\n cd Analysis\n ```\n then in your terminal type\n\n ```\n python get_sentiment.py --prediction \"Your string for prediction to be made upon\" --lang \"en\"\n ```\n\n here the --prediction flag collects the string for which you want to get the sentiment for.\n the --lang represents the language code representing the language you typed your string in.\n\n The output is a sentiment which is either positive or negative depending on your string.\n\n\n <summary>Statistics</summary>\n\n To get a comprehensive statistic on sentiment of datasets run the following command.\n\n In your terminal type\n\n ```\n cd Analysis\n ```\n\n then\n\n ```\n python analyse.py \n ```\n\n This will give you an output of a table1.csv file at the location 'TLA\\Analysis\\analysis\\table1.csv' comprising of statistics relating to the\n percentage of positive or negative tweets for a given language dataset.\n\n It will also give a table2.csv file at 'TLA\\Analysis\\analysis\\table2.csv' comprising of statistics for all languages combined.\n\n\n </details>  \n\n\n\n\n\n\n<details>\n<summary>Language Classification </summary>\n <summary>Training</summary>\n To train a model for language classfication on a given dataset run the following commands.\n\n In your terminal run\n\n ```\ncd Lang_Classify\n ```\n then run\n\n ```\n python train.py --data \"path for your dataset\" --model \"path to weights if pretrained\" --epochs 4\n ```\n\nThe --data flag requires the path to your training dataset.\n\n The --model flag requires the path to the model you want to implement\n\n The --epoch flag represents the epochs you want to train your model for.\n\n The output is a file with a .pt extention named saved_wieghts_full.pt where your trained wieghst are stored.\n\n\n <summary>Prediction</summary>\n To make prediction on any given string Us ethe following code.\n\n In your terminal type\n\n ```\n cd Lang_Classify\n ```\n then run the code\n\n ```\n python predict.py --predict \"Text/DataFrame for language to predicted\" --weights \" Path for the stored weights of your model \" \n ```\n\n The --predict flag requires the string you want to get the language for.\n\n The --wieghts flag is the path for the stored wieghts you want to run your model on to make predictions.\n\n\nThe outputs is the language your string was typed in.\n\n\n\n</details>\n\n\n\n\n---\n### Results:\n\n![img](ss/performance.png)\n\nPerformance of TLA ( Loss vs epochs)\n\n\n\n\n |Language | Total tweets | Positive Tweets Percentage | Negative Tweets Percentage |\n | ----------------  | ---------------- | ---------------- | ---------------- |\n |English | 500 | 66.8 | 33.2 |\n |Spanish | 500 |  61.4 | 38.6 |\n |Persian  | 50 | 52 | 48 |  \n |French | 500 | 53 | 47 | \n |Hindi | 500 | 62 | 38 | \n |Indonesian | 500 | 63.4 | 36.6|\n |Japanese | 500 | 85.6 |  14.4 |  \n |Dutch | 500 | 84.2 | 15.8  |\n |Portuguese|  500 |  61.2 | 38.8| \n |Romainain|  457 | 85.55 |  14.44| \n |Russian|  213 | 62.91 | 37.08 |\n |Swedish|  420 | 80.23 | 19.76 |\n |Thai|  424 | 71.46 | 28.53 |\n |Turkish|  500 | 67.8 | 32.2 | \n |Urdu| 42 | 69.04 |  30.95 |\n |Chinese| 500 | 80.6 | 19.4 | \n\n---\n#### Reference:\n\n ```\n@misc{sarkar2021tla,\n      title={TLA: Twitter Linguistic Analysis}, \n      author={Tushar Sarkar and Nishant Rajadhyaksha},\n      year={2021},\n      eprint={2107.09710},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n ```\n\n\n---\n #### Features to be added :\n- Access to more language\n- Creating GUI based system for better accesibility\n- Improving performance of the baseline model\n\n---\n\n<h3 align=\"center\"><b>Developed by <a href=\"https://github.com/tusharsarkar3\">Tushar Sarkar</a> and <a href=\"https://github.com/nishant42491\">Nishant Rajadhyaksha</a>\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/tusharsarkar3/TLA",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "TLAF",
    "package_url": "https://pypi.org/project/TLAF/",
    "platform": "",
    "project_url": "https://pypi.org/project/TLAF/",
    "project_urls": {
      "Homepage": "https://github.com/tusharsarkar3/TLA"
    },
    "release_url": "https://pypi.org/project/TLAF/1.1/",
    "requires_dist": [
      "beautifulsoup4 (==4.9.3)",
      "certifi (==2021.5.30)",
      "charset-normalizer (==2.0.3)",
      "click (==8.0.1)",
      "colorama (==0.4.4)",
      "cycler (==0.10.0)",
      "filelock (==3.0.12)",
      "huggingface-hub (==0.0.12)",
      "idna (==3.2)",
      "joblib (==1.0.1)",
      "kiwisolver (==1.3.1)",
      "lxml (==4.6.3)",
      "matplotlib (==3.4.2)",
      "nltk (==3.6.2)",
      "numpy (==1.21.1)",
      "packaging (==21.0)",
      "pandas (==1.3.0)",
      "Pillow (==8.3.1)",
      "pyparsing (==2.4.7)",
      "PySocks (==1.7.1)",
      "python-dateutil (==2.8.2)",
      "pytz (==2021.1)",
      "PyYAML (==5.4.1)",
      "regex (==2021.7.6)",
      "requests (==2.26.0)",
      "sacremoses (==0.0.45)",
      "scikit-learn (==0.24.2)",
      "scipy (==1.7.0)",
      "six (==1.16.0)",
      "sklearn (==0.0)",
      "snscrape (==0.3.4)",
      "soupsieve (==2.2.1)",
      "threadpoolctl (==2.2.0)",
      "tokenizers (==0.10.3)",
      "torch (==1.9.0)",
      "tqdm (==4.61.2)",
      "transformers (==4.8.2)",
      "typing-extensions (==3.10.0.0)",
      "urllib3 (==1.26.6)"
    ],
    "requires_python": "",
    "summary": "TLA is built using PyTorch, Transformers and several other State-of-the-Art machine learning techniques and it aims to expedite and structure the cumbersome process of collecting, labeling, and analyzing data from Twitter for a corpus of languages while providing detailed labeled datasets for all the languages.",
    "version": "1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10976790,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1eea5efd050256ed58f9b5ed13c5f35797fe26395c09adde5759c05b282d58c1",
          "md5": "de0f6165bc4bacec108cf37e4a67d85b",
          "sha256": "74d9846b8e893ddc1aade2e1d97774d26ab8b9d059d47c0697213d9dd3cb02c2"
        },
        "downloads": -1,
        "filename": "TLAF-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "de0f6165bc4bacec108cf37e4a67d85b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12235,
        "upload_time": "2021-07-21T17:45:00",
        "upload_time_iso_8601": "2021-07-21T17:45:00.121429Z",
        "url": "https://files.pythonhosted.org/packages/1e/ea/5efd050256ed58f9b5ed13c5f35797fe26395c09adde5759c05b282d58c1/TLAF-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9514ce2e90ba4793f2ad479699a8e9c37fa92baed187aa825c09c918ade83ff4",
          "md5": "63dd7201d63aa143c04b4179773389e6",
          "sha256": "9f8d119e254e37d8bb49707435ec923595c0ca73de55f1ba04ff1c2cc4747083"
        },
        "downloads": -1,
        "filename": "TLAF-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "63dd7201d63aa143c04b4179773389e6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 531606,
        "upload_time": "2021-07-21T18:23:01",
        "upload_time_iso_8601": "2021-07-21T18:23:01.466071Z",
        "url": "https://files.pythonhosted.org/packages/95/14/ce2e90ba4793f2ad479699a8e9c37fa92baed187aa825c09c918ade83ff4/TLAF-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1a819c8365777361bf6ed50d8ed6ffe4eb0aa70a6c2f8a162b02923b6d69420b",
          "md5": "a321e52f1c3d6738dd8798ab59ac0c78",
          "sha256": "3a903c1c074dc8a8e3aae85096b8cda9b2b5aca488e99e5a754a5ad3ec42fc8f"
        },
        "downloads": -1,
        "filename": "TLAF-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a321e52f1c3d6738dd8798ab59ac0c78",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 531937,
        "upload_time": "2021-07-21T19:37:30",
        "upload_time_iso_8601": "2021-07-21T19:37:30.198965Z",
        "url": "https://files.pythonhosted.org/packages/1a/81/9c8365777361bf6ed50d8ed6ffe4eb0aa70a6c2f8a162b02923b6d69420b/TLAF-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0738db3859f5f1432c28ae10568a8a72d88575cf0bc09f1fc4a79d6d9f3045b3",
          "md5": "04c7629ed22d53e1f714e2a5669f6c1a",
          "sha256": "cbed4cee6f94c877b0c3507b4c111c47a7f04cd44d176aef8cc6241f92b29674"
        },
        "downloads": -1,
        "filename": "TLAF-1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "04c7629ed22d53e1f714e2a5669f6c1a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 531988,
        "upload_time": "2021-07-22T11:11:56",
        "upload_time_iso_8601": "2021-07-22T11:11:56.036222Z",
        "url": "https://files.pythonhosted.org/packages/07/38/db3859f5f1432c28ae10568a8a72d88575cf0bc09f1fc4a79d6d9f3045b3/TLAF-1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c9ea33fef9e8df9ecc77cac394326e149c944798bfb34f5c531ed235edd9db31",
          "md5": "cb781a263769ae2e1423bd202e657c53",
          "sha256": "40272856facf762a02acfb04a1cb6d0f689d52f27fb62106e42e58e59bee8af6"
        },
        "downloads": -1,
        "filename": "TLAF-1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cb781a263769ae2e1423bd202e657c53",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 532027,
        "upload_time": "2021-07-22T11:54:52",
        "upload_time_iso_8601": "2021-07-22T11:54:52.582784Z",
        "url": "https://files.pythonhosted.org/packages/c9/ea/33fef9e8df9ecc77cac394326e149c944798bfb34f5c531ed235edd9db31/TLAF-1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c9ea33fef9e8df9ecc77cac394326e149c944798bfb34f5c531ed235edd9db31",
        "md5": "cb781a263769ae2e1423bd202e657c53",
        "sha256": "40272856facf762a02acfb04a1cb6d0f689d52f27fb62106e42e58e59bee8af6"
      },
      "downloads": -1,
      "filename": "TLAF-1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "cb781a263769ae2e1423bd202e657c53",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 532027,
      "upload_time": "2021-07-22T11:54:52",
      "upload_time_iso_8601": "2021-07-22T11:54:52.582784Z",
      "url": "https://files.pythonhosted.org/packages/c9/ea/33fef9e8df9ecc77cac394326e149c944798bfb34f5c531ed235edd9db31/TLAF-1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}