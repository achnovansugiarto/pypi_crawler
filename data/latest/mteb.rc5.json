{
  "info": {
    "author": "MTEB Contributors (https://github.com/embeddings-benchmark/mteb/graphs/contributors)",
    "author_email": "niklas@huggingface.co, nouamane@huggingface.co, info@nils-reimers.de",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Environment :: Console",
      "Intended Audience :: Developers",
      "Intended Audience :: Information Technology",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python"
    ],
    "description": "<h1 align=\"center\">Massive Text Embedding Benchmark</h1>\n\n<p align=\"center\">\n    <a href=\"https://github.com/mbeddings-benchmark/mteb/releases\">\n        <img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/embeddings-benchmark/mteb.svg\">\n    </a>\n    <a href=\"https://www.python.org/\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Made%20with-Python-1f425f.svg?color=purple\">\n    </a>\n    <a href=\"https://github.com/embeddings-benchmark/mteb/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/github/license/embeddings-benchmark/mteb.svg?color=green\">\n    </a>\n    <a href=\"https://pepy.tech/project/mteb\">\n        <img alt=\"Downloads\" src=\"https://static.pepy.tech/personalized-badge/mteb?period=total&units=international_system&left_color=grey&right_color=orange&left_text=Downloads\">\n    </a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <a href=\"https://arxiv.org/abs/2210.07316\">Paper</a> |\n        <a href=#leaderboard>Leaderboard</a> |\n        <a href=\"#installation\">Installation</a> |\n        <a href=\"#usage\">Usage</a> |\n        <a href=\"#available-tasks\">Tasks</a> |\n        <a href=\"https://huggingface.co/mteb\">Hugging Face</a>\n    <p>\n</h4>\n\n<h3 align=\"center\">\n    <a href=\"https://huggingface.co/\"><img style=\"float: middle; padding: 10px 10px 10px 10px;\" width=\"60\" height=\"55\" src=\"./images/hf_logo.png\" /></a>\n</h3>\n\n\n## Installation\n\n```bash\npip install mteb\n```\n\n## Usage\n\n* Using a python script (see [scripts/run_mteb_english.py](https://github.com/embeddings-benchmark/mteb/blob/main/scripts/run_mteb_english.py) and [mteb/mtebscripts](https://github.com/embeddings-benchmark/mtebscripts) for more):\n\n```python\nfrom mteb import MTEB\nfrom sentence_transformers import SentenceTransformer\n\n# Define the sentence-transformers model name\nmodel_name = \"average_word_embeddings_komninos\"\n\nmodel = SentenceTransformer(model_name)\nevaluation = MTEB(tasks=[\"Banking77Classification\"])\nresults = evaluation.run(model, output_folder=f\"results/{model_name}\")\n```\n\n* Using CLI\n\n```bash\nmteb --available_tasks\n\nmteb -m average_word_embeddings_komninos \\\n    -t Banking77Classification  \\\n    --output_folder results/average_word_embeddings_komninos \\\n    --verbosity 3\n```\n\n## Advanced usage\n\n### Dataset selection\n\nDatasets can be selected by providing the list of datasets, but also\n\n* by their task (e.g. \"Clustering\" or \"Classification\")\n\n````python\nevaluation = MTEB(task_types=['Clustering', 'Retrieval']) # Only select clustering and retrieval tasks\n````\n\n* by their categories e.g. \"S2S\" (sentence to sentence) or \"P2P\" (paragraph to paragraph)\n\n````python\nevaluation = MTEB(task_categories=['S2S']) # Only select sentence2sentence datasets\n````\n\n* by their languages\n\n````python\nevaluation = MTEB(task_langs=[\"en\", \"de\"]) # Only select datasets which are \"en\", \"de\" or \"en-de\"\n````\n\nYou can also specify which languages to load for multilingual/crosslingual tasks like below:\n\n````python\nfrom mteb.tasks import AmazonReviewsClassification, BUCCBitextMining\n\nevaluation = MTEB(tasks=[\n        AmazonReviewsClassification(langs=[\"en\", \"fr\"]) # Only load \"en\" and \"fr\" subsets of Amazon Reviews\n        BUCCBitextMining(langs=[\"de-en\"]), # Only load \"de-en\" subset of BUCC\n])\n````\n\n### Evaluation split\nYou can evaluate only on `test` splits of all tasks by doing the following:\n\n````python\nevaluation.run(model, eval_splits=[\"test\"])\n````\n\nNote that the public leaderboard uses the test splits for all datasets except MSMARCO, where the \"dev\" split is used.\n\n### Using a custom model\n\nModels should implement the following interface, implementing an `encode` function taking as inputs a list of sentences, and returning a list of embeddings (embeddings can be `np.array`, `torch.tensor`, etc.). For inspiration, you can look at the [mteb/mtebscripts repo](https://github.com/embeddings-benchmark/mtebscripts) used for running diverse models via SLURM scripts for the paper.\n\n```python\nclass MyModel():\n    def encode(self, sentences, batch_size=32, **kwargs):\n        \"\"\" Returns a list of embeddings for the given sentences.\n        Args:\n            sentences (`List[str]`): List of sentences to encode\n            batch_size (`int`): Batch size for the encoding\n\n        Returns:\n            `List[np.ndarray]` or `List[tensor]`: List of embeddings for the given sentences\n        \"\"\"\n        pass\n\nmodel = MyModel()\nevaluation = MTEB(tasks=[\"Banking77Classification\"])\nevaluation.run(model)\n```\n\nIf you'd like to use different encoding functions for query and corpus when evaluating a Dense Retrieval Exact Search (DRES) model on retrieval tasks from BeIR, you can make your model DRES compatible. If compatible like the below example, it will be used for BeIR upon evaluation.\n\n```python\nfrom mteb import AbsTaskRetrieval, DRESModel\n\nclass MyModel(DRESModel):\n    # Refer to the code of DRESModel for the methods to overwrite\n    pass\n\nassert AbsTaskRetrieval.is_dres_compatible(MyModel)\n```\n\n\n### Evaluating on a custom task\n\nTo add a new task, you need to implement a new class that inherits from the `AbsTask` associated with the task type (e.g. `AbsTaskReranking` for reranking tasks). You can find the supported task types in [here](https://github.com/embeddings-benchmark/mteb-draft/tree/main/mteb/abstasks).\n\n```python\nfrom mteb import MTEB\nfrom mteb.abstasks.AbsTaskReranking import AbsTaskReranking\nfrom sentence_transformers import SentenceTransformer\n\n\nclass MindSmallReranking(AbsTaskReranking):\n    @property\n    def description(self):\n        return {\n            \"name\": \"MindSmallReranking\",\n            \"hf_hub_name\": \"mteb/mind_small\",\n            \"description\": \"Microsoft News Dataset: A Large-Scale English Dataset for News Recommendation Research\",\n            \"reference\": \"https://www.microsoft.com/en-us/research/uploads/prod/2019/03/nl4se18LinkSO.pdf\",\n            \"type\": \"Reranking\",\n            \"category\": \"s2s\",\n            \"eval_splits\": [\"validation\"],\n            \"eval_langs\": [\"en\"],\n            \"main_score\": \"map\",\n        }\n\nmodel = SentenceTransformer(\"average_word_embeddings_komninos\")\nevaluation = MTEB(tasks=[MindSmallReranking()])\nevaluation.run(model)\n```\n\n> **Note:** for multilingual tasks, make sure your class also inherits from the `MultilingualTask` class like in [this](https://github.com/embeddings-benchmark/mteb-draft/blob/main/mteb/tasks/Classification/MTOPIntentClassification.py) example.\n\n## Leaderboard\n\nThe MTEB Leaderboard is available [here](https://huggingface.co/spaces/mteb/leaderboard). To submit:\n1. Run on MTEB: You can reference [scripts/run_mteb_english.py](https://github.com/embeddings-benchmark/mteb/blob/main/scripts/run_mteb_english.py) for all MTEB English datasets used in the main ranking. Advanced scripts with different models are available in the [mteb/mtebscripts repo](https://github.com/embeddings-benchmark/mtebscripts).\n2. Format the json files into metadata using the script at `scripts/mteb_meta.py`. For example\n`python scripts/mteb_meta.py path_to_results_folder`, which will create a `mteb_metadata.md` file. If you ran CQADupstack retrieval, make sure to merge the results first with `python scripts/merge_cqadupstack.py path_to_results_folder`.\n3. Copy the content of the `mteb_metadata.md` file to the top of a `README.md` file of your model on the Hub. See [here](https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit/blob/main/README.md) for an example.\n4. Refresh the leaderboard and you should see your scores ðŸ¥‡\n5. To have the scores appear without refreshing, you can open an issue on the [Community Tab of the LB](https://huggingface.co/spaces/mteb/leaderboard/discussions) and someone will restart the Space to cache your average scores.\n\n## Available tasks\n\n| Name | Hub URL | Description | Type | Category | #Languages | Train #Samples | Dev #Samples | Test #Samples | Avg. chars / train | Avg. chars / dev | Avg. chars / test\n|:-----|:-----|:-----|:-----|:-----|-----:|-----:|-----:|-----:|-----:|-----:|-----:|\n| [BUCC](https://comparable.limsi.fr/bucc2018/bucc2018-task.html) | [mteb/bucc-bitext-mining](https://huggingface.co/datasets/mteb/bucc-bitext-mining) | BUCC bitext mining dataset | BitextMining | s2s | 4 | 0 | 0 | 641684 | 0 | 0 | 101.3 |\n| [Tatoeba](https://github.com/facebookresearch/LASER/tree/main/data/tatoeba/v1) | [mteb/tatoeba-bitext-mining](https://huggingface.co/datasets/mteb/tatoeba-bitext-mining) | 1,000 English-aligned sentence pairs for each language based on the Tatoeba corpus | BitextMining | s2s | 112 | 0 | 0 | 2000 | 0 | 0 | 39.4 |\n| [AmazonCounterfactualClassification](https://arxiv.org/abs/2104.06893) | [mteb/amazon_counterfactual](https://huggingface.co/datasets/mteb/amazon_counterfactual) | A collection of Amazon customer reviews annotated for counterfactual detection pair classification. | Classification | s2s | 4 | 4018 | 335 | 670 | 107.3 | 109.2 | 106.1 |\n| [AmazonPolarityClassification](https://dl.acm.org/doi/10.1145/2507157.2507163) | [mteb/amazon_polarity](https://huggingface.co/datasets/mteb/amazon_polarity) | Amazon Polarity Classification Dataset. | Classification | s2s | 1 | 3600000 | 0 | 400000 | 431.6 | 0 | 431.4 |\n| [AmazonReviewsClassification](https://arxiv.org/abs/2010.02573) | [mteb/amazon_reviews_multi](https://huggingface.co/datasets/mteb/amazon_reviews_multi) | A collection of Amazon reviews specifically designed to aid research in multilingual text classification. | Classification | s2s | 6 | 1200000 | 30000 | 30000 | 160.5 | 159.2 | 160.4 |\n| [Banking77Classification](https://arxiv.org/abs/2003.04807) | [mteb/banking77](https://huggingface.co/datasets/mteb/banking77) | Dataset composed of online banking queries annotated with their corresponding intents. | Classification | s2s | 1 | 10003 | 0 | 3080 | 59.5 | 0 | 54.2 |\n| [EmotionClassification](https://www.aclweb.org/anthology/D18-1404) | [mteb/emotion](https://huggingface.co/datasets/mteb/emotion) | Emotion is a dataset of English Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. For more detailed information please refer to the paper. | Classification | s2s | 1 | 16000 | 2000 | 2000 | 96.8 | 95.3 | 96.6 |\n| [ImdbClassification](http://www.aclweb.org/anthology/P11-1015) | [mteb/imdb](https://huggingface.co/datasets/mteb/imdb) | Large Movie Review Dataset | Classification | p2p | 1 | 25000 | 0 | 25000 | 1325.1 | 0 | 1293.8 |\n| [MassiveIntentClassification](https://arxiv.org/abs/2204.08582#:~:text=MASSIVE%20contains%201M%20realistic%2C%20parallel,diverse%20languages%20from%2029%20genera.) | [mteb/amazon_massive_intent](https://huggingface.co/datasets/mteb/amazon_massive_intent) | MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages | Classification | s2s | 51 | 11514 | 2033 | 2974 | 35.0 | 34.8 | 34.6 |\n| [MassiveScenarioClassification](https://arxiv.org/abs/2204.08582#:~:text=MASSIVE%20contains%201M%20realistic%2C%20parallel,diverse%20languages%20from%2029%20genera.) | [mteb/amazon_massive_scenario](https://huggingface.co/datasets/mteb/amazon_massive_scenario) | MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages | Classification | s2s | 51 | 11514 | 2033 | 2974 | 35.0 | 34.8 | 34.6 |\n| [MTOPDomainClassification](https://arxiv.org/pdf/2008.09335.pdf) | [mteb/mtop_domain](https://huggingface.co/datasets/mteb/mtop_domain) | MTOP: Multilingual Task-Oriented Semantic Parsing | Classification | s2s | 6 | 15667 | 2235 | 4386 | 36.6 | 36.5 | 36.8 |\n| [MTOPIntentClassification](https://arxiv.org/pdf/2008.09335.pdf) | [mteb/mtop_intent](https://huggingface.co/datasets/mteb/mtop_intent) | MTOP: Multilingual Task-Oriented Semantic Parsing | Classification | s2s | 6 | 15667 | 2235 | 4386 | 36.6 | 36.5 | 36.8 |\n| [ToxicConversationsClassification](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview) | [mteb/toxic_conversations_50k](https://huggingface.co/datasets/mteb/toxic_conversations_50k) | Collection of comments from the Civil Comments platform together with annotations if the comment is toxic or not. | Classification | s2s | 1 | 50000 | 0 | 50000 | 298.8 | 0 | 296.6 |\n| [TweetSentimentExtractionClassification](https://www.kaggle.com/competitions/tweet-sentiment-extraction/overview) | [mteb/tweet_sentiment_extraction](https://huggingface.co/datasets/mteb/tweet_sentiment_extraction) |  | Classification | s2s | 1 | 27481 | 0 | 3534 | 68.3 | 0 | 67.8 |\n| [ArxivClusteringP2P](https://www.kaggle.com/Cornell-University/arxiv) | [mteb/arxiv-clustering-p2p](https://huggingface.co/datasets/mteb/arxiv-clustering-p2p) | Clustering of titles+abstract from arxiv. Clustering of 30 sets, either on the main or secondary category | Clustering | p2p | 1 | 0 | 0 | 732723 | 0 | 0 | 1009.9 |\n| [ArxivClusteringS2S](https://www.kaggle.com/Cornell-University/arxiv) | [mteb/arxiv-clustering-s2s](https://huggingface.co/datasets/mteb/arxiv-clustering-s2s) | Clustering of titles from arxiv. Clustering of 30 sets, either on the main or secondary category | Clustering | s2s | 1 | 0 | 0 | 732723 | 0 | 0 | 74.0 |\n| [BiorxivClusteringP2P](https://api.biorxiv.org/) | [mteb/biorxiv-clustering-p2p](https://huggingface.co/datasets/mteb/biorxiv-clustering-p2p) | Clustering of titles+abstract from biorxiv. Clustering of 10 sets, based on the main category. | Clustering | p2p | 1 | 0 | 0 | 75000 | 0 | 0 | 1666.2 |\n| [BiorxivClusteringS2S](https://api.biorxiv.org/) | [mteb/biorxiv-clustering-s2s](https://huggingface.co/datasets/mteb/biorxiv-clustering-s2s) | Clustering of titles from biorxiv. Clustering of 10 sets, based on the main category. | Clustering | s2s | 1 | 0 | 0 | 75000 | 0 | 0 | 101.6 |\n| [MedrxivClusteringP2P](https://api.biorxiv.org/) | [mteb/medrxiv-clustering-p2p](https://huggingface.co/datasets/mteb/medrxiv-clustering-p2p) | Clustering of titles+abstract from medrxiv. Clustering of 10 sets, based on the main category. | Clustering | p2p | 1 | 0 | 0 | 37500 | 0 | 0 | 1981.2 |\n| [MedrxivClusteringS2S](https://api.biorxiv.org/) | [mteb/medrxiv-clustering-s2s](https://huggingface.co/datasets/mteb/medrxiv-clustering-s2s) | Clustering of titles from medrxiv. Clustering of 10 sets, based on the main category. | Clustering | s2s | 1 | 0 | 0 | 37500 | 0 | 0 | 114.7 |\n| [RedditClustering](https://arxiv.org/abs/2104.07081) | [mteb/reddit-clustering](https://huggingface.co/datasets/mteb/reddit-clustering) | Clustering of titles from 199 subreddits. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences. | Clustering | s2s | 1 | 0 | 0 | 420464 | 0 | 0 | 64.7 |\n| [RedditClusteringP2P](https://huggingface.co/datasets/sentence-transformers/reddit-title-body) | [mteb/reddit-clustering-p2p](https://huggingface.co/datasets/mteb/reddit-clustering-p2p) | Clustering of title+posts from reddit. Clustering of 10 sets of 50k paragraphs and 40 sets of 10k paragraphs. | Clustering | p2p | 1 | 0 | 0 | 459399 | 0 | 0 | 727.7 |\n| [StackExchangeClustering](https://arxiv.org/abs/2104.07081) | [mteb/stackexchange-clustering](https://huggingface.co/datasets/mteb/stackexchange-clustering) | Clustering of titles from 121 stackexchanges. Clustering of 25 sets, each with 10-50 classes, and each class with 100 - 1000 sentences. | Clustering | s2s | 1 | 0 | 417060 | 373850 | 0 | 56.8 | 57.0 |\n| [StackExchangeClusteringP2P](https://huggingface.co/datasets/flax-sentence-embeddings/stackexchange_title_body_jsonl) | [mteb/stackexchange-clustering-p2p](https://huggingface.co/datasets/mteb/stackexchange-clustering-p2p) | Clustering of title+body from stackexchange. Clustering of 5 sets of 10k paragraphs and 5 sets of 5k paragraphs. | Clustering | p2p | 1 | 0 | 0 | 75000 | 0 | 0 | 1090.7 |\n| [TwentyNewsgroupsClustering](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) | [mteb/twentynewsgroups-clustering](https://huggingface.co/datasets/mteb/twentynewsgroups-clustering) | Clustering of the 20 Newsgroups dataset (subject only). | Clustering | s2s | 1 | 0 | 0 | 59545 | 0 | 0 | 32.0 |\n| [SprintDuplicateQuestions](https://www.aclweb.org/anthology/D18-1131/) | [mteb/sprintduplicatequestions-pairclassification](https://huggingface.co/datasets/mteb/sprintduplicatequestions-pairclassification) | Duplicate questions from the Sprint community. | PairClassification | s2s | 1 | 0 | 101000 | 101000 | 0 | 65.2 | 67.9 |\n| [TwitterSemEval2015](https://alt.qcri.org/semeval2015/task1/) | [mteb/twittersemeval2015-pairclassification](https://huggingface.co/datasets/mteb/twittersemeval2015-pairclassification) | Paraphrase-Pairs of Tweets from the SemEval 2015 workshop. | PairClassification | s2s | 1 | 0 | 0 | 16777 | 0 | 0 | 38.3 |\n| [TwitterURLCorpus](https://languagenet.github.io/) | [mteb/twitterurlcorpus-pairclassification](https://huggingface.co/datasets/mteb/twitterurlcorpus-pairclassification) | Paraphrase-Pairs of Tweets. | PairClassification | s2s | 1 | 0 | 0 | 51534 | 0 | 0 | 79.5 |\n| [AskUbuntuDupQuestions](https://github.com/taolei87/askubuntu) | [mteb/askubuntudupquestions-reranking](https://huggingface.co/datasets/mteb/askubuntudupquestions-reranking) | AskUbuntu Question Dataset - Questions from AskUbuntu with manual annotations marking pairs of questions as similar or non-similar | Reranking | s2s | 1 | 0 | 0 | 2255 | 0 | 0 | 52.5 |\n| [MindSmallReranking](https://msnews.github.io/assets/doc/ACL2020_MIND.pdf) | [mteb/mind_small](https://huggingface.co/datasets/mteb/mind_small) | Microsoft News Dataset: A Large-Scale English Dataset for News Recommendation Research | Reranking | s2s | 1 | 231530 | 0 | 107968 | 69.0 | 0 | 70.9 |\n| [SciDocsRR](https://allenai.org/data/scidocs) | [mteb/scidocs-reranking](https://huggingface.co/datasets/mteb/scidocs-reranking) | Ranking of related scientific papers based on their title. | Reranking | s2s | 1 | 0 | 19594 | 19599 | 0 | 69.4 | 69.0 |\n| [StackOverflowDupQuestions](https://www.microsoft.com/en-us/research/uploads/prod/2019/03/nl4se18LinkSO.pdf) | [mteb/stackoverflowdupquestions-reranking](https://huggingface.co/datasets/mteb/stackoverflowdupquestions-reranking) | Stack Overflow Duplicate Questions Task for questions with the tags Java, JavaScript and Python | Reranking | s2s | 1 | 23018 | 0 | 3467 | 49.6 | 0 | 49.8 |\n| [ArguAna](http://argumentation.bplaced.net/arguana/data) | [BeIR/arguana](https://huggingface.co/datasets/BeIR/arguana) | NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval | Retrieval | p2p | 1 | 0 | 0 | 10080 | 0 | 0 | 1052.9 |\n| [ClimateFEVER](https://www.sustainablefinance.uzh.ch/en/research/climate-fever.html) | [BeIR/climate-fever](https://huggingface.co/datasets/BeIR/climate-fever) | CLIMATE-FEVER is a dataset adopting the FEVER methodology that consists of 1,535 real-world claims regarding climate-change. | Retrieval | s2p | 1 | 0 | 0 | 5418128 | 0 | 0 | 539.1 |\n| [CQADupstackAndroidRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/android](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 23697 | 0 | 0 | 578.7 |\n| [CQADupstackEnglishRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/english](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 41791 | 0 | 0 | 467.1 |\n| [CQADupstackGamingRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/gaming](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 46896 | 0 | 0 | 474.7 |\n| [CQADupstackGisRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/gis](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 38522 | 0 | 0 | 991.1 |\n| [CQADupstackMathematicaRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/mathematica](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 17509 | 0 | 0 | 1103.7 |\n| [CQADupstackPhysicsRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/physics](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 39355 | 0 | 0 | 799.4 |\n| [CQADupstackProgrammersRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/programmers](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 33052 | 0 | 0 | 1030.2 |\n| [CQADupstackStatsRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/stats](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 42921 | 0 | 0 | 1041.0 |\n| [CQADupstackTexRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/tex](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 71090 | 0 | 0 | 1246.9 |\n| [CQADupstackUnixRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/unix](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 48454 | 0 | 0 | 984.7 |\n| [CQADupstackWebmastersRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/webmasters](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 17911 | 0 | 0 | 689.8 |\n| [CQADupstackWordpressRetrieval](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/) | [BeIR/cqadupstack/wordpress](https://huggingface.co/datasets/BeIR/cqadupstack-qrels) | CQADupStack: A Benchmark Data Set for Community Question-Answering Research | Retrieval | s2p | 1 | 0 | 0 | 49146 | 0 | 0 | 1111.9 |\n| [DBPedia](https://github.com/iai-group/DBpedia-Entity/) | [BeIR/dbpedia-entity](https://huggingface.co/datasets/BeIR/dbpedia-entity) | DBpedia-Entity is a standard test collection for entity search over the DBpedia knowledge base | Retrieval | s2p | 1 | 0 | 4635989 | 4636322 | 0 | 310.2 | 310.1 |\n| [FEVER](https://fever.ai/) | [BeIR/fever](https://huggingface.co/datasets/BeIR/fever) | FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. | Retrieval | s2p | 1 | 0 | 0 | 5423234 | 0 | 0 | 538.6 |\n| [FiQA2018](https://sites.google.com/view/fiqa/) | [BeIR/fiqa](https://huggingface.co/datasets/BeIR/fiqa) | Financial Opinion Mining and Question Answering | Retrieval | s2p | 1 | 0 | 0 | 58286 | 0 | 0 | 760.4 |\n| [HotpotQA](https://hotpotqa.github.io/) | [BeIR/hotpotqa](https://huggingface.co/datasets/BeIR/hotpotqa) | HotpotQA is a question answering dataset featuring natural, multi-hop questions, with strong supervision for supporting facts to enable more explainable question answering systems. | Retrieval | s2p | 1 | 0 | 0 | 5240734 | 0 | 0 | 288.6 |\n| [MSMARCO](https://microsoft.github.io/msmarco/) | [BeIR/msmarco](https://huggingface.co/datasets/BeIR/msmarco) | MS MARCO is a collection of datasets focused on deep learning in search. Note that the dev set is used for the leaderboard. | Retrieval | s2p | 1 | 0 | 8848803 | 8841866 | 0 | 336.6 | 336.8 |\n| [MSMARCOv2](https://microsoft.github.io/msmarco/TREC-Deep-Learning.html) | [BeIR/msmarco-v2](https://huggingface.co/datasets/BeIR/msmarco-v2) | MS MARCO is a collection of datasets focused on deep learning in search | Retrieval | s2p | 1 | 138641342 | 138368101 | 0 | 341.4 | 342.0 | 0 |\n| [NFCorpus](https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/) | [BeIR/nfcorpus](https://huggingface.co/datasets/BeIR/nfcorpus) | NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval | Retrieval | s2p | 1 | 0 | 0 | 3956 | 0 | 0 | 1462.7 |\n| [NQ](https://ai.google.com/research/NaturalQuestions/) | [BeIR/nq](https://huggingface.co/datasets/BeIR/nq) | NFCorpus: A Full-Text Learning to Rank Dataset for Medical Information Retrieval | Retrieval | s2p | 1 | 0 | 0 | 2684920 | 0 | 0 | 492.7 |\n| [QuoraRetrieval](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs) | [BeIR/quora](https://huggingface.co/datasets/BeIR/quora) | QuoraRetrieval is based on questions that are marked as duplicates on the Quora platform. Given a question, find other (duplicate) questions. | Retrieval | s2s | 1 | 0 | 0 | 532931 | 0 | 0 | 62.9 |\n| [SCIDOCS](https://allenai.org/data/scidocs) | [BeIR/scidocs](https://huggingface.co/datasets/BeIR/scidocs) | SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. | Retrieval | s2p | 1 | 0 | 0 | 26657 | 0 | 0 | 1161.9 |\n| [SciFact](https://github.com/allenai/scifact) | [BeIR/scifact](https://huggingface.co/datasets/BeIR/scifact) | SciFact verifies scientific claims using evidence from the research literature containing scientific paper abstracts. | Retrieval | s2p | 1 | 0 | 0 | 5483 | 0 | 0 | 1422.3 |\n| [Touche2020](https://webis.de/events/touche-20/shared-task-1.html) | [BeIR/webis-touche2020](https://huggingface.co/datasets/BeIR/webis-touche2020) | TouchÃ© Task 1: Argument Retrieval for Controversial Questions | Retrieval | s2p | 1 | 0 | 0 | 382594 | 0 | 0 | 1720.1 |\n| [TRECCOVID](https://ir.nist.gov/covidSubmit/index.html) | [BeIR/trec-covid](https://huggingface.co/datasets/BeIR/trec-covid) | TRECCOVID is an ad-hoc search challenge based on the CORD-19 dataset containing scientific articles related to the COVID-19 pandemic | Retrieval | s2p | 1 | 0 | 0 | 171382 | 0 | 0 | 1117.4 |\n| [BIOSSES](https://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html) | [mteb/biosses-sts](https://huggingface.co/datasets/mteb/biosses-sts) | Biomedical Semantic Similarity Estimation. | STS | s2s | 1 | 0 | 0 | 200 | 0 | 0 | 156.6 |\n| [SICK-R](https://www.aclweb.org/anthology/S14-2001.pdf) | [mteb/sickr-sts](https://huggingface.co/datasets/mteb/sickr-sts) | Semantic Textual Similarity SICK-R dataset as described here: | STS | s2s | 1 | 0 | 0 | 19854 | 0 | 0 | 46.1 |\n| [STS12](https://www.aclweb.org/anthology/S12-1051.pdf) | [mteb/sts12-sts](https://huggingface.co/datasets/mteb/sts12-sts) | SemEval STS 2012 dataset. | STS | s2s | 1 | 4468 | 0 | 6216 | 100.7 | 0 | 64.7 |\n| [STS13](https://www.aclweb.org/anthology/S13-1004/) | [mteb/sts13-sts](https://huggingface.co/datasets/mteb/sts13-sts) | SemEval STS 2013 dataset. | STS | s2s | 1 | 0 | 0 | 3000 | 0 | 0 | 54.0 |\n| [STS14](http://alt.qcri.org/semeval2014/task10/) | [mteb/sts14-sts](https://huggingface.co/datasets/mteb/sts14-sts) | SemEval STS 2014 dataset. Currently only the English dataset | STS | s2s | 1 | 0 | 0 | 7500 | 0 | 0 | 54.3 |\n| [STS15](http://alt.qcri.org/semeval2015/task2/) | [mteb/sts15-sts](https://huggingface.co/datasets/mteb/sts15-sts) | SemEval STS 2015 dataset | STS | s2s | 1 | 0 | 0 | 6000 | 0 | 0 | 57.7 |\n| [STS16](http://alt.qcri.org/semeval2016/task1/) | [mteb/sts16-sts](https://huggingface.co/datasets/mteb/sts16-sts) | SemEval STS 2016 dataset | STS | s2s | 1 | 0 | 0 | 2372 | 0 | 0 | 65.3 |\n| [STS17](http://alt.qcri.org/semeval2016/task1/) | [mteb/sts17-crosslingual-sts](https://huggingface.co/datasets/mteb/sts17-crosslingual-sts) | STS 2017 dataset | STS | s2s | 11 | 0 | 0 | 500 | 0 | 0 | 43.3 |\n| [STS22](https://competitions.codalab.org/competitions/33835) | [mteb/sts22-crosslingual-sts](https://huggingface.co/datasets/mteb/sts22-crosslingual-sts) | SemEval 2022 Task 8: Multilingual News Article Similarity | STS | s2s | 18 | 0 | 0 | 8060 | 0 | 0 | 1992.8 |\n| [STSBenchmark](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) | [mteb/stsbenchmark-sts](https://huggingface.co/datasets/mteb/stsbenchmark-sts) | Semantic Textual Similarity Benchmark (STSbenchmark) dataset. | STS | s2s | 1 | 11498 | 3000 | 2758 | 57.6 | 64.0 | 53.6 |\n| [SummEval](https://tabilab.cmpe.boun.edu.tr/BIOSSES/DataSet.html) | [mteb/summeval](https://huggingface.co/datasets/mteb/summeval) | News Article Summary Semantic Similarity Estimation. | Summarization | s2s | 1 | 0 | 0 | 2800 | 0 | 0 | 359.8 |\n\n\n## Citation\n\nIf you find MTEB useful, feel free to cite our publication [MTEB: Massive Text Embedding Benchmark](https://arxiv.org/abs/2210.07316):\n\n```bibtex\n@article{muennighoff2022mteb,\n  doi = {10.48550/ARXIV.2210.07316},\n  url = {https://arxiv.org/abs/2210.07316},\n  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\\\"\\i}c and Reimers, Nils},\n  title = {MTEB: Massive Text Embedding Benchmark},\n  publisher = {arXiv},\n  journal={arXiv preprint arXiv:2210.07316},  \n  year = {2022}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/embeddings-benchmark/mteb",
    "keywords": "deep learning,text embeddings,benchmark",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mteb",
    "package_url": "https://pypi.org/project/mteb/",
    "platform": null,
    "project_url": "https://pypi.org/project/mteb/",
    "project_urls": {
      "Homepage": "https://github.com/embeddings-benchmark/mteb",
      "Huggingface Organization": "https://huggingface.co/mteb",
      "Source Code": "https://github.com/embeddings-benchmark/mteb"
    },
    "release_url": "https://pypi.org/project/mteb/1.0.2/",
    "requires_dist": [
      "datasets (>=2.2.0)",
      "jsonlines",
      "numpy",
      "requests (>=2.26.0)",
      "scikit-learn (>=1.0.2)",
      "scipy",
      "sentence-transformers (>=2.2.0)",
      "torch",
      "tqdm",
      "rich",
      "beir ; extra == 'beir'"
    ],
    "requires_python": ">=3.7.0",
    "summary": "Massive Text Embedding Benchmark",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17476285,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e10fca4a2c0e221f24169c335a99ec34f357ce612df318583f80096d1f3711b8",
          "md5": "af17063e58b4ca43e837abc1e6397726",
          "sha256": "af62d0c58bfcbb7bf46b5775f893bbe75b83836f82b78b284b9f213a84233da3"
        },
        "downloads": -1,
        "filename": "mteb-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "af17063e58b4ca43e837abc1e6397726",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 63068,
        "upload_time": "2022-06-30T19:58:46",
        "upload_time_iso_8601": "2022-06-30T19:58:46.040295Z",
        "url": "https://files.pythonhosted.org/packages/e1/0f/ca4a2c0e221f24169c335a99ec34f357ce612df318583f80096d1f3711b8/mteb-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.9.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2df7ebd36104dd1ca5a5dcbd12e43520879313063f0bd67766fc516bf056d0ba",
          "md5": "2878c562f021e7f455c8681daf6b5ee9",
          "sha256": "467cca216b4a1bfd1c8965e537f4179e6ab69962ae50a866ef5162df56ee04ed"
        },
        "downloads": -1,
        "filename": "mteb-0.9.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2878c562f021e7f455c8681daf6b5ee9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 87304,
        "upload_time": "2022-10-13T15:09:14",
        "upload_time_iso_8601": "2022-10-13T15:09:14.996196Z",
        "url": "https://files.pythonhosted.org/packages/2d/f7/ebd36104dd1ca5a5dcbd12e43520879313063f0bd67766fc516bf056d0ba/mteb-0.9.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ce35aba7e0cafcc5e7ef483fbff11d66291cab25e3fa6cef0f322840755f36bf",
          "md5": "848f5328cd79cababeab73d6641e2824",
          "sha256": "110bc67423bbf19e4744fa3c9d4c6867618c02a4fb6114ac377f2a8b90b4a454"
        },
        "downloads": -1,
        "filename": "mteb-0.9.1.tar.gz",
        "has_sig": false,
        "md5_digest": "848f5328cd79cababeab73d6641e2824",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 51092,
        "upload_time": "2022-10-13T15:09:17",
        "upload_time_iso_8601": "2022-10-13T15:09:17.424200Z",
        "url": "https://files.pythonhosted.org/packages/ce/35/aba7e0cafcc5e7ef483fbff11d66291cab25e3fa6cef0f322840755f36bf/mteb-0.9.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1ea44d66d1d3c2cc9f4c4146ce42ea40c00b56efb0eea3f326255282575daf39",
          "md5": "dfb1dbe75cba1e8a81ee7046ae0e30c9",
          "sha256": "83ed703093a03e5fdcdb5abccafcaada531c8e40a94eca9e51c57422dcbb89d6"
        },
        "downloads": -1,
        "filename": "mteb-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dfb1dbe75cba1e8a81ee7046ae0e30c9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 87610,
        "upload_time": "2022-10-17T07:09:42",
        "upload_time_iso_8601": "2022-10-17T07:09:42.133504Z",
        "url": "https://files.pythonhosted.org/packages/1e/a4/4d66d1d3c2cc9f4c4146ce42ea40c00b56efb0eea3f326255282575daf39/mteb-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1d954ba037adaceb5c8911222411e56ade33d458cba20e4d807e9022a99125b8",
          "md5": "dd638d833e8e2259545f33adcdc676d4",
          "sha256": "d846168ed60436c9a6ebc3606513bc67bd73ad251c2c65aae8f1f1f13836f84d"
        },
        "downloads": -1,
        "filename": "mteb-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "dd638d833e8e2259545f33adcdc676d4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 52615,
        "upload_time": "2022-10-17T07:09:44",
        "upload_time_iso_8601": "2022-10-17T07:09:44.594847Z",
        "url": "https://files.pythonhosted.org/packages/1d/95/4ba037adaceb5c8911222411e56ade33d458cba20e4d807e9022a99125b8/mteb-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fc6abc06e8662d2bc25b2991903711111f00dd31efff3232004f3df9908fbcf8",
          "md5": "cbed4747f2200af83b5a44a11da3bbab",
          "sha256": "96be5a39691289eb0e322ef8e6159fb0723256f00c8752aca5ead0714e4313f6"
        },
        "downloads": -1,
        "filename": "mteb-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cbed4747f2200af83b5a44a11da3bbab",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 87678,
        "upload_time": "2022-11-29T18:04:01",
        "upload_time_iso_8601": "2022-11-29T18:04:01.425783Z",
        "url": "https://files.pythonhosted.org/packages/fc/6a/bc06e8662d2bc25b2991903711111f00dd31efff3232004f3df9908fbcf8/mteb-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8a2607cfaddc5ea43b589c4fa98c517ca32229a59c3ea6eefd28fea044d64b65",
          "md5": "d6a7098a7a6ee9be11ffb98a24474f9e",
          "sha256": "dc0bdc6397ef3b16825f6b335c30d154deb1db946284847ab2d35367ededfa8b"
        },
        "downloads": -1,
        "filename": "mteb-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "d6a7098a7a6ee9be11ffb98a24474f9e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 52715,
        "upload_time": "2022-11-29T18:04:04",
        "upload_time_iso_8601": "2022-11-29T18:04:04.427330Z",
        "url": "https://files.pythonhosted.org/packages/8a/26/07cfaddc5ea43b589c4fa98c517ca32229a59c3ea6eefd28fea044d64b65/mteb-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1232d1a14c69248af79f4656208603dbbef1302890d5f37a5cc1e6fef0ec5191",
          "md5": "a4beccf4f963442a36073a153de0852c",
          "sha256": "9c7500ab4bfcc0bba59cd81b9b0d9fe58a89c84b72a7e8bfb67627cef9396c31"
        },
        "downloads": -1,
        "filename": "mteb-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a4beccf4f963442a36073a153de0852c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 88101,
        "upload_time": "2023-03-28T10:26:57",
        "upload_time_iso_8601": "2023-03-28T10:26:57.236800Z",
        "url": "https://files.pythonhosted.org/packages/12/32/d1a14c69248af79f4656208603dbbef1302890d5f37a5cc1e6fef0ec5191/mteb-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a32baf5bba038485ea1f1756391febe9c63393a4df82af88f92fbbefacb0b668",
          "md5": "a7d9a93ed0b3f829d0485ea7c110bbad",
          "sha256": "13a13adaff88e5a1d78984b8ef65465d6536b21f76097ad8e566acdb13e2d963"
        },
        "downloads": -1,
        "filename": "mteb-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "a7d9a93ed0b3f829d0485ea7c110bbad",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 55919,
        "upload_time": "2023-03-28T10:26:59",
        "upload_time_iso_8601": "2023-03-28T10:26:59.488039Z",
        "url": "https://files.pythonhosted.org/packages/a3/2b/af5bba038485ea1f1756391febe9c63393a4df82af88f92fbbefacb0b668/mteb-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1232d1a14c69248af79f4656208603dbbef1302890d5f37a5cc1e6fef0ec5191",
        "md5": "a4beccf4f963442a36073a153de0852c",
        "sha256": "9c7500ab4bfcc0bba59cd81b9b0d9fe58a89c84b72a7e8bfb67627cef9396c31"
      },
      "downloads": -1,
      "filename": "mteb-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a4beccf4f963442a36073a153de0852c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7.0",
      "size": 88101,
      "upload_time": "2023-03-28T10:26:57",
      "upload_time_iso_8601": "2023-03-28T10:26:57.236800Z",
      "url": "https://files.pythonhosted.org/packages/12/32/d1a14c69248af79f4656208603dbbef1302890d5f37a5cc1e6fef0ec5191/mteb-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a32baf5bba038485ea1f1756391febe9c63393a4df82af88f92fbbefacb0b668",
        "md5": "a7d9a93ed0b3f829d0485ea7c110bbad",
        "sha256": "13a13adaff88e5a1d78984b8ef65465d6536b21f76097ad8e566acdb13e2d963"
      },
      "downloads": -1,
      "filename": "mteb-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "a7d9a93ed0b3f829d0485ea7c110bbad",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.0",
      "size": 55919,
      "upload_time": "2023-03-28T10:26:59",
      "upload_time_iso_8601": "2023-03-28T10:26:59.488039Z",
      "url": "https://files.pythonhosted.org/packages/a3/2b/af5bba038485ea1f1756391febe9c63393a4df82af88f92fbbefacb0b668/mteb-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}