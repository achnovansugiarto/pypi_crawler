{
  "info": {
    "author": "Radostin Cholakov",
    "author_email": "radicho123@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# datasetGPT\n\n`datasetGPT` is a command-line interface and a Python library for inferencing Large Language Models to generate textual datasets.\n\nPossible use cases may include:\n\n- Constructing textual corpora to train/fine-tune detectors for content written by AI.\n- Collecting datasets of LLM-produced conversations for research purposes, analysis of AI performance/impact/ethics, etc.\n- Automating a task that a LLM can handle over big amounts of input texts. For example, using GPT-3 to summarize 1000 paragraphs with a single CLI command.\n- Leveraging APIs of especially big LLMs to produce diverse texts for a specific task and then fine-tune a smaller model with them.\n\n> This tool is distributed freely and doesn't imply any restrictions on the downstream use cases.\n> However, you should make sure to follow the **Terms of use** of the backend APIs (OpenAI, Cohere, Petals, etc.) in your specific context.\n\n## Installation\n\n```\npip install datasetGPT\n```\n\nMost of the generation features rely on third-party APIs. Install their respective packages:\n\n```\npip install openai cohere petals\n```\n\n## Usage examples\n\n### Inference LLMs at scale\n\n```bash\nexport OPENAI_API_KEY=\"...\"\nexport COHERE_API_KEY=\"...\"\n\ndatasetGPT texts \\             \n    --prompt \"If {country} was a planet in the Star Wars universe it would be called\" \\\n    --backend \"openai|text-davinci-003\" \\\n    --backend \"cohere|medium\" \\\n    --temperature 0.9 \\\n    --option country Germany \\\n    --option country France \\\n    --max-length 50 \\\n    --num-samples 1 \\\n    --single-file\n```\n\nThe command above should produce a dataset file with 4 texts. Each possible combination of options is used for each of the backend LLMs. Check out the [CLI reference](#cli-reference) for more details. A dataset file looks like this:\n\n```json\n[\n    {\n        \"sample_id\": 0,\n        \"backend\": \"openai|text-davinci-003\",\n        \"max_length\": 50,\n        \"temperature\": 0.9,\n        \"country\": \"Germany\",\n        \"prompt\": \"If Germany was a planet in the Star Wars universe it would be called\",\n        \"output\": \" Euron. The planet would be home to a powerful and diverse species of aliens, known as the Eurons, that have evolved to a higher level of understanding and technological advancement compared to many of the other planets in the galaxy. The planet would be\"\n    },\n    {\n        \"sample_id\": 0,\n        \"backend\": \"openai|text-davinci-003\",\n        \"max_length\": 50,\n        \"temperature\": 0.9,\n        \"country\": \"France\",\n        \"prompt\": \"If France was a planet in the Star Wars universe it would be called\",\n        \"output\": \" The Empire of Liberty. It would be a peaceful, democratic planet with a strong sense of justice and equality. The planet would be home to many different species of aliens but the majority of its population would be humans. It would have a strong military and\"\n    },\n    {\n        \"sample_id\": 0,\n        \"backend\": \"cohere|medium\",\n        \"max_length\": 50,\n        \"temperature\": 0.9,\n        \"country\": \"Germany\",\n        \"prompt\": \"If Germany was a planet in the Star Wars universe it would be called\",\n        \"output\": \" the Hoth of the universe.\\nAfter the Soviet invasion of Eastern Europe and the subsequent Western anti-Soviet sentiment, Germany's arms manufacturers went into hyperdrive and the country churned out guns at a frightening pace. By the early 1930\"\n    },\n    ...\n]\n```\n\nAlternatively, you can use our [`TextsGenerator`](https://github.com/radi-cho/datasetGPT/blob/main/datasetGPT/texts.py#L27) to produce texts in Python.\n\n### Generate conversations with the ChatGPT API\n\n```bash\nexport OPENAI_API_KEY=\"...\"\ndatasetGPT conversations \\\n    --length 4 \\\n    --length 5 \\\n    --agent1 \"You're a shop assistant in a pet store. Answer to customer questions politely.\" \\\n    --agent2 \"You're a customer in a pet store. You should behave like a human. You want to buy {n} pets. Ask questions about the pets in the store.\" \\\n    --temperature 0.1 \\\n    --temperature 0.2 \\\n    --option n 2 \\\n    --option n 3 \\\n    --path dataset\n```\n\nThe command above should produce a dataset directory with 16 conversations saved as JSON files. You can specify if you want all of them to be saved in a single file. But why 16? Because `num_samples` dialogues are generated for each possible combination of parameters (conversation length, LLM temperature, and custom prompt options). A dataset item looks like this:\n\n```json\n{\n    \"length\": 5,\n    \"temperature\": 0.1,\n    \"n\": \"2\",\n    \"agent1\": \"You're a shop assistant in a pet store. Answer to customer questions politely. When the whole conversation is over end with \\\"Goodbye\\\".\",\n    \"agent2\": \"You're a customer in a pet store. You should behave like a human. You want to buy 2 pets. Ask questions about the pets in the store. When the whole conversation is over end with \\\"Goodbye\\\".\",\n    \"utterances\": [\n        [\n            \"agent1\",\n            \"Hello! How can I assist you today?\"\n        ],\n        [\n            \"agent2\",\n            \"Hi! I'm interested in buying two pets. Can you tell me what kind of pets you have available in the store?\"\n        ],\n        [\n            \"agent1\",\n            \"Certainly! We have a variety of pets available, including dogs, cats, birds, fish, hamsters, guinea pigs, rabbits, and reptiles. Is there a specific type of pet you're interested in?\"\n        ],\n        [\n            \"agent2\",\n            \"I'm not sure yet. Can you tell me more about the dogs and cats you have available? What breeds do you have?\"\n        ],\n        ...\n    ]\n}\n```\n\nYou can also use our [`ConversationsGenerator`](https://github.com/radi-cho/datasetGPT/blob/main/datasetGPT/conversations.py#L43) to produce texts with the `gpt-3.5-turbo` API programatically.\n\n```python\nfrom datasetGPT import ConversationsGenerator, ConversationsGeneratorConfig, DatasetWriter\n\ndataset_writer = DatasetWriter() # single_file=True\n\ngenerator_config = ConversationsGeneratorConfig(agent1=\"You're a shop assistant in a pet store. Answer to customer questions politely.\",\n                                                agent2=\"You're a customer in a pet store. You should behave like a human. You want to buy {n} pets. Ask questions about the pets in the store.\",\n                                                num_samples=2,\n                                                interruption=\"length\",\n                                                lengths=[4, 5],\n                                                temperatures=[0.1, 0.2],\n                                                options=[(\"n\", \"2\"), (\"n\", \"3\")])\n\nconversations_generator = ConversationsGenerator(generator_config)\n\nfor conversation in conversations_generator:\n    dataset_writer.save_intermediate_result(conversation)\n```\n\n## Contributing\n\n> Still under active development.\n\nContributions will be highly appreciated. Currently these features are under development:\n- [x] `datasetGPT conversations` - Make two ChatGPT agents talk with one another and record the conversation history.\n- [x] `datasetGPT texts` - Inference different LLMs with a given input prompt and generate multiple outputs by varying parameters.\n- [ ] `datasetGPT transformations` - Apply a list of transformations to a list of texts. For example, summarizing a list of texts for a {child | university student | PhD candidate} to understand.\n- [ ] Support more backend LLMs.\n\nTo set up a local development environment:\n\n```bash\ngit clone https://github.com/radi-cho/datasetGPT/\ncd datasetGPT\npip install -e .\n```\n\n## CLI Reference\n\n```\ndatasetGPT [OPTIONS] COMMAND [ARGS]...\n\n  Command line interface that generates datasets with LLMs.\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  conversations  Produce conversations between two gpt-3.5-turbo agents...\n  texts          Inference multiple LLMs at scale.\n```\n\n```\ndatasetGPT texts [OPTIONS]\n\n  Inference multiple LLMs at scale.\n\nOptions:\n  -p, --prompt TEXT            Input prompt.  [required]\n  -b, --backend TEXT           LLM APIs to use as backends. Use\n                               \"backend|model_name\" notation. For example:\n                               \"openai|text-davinci-003\".\n  -l, --max-length INTEGER     Maximum number of tokens to generate for each\n                               prompt.\n  -t, --temperature FLOAT      Possible temperature values for the backend\n                               language model.\n  -n, --num-samples INTEGER    Number of conversations for each configuration.\n  -o, --option <TEXT TEXT>...  Values for additional options denoted in your\n                               prompts by {OPTION_NAME}.\n  -f, --path PATH              Where to save the dataset. Either a file or a\n                               directory (folder).\n  -s, --single-file            Either save the whole dataset to a single file\n                               or create multiple files.\n  --help                       Show this message and exit.\n```\n\n- You can specify multiple variants for the following options: `--length`, `--temperature`, `--num-samples`, `--option`. A dataset item will be generated for each possible combination of the supplied values.\n- Each `--option` provided must be formatted as follows: `--option option_name \"Some option value\"`.\n- Currently supported backends: GPT-3 model variants by [OpenAI](https://openai.com/blog/openai-api), the language models by [Cohere](https://pypi.org/project/cohere/), BLOOM through the [Petals API](https://petals.ml/).\n\n```\ndatasetGPT conversations [OPTIONS]\n\n  Produce conversations between two gpt-3.5-turbo agents with given roles.\n\nOptions:\n  -k, --openai-api-key TEXT       OpenAI API key.\n  -a, --agent1 TEXT               Agent role description.  [required]\n  -b, --agent2 TEXT               Agent role description.  [required]\n  -i, --interruption [length|end_phrase]\n                                  Interruption mode.\n  -e, --end-phrase TEXT           Interrupt after this phrase is outputted by\n                                  one of the agents.\n  -d, --end-agent [agent1|agent2|both]\n                                  In which agent's messages to look for the\n                                  end phrase.\n  -l, --length INTEGER            Maximum number of utterances for each agent.\n                                  A conversation sample will be generated for\n                                  each length.\n  -t, --temperature FLOAT         Possible temperature values for the backend\n                                  language model.\n  -n, --num-samples INTEGER       Number of conversations for each\n                                  configuration.\n  -o, --option <TEXT TEXT>...     Values for additional options denoted in\n                                  your prompts by {OPTION_NAME}.\n  -f, --path PATH                 Where to save the dataset. Either a file or\n                                  a directory (folder).\n  -s, --single-file               Either save the whole dataset to a single\n                                  file or create multiple files.\n  --help                          Show this message and exit.\n```\n\n- The length parameter specifies how many utterances each agent should make. A length of 4 typically produces 8 utterances in total.\n- You can specify either `length` (default) or `end_phrase` as an interruption strategy. When using `end_phrase` a conversation will be interrupted once the `--end-phrase` has appeared in the messages of the `--end-agent` (could be both). In this case, the lengths provided will be treated as maximum conversation lengths.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/radi-cho/datasetGPT",
    "keywords": "dataset,llm,langchain,openai",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "datasetGPT",
    "package_url": "https://pypi.org/project/datasetGPT/",
    "platform": null,
    "project_url": "https://pypi.org/project/datasetGPT/",
    "project_urls": {
      "Homepage": "https://github.com/radi-cho/datasetGPT"
    },
    "release_url": "https://pypi.org/project/datasetGPT/0.0.2/",
    "requires_dist": [
      "langchain (>=0.0.113)",
      "click (>=8.1)"
    ],
    "requires_python": "",
    "summary": "Generate textual and conversational datasets with LLMs.",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17488444,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e6741b731c70a393b74887395d5007fa8e4c8a8301f311932d0f8ecd5285d95f",
          "md5": "07e248f96d713716a461c692be7033ec",
          "sha256": "48c8eda8acc0e8ac94a5285a8ba95fe331de3da1dfbf2a12e8f28edb8749055b"
        },
        "downloads": -1,
        "filename": "datasetGPT-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "07e248f96d713716a461c692be7033ec",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12091,
        "upload_time": "2023-03-18T21:01:57",
        "upload_time_iso_8601": "2023-03-18T21:01:57.227184Z",
        "url": "https://files.pythonhosted.org/packages/e6/74/1b731c70a393b74887395d5007fa8e4c8a8301f311932d0f8ecd5285d95f/datasetGPT-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "629a9a130f91949294cdcf1a48e884b7a68a1dbc7257092be8bfcb69c5feacdd",
          "md5": "48a973060e2fde115dc74957150059ce",
          "sha256": "1d0ebbb4db6c37946d5e1ab4fca2198d1da3d74d478a44a0a6c5e460971241e7"
        },
        "downloads": -1,
        "filename": "datasetGPT-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "48a973060e2fde115dc74957150059ce",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13215,
        "upload_time": "2023-03-18T21:01:58",
        "upload_time_iso_8601": "2023-03-18T21:01:58.767542Z",
        "url": "https://files.pythonhosted.org/packages/62/9a/9a130f91949294cdcf1a48e884b7a68a1dbc7257092be8bfcb69c5feacdd/datasetGPT-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9803d17c5fc68de7a236886cdd7624622c3b79dcd364ed66838d7962c447d4e8",
          "md5": "f815974351c47ddbdc4a78d36507d676",
          "sha256": "0fe2cf2718186ba6348f44115adf5483af1e60dd4ea8f8c368b159aac56a7820"
        },
        "downloads": -1,
        "filename": "datasetGPT-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f815974351c47ddbdc4a78d36507d676",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12378,
        "upload_time": "2023-03-29T04:46:46",
        "upload_time_iso_8601": "2023-03-29T04:46:46.379666Z",
        "url": "https://files.pythonhosted.org/packages/98/03/d17c5fc68de7a236886cdd7624622c3b79dcd364ed66838d7962c447d4e8/datasetGPT-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "72479918360ec20300d63cc9e1de2fc441a9e1fb5dc68a3623cdd07936b8983b",
          "md5": "050029760b2c303de90921aec5ec7bad",
          "sha256": "46d33f0d4de87503f0a650d6f21b504198382345d83ac0f72b5716b320ffea63"
        },
        "downloads": -1,
        "filename": "datasetGPT-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "050029760b2c303de90921aec5ec7bad",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13371,
        "upload_time": "2023-03-29T04:46:48",
        "upload_time_iso_8601": "2023-03-29T04:46:48.296969Z",
        "url": "https://files.pythonhosted.org/packages/72/47/9918360ec20300d63cc9e1de2fc441a9e1fb5dc68a3623cdd07936b8983b/datasetGPT-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9803d17c5fc68de7a236886cdd7624622c3b79dcd364ed66838d7962c447d4e8",
        "md5": "f815974351c47ddbdc4a78d36507d676",
        "sha256": "0fe2cf2718186ba6348f44115adf5483af1e60dd4ea8f8c368b159aac56a7820"
      },
      "downloads": -1,
      "filename": "datasetGPT-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f815974351c47ddbdc4a78d36507d676",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 12378,
      "upload_time": "2023-03-29T04:46:46",
      "upload_time_iso_8601": "2023-03-29T04:46:46.379666Z",
      "url": "https://files.pythonhosted.org/packages/98/03/d17c5fc68de7a236886cdd7624622c3b79dcd364ed66838d7962c447d4e8/datasetGPT-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "72479918360ec20300d63cc9e1de2fc441a9e1fb5dc68a3623cdd07936b8983b",
        "md5": "050029760b2c303de90921aec5ec7bad",
        "sha256": "46d33f0d4de87503f0a650d6f21b504198382345d83ac0f72b5716b320ffea63"
      },
      "downloads": -1,
      "filename": "datasetGPT-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "050029760b2c303de90921aec5ec7bad",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 13371,
      "upload_time": "2023-03-29T04:46:48",
      "upload_time_iso_8601": "2023-03-29T04:46:48.296969Z",
      "url": "https://files.pythonhosted.org/packages/72/47/9918360ec20300d63cc9e1de2fc441a9e1fb5dc68a3623cdd07936b8983b/datasetGPT-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}