{
  "info": {
    "author": "XuMing",
    "author_email": "xuming624@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Text Processing :: Linguistic"
    ],
    "description": "[![PyPI version](https://badge.fury.io/py/textgen.svg)](https://badge.fury.io/py/textgen)\n[![Downloads](https://pepy.tech/badge/textgen)](https://pepy.tech/project/textgen)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![GitHub contributors](https://img.shields.io/github/contributors/shibing624/textgen.svg)](https://github.com/shibing624/textgen/graphs/contributors)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)\n[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)\n\n# TextGen\n\n🌈 Implementation of Text Generation models.\n\n**textgen**实现了多种文本生成模型，包括：UDA、GPT2、Seq2Seq、BART、T5、SongNet等模型，开箱即用。\n\n**Guide**\n\n- [Question](#Question)\n- [Solution](#Solution)\n- [Feature](#Feature)\n- [Install](#install)\n- [Usage](#usage)\n- [Contact](#Contact)\n- [Reference](#reference)\n\n# Question\n\n文本生成，文本数据增强怎么做？\n\n# Solution\n## 文本生成模型\n\n1. Seq2Seq、ConvSeq2Seq、BART\n2. GPT2、SongNet\n3. T5、CopyT5\n\n## 文本扩增\n### 词粒度扩增\n1. UDA，非核心词替换\n2. EDA，简单数据增强技术：相似词、同义词替换，随机词插入、删除、替换\n\n### 句粒度扩增\n1. 回译（BT, Back Translate）：中文-英文-中文\n2. GPT2模型续写：短文本->长文本\n3. BART摘要模型：长文本->短文本\n4. TGLS：无监督相似文本生成模型\n\n\n# Feature\n\n- [UDA(非核心词替换)/EDA](textgen/augment/word_level_augment.py)：本项目参考Google的UDA(非核心词替换)算法和EDA算法，基于TF-IDF将句子中部分不重要词替换为同义词，随机词插入、删除、替换等方法，产生新的文本，实现了文本扩增\n- [BT(回译)](textgen/augment/sentence_level_augment.py)：本项目基于百度翻译API实现了回译功能，先把中文句子翻译为英文，再把英文翻译为新的中文\n- [Seq2Seq](textgen/seq2seq)：本项目基于PyTorch实现了Seq2Seq、ConvSeq2Seq、BART模型的训练和预测，可以用于文本翻译、对话生成、摘要生成等文本生成任务\n- [T5](textgen/t5)：本项目基于PyTorch实现了T5和CopyT5模型训练和预测，可以用于文本翻译、对话生成、对联生成、文案撰写等文本生成任务\n- [GPT2](textgen/language_modeling)：本项目基于PyTorch实现了GTP2模型训练和预测，可以用于文章生成、对联生成等文本生成任务\n- [SongNet](textgen/language_modeling/songnet_model.py)：本项目基于PyTorch实现了SongNet模型训练和预测，可以用于规范格式的诗词、歌词等文本生成任务\n- [TGLS](textgen/unsup_generation)：本项目实现了[TGLS](https://www.jiqizhixin.com/articles/2020-08-11-5)无监督相似文本生成模型，是一种“先搜索后学习”的文本生成方法，通过反复迭代学习候选集，最终模型能生成类似候选集的高质量相似文本\n\n\n# Demo\n\nHuggingFace Demo: https://huggingface.co/spaces/shibing624/chinese-couplet-generate\n\n![](docs/hf.png)\n\nrun example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:\n\n```shell\npython examples/gradio_demo.py\n```\n\nmodel trained by [examples/T5/T5_Finetune_Chinese_Couplet.ipynb](https://github.com/shibing624/textgen/blob/main/examples/T5/T5_Finetune_Chinese_Couplet.ipynb)\n\n# Install\n\n```shell\npip install torch # conda install pytorch\npip install -U textgen\n```\n\nor\n\n```shell\npip install torch # conda install pytorch\ngit clone https://github.com/shibing624/textgen.git\ncd textgen\npython3 setup.py install\n```\n\n# Usage\n\n## ConvSeq2Seq 模型\n训练并预测ConvSeq2Seq模型：\n\nexample: [examples/seq2sesq/training_convseq2seq_model_demo.py](examples/seq2seq/training_convseq2seq_model_demo.py)\n\n```python\nimport argparse\nfrom loguru import logger\nimport sys\n\nsys.path.append('../..')\nfrom textgen.seq2seq.conv_seq2seq_model import ConvSeq2SeqModel\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--train_file', default='../data/zh_dialog.tsv', type=str, help='Training data file')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_predict', action='store_true', help='Whether to run predict.')\n    parser.add_argument('--output_dir', default='./outputs/convseq2seq_zh/', type=str, help='Model output directory')\n    parser.add_argument('--max_seq_length', default=50, type=int, help='Max sequence length')\n    parser.add_argument('--num_epochs', default=200, type=int, help='Number of training epochs')\n    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')\n    args = parser.parse_args()\n    logger.info(args)\n\n    if args.do_train:\n        logger.info('Loading data...')\n        model = ConvSeq2SeqModel(epochs=args.num_epochs, batch_size=args.batch_size,\n                                 model_dir=args.output_dir, max_length=args.max_seq_length)\n        model.train_model(args.train_file)\n        print(model.eval_model(args.train_file))\n\n    if args.do_predict:\n        model = ConvSeq2SeqModel(epochs=args.num_epochs, batch_size=args.batch_size,\n                                 model_dir=args.output_dir, max_length=args.max_seq_length)\n        sentences = [\"什么是ai\", \"你是什么类型的计算机\", \"你知道热力学吗\"]\n        print(\"inputs:\", sentences)\n        print('outputs:', model.predict(sentences))\n\n\nif __name__ == '__main__':\n    main()\n```\n\noutput:\n\n```bash\ninputs: [\"什么是ai\", \"你是什么类型的计算机\", \"你知道热力学吗\"]\noutputs: ['人工智能是工程和科学的分支,致力于构建思维的机器。', '我的程序运行在python,所以我在任何运脑上工作！', '我不能错热是一个疯狂的人工智能\"200年。']\n```\n\n## BART 模型\n训练并预测BART模型：\n\nexample: [examples/seq2sesq/training_bartseq2seq_zh_demo.py](examples/seq2seq/training_bartseq2seq_zh_demo.py)\n\n\noutput:\n\n```shell\ninputs: ['什么是ai', '你是什么类型的计算机', '你知道热力学吗']\noutputs: ['人工智能是工程和科学的分支,致力于构', '我的程序运行在python,所以我在任何电脑上', '什么是热力学吗？']\n```\n\n\n## T5 模型\n\nexample: [examples/T5/training_zh_t5_model_demo.py](https://github.com/shibing624/textgen/blob/main/examples/T5/training_zh_t5_model_demo.py)\n\n```python\nimport argparse\nfrom loguru import logger\nimport pandas as pd\nimport sys\n\nsys.path.append('../..')\nfrom textgen.t5 import T5Model\n\n\ndef load_data(file_path):\n    data = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip('\\n')\n            terms = line.split('\\t')\n            if len(terms) == 2:\n                data.append(['QA', terms[0], terms[1]])\n            else:\n                logger.warning(f'line error: {line}')\n    return data\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--train_file', default='../data/zh_dialog.tsv', type=str, help='Training data file')\n    parser.add_argument('--model_type', default='t5', type=str, help='Transformers model type')\n    parser.add_argument('--model_name', default='Langboat/mengzi-t5-base', type=str, help='Transformers model or path')\n    parser.add_argument('--do_train', action='store_true', help='Whether to run training.')\n    parser.add_argument('--do_predict', action='store_true', help='Whether to run predict.')\n    parser.add_argument('--output_dir', default='./outputs/mengzi_t5_zh/', type=str, help='Model output directory')\n    parser.add_argument('--max_seq_length', default=50, type=int, help='Max sequence length')\n    parser.add_argument('--num_epochs', default=10, type=int, help='Number of training epochs')\n    parser.add_argument('--batch_size', default=32, type=int, help='Batch size')\n    args = parser.parse_args()\n    logger.info(args)\n\n    if args.do_train:\n        logger.info('Loading data...')\n        # train_data: Pandas DataFrame containing the 3 columns - `prefix`, `input_text`, `target_text`.\n        #   - `prefix`: A string indicating the task to perform. (E.g. `\"question\"`, `\"stsb\"`)\n        #   - `input_text`: The input text. `prefix` is prepended to form the full input. (<prefix>: <input_text>)\n        #   - `target_text`: The target sequence\n        train_data = load_data(args.train_file)\n        logger.debug('train_data: {}'.format(train_data[:10]))\n        train_df = pd.DataFrame(train_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n\n        eval_data = load_data(args.train_file)[:10]\n        eval_df = pd.DataFrame(eval_data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n\n        model_args = {\n            \"reprocess_input_data\": True,\n            \"overwrite_output_dir\": True,\n            \"max_seq_length\": args.max_seq_length,\n            \"train_batch_size\": args.batch_size,\n            \"num_train_epochs\": args.num_epochs,\n            \"save_eval_checkpoints\": False,\n            \"save_model_every_epoch\": False,\n            \"evaluate_generated_text\": True,\n            \"evaluate_during_training\": True,\n            \"evaluate_during_training_verbose\": True,\n            \"use_multiprocessing\": True,\n            \"save_best_model\": True,\n            \"output_dir\": args.output_dir,\n            \"use_early_stopping\": True,\n        }\n        # model_type: t5  model_name: Langboat/mengzi-t5-base\n        model = T5Model(args.model_type, args.model_name, args=model_args)\n\n        def count_matches(labels, preds):\n            logger.debug(f\"labels: {labels[:10]}\")\n            logger.debug(f\"preds: {preds[:10]}\")\n            match = sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n            logger.debug(f\"match: {match}\")\n            return match\n\n        model.train_model(train_df, eval_data=eval_df, matches=count_matches)\n        print(model.eval_model(eval_df, matches=count_matches))\n\n    if args.do_predict:\n        model = T5Model(args.model_type, args.output_dir)\n        sentences = [\"什么是ai\", \"你是什么类型的计算机\", \"你知道热力学吗\"]\n        print(\"inputs:\", sentences)\n        print(\"outputs:\", model.predict(sentences))\n\n\nif __name__ == '__main__':\n    main()\n```\n\noutput:\n```shell\ninputs: ['什么是ai', '你是什么类型的计算机', '你知道热力学吗']\noutputs: ['人工智能有两个广义的定义,任何拟人的机械,如在卡雷尔capeks', '我的程序运行在Python,所以我在任何电脑上工作!', '什么是热力学']\n```\n\n\n### T5 模型应用\n\nrelease基于T5的fine-tuned后的中文模型，模型全部release到HuggingFace models，`textgen`可自动下载，可直接使用。\n\n|Model|Arch|Intro|Training|Inference|\n|:-- |:--- |:--- |:--- |:--- |\n|[shibing624/prompt-t5-base-chinese](https://huggingface.co/shibing624/prompt-t5-base-chinese)|T5|中文NLP多任务Prompt模型|[prompt-t5-base-chinese.md](https://github.com/shibing624/textgen/blob/main/docs/prompt-t5-base-chinese.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_prompt_demo.py)|\n|[shibing624/t5-chinese-couplet](https://huggingface.co/shibing624/t5-chinese-couplet)|T5|fine-tuned中文对联后的模型|[对联生成模型调研](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/t5_couplet_demo.py)|\n\n\n## GPT2 模型\n\n### 中文GPT2 - 文章生成\n\n使用中文数据集（段落格式，`\\n`间隔），训练GPT2模型，可以用于诗歌生成、文章生成等任务。\n\nexample: [examples/language_generation/training_zh_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_gpt2_demo.py)\n\n### 中文GPT2 - 对联生成\n\n使用中文对联数据集（tsv格式，`\\t`间隔），自定义数据集读取Dataset，训练GPT2模型，可以用于对联生成、对话生成等任务。\n\nexample: [examples/language_generation/training_couplet_gpt2_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_couplet_gpt2_demo.py)\n\n#### GPT2 vs T5：\n1. 都是从Transformer改进来的，T5同时有编码器和解码器，GPT2只有解码器\n2. T5的模型优势是处理给定输入，产出对应输出的任务，如翻译、对话、问答等\n3. GPT2的模型优势是自由创作，如写一篇短文\n4. T5的对联生成效果好于GPT2、GPT2的诗词生成效果好于T5\n\n- [对联生成模型调研](https://github.com/shibing624/textgen/blob/main/docs/%E5%AF%B9%E8%81%94%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)\n- [古诗生成模型调研](https://github.com/shibing624/textgen/blob/main/docs/%E5%8F%A4%E8%AF%97%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94.md)\n\n## SongNet 模型\n\n格式控制的文本生成模型，paper见[SongNet: Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)，\n适用于强韵律格式要求的诗歌、对联、歌词生成等任务。\n\nexample: [examples/language_generation/training_zh_songnet_demo.py](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)\n\n### SongNet 模型应用\n\nrelease基于SongNet的中文模型，模型全部release到HuggingFace models，`textgen`可自动下载，可直接使用。\n\n|Model|Arch|Intro|Training|Inference|\n|:-- |:--- |:--- |:--- |:--- |\n|[shibing624/songnet-base-chinese](https://huggingface.co/shibing624/songnet-base-chinese)|SongNet|SongNet预训练模型|-|-|\n|[shibing624/songnet-base-chinese-songci](https://huggingface.co/shibing624/songnet-base-chinese-songci)|SongNet|fine-tuned宋词后的模型|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_songci_demo.py)|\n|[shibing624/songnet-base-chinese-couplet](https://huggingface.co/shibing624/songnet-base-chinese-couplet)|SongNet|fine-tuned对联后的模型|[training script](https://github.com/shibing624/textgen/blob/main/examples/language_generation/training_zh_songnet_demo.py)|[predict script](https://github.com/shibing624/textgen/blob/main/examples/songnet_couplet_demo.py)|\n\n\n## Keyword Text Augmentation(EDA/UDA)\n\nexample: [examples/text_augmentation_demo.py](examples/text_augmentation_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom textgen.augment import TextAugment\n\nif __name__ == '__main__':\n    docs = ['主要研究机器学习、深度学习、计算机视觉、智能对话系统相关内容',\n            '晚上肚子好难受',\n            '你会武功吗，我不会',\n            '组装标题质量受限于广告主自提物料的片段质量，且表达丰富度有限',\n            ]\n    m = TextAugment(sentence_list=docs)\n    a = docs[0]\n    print(a)\n\n    b = m.augment(a, aug_ops='random-0.2')\n    print('random-0.2:', b)\n\n    b = m.augment(a, aug_ops='insert-0.2')\n    print('insert-0.2:', b)\n\n    b = m.augment(a, aug_ops='delete-0.2')\n    print('delete-0.2:', b)\n\n    b = m.augment(a, aug_ops='tfidf-0.2')\n    print('tfidf-0.2:', b)\n\n    b = m.augment(a, aug_ops='mix-0.2')\n    print('mix-0.2:', b)\n```\n\noutput:\n\n```bash\n主要研究机器学习、深度学习、计算机视觉、智能对话系统相关内容\nrandom-0.2: ('主要陪陪机器学习、深度学习主要计算机视觉、智能对话系统受限于内容', [('研究', '陪陪', 2, 4), ('、', '主要', 13, 15), ('相关', '受限于', 27, 30)])\ninsert-0.2: ('主要研究机器机器学习学习、深度深度学习、计算机视觉、智能对话系统相关内容', [('机器', '机器机器', 4, 8), ('学习', '学习学习', 8, 12), ('深度', '深度深度', 13, 17)])\ndelete-0.2: ('主要研究机器学习、深度学习、计算机视觉、对话系统相关内容', [('智能', '', 20, 20)])\ntfidf-0.2: ('一是研究机器学习、深度学习、计算机听觉、智能交谈系统密切相关内容', [('主要', '一是', 0, 2), ('视觉', '听觉', 17, 19), ('对话', '交谈', 22, 24), ('相关', '密切相关', 26, 30)])\nmix-0.2: ('主要研究机器学习、深度学、计算机听觉、智能对话软件系统相关内容', [('学习', '学', 11, 12), ('视觉', '听觉', 16, 18), ('系统', '软件系统', 23, 27)])\n```\n\n## TGLS 模型（无监督相似文本生成模型）\n\n无监督的中文电商评论生成：从**电商评论**中提取用户表达观点的短句并进行组合来生成仿真评论。\n\nexample: [examples/unsup_generation_demo.py](examples/unsup_generation_demo.py)\n\n```python\nimport os\nimport sys\n\nsys.path.append('..')\nfrom textgen.unsup_generation import TglsModel, load_list\n\npwd_path = os.path.abspath(os.path.dirname(__file__))\n\nsamples = load_list(os.path.join(pwd_path, './data/ecommerce_comments.txt'))\ndocs_text = [\n    [\"挺好的，速度很快，也很实惠，不知效果如何\",\n     \"产品没得说，买了以后就降价，心情不美丽。\",\n     \"刚收到，包装很完整，不错\",\n     \"发货速度很快，物流也不错，同一时间买的两个东东，一个先到一个还在路上。这个水水很喜欢，不过盖子真的开了。盖不牢了现在。\",\n     \"包装的很好，是正品\",\n     \"被种草兰蔻粉水三百元一大瓶囤货，希望是正品好用，收到的时候用保鲜膜包裹得严严实实，只敢买考拉自营的护肤品\",\n     ],\n    ['很温和，清洗的也很干净，不油腻，很不错，会考虑回购，第一次考拉买护肤品，满意',\n     '这款卸妆油我会无限回购的。即使我是油痘皮，也不会闷痘，同时在脸部按摩时，还能解决白头的脂肪粒的问题。用清水洗完脸后，非常的清爽。',\n     '自从用了fancl之后就不用其他卸妆了，卸的舒服又干净',\n     '买贵了，大润发才卖79。9。',\n     ],\n    samples\n]\nm = TglsModel(docs_text)\nr = m.generate(samples[:500])\nprint('size:', len(r))\nfor review in r:\n    print('\\t' + review)\n```\n\noutput:\n\n[美迪惠尔 N.M.F针剂水库保湿面膜](https://goods.kaola.com/product/2227311.html)有如下的20句评论，其中有10句是真实用户评论，10句是生成的评论，能看出来么?😂\n\n```\n还不错还不错还不错还不错。\n东西到了，不知道好不好用。试用过后再来评价。到时看网评都还可以。\n哺乳期唯一使用的护肤品，每天都是素颜，脸面全靠面膜吊着😄补水💦不粘腻一如既往的支持，喜欢💕\n搞活动时买的面膜，不知道这个面膜是真是假敷在脸上面膜纸都有小水泡鼓起来。\n很不错，非常补水，用过的都知道，性价比之王，好用又不贵，正品，用着放心，物流也很快。\n面膜非常好用哦。面膜薄薄的。好像是蚕丝面膜啊。精华很多呢。敷在脸上很舒服。感觉挺保湿的，味道也挺好闻的。就是里面只有单纯的面膜直接敷脸上有点不好弄，哈哈哈\n还可以保湿效果不错水润润的每天贴一片脸也不干了用完了在买点，不错还会继续回购的。\n快递很快，东西很赞！想要得点考拉豆不容易，还要三十个字。时间宝贵，废话不说！用过了就知道了\n挺好用的，朋友推荐来的\n挺好用的，淡淡的，虽然不是很浓精华的感觉，但是效果也蛮好的。划算\n不得不说美迪惠尔的面膜是我用过的最好的面膜之一😎补水效果非常好，没想到这么便宜的价格竟真的能买到真品。\n保湿效果挺好的，面膜很好用。\n期待好的产品。\n一打开包装里面的精华刚刚好，用了补水补水效果不错，物流非常快。\n皮肤很光滑😇比上去速度快三天就到了。\n前两天皮肤干燥连续敷了两个晚上感觉还不错😂补水效果明显！可想而知精华液又多充足😍敷上以后凉凉的很舒服。\n补水效果一般吧～但是我用的韩国背回来的面膜纸不算薄，希望好用会回购的，敷上脸感觉比较清爽～价格还不便宜。\n希望好用，面膜用过了很好用，皮肤水嫩光滑白皙，补水不错，价格也合适。\n就是精华液太少了，保湿效果不错。\n面膜的补水效果非常好，保湿效果确实很赞，这个面膜相对于胶原蛋白和美白的那两款的面膜纸要厚一些，看着价格合适。\n```\n\n前10句是真实用户评论，后10句是生成的。\n\n# Contact\n\n- Issue(建议)\n  ：[![GitHub issues](https://img.shields.io/github/issues/shibing624/textgen.svg)](https://github.com/shibing624/textgen/issues)\n- 邮件我：xuming: xuming624@qq.com\n- 微信我： 加我*微信号：xuming624, 备注：姓名-公司名-NLP* 进NLP交流群。\n\n<img src=\"docs/wechat.jpeg\" width=\"200\" />\n\n# License\n\n授权协议为 [The Apache License 2.0](/LICENSE)，可免费用做商业用途。请在产品说明中附加textgen的链接和授权协议。\n\n# Contribute\n\n项目代码还很粗糙，如果大家对代码有所改进，欢迎提交回本项目，在提交之前，注意以下两点：\n\n- 在`tests`添加相应的单元测试\n- 使用`python -m pytest`来运行所有单元测试，确保所有单测都是通过的\n\n之后即可提交PR。\n\n## Reference\n\n- [PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)\n- [minimaxir/textgenrnn](https://github.com/minimaxir/textgenrnn)\n- [minimaxir/gpt-2-simple](https://github.com/minimaxir/gpt-2-simple)\n- [asyml/texar](https://github.com/asyml/texar)\n- [yangjianxin1/GPT2-chitchat](https://github.com/yangjianxin1/GPT2-chitchat)\n- [williamSYSU/TextGAN-PyTorch](https://github.com/williamSYSU/TextGAN-PyTorch)\n- [RUCAIBox/TextBox](https://github.com/RUCAIBox/TextBox)\n- [Tiiiger/bert_score]()\n- [1YCxZ/Fake-review-generation](https://github.com/1YCxZ/Fake-review-generation)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/shibing624/textgen",
    "keywords": "textgen,text-generation,Text Generation Tool,ernie-gen,chinese text generation",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "textgen",
    "package_url": "https://pypi.org/project/textgen/",
    "platform": null,
    "project_url": "https://pypi.org/project/textgen/",
    "project_urls": {
      "Homepage": "https://github.com/shibing624/textgen"
    },
    "release_url": "https://pypi.org/project/textgen/0.1.7/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Text Generation Model",
    "version": "0.1.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15909672,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "671c2e45c980177695321d03ae725ffd3a8d01ab0142debc848c02e828c4b67a",
          "md5": "4018a1e0a31a9735a32a2bc9b37cf43b",
          "sha256": "64d524c28b44baf9abdc388e56d3e041a8a281d6a682e930bb48ca9dec912982"
        },
        "downloads": -1,
        "filename": "textgen-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4018a1e0a31a9735a32a2bc9b37cf43b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7646,
        "upload_time": "2021-04-07T14:21:20",
        "upload_time_iso_8601": "2021-04-07T14:21:20.585094Z",
        "url": "https://files.pythonhosted.org/packages/67/1c/2e45c980177695321d03ae725ffd3a8d01ab0142debc848c02e828c4b67a/textgen-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8eec49732198778280cc49956d28b767d57a3be645ada3557a81c49731b44ee6",
          "md5": "b38dbebb15a02ea52a1cd502ccc19369",
          "sha256": "d50838f9481041d439ff6620700450482d3214e738ba6cac18d509c7fb8b31a1"
        },
        "downloads": -1,
        "filename": "textgen-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "b38dbebb15a02ea52a1cd502ccc19369",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 85917,
        "upload_time": "2021-06-23T10:06:27",
        "upload_time_iso_8601": "2021-06-23T10:06:27.390827Z",
        "url": "https://files.pythonhosted.org/packages/8e/ec/49732198778280cc49956d28b767d57a3be645ada3557a81c49731b44ee6/textgen-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ea0cbafc785f110968448cbc08e91c02222ccd5fdd2e42649e7fa51aa1de14e1",
          "md5": "2c38e26e002c7bd03cc7c15b84070956",
          "sha256": "169b5d6ab47629c16eafc8bd776b44def7615ba50334f546bf58f1f88c364df8"
        },
        "downloads": -1,
        "filename": "textgen-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "2c38e26e002c7bd03cc7c15b84070956",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 85936,
        "upload_time": "2021-06-23T12:52:43",
        "upload_time_iso_8601": "2021-06-23T12:52:43.733067Z",
        "url": "https://files.pythonhosted.org/packages/ea/0c/bafc785f110968448cbc08e91c02222ccd5fdd2e42649e7fa51aa1de14e1/textgen-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "38bc1a1828b7f7c2f4390aeaeae7323de77987f182c495bd84317186e4463a8b",
          "md5": "df52ed52d82413dc21665b3755e2f3cf",
          "sha256": "dee14e084ebf702bcc9e5ac01078fc707183523e723a4671d38b38beb7cbb362"
        },
        "downloads": -1,
        "filename": "textgen-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "df52ed52d82413dc21665b3755e2f3cf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 105629,
        "upload_time": "2022-05-10T13:07:04",
        "upload_time_iso_8601": "2022-05-10T13:07:04.216084Z",
        "url": "https://files.pythonhosted.org/packages/38/bc/1a1828b7f7c2f4390aeaeae7323de77987f182c495bd84317186e4463a8b/textgen-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6413eabacbb3b750a00871def97118a38fb6baaa808d1529a3f8f90e61e28f7e",
          "md5": "8b4b6b60bce393d4833cada231c99ec7",
          "sha256": "941d3bdc3191f29226a9bad7ad708d0f5cff19918247ecbb2b98da87a4889f9f"
        },
        "downloads": -1,
        "filename": "textgen-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "8b4b6b60bce393d4833cada231c99ec7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 129371,
        "upload_time": "2022-06-20T06:43:47",
        "upload_time_iso_8601": "2022-06-20T06:43:47.495324Z",
        "url": "https://files.pythonhosted.org/packages/64/13/eabacbb3b750a00871def97118a38fb6baaa808d1529a3f8f90e61e28f7e/textgen-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2692a6074c5b931cc1a47387e910f04647d3d5e14b216d2424100cdeb158bc84",
          "md5": "0a668bed239f1ece3e98664426490802",
          "sha256": "39cf7943d04cf26a2fc3e7d9bc33fed1e8fad3f534449de7c6a79575828def44"
        },
        "downloads": -1,
        "filename": "textgen-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0a668bed239f1ece3e98664426490802",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 130260,
        "upload_time": "2022-06-30T08:22:27",
        "upload_time_iso_8601": "2022-06-30T08:22:27.208021Z",
        "url": "https://files.pythonhosted.org/packages/26/92/a6074c5b931cc1a47387e910f04647d3d5e14b216d2424100cdeb158bc84/textgen-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "28e96789a758f0ecea459d625bb5984169ff55a40db2b72baca6092a7041284f",
          "md5": "67622d4e2568177272d4c4d96e7c05c9",
          "sha256": "e86327a0c7028d887bb1e6ac9b6bfbd19a0f03f5990b29fb49ac967f5a2b9ee1"
        },
        "downloads": -1,
        "filename": "textgen-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "67622d4e2568177272d4c4d96e7c05c9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 130087,
        "upload_time": "2022-07-04T08:04:21",
        "upload_time_iso_8601": "2022-07-04T08:04:21.497394Z",
        "url": "https://files.pythonhosted.org/packages/28/e9/6789a758f0ecea459d625bb5984169ff55a40db2b72baca6092a7041284f/textgen-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb2f0dc39f213aa8c95b343237b131d82645dbe45c822d0747e6a02ea4f7350c",
          "md5": "08a1d26afa835cba142de69bd84c43b3",
          "sha256": "41fa58a5f8296a3c189930f3b5ab23850cfacfef63d94c8300a46f3a71edddf2"
        },
        "downloads": -1,
        "filename": "textgen-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "08a1d26afa835cba142de69bd84c43b3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 270884,
        "upload_time": "2022-08-05T03:06:08",
        "upload_time_iso_8601": "2022-08-05T03:06:08.296710Z",
        "url": "https://files.pythonhosted.org/packages/bb/2f/0dc39f213aa8c95b343237b131d82645dbe45c822d0747e6a02ea4f7350c/textgen-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cf0cde7aa88c495e1bf21a0c6ef726973e898d71e5b0743babada271d11246f5",
          "md5": "5546cdb2af98706638695b70918c5da2",
          "sha256": "399915dbbdf03ed87d22562c5cf8221295c6bf6f05761f7d40932e8f4c2f81b1"
        },
        "downloads": -1,
        "filename": "textgen-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "5546cdb2af98706638695b70918c5da2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 267986,
        "upload_time": "2022-08-23T08:43:49",
        "upload_time_iso_8601": "2022-08-23T08:43:49.526357Z",
        "url": "https://files.pythonhosted.org/packages/cf/0c/de7aa88c495e1bf21a0c6ef726973e898d71e5b0743babada271d11246f5/textgen-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "893290f533db1889a8ec1832c885bb6015d09be0c50106e47b54e87bb01ca5e6",
          "md5": "650be0c31b53a6bbe02ae1fee5881aa4",
          "sha256": "5aab389a433482c8cf76d56af131164d9a83ab8001e917b8572f84d6a260d4ed"
        },
        "downloads": -1,
        "filename": "textgen-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "650be0c31b53a6bbe02ae1fee5881aa4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 282030,
        "upload_time": "2022-09-11T08:26:36",
        "upload_time_iso_8601": "2022-09-11T08:26:36.720313Z",
        "url": "https://files.pythonhosted.org/packages/89/32/90f533db1889a8ec1832c885bb6015d09be0c50106e47b54e87bb01ca5e6/textgen-0.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fe104e5de391f040e65451797b022df709e1b543af7c81d724f44c263c2996db",
          "md5": "d56d2c86aa7d8aa5624af7866a7263ce",
          "sha256": "5350d2b00392675a7d0bf48f588cf91f87916826ac84a94218307a402faf22f8"
        },
        "downloads": -1,
        "filename": "textgen-0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "d56d2c86aa7d8aa5624af7866a7263ce",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 174674,
        "upload_time": "2022-11-21T11:21:09",
        "upload_time_iso_8601": "2022-11-21T11:21:09.041799Z",
        "url": "https://files.pythonhosted.org/packages/fe/10/4e5de391f040e65451797b022df709e1b543af7c81d724f44c263c2996db/textgen-0.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f0425c22e9c4cb094f40c1a8316f4cf9d708291b12c19dd4156c0a1b0fc6ea62",
          "md5": "57ed4d60062027880acea228595bb976",
          "sha256": "cc4d855922bf445cedd52e4b2af92028fc8fdecbdebe132f85f3eb9ef7cfbcae"
        },
        "downloads": -1,
        "filename": "textgen-0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "57ed4d60062027880acea228595bb976",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 175713,
        "upload_time": "2022-11-28T04:01:02",
        "upload_time_iso_8601": "2022-11-28T04:01:02.630142Z",
        "url": "https://files.pythonhosted.org/packages/f0/42/5c22e9c4cb094f40c1a8316f4cf9d708291b12c19dd4156c0a1b0fc6ea62/textgen-0.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f0425c22e9c4cb094f40c1a8316f4cf9d708291b12c19dd4156c0a1b0fc6ea62",
        "md5": "57ed4d60062027880acea228595bb976",
        "sha256": "cc4d855922bf445cedd52e4b2af92028fc8fdecbdebe132f85f3eb9ef7cfbcae"
      },
      "downloads": -1,
      "filename": "textgen-0.1.7.tar.gz",
      "has_sig": false,
      "md5_digest": "57ed4d60062027880acea228595bb976",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 175713,
      "upload_time": "2022-11-28T04:01:02",
      "upload_time_iso_8601": "2022-11-28T04:01:02.630142Z",
      "url": "https://files.pythonhosted.org/packages/f0/42/5c22e9c4cb094f40c1a8316f4cf9d708291b12c19dd4156c0a1b0fc6ea62/textgen-0.1.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}