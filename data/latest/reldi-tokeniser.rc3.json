{
  "info": {
    "author": "CLARIN.SI",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# reldi-tokeniser\n\nA tokeniser developed inside the [ReLDI project](https://reldi.spur.uzh.ch). Supports currently five languages -- Slovene, Croatian, Serbian, Macedonian and Bulgarian, and two modes -- standard and non-standard text.\n\n## Usage\n\n### Command line\n```\n$ echo 'kaj sad s tim.daj se nasmij ^_^.' | ./tokeniser.py hr -n\n1.1.1.1-3\tkaj\n1.1.2.5-7\tsad\n1.1.3.9-9\ts\n1.1.4.11-13\ttim\n1.1.5.14-14\t.\n\n1.2.1.15-17\tdaj\n1.2.2.19-20\tse\n1.2.3.22-27\tnasmij\n1.2.4.29-31\t^_^\n1.2.5.32-32\t.\n\n\n```\n\nLanguage is a positional argument while tokenisation of non-standard text, tagging and lemmatization of symbols and punctuation, and diferent output formats are an optional one.\n\n```\n$ python tokeniser.py -h\nusage: tokeniser.py [-h] [-c] [-b] [-d] [-n] [-t] {sl,hr,sr,mk,bg}\n\nTokeniser for (non-)standard Slovene, Croatian, Serbian, Macedonian and\nBulgarian\n\npositional arguments:\n  {sl,hr,sr,mk,bg}   language of the text\n\noptional arguments:\n  -h, --help         show this help message and exit\n  -c, --conllu       generates CONLLU output\n  -b, --bert         generates BERT-compatible output\n  -d, --document     passes through ConLL-U-style document boundaries\n  -n, --nonstandard  invokes the non-standard mode\n  -t, --tag          adds tags and lemmas to punctuations and symbols\n```\n\n### Python module\n```python\n# string mode\nimport reldi_tokeniser\n\ntext = 'kaj sad s tim.daj se nasmij ^_^.'\n\noutput = reldi_tokeniser.run(text, 'hr', nonstandard=True, tag=True)\n\n# object mode\nfrom reldi_tokeniser.tokeniser import ReldiTokeniser\n\nreldi = ReldiTokeniser('hr', conllu=True, nonstandard=True, tag=True)\nlist_of_lines = [el + '\\n' for el in text.split('\\n')]\ntest = reldi.run(list_of_lines, mode='object')\n```\n\nPython module has two mandatory parameters - text and language. Other optional parameters are `conllu`, `bert`, `document`, `nonstandard` and `tag`.\n\n## CoNLL-U output\n\nThis tokeniser outputs also CoNLL-U format (flag `-c`/`--conllu`). If the additional ```-d```/```--document``` flag is given, the tokeniser passes through lines starting with ```# newdoc id =``` to preserve document structure.\n\n```\n$ echo '# newdoc id = prvi\nkaj sad s tim.daj se nasmij ^_^.\nhaha\n# newdoc id = gidru\nštaš' | ./tokeniser.py hr -n -c -d\n# newdoc id = prvi\n# newpar id = 1\n# sent_id = 1.1\n# text = kaj sad s tim.\n1\tkaj\t_\t_\t_\t_\t_\t_\t_\t_\n2\tsad\t_\t_\t_\t_\t_\t_\t_\t_\n3\ts\t_\t_\t_\t_\t_\t_\t_\t_\n4\ttim\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n5\t.\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n\n# sent_id = 1.2\n# text = daj se nasmij ^_^.\n1\tdaj\t_\t_\t_\t_\t_\t_\t_\t_\n2\tse\t_\t_\t_\t_\t_\t_\t_\t_\n3\tnasmij\t_\t_\t_\t_\t_\t_\t_\t_\n4\t^_^\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n5\t.\t_\t_\t_\t_\t_\t_\t_\t_\n\n# newpar id = 2\n# sent_id = 2.1\n# text = haha\n1\thaha\t_\t_\t_\t_\t_\t_\t_\t_\n\n# newdoc id = gidru\n# newpar id = 1\n# sent_id = 1.1\n# text = štaš\n1\tštaš\t_\t_\t_\t_\t_\t_\t_\t_\n\n```\n## Pre-tagging\n\nThe tokeniser can also pre-annotate text on the part-of-speech (UPOS and XPOS) and lemma level (flag `-t` or `--tag`), if the available tokenisation regexes have sufficient evidence (punctuations, mentions, hashtags, URL-s, e-mails, emoticons, emojis). Default output format in case of pre-tagging is CoNLL-U.\n\n```\n$ echo -e \"kaj sad s tim.daj se nasmij ^_^. haha\" | python tokeniser.py hr -n -t\n# newpar id = 1\n# sent_id = 1.1\n# text = kaj sad s tim.\n1\tkaj\t_\t_\t_\t_\t_\t_\t_\t_\n2\tsad\t_\t_\t_\t_\t_\t_\t_\t_\n3\ts\t_\t_\t_\t_\t_\t_\t_\t_\n4\ttim\t_\t_\t_\t_\t_\t_\t_\tSpaceAfter=No\n5\t.\t.\tPUNCT\tZ\t_\t_\t_\t_\tSpaceAfter=No\n\n# sent_id = 1.2\n# text = daj se nasmij ^_^.\n1\tdaj\t_\t_\t_\t_\t_\t_\t_\t_\n2\tse\t_\t_\t_\t_\t_\t_\t_\t_\n3\tnasmij\t_\t_\t_\t_\t_\t_\t_\t_\n4\t^_^\t^_^\tSYM\tXe\t_\t_\t_\t_\tSpaceAfter=No\n5\t.\t.\tPUNCT\tZ\t_\t_\t_\t_\t_\n\n# sent_id = 1.3\n# text = haha\n1\thaha\t_\t_\t_\t_\t_\t_\t_\t_\n\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://www.github.com/clarinsi/reldi-tokeniser",
    "keywords": "",
    "license": "apache-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "reldi-tokeniser",
    "package_url": "https://pypi.org/project/reldi-tokeniser/",
    "platform": null,
    "project_url": "https://pypi.org/project/reldi-tokeniser/",
    "project_urls": {
      "Homepage": "https://www.github.com/clarinsi/reldi-tokeniser"
    },
    "release_url": "https://pypi.org/project/reldi-tokeniser/1.0.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Sentence splitting and tokenization for South Slavic languages",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14272472,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f4f2570ff15a2de0b9cef588632c749053b68a4527f4be6eaaeb62b4fecfb55c",
          "md5": "c7aad03a19872629e48d20e2230399c5",
          "sha256": "7226c664e9424792a5f33aaea2e3f8afef10d02ffe630484b26af3b88cf65c45"
        },
        "downloads": -1,
        "filename": "reldi_tokeniser-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c7aad03a19872629e48d20e2230399c5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16593,
        "upload_time": "2021-12-29T08:10:03",
        "upload_time_iso_8601": "2021-12-29T08:10:03.828001Z",
        "url": "https://files.pythonhosted.org/packages/f4/f2/570ff15a2de0b9cef588632c749053b68a4527f4be6eaaeb62b4fecfb55c/reldi_tokeniser-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9faf7a99925b8bbecbabfc7af78b4d9d05144def137e88ed02041af05358eafb",
          "md5": "720cb0c56703cf332a1dca961b788142",
          "sha256": "ff9ab77498861116c5dc344be346b4a569399980b6bfef9a72344fc5314d3eeb"
        },
        "downloads": -1,
        "filename": "reldi-tokeniser-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "720cb0c56703cf332a1dca961b788142",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16546,
        "upload_time": "2021-12-29T08:10:05",
        "upload_time_iso_8601": "2021-12-29T08:10:05.332892Z",
        "url": "https://files.pythonhosted.org/packages/9f/af/7a99925b8bbecbabfc7af78b4d9d05144def137e88ed02041af05358eafb/reldi-tokeniser-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "03dcdacf1ea549fea166127c236602cd59f572edc06ff7c1a82c56f5f77d33d3",
          "md5": "ce62d8c2b796ff508361b238def16d78",
          "sha256": "dd315fdd7fd5ffa4dc850bf8753504d02bca97ac98305c326f260ed0a77b7963"
        },
        "downloads": -1,
        "filename": "reldi_tokeniser-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ce62d8c2b796ff508361b238def16d78",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16602,
        "upload_time": "2022-01-25T13:06:12",
        "upload_time_iso_8601": "2022-01-25T13:06:12.390472Z",
        "url": "https://files.pythonhosted.org/packages/03/dc/dacf1ea549fea166127c236602cd59f572edc06ff7c1a82c56f5f77d33d3/reldi_tokeniser-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b94883a140282320458d6b021f570131ddc61ddebb5a751c586963ac89a43ab2",
          "md5": "419c522fbe35831b792c0572d2278d4b",
          "sha256": "df58a4b9628d3aacae4bc53101e9be9b20738a71f4c15eed5fb076ed1fd61c40"
        },
        "downloads": -1,
        "filename": "reldi-tokeniser-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "419c522fbe35831b792c0572d2278d4b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16562,
        "upload_time": "2022-01-25T13:06:13",
        "upload_time_iso_8601": "2022-01-25T13:06:13.659390Z",
        "url": "https://files.pythonhosted.org/packages/b9/48/83a140282320458d6b021f570131ddc61ddebb5a751c586963ac89a43ab2/reldi-tokeniser-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ef421c0c8298f9f950d66aa5bde4ab96f360b2c1e09b2c631476e4037b8960d9",
          "md5": "59812f3c95090c71da0ee159783a4d1e",
          "sha256": "9e879440c9164b12ca9b7a41623ae50f1b48eb922319df2b64595c5fe7fb99dc"
        },
        "downloads": -1,
        "filename": "reldi_tokeniser-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "59812f3c95090c71da0ee159783a4d1e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16612,
        "upload_time": "2022-06-28T12:30:26",
        "upload_time_iso_8601": "2022-06-28T12:30:26.936289Z",
        "url": "https://files.pythonhosted.org/packages/ef/42/1c0c8298f9f950d66aa5bde4ab96f360b2c1e09b2c631476e4037b8960d9/reldi_tokeniser-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9d3f56069f9eb5294e545f31cda44c90ae8a8b8dad86d56a032f52b4b09311b6",
          "md5": "1118b7ba14d3f40fb52fb6378f4195a5",
          "sha256": "d86016c5f3a9d39372e578fa657516ff898851895101623c45940f23c9f5013a"
        },
        "downloads": -1,
        "filename": "reldi-tokeniser-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1118b7ba14d3f40fb52fb6378f4195a5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16565,
        "upload_time": "2022-06-28T12:30:28",
        "upload_time_iso_8601": "2022-06-28T12:30:28.537778Z",
        "url": "https://files.pythonhosted.org/packages/9d/3f/56069f9eb5294e545f31cda44c90ae8a8b8dad86d56a032f52b4b09311b6/reldi-tokeniser-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ef421c0c8298f9f950d66aa5bde4ab96f360b2c1e09b2c631476e4037b8960d9",
        "md5": "59812f3c95090c71da0ee159783a4d1e",
        "sha256": "9e879440c9164b12ca9b7a41623ae50f1b48eb922319df2b64595c5fe7fb99dc"
      },
      "downloads": -1,
      "filename": "reldi_tokeniser-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "59812f3c95090c71da0ee159783a4d1e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 16612,
      "upload_time": "2022-06-28T12:30:26",
      "upload_time_iso_8601": "2022-06-28T12:30:26.936289Z",
      "url": "https://files.pythonhosted.org/packages/ef/42/1c0c8298f9f950d66aa5bde4ab96f360b2c1e09b2c631476e4037b8960d9/reldi_tokeniser-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9d3f56069f9eb5294e545f31cda44c90ae8a8b8dad86d56a032f52b4b09311b6",
        "md5": "1118b7ba14d3f40fb52fb6378f4195a5",
        "sha256": "d86016c5f3a9d39372e578fa657516ff898851895101623c45940f23c9f5013a"
      },
      "downloads": -1,
      "filename": "reldi-tokeniser-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "1118b7ba14d3f40fb52fb6378f4195a5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 16565,
      "upload_time": "2022-06-28T12:30:28",
      "upload_time_iso_8601": "2022-06-28T12:30:28.537778Z",
      "url": "https://files.pythonhosted.org/packages/9d/3f/56069f9eb5294e545f31cda44c90ae8a8b8dad86d56a032f52b4b09311b6/reldi-tokeniser-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}