{
  "info": {
    "author": "Maximilian Gartz",
    "author_email": "maximilian.gartz@outlook.de",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Get Started\n\nInstall the package https://pypi.org/project/bayes-vi/ using pip:\n\n```bash\npip install bayes-vi\n```\n\nCheck out the documentation at https://maxgrtz.github.io/bayesian-inference/ !\n\n&nbsp;\n\n# Imports\n\n\n```python\nimport collections\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport arviz as az\n\n# use ggplot styles for graphs\nplt.style.use('ggplot')\n```\n\n\n```python\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\ntfd = tfp.distributions\ntfb = tfp.bijectors\ntfk = tf.keras\n\n# set tf logger to log level ERROR to avoid warnings\ntf.get_logger().setLevel('ERROR')\n```\n\n\n```python\n# import probabilistic models\nfrom bayes_vi.model import Model\n\n# import utils\nfrom bayes_vi.utils import to_ordered_dict\nfrom bayes_vi.utils.datasets import make_dataset_from_df\nfrom bayes_vi.utils.symplectic_integrators import LeapfrogIntegrator\n```\n\n\n```python\n# mcmc imports\nfrom bayes_vi.inference.mcmc import MCMC\nfrom bayes_vi.inference.mcmc.transition_kernels import HamiltonianMonteCarlo, NoUTurnSampler, RandomWalkMetropolis\nfrom bayes_vi.inference.mcmc.stepsize_adaptation_kernels import SimpleStepSizeAdaptation, DualAveragingStepSizeAdaptation\n```\n\n\n```python\n# vi imports \nfrom bayes_vi.inference.vi import VI\nfrom bayes_vi.inference.vi.surrogate_posteriors import ADVI, NormalizingFlow\nfrom bayes_vi.inference.vi.flow_bijectors import HamiltonianFlow, AffineFlow, make_energy_fn, make_scale_fn, make_shift_fn\n```\n\n&nbsp;\n\n# 1. Example Workflow\n\nA Bayesian modeling worklow using this package could look like this.\n\n&nbsp;\n\n## 1.1. Create a Dataset\nCreate a `tf.data.Dataset` from your data. For this example consider some univariate Gaussian fake data.\n\n\n```python\nnum_datapoints = 20\nloc = 7\nscale = 3\ntrue_dist = tfd.Normal(loc=loc, scale=scale)\ny = true_dist.sample(num_datapoints)\n```\n\nThe package provides a utility function `make_dataset_from_df` for easy construction of a dataset from a `pd.DataFrame`.\n\n\n```python\ndata = pd.DataFrame(dict(y=y))\ndataset = make_dataset_from_df(data, target_names=['y'], format_features_as='dict')\n```\n\n\n```python\nax = data.plot(kind='kde', title='Univariate Gaussian Test Data')\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_15_0.png)\n\n\n&nbsp;\n\n## 1.2. Define a Bayesian Model\n\nTo define a Bayesian `Model`, simply provide an `collections.OrderedDict` of prior distributions as `tfp.distribution.Distirbution`s and a likelihood function, which takes the parameters as inputs by name.\n\n\n```python\npriors = collections.OrderedDict(\n    loc = tfd.Normal(loc=0., scale=10.),\n    scale = tfd.HalfNormal(scale=10.)\n)\n\ndef likelihood(loc, scale): \n    return tfd.Normal(loc=loc, scale=scale)\n```\n\n\n```python\nmodel = Model(priors=priors, likelihood=likelihood)\n```\n\nIt is possible to provide a list of constraining bijectors on construction of the model, which allows for computations on an unconstrained space.\n\nIf no such list is defined, default bijectors for each parameter are chosen.\n\n&nbsp;\n\n## 1.3. Run Inference Algorithm of Choice\n\nWith this simple setup, you can now use either MCMC or variational inference to do Bayesian inference.\n\n&nbsp;\n\n### 1.3.1. Run MCMC (e.g. with No-U-Turn Sampler with dual averaging stepsize adaptation)\n\nNote that the transition kernel may have various additional parameters. Here we use just the defaults and only provide the stepsize adaptation kernel.\n\n\n```python\nNUM_CHAINS = 2\nNUM_SAMPLES = 4000\nNUM_BURNIN_STEPS = 1000\n\nstepsize_adaptation_kernel = DualAveragingStepSizeAdaptation(num_adaptation_steps=int(NUM_BURNIN_STEPS*0.8))\n\nkernel = NoUTurnSampler(stepsize_adaptation_kernel=stepsize_adaptation_kernel)\n\nmcmc = MCMC(model=model, dataset=dataset, transition_kernel=kernel)\n\nmcmc_result = mcmc.fit(\n    num_chains=NUM_CHAINS, \n    num_samples=NUM_SAMPLES, \n    num_burnin_steps=NUM_BURNIN_STEPS,\n    progress_bar=True,\n)\n```\n\n<div>\n  99.08% [5000/5000 00:15<00:00]\n</div>\n\n&nbsp;\n\n\nThe `mcmc_result` is a wrapper containing the sample results `mcmc_result.samples` and additional traced metrics specific to the kernel `mcmc_result.trace`.\n\nTo visualize the results, one can use existing libraries like arviz (https://arviz-devs.github.io/arviz/index.html).\n\nLater versions of this package should allow for a seamless integration with arviz, but for now we have to reformat the sample results to conform to arviz conventions.\n\n\n```python\n# adapt sample results to arviz required format --- later versions should allow for seamless integration with arviz\nposterior_samples = to_ordered_dict(model.param_names, tf.nest.map_structure(lambda v: v.numpy().T, mcmc_result.samples))\n```\n\n\n```python\naz.summary(posterior_samples)\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_mean</th>\n      <th>ess_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>loc</th>\n      <td>6.837</td>\n      <td>0.546</td>\n      <td>5.818</td>\n      <td>7.853</td>\n      <td>0.008</td>\n      <td>0.005</td>\n      <td>5210.0</td>\n      <td>5210.0</td>\n      <td>5240.0</td>\n      <td>4550.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>scale</th>\n      <td>2.444</td>\n      <td>0.426</td>\n      <td>1.708</td>\n      <td>3.216</td>\n      <td>0.006</td>\n      <td>0.005</td>\n      <td>4639.0</td>\n      <td>4273.0</td>\n      <td>5315.0</td>\n      <td>4424.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nax = az.plot_trace(posterior_samples)\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_29_0.png)\n\n\n&nbsp;\n\n### 1.3.2. Run Variational Inference (e.g. meanfield ADVI)\n\n\nInstead of MCMC methods, we might want to use variational inference.\n\nChoose a surrogate posterior, e.g. meanfield `ADVI`, a discrepancy function from `tfp.vi`, e.g. `tfp.vi.kl_reverse` , and select some optimization parameters (number of optimzation steps, sample size to approximate variational loss, optimizer, learning rate).\n\n\n```python\nNUM_STEPS = 1000\nSAMPLE_SIZE = 10\nLEARNING_RATE = 1e-2\n\nsurrogate_posterior = ADVI(model=model, mean_field=True)\n\nvi = VI(model=model, dataset=dataset, surrogate_posterior=surrogate_posterior, discrepancy_fn=tfp.vi.kl_reverse)\n\napprox_posterior, meanfield_advi_losses = vi.fit(\n    optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), \n    num_steps=NUM_STEPS, \n    sample_size=SAMPLE_SIZE, \n    progress_bar=True\n)\n\n```\n\n\n\n<div>\n  100.00% [1000/1000 00:02<00:00 avg loss: 50.336]\n</div>\n&nbsp;\n\n\n\n\n```python\nax = plt.plot(meanfield_advi_losses)\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_33_0.png)\n\n\n\n\n```python\ndf = pd.DataFrame(approx_posterior.sample(5000))\n\ndf.describe(percentiles=[0.03, 0.5, 0.97]).T\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>3%</th>\n      <th>50%</th>\n      <th>97%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>loc</th>\n      <td>5000.0</td>\n      <td>6.819653</td>\n      <td>0.511264</td>\n      <td>4.710279</td>\n      <td>5.844515</td>\n      <td>6.825340</td>\n      <td>7.793183</td>\n      <td>8.429766</td>\n    </tr>\n    <tr>\n      <th>scale</th>\n      <td>5000.0</td>\n      <td>2.420403</td>\n      <td>0.367414</td>\n      <td>1.174533</td>\n      <td>1.760495</td>\n      <td>2.421083</td>\n      <td>3.117724</td>\n      <td>3.836521</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nax = df.plot(kind='kde')\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_35_0.png)\n\n\n&nbsp;\n\n# 2. Normalizing Flows for Variational Inference\n\nContinuing the simple example above, this is how you can flexibly define surrogate posteriors based on Normalizing flows for variational Bayesian inference.\n\nBy default, the base distribution is a standard diagonal Gaussian.\n\nThis example uses the predefined trainable `AffineFlow` bijector, but any parameterized `tfp.bijectors.Bijector` can be used. Just define your own!\nYou can construct more expressive flows by making use of the `tfp.bijectors.Chain` bijector to define compositions of flows.\n\n\n```python\nndims = model.flat_unconstrained_param_event_ndims # this is the number of dimensions of the unconstrained parameter space\n```\n\n&nbsp;\n\n## 2.1. Affine Flows\n\n\n```python\nflow_bijector = AffineFlow(ndims)\n\nsurrogate_posterior = NormalizingFlow(model=model, flow_bijector=flow_bijector)\n\nvi = VI(model=model, dataset=dataset, surrogate_posterior=surrogate_posterior, discrepancy_fn=tfp.vi.kl_reverse)\n\napprox_posterior, affine_flow_losses = vi.fit(\n    optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), \n    num_steps=NUM_STEPS, \n    sample_size=SAMPLE_SIZE, \n    progress_bar=True\n)\n```\n\n\n\n<div>\n  100.00% [1000/1000 00:02<00:00 avg loss: 50.336]\n</div>\n\n\n&nbsp;\n\n## 2.2. Continuous Flows\nTo define continuous normalizing flows, you may use is the `tfp.bijectors.FFJORD` bijector.\n\n\n```python\ndef get_continuous_flow_bijector(unconstrained_event_dims):\n    state_fn = tfk.Sequential()\n    state_fn.add(tfk.layers.Dense(64, activation=tfk.activations.tanh))\n    state_fn.add(tfk.layers.Dense(unconstrained_event_dims))\n    state_fn.build((None, unconstrained_event_dims+1))\n    state_time_derivative_fn = lambda t, state: state_fn(tf.concat([tf.fill((state.shape[0],1), t), state], axis=-1))\n    return tfb.FFJORD(state_time_derivative_fn, \n                      ode_solve_fn=tfp.math.ode.DormandPrince(first_step_size=0.1).solve, \n                      trace_augmentation_fn=tfb.ffjord.trace_jacobian_hutchinson)\n\nflow_bijector = get_continuous_flow_bijector(ndims)\n\nsurrogate_posterior = NormalizingFlow(model=model, flow_bijector=flow_bijector)\n\nvi = VI(model=model, dataset=dataset, surrogate_posterior=surrogate_posterior, discrepancy_fn=tfp.vi.kl_reverse)\n\napprox_posterior, cnf_losses = vi.fit(\n    optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), \n    num_steps=NUM_STEPS, \n    sample_size=SAMPLE_SIZE, \n    progress_bar=True\n)\n```\n\n\n\n<div>\n  100.00% [1000/1000 01:02<00:00 avg loss: 50.439]\n</div>\n\n&nbsp;\n\n## 2.3. Comparison\n\nHave a look at the optimzation behaviors in comparison.\n\n\n```python\nax = pd.DataFrame({'meanfield_advi': meanfield_advi_losses, 'affine_flow': affine_flow_losses, 'cnf': cnf_losses}).plot(ylim=(-10,500))\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_46_0.png)\n\n\n&nbsp;\n\n# 3. Augmented Normalizing Flows for Variational Inference\n\nThe package allows to easily define augmented normalizing flows. This obviously has no advantages for such a simple example model.\n\nSimply choose a number of extra dimensions `extra_ndims` and define the flow bijector on the augmented space, i.e. on dimensions `ndims + extra_ndims`.\n\nThe optimization problem is solved on the augmented space by lifting the target distribution with the conditional `posterior_lift_distribution`, which by default is simply `lambda _: tfd.MultivariateNormalDiag(loc=tf.zeros(extra_ndims), scale_diag=tf.ones(extra_ndims))`.\n\n\n```python\nndims = model.flat_unconstrained_param_event_ndims # this is the number of dimensions of the unconstrained parameter space\n\nextra_ndims = 2 # dimensions of the augmentation space \n```\n\n\n```python\nflow_bijector = AffineFlow(ndims + extra_ndims)\n\nsurrogate_posterior = NormalizingFlow(model=model, flow_bijector=flow_bijector, extra_ndims=extra_ndims)\n\nvi = VI(model=model, dataset=dataset, surrogate_posterior=surrogate_posterior, discrepancy_fn=tfp.vi.kl_reverse)\n\napprox_posterior, affine_flow_losses = vi.fit(\n    optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), \n    num_steps=NUM_STEPS, \n    sample_size=SAMPLE_SIZE, \n    progress_bar=True\n)\n```\n\n\n\n<div>\n  100.00% [1000/1000 00:04<00:00 avg loss: 50.310]\n</div>\n&nbsp;\n\n\n\n\n```python\ndef get_continuous_flow_bijector(unconstrained_event_dims):\n    state_fn = tfk.Sequential()\n    state_fn.add(tfk.layers.Dense(64, activation=tfk.activations.tanh))\n    state_fn.add(tfk.layers.Dense(unconstrained_event_dims))\n    state_fn.build((None, unconstrained_event_dims+1))\n    state_time_derivative_fn = lambda t, state: state_fn(tf.concat([tf.fill((state.shape[0],1), t), state], axis=-1))\n    return tfb.FFJORD(state_time_derivative_fn, \n                      ode_solve_fn=tfp.math.ode.DormandPrince(first_step_size=0.1).solve, \n                      trace_augmentation_fn=tfb.ffjord.trace_jacobian_hutchinson)\n\nflow_bijector = get_continuous_flow_bijector(ndims + extra_ndims)\n\nsurrogate_posterior = NormalizingFlow(model=model, flow_bijector=flow_bijector, extra_ndims=extra_ndims)\n\nvi = VI(model=model, dataset=dataset, surrogate_posterior=surrogate_posterior, discrepancy_fn=tfp.vi.kl_reverse)\n\napprox_posterior, cnf_losses = vi.fit(\n    optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), \n    num_steps=NUM_STEPS, \n    sample_size=SAMPLE_SIZE, \n    progress_bar=True\n)\n```\n\n\n\n<div>\n  100.00% [1000/1000 01:08<00:00 avg loss: 50.633]\n</div>\n&nbsp;\n\n\n\n\n```python\nax = pd.DataFrame({'meanfield_advi': meanfield_advi_losses, 'affine_flow': affine_flow_losses, 'cnf': cnf_losses}).plot(ylim=(-10,500))\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_52_0.png)\n\n\n&nbsp;\n\n# 4. Hamiltonian Normalizing Flows\n\nAs another example consider Hamiltonian Normalizing Flows (HNF) as motivated by Toth, P., Rezende, D. J., Jaegle, A., Racanière, S., Botev, A., & Higgins, I. (2019). Hamiltonian Generative Networks. ArXiv. http://arxiv.org/abs/1909.13789 .\n\nHNFs are based on defining augmented flows as the Hamiltonian dynamics of a family of Hamiltonian systems. \n\nTo compute such dynamics, we use the symplectic leapfrog integrator. \n\nThe package provides an implementation of a general Hamiltonian Flow bijector.\n\n&nbsp;\n\n## 4.1. Verify Hamiltonian Flow Bijector\nTo test the implementation the Hamiltonian Flow Bijector, consider the Hamiltonian of a 1d harmonic oscillator with mass m=1 and constant k=1:\n\nH(q,p) = T(p) + U(q) = 0.5 p^2 + 0.5 q^2.\n\nWe know that this should yield an oscillating dynamic, which manifests as a circle on phase space.\n\n\n```python\nT = lambda p: 0.5*p**2\nU = lambda q: 0.5*q**2\n\n# event dims can be ignored in this example, because they are only used to construct trainable default energy function if none are provided explicitly\nhamiltonian_flow = HamiltonianFlow(\n    event_dims=1, \n    potential_energy_fn=U, \n    kinetic_energy_fn=T, \n    symplectic_integrator=LeapfrogIntegrator(), \n    step_sizes=0.5, \n    num_integration_steps=1\n) \n\ninitial_state = tf.constant([1.0, 0.0])\npath = [initial_state.numpy()]\n\nfor _ in range(20):\n    path.append(hamiltonian_flow.forward(path[-1]).numpy())\npath = np.stack(path)\n```\n\n\n```python\nfig, [ax1, ax2] = plt.subplots(1,2, figsize=(6,3))\n\nax1.plot(path[:, 0])\nax1.set_title('configuration plot')\n\nax2.plot(path[:,0], path[:,1])\nax2.set_title('phase space plot')\nplt.tight_layout()\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_58_0.png)\n\n&nbsp;\n\n## 4.2. Hamiltonian Normalizing Flows\n\nIn the case of HNFs the augmentation space has the same dimension as the parameter space.\n\n\n```python\nndims = model.flat_unconstrained_param_event_ndims # this is the number of dimensions of the unconstrained parameter space\n\nextra_ndims = ndims # dimensions of the augmentation space \n```\n\nFor simplicity, we use the default MLP based potential and kinetic energy functions.\n\n\n```python\nflow_bijector = HamiltonianFlow(ndims, step_sizes=tf.Variable(0.1), num_integration_steps=2)\n\nsurrogate_posterior = NormalizingFlow(model=model, flow_bijector=flow_bijector, extra_ndims=extra_ndims)\n\nvi = VI(model=model, dataset=dataset, surrogate_posterior=surrogate_posterior, discrepancy_fn=tfp.vi.kl_reverse)\n\napprox_posterior, hnf_losses = vi.fit(\n    optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), \n    num_steps=NUM_STEPS, \n    sample_size=SAMPLE_SIZE, \n    progress_bar=True\n)\n```\n\n\n\n<div>\n  100.00% [1000/1000 00:07<00:00 avg loss: 54.512]\n</div>\n\n&nbsp;\n\n\n\n```python\nax = pd.DataFrame({'meanfield_advi': meanfield_advi_losses, 'affine_flow': affine_flow_losses, 'cnf': cnf_losses, 'hnf': hnf_losses}).plot(ylim=(-10,500))\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_64_0.png)\n\n\n\nAn interesting observation is that HNFs become more stable and perform better, if the base distribution is trainable.\n\nThis compensates for the problem, that volume preserving flows like HNFs can only permute the densities of the base distribution. \n\n\n```python\nflow_bijector = HamiltonianFlow(ndims, step_sizes=tf.Variable(0.1), num_integration_steps=2)\n\nbase_distribution = tfd.MultivariateNormalDiag(loc=tf.Variable([0.0]*2*ndims), scale_diag=tf.Variable([0.1]*2*ndims))\n\nsurrogate_posterior = NormalizingFlow(model=model, flow_bijector=flow_bijector, base_distribution=base_distribution, extra_ndims=extra_ndims)\n\nvi = VI(model=model, dataset=dataset, surrogate_posterior=surrogate_posterior, discrepancy_fn=tfp.vi.kl_reverse)\n\napprox_posterior, hnf_losses = vi.fit(\n    optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), \n    num_steps=NUM_STEPS, \n    sample_size=SAMPLE_SIZE, \n    progress_bar=True\n)\n```\n\n\n\n<div>\n  100.00% [1000/1000 00:07<00:00 avg loss: 50.746]\n</div>\n&nbsp;\n\n\n\n\n```python\nax = pd.DataFrame({'meanfield_advi': meanfield_advi_losses, 'affine_flow': affine_flow_losses, 'cnf': cnf_losses, 'hnf': hnf_losses}).plot(ylim=(-10,500))\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_67_0.png)\n\n&nbsp;\n\n# 5. Regression Model Example\n\nAs a final example, this is how to define regression models.\n\n&nbsp;\n\n## 5.1. Create a Dataset\nAgain, create a `tf.data.Dataset` from your data or, in this case, generate some fake dataset.\n\n\n```python\nnum_datapoints = 50\nbeta0 = 7\nbeta1 = 3\nscale = 10\nx = tf.random.normal((num_datapoints,), mean=0.0, stddev=10.0)\ntrue_dist = tfd.Normal(loc=beta0 + x*beta1, scale=scale)\ny = true_dist.sample()\n```\n\n\n```python\ndata = pd.DataFrame(dict(x=x, y=y))\ndataset = make_dataset_from_df(data, target_names=['y'], feature_names=['x'], format_features_as='dict')\n```\n\n\n```python\nax = data.plot(x='x', y='y', kind='scatter', title='Linear Regression Test Data')\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_73_0.png)\n\n\n&nbsp;\n\n## 1.2. Define a Bayesian Model\n\nTo define the `Model`, again provide an `collections.OrderedDict` of prior distributions as `tfp.distribution.Distirbution`s and a likelihood function, which takes the parameters and the features as inputs.\n\n\n```python\n# define priors\npriors = collections.OrderedDict(\n    beta_0 = tfd.Normal(loc=0., scale=10.),\n    beta_1 = tfd.Normal(loc=0., scale=10.),\n    scale = tfd.HalfNormal(scale=10.),\n)\n\n# define likelihood\ndef likelihood(beta_0, beta_1, scale, features):\n    linear_response = beta_0 + beta_1*features['x']\n    return tfd.Normal(loc=linear_response, scale=scale)\n```\n\n\n```python\nmodel = Model(priors=priors, likelihood=likelihood)\n```\n\n&nbsp;\n\n## 1.3. Run Inference Algorithm of Choice\n\nYou can now again use either MCMC or variational inference to do Bayesian inference.\n\n&nbsp;\n\n### 1.3.1. Run MCMC (e.g. with Random Walk Metropolis kernel)\n\n\n```python\nNUM_CHAINS = 2\nNUM_SAMPLES = 4000\nNUM_BURNIN_STEPS = 1000\n\n\nkernel = RandomWalkMetropolis()\n\nmcmc = MCMC(model=model, dataset=dataset, transition_kernel=kernel)\n\nmcmc_result = mcmc.fit(\n    num_chains=NUM_CHAINS, \n    num_samples=NUM_SAMPLES, \n    num_burnin_steps=NUM_BURNIN_STEPS,\n    progress_bar=True,\n)\n```\n\n\n\n<div>\n  92.70% [4635/5000 00:01<00:00]\n</div>\n\n&nbsp;\n\n\n\n```python\n# adapt sample results to arviz required format --- later versions should allow for seamless integration with arviz\nposterior_samples = to_ordered_dict(model.param_names, tf.nest.map_structure(lambda v: v.numpy().T, mcmc_result.samples))\n```\n\n\n```python\naz.summary(posterior_samples)\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean</th>\n      <th>sd</th>\n      <th>hdi_3%</th>\n      <th>hdi_97%</th>\n      <th>mcse_mean</th>\n      <th>mcse_sd</th>\n      <th>ess_mean</th>\n      <th>ess_sd</th>\n      <th>ess_bulk</th>\n      <th>ess_tail</th>\n      <th>r_hat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>beta_0</th>\n      <td>6.744</td>\n      <td>1.296</td>\n      <td>4.561</td>\n      <td>9.264</td>\n      <td>0.124</td>\n      <td>0.088</td>\n      <td>109.0</td>\n      <td>109.0</td>\n      <td>110.0</td>\n      <td>146.0</td>\n      <td>1.01</td>\n    </tr>\n    <tr>\n      <th>beta_1</th>\n      <td>2.859</td>\n      <td>0.144</td>\n      <td>2.570</td>\n      <td>3.110</td>\n      <td>0.006</td>\n      <td>0.004</td>\n      <td>596.0</td>\n      <td>591.0</td>\n      <td>600.0</td>\n      <td>689.0</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>scale</th>\n      <td>9.217</td>\n      <td>0.924</td>\n      <td>7.724</td>\n      <td>10.978</td>\n      <td>0.063</td>\n      <td>0.045</td>\n      <td>215.0</td>\n      <td>214.0</td>\n      <td>220.0</td>\n      <td>395.0</td>\n      <td>1.01</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nax = az.plot_trace(posterior_samples)\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_84_0.png)\n\n\n&nbsp;\n\n### 1.3.2. Run Variational Inference (e.g. full rank ADVI)\n\n\n\n```python\nNUM_STEPS = 1000\nSAMPLE_SIZE = 10\nLEARNING_RATE = 1e-2\n\nsurrogate_posterior = ADVI(model=model)\n\nvi = VI(model=model, dataset=dataset, surrogate_posterior=surrogate_posterior, discrepancy_fn=tfp.vi.kl_reverse)\n\napprox_posterior, meanfield_advi_losses = vi.fit(\n    optimizer=tf.optimizers.Adam(learning_rate=LEARNING_RATE), \n    num_steps=NUM_STEPS, \n    sample_size=SAMPLE_SIZE, \n    progress_bar=True\n)\n\n```\n\n\n\n<div>\n  100.00% [1000/1000 00:04<00:00 avg loss: 188.817]\n</div>\n\n&nbsp;\n\n\n\n```python\nax = plt.plot(meanfield_advi_losses)\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_87_0.png)\n\n\n\n\n```python\ndf = pd.DataFrame(approx_posterior.sample(5000))\n\ndf.describe(percentiles=[0.03, 0.5, 0.97]).T\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>3%</th>\n      <th>50%</th>\n      <th>97%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>beta_0</th>\n      <td>5000.0</td>\n      <td>6.877542</td>\n      <td>1.259172</td>\n      <td>2.323422</td>\n      <td>4.499239</td>\n      <td>6.854323</td>\n      <td>9.263641</td>\n      <td>11.693712</td>\n    </tr>\n    <tr>\n      <th>beta_1</th>\n      <td>5000.0</td>\n      <td>2.863648</td>\n      <td>0.136708</td>\n      <td>2.382410</td>\n      <td>2.609725</td>\n      <td>2.861714</td>\n      <td>3.120923</td>\n      <td>3.358475</td>\n    </tr>\n    <tr>\n      <th>scale</th>\n      <td>5000.0</td>\n      <td>9.005477</td>\n      <td>0.870540</td>\n      <td>5.714116</td>\n      <td>7.380205</td>\n      <td>8.992948</td>\n      <td>10.633552</td>\n      <td>12.097922</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nax = df.plot(kind='kde')\n```\n\n\n\n![](https://raw.githubusercontent.com/MaxGrtz/bayesian-inference/thesis/readme_images/output_89_0.png)\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/MaxGrtz/bayesian-inference",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bayes-vi",
    "package_url": "https://pypi.org/project/bayes-vi/",
    "platform": "",
    "project_url": "https://pypi.org/project/bayes-vi/",
    "project_urls": {
      "Homepage": "https://github.com/MaxGrtz/bayesian-inference"
    },
    "release_url": "https://pypi.org/project/bayes-vi/0.3.0/",
    "requires_dist": [
      "numpy (==1.19.5)",
      "pandas (>=1.1.5)",
      "tensorflow (>=2.4.1)",
      "tensorflow-probability (>=0.12.1)",
      "fastprogress (>=1.0.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "A package for bayesian inference",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9796448,
  "releases": {
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a73626a9e5b54e355572445f798d0c80629ac920c440df9f5acf2e3f2cc8548d",
          "md5": "e804b95d1d1749d48a6fd07ca4582a94",
          "sha256": "2d7a6333fb6973cc5268f0b94432d6882cc4e67a83a9e2fc73e34371e2b20522"
        },
        "downloads": -1,
        "filename": "bayes_vi-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e804b95d1d1749d48a6fd07ca4582a94",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 43243,
        "upload_time": "2021-03-09T21:10:45",
        "upload_time_iso_8601": "2021-03-09T21:10:45.594190Z",
        "url": "https://files.pythonhosted.org/packages/a7/36/26a9e5b54e355572445f798d0c80629ac920c440df9f5acf2e3f2cc8548d/bayes_vi-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "21091c444563255faad0f3a1f3ecb0df378f2f5a5cfb2a07bbc107f4460622f3",
          "md5": "eea8d3e97fdc9f428f40f5eacdd17906",
          "sha256": "9c6b14e707e738e89728024a39052c840b4a5665c63bb21cfd1c89b0a7fca3ea"
        },
        "downloads": -1,
        "filename": "bayes-vi-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "eea8d3e97fdc9f428f40f5eacdd17906",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 26270,
        "upload_time": "2021-03-09T21:10:47",
        "upload_time_iso_8601": "2021-03-09T21:10:47.913800Z",
        "url": "https://files.pythonhosted.org/packages/21/09/1c444563255faad0f3a1f3ecb0df378f2f5a5cfb2a07bbc107f4460622f3/bayes-vi-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a73626a9e5b54e355572445f798d0c80629ac920c440df9f5acf2e3f2cc8548d",
        "md5": "e804b95d1d1749d48a6fd07ca4582a94",
        "sha256": "2d7a6333fb6973cc5268f0b94432d6882cc4e67a83a9e2fc73e34371e2b20522"
      },
      "downloads": -1,
      "filename": "bayes_vi-0.3.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e804b95d1d1749d48a6fd07ca4582a94",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 43243,
      "upload_time": "2021-03-09T21:10:45",
      "upload_time_iso_8601": "2021-03-09T21:10:45.594190Z",
      "url": "https://files.pythonhosted.org/packages/a7/36/26a9e5b54e355572445f798d0c80629ac920c440df9f5acf2e3f2cc8548d/bayes_vi-0.3.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "21091c444563255faad0f3a1f3ecb0df378f2f5a5cfb2a07bbc107f4460622f3",
        "md5": "eea8d3e97fdc9f428f40f5eacdd17906",
        "sha256": "9c6b14e707e738e89728024a39052c840b4a5665c63bb21cfd1c89b0a7fca3ea"
      },
      "downloads": -1,
      "filename": "bayes-vi-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "eea8d3e97fdc9f428f40f5eacdd17906",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 26270,
      "upload_time": "2021-03-09T21:10:47",
      "upload_time_iso_8601": "2021-03-09T21:10:47.913800Z",
      "url": "https://files.pythonhosted.org/packages/21/09/1c444563255faad0f3a1f3ecb0df378f2f5a5cfb2a07bbc107f4460622f3/bayes-vi-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}