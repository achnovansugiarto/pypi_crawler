{
  "info": {
    "author": "Guillaume Wenzek",
    "author_email": "guw@fb.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# cc_net\n\nTools to download and clean Common Crawl as introduced in our paper [CCNet](https://arxiv.org/abs/1911.00359).\n\nIf you found these resources useful, please consider citing:\n\n```\n@article{wenzek2019ccnet,\n  title={Ccnet: Extracting high quality monolingual datasets from web crawl data},\n  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzman, Francisco and Joulin, Armand and Grave, Edouard},\n  journal={arXiv preprint arXiv:1911.00359},\n  year={2019}\n}\n```\n\n[![CircleCI](https://circleci.com/gh/facebookresearch/cc_net.svg?style=svg)](https://circleci.com/gh/facebookresearch/cc_net)\n\n\n## Installation\n\nWe only tried this on Linux but installation should be possible on MacOS too.\n\n1. Create or simlink a `data` folder to where you want to download the corpus.\n\n2. Run `make install`. This will download some resources and install required packages.\n\n3. If you have a C++ 17 compiler you can also run\n`pip install .[getpy]`, it provides more memory efficient hashset.\n\n4. Install the following tools manually if `make install` failed:\n- `lmplz` and `build_binary` from [KenLM](https://github.com/kpu/kenlm)\n- `spm_train` and `spm_encode` from [Sentence Piece](https://github.com/google/sentencepiece)\n\n## Training Language Models\n\nThe `Makefile` is used to train Sentence Piece and LM on Wikipedia data.\n\n* `make help` shows help\n* `make lang=de lm` trains a Sentence Piece and a LM on German Wikipedia\n* `make all_lm` trains the same model than in the paper\n* `make lang=de dl_lm` downloads the LM trained for the paper\n* `make dl_all_lm` downloads all of them\n\n## Pipeline overview\n\nThe full mining pipeline is divided in 3 steps:\n\n- `hashes` downloads one Common-Crawl snapshot, and compute hashes for each paragraph\n- `mine` removes duplicates, detects language, run the LM and split by lang/perplexity buckets\n- `regroup` regroup the files created by `mine` in chunks of 4Gb\n\nEach step needs the previous step to be over before starting.\nYou can launch the full pipeline using `python -m cc_net`.\n\n* `python -m cc_net --help` shows help\n* `python -m cc_net --dump 2019-13` treats a specific snapshot\n* `python -m cc_net -l my -l gu` \nrestricts to specific languages\n* `python -m cc_net --lm_dir my_lms/` uses custom LMs\n* `python -m cc_net --lang_threshold 0.3` set a specific field in `mine.Config`\n* `python -m cc_net --config test` runs on a tiny subset of a snapshot\n* `python -m cc_net --config config/my_config.json` uses configuration from the given config file\n\n## Reproducing our work\n\nGiven the CPU required to run the full pipeline on such a big corpus we share a mapping from url to the information we computed.\nYou can reconstruct the corpus used in the paper by using:\n\n```sh\npython -m cc_net --conf reproduce --dump 2019-09\n```\n\n## Extract XLM-R data\n\n[Unsupervised Cross-lingual Representation Learning at Scale (XLM-RoBERTa)](https://arxiv.org/pdf/1911.02116.pdf)\npaper was trained on data extracted by an internal version of cc_net.\n\nDue to the format being a little bit different please use the following command instead:\n\n```sh\npython cc_net/tools/dl_cc_100.py --help\npython cc_net/tools/dl_cc_100.py --outdir data_cc100 --process 8\n```\n\nIf you use this version of the data please also consider citing:\n\n```bibtex\n@article{conneau2019unsupervised,\n  title={Unsupervised Cross-lingual Representation Learning at Scale},\n  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},\n  journal={arXiv preprint arXiv:1911.02116},\n  year={2019}\n}\n```\n\n\n## Adapting to your infrastructure\n\nGiven the computation cost of running the full pipeline we distributed the computation\non a [Slurm](https://slurm.schedmd.com/) cluster using [submitit](https://github.com/facebookincubator/submitit).\n`submitit` will default to spawning processes on your machine if Slurm cluster is found.\nYou should tweak `--task_parallelism` to something adapated to your machine.\nDefaults are 512 for mining and 20 for reproducing.\n\nTo run the tasks in-process use `--execution debug`.\n\n\n## Output format\n\nGenerated files are compressed JSON files. There is one JSON object per line.\n\n__List of fields__:\n\n- url: webpage URL (part of CC)\n- date_download: date of download (part of CC)\n- digest: sha1 digest of the webpage (part of CC)\n- length: number of chars\n- nlines: number of lines\n- source_domain: web domain of the webpage\n- title: page title (part of CC)\n- raw_content: webpage content after deduplication\n- original_nlines: number of lines before deduplication\n- original_length: number of chars before deduplication\n- language: language detected by FastText LID\n- language_score: language score\n- perplexity: perplexity of a LM trained on Wikipedia\n\n__Sample JSON object__:\n```json\n{\n  \"url\": \"http://www.pikespeakhospice.org/members/1420\",\n  \"date_download\": \"2019-02-15T18:40:25Z\",\n  \"digest\": \"sha1:VQW3KXUOALO543IJGTK2JLVEAN2XXKHI\",\n  \"length\": 752,\n  \"nlines\": 5,\n  \"source_domain\": \"www.pikespeakhospice.org\",\n  \"title\": \"LeeRoy Aragon\",\n  \"raw_content\": \"Date Honored: March 2017\\nHe was a man of integrity, a hard worker, and a dedicated family man. He loved spending time with family camping, fishing, hunting, boating and just hanging out.\\nHis Catholic faith was extremely important to him as he gave of his time and talents to the community. He had many friends through church and the Knights of Columbus. He was a meticulous handyman, and enjoyed building and fixing things and restoring antique furniture to perfection. He was a fan and supported his Colorado Rockies and Denver Broncos. Throughout the years he had devoted four-legged friends (his dogs and a horse named Sunny Boy).\\nWe have many cherished memories of him that we will treasure until we are with him again.\\n~ Family of LeeRoy F. Aragon\",\n  \"original_nlines\": 7,\n  \"original_length\": 754,\n  \"language\": \"en\",\n  \"language_score\": 0.99,\n  \"perplexity\": 255.11,\n}\n```\n\nYou can peak at those files using UNIX tools `zcat` and [`jq`](https://stedolan.github.io/jq/manual/), eg:\n`zcat data/mined/2019-09/en_head_0000.json.gz | head -1 | jq .`\n\n`jq` can do some complicated filtering.\n`jsonql.py` provides a Python API with multiprocess support to do more complicated operations like LM scoring of the document.\n\n## License\n\nBy contributing to `cc_net`, you agree that your contributions will be licensed\nunder the LICENSE file in the root directory of this source tree.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/facebookresearch/cc_net",
    "keywords": "common crawl dataset",
    "license": "CC-BY-NC-4.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cc-net",
    "package_url": "https://pypi.org/project/cc-net/",
    "platform": "",
    "project_url": "https://pypi.org/project/cc-net/",
    "project_urls": {
      "Bug Tracker": "https://github.com/facebookresearch/cc_net/issues",
      "Homepage": "https://github.com/facebookresearch/cc_net",
      "Source Code": "https://github.com/facebookresearch/cc_net"
    },
    "release_url": "https://pypi.org/project/cc-net/1.0.0/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Tools to download and clean Common Crawl",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10625413,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cc015e2756bbcf0ec766e5b983f172a297a2dbab66dcad4180900bb99d531874",
          "md5": "f8bd3705e5e90fc3cc0889d2d52f587d",
          "sha256": "18f59649809d840187b0ebeda2646ff5bc538a03218bde3826ad8434729cb125"
        },
        "downloads": -1,
        "filename": "cc_net-0.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f8bd3705e5e90fc3cc0889d2d52f587d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 61383,
        "upload_time": "2019-10-30T22:07:57",
        "upload_time_iso_8601": "2019-10-30T22:07:57.956716Z",
        "url": "https://files.pythonhosted.org/packages/cc/01/5e2756bbcf0ec766e5b983f172a297a2dbab66dcad4180900bb99d531874/cc_net-0.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "93710cc5a47c4aca34420b337545519c2d891ef3b817e419be80430f30e6b23c",
          "md5": "1a87acde137f2c0140e5055e3ca0be56",
          "sha256": "77f2cb03914a87088605fb88a08b0acf3a9472ca5f3397afea9a461b1217fe10"
        },
        "downloads": -1,
        "filename": "cc_net-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "1a87acde137f2c0140e5055e3ca0be56",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 50183,
        "upload_time": "2019-10-30T22:08:00",
        "upload_time_iso_8601": "2019-10-30T22:08:00.668212Z",
        "url": "https://files.pythonhosted.org/packages/93/71/0cc5a47c4aca34420b337545519c2d891ef3b817e419be80430f30e6b23c/cc_net-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb46ca08b95f9164a01a01448cd41524c9b21c34b1b79b1cacb92ad7a14be608",
          "md5": "7591c4493e2127f2971dee5fe32830c4",
          "sha256": "60131c23498bfa1428b4c6d311cceb60e8f298d5d4d1900eca4f8402543d75f3"
        },
        "downloads": -1,
        "filename": "cc_net-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7591c4493e2127f2971dee5fe32830c4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 81276,
        "upload_time": "2020-11-02T15:46:05",
        "upload_time_iso_8601": "2020-11-02T15:46:05.902887Z",
        "url": "https://files.pythonhosted.org/packages/bb/46/ca08b95f9164a01a01448cd41524c9b21c34b1b79b1cacb92ad7a14be608/cc_net-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bb46ca08b95f9164a01a01448cd41524c9b21c34b1b79b1cacb92ad7a14be608",
        "md5": "7591c4493e2127f2971dee5fe32830c4",
        "sha256": "60131c23498bfa1428b4c6d311cceb60e8f298d5d4d1900eca4f8402543d75f3"
      },
      "downloads": -1,
      "filename": "cc_net-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "7591c4493e2127f2971dee5fe32830c4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 81276,
      "upload_time": "2020-11-02T15:46:05",
      "upload_time_iso_8601": "2020-11-02T15:46:05.902887Z",
      "url": "https://files.pythonhosted.org/packages/bb/46/ca08b95f9164a01a01448cd41524c9b21c34b1b79b1cacb92ad7a14be608/cc_net-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}