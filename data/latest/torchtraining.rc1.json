{
  "info": {
    "author": "Szymon Maszke",
    "author_email": "szymon.maszke@protonmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "<img align=\"left\" width=\"256\" height=\"256\" src=\"https://github.com/szymonmaszke/torchtraining/blob/master/assets/logo.png\">\n\nSo you want to train neural nets with PyTorch? Here are your options:\n\n- __plain PyTorch__ - a lot of tedious work like writing [metrics](https://github.com/pytorch/pytorch/issues/22439) or `for` loops\n- __external frameworks__ - more automated in exchange for less freedom,\n[less flexibility](https://github.com/skorch-dev/skorch), [lots of esoteric functions](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/datamodule.py) and\n[stuff under the hood](https://github.com/fastai/fastai/blob/master/fastai2/optimizer.py)\n\n\nEnter [__torchtraining__](https://github.com/szymonmaszke/torchtraining/) - we try to get what's best from both worlds while adding:\nexplicitness, functional approach, easy extensions and freedom to structure your code!\n\n__All of that using single `**` piping operator!__\n\n| Version | Docs | Tests | Coverage | Style | PyPI | Python | PyTorch | Docker | LOC |\n|---------|------|-------|----------|-------|------|--------|---------|--------|-----|\n| [![Version](https://img.shields.io/static/v1?label=&message=0.0.1&color=377EF0&style=for-the-badge)](https://github.com/szymonmaszke/torchtraining/releases) | [![Documentation](https://img.shields.io/static/v1?label=&message=docs&color=EE4C2C&style=for-the-badge)](https://szymonmaszke.github.io/torchtraining/)  | ![Tests](https://github.com/szymonmaszke/torchtraining/workflows/test/badge.svg) | ![Coverage](https://img.shields.io/codecov/c/github/szymonmaszke/torchtraining?label=%20&logo=codecov&style=for-the-badge) | [![codebeat](https://img.shields.io/static/v1?label=&message=CB&color=27A8E0&style=for-the-badge)](https://codebeat.co/projects/github-com-szymonmaszke-torchtraining-master) | [![PyPI](https://img.shields.io/static/v1?label=&message=PyPI&color=377EF0&style=for-the-badge)](https://pypi.org/project/torchtraining/) | [![Python](https://img.shields.io/static/v1?label=&message=>3.6&color=377EF0&style=for-the-badge&logo=python&logoColor=F8C63D)](https://www.python.org/) | [![PyTorch](https://img.shields.io/static/v1?label=&message=1.6.0&color=EE4C2C&style=for-the-badge)](https://pytorch.org/) | [![Docker](https://img.shields.io/static/v1?label=&message=docker&color=309cef&style=for-the-badge)](https://hub.docker.com/r/szymonmaszke/torchtraining) | ![LOC](https://img.shields.io/static/v1?label=&message=3000&color=327E50&style=for-the-badge)\n\n## Tutorials\n\nSee tutorials to get a grasp of what's the fuss is all about:\n\n- [__Introduction__](https://colab.research.google.com/drive/19oI8RlpDT9JZnkW8BbFzrLL1Wse6wD5G?usp=sharing) - quick tour around functionalities with CIFAR100 classification\nand `tensorboard`.\n- [__GAN training__](https://colab.research.google.com/drive/19oI8RlpDT9JZnkW8BbFzrLL1Wse6wD5G?usp=sharing) - more advanced example and creating you own pipeline components.\n\n\n## Why `torchtraining`?\n\nThere are a lot of training libraries around for a lot of frameworks. Why would\nyou choose this one?\n\n### `torchtrain` fits you, not the other way around\n\nWe think it's impossible to squeeze user's code in an overly strict API.\nWe __are not__ trying to `fit` everything into a single... `.fit()` method (or `Trainer` god class,\nsee `40!` [arguments in PyTorch-Lightning trainer](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L155)).\nThis approach has shown time and time again it does not work for more complicated\nuse cases as one cannot foresee the endless possibilities\nof training neural network and data generation user might require.\n`torchtrain` gives you building blocks to calculate metrics, log results,\ndistribute training instead.\n\n\n### Implement single `forward` instead of 40 methods\n\nImplementing `forward` with `data` argument is __all__ you will __ever__ need (okay, `accumulators` also need `calculate`,\nbut that's it), we add thin `__call__`.\nCompare that to `PyTorch-Lightning`'s `LightningModule` (source code [here](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L51))\n\n- `training_step`\n- `training_step_end`\n- `training_epoch_end` (repeat all the above for `validation` and `test`)\n- `validation_end`, `test_end`\n- `configure_sync_batchnorm`\n- `configure_ddp`\n- `init_ddp_connection`\n- `configure_apex`\n- `configure_optimizers`\n- `optimizer_step`\n- `optimizer_zero_grad`\n- `tbptt_split_batch` (?)\n- `prepare_data`\n- `train_dataloader`\n- `tng_dataloader`\n- `test_dataloader`\n- `val_dataloader`\n\nThis list could go on (and probably will grow even bigger as time passes).\nWe believe in functional approach and using only what you need (a lot of decoupled building blocks instead\nof gigantic god classes __trying__ to do everything). Once again: we can't foresee\nfuture and __won't__ squash everything into single `class`.\n\n### Explicitness\n\nYou are offered building blocks and it's up to you what you want to use.\nStill, you are explicit about __everything__ going on in your code, for example:\n- when, where and what to log to `tensorboard`\n- when and how often to run optimization\n- what `neural network(s)` go into what step\n- what data you choose to accumulate and how often\n- which component of your pipeline should log via `loguru`\n- and how to log (e.g. to `stdout` and `file` or maybe over the web?)\n\nSee introduction tutorial to see how it's done\n\n### Neural network != training\n\nWe don't think your neural network source code should be polluted with training.\nWe think it's better to have `data` preparation in `data.py` module,\n`optimizers` in `optimizers.py` and so on. With `torchtrain` you don't have to\ncrunch any functionalities into single god `class`.\n\n### Nothing under the hood (almost)\n\n`~3000` lines of code (including `comet-ml`, `neptune` and `horovod` integration)\nand short functions/classes allow you to quickly dig\ninto the source if you find something odd/not working. It's leverages what exists\ninstead of reinventing the wheel.\n\n\n### PyTorch first\n\nWe don't force you to jump into and from `numpy` as most of the tasks can already be\ndone in `PyTorch`. We are `pytorch` first.\nUnless we have to integrate third party tool... In that case __you don't pay for\nthis feature if you don't use it!__\n\n### Easy integration with other tools\n\nIf we don't provide an integration out of the box, you can request it via `issues`\nor make your own `PR`. Any code you want can almost always be integrated via following steps:\n\n- make a new module (say `amazing.py`)\n- create new classes inheriting from `torchtraining.Operation`\n- implement `forward` for each operation which takes single argument `data`\nwhich can be anything (`Tuple`, `List`, `torch.Tensor`, `str`, whatever really)\n- process this data in `forward` and return results\n- you have your own operator compatible with `**`!\n\nOther tools integrate components by trying to squash them into their predefined APIs\nand/or trying to be smart and guess what the user does (which often fails).\nHere's how we do:\n\n__Example of integration of `neptune` image logging:__\n\n\n```python\nimport torchtraining as tt\n\nclass Image(tt.Operation):\n    def __init__(\n        self,\n        experiment,\n        log_name: str,\n        image_name: str = None,\n        description: str = None,\n        timestamp=None,\n        experiment=None,\n    ):\n        super().__init__()\n        self.experiment = experiment\n        self.log_name = log_name\n        self.image_name = image_name\n        self.description = description\n        self.timestamp = timestamp\n\n    # Always forward some data so it can be reused\n    def forward(self, data):\n        self.experiment.log_image(\n            self.log_name, data, self.image_name, self.description, self.timestamp\n        )\n        return data\n```\n\n## Contributing\n\nThis project is currently in it's infancy and __we would love to get some help from you!__\nYou can find current ideas inside `issues` tagged by `[DISCUSSION]` (see [here](https://github.com/szymonmaszke/torchtraining/issues?q=DISCUSSION)).\n\n- [`accelerators.py` module for distributed training](https://github.com/szymonmaszke/torchtraining/issues/1)\n- [`callbacks.py` third party integrations (experiment handlers like `comet-ml` or `neptune`)](https://github.com/szymonmaszke/torchtraining/issues/2)\n\nAlso feel free to make your own feature requests and give us your thoughts in `issues`!\n\n__Remember: It's only `0.0.1` version, direction is there but you can be sure\nto encounter a lot of bugs along the way at the moment__\n\n### Why `**` as an operator?\n\nIndeed, operators like `|`, `>>` or `>` would be way more intuitive, __but__:\n- Those are left associative and would require users to explicitly uses\nparentheses around pipes\n- `>` cannot be piped as easily\n- __Way more__ complicated code on our side to handle `>>` or `|`\n\nCurrently `**` seems like a reasonable trade-off, still it may be subject to\nchange in future.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/pypa/torchtraining",
    "keywords": "pytorch train functional flexible research fit epoch step simple fast",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torchtraining",
    "package_url": "https://pypi.org/project/torchtraining/",
    "platform": "",
    "project_url": "https://pypi.org/project/torchtraining/",
    "project_urls": {
      "Documentation": "https://szymonmaszke.github.io/torchtraining/#torchtraining",
      "Homepage": "https://github.com/pypa/torchtraining",
      "Issues": "https://github.com/szymonmaszke/torchtraining/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc",
      "Website": "https://szymonmaszke.github.io/torchtraining"
    },
    "release_url": "https://pypi.org/project/torchtraining/0.0.1/",
    "requires_dist": [
      "torch (>=1.3.0)",
      "loguru (>=0.5.1)",
      "rich (>=2.3.0)",
      "PyYAML (>=5.3.1)",
      "horovod[pytorch] ; extra == 'accelerators'",
      "horovod[pytorch] ; extra == 'all'",
      "tensorboard (>=2.3.0) ; extra == 'all'",
      "matplotlib (>=3.3.1) ; extra == 'all'",
      "pillow (>=7.2.0) ; extra == 'all'",
      "moviepy (>=1.0.3) ; extra == 'all'",
      "neptune-client (>=0.4.119) ; extra == 'all'",
      "comet-ml (>=3.1.17) ; extra == 'all'",
      "tensorboard (>=2.3.0) ; extra == 'callbacks'",
      "matplotlib (>=3.3.1) ; extra == 'callbacks'",
      "pillow (>=7.2.0) ; extra == 'callbacks'",
      "moviepy (>=1.0.3) ; extra == 'callbacks'",
      "neptune-client (>=0.4.119) ; extra == 'callbacks'",
      "comet-ml (>=3.1.17) ; extra == 'callbacks'",
      "comet-ml (>=3.1.17) ; extra == 'comet'",
      "horovod[pytorch] ; extra == 'horovod'",
      "neptune-client (>=0.4.119) ; extra == 'neptune'",
      "tensorboard (>=2.3.0) ; extra == 'tensorboard'",
      "matplotlib (>=3.3.1) ; extra == 'tensorboard'",
      "pillow (>=7.2.0) ; extra == 'tensorboard'",
      "moviepy (>=1.0.3) ; extra == 'tensorboard'",
      "tensorboard (>=2.3.0) ; extra == 'tests'",
      "matplotlib (>=3.3.1) ; extra == 'tests'",
      "pillow (>=7.2.0) ; extra == 'tests'",
      "moviepy (>=1.0.3) ; extra == 'tests'",
      "neptune-client (>=0.4.119) ; extra == 'tests'",
      "comet-ml (>=3.1.17) ; extra == 'tests'"
    ],
    "requires_python": ">=3.6",
    "summary": "Functional & flexible neural network training with PyTorch.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8036160,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3476ecbeef84172fc6972dfa49caf93304384b8204d69d96cba5320c9cd7eb46",
          "md5": "f546d21e1e0b522caa6fe0be48c213f1",
          "sha256": "3d0f85cb8098466ba9a3eb975393645c8d5b1a915f3dcd35e1c24c54fd7a3000"
        },
        "downloads": -1,
        "filename": "torchtraining-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f546d21e1e0b522caa6fe0be48c213f1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 81812,
        "upload_time": "2020-08-25T15:12:07",
        "upload_time_iso_8601": "2020-08-25T15:12:07.259224Z",
        "url": "https://files.pythonhosted.org/packages/34/76/ecbeef84172fc6972dfa49caf93304384b8204d69d96cba5320c9cd7eb46/torchtraining-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bdd36d716f534e16e7e6656fccd107b0586d3cddeca73387302438bd2348f519",
          "md5": "9949fd761d6c25949cc21bd43aad47e6",
          "sha256": "d7209bbb412709eaa96172f817a7e3e06ad23f7486851fcad4e3c514d2fa7824"
        },
        "downloads": -1,
        "filename": "torchtraining-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9949fd761d6c25949cc21bd43aad47e6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 57147,
        "upload_time": "2020-08-25T15:12:09",
        "upload_time_iso_8601": "2020-08-25T15:12:09.894779Z",
        "url": "https://files.pythonhosted.org/packages/bd/d3/6d716f534e16e7e6656fccd107b0586d3cddeca73387302438bd2348f519/torchtraining-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3476ecbeef84172fc6972dfa49caf93304384b8204d69d96cba5320c9cd7eb46",
        "md5": "f546d21e1e0b522caa6fe0be48c213f1",
        "sha256": "3d0f85cb8098466ba9a3eb975393645c8d5b1a915f3dcd35e1c24c54fd7a3000"
      },
      "downloads": -1,
      "filename": "torchtraining-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f546d21e1e0b522caa6fe0be48c213f1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 81812,
      "upload_time": "2020-08-25T15:12:07",
      "upload_time_iso_8601": "2020-08-25T15:12:07.259224Z",
      "url": "https://files.pythonhosted.org/packages/34/76/ecbeef84172fc6972dfa49caf93304384b8204d69d96cba5320c9cd7eb46/torchtraining-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "bdd36d716f534e16e7e6656fccd107b0586d3cddeca73387302438bd2348f519",
        "md5": "9949fd761d6c25949cc21bd43aad47e6",
        "sha256": "d7209bbb412709eaa96172f817a7e3e06ad23f7486851fcad4e3c514d2fa7824"
      },
      "downloads": -1,
      "filename": "torchtraining-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "9949fd761d6c25949cc21bd43aad47e6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 57147,
      "upload_time": "2020-08-25T15:12:09",
      "upload_time_iso_8601": "2020-08-25T15:12:09.894779Z",
      "url": "https://files.pythonhosted.org/packages/bd/d3/6d716f534e16e7e6656fccd107b0586d3cddeca73387302438bd2348f519/torchtraining-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}