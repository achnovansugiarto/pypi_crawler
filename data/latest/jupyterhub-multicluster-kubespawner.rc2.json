{
  "info": {
    "author": "Yuvi Panda",
    "author_email": "yuvipanda@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# jupyterhub-multicluster-kubespawner\n\nLaunch user pods into many different kubernetes clusters from the same JupyterHub!\n\n## Why?\n\nA single JupyterHub as an 'entrypoint' to compute in a variety of clusters\ncan be extremely useful. Users can dynamically decide to launch their notebooks\n(and dask clusters) dynamically based on a variety of factors - closer to their\ndata, on different cloud providers, paid for by different billing accounts, etc.\nIt also makes life much easier for JupyterHub operators.\n\nYou can check out an early demo of the spawner in action\n[here](https://twitter.com/yuvipanda/status/1480938588523008001).\n\n## Installation\n\n`jupyterhub-multicluster-kubespawner` is available from PyPI:\n\n```bash\npip install jupyterhub-multicluster-kubespawner\n```\n\nYou'll also need to install [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/)\nas well as any tools needed to *authenticate* to your target clusters.\n\n| Cloud Provider | Tool |\n| - | - |\n| Google Cloud | [gcloud](https://cloud.google.com/sdk/docs/install) |\n| AWS | [aws](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) |\n| Azure | [az](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) |\n| DigitalOcean | [doctl](https://github.com/digitalocean/doctl) |\n\n## Configuration\n\nYou can ask JupyterHub to use `MultiClusterKubeSpawner` with the following config\nsnippet in your `jupyterhub_config.py` file, although more configuration is\nneeded to connect the hub to different clusters.\n\n### Configuration philosophy\n\n`MultiClusterKubeSpawner` tries to be as kubernetes-native as possible, unlike\nthe venerable [kubespawner](https://github.com/jupyterhub/kubespawner). It doesn't\ntry to provide a layer of abstraction over what kubernetes offers, as we have\nfound that is often a very leaky abstraction. This makes it difficult for JupyterHub\noperators to take advantage of all the powerful features Kubernetes offers, and\nincreases maintenance burden for the maintainers.\n\n`MultiClusterKubeSpawner` uses the popular [kubectl](https://kubernetes.io/docs/reference/kubectl/kubectl/)\nunder the hood, making the configuration familiar for anyone who has a basic\nunderstanding of working with Kubernetes clusters. The flip side is that *some*\nfamiliarity with Kubernetes is required to successfully configure this spawner,\nbut the tradeoff seems beneficial for everyone.\n\n### Setting up `KUBECONFIG`\n\nSince multicluster-kubespawner talks to multiple Kubernetes clusters, it uses a\n[kubeconfig](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/)\nfile connect to the kubernetes clusters. It looks for the file in `~/.kube/config` -\nin production environments, your file probably exists elsewhere - you can set the\n`KUBECONFIG` environment variable to point to the location of the file.\n\nEach cluster is represented by a [context](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/),\nwhich is a combination of a pointer to where the cluster's kubernetes API endpoint\nis as well as what credentials to use to authenticate to it. More details\n[here](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/).\n\nThe easiest way to construct a `kubeconfig` that will work with all the clusters\nyou want to use is to **carefully** construct it locally on your laptop and then\ncopy that file to your deployment.\n\nStart by setting your `KUBECONFIG` env var locally to a file that you can then copy\nover.\n\n```bash\nexport KUBECONFIG=jupyterhub-mcks-kubeconfig\n```\n\n#### On Google Cloud\n\n1. Create a [Google Cloud Service Account](https://cloud.google.com/iam/docs/service-accounts),\n   and give it enough permissions to access your Kubernetes cluster. `roles/container.developer`\n   *should* be enough permission.\n\n2. Create a JSON [Service Account Key](https://cloud.google.com/iam/docs/creating-managing-service-account-keys)\n   for your service account. This is what `kubectl` will eventually use to authenticate to\n   the kubernetes cluster. You'll need to put this service account key in your production\n   JupyterHub environment as well.\n\n3. Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to the location of\n   this JSON Service Account key.\n\n   ```bash\n   export GOOGLE_APPLICATION_CREDENTIALS=<path-to-json-service-account-key>\n   ```\n\n4. Generate an appropriate entry in your custom `kubeconfig` file.\n\n   ```bash\n   gcloud container clusters get-credentials <cluster-name> --zone=<zone>\n   ```\n\nWhen you deploy your JupyterHub, make sure that you set the environment variable\n`GOOGLE_APPLICATION_CREDENTIALS` to point to the service account key in a place where\nJupyterHub can find it.\n\n#### On AWS with EKS\n\nAWS has a plethora of interesting options to authenticate with it, here we will\nspecify the simplest (albeit maybe not the most secure or 'best practice').\n\n1. Create an AWS [IAM User](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users.html)\n   for use by `kubectl` to authenticate to AWS. This user will need a access key\n   and access secret, but no console access.\n\n2. Create an [access key](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)\n   for this user. JupyterHub will need these while running to make requests to the kubernetes\n   API, set as [environment variables](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html).\n\n3. Grant the user access to the `eks:DescribeCluster` permission, either directly or\n   via a group you create specifically for this purpose.\n\n4. Grant the user access to the Kubernetes API by editing the `aws-auth` configmap\n   as [described in this document](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html#aws-auth-users).\n\n5. Generate an appropriate entry in your `KUBECONFIG` file:\n\n   ```bash\n   export AWS_ACCESS_KEY_ID=<access-key-id>\n   export AWS_SECRET_ACCESS_KEY=<access-key-secret>\n   aws eks update-kubeconfig --name=<cluster-name> --region=<aws-region>\n   ```\n\nWhen you deploy your JupyterHub, you need to set the environment variables\n`AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` on the JupyterHub process itself\nso it can talk to the Kubernetes API properly.\n\n\n#### On DigitalOcean\n\n1. Install [doctl](https://github.com/digitalocean/doctl)\n\n2. Create a [personal access token](https://docs.digitalocean.com/reference/api/create-personal-access-token/)\n   to access your kubernetes cluster. Unlike the other cloud providers, this\n   can not be scoped - it grants full access to your entire DO account! So\n   use with care.\n\n3. Generate an appropriate entry in your `KUBECONFIG` file:\n\n   ```bash\n   export DIGITALOCEAN_ACCESS_TOKEN=<your-digitalocean-access-token>\n   doctl kubernetes cluster kubeconfig save <cluster-name>\n   ```\n\nWhen you deploy your JupyterHub, you need to set the environment variable\n`DIGITALOCEAN_ACCESS_TOKEN` on the JupyterHub process itself so it can talk\nto the Kubernetes API properly.\n\n### Setting up target clusters\n\nEach target cluster needs to have an [Ingress Controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)\ninstalled that the spawner can talk to. This provides a *public IP* that\nJupyterHub can use to proxy traffic to the user pods on that cluster.\n\nAny ingress provider will do, although the current suggested one is\nto use [Project Contour](https://projectcontour.io/getting-started/),\nas it's faster than the more popular [nginx-ingress](https://kubernetes.github.io/ingress-nginx/)\nat picking up routing changes.\n\nA 'production' install might use [helm](https://helm.sh) and the\n[contour helm chart](https://github.com/bitnami/charts/tree/master/bitnami/contour/#installing-the-chart).\nBut to quickly get started, you can also just configure your `kubectl`\nto point to the correct kubernes cluster and run\n`kubectl apply --wait -f https://projectcontour.io/quickstart/contour.yaml`. After\nit succeeds, you can get the public IP of the ingress controller with\n`kubectl -n projectcontour get svc envoy`. The `EXTERNAL-IP` value here\ncan be passed to the `ingress_public_url` configuration option for your\ncluster.\n\n### Setup `profile_list`\n\nAfter login, each user will be provided with a list of `profile`s to choose from.\nEach profile can point to a different kubernetes cluster, as well as other\ncustomizations such as image to use, amount of RAM / CPU, GPU use, etc.\n\nEach item in the list is a python dictionary, with the following keys recognized:\n\n1. **`display_name`**: Name to display to the user in the profile selection screen\n2. **`description`**: Description to display to the user in the profile selection screen\n3. **`spawner_override`**: Dictionary of spawner options for this profile, determining\n   where the user pod is spawned and other properties of it.\n\nThe following properties are supported under `spawner_override`.\n\n1. **`kubernetes_context`**: Name of the kubernetes context to use when connecting to\n   this cluster. You can use `kubectl config get-contexts` to get a list of contexts\n   available in your `KUBECONFIG` file.\n2. **`ingress_public_url`**: URL to the public endpoint of the Ingress controller you\n   setup earlier. This should be formatted as a URL, so don't forget the `http://`.\n   For production systems, you should also setup HTTPS for your ingress controller,\n   and provide the `https://<domain-name>` here.\n3. **`patches`**: A list of patches (as passed to\n   [`kubectl patch`](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/)).\n   Described in more detail below. This is the primary method of customizing the\n   user pod, although some convenience methods are also offered (detailed below).\n4. **`environment`**: A dictionary of extra environment variables to set for the\n   user pod.\n5. **`image`**: The image to use for the user pod. Defaults to `pangeo/pangeo-notebook:latest`.\n6. **`mem_limit`** and **`mem_guarantee`**, as [understood by JupyterHub](https://jupyterhub.readthedocs.io/en/stable/reference/spawners.html#memory-limits-guarantees)\n7. **`cpu_limit`** and **`cpu_guarantee`**, as [understood by JupyterHub](https://jupyterhub.readthedocs.io/en/stable/reference/spawners.html#cpu-limits-guarantees)\n\nHere is a simple example:\n\n```python\nc.MultiClusterKubeSpawner.profile_list = [\n    {\n        \"display_name\": \"Google Cloud in us-central1\",\n        \"description\": \"Compute paid for by funder A, closest to dataset X\",\n        \"spawner_override\": {\n            \"kubernetes_context\": \"<kubernetes-context-name-for-this-cluster\">,\n            \"ingress_public_url\": \"http://<ingress-public-ip-for-this-cluster>\"\n        }\n    },\n    {\n        \"display_name\": \"AWS on us-east-1\",\n        \"description\": \"Compute paid for by funder B, closest to dataset Y\",\n        \"spawner_override\": {\n            \"kubernetes_context\": \"<kubernetes-context-name-for-this-cluster\">,\n            \"ingress_public_url\": \"http://<ingress-public-ip-for-this-cluster>\",\n            \"patches\": {\n                \"01-memory\": \"\"\"\n                    kind: Pod\n                    metadata:\n                        name: {{key}}\n                    spec:\n                        containers:\n                        - name: notebook\n                        resources:\n                            requests:\n                                memory: 16Mi\n                    \"\"\",\n            }\n        }\n    },\n]\n```\n\n### Customizations with `patches`\n\nTo try and be as kubernetes-native as possible, we use [strategic merge patch](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/#notes-on-the-strategic-merge-patch)\nas implemented by kubectl to allow JupyterHub operators to customize per-user resources.\nThis lets operators have fine grained control over what gets spawned per-user, without\nrequiring a lot of effort by the maintainers of this spawner to support each possible\ncustomization.\n\nBehind the scenes, `kubectl patch` is used to merge the initial list of generated\nkubernetes resources for each user with some customizations before they are passed to\n`kubectl apply`. Operators set these by customizing the `patches` traitlet. It can\nbe either set for all profiles by setting `c.MultiClusterKubeSpawner.patches` or just\nfor a particular set of profiles by setting `patches` under `spawner_override` for that\nparticular profile.\n\n`patches` is a dictionary, where the key is used just for sorting and the value is\na string that should be a valid YAML object when parsed after template substitution.\nResources are merged based on the value for `kind` and `metadata.name` keys in the YAML.\n`kubectl` knows when to add items to a list or merge their properties on appropriate\nattributes.\n\nTo patch the user pod to add some extra annotations to the pod and request a GPU,\nyou could set the following:\n\n```python\n\nc.MultiClusterKubernetesSpawner.patches = {\n    \"01-annotations\": \"\"\"\n    kind: Pod\n    metadata:\n        name: {{key}}\n        annotations:\n            something-else: hey\n    \"\"\",\n    \"02-gpu\": \"\"\"\n    kind: Pod\n    metadata:\n        name: {{key}}\n    spec:\n        containers:\n        - name: notebook\n          resources:\n            limits:\n                nvidia.com/gpu: 1\n    \"\"\",\n}\n```\n\nThe values are first expanded via [jinja2 templates](https://jinja.palletsprojects.com/)\nbefore being passed to `kubectl patch`. `{{key}}` expands to the name of the resource\ncreated, and you should use it for all your modifications. In the `02-gpu` patch, `kubectl`\nknows to merge this with the existing notebook container instead of create a new container\nor replace all the existing values, because it knows there already exists a container with\nthe `name` property set to `notebook`. Hence it merges values provides in this patch with\nthe existing configuration for the container.\n\nPlease read the [`kubectl` documentation](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch)\nto understand how strategic merge patch works.\n\n### Additional per-user kubernetes resources with `resources`\n\nYou can also create arbitrary additional kubernetes resources for each user\nby setting the `resources` configuration. It's a dictionary where the key is used\nfor sorting, and the value should be valid YAML after expansion via jinja2 template.\n\nFor example, the following config creates a Kubernetes [Role and RoleBinding](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)\nfor each user to allow them to (insecurely!) run a [dask-kubernetes](https://kubernetes.dask.org/en/latest/)\ncluster.\n\n```python\nc.MultiClusterKubernetesSpawner.resources = {\n    \"10-dask-role\": \"\"\"\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: Role\n    metadata:\n      name: {{key}}-dask\n    rules:\n    - apiGroups:\n      - \"\"\n      resources:\n      - pods\n      verbs:\n      - list\n      - create\n      - delete\n    \"\"\",\n    \"11-dask-rolebinding\": \"\"\"\n    apiVersion: rbac.authorization.k8s.io/v1\n    kind: RoleBinding\n    metadata:\n      name: {{key}}-dask\n    roleRef:\n      apiGroup: rbac.authorization.k8s.io\n      kind: Role\n      name: {{key}}-dask\n    subjects:\n    - apiGroup: \"\"\n      kind: ServiceAccount\n      name: {{key}}\n    \"\"\",\n)\n```\n\nThis takes advantage of the fact that by default a Kubernetes\n[Service Account](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/)\nis already created for each pod by `MultiClusterUserSpawner`, and gives it just\nenough rights to create, list and delete pods.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/yuvipanda/jupyterhub-multicluster-kubespawner",
    "keywords": "",
    "license": "3-BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "jupyterhub-multicluster-kubespawner",
    "package_url": "https://pypi.org/project/jupyterhub-multicluster-kubespawner/",
    "platform": "",
    "project_url": "https://pypi.org/project/jupyterhub-multicluster-kubespawner/",
    "project_urls": {
      "Homepage": "https://github.com/yuvipanda/jupyterhub-multicluster-kubespawner",
      "Source": "https://github.com/yuvipanda/jupyterhub-multicluster-kubespawner",
      "Tracker": "https://github.com/yuvipanda/jupyterhub-multicluster-kubespawner/issues"
    },
    "release_url": "https://pypi.org/project/jupyterhub-multicluster-kubespawner/0.2/",
    "requires_dist": [
      "escapism",
      "jupyterhub (>=1.5)",
      "jinja2",
      "ruamel.yaml",
      "traitlets"
    ],
    "requires_python": ">=3.9",
    "summary": "JupyterHub Spawner for spawning into multiple kubernetes clusters",
    "version": "0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12600486,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b8c91fc9fe8f539e717a92f534c3870c5c9f2abdef3b21299db800125d164952",
          "md5": "74104b5339143ef13868bf48f35d99d7",
          "sha256": "07fa5ce77401a968c75fd4800f72b9b02c707c856101c562895db1f86ba34d51"
        },
        "downloads": -1,
        "filename": "jupyterhub_multicluster_kubespawner-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "74104b5339143ef13868bf48f35d99d7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 16814,
        "upload_time": "2022-01-17T20:00:57",
        "upload_time_iso_8601": "2022-01-17T20:00:57.071587Z",
        "url": "https://files.pythonhosted.org/packages/b8/c9/1fc9fe8f539e717a92f534c3870c5c9f2abdef3b21299db800125d164952/jupyterhub_multicluster_kubespawner-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "678a7ab34c0a5937f7c0e31051c5b58165f8d1a7d87203efd94708333a814b53",
          "md5": "638134ad56999b6537d24c5257bb350b",
          "sha256": "c4bdcbe55e54264726a6726b4cb895bb83bbb66901299eb9fad2c6842de41c4e"
        },
        "downloads": -1,
        "filename": "jupyterhub-multicluster-kubespawner-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "638134ad56999b6537d24c5257bb350b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 15811,
        "upload_time": "2022-01-17T20:00:59",
        "upload_time_iso_8601": "2022-01-17T20:00:59.681462Z",
        "url": "https://files.pythonhosted.org/packages/67/8a/7ab34c0a5937f7c0e31051c5b58165f8d1a7d87203efd94708333a814b53/jupyterhub-multicluster-kubespawner-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "861b768d740128ae55201e9c56040f5a1a14d3941c5bb7dea42e9d2702308079",
          "md5": "a812a301bb594115457637638190a08b",
          "sha256": "4ab91e6e7d251098e5bd28ec405a7fe8183d2ed6a3b6d2c7f2f92c775badca41"
        },
        "downloads": -1,
        "filename": "jupyterhub_multicluster_kubespawner-0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a812a301bb594115457637638190a08b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 16867,
        "upload_time": "2022-01-17T21:20:32",
        "upload_time_iso_8601": "2022-01-17T21:20:32.049995Z",
        "url": "https://files.pythonhosted.org/packages/86/1b/768d740128ae55201e9c56040f5a1a14d3941c5bb7dea42e9d2702308079/jupyterhub_multicluster_kubespawner-0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "93ee934a93116bfc60afd8abfaf01141d66d48517fe92683157d57cc93527588",
          "md5": "5740f5d2c57ec85a13e4efac9130c1e9",
          "sha256": "74cc2f4d55c819394351a206a0a0cc553a88dedc28be06bc74b37d2d86343921"
        },
        "downloads": -1,
        "filename": "jupyterhub-multicluster-kubespawner-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "5740f5d2c57ec85a13e4efac9130c1e9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 15866,
        "upload_time": "2022-01-17T21:20:34",
        "upload_time_iso_8601": "2022-01-17T21:20:34.048001Z",
        "url": "https://files.pythonhosted.org/packages/93/ee/934a93116bfc60afd8abfaf01141d66d48517fe92683157d57cc93527588/jupyterhub-multicluster-kubespawner-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "861b768d740128ae55201e9c56040f5a1a14d3941c5bb7dea42e9d2702308079",
        "md5": "a812a301bb594115457637638190a08b",
        "sha256": "4ab91e6e7d251098e5bd28ec405a7fe8183d2ed6a3b6d2c7f2f92c775badca41"
      },
      "downloads": -1,
      "filename": "jupyterhub_multicluster_kubespawner-0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a812a301bb594115457637638190a08b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.9",
      "size": 16867,
      "upload_time": "2022-01-17T21:20:32",
      "upload_time_iso_8601": "2022-01-17T21:20:32.049995Z",
      "url": "https://files.pythonhosted.org/packages/86/1b/768d740128ae55201e9c56040f5a1a14d3941c5bb7dea42e9d2702308079/jupyterhub_multicluster_kubespawner-0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "93ee934a93116bfc60afd8abfaf01141d66d48517fe92683157d57cc93527588",
        "md5": "5740f5d2c57ec85a13e4efac9130c1e9",
        "sha256": "74cc2f4d55c819394351a206a0a0cc553a88dedc28be06bc74b37d2d86343921"
      },
      "downloads": -1,
      "filename": "jupyterhub-multicluster-kubespawner-0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "5740f5d2c57ec85a13e4efac9130c1e9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.9",
      "size": 15866,
      "upload_time": "2022-01-17T21:20:34",
      "upload_time_iso_8601": "2022-01-17T21:20:34.048001Z",
      "url": "https://files.pythonhosted.org/packages/93/ee/934a93116bfc60afd8abfaf01141d66d48517fe92683157d57cc93527588/jupyterhub-multicluster-kubespawner-0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}