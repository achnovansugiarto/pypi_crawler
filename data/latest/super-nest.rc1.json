{
  "info": {
    "author": "Aleksandr Petrosyan",
    "author_email": "a-p-petrosyan@yandex.ru",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# SSPR\n\nStochastic superpositional posterior repartitioning used in\nconjunction with Bayesian inference engines such as PolyChord and\nMultiNest.\n\n# What is Bayesian inference\n\nBayesian inference is a suitable framework for model fitting.  You\ngive it a theory and some data and it tells you how well the theory\nfits the data, while also telling you what the theory's parameters\nshould be for the fit to be the best.\n\n# Installation\n\nThe preferred way is to use \n```\npip install super-nest\n```\n\nthough other packages will be developed as needed.\n\nIf you know what you're doing you can clone this repository and\ninstall manually, using\n\n```\ncd super-nest && python3 setup.py\n```\n\nYou also need to have either\n[PolyChord](https://github.com/PolyChord/PolyChordLite) or\n[MultiNest](https://github.com/farhanferoz/MultiNest) (or both)\ninstalled.\n# Usage\n\nThis is planned as a simple convenient package that you use with a sampler. \n\nSuppose that you had a set-up with `PolyChord` that involved a prior\nquantile `prior` a log-likelihood function, `loglike` all of which\noperated on an `nDims` dimensional space. \n\nOften you'd be using a uniform prior. You're sure that using a\ndifferent prior would cause an imprint, which is fine if that prior\nwas based on a previous run, but not so if you just used it as a\nhunch. As hunch is used liberally here, you could have an intuition,\nor you could have done a run using a different model and extrapolated\nthe posterior. \n\nPreviously all such information could not be used. Nested sampling\nwould return all of that information to you and you wouldn't be able\nto tell, if this is actually what the data suggests, or that you\nsimply gave it too much info out of thin air.\n\nWell, now you can use that information and get your sampling to run\nfaster, but to produce the output you would have gotten had you used a\nuniform prior (and a lot more live points).\n\n## General overview.\nSuppose you had a unfirom prior `pi` and likelihood `l`. To use them\nin nested sampling, you need a prior quantile, and the logarithm of\nthe likelihood. You need to pass both to the sampler, e.g.\n```\n# This is pseudo-code.\nsettings = PolyChordSettings(nDims, nDerived, ...)\nrun_polychord(log(l), functional_inverse(cumulative_dist(pi)), nDims,\nnDerived ...)  \n``` \nif the prior is uniform (a function that maps to a\nconstant), you expect the quantile to be dependent on the boundaries\nof your prior space. If you have a box with corners at `a` and `b`,\nthen the uniform prior quantile is:\n```\ndef quantile(cube):\n\treturn a + (b-a) * cube\n```\n\nLet's say you have a hunch that the posterior would be a Gaussian at\n`mu` with covariance matrix `Sigma`. How do you formulate it in such a\nway that supernest can understand?\n\nWell, for starters, the prior quantile would look like\n```\ndef gaussian_quantile(cube):\n\tdef _erf_term(d, g):\n\t\t\treturn erf(d * sqrt(1 / 2) / g)\n\tsigma = diag(m.cov)\n    da = _erf_term(a - mu, sigma)\n    db = _erf_term(b - mu, sigma)\n    ret = erfinv((1 - cube) * da + cube * db)\n    return mu + sqrt(2 / beta) * sigma * ret\n```\nwhich is already quite a mouthful. If you tried to use this quantile directly, you would have a few problems:\n\n1) Your posterior will have a Gaussian imprint at `mu` and `Sigma`,\nwhether or not you want it.\n2) If most of your posterior is far away, it would have been picked up by the uniform prior, but not the one you\nhave.\n3) You would get the wrong (larger) evidence than you would with a uniform prior. \n\nSo what do you do?\n\n## Creating a proposal. \n\nYou create a consistent proposal. This means that you need to change the likelihood specifically, you need to make it so that the product of the prior pdf and the new likelihood is the same as the original uniform prior times the original `log(l)`. \n\nSo what do you do for a Gaussian? Well, it's easy:\n\n```\ndef logl_gaussian(theta):\n\tll=0\n\tdef ln_z(t, b):\n        ret = - b * (t - mu) ** 2 / 2 / sigma ** 2\n        ret -= log(pi * sigma ** 2 / 2 / b) / 2\n        db = _erf_term(b - mu, sigma)\n        da = _erf_term(a - mu, sigma)\n        ret -= log(db - da)\n        return ret\n\n\tll -= log(b-a).sum()\n    ll -= ln_z(theta, beta).sum()\n\n    return ll\n```\n\nAnd then you pass it and it works? **Wrong**. This will help you with\nthe wrong evidence, but you would still get an imprint in the\nposterior, and sampling far away would still be an issue.\n\n## Using this package. \nTo solve the problem you need to put both your original prior and\nlikelihood and the newly created gaussian proposa. into a\nsuperposition. This what this package does, and this is what this package is. \n\nSo to use the things that we've defined previously, we should do the\nfollowing: (I'm assuming polyChord, but this works with **any** nested\nsampler).\n\n```\nfrom polychord import run_polychord\nfrom polychord.settings import PolyChordSettings\nfrom supernest import superimpose\n\n\ndef uniform_quantile(cube):\n\t...\n\n\ndef uniform_like(theta):\n\t...\n\treturn log(l), derived\n\n\ndef gaussian_quantile(cube):\n\t...\n\n\n# In later versions, you will have a function that generates the correctsion for you. \ndef gaussian_like(theta):\n    ...\n\n# Note taht you can have as many proposals as you like. \nn_dims, prior, log_like = superimpose([(uniform_prior, uniform_like), (gaussian_prior, gaussian_like), ...], nDims)\n\n...\n\n# Be wary of using grade_dims and grade_frac. If you have n models, you should \n# grade_dims.append(n)\n# grade_frac.append([1.0]*n)\n# before passing them to settings. \n\nsettings = PolyChordSettings(n_dims,nDerived, ...)\n\n...\n\nrun_polychord(prior, n_dims, nDerived, log_like, ...)\n```\n## What do I get in return for my work?\n\nA run-time reduction starting at 37x. A precision increase of about\n100x. The posterior you get should contain all the usual stuff, plus\nsome extra parameters at the end, which tell you how good your\nproposals were. These should be interpreted as probabilities that the\nproposal described the posterior.\n\n## Is this the best that superpositional mixtures have to offer. \n\nNot at all. We're writing a purpose built sampler that eliminates all\noutward appearances of using superposition under the hood, while also\nmaking use of some clever tricks, that you normally can't\ndo. Effectively we're creating a quantum computer of nested samplers.\n\n## Is there a neater way? \n\nYes. If you know exactly what you want, you can (optionally) use the ``super_nest.framework`` module. This is a more class oriented interface that's much nicer to deal with, as it infers the dimensionality of the problem and is far simpler to use than the non-OOP direct interface. \n\nSo in that case, you would do something like \n\n```\nfrom super_nest.framework.gaussian_models.uniform import BoxUniformModel\n\nmdl = BoxUniformModel((a, b), mu, cov, file_root='Uniform')\n```\nwhich creates a model with a box uniform prior which has corners at ``a`` and `b`, and a single Gaussian peak at `mu` with covariance `cov`. \n\nWhy would you want to use this? Well, how about if you wanted to create a mixture of this model, along with a custom model, that you defined as a class? \n\n```\nfrom super_nest.framework.polychord_model import Model\n\nclass MyCustomModel(Model):\n\tdef __init__(self, *args, **kwargs):\n\t\t# get some stuff you need\n\t\tpass\n\n\tdef log_likelihood(self, theta):\n\t\treturn myCustomLogLike(theta)\n\n\tdef quantiel(self, hypercube):\n\t\treturn myPriorQuantile(hypercube)\n\n\t@property\n\tdef dimensionality(self):\n\t\treturn nDims\n\n\t@property\n\tdef num_derived(self):\n\t\treturn nDerived\n\n```\n\nWhich is admittedly more verbose, than just using base\n`super_nest.superimpose`. The reason why you'd want to do that, is\nthat this will automatically track the dimensionality for you, create\nversions of ``loglike`` and prior quantile that correspond to what you\nwant, and allows you to do some non-standard posterior repartitioning,\nthat would otherwise be a headache to deal with.\n\nTL;DR, you could just do things like\n```\nfrom super_nest.framework.mixture import StochasticMixtureModel\nfrom super_nest.framework.gaussian_models import (GaussianPeakedPrior,\n                                                  Uniform,\n                                                  PowerPosteriorPrior)\n\nuniform = Uniform((a,b), mu[0], cov[0], ...)\ngaussian = GaussianPeakedPrior((a,b), mu[1], cov[1], ...)\nppr = PowerPosteriorPrior((a,b), mu[2], cov[2],...)\n\nmix = StochasticMixtureModel([uniform, gaussian, ppr])\n\n...\n\nmix.nested_sample()\n```\n\nwhich for larger projects where the off by one error doesn't always\ncause a segfault, but is something that you discover several HPC hours\nlater, is a necessity. I created this, despite having a decent-enough\ninterface to PolyChord, because the situation of a Gaussian proposal\nmixed with some arbitrary code is a common use case. And in some\ncircumstances (e.g. where you have no idea what the confidence\nintervals might be), you could prefer to use **power posterior\nrepartitioning**, inside of a stochastic mixture. \n\nYou can do it with just `super_nest.superimpose`, but it would be a\nmuch bigger hassle. The domain of Bayesian inference has many common\npatters, so it really does pay to use a slightly more complicated\nsystem, but not to worry about \"did I get the linear transformation\nright?\" kinds of questions. This is MIT licensed, so why bother\nimplementing something in your own code, when you can just create a PR\n(I promise I don't bite).\n\n\nUnfortunately this framework only works with PolyChord (yet), but work\nis in place to make it universal. The idea is that different samplers\nhave different use-cases, and ideally the `super_nest.framework` would\nchoose the right tool for the right job.  Specifically, there's work\nplanned to include support for ``PyMultiNest``, as it's much faster for\nlow-dimensional inference problems (it scales exponentially, so low\nreally means no more than five parameters). In some cases, when the\nlog-likelihood evaluation is **very slow**, it would make sense to use\nsomething like ``dynesty``, while pure-python has its drawbacks (the\nonly thing this language is good at is racking massive technical debt,\nthat you hope never to reap), ``dynesty`` is a dynamic sampler,\nmeaning that it can potentially outperform any of the ones on the list\n(there's also `dypolychord`, which is a very well-made package). \n\nIt will likely be the chief influence on the API for the next\ngeneration dynamic superpositional trans-dimensional sampler, so\ngetting to know this API might be worth the extra time.\n\n# Does this support the X sampler. \nYes. This is a very thin module, and while I have assumed a\n`PolyChord`-like calling convention (also `dynesty`), you can use it\nwith other samplers that support using python functions as callbacks.\n\n## Pymultinest\nVery similar to PolyChord, (and also quite popular). Use as follows:\n```\nfrom super_nest import superimpose\n\nn_dims, my_prior_quantile, my_likelihood = superimpose(\n    [(prior1, like1), (proposal_prior1, proposal_like1)],\n\tnDims)\nsolve(LogLikelihood=my_likelihood, Prior=my_prior_quantile, n_dims=n_dims)\n```\n\n## dynesty\n\nHas the same standard interface. \n\n## MultinestAE\n\nIF you want to use `Multinest`, you should wait for a FORTRAN version of\nthis package to use directly with MultiNest. If you want to use\nMultiNest with `super_nest` consider `pymultinest` a hard\nrequirement for now. \n\n\n# What is this useful for \n\nSuppose that you were running a complex Bayesian inference,\ne.g. [Cobaya](https://cobaya.readthedocs.io/en/latest/). You have a\nchoice of sampler, e.g. Monte-Carlo, Metropolis Hastings or you could\nchoose to use Nested Sampling, among a family of other inference\nmethods. If you choose the former, you get a good idea of what the\nmodel parameters should be quick, but you have no idea how good the\nfit is, because MCMC and MH don't evaluate the evidence. You think to\nuse Nested Sampling, but that takes too long, and you can't give it\nmore information to run faster... At least not without this package.\n\nThis is a thin wrapper around both PolyChord and Multinest's code,\nthat allows you to specify a proposal distribution. For now it's\nmainly a multivariate correlated Gaussian, but other distributions are\nbeing planned.\n\n## How much faster/more precise can this make inference? \n\nOur preliminary academic benchmarks showed a runtime reduction by a\nfactor of 20. Your mileage may vary, but if you're willing to\nsacrifice some precision, you can make it go even faster.\n\nIf you really want the extra precision, you can expect an uplift by\ntwo orders of magnitude, if you started with a uniform prior (as we\noften do in nested sampling) and used the posterior chains to generate\nthe distribution.\n\n\n# License - LGPLv3\nTL;DR; \n\nIt's a permissive license that's GPL compatible. if you want a deeper\nunderstanding: (https://www.gnu.org/licenses/lgpl-3.0.en.html)[go\nhere].\n# Contributing. \nThis is a python package. You don't normally need much beyond creating\npull requests, for features that you think should be in. \n\nI don't care about Pep8. It's a misguided collection of near-sighted\npractices, that are emblematic of the problems python has. Namely,\nthat it's an interpreted language that's very hard to read for a\ncomputer, making python slower than it has to be. \n\nSo this means that \n- if you have a proposal, and it's not pep8. It's fine. \n- if you have a proposal and it's pep8, it's also fine. \n- if you have a proposal that's just about making the code pep8. It's fine. \n\nI don't bite.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.com/a-p-petrosyan/sspr",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "super-nest",
    "package_url": "https://pypi.org/project/super-nest/",
    "platform": "",
    "project_url": "https://pypi.org/project/super-nest/",
    "project_urls": {
      "Homepage": "https://gitlab.com/a-p-petrosyan/sspr"
    },
    "release_url": "https://pypi.org/project/super-nest/1.0.0/",
    "requires_dist": [
      "anesthetic",
      "pypolychord",
      "numpy",
      "matplotlib"
    ],
    "requires_python": ">=3.6",
    "summary": "A wrapper for use of SSPR in nested sampling packages such as PolyChord and Multinest",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7613869,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4899a220fe36480496a57d4b1bb16d026bd0946e95dc57e0b244187ee5f1e611",
          "md5": "b91509d222dc14a88673b4f19d4e0331",
          "sha256": "e072f0998ac695ae91e68f63973fed679377e931fcd0fec79672859f8a9c0241"
        },
        "downloads": -1,
        "filename": "super_nest-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b91509d222dc14a88673b4f19d4e0331",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 22492,
        "upload_time": "2020-07-02T11:16:25",
        "upload_time_iso_8601": "2020-07-02T11:16:25.563587Z",
        "url": "https://files.pythonhosted.org/packages/48/99/a220fe36480496a57d4b1bb16d026bd0946e95dc57e0b244187ee5f1e611/super_nest-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9cf3a0081d6f197ce327d951dbe465c915293934e4d33c058b595193a69231cd",
          "md5": "82b5424fc9dbbff0421efa3034916d8e",
          "sha256": "6d141e21993dbff47f7ca3075594341623034b60cdf8117be9f87a0337ab7054"
        },
        "downloads": -1,
        "filename": "super-nest-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "82b5424fc9dbbff0421efa3034916d8e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 12253,
        "upload_time": "2020-07-02T11:16:29",
        "upload_time_iso_8601": "2020-07-02T11:16:29.617212Z",
        "url": "https://files.pythonhosted.org/packages/9c/f3/a0081d6f197ce327d951dbe465c915293934e4d33c058b595193a69231cd/super-nest-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4899a220fe36480496a57d4b1bb16d026bd0946e95dc57e0b244187ee5f1e611",
        "md5": "b91509d222dc14a88673b4f19d4e0331",
        "sha256": "e072f0998ac695ae91e68f63973fed679377e931fcd0fec79672859f8a9c0241"
      },
      "downloads": -1,
      "filename": "super_nest-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b91509d222dc14a88673b4f19d4e0331",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 22492,
      "upload_time": "2020-07-02T11:16:25",
      "upload_time_iso_8601": "2020-07-02T11:16:25.563587Z",
      "url": "https://files.pythonhosted.org/packages/48/99/a220fe36480496a57d4b1bb16d026bd0946e95dc57e0b244187ee5f1e611/super_nest-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9cf3a0081d6f197ce327d951dbe465c915293934e4d33c058b595193a69231cd",
        "md5": "82b5424fc9dbbff0421efa3034916d8e",
        "sha256": "6d141e21993dbff47f7ca3075594341623034b60cdf8117be9f87a0337ab7054"
      },
      "downloads": -1,
      "filename": "super-nest-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "82b5424fc9dbbff0421efa3034916d8e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 12253,
      "upload_time": "2020-07-02T11:16:29",
      "upload_time_iso_8601": "2020-07-02T11:16:29.617212Z",
      "url": "https://files.pythonhosted.org/packages/9c/f3/a0081d6f197ce327d951dbe465c915293934e4d33c058b595193a69231cd/super-nest-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}