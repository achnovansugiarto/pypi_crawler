{
  "info": {
    "author": "Alex Immer",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "<div align=\"center\">\n\t<img src=\"https://raw.githubusercontent.com/AlexImmer/Laplace/main/logo/laplace_logo.png\" alt=\"Laplace\" width=\"300\"/>\n</div>\n\n[![Main](https://travis-ci.com/AlexImmer/Laplace.svg?token=rpuRxEjQS6cCZi7ptL9y&branch=main)](https://travis-ci.com/AlexImmer/Laplace)\n\nThe laplace package facilitates the application of Laplace approximations for entire neural networks, subnetworks of neural networks, or just their last layer.\nThe package enables posterior approximations, marginal-likelihood estimation, and various posterior predictive computations.\nThe library documentation is available at [https://aleximmer.github.io/Laplace](https://aleximmer.github.io/Laplace).\n\nThere is also a corresponding paper, [*Laplace Redux â€” Effortless Bayesian Deep Learning*](https://arxiv.org/abs/2106.14806), which introduces the library, provides an introduction to the Laplace approximation, reviews its use in deep learning, and empirically demonstrates its versatility and competitiveness. Please consider referring to the paper when using our library:\n```bibtex\n@inproceedings{laplace2021,\n  title={Laplace Redux--Effortless {B}ayesian Deep Learning},\n  author={Erik Daxberger and Agustinus Kristiadi and Alexander Immer \n          and Runa Eschenhagen and Matthias Bauer and Philipp Hennig},\n  booktitle={{N}eur{IPS}},\n  year={2021}\n}\n```\n\n## Setup\n\nWe assume `python3.8` since the package was developed with that version.\nTo install laplace with `pip`, run the following:\n```bash\npip install laplace-torch\n```\n\nFor development purposes, clone the repository and then install:\n```bash\n# or after cloning the repository for development\npip install -e .\n# run tests\npip install -e .[tests]\npytest tests/\n```\n\n## Structure\nThe laplace package consists of two main components:\n\n1. The subclasses of [`laplace.BaseLaplace`](https://github.com/AlexImmer/Laplace/blob/main/laplace/baselaplace.py) that implement different sparsity structures: different subsets of weights (`'all'`, `'subnetwork'` and `'last_layer'`) and different structures of the Hessian approximation (`'full'`, `'kron'`, `'lowrank'` and `'diag'`). This results in _eight_ currently available options: `laplace.FullLaplace`, `laplace.KronLaplace`, `laplace.DiagLaplace`, the corresponding last-layer variations `laplace.FullLLLaplace`, `laplace.KronLLLaplace`,  and `laplace.DiagLLLaplace` (which are all subclasses of [`laplace.LLLaplace`](https://github.com/AlexImmer/Laplace/blob/main/laplace/lllaplace.py)), [`laplace.SubnetLaplace`](https://github.com/AlexImmer/Laplace/blob/main/laplace/subnetlaplace.py) (which only supports a `'full'` Hessian approximation) and `laplace.LowRankLaplace` (which only supports inference over `'all'` weights). All of these can be conveniently accessed via the [`laplace.Laplace`](https://github.com/AlexImmer/Laplace/blob/main/laplace/laplace.py) function.\n2. The backends in [`laplace.curvature`](https://github.com/AlexImmer/Laplace/blob/main/laplace/curvature/) which provide access to Hessian approximations of\nthe corresponding sparsity structures, for example, the diagonal GGN.\n\nAdditionally, the package provides utilities for\ndecomposing a neural network into feature extractor and last layer for `LLLaplace` subclasses ([`laplace.utils.feature_extractor`](https://github.com/AlexImmer/Laplace/blob/main/laplace/utils/feature_extractor.py))\nand\neffectively dealing with Kronecker factors ([`laplace.utils.matrix`](https://github.com/AlexImmer/Laplace/blob/main/laplace/utils/matrix.py)).\n\nFinally, the package implements several options to select/specify a subnetwork for `SubnetLaplace` (as subclasses of [`laplace.utils.subnetmask.SubnetMask`](https://github.com/AlexImmer/Laplace/blob/main/laplace/utils/subnetmask.py)).\nAutomatic subnetwork selection strategies include: uniformly at random (`laplace.utils.subnetmask.RandomSubnetMask`), by largest parameter magnitudes (`LargestMagnitudeSubnetMask`), and by largest marginal parameter variances (`LargestVarianceDiagLaplaceSubnetMask` and `LargestVarianceSWAGSubnetMask`).\nIn addition to that, subnetworks can also be specified manually, by listing the names of either the model parameters (`ParamNameSubnetMask`) or modules (`ModuleNameSubnetMask`) to perform Laplace inference over.\n\n## Extendability\nTo extend the laplace package, new `BaseLaplace` subclasses can be designed, for example,\nLaplace with a block-diagonal Hessian structure.\nOne can also implement custom subnetwork selection strategies as new subclasses of `SubnetMask`.\n\nAlternatively, extending or integrating backends (subclasses of [`curvature.curvature`](https://github.com/AlexImmer/Laplace/blob/main/laplace/curvature/curvature.py)) allows to provide different Hessian\napproximations to the Laplace approximations.\nFor example, currently the [`curvature.BackPackInterface`](https://github.com/AlexImmer/Laplace/blob/main/laplace/curvature/backpack.py) based on [BackPACK](https://github.com/f-dangel/backpack/) and [`curvature.AsdlInterface`](https://github.com/AlexImmer/Laplace/blob/main/laplace/curvature/asdl.py) based on [ASDL](https://github.com/kazukiosawa/asdfghjkl) are available.\nThe `curvature.AsdlInterface` provides a Kronecker factored empirical Fisher while the `curvature.BackPackInterface`\ndoes not, and only the `curvature.BackPackInterface` provides access to Hessian approximations\nfor a regression (MSELoss) loss function.\n\n## Example usage\n\n### *Post-hoc* prior precision tuning of diagonal LA\n\nIn the following example, a pre-trained model is loaded,\nthen the Laplace approximation is fit to the training data\n(using a diagonal Hessian approximation over all parameters),\nand the prior precision is optimized with cross-validation `'CV'`.\nAfter that, the resulting LA is used for prediction with\nthe `'probit'` predictive for classification.\n\n```python\nfrom laplace import Laplace\n\n# Pre-trained model\nmodel = load_map_model()  \n\n# User-specified LA flavor\nla = Laplace(model, 'classification',\n             subset_of_weights='all',\n             hessian_structure='diag')\nla.fit(train_loader)\nla.optimize_prior_precision(method='CV', val_loader=val_loader)\n\n# User-specified predictive approx.\npred = la(x, link_approx='probit')\n```\n\n### Differentiating the log marginal likelihood w.r.t. hyperparameters\n\nThe marginal likelihood can be used for model selection [10] and is differentiable\nfor continuous hyperparameters like the prior precision or observation noise.\nHere, we fit the library default, KFAC last-layer LA and differentiate\nthe log marginal likelihood.\n\n```python\nfrom laplace import Laplace\n\n# Un- or pre-trained model\nmodel = load_model()  \n\n# Default to recommended last-layer KFAC LA:\nla = Laplace(model, likelihood='regression')\nla.fit(train_loader)\n\n# ML w.r.t. prior precision and observation noise\nml = la.log_marginal_likelihood(prior_prec, obs_noise)\nml.backward()\n```\n\n### Applying the LA over only a subset of the model parameters\n\nThis example shows how to fit the Laplace approximation over only\na subnetwork within a neural network (while keeping all other parameters\nfixed at their MAP estimates), as proposed in [11]. It also exemplifies\ndifferent ways to specify the subnetwork to perform inference over.\n\n```python\nfrom laplace import Laplace\n\n# Pre-trained model\nmodel = load_model()\n\n# Examples of different ways to specify the subnetwork\n# via indices of the vectorized model parameters\n#\n# Example 1: select the 128 parameters with the largest magnitude\nfrom laplace.utils import LargestMagnitudeSubnetMask\nsubnetwork_mask = LargestMagnitudeSubnetMask(model, n_params_subnet=128)\nsubnetwork_indices = subnetwork_mask.select()\n\n# Example 2: specify the layers that define the subnetwork\nfrom laplace.utils import ModuleNameSubnetMask\nsubnetwork_mask = ModuleNameSubnetMask(model, module_names=['layer.1', 'layer.3'])\nsubnetwork_mask.select()\nsubnetwork_indices = subnetwork_mask.indices\n\n# Example 3: manually define the subnetwork via custom subnetwork indices\nimport torch\nsubnetwork_indices = torch.tensor([0, 4, 11, 42, 123, 2021])\n\n# Define and fit subnetwork LA using the specified subnetwork indices\nla = Laplace(model, 'classification',\n             subset_of_weights='subnetwork',\n             hessian_structure='full',\n             subnetwork_indices=subnetwork_indices)\nla.fit(train_loader)\n```\n\n## Documentation\n\nThe documentation is available [here](https://aleximmer.github.io/Laplace) or can be generated and/or viewed locally:\n\n```bash\n# assuming the repository was cloned\npip install -e .[docs]\n# create docs and write to html\nbash update_docs.sh\n# .. or serve the docs directly\npdoc --http 0.0.0.0:8080 laplace --template-dir template\n```\n\n## References\n\nThis package relies on various improvements to the Laplace approximation for neural networks, which was originally due to MacKay [1]. Please consider citing the respective papers if you use any of their proposed methods via our laplace library.\n\n- [1] MacKay, DJC. [*A Practical Bayesian Framework for Backpropagation Networks*](https://authors.library.caltech.edu/13793/). Neural Computation 1992.\n- [2] Gibbs, M. N. [*Bayesian Gaussian Processes for Regression and Classification*](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.1130&rep=rep1&type=pdf). PhD Thesis 1997.\n- [3] Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., Prabhat, M., Adams, R. [*Scalable Bayesian Optimization Using Deep Neural Networks*](https://arxiv.org/abs/1502.05700). ICML 2015.\n- [4] Ritter, H., Botev, A., Barber, D. [*A Scalable Laplace Approximation for Neural Networks*](https://openreview.net/forum?id=Skdvd2xAZ). ICLR 2018.\n- [5] Foong, A. Y., Li, Y., HernÃ¡ndez-Lobato, J. M., Turner, R. E. [*'In-Between' Uncertainty in Bayesian Neural Networks*](https://arxiv.org/abs/1906.11537). ICML UDL Workshop 2019.\n- [6] Khan, M. E., Immer, A., Abedi, E., Korzepa, M. [*Approximate Inference Turns Deep Networks into Gaussian Processes*](https://arxiv.org/abs/1906.01930). NeurIPS 2019.\n- [7] Kristiadi, A., Hein, M., Hennig, P. [*Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks*](https://arxiv.org/abs/2002.10118). ICML 2020.\n- [8] Immer, A., Korzepa, M., Bauer, M. [*Improving predictions of Bayesian neural nets via local linearization*](https://arxiv.org/abs/2008.08400). AISTATS 2021.\n- [9] Sharma, A., Azizan, N., Pavone, M. [*Sketching Curvature for Efficient Out-of-Distribution Detection for Deep Neural Networks*](https://arxiv.org/abs/2102.12567). UAI 2021.\n- [10] Immer, A., Bauer, M., Fortuin, V., RÃ¤tsch, G., Khan, EM. [*Scalable Marginal Likelihood Estimation for Model Selection in Deep Learning*](https://arxiv.org/abs/2104.04975). ICML 2021.\n- [11] Daxberger, E., Nalisnick, E., Allingham, JU., AntorÃ¡n, J., HernÃ¡ndez-Lobato, JM. [*Bayesian Deep Learning via Subnetwork Inference*](https://arxiv.org/abs/2010.14689). ICML 2021.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/AlexImmer/Laplace",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "laplace-torch",
    "package_url": "https://pypi.org/project/laplace-torch/",
    "platform": "any",
    "project_url": "https://pypi.org/project/laplace-torch/",
    "project_urls": {
      "Bug Tracker": "https://github.com/AlexImmer/Laplace/issues",
      "Homepage": "https://github.com/AlexImmer/Laplace"
    },
    "release_url": "https://pypi.org/project/laplace-torch/0.1a2/",
    "requires_dist": [
      "torch",
      "torchvision",
      "torchaudio",
      "backpack-for-pytorch",
      "asdfghjkl",
      "matplotlib ; extra == 'docs'",
      "pdoc3 ; extra == 'docs'",
      "pytest ; extra == 'tests'",
      "pytest-cov ; extra == 'tests'",
      "coveralls ; extra == 'tests'",
      "scipy ; extra == 'tests'"
    ],
    "requires_python": ">=3.8",
    "summary": "laplace - Laplace approximations for deep learning",
    "version": "0.1a2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12554576,
  "releases": {
    "0.1a1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ed6266866e6f08975cfa928addfd8b49b0373eb4c190895e06130f1df0b76ac4",
          "md5": "6d7efe696466e06af9ef33c81ccb6f30",
          "sha256": "6088054c0e1d807c0787d8bf548d9b19a84bada0a7da520d3bd4e74e2347c786"
        },
        "downloads": -1,
        "filename": "laplace_torch-0.1a1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6d7efe696466e06af9ef33c81ccb6f30",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 30486,
        "upload_time": "2021-07-24T13:57:08",
        "upload_time_iso_8601": "2021-07-24T13:57:08.122835Z",
        "url": "https://files.pythonhosted.org/packages/ed/62/66866e6f08975cfa928addfd8b49b0373eb4c190895e06130f1df0b76ac4/laplace_torch-0.1a1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8e33bcd054eee803e6c4bea20fbb6faf56e431aef0041e974d9af0c1e693b57a",
          "md5": "c396f5521d2d3fe1f34b9075bd4df2e8",
          "sha256": "022c534d707247fd22a23b3ff33fd65020b6f52ff1a461011b8ea25e67094886"
        },
        "downloads": -1,
        "filename": "laplace-torch-0.1a1.tar.gz",
        "has_sig": false,
        "md5_digest": "c396f5521d2d3fe1f34b9075bd4df2e8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 206569,
        "upload_time": "2021-07-24T13:57:14",
        "upload_time_iso_8601": "2021-07-24T13:57:14.103499Z",
        "url": "https://files.pythonhosted.org/packages/8e/33/bcd054eee803e6c4bea20fbb6faf56e431aef0041e974d9af0c1e693b57a/laplace-torch-0.1a1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1a2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "417af8141ffa6b19ffc92eb3afaf7f85f9f1218c79a689479578df4edda283a6",
          "md5": "b8d3a0087844aa8b7117621f35349b49",
          "sha256": "d4b512f6e423f83514532ef5419cccdee74cd5dfe31d0e8f95cf5f5460cee7a1"
        },
        "downloads": -1,
        "filename": "laplace_torch-0.1a2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b8d3a0087844aa8b7117621f35349b49",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 45556,
        "upload_time": "2022-01-12T20:47:22",
        "upload_time_iso_8601": "2022-01-12T20:47:22.998481Z",
        "url": "https://files.pythonhosted.org/packages/41/7a/f8141ffa6b19ffc92eb3afaf7f85f9f1218c79a689479578df4edda283a6/laplace_torch-0.1a2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4f1d806fc3f8a13b340611954d7afcecd3a6d9cf4c36f8e351ca48d482a525a1",
          "md5": "32dee5a7aee1fe84190630ee576c730a",
          "sha256": "9e9329ad7463eddd3a8c9c0b3647800a8f200886d941ba90a3b664739e4a8500"
        },
        "downloads": -1,
        "filename": "laplace-torch-0.1a2.tar.gz",
        "has_sig": false,
        "md5_digest": "32dee5a7aee1fe84190630ee576c730a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 196461,
        "upload_time": "2022-01-12T20:47:25",
        "upload_time_iso_8601": "2022-01-12T20:47:25.355628Z",
        "url": "https://files.pythonhosted.org/packages/4f/1d/806fc3f8a13b340611954d7afcecd3a6d9cf4c36f8e351ca48d482a525a1/laplace-torch-0.1a2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "417af8141ffa6b19ffc92eb3afaf7f85f9f1218c79a689479578df4edda283a6",
        "md5": "b8d3a0087844aa8b7117621f35349b49",
        "sha256": "d4b512f6e423f83514532ef5419cccdee74cd5dfe31d0e8f95cf5f5460cee7a1"
      },
      "downloads": -1,
      "filename": "laplace_torch-0.1a2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b8d3a0087844aa8b7117621f35349b49",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 45556,
      "upload_time": "2022-01-12T20:47:22",
      "upload_time_iso_8601": "2022-01-12T20:47:22.998481Z",
      "url": "https://files.pythonhosted.org/packages/41/7a/f8141ffa6b19ffc92eb3afaf7f85f9f1218c79a689479578df4edda283a6/laplace_torch-0.1a2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4f1d806fc3f8a13b340611954d7afcecd3a6d9cf4c36f8e351ca48d482a525a1",
        "md5": "32dee5a7aee1fe84190630ee576c730a",
        "sha256": "9e9329ad7463eddd3a8c9c0b3647800a8f200886d941ba90a3b664739e4a8500"
      },
      "downloads": -1,
      "filename": "laplace-torch-0.1a2.tar.gz",
      "has_sig": false,
      "md5_digest": "32dee5a7aee1fe84190630ee576c730a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 196461,
      "upload_time": "2022-01-12T20:47:25",
      "upload_time_iso_8601": "2022-01-12T20:47:25.355628Z",
      "url": "https://files.pythonhosted.org/packages/4f/1d/806fc3f8a13b340611954d7afcecd3a6d9cf4c36f8e351ca48d482a525a1/laplace-torch-0.1a2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}