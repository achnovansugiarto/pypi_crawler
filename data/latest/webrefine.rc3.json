{
  "info": {
    "author": "Edward Ross",
    "author_email": "edward@skeptric.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Natural Language :: English",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# webrefine\n> A workflow for refining web pages into useful datasets.\n\n\nRead the [full documentation](https://edwardjross.github.io/webrefine/)\n\n## Install\n\n`pip install webrefine`\n\n## How to use\n\nWe'll go through an example of getting some titles from my blog at [skeptric.com](https://skeptric.com).\n\nThe process consists of:\n\n* Defining Queries\n* Defining Extraction and Filters\n* Running the process\n\n### Querying data\n\nTo start we'll need some captures of my blog, and so we'll get them from the Internet Archive's Wayback Machine.\n\n```python\nfrom webrefine.query import WaybackQuery\n```\n\nWe could get some HTML pages:\n\n```python\nskeptric_wb = WaybackQuery('skeptric.com/*', start='2020', end='2020', mime='text/html')\nsample = list(skeptric_wb.query(limit=20))\n```\n\nWe can get some sample records\n\n```python\nsample[0]\n```\n\n\n\n\n    WaybackRecord(url='https://skeptric.com/', timestamp=datetime.datetime(2020, 11, 26, 6, 41, 2), mime='text/html', status=200, digest='WDYU3RU7ZMFFSZPAPE56PC4L3EK4FE3D')\n\n\n\n```python\nsample[1]\n```\n\n\n\n\n    WaybackRecord(url='https://skeptric.com/casper-2-to-3/', timestamp=datetime.datetime(2020, 11, 26, 7, 52, 8), mime='text/html', status=200, digest='3XDBGHY77ZEA2Z7IVBARXEQT6UDLYAL7')\n\n\n\nAnd view them on the Wayback Machine to work out how to get the information we want\n\n```python\nsample[1].preview()\n```\n\n\n\n\n<a href=\"http://web.archive.org/web/20201126075208/https://skeptric.com/casper-2-to-3/\">http://web.archive.org/web/20201126075208/https://skeptric.com/casper-2-to-3/</a>\n\n\n\nWe could also query CommonCrawl similarly with a `CommonCrawlQuery`.\nThis has more captures but takes a bit longer to run.\n\n```python\nfrom webrefine.query import CommonCrawlQuery\nskeptric_cc = CommonCrawlQuery('skeptric.com/*')\n```\n\nAnother option is to add local Warc Files (e.g. produced using [`warcio`](https://github.com/webrecorder/warcio) or `wget` with `warc` parameters)\n\n```python\nfrom webrefine.query import WarcFileQuery\ntest_data = '../resources/test/skeptric.warc.gz'\n\nskeptric_file_query = WarcFileQuery(test_data)\n```\n\n```python\n[r.url for r in skeptric_file_query.query()]\n```\n\n\n\n\n    ['https://skeptric.com/pagination-wayback-cdx/',\n     'https://skeptric.com/robots.txt',\n     'https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css',\n     'https://skeptric.com/',\n     'https://skeptric.com/about/',\n     'https://skeptric.com/tags/data',\n     'https://skeptric.com/tags/data/',\n     'https://skeptric.com/images/wayback_empty_returns.png',\n     'https://skeptric.com/searching-100b-pages-cdx',\n     'https://skeptric.com/searching-100b-pages-cdx/',\n     'https://skeptric.com/fast-web-data-workflow/',\n     'https://skeptric.com/key-web-captures/',\n     'https://skeptric.com/emacs-tempfile-hugo/']\n\n\n\n### Filtering and Extracting the Data\n\nFrom Inspecting some web results we can see that the titles are written like:\n\n```html\n<h1 class=\"post-full-title\">{TITLE}</h1>\n```\n\nIn a real example we'd parse the HTML, but for simplicity we'll extract it with a regular expression\n\n```python\nimport re\ndef skeptric_extract(content, record):\n    html = content.decode('utf-8')\n    title = next(re.finditer('<h1 class=\"post-full-title\">([^<]+)</h1>', html)).group(1)\n    return {\n        'title': title,\n        'url': record.url,\n        'timestamp': record.timestamp\n    }\n```\n\nWe can then test it on some content we fetch from the Wayback Machine\n\n```python\nskeptric_extract(sample[1].content, sample[1])\n```\n\n\n\n\n    {'title': 'Hugo Casper 2 to 3',\n     'url': 'https://skeptric.com/casper-2-to-3/',\n     'timestamp': datetime.datetime(2020, 11, 26, 7, 52, 8)}\n\n\n\nSome pages don't have it so we filter them out, and we remove duplicates\n\n```python\ndef skeptric_filter(records):\n    last_url = None\n    for record in records:\n        # Only use ok HTML captures\n        if record.mime != 'text/html' or record.status != 200:\n            continue\n        # Pages that are not articles (and so do not have a title)\n        if record.url == 'https://skeptric.com/' or '/tags/' in record.url:\n            continue\n        # Duplicates (using the fact that here the posts come in order)\n        if record.url == last_url:\n            continue\n        last_url = record.url\n        yield record\n```\n\n```python\n[r.url for r in skeptric_filter(sample)]\n```\n\n\n\n\n    ['https://skeptric.com/casper-2-to-3/',\n     'https://skeptric.com/common-crawl-index-athena/',\n     'https://skeptric.com/common-crawl-job-ads/',\n     'https://skeptric.com/considering-vscode/',\n     'https://skeptric.com/decorating-pandas-tables/',\n     'https://skeptric.com/drive-metrics/',\n     'https://skeptric.com/emacs-buffering/',\n     'https://skeptric.com/ngram-python/',\n     'https://skeptric.com/portable-custom-config/',\n     'https://skeptric.com/searching-100b-pages-cdx/',\n     'https://skeptric.com/text-meta-data-commoncrawl/']\n\n\n\n## Running the process\n\nNow we've written all the logic we need, we can collect it all in a process to run\n\n```python\nfrom webrefine.runners import Process\n```\n\n```python\nskeptric_process = Process(\n    queries=[skeptric_file_query,\n             # commented out to make faster\n             #skeptric_wb,\n             #skeptric_cc,\n          ],\n    filter=skeptric_filter,\n    steps = [skeptric_extract])\n```\n\nWe can wrap it in a runner and run it all with `.run`.\n\n```python\n%%time\nfrom webrefine.runners import RunnerMemory\ndata = list(RunnerMemory(skeptric_process).run())\ndata\n```\n\n    CPU times: user 290 ms, sys: 14.8 ms, total: 305 ms\n    Wall time: 304 ms\n\n\n\n\n\n    [{'title': 'Pagination in Internet Archive&#39;s Wayback Machine with CDX',\n      'url': 'https://skeptric.com/pagination-wayback-cdx/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 34)},\n     {'title': 'About Skeptric',\n      'url': 'https://skeptric.com/about/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 37)},\n     {'title': 'Searching 100 Billion Webpages Pages With Capture Index',\n      'url': 'https://skeptric.com/searching-100b-pages-cdx/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 39)},\n     {'title': 'Fast Web Dataset Extraction Worfklow',\n      'url': 'https://skeptric.com/fast-web-data-workflow/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 39)},\n     {'title': 'Unique Key for Web Captures',\n      'url': 'https://skeptric.com/key-web-captures/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 40)},\n     {'title': 'Hugo Readdir Error with Emacs',\n      'url': 'https://skeptric.com/emacs-tempfile-hugo/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 40)}]\n\n\n\nFor larger jobs `RunnerFile` is better which caches intermediate results to a file\n\n```python\n%%time\nfrom webrefine.runners import RunnerCached\n\ncache_path = './test_cache.sqlite'\n\ndata = list(RunnerCached(skeptric_process, path=cache_path).run())\ndata\n```\n\n    CPU times: user 252 ms, sys: 10.7 ms, total: 263 ms\n    Wall time: 286 ms\n\n\n\n\n\n    [{'title': 'Pagination in Internet Archive&#39;s Wayback Machine with CDX',\n      'url': 'https://skeptric.com/pagination-wayback-cdx/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 34)},\n     {'title': 'About Skeptric',\n      'url': 'https://skeptric.com/about/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 37)},\n     {'title': 'Searching 100 Billion Webpages Pages With Capture Index',\n      'url': 'https://skeptric.com/searching-100b-pages-cdx/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 39)},\n     {'title': 'Fast Web Dataset Extraction Worfklow',\n      'url': 'https://skeptric.com/fast-web-data-workflow/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 39)},\n     {'title': 'Unique Key for Web Captures',\n      'url': 'https://skeptric.com/key-web-captures/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 40)},\n     {'title': 'Hugo Readdir Error with Emacs',\n      'url': 'https://skeptric.com/emacs-tempfile-hugo/',\n      'timestamp': datetime.datetime(2021, 11, 26, 11, 28, 40)}]\n\n\n\n```python\nimport os\nos.unlink(cache_path)\n```\n\nNote that in the case of errors in the steps the process keeps going, and logs the errors\n\n```python\nskeptric_error_process = Process(\n    queries=[skeptric_file_query,\n             # commented out to make faster\n             #skeptric_wb,\n             #skeptric_cc,\n          ],\n    filter=lambda x: x,\n    steps = [skeptric_extract])\n```\n\n```python\ndata = list(RunnerMemory(skeptric_error_process).run())\n```\n\n    ERROR:root:Error processing WarcFileRecord(url='https://skeptric.com/robots.txt', timestamp=datetime.datetime(2021, 11, 26, 11, 28, 34), mime='text/html', status=404, path=PosixPath('../resources/test/skeptric.warc.gz'), offset=5804, digest='QRNGXIUXE4LAI3XR5RVATIUX5GTB33HX') at step skeptric_extract: \n    ERROR:root:Error processing WarcFileRecord(url='https://skeptric.com/style.main.min.5ea2f07be7e07e221a7112a3095b89d049b96c48b831f16f1015bf2d95d914e5.css', timestamp=datetime.datetime(2021, 11, 26, 11, 28, 35), mime='text/css', status=200, path=PosixPath('../resources/test/skeptric.warc.gz'), offset=7197, digest='LINCDTSPQGAQGZZ6LY2XFXZHG2X476H6') at step skeptric_extract: \n    ERROR:root:Error processing WarcFileRecord(url='https://skeptric.com/', timestamp=datetime.datetime(2021, 11, 26, 11, 28, 36), mime='text/html', status=200, path=PosixPath('../resources/test/skeptric.warc.gz'), offset=17122, digest='JJVB3MQERHRZJCHOJNKS5VDOODXPZAV2') at step skeptric_extract: \n    ERROR:root:Error processing WarcFileRecord(url='https://skeptric.com/tags/data', timestamp=datetime.datetime(2021, 11, 26, 11, 28, 37), mime='text/html', status=302, path=PosixPath('../resources/test/skeptric.warc.gz'), offset=129093, digest='ZZZXDZTTV2KTABRO64ESHVWFPNKB4I5H') at step skeptric_extract: \n    ERROR:root:Error processing WarcFileRecord(url='https://skeptric.com/tags/data/', timestamp=datetime.datetime(2021, 11, 26, 11, 28, 38), mime='text/html', status=200, path=PosixPath('../resources/test/skeptric.warc.gz'), offset=130269, digest='R7CLAACFU5L7T5LKI5G53RZSMCNUNV6F') at step skeptric_extract: \n    ERROR:root:Error processing WarcFileRecord(url='https://skeptric.com/images/wayback_empty_returns.png', timestamp=datetime.datetime(2021, 11, 26, 11, 28, 38), mime='image/png', status=200, path=PosixPath('../resources/test/skeptric.warc.gz'), offset=160971, digest='SU7JRTHNW6KFCJQFL5PMMKV33U2VLV7T') at step skeptric_extract: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n    ERROR:root:Error processing WarcFileRecord(url='https://skeptric.com/searching-100b-pages-cdx', timestamp=datetime.datetime(2021, 11, 26, 11, 28, 39), mime='text/html', status=302, path=PosixPath('../resources/test/skeptric.warc.gz'), offset=173368, digest='AYVHQLVFIVGZGUYPEHX46CHMZ5NUDDBF') at step skeptric_extract: \n\n\nWe could then investigate them to see what happened\n\n```python\nimport datetime\nfrom pathlib import PosixPath\nfrom webrefine.query import WarcFileRecord\n\nrecord = WarcFileRecord(url='https://skeptric.com/tags/data/', timestamp=datetime.datetime(2021, 11, 26, 11, 28, 38), mime='text/html', status=200, path=PosixPath('../resources/test/skeptric.warc.gz'), offset=130269, digest='R7CLAACFU5L7T5LKI5G53RZSMCNUNV6F')\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/EdwardJRoss/webrefine/tree/master/",
    "keywords": "data",
    "license": "Apache Software License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "webrefine",
    "package_url": "https://pypi.org/project/webrefine/",
    "platform": null,
    "project_url": "https://pypi.org/project/webrefine/",
    "project_urls": {
      "Homepage": "https://github.com/EdwardJRoss/webrefine/tree/master/"
    },
    "release_url": "https://pypi.org/project/webrefine/0.0.3/",
    "requires_dist": [
      "joblib",
      "packaging",
      "pip",
      "requests",
      "sqlitedict",
      "tqdm",
      "warcio"
    ],
    "requires_python": ">=3.7",
    "summary": "Workflow for refining datasets from World Wide Web data",
    "version": "0.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13551533,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4071f69f26f80784b0c6001198af8c506457cc0ebeecf14d8bf626e7ddc067a4",
          "md5": "7caabdce12d568bc8848dc4edd2d3c8c",
          "sha256": "8250fbf7019140044a8c335869161149459f929905fab064e88f48fd0c4ec8ca"
        },
        "downloads": -1,
        "filename": "webrefine-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7caabdce12d568bc8848dc4edd2d3c8c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 20160,
        "upload_time": "2021-12-05T11:44:24",
        "upload_time_iso_8601": "2021-12-05T11:44:24.396714Z",
        "url": "https://files.pythonhosted.org/packages/40/71/f69f26f80784b0c6001198af8c506457cc0ebeecf14d8bf626e7ddc067a4/webrefine-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e880ffbccf3d3ab63d4caee6697d67d5d655770f574529b808b45f836cef8a4",
          "md5": "1edcf62678857d2c83b21e46a3be66fe",
          "sha256": "4b2a641cbe4c6ec036cc0d1b6864c499153f7f6e00d298cd84c946ad69051a22"
        },
        "downloads": -1,
        "filename": "webrefine-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1edcf62678857d2c83b21e46a3be66fe",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 349239,
        "upload_time": "2021-12-05T11:44:27",
        "upload_time_iso_8601": "2021-12-05T11:44:27.225269Z",
        "url": "https://files.pythonhosted.org/packages/5e/88/0ffbccf3d3ab63d4caee6697d67d5d655770f574529b808b45f836cef8a4/webrefine-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ed48f1c0727ee8afe0f2c1ee426c18605f5596f4bda321ae33c7f729b69b875e",
          "md5": "a09873ed68b4aef4857ddb322200f49c",
          "sha256": "51556f4fdd11fc8629c3da5b287e072dabfa1e7efefc29d0ca3bad0a1a24563a"
        },
        "downloads": -1,
        "filename": "webrefine-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a09873ed68b4aef4857ddb322200f49c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 21007,
        "upload_time": "2021-12-06T04:05:16",
        "upload_time_iso_8601": "2021-12-06T04:05:16.800246Z",
        "url": "https://files.pythonhosted.org/packages/ed/48/f1c0727ee8afe0f2c1ee426c18605f5596f4bda321ae33c7f729b69b875e/webrefine-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "61081e4d5051d8cafe0a7ac35d146c8852fe7691dc53e16a749c700ff5fc57c9",
          "md5": "f3fb3562c10aaf962ba6fc784ed0990a",
          "sha256": "0691d3be572f58e3f6d6c2e9dcd5464ad6f615ac7526048c4ef66dc70cf9a4ee"
        },
        "downloads": -1,
        "filename": "webrefine-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "f3fb3562c10aaf962ba6fc784ed0990a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 352158,
        "upload_time": "2021-12-06T04:05:19",
        "upload_time_iso_8601": "2021-12-06T04:05:19.612252Z",
        "url": "https://files.pythonhosted.org/packages/61/08/1e4d5051d8cafe0a7ac35d146c8852fe7691dc53e16a749c700ff5fc57c9/webrefine-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2b32942c4a96dffce54bd21783711914c3df5c96fb2fd803259e72f490404b39",
          "md5": "c39abd9f02252555ed20ff02079b3cae",
          "sha256": "a527b4bb1f3f0b5c936ed434b4600041c3d10481aaccd6c5d2af2f32920bd62d"
        },
        "downloads": -1,
        "filename": "webrefine-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c39abd9f02252555ed20ff02079b3cae",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 21385,
        "upload_time": "2022-04-19T01:57:06",
        "upload_time_iso_8601": "2022-04-19T01:57:06.693468Z",
        "url": "https://files.pythonhosted.org/packages/2b/32/942c4a96dffce54bd21783711914c3df5c96fb2fd803259e72f490404b39/webrefine-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ee1d816cee4617b668957de7ccac9b4fb9edefc6977dd3c17d321fba4e0b88ca",
          "md5": "88a19404046a0e39bbf29fb22b922d9d",
          "sha256": "4ff273117c4978def42d377036e20beb7ea27c93686afa73dd1b1f126cc9b9db"
        },
        "downloads": -1,
        "filename": "webrefine-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "88a19404046a0e39bbf29fb22b922d9d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 22844,
        "upload_time": "2022-04-19T01:57:08",
        "upload_time_iso_8601": "2022-04-19T01:57:08.438120Z",
        "url": "https://files.pythonhosted.org/packages/ee/1d/816cee4617b668957de7ccac9b4fb9edefc6977dd3c17d321fba4e0b88ca/webrefine-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2b32942c4a96dffce54bd21783711914c3df5c96fb2fd803259e72f490404b39",
        "md5": "c39abd9f02252555ed20ff02079b3cae",
        "sha256": "a527b4bb1f3f0b5c936ed434b4600041c3d10481aaccd6c5d2af2f32920bd62d"
      },
      "downloads": -1,
      "filename": "webrefine-0.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c39abd9f02252555ed20ff02079b3cae",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 21385,
      "upload_time": "2022-04-19T01:57:06",
      "upload_time_iso_8601": "2022-04-19T01:57:06.693468Z",
      "url": "https://files.pythonhosted.org/packages/2b/32/942c4a96dffce54bd21783711914c3df5c96fb2fd803259e72f490404b39/webrefine-0.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ee1d816cee4617b668957de7ccac9b4fb9edefc6977dd3c17d321fba4e0b88ca",
        "md5": "88a19404046a0e39bbf29fb22b922d9d",
        "sha256": "4ff273117c4978def42d377036e20beb7ea27c93686afa73dd1b1f126cc9b9db"
      },
      "downloads": -1,
      "filename": "webrefine-0.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "88a19404046a0e39bbf29fb22b922d9d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 22844,
      "upload_time": "2022-04-19T01:57:08",
      "upload_time_iso_8601": "2022-04-19T01:57:08.438120Z",
      "url": "https://files.pythonhosted.org/packages/ee/1d/816cee4617b668957de7ccac9b4fb9edefc6977dd3c17d321fba4e0b88ca/webrefine-0.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}