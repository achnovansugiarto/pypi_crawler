{
  "info": {
    "author": "Iván Arias Rodríguez",
    "author_email": "ivan.arias.rodriguez@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 1 - Planning",
      "Intended Audience :: Science/Research",
      "License :: Other/Proprietary License",
      "Natural Language :: Spanish",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3 :: Only",
      "Topic :: Scientific/Engineering"
    ],
    "description": "# IAR Tokenizer\r\n\r\nThe IAR (Iván Arias Rodríguez) Tokenizer is a tokenizer developed mainly for Spanish. It is able to divide a text in paragraphs, those in sentences, and each sentence in a list of tokens.\r\n\r\nThe creation of this software was supported by the Spanish Ministry of Education, Culture and Sport via a doctoral grant to Iván Arias Rodríguez (FPU16/04039). It has also been funded by research projects TIN2014-52010-R (RedR+Human) and TIN2017-88092 R (CetrO+Spec).\r\n\r\nYou can install this software with pip :\r\n\r\n\tpip install iar-tokenizer\r\n\r\n*If you change or adapt a function, change its name (for example add your initial after the name)*\r\n\r\n[![License: CC BY-NC-SA 4.0](https://licensebuttons.net/l/by-nc-sa/4.0/80x15.png)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\r\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\r\nhttp://creativecommons.org/licenses/by-nc-sa/4.0/\r\n\r\nThis code is given as is without warranty of any kind.\r\nIn no event shall the authors or copyright holder be liable for any claim damages or other liability.\r\n\r\n------------------------------------------\r\n\r\nEl IAR (Iván Arias Rodríguez) Tokenizer es un tokenizador desarrollado principalmente para la lengua española. Es capaz de dividir un texto en párrafo, estos en frases, y cada frase en una lista de tokens.\r\n\r\nLa creación de este software fue apoyada por el Ministerio de Educación, Cultura y Deporte de España, a través de una beca doctoral otorgada a Iván Arias Rodríguez (FPU16/04039). También ha sido financiado por los proyectos de investigación TIN2014-52010-R (RedR+Human) y TIN2017-88092 R (CetrO+Spec).\r\n\r\nPuedes instalar este software con pip :\r\n\r\n\tpip install iar-tokenizer\r\n\r\n*Si cambias o adaptas una función, cambia su nombre (por ejemplo añade tus iniciales tras el nombre)*\r\n\r\n[![License: CC BY-NC-SA 4.0](https://licensebuttons.net/l/by-nc-sa/4.0/80x15.png)](https://creativecommons.org/licenses/by-nc-sa/4.0/)\r\nEste trabajo se ofrece bajo la licencia Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\r\nhttp://creativecommons.org/licenses/by-nc-sa/4.0/\r\n\r\nEste código se ofrece como es sin ninguna garantía de ningún tipo.\r\nEn ningún caso los autores o el titular de los derechos de autor serán responsables de cualquier reclamación por daños y perjuicios u otra responsabilidad.\r\n\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://pypi.org/project/iar-tokenizer/",
    "keywords": "",
    "license": "Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "iar-tokenizer",
    "package_url": "https://pypi.org/project/iar-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/iar-tokenizer/",
    "project_urls": {
      "Homepage": "https://pypi.org/project/iar-tokenizer/"
    },
    "release_url": "https://pypi.org/project/iar-tokenizer/1.0.12/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A tokenizer focused on Spanish language.",
    "version": "1.0.12",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11745874,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "37c84b8e37d27cc2041a3920e1ca3526eec1d21ecbd9668d9017d7a5d889b5c1",
          "md5": "e068e32b3baea20b7f89824a2d36479c",
          "sha256": "4c394ee333d4dd0c26bf2a6573a1975fe7bf8416a0c01715a19b1f8dbc378353"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e068e32b3baea20b7f89824a2d36479c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 11740,
        "upload_time": "2019-05-17T16:14:59",
        "upload_time_iso_8601": "2019-05-17T16:14:59.955194Z",
        "url": "https://files.pythonhosted.org/packages/37/c8/4b8e37d27cc2041a3920e1ca3526eec1d21ecbd9668d9017d7a5d889b5c1/iar_tokenizer-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "84941033436402bf7ca5526b0b7dcaa66efe3933893b13326d2137935be86ede",
          "md5": "572b68445c8e6785b512aa2242de05f7",
          "sha256": "4d9809b5b13c18e854d7bdbee6efed389f8894f65a4e36bd6e6e20e84175091b"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "572b68445c8e6785b512aa2242de05f7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 11801,
        "upload_time": "2019-05-18T09:25:54",
        "upload_time_iso_8601": "2019-05-18T09:25:54.385887Z",
        "url": "https://files.pythonhosted.org/packages/84/94/1033436402bf7ca5526b0b7dcaa66efe3933893b13326d2137935be86ede/iar_tokenizer-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0e18cd66e70ef1327a75ed86c4dc78f2c7806be1e031c21edf13b714f651b598",
          "md5": "c4578d58f9a56297305e73c5681d084c",
          "sha256": "a5bd6ad6facab7ad064c192cd0da3c8a7d1492f7f0eaf5b493482a4419257df5"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c4578d58f9a56297305e73c5681d084c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10887,
        "upload_time": "2019-05-18T09:25:55",
        "upload_time_iso_8601": "2019-05-18T09:25:55.729952Z",
        "url": "https://files.pythonhosted.org/packages/0e/18/cd66e70ef1327a75ed86c4dc78f2c7806be1e031c21edf13b714f651b598/iar_tokenizer-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.10": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4cf5680c56f689d67d6471469cca6d4be7a10673966204dd47ccf3901b136e48",
          "md5": "20fa152d03875c1488c7a8404707ee9e",
          "sha256": "bc28adc449f9afcfabf6e439e87064e737129c19bec3dc6dbc4989d894afae6d"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.10-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "20fa152d03875c1488c7a8404707ee9e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21591,
        "upload_time": "2019-06-07T18:16:10",
        "upload_time_iso_8601": "2019-06-07T18:16:10.400656Z",
        "url": "https://files.pythonhosted.org/packages/4c/f5/680c56f689d67d6471469cca6d4be7a10673966204dd47ccf3901b136e48/iar_tokenizer-1.0.10-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aaf8c452ffe617110ec6618d54155b3a0ca1898f06d40f04e6845f000d5f22f7",
          "md5": "86950cdf470a1f98f5ca645632cbe9c0",
          "sha256": "5c54125b73d759edb75ccbc2319d36b3a2fe40ed4186e2014fc2cb76a405afec"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.10.tar.gz",
        "has_sig": false,
        "md5_digest": "86950cdf470a1f98f5ca645632cbe9c0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11165,
        "upload_time": "2019-06-07T18:16:11",
        "upload_time_iso_8601": "2019-06-07T18:16:11.880809Z",
        "url": "https://files.pythonhosted.org/packages/aa/f8/c452ffe617110ec6618d54155b3a0ca1898f06d40f04e6845f000d5f22f7/iar_tokenizer-1.0.10.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d40b0e31a2d2871f214056b3178c53956ba6a2085c6d2f278c52183d2b9d0260",
          "md5": "0db3883ac22bba0c39c52985b4c8e65c",
          "sha256": "c7ccc24fa91b4f33345461d6025b025ff7c14edc591796195864b64bc466b4a5"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.11-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0db3883ac22bba0c39c52985b4c8e65c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 13413,
        "upload_time": "2021-10-16T15:24:09",
        "upload_time_iso_8601": "2021-10-16T15:24:09.916892Z",
        "url": "https://files.pythonhosted.org/packages/d4/0b/0e31a2d2871f214056b3178c53956ba6a2085c6d2f278c52183d2b9d0260/iar_tokenizer-1.0.11-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fe50ed822e59a5ee0e6e48ce5252899891f02af4a6eaf72c01cedf07cc80fb90",
          "md5": "a50bd54111bea517da362e22fe367fbe",
          "sha256": "8e1e0d84d8e13b39c1c2430f0cebb10fb93f5af773c35f23c8f3107e3893344d"
        },
        "downloads": -1,
        "filename": "iar-tokenizer-1.0.11.tar.gz",
        "has_sig": false,
        "md5_digest": "a50bd54111bea517da362e22fe367fbe",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14122,
        "upload_time": "2021-10-16T15:24:12",
        "upload_time_iso_8601": "2021-10-16T15:24:12.838508Z",
        "url": "https://files.pythonhosted.org/packages/fe/50/ed822e59a5ee0e6e48ce5252899891f02af4a6eaf72c01cedf07cc80fb90/iar-tokenizer-1.0.11.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.12": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "15b667f4fe48c064e844e7f6792927903f8b50f2bd64da2a492d6e31417d38b5",
          "md5": "9bbb2658318e5fa560b2b24ff22a26db",
          "sha256": "839e56d5bf1c1cd64e4856997738ff84d0f0fd3b67d2a3538db6743cea7175a4"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.12-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9bbb2658318e5fa560b2b24ff22a26db",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 13405,
        "upload_time": "2021-10-16T15:59:43",
        "upload_time_iso_8601": "2021-10-16T15:59:43.685045Z",
        "url": "https://files.pythonhosted.org/packages/15/b6/67f4fe48c064e844e7f6792927903f8b50f2bd64da2a492d6e31417d38b5/iar_tokenizer-1.0.12-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fb21b7e5656a61ef0659b25e52d9a5efb2ab38500ba2845daf877a989fd5e3ae",
          "md5": "cf64bb8cc619b9695e6a2581a20f67bf",
          "sha256": "a6be88ae33fc3217a5819063208827dd64a7300af747c98382dc03576ff06616"
        },
        "downloads": -1,
        "filename": "iar-tokenizer-1.0.12.tar.gz",
        "has_sig": false,
        "md5_digest": "cf64bb8cc619b9695e6a2581a20f67bf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14102,
        "upload_time": "2021-10-16T15:29:54",
        "upload_time_iso_8601": "2021-10-16T15:29:54.144887Z",
        "url": "https://files.pythonhosted.org/packages/fb/21/b7e5656a61ef0659b25e52d9a5efb2ab38500ba2845daf877a989fd5e3ae/iar-tokenizer-1.0.12.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9dc146acfe469720da92500fded65ee5ea6f1aef847e7db49b603181638931cb",
          "md5": "8ff5ff320a642070d7a9f49ac56b51c2",
          "sha256": "8ad6b9cbcf9df237576d9ac843c37ca4b4ac2fb858ce9c3a300d5637494bd872"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8ff5ff320a642070d7a9f49ac56b51c2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 11801,
        "upload_time": "2019-05-18T09:40:05",
        "upload_time_iso_8601": "2019-05-18T09:40:05.591952Z",
        "url": "https://files.pythonhosted.org/packages/9d/c1/46acfe469720da92500fded65ee5ea6f1aef847e7db49b603181638931cb/iar_tokenizer-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c287b4b6e1abc46e72f61aedfda70d5dd204b17ac27e92fb52d815f6fb50c6b9",
          "md5": "7652668ba9fdcd1ff95606bedb57c33a",
          "sha256": "ad4c8a9cec2fd327de3e55a764c7eaae0934f1b9670ad651bd1f23e1921f88e9"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "7652668ba9fdcd1ff95606bedb57c33a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10874,
        "upload_time": "2019-05-18T09:40:07",
        "upload_time_iso_8601": "2019-05-18T09:40:07.217156Z",
        "url": "https://files.pythonhosted.org/packages/c2/87/b4b6e1abc46e72f61aedfda70d5dd204b17ac27e92fb52d815f6fb50c6b9/iar_tokenizer-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dc3f2afce4d6202dd25a4c7b53c508afe8ae9cb15c22d5586753ed822b3d2ce1",
          "md5": "0df4ba5987da8ae3a03d3108a3354ae1",
          "sha256": "91b768612c2789a3a1eae6591e5dfe607a1862bd67202c8a89062d0b6229e310"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0df4ba5987da8ae3a03d3108a3354ae1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 11820,
        "upload_time": "2019-05-22T00:46:44",
        "upload_time_iso_8601": "2019-05-22T00:46:44.891081Z",
        "url": "https://files.pythonhosted.org/packages/dc/3f/2afce4d6202dd25a4c7b53c508afe8ae9cb15c22d5586753ed822b3d2ce1/iar_tokenizer-1.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "91ce2447428960f18d12c14eb4db0660eada4dcca2c7fee6631cd749d55aaae1",
          "md5": "c414ce614b82d764abdc6ae8d5ce444b",
          "sha256": "cf1bb5170286470faddb9485670a83743a8ac9e4b7578f2c7b38cf4edb633ddc"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "c414ce614b82d764abdc6ae8d5ce444b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10936,
        "upload_time": "2019-05-22T00:46:46",
        "upload_time_iso_8601": "2019-05-22T00:46:46.121458Z",
        "url": "https://files.pythonhosted.org/packages/91/ce/2447428960f18d12c14eb4db0660eada4dcca2c7fee6631cd749d55aaae1/iar_tokenizer-1.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "de19414047d25dff6e155ca2fef9fb6bffce9c2de6899cc1136e03c6084e277b",
          "md5": "c882cb85c554d04642f75e8b81bde1a3",
          "sha256": "9448cf68ceaaa477f4d9d2d4548b14c334aafb3b1fd472e055f6599a8c02a5ce"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c882cb85c554d04642f75e8b81bde1a3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12102,
        "upload_time": "2019-06-07T16:17:43",
        "upload_time_iso_8601": "2019-06-07T16:17:43.855336Z",
        "url": "https://files.pythonhosted.org/packages/de/19/414047d25dff6e155ca2fef9fb6bffce9c2de6899cc1136e03c6084e277b/iar_tokenizer-1.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "200af1aa6474b2ab26e30828c72e3486a2ad135fad62823d21bba32ff6eb7494",
          "md5": "c13c093bd3cf04ca0704aeedd37ed023",
          "sha256": "bc1d119450e625857fcbaf4e34c0d51ee211c21e1046fa2b09b8ad487ed9191d"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "c13c093bd3cf04ca0704aeedd37ed023",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11169,
        "upload_time": "2019-06-07T16:17:53",
        "upload_time_iso_8601": "2019-06-07T16:17:53.731755Z",
        "url": "https://files.pythonhosted.org/packages/20/0a/f1aa6474b2ab26e30828c72e3486a2ad135fad62823d21bba32ff6eb7494/iar_tokenizer-1.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5a564802326209df979a705046eeefa3025f6664cfa371a1fcecba0518d7a894",
          "md5": "eacbb8fa2758f4cd61e6f507afe82e3f",
          "sha256": "66985deb46570d23e53e8a3936113c7158ddc948e99745ea8791711c1fc4e725"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eacbb8fa2758f4cd61e6f507afe82e3f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12097,
        "upload_time": "2019-06-07T16:22:42",
        "upload_time_iso_8601": "2019-06-07T16:22:42.370018Z",
        "url": "https://files.pythonhosted.org/packages/5a/56/4802326209df979a705046eeefa3025f6664cfa371a1fcecba0518d7a894/iar_tokenizer-1.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "129223f9c0683e2b69cf290ea86ba720b9084e69e2052f69c5b2467675fe7366",
          "md5": "30d5da165512485f537d38d9299d0f77",
          "sha256": "681bcad324615c3ad46b25da46adbb79f95e2a09597d3dff8ba637dd3c7ac996"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "30d5da165512485f537d38d9299d0f77",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11165,
        "upload_time": "2019-06-07T16:22:43",
        "upload_time_iso_8601": "2019-06-07T16:22:43.837428Z",
        "url": "https://files.pythonhosted.org/packages/12/92/23f9c0683e2b69cf290ea86ba720b9084e69e2052f69c5b2467675fe7366/iar_tokenizer-1.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "22d2d61942beabd7b8138b83fb2eccbcf9596e76e983e7557df5de3740538f24",
          "md5": "64be1664e42affcea2b09aed47f82453",
          "sha256": "8d9831e6e248397f89c51c0c1d9f00327565ca9acbc8776f452ec00782c37cd9"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "64be1664e42affcea2b09aed47f82453",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12019,
        "upload_time": "2019-06-07T17:18:04",
        "upload_time_iso_8601": "2019-06-07T17:18:04.761835Z",
        "url": "https://files.pythonhosted.org/packages/22/d2/d61942beabd7b8138b83fb2eccbcf9596e76e983e7557df5de3740538f24/iar_tokenizer-1.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6ea0bb6a26e54a98fc1ced4d223ab945ea2926bcb031a6f6a4e62d8596924354",
          "md5": "6b850bb709a0ab028afada28d1b57545",
          "sha256": "7dd4bef0765fdf3c99c2753d52ae22064b7eae71b59e8ffc4660e1aac4c6009b"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "6b850bb709a0ab028afada28d1b57545",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11117,
        "upload_time": "2019-06-07T17:18:05",
        "upload_time_iso_8601": "2019-06-07T17:18:05.982009Z",
        "url": "https://files.pythonhosted.org/packages/6e/a0/bb6a26e54a98fc1ced4d223ab945ea2926bcb031a6f6a4e62d8596924354/iar_tokenizer-1.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "930184bb3ae34bd8593b94d860989c46b38ac56db5c30f205c6b3bac86d9cadc",
          "md5": "27de4ddf1ace96bd5743de0784cef9b2",
          "sha256": "011f5c6cc01a229faeb67f3fad9e86849bf63a404e3c5ef8cc72872f341ebd98"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "27de4ddf1ace96bd5743de0784cef9b2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 21577,
        "upload_time": "2019-06-07T17:45:29",
        "upload_time_iso_8601": "2019-06-07T17:45:29.627753Z",
        "url": "https://files.pythonhosted.org/packages/93/01/84bb3ae34bd8593b94d860989c46b38ac56db5c30f205c6b3bac86d9cadc/iar_tokenizer-1.0.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "30848abe8318231b7d5f847240006693d1116e9cde71d567118f17b2cdb83ad9",
          "md5": "a79b8447af61b5ccf1e98e340d0d792e",
          "sha256": "218e9e5f92f28b6351f8f55e4aa886965a3f1031941524834ec3e2587fbfe919"
        },
        "downloads": -1,
        "filename": "iar_tokenizer-1.0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "a79b8447af61b5ccf1e98e340d0d792e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11123,
        "upload_time": "2019-06-07T17:45:30",
        "upload_time_iso_8601": "2019-06-07T17:45:30.902932Z",
        "url": "https://files.pythonhosted.org/packages/30/84/8abe8318231b7d5f847240006693d1116e9cde71d567118f17b2cdb83ad9/iar_tokenizer-1.0.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "15b667f4fe48c064e844e7f6792927903f8b50f2bd64da2a492d6e31417d38b5",
        "md5": "9bbb2658318e5fa560b2b24ff22a26db",
        "sha256": "839e56d5bf1c1cd64e4856997738ff84d0f0fd3b67d2a3538db6743cea7175a4"
      },
      "downloads": -1,
      "filename": "iar_tokenizer-1.0.12-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "9bbb2658318e5fa560b2b24ff22a26db",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 13405,
      "upload_time": "2021-10-16T15:59:43",
      "upload_time_iso_8601": "2021-10-16T15:59:43.685045Z",
      "url": "https://files.pythonhosted.org/packages/15/b6/67f4fe48c064e844e7f6792927903f8b50f2bd64da2a492d6e31417d38b5/iar_tokenizer-1.0.12-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fb21b7e5656a61ef0659b25e52d9a5efb2ab38500ba2845daf877a989fd5e3ae",
        "md5": "cf64bb8cc619b9695e6a2581a20f67bf",
        "sha256": "a6be88ae33fc3217a5819063208827dd64a7300af747c98382dc03576ff06616"
      },
      "downloads": -1,
      "filename": "iar-tokenizer-1.0.12.tar.gz",
      "has_sig": false,
      "md5_digest": "cf64bb8cc619b9695e6a2581a20f67bf",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 14102,
      "upload_time": "2021-10-16T15:29:54",
      "upload_time_iso_8601": "2021-10-16T15:29:54.144887Z",
      "url": "https://files.pythonhosted.org/packages/fb/21/b7e5656a61ef0659b25e52d9a5efb2ab38500ba2845daf877a989fd5e3ae/iar-tokenizer-1.0.12.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}