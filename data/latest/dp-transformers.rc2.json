{
  "info": {
    "author": "Microsoft",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# dp-transformers\n\n:warning: This repo is intended for research projects and prototypes.\nWhile we try to provide tests for all the functionality, the repo has not (yet) undergone the detailed review process that is necessary for deploying a system of critical nature such as privacy.\n\n## Introduction \n\nSee [dp-transformers](https://www.microsoft.com/en-us/research/project/dp-transformers) for a brief introduction to our repository.\n\n## Installation\n\nFor installing the `dp-transformers` package, you can just type\n\n```\npip install .\n```\n\n## Examples\n\nSee `./examples` for end to end examples of how to use the library.\n\nA basic example can be found in `examples/nlg-reddit/sample-level-dp/fine-tune-dp.py`.\nFirst, create an Anaconda environment by doing `conda env create -f examples/nlg-reddit/sample-level-dp/environment.yml`.\nThen, you can run the example using the following command (here we assume there are 16 GPUs in the machine, and thus set `--nproc_per_node 16`):\n\n```\npython -m torch.distributed.run --nproc_per_node 16 examples/nlg-reddit/sample-level-dp/fine-tune-dp.py \\\n--output_dir scratch \\\n--model_name gpt2 \\\n--sequence_len 128 \\\n--per_device_train_batch_size 32 \\\n--gradient_accumulation_steps 2 \\\n--evaluation_strategy steps \\\n--eval_steps 45 \\\n--log_level info \\\n--per_device_eval_batch_size 64 \\\n--eval_accumulation_steps 1 \\\n--seed 42 \\\n--target_epsilon 8 \\\n--per_sample_max_grad_norm 1.0 \\\n--prediction_loss_only \\\n--weight_decay 0.01 \\\n--remove_unused_columns False \\\n--num_train_epochs 3 \\\n--logging_steps 5 \\\n--max_grad_norm 0 \\\n--lr_scheduler_type constant \\\n--learning_rate 1e-4 \\\n--disable_tqdm True \\\n--dataloader_num_workers 2\n```\n\n## ðŸ¤— Transformers with Opacus\n\n### Trainer\n\nHuggingface's trainer provides callback hooks which we can use to make sure the required methods in the privacy engine are called.\n\nYou can use the callback as demonstrated in the example below\n\n``` python\nprivacy_engine = opacus.PrivacyEngine(module=model, ...)\n\n# No need to attach the privacy engine to the optimizer. The callback will automatically attach the optimizer.\n\ntrainer = transformers.Trainer(\n    model = model,\n    [...],\n    callbacks = [dp_transformers.PrivacyEngineCallback(privacy_engine)]  # <-- Add this line to make sure the privacy engine is used in the trainer\n    [...]\n)\n```\n\n### Data Collation\n\nðŸ¤— Transformers library often provides sensible default arguments.\nFor example, when no `position_ids` are provided, the library automatically will use incrementing integers.\nThe way this is implemented is by first creating a tensor of shape `[1, sequence_length]` filled with increasing integers.\nDuring a second step that tensor is replicated for the whole batch.\nHowever, the replication is part of the computational graph and hence Opacus cannot infer the batch size from this input tensor.\n\nWe have therefore implemented a custom data collator (see `dp_transformers.DataCollatorForPrivateCausalLanguageModeling`) which automatically creates the `position_ids` input tensor by using `torch.repeat`.\nThis works with opacus since the `position_ids` tensor appears as batch size different inputs in the computation graph.\n\n### GPT2\n\nThe ðŸ¤— Transformers implementation for GPT2 uses a custom layer type namely `Conv1D`.\nIt is not quite clear why this was introduced since it is essentially a regular linear layer.\nThis causes problems with Opacus though since it is not sure how to apply the backward hooks for this layer.\n\nIn this repo we provide an implementation for handling this type of layer.\nSee `dp_transformers.grad_sample.transformers.conv_1d`\n\nAll necessary grad samplers can be registered by merely importing `conv_1d` before the model training.\nSee the Known Issues section below for more details.\n\n## General tips for DP training\n\nIn this section, we collect a few helpful strategies for training models with DP.\nAlso Opacus's FAQs have a few tips on how to get started with DP training (see [Opacus FAQ](https://opacus.ai/docs/faq))\n\n### Hyper-parameters\n\nLarger batch sizes help DP training.\nAs a general rule, try starting with $\\sqrt{|D|}$ where $D$ is the training dataset.\nSince Opacus increases memory consumption significantly, this is only possible using gradient accumulation.\n\nWe have found a surprisingly small dependence on the clipping norm.\nAs a general rule of thumb start with a clipping parameter of 0.1\n\nFine-tuning the model longer is also helpful.\n\n\n### Deploying DP trained models\n\nPay attention which pseudo random number generator (PRNG) was used.\nPytorch's default (Mersenne Twister) might be attackable.\nSee [Opacus FAQ](https://opacus.ai/docs/faq#what-is-the-secure_rng-argument-in-privacyengine)\nMake sure to use a better PRNG before deploying models.\n\n## Known issues\n\n### Register custom grad samplers late in the training process\n\nWhen registering custom grad sampler like `dp_transformers.grad_sample.transformers.conv_1d`, functions are added to a global dictionary that Opacus handles.\nThis global dictionary is used to establish whether models are compatible with Opacus and how to handle the per-sample gradient computation.\nAll grad samplers need to be registered as early as possible in the training process.\nDefinitely, before the model is wrapped with `GradSampleModule`.\n\n## How to Cite\n\n```\n@misc{dp-transformers,\n  author        = {Lukas Wutschitz and Huseyin A. Inan and Andre Manoel},\n  title         = {dp-transformers: Training transformer models with differential privacy},\n  year          = {2022},\n  month         = {August},\n  howpublished  = {\\url{https://www.microsoft.com/en-us/research/project/dp-transformers}}\n}\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\nand actually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\ninstructions provided by the bot. You will only need to do this once across all repositories using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nFor any other questions, feel free to open an issue on GitHub.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://www.github.com/microsoft/dp-transformers",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dp-transformers",
    "package_url": "https://pypi.org/project/dp-transformers/",
    "platform": null,
    "project_url": "https://pypi.org/project/dp-transformers/",
    "project_urls": {
      "Homepage": "https://www.github.com/microsoft/dp-transformers"
    },
    "release_url": "https://pypi.org/project/dp-transformers/1.0.0/",
    "requires_dist": [
      "transformers (>=4.20.1)",
      "datasets (>=2.0.0)",
      "opacus (>=1.2.0)",
      "prv-accountant (<0.2.0)",
      "torch (<=1.12.1,>=1.9.1)",
      "pytest ; extra == 'test'",
      "loralib ; extra == 'test'"
    ],
    "requires_python": ">=3.7.0",
    "summary": "Differentially-private transformers using HuggingFace and Opacus",
    "version": "1.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15729087,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "93c076ba169014a0c65a7a79f27073ba7b219c69b9a5328dde13c28d8e9090b4",
          "md5": "bde6f0334de95ce2a6698217302c42c2",
          "sha256": "fa1ff17a950c53c16fceb9191dcc894ac9fc2116e0eec905ab58884c83d4539b"
        },
        "downloads": -1,
        "filename": "dp_transformers-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bde6f0334de95ce2a6698217302c42c2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0,<3.10.0",
        "size": 17612,
        "upload_time": "2022-08-02T22:51:14",
        "upload_time_iso_8601": "2022-08-02T22:51:14.653307Z",
        "url": "https://files.pythonhosted.org/packages/93/c0/76ba169014a0c65a7a79f27073ba7b219c69b9a5328dde13c28d8e9090b4/dp_transformers-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a1024d7bb14fbb926fc993df1ddfb4849675adcb6a5590331d7666397e1ff0ff",
          "md5": "aa14574e750de3bf41b7b4297bb901f5",
          "sha256": "f751291519fb0233b518b695309228d2e5b97906f5ee0bf39dfe0c9f2fa4398b"
        },
        "downloads": -1,
        "filename": "dp-transformers-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "aa14574e750de3bf41b7b4297bb901f5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0,<3.10.0",
        "size": 16818,
        "upload_time": "2022-08-02T22:51:16",
        "upload_time_iso_8601": "2022-08-02T22:51:16.069156Z",
        "url": "https://files.pythonhosted.org/packages/a1/02/4d7bb14fbb926fc993df1ddfb4849675adcb6a5590331d7666397e1ff0ff/dp-transformers-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e310ac8565cdb6a0e48545da245b65ae3cf9ab0339fedd8d25873d3f2162cdeb",
          "md5": "19ff642a8c73d871db1963238c1a9da8",
          "sha256": "716dff8787cdeb55d0fb2da72a07acc946e151b4a7c772e3c0b70b8feef3542e"
        },
        "downloads": -1,
        "filename": "dp_transformers-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "19ff642a8c73d871db1963238c1a9da8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 17962,
        "upload_time": "2022-11-10T21:55:31",
        "upload_time_iso_8601": "2022-11-10T21:55:31.421990Z",
        "url": "https://files.pythonhosted.org/packages/e3/10/ac8565cdb6a0e48545da245b65ae3cf9ab0339fedd8d25873d3f2162cdeb/dp_transformers-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ae714f8237352a21df06f578ccbed21bc7ec27c58b0b0d3c3ce5ddfc1ac2f3e4",
          "md5": "0abf1e43b30aa5c2eb865baca1962145",
          "sha256": "d8fa6c549531006239efcc27f4afcc2ee0558923f33acb748713febb41a28953"
        },
        "downloads": -1,
        "filename": "dp-transformers-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0abf1e43b30aa5c2eb865baca1962145",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 17094,
        "upload_time": "2022-11-10T21:55:34",
        "upload_time_iso_8601": "2022-11-10T21:55:34.343829Z",
        "url": "https://files.pythonhosted.org/packages/ae/71/4f8237352a21df06f578ccbed21bc7ec27c58b0b0d3c3ce5ddfc1ac2f3e4/dp-transformers-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e310ac8565cdb6a0e48545da245b65ae3cf9ab0339fedd8d25873d3f2162cdeb",
        "md5": "19ff642a8c73d871db1963238c1a9da8",
        "sha256": "716dff8787cdeb55d0fb2da72a07acc946e151b4a7c772e3c0b70b8feef3542e"
      },
      "downloads": -1,
      "filename": "dp_transformers-1.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "19ff642a8c73d871db1963238c1a9da8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7.0",
      "size": 17962,
      "upload_time": "2022-11-10T21:55:31",
      "upload_time_iso_8601": "2022-11-10T21:55:31.421990Z",
      "url": "https://files.pythonhosted.org/packages/e3/10/ac8565cdb6a0e48545da245b65ae3cf9ab0339fedd8d25873d3f2162cdeb/dp_transformers-1.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ae714f8237352a21df06f578ccbed21bc7ec27c58b0b0d3c3ce5ddfc1ac2f3e4",
        "md5": "0abf1e43b30aa5c2eb865baca1962145",
        "sha256": "d8fa6c549531006239efcc27f4afcc2ee0558923f33acb748713febb41a28953"
      },
      "downloads": -1,
      "filename": "dp-transformers-1.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "0abf1e43b30aa5c2eb865baca1962145",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.0",
      "size": 17094,
      "upload_time": "2022-11-10T21:55:34",
      "upload_time_iso_8601": "2022-11-10T21:55:34.343829Z",
      "url": "https://files.pythonhosted.org/packages/ae/71/4f8237352a21df06f578ccbed21bc7ec27c58b0b0d3c3ce5ddfc1ac2f3e4/dp-transformers-1.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}