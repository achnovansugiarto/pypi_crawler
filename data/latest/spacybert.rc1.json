{
  "info": {
    "author": "Suraj Iyer",
    "author_email": "me@surajiyer.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# spacybert: Bert inference for spaCy\n[spaCy v2.0](https://spacy.io/usage/v2) extension and pipeline component for loading BERT sentence / document embedding meta data to `Doc`, `Span` and `Token` objects. The Bert backend itself is supported by the [Hugging Face transformers](https://github.com/huggingface/transformers) library.\n\n## Installation\n`spacybert` requires `spacy` v2.0.0 or higher.\n\n## Usage\n### Getting BERT embeddings for single language dataset\n```\nimport spacy\nfrom spacybert import BertInference\nnlp = spacy.load('en')\n```\n\nThen either use BertInference as part of a pipeline,\n```\nbert = BertInference(\n    from_pretrained='path/to/pretrained_bert_weights_dir',\n    set_extension=False)\nnlp.add_pipe(bert, last=True)\n```\nOr not...\n```\nbert = BertInference(\n    from_pretrained='path/to/pretrained_bert_weights_dir',\n    set_extension=True)\n```\nThe difference is that when `set_extension=True`, `bert_repr` is set as a property extension for the Doc, Span and Token spacy objects. If `set_extension=False`, the `bert_repr` is set as an attribute extension with a default value (`=None`). The attribute computes the correct value when `doc._.bert_repr` is called.\n\nGet the Bert representation / embedding.\n```\ndoc = nlp(\"This is a test\")\nprint(doc._.bert_repr)  # <-- torch.Tensor\n```\n\n### Getting BERT embeddings for multiple languages dataset.\n```\nimport spacy\nfrom spacy_langdetect import LanguageDetector\nfrom spacybert import MultiLangBertInference\n\nnlp = spacy.load('en')\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\nbert = MultiLangBertInference(\n    from_pretrained={\n        'en': 'path/to/en_pretrained_bert_weights_dir',\n        'nl': 'path/to/nl_pretrained_bert_weights_dir'\n    },\n    set_extension=False)\nnlp.add_pipe(bert, after='language_detector')\n\ntexts = [\n    \"This is a test\",  # English\n    \"Dit is een test\"  # Dutch\n]\nfor doc in nlp.pipe(texts):\n    print(doc._.bert_repr)  # <-- torch.Tensor\n```\nWhen language_detector detects languages other than the ones for which pre-trained weights is specified, by default `doc._.bert_repr = None`.\n\n## Available attributes\nThe extension sets attributes on the `Doc`, `Span` and `Token`. You can change the attribute name on initializing the extension.\n| | | |\n|-|-|-|\n| `Doc._.bert_repr` | `torch.Tensor` | Document BERT embedding |\n| `Span._.bert_repr` | `torch.Tensor` | Span BERT embedding |\n| `Token._.bert_repr` | `torch.Tensor` | Token BERT embedding |\n| | | |\n\n## Settings\nOn initialization of `BertInference`, you can define the following:\n\n| name | type | default | description |\n|-|-|-|-|\n| `from_pretrained` | `str` | `None` | Path to Bert model directory or name of HuggingFace transformers pre-trained Bert weights, e.g., `bert-base-uncased` |\n| `attr_name` | `str` | `'bert_repr'` | Name of the BERT embedding attribute to set to the `._` property |\n| `max_seq_len` | `int` | 512 | Max sequence length for input to Bert |\n| `pooling_strategy` | `str` | `'REDUCE_MEAN'` | Strategy to generate single sentence embedding from multiple word embeddings. See below for the various pooling strategies available. |\n| `set_extension` | `bool` | `True` | If `True`, then `'bert_repr'` is set as a property extension for the `Doc`, `Span` and `Token` spacy objects. If `False`, the `'bert_repr'` is set as an attribute extension with a default value (`None`) which gets filled correctly when called in a pipeline. Set it to `False` if you want to use this extension in a spacy pipeline. |\n| `force_extension` | `bool` | `True` | A boolean value to create the same 'Extension Attribute' upon being executed again |\n\nOn initialization of `MultiLangBertInference`, you can define the following:\n\n| name | type | default | description |\n|-|-|-|-|\n| `from_pretrained` | `Dict[LANG_ISO_639_1, str]` | `None` | Mapping between two-letter language codes to path to model directory or HuggingFace transformers pre-trained Bert weights |\n| `attr_name` | `str` | `'bert_repr'` | Same as in BertInference |\n| `max_seq_len` | `int` | 512 | Same as in BertInference |\n| `pooling_strategy` | `str` | `'REDUCE_MEAN'` | Same as in BertInference |\n| `set_extension` | `bool` | `True` | Same as in BertInference |\n| `force_extension` | `bool` | `True` | Same as in BertInference |\n\n## Pooling strategies\n| strategy | description |\n|-|-|\n| `REDUCE_MEAN` | Element-wise average the word embeddings |\n| `REDUCE_MAX` | Element-wise maximum of the word embeddings |\n| `REDUCE_MEAN_MAX` | Apply both `'REDUCE_MEAN'` and `'REDUCE_MAX'` and concatenate. So if the original word embedding is of dimensions `(768,)`, then the output will have shape `(1536,)` |\n| `CLS_TOKEN`, `FIRST_TOKEN` | Take the embedding of only the first `[CLS]` token |\n| `SEP_TOKEN`, `LAST_TOKEN` | Take the embedding of only the last `[SEP]` token |\n| `None` | No reduction is applied and a matrix of embeddings per word in the sentence is returned |\n\n## Roadmap\nThis extension is still experimental. Possible future updates include:\n* Getting document representation from other state-of-the-art NLP models other than Google's BERT.\n* Method for computing similarity between `Doc`, `Span` and `Token` objects using the `bert_repr` tensor.\n* Getting representation from multiple / other layers in the models.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/surajiyer/spacybert",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "spacybert",
    "package_url": "https://pypi.org/project/spacybert/",
    "platform": "",
    "project_url": "https://pypi.org/project/spacybert/",
    "project_urls": {
      "Homepage": "https://github.com/surajiyer/spacybert"
    },
    "release_url": "https://pypi.org/project/spacybert/1.0.1/",
    "requires_dist": [
      "torch (>=1.4.0)",
      "transformers (>=3.0.0)",
      "spacy (<3.0.0,>=2.2.1)",
      "spacy-langdetect (>=0.1.2)"
    ],
    "requires_python": "",
    "summary": "spaCy pipeline component for adding Bert embedding meta data to Doc, Token and Span objects.",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7892352,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f2ecf693d08a1d2c16901123c7a07247df202b3b95375263feae501bb39ffe88",
          "md5": "c2c7899542b9a51c13b3173f9309fd11",
          "sha256": "c758337ba969e425d0ecdb9a3fadfc29bf4ecfc344f00367c2ac97bb8ae4d1b4"
        },
        "downloads": -1,
        "filename": "spacybert-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c2c7899542b9a51c13b3173f9309fd11",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 7620,
        "upload_time": "2020-08-05T18:02:27",
        "upload_time_iso_8601": "2020-08-05T18:02:27.477468Z",
        "url": "https://files.pythonhosted.org/packages/f2/ec/f693d08a1d2c16901123c7a07247df202b3b95375263feae501bb39ffe88/spacybert-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "76174197535abe3ab19fcb3e01126c87fdfb4aeb7657c55ac0646c8407aa0145",
          "md5": "74de37b23b23d3f5767b765244c2acb1",
          "sha256": "7e199e1197a2d977da2e1f37d90a115169b9be0d43de88b0449b33f495556e60"
        },
        "downloads": -1,
        "filename": "spacybert-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "74de37b23b23d3f5767b765244c2acb1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5941,
        "upload_time": "2020-08-05T18:02:28",
        "upload_time_iso_8601": "2020-08-05T18:02:28.771200Z",
        "url": "https://files.pythonhosted.org/packages/76/17/4197535abe3ab19fcb3e01126c87fdfb4aeb7657c55ac0646c8407aa0145/spacybert-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f2ecf693d08a1d2c16901123c7a07247df202b3b95375263feae501bb39ffe88",
        "md5": "c2c7899542b9a51c13b3173f9309fd11",
        "sha256": "c758337ba969e425d0ecdb9a3fadfc29bf4ecfc344f00367c2ac97bb8ae4d1b4"
      },
      "downloads": -1,
      "filename": "spacybert-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c2c7899542b9a51c13b3173f9309fd11",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 7620,
      "upload_time": "2020-08-05T18:02:27",
      "upload_time_iso_8601": "2020-08-05T18:02:27.477468Z",
      "url": "https://files.pythonhosted.org/packages/f2/ec/f693d08a1d2c16901123c7a07247df202b3b95375263feae501bb39ffe88/spacybert-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "76174197535abe3ab19fcb3e01126c87fdfb4aeb7657c55ac0646c8407aa0145",
        "md5": "74de37b23b23d3f5767b765244c2acb1",
        "sha256": "7e199e1197a2d977da2e1f37d90a115169b9be0d43de88b0449b33f495556e60"
      },
      "downloads": -1,
      "filename": "spacybert-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "74de37b23b23d3f5767b765244c2acb1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 5941,
      "upload_time": "2020-08-05T18:02:28",
      "upload_time_iso_8601": "2020-08-05T18:02:28.771200Z",
      "url": "https://files.pythonhosted.org/packages/76/17/4197535abe3ab19fcb3e01126c87fdfb4aeb7657c55ac0646c8407aa0145/spacybert-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}