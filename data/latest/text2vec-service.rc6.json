{
  "info": {
    "author": "XuMing",
    "author_email": "xuming624@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "[![PyPI version](https://badge.fury.io/py/text2vec-service.svg)](https://badge.fury.io/py/text2vec-service)\n[![Downloads](https://pepy.tech/badge/text2vec-service)](https://pepy.tech/project/text2vec-service)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![GitHub contributors](https://img.shields.io/github/contributors/shibing624/text2vec-service.svg)](https://github.com/shibing624/text2vec-service/graphs/contributors)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.7%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec-service.svg)](https://github.com/shibing624/text2vec-service/issues)\n[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)\n\n# text2vec-service\nBert model to vector service.\n\n**text2vec-service**搭建了一个高效的文本转向量(Text-To-Vector)服务。\n\n\n**Guide**\n- [Feature](#Feature)\n- [Install](#install)\n- [Usage](#usage)\n- [Contact](#Contact)\n- [Reference](#reference)\n\n\n# Feature\nBERT service with C/S.\n\n# Install\n```shell\npip install torch # conda install pytorch\npip install -U text2vec-service\n```\n\nor\n\n```shell\npip install torch # conda install pytorch\npip install -r requirements.txt\n\ngit clone https://github.com/shibing624/text2vec-service.git\ncd text2vec-service\npip install --no-deps .\n```\n\n# Usage\n#### 1. Start the BERT service\nAfter installing the server, you should be able to use `service-server-start` CLI as follows:\n```bash\nservice-server-start -model_dir shibing624/text2vec-base-chinese \n```\nThis will start a service with four workers, meaning that it can handle up to four **concurrent** requests. \nMore concurrent requests will be queued in a load balancer. \n\n\n<details>\n <summary>Alternatively, one can start the BERT Service in a Docker Container (click to expand...)</summary>\n\n```bash\ndocker build -t text2vec-service -f ./docker/Dockerfile .\nNUM_WORKER=1\nPATH_MODEL=/PATH_TO/_YOUR_MODEL/\ndocker run --runtime nvidia -dit -p 5555:5555 -p 5556:5556 -v $PATH_MODEL:/model -t text2vec-service $NUM_WORKER\n```\n</details>\n\n\n#### 2. Use Client to Get Sentence Encodes\nNow you can encode sentences simply as follows:\n```python\nfrom service.client import BertClient\nbc = BertClient()\nbc.encode(['如何更换花呗绑定银行卡', '花呗更改绑定银行卡'])\n```\nIt will return a `ndarray` (or `List[List[float]]` if you wish), in which each row is a fixed-length vector \nrepresenting a sentence. Having thousands of sentences? Just `encode`! *Don't even bother to batch*, \nthe server will take care of it.\n\n\n\n#### Use BERT Service Remotely\nOne may also start the service on one (GPU) machine and call it from another (CPU) machine as follows:\n\n```python\n# on another CPU machine\nfrom service.client import BertClient\nbc = BertClient(ip='xx.xx.xx.xx')  # ip address of the GPU machine\nbc.encode(['如何更换花呗绑定银行卡', '花呗更改绑定银行卡'])\n```\n\n\n<h2 align=\"center\">Server and Client API</h2>\n<p align=\"right\"><a href=\"#text2vec-service\"><sup>▴ Back to top</sup></a></p>\n\n\n### Server API\n\n```bash\nservice-server-start --help\nservice-server-terminate --help\nservice-server-benchmark --help\n```\n\n| Argument | Type | Default | Description |\n|--------------------|------|-------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `model_dir` | str | *Required* | folder path of the pre-trained BERT model. |\n| `max_seq_len` | int | `25` | maximum length of sequence, longer sequence will be trimmed on the right side. Set it to NONE for dynamically using the longest sequence in a (mini)batch. |\n| `cased_tokenization` | bool | False | Whether tokenizer should skip the default lowercasing and accent removal. Should be used for e.g. the multilingual cased pretrained BERT model. |\n| `num_worker` | int | `1` | number of (GPU/CPU) worker runs BERT model, each works in a separate process. |\n| `max_batch_size` | int | `256` | maximum number of sequences handled by each worker, larger batch will be partitioned into small batches. |\n| `priority_batch_size` | int | `16` | batch smaller than this size will be labeled as high priority, and jumps forward in the job queue to get result faster |\n| `port` | int | `5555` | port for pushing data from client to server |\n| `port_out` | int | `5556`| port for publishing results from server to client |\n| `http_port` | int | None | server port for receiving HTTP requests |\n| `cors` | str | `*` | setting \"Access-Control-Allow-Origin\" for HTTP requests |\n| `gpu_memory_fraction` | float | `0.5` | the fraction of the overall amount of memory that each GPU should be allocated per worker |\n| `cpu` | bool | False | run on CPU instead of GPU |\n| `xla` | bool | False | enable [XLA compiler](https://www.tensorflow.org/xla/jit) for graph optimization (*experimental!*) |\n| `fp16` | bool | False | use float16 precision (experimental) |\n| `device_map` | list | `[]` | specify the list of GPU device ids that will be used (id starts from 0)|\n\n### Client API\n\n\n| Argument | Type | Default | Description |\n|----------------------|------|-----------|-------------------------------------------------------------------------------|\n| `ip` | str | `localhost` | IP address of the server |\n| `port` | int | `5555` | port for pushing data from client to server, *must be consistent with the server side config* |\n| `port_out` | int | `5556`| port for publishing results from server to client, *must be consistent with the server side config* |\n| `output_fmt` | str | `ndarray` | the output format of the sentence encodes, either in numpy array or python List[List[float]] (`ndarray`/`list`) |\n| `show_server_config` | bool | `False` | whether to show server configs when first connected |\n| `check_version` | bool | `True` | whether to force client and server to have the same version |\n| `identity` | str | `None` | a UUID that identifies the client, useful in multi-casting |\n| `timeout` | int | `-1` | set the timeout (milliseconds) for receive operation on the client |\n\nA `BertClient` implements the following methods and properties:\n\n| Method |  Description |\n|--------|------|\n|`.encode()`|Encode a list of strings to a list of vectors|\n|`.encode_async()`|Asynchronous encode batches from a generator|\n|`.fetch()`|Fetch all encoded vectors from server and return them in a generator, use it with `.encode_async()` or `.encode(blocking=False)`. Sending order is NOT preserved.|\n|`.fetch_all()`|Fetch all encoded vectors from server and return them in a list, use it with `.encode_async()` or `.encode(blocking=False)`. Sending order is preserved.|\n|`.close()`|Gracefully close the connection between the client and the server|\n|`.status`|Get the client status in JSON format|\n|`.server_status`|Get the server status in JSON format|\n\n\n<h2 align=\"center\">:book: Tutorial</h2>\n<p align=\"right\"><a href=\"#text2vec-service\"><sup>▴ Back to top</sup></a></p>\n\nThe full list of examples can be found in [`examples/`](examples). You can run each via `python examples/base-demo.py`.\n\n\n### Serving a fine-tuned BERT model\n\nPretrained BERT models often show quite \"okayish\" performance on many tasks. However, to release the true power of \nBERT a fine-tuning on the downstream task (or on domain-specific data) is necessary. \n\nIn this example, serve a fine-tuned BERT model.\n\n```bash\nservice-server-start -model_dir shibing624/bert-base-chinese\n```\n\n\n### Asynchronous encoding\n\n> The complete example can be found [examples/async_demo.py](examples/async_demo.py).\n\n`BertClient.encode()` offers a nice synchronous way to get sentence encodes. \nHowever, sometimes we want to do it in an asynchronous manner by feeding all textual data to the server first, \nfetching the encoded results later. This can be easily done by:\n```python\n# an endless data stream, generating data in an extremely fast speed\ndef text_gen():\n    while True:\n        yield lst_str  # yield a batch of text lines\n\nbc = BertClient()\n\n# get encoded vectors\nfor j in bc.encode_async(text_gen(), max_num_batch=10):\n    print('received %d x %d' % (j.shape[0], j.shape[1]))\n```\n\n### Broadcasting to multiple clients\n\n> example: [examples/multicast_demo.py](examples/multicast_demo.py).\n\nThe encoded result is routed to the client according to its identity. If you have multiple clients with \nsame identity, then they all receive the results! You can use this *multicast* feature to do some cool things, \ne.g. training multiple different models (some using `scikit-learn` some using `pytorch`) in multiple \nseparated processes while only call `BertServer` once. In the example below, `bc` and its two clones will \nall receive encoded vector.\n\n```python\n# clone a client by reusing the identity \ndef client_clone(id, idx):\n    bc = BertClient(identity=id)\n    for j in bc.listen():\n        print('clone-client-%d: received %d x %d' % (idx, j.shape[0], j.shape[1]))\n\nbc = BertClient()\n# start two cloned clients sharing the same identity as bc\nfor j in range(2):\n    threading.Thread(target=client_clone, args=(bc.identity, j)).start()\n\nfor _ in range(3):\n    bc.encode(lst_str)\n```\n\n### Monitoring the service status in a dashboard\n\n> The complete example can [be found in plugin/dashboard/](plugin/dashboard).\n\nAs a part of the infrastructure, one may also want to monitor the service status and show it in a dashboard. To do that, we can use:\n```python\nbc = BertClient(ip='server_ip')\n\njson.dumps(bc.server_status, ensure_ascii=False)\n```\n\nThis gives the current status of the server including number of requests, number of clients etc. in JSON format. The only thing remained is to start a HTTP server for returning this JSON to the frontend that renders it.\n\nAlternatively, one may simply expose an HTTP port when starting a server via:\n\n```bash\nbert-serving-start -http_port 8081\n```\n\nThis will allow one to use javascript or `curl` to fetch the server status at port 8081.\n\n`plugin/dashboard/index.html` shows a simple dashboard based on Bootstrap and Vue.js.\n\n<p align=\"center\"><img src=\"docs/dashboard.png?raw=true\"/></p>\n\n### Using `text2vec-service` to serve HTTP requests in JSON\n\nBesides calling `text2vec-service` from Python, one can also call it via HTTP request in JSON. It is quite \nuseful especially when low transport layer is prohibited. Behind the scene, `text2vec-service` spawns a Flask \nserver in a separate process and then reuse a `BertClient` instance as a proxy to communicate with the ventilator.\n\nTo enable the build-in HTTP server, we need to first (re)install the server with some extra Python dependencies:\n```bash\npip install \"text2vec-service[http]\"\n```\n\nThen simply start the server with:\n```bash\nbert-serving-start -model_dir=/YOUR_MODEL -http_port 8081\n```\n\nDone! Your server is now listening HTTP and TCP requests at port `8081` simultaneously!\n\nTo send a HTTP request, first prepare the payload in JSON as following:\n```json\n{\n    \"id\": 123,\n    \"texts\": [\"hello world\", \"good day!\"]\n}\n```\n, where `id` is a unique identifier helping you to synchronize the results.\n\nThen simply call the server at `/encode` via HTTP POST request. You can use javascript or whatever, here is an \nexample using `curl`:\n```bash\ncurl -X POST http://xx.xx.xx.xx:8081/encode \\\n  -H 'content-type: application/json' \\\n  -d '{\"id\": 123,\"texts\": ['如何更换花呗绑定银行卡', '花呗更改绑定银行卡']}'\n```\n, which returns a JSON:\n```json\n{\n    \"id\": 123,\n    \"results\": [[768 float-list], [768 float-list]],\n    \"status\": 200\n}\n```\n\nTo get the server's status and client's status, you can send GET requests at `/status/server` and `/status/client`, \nrespectively.\n\nFinally, one may also config CORS to restrict the public access of the server by specifying `-cors` when \nstarting `bert-serving-start`. By default `-cors=*`, meaning the server is public accessible.\n\n\n### Starting `BertServer` from Python\n\nBesides shell, one can also start a `BertServer` from python. Simply do\n```python\nfrom service.server.helper import get_args_parser\nfrom service.server import BertServer\nargs = get_args_parser().parse_args(['-model_dir', 'YOUR_MODEL_PATH_HERE',\n                                     '-port', '5555',\n                                     '-port_out', '5556',\n                                     '-max_seq_len', 'NONE',\n                                     '-mask_cls_sep',\n                                     '-cpu'])\nserver = BertServer(args)\nserver.start()\n``` \n\nNote that it's basically mirroring the arg-parsing behavior in CLI, so everything in that `.parse_args([])` list \nshould be string, e.g. `['-port', '5555']` not `['-port', 5555]`.\n\nTo shutdown the server, you may call the static method in `BertServer` class via with args:\n```python\nshut_args = get_shutdown_parser().parse_args(['-ip','localhost','-port','5555','-timeout','5000'])\nBertServer.shutdown(shutdown_args)\n```\n\nOr via shell CLI:\n```bash\nbert-serving-terminate -port 5555\n```\n\nThis will terminate the server running on localhost at port 5555. You may also use it to terminate a remote server, \nsee `bert-serving-terminate --help` for details.\n\n\n\n# Contact\n\n- Issue(建议)：[![GitHub issues](https://img.shields.io/github/issues/shibing624/text2vec-service.svg)](https://github.com/shibing624/text2vec-service/issues)\n- 邮件我：xuming: xuming624@qq.com\n- 微信我：\n加我*微信号：xuming624, 备注：姓名-公司-NLP* 进NLP交流群。\n\n<img src=\"docs/wechat.jpeg\" width=\"200\" />\n\n\n# Citation\n\n如果你在研究中使用了text2vec-service，请按如下格式引用：\n\nAPA:\n```latex\nXu, M. text2vec-service: Bert model embedding service (Version 0.0.2) [Computer software]. https://github.com/shibing624/text2vec-service\n```\n\nBibTeX:\n```latex\n@software{Xu_text2vec-service_Text_to,\nauthor = {Xu, Ming},\ntitle = {{text2vec-service: Bert model embedding service}},\nurl = {https://github.com/shibing624/text2vec-service},\nversion = {0.0.2}\n}\n```\n\n# License\n\n\n授权协议为 [The Apache License 2.0](LICENSE)，可免费用做商业用途。请在产品说明中附加text2vec-service的链接和授权协议。\n\n\n# Contribute\n项目代码还很粗糙，如果大家对代码有所改进，欢迎提交回本项目，在提交之前，注意以下两点：\n\n - 在`tests`添加相应的单元测试\n - 使用`python -m pytest -v`来运行所有单元测试，确保所有单测都是通过的\n\n之后即可提交PR。\n\n# Reference\n- [jina-ai/clip-as-service](https://github.com/jina-ai/clip-as-service)\n- [huggingface/transformers](https://github.com/huggingface/transformers)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/shibing624/text2vec-service",
    "keywords": "bert,nlp,pytorch,machine learning,sentence encoding embedding serving",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "text2vec-service",
    "package_url": "https://pypi.org/project/text2vec-service/",
    "platform": null,
    "project_url": "https://pypi.org/project/text2vec-service/",
    "project_urls": {
      "Homepage": "https://github.com/shibing624/text2vec-service"
    },
    "release_url": "https://pypi.org/project/text2vec-service/0.0.6/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Mapping a variable-length sentence to a fixed-length vector using BERT model (Server)",
    "version": "0.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13915922,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bc511b1fa652d7f6c4eca64c0a8b7d37d589651fd2360f108c1d7d2280bbea3f",
          "md5": "6f0a65928b544ad37e9b892644707f38",
          "sha256": "2f0bc2b7a814767312e9abf20f199bd9e61e7bdad3775fd9052c595cec80c5e3"
        },
        "downloads": -1,
        "filename": "text2vec-service-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "6f0a65928b544ad37e9b892644707f38",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 29380,
        "upload_time": "2022-05-16T11:52:10",
        "upload_time_iso_8601": "2022-05-16T11:52:10.625464Z",
        "url": "https://files.pythonhosted.org/packages/bc/51/1b1fa652d7f6c4eca64c0a8b7d37d589651fd2360f108c1d7d2280bbea3f/text2vec-service-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "21b9e2c16e62d0c50c15093308ecd58fa64b8ff6c212b8c739321bc5baf7d63e",
          "md5": "4a4bb51c19566157115733990a5d9da3",
          "sha256": "a6688db0afd4060888574e06afe73a0556e181aeaeb390c92c5d7a84f02d821b"
        },
        "downloads": -1,
        "filename": "text2vec-service-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "4a4bb51c19566157115733990a5d9da3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30656,
        "upload_time": "2022-05-16T12:21:53",
        "upload_time_iso_8601": "2022-05-16T12:21:53.076907Z",
        "url": "https://files.pythonhosted.org/packages/21/b9/e2c16e62d0c50c15093308ecd58fa64b8ff6c212b8c739321bc5baf7d63e/text2vec-service-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "df47bd5def6939b4e846735b173af966426be60ae9c0a3f3c07c828fddbe8aba",
          "md5": "b5c2c75fe66fc7224307675a1e9329d5",
          "sha256": "8a3ce7bc7e7110444dc5ea15cfd505ea3e6ddacee4f91ec328b943f4666514ab"
        },
        "downloads": -1,
        "filename": "text2vec-service-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "b5c2c75fe66fc7224307675a1e9329d5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30485,
        "upload_time": "2022-05-16T13:17:50",
        "upload_time_iso_8601": "2022-05-16T13:17:50.692031Z",
        "url": "https://files.pythonhosted.org/packages/df/47/bd5def6939b4e846735b173af966426be60ae9c0a3f3c07c828fddbe8aba/text2vec-service-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5a7300cabbbeb639a723207ad96010150ec3d887d5674461cacc26f7009afb18",
          "md5": "b1d327a29a5ae2670e73beedf7e7595b",
          "sha256": "c1ad6e94026f69e04fc1afa458392e0c996057aef57abfa932f76723863f6dc6"
        },
        "downloads": -1,
        "filename": "text2vec-service-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "b1d327a29a5ae2670e73beedf7e7595b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30510,
        "upload_time": "2022-05-16T15:13:49",
        "upload_time_iso_8601": "2022-05-16T15:13:49.038893Z",
        "url": "https://files.pythonhosted.org/packages/5a/73/00cabbbeb639a723207ad96010150ec3d887d5674461cacc26f7009afb18/text2vec-service-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "abf9d4c1a1635119dc28b089c4380498517e375f52f2abfb6d05e8878c7bb717",
          "md5": "266b064dc960d06d55989b77783b09f6",
          "sha256": "c08e218d291923264482ddcd2d843e8038f7f1d3d34f81921795c3835de5309a"
        },
        "downloads": -1,
        "filename": "text2vec-service-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "266b064dc960d06d55989b77783b09f6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 31086,
        "upload_time": "2022-05-19T05:30:23",
        "upload_time_iso_8601": "2022-05-19T05:30:23.231076Z",
        "url": "https://files.pythonhosted.org/packages/ab/f9/d4c1a1635119dc28b089c4380498517e375f52f2abfb6d05e8878c7bb717/text2vec-service-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "28139e819755cb07db359f080239b8e39f9c891d2c203ed1325ced088037048e",
          "md5": "8b16630d2b811cb5163c3d279088c9b4",
          "sha256": "8f03d36d43b3acbf71871e8561f76eb06295d2d0d0a8850f3fc13aaee6d3ac70"
        },
        "downloads": -1,
        "filename": "text2vec-service-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "8b16630d2b811cb5163c3d279088c9b4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30976,
        "upload_time": "2022-05-24T03:26:56",
        "upload_time_iso_8601": "2022-05-24T03:26:56.394923Z",
        "url": "https://files.pythonhosted.org/packages/28/13/9e819755cb07db359f080239b8e39f9c891d2c203ed1325ced088037048e/text2vec-service-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "28139e819755cb07db359f080239b8e39f9c891d2c203ed1325ced088037048e",
        "md5": "8b16630d2b811cb5163c3d279088c9b4",
        "sha256": "8f03d36d43b3acbf71871e8561f76eb06295d2d0d0a8850f3fc13aaee6d3ac70"
      },
      "downloads": -1,
      "filename": "text2vec-service-0.0.6.tar.gz",
      "has_sig": false,
      "md5_digest": "8b16630d2b811cb5163c3d279088c9b4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 30976,
      "upload_time": "2022-05-24T03:26:56",
      "upload_time_iso_8601": "2022-05-24T03:26:56.394923Z",
      "url": "https://files.pythonhosted.org/packages/28/13/9e819755cb07db359f080239b8e39f9c891d2c203ed1325ced088037048e/text2vec-service-0.0.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}