{
  "info": {
    "author": "wangzejun",
    "author_email": "wangzejunscut@126.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "gpt2ppl-zh: 基于中文 GPT2 预训练模型的语句困惑度计算\n====================================================\n\n本项目展示了使用中文 GPT2 预训练模型实现文档/语句的困惑度计算。\n\n.. code:: shell\n\n   pip install gpt2ppl-zh\n\n或者\n\n.. code:: shell\n\n   pip install git+https://github.com/zejunwang1/gpt2ppl-zh\n\n句子困惑度计算示例可参考 gpt2ppl/demo_sent.py:\n\n.. code:: python\n\n   # coding=utf-8\n\n   import numpy as np\n   from gpt2ppl import GPT2PPL\n\n   def main():\n       sent1 = \"机器学习与人工智能\"\n       sent2 = \"机器学习与人工只能\"\n\n       model = GPT2PPL()\n       \"\"\"\n       model = GPT2PPL(\n           model_name_or_path=\"uer/gpt2-chinese-cluecorpussmall\",\n           tokenizer_mode=\"bert\",\n           device=\"cuda\",\n           stride=512\n       )\n       \"\"\"\n\n       ppl1 = model.get_ppl(sent1)\n       ppl2 = model.get_ppl(sent2)\n\n       print(\"Sentence: {}, Perplexity Score: {}\".format(sent1, ppl1))\n       print(\"Sentence: {}, Perplexity Score: {}\".format(sent2, ppl2))\n\n   if __name__ == \"__main__\":\n       main()\n\n运行结果：\n\n::\n\n   Sentence: 机器学习与人工智能, Perplexity Score: 5.058856964111328\n   Sentence: 机器学习与人工只能, Perplexity Score: 74.32606506347656\n\n文档困惑度计算示例可参考\ngpt2ppl/demo_doc.py，程序会计算文档中每个句子的困惑度：\n\n.. code:: python\n\n   result = model.get_ppl_per_sentence(text, min_length=8)\n   \"\"\"\n   result = model.get_ppl_per_sentence(\n       text,\n       ratio=0.4, \n       min_length=8, \n       max_length=510, \n       return_loc=False\n   )\n   min_length: 分句的最小句子长度\n   max_length: 分句的最大句子长度\n   ratio: 句子中汉字字符的最小比例，若低于该阈值，则句子不参与困惑度计算\n   return_loc: 是否返回每个句子在原始文档中的字符位置\n   \"\"\"\n\n运行结果如下：\n\n::\n\n   The Perplexity Score of the whole text is:  5.978885650634766\n   The Perplexity Score for single sentences:\n   ('中文分词是一种将连续的中文文本划分成有意义的词汇序列的自然语言处理技术。', 17.03954315185547)\n   ('中文分词在中文文本处理中非常重要，因为汉字并没有空格或其他分隔符来区分单词，这给中文文本处理带来了困难。', 10.517306327819824)\n   ('本文将介绍中文分词的原理和常用方法。', 19.19382667541504)\n   ('中文分词的原理是基于词语在语料库中的频率来确定一个句子中的分词方案。', 18.930423736572266)\n   ('分词的目标是将句子中的每个字符分配到一个词语中，从而获得有意义的词语序列。', 13.634822845458984)\n   ('这样，文本就可以被更好地理解和分析。', 10.56257438659668)\n   ('中文分词的方法有很多，其中最常用的是基于统计学的方法和基于规则的方法。', 8.299059867858887)\n   ('基于统计学的方法通过对大量中文文本语料库的分析来得出一个概率模型，该模型根据词频和词语之间的关联性来判断每个字符的最佳分词位置。', 12.321337699890137)\n   ('这种方法的优点是可以自适应地适应不同类型的文本，并且能够处理新词和生僻词，但是也有一些缺点，例如需要大量的语料库来训练模型，模型的效果受到数据质量和模型参数的影响。', 10.90810489654541)\n   ('基于规则的方法则是依赖于一系列语法规则和词库，例如词典、词性标注、句法分析等，来判断每个字符的最佳分词位置。', 17.036846160888672)\n   ('这种方法的优点是可以精确地控制分词结果，适合处理特定领域的文本，但是需要手动构建规则和词库，工作量比较大。', 14.91514778137207)\n   ('除了基于统计学的方法和基于规则的方法，还有一些混合方法，例如将基于统计学的方法和基于规则的方法结合起来，以获得更好的分词效果。', 5.419482707977295)\n   ('总之，中文分词是中文文本处理中的重要技术之一。', 15.306158065795898)\n   ('在实际应用中，根据不同的需求和场景，可以选择不同的分词方法来获得最佳的分词结果。', 7.240055084228516)\n   Average Perplexity Score:  12.95176352773394\n   Burstiness Score:  4.184883485157172\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/zejunwang1/gpt2ppl-zh",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "gpt2ppl-zh",
    "package_url": "https://pypi.org/project/gpt2ppl-zh/",
    "platform": null,
    "project_url": "https://pypi.org/project/gpt2ppl-zh/",
    "project_urls": {
      "Homepage": "https://github.com/zejunwang1/gpt2ppl-zh"
    },
    "release_url": "https://pypi.org/project/gpt2ppl-zh/0.2.0/",
    "requires_dist": [
      "transformers",
      "torch"
    ],
    "requires_python": "",
    "summary": "Chinese sentence perplexity calculation based on GPT2 pre-trained model",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17458602,
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e6859bab0b3d6afc340d776492e3123b596636855c67389abab47159c9a9d5d3",
          "md5": "f30f98ce19d68251b7a350d6866fc09f",
          "sha256": "4319b518586e6de264aa3097cb4feb5b1dd13e28a19e45efb380fd9d1391512c"
        },
        "downloads": -1,
        "filename": "gpt2ppl_zh-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f30f98ce19d68251b7a350d6866fc09f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 7270,
        "upload_time": "2023-03-27T08:58:45",
        "upload_time_iso_8601": "2023-03-27T08:58:45.395455Z",
        "url": "https://files.pythonhosted.org/packages/e6/85/9bab0b3d6afc340d776492e3123b596636855c67389abab47159c9a9d5d3/gpt2ppl_zh-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7faf41efc3cd3131c6f6abfa2d6db49d39b97c2af74b0f8f3f3596c4977198d6",
          "md5": "a4f2ab9dda6efc4b34b0457593d1a0cb",
          "sha256": "761b8afb403703ca4a6d4d5f8107c1f0babeda572095661652a15b14151143fc"
        },
        "downloads": -1,
        "filename": "gpt2ppl-zh-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a4f2ab9dda6efc4b34b0457593d1a0cb",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5277,
        "upload_time": "2023-03-27T08:58:48",
        "upload_time_iso_8601": "2023-03-27T08:58:48.145673Z",
        "url": "https://files.pythonhosted.org/packages/7f/af/41efc3cd3131c6f6abfa2d6db49d39b97c2af74b0f8f3f3596c4977198d6/gpt2ppl-zh-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e6859bab0b3d6afc340d776492e3123b596636855c67389abab47159c9a9d5d3",
        "md5": "f30f98ce19d68251b7a350d6866fc09f",
        "sha256": "4319b518586e6de264aa3097cb4feb5b1dd13e28a19e45efb380fd9d1391512c"
      },
      "downloads": -1,
      "filename": "gpt2ppl_zh-0.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f30f98ce19d68251b7a350d6866fc09f",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 7270,
      "upload_time": "2023-03-27T08:58:45",
      "upload_time_iso_8601": "2023-03-27T08:58:45.395455Z",
      "url": "https://files.pythonhosted.org/packages/e6/85/9bab0b3d6afc340d776492e3123b596636855c67389abab47159c9a9d5d3/gpt2ppl_zh-0.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7faf41efc3cd3131c6f6abfa2d6db49d39b97c2af74b0f8f3f3596c4977198d6",
        "md5": "a4f2ab9dda6efc4b34b0457593d1a0cb",
        "sha256": "761b8afb403703ca4a6d4d5f8107c1f0babeda572095661652a15b14151143fc"
      },
      "downloads": -1,
      "filename": "gpt2ppl-zh-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "a4f2ab9dda6efc4b34b0457593d1a0cb",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 5277,
      "upload_time": "2023-03-27T08:58:48",
      "upload_time_iso_8601": "2023-03-27T08:58:48.145673Z",
      "url": "https://files.pythonhosted.org/packages/7f/af/41efc3cd3131c6f6abfa2d6db49d39b97c2af74b0f8f3f3596c4977198d6/gpt2ppl-zh-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}