{
  "info": {
    "author": "Pablo Hoffman",
    "author_email": "pablo@pablohoffman.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: Console",
      "Framework :: Scrapy",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Topic :: Internet :: WWW/HTTP",
      "Topic :: Software Development :: Libraries :: Application Frameworks",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "======\nScrapy\n======\n\n.. image:: https://img.shields.io/pypi/v/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: PyPI Version\n\n.. image:: https://img.shields.io/pypi/dm/Scrapy.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: PyPI Monthly downloads\n\n.. image:: https://img.shields.io/travis/scrapy/scrapy/master.svg\n   :target: http://travis-ci.org/scrapy/scrapy\n   :alt: Build Status\n\n.. image:: https://img.shields.io/badge/wheel-yes-brightgreen.svg\n   :target: https://pypi.python.org/pypi/Scrapy\n   :alt: Wheel Status\n   \n.. image:: http://static.scrapy.org/py3progress/badge.svg\n   :target: https://github.com/scrapy/scrapy/wiki/Python-3-Porting\n   :alt: Python 3 Porting Status\n\n.. image:: https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg\n   :target: http://codecov.io/github/scrapy/scrapy?branch=master\n   :alt: Coverage report\n\n\nOverview\n========\n\nScrapy is a fast high-level web crawling and web scraping framework, used to\ncrawl websites and extract structured data from their pages. It can be used for\na wide range of purposes, from data mining to monitoring and automated testing.\n\nFor more information including a list of features check the Scrapy homepage at:\nhttp://scrapy.org\n\nRequirements\n============\n\n* Python 2.7 or Python 3.3+\n* Works on Linux, Windows, Mac OSX, BSD\n\nInstall\n=======\n\nThe quick way::\n\n    pip install scrapy\n\nFor more details see the install section in the documentation:\nhttp://doc.scrapy.org/en/latest/intro/install.html\n\nReleases\n========\n\nYou can download the latest stable and development releases from:\nhttp://scrapy.org/download/\n\nDocumentation\n=============\n\nDocumentation is available online at http://doc.scrapy.org/ and in the ``docs``\ndirectory.\n\nCommunity (blog, twitter, mail list, IRC)\n=========================================\n\nSee http://scrapy.org/community/\n\nContributing\n============\n\nPlease note that this project is released with a Contributor Code of Conduct\n(see https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).\n\nBy participating in this project you agree to abide by its terms.\nPlease report unacceptable behavior to opensource@scrapinghub.com.\n\nSee http://doc.scrapy.org/en/master/contributing.html\n\nCompanies using Scrapy\n======================\n\nSee http://scrapy.org/companies/\n\nCommercial Support\n==================\n\nSee http://scrapy.org/support/",
    "description_content_type": null,
    "docs_url": null,
    "download_url": "UNKNOWN",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://scrapy.org",
    "keywords": null,
    "license": "BSD",
    "maintainer": null,
    "maintainer_email": null,
    "name": "cyberplant-Scrapy",
    "package_url": "https://pypi.org/project/cyberplant-Scrapy/",
    "platform": "UNKNOWN",
    "project_url": "https://pypi.org/project/cyberplant-Scrapy/",
    "project_urls": {
      "Download": "UNKNOWN",
      "Homepage": "http://scrapy.org"
    },
    "release_url": "https://pypi.org/project/cyberplant-Scrapy/1.2.0.dev2/",
    "requires_dist": null,
    "requires_python": null,
    "summary": "A high-level Web Crawling and Web Scraping framework",
    "version": "1.2.0.dev2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 2172419,
  "releases": {
    "1.2.0.dev2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "424198d4ed775d4122ccea52e09fb1b5f4471686edd678c39896f51437720856",
          "md5": "29819d0e3f51745d1667a66dfa4dde23",
          "sha256": "41393a25df0bb371a2c7a05ead2e64b8d2df1c263109e3fac2971ca0f35974cb"
        },
        "downloads": -1,
        "filename": "cyberplant_Scrapy-1.2.0.dev2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "29819d0e3f51745d1667a66dfa4dde23",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 294573,
        "upload_time": "2016-06-17T05:10:45",
        "upload_time_iso_8601": "2016-06-17T05:10:45.536380Z",
        "url": "https://files.pythonhosted.org/packages/42/41/98d4ed775d4122ccea52e09fb1b5f4471686edd678c39896f51437720856/cyberplant_Scrapy-1.2.0.dev2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dc4488b02cbc6b924e6e4239befbe5e29c5ed3325640efb1c7c3a60754c97e2a",
          "md5": "18bd13f36466d2596d6e82389ddebb7d",
          "sha256": "1c2e64205ea728fa177147eb8179c94aebbe6a919b5f56c38f75b558bb0bfb46"
        },
        "downloads": -1,
        "filename": "cyberplant-Scrapy-1.2.0.dev2.tar.gz",
        "has_sig": false,
        "md5_digest": "18bd13f36466d2596d6e82389ddebb7d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 821356,
        "upload_time": "2016-06-17T05:10:50",
        "upload_time_iso_8601": "2016-06-17T05:10:50.546491Z",
        "url": "https://files.pythonhosted.org/packages/dc/44/88b02cbc6b924e6e4239befbe5e29c5ed3325640efb1c7c3a60754c97e2a/cyberplant-Scrapy-1.2.0.dev2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "424198d4ed775d4122ccea52e09fb1b5f4471686edd678c39896f51437720856",
        "md5": "29819d0e3f51745d1667a66dfa4dde23",
        "sha256": "41393a25df0bb371a2c7a05ead2e64b8d2df1c263109e3fac2971ca0f35974cb"
      },
      "downloads": -1,
      "filename": "cyberplant_Scrapy-1.2.0.dev2-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "29819d0e3f51745d1667a66dfa4dde23",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 294573,
      "upload_time": "2016-06-17T05:10:45",
      "upload_time_iso_8601": "2016-06-17T05:10:45.536380Z",
      "url": "https://files.pythonhosted.org/packages/42/41/98d4ed775d4122ccea52e09fb1b5f4471686edd678c39896f51437720856/cyberplant_Scrapy-1.2.0.dev2-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dc4488b02cbc6b924e6e4239befbe5e29c5ed3325640efb1c7c3a60754c97e2a",
        "md5": "18bd13f36466d2596d6e82389ddebb7d",
        "sha256": "1c2e64205ea728fa177147eb8179c94aebbe6a919b5f56c38f75b558bb0bfb46"
      },
      "downloads": -1,
      "filename": "cyberplant-Scrapy-1.2.0.dev2.tar.gz",
      "has_sig": false,
      "md5_digest": "18bd13f36466d2596d6e82389ddebb7d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 821356,
      "upload_time": "2016-06-17T05:10:50",
      "upload_time_iso_8601": "2016-06-17T05:10:50.546491Z",
      "url": "https://files.pythonhosted.org/packages/dc/44/88b02cbc6b924e6e4239befbe5e29c5ed3325640efb1c7c3a60754c97e2a/cyberplant-Scrapy-1.2.0.dev2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}