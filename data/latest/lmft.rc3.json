{
  "info": {
    "author": "XuMing",
    "author_email": "xuming624@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "[![PyPI version](https://badge.fury.io/py/lmft.svg)](https://badge.fury.io/py/lmft)\n[![Downloads](https://pepy.tech/badge/lmft)](https://pepy.tech/project/lmft)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![GitHub contributors](https://img.shields.io/github/contributors/shibing624/lmft.svg)](https://github.com/shibing624/lmft/graphs/contributors)\n[![License Apache 2.0](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)\n[![python_version](https://img.shields.io/badge/Python-3.8%2B-green.svg)](requirements.txt)\n[![GitHub issues](https://img.shields.io/github/issues/shibing624/lmft.svg)](https://github.com/shibing624/lmft/issues)\n[![Wechat Group](http://vlog.sfyc.ltd/wechat_everyday/wxgroup_logo.png?imageView2/0/w/60/h/20)](#Contact)\n\n# LMFT: Language Model Fine-Tuning\nLanguage Model Fine-Tuning, for ChatGLM, BELLE, LLaMA fine-tuning.\n\n\n**lmft**实现了ChatGLM-6B的模型finetune。\n\n\n**Guide**\n- [Feature](#Feature)\n- [Evaluation](#Evaluation)\n- [Demo](#Demo)\n- [Install](#install)\n- [Usage](#usage)\n- [Contact](#Contact)\n- [Reference](#reference)\n\n\n# Feature\n### ChatGLM\n#### [THUDM/chatglm-6b](https://huggingface.co/THUDM/chatglm-6b) 模型的Finetune训练\n\n[THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)放出的默认模型，模型以 FP16 精度加载，模型运行需要 13GB 显存，训练需要 29GB 显存。\n#### [THUDM/chatglm-6b-int4-qe](https://huggingface.co/THUDM/chatglm-6b-int4-qe) 模型的Finetune训练\n\n[THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)放出的int4并对Embedding量化后的模型，模型运行需要 4.3GB 显存，训练需要 8GB 以上显存。\n\n\n# Evaluation\n\n### 对话生成\n\n# Demo\n\nHuggingFace Demo: https://huggingface.co/spaces/shibing624/lmft\n\n![](docs/hf.png)\n\nrun example: [examples/gradio_demo.py](examples/gradio_demo.py) to see the demo:\n```shell\npython examples/gradio_demo.py\n```\n\n# Install\n```shell\npip install -U lmft\n```\n\nor\n\n```shell\npip install -r requirements.txt\n\ngit clone https://github.com/shibing624/lmft.git\ncd lmft\npip install --no-deps .\n```\n\n# Usage\n\n## 训练ChatGLM-6B模型\n\nexample: [examples/train_chatglm_demo.py](examples/train_chatglm_demo.py)\n\n```python\nimport sys\n\nsys.path.append('..')\nfrom lmft import ChatGLMTune\n\n\ndef finetune_demo():\n    m = ChatGLMTune('chatglm', \"THUDM/chatglm-6b\", args={'use_lora': True})\n    m.train_model(train_data='shibing624/alpaca-zh')\n    r = m.predict(['你是谁', '三原色是啥'])\n    print(r)\n    response, history = m.chat(\"你好\", history=[])\n    print(response)\n    response, history = m.chat(\"晚上睡不着应该怎么办\", history=history)\n    print(response)\n\n\ndef origin_chat_demo():\n    m = ChatGLMTune('chatglm', \"THUDM/chatglm-6b\", args={'use_lora': False})\n    response, history = m.chat(\"你好\", history=[])\n    print(response)\n    response, history = m.chat(\"晚上睡不着应该怎么办\", history=history)\n    print(response)\n\n\nif __name__ == '__main__':\n    origin_chat_demo()\n    finetune_demo()\n```\n\noutput:\n```\n问:hi\n答:hi \n\n[Round 1]\n问:晚上睡不着应该怎么办\n答: 想要在晚上入睡,但并不容易,可以参考下述技巧:\n1. 睡前放松:尝试进行一些放松的活动,如冥想、深呼吸或瑜伽,帮助放松身心,减轻压力和焦虑。\n2. 创造一个舒适的睡眠环境:保持房间安静、黑暗和凉爽,使用舒适的床垫和枕头,确保床铺干净整洁。\n3. 规律的睡眠时间表:保持规律的睡眠时间表,尽可能在同一时间上床,并创造一个固定的起床时间。\n4. 避免刺激性食物和饮料:避免在睡前饮用含咖啡因的饮料,如咖啡、茶和可乐,以及吃辛辣、油腻或难以消化的食物。\n5. 避免过度使用电子设备:避免在睡前使用电子设备,如手机、电视和电脑。这些设备会发出蓝光,干扰睡眠。\n如果尝试了这些技巧仍然无法入睡,建议咨询医生或睡眠专家,获取更专业的建议和帮助。\n```\n\n\n#### dataset\n1. [0.5M生成的中文ChatGPT结果数据](https://huggingface.co/datasets/BelleGroup/generated_train_0.5M_CN)\n2. [50k English Stanford Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca#data-release)\n\n\n# Contact\n\n- Issue(建议)：[![GitHub issues](https://img.shields.io/github/issues/shibing624/lmft.svg)](https://github.com/shibing624/lmft/issues)\n- 邮件我：xuming: xuming624@qq.com\n- 微信我：加我*微信号：xuming624, 备注：姓名-公司-NLP* 进NLP交流群。\n\n<img src=\"docs/wechat.jpeg\" width=\"200\" />\n\n\n# Citation\n\n如果你在研究中使用了lmft，请按如下格式引用：\n\nAPA:\n```latex\nXu, M. lmft: Lanauge Model Fine-Tuning toolkit (Version 1.1.2) [Computer software]. https://github.com/shibing624/lmft\n```\n\nBibTeX:\n```latex\n@misc{lmft,\n  author = {Xu, Ming},\n  title = {lmft: Language Model Fine-Tuning toolkit},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\\url{https://github.com/shibing624/lmft}},\n}\n```\n\n# License\n\n\n授权协议为 [The Apache License 2.0](LICENSE)，可免费用做商业用途。请在产品说明中附加lmft的链接和授权协议。\n\n\n# Contribute\n项目代码还很粗糙，如果大家对代码有所改进，欢迎提交回本项目，在提交之前，注意以下两点：\n\n - 在`tests`添加相应的单元测试\n - 使用`python -m pytest -v`来运行所有单元测试，确保所有单测都是通过的\n\n之后即可提交PR。\n\n# Reference\n- [LianjiaTech/BELLE](https://github.com/LianjiaTech/BELLE)\n- [tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)\n- [huggingface/peft](https://github.com/huggingface/peft)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/shibing624/lmft",
    "keywords": "lmft,GPT2,transformers,pytorch,language model",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lmft",
    "package_url": "https://pypi.org/project/lmft/",
    "platform": null,
    "project_url": "https://pypi.org/project/lmft/",
    "project_urls": {
      "Homepage": "https://github.com/shibing624/lmft"
    },
    "release_url": "https://pypi.org/project/lmft/0.2.2/",
    "requires_dist": null,
    "requires_python": ">=3.7.0",
    "summary": "Language Model Fine-tuning Toolkit (LMFT)",
    "version": "0.2.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17461306,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a27da82c0897d65172e82c178f53b5bf37ee88f1508e8e01d32ccdfae1e8efb1",
          "md5": "233bbce80ba0424780138a1c61606754",
          "sha256": "a87b6228436dce973b7035924571897bc5cf48ab5f239359f69976573c229942"
        },
        "downloads": -1,
        "filename": "lmft-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "233bbce80ba0424780138a1c61606754",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 13994,
        "upload_time": "2023-03-23T14:03:23",
        "upload_time_iso_8601": "2023-03-23T14:03:23.269588Z",
        "url": "https://files.pythonhosted.org/packages/a2/7d/a82c0897d65172e82c178f53b5bf37ee88f1508e8e01d32ccdfae1e8efb1/lmft-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "738499f720c073fbbaa46a058b03c41b9a7dd250cf08dec8264062508c56df4e",
          "md5": "20ca00d4f316f6b7b6ba69c4e6222517",
          "sha256": "f4ed2401db7052be7d759c552fb80fa4250431e50685cb88709a82590c81d781"
        },
        "downloads": -1,
        "filename": "lmft-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "20ca00d4f316f6b7b6ba69c4e6222517",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 54669,
        "upload_time": "2023-03-27T10:13:44",
        "upload_time_iso_8601": "2023-03-27T10:13:44.248958Z",
        "url": "https://files.pythonhosted.org/packages/73/84/99f720c073fbbaa46a058b03c41b9a7dd250cf08dec8264062508c56df4e/lmft-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "37845dcc51efef4f2bcb40a4584e24330eabe55a1afb2c73dfd7de1a5d73d96c",
          "md5": "b7d1247a5817a852b8eafad9c00a661e",
          "sha256": "2491234efca7c8a3f4e4b7c9592e5bea4b81794843d295b0e24439a39b5ba3a0"
        },
        "downloads": -1,
        "filename": "lmft-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "b7d1247a5817a852b8eafad9c00a661e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 54708,
        "upload_time": "2023-03-27T13:40:48",
        "upload_time_iso_8601": "2023-03-27T13:40:48.862629Z",
        "url": "https://files.pythonhosted.org/packages/37/84/5dcc51efef4f2bcb40a4584e24330eabe55a1afb2c73dfd7de1a5d73d96c/lmft-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "37845dcc51efef4f2bcb40a4584e24330eabe55a1afb2c73dfd7de1a5d73d96c",
        "md5": "b7d1247a5817a852b8eafad9c00a661e",
        "sha256": "2491234efca7c8a3f4e4b7c9592e5bea4b81794843d295b0e24439a39b5ba3a0"
      },
      "downloads": -1,
      "filename": "lmft-0.2.2.tar.gz",
      "has_sig": false,
      "md5_digest": "b7d1247a5817a852b8eafad9c00a661e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.0",
      "size": 54708,
      "upload_time": "2023-03-27T13:40:48",
      "upload_time_iso_8601": "2023-03-27T13:40:48.862629Z",
      "url": "https://files.pythonhosted.org/packages/37/84/5dcc51efef4f2bcb40a4584e24330eabe55a1afb2c73dfd7de1a5d73d96c/lmft-0.2.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}