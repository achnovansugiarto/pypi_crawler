{
  "info": {
    "author": "Andrew Svetlov",
    "author_email": "andrew.svetlov@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Framework :: AsyncIO",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: System :: Distributed Computing",
      "Topic :: System :: Networking"
    ],
    "description": "aiokafka\n========\n.. image:: https://travis-ci.com/aio-libs/aiokafka.svg?branch=master\n    :target: https://travis-ci.com/aio-libs/aiokafka\n    :alt: |Build status|\n.. image:: https://codecov.io/github/aio-libs/aiokafka/coverage.svg?branch=master\n    :target: https://codecov.io/gh/aio-libs/aiokafka/branch/master\n    :alt: |Coverage|\n.. image:: https://badges.gitter.im/Join%20Chat.svg\n    :target: https://gitter.im/aio-libs/Lobby\n    :alt: |Chat on Gitter|\n\nasyncio client for Kafka\n\n\nAIOKafkaProducer\n****************\n\nAIOKafkaProducer is a high-level, asynchronous message producer.\n\nExample of AIOKafkaProducer usage:\n\n.. code-block:: python\n\n    from aiokafka import AIOKafkaProducer\n    import asyncio\n\n    async def send_one():\n        producer = AIOKafkaProducer(bootstrap_servers='localhost:9092')\n        # Get cluster layout and initial topic/partition leadership information\n        await producer.start()\n        try:\n            # Produce message\n            await producer.send_and_wait(\"my_topic\", b\"Super message\")\n        finally:\n            # Wait for all pending messages to be delivered or expire.\n            await producer.stop()\n\n    asyncio.run(send_one())\n\n\nAIOKafkaConsumer\n****************\n\nAIOKafkaConsumer is a high-level, asynchronous message consumer.\nIt interacts with the assigned Kafka Group Coordinator node to allow multiple\nconsumers to load balance consumption of topics (requires kafka >= 0.9.0.0).\n\nExample of AIOKafkaConsumer usage:\n\n.. code-block:: python\n\n    from aiokafka import AIOKafkaConsumer\n    import asyncio\n\n    async def consume():\n        consumer = AIOKafkaConsumer(\n            'my_topic', 'my_other_topic',\n            bootstrap_servers='localhost:9092',\n            group_id=\"my-group\")\n        # Get cluster layout and join group `my-group`\n        await consumer.start()\n        try:\n            # Consume messages\n            async for msg in consumer:\n                print(\"consumed: \", msg.topic, msg.partition, msg.offset,\n                      msg.key, msg.value, msg.timestamp)\n        finally:\n            # Will leave consumer group; perform autocommit if enabled.\n            await consumer.stop()\n\n    asyncio.run(consume())\n\nRunning tests\n-------------\n\nDocker is required to run tests. See https://docs.docker.com/engine/installation for installation notes. Also note, that `lz4` compression libraries for python will require `python-dev` package,\nor python source header files for compilation on Linux.\nNOTE: You will also need a valid java installation. It's required for the ``keytool`` utility, used to\ngenerate ssh keys for some tests.\n\nSetting up tests requirements (assuming you're within virtualenv on ubuntu 14.04+)::\n\n    sudo apt-get install -y libsnappy-dev\n    make setup\n\nRunning tests with coverage::\n\n    make cov\n\nTo run tests with a specific version of Kafka (default one is 1.0.2) use KAFKA_VERSION variable::\n\n    make cov KAFKA_VERSION=0.10.2.1\n\nTest running cheatsheat:\n\n * ``make test FLAGS=\"-l -x --ff\"`` - run until 1 failure, rerun failed tests fitst. Great for cleaning up a lot of errors, say after a big refactor.\n * ``make test FLAGS=\"-k consumer\"`` - run only the consumer tests.\n * ``make test FLAGS=\"-m 'not ssl'\"`` - run tests excluding ssl.\n * ``make test FLAGS=\"--no-pull\"`` - do not try to pull new docker image before test run.\n\n=========\nChangelog\n=========\n\n\n659.bugfix\nAllow group coordinator to close when all brokers are unavailable\n\n681.bugfix\nExclude `.so` from source distribution\n\n689.bugfix\nAdd `dataclasses` backport package to dependencies for Python 3.6\nFix initialization without running loop\n\n693.doc\nUpdate docs and examples to not use deprecated practices like passing loop explicitly\n\n699.removal\nAdd deprecation warning when loop argument to AIOKafkaConsumer and AIOKafkaProducer is passed.\nIt's scheduled for removal in 0.8.0 as a preparation step towards upcoming Python 3.10\n\n\n0.7.0 (2020-10-28)\n==================\n\nNew features:\n\n* Add support for Python 3.8 and 3.9. (issue #569, pr #669 and #676 by @ods)\n* Drop support for Python 3.5. (pr #667 by @ods)\n* Add `OAUTHBEARER` as a new `sasl_mechanism`. (issue #618 and pr #630 by @oulydna)\n\n\nBugfixes:\n\n* Fix memory leak in kafka consumer when consumer is in idle state not consuming any message.\n  (issue #628 and pr #629 by @iamsinghrajat)\n\n\n0.6.0 (2020-05-15)\n==================\n\nNew features:\n\n* Add async context manager support for both Producer and Consumer. (pr #613 and #494 by @nimish)\n* Upgrade to kafka-python version 2.0.0 and set it as non-strict\n  parameter. (issue #590 by @yumendy and #558 by @originalgremlin)\n* Make loop argument optional (issue #544)\n* SCRAM-SHA-256 and SCRAM-SHA-512 support for SASL authentication (issue #571 and pr #588 by @SukiCZ)\n* Added headers param to AIOKafkaProducer.send_and_wait (pr #553 by @megabotan)\n* Add `consumer.last_poll_timestamp(partition)` which gives the ms timestamp of the last\n  update of `highwater` and `lso`. (issue #523 and pr #526 by @aure-olli)\n* Change all code base to async-await (pr #522)\n* Minor: added PR and ISSUE templates to GitHub\n\n\nBugfixes:\n\n* Ignore debug package generation on bdist_rpm command. (issue #599 by @gabriel-tincu)\n* UnknownMemberId was raised to the user instead of retrying on auto commit. (issue #611)\n* Fix issue with messages not being read after subscriptions change with group_id=None. (issue #536)\n* Handle `RequestTimedOutError` in `coordinator._do_commit_offsets()` method to explicitly mark\n  coordinator as dead. (issue #584 and pr #585 by @FedirAlifirenko)\n* Added handling `asyncio.TimeoutError` on metadata request to broker and metadata update.\n  (issue #576 and pr #577 by @MichalMazurek)\n* Too many reqs on kafka not available (issue #496 by @lud4ik)\n* Consumer.seek_to_committed now returns mapping of committed offsets (pr #531 by @ask)\n* Message Accumulator: add_message being recursive eventually overflows (pr #530 by @ask)\n\n\nImproved Documentation:\n\n* Clarify auto_offset_reset usage. (pr 601 by @dargor)\n* Fix spelling errors in comments and documentation using codespell (pr #567 by mauritsvdvijgh)\n* Delete old benchmark file (issue #546 by @jeffwidman)\n* Fix a few typos in docs (pr #573 and pr #563 by @ultrabug)\n* Fix typos, spelling, grammar, etc (pr #545 and pr #547 by @jeffwidman)\n* Fix typo in docs (pr #541 by @pablogamboa)\n* Fix documentation for benchmark (pr #537 by @abhishekray07)\n* Better logging for bad CRC (pr #529 by @ask)\n\n\n0.5.2 (2019-03-10)\n==================\n\nBugfixes:\n\n* Fix ConnectionError breaking metadata sync background task (issue #517 and #512)\n* Fix event_waiter reference before assignment (pr #504 by @romantolkachyov)\n* Bump version of kafka-python\n\n\n0.5.1 (2019-03-10)\n==================\n\nNew features:\n\n* Add SASL support with both SASL plain and SASL GGSAPI. Support also includes\n  Broker v0.9.0, but you will need to explicitly pass ``api_version=\"0.9\"``.\n  (Big thanks to @cyrbil and @jsurloppe for working on this)\n* Added support for max_poll_interval_ms and rebalance_timeout_ms settings (\n  issue #67)\n* Added pause/resume API for AIOKafkaConsumer. (issue #304)\n* Added header support to both AIOKafkaConsumer and AIOKafkaProducer for\n  brokers v0.11 and above. (issue #462)\n\nBugfixes:\n\n* Made sure to not request metadata for all topics if broker version is passed\n  explicitly and is 0.10 and above. (issue #440, thanks to @ulrikjohansson)\n* Make sure heartbeat task will close if group is reset. (issue #372)\n\n\n0.5.0 (2018-12-28)\n==================\n\nNew features:\n\n* Add full support for V2 format messages with a Cython extension. Those are\n  used for Kafka >= 0.11.0.0\n* Added support for transactional producing (issue #182)\n* Added support for idempotent producing with `enable_idempotence` parameter\n* Added support for `fetch_max_bytes` in AIOKafkaConsumer. This can help limit\n  the amount of data transferred in a single roundtrip to broker, which is\n  essential for consumers with large amount of partitions\n\nBugfixes:\n\n* Fix issue with connections not propagating serialization errors\n* Fix issue with `group=None` resetting offsets on every metadata update\n  (issue #441)\n* Fix issue with messages not delivered in order when Leader changes (issue\n  #228)\n* Fixed version parsing of `api_version` parameter. Before it ignored the\n  parameter\n\n\n0.4.3 (2018-11-01)\n==================\n\nBugfix:\n\n* Fixed memory issue introduced as a result of a bug in `asyncio.shield` and\n  not cancelling coroutine after usage. (see issue #444 and #436)\n\n\n0.4.2 (2018-09-12)\n==================\n\nBugfix:\n\n* Added error propagation from coordinator to main consumer. Before consumer\n  just stopped with error logged. (issue #294)\n* Fix manual partition assignment, broken in 0.4.0 (issue #394)\n* Fixed RecursionError in MessageAccumulator.add_message (issue #409)\n* Update kafka-python to latest 1.4.3 and added support for Python3.7\n* Dropped support for Python3.3 and Python3.4\n\nInfrastructure:\n\n* Added Kafka 1.0.2 broker for CI test runner\n* Refactored travis CI build pipeline\n\n0.4.1 (2018-05-13)\n==================\n\n* Fix issue when offset commit error reports wrong partition in log (issue #353)\n* Add ResourceWarning when Producer, Consumer or Connections are not closed\n  properly (issue #295)\n* Fix Subscription None in GroupCoordinator._do_group_rejoin (issue #306)\n\n\n0.4.0 (2018-01-30)\n==================\n\nMajor changes:\n\n* Full refactor of the internals of AIOKafkaConsumer. Needed to avoid several\n  race conditions in code (PR #286, fixes #258, #264 and #261)\n* Rewrote Records parsing protocol to allow implementation of newer protocol\n  versions later\n* Added C extension for Records parsing protocol, boosting the speed of\n  produce/consume routines significantly\n* Added an experimental batch producer API for unique cases, where user wants\n  to control batching himself (by @shargan)\n\n\nMinor changes:\n\n* Add `timestamp` field to produced message's metadata. This is needed to find\n  LOG_APPEND_TIME configured timestamps.\n* `Consumer.seek()` and similar API's now raise proper ``ValueError``'s on\n  validation failure instead of ``AssertionError``.\n\n\nBug fixes:\n\n* Fix ``connections_max_idle_ms`` option, as earlier it was only applied to\n  bootstrap socket. (PR #299)\n* Fix ``consumer.stop()`` side effect of logging an exception\n  ConsumerStoppedError (issue #263)\n* Problem with Producer not able to recover from broker failure (issue #267)\n* Traceback containing duplicate entries due to exception sharing (PR #247\n  by @Artimi)\n* Concurrent record consumption rasing `InvalidStateError('Exception is not\n  set.')` (PR #249 by @aerkert)\n* Don't fail ``GroupCoordinator._on_join_prepare()`` if ``commit_offset()``\n  throws exception (PR #230 by @shargan)\n* Send session_timeout_ms to GroupCoordinator constructor (PR #229 by @shargan)\n\nBig thanks to:\n\n* @shargan for Producer speed enhancements and the batch produce API\n  proposal/implementation.\n* @vineet-rh and other contributors for constant feedback on Consumer\n  problems, leading to the refactor mentioned above.\n\n\n0.3.1 (2017-09-19)\n==================\n\n* Added `AIOKafkaProducer.flush()` method. (PR #209 by @vineet-rh)\n* Fixed a bug with uvloop involving `float(\"inf\")` for timeout. (PR #210 by\n   dmitry-moroz)\n* Changed test runner to allow running tests on OSX. (PR #213 by @shargan)\n\n\n0.3.0 (2017-08-17)\n==================\n\n* Moved all public structures and errors to `aiokafka` namespace. You will no\n  longer need to import from `kafka` namespace.\n* Changed ConsumerRebalanceListener to support either function or coroutine\n  for `on_partitions_assigned` and `on_partitions_revoked` callbacks. (PR #190\n  by @ask)\n* Added support for `offsets_for_times`, `beginning_offsets`, `end_offsets`\n  API's. (issue #164)\n* Coordinator requests are now sent using a separate socket. Fixes slow commit\n  issue. (issuer #137, issue #128)\n* Added `seek_to_end`, `seek_to_beginning` API's. (issue #154)\n* Updated documentation to provide more useful usage guide on both Consumer and\n  Producer interface.\n\n0.2.3 (2017-07-23)\n==================\n\n* Fixed retry problem in Producer, when buffer is not reset to 0 offset.\n  Thanks to @ngavrysh for the fix in Tubular/aiokafka fork. (issue #184)\n* Fixed how Producer handles retries on Leader node failure. It just did not\n  work before... Thanks to @blugowski for the help in locating the problem.\n  (issue #176, issue #173)\n* Fixed degrade in v0.2.2 on Consumer with no group_id. (issue #166)\n\n\n0.2.2 (2017-04-17)\n==================\n\n* Reconnect after KafkaTimeoutException. (PR #149 by @Artimi)\n* Fixed compacted topic handling. It could skip messages if those were\n  compacted (issue #71)\n* Fixed old issue with new topics not adding to subscription on pattern\n  (issue #46)\n* Another fix for Consumer race condition on JoinGroup. This forces Leader to\n  wait for new metadata before assigning partitions. (issue #118)\n* Changed metadata listener in Coordinator to avoid 2 rejoins in a rare\n  condition (issue #108)\n* `getmany` will not return 0 results until we hit timeout. (issue #117)\n\nBig thanks to @Artimi for pointing out several of those issues.\n\n\n0.2.1 (2017-02-19)\n==================\n\n* Add a check to wait topic autocreation in Consumer, instead of raising\n  UnknownTopicOrPartitionError (PR #92 by fabregas)\n* Consumer now stops consumption after `consumer.stop()` call. Any new `get*` calls\n  will result in ConsumerStoppedError (PR #81)\n* Added `exclude_internal_topics` option for Consumer (PR #111)\n* Better support for pattern subscription when used with `group_id` (part of PR #111)\n* Fix for Consumer `subscribe` and JoinGroup race condition (issue #88). Coordinator will now notice subscription changes during rebalance and will join group again. (PR #106)\n* Changed logging messages according to KAFKA-3318. Now INFO level should be less messy and more informative. (PR #110)\n* Add support for connections_max_idle_ms config (PR #113)\n\n\n0.2.0 (2016-12-18)\n==================\n\n* Added SSL support. (PR #81 by Drizzt1991)\n* Fixed UnknownTopicOrPartitionError error on first message for autocreated topic (PR #96 by fabregas)\n* Fixed `next_record` recursion (PR #94 by fabregas)\n* Fixed Heartbeat fail if no consumers (PR #92 by fabregas)\n* Added docs addressing kafka-python and aiokafka differences (PR #70 by Drizzt1991)\n* Added `max_poll_records` option for Consumer (PR #72 by Drizzt1991)\n* Fix kafka-python typos in docs (PR #69 by jeffwidman)\n* Topics and partitions are now randomized on each Fetch request (PR #66 by Drizzt1991)\n\n\n0.1.4 (2016-11-07)\n==================\n\n* Bumped kafka-python version to 1.3.1 and Kafka to 0.10.1.0.\n* Fixed auto version detection, to correctly handle 0.10.0.0 version\n* Updated Fetch and Produce requests to use v2 with v0.10.0 message format on brokers.\n  This allows a ``timestamp`` to be associated with messages.\n* Changed lz4 compression framing, as it was changed due to KIP-57 in new message format.\n* Minor refactorings\n\nBig thanks to @fabregas for the hard work on this release (PR #60)\n\n\n0.1.3 (2016-10-18)\n==================\n\n* Fixed bug with infinite loop on heartbeats with autocommit=True. #44\n* Bumped kafka-python to version 1.1.1\n* Fixed docker test runner with multiple interfaces\n* Minor documentation fixes\n\n\n0.1.2 (2016-04-30)\n==================\n\n* Added Python3.5 usage example to docs\n* Don't raise retriable exceptions in 3.5's async for iterator\n* Fix Cancellation issue with producer's `send_and_wait` method\n\n\n0.1.1 (2016-04-15)\n==================\n\n* Fix packaging issues. Removed unneeded files from package.\n\n0.1.0 (2016-04-15)\n==================\n\nInitial release\n\nAdded full support for Kafka 9.0. Older Kafka versions are not tested.",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "https://pypi.python.org/pypi/aiokafka",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://aiokafka.readthedocs.org",
    "keywords": "",
    "license": "Apache 2",
    "maintainer": "",
    "maintainer_email": "",
    "name": "beamm-aiokafka",
    "package_url": "https://pypi.org/project/beamm-aiokafka/",
    "platform": "POSIX",
    "project_url": "https://pypi.org/project/beamm-aiokafka/",
    "project_urls": {
      "Download": "https://pypi.python.org/pypi/aiokafka",
      "Homepage": "http://aiokafka.readthedocs.org"
    },
    "release_url": "https://pypi.org/project/beamm-aiokafka/0.7.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Kafka integration with asyncio.",
    "version": "0.7.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10238288,
  "releases": {
    "0.7.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ca6c2cdcb1bfc0cb78aa528830c3e842aafe13ef1a3a7a9b329f245d4869d75f",
          "md5": "70fa28bceff72b1c6df9ed1fd3a2d196",
          "sha256": "53b26dc6b1967b215d34dd5f37b057ded7a1e7e735783a4871a1464ee2ecdc17"
        },
        "downloads": -1,
        "filename": "beamm-aiokafka-0.7.0.tar.gz",
        "has_sig": false,
        "md5_digest": "70fa28bceff72b1c6df9ed1fd3a2d196",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 373605,
        "upload_time": "2021-05-03T08:40:26",
        "upload_time_iso_8601": "2021-05-03T08:40:26.896961Z",
        "url": "https://files.pythonhosted.org/packages/ca/6c/2cdcb1bfc0cb78aa528830c3e842aafe13ef1a3a7a9b329f245d4869d75f/beamm-aiokafka-0.7.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ca6c2cdcb1bfc0cb78aa528830c3e842aafe13ef1a3a7a9b329f245d4869d75f",
        "md5": "70fa28bceff72b1c6df9ed1fd3a2d196",
        "sha256": "53b26dc6b1967b215d34dd5f37b057ded7a1e7e735783a4871a1464ee2ecdc17"
      },
      "downloads": -1,
      "filename": "beamm-aiokafka-0.7.0.tar.gz",
      "has_sig": false,
      "md5_digest": "70fa28bceff72b1c6df9ed1fd3a2d196",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 373605,
      "upload_time": "2021-05-03T08:40:26",
      "upload_time_iso_8601": "2021-05-03T08:40:26.896961Z",
      "url": "https://files.pythonhosted.org/packages/ca/6c/2cdcb1bfc0cb78aa528830c3e842aafe13ef1a3a7a9b329f245d4869d75f/beamm-aiokafka-0.7.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}