{
  "info": {
    "author": "Luigi Di Sotto, Diego Giorgini, Saeed Choobani",
    "author_email": "luigi.disotto@aitechnologies.it, diego.giorgini@aitechnologies.it, saeed.choobani@aitechnologies.it",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# üçª datafun [![Downloads](https://pepy.tech/badge/datafun)](https://pepy.tech/project/datafun)\n\nThe datafun allows for loading existing datasets or files with different formats and treat them as streaming pipelines. It also allows one to add operations like filter and map in a functional way executed lazily while loading data.\n\n<!-- toc -->\n\n- [Overview](#overview)\n- [Architecture](#architecture)\n- [Install](#install)\n- [Usage](#usage)\n- [Available datasets](#available-datasets)\n- [Custom Datasets](#custom-datasets)\n\n<!-- tocstop -->\n\n# Overview\n\nWith datafun you can:\n\n- Load files locally from specific file formats, eg. CSV, JSON etc.\n- Load files from remote sources, eg. a CSV or a JSON from a Google Cloud Storage bucket.\n- Apply streaming transformations on the fly by applying functional operations (filter, map...) to it.\n- Define data transformations to be stored and later used and extended by anyone. This will contribute to an internal hub of common datasets and pipelines used many times.\n\n# Architecture\n\nA datafun Dataset is both a lazy stream of data and lazy sequential pipeline of operations.\n\n- It is a stream of data: when you iterate over the dataset, one element at a time is generated and returned to the caller.\n- It is a lazy sequential pipeline: every time an element needs to be returned, the pipeline of operations is executed on the current element.\n\n**Node types.** The DatasetSource class is the first node of the pipeline and is responsible for loading the data from a local or remote storage, then passing it to the next node in the pipeline. DatasetNode objects representing filter or map operations can be added to the pipeline.\n\n**Communication.** The pipeline communicates in a message-passing fashion. Stream objects are passed between nodes, representing the state of the stream and containing the current element. A stream may be a StartOfStream, EndOfStream, PullStream, or just Stream (when containing data). To give an example of the communication, consider the following pipeline:\n\n```python\nTextDataSource -> Map (lambda x: x.lower()) -> Filter (lambda x: \"messiah\" in x))\n```\n\nand the following stream of data:\n\n```\n\"Dune\" -> \"Dune Messiah\" -> \"Children of Dune\" -> ...\n```\n\nThis is what happens when you begin iterating over the stream:\n\n- StartOfStream triggers TextDataSource, which reads the first element `Dune`\n- Stream is forwarded to Map, which transforms `Dune` into `dune`\n- Stream is forwarded to Filter, the lambda inside fails the check, so a PullStream is sent backward\n- PullStream arrives to Map, which sends it back to TextDataSource\n- TextDataSource reads the PullStream, generates another element, `Dune Messiah`, then issues a Stream forward.\n- ...\n- This Stream arrives at the end, so `messiah` it is returned to the caller.\n- ...\n\n**Available transformation operations.**\n\n| name      | Arguments                                                                         | Description                                                                                                                                                                                                                                                                                                                                              |\n|-----------|-----------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| filter    | f: Callable[T, bool]                                                              | Filters elements according to **f**.                                                                                                                                                                                                                                                                                                                     |\n| map       | f: Callable[T, S]                                                                 | Maps every element to another with function **f**.                                                                                                                                                                                                                                                                                                       |\n| flat_map  | f: Callable[T, Iterable[S]] = lambda x: x                                         | A Flat Map takes a list and returns every element, one at a time. The given function **f**, given element of time **T**, must return an Iterable of type **S**. If not provided, **f** is the identity.                                                                                                                                                  |\n| sampling  | p: float, seed: int                                                               | Samples elements according to the sampling ratio **p**: `0. < p <= 1.`                                                                                                                                                                                                                                                                                   |\n| unique    | by: Callable[T, U]                                                                | Removes duplicates. **by** argument specifies a function that extracts the desired information                                                                                                                                                                                                                                                           |\n| aggregate | init: Callable[[], S] <br> agg: Callable[[T, S], S] <br> reduce: Callable[[S], R] | Aggregates stream using function **f**. Hint: given initial value **init** of type **S** for <br> aggregated stream, maps **f** to stream (**value**: **T**, **aggregated**: **S**) where **value** is an <br> element of the stream, and **aggregated** is the current aggregate value. <br> **reduce** is applied to the aggregated value, if provided |\n| zip       | *iterables                                                                        | Zips elements from multiple dataset, like python zip()                                                                                                                                                                                                                                                                                                   |\n| join      | other: Dataset, key: Callable, key_left: Callable, key_right: Callable            | Joins two datasets based on provided key functions that specify the path in the dictionary. Either specify **key** for both, or **key_left** and **key_right**.                                                                                                                                                                                          |\n| limit     | n: int                                                                            | Limits the number of elements in the streams to the first **n**                                                                                                                                                                                                                                                                                          |\n| cache     |                                                                                   | Caches result of previous nodes into memory. Useful when the pipeline is executed multiple times. Be careful with memory intensive operations.                                                                                                                                                                                                           |\n\nDatasets overloads basic python operations: ```+```, ```-```, ```*```, ```/```.\n\nYou can see examples for every operation in the [dedicated notebook](./examples/operations.ipynb).\n\n**Available info operations.**\n\n| name    | Arguments | Description                                           |\n|---------|-----------|-------------------------------------------------------|\n| info    | /         | Prints dataset description                            |\n| summary | /         | Prints list of transformations in the pipeline        |\n| schema  | /         | Prints schema of dataset. Useful for csv, json, jsonl |\n\n**Available output operations.**\n| name       | Arguments              | Return type | Description                                                                    |\n|------------|------------------------|-------------|--------------------------------------------------------------------------------|\n| show       | n: int                 | str         | Print first n elements as string                                               |\n| take       | n: int                 | List[T]     | Returns the first n elements of the dataset                                    |\n| take_while | f: Callable[..., bool] | List[T]     | Returns all elements satisfying f                                              |\n| collect    | /                      | List[T]     | Returns a list containing all elements. Also prints progress while collecting. |\n\n# Install\n\n```bash\npip install datafun\n```\n\n# Usage\n\n```python\nimport datafun as dfn\n```\n\n## Load a raw dataset (local|remote)\n\n```python\nds: Dataset = dfn.load('text', path=\"path/to/dune_frank_herbert.txt\")\n\nfor el in ds:\n    # do something with el\n```\n\n## Load a dataset from iterable data structure\n\n```python\nds = dfn.load(range(N))\n\nds.take(5) # take first 5 integers\n>> [0, 1, 2, 3, 4]\n```\n\n## Adding ops to the pipeline, evaluated lazily when generating data\n```python\nds = dfn.load('json', path=\"path/to/file.json\")\n\nds2 = ds.filter(lambda x: x['KEY'] > 10)\nds2 = ds2.map(lambda x: x**2)\n\nprint(ds2.summary()) # Shows pipeline ops\n\nfor el in ds2:\n    # do something with el.\n    # This will execute the defined ops one element at a time\n```\n\n## Easy streaming transformation of remote dataset!\n```python\n# Streaming normalize on the fly\nds = (\n    dfn\n    .load('gcs-jsonl', path='gs://my_bucket/timeseries.jsonl')\n    .map(lambda x: x['value'])\n)\nmean = 3.14 # let's assume to have a mean\nnorm_ds = ds - mean\nfor value in norm_ds:\n    print(value)\n```\n\nYou can see examples for every operation in the [dedicated notebook](./examples/operations.ipynb).\n\n# Available datasets\n\n| ID            | Class               | Elem. type | Description                                                                                                  |\n|---------------|---------------------|------------|--------------------------------------------------------------------------------------------------------------|\n| **text**      | TextDataset         | str        | Streams the local text file(s) one line at a time. Glob supported                                            |\n| **csv**       | CSVDataset          | dict       | Streams the local CSV file(s) one row at a time. Glob supported                                              |\n| **json**      | JSONDataset         | dict       | Streams the local JSON file(s) one at a time. Glob supported                                                 |\n| **jsonl**     | JSONLinesDataset    | dict       | Streams the local JSONLines file(s) one line at a time. Glob supported                                       |\n| **gcs**       | GCSDataset          | str        | Download files from Google Cloud Storage, then streams paths to be loaded locally. Glob supported            |\n| **gcs-text**  | GCSTextDataset      | str        | As gcs, but also opens the text file(s) and streams one line a time. Glob supported                          |\n| **gcs-csv**   | GCSCSVDataset       | dict       | As gcs, but also opens the CSV file(s) and streams one row a time. Glob supported                            |\n| **gcs-json**  | GCSJSONDataset      | dict       | As gcs, but also opens the JSON file(s) and streams it one at a time. Glob supported                         |\n| **gcs-jsonl** | GCSJSONLinesDataset | dict       | As gcs, but also opens the JSONLines file(s) and streams one line at a time. Glob supported                  |\n| **elk**       | ELKDataset          | dict       | Connects to ELK instance, runs json query specified in `path`, then returns the resulting docs one at a time |\n| **rest**      | RESTDataset         | dict       | Streams the response from a REST API.                                                                        |\n\n## TextDataset\n\n| Name                   | Type                   | Required | Default | Description                                                                    |\n|------------------------|------------------------|----------|---------|--------------------------------------------------------------------------------|\n| **path**               | Union[List[str]], str] | Yes      |         | The local path to be loaded                                                    |\n| **encoding**           | str                    | No       | 'utf-8' | The encoding to be used when opening the file                                  |\n| **allowed_extensions** | Sequence[str]          | No       | None    | The supported extensions (others will be discarded). None means all extensions |\n\n**Returned element type**: ```str```\n\n## CSVDataset\n\n| Name                   | Type                  | Required | Default | Description                                                                    |\n|------------------------|-----------------------|----------|---------|--------------------------------------------------------------------------------|\n| **path**               | Union[List[str], str] | Yes      |         | The local path to be loaded                                                    |\n| **encoding**           | str                   | No       | 'utf-8' | The encoding to be used when opening the file                                  |\n| **allowed_extensions** | Sequence[str]         | No       | ('csv') | The supported extensions (others will be discarded). None means all extensions |\n| **delimiter**          | str                   | No       | ','     | The column delimiter                                                           |\n\n**Returned element type**: ```dict```. Each element represent a row from the CSV(s). The keys are the CSV column names.\n\n## JSONDataset\n\n| Name                   | Type                  | Required | Default  | Description                                                                    |\n|------------------------|-----------------------|----------|----------|--------------------------------------------------------------------------------|\n| **path**               | Union[List[str], str] | Yes      |          | The local path to be loaded                                                    |\n| **encoding**           | str                   | No       | 'utf-8'  | The encoding to be used when opening the file                                  |\n| **allowed_extensions** | Sequence[str]         | No       | ('json') | The supported extensions (others will be discarded). None means all extensions |\n\n**Returned element type**: ```dict```. This dictionary directly maps the JSON file.\n\n## JSONLinesDataset\n\n| Name                   | Type                  | Required | Default           | Description                                                                    |\n|------------------------|-----------------------|----------|-------------------|--------------------------------------------------------------------------------|\n| **path**               | Union[List[str], str] | Yes      |                   | The local path to be loaded                                                    |\n| **encoding**           | str                   | No       | 'utf-8'           | The encoding to be used when opening the file                                  |\n| **allowed_extensions** | Sequence[str]         | No       | ('json', 'jsonl') | The supported extensions (others will be discarded). None means all extensions |\n\n**Returned element type**: ```dict```. This dictionary maps a single JSON line from the file.\n\n## GCSDataset\n\n| Name                | Type                  | Required | Default                         | Description                                                                   |\n|---------------------|-----------------------|----------|---------------------------------|-------------------------------------------------------------------------------|\n| **path**            | Union[List[str], str] | Yes      |                                 | The remote path to be loaded                                                  |\n| **download_path**   | str                   | No       | '$HOME/.cache/data-engineering' | The path where the file(s) will be downloaded                                 |\n| **service_account** | str                   | No       | None                            | The service account path to use. If missing will use the default user account |\n| **project**         | str                   | No       | None                            | The project id to use. If missing will use the user configured project it     |\n\n**Returned element type**: ```str```. Each element represent a local path where to find the file downloaded from GCS.\n\n## GCSTextDataset\n\n| Name                | Type                  | Required | Default                         | Description                                                                   |\n|---------------------|-----------------------|----------|---------------------------------|-------------------------------------------------------------------------------|\n| **path**            | Union[List[str], str] | Yes      |                                 | The remote path to be loaded                                                  |\n| **download_path**   | str                   | No       | '$HOME/.cache/data-engineering' | The path where the file(s) will be downloaded                                 |\n| **service_account** | str                   | No       | None                            | The service account path to use. If missing will use the default user account |\n| **project**         | str                   | No       | None                            | The project id to use. If missing will use the user configured project it     |\n| **encoding**        | str                   | No       | 'utf-8'                         | The encoding to be used when opening the file                                 |\n\n**Returned element type**: ```str```. Each element represent a line from the remote file.\n\n## GCSCSVDataset\n\n| Name                   | Type                  | Required | Default                         | Description                                                                    |\n|------------------------|-----------------------|----------|---------------------------------|--------------------------------------------------------------------------------|\n| **path**               | Union[List[str], str] | Yes      |                                 | The remote path to be loaded                                                   |\n| **download_path**      | str                   | No       | '$HOME/.cache/data-engineering' | The path where the file(s) will be downloaded                                  |\n| **service_account**    | str                   | No       | None                            | The service account path to use. If missing will use the default user account  |\n| **project**            | str                   | No       | None                            | The project id to use. If missing will use the user configured project it      |\n| **encoding**           | str                   | No       | 'utf-8'                         | The encoding to be used when opening the file                                  |\n| **allowed_extensions** | Sequence[str]         | No       | ('csv')                         | The supported extensions (others will be discarded). None means all extensions |\n| **delimiter**          | str                   | No       | ','                             | The column delimiter                                                           |\n\n**Returned element type**: ```dict```. Each element represent a row from the CSV(s). The keys are the CSV column names.\n\n## GCSJSONDataset\n\n| Name                   | Type                  | Required | Default                         | Description                                                                    |\n|------------------------|-----------------------|----------|---------------------------------|--------------------------------------------------------------------------------|\n| **path**               | Union[List[str], str] | Yes      |                                 | The remote path to be loaded                                                   |\n| **download_path**      | str                   | No       | '$HOME/.cache/data-engineering' | The path where the file(s) will be downloaded                                  |\n| **service_account**    | str                   | No       | None                            | The service account path to use. If missing will use the default user account  |\n| **project**            | str                   | No       | None                            | The project id to use. If missing will use the user configured project it      |\n| **encoding**           | str                   | No       | 'utf-8'                         | The encoding to be used when opening the file                                  |\n| **allowed_extensions** | Sequence[str]         | No       | ('json')                        | The supported extensions (others will be discarded). None means all extensions |\n\n**Returned element type**: ```dict```. Each element is a dict directly mapping a JSON file.\n\n## GCSJSONLinesDataset\n\n| Name                   | Type                  | Required | Default                         | Description                                                                    |\n|------------------------|-----------------------|----------|---------------------------------|--------------------------------------------------------------------------------|\n| **path**               | Union[List[str], str] | Yes      |                                 | The remote path to be loaded                                                   |\n| **download_path**      | str                   | No       | '$HOME/.cache/data-engineering' | The path where the file(s) will be downloaded                                  |\n| **service_account**    | str                   | No       | None                            | The service account path to use. If missing will use the default user account  |\n| **project**            | str                   | No       | None                            | The project id to use. If missing will use the user configured project it      |\n| **encoding**           | str                   | No       | 'utf-8'                         | The encoding to be used when opening the file                                  |\n| **allowed_extensions** | Sequence[str]         | No       | ('json', 'jsonl')               | The supported extensions (others will be discarded). None means all extensions |\n\n**Returned element type**: ```dict```. Each element is a dict mapping a JSON line from the file(s).\n\n## ELKDataset\n\n| Name              | Type                  | Required | Default    | Description                                                      |\n|-------------------|-----------------------|----------|------------|------------------------------------------------------------------|\n| **path**          | Union[List[str], str] | Yes      |            | The local path to a query written in JSON                        |\n| **host**          | str                   | Yes      |            | Elastic host URL                                                 |\n| **port**          | str                   | Yes      |            | Elastic host port                                                |\n| **username**      | str                   | Yes      |            | Elastic authentication username                                  |\n| **password**      | str                   | Yes      |            | Elastic authentication password                                  |\n| **index**         | str                   | Yes      |            | Elastic index to query                                           |\n| **start_isodate** | str (ISO datetime)    | Yes      |            | Elastic start date range with format: \"2021-09-15T10:00:00.000Z\" |\n| **end_isodate**   | str (ISO datetime)    | Yes      |            | Elastic end date range with format: \"2021-09-15T10:00:00.000Z\"   |\n| **date_field**    | str                   | No       | @timestamp | Elastic date field. Can be nested into list, eg. \"messages.date\" |\n\n**Returned element type**: ```dict```. Each element is a document matching the given query.\n\n## RESTDataset\n\n| Name              | Type                            | Required | Default                         | Description                                                                                                                              |\n|-------------------|---------------------------------|----------|---------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| **download_path** | str                             | No       | '$HOME/.cache/data-engineering' | Where to download data                                                                                                                   |\n| **headers**       | dict                            | No       | {}                              | Call Headers                                                                                                                             |\n| **auth_headers**  | dict                            | No       | {}                              | Authentication headers                                                                                                                   |\n| **params**        | Optional[dict]                  | No       | None                            | URL key value params                                                                                                                     |\n| **method**        | str                             | No       | 'GET'                           | Call method, 'GET', 'POST'...                                                                                                            |\n| **next_url_f**    | Callable[[dict], Optional[str]] | No       | lambda x: None                  | Function to extract the next url to call from the json response for paginated APIs. Should return None to mark the end or the URL string |\n\n**Returned element type**: ```dict``` representing the json response.\n\n# Custom datasets\n\n## Method 1. Functional style\n\n```python\ndef my_load(path, **kwargs):\n    ds = (dfn\n        .load('json', path=path, **kwargs)\n        .filter(lambda x: x['KEY'] > 10)\n        .map(lambda x: x**2)\n    )\n    return ds\n```\n\n## Method 2. Extend Dataset\n\n**Write classes**\n\n```python\n@dataclass\nclass ELKDatasetConfig(dfn.Config):\n    # Additional arguments to base class\n    host: str\n    port: int\n\nclass ELKDataset(dfn.DatasetSource):\n    def __init__(self, config, **kwargs):\n        super().__init__(config=config, **kwargs)\n        self.elk_client = elklib(self.config.host, self.config.port)\n\n    def dataset_name() -> str:\n        return 'elk-dataset'\n\n    def info(self) -> dict:\n        return {\n            'description': 'My ELK dataset class.',\n            'author': 'foo@company.org',\n            'date': '2021-09-01',\n        }\n\n    def schema(self) -> dict:\n        return {\n            'column_1': 'int',\n            ...\n        }\n        # this method may also load a row and infer it, like CSVDataset does\n\n    def _generate_examples(self) -> Generator:\n        for doc in self.elk_client.scroll_function():\n            # Apply some prep to the document\n            yield doc\n```\n\n**Add new dataset to list of supported datasets**\n\n- Add a line to ```DATASETS``` in ```datafun/__init__.py```:\n\n```\nDATASETS = {\n    ...\n    \"elk-dataset\": ELKDataset,\n}\n```\n\n**Use it!**\n\n```\nds = dfn.load('elk-dataset', config={\n    'host': 'localhost',\n    'port': '8888',\n})\n\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/aitechnologies-it/datafun",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "datafun",
    "package_url": "https://pypi.org/project/datafun/",
    "platform": null,
    "project_url": "https://pypi.org/project/datafun/",
    "project_urls": {
      "Homepage": "https://github.com/aitechnologies-it/datafun"
    },
    "release_url": "https://pypi.org/project/datafun/0.4.3/",
    "requires_dist": [
      "backoff",
      "pydlib",
      "tqdm",
      "google-cloud-storage",
      "elasticsearch (<8)",
      "requests"
    ],
    "requires_python": ">=3.8",
    "summary": "datafun brings the fun back to data pipelines",
    "version": "0.4.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15229355,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0895e96467a2fa21f9c2e89a7dfc3190630e0bfb281e60a6e00d370e64f64574",
          "md5": "aee8d6d7c51f57044d86705ab166f3d1",
          "sha256": "a0abdee7c860cf5c834e7f6b85e71ba671789224afef97aa6ae796ff27148cd2"
        },
        "downloads": -1,
        "filename": "datafun-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "aee8d6d7c51f57044d86705ab166f3d1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 17378,
        "upload_time": "2022-07-25T13:58:30",
        "upload_time_iso_8601": "2022-07-25T13:58:30.185675Z",
        "url": "https://files.pythonhosted.org/packages/08/95/e96467a2fa21f9c2e89a7dfc3190630e0bfb281e60a6e00d370e64f64574/datafun-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fc0871c458c3403f43ecab2be1767b09d7358cefb19048f4b70b1502a1787fd2",
          "md5": "d2043aa365defbdb2c5b120222983f19",
          "sha256": "f2936c729815b8b568ed0d4cad92039a4f8bbb9991822c2481944aec3b1454d8"
        },
        "downloads": -1,
        "filename": "datafun-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "d2043aa365defbdb2c5b120222983f19",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 21296,
        "upload_time": "2022-07-25T13:58:32",
        "upload_time_iso_8601": "2022-07-25T13:58:32.182629Z",
        "url": "https://files.pythonhosted.org/packages/fc/08/71c458c3403f43ecab2be1767b09d7358cefb19048f4b70b1502a1787fd2/datafun-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d621b4fd2f45c7b47360736f75a5e57eb4c79bb16b32135bec1928bc591d89ee",
          "md5": "8bbaddc3e38cc3030dbed2d25b5d22c5",
          "sha256": "d55ff06a33dc360f76def4c1c70e7b6f1d65c68b277d879b96739e232cad4620"
        },
        "downloads": -1,
        "filename": "datafun-0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8bbaddc3e38cc3030dbed2d25b5d22c5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 18546,
        "upload_time": "2022-07-26T15:17:54",
        "upload_time_iso_8601": "2022-07-26T15:17:54.192555Z",
        "url": "https://files.pythonhosted.org/packages/d6/21/b4fd2f45c7b47360736f75a5e57eb4c79bb16b32135bec1928bc591d89ee/datafun-0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b7c56245784bfd45d23ea619498b3b3774209afee8cdd8afc3eda65080f0bba0",
          "md5": "98fda7ee415d37ee494cc3c2fe8b76ca",
          "sha256": "07bd8f7d75410540a795e3d1a84c38234b8a8571dcf6ae110519086d46619645"
        },
        "downloads": -1,
        "filename": "datafun-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "98fda7ee415d37ee494cc3c2fe8b76ca",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 22762,
        "upload_time": "2022-07-26T15:17:56",
        "upload_time_iso_8601": "2022-07-26T15:17:56.532529Z",
        "url": "https://files.pythonhosted.org/packages/b7/c5/6245784bfd45d23ea619498b3b3774209afee8cdd8afc3eda65080f0bba0/datafun-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b7a6ffc2357e631c8bd6ca94754e2b6ccac97b2225bb7b7998721ab0bc073c28",
          "md5": "29ad50947ec465c337b8e1d47d318cd5",
          "sha256": "1c18f275059c1fd7db67de088d36257bcb2c2a5ca8fdf414888e1b548f94963f"
        },
        "downloads": -1,
        "filename": "datafun-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "29ad50947ec465c337b8e1d47d318cd5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 18629,
        "upload_time": "2022-07-28T13:21:08",
        "upload_time_iso_8601": "2022-07-28T13:21:08.659632Z",
        "url": "https://files.pythonhosted.org/packages/b7/a6/ffc2357e631c8bd6ca94754e2b6ccac97b2225bb7b7998721ab0bc073c28/datafun-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2bc50a435f79b6e873d0ed987319b62222c2354b07a6fa25b6ee7ba8fc8fabe4",
          "md5": "3dd40c4157863b4ae963609069f66b83",
          "sha256": "da527833ac2a755f259d5e193dd3f08907a3ecd47631d989ba1a98a784bb9acd"
        },
        "downloads": -1,
        "filename": "datafun-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3dd40c4157863b4ae963609069f66b83",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 22812,
        "upload_time": "2022-07-28T13:21:10",
        "upload_time_iso_8601": "2022-07-28T13:21:10.126947Z",
        "url": "https://files.pythonhosted.org/packages/2b/c5/0a435f79b6e873d0ed987319b62222c2354b07a6fa25b6ee7ba8fc8fabe4/datafun-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d47a5f0ea9fb11de6aefcf129af445ca4c1cd3a24a5680b29ba30f6732ee110e",
          "md5": "2d37d50063a192a9f92d773deb2deec7",
          "sha256": "ea3086dc4e2e57c9a7867a5ca94cda51cc6aed7c5b9fcffe7d9c114b6a5cdcf1"
        },
        "downloads": -1,
        "filename": "datafun-0.4.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2d37d50063a192a9f92d773deb2deec7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 22093,
        "upload_time": "2022-08-04T10:52:10",
        "upload_time_iso_8601": "2022-08-04T10:52:10.018043Z",
        "url": "https://files.pythonhosted.org/packages/d4/7a/5f0ea9fb11de6aefcf129af445ca4c1cd3a24a5680b29ba30f6732ee110e/datafun-0.4.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9d4dad2906eb82b5d5047ddc1b410372b78ace516d71bae987cae881f8f07037",
          "md5": "12ff7d41e34c9c520ffc150b17b497f2",
          "sha256": "ce1b1b7b9dc27a426fce1a8a64679267919378bd8f0c5304e9b19bc4ee75c185"
        },
        "downloads": -1,
        "filename": "datafun-0.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "12ff7d41e34c9c520ffc150b17b497f2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 24232,
        "upload_time": "2022-08-04T10:52:11",
        "upload_time_iso_8601": "2022-08-04T10:52:11.868390Z",
        "url": "https://files.pythonhosted.org/packages/9d/4d/ad2906eb82b5d5047ddc1b410372b78ace516d71bae987cae881f8f07037/datafun-0.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9c2e5221b5c1fc59299432c8b98258d9343b7422f668666674f1e4ffb47910ef",
          "md5": "ff08574e997644f1c07ddfafff2f6ed1",
          "sha256": "c7ebb41332002e655c7b5cee918259d040e43e13c34694c38e56bd1960bd4db1"
        },
        "downloads": -1,
        "filename": "datafun-0.4.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ff08574e997644f1c07ddfafff2f6ed1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 22089,
        "upload_time": "2022-08-04T11:08:02",
        "upload_time_iso_8601": "2022-08-04T11:08:02.798626Z",
        "url": "https://files.pythonhosted.org/packages/9c/2e/5221b5c1fc59299432c8b98258d9343b7422f668666674f1e4ffb47910ef/datafun-0.4.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "52d54ef7a6a9166b50f9955b87602a328e8f81e17e6b15daf0c32c642a7a5e24",
          "md5": "99533c0a42ca51ed39eb914a1a30e292",
          "sha256": "1d04f7869c2ff8a66c9620fd8ba135e77d159820f9491b98eec45bad09762ac4"
        },
        "downloads": -1,
        "filename": "datafun-0.4.1.tar.gz",
        "has_sig": false,
        "md5_digest": "99533c0a42ca51ed39eb914a1a30e292",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 24228,
        "upload_time": "2022-08-04T11:08:07",
        "upload_time_iso_8601": "2022-08-04T11:08:07.724922Z",
        "url": "https://files.pythonhosted.org/packages/52/d5/4ef7a6a9166b50f9955b87602a328e8f81e17e6b15daf0c32c642a7a5e24/datafun-0.4.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b97a4fbc114e4929ca8fea933893f3dc41400e438c3e7a80f10ffbf07defac70",
          "md5": "eca534c6ed3df01b068258356493bbef",
          "sha256": "ed9f781e749d06e1168cce5e917b112d5415602f427244428a76aaa7b3338752"
        },
        "downloads": -1,
        "filename": "datafun-0.4.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eca534c6ed3df01b068258356493bbef",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 22508,
        "upload_time": "2022-09-21T12:09:26",
        "upload_time_iso_8601": "2022-09-21T12:09:26.802527Z",
        "url": "https://files.pythonhosted.org/packages/b9/7a/4fbc114e4929ca8fea933893f3dc41400e438c3e7a80f10ffbf07defac70/datafun-0.4.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3707de7f83af683b7b5fc3f360f2cefbca6220180c62e60717754f328459e17e",
          "md5": "66c47d5392e12d5655a095903cc3e035",
          "sha256": "f5f8495172d4298e637333a320e8b4638959569048c4d4e3cf80fcc408936a3d"
        },
        "downloads": -1,
        "filename": "datafun-0.4.2.tar.gz",
        "has_sig": false,
        "md5_digest": "66c47d5392e12d5655a095903cc3e035",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 24779,
        "upload_time": "2022-09-21T12:09:28",
        "upload_time_iso_8601": "2022-09-21T12:09:28.862459Z",
        "url": "https://files.pythonhosted.org/packages/37/07/de7f83af683b7b5fc3f360f2cefbca6220180c62e60717754f328459e17e/datafun-0.4.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ab06e63d6773e4e5ab56bc8f9a2cf22e262d37ea2202aa865e857e81fc8f4b91",
          "md5": "8e8ce612eb4eba996887e8f1146c31f5",
          "sha256": "6019efcf50836a303ceba7dc5144ce3580f9a29ab8cc62fed30bddabcb08c158"
        },
        "downloads": -1,
        "filename": "datafun-0.4.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8e8ce612eb4eba996887e8f1146c31f5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 22515,
        "upload_time": "2022-09-27T14:26:43",
        "upload_time_iso_8601": "2022-09-27T14:26:43.784607Z",
        "url": "https://files.pythonhosted.org/packages/ab/06/e63d6773e4e5ab56bc8f9a2cf22e262d37ea2202aa865e857e81fc8f4b91/datafun-0.4.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4265a587879e894567d48953998290c7c237f1a9c52876c1063d559c911acefe",
          "md5": "1637c3ddcc420a805258822235d5087b",
          "sha256": "2c70740b7f18f01ec9f105a263746deb1b2808627d9c0f31d9d4ba98257ea42a"
        },
        "downloads": -1,
        "filename": "datafun-0.4.3.tar.gz",
        "has_sig": false,
        "md5_digest": "1637c3ddcc420a805258822235d5087b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 24797,
        "upload_time": "2022-09-27T14:26:45",
        "upload_time_iso_8601": "2022-09-27T14:26:45.313587Z",
        "url": "https://files.pythonhosted.org/packages/42/65/a587879e894567d48953998290c7c237f1a9c52876c1063d559c911acefe/datafun-0.4.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ab06e63d6773e4e5ab56bc8f9a2cf22e262d37ea2202aa865e857e81fc8f4b91",
        "md5": "8e8ce612eb4eba996887e8f1146c31f5",
        "sha256": "6019efcf50836a303ceba7dc5144ce3580f9a29ab8cc62fed30bddabcb08c158"
      },
      "downloads": -1,
      "filename": "datafun-0.4.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8e8ce612eb4eba996887e8f1146c31f5",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 22515,
      "upload_time": "2022-09-27T14:26:43",
      "upload_time_iso_8601": "2022-09-27T14:26:43.784607Z",
      "url": "https://files.pythonhosted.org/packages/ab/06/e63d6773e4e5ab56bc8f9a2cf22e262d37ea2202aa865e857e81fc8f4b91/datafun-0.4.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4265a587879e894567d48953998290c7c237f1a9c52876c1063d559c911acefe",
        "md5": "1637c3ddcc420a805258822235d5087b",
        "sha256": "2c70740b7f18f01ec9f105a263746deb1b2808627d9c0f31d9d4ba98257ea42a"
      },
      "downloads": -1,
      "filename": "datafun-0.4.3.tar.gz",
      "has_sig": false,
      "md5_digest": "1637c3ddcc420a805258822235d5087b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 24797,
      "upload_time": "2022-09-27T14:26:45",
      "upload_time_iso_8601": "2022-09-27T14:26:45.313587Z",
      "url": "https://files.pythonhosted.org/packages/42/65/a587879e894567d48953998290c7c237f1a9c52876c1063d559c911acefe/datafun-0.4.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}