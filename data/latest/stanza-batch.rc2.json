{
  "info": {
    "author": "Andrew Moore",
    "author_email": "andrew.p.moore94@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# stanza-batch\n\n<p align=\"center\">\n    <a href=\"https://github.com/apmoore1/stanza-batch/actions?query=workflow%3Atest-action\"> <img alt=\"CI\" src=\"https://github.com/apmoore1/stanza-batch/workflows/test-action/badge.svg?event=push&branch=main\"></a>\n    <a href=\"https://codecov.io/gh/apmoore1/stanza-batch\"> <img alt=\"Codecov\" src=\"https://codecov.io/gh/apmoore1/stanza-batch/branch/main/graph/badge.svg\"></a>\n    <a href=\"https://github.com/apmoore1/stanza-batch/blob/main/LICENSE\"> <img alt=\"License\" src=\"https://img.shields.io/github/license/apmoore1/stanza-batch\"></a>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://pypi.org/project/stanza-batch/\"> <img alt=\"PyPi\" src=\"https://img.shields.io/pypi/v/stanza-batch\"> </a>\n    <a href=\"https://pypi.org/project/stanza-batch/\"> <img alt=\"Supported Python Versions\" src=\"https://img.shields.io/pypi/pyversions/stanza-batch\"> </a>\n</p>\n\n## Quick links\n\n1. [Introduction](#introduction)\n2. [Installation](#installation)\n3. [Edge cases](#edge-cases)\n4. [Handling Out Of Memory (OOM) errors](#handling-out-of-memory-oom-errors)\n5. [Developing/Testing](#developingtesting)\n6. [Memory management](#memory-management)\n7. [Acknowledgements](#acknowledgements)\n\n## Introduction\n\n**Currently supports [Stanza](https://github.com/stanfordnlp/stanza) version 1.1.1 and 1.2.0**\n\n\nIs a batching utility for [Stanza](https://github.com/stanfordnlp/stanza) making processing documents/texts with Stanza quicker and easier. The current recommendation for batching by [Stanza is to concatenate documents together with each document separated by a blank line (`\\n\\n`)](https://github.com/stanfordnlp/stanza#batching-to-maximize-pipeline-speed). This way of batching has one main drawback:\n\n1. The return of processing this document is one Stanza Document with lots of sentences, thus you don't know where one document ends and another starts, easily.\n\nThis batching utility solves this problem, when given a list of documents, it will return a list of corresponding processed Stanza documents. Below we show a comparison of the current Stanza batching system and how batching works with this utility:\n\n```python\nimport stanza\nfrom stanza.models.common.doc import Document\n# Documents to process\ndocument_1 = 'Hello how are you\\n\\nI am good thank you'\ndocument_2 = 'This is a different document'\n# Create the document batch\nbatch_document = '\\n\\n'.join([document_1, document_2])\n# Download Stanza English model\nstanza.download(\"en\")\n# stanza model\nnlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize\",)\nstanza_batch = nlp(batch_document)\nassert isinstance(stanza_batch, Document)\nstanza_document = 'Hello how are you\\n\\nI am good thank you\\n\\nThis is a different document'\nassert stanza_batch.text == stanza_document\nassert len(stanza_batch.sentences) == 3\n\n# Using Stanza Batch\nfrom typing import List\nfrom stanza_batch import batch\nstanza_documents: List[Document] = []\nfor document in batch([document_1, document_2], nlp, batch_size=32): # Default batch size is 32\n    stanza_documents.append(document)\nassert len(stanza_documents) == 2 # 2 Documents after processing\n# Each document contains the same raw text after processing\nassert stanza_documents[0].text == document_1 \nassert stanza_documents[1].text == document_2\n# Each document contains the expected number of sentences\nassert len(stanza_documents[0].sentences) == 2\nassert len(stanza_documents[1].sentences) == 1\n```\n\nAs we can see above the new `batch` function yields corresponding processed documents from the given iterable `[document_1, document_2]`. Compared to the original version which only yields one processed document for all the documents in the iterable.\n\n## Installation\n\nRequires Python 3.6.1 or later. As the package depends on [Stanza](https://github.com/stanfordnlp/stanza) which also depends on [PyTorch](https://pytorch.org/) we recommend that you install the version of PyTorch that suits your setup first (e.g. CPU or GPU PyTorch and then if GPU a specific CUDA version).\n\n``` bash\npip install stanza-batch\n```\n\n## Edge cases\n\nThe way `stanza-batch` performs batching it does so by making use of the `\\n\\n` batching approaching, but it keeps track of the documents given to the batch process. However by making use of the `\\n\\n` batching approach and other assumptions it does come with some edge cases. All of these edge cases will mean that the length of the document in characters will be different as whitespace is removed from the documents, but content characters will not be removed:\n\n### Removal of whitespace at the start and end of documents\n\nAs we only keep track of token offsets any whitespace at the start or end of document will be removed:\n```python\nimport stanza\nfrom stanza_batch import batch\n# Download Stanza English model\nstanza.download(\"en\")\n# stanza model\nnlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize\",)\ndocument_1 = '\\n  Hello how are you\\n\\nI am good thank you  \\n'\nstanza_document = [doc for doc in batch([document_1], nlp)][0]\nassert stanza_document.text == 'Hello how are you\\n\\nI am good thank you'\n```\n\n### Swapping of whitespace characters\n\nThe batching approach does not actually split on `\\n\\n` it actually makes use of the following regex `\\n\\s*\\n`, to ensure that we can keep track of which Stanza sentence belongs to which document we split each document with this regex and replace that whitespace with `\\n\\n`. Therefore this means that if you had a document that contains `\\n  \\n` this will be replaced with `\\n\\n`:\n```python\nimport stanza\nfrom stanza_batch import batch\n# Download Stanza English model\nstanza.download(\"en\")\n# stanza model\nnlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize\",)\ndocument_1 = 'Hello how are you\\n \\n \\nI am good thank you'\nstanza_document = [doc for doc in batch([document_1], nlp)][0]\nassert stanza_document.text == 'Hello how are you\\n\\nI am good thank you'\n```\n\n## Handling Out Of Memory (OOM) errors\n\nWhen batching for inference you normally want the largest possible batch size without causing an OOM (for either CPU or GPU memory). A package that I found useful for this is the [toma package](https://github.com/BlackHC/toma). The [`toma.simple.batch` method](https://github.com/BlackHC/toma/blob/master/toma/__init__.py#L25) wraps a function and if a RuntimeError is caused it will reduce the batch size by half and re-run the function until it finishes or it causes another RuntimeError and then the process is repeated. Example of how to use it with this project, in this example we use part of the Jane Austin Emma book as the text data which can be found in the test data [./tests/data/jane_austin_emma_data.txt](./tests/data/jane_austin_emma_data.txt):\n```python\nfrom typing import List\nfrom pathlib import Path\nimport stanza\nfrom stanza.models.common.doc import Document\nimport toma\n\nfrom stanza_batch import batch\n\n# toma requires the first argument of the method to be the batch size\ndef run_batch(batch_size: int, stanza_nlp: stanza.Pipeline, data: List[str]\n              ) -> List[Document]:\n    # So that we can see what the batch size changes to.\n    print(batch_size)\n    return [doc for doc in batch(data, stanza_nlp, batch_size=batch_size)]\n\nemma_fp = Path(__file__, '..', 'tests', 'data', 'jane_austin_emma_data.txt').resolve()\njane_austin_data: List[str] = []\nwith emma_fp.open('r') as emma_file:\n    jane_austin_data = [line for line in emma_file]\n# Make the text much larger to cause OOM\njane_austin_data = jane_austin_data * 5\nassert len(jane_austin_data) == 2450\n# Download Stanza English model\nstanza.download(\"en\")\n# We set the initial batchsize as 500 and add POS and NER tagging\nnlp = stanza.Pipeline(lang=\"en\", processors=\"tokenize,pos,ner\", \n                      tokenize_batch_size=500,\n                      ner_batch_size=500,)\n\nprocessed_documents: List[Document] = toma.simple.batch(run_batch, 500, \n                                                        nlp, jane_austin_data)\nassert len(processed_documents) == 2450\n```\nRunning this code on my local machine which had 16GB RAM and Nvidia 1060 6GB GPU reduces the batch size from 500 to 62 and took around 102 seconds to run. If I had ran this code knowing 62 is close to the largest batch size that my computer can mange the code would have taken 90 seconds (keeping the ner_batch_size=500 but all others to 62), showing the time/processing cost of causing an OOM exception.\n\n## Developing/Testing\n\nThis code base uses `flake8`, `mypy`, and `black` to ensure that the format of the code is consistent. The `flake8` defaults ([./.flake8](./.flake8)) are the same as those in the [Spacy project](https://github.com/explosion/spaCy/blob/master/setup.cfg#L94) and the `black` defaults follow those as well. The `mypy` requirements is mainly here due to my own preference on having type hints in the code base.\n\nTo install all of these requirements:\n```bash\npip install -r dev-requirements.txt\n```\n\nTo use black, flake8, mypy, and pytest use the following commands:\n``` bash\nblack --exclude profile_stanza.py --line-length 80 .\nflake8 .\nmypy\npython -m pytest --cov=stanza_batch --cov-report term-missing\n```\n\nThe flake8, mypy, and pytest have to pass whereby the pytest test coverage should be 100% for a pull request to be accepted. If these requirements are not met in your pull request we will work with you to resolve any issues, so please do not get put off creating a pull request if you cannot pass any/all of these requirements.\n\n## Memory management\n\nOne of the arguments to `batch` is `clear_cache` which clears the GPU memory after every batch. This is important as Python often does not clear this up quickly this can quickly cause an Out Of Memory (OOM) problem. Below we profile the `batch` function using `clear_cache` and not. For this we use [gputil](https://github.com/anderskm/gputil):\n```python\nimport argparse\nfrom pathlib import Path\nfrom typing import List\nfrom time import time\nfrom pathlib import Path\n\nimport stanza\nfrom stanza.models.common.doc import Document\nimport stanza_batch\nimport GPUtil\nimport matplotlib.pyplot as plt\n\ndef path_type(fp: str) -> Path:\n    return Path(fp).resolve()\n\nif __name__ == \"__main__\":\n    save_fp_help = ('File path to save the GPU memory as a '\n                    'function of documents processed plot. '\n                    'As we have to get the GPU memory usage for each '\n                    'document this slows the program down a lot.')\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--save-fp', help=save_fp_help, type=path_type)\n    parser.add_argument('--clear-cache', action='store_true')\n    args = parser.parse_args()\n\n    stanza.download(\"en\", processors=\"tokenize,pos,sentiment\")\n    nlp = stanza.Pipeline(\n        lang=\"en\", processors=\"tokenize,pos,sentiment\", use_gpu=True\n    )\n\n    book_data: List[str] = []\n    test_data_dir = Path(__file__, \"..\", \"tests\", \"data\").resolve()\n    with Path(test_data_dir, \"jane_austin_emma_data.txt\").open(\n        \"r\"\n    ) as emma_file:\n        book_data = [line for line in emma_file]\n    assert len(book_data) == 490\n\n    t = time()\n    gpu_memory_used: List[float] = []\n    processed_book_data: List[Document] = []\n    for document in stanza_batch.batch(book_data, nlp, clear_cache=args.clear_cache):\n        processed_book_data.append(document)\n        if args.save_fp:\n            # assuming the first GPU is the one being used.\n            gpu_memory_used.append(GPUtil.getGPUs()[0].memoryUsed)\n    print(f'Time taken: {time() - t}')\n\n    if args.save_fp:\n        number_documents_processed = range(len(processed_book_data))\n        plt.plot(number_documents_processed, gpu_memory_used)\n        plt.xlabel('Number of documents processed')\n        plt.ylabel('GPU Memory used (MB)')\n        plt.grid(True)\n        plt.savefig(str(args.save_fp))\n```\n\nTo run the following need to install `gputil`:\n```bash\npip install \"gputil>=1.4.0\"\n```\n\nAs we can see below using the `clear_cache` does uses less memory overall and has a lower maximum GPU memory usage, there is a slight time plenty of 2% but that could be due to other factors and is marginal compared to the memory difference of maximum GPU usage of <3200MB for `clear_cache` compared to just over 3400MB for not using `clear_cache` (at least 6% difference). Furthermore the `clear_cache` does not accumulate GPU memory thus over a larger job the memory issue for **not** using `clear_cache` will just keep getting worse.\n\n### Clear cache = True\n```bash\npython profile_stanza.py --clear-cache --save-fp ./gpu_profile_plots/clear_cache.png\n```\n\nIgnore the time that comes from this as sampling the memory usage takes a long time. To get the time taken to run this script use the script without `--save-fp`:\n\n```bash\npython profile_stanza.py --clear-cache\n```\n\nTime taken: 11.91\n\nGPU memory usage plot:\n\n![GPU memory usage vs the number of documents processed](./gpu_profile_plots/clear_cache.png \"GPU memory usage vs the number of documents processed\")\n\n### Clear cache = False\n```bash\npython profile_stanza.py --save-fp ./gpu_profile_plots/non_clear_cache.png\n```\n\nIgnore the time that comes from this as sampling the memory usage takes a long time. To get the time taken to run this script use the script without `--save-fp`:\n\n```bash\npython profile_stanza.py\n```\n\nTime taken: 11.61\n\nGPU memory usage plot:\n\n![GPU memory usage vs the number of documents processed](./gpu_profile_plots/non_clear_cache.png \"GPU memory usage vs the number of documents processed\")\n\n## Acknowledgements\n\nThis work has been funded through the [UCREL research centre at Lancaster University](http://ucrel.lancs.ac.uk/).\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/apmoore1/stanza-batch",
    "keywords": "natural-language-processing nlp natural-language-understanding stanford-nlp deep-learning",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "stanza-batch",
    "package_url": "https://pypi.org/project/stanza-batch/",
    "platform": "",
    "project_url": "https://pypi.org/project/stanza-batch/",
    "project_urls": {
      "Homepage": "https://github.com/apmoore1/stanza-batch"
    },
    "release_url": "https://pypi.org/project/stanza-batch/0.2.2/",
    "requires_dist": [
      "stanza (<=1.2.0,>=1.1.1)"
    ],
    "requires_python": ">=3.6.1",
    "summary": "A batching utility for Stanza",
    "version": "0.2.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9831618,
  "releases": {
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "93f322e7084b9ccd678c35b509bf36c26505b3dec6d45b71658bf9217fc90e3a",
          "md5": "3aae0e0dfed8f292a37d65232d25ab8f",
          "sha256": "fbfc033b6d418e2a6589a923f08731591a2a027c286ab874b72ca0d0c74d8e2e"
        },
        "downloads": -1,
        "filename": "stanza_batch-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3aae0e0dfed8f292a37d65232d25ab8f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.1",
        "size": 14042,
        "upload_time": "2021-02-08T09:32:59",
        "upload_time_iso_8601": "2021-02-08T09:32:59.348477Z",
        "url": "https://files.pythonhosted.org/packages/93/f3/22e7084b9ccd678c35b509bf36c26505b3dec6d45b71658bf9217fc90e3a/stanza_batch-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "130988cbba56aaf276817c85bb5ac1331310c459d03c3183e2348c5ab7100330",
          "md5": "524e69649f3ad916ecfd4611643aa52a",
          "sha256": "8b339f64c71a51bfe38e31431b7a7dc1ef1cd9b318410e1425422512ad5fcbe2"
        },
        "downloads": -1,
        "filename": "stanza_batch-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "524e69649f3ad916ecfd4611643aa52a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.1",
        "size": 14648,
        "upload_time": "2021-02-08T09:33:00",
        "upload_time_iso_8601": "2021-02-08T09:33:00.646673Z",
        "url": "https://files.pythonhosted.org/packages/13/09/88cbba56aaf276817c85bb5ac1331310c459d03c3183e2348c5ab7100330/stanza_batch-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a19d5c3449b089d91a9ef1603a3da560edc9bb27cde7a6ebeb7dde0c1b030aaf",
          "md5": "19b2893567256507506603158ce477dc",
          "sha256": "606fb90586c19755304592799c51b52b84f480f087b34b6eafdc65b074e96050"
        },
        "downloads": -1,
        "filename": "stanza_batch-0.2.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "19b2893567256507506603158ce477dc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.1",
        "size": 14103,
        "upload_time": "2021-03-20T22:36:02",
        "upload_time_iso_8601": "2021-03-20T22:36:02.501800Z",
        "url": "https://files.pythonhosted.org/packages/a1/9d/5c3449b089d91a9ef1603a3da560edc9bb27cde7a6ebeb7dde0c1b030aaf/stanza_batch-0.2.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "161deafef3e793d68dd3bf93387fe4d1ed56ca96759346f1cf93bec922ef2ae3",
          "md5": "dda8247b254cd1d969489e45f12bdc10",
          "sha256": "c4e397efdcb9d41b0067dcb16d49a4f743240c22fb62c6d2dd8191ea433ef734"
        },
        "downloads": -1,
        "filename": "stanza_batch-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "dda8247b254cd1d969489e45f12bdc10",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.1",
        "size": 14795,
        "upload_time": "2021-03-20T22:36:04",
        "upload_time_iso_8601": "2021-03-20T22:36:04.423078Z",
        "url": "https://files.pythonhosted.org/packages/16/1d/eafef3e793d68dd3bf93387fe4d1ed56ca96759346f1cf93bec922ef2ae3/stanza_batch-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a19d5c3449b089d91a9ef1603a3da560edc9bb27cde7a6ebeb7dde0c1b030aaf",
        "md5": "19b2893567256507506603158ce477dc",
        "sha256": "606fb90586c19755304592799c51b52b84f480f087b34b6eafdc65b074e96050"
      },
      "downloads": -1,
      "filename": "stanza_batch-0.2.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "19b2893567256507506603158ce477dc",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.1",
      "size": 14103,
      "upload_time": "2021-03-20T22:36:02",
      "upload_time_iso_8601": "2021-03-20T22:36:02.501800Z",
      "url": "https://files.pythonhosted.org/packages/a1/9d/5c3449b089d91a9ef1603a3da560edc9bb27cde7a6ebeb7dde0c1b030aaf/stanza_batch-0.2.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "161deafef3e793d68dd3bf93387fe4d1ed56ca96759346f1cf93bec922ef2ae3",
        "md5": "dda8247b254cd1d969489e45f12bdc10",
        "sha256": "c4e397efdcb9d41b0067dcb16d49a4f743240c22fb62c6d2dd8191ea433ef734"
      },
      "downloads": -1,
      "filename": "stanza_batch-0.2.2.tar.gz",
      "has_sig": false,
      "md5_digest": "dda8247b254cd1d969489e45f12bdc10",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.1",
      "size": 14795,
      "upload_time": "2021-03-20T22:36:04",
      "upload_time_iso_8601": "2021-03-20T22:36:04.423078Z",
      "url": "https://files.pythonhosted.org/packages/16/1d/eafef3e793d68dd3bf93387fe4d1ed56ca96759346f1cf93bec922ef2ae3/stanza_batch-0.2.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}