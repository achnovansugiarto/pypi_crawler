{
  "info": {
    "author": "ssube",
    "author_email": "seansube@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# ONNX Web\n\nThis is a web UI for running [ONNX models](https://onnx.ai/) with hardware acceleration on both AMD and Nvidia system,\nwith a CPU software fallback.\n\nThe API runs on both Linux and Windows and provides access to the major functionality of [`diffusers`](https://huggingface.co/docs/diffusers/main/en/index),\nalong with metadata about the available models and accelerators, and the output of previous runs. Hardware acceleration\nis supported on both AMD and Nvidia for both Linux and Windows, with a CPU fallback capable of running on laptop-class\nmachines.\n\nThe GUI is [hosted on Github Pages](https://ssube.github.io/onnx-web/) and runs in all major browsers, including on\nmobile devices. It allows you to select the model and accelerator being used for each image pipeline. Image parameters\nare shown for each of the major modes, and you can either upload or paint the mask for inpainting and outpainting. The\nlast few output images are shown below the image controls, making it easy to refer back to previous parameters or save\nan image from earlier.\n\nPlease [see the User Guide](https://github.com/ssube/onnx-web/blob/main/docs/user-guide.md) for more details.\n\n![txt2img with detailed knollingcase renders of a soldier in an alien jungle](./docs/readme-knolling.png)\n\n## Features\n\nThis is an incomplete list of new and interesting features, with links to the user guide:\n\n- hardware acceleration on both AMD and Nvidia\n  - [tested on CUDA, DirectML, and ROCm](#install-pip-packages)\n  - [half-precision support for low-memory GPUs](docs/user-guide.md#optimizing-models-for-lower-memory-usage) on both\n    AMD and Nvidia\n  - software fallback for CPU-only systems\n- web app to generate and view images\n  - [hosted on Github Pages](https://ssube.github.io/onnx-web), from your CDN, or locally\n  - [persists your recent images and progress as you change tabs](docs/user-guide.md#image-history)\n  - queue up multiple images and retry errors\n  - translations available for English, French, German, and Spanish (please open an issue for more)\n- supports many `diffusers` pipelines\n  - [txt2img](docs/user-guide.md#txt2img-tab)\n  - [img2img](docs/user-guide.md#img2img-tab)\n  - [inpainting](docs/user-guide.md#inpaint-tab), with mask drawing and upload\n  - [upscaling](docs/user-guide.md#upscale-tab), with ONNX acceleration\n- [add and use your own models](docs/user-guide.md#adding-your-own-models)\n  - [convert models from diffusers and SD checkpoints](docs/converting-models.md)\n  - [download models from HuggingFace hub, Civitai, and HTTPS sources](docs/user-guide.md#model-sources)\n- blend in additional networks\n  - [permanent and prompt-based blending](docs/user-guide.md#permanently-blending-additional-networks)\n  - [supports LoRA weights](docs/user-guide.md#lora-tokens)\n  - [supports Textual Inversion concepts and embeddings](docs/user-guide.md#textual-inversion-tokens)\n- infinite prompt length\n  - [with long prompt weighting](docs/user-guide.md#long-prompt-weighting)\n  - expand and control Textual Inversions per-layer\n- [image blending mode](docs/user-guide.md#blend-tab)\n  - combine images from history\n- upscaling and face correction\n  - upscaling with Real ESRGAN or Stable Diffusion\n  - face correction with CodeFormer or GFPGAN\n- [API server can be run remotely](docs/server-admin.md)\n  - REST API can be served over HTTPS or HTTP\n  - background processing for all image pipelines\n  - polling for image status, plays nice with load balancers\n- OCI containers provided\n  - for all supported hardware accelerators\n  - includes both the API and GUI bundle in a single container\n  - runs well on [RunPod](https://www.runpod.io/) and other GPU container hosting services\n\n## Contents\n\n- [ONNX Web](#onnx-web)\n  - [Features](#features)\n  - [Contents](#contents)\n  - [Setup](#setup)\n    - [Install Git and Python](#install-git-and-python)\n    - [Note about setup paths](#note-about-setup-paths)\n    - [Create a virtual environment](#create-a-virtual-environment)\n    - [Install pip packages](#install-pip-packages)\n      - [For AMD on Linux: PyTorch ROCm and ONNX runtime ROCm](#for-amd-on-linux-pytorch-rocm-and-onnx-runtime-rocm)\n      - [For AMD on Windows: PyTorch CPU and ONNX runtime DirectML](#for-amd-on-windows-pytorch-cpu-and-onnx-runtime-directml)\n      - [For CPU everywhere: PyTorch CPU and ONNX runtime CPU](#for-cpu-everywhere-pytorch-cpu-and-onnx-runtime-cpu)\n      - [For Nvidia everywhere: Install PyTorch GPU and ONNX GPU](#for-nvidia-everywhere-install-pytorch-gpu-and-onnx-gpu)\n    - [Download and convert models](#download-and-convert-models)\n      - [Converting your own models](#converting-your-own-models)\n    - [Test the models](#test-the-models)\n  - [Usage](#usage)\n    - [Running the containers](#running-the-containers)\n    - [Configuring and running the server](#configuring-and-running-the-server)\n      - [Securing the server](#securing-the-server)\n    - [Updating the server](#updating-the-server)\n    - [Building the client](#building-the-client)\n    - [Hosting the client](#hosting-the-client)\n    - [Customizing the client config](#customizing-the-client-config)\n    - [Known errors and solutions](#known-errors-and-solutions)\n  - [Credits](#credits)\n\n## Setup\n\nTo run the server and generate images, you need to [install Git and Python](#install-git-and-python) along with [a few\npip libraries](#install-pip-packages), then [run the conversion script](#converting-your-own-models) to download and\nconvert [the models you want to use](#download-and-convert-models).\n\n### Install Git and Python\n\nInstall Git and Python 3.10 for your environment:\n\n- https://gitforwindows.org/\n- https://www.python.org/downloads/\n\nThe latest version of git should be fine. Python should be 3.9 or 3.10, although 3.8 and 3.11 may work if the correct\npackages are available for your platform. If you already have Python installed for another form of Stable Diffusion,\nthat should work, but make sure to verify the version in the next step.\n\nMake sure you have Python 3.9 or 3.10:\n\n```shell\n> python --version\nPython 3.10\n```\n\nIf your system differentiates between Python 2 and 3 and uses the `python3` and `pip3` commands for the Python 3.x\ntools, make sure to adjust the commands shown here. They should otherwise be the same: `python3 --version`.\n\nOnce you have those basic packages installed, clone this git repository:\n\n```shell\n> git clone https://github.com/ssube/onnx-web.git\n```\n\n### Note about setup paths\n\nThis project contains both Javascript and Python, for the client and server respectively. Make sure you are in the\ncorrect directory when working with each part.\n\nMost of these setup commands should be run in the Python environment and the `api/` directory:\n\n```shell\n> cd api\n> pwd\n/home/ssube/code/github/ssube/onnx-web/api\n```\n\nThe Python virtual environment will be created within the `api/` directory.\n\nThe Javascript client can be built and run within the `gui/` directory.\n\n### Create a virtual environment\n\nChange into the `api/` directory, then create a virtual environment:\n\n```shell\n> pip install virtualenv\n> python -m venv onnx_env\n```\n\nThis will contain all of the pip libraries. If you update or reinstall Python, you will need to recreate the virtual\nenvironment.\n\nIf you receive an error like `Error: name 'cmd' is not defined`, there may be [a bug in the `venv` module](https://www.mail-archive.com/debian-bugs-dist@lists.debian.org/msg1884072.html) on certain\nDebian-based systems. You may need to install venv through apt instead:\n\n```shell\n> sudo apt install python3-venv   # only if you get an error\n```\n\nEvery time you start using ONNX web, activate the virtual environment:\n\n```shell\n# on linux:\n> source ./onnx_env/bin/activate\n\n# on windows:\n> .\\onnx_env\\Scripts\\Activate.bat\n```\n\nUpdate pip itself:\n\n```shell\n> python -m pip install --upgrade pip\n```\n\n### Install pip packages\n\nYou can install all of the necessary packages at once using [the `requirements/base.txt` file](./api/requirements/base.txt)\nand the `requirements/` file for your platform. Install them in separate commands and make sure to install the\nplatform-specific packages first:\n\n```shell\n> pip install -r requirements/amd-linux.txt\n> pip install -r requirements/base.txt\n# or\n> pip install -r requirements/amd-windows.txt\n> pip install -r requirements/base.txt\n# or\n> pip install -r requirements/cpu.txt\n> pip install -r requirements/base.txt\n# or\n> pip install -r requirements/nvidia.txt\n> pip install -r requirements/base.txt\n```\n\nOnly install one of the platform-specific requirements files, otherwise you may end up with the wrong version of\nPyTorch or the ONNX runtime. The full list of available ONNX runtime packages [can be found here\n](https://download.onnxruntime.ai/).\n\nIf you have successfully installed both of the requirements files for your platform, you do not need to install\nany of the packages shown in the following platform-specific sections.\n\nThe ONNX runtime nightly packages used by the `requirements/*-nightly.txt` files can be substantially faster than the\nlast release, but may not always be stable. Many of the nightly packages are specific to one version of Python and\nsome are only available for Python 3.8 and 3.9, so you may need to find the correct package for your environment. If\nyou are using Python 3.10, download the `cp310` package. For Python 3.9, download the `cp39` package, and so on.\nInstalling with pip will figure out the correct package for you.\n\n#### For AMD on Linux: PyTorch ROCm and ONNX runtime ROCm\n\nIf you are running on Linux with an AMD GPU, install the ROCm versions of PyTorch and `onnxruntime`:\n\n```shell\n> pip install \"torch==1.13.1\" \"torchvision==0.14.1\" --extra-index-url https://download.pytorch.org/whl/rocm5.2\n# and one of\n> pip install https://download.onnxruntime.ai/onnxruntime_training-1.14.1%2Brocm54-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n# or\n> pip install https://download.onnxruntime.ai/onnxruntime_training-1.15.0.dev20230326001%2Brocm542-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n```\n\nMake sure you have installed ROCm 5.x ([see their documentation\n](https://docs.amd.com/bundle/ROCm-Installation-Guide-v5.2.3/page/How_to_Install_ROCm.html#_How_to_Install) for more\ndetails) and that the version of `onnxruntime` matches your ROCm drivers. The version of PyTorch does not need to match\nexactly, and they only have limited versions available.\n\nUbuntu 20.04 supports ROCm 5.2 and Ubuntu 22.04 supports ROCm 5.4, unless you want to build custom packages. The ROCm\n5.x series supports many discrete AMD cards since the Vega 20 architecture, with [a partial list of supported cards\nshown here](https://docs.amd.com/bundle/ROCm-Installation-Guide-v5.4.3/page/Prerequisites.html#d5434e465).\n\n#### For AMD on Windows: PyTorch CPU and ONNX runtime DirectML\n\nIf you are running on Windows with an AMD GPU, install the DirectML ONNX runtime as well:\n\n```shell\n> pip install \"torch==1.13.1\" \"torchvision==0.14.1\" --extra-index-url https://download.pytorch.org/whl/cpu\n# and one of\n> pip install onnxruntime-directml\n# or\n> pip install ort-nightly-directml --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ --force-reinstall\n```\n\nIf you DirectML package upgrades numpy to an incompatible version >= 1.24, downgrade it:\n\n```shell\n> pip install \"numpy>=1.20,<1.24\" --force-reinstall  # the DirectML package will upgrade numpy to 1.24, which will not work\n```\n\nYou can optionally install the latest DirectML ORT nightly package, which may provide a substantial performance\nincrease.\n\n#### For CPU everywhere: PyTorch CPU and ONNX runtime CPU\n\nIf you are running with a CPU and no hardware acceleration, install `onnxruntime` and the CPU version of PyTorch:\n\n```shell\n> pip install \"torch==1.13.1\" \"torchvision==0.14.1\" --extra-index-url https://download.pytorch.org/whl/cpu\n# and\n> pip install onnxruntime\n# or\n> pip install ort-nightly --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ --force-reinstall\n```\n\n#### For Nvidia everywhere: Install PyTorch GPU and ONNX GPU\n\nIf you are running with an Nvidia GPU on any operating system, install `onnxruntime-gpu` and the CUDA version of\nPyTorch:\n\n```shell\n> pip install \"torch==1.13.1\" \"torchvision==0.14.1\" --extra-index-url https://download.pytorch.org/whl/cu117\n# and\n> pip install onnxruntime-gpu\n# or\n> pip install ort-nightly-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/ORT-Nightly/pypi/simple/ --force-reinstall\n```\n\nMake sure you have installed CUDA 11.x and that the version of PyTorch matches the version of CUDA\n([see their documentation](https://pytorch.org/get-started/locally/) for more details).\n\n### Download and convert models\n\nSign up for an account at https://huggingface.co and find the models you want to use. Some popular options are\nalready listed in the `convert.py` script, including:\n\n- https://huggingface.co/runwayml/stable-diffusion-v1-5\n- https://huggingface.co/runwayml/stable-diffusion-inpainting\n- https://huggingface.co/stabilityai/stable-diffusion-2-1\n- https://huggingface.co/stabilityai/stable-diffusion-2-inpainting\n- https://huggingface.co/Aybeeceedee/knollingcase\n- https://huggingface.co/prompthero/openjourney\n\nYou will need at least one of the base models for txt2img and img2img mode. If you want to use inpainting, you will\nalso need one of the inpainting models. The upscaling and face correction models are downloaded from Github by the\nsame script.\n\nLog into the HuggingFace CLI:\n\n```shell\n# on linux:\n> huggingface-cli login\n\n# on Windows:\n> huggingface-cli.exe login\n```\n\nIssue an API token from https://huggingface.co/settings/tokens, naming it something memorable like `onnx-web`, and then\npaste it into the prompt.\n\nRunning the launch script from the `api/` directory will convert the base models before starting the API server:\n\n```shell\n# on Linux:\n> ./launch.sh\n\n# on Windows:\n> launch.bat\n```\n\nModels that have already been downloaded and converted will be skipped, so it should be safe to run this script after\nevery update. Some additional, more specialized models are available using the `--extras` flag.\n\nThe conversion script has a few other options, which can be printed using `python -m onnx_web.convert --help`. If you\nare using CUDA on Nvidia hardware, using the `--half` option may make things faster.\n\nThis will take a little while to convert each model. Stable diffusion v1.4 is about 6GB, v1.5 is at least 10GB or so.\nYou can skip certain models by including a `--skip names` argument if you want to save time or disk space. For example,\nusing `--skip stable-diffusion-onnx-v2-inpainting stable-diffusion-onnx-v2-1` will not download the Stable\nDiffusion v2 models.\n\n#### Converting your own models\n\nYou can include your own models in the conversion script without making any code changes and download additional\nnetworks to blend during conversion or by using prompt tokens. For more details, please [see the user guide\n](./docs/user-guide.md#adding-your-own-models).\n\nMake a copy of the `api/extras.json` file and edit it to include the models you want to download and convert:\n\n```json\n{\n  \"diffusion\": [\n    {\n      \"name\": \"diffusion-knollingcase\",\n      \"label\": \"Knollingcase\",\n      \"source\": \"Aybeeceedee/knollingcase\"\n    },\n    {\n      \"name\": \"diffusion-openjourney\",\n      \"source\": \"prompthero/openjourney\"\n    },\n    {\n      \"name\": \"diffusion-stablydiffused-aesthetic-v2-6\",\n      \"label\": \"Stably Diffused Aesthetic Mix v2.6\",\n      \"source\": \"civitai://6266?type=Pruned%20Model&format=SafeTensor\",\n      \"format\": \"safetensors\"\n    },\n    {\n      \"name\": \"diffusion-unstable-ink-dream-v6\",\n      \"label\": \"Unstable Ink Dream v6\",\n      \"source\": \"civitai://5796\",\n      \"format\": \"safetensors\"\n    },\n    {\n      \"name\": \"sonic-diffusion-v1-5\",\n      \"source\": \"runwayml/stable-diffusion-v1-5\",\n      \"inversions\": [\n        {\n          \"name\": \"ugly-sonic\",\n          \"source\": \"huggingface://sd-concepts-library/ugly-sonic\",\n          \"model\": \"concept\"\n        }\n      ]\n    }\n  ],\n  \"correction\": [],\n  \"upscaling\": [],\n  \"networks\": [\n    {\n      \"name\": \"cubex\",\n      \"source\": \"huggingface://sd-concepts-library/cubex\",\n      \"label\": \"Cubex\",\n      \"model\": \"concept\",\n      \"type\": \"inversion\"\n    },\n    {\n      \"name\": \"birb\",\n      \"source\": \"huggingface://sd-concepts-library/birb-style\",\n      \"label\": \"Birb\",\n      \"model\": \"concept\",\n      \"type\": \"inversion\"\n    },\n    {\n      \"name\": \"minecraft\",\n      \"source\": \"huggingface://sd-concepts-library/minecraft-concept-art\",\n      \"label\": \"Minecraft Concept\",\n      \"model\": \"concept\",\n      \"type\": \"inversion\"\n    },\n  ],\n  \"sources\": []\n}\n```\n\nSet the `ONNX_WEB_EXTRA_MODELS` environment variable to the path to your new `extras.json` file before running the\nlaunch script:\n\n```shell\n# on Linux:\n> export ONNX_WEB_EXTRA_MODELS=\"/home/ssube/onnx-web-extras.json\"\n> ./launch-extras.sh\n\n# on Windows:\n> set ONNX_WEB_EXTRA_MODELS=C:\\Users\\ssube\\onnx-web-extras.json\n> launch-extras.bat\n```\n\nMake sure to use the `launch-extras.sh` or `.bat` script if you want to convert the extra models, especially if you\nhave added your own.\n\n### Test the models\n\nYou should verify that all of the steps up to this point have worked correctly by attempting to run the\n`api/scripts/test-diffusers.py` script, which is a slight variation on the original txt2img script.\n\nIf the script works, there will be an image of an astronaut in `outputs/test.png`.\n\nIf you get any errors, check [the known errors section](#known-errors-and-solutions).\n\n## Usage\n\n### Running the containers\n\nOCI images are available for both the API and GUI, `ssube/onnx-web-api` and `ssube/onnx-web-gui`, respectively. These\nare regularly built from the `main` branch and for all tags.\n\nWhile two containers are provided, the API container also includes the GUI bundle. In most cases, you will only need to\nrun the API container. You may need both if you are hosting the API and GUI from separate pods or on different machines.\n\nWhen using the containers, make sure to mount the `models/` and `outputs/` directories. The models directory can be\nread-only, but outputs should be read-write.\n\n```shell\n> podman run -p 5000:5000 --rm -v ../models:/models:ro -v ../outputs:/outputs:rw docker.io/ssube/onnx-web-api:main-buster\n\n> podman run -p 8000:80 --rm docker.io/ssube/onnx-web-gui:main-nginx-bullseye\n```\n\nThe `ssube/onnx-web-gui` image is available in both Debian and Alpine-based versions, but the `ssube/onnx-web-api`\nimage is only available as a Debian-based image, due to [this Github issue with `onnxruntime`](https://github.com/microsoft/onnxruntime/issues/2909#issuecomment-593591317).\n\n### Configuring and running the server\n\nThe server relies mostly on two paths, the models and outputs. It will make sure both paths exist when it starts up,\nand will exit with an error if the models path does not.\n\nBoth of those paths exist in the git repository, with placeholder files to make sure they exist. You should not have to\ncreate them, if you are using the default settings. You can customize the paths by setting `ONNX_WEB_MODEL_PATH` and\n`ONNX_WEB_OUTPUT_PATH`, if your models exist somewhere else or you want output written to another disk, for example.\n\nFrom within the `api/` directory, run the Flask server with the launch script:\n\n```shell\n# on Linux:\n> ./launch.sh\n\n# on Windows:\n> launch.bat\n```\n\nThis will allow access from other machines on your local network, but does not automatically make the server\naccessible from the internet. You can access the server through the IP address printed in the console.\n\nIf you _do not_ want to allow access to the server from other machines on your local network, run the Flask server\n_without_ the `--host` argument:\n\n```shell\n> flask --app=onnx_web.serve run\n```\n\nYou can stop the server by pressing `Ctrl+C`.\n\n#### Securing the server\n\nWhen making the server publicly visible, make sure to use appropriately restrictive firewall rules along with it, and\nconsider using a web application firewall to help prevent malicious requests.\n\n### Updating the server\n\nMake sure to update your server occasionally. New features in the GUI may not be available on older servers, leading to\noptions being ignored or menus not loading correctly.\n\nTo update the server, make sure you are on the `main` branch and pull the latest version from Github:\n\n```shell\n> git branch\n* main\n\n> git pull\n```\n\nIf you want to run a specific tag of the server, run `git checkout v0.9.0` with the desired tag.\n\n### Building the client\n\nIf you plan on building the GUI bundle, instead of using a hosted version [like on Github Pages](https://ssube.github.io/onnx-web),\nyou will also need to install NodeJS 18:\n\n- https://nodejs.org/en/download/\n\nIf you are using Windows and Git Bash, you may not have `make` installed. You can [add some of the missing tools](https://gist.github.com/evanwill/0207876c3243bbb6863e65ec5dc3f058) from [the `ezwinports` project](https://sourceforge.net/projects/ezwinports/files/) and others.\n\nFrom within the `gui/` directory, edit the `gui/examples/config.json` file so that `api.root` matches the URL printed\nout by the `flask run` command you ran earlier. It should look something like this:\n\n```json\n{\n  \"api\": {\n    \"root\": \"http://127.0.0.1:5000\"\n  }\n}\n```\n\nStill in the `gui/` directory, build the UI bundle and run the dev server with Node:\n\n```shell\n> npm install -g yarn   # update the package manager\n\n> make bundle\n\n> node serve.js\n```\n\n### Hosting the client\n\nYou should be able to access the web interface at http://127.0.0.1:8000/index.html or your local machine's hostname.\n\n- If you get a `Connection Refused` error, make sure you are using the correct address and the dev server is still running.\n- If you get a `File not found` error, make sure you have built the UI bundle (`make bundle`) and are using the `/index.html` path\n\nThe txt2img tab will be active by default, with an example prompt. When you press the `Generate` button, an image should\nappear on the page 10-15 seconds later (depending on your GPU and other hardware). Generating images on CPU will take\nsubstantially longer, at least 2-3 minutes. The last four images will be shown, along with the parameters used to\ngenerate them.\n\n### Customizing the client config\n\nYou can customize the config file if you want to change the default model, platform (hardware acceleration), scheduler,\nand prompt. If you have a good base prompt or always want to use the CPU fallback, you can set that in the config file:\n\n```json\n{\n  \"default\": {\n    \"model\": \"stable-diffusion-onnx-v1-5\",\n    \"platform\": \"amd\",\n    \"scheduler\": \"euler-a\",\n    \"prompt\": \"an astronaut eating a hamburger\"\n  }\n}\n```\n\nWhen running the dev server, `node serve.js`, the config file will be loaded from `out/config.json`. If you want to load\na different config file, save it to your home directory named `onnx-web-config.json` and copy it into the output\ndirectory after building the bundle:\n\n```shell\n> make bundle && cp -v ~/onnx-web-config.json out/config.json\n```\n\nWhen running the container, the config will be loaded from `/usr/share/nginx/html/config.json` and you can mount a\ncustom config using:\n\n```shell\n> podman run -p 8000:80 --rm -v ~/onnx-web-config.json:/usr/share/nginx/html/config.json:ro docker.io/ssube/onnx-web-gui:main-nginx-bullseye\n```\n\n### Known errors and solutions\n\nPlease see [the Known Errors section of the user guide](https://github.com/ssube/onnx-web/blob/main/docs/user-guide.md#known-errors).\n\n## Credits\n\nSome of the conversion code was copied or derived from code in:\n\n- https://github.com/huggingface/diffusers/blob/main/scripts/convert_stable_diffusion_checkpoint_to_onnx.py\n  - https://github.com/huggingface/diffusers/blob/main/LICENSE\n- https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/dreambooth/sd_to_diff.py\n  - https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/license.md\n\nThose parts have their own license with additional restrictions and may need permission for commercial usage.\n\nGetting this set up and running on AMD would not have been possible without guides by:\n\n- https://gist.github.com/harishanand95/75f4515e6187a6aa3261af6ac6f61269\n- https://gist.github.com/averad/256c507baa3dcc9464203dc14610d674\n- https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs\n- https://www.travelneil.com/stable-diffusion-updates.html\n- https://github.com/Amblyopius/AMD-Stable-Diffusion-ONNX-FP16\n\nThere are many other good options for using Stable Diffusion with hardware acceleration, including:\n\n- https://github.com/azuritecoin/OnnxDiffusersUI\n- https://github.com/ForserX/StableDiffusionUI\n- https://github.com/pingzing/stable-diffusion-playground\n- https://github.com/quickwick/stable-diffusion-win-amd-ui\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ssube/onnx-web",
    "keywords": "onnx",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "onnx-web",
    "package_url": "https://pypi.org/project/onnx-web/",
    "platform": null,
    "project_url": "https://pypi.org/project/onnx-web/",
    "project_urls": {
      "Homepage": "https://github.com/ssube/onnx-web"
    },
    "release_url": "https://pypi.org/project/onnx-web/0.9.0/",
    "requires_dist": null,
    "requires_python": ">=3.8,<3.11",
    "summary": "web UI for running ONNX models",
    "version": "0.9.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17484966,
  "releases": {
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "893b57ffecd9916742bad134d0dc2009840d3829e363f83e27408a91a6edc444",
          "md5": "4d825f406680e231a9d16ccd27e8e832",
          "sha256": "c4df069fc272ec369a25d271b1bf18fdae9feb09c5078e344336bdf5984140e2"
        },
        "downloads": -1,
        "filename": "onnx-web-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4d825f406680e231a9d16ccd27e8e832",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 4539,
        "upload_time": "2023-01-10T14:47:50",
        "upload_time_iso_8601": "2023-01-10T14:47:50.028404Z",
        "url": "https://files.pythonhosted.org/packages/89/3b/57ffecd9916742bad134d0dc2009840d3829e363f83e27408a91a6edc444/onnx-web-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eede69bf9228c74f82e89b393bdbf33f09b362ad4afa33af2eb1da97f0db65e4",
          "md5": "912c6d25b31d21ab9133df54cee5a276",
          "sha256": "d5e86d40752a73a4a805154364b8b6d037ca929409db20afd4cc64f812906966"
        },
        "downloads": -1,
        "filename": "onnx-web-0.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "912c6d25b31d21ab9133df54cee5a276",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 6715,
        "upload_time": "2023-01-15T21:07:05",
        "upload_time_iso_8601": "2023-01-15T21:07:05.528607Z",
        "url": "https://files.pythonhosted.org/packages/ee/de/69bf9228c74f82e89b393bdbf33f09b362ad4afa33af2eb1da97f0db65e4/onnx-web-0.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "16496bd7619039f18b090df81ce671b56a8f47fde96d30396df8f765025f040c",
          "md5": "af350cf22f8b85bcf5f7bf0d5ce3d6d9",
          "sha256": "43294e7f6eb484a77aae9c13b764c96d9fb0cc30ed38c56b8989d7a7496c9b0e"
        },
        "downloads": -1,
        "filename": "onnx-web-0.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "af350cf22f8b85bcf5f7bf0d5ce3d6d9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 14227,
        "upload_time": "2023-01-24T14:53:18",
        "upload_time_iso_8601": "2023-01-24T14:53:18.928923Z",
        "url": "https://files.pythonhosted.org/packages/16/49/6bd7619039f18b090df81ce671b56a8f47fde96d30396df8f765025f040c/onnx-web-0.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0b64312cf5e6a852db6174982a98a3e22a3236bfe4aeaf05ddc31c8e1fc2c68b",
          "md5": "f365f2e83d9c15210af668695167c765",
          "sha256": "d7cfa37d6489921cf5a182963f4e900e9a286912ea8370578e5fe48e49e47f41"
        },
        "downloads": -1,
        "filename": "onnx-web-0.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f365f2e83d9c15210af668695167c765",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 24156,
        "upload_time": "2023-02-07T14:18:28",
        "upload_time_iso_8601": "2023-02-07T14:18:28.695104Z",
        "url": "https://files.pythonhosted.org/packages/0b/64/312cf5e6a852db6174982a98a3e22a3236bfe4aeaf05ddc31c8e1fc2c68b/onnx-web-0.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5dd5a9b1897cc3d586fb55da37616362dfede5abec4e39eb83d62fb49b4830a4",
          "md5": "7c40930f33b3a2fa5225189c888351cf",
          "sha256": "c9adc3fe4efb346c232c64fac2ffd9a821df4cdb6bc3877c9da81d3c6597764d"
        },
        "downloads": -1,
        "filename": "onnx-web-0.6.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7c40930f33b3a2fa5225189c888351cf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 24171,
        "upload_time": "2023-02-07T14:32:54",
        "upload_time_iso_8601": "2023-02-07T14:32:54.484497Z",
        "url": "https://files.pythonhosted.org/packages/5d/d5/a9b1897cc3d586fb55da37616362dfede5abec4e39eb83d62fb49b4830a4/onnx-web-0.6.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.7.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dfc727fd0630e7adfdfc2bd55d65e575cf0700ed301148c19d689b80a8a79bf0",
          "md5": "0d72154452624dd8f52c1e12c9dde0d3",
          "sha256": "64ac5325d34a1803b2a5393e14f8c9b8adb53285ec1bc512f37aeb0f9df2a1c6"
        },
        "downloads": -1,
        "filename": "onnx_web-0.7.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0d72154452624dd8f52c1e12c9dde0d3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<3.11",
        "size": 36363,
        "upload_time": "2023-02-16T03:25:00",
        "upload_time_iso_8601": "2023-02-16T03:25:00.123225Z",
        "url": "https://files.pythonhosted.org/packages/df/c7/27fd0630e7adfdfc2bd55d65e575cf0700ed301148c19d689b80a8a79bf0/onnx_web-0.7.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "18141e34664c11734ec71fbfd5ee9e185ff4df199d35dd59b8906231f4591173",
          "md5": "c85b3a8093534ffa74bed8c1301e91bc",
          "sha256": "24a4881baa1d21673628ebd2998ec4fcc3798e04880ea0d71dfdea402819f10f"
        },
        "downloads": -1,
        "filename": "onnx-web-0.7.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c85b3a8093534ffa74bed8c1301e91bc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 34448,
        "upload_time": "2023-02-16T03:25:01",
        "upload_time_iso_8601": "2023-02-16T03:25:01.961763Z",
        "url": "https://files.pythonhosted.org/packages/18/14/1e34664c11734ec71fbfd5ee9e185ff4df199d35dd59b8906231f4591173/onnx-web-0.7.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.8.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "49bcf5c034dec2c4a3c7add252de15c92cbd8fa76a873de34435175cb97c5302",
          "md5": "04e8afb80657689a6ee3e47cce341586",
          "sha256": "f697a6c387f78e10d0572a2b56d0630c8fa539ad5bb3b69412a0d256f6be8eb0"
        },
        "downloads": -1,
        "filename": "onnx_web-0.8.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "04e8afb80657689a6ee3e47cce341586",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<3.11",
        "size": 52855,
        "upload_time": "2023-03-11T04:27:17",
        "upload_time_iso_8601": "2023-03-11T04:27:17.263339Z",
        "url": "https://files.pythonhosted.org/packages/49/bc/f5c034dec2c4a3c7add252de15c92cbd8fa76a873de34435175cb97c5302/onnx_web-0.8.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "96f98cf1dd17dfd4f18e0881260b8241c8cf6b0e58c81c2e005977c6daeddd56",
          "md5": "2bfa21b80273353274d3a2db6d7d45d2",
          "sha256": "1492af80c83b5656e3267f2ebc01026693960e3faa66c2ed5e6568b7390bcc66"
        },
        "downloads": -1,
        "filename": "onnx-web-0.8.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2bfa21b80273353274d3a2db6d7d45d2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 47153,
        "upload_time": "2023-03-11T04:27:18",
        "upload_time_iso_8601": "2023-03-11T04:27:18.871206Z",
        "url": "https://files.pythonhosted.org/packages/96/f9/8cf1dd17dfd4f18e0881260b8241c8cf6b0e58c81c2e005977c6daeddd56/onnx-web-0.8.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.8.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "342fdbaded37ecc4f225791709caaeefc7a3995d04ea70218d1c45d9eecd0ed2",
          "md5": "7ed08f692b4d777d8e72c29614a16ae7",
          "sha256": "dc347885eb3180223419d1617ba11af84253e1649e41c841d81edcae34dc4b14"
        },
        "downloads": -1,
        "filename": "onnx_web-0.8.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7ed08f692b4d777d8e72c29614a16ae7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<3.11",
        "size": 52877,
        "upload_time": "2023-03-11T20:51:11",
        "upload_time_iso_8601": "2023-03-11T20:51:11.338538Z",
        "url": "https://files.pythonhosted.org/packages/34/2f/dbaded37ecc4f225791709caaeefc7a3995d04ea70218d1c45d9eecd0ed2/onnx_web-0.8.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aca476614dd5438c0c48bb5db2289ee565dfec4d8a0c9157669bad8551cb8155",
          "md5": "4449c15c462514bc6d7aad3fd1081192",
          "sha256": "35b361d9cc043cf4d2b83e7748cc921b3787eff3beb2d18ab897f93ee615793b"
        },
        "downloads": -1,
        "filename": "onnx-web-0.8.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4449c15c462514bc6d7aad3fd1081192",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 47178,
        "upload_time": "2023-03-11T20:51:13",
        "upload_time_iso_8601": "2023-03-11T20:51:13.591748Z",
        "url": "https://files.pythonhosted.org/packages/ac/a4/76614dd5438c0c48bb5db2289ee565dfec4d8a0c9157669bad8551cb8155/onnx-web-0.8.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.9.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "395f31dc3352b2c0b8922afbe5d07096874a9a004fad57589a3f64c7a0550f8c",
          "md5": "ae9886957acc024321662b078eafe373",
          "sha256": "3ed2f3e7efb73158384b89140e0f5648bf6a97b52a3071ae99ccc6fa6cc408f2"
        },
        "downloads": -1,
        "filename": "onnx_web-0.9.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ae9886957acc024321662b078eafe373",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8,<3.11",
        "size": 57504,
        "upload_time": "2023-03-29T00:19:47",
        "upload_time_iso_8601": "2023-03-29T00:19:47.741264Z",
        "url": "https://files.pythonhosted.org/packages/39/5f/31dc3352b2c0b8922afbe5d07096874a9a004fad57589a3f64c7a0550f8c/onnx_web-0.9.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d518bc4c9c1dc96f3d2a977262352bbe8be1b3acc7374e6ecb876a62bc645106",
          "md5": "ea6ca4bcc60505ec1c21ce4617aa11d7",
          "sha256": "67d1a424d9e64b3720247690ef28b9bad6040e3c29d7e38cf63896ce4fa97758"
        },
        "downloads": -1,
        "filename": "onnx-web-0.9.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ea6ca4bcc60505ec1c21ce4617aa11d7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8,<3.11",
        "size": 51516,
        "upload_time": "2023-03-29T00:19:50",
        "upload_time_iso_8601": "2023-03-29T00:19:50.107990Z",
        "url": "https://files.pythonhosted.org/packages/d5/18/bc4c9c1dc96f3d2a977262352bbe8be1b3acc7374e6ecb876a62bc645106/onnx-web-0.9.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "395f31dc3352b2c0b8922afbe5d07096874a9a004fad57589a3f64c7a0550f8c",
        "md5": "ae9886957acc024321662b078eafe373",
        "sha256": "3ed2f3e7efb73158384b89140e0f5648bf6a97b52a3071ae99ccc6fa6cc408f2"
      },
      "downloads": -1,
      "filename": "onnx_web-0.9.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ae9886957acc024321662b078eafe373",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8,<3.11",
      "size": 57504,
      "upload_time": "2023-03-29T00:19:47",
      "upload_time_iso_8601": "2023-03-29T00:19:47.741264Z",
      "url": "https://files.pythonhosted.org/packages/39/5f/31dc3352b2c0b8922afbe5d07096874a9a004fad57589a3f64c7a0550f8c/onnx_web-0.9.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d518bc4c9c1dc96f3d2a977262352bbe8be1b3acc7374e6ecb876a62bc645106",
        "md5": "ea6ca4bcc60505ec1c21ce4617aa11d7",
        "sha256": "67d1a424d9e64b3720247690ef28b9bad6040e3c29d7e38cf63896ce4fa97758"
      },
      "downloads": -1,
      "filename": "onnx-web-0.9.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ea6ca4bcc60505ec1c21ce4617aa11d7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8,<3.11",
      "size": 51516,
      "upload_time": "2023-03-29T00:19:50",
      "upload_time_iso_8601": "2023-03-29T00:19:50.107990Z",
      "url": "https://files.pythonhosted.org/packages/d5/18/bc4c9c1dc96f3d2a977262352bbe8be1b3acc7374e6ecb876a62bc645106/onnx-web-0.9.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}