{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: GPU :: NVIDIA CUDA",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: BSD License",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: C++",
      "Programming Language :: Python",
      "Programming Language :: Python :: Implementation :: CPython",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries"
    ],
    "description": "# torch_tensorrt\r\n\r\n> Ahead of Time (AOT) compiling for PyTorch JIT\r\n\r\nTorch-TensorRT is a compiler for PyTorch/TorchScript, targeting NVIDIA GPUs via NVIDIA's TensorRT Deep Learning Optimizer and Runtime. Unlike PyTorch's Just-In-Time (JIT) compiler, Torch-TensorRT is an Ahead-of-Time (AOT) compiler, meaning that before you deploy your TorchScript code, you go through an explicit compile step to convert a standard TorchScript program into an module targeting a TensorRT engine. Torch-TensorRT operates as a PyTorch extention and compiles modules that integrate into the JIT runtime seamlessly. After compilation using the optimized graph should feel no different than running a TorchScript module. You also have access to TensorRT's suite of configurations at compile time, so you are able to specify operating precision (FP32/FP16/INT8) and other settings for your module.\r\n\r\n## Example Usage\r\n\r\n``` python\r\nimport torch_tensorrt\r\n\r\n...\r\n\r\ntrt_ts_module = torch_tensorrt.compile(torch_script_module,\r\n    inputs = [example_tensor, # Provide example tensor for input shape or...\r\n        torch_tensorrt.Input( # Specify input object with shape and dtype\r\n            min_shape=[1, 3, 224, 224],\r\n            opt_shape=[1, 3, 512, 512],\r\n            max_shape=[1, 3, 1024, 1024],\r\n            # For static size shape=[1, 3, 224, 224]\r\n            dtype=torch.half) # Datatype of input tensor. Allowed options torch.(float|half|int8|int32|bool)\r\n    ],\r\n    enabled_precisions = {torch.half}, # Run with FP16)\r\n\r\nresult = trt_ts_module(input_data) # run inference\r\ntorch.jit.save(trt_ts_module, \"trt_torchscript_module.ts\") # save the TRT embedded Torchscript\r\n\r\n```\r\n\r\n## Installation\r\n\r\n| ABI / Platform                          | Installation command                                         |\r\n| --------------------------------------- | ------------------------------------------------------------ |\r\n| Pre CXX11 ABI (Linux x86_64)            | python3 setup.py install                                     |\r\n| CXX ABI  (Linux x86_64)                 | python3 setup.py install --use-cxx11-abi                     |\r\n| Pre CXX11 ABI (Jetson platform aarch64) | python3 setup.py install --jetpack-version 4.6               |\r\n| CXX11 ABI (Jetson platform aarch64)     | python3 setup.py install --jetpack-version 4.6 --use-cxx11-abi |\r\n\r\nFor Linux x86_64 platform, Pytorch libraries default to pre cxx11 abi. So, please use `python3 setup.py install`.\r\n\r\nOn Jetson platforms, NVIDIA hosts <a href=\"https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048\">pre-built Pytorch wheel files</a>. These wheel files are built with CXX11 ABI. So on jetson platforms, please use `python3 setup.py install --jetpack-version 4.6 --use-cxx11-abi`\r\n\r\n## Under the Hood\r\n\r\nWhen a traced module is provided to Torch-TensorRT, the compiler takes the internal representation and transforms it into one like this:\r\n\r\n```\r\ngraph(%input.2 : Tensor):\r\n    %2 : Float(84, 10) = prim::Constant[value=<Tensor>]()\r\n    %3 : Float(120, 84) = prim::Constant[value=<Tensor>]()\r\n    %4 : Float(576, 120) = prim::Constant[value=<Tensor>]()\r\n    %5 : int = prim::Constant[value=-1]() # x.py:25:0\r\n    %6 : int[] = prim::Constant[value=annotate(List[int], [])]()\r\n    %7 : int[] = prim::Constant[value=[2, 2]]()\r\n    %8 : int[] = prim::Constant[value=[0, 0]]()\r\n    %9 : int[] = prim::Constant[value=[1, 1]]()\r\n    %10 : bool = prim::Constant[value=1]() # ~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py:346:0\r\n    %11 : int = prim::Constant[value=1]() # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\r\n    %12 : bool = prim::Constant[value=0]() # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\r\n    %self.classifer.fc3.bias : Float(10) = prim::Constant[value= 0.0464  0.0383  0.0678  0.0932  0.1045 -0.0805 -0.0435 -0.0818  0.0208 -0.0358 [ CUDAFloatType{10} ]]()\r\n    %self.classifer.fc2.bias : Float(84) = prim::Constant[value=<Tensor>]()\r\n    %self.classifer.fc1.bias : Float(120) = prim::Constant[value=<Tensor>]()\r\n    %self.feat.conv2.weight : Float(16, 6, 3, 3) = prim::Constant[value=<Tensor>]()\r\n    %self.feat.conv2.bias : Float(16) = prim::Constant[value=<Tensor>]()\r\n    %self.feat.conv1.weight : Float(6, 1, 3, 3) = prim::Constant[value=<Tensor>]()\r\n    %self.feat.conv1.bias : Float(6) = prim::Constant[value= 0.0530 -0.1691  0.2802  0.1502  0.1056 -0.1549 [ CUDAFloatType{6} ]]()\r\n    %input0.4 : Tensor = aten::_convolution(%input.2, %self.feat.conv1.weight, %self.feat.conv1.bias, %9, %8, %9, %12, %8, %11, %12, %12, %10) # ~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py:346:0\r\n    %input0.5 : Tensor = aten::relu(%input0.4) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\r\n    %input1.2 : Tensor = aten::max_pool2d(%input0.5, %7, %6, %8, %9, %12) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\r\n    %input0.6 : Tensor = aten::_convolution(%input1.2, %self.feat.conv2.weight, %self.feat.conv2.bias, %9, %8, %9, %12, %8, %11, %12, %12, %10) # ~/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py:346:0\r\n    %input2.1 : Tensor = aten::relu(%input0.6) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\r\n    %x.1 : Tensor = aten::max_pool2d(%input2.1, %7, %6, %8, %9, %12) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:539:0\r\n    %input.1 : Tensor = aten::flatten(%x.1, %11, %5) # x.py:25:0\r\n    %27 : Tensor = aten::matmul(%input.1, %4)\r\n    %28 : Tensor = trt::const(%self.classifer.fc1.bias)\r\n    %29 : Tensor = aten::add_(%28, %27, %11)\r\n    %input0.2 : Tensor = aten::relu(%29) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\r\n    %31 : Tensor = aten::matmul(%input0.2, %3)\r\n    %32 : Tensor = trt::const(%self.classifer.fc2.bias)\r\n    %33 : Tensor = aten::add_(%32, %31, %11)\r\n    %input1.1 : Tensor = aten::relu(%33) # ~/.local/lib/python3.6/site-packages/torch/nn/functional.py:1063:0\r\n    %35 : Tensor = aten::matmul(%input1.1, %2)\r\n    %36 : Tensor = trt::const(%self.classifer.fc3.bias)\r\n    %37 : Tensor = aten::add_(%36, %35, %11)\r\n    return (%37)\r\n(CompileGraph)\r\n```\r\n\r\nThe graph has now been transformed from a collection of modules much like how your PyTorch Modules are collections of modules, each managing their own parameters into a single graph\r\nwith the parameters inlined into the graph and all of the operations laid out. Torch-TensorRT has also executed a number of optimizations and mappings to make the graph easier to translate\r\nto TensorRT. From here the compiler can assemble the TensorRT engine by following the dataflow through the graph.\r\n\r\nWhen the graph construction phase is complete, Torch-TensorRT produces a serialized TensorRT engine. From here depending on the API, this engine is returned to the user or moves into the graph\r\nconstruction phase. Here Torch-TensorRT creates a JIT Module to execute the TensorRT engine which will be instantiated and managed by the Torch-TensorRT runtime.\r\n\r\nHere is the graph that you get back after compilation is complete:\r\n\r\n```\r\ngraph(%self.1 : __torch__.___torch_mangle_10.LeNet_trt,\r\n    %2 : Tensor):\r\n    %1 : int = prim::Constant[value=94106001690080]()\r\n    %3 : Tensor = trt::execute_engine(%1, %2)\r\n    return (%3)\r\n(AddEngineToGraph)\r\n```\r\n\r\nYou can see the call where the engine is executed, based on a constant which is the ID of the engine, telling JIT how to find the engine and the input tensor which will be fed to TensorRT.\r\nThe engine represents the exact same calculations as what is done by running a normal PyTorch module but optimized to run on your GPU.\r\n\r\nTorch-TensorRT converts from TorchScript by generating layers or subgraphs in correspondance with instructions seen in the graph. Converters are small modules of code used to map one specific\r\noperation to a layer or subgraph in TensorRT. Not all operations are support, but if you need to implement one, you can in C++.\r\n\r\n## Registering Custom Converters\r\n\r\nOperations are mapped to TensorRT through the use of modular converters, a function that takes a node from a the JIT graph and produces an equivalent layer or subgraph in TensorRT. Torch-TensorRT\r\nships with a library of these converters stored in a registry, that will be executed depending on the node being parsed. For instance a `aten::relu(%input0.4)` instruction will trigger the\r\nrelu converter to be run on it, producing an activation layer in the TensorRT graph. But since this library is not exhaustive you may need to write your own to get Torch-TensorRT to support your module.\r\n\r\nShipped with the Torch-TensorRT distribution are the internal core API headers. You can therefore access the converter registry and add a converter for the op you need.\r\n\r\nFor example, if we try to compile a graph with a build of Torch-TensorRT that doesnâ€™t support the flatten operation (`aten::flatten`) you may see this error:\r\n\r\n```\r\nterminate called after throwing an instance of 'torch_tensorrt::Error'\r\nwhat():  [enforce fail at core/conversion/conversion.cpp:109] Expected converter to be true but got false\r\nUnable to convert node: %input.1 : Tensor = aten::flatten(%x.1, %11, %5) # x.py:25:0 (conversion.AddLayer)\r\nSchema: aten::flatten.using_ints(Tensor self, int start_dim=0, int end_dim=-1) -> (Tensor)\r\nConverter for aten::flatten requested, but no such converter was found.\r\nIf you need a converter for this operator, you can try implementing one yourself\r\nor request a converter: https://www.github.com/NVIDIA/Torch-TensorRT/issues\r\n```\r\n\r\nWe can register a converter for this operator in our application. All of the tools required to build a converter can be imported by including `Torch-TensorRT/core/conversion/converters/converters.h`.\r\nWe start by creating an instance of the self-registering `class torch_tensorrt::core::conversion::converters::RegisterNodeConversionPatterns()` which will register converters in the global converter\r\nregistry, associating a function schema like `aten::flatten.using_ints(Tensor self, int start_dim=0, int end_dim=-1) -> (Tensor)` with a lambda that will take the state of the conversion, the\r\nnode/operation in question to convert and all of the inputs to the node and produces as a side effect a new layer in the TensorRT network. Arguments are passed as a vector of inspectable unions\r\nof TensorRT ITensors and Torch IValues in the order arguments are listed in the schema.\r\n\r\nBelow is a implementation of a `aten::flatten` converter that we can use in our application. You have full access to the Torch and TensorRT libraries in the converter implementation. So for example\r\nwe can quickly get the output size by just running the operation in PyTorch instead of implementing the full calculation outself like we do below for this flatten converter.\r\n\r\n```c++\r\n#include \"torch/script.h\"\r\n#include \"torch_tensorrt/torch_tensorrt.h\"\r\n#include \"torch_tensorrt/core/conversion/converters/converters.h\"\r\n\r\nstatic auto flatten_converter = torch_tensorrt::core::conversion::converters::RegisterNodeConversionPatterns()\r\n    .pattern({\r\n        \"aten::flatten.using_ints(Tensor self, int start_dim=0, int end_dim=-1) -> (Tensor)\",\r\n        [](torch_tensorrt::core::conversion::ConversionCtx* ctx,\r\n           const torch::jit::Node* n,\r\n           torch_tensorrt::core::conversion::converters::args& args) -> bool {\r\n            auto in = args[0].ITensor();\r\n            auto start_dim = args[1].unwrapToInt();\r\n            auto end_dim = args[2].unwrapToInt();\r\n            auto in_shape = torch_tensorrt::core::util::toVec(in->getDimensions());\r\n            auto out_shape = torch::flatten(torch::rand(in_shape), start_dim, end_dim).sizes();\r\n\r\n            auto shuffle = ctx->net->addShuffle(*in);\r\n            shuffle->setReshapeDimensions(torch_tensorrt::core::util::toDims(out_shape));\r\n            shuffle->setName(torch_tensorrt::core::util::node_info(n).c_str());\r\n\r\n            auto out_tensor = ctx->AssociateValueAndTensor(n->outputs()[0], shuffle->getOutput(0));\r\n            return true;\r\n        }\r\n    });\r\n```\r\n\r\nTo use this converter in Python, it is recommended to use PyTorchâ€™s [C++ / CUDA Extention](https://pytorch.org/tutorials/advanced/cpp_extension.html#custom-c-and-cuda-extensions) template to wrap\r\nyour library of converters into a `.so` that you can load with `ctypes.CDLL()` in your Python application.\r\n\r\nYou can find more information on all the details of writing converters in the contributors documentation ([Writing Converters](https://nvidia.github.io/Torch-TensorRT/contributors/writing_converters.html#writing-converters)). If you\r\nfind yourself with a large library of converter implementations, do consider upstreaming them, PRs are welcome and it would be great for the community to benefit as well.\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://nvidia.github.io/torch-tensorrt",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torch-tensorrt-fx-only",
    "package_url": "https://pypi.org/project/torch-tensorrt-fx-only/",
    "platform": null,
    "project_url": "https://pypi.org/project/torch-tensorrt-fx-only/",
    "project_urls": {
      "Homepage": "https://nvidia.github.io/torch-tensorrt"
    },
    "release_url": "https://pypi.org/project/torch-tensorrt-fx-only/1.3.0/",
    "requires_dist": [
      "torch (<1.14.0,>=1.13.0)",
      "tensorrt (<8.6.0,>=8.5.1.7)"
    ],
    "requires_python": ">=3.7",
    "summary": "Unofficial Torch-TensorRT Python package built with FX path only",
    "version": "1.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15955296,
  "releases": {
    "1.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "444f5bf9083abfe1c6de0ca7910bb7863fad6a5c230628d171f0483ea76c7124",
          "md5": "78f5a5d27fb3df9feb9be9c599a51a1d",
          "sha256": "3226eb331265c860a32f62e4b6dbcf0fa820d6a150f8bdd519c9cacad8e7c0b3"
        },
        "downloads": -1,
        "filename": "torch_tensorrt_fx_only-1.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "78f5a5d27fb3df9feb9be9c599a51a1d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 117738,
        "upload_time": "2022-11-06T03:50:27",
        "upload_time_iso_8601": "2022-11-06T03:50:27.452239Z",
        "url": "https://files.pythonhosted.org/packages/44/4f/5bf9083abfe1c6de0ca7910bb7863fad6a5c230628d171f0483ea76c7124/torch_tensorrt_fx_only-1.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c2b4f2429c63b92cdbdd9145ffd82162a686af9c710c2df7bedab29c6ee2b2c9",
          "md5": "a1c642072c4a6f12546774aaad3ae1c7",
          "sha256": "d3cc177cd39f1fe0bbedcccd334d5282f6a3301330841aa19e3d9ebbadb202ce"
        },
        "downloads": -1,
        "filename": "torch_tensorrt_fx_only-1.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a1c642072c4a6f12546774aaad3ae1c7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 128414,
        "upload_time": "2022-12-01T13:36:14",
        "upload_time_iso_8601": "2022-12-01T13:36:14.715850Z",
        "url": "https://files.pythonhosted.org/packages/c2/b4/f2429c63b92cdbdd9145ffd82162a686af9c710c2df7bedab29c6ee2b2c9/torch_tensorrt_fx_only-1.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c2b4f2429c63b92cdbdd9145ffd82162a686af9c710c2df7bedab29c6ee2b2c9",
        "md5": "a1c642072c4a6f12546774aaad3ae1c7",
        "sha256": "d3cc177cd39f1fe0bbedcccd334d5282f6a3301330841aa19e3d9ebbadb202ce"
      },
      "downloads": -1,
      "filename": "torch_tensorrt_fx_only-1.3.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a1c642072c4a6f12546774aaad3ae1c7",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 128414,
      "upload_time": "2022-12-01T13:36:14",
      "upload_time_iso_8601": "2022-12-01T13:36:14.715850Z",
      "url": "https://files.pythonhosted.org/packages/c2/b4/f2429c63b92cdbdd9145ffd82162a686af9c710c2df7bedab29c6ee2b2c9/torch_tensorrt_fx_only-1.3.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}