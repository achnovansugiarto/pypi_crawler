{
  "info": {
    "author": "",
    "author_email": "allrank@allegro.pl",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "Modified version of https://github.com/allegro/allRank\n\n# allRank : Learning to Rank in PyTorch\n\n## About\n\nallRank is a PyTorch-based framework for training neural Learning-to-Rank (LTR) models, featuring implementations of:\n* common pointwise, pairwise and listwise loss functions\n* fully connected and Transformer-like scoring functions\n* commonly used evaluation metrics like Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR)\n* click-models for experiments on simulated click-through data\n\n### Motivation\n\nallRank provides an easy and flexible way to experiment with various LTR neural network models and loss functions.\nIt is easy to add a custom loss, and to configure the model and the training procedure. \nWe hope that allRank will facilitate both research in neural LTR and its industrial applications.\n\n## Features\n\n### Implemented loss functions:\n 1. ListNet (For a binary and graded relevance)\n 2. ListMLE\n 3. RankNet\n 4. Ordinal loss\n 5. LambdaRank\n 6. LambdaLoss\n 7. ApproxNDCG\n 8. RMSE\n\n### Getting started guide\n\nTo help you get started, we provide a ```run_example.sh``` script which generates dummy ranking data in libsvm format and trains\n a Transformer model on the data using provided example ```config.json``` config file. Once you run the script, the dummy data can be found in `dummy_data` directory\n and the results of the experiment in `test_run` directory. To run the example, Docker is required.\n\n### Configuring your model & training\n\nTo train your own model, configure your experiment in ```config.json``` file and run  \n\n```python allrank/main.py --config_file_name allrank/config.json --run_id <the_name_of_your_experiment> --job_dir <the_place_to_save_results>```\n\nAll the hyperparameters of the training procedure: i.e. model defintion, data location, loss and metrics used, training hyperparametrs etc. are controlled\nby the ```config.json``` file. We provide a template file ```config_template.json``` where supported attributes, their meaning and possible values are explained.\n Note that following MSLR-WEB30K convention, your libsvm file with training data should be named `train.txt`. You can specify the name of the validation dataset \n (eg. valid or test) in the config. Results will be saved under the path ```<job_dir>/results/<run_id>```\n\nGoogle Cloud Storage is supported in allRank as a place for data and job results.\n\n\n### Implementing custom loss functions\n\nTo experiment with your own custom loss, you need to implement a function that takes two tensors (model prediction and ground truth) as input\n and put it in the `losses` package, making sure it is exposed on a package level.\nTo use it in training, simply pass the name (and args, if your loss method has some hyperparameters) of your function in the correct place in the config file:\n\n```\n\"loss\": {\n    \"name\": \"yourLoss\",\n    \"args\": {\n        \"arg1\": val1,\n        \"arg2: val2\n    }\n  }\n```\n\n### Applying click-model\n\nTo apply a click model you need to first have an allRank model trained.\nNext, run:\n\n```python allrank/rank_and_click.py --input-model-path <path_to_the_model_weights_file> --roles <comma_separated_list_of_ds_roles_to_process e.g. train,valid> --config_file_name allrank/config.json --run_id <the_name_of_your_experiment> --job_dir <the_place_to_save_results>``` \n\nThe model will be used to rank all slates from the dataset specified in config. Next - a click model configured in config will be applied and the resulting click-through dataset will be written under ```<job_dir>/results/<run_id>``` in a libSVM format.\nThe path to the results directory may then be used as an input for another allRank model training.\n\n## Continuous integration\n\nYou should run `scripts/ci.sh` to verify that code passes style guidelines and unit tests.\n\n## Research\n\nThis framework was developed to support the research project [Context-Aware Learning to Rank with Self-Attention](https://arxiv.org/abs/2005.10084). If you use allRank in your research, please cite:\n```\n@article{Pobrotyn2020ContextAwareLT,\n  title={Context-Aware Learning to Rank with Self-Attention},\n  author={Przemyslaw Pobrotyn and Tomasz Bartczak and Mikolaj Synowiec and Radoslaw Bialobrzeski and Jaroslaw Bojar},\n  journal={ArXiv},\n  year={2020},\n  volume={abs/2005.10084}\n}\n```\n\n## License\n\nApache 2 License",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/allegro/allRank",
    "keywords": "",
    "license": "Apache 2",
    "maintainer": "",
    "maintainer_email": "",
    "name": "allrank-mod",
    "package_url": "https://pypi.org/project/allrank-mod/",
    "platform": "",
    "project_url": "https://pypi.org/project/allrank-mod/",
    "project_urls": {
      "Homepage": "https://github.com/allegro/allRank"
    },
    "release_url": "https://pypi.org/project/allrank-mod/1.3.0a0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "allRank is a framework for training learning-to-rank neural models",
    "version": "1.3.0a0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7508531,
  "releases": {
    "1.3.0a0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e7e9245a4c2b91d5e4dc22f16e97cc01bf922d41f83bae5d45bfe1f4956b1f5f",
          "md5": "5ec5d8764904b2a564118035c7dff606",
          "sha256": "d6b669bccc7402b851e4a42bb22d972f8819bf5c01d9f4d2288707fe7ad46033"
        },
        "downloads": -1,
        "filename": "allrank_mod-1.3.0a0.tar.gz",
        "has_sig": false,
        "md5_digest": "5ec5d8764904b2a564118035c7dff606",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 34100,
        "upload_time": "2020-06-18T19:26:35",
        "upload_time_iso_8601": "2020-06-18T19:26:35.053520Z",
        "url": "https://files.pythonhosted.org/packages/e7/e9/245a4c2b91d5e4dc22f16e97cc01bf922d41f83bae5d45bfe1f4956b1f5f/allrank_mod-1.3.0a0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e7e9245a4c2b91d5e4dc22f16e97cc01bf922d41f83bae5d45bfe1f4956b1f5f",
        "md5": "5ec5d8764904b2a564118035c7dff606",
        "sha256": "d6b669bccc7402b851e4a42bb22d972f8819bf5c01d9f4d2288707fe7ad46033"
      },
      "downloads": -1,
      "filename": "allrank_mod-1.3.0a0.tar.gz",
      "has_sig": false,
      "md5_digest": "5ec5d8764904b2a564118035c7dff606",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 34100,
      "upload_time": "2020-06-18T19:26:35",
      "upload_time_iso_8601": "2020-06-18T19:26:35.053520Z",
      "url": "https://files.pythonhosted.org/packages/e7/e9/245a4c2b91d5e4dc22f16e97cc01bf922d41f83bae5d45bfe1f4956b1f5f/allrank_mod-1.3.0a0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}