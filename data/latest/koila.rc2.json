{
  "info": {
    "author": "RenChu Wang",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# 🐨 Koila\n\n> Koila solves `CUDA error: out of memory error` painlessly.\n> Fix it with just one line of code, and forget it.\n\n![Type Checking](https://github.com/rentruewang/koila/actions/workflows/typecheck.yaml/badge.svg)\n![Formatting](https://github.com/rentruewang/koila/actions/workflows/format.yaml/badge.svg)\n![Unit testing](https://github.com/rentruewang/koila/actions/workflows/unittest.yaml/badge.svg)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![Tweet](https://img.shields.io/twitter/url/http/shields.io.svg?style=social)](https://twitter.com/intent/tweet?text=Never%20worry%20about%20out%20of%20memory%20errors%20again&url=https://github.com/rentruewang/koila&hashtags=pytorch,outofmemory)\n\n![Koila](./assets/koila.png)\n\n## 🚀 Features\n\n- 🙅 Prevents `CUDA error: out of memory error` with one single line of code.\n\n- 🦥 Lazily evaluates pytorch code to save computing power.\n\n- ✂️ Automatically splits along the batch dimension to more GPU friendly numbers (2's powers) to speed up the execution.\n\n- 🤏 Minimal API (wrapping all inputs will be enough).\n\n## 🤔 Why Koila?\n\nEver encountered `RuntimeError: CUDA error: out of memory`?\nWe all love `PyTorch` because of its speed, efficiency, and transparency, but that means it doesn't do extra things. Things like preventing a very common error that has been bothering many users since [2017](https://github.com/pytorch/pytorch/issues/958#issuecomment-285090162).\n\nThis library aims to prevent that by being a light-weight wrapper over native `PyTorch`. When a tensor is wrapped, the library **automatically computes the amount of remaining GPU memory and uses the right batch size**, saving everyone from having to manually fine-tune the batch size whenever a model is used.\n\nAlso, the library automatically uses the right batch size to GPU. Did you know that using bigger batches doesn't always speed up processing? It's handled automatically in this library too.\n\nBecause `Koila` code is `PyTorch` code, as it runs `PyTorch` under the hood, you can use both together without worrying compatibility.\n\nOh, and all that in 1 line of code! 😊\n\n## ⬇️ Installation\n\n`Koila` is available on [PyPI](https://pypi.org/project/koila/). To install, run the following command.\n\n```bash\npip install koila\n```\n\n## 🏃 Getting started\n\nThe usage is dead simple. For example, you have the following `PyTorch` code (copied from `PyTorch`'s [tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html))\n\nDefine the input, label, and model:\n\n```python\n# A batch of MNIST image\ninput = torch.randn(8, 28, 28)\n\n# A batch of labels\nlabel = torch.randn(0, 10, [8])\n\nclass NeuralNetwork(Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.flatten = Flatten()\n        self.linear_relu_stack = Sequential(\n            Linear(28 * 28, 512),\n            ReLU(),\n            Linear(512, 512),\n            ReLU(),\n            Linear(512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n```\n\nDefine the loss function, calculate output and losses.\n\n```python\nloss_fn = CrossEntropyLoss()\n\n# Calculate losses\nout = nn(t)\nloss = loss_fn(out, label)\n\n# Backward pass\nnn.zero_grad()\nloss.backward()\n```\n\nOk. How to adapt the code to use `Koila`'s features?\n\nYou add this line of code:\n\n```python\n# Wrap the input tensor and label tensor.\n# If a batch argument is provided, that dimension of the tensor would be treated as the batch.\n# In this case, the first dimension (dim=0) is used as batch's dimension.\n(input, label) = lazy(input, label, batch=0)\n```\n\nDone. You will not run out of memory again.\n\nSee `examples/getting-started.py` for the full example.\n\n## 🏋️ How does it work under the hood?\n\n`CUDA error: out of memory` generally happens in forward pass, because temporary variables will need to be saved in memory.\n\n`Koila` is a thin wrapper around `PyTorch`. It is inspired by TensorFlow's static/lazy evaluation. By building the graph first, and run the model only when necessarily, the model has access to all the information necessarily to determine how much resources is really need to compute the model.\n\nIn terms of memory usage, only **shapes of temporary variables are required to calculate the memory usage of those variables used in the model**. For example, `+` takes in two tensors with equal sizes, and outputs a tensor with a size equal to the input size, and `log` takes in one tensor, and outputs another tensor with the same shape. Broadcasting makes it a little more complicated than that, but the general ideas are the same. By tracking all these shapes, one could easily tell how much memory is used in a forward pass. And select the optimal batch size accordingly.\n\n## 🐌 It sounds slow. Is it?\n\n**NO**. Indeed, calculating shapes and computing the size and memory usage sound like a lot of work. However, keep in mind that even a gigantic model like GPT-3, which has 96 layers, has only a few hundred nodes in its computing graph. Because `Koila`'s algorithms run in linear time, any modern computer will be able to handle a graph like this instantly.\n\nMost of the computing is spent on computing individual tensors, and transferring tensors across devices. And bear in mind that those checks happen in vanilla `PyTorch` anyways. So no, not slow at all.\n\n## 🔊 How to pronounce koila?\n\nThis project was originally named _koala_, the laziest species in the world, and this project is about lazy evaluation of tensors. However, as that name is taken on [PyPI](https://pypi.org/project/koala/), I had no choice but to use another name. `Koila` is a word made up by me, pronounced similarly to _voila_ (It's a French word), so sounds like koala.\n\n## ⭐ Give me a star!\n\nIf you like what you see, please consider giving this a star (★)!\n\n## 🏗️ Why did I build this?\n\nBatch size search is not new. In fact, the mighty popular [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) has it. So why did I go through the trouble and build this project?\n\nPyTorch Lightning's batch size search is deeply integrated in its own ecosystem. You have to use its `DataLoader`, subclass from their models, and train your models accordingly. While it works well with supervised learning tasks, it's really painful to use in a reinforcement learning task, where interacting with the environment is a must.\n\nIn comparison, because `Koila` is a super lightweight PyTorch wrapper, it works when PyTorch works, thus providing maximum flexibility and minimal changes to existing code.\n\n## 📝 Todos\n\n- 😌 Simplify internal workings even further. (Especially interaction between `Tensor`s and `LazyTensor`s).\n- 🧩 Provide an extensible API to write custom functions for the users.\n- 🍪 Work with multiple GPUs.\n\n## 🚧 Warning\n\nThe code works on many cases, but it's still a work in progress. This is not (yet) a fully `PyTorch` compatible library due to limited time. Avoid using it in production environments!\n\n## 🥰 Contributing\n\nWe take openness and inclusiveness very seriously. We have adopted the following [Code of Conduct](./CODE_OF_CONDUCT.md).\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "koila",
    "package_url": "https://pypi.org/project/koila/",
    "platform": "",
    "project_url": "https://pypi.org/project/koila/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/koila/0.1.1/",
    "requires_dist": [
      "numpy",
      "pynvml",
      "rich",
      "torch"
    ],
    "requires_python": ">=3.8",
    "summary": "Prevent PyTorch's `CUDA error: out of memory` in one line of code.",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12319864,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d50601f19a4b21e2ed09268f1a34886b59c5e05dfc9b160a1241eb9192c87ff7",
          "md5": "793bfb97de2f012e733996fe1dd11c6f",
          "sha256": "e5fed7392bb5a3bd607d8164b0858bd3c406444a756613d1443e47bd39d3cc46"
        },
        "downloads": -1,
        "filename": "koila-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "793bfb97de2f012e733996fe1dd11c6f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 16815,
        "upload_time": "2021-11-29T14:01:39",
        "upload_time_iso_8601": "2021-11-29T14:01:39.297877Z",
        "url": "https://files.pythonhosted.org/packages/d5/06/01f19a4b21e2ed09268f1a34886b59c5e05dfc9b160a1241eb9192c87ff7/koila-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4f6bc6f3bef2d23a606899671b2d7b9a795c2eb37c59c59fb4376aab96a693be",
          "md5": "31e53fca61c310d3632aaf1db929856c",
          "sha256": "3d7ccb40cddb4ede3c8676f281ea4d08184b2fe4481feadca48a14fcfddc2672"
        },
        "downloads": -1,
        "filename": "koila-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "31e53fca61c310d3632aaf1db929856c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 18495,
        "upload_time": "2021-11-29T14:01:41",
        "upload_time_iso_8601": "2021-11-29T14:01:41.120549Z",
        "url": "https://files.pythonhosted.org/packages/4f/6b/c6f3bef2d23a606899671b2d7b9a795c2eb37c59c59fb4376aab96a693be/koila-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0abc71f019282a148c54d4ce3cf3e7e64f59fd455d13682c5aab1645ca276704",
          "md5": "425eeb5c2ea7dc3cade9c6fd79a185f9",
          "sha256": "45c8430c9dae16e1cc17822ad91466ec3ca823aa4f077db9e8cd4f2adcaf475b"
        },
        "downloads": -1,
        "filename": "koila-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "425eeb5c2ea7dc3cade9c6fd79a185f9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 18985,
        "upload_time": "2021-12-16T05:19:45",
        "upload_time_iso_8601": "2021-12-16T05:19:45.330086Z",
        "url": "https://files.pythonhosted.org/packages/0a/bc/71f019282a148c54d4ce3cf3e7e64f59fd455d13682c5aab1645ca276704/koila-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b5f484d59e2461f81ae74c92d5b890d3759e36992a2a7859da583e96bfedc2af",
          "md5": "0505d6ac25b4388e0ec18c08f25ed38a",
          "sha256": "2abfedb982200a45bbc17bc23ba840c6e76537471d722ef243339f50d6c224ae"
        },
        "downloads": -1,
        "filename": "koila-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0505d6ac25b4388e0ec18c08f25ed38a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 20156,
        "upload_time": "2021-12-16T05:19:47",
        "upload_time_iso_8601": "2021-12-16T05:19:47.517339Z",
        "url": "https://files.pythonhosted.org/packages/b5/f4/84d59e2461f81ae74c92d5b890d3759e36992a2a7859da583e96bfedc2af/koila-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0abc71f019282a148c54d4ce3cf3e7e64f59fd455d13682c5aab1645ca276704",
        "md5": "425eeb5c2ea7dc3cade9c6fd79a185f9",
        "sha256": "45c8430c9dae16e1cc17822ad91466ec3ca823aa4f077db9e8cd4f2adcaf475b"
      },
      "downloads": -1,
      "filename": "koila-0.1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "425eeb5c2ea7dc3cade9c6fd79a185f9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 18985,
      "upload_time": "2021-12-16T05:19:45",
      "upload_time_iso_8601": "2021-12-16T05:19:45.330086Z",
      "url": "https://files.pythonhosted.org/packages/0a/bc/71f019282a148c54d4ce3cf3e7e64f59fd455d13682c5aab1645ca276704/koila-0.1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b5f484d59e2461f81ae74c92d5b890d3759e36992a2a7859da583e96bfedc2af",
        "md5": "0505d6ac25b4388e0ec18c08f25ed38a",
        "sha256": "2abfedb982200a45bbc17bc23ba840c6e76537471d722ef243339f50d6c224ae"
      },
      "downloads": -1,
      "filename": "koila-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "0505d6ac25b4388e0ec18c08f25ed38a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 20156,
      "upload_time": "2021-12-16T05:19:47",
      "upload_time_iso_8601": "2021-12-16T05:19:47.517339Z",
      "url": "https://files.pythonhosted.org/packages/b5/f4/84d59e2461f81ae74c92d5b890d3759e36992a2a7859da583e96bfedc2af/koila-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}