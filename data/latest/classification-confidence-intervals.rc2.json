{
  "info": {
    "author": "Kenneth S. Chen",
    "author_email": "kennychen12@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "[![Build Status](https://travis-ci.com/kennethschen/Classification-Confidence-Intervals.svg?branch=master)](https://travis-ci.com/kennethschen/Classification-Confidence-Intervals)\n[![codecov](https://codecov.io/gh/kennethschen/Classification-Confidence-Intervals/branch/master/graph/badge.svg)](https://codecov.io/gh/kennethschen/Classification-Confidence-Intervals)\n\n# Classification Confidence Intervals\n\nA package to calculate confidence intervals for classification positive rate, precision, NPV, and recall using a labeled sample of the population via exact & approximate Frequentist & Bayesian setups.\n\n## Summary\n\n### Description\n\nPrecision and recall are important metrics for evaluating classification model performance;\nhowever, they are only calculable from labeled data. In situations with unlabeled data, especially\nwith large datasets or datastreams where labeling and remediating flagged points is incredibly\nexpensive, exact precision and recall are not obtainable. There are solutions to calculating\npopulation precision and recall confidence intervals in such cases using a labeled sample of the\nentire dataset, but much of this information is scattered. This package combines Frequentist and\nBayesian statistical approaches to produce various confidence intervals and present a holistic\npicture into model performance and dataset quality.\n\n### Metrics\n\nThe classification metrics for which confidence intervals are constructed are defined here.\nThese metrics are also our parameters of interest in the models.\n\nLet TP, FP, TN, FN mean true positives, false positives, true negatives, and false negatives,\nrespectively.\n\n* **Positive Rate:** (TP+FN) / (TP+FN+FP+TN)\n* **Precision (PPV):** (TP) / (TP+FP)\n* **Negative Predictive Value (NPV):** (TN) / (TN+FN)\n* **Recall** (TP) / (TP+FN)\n\nIf helpful, see the precision recall image [here](https://github.com/kennethschen/Classification-Confidence-Intervals/blob/master/images/precision_recall_image.png).\n\n## Usage\n\nThe relevant class is [**ClassificationConfidenceIntervals**](https://github.com/kennethschen/Classification-Confidence-Intervals/blob/master/src/classification_confidence_intervals.py).\n\n### Installation\n\n```\n$ pip install classification-confidence-intervals\n```\n\n[PyPI](https://pypi.org/project/classification-confidence-intervals/)\n[GitHub](https://github.com/kennethschen/Classification-Confidence-Intervals)\n\n\n### Instantiation\n\nDuring instantiation, all distributions are fitted. By default, **exact_precision**=None.\n\nArgs:\n* **sample_labels (list):** Binary labels of datapoints in sample, with labels as boolean or\n    binary in [0,1] or in [-1,1].\n* **sample_predictions (list):** Binary labels of datapoints in sample flagged as positives\n    by algorithm, with labels as boolean or binary in [0,1] or in [-1,1].\n* **population_size (int):** Size of population.\n* **population_flagged_count, (int):** Number of datapoints in population flagged as positives\n    by algorithm.\n* **confidence_level (float):** Confidence level, equal to area desired under curve.\n* **exact_precision (float):** If provided, the actual population precision.\n\n```python\n>>> from classificationconfidenceintervals import ClassificationConfidenceIntervals\n>>> confidenceintervals = ClassificationConfidenceIntervals(\n...     sample_labels=[1, 1, 1, 0, 0, 0, 0, 0] * 30,\n...     sample_predictions=[1, 1, 0, 0, 1, 1, 0, 0] * 30,\n...     population_size=4000,\n...     population_flagged_count=2000,\n...     confidence_level=0.95,\n... )\n```\n\nWith regards to your labled sample, in the example above using fake labels and predictions,\n**sample_labels** refer to the truth values of each datapoint (the 1st to 3rd datapoint are\npositives; the 4th to 8th datapoints are negatives). **sample_predictions** refer to whether or not your\nalgorithm flagged each datapoint (the 1st, 2nd, 5th, and 6th datapoints were flagged; the 3rd, 4th, 7th, and 8th\ndatapoints were not flagged). The arrays are artificially duplicated by a factor of 30 in the example. Do not do this in practice.\n\nWith regards to your population, the only information required are **population_size** and\nhow many points your algorithm flags in the population as **population_flagged_count**. In the\nexample above, and the algorithm flagged 2000 datapoints among the 4000 datapoints in the population.\n\nDegenerate samples cause the models to become degenerate, and instantiation will throw errors if\nyour sample does not include at least one of each of TP, FP, TN, FN.\n\n**confidence_level** by convention should generally be set at .95. The higher the value,\nthe wider the confidence intervals.\n\nIn cases where all flagged datapoints in the population have been remediated (i.e. are labeled),\nyou may provide the **exact_precision** value during instantiation; otherwise, confidence intervals\nwill also be created for precision. A case where not all flagged datapoints in the population\nare remediated is when the total number of flagged items is simply too large and only a subset\nof them have been remediated.\n\nYour sample size should be sufficiently large to make the confidence intervals narrow.\nSee the [appendix](#sample-size-and-margin-of-error) below.\n\n### Getting Confidence Intervals\n\nAfter instantiation, calculations for confidence intervals can now be performed via the **get_cis()**\nmethod. By default, **n_iters**=1000000 and **plot_filename** is an empty string.\n\nArgs:\n* **n_iters (int):** Number of iterations to simulate posterior models.\n* **plot_filename (str):** If not empty, save plots using filename as relative path.\n\nReturns:\n* **pos_rate_cis (CIDataClass):** Confidence intervals for pos rate based on multiple methods.\n* **ppv_cis (CIDataClass):** Confidence intervals for precision based on multiple methods.\n* **npv_cis (CIDataClass):** Confidence intervals for NPV based on multiple methods.\n* **recall_cis (CIDataClass):** Confidence intervals for recall based on multiple methods.\n\n\n```python\n>>> pos_rate_cis, ppv_cis, npv_cis, recall_cis = confidenceintervals.get_cis(n_iters=100000, plot_filename='example_metrics_plot')\n>>> print(pos_rate_cis)\nread_only_properties.<locals>.class_rebuilder.<locals>.CustomClass(tnorm_ci=(0.31375112548312334, 0.43624887451687666), poisson_ci=(0.3, 0.45416666666666666), lrt_ci=(0.3154, 0.4373), score_ci=(0.3162, 0.43770000000000003), posterior_ci=(0.3155447558316776, 0.4374497740783102), simulated_ci=(0.3159835357403228, 0.4374219880675257))\n>>> print(ppv_cis)\nread_only_properties.<locals>.class_rebuilder.<locals>.CustomClass(tnorm_ci=(0.41054029281414217, 0.5894597071858578), poisson_ci=(0.375, 0.6333333333333333), lrt_ci=(0.4113, 0.5887), score_ci=(0.41200000000000003, 0.588), posterior_ci=(0.41143373746262163, 0.5885662625373784), simulated_ci=(0.4119916854809507, 0.5890112681445884))\n>>> print(npv_cis)\nread_only_properties.<locals>.class_rebuilder.<locals>.CustomClass(tnorm_ci=(0.6725256210456648, 0.8274743790402173), poisson_ci=(0.6, 0.9083333333333333), lrt_ci=(0.6678000000000001, 0.8216), score_ci=(0.6656, 0.8189000000000001), posterior_ci=(0.667214568760124, 0.8208930781209349), simulated_ci=(0.6675875145345339, 0.8210540703047337))\n>>> print(recall_cis)\nread_only_properties.<locals>.class_rebuilder.<locals>.CustomClass(tnorm_ci=(0.5562766006133449, 0.7735840644338399), poisson_ci=(0.4838709677419355, 0.8735632183908046), lrt_ci=(0.5531943510423672, 0.767435797158128), score_ci=(0.5519828510182208, 0.7645299700949162), posterior_ci=(0.5528394789668374, 0.7666885785320019), simulated_ci=(0.5856829486565244, 0.7438937436565682))\n```\n\nThe **CIDataClass** is a modified class that supports dot notation access and forces the returned\nconfidence intervals to be read-only.\n\n```python\n>>> print(pos_rate_cis.tnorm_ci)\n(0.31375112548312334, 0.43624887451687666)\n>>> pos_rate_cis.tnorm_ci = tuple([.5, .8])\nAttributeError: Can't modify tnorm_ci\n```\n\nFor variable access, a **get()** method is provided as well in **CIDataClass**.\n\n```python\n>>> key = \"tnorm_ci\"\n>>> print(pos_rate_cis.get(key))\n(0.31375112548312334, 0.43624887451687666)\n```\n\n### Visualization\n\nIf **plot_filename** is not an empty string when running **get_cis()**, you will have an image file\nlocated at **plot_filename**.png. An example is provided below.\n\n![](images/example_metrics_plot.png)\n\n\n### Interpretation\n\nLet *p* be the probability for a metric. There are three cases to consider.\n\n* *p* is not close to 0 nor 1: Ensure that the non-Poisson confidence intervals are similar before using\nany of the non-Poisson confidence intervals (as they are all similar).\n    * If non-Poisson confidence intervals are not similar, then include more labeled samples as inputs\n    to the class. This helps to ensures that all non-Poisson methods agree\n    asymptotically on the confidence intervals.\n\n* *p* is close to 0: Use the Poisson confidence intervals.\n    * Increasing the labeled sample size does not necessarily ensure that non-Poisson models will agree\n    asymptotically on what the confidence intervals should be.\n    See the [appendix](#examples-of-extreme-values-of-p) for more details.\n\n* *p* is close to 1: Use one of the non-Truncated-Normal/non-Poisson confidence intervals.\n    * Increasing the labeled sample size does not necessarily ensure that non-Truncated-Normal/non-Poisson\n    models will agree asymptotically on what the confidence intervals should be.\n    See the [appendix](#examples-of-extreme-values-of-p) for more details.\n\n## Models\n\nFor the four metrics of interest, the following models are used. Please note that all models\nare fitted for **Positive Rate**, **Precision**, and **NPV**. The only\nmodel used for **Recall** is the Simulated Distribution. As discussed [below](#Confidence-Interval-Methods),\nno parametric distributions can be fit on recall due to independence constraints for calculating confidence\nintervals.\n\n### Frequentist Models:\n\n#### Exact distribution:\n\nThe exact underlying model fits the story of a binomial distribution. From some total sample size,\neach item in that sample has some probability of being a success or a failure.\n\n* **Binomial**: Binomial distribution.\n\n#### Approximating distribution:\n\nThese models approximate the exact binomial distribution as the sample size grows.\n\n* **Truncated-Normal**: A normal distribution truncated to (0, 1). Theoretically preferred if\nthe estimate for the parameter is not close to 0 nor 1.\n* **Poisson:** Poisson distribution. Theoretically preferred if\nthe estimate for the parameter is close to 0.\n\n### Bayesian Models\n\n#### Exact distribution:\n\nWhile Frequentist statistics view estimates for the parameter as completely determined by observed data,\nBayesian statistics models the parameter with a prior belief via a distribution that causes the parameter\nto depend on both the prior belief and the observed data. A flat prior of Beta(.5,.5)\nis used in this package.\n\n* **Beta-Posterior:** A Beta-Posterior distribution with the Beta distribution as the conjugate\n    prior to the Binomial distribution.\n\n#### Approximating distribution:\n\nThis model approximates the Beta-Posterior distribution as the number of Monte Carlo draws grows.\n\n* **Simulated Distribution:** A non-parametric distribution created from Monte Carlo samples from the\nBeta-Posterior distribution.\n\n## Confidence Interval Methods\n\nThe models above are used to obtain confidence intervals for each of the metrics in three ways.\nFor ease of discussion, confidence intervals also refer to credible intervals under the Bayesian\nframework. This section focuses on derivations, not proofs, the latter of which are\nreadily available at online sources.\n\n### Distributional Confidence Intervals (Truncated-Normal, Poisson, Beta-Posterior)\n\n#### Positive Rate, Precision, and NPV\n\nGiven *c*=**confidence_level**, confidence intervals are drawn from the quantiles of the model's\nprobability mass/density function such that the center (**confidence_level**)% of area lies\nwithin the confidence interval.\n\nFor the Poisson model, an extra adjustment is performed.\nThe support represents the count of successes and is divided by\nthe model's relevant sample size to map to proportions;\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/poisson_to_prop.png\" height=\"60%\" width=\"60%\" alt=\"Proportion Confidence Intervals under Poisson Distribution\"></p>\n\n#### Recall\n\nFor recall, let *X<sub>m* be the precision distribution under model *m* and\n*Y<sub>m* be the NPV distribution under model *m*. Let *N* be the population size. Let\n*N<sub>f* be number of datapoints in population flagged as positives by the algorithm.\nThen, for model *m*, the population recall is\n*X<sub>m* *N<sub>f* / (*X<sub>m* *N<sub>f* + *Y<sub>m* (*N*-*N<sub>f*)).\n\nThere is no clean distributional form for recall due to dependence between the numerator and denominator,\nbut the confidence interval for recall is obtainable by optimizing\nfor the endpoints of the confidence interval for recall using *X<sub>m* and *Y<sub>m*.\nLet *X<sub>m,l* and *X<sub>m,h* be the respective lower and upper bounds of the confidence interval\nfor *X<sub>m*. Then the confidence intervals for population recall are:\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/recall_parametric.png\" height=\"70%\" width=\"70%\" alt=\"Parametric Population Recall Confidence Intervals\"></p>\n\nIndependence between *X<sub>m* and *Y<sub>m* grants the ability to use each each distribution's confidence intervals without\nconcern for dependence effects. Had recall been defined as **precision** / **positives**, the\nparameters of the two distributions are not independent and confidence intervals could not\nbe created in the above manner.\n\n### Hypothesis Test Inversion Confidence Intervals (Binomial)\n\n#### Positive Rate, Precision, and NPV\n\nConfidence intervals are created by inverting the Binomial Likelihood\nRatio Test (LRT) and Score Test. Considering only the parameter values for where the test statistic does\nnot lie in the rejection region determined by **confidence_level**, we take the min and max of\nthose values to get confidence intervals.\n\nLet *n* be the number of observations and *y* be the number of successes.\nThe binomial loglikelihood and score function are respectively defined as:\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/ll_score_formulas.png\" height=\"40%\" width=\"40%\" alt=\"Binomial Loglikelihood and Score Function\"></p>\n\nThen the LR test statistic and Score test statistic respectively are:\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/ll_score_test_stats.png\" height=\"72%\" width=\"72%\" alt=\"LRT and Score Test Statistics\"></p>\n\nUnder the null hypothesis:\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/ll_score_chisq.png\" height=\"16%\" width=\"16%\" alt=\"Chi-Squared Appromxiations to Test Statistics\"></p>\n\nInvoking a one-sided test on the chi-squared distribution letting *c* = **confidence_level**,\nthe confidence intervals for the LRT and Score Test are respectively given by taking the min and max of:\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/ll_score_cis.png\" height=\"25%\" width=\"25%\" alt=\"Inverted LRT and Score Confidence Intervals\"></p>\n\n#### Recall\n\nFor recall, the same logic follows as for distributional confidence intervals.\n\n### Simulated Confidence Intervals (Simulated)\n\n#### Positive Rate, Precision, and NPV\n\nGiven **confidence_level**, confidence intervals are drawn from the quantiles of the Monte Carlo\nsimulated draws from the Beta-Posterior such that the center (**confidence_level**)% of area lies\nwithin the confidence interval.\n\n#### Recall\n\nFor recall, let *p|X* be the Beta-Posterior distribution for precision,\nwhere *X* is the observed binary labeled data, as such:\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/beta_posterior.png\" height=\"45%\" width=\"45%\" alt=\"Beta-Binomial Conjugacy\"></p>\n\nLet *p|Y* similarly be the Beta-Posterior distribution for NPV.\n\nLet *R<sup>(i)* be the *i*'th Monte Carlo sample value for population recall. Let *N* be the population size. Let\n*N<sub>f* be number of datapoints in population flagged as positives by the algorithm.\nThen, based on the independent draws for *p|X* and *p|Y* on the *i*'th iteration:\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/recall_nonparametric.png\" height=\"40%\" width=\"40%\" alt=\"Non-Parametric Population Recall Confidence Intervals\"></p>\n\nfrom which quantiles are drawn.\n\n## Appendix\n\n### Sample Size and Margin of Error\n\nUnder the convention of **confidence_level**=.95, the plot below shows the necessary sample size\nfor a particular proportion to have a certain margin of error under a Wald confidence interval\n(Normal model). Margin of error gives the number of percentage points (on a decimal scale)\nfrom the estimate of the proportion to the confidence interval endpoint on either side of the estimate.\n\nWith a desired Margin of Error *M*, one can invert the equation for the spread in a Wald confidence\ninterval to solve for sample size *n*. Since the true parameter *p* is unknown, to be conservative\nand maximize the required sample size, *p* is set to .5.\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/margin_of_error.png\" height=\"25%\" width=\"25%\" alt=\"Wald Margin of Error and Sample Size Derivation\"></p>\n\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/n_vs_moe_plot_confidence_level=0.95.png\" alt=\"Wald Margin of Error and Sample Size Plot\"></p>\n\n### Examples of Extreme Values of *p*\n\n```python\n>>> # estimate near 0\n>>> for n in [10, 100]:\n...     module = ClassificationConfidenceIntervals(\n...         sample_labels=[0] * n + [1] + [1] * n + [0],\n...         sample_predictions=[1] * (n + 1) + [0] * (n + 1),\n...         population_size=4000,\n...         population_flagged_count=200,\n...         confidence_level=0.95,\n...     )\n...     module.get_cis(plot_filename=f\"near_0_n={n}\")\n\n>>> # estimate near 1\n>>> for n in [10, 100]:\n...     module = ClassificationConfidenceIntervals(\n...         sample_labels=[1] * n + [0] + [0] * n + [1],\n...         sample_predictions=[1] * (n + 1) + [0] * (n + 1),\n...         population_size=4000,\n...         population_flagged_count=200,\n...         confidence_level=0.95,\n...     )\n...     module.get_cis(plot_filename=f\"near_1_n={n}\")\n```\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/near_0_n=10.png\" height=\"45%\" width=\"45%\" alt=\"Low p with low n\">\n    <img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/near_0_n=100.png\" height=\"45%\" width=\"45%\" alt=\"Low p with high n\">\n</p>\n\nFor the case above where precision and NPV are low, notice how the Truncated-Normal confidence\nintervals include negative values, rendering Truncated-Normal unusable in this situation.\nEven as sample size increases, this issue persists; additionally, we do not see convergence\nof the confidence intervals. As such, the best model to use is Poisson.\n\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/near_1_n=10.png\" height=\"45%\" width=\"45%\" alt=\"High p with low n\">\n    <img src=\"https://raw.githubusercontent.com/kennethschen/Classification-Confidence-Intervals/master/images/near_1_n=100.png\" height=\"45%\" width=\"45%\" alt=\"High p with high n\">\n</p>\n\n\nFor the case above where precision and NPV are high, notice how the Truncated-Normal and Poisson\nconfidence\nintervals include values above 1, rendering them unusable in this situation.\nEven as sample size increases, this issue persists; additionally, we do not see convergence\nof the confidence intervals. As such, one of the other models should be used (LRT or Bayesian).\n\n### Comparison of Wald, LRT, and Score Confidence Intervals\n\n* Wald (Truncated-Normal): Inflates Type I error because standard error is calculated under the alternative.\n* LRT (Binomial): Conserves Type I error and power.\n* Score (Binomial): Less powerful than LRT but more robust than Wald.\n\nGenerally, LRT is the best choice among Frequentist confidence intervals.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/kennethschen/Classification-Confidence-Intervals",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "classification-confidence-intervals",
    "package_url": "https://pypi.org/project/classification-confidence-intervals/",
    "platform": "",
    "project_url": "https://pypi.org/project/classification-confidence-intervals/",
    "project_urls": {
      "Homepage": "https://github.com/kennethschen/Classification-Confidence-Intervals"
    },
    "release_url": "https://pypi.org/project/classification-confidence-intervals/1.0.2/",
    "requires_dist": [
      "matplotlib (>=3.2.2)",
      "numpy (>=1.19.0)",
      "scipy (>=1.5.0)",
      "seaborn (>=0.10.1)"
    ],
    "requires_python": "",
    "summary": "A package to calculate confidence intervals for classification positive rate, precision, NPV, and recall using a labeled sample of the population via exact & approximate Frequentist & Bayesian setups.",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7744348,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0c5002ba846881ed64382be26834f11a9825c1143cb1eca3662f1991130c6cf6",
          "md5": "dfdd8d218294791ab93583c03b1cbe9f",
          "sha256": "6c6df4d2004cbf348d2cb287e2d49432c9150a5ab14cbc87a8b89514a0fe23ef"
        },
        "downloads": -1,
        "filename": "classification_confidence_intervals-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dfdd8d218294791ab93583c03b1cbe9f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 8538,
        "upload_time": "2020-07-03T18:19:09",
        "upload_time_iso_8601": "2020-07-03T18:19:09.225316Z",
        "url": "https://files.pythonhosted.org/packages/0c/50/02ba846881ed64382be26834f11a9825c1143cb1eca3662f1991130c6cf6/classification_confidence_intervals-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1d936861ce1036d1407e7521560ff6442f7644a11c79aa39ae8fa4f9c12bec71",
          "md5": "1a87f94e3cfbfe7d2df47723d9809c18",
          "sha256": "84ba70e2ba5b33ae089b10a2e7bd745e2e05b2eaf87af06bab47033a3cbd1bbd"
        },
        "downloads": -1,
        "filename": "classification-confidence-intervals-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1a87f94e3cfbfe7d2df47723d9809c18",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10593,
        "upload_time": "2020-07-03T18:19:11",
        "upload_time_iso_8601": "2020-07-03T18:19:11.532723Z",
        "url": "https://files.pythonhosted.org/packages/1d/93/6861ce1036d1407e7521560ff6442f7644a11c79aa39ae8fa4f9c12bec71/classification-confidence-intervals-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "922373a094d5accd86c23af8e7880104f13df4968ce8c0fef9b547e4e9b60900",
          "md5": "0c5f2374e9cde87369835c7cb7a25b7b",
          "sha256": "bc76e2537fb38be3dc6ae873e63f877bcc9ef9532e9111263de1952bde3d5a39"
        },
        "downloads": -1,
        "filename": "classification_confidence_intervals-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0c5f2374e9cde87369835c7cb7a25b7b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 8538,
        "upload_time": "2020-07-20T21:24:10",
        "upload_time_iso_8601": "2020-07-20T21:24:10.517352Z",
        "url": "https://files.pythonhosted.org/packages/92/23/73a094d5accd86c23af8e7880104f13df4968ce8c0fef9b547e4e9b60900/classification_confidence_intervals-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "76ab989cc088051dc137d9f9bc2d13a9235af7a2d849fe8a23db731ffa1f4d8f",
          "md5": "6cc685d8efbad3376ddc827935e9874a",
          "sha256": "d1eb043cdf49ad5e4418f56bb609dff192adecdfc1eb01bed55eac98407d9577"
        },
        "downloads": -1,
        "filename": "classification-confidence-intervals-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "6cc685d8efbad3376ddc827935e9874a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10593,
        "upload_time": "2020-07-20T21:24:11",
        "upload_time_iso_8601": "2020-07-20T21:24:11.330931Z",
        "url": "https://files.pythonhosted.org/packages/76/ab/989cc088051dc137d9f9bc2d13a9235af7a2d849fe8a23db731ffa1f4d8f/classification-confidence-intervals-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "922373a094d5accd86c23af8e7880104f13df4968ce8c0fef9b547e4e9b60900",
        "md5": "0c5f2374e9cde87369835c7cb7a25b7b",
        "sha256": "bc76e2537fb38be3dc6ae873e63f877bcc9ef9532e9111263de1952bde3d5a39"
      },
      "downloads": -1,
      "filename": "classification_confidence_intervals-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0c5f2374e9cde87369835c7cb7a25b7b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 8538,
      "upload_time": "2020-07-20T21:24:10",
      "upload_time_iso_8601": "2020-07-20T21:24:10.517352Z",
      "url": "https://files.pythonhosted.org/packages/92/23/73a094d5accd86c23af8e7880104f13df4968ce8c0fef9b547e4e9b60900/classification_confidence_intervals-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "76ab989cc088051dc137d9f9bc2d13a9235af7a2d849fe8a23db731ffa1f4d8f",
        "md5": "6cc685d8efbad3376ddc827935e9874a",
        "sha256": "d1eb043cdf49ad5e4418f56bb609dff192adecdfc1eb01bed55eac98407d9577"
      },
      "downloads": -1,
      "filename": "classification-confidence-intervals-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "6cc685d8efbad3376ddc827935e9874a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 10593,
      "upload_time": "2020-07-20T21:24:11",
      "upload_time_iso_8601": "2020-07-20T21:24:11.330931Z",
      "url": "https://files.pythonhosted.org/packages/76/ab/989cc088051dc137d9f9bc2d13a9235af7a2d849fe8a23db731ffa1f4d8f/classification-confidence-intervals-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}