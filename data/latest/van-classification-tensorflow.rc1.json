{
  "info": {
    "author": "EMalagoli92",
    "author_email": "emala.892@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "<div align=\"center\">\n\n  <a href=\"https://www.tensorflow.org\">![TensorFLow](https://img.shields.io/badge/TensorFlow-2.X-orange?style=for-the-badge) \n  <a href=\"https://github.com/EMalagoli92/VAN-Classification-TensorFlow/blob/main/LICENSE\">![License](https://img.shields.io/github/license/EMalagoli92/VAN-Classification-TensorFlow?style=for-the-badge) \n  <a href=\"https://www.python.org\">![Python](https://img.shields.io/badge/python-%3E%3D%203.9-blue?style=for-the-badge)</a>  \n  \n</div>\n\n# VAN-Classification-TensorFlow\nTensorFlow 2.X reimplementation of [Visual Attention Network](https://arxiv.org/abs/2202.09741v5), Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n- Exact TensorFlow reimplementation of official PyTorch repo, including `timm` modules used by authors, preserving models and layers structure.\n- ImageNet pretrained weights ported from PyTorch official implementation.\n\n## Table of contents\n- [Abstract](#abstract)\n- [Results](#results)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Acknowledgement](#acknowledgement)\n- [Citations](#citations)\n- [License](#license)\n\n<div id=\"abstract\"/>\n\n## Abstract\n*While originally designed for natural language processing (NLP) tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, the authors propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. The authors further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple and efficient, VAN outperforms the state-of-the-art vision transformers (ViTs) and convolutional neural networks (CNNs) with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc.*\n\n\n![Alt text](https://github.com/EMalagoli92/VAN-Classification-TensorFlow/blob/main/assets/images/Comparsion.png?raw=true) \n<p align = \"center\"><sub>Figure 1. Compare with different vision backbones on ImageNet-1K validation set.</sub></p>\n\n\n![Alt text](https://github.com/EMalagoli92/VAN-Classification-TensorFlow/blob/main/assets/images/decomposition.png?raw=true)\n<p align = \"center\"><sub>Figure 2. Decomposition diagram of large-kernel convolution. A standard convolution can be decomposed into three parts: a depth-wise convolution (DW-Conv), a depth-wise dilation convolution (DW-D-Conv) and a 1×1 convolution (1×1 Conv).</sub></p>\n\n\n![Alt text](https://github.com/EMalagoli92/VAN-Classification-TensorFlow/blob/main/assets/images/LKA.png?raw=true)\n<p align = \"center\"><sub>Figure 3. The structure of different modules: (a) the proposed Large Kernel Attention (LKA); (b) non-attention module; (c) the self-attention module (d) a stage of our Visual Attention Network (VAN). CFF means convolutional feed-forward network. The difference between (a) and (b) is the element-wise multiply. It is worth noting that (c) is designed for 1D sequences.</sub></p>\n\n\n<div id=\"results\"/>\n\n## Results\nTensorFlow implementation and ImageNet ported weights have been compared to the official PyTorch implementation on [ImageNet-V2](https://www.tensorflow.org/datasets/catalog/imagenet_v2) test set.\n\n### Models pre-trained on ImageNet-1K\n| Configuration  | Resolution | Top-1 (Original) | Top-1 (Ported) | Top-5 (Original) | Top-5 (Ported) | #Params\n| ------------- | ------------- | ------------- | ------------- | ------------- | ------------- | ------------- |\n| VAN-B0 | 224x224 | 0.59 | 0.59 | 0.81 | 0.81 | 4.1M |\n| VAN-B1 | 224x224 | 0.64 | 0.64 | 0.84 | 0.84 | 13.9M |\n| VAN-B2 | 224x224 | 0.69 | 0.69 | 0.88 | 0.88 | 26.6M |\n| VAN-B3 | 224x224 | 0.71 | 0.71 | 0.89 | 0.89 | 44.8M |\n\nMetrics difference: `0`.\n\n\n<div id=\"installation\"/>\n\n## Installation\n- Install from PyPI.\n```\npip install van-classification-tensorflow\n```\n- Install from GitHub.\n```\npip install git+https://github.com/EMalagoli92/VAN-Classification-TensorFlow\n```\n- Clone the repo and install necessary packages.\n```\ngit clone https://github.com/EMalagoli92/VAN-Classification-TensorFlow.git\npip install -r requirements.txt\n```\nTested on *Ubuntu 20.04.4 LTS x86_64*, *python 3.9.7*.\n\n<div id=\"usage\"/>\n\n## Usage\n- Define a custom VAN configuration.\n```python\nfrom van_classification_tensorflow import VAN\n\n# Define a custom VAN configuration\nmodel = VAN(\n    in_chans=3,\n    num_classes=1000,\n    embed_dims=[64, 128, 256, 512],\n    mlp_ratios=[4, 4, 4, 4],\n    drop_rate=0.0,\n    drop_path_rate=0.0,\n    depths=[3, 4, 6, 3],\n    num_stages=4,\n    include_top=True,\n    classifier_activation=\"softmax\",\n    data_format=\"channels_last\",\n)\n```\n- Use a predefined VAN configuration.\n```python\nfrom van_classification_tensorflow import VAN\n\nmodel = VAN(\n    configuration=\"van_b0\", data_format=\"channels_last\", classifier_activation=\"softmax\"\n)\n\nmodel.build((None, 224, 224, 3))\nprint(model.summary())\n```\n```\nModel: \"van_b0\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n patch_embed1 (OverlapPatchE  ((None, 32, 56, 56),     4864      \n mbed)                        (),                                \n                              ())                                \n                                                                 \n block1/0 (Block)            (None, 32, 56, 56)        25152     \n                                                                 \n block1/1 (Block)            (None, 32, 56, 56)        25152     \n                                                                 \n block1/2 (Block)            (None, 32, 56, 56)        25152     \n                                                                 \n norm1 (LayerNorm_)          (None, 3136, 32)          64        \n                                                                 \n patch_embed2 (OverlapPatchE  ((None, 64, 28, 28),     18752     \n mbed)                        (),                                \n                              ())                                \n                                                                 \n block2/0 (Block)            (None, 64, 28, 28)        89216     \n                                                                 \n block2/1 (Block)            (None, 64, 28, 28)        89216     \n                                                                 \n block2/2 (Block)            (None, 64, 28, 28)        89216     \n                                                                 \n norm2 (LayerNorm_)          (None, 784, 64)           128       \n                                                                 \n patch_embed3 (OverlapPatchE  ((None, 160, 14, 14),    92960     \n mbed)                        (),                                \n                              ())                                \n                                                                 \n block3/0 (Block)            (None, 160, 14, 14)       303040    \n                                                                 \n block3/1 (Block)            (None, 160, 14, 14)       303040    \n                                                                 \n block3/2 (Block)            (None, 160, 14, 14)       303040    \n                                                                 \n block3/3 (Block)            (None, 160, 14, 14)       303040    \n                                                                 \n block3/4 (Block)            (None, 160, 14, 14)       303040    \n                                                                 \n norm3 (LayerNorm_)          (None, 196, 160)          320       \n                                                                 \n patch_embed4 (OverlapPatchE  ((None, 256, 7, 7),      369920    \n mbed)                        (),                                \n                              ())                                \n                                                                 \n block4/0 (Block)            (None, 256, 7, 7)         755200    \n                                                                 \n block4/1 (Block)            (None, 256, 7, 7)         755200    \n                                                                 \n norm4 (LayerNorm_)          (None, 49, 256)           512       \n                                                                 \n head (Linear_)              (None, 1000)              257000    \n                                                                 \n pred (Activation)           (None, 1000)              0         \n                                                                 \n=================================================================\nTotal params: 4,113,224\nTrainable params: 4,105,800\nNon-trainable params: 7,424\n_________________________________________________________________\n```\n- Train from scratch the model.\n```python\n# Example\nmodel.compile(\n    optimizer=\"sgd\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\", \"sparse_top_k_categorical_accuracy\"],\n)\nmodel.fit(x, y)\n```\n- Use ported ImageNet pretrained weights.\n```python\n# Example\nfrom van_classification_tensorflow import VAN\n\nmodel = VAN(\n    configuration=\"van_b1\",\n    pretrained=True,\n    include_top=True,\n    classifier_activation=\"softmax\",\n)\ny_pred = model(image)\n```\n\n- Use ported ImageNet pretrained weights for feature extraction (`include_top=False`).\n```python\nimport tensorflow as tf\n\nfrom van_classification_tensorflow import VAN\n\n# Get Features\ninputs = tf.keras.layers.Input(shape=(224, 224, 3), dtype=\"float32\")\nfeatures = VAN(configuration=\"van_b0\", pretrained=True, include_top=False)(inputs)\n\n\n# Custom classification\nnum_classes = 10\noutputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(features)\nmodel = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n```\n\n<div id=\"acknowledgement\"/>\n\n## Acknowledgement\n[VAN-Classification](https://github.com/Visual-Attention-Network/VAN-Classification) (Official PyTorch implementation).\n\n\n<div id=\"citations\"/>\n\n## Citations\n```bibtex\n@article{guo2022visual,\n  title={Visual Attention Network},\n  author={Guo, Meng-Hao and Lu, Cheng-Ze and Liu, Zheng-Ning and Cheng, Ming-Ming and Hu, Shi-Min},\n  journal={arXiv preprint arXiv:2202.09741},\n  year={2022}\n}\n```\n\n\n<div id=\"license\"/>\n\n## License\nThis work is made available under the [MIT License](https://github.com/EMalagoli92/VAN-Classification-TensorFlow/blob/main/LICENSE).\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/EMalagoli92/VAN-Classification-TensorFlow",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "van-classification-tensorflow",
    "package_url": "https://pypi.org/project/van-classification-tensorflow/",
    "platform": null,
    "project_url": "https://pypi.org/project/van-classification-tensorflow/",
    "project_urls": {
      "Homepage": "https://github.com/EMalagoli92/VAN-Classification-TensorFlow"
    },
    "release_url": "https://pypi.org/project/van-classification-tensorflow/1.0.3/",
    "requires_dist": [
      "absl-py (==1.4.0)",
      "astunparse (==1.6.3)",
      "cachetools (==5.3.0)",
      "certifi (==2022.12.7)",
      "charset-normalizer (==3.0.1)",
      "cloudpickle (==2.2.1)",
      "decorator (==5.1.1)",
      "dm-tree (==0.1.8)",
      "flatbuffers (==1.12)",
      "gast (==0.4.0)",
      "google-auth (==2.16.2)",
      "google-auth-oauthlib (==0.4.6)",
      "google-pasta (==0.2.0)",
      "grpcio (==1.51.3)",
      "h5py (==3.8.0)",
      "idna (==3.4)",
      "importlib-metadata (==6.0.0)",
      "keras (==2.9.0)",
      "Keras-Preprocessing (==1.1.2)",
      "libclang (==15.0.6.1)",
      "Markdown (==3.4.1)",
      "MarkupSafe (==2.1.2)",
      "numpy (==1.24.2)",
      "oauthlib (==3.2.2)",
      "opt-einsum (==3.3.0)",
      "packaging (==23.0)",
      "protobuf (==3.19.6)",
      "pyasn1 (==0.4.8)",
      "pyasn1-modules (==0.2.8)",
      "requests (==2.28.2)",
      "requests-oauthlib (==1.3.1)",
      "rsa (==4.9)",
      "six (==1.16.0)",
      "tensorboard (==2.9.1)",
      "tensorboard-data-server (==0.6.1)",
      "tensorboard-plugin-wit (==1.8.1)",
      "tensorflow (==2.9.0)",
      "tensorflow-addons (==0.17.1)",
      "tensorflow-estimator (==2.9.0)",
      "tensorflow-io-gcs-filesystem (==0.31.0)",
      "tensorflow-probability (==0.17.0)",
      "termcolor (==2.2.0)",
      "typeguard (==2.13.3)",
      "typing-extensions (==4.5.0)",
      "urllib3 (==1.26.14)",
      "Werkzeug (==2.2.3)",
      "wrapt (==1.15.0)",
      "zipp (==3.15.0)"
    ],
    "requires_python": ">=3.9",
    "summary": "TensorFlow 2.X reimplementation of Visual Attention Network, Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17164923,
  "releases": {
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e052b2f96683bd7aa1d743271c818521d217d3353242b92e01df0ce304e041c1",
          "md5": "6ee111952bed3b4bcad9b1ea5aa88d7e",
          "sha256": "8d0b939212ef90c1cb87146e3a6c0de21df52d7dbe94f9aca1ebf21c226ebd7b"
        },
        "downloads": -1,
        "filename": "van_classification_tensorflow-1.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6ee111952bed3b4bcad9b1ea5aa88d7e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 21290,
        "upload_time": "2023-03-05T15:41:43",
        "upload_time_iso_8601": "2023-03-05T15:41:43.992426Z",
        "url": "https://files.pythonhosted.org/packages/e0/52/b2f96683bd7aa1d743271c818521d217d3353242b92e01df0ce304e041c1/van_classification_tensorflow-1.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f495a24848f30ef7181fd746676e2bd87d1af701890c35da73e5fa686a1bbd97",
          "md5": "f4bedf234ae4394926b320f2044d4b53",
          "sha256": "63e0c680b4f6e0f0c6688037b21fe2fb6cdfd9d637a2d0c62541ac159409be57"
        },
        "downloads": -1,
        "filename": "van_classification_tensorflow-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "f4bedf234ae4394926b320f2044d4b53",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 18286,
        "upload_time": "2023-03-05T15:41:45",
        "upload_time_iso_8601": "2023-03-05T15:41:45.672106Z",
        "url": "https://files.pythonhosted.org/packages/f4/95/a24848f30ef7181fd746676e2bd87d1af701890c35da73e5fa686a1bbd97/van_classification_tensorflow-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e052b2f96683bd7aa1d743271c818521d217d3353242b92e01df0ce304e041c1",
        "md5": "6ee111952bed3b4bcad9b1ea5aa88d7e",
        "sha256": "8d0b939212ef90c1cb87146e3a6c0de21df52d7dbe94f9aca1ebf21c226ebd7b"
      },
      "downloads": -1,
      "filename": "van_classification_tensorflow-1.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "6ee111952bed3b4bcad9b1ea5aa88d7e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.9",
      "size": 21290,
      "upload_time": "2023-03-05T15:41:43",
      "upload_time_iso_8601": "2023-03-05T15:41:43.992426Z",
      "url": "https://files.pythonhosted.org/packages/e0/52/b2f96683bd7aa1d743271c818521d217d3353242b92e01df0ce304e041c1/van_classification_tensorflow-1.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f495a24848f30ef7181fd746676e2bd87d1af701890c35da73e5fa686a1bbd97",
        "md5": "f4bedf234ae4394926b320f2044d4b53",
        "sha256": "63e0c680b4f6e0f0c6688037b21fe2fb6cdfd9d637a2d0c62541ac159409be57"
      },
      "downloads": -1,
      "filename": "van_classification_tensorflow-1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "f4bedf234ae4394926b320f2044d4b53",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.9",
      "size": 18286,
      "upload_time": "2023-03-05T15:41:45",
      "upload_time_iso_8601": "2023-03-05T15:41:45.672106Z",
      "url": "https://files.pythonhosted.org/packages/f4/95/a24848f30ef7181fd746676e2bd87d1af701890c35da73e5fa686a1bbd97/van_classification_tensorflow-1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}