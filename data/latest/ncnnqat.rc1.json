{
  "info": {
    "author": "Shisen Chen",
    "author_email": "napoleo54css@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "<div id=\"ncnnqat\"></div>\n\n# ncnnqat\n\nncnnqat is a quantize aware training package for NCNN on pytorch.\n\n<div id=\"table-of-contents\"></div>\n\n## Table of Contents\n\n- [ncnnqat](#ncnnqat)\n  - [Table of Contents](#table-of-contents)\n  - [Installation](#installation)\n  - [Usage](#usage)\n  - [Code Examples](#code-examples)\n  - [Results](#results)\n  - [Todo](#todo)\n\n\n<div id=\"installation\"></div>  \n\n## Installation\n\n* Supported Platforms: Linux\n* Accelerators and GPUs: NVIDIA GPUs via CUDA driver ***10.1***.\n* Dependencies:\n  * python >= 3.5, < 4\n  * pytorch >= 1.6\n  * numpy >= 1.18.1\n  * onnx >= 1.7.0\n  * onnx-simplifier >= 0.3.5\n\n* Install ncnnqat via pypi:  \n  ```shell\n  $ pip install ncnnqat\n  ```\n\n* or Install ncnnqat via repo：\n  ```shell\n  $ git clone https://github.com/ChenShisen/ncnnqat\n  $ cd ncnnqat\n  $ make install\n  ```\n\n<div id=\"usage\"></div>\n\n## Usage\n\n\n* merge bn weight into conv and freeze bn\n\n  suggest finetuning from a well-trained model, register_quantization_hook and merge_freeze_bn at beginning. do it after a few epochs of training otherwise.\n\n  ```python\n  from ncnnqat import quant_dequant_weight, unquant_weight, merge_freeze_bn, register_quantization_hook\n  ...\n  ...\n      for epoch in range(epoch_train):\n\t\t  model.train()\n\t\t  if epoch==well_epoch:\n\t\t\t  register_quantization_hook(model)\n\t\t  if epoch>=well_epoch:\n\t\t\t  model = merge_freeze_bn(model)  #it will change bn to eval() mode during training\n  ...\n  ```\n\n* Unquantize weight before update it\n\n  ```python\n  ...\n  ...\n      model.apply(unquant_weight)  # using original weight while updating\n      optimizer.step()\n  ...\n  ```\n\n* Save weight and save ncnn quantize table after train\n\n\n  ```python\n  ...\n  ...\n      onnx_path = \"./xxx/model.onnx\"\n\t  table_path=\"./xxx/model.table\"\n\t  dummy_input = torch.randn(1, 3, img_size, img_size, device='cuda')\n      input_names = [ \"input\" ]\n      output_names = [ \"fc\" ]\n      torch.onnx.export(model, dummy_input, onnx_path, verbose=False, input_names=input_names, output_names=output_names)\n\t  save_table(model,onnx_path=onnx_path,table=table_path)\n\n  ...\n  ```\n  if use \"model = nn.DataParallel(model)\",pytorch unsupport torch.onnx.export,you should save state_dict first and  prepare a new model with one gpu,then you will export onnx model.\n  \n  ```python\n  ...\n  ...\n      model_s = new_net() #\n\t  model_s.cuda()\n\t  register_quantization_hook(model_s)\n\t  #model_s = merge_freeze_bn(model_s)\n      onnx_path = \"./xxx/model.onnx\"\n\t  table_path=\"./xxx/model.table\"\n\t  dummy_input = torch.randn(1, 3, img_size, img_size, device='cuda')\n      input_names = [ \"input\" ]\n      output_names = [ \"fc\" ]\n\t  model_s.load_state_dict({k.replace('module.',''):v for k,v in model.state_dict().items()}) #model_s = model     model = nn.DataParallel(model)\n            \n      torch.onnx.export(model_s, dummy_input, onnx_path, verbose=False, input_names=input_names, output_names=output_names)\n\t  save_table(model_s,onnx_path=onnx_path,table=table_path)\n\t  \n\n  ...\n  ```\n\n* Using EMA with caution(Not recommended).\n\n<div id=\"code-examples\"></div>\n\n## Code Examples\n\n  Cifar10 quantization aware training example.\n\n  ```python test/test_cifar10.py```\n\n<div id=\"results\"></div>\n\n## Results  \n\n* Cifar10\n\n\n  result：\n\n    |  net   | fp32(onnx) | ncnnqat     | ncnn aciq     | ncnn kl |\n    | -------- |  -------- | -------- | -------- | -------- |\n    | mobilenet_v2     | 0.91  | 0.9066  | 0.9033 | 0.9066 |\n    | resnet18 | 0.94   | 0.93333   | 0.9367 | 0.937|\n\n\n* coco\n\n  ....\n\n\n<div id=\"todo\"></div>\n\n## Todo\n\n   ....\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ChenShisen/ncnnqat",
    "keywords": "ncnnquantization aware training,deep learning,neural network,CNN,machine learning",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ncnnqat",
    "package_url": "https://pypi.org/project/ncnnqat/",
    "platform": "",
    "project_url": "https://pypi.org/project/ncnnqat/",
    "project_urls": {
      "Homepage": "https://github.com/ChenShisen/ncnnqat"
    },
    "release_url": "https://pypi.org/project/ncnnqat/0.1.0/",
    "requires_dist": null,
    "requires_python": ">=3.5, <4",
    "summary": "A ncnn quantization aware training tool on pytorch.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10711531,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dd19329076cc242fb8db173c519dd0b15f0c446b6fcd592f86a222866cf08320",
          "md5": "f2bd91860a5b669205647b19c5949a16",
          "sha256": "20d43dfede591af11879315149c754c07adf909cc0fa0ad0715a2a02f8d27069"
        },
        "downloads": -1,
        "filename": "ncnnqat-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f2bd91860a5b669205647b19c5949a16",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5, <4",
        "size": 8044,
        "upload_time": "2021-06-22T11:17:59",
        "upload_time_iso_8601": "2021-06-22T11:17:59.545623Z",
        "url": "https://files.pythonhosted.org/packages/dd/19/329076cc242fb8db173c519dd0b15f0c446b6fcd592f86a222866cf08320/ncnnqat-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dd19329076cc242fb8db173c519dd0b15f0c446b6fcd592f86a222866cf08320",
        "md5": "f2bd91860a5b669205647b19c5949a16",
        "sha256": "20d43dfede591af11879315149c754c07adf909cc0fa0ad0715a2a02f8d27069"
      },
      "downloads": -1,
      "filename": "ncnnqat-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "f2bd91860a5b669205647b19c5949a16",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5, <4",
      "size": 8044,
      "upload_time": "2021-06-22T11:17:59",
      "upload_time_iso_8601": "2021-06-22T11:17:59.545623Z",
      "url": "https://files.pythonhosted.org/packages/dd/19/329076cc242fb8db173c519dd0b15f0c446b6fcd592f86a222866cf08320/ncnnqat-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}