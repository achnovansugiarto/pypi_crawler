{
  "info": {
    "author": "王文皓(wangwenhao)",
    "author_email": "DATA-OG@139.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Install\npip install MultiProcessMStepRegression\n# Function Description\nA step-wise regression with python.It has step-wise logstic regression and step-wise linear regression.  \nIt uses multiprocessing when deciding to add or remove features.   \nIt works with multi-processing.Supporting Windows system multi-processing too.  \n\n# All characteristics\n1.Supporting forward-backward Step-Wise.  \n2.Supporting multi-processing.When adding or removing features,multi-processing is used to traversal all candidate features.  \n3.Supporting that user could point the index instead of AIC/BIC for measuring model performance when adding or removing feaures.There is a benefit when data is unbalanced.  \n4.Supporting that user could point p-value threshold.If max p-value is more than this threshold,the current features will not be added,although getting a lift on performance of model.  \n5.Supporting that user could point VIF threshold.If max VIF is more than this threshold,the current features will not be added,although getting a lift on performance of model.  \n6.Supporting that user could point coefficient of correlation threshold.If max coefficient of correlation is more than this threshold,the current features will not be added,although getting a lift on performance of model.  \n7.Supporting that user could point sign to coefficients of regression. A part of features have sense in some business like woe transfer which require that all coefficients of regression are postive or negtive.If the signs requirement is not met,the current features will not be added,although getting a lift on performance of model.  \n8.[4,5,6,7] above are completed in step-wise procedure.Picking features and verifing those thresholds and signs are simultaneous.  \n9.Users will get reasons of the features aren`t picked up,as performance is fall or p-value is more than threshold or signs is not in accord with expect of user and so on after adding this feature.   \n10.Supporting the Chinese and English log in whcih user can get record of every iteration.  \n\n#### News in 0.2.1\n11.When deciding that current feature should be in model or not,sorts probability output by current model descending and takes top(last) n% to evaluate model.The benefit is raising the ability of catching some one class sample.For example in credit risk,lifts proportion of overdue user in high risk sublevel(LIFT 1 or LIFT 5).Available in logistic.  \n12.When deciding that current feature should be in model or not, uses a new data set diffenent with trainning data set to evaluate model.(Try best to avoid using validation data set and test data set with confusing.When feature selecting ,it means data leak in the final model evaluating that using test data set,so that may over evaludate model performance.It should reserve a cleaning test data set to evaluate model).  \n\n#### News in 0.3.1\n13.User can set some features that must be imported by model.\n\n# Q&A\nWeChat: DATA_OG  \nmark: github\n\n# Usage:\n```\nimport MultiProcessMStepRegression as mpmr\nfrom sklearn.datasets import make_classification,make_regression\nimport pandas as pd\n\ndef get_X_y(data_type,n_samples=200,random_state=0):\n    if data_type == 'logistic':\n        #number of informative features = 4\n        #number of redundant features = 2.redundant feature is linear combinations of the informative features\n        #number of useless features = 10-4-2=4\n        X, y = make_classification(n_samples=n_samples,n_features=10,n_informative=4,n_redundant=2,shuffle=False,random_state=random_state,class_sep=2)\n        X = pd.DataFrame(X,columns=['informative_1','informative_2','informative_3','informative_4','redundant_1','redundant_2','useless_1','useless_2','useless_3','useless_4']).sample(frac=1)\n        y=pd.Series(y).loc[X.index]\n        \n    if data_type == 'linear':\n        # number of informative features = 6\n        # matrix rank = 4 (implying collinearity between six informative features)\n        X, y = make_regression(n_samples=n_samples,n_features=10,n_informative=6,effective_rank=4,noise=5,shuffle=False,random_state=random_state)\n        X = pd.DataFrame(X,columns=['informative_1','informative_2','informative_3','informative_4','informative_5','informative_6','useless_1','useless_2','useless_3','useless_4']).sample(frac=1)\n        y=pd.Series(y).loc[X.index]\n    return X, y\n    \ndef test_logit(X,y):    \n  #   As can be seen:\n  #   1.All informative features are picked up by this algorithm\n  #   2.All linear combinations features are excluded and the reasons are over the max_vif_limit and over max_corr_limit and over max_pvalue_limit and no lift on the model perfermance.\n  #   3.All useless features are excluded. The reasons of that are no lift on the perfermance of model or over max_pvalue_limit.\n    \n  #   return\n  #    in_vars = ['informative_3', 'informative_4', 'informative_2', 'informative_1']\n  #\n  #    dr = {'redundant_1': (['模型性能=0.956100,小于等于最终模型的性能=0.956100',\n  #   '最大VIF=inf,大于设置的阈值=3.000000',\n  #   '最大相关系数=0.925277,大于设置的阈值=0.600000',\n  #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n  #  ['the performance index of model=0.956100,less or equals than the performance index of final model=0.956100',\n  #   'the max VIF=inf,more than the setting of max_vif_limit=3.000000',\n  #   'the max correlation coefficient=0.925277,more than the setting of max_corr_limit=0.600000',\n  #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n  # 'redundant_2': (['模型性能=0.956100,小于等于最终模型的性能=0.956100',\n  #   '最大VIF=inf,大于设置的阈值=3.000000',\n  #   '最大相关系数=0.676772,大于设置的阈值=0.600000',\n  #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n  #  ['the performance index of model=0.956100,less or equals than the performance index of final model=0.956100',\n  #   'the max VIF=inf,more than the setting of max_vif_limit=3.000000',\n  #   'the max correlation coefficient=0.676772,more than the setting of max_corr_limit=0.600000',\n  #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n  # 'useless_1': (['模型性能=0.955200,小于等于最终模型的性能=0.956100',\n  #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n  #  ['the performance index of model=0.955200,less or equals than the performance index of final model=0.956100',\n  #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n  # 'useless_2': (['有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n  #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n  # 'useless_3': (['有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n  #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n  # 'useless_4': (['模型性能=0.955800,小于等于最终模型的性能=0.956100',\n  #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n  #  ['the performance index of model=0.955800,less or equals than the performance index of final model=0.956100',\n  #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000'])}\n    lr  =  mpmr.LogisticReg(X,y,measure='roc_auc',iter_num=20,logger_file_EN='c:/temp/mstep_en.log',logger_file_CH='c:/temp/mstep_ch.log')\n    in_vars,clf_final,dr = lr.fit()\n    return in_vars,clf_final,dr\n    \n\ndef test_linear(X,y):\n    \n #    As can be seen:\n #    The picked features is from informative features\n #    The number of picked features equals matrix rank\n    \n #    return \n #    ['informative_2', 'informative_5', 'informative_3', 'informative_4']\n#   dr={'informative_1': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000']),\n # 'informative_6': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000']),\n # 'useless_1': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000']),\n # 'useless_2': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000']),\n # 'useless_3': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000']),\n # 'useless_4': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000'])}\n    lr  =  mpmr.LinearReg(X,y,max_pvalue_limit=0.01,logger_file_EN='c:/temp/mstep_en.log',logger_file_CH='c:/temp/mstep_ch.log')\n    in_vars,rg_final,dr = lr.fit()\n    return in_vars,rg_final,dr\n\ndef test_measureXy(X,y):\n  #   As can be seen:\n  #   1.The features are picked up by this algorithm are informative.\n  #   2.It is due to its correlation is 0.65 and greater than 0.6 you set that excluding only one informative features.\n  #   3.All linear combinations features are excluded and the reasons are over the max_vif_limit and max_corr_limit.\n  #   4.All useless features are excluded and the reasons are no lift on the perfermance of model and over max_pvalue_limit.\n    \n  #   return\n  #    in_vars = ['informative_4', 'informative_1', 'informative_3']\n  #\n  #    dr = {'informative_2': (['最大相关系数=0.656879,大于设置的阈值=0.600000'],\n   #  ['the max correlation coefficient=0.656879,more than the setting of max_corr_limit=0.600000']),\n   # 'redundant_1': (['最大VIF=7.957338,大于设置的阈值=3.000000',\n   #   '最大相关系数=0.886775,大于设置的阈值=0.600000'],\n   #  ['the max VIF=7.957338,more than the setting of max_vif_limit=3.000000',\n   #   'the max correlation coefficient=0.886775,more than the setting of max_corr_limit=0.600000']),\n   # 'redundant_2': (['最大VIF=532.953471,大于设置的阈值=3.000000',\n   #   '最大相关系数=0.883488,大于设置的阈值=0.600000'],\n   #  ['the max VIF=532.953471,more than the setting of max_vif_limit=3.000000',\n   #   'the max correlation coefficient=0.883488,more than the setting of max_corr_limit=0.600000']),\n   # 'useless_1': (['模型性能=0.640169,小于等于最终模型的性能=0.648233',\n   #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n   #  ['the performance index of model=0.640169,less or equals than the performance index of final model=0.648233',\n   #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n   # 'useless_2': (['模型性能=0.640297,小于等于最终模型的性能=0.648233',\n   #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n   #  ['the performance index of model=0.640297,less or equals than the performance index of final model=0.648233',\n   #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n   # 'useless_3': (['模型性能=0.641577,小于等于最终模型的性能=0.648233',\n   #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n   #  ['the performance index of model=0.641577,less or equals than the performance index of final model=0.648233',\n   #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n   # 'useless_4': (['模型性能=0.640553,小于等于最终模型的性能=0.648233',\n   #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n   #  ['the performance index of model=0.640553,less or equals than the performance index of final model=0.648233',\n   #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000'])}\n    from sklearn.model_selection import train_test_split\n    X_train,X_measure,y_train,y_measure = train_test_split(X,y, test_size=0.5,random_state=10)\n    lr  =  mpmr.LogisticReg(X_train,y_train,measure_X=X_measure,measure_y=y_measure,iter_num=20)\n    in_vars,clf_final,dr = lr.fit()\n    return in_vars,clf_final,dr\n\ndef test_givenX_logistic(X,y):\n    #   As can be seen:\n    #   1.The features picked up by this algorithm are informative features except for features imported by force\n    #   2.The reason the informative features are excluded is the VIF and correlation is greater than the value from user set.That is due to import a redundant combination feature forcedly\n    #   3.All linear combination features are excluded except for combination features imported by force and the reasons of that are over the max_vif_limit and max_corr_limit\n    #   4.All useless features are excluded except for useless features imported by force and the reasons of that are no lift on the perfermance of model and over max_pvalue_limit\n    #   5.The pvalue of useless features imported forcedly is 0.32\n      \n    #   return\n    #    in_vars = ['redundant_1', 'useless_4', 'informative_2', 'informative_1']\n    \n    # pvalues: \n    # const            2.041083e-07\n    # redundant_1      2.010675e-07\n    # useless_4        3.264707e-01\n    # informative_2    4.459015e-11\n    # informative_1    1.700265e-07\n    #\n    #    dr = {'informative_3': (['最大VIF=3.265828,大于设置的阈值=3.000000'],\n     #  ['the max VIF=3.265828,more than the setting of max_vif_limit=3.000000']),\n     # 'informative_4': (['最大VIF=313226.371738,大于设置的阈值=3.000000',\n     #   '最大相关系数=0.925277,大于设置的阈值=0.600000'],\n     #  ['the max VIF=313226.371738,more than the setting of max_vif_limit=3.000000',\n     #   'the max correlation coefficient=0.925277,more than the setting of max_corr_limit=0.600000']),\n     # 'redundant_2': (['最大VIF=7.226070,大于设置的阈值=3.000000',\n     #   '最大相关系数=0.814790,大于设置的阈值=0.600000'],\n     #  ['the max VIF=7.226070,more than the setting of max_vif_limit=3.000000',\n     #   'the max correlation coefficient=0.814790,more than the setting of max_corr_limit=0.600000']),\n     # 'useless_1': (['有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n     #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n     # 'useless_2': (['模型性能=0.892900,小于等于最终模型的性能=0.893000',\n     #   '有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n     #  ['the performance index of model=0.892900,less or equals than the performance index of final model=0.893000',\n     #   'some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000']),\n     # 'useless_3': (['有些系数不显著，P_VALUE大于设置的阈值=0.050000'],\n     #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.050000'])}\n    lr  =  mpmr.LogisticReg(X,y,given_cols=['redundant_1','useless_4'],measure='roc_auc')\n    in_vars,clf_final,dr = lr.fit()\n    return in_vars,clf_final,dr   \n\ndef test_givenX_linear(X,y):\n #    As can be seen:\n #    1.Compare with test_linear, 'informative_1' and 'useless_1' is chosen more.They are imported by user forcedly.\n #    2.The pvalue of 'informative_1' and 'useless_1' are 0.31 and 0.095\n    \n #    return \n #    ['informative_1','useless_1','informative_2','informative_5','informative_3','informative_4']\n \n # const            4.096327e-01\n # informative_1    3.087456e-01\n # useless_1        9.528599e-02\n # informative_2    1.897154e-12\n # informative_5    8.468702e-08\n # informative_3    3.985832e-04\n # informative_4    4.351842e-03\n \n #    dr = {'informative_6': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n  #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000']),\n  # 'useless_2': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n  #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000']),\n  # 'useless_3': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n  #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000']),\n  # 'useless_4': (['有些系数不显著，P_VALUE大于设置的阈值=0.010000'],\n  #  ['some coefficients are not significant,P_VALUE is more than the setting of max_pvalue_limit=0.010000'])}\n    lr  =  mpmr.LinearReg(X,y,given_cols=['informative_1','useless_1'],iter_num=10,max_pvalue_limit=0.01)\n    in_vars,clf_final,dr = lr.fit()\n    return in_vars,clf_final,dr\n\nif __name__ == '__main__':    \n    X_logit, y_logit = get_X_y('logistic')\n    in_vars_logit,clf_final_logit,dr_logit = test_logit(X_logit,y_logit)\n    \n    X_linear, y_linear = get_X_y('linear',500)\n    in_vars_linear,rg_final_linear,dr_linear = test_linear(X_linear,y_linear)\n    \n    X_measure,y_measure = get_X_y('logistic',500)   \n    in_vars_measure,clf_final_measure,dr_measure = test_measureXy(X_measure,y_measure)\n    \n    X_given, y_given = get_X_y('logistic')\n    in_vars_given,clf_final_given,dr_given = test_givenX_logistic(X_given,y_given)\n    \n    X_given, y_given = get_X_y('linear',500)\n    in_vars_given,rg_final_given,dr_given = test_givenX_linear(X_given,y_given)\n``` \n# Document and API\n## class  \n## class MultiProcessMStepRegression.LinearReg(MultiProcessMStepRegression.Reg_Sup_Step_Wise_MP.Regression) \n\nA Step-Wise Linear Regression handling with multi-processing.It bases on statsmodels.api.OLS or statsmodels.api.WLS supplying a linear regression algorithm.Which algorithm should be used depends on the setting of train sample weight.   \nIn adding feature process,multi-processing is used to traversal several features concurrently.The feature which meets the conditions which the user set and get a max lift on measure index is added in the model.If any feature can\\`t improve the performance of model undering the conditions set by user ,no feature is added in current iteration.   \nThe removing feature process has same policy with adding feature process to decide which feature should be removed.  \nWhen adding process, if there is improving on performance of model but some conditions user set are missed,a additional removing process will start to run.If the feature to remove is same with the feature to add,the feature will not be added and the adding process is over.If They are not same,the feature to add is added in and the feature to remove is excluded from current list in which the picked features stay.The additional removing process has same procedure with removing process.   \nWhen modeling is compeleted,the features not picked up will respectively be added in picked features list. And then by rebuilding model with those features,a exact deletion reasons will return.   \n\nNote:As X and y is a property in a instance of MultiProcessMStepRegression.LinearReg class,so that instance will be very large.Saving that instance is not recommended instead of saving the returned model and remove reasons.  \n### __init__ function \n### LinearReg(self,X,y,given_cols=[],fit_weight=None,measure='r2',measure_weight=None,measure_X=None,measure_y=None,kw_measure_args=None,max_pvalue_limit=0.05,max_vif_limit=3,max_corr_limit=0.6,coef_sign=None,iter_num=20,kw_algorithm_class_args=None,n_core=None,logger_file_CH=None,logger_file_EN=None)    \n    \n#### Parameters  \n\nX:DataFrame  \nfeatures  \n \ny:Series  \ntarget  \n\ngiven_cols:list | [](Default) New in 0.3.1\nThe features appointed by user to be imoprted by model.\n \nfit_weight:Series  \nThe length of fit_weight is same with length of y.The fit_weight is for trainning data.If None(default),every sample has a same trainning weight and statsmodels.api.OLS is used as base linear algorithm.If not None,statsmodels.api.WLS is used as base linear algorithm.In linear regression,the goal of setting weight is for getting a stable model with Heteroscedasticity.  \n \nmeasure:str r2(default) | explained_variance_score | max_error   \nPerformance evaluate function.The y_true,y_hat and measure_weight will be put into measure function automatically and the other parameters will be put into measure function with kw_measure_args  \n \nmeasure_weight:Series  \nThe length of measure_weight is same with length of y.The measure_weight is for measuring function.If None(default),every sample has a same measuring weight.  \nSee also fit_weight  \n\nmeasure_X:DataFrame | None(Default) New in 0.2.1  \nWhen selecting feature,use a data set different with trainning X to evaluate model.  \nDefault is None that means trainning X is used for evaluating model.  \n\nmeasure_y:Series | None(Default) New in 0.2.1  \nWhen selecting feature,use a data set different with trainning y to evaluate model.  \nDefault is None that means trainning y is used for evaluating model.  \nNote:Try best to avoid using validation data set and test data set with confusing.When selecting feature,it means data leak that using test data set,so that may over evaludate model performance.It shoule reserve a cleaning test data set to evaluate model finally.   \n \nkw_measure_args:dict | None(default)  \nExcept y_true,y_hat and measure_weight,the other parameters need be put in kw_measure_args to deliver into measure function.  \nNone means that no other parameters delivers into measure function.  \n \nmax_pvalue_limit:float  \nThe max P-VALUE limit.0.05(default)  \n \nmax_vif_limit:float\nThe max VIF limit.3(default)  \n \nmax_corr_limit:float  \nThe max coefficient of correlation limit.0.6(default)  \n \ncoef_sign:'+','-',dict,None（default）  \nIf the user have a priori knowledge on relation between X and y,like positive correlation or negtive correlation,user can make a constraint restriction on sign of resression coefficient by this parameter.  \n'+':all signs of resression coefficients(not in given_cols)  are positive  \n'-':all signs of resression coefficients(not in given_cols)  are negtive  \ndict:the format is as {'x_name1':'+','x_name2':'-'}.Put coefficient and coefficient\\`s sign on which you have a priori knowledge into a dict and then constraint these signs that are in this dict. The coefficients not included in this dict will not be constrainted.  \nNone:all coefficients are not constrainted.  \n    \niter_num:int   \nThe iteration num for picking features.Default is 20.When np.inf,no limit to iteration num,if features are many,then the running time is long.If all features are already picked in model or no imporve on perfermance by adding/removing any feature,the actual iteration num should be samller than iter_num.The steps inclueed in every iteration is:1.Try adding feature which is not added in current model yet and then pick up one feature that makes most promotion for performance of model with satisfying user\\`s setting. 2.Try removing feature and then remove out one feature that makes most promotion for performance of model with satisfying user\\`s setting.It is means finshing one time iteration that step 1 and step 2 is completed.If all step 1 and step 2 can\\`t pick up any feature then iteration is pre-terminated,no matter whether iter_num is reached.  \n    \nkw_algorithm_class_args:dict  \nExcept X,y,fit_weight,the other parameters that are delivered into linear regression algorithm is in kw_algorithm_class_args  \nNote:y,X is called endog and exog in statsmodels.genmod.generalized_linear_model.GLM  \n \nn_core:int | float | None  \nCount of CPU processing.If int,user point the count.If float,the count is as percentage of all count transfered to int(ceil).If None(default),all count of CPU processing -1.  \n \nlogger_file_CH:str  \nA log file name where log for step-wise procedure is recorded in Chinese.If None(default),not recording Chinese log.  \n \nlogger_file_EN:str  \nA log file name where log for step-wise procedure is recorded in English.If None(default),not recording English log.  \n### method \n### LinearReg.fit(self)  \nFitting a model  \n\n#### Returns  \n\nin_vars : list  \nAll variables to be picked up by model.The order in list is same with the order of to be added  \n    \nclf_final :statsmodels.regression.linear_model.RegressionResultsWrapper  \nA final step-wise model  \n \ndr : dict  \ndeletion reason.It\\`s format is {'var_name':([...],[...])}  \nEvery value in dr contains a tuple including two elements.The first element is reason in Chinese and the second in English.Every element is a list and record all deletion reason of variable(matching key).Some features should be added into model manually,if a list corresponding these features has no any element.  \n\n## class  \n## class MultiProcessMStepRegression.LogisticReg(MultiProcessMStepRegression.Reg_Sup_Step_Wise_MP.Regression)  \nMultiProcessMStepRegression.LogisticReg:A Step-Wise Logistic Regression handling with multi-processing.It bases on statsmodels.genmod.generalized_linear_model.GLM supplying a logistic regression algorithm  \nIn adding feature process,multi-processing is used to traversal several features concurrently.The feature which meets the conditions which the user set and get a max lift on measure index is added in the model.If any feature can\\`t improve the performance of model undering the conditions set by user ,no feature is added in current iteration. \nThe removing feature process has same policy with adding feature process to decide which feature should be removed.\nWhen adding process, if there is improving on performance of model but some conditions user set are missed,a additional removing process will start to run.If the feature to remove is same with the feature to add,the feature will not be added and the adding process is over.If They are not same,the feature to add is added in and the feature to remove is excluded from current list in which the picked features stay.The additional removing process has same procedure with removing process. \nWhen modeling is compeleted,the features not picked up will respectively be added in picked features list. And then by rebuilding model with those features,a exact deletion reasons will return. \n\nNote:As X and y is a property in a instance of MultiProcessMStepRegression.LogisticReg class,so that instance will be very large.Saving that instance is not recommended instead of saving the returned model and remove reasons.\n\n### __init__ function\n### LogisticReg(self,X,y,given_cols=[],fit_weight=None,measure='ks',measure_weight=None,measure_frac=None,measure_X=None,measure_y=None,kw_measure_args=None,max_pvalue_limit=0.05,max_vif_limit=3,max_corr_limit=0.6,coef_sign=None,iter_num=20,kw_algorithm_class_args=None,n_core=None,logger_file_CH=None,logger_file_EN=None)\n\n#### Parameters  \nX:DataFrame  \nfeatures  \n \ny:Series  \ntarget \n\ngiven_cols:list  | [](Default) New in 0.3.1\nThe features appointed by user to be imoprted by model.\n    \nfit_weight:Series  \nThe length of fit_weight is same with length of y.The fit_weight is for trainning data.If None(default),every sample has a same trainning weight.Don\\`t confuse fit_weight with measure_weight(mentioned below) that is for measuring model.It depends on user\\`s design on sample whether fit_weight is same with measure_weight or not.For example,for reducing effect from large class sample,it\\`s a good way to improve weights of small class sample when trainning model but the weight between large class sample and small class sample returns back to original weight value when measuring with some index like KS or ROC_AUC.Why doing like this is that the lost function of regression is large class sensitive.So the user need adjust sample weights.Some index like KS or ROC_AUC,their calculate way is non-sensitive in unbalanced sample situation,so the user need not adjust sample weights unless the user thinks that the loss penalty between samples is different.  \nnote:Although the user set measure='KS' or 'ROC_AUC' to measure performance and pick features,but the MultiProcessMStepRegression.LogisticReg is still large class sensitive,due to the base algorithm is standard logistic regression yet.  \n \nmeasure:str ks(default) | accuracy | roc_auc | balanced_accuracy | average_precision  \nPerformance evaluate function.The y_true,y_hat and measure_weight will be put into measure function automatically and the other parameters will be put into measure function with kw_measure_args  \n \nmeasure_weight:Series  \nThe length of measure_weight is same with length of y.The measure_weight is for measuring function.If None(default),every sample has a same measuring weight.  \nSee also fit_weight  \n\nmeasure_frac:float | None(Default) New in 0.2.1  \nThe percent of sample used to evaluate model.  \nSorts the probability output by model as descending and then takes top(or last) measure_frac sample to evaluate model.  \nIf measure_frac>0,top measure_frac else last abs(measure_frac).  \nIt can be used as promoting ability catching some one class sample.For example in credit risk,raises proportion of overdue user in high risk sublevel(LIFT 1 or LIFT 5).  \nDefault is None that means all samples are taken.  \n\nmeasure_X:DataFrame | None(Default) New in 0.2.1  \nWhen selecting feature,use a data set different with trainning X to evaluate model.  \nDefault is None that means trainning X is used for evaluating model.  \n\nmeasure_y:Series | None(Default) New in 0.2.1  \nWhen selecting feature,use a data set different with trainning y to evaluate model.  \nDefault is None that means trainning y is used for evaluating model.  \nNote:Try best to avoid using validation data set and test data set with confusing.When selecting feature,it means data leak that using test data set,so that may over evaludate model performance.It shoule reserve a cleaning test data set to evaluate model finally.  \n    \nkw_measure_args:dict | None(default)  \nExcept y_true,y_hat and measure_weight,the other parameters need be put in kw_measure_args to deliver into measure function.  \nNone means that no other parameters delivers into measure function.  \n \nmax_pvalue_limit:float  \nThe max P-VALUE limit.0.05(default)  \n \nmax_vif_limit:float\nThe max VIF limit.3(default)  \n \nmax_corr_limit:float\nThe max coefficient of correlation limit. 0.6(default)  \n \ncoef_sign:'+','-',dict,None（default）  \nIf the user have a priori knowledge on relation between X and y,like positive correlation or negtive correlation,user can make a constraint restriction on sign of resression coefficient by this parameter.  \n'+':all signs of resression coefficients(not in given_cols)  are positive  \n'-':all signs of resression coefficients(not in given_cols)  are negtive  \ndict:the format is as {'x_name1':'+','x_name2':'-'}.Put coefficient and coefficient\\`s sign on which you have a priori knowledge into a dict and then constraint these signs that are in this dict. The coefficients not included in this dict will not be constrainted.  \nNone:all coefficients are not constrainted.  \n    \niter_num:int  \nThe iteration num for picking features.Default is 20.When np.inf,no limit to iteration num,if features are many,then the running time is long.If all features are already picked in model or no imporve on perfermance by adding/removing any feature,the actual iteration num should be samller than iter_num.The steps inclueed in every iteration is:1.Try adding feature which is not added in current model yet and then pick up one feature that makes most promotion for performance of model with satisfying user\\`s setting. 2.Try removing feature and then remove out one feature that makes most promotion for performance of model with satisfying user\\`s setting.It is means finshing one time iteration that step 1 and step 2 is completed.If all step 1 and step 2 can\\`t pick up any feature then iteration is pre-terminated,no matter whether iter_num is reached.  \n    \nkw_algorithm_class_args:dict  \nExcept X，y，fit_weight,the other parameters that are delivered into logistic regression algorithm is in kw_algorithm_class_args  \nNote:y,X is called endog and exog in statsmodels.genmod.generalized_linear_model.GLM  \n \nn_core:int | float | None  \nCount of CPU processing.If int,user point the count.If float,the count is as percentage of all count transfered to int(ceil).If None(default),all count of CPU processing -1.  \n \nlogger_file_CH:str  \nA log file name where log for step-wise procedure is recorded in Chinese.If None(default),not recording Chinese log.  \n \nlogger_file_EN:str  \nA log file name where log for step-wise procedure is recorded in English.If None(default),not recording English log.  \n \n### method \n### LogisticReg.fit(self)  \nFitting a model  \n\n#### Returns  \n\nin_vars : list  \nAll variables to be picked up by model.The order in list is same with the order of to be added  \n    \nclf_final : statsmodels.genmod.generalized_linear_model.GLMResultsWrapper  \nA final step-wise model  \n \ndr : dict  \ndeletion reason.It\\`s format is {'var_name':([...],[...])}  \nEvery value in dr contains a tuple including two elements.The first element is reason in Chinese and the second in English.Every element is a list and record all deletion reason of variable(matching key).Some features should be added into model manually,if a list corresponding these features has no any element.  \n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/wangwenhao-DATA-OG/MultiProcessMStepRegression",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "MultiProcessMStepRegression",
    "package_url": "https://pypi.org/project/MultiProcessMStepRegression/",
    "platform": null,
    "project_url": "https://pypi.org/project/MultiProcessMStepRegression/",
    "project_urls": {
      "Homepage": "https://github.com/wangwenhao-DATA-OG/MultiProcessMStepRegression"
    },
    "release_url": "https://pypi.org/project/MultiProcessMStepRegression/0.4.1/",
    "requires_dist": [
      "scikit-learn (>=0.20.4)",
      "statsmodels (>=0.10.0)"
    ],
    "requires_python": ">=3.4",
    "summary": "python多进程逐步回归。python step-wise regression with multi-processing.",
    "version": "0.4.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17415457,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d4d7681dcb4d2fd23ef7544d04bdcf7a581a43ac39347cf184ec2c4abaded4fa",
          "md5": "37341e5ad5c3dc427fc576cb89dac8ad",
          "sha256": "b6ef50dd883e88cb03ceffe6cd555649fbfa41663940522c41ed5e0e73816563"
        },
        "downloads": -1,
        "filename": "MultiProcessMStepRegression-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "37341e5ad5c3dc427fc576cb89dac8ad",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.4",
        "size": 28099,
        "upload_time": "2021-12-27T02:16:45",
        "upload_time_iso_8601": "2021-12-27T02:16:45.607333Z",
        "url": "https://files.pythonhosted.org/packages/d4/d7/681dcb4d2fd23ef7544d04bdcf7a581a43ac39347cf184ec2c4abaded4fa/MultiProcessMStepRegression-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "582ebbb05a47f70466e346f0abc0b2b9f4ebbe8e3abc598e0cee0e55721be0c4",
          "md5": "332445746461c55ae9f2cbb4e8693311",
          "sha256": "0195d6fb5f21b151032f61e4ebc56934da57a4dfcc0ce87c8433c09dae5d2f22"
        },
        "downloads": -1,
        "filename": "MultiProcessMStepRegression-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "332445746461c55ae9f2cbb4e8693311",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.4",
        "size": 41809,
        "upload_time": "2021-12-27T02:16:47",
        "upload_time_iso_8601": "2021-12-27T02:16:47.549202Z",
        "url": "https://files.pythonhosted.org/packages/58/2e/bbb05a47f70466e346f0abc0b2b9f4ebbe8e3abc598e0cee0e55721be0c4/MultiProcessMStepRegression-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7b9c453a555c59a1a5d936588c646986c20b25d8faf72297dd5ec2ce763d9f2e",
          "md5": "c1140d08381c272cda663a657a19131b",
          "sha256": "50ed6806a714bae6cfb5f70bba952f15d606a8c3c5ca7ea0f8bd6123e39840f4"
        },
        "downloads": -1,
        "filename": "MultiProcessMStepRegression-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c1140d08381c272cda663a657a19131b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.4",
        "size": 32557,
        "upload_time": "2021-12-27T02:11:20",
        "upload_time_iso_8601": "2021-12-27T02:11:20.626439Z",
        "url": "https://files.pythonhosted.org/packages/7b/9c/453a555c59a1a5d936588c646986c20b25d8faf72297dd5ec2ce763d9f2e/MultiProcessMStepRegression-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ecf7e59769817701fde0221a6d448566f904a512f9b59d7c3727db998bc56fae",
          "md5": "d8c8b1d608177ecf6bea1a6b7c632b68",
          "sha256": "b6b0cce9eb6f3e3163a2b84344ad54e328b3d7b9754bf876864f8a4d7f06e4ab"
        },
        "downloads": -1,
        "filename": "MultiProcessMStepRegression-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "d8c8b1d608177ecf6bea1a6b7c632b68",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.4",
        "size": 49362,
        "upload_time": "2021-12-27T02:11:22",
        "upload_time_iso_8601": "2021-12-27T02:11:22.769027Z",
        "url": "https://files.pythonhosted.org/packages/ec/f7/e59769817701fde0221a6d448566f904a512f9b59d7c3727db998bc56fae/MultiProcessMStepRegression-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "898249cb12569eadce739a6aae5f43a466d416449c217a3258863eb2a2d6ec04",
          "md5": "eeb284fe7b4d64862c4405aba74b5624",
          "sha256": "c7299779966b188e8302f6271e31bb3d9337a78eb79fc2d533da65bd4bc5dc10"
        },
        "downloads": -1,
        "filename": "MultiProcessMStepRegression-0.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eeb284fe7b4d64862c4405aba74b5624",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.4",
        "size": 18082,
        "upload_time": "2022-05-18T05:27:55",
        "upload_time_iso_8601": "2022-05-18T05:27:55.990236Z",
        "url": "https://files.pythonhosted.org/packages/89/82/49cb12569eadce739a6aae5f43a466d416449c217a3258863eb2a2d6ec04/MultiProcessMStepRegression-0.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b263e7164bb1215147ffdd745bf2e7a161fb0ce767f608f89be5291dd93c3e5d",
          "md5": "f236d1d61c392b52dc84f5fb74832219",
          "sha256": "493dcbe3569e78dfb46d6029c997e309dee6600ab6c44c86c952be8a14ec1492"
        },
        "downloads": -1,
        "filename": "MultiProcessMStepRegression-0.3.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f236d1d61c392b52dc84f5fb74832219",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.4",
        "size": 18080,
        "upload_time": "2022-06-02T09:09:53",
        "upload_time_iso_8601": "2022-06-02T09:09:53.594681Z",
        "url": "https://files.pythonhosted.org/packages/b2/63/e7164bb1215147ffdd745bf2e7a161fb0ce767f608f89be5291dd93c3e5d/MultiProcessMStepRegression-0.3.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c654c205ddde16dc7e040c9aa88cc0a4a51cb36d6528e08f63df576c6bc17333",
          "md5": "fcba591b49645cb009b723436aa710ed",
          "sha256": "766dfa2bd786a548a4c24227e84426cfc04cfa402c184365e30824b1aed77d42"
        },
        "downloads": -1,
        "filename": "MultiProcessMStepRegression-0.4.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fcba591b49645cb009b723436aa710ed",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.4",
        "size": 18050,
        "upload_time": "2023-03-23T13:05:30",
        "upload_time_iso_8601": "2023-03-23T13:05:30.080412Z",
        "url": "https://files.pythonhosted.org/packages/c6/54/c205ddde16dc7e040c9aa88cc0a4a51cb36d6528e08f63df576c6bc17333/MultiProcessMStepRegression-0.4.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c654c205ddde16dc7e040c9aa88cc0a4a51cb36d6528e08f63df576c6bc17333",
        "md5": "fcba591b49645cb009b723436aa710ed",
        "sha256": "766dfa2bd786a548a4c24227e84426cfc04cfa402c184365e30824b1aed77d42"
      },
      "downloads": -1,
      "filename": "MultiProcessMStepRegression-0.4.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fcba591b49645cb009b723436aa710ed",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.4",
      "size": 18050,
      "upload_time": "2023-03-23T13:05:30",
      "upload_time_iso_8601": "2023-03-23T13:05:30.080412Z",
      "url": "https://files.pythonhosted.org/packages/c6/54/c205ddde16dc7e040c9aa88cc0a4a51cb36d6528e08f63df576c6bc17333/MultiProcessMStepRegression-0.4.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}