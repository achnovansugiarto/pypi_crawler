{
  "info": {
    "author": "Å½iga Avsec",
    "author_email": "avsec@in.tum.de",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# kopt - Hyper-parameter optimization for Keras\n\n[![Build Status](https://travis-ci.org/Avsecz/keras-hyperopt.svg?branch=master)](https://travis-ci.org/avsecz/keras-hyperopt)\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)](https://github.com/avsecz/keras-hyperopt/blob/master/LICENSE)\n\nkopt is a hyper-parameter optimization library for Keras. It is based on [hyperopt](https://github.com/hyperopt/hyperopt).\n\n## Getting started\n\nHere is an example of hyper-parameter optimization for the Keras IMDB\nexample model.\n\n```python\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nimport keras.layers as kl\nfrom keras.optimizers import Adam\n# kopt and hyoperot imports\nfrom kopt import CompileFN, KMongoTrials, test_fn\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n\n# 1. define the data function returning training, (validation, test) data\ndef data(max_features=5000, maxlen=80):\n    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n    x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n    x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n    return (x_train[:100], y_train[:100], max_features), (x_test, y_test)\n\n\n# 2. Define the model function returning a compiled Keras model\ndef model(train_data, lr=0.001,\n          embedding_dims=128, rnn_units=64,\n          dropout=0.2):\n\t# extract data dimensions\n    max_features = train_data[2]\n\n    model = Sequential()\n    model.add(kl.Embedding(max_features, embedding_dims))\n    model.add(kl.LSTM(rnn_units, dropout=dropout, recurrent_dropout=dropout))\n    model.add(kl.Dense(1, activation='sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(lr=lr),\n                  metrics=['accuracy'])\n    return model\n\n# Specify the optimization metrics\ndb_name=\"imdb\"\nexp_name=\"myexp1\"\nobjective = CompileFN(db_name, exp_name,\n                      data_fn=data,\n                      model_fn=model,\n                      loss_metric=\"acc\", # which metric to optimize for\n                      loss_metric_mode=\"max\",  # try to maximize the metric\n                      valid_split=.2, # use 20% of the training data for the validation set\n                      save_model='best', # checkpoint the best model\n                      save_results=True, # save the results as .json (in addition to mongoDB)\n                      save_dir=\"./saved_models/\")  # place to store the models\n\n# define the hyper-parameter ranges\n# see https://github.com/hyperopt/hyperopt/wiki/FMin for more info\nhyper_params = {\n\t\"data\": {\n\t    \"max_features\": 100,\n\t\t\"maxlen\": 80,\n\t},\n\t\"model\": {\n     \t\"lr\": hp.loguniform(\"m_lr\", np.log(1e-4), np.log(1e-2)), # 0.0001 - 0.01\n\t    \"embedding_dims\": hp.choice(\"m_emb\", (64, 128)),\n\t    \"rnn_units\": 64,\n\t\t\"dropout\": hp.uniform(\"m_do\", 0, 0.5),\n\t},\n\t\"fit\": {\n\t    \"epochs\": 20\n\t}\n}\n\n# test model training, on a small subset for one epoch\ntest_fn(objective, hyper_params)\n\n# run hyper-parameter optimization sequentially (without any database)\ntrials = Trials()\nbest = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n\n# run hyper-parameter optimization in parallel (saving the results to MonogoDB)\n# Follow the hyperopt guide:\n# https://github.com/hyperopt/hyperopt/wiki/Parallelizing-Evaluations-During-Search-via-MongoDB\n# KMongoTrials extends hyperopt.MongoTrials with convenience methods\ntrials = KMongoTrials(db_name, exp_name,\n                      ip=\"localhost\",\n\t              port=22334)\nbest = fmin(objective, hyper_params, trials=trials, algo=tpe.suggest, max_evals=2)\n```\n\n## See also\n\n- [nbs/imdb_example.ipynb](nbs/imdb_example.ipynb)\n\nThe documentation of `concise.hyopt` (`kopt` was ported from `concise.hyopt`):\n\n- [Tutorial](https://i12g-gagneurweb.in.tum.de/public/docs/concise/tutorials/hyper-parameter_optimization/)\n- [API documentation](https://i12g-gagneurweb.in.tum.de/public/docs/concise/hyopt/)\n- [Jupyter notebook](https://github.com/gagneurlab/concise/blob/master/nbs/hyper-parameter_optimization.ipynb)\n",
    "description_content_type": null,
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/avsecz/keras-hyperopt",
    "keywords": "hyper-parameter tuning",
    "license": "MIT license",
    "maintainer": "",
    "maintainer_email": "",
    "name": "kopt",
    "package_url": "https://pypi.org/project/kopt/",
    "platform": "",
    "project_url": "https://pypi.org/project/kopt/",
    "project_urls": {
      "Homepage": "https://github.com/avsecz/keras-hyperopt"
    },
    "release_url": "https://pypi.org/project/kopt/0.1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Keras-hyperopt (kopt); Hyper-parameter tuning for Keras using hyperopt.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 3648066,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a7e2650da402870396bcf6660e98e55499f6ec7d05c8443f3bfb2c0e64dda172",
          "md5": "18b48e691e7fe5d40fde2210a427539f",
          "sha256": "d90fe5f63dbf89dff514d0903c5cc933c5147e7bb7b109c2ad1d57b6505c7e85"
        },
        "downloads": -1,
        "filename": "kopt-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "18b48e691e7fe5d40fde2210a427539f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18510,
        "upload_time": "2018-03-07T15:05:15",
        "upload_time_iso_8601": "2018-03-07T15:05:15.612798Z",
        "url": "https://files.pythonhosted.org/packages/a7/e2/650da402870396bcf6660e98e55499f6ec7d05c8443f3bfb2c0e64dda172/kopt-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a7e2650da402870396bcf6660e98e55499f6ec7d05c8443f3bfb2c0e64dda172",
        "md5": "18b48e691e7fe5d40fde2210a427539f",
        "sha256": "d90fe5f63dbf89dff514d0903c5cc933c5147e7bb7b109c2ad1d57b6505c7e85"
      },
      "downloads": -1,
      "filename": "kopt-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "18b48e691e7fe5d40fde2210a427539f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 18510,
      "upload_time": "2018-03-07T15:05:15",
      "upload_time_iso_8601": "2018-03-07T15:05:15.612798Z",
      "url": "https://files.pythonhosted.org/packages/a7/e2/650da402870396bcf6660e98e55499f6ec7d05c8443f3bfb2c0e64dda172/kopt-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}