{
  "info": {
    "author": "Giovanni Pizzi",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "License :: OSI Approved :: MIT License",
      "Operating System :: MacOS :: MacOS X",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Programming Language :: Python :: Implementation :: CPython",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# disk-objectstore\n\nAn implementation of an efficient object store that writes directly on disk\nand does not require a server running.\n\n|                |                                                                                                                                         |\n| -------------- | --------------------------------------------------------------------------------------------------------------------------------------- |\n| Latest release | [![PyPI version][pypi-badge]][pypi-link] [![PyPI pyversions][pypi-pyversions]][pypi-link]                                               |\n| Build status   | [![Build Status][build-badge]](https://github.com/aiidateam/disk-objectstore/actions) [![Coverage Status][codecov-badge]][codecov-link] |\n| Performance    | [Benchmarks][bench-link]                                                                                                                |\n\n## Goal\n\nThe goal of this project is to have a very efficient implementation of an \"object store\"\nthat works directly on a disk folder, does not require a server to run, and addresses\na number of performance issues, discussed also below.\n\nThis project targets objects that range from very few bytes up to tens of GB each, with\nperformance tuned to support tens of millions of objects or more.\n\nThis project originated from the requirements needed by an efficient repository\nimplementation in [AiiDA](http://www.aiida.net) (note, however, that this\npackage is completely independent of AiiDA).\n\n## How to install\n\nTo install, run:\n\n```\npip install disk-objectstore\n```\n\nTo install in development mode, run, after checking out, in a (python 3) virtual environment:\n\n```\npip install -e .[examples,dev]\n```\n\n## Basic API usage\n\nLet us run a quick demo of how to store and retrieve objects in a container:\n\n```python\nfrom disk_objectstore import Container\n\n# Let's create a new container in the local folder `temp_container`, and initialise it\ncontainer = Container(\"temp_container\")\ncontainer.init_container(clear=True)\n\n# Let's add two objects\nhash1 = container.add_object(b\"some_content\")\nhash2 = container.add_object(b\"some_other_content\")\n\n# Let's look at the hashes\nprint(hash1)\n# Output: 6a96df63699b6fdc947177979dfd37a099c705bc509a715060dbfd3b7b605dbe\nprint(hash2)\n# Output: cfb487fe419250aa790bf7189962581651305fc8c42d6c16b72384f96299199d\n\n# Let's retrieve the objects from the hash\ncontainer.get_object_content(hash1)\n# Output: b'some_content'\ncontainer.get_object_content(hash2)\n# Output: b'some_other_content'\n\n# Let's add a new object with the same content of an existing one: it will get the same\n# hash and will not be stored twice\nhash1bis = container.add_object(b\"some_content\")\nassert hash1bis == hash1\n\n# Let's pack all objects: instead of having a lot of files, one per object, all objects\n# are written in a few big files (great for performance, e.g. when using rsync) +\n# internally a SQLite database is used to know where each object is in the pack files\ncontainer.pack_all_loose()\n\n# After packing, everthing works as before\ncontainer.get_object_content(hash2)\n# Output: b'some_other_content'\n\n# This third object will be stored as loose\nhash3 = container.add_object(b\"third_content\")\n```\n\n## CLI usage\n\nThe package comes with a CLI tool that can be used to interact with a container.\n\n```console\n$ dostore\nUsage: dostore [OPTIONS] COMMAND [ARGS]...\n\n  Manage a disk objectstore\n\nOptions:\n  --version        Show the version and exit.\n  -p, --path TEXT  Path to the container (or set env DOSTORE_PATH)\n                   [default: /path/to/dostore]\n  --help           Show this message and exit.\n\nCommands:\n  add-files  Add file(s) to the container\n  create     Create a container\n  optimize   Optimize the container's memory use\n  status     Print details about the container\n```\n\nCreate a container:\n\n```console\n$ dostore create\nCreated container: /path/to/dostore\n```\n\nInspect the container:\n\n```console\n$ dostore status\n{\n  \"path\": \"/path/to/dostore\",\n  \"id\": \"da81094c07ac4ae9aa730d6b59fe353a\",\n  \"compression\": \"zlib+1\",\n  \"count\": {\n    \"packed\": 0,\n    \"loose\": 0,\n    \"pack_files\": 0\n  },\n  \"size\": {\n    \"total_size_packed\": 0,\n    \"total_size_packed_on_disk\": 0,\n    \"total_size_packfiles_on_disk\": 0,\n    \"total_size_packindexes_on_disk\": 12288,\n    \"total_size_loose\": 0\n  }\n}\n```\n\nAdd files to the container:\n\n```console\n$ dostore add-files README.md CHANGELOG.md\nAdding 2 file(s) to container: /path/to/dostore\n4911fc17759f2260e7674094eadb71b882ec50de2b771fda0410b19501862071: README.md\nc19f337aff836a2a894333a55713ce29e31ab8091b4c55c6654e7e8e5c8e0fa7: CHANGELOG.md\n```\n\nOptimize memory usage of the container (i.e. pack objects):\n\n```console\n$ dostore optimize\nIs this the only process accessing the container? [y/N]: y\nInitial container size: 39.33 Mb\nFinal container size: 23.91 Mb\n```\n\n## Advanced usage\n\nThis repository is designed both for performance and for having a low memory footprint.\nTherefore, it provides bulk operations and the possibility to access objects as streams.\nWe **strongly suggest** to use these methods if you use the `disk-objecstore` as a library,\nunless you are absolutely sure that objects always fit in memory, and you never have to\naccess tens of thousands of objects or more.\n\n### Bulk access\n\nWe continue from the commands of the basic usage. We can get the content of more objects at once:\n\n```python\ncontainer.get_objects_content([hash1, hash2])\n# Output: {'6a96df63699b6fdc947177979dfd37a099c705bc509a715060dbfd3b7b605dbe': b'some_content',  'cfb487fe419250aa790bf7189962581651305fc8c42d6c16b72384f96299199d': b'some_other_content'}\n```\n\nFor many objects (especially if they are packed), retrieving in bulk can give orders-of-magnitude speed-up.\n\n### Using streams\n\n#### Interface\n\nFirst, let's look at the interface:\n\n```python\nwith container.get_object_stream(hash1) as stream:\n    print(stream.read())\n# Output: b'some_content'\n```\n\nFor bulk access, the syntax is a bit more convoluted (the reason is efficiency, as discussed below):\n\n```python\nwith container.get_objects_stream_and_meta([hash3, hash1, hash2]) as triplets:\n    for hashkey, stream, meta in triplets:\n        print(\"Meta for hashkey {}: {}\".format(hashkey, meta))\n        print(\"  Content: {}\".format(stream.read()))\n```\n\nwhose output is:\n\n```\nMeta for hashkey 6a96df63699b6fdc947177979dfd37a099c705bc509a715060dbfd3b7b605dbe: {'type': 'packed', 'size': 12, 'pack_id': 0, 'pack_compressed': False, 'pack_offset': 0, 'pack_length': 12}\n  Content: b'some_content'\nMeta for hashkey cfb487fe419250aa790bf7189962581651305fc8c42d6c16b72384f96299199d: {'type': 'packed', 'size': 18, 'pack_id': 0, 'pack_compressed': False, 'pack_offset': 12, 'pack_length': 18}\n  Content: b'some_other_content'\nMeta for hashkey d1e4103ce093e26c63ce25366a9a131d60d3555073b8424d3322accefc36bf08: {'type': 'loose', 'size': 13, 'pack_id': None, 'pack_compressed': None, 'pack_offset': None, 'pack_length': None}\n  Content: b'third_content'\n```\n\n**IMPORTANT NOTE**: As you see above, the order of the triplets **IS NOT** the order in which you passed the hash keys to\n`get_objects_stream_and_meta`. The reason is efficiency: the library will try to keep a (pack) file open as long as possible, and read it in order, to exploit efficiently disk caches.\n\n#### Memory-savvy approach\n\nIf you don't know the size of the object, you don't want to just call `stream.read()` (you could have just called `get_object_content()` in that case!) because if the object does not fit in memory, your application will crash.\nYou will need to read it in chunks and process it chunk by chunk.\n\nA very simple pattern:\n\n```python\n# The optimal chunk size depends on your application and needs some benchmarking\nCHUNK_SIZE = 100000\nwith container.get_object_stream(hash1) as stream:\n    chunk = stream.read(CHUNK_SIZE)\n    while chunk:\n        # process chunk here\n        # E.g. write to a different file, pass to a method to compress it, ...\n        chunk = stream.read(CHUNK_SIZE)\n```\n\nYou can find various examples of this pattern in the utility wrapper classes in `disk_objectstore.utils`.\n\nNote also that if you use `get_objects_stream_and_meta`, you can use `meta['size']` to know the size\nof the object before starting to read, so you can e.g. simply do a `.read()` if you know the size is small.\n\nFinally, when writing objects, if the objects are big, instead of reading in memory the whole content, you should use\nthe methods `container.add_streamed_object(stream)` (loose objects) or `add_streamed_objects_to_pack(stream_list)`\n(directly to packs).\n\n## Packing\n\nAs said above, from the user point of view, accessing a `Container` where objects are all loose, all packed, or partially loose and partially packed, does not change anything from the user-interface point of view, but performance might improve a lot after packing.\n\nNote that only one process can pack (or write in packs in general) at a given time, while any number of\nprocesses can write concurrently loose objects, and read objects (both loose and packed).\n\nThe continuous integration tests check also that any number of processes can continue to write concurrently loose objects and read from packs even while a *single process* is performing the packing operation.\n\nFinally, in specific applications (for which you have to write a lot of objects, and you know that there\nare no concurrent processes accessing the packs) you can directly write to the packs for performance reasons.\n\nThe interface is the following:\n\n```python\ncontainer.add_objects_to_pack([b\"obj1\", b\"obj2\"])\n# Output: ['7e485fc048df85f62cb1ec17174072380519e3064a0510ec00daaa381a680942', '71d00f404e92546cba0e69b27b13394af4592e4da22bf24c58a95ec3f4f45584']\n```\n\nor, better, for big objects using streams, you can use `add_streamed_objects_to_pack`.\n\nAs an example, let's create two files:\n\n```python\nwith open(\"file1.txt\", \"wb\") as fhandle:\n    fhandle.write(b\"file1content\")\nwith open(\"file2.txt\", \"wb\") as fhandle:\n    fhandle.write(b\"file2content\")\n```\n\nNow you can exploit the `LazyOpener` wrapper to lazily create handles to files, that are actually open only when accessed.\nLet's now add their content to the `Container`, in a way that works even for TB files without filling up all your RAM:\n\n```python\nfrom disk_objectstore.utils import LazyOpener\n\ncontainer.add_streamed_objects_to_pack(\n    [LazyOpener(\"file1.txt\"), LazyOpener(\"file2.txt\")], open_streams=True\n)\n```\n\nOutput:\n\n```python\n[\n    \"ce3e75d02effb66eda58779e3b0f9e454aad218b9d5a38903a105f177f2dde23\",\n    \"eeeb27c2f0348e327ec8e66e7f5667798df601e6d1c62209dde749d370732a48\",\n]\n```\n\nNote that we use the `LazyOpener` here, because there is an operating-system limit on the number of\nopen files you can have at the same time (and this limit is quite low e.g. on Mac OS). The snippet above works with any number of files thanks to the use of the `LazyOpener` and the fact that `add_streamed_objects_to_pack()` will open the files only when needed (thanks to the `open_streams=True` parameter) and close them as soon as not needed anymore.\n\nIf you instead don't need to \"open\" the streams, but you can just call `.read(SIZE)` on them,\nyou can simply do:\n\n```python\nfrom io import BytesIO\n\nstream1 = BytesIO(b\"file1content\")\nstream2 = BytesIO(b\"file2content\")\ncontainer.add_streamed_objects_to_pack([stream1, stream2])\n```\n\nwhich has the same output as before.\nNote that this is just to demonstrate the interface: the `BytesIO` object will store the whole data in memory!\n\n### Reclaiming space\n\nTo avoid race conditions, while packing the corresponding loose files are not deleted.\nIn order to reclaim that space, after making sure that no process is still accessing the loose objects, one can do\n\n```python\ncontainer.clean_storage()\n```\n\nNote: Technically processes can still continue using the container during this operation, with the following caveats:\n\n- on Linux, the operation should be callable at any time; files will be deleted, but if they are open can still be\n  accessed correctly. Once closed, they will actually be removed from disk and don't occupy space anymore.\n- on Windows, the operation should be callable at any time; if a loose object is open, it will not be deleted.\n  A future `clean_storage` call will delete it once it's not used anymore.\n- on Mac OS X, it is better not to call it while processes are still accessing the file, because there are race\n  conditions under which the file might be read as empty. If the file is already open, the same notes as Linux apply.\n  However, once objects are packed, new implementations will prefer the packed version and open that one. So, it is\n  OK to call the `clean_storage`. However, one should be careful with concurrently continuing to write loose objects and\n  accessing them for the aforementioned race condition.\n\n## Implementation considerations\n\nThis implementation, in particular, addresses the following aspects:\n\n- objects are written, by default, in loose format. They are also uncompressed.\n  This gives maximum performance when writing a file, and ensures that many writers\n  can write at the same time without data corruption.\n\n- the key of the object is its hash key. While support for multiple types of cryptographically\n  strong hash keys is trivial, in the current version only `sha256` is used.\n  The package assumes that there will never be hash collision.\n  At a small cost for computing the hash (that is anyway small, see performance tests)\n  one gets automatic de-duplication of objects written in the object store (git does something very\n  similar).\n  In addition, it automatically provides a way to check for corrupted data.\n\n- loose objects are stored in a one-level sharding format: `aa/bbccddeeff00...`\n  where `aabbccddeeff00...` is the hash key of the file.\n  Current experience (with AiiDA) shows that it's actually not so good to use two\n  levels of nesting.\n  And anyway when there are too many loose objects, the idea\n  is that we will pack them in few files (see below).\n  The number of characters in the first part can be chosen, but a good compromise is\n  2 (default, also used by git).\n\n- for maximum performance, loose objects are simply written as they are,\n  without compression.\n  They are actually written first to a sandbox folder (in the same filesystem),\n  and then moved in place with the correct key (being the hash key) only when the file is closed.\n  This should prevent having leftover objects if the process dies, and\n  the move operation should be hopefully a fast atomic operation on most filesystems.\n\n- When the user wants, loose objects are repacked in a few pack files. Indeed,\n  just the fact of storing too many files is quite expensive\n  (e.g. storing 65536 empty files in the same folder took over 3 minutes to write\n  and over 4 minutes to delete on a Mac SSD). This is the main reason for implementing\n  this package, and not just storing each object as a file.\n\n- packing can be triggered by the user periodically.\n\n  **Note**: only one process can act on packs at a given time.\n\n  **Note**: (one single) packer project can happen also while many other processes are\n  writing *loose* objects and reading *any type* of object.\n  To guarantee the possibility of concurrent operations, the loose objects are not removed\n  while repacking.\n  It is instead needed to run the `clean_storage()` method as discussed earlier,\n  but this is a maintenance operation, so this can be run when no one is using\n  the container in read or write mode.\n\n  This packing operation takes all loose objects and puts them together in packs.\n  Pack files are just concatenation of bytes of the packed objects. Any new object\n  is appended to the pack (thanks to the efficiency of opening a file for appending).\n  The information for the offset and length of each pack is kept in a single SQLite\n  database.\n\n- The name of the packs is a sequential integer. A new pack is generated when the\n  pack size becomes larger than a per-container configurable target size.\n  (`pack_size_target`, default: 4GB).\n  This means that (except for the \"last\" pack), packs will always have a dimension\n  larger or equal than this target size (typically around the target size, but\n  it could be much larger if the last object that is added to the pack is very big).\n\n- For each packed object, the SQLite database contains: the `uuid`, the `offset` (starting\n  position of the bytestream in the file), the `length` (number of bytes to read),\n  a boolean `compressed` flag, meaning if the bytestream has been zlib-compressed,\n  and the `size` of the actual data (equal to `length` if `compressed` is false,\n  otherwise the size of the uncompressed stream, useful for statistics), and an integer\n  specifying in which pack it is stored. **Note** that the SQLite DB tracks only packed\n  objects. Instead, loose objects are not tracked, and their sole presence in the\n  loose folder marks their existence in the container.\n\n- Note that compression is on a per-object level. This allows much greater flexibility\n  (the API still does not allow for this, but this is very easy to implement).\n  The current implementation only supports zlib compression with a default hardcoded\n  level of 1 (good compression at affordable computational cost).\n  Future extensions envision the possibility to choose the compression algorithm.\n\n- the repository configuration is kept in a top-level json (number of nesting levels\n  for loose objects, hashing algorithm, target pack size, ...)\n\n- API exists both to get and write a single object, but also to write directly\n  to pack files (this **cannot** be done by multiple processes at the same time, though),\n  and to read in bulk a given number of objects.\n  This is particularly convenient when using the object store for bulk import and\n  export, and very fast. Also, it is useful when getting all files of a given node.\n\n  In normal operation, however, we expect the client to write loose objects,\n  to be repacked periodically (e.g. once a week).\n\n- **PERFORMANCE**: Some reference results for bulk operations, performed on a\n  Ubuntu 16.04 machine, 16 cores, 64GBs of RAM, with two SSD disks in RAID1 configuration),\n  using the `examples/example_objectstore.py` script.\n\n  - Storing 100'000 small objects (with random size between 0 and 1000 bytes, so a total size of around\n    50 MB) directly to the packs takes about 21s.\n\n  - The time to retrieve all of them is ~3.1s when using a single bulk call,\n    compared to ~54s when using 100'000 independent calls (still probably acceptable).\n    Moreover, even getting, in 10 bulk calls, 10 random chunks of the objects (eventually\n    covering the whole set of 100'000 objects) is equally efficient as getting them\n    all in one shot (note that for this size, only one pack file is created with the default\n    configuration settings). This should demonstrate that exporting a subset of the graph should\n    be efficient (and the object store format could be used also inside the export file).\n\n    **Note**: these times are measured without flushing any disk cache.\n    In any case, there is only a single pack file of about 50MB, so the additional time to\n    fetch it back from disk is small. Anyway, for completeness, if we instead flush the caches\n    after writing and before reading, so data needs to be read back from disk:\n\n    - the time to retrieve 100000 packed objects in random order with a single bulk call is\n      of about 3.8s, and in 10 bulk calls (by just doing this operation\n      right after flushing the cache) is ~3.5s.\n    - the time to retrieve 100000 packed objects in random order, one by one (right after\n      flushing the cache, without doing other reads that would put the data in the cache already)\n      is of about 56s.\n\n- All operations internally (storing to a loose object, storing to a pack, reading\n  from a loose object or from a pack, compression) are all happening via streaming.\n  So, even when dealing with huge files, these never fill the RAM (e.g. when reading\n  or writing a multi-GB file, the memory usage has been tested to be capped at ~150MB).\n  Convenience methods are available, anyway, to get directly an object content, if\n  the user wants.\n\n- A number of streamins APIs are exposed to the users, who are encouraged to use this if they\n  are not sure of the size of the objects and want to avoid out-of-memory crashes.\n\n## Further design choices\n\nIn addition, the following design choices have been made:\n\n- Each given object is tracked with its hash key.\n  It's up to the caller to track this into a filename or a folder structure.\n  To guarantee correctness, the hash is computed by the implementation\n  and cannot be passed from the outside.\n\n- Pack naming and strategy is not determined by the user, except for the specification\n  of a `pack_size_target`. Pack are stored consecutively, so that when a pack file\n  is \"full\", new ones will be used. In this way, once a pack it's full, it's not changed\n  anymore (unless a full repack is performed), meaning that when performing backups using\n  rsync, those full packs don't need to be checked every time.\n\n- A single index file is used. Having one pack index per file turns out not\n  to be very effective, mostly because for efficiency one would need to keep all\n  indexes open (but then one quickly hits the maximum number of open files for a big repo with\n  many pack files; this limit is small e.g. on Mac OS, where it is of the order of ~256).\n  Otherwise, one would need to open the correct index at every request, that risks to\n  be quite inefficient (not only to open, but also to load the DB, perform the query,\n  return the results, and close again the file).\n\n- Deletion (not implemented yet), can just occur as a deletion of the loose object or\n  a removal from the index file. Later repacking of the packs can be used to recover\n  the disk space still occupied in the pack files (care needs to be taken if concurrent\n  processes are using the container, though).\n\n- The current packing format is `rsync`-friendly. `rsync` has an algorithm to just\n  send the new part of a file, when appending. Actually, `rsync` has a clever rolling\n  algorithm that can also detect if the same block is in the file, even if at a\n  different position. Therefore, even if a pack is \"repacked\" (e.g. reordering\n  objects inside it, or removing deleted objects) does not prevent efficient\n  rsync transfer.\n\n  Some results: Let's considering a 1GB file that took ~4.5 mins to transfer fully\n  the first time  over my network.\n  After transferring this 1GB file, `rsync` only takes 14 seconds\n  to check the difference and transfer the additional 10MB appended to the 1GB file\n  (and it indeed transfers only ~10MB).\n\n  In addition,  if the contents are randomly reshuffled, the second time the `rsync`\n  process took only 14 seconds, transferring only ~32MB, with a speedup of ~30x\n  (in this test, I divided the file in 1021 chunks of random size, uniformly\n  distributed between 0 bytes and 2MB, so with a total size of ~1GB, and in the\n  second `rsync` run I randomly reshuffled the chunks).\n\n- Appending files to a single file does not prevent the Linux disk cache to work.\n  To test this, I created a ~3GB file, composed of a ~1GB file (of which I know the MD5)\n  and of a ~2GB file (of which I know the MD5).\n  They are concatenated on a single file on disk.\n  File sizes are not multiples of a power of 2 to avoid alignment with block size.\n\n  After flushing the caches, if one reads only the second half, 2GB are added to the\n  kernel memory cache.\n\n  After re-flushing the caches, if one reads only the first half, only 1GB is added\n  to the memory cache.\n  Without further flushing the caches, if one reads also the first half,\n  2 more GBs are added to the memory cache (totalling 3GB more).\n\n  Therefore, caches are per blocks/pages in linux, not per file.\n  Concatenating files does not impact performance on cache efficiency.\n\n[bench-link]: https://aiidateam.github.io/disk-objectstore/dev/bench/\n[build-badge]: https://github.com/aiidateam/disk-objectstore/workflows/Continuous%20integration/badge.svg\n[codecov-badge]: https://codecov.io/gh/aiidateam/disk-objectstore/branch/develop/graph/badge.svg\n[codecov-link]: https://codecov.io/gh/aiidateam/disk-objectstore\n[pypi-badge]: https://badge.fury.io/py/disk-objectstore.svg\n[pypi-link]: https://pypi.python.org/pypi/disk-objectstore\n[pypi-pyversions]: https://img.shields.io/badge/Supported%20platforms-windows%20%7c%20macos%20%7c%20linux-1f425f.svg\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://github.com/aiidateam/disk-objectstore",
    "keywords": "object store,repository,file store",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "disk-objectstore",
    "package_url": "https://pypi.org/project/disk-objectstore/",
    "platform": "",
    "project_url": "https://pypi.org/project/disk-objectstore/",
    "project_urls": {
      "Homepage": "http://github.com/aiidateam/disk-objectstore"
    },
    "release_url": "https://pypi.org/project/disk-objectstore/0.6.0/",
    "requires_dist": [
      "click",
      "sqlalchemy (~=1.4.22)",
      "typing-extensions ; python_version < \"3.8\"",
      "coverage ; extra == 'dev'",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-benchmark ; extra == 'dev'",
      "pytest-cov ; extra == 'dev'",
      "memory-profiler ; extra == 'examples'",
      "profilehooks ; extra == 'examples'",
      "psutil ; extra == 'examples'",
      "pywin32 ; (platform_system == \"Windows\") and extra == 'examples'"
    ],
    "requires_python": "~=3.7",
    "summary": "An implementation of an efficient object store writing directly into a disk folder",
    "version": "0.6.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11388540,
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "12ea8ca673054d9fd7dbc9e17cc164aea9c304225a9fb4f5f5e40eea729d8c04",
          "md5": "62e8336c76f3825a6903e7a90a7ff9b3",
          "sha256": "883948de1e5151a398795b3f3b54ebd3f7a3f5673e59090e3c883e08c8d4e57f"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.2.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "62e8336c76f3825a6903e7a90a7ff9b3",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 21845,
        "upload_time": "2020-04-11T14:12:28",
        "upload_time_iso_8601": "2020-04-11T14:12:28.421257Z",
        "url": "https://files.pythonhosted.org/packages/12/ea/8ca673054d9fd7dbc9e17cc164aea9c304225a9fb4f5f5e40eea729d8c04/disk_objectstore-0.2.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6a7ddf427b2503d8fc80ebaec06301bbb9315c2388df1717357aa029d92984a3",
          "md5": "9175e4bdde130388dea079b18c4891ba",
          "sha256": "c76daa62f3c256446b432078332618598895749abbcb6d10c2741466c8527f53"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "9175e4bdde130388dea079b18c4891ba",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 24242,
        "upload_time": "2020-04-11T14:12:30",
        "upload_time_iso_8601": "2020-04-11T14:12:30.698185Z",
        "url": "https://files.pythonhosted.org/packages/6a/7d/df427b2503d8fc80ebaec06301bbb9315c2388df1717357aa029d92984a3/disk_objectstore-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "19dbcfb4ff509ea4afbdf31c6242576a745623e95d3ccec6ae78fc89f9f3b633",
          "md5": "5a0527125e9166d4f4291e2c15d0a42b",
          "sha256": "0a8665d27760460a64286f862c5f5bb73a725a90e93ab8260cf31fa765b46124"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.2.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5a0527125e9166d4f4291e2c15d0a42b",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 36485,
        "upload_time": "2020-04-13T19:06:02",
        "upload_time_iso_8601": "2020-04-13T19:06:02.824033Z",
        "url": "https://files.pythonhosted.org/packages/19/db/cfb4ff509ea4afbdf31c6242576a745623e95d3ccec6ae78fc89f9f3b633/disk_objectstore-0.2.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d24a98cbcb9bf83f9e8790e118a261282fafd11d78cb36c6149f88a9fb798fa",
          "md5": "52624c4b79edfd5bf527478048d737f7",
          "sha256": "fc05aa446f8fa14edc88c843ab98ef80751c8748056113922119f589709f8390"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "52624c4b79edfd5bf527478048d737f7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 39834,
        "upload_time": "2020-04-13T19:06:04",
        "upload_time_iso_8601": "2020-04-13T19:06:04.212075Z",
        "url": "https://files.pythonhosted.org/packages/5d/24/a98cbcb9bf83f9e8790e118a261282fafd11d78cb36c6149f88a9fb798fa/disk_objectstore-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b25e465884f8705e464166b9920886a1de69b1e6472920a214f75fc8a44e6443",
          "md5": "3a1955801d8096a9cb187dda6dcfa8c7",
          "sha256": "879b5e74a73e3d5d825f45a2c005957ccab46d2d89305d797c28be9bf73e8e43"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.3.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3a1955801d8096a9cb187dda6dcfa8c7",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 48128,
        "upload_time": "2020-06-22T14:37:37",
        "upload_time_iso_8601": "2020-06-22T14:37:37.431556Z",
        "url": "https://files.pythonhosted.org/packages/b2/5e/465884f8705e464166b9920886a1de69b1e6472920a214f75fc8a44e6443/disk_objectstore-0.3.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "54bd98ec31529dad4a9b1344e9fdf1e410a4f7fd51b74f43ae1a1b3140365e80",
          "md5": "7a5e32b8264fee4f4de65f0dadc29956",
          "sha256": "d4634f9ad98dcc64501df764c43e138e0cad7eb8f2bed837dc81e7b2b486b02e"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7a5e32b8264fee4f4de65f0dadc29956",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 51818,
        "upload_time": "2020-06-22T14:37:38",
        "upload_time_iso_8601": "2020-06-22T14:37:38.735975Z",
        "url": "https://files.pythonhosted.org/packages/54/bd/98ec31529dad4a9b1344e9fdf1e410a4f7fd51b74f43ae1a1b3140365e80/disk_objectstore-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1a5619d0dabefeff6727e1c17ecba5edece3a29178151809df5cb37a2e20a53b",
          "md5": "fe368f42739831e5811e95619d8bb9ad",
          "sha256": "a40f7c01231e8cc46190cbae7c03c0da80ff1457b00662b158bbc9cafe9f4126"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.4.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fe368f42739831e5811e95619d8bb9ad",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.5",
        "size": 86227,
        "upload_time": "2020-07-20T17:53:01",
        "upload_time_iso_8601": "2020-07-20T17:53:01.986783Z",
        "url": "https://files.pythonhosted.org/packages/1a/56/19d0dabefeff6727e1c17ecba5edece3a29178151809df5cb37a2e20a53b/disk_objectstore-0.4.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1af4c752f777606957256e5024e835766e26ad15ec3625c6a0775088e322b73a",
          "md5": "c9851358353a973e57a7bd8cc2046727",
          "sha256": "04af62ed0ca62fd2c0666c3406d01a827609dd1957e0e65eb903f6e7a448d23a"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c9851358353a973e57a7bd8cc2046727",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 95758,
        "upload_time": "2020-07-20T17:53:03",
        "upload_time_iso_8601": "2020-07-20T17:53:03.850016Z",
        "url": "https://files.pythonhosted.org/packages/1a/f4/c752f777606957256e5024e835766e26ad15ec3625c6a0775088e322b73a/disk_objectstore-0.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "27275ffe52ca4a82822270986ebbb8b47b29bd4ff966826f04c1d6fb795bee49",
          "md5": "48d867fddd2909bbad183b72038d81a8",
          "sha256": "fcd2d5034f42c805f0c68679e7e6359aa01bf782c01e3847275bc22e1545ca94"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.5.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "48d867fddd2909bbad183b72038d81a8",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.5",
        "size": 107397,
        "upload_time": "2020-11-11T14:40:57",
        "upload_time_iso_8601": "2020-11-11T14:40:57.856776Z",
        "url": "https://files.pythonhosted.org/packages/27/27/5ffe52ca4a82822270986ebbb8b47b29bd4ff966826f04c1d6fb795bee49/disk_objectstore-0.5.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "efe5f9754416bd6adf602644810d4b4867a30b07c6135cacfaa58def0190ae47",
          "md5": "e2d8d1d53d1a55f631ace5af432dbd65",
          "sha256": "d640234a338b453b1ba576c08e5263ef7fb97e35bb31e1a4f6029b4c2535c88f"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "e2d8d1d53d1a55f631ace5af432dbd65",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 181217,
        "upload_time": "2020-11-11T14:40:59",
        "upload_time_iso_8601": "2020-11-11T14:40:59.515058Z",
        "url": "https://files.pythonhosted.org/packages/ef/e5/f9754416bd6adf602644810d4b4867a30b07c6135cacfaa58def0190ae47/disk_objectstore-0.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f35424f6cc6b67ab5efe9ab3322fd2370ffc8996f776be8afddab0e344422fe",
          "md5": "9e79b4734fdc3b41d807b58387f6cad9",
          "sha256": "cb22c9360241a3e662e663d285937d8fea358dde7bfa68ef5472a9e0025eff49"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.6.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9e79b4734fdc3b41d807b58387f6cad9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "~=3.7",
        "size": 64854,
        "upload_time": "2021-09-07T20:14:12",
        "upload_time_iso_8601": "2021-09-07T20:14:12.723536Z",
        "url": "https://files.pythonhosted.org/packages/5f/35/424f6cc6b67ab5efe9ab3322fd2370ffc8996f776be8afddab0e344422fe/disk_objectstore-0.6.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "43b9158c2ce983ee199c0c951eed942247ed9f3e9a6fcf379f647f8125a9a67d",
          "md5": "0ea44a73a08b6b75874d6ac170f78145",
          "sha256": "ad3404eff452eacb2cf3fcd25d45cfc933eb32b4fd69fba81f4c3cbda47b1777"
        },
        "downloads": -1,
        "filename": "disk_objectstore-0.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0ea44a73a08b6b75874d6ac170f78145",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "~=3.7",
        "size": 73801,
        "upload_time": "2021-09-07T20:14:13",
        "upload_time_iso_8601": "2021-09-07T20:14:13.734308Z",
        "url": "https://files.pythonhosted.org/packages/43/b9/158c2ce983ee199c0c951eed942247ed9f3e9a6fcf379f647f8125a9a67d/disk_objectstore-0.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5f35424f6cc6b67ab5efe9ab3322fd2370ffc8996f776be8afddab0e344422fe",
        "md5": "9e79b4734fdc3b41d807b58387f6cad9",
        "sha256": "cb22c9360241a3e662e663d285937d8fea358dde7bfa68ef5472a9e0025eff49"
      },
      "downloads": -1,
      "filename": "disk_objectstore-0.6.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "9e79b4734fdc3b41d807b58387f6cad9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "~=3.7",
      "size": 64854,
      "upload_time": "2021-09-07T20:14:12",
      "upload_time_iso_8601": "2021-09-07T20:14:12.723536Z",
      "url": "https://files.pythonhosted.org/packages/5f/35/424f6cc6b67ab5efe9ab3322fd2370ffc8996f776be8afddab0e344422fe/disk_objectstore-0.6.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "43b9158c2ce983ee199c0c951eed942247ed9f3e9a6fcf379f647f8125a9a67d",
        "md5": "0ea44a73a08b6b75874d6ac170f78145",
        "sha256": "ad3404eff452eacb2cf3fcd25d45cfc933eb32b4fd69fba81f4c3cbda47b1777"
      },
      "downloads": -1,
      "filename": "disk_objectstore-0.6.0.tar.gz",
      "has_sig": false,
      "md5_digest": "0ea44a73a08b6b75874d6ac170f78145",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "~=3.7",
      "size": 73801,
      "upload_time": "2021-09-07T20:14:13",
      "upload_time_iso_8601": "2021-09-07T20:14:13.734308Z",
      "url": "https://files.pythonhosted.org/packages/43/b9/158c2ce983ee199c0c951eed942247ed9f3e9a6fcf379f647f8125a9a67d/disk_objectstore-0.6.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}