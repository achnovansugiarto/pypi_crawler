{
  "info": {
    "author": "Dmtry Barsukov",
    "author_email": "riZZZhik@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Всем привет )\nЭтот код нужен что-бы оптимизировать модель в TensorRT и инференсить в лайве.\n\nВажный момент: engine файлы не совместимы между системами, нужно оптимизировать под каждую систему отдельно.\n\n\n# Recommended pipeline\n- Заранее перегнать модель из фреймворка в ONNX. Для этого есть несколько функций в этом модуле. `python -c \"import onnx_converter; help(onnx_converter)\"`\n- Скачать эту либу и onnx файл модели на необходимое железо.\n- Поставить все необходимые зависимости\n- Указать все пути для энвов из пункта _Installation_\n- Запустить оптимизатор onnx2trt: `python onnx2trt.py --help`\n  - Для INT8 необходимо заранее создать calibration файл через onnx2trt_handmade. `python -c \"from onnx2trt_handmade import onnx2trt_handmade; help(onnx2trt_handmade)\"`\n- Запомнить куда сохранился итоговый engine файл.\n- Для инференса использовать класс InferenceTRT. `python -c \"from inference import InferenceTRT; help(InferenceTRT)\"`\n\n\n# Installation\n\n## Envs\n- `CUDA_HOME`: Путь до папки с кудой.\n- `LD_LIBRARY_PATH`:\n  - Путь до `lib64` в папке с кудой.\n  - Путь до `targets/x86_64-linux/lib/` в папке с кудой.\n  - Путь до `lib` в папке с TensorRT.\n\n## `trtexec`\nДля запуска onnx2trt необходимо указывать путь до trtexec. Как правило, он хранится по пути `/targets/x86_64-linux-gnu/bin/trtexec` в папке с TensorRT.\n\n## `TensorRT`\nTested on version `8.2.2`\n\n## `CUDA`\n\n## `PyCUDA`\n\n\n# Convert to ONNX\n\n## `fastai2onnx`\nКонвертируем модельку из FastAI в ONNX.\n\n### Args:\n- `fastai_path` (str): Path to fastai model file.\n- `input_shape` (tuple or list): Input shape (with batch dimension).\n- `onnx_path` (str): Path to save converted model.\n- `input_names` (tuple or list): Model's input names.\n- `output_names` (tuple or list): Model's output names.\n\n## `torch2onnx`\nКонвертируем модельку из PyTorch в ONNX.\n\n### Args:\n- `model` (torch.nn.Module): PyTorch model.\n- `input_shape` (tuple or list): Input shape (with batch dimension).\n- `onnx_path` (str): Path to save converted model.\n- `input_names` (tuple or list): Model's input names.\n- `output_names` (tuple or list): Model's output names.\n\n## `keras2onnx`\nКонвертируем модельку из Keras в ONNX.\n\n### Args:\n- `model` (): Keras model.\n- `input_shape` (tuple or list): Input shape (with batch dimension).\n- `onnx_path` (str): Path to save converted model.\n\n\n# Optimize\n\n## `onnx2trt`\nСамый оптимальный метод для оптимизации модельки под TensorRT. В конце конвертации выводит бенчмарки. Не поддерживает калибровку для INT8. \n\n_NB! Для каждой машины нужно конвертировать отдельно, просто перетащить файлик engine не сработает._\n\n### Args:\n- `onnx_path` (str): Path to onnx model (opset9).\n- `engine_path` (str): Path to save engine.\n- `dtype` (str or np.dtype): Model data type.\n- `use_sparsity` (bool): Use sparsity boolean.\n- `trtexec_path` (str): Path to trtexec binary.\n- `workspace_size` (int): Size of workspace in MB.\n\n## `onnx2trt_handmade`\nОптимизация модельки с использованием TensorRT Python API.\nЛучше всего использовать один раз для создания calibration файлика, его можно переносить между системами. И под каждую оптимизировать через trtexec. \n\n### Args:\n- `onnx_path` (str): Path to ONNX model.\n- `engine_path` (str): Path to save TensorRT engine.\n- `dtype` (str): Model data type. (fp32, fp16 or int8).\n- `calib` (trt.IInt8EntropyCalibrator2): Calibration class. Only for INT8.\n\n## `Calibration`\nКласс для агрегации данных во время калибровки INT8.\n\n### `__init__`\n#### Args:\n- `calibration_files` (list or tuple): List of paths to images for calibration.\n- `preprocessor` (function): Image preprocessing function.\n- `shape` (list or tuple): Input image shape. With batch_size.\n- `cache_file` (str): Path to save calibration.\n\n\n# Inference\n\n## InferenceTRT\nКласс для инференса модельки оптимизированной в engine.\n\n### `__init__`\nInitialize class variables.\n#### Args:\n- `engine_path` (str): Path to TensorRT engine file.\n- `device` (cuda.Device): Cuda device to run inference on.\n\n### `load_engine`\nLoad TensorRT engine from file.\n#### Args:\n- `engine_path` (str): Path to TensorRT engine file.\n\n### `predict`\nGet model prediction.\n#### Args:\n- `inputs` (np.ndarray): Model inputs array.\n#### Returns:\n- Model prediction as np.ndarray.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/riZZZhik/optimize_tensorrt",
    "keywords": "",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "optimize-tensorrt",
    "package_url": "https://pypi.org/project/optimize-tensorrt/",
    "platform": null,
    "project_url": "https://pypi.org/project/optimize-tensorrt/",
    "project_urls": {
      "Homepage": "https://github.com/riZZZhik/optimize_tensorrt"
    },
    "release_url": "https://pypi.org/project/optimize-tensorrt/1.0.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Convert and inference TensorRT models",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14300230,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9ac1d9b04dd20eca47ab471ff9aa524288c92a69ed7b88e7ac8937a3e3e42a8b",
          "md5": "bff0bdb1e20e3c38a5a70ec1a77df68c",
          "sha256": "0ff9404a026c9684448726a989cb9a50ceb7dff8988737b6298c4156b365f1d1"
        },
        "downloads": -1,
        "filename": "optimize_tensorrt-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "bff0bdb1e20e3c38a5a70ec1a77df68c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13200,
        "upload_time": "2022-06-30T20:45:27",
        "upload_time_iso_8601": "2022-06-30T20:45:27.628378Z",
        "url": "https://files.pythonhosted.org/packages/9a/c1/d9b04dd20eca47ab471ff9aa524288c92a69ed7b88e7ac8937a3e3e42a8b/optimize_tensorrt-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9ac1d9b04dd20eca47ab471ff9aa524288c92a69ed7b88e7ac8937a3e3e42a8b",
        "md5": "bff0bdb1e20e3c38a5a70ec1a77df68c",
        "sha256": "0ff9404a026c9684448726a989cb9a50ceb7dff8988737b6298c4156b365f1d1"
      },
      "downloads": -1,
      "filename": "optimize_tensorrt-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "bff0bdb1e20e3c38a5a70ec1a77df68c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 13200,
      "upload_time": "2022-06-30T20:45:27",
      "upload_time_iso_8601": "2022-06-30T20:45:27.628378Z",
      "url": "https://files.pythonhosted.org/packages/9a/c1/d9b04dd20eca47ab471ff9aa524288c92a69ed7b88e7ac8937a3e3e42a8b/optimize_tensorrt-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}