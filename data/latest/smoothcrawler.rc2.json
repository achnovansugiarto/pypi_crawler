{
  "info": {
    "author": "Liu, Bryant",
    "author_email": "chi10211201@cycu.org.tw",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# SmoothCrawler\n\n[![Supported Versions](https://img.shields.io/pypi/pyversions/smoothcrawler.svg?logo=python&logoColor=FBE072)](https://pypi.org/project/smoothcrawler)\n[![Release](https://img.shields.io/github/release/Chisanan232/SmoothCrawler.svg?label=Release&logo=github)](https://github.com/Chisanan232/SmoothCrawler/releases)\n[![PyPI version](https://badge.fury.io/py/SmoothCrawler.svg?logo=pypi)](https://badge.fury.io/py/SmoothCrawler)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg?logo=apache)](https://opensource.org/licenses/Apache-2.0)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/cf25e1acc34a4c44b6b1ac7084cfe7c5)](https://www.codacy.com/gh/Chisanan232/smoothcrawler/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=Chisanan232/smoothcrawler&amp;utm_campaign=Badge_Grade)\n[![Documentation Status](https://readthedocs.org/projects/smoothcrawler/badge/?version=latest)](https://smoothcrawler.readthedocs.io/en/latest/?badge=latest)\n\n| OS | Building Status | Coverage Status |\n|------------|------------|--------|\n| Linux/MacOS |[![SmoothCrawler CI/CD](https://github.com/Chisanan232/smoothcrawler/actions/workflows/ci-cd-master.yml/badge.svg)](https://github.com/Chisanan232/smoothcrawler/actions/workflows/ci-cd-master.yml)|[![codecov](https://codecov.io/gh/Chisanan232/smoothcrawler/branch/master/graph/badge.svg?token=BTYTU20FBT)](https://codecov.io/gh/Chisanan232/smoothcrawler)|\n| Windows |[![Build status](https://ci.appveyor.com/api/projects/status/1eri78jtxvu5r0q2?svg=true)](https://ci.appveyor.com/project/Chisanan232/smoothcrawler)|[![Coverage Status](https://coveralls.io/repos/github/Chisanan232/smoothcrawler/badge.svg?branch=master)](https://coveralls.io/github/Chisanan232/smoothcrawler?branch=master)|\n\n[comment]: <> (| Linux |[![Build Status]&#40;https://app.travis-ci.com/Chisanan232/smoothcrawler.svg?branch=master&#41;]&#40;https://app.travis-ci.com/Chisanan232/smoothcrawler&#41;|Deprecated|)\n\n[comment]: <> (| Linux |[![CircleCI]&#40;https://circleci.com/gh/Chisanan232/smoothcrawler.svg?style=shield&#41;]&#40;https://app.circleci.com/pipelines/github/Chisanan232/smoothcrawler&#41;|[![codecov]&#40;https://codecov.io/gh/Chisanan232/smoothcrawler/branch/master/graph/badge.svg?token=BTYTU20FBT&#41;]&#40;https://codecov.io/gh/Chisanan232/smoothcrawler&#41;|)\n\n*SmoothCrawler* is a Python framework for being faster and easier to build crawler (or be called web spider).\nThe core concept of its implementation is SoC (Separation of Concerns). It could build crawler humanly as different \nroles which be combined with different components.\n\n[Overview](#overview) | [Quickly Demo](#quickly-demo) | [Documentation](#documentation)  | [Code Example](https://github.com/Chisanan232/smoothcrawler/tree/master/example)\n<hr>\n\n\n## Overview\n\nImplementing a web crawler in Python is very easy and simple. It already has many frameworks or libraries to do it.\nHowever, they focus on one point. It means that they all have their own responsibility to face different things:\n\n* For HTTP, you must think about *urllib3* or *requests*.\n* For parsing HTTP response, *BeautifulSoup* (*bs4*).\n* A framework to do it, *scrapy* or *selenium*.\n\nHow about a library to build a **crawler system**?\n\nEvery crawler should do mostly same things and procedures:\n\n![image](https://github.com/Chisanan232/smoothcrawler/tree/master/docs/source/images/Work_Process(Briefly).drawio.png)\n\nIn generally, a crawler code usually be unstable and even be difficult (e.g. parsing a complex HTML elements content). \nSo it's keeping facing many challenges when you're developing web spider, much less maintain the crawler program (for \nexample, web element locations changing will be your nightmare) or change requirement.\n\n_smoothcrawler_ like LEGO blocks, it classifies crawling to be some components. Every component has its own responsibility to do something. \nComponents could reuse others if it needs. One component focus one thing. Finally, the components combines to form a crawler.\n\n\n## Quickly Demo\n\nInstall _smoothcrawler_ via **pip**:\n\n    pip install smoothcrawler\n\nLet's write a simple crawler to crawl data.\n\n* Component 1: Send HTTP requests\n\nImplement with Python package _requests_. Of course, it could implement by _urllib3_, too.\n\n```python\nfrom smoothcrawler.components.httpio import HTTP\nimport requests\n\nclass FooHTTPRequest(HTTP):\n\n    __Http_Response = None\n\n    def get(self, url: str, *args, **kwargs):\n            self.__Http_Response = requests.get(url)\n            return self.__Http_Response\n```\n\n* Component 2: Get and parse HTTP response\n\nGet the HTTP response object and parse the content data from it.\n\n```python\nfrom smoothcrawler.components.data import BaseHTTPResponseParser\nfrom bs4 import BeautifulSoup\nimport requests\n\n\nclass FooHTTPResponseParser(BaseHTTPResponseParser):\n\n    def get_status_code(self, response: requests.Response) -> int:\n        return response.status\n\n    def handling_200_response(self, response: requests.Response) -> str:\n        _bs = BeautifulSoup(response.text, \"html.parser\")\n        _example_web_title = _bs.find_all(\"h1\")\n        return _example_web_title[0].text\n```\n\n* Component 3: Handle data processing\n\nDemonstrate it could do some data processing here.\n\n```python\nfrom smoothcrawler.components.data import BaseDataHandler\n\nclass FooDataHandler(BaseDataHandler):\n\n    def process(self, result):\n        return \"This is the example.com website header text: \" + result\n```\n\n* Product: Components combine to form a  crawler\n\nIt has 3 components now: HTTP sender, HTTP response parser and data processing handler.\nThey could combine to form a crawler and crawl data from target URL(s) via crawler role _SimpleCrawler_.\n\n```python\nfrom smoothcrawler.crawler import SimpleCrawler\nfrom smoothcrawler.factory import CrawlerFactory\n\n_cf = CrawlerFactory()\n_cf.http_factory = FooHTTPRequest()\n_cf.parser_factory = FooHTTPResponseParser()\n_cf.data_handling_factory = FooDataHandler()\n\n# Crawler Role: Simple Crawler\nsc = SimpleCrawler(factory=_cf)\ndata = sc.run(\"GET\", \"http://www.example.com\")\nprint(data)\n# This is the example.com website header text: Example Domain\n```\n\n* Be more easier implementation in one object.\n\nYou may think: come on, I just want to get a simple data easily, so I don't want to \ndivergent simple implementations to many different objects. It's not clear and graceful.\n\nDon't worry, it also could implement that in one object which extends _SimpleCrawler_ like following:\n\n```python\nfrom smoothcrawler.crawler import SimpleCrawler\nfrom bs4 import BeautifulSoup\nimport requests\n\nclass ExampleEasyCrawler(SimpleCrawler):\n\n   def send_http_request(self, method: str, url: str, retry: int = 1, *args, **kwargs) -> requests.Response:\n       _response = requests.get(url)\n       return _response\n\n\n   def parse_http_response(self, response: requests.Response) -> str:\n       _bs = BeautifulSoup(response.text, \"html.parser\")\n       _example_web_title = _bs.find_all(\"h1\")\n       return _example_web_title[0].text\n\n\n   def data_process(self, parsed_response: str) -> str:\n       return \"This is the example.com website header text: \" + parsed_response\n```\n\nFinally, you could instantiate and use it directly:\n\n```python\n_example_easy_crawler = ExampleEasyCrawler()    # Instantiate your own crawler object\n_example_result = _example_easy_crawler.run(\"get\", \"http://www.example.com\")    # Run the web spider task with function *run* and get the result\nprint(_example_result)\n# This is the example.com website header text: Example Domain\n```\n\nHow the usage easy and code clear is!\n\n\n## Documentation\n\nThe [documentation](https://smoothcrawler.readthedocs.io) contains more details, and examples.\n\n* [Quickly Start](https://smoothcrawler.readthedocs.io/en/latest/quickly_start.html) to develop web spider with *SmoothCrawler*\n\n\n## Download \n\n*SmoothCrawler* still a young open source which keep growing. Here's its download state: \n\n[![Downloads](https://pepy.tech/badge/smoothcrawler)](https://pepy.tech/project/smoothcrawler)\n[![Downloads](https://pepy.tech/badge/smoothcrawler/month)](https://pepy.tech/project/smoothcrawler)\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://smoothcrawler.readthedocs.io",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "SmoothCrawler",
    "package_url": "https://pypi.org/project/SmoothCrawler/",
    "platform": null,
    "project_url": "https://pypi.org/project/SmoothCrawler/",
    "project_urls": {
      "Documentation": "https://smoothcrawler.readthedocs.io",
      "Homepage": "https://smoothcrawler.readthedocs.io",
      "Source": "https://github.com/Chisanan232/smoothcrawler"
    },
    "release_url": "https://pypi.org/project/SmoothCrawler/0.2.0/",
    "requires_dist": [
      "multipledispatch (>=0.6.0)",
      "multirunnable (>=0.17.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "Build crawler humanly as different roles which be combined with different components.",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14030400,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "20614229a755ef618133410b1039a75ac472e79f43b8d8f33bb464d16f16e379",
          "md5": "d3482a8c57584d4e05d2456305e40d3c",
          "sha256": "35ab9d27b7abc29f0c0688b78039117aad28c54ddafe0fe05e3d50e9471545d6"
        },
        "downloads": -1,
        "filename": "SmoothCrawler-0.1.0-py3.10.egg",
        "has_sig": false,
        "md5_digest": "d3482a8c57584d4e05d2456305e40d3c",
        "packagetype": "bdist_egg",
        "python_version": "0.1.0",
        "requires_python": ">=3.6",
        "size": 85511,
        "upload_time": "2022-01-20T15:05:54",
        "upload_time_iso_8601": "2022-01-20T15:05:54.540887Z",
        "url": "https://files.pythonhosted.org/packages/20/61/4229a755ef618133410b1039a75ac472e79f43b8d8f33bb464d16f16e379/SmoothCrawler-0.1.0-py3.10.egg",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f293a414601385d2f5ef8819fc014c352f9daa8d0623f61d031e3fdfd74defb0",
          "md5": "3b3afa562052f7877582950db1926aee",
          "sha256": "043f7c5685726bbb82d0a1af9dded08c2b28d531905b833f3833e32b6410d3ef"
        },
        "downloads": -1,
        "filename": "SmoothCrawler-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3b3afa562052f7877582950db1926aee",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 54114,
        "upload_time": "2022-01-20T15:05:51",
        "upload_time_iso_8601": "2022-01-20T15:05:51.518481Z",
        "url": "https://files.pythonhosted.org/packages/f2/93/a414601385d2f5ef8819fc014c352f9daa8d0623f61d031e3fdfd74defb0/SmoothCrawler-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6efbe0e90945ff46dd3d15d2e1f0f3d8c7d4b23102cc06a10693074c2d6ec242",
          "md5": "c5f45d0c53bd70c2cd44e398b8db464e",
          "sha256": "e1e94baf01e049218c35ca764753f54894af2eed7973c68ca9611d9289e02975"
        },
        "downloads": -1,
        "filename": "SmoothCrawler-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c5f45d0c53bd70c2cd44e398b8db464e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 36634,
        "upload_time": "2022-01-20T15:05:56",
        "upload_time_iso_8601": "2022-01-20T15:05:56.074662Z",
        "url": "https://files.pythonhosted.org/packages/6e/fb/e0e90945ff46dd3d15d2e1f0f3d8c7d4b23102cc06a10693074c2d6ec242/SmoothCrawler-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0a28651ee84c656349e49109a7b3f10e5f9f84875085f4da7c8b440ebe380522",
          "md5": "25c394ba65501706ae9947eb5efe14a6",
          "sha256": "a31058984fc0d3cb5a3986717ab50c5e2eb4c9318b9b9065fe8442e388bafe10"
        },
        "downloads": -1,
        "filename": "SmoothCrawler-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "25c394ba65501706ae9947eb5efe14a6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 39923,
        "upload_time": "2022-06-04T06:50:10",
        "upload_time_iso_8601": "2022-06-04T06:50:10.425750Z",
        "url": "https://files.pythonhosted.org/packages/0a/28/651ee84c656349e49109a7b3f10e5f9f84875085f4da7c8b440ebe380522/SmoothCrawler-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fea9a5cf896ae482b1c2c7071e7c42959c5a64c4080a785f1325eb50810ff31b",
          "md5": "8b10a1b3c982fdee6b364e18917ac8ca",
          "sha256": "daf47be5e76fe0b55a8f00dc78b8a536aa1d8bb034bc1d6751a513bb63998bd5"
        },
        "downloads": -1,
        "filename": "SmoothCrawler-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "8b10a1b3c982fdee6b364e18917ac8ca",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 31480,
        "upload_time": "2022-06-04T06:50:11",
        "upload_time_iso_8601": "2022-06-04T06:50:11.732650Z",
        "url": "https://files.pythonhosted.org/packages/fe/a9/a5cf896ae482b1c2c7071e7c42959c5a64c4080a785f1325eb50810ff31b/SmoothCrawler-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0a28651ee84c656349e49109a7b3f10e5f9f84875085f4da7c8b440ebe380522",
        "md5": "25c394ba65501706ae9947eb5efe14a6",
        "sha256": "a31058984fc0d3cb5a3986717ab50c5e2eb4c9318b9b9065fe8442e388bafe10"
      },
      "downloads": -1,
      "filename": "SmoothCrawler-0.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "25c394ba65501706ae9947eb5efe14a6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 39923,
      "upload_time": "2022-06-04T06:50:10",
      "upload_time_iso_8601": "2022-06-04T06:50:10.425750Z",
      "url": "https://files.pythonhosted.org/packages/0a/28/651ee84c656349e49109a7b3f10e5f9f84875085f4da7c8b440ebe380522/SmoothCrawler-0.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fea9a5cf896ae482b1c2c7071e7c42959c5a64c4080a785f1325eb50810ff31b",
        "md5": "8b10a1b3c982fdee6b364e18917ac8ca",
        "sha256": "daf47be5e76fe0b55a8f00dc78b8a536aa1d8bb034bc1d6751a513bb63998bd5"
      },
      "downloads": -1,
      "filename": "SmoothCrawler-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "8b10a1b3c982fdee6b364e18917ac8ca",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 31480,
      "upload_time": "2022-06-04T06:50:11",
      "upload_time_iso_8601": "2022-06-04T06:50:11.732650Z",
      "url": "https://files.pythonhosted.org/packages/fe/a9/a5cf896ae482b1c2c7071e7c42959c5a64c4080a785f1325eb50810ff31b/SmoothCrawler-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}