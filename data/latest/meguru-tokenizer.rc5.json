{
  "info": {
    "author": "MokkeMeguru",
    "author_email": "meguru.mokke@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# meguru tokenizer\n\n# installation and initialization\n\n```shell\npip install meguru_tokenizer\nsudachipy link -t full\n```\n\n# Abstruction of Usage\n\n1.  Preprocess Using Each Tokenizer\n    e.g. sentencepiece preprocess / sudachi preprocess\n2.  Tokenize in your code using its Tokenizer\n    - basis    \n      see. [official docs](https://mokkemeguru.github.io/meguru_tokenizer/index.html)\n    - Tensorflow    \n\t  see. [tutorial](./tutorials/01_tokenize_tf.ipynb)\n    - TODO: PyTorch\n\n# RealWorld Example\n\n```python\nimport meguru_tokenizer.whitespace_tokenizer import WhitespaceTokenizer\nimport pprint\n\nsentences = [\n    \"Hello, I don't know how to use it?\",\n    \"Tensorflow is awesome!\",\n    \"it is good framework.\",\n]\n\n# define tokenizer and vocaburary\ntokenizer = WhitespaceTokenizer(lower=True)\nvocab = Vocab()\n\n# build vocaburary\nfor sentence in sentences:\n    vocab.add_vocabs(tokenizer.tokenize(sentence))\nvocab.build_vocab()\n\n# set vocaburary into tokenizer to enable encoding\ntokenizer.vocab = vocab\n\n# save vocaburary information\nvocab.dump_vocab(Path(\"vocab.txt\"))\nprint(\"vocabs:\")\npprint.pprint(vocab.i2w)\n\n# tokenize\nprint(\"tokenized sentence\")\npprint.pprint(tokenizer.tokenize_list(sentences))\n\n# [['hello', ',', 'i', 'do', \"n't\", 'know', 'how', 'to', 'use', 'it', '?'],\n#  ['tensorflow', 'is', 'awesome', '!'],\n#  ['it', 'is', 'good', 'framework', '.']]\n\n# encode\nprint(\"encoded sentence\")\npprint.pprint([tokenizer.encode(sentence) for sentence in sentences])\n\n# [[7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 16], [17, 6, 18, 19], [5, 6, 20, 21, 22]]\n\nprint(\"decoded sentence\")\npprint.pprint([tokenizer.decode(tokens) for tokens in encodes])\n# [\"hello , i do n't know how to use it ?\",\n#  'tensorflow is awesome !',\n#  'it is good framework .']\n\nvocab_size = len(vocab)\n\n# restore the vocaburary from dumped file\nprint(\"reload from dump file\")\nvocab = Vocab()\nvocab.load_vocab(Path(\"vocab.txt\"))\nassert vocab_size == len(vocab)\n\ntokenizer = WhitespaceTokenizer(vocab=vocab)\npprint.pprint([tokenizer.encode(sentence) for sentence in sentences])\n\n# [[7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 16], [17, 6, 18, 19], [5, 6, 20, 21, 22]]\n\n# vocaburary with minimum frequency limitation\nvocab = Vocab()\nfor sentence in sentences:\n    vocab.add_vocabs(tokenizer.tokenize(sentence))\nvocab.build_vocab(min_freq=2)\nassert vocab_size != len(vocab)\n\n# vocaburary with maximum voaburary size\nvocab = Vocab()\nfor sentence in sentences:\n    vocab.add_vocabs(tokenizer.tokenize(sentence))\nvocab.build_vocab(vocab_size=10)\nassert 10 == len(vocab)\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/MokkeMeguru/meguru_tokenizer",
    "keywords": "tensorflow,pytorch,tokenizer,nlp",
    "license": "MIT license",
    "maintainer": "",
    "maintainer_email": "",
    "name": "meguru-tokenizer",
    "package_url": "https://pypi.org/project/meguru-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/meguru-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/MokkeMeguru/meguru_tokenizer"
    },
    "release_url": "https://pypi.org/project/meguru-tokenizer/0.3.1/",
    "requires_dist": [
      "ginza (>=4.0.0)",
      "sentencepiece",
      "neologdn",
      "nltk",
      "spacy (>=2.2.4)",
      "sudachidict-full",
      "torch",
      "tensorflow (>=2.2.0)"
    ],
    "requires_python": ">=3.5",
    "summary": "simple tokenizer for tensorflow 2.x and PyTorch",
    "version": "0.3.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8226756,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "586c869b945e7b5446458c7b688aa4e543fb27d55736ee488cec0cad5044945e",
          "md5": "8a4c614c41684e6242c30591ba25ca32",
          "sha256": "c0eeb767ff5c0d447b306cf1e9666aff1cd85fed74c216065185b2e9f9dbd306"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8a4c614c41684e6242c30591ba25ca32",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 10576,
        "upload_time": "2020-07-12T14:24:58",
        "upload_time_iso_8601": "2020-07-12T14:24:58.640888Z",
        "url": "https://files.pythonhosted.org/packages/58/6c/869b945e7b5446458c7b688aa4e543fb27d55736ee488cec0cad5044945e/meguru_tokenizer-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6a0a235a96912939633bfaf4a5b426563dee060c2d92f0a459a4640da04a57a9",
          "md5": "b89b1c2e66e3594cd551f033ac28c165",
          "sha256": "c8924642cccf9a87d8060ff25cf04fbb7d60d24066548b2131b65b26e409b249"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "b89b1c2e66e3594cd551f033ac28c165",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 8147,
        "upload_time": "2020-07-12T14:25:00",
        "upload_time_iso_8601": "2020-07-12T14:25:00.970841Z",
        "url": "https://files.pythonhosted.org/packages/6a/0a/235a96912939633bfaf4a5b426563dee060c2d92f0a459a4640da04a57a9/meguru_tokenizer-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d062d6616a35b3ad6cb35d6585959bcfcbb0b1a83b006ba3411295152f6d3414",
          "md5": "0780c2134de2ada4c15caa1e391a3b92",
          "sha256": "6ae865dc9baec51fe841d7a11dea62667503e9fa93ef25409dbaa9b418870782"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0780c2134de2ada4c15caa1e391a3b92",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 14162,
        "upload_time": "2020-07-14T05:36:23",
        "upload_time_iso_8601": "2020-07-14T05:36:23.994417Z",
        "url": "https://files.pythonhosted.org/packages/d0/62/d6616a35b3ad6cb35d6585959bcfcbb0b1a83b006ba3411295152f6d3414/meguru_tokenizer-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "adeeeb5a3a26c9220bb4ae33ee2ef53cb6be758c10e4eefb0dbb522e08b271db",
          "md5": "24b60a3d42ee95fff74e68270734b4d6",
          "sha256": "1062bf064a1697f9556a2ded45d627208e8986c90a409e9d291dbd5ac988a206"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "24b60a3d42ee95fff74e68270734b4d6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 10553,
        "upload_time": "2020-07-14T05:36:25",
        "upload_time_iso_8601": "2020-07-14T05:36:25.223091Z",
        "url": "https://files.pythonhosted.org/packages/ad/ee/eb5a3a26c9220bb4ae33ee2ef53cb6be758c10e4eefb0dbb522e08b271db/meguru_tokenizer-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "401a8f2e411a26995670126c32e6c740857c035f2256fd43da36801b373e8fc3",
          "md5": "531258f73731819febca9a7b6c3eedaa",
          "sha256": "f99d94e8a3ff647a4b752e82007c7423011d349d0bb9c7305b080a2bf36a30bf"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "531258f73731819febca9a7b6c3eedaa",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 14159,
        "upload_time": "2020-09-19T14:34:58",
        "upload_time_iso_8601": "2020-09-19T14:34:58.100478Z",
        "url": "https://files.pythonhosted.org/packages/40/1a/8f2e411a26995670126c32e6c740857c035f2256fd43da36801b373e8fc3/meguru_tokenizer-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5ec44e8ff0e25582effdad0a208acb6bda8717485d24dc5e4516f9ed2fa47e40",
          "md5": "ae2cc734e64302c1cb75934e0c18dc93",
          "sha256": "e7dce921d114db221546a9ffbd5a84065a1f118cafb7a55577462ae45ac9b18e"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ae2cc734e64302c1cb75934e0c18dc93",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 10560,
        "upload_time": "2020-09-19T14:34:59",
        "upload_time_iso_8601": "2020-09-19T14:34:59.950788Z",
        "url": "https://files.pythonhosted.org/packages/5e/c4/4e8ff0e25582effdad0a208acb6bda8717485d24dc5e4516f9ed2fa47e40/meguru_tokenizer-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "711ef1bd799fd7421073f36ad9c744198621c6195f12bec6e54b57210d27343d",
          "md5": "6ca47784d65b7bac0c7751e1257ef9ed",
          "sha256": "06f9815f2d5d13126bafe5b626dbde9b9d77c1998b3382e6339dc5c7daed7139"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6ca47784d65b7bac0c7751e1257ef9ed",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 14092,
        "upload_time": "2020-09-20T05:03:09",
        "upload_time_iso_8601": "2020-09-20T05:03:09.506123Z",
        "url": "https://files.pythonhosted.org/packages/71/1e/f1bd799fd7421073f36ad9c744198621c6195f12bec6e54b57210d27343d/meguru_tokenizer-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "16e0e049b7e0994da655b2f7651a9acbfc9a135d90c2ad0c260e465f4aee7791",
          "md5": "ff2fcf200ae46a12e333646a203d53ff",
          "sha256": "ed0569bab301211997ffcd2d87d3b6ab654d9092d5593c3335fe0641d873992f"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ff2fcf200ae46a12e333646a203d53ff",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 10498,
        "upload_time": "2020-09-20T05:03:12",
        "upload_time_iso_8601": "2020-09-20T05:03:12.916078Z",
        "url": "https://files.pythonhosted.org/packages/16/e0/e049b7e0994da655b2f7651a9acbfc9a135d90c2ad0c260e465f4aee7791/meguru_tokenizer-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "39be263bf90a36817a5148bc77424443949e7fb4b694ec281a3817a177772979",
          "md5": "a132a01711db50ac322cae6c9b2ba93d",
          "sha256": "db94b3f78b6d04ab65b033d91a5fdb266e0c0dbe39fa5b9383cbe7524eeb2d40"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a132a01711db50ac322cae6c9b2ba93d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.5",
        "size": 14101,
        "upload_time": "2020-09-20T05:21:38",
        "upload_time_iso_8601": "2020-09-20T05:21:38.856652Z",
        "url": "https://files.pythonhosted.org/packages/39/be/263bf90a36817a5148bc77424443949e7fb4b694ec281a3817a177772979/meguru_tokenizer-0.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0d097aa91f918d057302aa9311b60630436acb834a96ca0adaa8c2687f1da797",
          "md5": "6b29afd7513de71f52940ef0dcfaffe3",
          "sha256": "5a2e8af6123f94ed6c59115327f0ea00803454d8b13c965efff9b12e4ba1bd19"
        },
        "downloads": -1,
        "filename": "meguru_tokenizer-0.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "6b29afd7513de71f52940ef0dcfaffe3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5",
        "size": 10487,
        "upload_time": "2020-09-20T05:21:40",
        "upload_time_iso_8601": "2020-09-20T05:21:40.490780Z",
        "url": "https://files.pythonhosted.org/packages/0d/09/7aa91f918d057302aa9311b60630436acb834a96ca0adaa8c2687f1da797/meguru_tokenizer-0.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "39be263bf90a36817a5148bc77424443949e7fb4b694ec281a3817a177772979",
        "md5": "a132a01711db50ac322cae6c9b2ba93d",
        "sha256": "db94b3f78b6d04ab65b033d91a5fdb266e0c0dbe39fa5b9383cbe7524eeb2d40"
      },
      "downloads": -1,
      "filename": "meguru_tokenizer-0.3.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "a132a01711db50ac322cae6c9b2ba93d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.5",
      "size": 14101,
      "upload_time": "2020-09-20T05:21:38",
      "upload_time_iso_8601": "2020-09-20T05:21:38.856652Z",
      "url": "https://files.pythonhosted.org/packages/39/be/263bf90a36817a5148bc77424443949e7fb4b694ec281a3817a177772979/meguru_tokenizer-0.3.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0d097aa91f918d057302aa9311b60630436acb834a96ca0adaa8c2687f1da797",
        "md5": "6b29afd7513de71f52940ef0dcfaffe3",
        "sha256": "5a2e8af6123f94ed6c59115327f0ea00803454d8b13c965efff9b12e4ba1bd19"
      },
      "downloads": -1,
      "filename": "meguru_tokenizer-0.3.1.tar.gz",
      "has_sig": false,
      "md5_digest": "6b29afd7513de71f52940ef0dcfaffe3",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5",
      "size": 10487,
      "upload_time": "2020-09-20T05:21:40",
      "upload_time_iso_8601": "2020-09-20T05:21:40.490780Z",
      "url": "https://files.pythonhosted.org/packages/0d/09/7aa91f918d057302aa9311b60630436acb834a96ca0adaa8c2687f1da797/meguru_tokenizer-0.3.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}