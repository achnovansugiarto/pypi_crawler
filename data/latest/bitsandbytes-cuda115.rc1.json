{
  "info": {
    "author": "Tim Dettmers",
    "author_email": "dettmers@cs.washington.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# bitsandbytes\n\nBitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers and quantization functions.\n\n[Paper](https://arxiv.org/abs/2110.02861) -- [Video](https://www.youtube.com/watch?v=IxrlHAJtqKE) -- [Docs](https://bitsandbytes.readthedocs.io/en/latest/)\n\n## TL;DR\n**Installation**:\n1. Note down version: ``conda list | grep cudatoolkit``\n2. Replace 111 with the version that you see: ``pip install bitsandbytes-cuda111``\n\n**Usage**:\n1. Comment out optimizer: ``#torch.optim.Adam(....)``\n2. Add 8-bit optimizer of your choice ``bnb.optim.Adam8bit(....)`` (arguments stay the same)\n3. Replace embedding layer if necessary: ``torch.nn.Embedding(..) -> bnb.nn.Embedding(..)``\n\n\n## Features\n- 8-bit Optimizers: Adam, AdamW, RMSProp, LARS, LAMB (saves 75% memory)\n- Stable Embedding Layer: Improved stability through better initialization, and normalization\n- 8-bit quantization: Quantile, Linear, and Dynamic quantization\n- Fast quantile estimation: Up to 100x faster than other algorithms\n\n## Requirements & Installation\n\nRequirements: anaconda, cudatoolkit, pytorch\nHardware requirements: NVIDIA Maxwell GPU or newer (>=GTX 9XX)\nSupported CUDA versions: 9.2 - 11.3\n\nThe requirements can best be fulfilled by installing pytorch via anaconda. You can install PyTorch by following the [\"Get Started\"](https://pytorch.org/get-started/locally/) instructions on the official website.\n\nbitsandbytes is compatible with all major PyTorch releases and cudatoolkit versions, but for now, you need to select the right version manually. To do this run:\n\n```conda list | grep cudatoolkit```\n\nand take note of the Cuda version that you have installed. Then you can install bitsandbytes via:\n```bash\n# choices: {cuda92, cuda 100, cuda101, cuda102, cuda110, cuda111, cuda113}\n# replace XXX with the respective number\npip install bitsandbytes-cudaXXX\n```\n\nTo check if your installation was successful, you can execute the following command, which runs a single bnb Adam update.\n```\nwget https://gist.githubusercontent.com/TimDettmers/1f5188c6ee6ed69d211b7fe4e381e713/raw/4d17c3d09ccdb57e9ab7eca0171f2ace6e4d2858/check_bnb_install.py && python check_bnb_install.py\n```\n\n## Using bitsandbytes\n\n### Using the 8-bit Optimizers\n\nWith bitsandbytes 8-bit optimizers can be used by changing a single line of code in your codebase. For NLP models we recommend also to use the StableEmbedding layers (see below) which improves results and helps with stable 8-bit optimization.  To get started with 8-bit optimizers, it is sufficient to replace your old optimizer with the 8-bit optimizer in the following way:\n```python\nimport bitsandbytes as bnb\n\n# adam = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.995)) # comment out old optimizer\nadam = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995)) # add bnb optimizer\nadam = bnb.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.995), optim_bits=8) # equivalent\n\n\ntorch.nn.Embedding(...) ->  bnb.nn.StableEmbedding(...) # recommended for NLP models\n```\n\nNote that by default all parameter tensors with less than 4096 elements are kept at 32-bit even if you initialize those parameters with 8-bit optimizers. This is done since such small tensors do not save much memory and often contain highly variable parameters (biases) or parameters that require high precision (batch norm, layer norm). You can change this behavior like so:\n```\n# parameter tensors with less than 16384 values are optimized in 32-bit\n# it is recommended to use multiplies of 4096\nadam = bnb.optim.Adam8bit(model.parameters(), min_8bit_size=16384) \n```\n\n### Change Bits and other Hyperparameters for Individual Parameters\n\nIf you want to optimize some unstable parameters with 32-bit Adam and others with 8-bit Adam, you can use the `GlobalOptimManager`. With this, we can also configure specific hyperparameters for particular layers, such as embedding layers. To do that, we need two things: (1) register the parameter while they are still on the CPU, (2) override the config with the new desired hyperparameters (anytime, anywhere). See our [guide](howto_config_override.md) for more details\n\n### Fairseq Users\n\nTo use the Stable Embedding Layer, override the respective `build_embedding(...)` function of your model. Make sure to also use the `--no-scale-embedding` flag to disable scaling of the word embedding layer (nor replaced with layer norm). You can use the optimizers by replacing the optimizer in the respective file (`adam.py` etc.).\n\n## Release and Feature History\n\nFor upcoming features and changes and full history see [Patch Notes](CHANGELOG.md).\n\n## Errors\n\n1. RuntimeError: CUDA error: no kernel image is available for execution on the device. [Solution](errors_and_solutions.md#No-kernel-image-available)\n2. __fatbinwrap_.. [Solution](errors_and_solutions.md#fatbinwrap_)\n\n## Compile from source\n\nTo compile from source, please follow the [compile_from_source.md](compile_from_source.md) instructions.\n\n## License\n\nThe majority of bitsandbytes is licensed under MIT, however portions of the project are available under separate license terms: Pytorch is licensed under the BSD license.\n\nWe thank Fabio Cannizzo for his work on [FastBinarySearch](https://github.com/fabiocannizzo/FastBinarySearch) which we use for CPU quantization.\n\n## Citation\nIf you found this library and 8-bit optimizers or quantization routines useful, please consider citing out work.\n```\n@misc{dettmers2021optim8bit,\n      title={8-bit Optimizers via Block-wise Quantization},\n      author={Tim Dettmers and Mike Lewis and Sam Shleifer and Luke Zettlemoyer},\n      year={2021},\n      eprint={2110.02861},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://packages.python.org/bitsandbytes",
    "keywords": "gpu optimizers optimization 8-bit quantization compression",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bitsandbytes-cuda115",
    "package_url": "https://pypi.org/project/bitsandbytes-cuda115/",
    "platform": null,
    "project_url": "https://pypi.org/project/bitsandbytes-cuda115/",
    "project_urls": {
      "Homepage": "http://packages.python.org/bitsandbytes"
    },
    "release_url": "https://pypi.org/project/bitsandbytes-cuda115/0.26.0.post2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "8-bit optimizers and quantization routines.",
    "version": "0.26.0.post2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14719758,
  "releases": {
    "0.26.0.post2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "00a18b7e0b83686276c5036333c7a0f2d2c0fa70dff7646a1ac0cfdc6a2501ee",
          "md5": "827b01901eaf19a33cd9580238b8b782",
          "sha256": "2949f39586fed2c7729325037d312cbf7ffc4cf0986de1cd242e7a46af787847"
        },
        "downloads": -1,
        "filename": "bitsandbytes_cuda115-0.26.0.post2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "827b01901eaf19a33cd9580238b8b782",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 4309336,
        "upload_time": "2022-08-10T17:14:07",
        "upload_time_iso_8601": "2022-08-10T17:14:07.602011Z",
        "url": "https://files.pythonhosted.org/packages/00/a1/8b7e0b83686276c5036333c7a0f2d2c0fa70dff7646a1ac0cfdc6a2501ee/bitsandbytes_cuda115-0.26.0.post2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "55fe7e85e7f52c52a0e504f9f22edcc48c9916f33c751d47f61debe7c5ebae04",
          "md5": "a0766e615a1c8b63c4be6cb5e9ecf394",
          "sha256": "952b81d486ae69601dce0def306e73afe54ddf845dc01e405cc7e447004b5c04"
        },
        "downloads": -1,
        "filename": "bitsandbytes-cuda115-0.26.0.post2.tar.gz",
        "has_sig": false,
        "md5_digest": "a0766e615a1c8b63c4be6cb5e9ecf394",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4262422,
        "upload_time": "2022-08-10T17:14:10",
        "upload_time_iso_8601": "2022-08-10T17:14:10.043067Z",
        "url": "https://files.pythonhosted.org/packages/55/fe/7e85e7f52c52a0e504f9f22edcc48c9916f33c751d47f61debe7c5ebae04/bitsandbytes-cuda115-0.26.0.post2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "00a18b7e0b83686276c5036333c7a0f2d2c0fa70dff7646a1ac0cfdc6a2501ee",
        "md5": "827b01901eaf19a33cd9580238b8b782",
        "sha256": "2949f39586fed2c7729325037d312cbf7ffc4cf0986de1cd242e7a46af787847"
      },
      "downloads": -1,
      "filename": "bitsandbytes_cuda115-0.26.0.post2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "827b01901eaf19a33cd9580238b8b782",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 4309336,
      "upload_time": "2022-08-10T17:14:07",
      "upload_time_iso_8601": "2022-08-10T17:14:07.602011Z",
      "url": "https://files.pythonhosted.org/packages/00/a1/8b7e0b83686276c5036333c7a0f2d2c0fa70dff7646a1ac0cfdc6a2501ee/bitsandbytes_cuda115-0.26.0.post2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "55fe7e85e7f52c52a0e504f9f22edcc48c9916f33c751d47f61debe7c5ebae04",
        "md5": "a0766e615a1c8b63c4be6cb5e9ecf394",
        "sha256": "952b81d486ae69601dce0def306e73afe54ddf845dc01e405cc7e447004b5c04"
      },
      "downloads": -1,
      "filename": "bitsandbytes-cuda115-0.26.0.post2.tar.gz",
      "has_sig": false,
      "md5_digest": "a0766e615a1c8b63c4be6cb5e9ecf394",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 4262422,
      "upload_time": "2022-08-10T17:14:10",
      "upload_time_iso_8601": "2022-08-10T17:14:10.043067Z",
      "url": "https://files.pythonhosted.org/packages/55/fe/7e85e7f52c52a0e504f9f22edcc48c9916f33c751d47f61debe7c5ebae04/bitsandbytes-cuda115-0.26.0.post2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}