{
  "info": {
    "author": "Ivan Begtin",
    "author_email": "ivan@begtin.tech",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: Console",
      "Intended Audience :: Developers",
      "Intended Audience :: System Administrators",
      "License :: OSI Approved :: BSD License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: Software Development",
      "Topic :: System :: Networking",
      "Topic :: Terminals",
      "Topic :: Text Processing",
      "Topic :: Utilities"
    ],
    "description": "spcrawler: a command-line tool to backup Sharepoint public installation data from open API endpoint\r\n########################################################################################################################\r\n\r\nspcrawler is a command-line tool to backup Sharepoint public installation data from open API endpoint\r\nIt uses Sharepoint API located at \"/_api/web\" and dumps all data and resources.\r\n\r\n\r\n.. contents::\r\n\r\n.. section-numbering::\r\n\r\n\r\n\r\nMain features\r\n=============\r\n\r\n* Metadata extraction\r\n* Download all files (resources) from Sharepoint installation\r\n\r\n\r\n\r\nInstallation\r\n============\r\n\r\n\r\nAny OS\r\n-------------\r\n\r\nA universal installation method (that works on Windows, Mac OS X, Linux, â€¦,\r\nand always provides the latest version) is to use pip:\r\n\r\n\r\n.. code-block:: bash\r\n\r\n    # Make sure we have an up-to-date version of pip and setuptools:\r\n    $ pip install --upgrade pip setuptools\r\n\r\n    $ pip install --upgrade spcrawler\r\n\r\n\r\n(If ``pip`` installation fails for some reason, you can try\r\n``easy_install spcrawler`` as a fallback.)\r\n\r\n\r\nPython version\r\n--------------\r\n\r\nPython version 3.6 or greater is required.\r\n\r\nUsage\r\n=====\r\n\r\n\r\nSynopsis:\r\n\r\n.. code-block:: bash\r\n\r\n    $ spcrawler [command] [flags]\r\n\r\n\r\nSee also ``python -m spcrawler`` and ``spcrawler [command] --help`` for help for each command.\r\n\r\n\r\n\r\nCommands\r\n========\r\n\r\nPing command\r\n------------\r\nPings API endpoint located at url + \"/_api/web\" and returns OK if it's available.\r\n\r\n\r\nPing asutk.ru API endpoint\r\n\r\n.. code-block:: bash\r\n\r\n    $ spcrawler ping --url https://asutk.ru\r\n\r\n\r\n\r\nWalk command\r\n------------\r\nLists objects in Sharepoint installation\r\n\r\nWalks over FA.ru website objects\r\n\r\n.. code-block:: bash\r\n\r\n    $ spcrawler walk --url http://fa.ru\r\n\r\nDump command\r\n------------\r\nDumps all objects/lists/data from API to JSON lines files. Stores all data to local path \"domainname/data\"\r\n\r\nDumps data from FA.ru (Financial university) website\r\n\r\n.. code-block:: bash\r\n\r\n    $ spcrawler dump --url http://fa.ru\r\n\r\n\r\n\r\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "https://github.com/ruarxive/spcrawler/",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ruarxive/spcrawler/",
    "keywords": "backup archive metadata sharepoint",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "spcrawler",
    "package_url": "https://pypi.org/project/spcrawler/",
    "platform": null,
    "project_url": "https://pypi.org/project/spcrawler/",
    "project_urls": {
      "Download": "https://github.com/ruarxive/spcrawler/",
      "Homepage": "https://github.com/ruarxive/spcrawler/"
    },
    "release_url": "https://pypi.org/project/spcrawler/1.0.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "spcrawler:  A command-line tool to backup Sharepoint public installations data from open API endpoint",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13574818,
  "releases": {
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08176a622f12267b5ec232e5b5879deb16ea3efd008ecda586a331a9c3f388b7",
          "md5": "353ae57276d22bb14179521f5d21cdd8",
          "sha256": "bac072c72b50cfa40ce8dc1fa0429351acf93744c149995274387d39f80e060d"
        },
        "downloads": -1,
        "filename": "spcrawler-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "353ae57276d22bb14179521f5d21cdd8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7370,
        "upload_time": "2022-04-21T03:46:38",
        "upload_time_iso_8601": "2022-04-21T03:46:38.998747Z",
        "url": "https://files.pythonhosted.org/packages/08/17/6a622f12267b5ec232e5b5879deb16ea3efd008ecda586a331a9c3f388b7/spcrawler-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "08176a622f12267b5ec232e5b5879deb16ea3efd008ecda586a331a9c3f388b7",
        "md5": "353ae57276d22bb14179521f5d21cdd8",
        "sha256": "bac072c72b50cfa40ce8dc1fa0429351acf93744c149995274387d39f80e060d"
      },
      "downloads": -1,
      "filename": "spcrawler-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "353ae57276d22bb14179521f5d21cdd8",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 7370,
      "upload_time": "2022-04-21T03:46:38",
      "upload_time_iso_8601": "2022-04-21T03:46:38.998747Z",
      "url": "https://files.pythonhosted.org/packages/08/17/6a622f12267b5ec232e5b5879deb16ea3efd008ecda586a331a9c3f388b7/spcrawler-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}