{
  "info": {
    "author": "Karim Kanso",
    "author_email": "kaz.kanso@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Security"
    ],
    "description": "# wordlist-knife: your illiterate friend\n\nTool for merging, subtracting and generating wordlists.\n\n## Why was it made?\n\nWhen faced with the plethora for wordlists in [SecLists][SecLists], I\nfound it overwhelming and so stick to the few lists that I have\nbeen lucky with.\n\nIt is often not clear which wordlists are super/sub wordlists of\nothers. Worse, many of the wordlists are disjoint, but contain many\n1000's of identical entries. Thus, if just trying another wordlist\neither it is needed to waste time filtering them (e.g. with `grep\n-Fvxf`, but only to find one wordlist had `/` prepended to each entry,\nso its needed to use `sed` too) or just run with the larger wordlist\nand accept time is wasted.\n\nFurther, adding entries to wordlists is also a pain. The choices are:\n\n1. Maintain separate wordlists and concatenate them before using\n   (e.g. with `bash` process substitution). Alas, this just adds\n   complexity to commands.\n\n2. Create branch of SecLists and edit wordlists directly, then rebase\n   `branch` instead of `pull`ing so its clear what words you have\n   added over time.\n\n3. Just do it adhock.\n\nFinally, using standard command line tools for processing wordlists is\nfine for one-off operations. However, when doing this many times it\nobfuscates what is really happening on the command line and wastes\ntime on repetitive tasks. For example, a typical first scan of an\n`apache` server I might run is as follow:\n\n```\n$ grep -ve '^/\\.ht' quickhits.txt | ffuf -w - -u http://xyz/FUZZ\n```\n\nHowever, then the second scan could be (notice the regex also\nchanged):\n\n```\n$ grep -ve '^\\.ht' raft-files-small.txt | ffuf -w - -u http://xyz/FUZZ\n```\n\n\nThis tool aims to remove lots of the pain of using wordlists as\ndescribed above. At a high level, it reads a bunch or wordlists,\nremoves duplicate entries (possibly mangles them, e.g. like `sed`) and\nprints them out. Further more, it is possible to define composite\nwordlists in a config file can easily be referenced by a friendly\nname.\n\nThus, its easy to get some understanding how related two wordlists are\nby just subtracting one wordlist from another (or many others) and\nviewing the results. This is useful for getting some understanding\nabout the content of different wordlists.\n\nHowever, the real advantage to the tool is when using the config file\nto setup pre-defined lists. Each of these lists could further consist\nof a number of other lists, mangled (i.e. placing words into a normal\nform, like removing leading slashes or comments), de-duplicated, and\nfinally filtered by either other lists or a regex.\n\n\n## Installation\n\nThere are no special dependencies required. So a simple `pip install wordlist-knife` is\nall that is needed.\n\n## Examples\n\n[See command line args](#usage). \n\n### Checking for duplicates in wordlist\n\nOften this is a mark of quality of how well a wordlist was put\ntogether.\n\n```\n$ diff raft-large-directories.txt <(wlk raft-large-directories.txt)\n48562d48561\n< index\n59093,59094d59091\n< G\n< Ä¼\n```\n\nSometimes it is required to specify the encoding:\n\n```\n$ md5sum  rockyou.txt <(wlk --encoding latin1 rockyou.txt)\n9076652d8ae75ce713e23ab09e10d9ee  rockyou.txt\n9076652d8ae75ce713e23ab09e10d9ee  /dev/fd/63\n```\n\nThe size of rockyou will take a few seconds to fully check for\nduplicates. However, it will start printing checked lines immediately\nas there is no need to sort it.\n\n### Entries in one wordlist that are not in the other.\n\nUseful in cases where you have already used the first wordlist and\nwant to quickly check another wordlist without repeating the work\ndone, or you are just interested in the difference of two with a\nsimilar name.\n\n```\n$ wlk raft-large-directories-lowercase.txt --filters raft-large-directories.txt | wc -l\n4079\n```\n\nPossible to concatenate multiple lists first and then check:\n\n```\n$ wlk directory-list-2.3-medium.txt directory-list-2.3-small.txt --filter directory-list-2.3-big.txt 'regex:^#.*$'\n```\n\nHere a regex is used to also filter comments from the output. This\nregex is also hardcoded and could have been given as:\n\n```\n$ wlk directory-list-2.3-medium.txt directory-list-2.3-small.txt --filter directory-list-2.3-big.txt save:comments\n```\n\n### Wordlist generation\n\nSupport for quickly generating permutations of a *small* number of\nwords is provided. This is useful for generating possible combinations\nof first and last names, names of companies.\n\nSo for a user `j bloggs` the following could be generated:\n\n```\n$ wlk fancy:/j/bloggs --fancy-concat . --fancy-no-empty --filter list:/J.BLOGGS/bloggs.J\nj\nJ\nbloggs\nBLOGGS\nBloggs\nj.bloggs\nj.BLOGGS\nj.Bloggs\nJ.bloggs\nJ.Bloggs\nbloggs.j\nBLOGGS.j\nBLOGGS.J\nBloggs.j\nBloggs.J\n```\n\nNotice, here also two items are explicitly removed from the list for\ndemonstration purposes of the `list` syntax.\n\nCurrently only capitalisation of the base words are varied. However,\nthe possibilities quickly blow up exponentially, so its advisable to\nkeep the number of words low and limit the number of chars passed to\n`--fancy-concat` based on the context.\n\n\nIt is also possible to combine generated wordlists into existing\nlists. For example to include a company name into a wordlist used for\nfuzzing:\n\n```\n$ wlk fancy:/company/name directory-list-2.3-small.txt\n```\n\nThe output of this could be piped to the application or via process\nsubstitution.\n\n### Mangle lists (i.e. apply `sed` like modifications)\n\nThere are occasions where its handy to tweak entries in a\nwordlist. The following is an example where it both mangles any php\nfile to search for temporary backup files and then filters the list to\nonly select words with php in.\n\n```\n$ wlk list:/index.php/123/def/isphpok.txt --manglers 'subst:/^([^.]+)\\.php/#\\1.php#' --filter 'regex:^(?!.*php)'\n#index.php#\nisphpok.txt\n```\n\nIn addition, there is also an `upper` and `lower` mangle that does a\nsimple upper or lower case operation on the word.\n\nOther manglers, such as urlencode, base64, computing digests have not\nbeen implemented.\n\n### Saved lists\n\nTypically, only a few lists are used regularly. It is possible to\ncreate aliases for these lists in `~/.wordlist-knife`. In fact, its\npossible to define lists that are aggregated, filtered and mangled in\nthis file. \n\nFor example, if there is a wordlist that is used lots but needs some\nadditional items its possible to define the list in this file with a\nfew extra items appended to it, or a filter applied or even mangled.\n\nThe format of this file is `json`. As an example of this applied to\nthe `quickhits.txt` file (see [SecLists][SecLists]) could be as\nfollows:\n\n```json\n{\n  \"quick\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/quickhits.txt\"\n    ],\n    \"manglers\": [\n      \"subst:|^/|\"\n    ],\n    \"filters\": [],\n    \"desc\": \"quickhits.txt with leading slashes stripped\"\n  }\n}\n```\n\nHere, `wordlists`, `filters` and `manglers` follow the same convention\nas when passed on the command line. With the difference, when passed\non the command line they are applied globally, when defined here they\nare only applied to the wordlist being defined. The `desc` is\noptional, and displayed in the help.\n\nThen its possible use this defined list as follows:\n\n```\n$ wlk save:quick\n```\n\nA major advantage of this is that it greatly simplifies management of\nwordlists. Especially when using 3rd party ones, such as from\n[SecLists][SecLists]. Before I would create my own git commits with\ncustom tweaks in the files that I have found useful, however this\nwould require rebaseing the repo from time to time. In addition, it is\nneeded to remember the full name file and path of the repo, and then\noften filter the wordlist through `grep` to remove undesirable\nlines. Saving these configurations into a config file with names that\nare meaningful to myself greatly reduces the burden.\n\n\n### Full example\n\nWithin my config file, I define a number of wordlists that dont\noverlap and are aggregates of standard wordlists. This means its\npossible to use smaller wordlists before going to larger ones. When\nusing the larger ones, I dont waste time repeating already used words\n(or remembering names of lesser  wordlist or typing paths):\n\n```\n$ wlk save:dir1 --filt save:apache | ffuf -w - -u http://xyz/FUZZ\n```\n\nor with files too:\n\n```\n$ wlk save:dir1 save:files1 --filt save:apache | ffuf -w - -u http://xyz/FUZZ\n```\n\n\nThen, if there is nothing of interest run a larger scan:\n\n```\n$ wlk save:dir2 --filt save:apache | ffuf -w - -u http://xyz/FUZZ\n```\n\n\nAs an example of a config file that is based off wordlists available\nfrom [SecLists][SecLists], see below. It makes use of many of the\nfeatures:\n\n```json\n{\n  \"tomcat\": {\n      \"filters\": [\n          \"regex:[\\\\[\\\\]]\"\n      ]\n  },\n  \"dir0\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/raft-small-directories.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/raft-small-directories-lowercase.txt\",\n      \"list:|cgi-bin/\"\n    ],\n    \"desc\": \"raft-directories-words.txt + raft-small-directories-lowercase.txt\"\n  },\n  \"dir1\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/raft-large-directories.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/raft-large-directories-lowercase.txt\",\n      \"list:,.hg\"\n    ],\n    \"filters\": [\n      \"save:dir0\"\n    ],\n    \"desc\": \"raft-large-directories.txt + raft-large-directories-lowercase.txt | filtered by dir0\"\n  },\n  \"dir2\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/directory-list-2.3-big.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/directory-list-lowercase-2.3-big.txt\"\n    ],\n    \"filters\": [\n      \"save:comments\",\n      \"save:dir0\",\n      \"save:dir1\",\n      \"regex:%0[aAdD]\"\n    ],\n    \"desc\": \"directory-list-2.3-big.txt | filtered by dir0 and dir1\"\n  },\n  \"file0\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/raft-small-files.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/raft-small-files-lowercase.txt\"\n    ],\n    \"desc\": \"raft-small-files.txt + raft-small-files-lowercase.txt\"\n  },\n  \"file1\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/raft-large-files.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/raft-large-files-lowercase.txt\",\n      \"list:,composer.json,vendor/composer/installed.json\"\n    ],\n    \"filters\": [\n      \"save:file0\"\n    ],\n    \"desc\": \"raft-large-files.txt + raft-large-files-lowercase.txt | filtered by file0\"\n  },\n  \"file2\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/common-and-french.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/PHP.fuzz.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/common-and-spanish.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/IIS.fuzz.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/CGI-XPlatform.fuzz.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/common-and-italian.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/CGIs.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/ascx.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/asp.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/aspx.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/cfm.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/cpp.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/css.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/cs.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/c.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/html.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/jar.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/java.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/jspf.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/jsp.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/js.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/php3.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/php5.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/phpt.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/php.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/pl.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/py.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/rb.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/sh.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/swf.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/tpl.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/vb.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/SVNDigger/cat/Language/wsdl.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/CommonBackdoors-ASP.fuzz.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/CommonBackdoors-JSP.fuzz.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/CommonBackdoors-PHP.fuzz.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/CommonBackdoors-PL.fuzz.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/Common-DB-Backups.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/Common-PHP-Filenames.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/Logins.fuzz.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/Roundcube-123.txt\"\n    ],\n    \"filters\": [\n      \"save:comments\",\n      \"save:dir0\",\n      \"save:dir1\",\n      \"save:dir2\",\n      \"save:file0\",\n      \"save:file1\",\n      \"regex:/$\",\n      \"regex:%0[aAdD]\"\n    ],\n    \"manglers\": [\n      \"subst:/\\\\?.*$/\",\n      \"subst:|^/|\"\n    ],\n    \"desc\": \"assorted selection of filenames from SecLists/Discovery/Web-Content\"\n  },\n  \"quick\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/quickhits.txt\",\n      \"list:,robots.txt,~root/,cgi-bin/,sitemap.xml\"\n    ],\n    \"manglers\": [\n      \"subst:|^/|\"\n    ],\n    \"desc\": \"quickhits.txt with leading slashes stripped\"\n  },\n  \"words\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/raft-large-words.txt\",\n      \"/path/to/SecLists/Discovery/Web-Content/raft-large-words-lowercase.txt\",\n      \"list:,.hg\"\n    ],\n    \"desc\": \"raft-large-words.txt + raft-large-words-lowercase.txt\"\n  },\n  \"dir\": {\n    \"wordlists\": [\n      \"save:dir0\",\n      \"save:dir1\"\n    ],\n    \"desc\": \"raft-large-directories.txt + raft-large-directories-lowercase.txt, i.e. dir0 and dir1\"\n  },\n  \"file\": {\n    \"wordlists\": [\n      \"save:file0\",\n      \"save:file1\"\n    ],\n    \"desc\": \"raft-large-files.txt + raft-large-files-lowercase.txt, i.e. file0 and file1\"\n  },\n  \"dns\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/DNS/subdomains-top1million-20000.txt\"\n    ],\n    \"filters\": [\n      \"list:/gc._msdcs/#www/#mail/_domainkey\"\n    ]\n  },\n  \"dark-1k\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Passwords/darkweb2017-top1000.txt\"\n    ]\n  },\n  \"dark-10k\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Passwords/darkweb2017-top10000.txt\"\n    ]\n  },\n  \"params\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Discovery/Web-Content/burp-parameter-names.txt\"\n    ]\n  },\n  \"names\": {\n    \"wordlists\": [\n      \"/path/to/SecLists/Usernames/xato-net-10-million-usernames-dup.txt\"\n    ]\n  }\n}\n```\n\n## Usage\n\n```\n$ wlk -h         \nusage: wlk [-h] [--filters FILT [FILT ...]] [--manglers MNGL [MNGL ...]]\n           [--encoding ENC] [--fancy-no-empty-concat]\n           [--fancy-concat-chars FANCY_CHARS]\n           LIST [LIST ...]\n\n  Tool for mangling wordlists. Works by reading in one ore more source\nwordlists, removing duplicates entries and then filtering the words. When the\nwords are output, the original order is preserved (with exception of removed\nentries). Also has support for basic wordlist generation from keywords.\n\n  Can be used in number of ways from merging wordlists, or just appending new\nwords to an existing wordlist, filtering existing wordlists or supplementing\nwordlists with keywords (and some derivations).\n\n  Currently there is no support for sorting, as its often preferable to have\nwordlists start with common words.\n\npositional arguments:\n  LIST                  Source wordlists. These must occur before the \"--\n                        filters\" argument, if used, otherwise argparse can not\n                        determine where the parameters belong. These arguments\n                        can be paths to wordlist files or specifications of\n                        wordlists to generate.\n\n                        Each argument is processed as follows: First, it is\n                        check to determine if its a file that can be opened. If\n                        so, its opened and the lines are used. Otherwise it\n                        will be interpreted as \"tag:value\" where the tag\n                        determines the meaning of the value. The following tags\n                        are supported:\n\n                        * save: Pre-configured lists, e.g. save:apache. These\n                        are either hardcoded (such as apache) and typically for\n                        use with with --filters, or read from the ~/.wordlist-\n                        knife file as aliases for wordlist paths. This file\n                        should be json, see\n                        https://github.com/kazkansouh/wordlist-knife for an\n                        example.\n\n                        * fancy: Generate variants of list with\n                        upper/lower/title cases. Same format as list. Useful\n                        for deriving a number of words from a small number of\n                        known words, e.g. a company name or a person. Given as:\n                        \"fancy:/jo/bloggs\" to generate variants.\n\n                        * list: Define constant list, first char is delimiter.\n                        e.g. list:/abc/123. Useful for quickly adding/removing\n                        a few words to a wordlist.\n\n                        * file: Read a wordlist file from path.\n\n\n                        The following lists are predefined (i.e. for use with\n                        save):\n\n                        * file: raft-large-files.txt + raft-large-files-\n                        lowercase.txt, i.e. file0 and file1\n\n                        * dir: raft-large-directories.txt + raft-large-\n                        directories-lowercase.txt, i.e. dir0 and dir1\n\n                        * words: raft-large-words.txt + raft-large-words-\n                        lowercase.txt\n\n                        * quick: quickhits.txt with leading slashes stripped\n\n                        * file2: assorted selection of filenames from\n                        SecLists/Discovery/Web-Content\n\n                        * file1: raft-large-files.txt + raft-large-files-\n                        lowercase.txt | filtered by file0\n\n                        * file0: raft-small-files.txt + raft-small-files-\n                        lowercase.txt\n\n                        * dir2: directory-list-2.3-big.txt | filtered by dir0\n                        and dir1\n\n                        * dir1: raft-large-directories.txt + raft-large-\n                        directories-lowercase.txt | filtered by dir0\n\n                        * dir0: raft-directories-words.txt + raft-small-\n                        directories-lowercase.txt\n\n                        * comments: ^#.*$ (regex)\n\n                        * apache: (^|/)\\.ht (regex), ^\\.php[34567]?$ (regex),\n                        \\.phps$ (regex)\n\n\noptional arguments:\n  -h, --help            show this help message and exit\n\n  --filters FILT [FILT ...]\n                        Items of the filter argument are removed from the\n                        source wordlists. Each filter follows the same rules as\n                        the wordlists, i.e. can be a file or specified with a\n                        tag.\n\n                        However, in addition, it also supports the \"regex\" tag\n                        that filters the words based on a given regex. E.g.\n                        \"regex:\\.phps$\".\n\n  --manglers MNGL [MNGL ...]\n                        Mangle (i.e. mutate) words. Each mangler is applied in\n                        order given on command line.\n\n                        The following manglers are supported:\n\n                        * subst: Regex substitution, first char is delimiter.\n                        e.g. to prefix php pages with \"x_\" the following could\n                        be used: \"subst:/^([^.]+)\\.php/x_.php\".\n\n                        * lower: Convert words to lowercase.\n\n                        * upper: Convert words to uppercase.\n\n  --encoding ENC        Specify the encoding to use for io. E.g. when using\n                        rockyou.txt it is required to set this to latin1.\n                        (default: utf8).\n\n  --fancy-no-empty-concat\n                        Disable joining words with the empty string for fancy.\n\n  --fancy-concat-chars FANCY_CHARS\n                        List of chars to use for joining words for fancy.\n                        (default: -_)\n\n\n  wordlist-knife v0.0.1. Copyright (C) 2020 Karim Kanso. All Rights Reserved.\n```\n\n# Other bits\n\nCopyright 2020, Karim Kanso. All rights reserved. Project licensed under GPLv3.\n\n[SecLists]: https://github.com/danielmiessler/SecLists \"GitHub.com: SecLists\"\n[git]: https://github.com/kazkansouh/wordlist-knife \"GitHub.com: wordlist-knife\"\n[pypi]: https://pypi.org/project/wordlist-knife/ \"PyPI: wordlist-knife\"\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/kazkansouh/wordlist-knife",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "wordlist-knife",
    "package_url": "https://pypi.org/project/wordlist-knife/",
    "platform": "",
    "project_url": "https://pypi.org/project/wordlist-knife/",
    "project_urls": {
      "Homepage": "https://github.com/kazkansouh/wordlist-knife"
    },
    "release_url": "https://pypi.org/project/wordlist-knife/0.0.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Tool to mutate wordlists.",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7703395,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "deeb901c77a4d4a1f1d353ada74bc6026c6b682edde9cbc14766a355dc81db53",
          "md5": "b471933211283f6eadabb25b1da6834d",
          "sha256": "97321552d5e131ebb02a3da518b6a44cfcdf8e7fed99e4025c621078dae38d33"
        },
        "downloads": -1,
        "filename": "wordlist_knife-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b471933211283f6eadabb25b1da6834d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 28453,
        "upload_time": "2020-05-23T11:10:37",
        "upload_time_iso_8601": "2020-05-23T11:10:37.018685Z",
        "url": "https://files.pythonhosted.org/packages/de/eb/901c77a4d4a1f1d353ada74bc6026c6b682edde9cbc14766a355dc81db53/wordlist_knife-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c5567753b986ec862eac87a1490b47788b57456844668e4f41689836dba5c6dd",
          "md5": "9e10d4d2211d11e874b5391c4246083e",
          "sha256": "d73d13dac75ba50066e6e2e3611478c4c2aa4f87c7d53f5673b3d83d5937be98"
        },
        "downloads": -1,
        "filename": "wordlist-knife-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "9e10d4d2211d11e874b5391c4246083e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18132,
        "upload_time": "2020-05-23T11:10:39",
        "upload_time_iso_8601": "2020-05-23T11:10:39.289371Z",
        "url": "https://files.pythonhosted.org/packages/c5/56/7753b986ec862eac87a1490b47788b57456844668e4f41689836dba5c6dd/wordlist-knife-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "39ba957dcf5533dcfc1029dcd062fc2257cbeab566f6feffe4d509acfa9d706a",
          "md5": "83554bd0b67cf134d493504ea9a6a5f2",
          "sha256": "3bc2328878b20734b01b5ad106922d1ce450f4527c045f32b19717861758d1dd"
        },
        "downloads": -1,
        "filename": "wordlist_knife-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "83554bd0b67cf134d493504ea9a6a5f2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 28805,
        "upload_time": "2020-07-15T07:47:15",
        "upload_time_iso_8601": "2020-07-15T07:47:15.325081Z",
        "url": "https://files.pythonhosted.org/packages/39/ba/957dcf5533dcfc1029dcd062fc2257cbeab566f6feffe4d509acfa9d706a/wordlist_knife-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4b1a41e37148655e5de9d45503105b461265b1ed035b371dc9fa0d1f86c86fe1",
          "md5": "091828af1f299fba7a116349e4551031",
          "sha256": "e156a8acf37c3e9e950a9a0cd8b9e3724d2084f3d1cdc9c7fb7eb3c7e913563e"
        },
        "downloads": -1,
        "filename": "wordlist-knife-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "091828af1f299fba7a116349e4551031",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 19273,
        "upload_time": "2020-07-15T07:47:16",
        "upload_time_iso_8601": "2020-07-15T07:47:16.893382Z",
        "url": "https://files.pythonhosted.org/packages/4b/1a/41e37148655e5de9d45503105b461265b1ed035b371dc9fa0d1f86c86fe1/wordlist-knife-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "39ba957dcf5533dcfc1029dcd062fc2257cbeab566f6feffe4d509acfa9d706a",
        "md5": "83554bd0b67cf134d493504ea9a6a5f2",
        "sha256": "3bc2328878b20734b01b5ad106922d1ce450f4527c045f32b19717861758d1dd"
      },
      "downloads": -1,
      "filename": "wordlist_knife-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "83554bd0b67cf134d493504ea9a6a5f2",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 28805,
      "upload_time": "2020-07-15T07:47:15",
      "upload_time_iso_8601": "2020-07-15T07:47:15.325081Z",
      "url": "https://files.pythonhosted.org/packages/39/ba/957dcf5533dcfc1029dcd062fc2257cbeab566f6feffe4d509acfa9d706a/wordlist_knife-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4b1a41e37148655e5de9d45503105b461265b1ed035b371dc9fa0d1f86c86fe1",
        "md5": "091828af1f299fba7a116349e4551031",
        "sha256": "e156a8acf37c3e9e950a9a0cd8b9e3724d2084f3d1cdc9c7fb7eb3c7e913563e"
      },
      "downloads": -1,
      "filename": "wordlist-knife-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "091828af1f299fba7a116349e4551031",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 19273,
      "upload_time": "2020-07-15T07:47:16",
      "upload_time_iso_8601": "2020-07-15T07:47:16.893382Z",
      "url": "https://files.pythonhosted.org/packages/4b/1a/41e37148655e5de9d45503105b461265b1ed035b371dc9fa0d1f86c86fe1/wordlist-knife-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}