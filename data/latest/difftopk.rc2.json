{
  "info": {
    "author": "Felix Petersen",
    "author_email": "ads0600@felix-petersen.de",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Scientific/Engineering :: Mathematics",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# Differentiable Top-k Classification Learning\n\n![difftopk_logo](difftopk_logo.png)\n\nOfficial implementation for our ICML 2022 Paper \"Differentiable Top-k Classification Learning\".\n\nThe `difftopk` library provides different differentiable sorting and ranking methods as well as a wrapper for using them\nin a `TopKCrossEntropyLoss`. `difftopk` builds on PyTorch.\n\nPaper @ [ArXiv](https://arxiv.org/abs/2206.07290),\nVideo @ [Youtube](https://www.youtube.com/watch?v=J-lZV72DCic).\n\n## 💻 Installation\n\n`difftopk` can be installed via pip from PyPI with\n```shell\npip install difftopk\n```\n\n### Sparse Computation\n\nFor the functionality of evaluating the differentiable topk operators in a sparse way, the package `torch-sparse` has to be installed.\nThis can be done, e.g., via\n```shell\npip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.0+cpu.html\n```\nFor more information on how to install `torch-sparse`, see [here](https://github.com/rusty1s/pytorch_sparse#installation).\n\n### Example for Full Installation from Scratch and with all Dependencies\n\n<details>\n  <summary>(<i>click to expand</i>)</summary>\n\nDepending on your system, the following commands will have to be adjusted.\n\n```shell\nvirtualenv -p python3 .env_topk\n. .env_topk/bin/activate\npip install boto3 numpy requests scikit-learn tqdm \npip install torch==1.13.0+cu116 torchvision==0.14.0+cu116 -f https://download.pytorch.org/whl/torch_stable.html\npip install diffsort\n\n# optional for torch-sparse\nFORCE_CUDA=1 pip install --no-cache-dir torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n\npip install .\n```\n\n</details>\n\n---\n\n## 👩‍💻 Documentation\n\nThe `difftopk` library provides of differentiable sorting and ranking methods as well as a wrapper for using them in a\n`TopKCrossEntropyLoss`. The differentiable sorting and ranking methods included are:\n\n* Variants of Differentiable Sorting Networks\n  * `bitonic` Bitonic Differentiable Sorting Networks (sparse)\n  * `bitonic__non_sparse` Bitonic Differentiable Sorting Networks\n  * `splitter_selection` Differentiable Splitter Selection Networks (sparse)\n  * `odd_even` Odd-Even Differentiable Sorting Networks\n* `neuralsort` NeuralSort\n* `softsort` SoftSort\n\nFurthermore, this library also includes the smooth top-k loss from Lapin et al. (`SmoothTopKLoss` and `SmoothHardTopKLoss`.)\n\n### `TopKCrossEntropyLoss`\nIn the center of the library lies the `difftopk.TopKCrossEntropyLoss`, which may be used as a drop-in replacement for\n`torch.nn.CrossEntropyLoss`. The signature of `TopKCrossEntropyLoss` is defined as follows:\n\n```python\nloss_fn = difftopk.TopKCrossEntropyLoss(\n    diffsort_method='odd_even',       # the sorting / ranking method as discussed above\n    inverse_temperature=2,            # the inverse temperature / steepness\n    p_k=[.5, 0., 0., 0., .5],         # the distribution P_K\n    n=1000,                           # number of classes\n    m=16,                             # the number m of scores to be sorted (can be smaller than n to make it efficient)\n    distribution='cauchy',            # the distribution used for differentiable sorting networks\n    art_lambda=None,                  # the lambda for the ART used if `distribution='logistic_phi'`\n    device='cpu',                     # the device to compute the loss on\n    top1_mode='sm'                    # makes training more stable and is the default value\n)\n```\n\nIt can be used as `loss_fn(outputs, labels)`.\n\n### `DiffTopkNet`\n\n`difftopk.DiffTopkNet` follows the signature of `diffsort.DiffSortNet` from the [`diffsort` package](https://github.com/Felix-Petersen/diffsort#-usage).\nHowever, instead of returning the full differentiable permutation matrices of size `n`x`n`, it returns differentiable top-k attribution matrices of size `n`x`k`.\nMore specifically, given an input of shape `b`x`n`, the module returns a tuple of `None` and a Tensor of shape `b`x`n`x`k`. \n(It returns a tuple to maintain the signature of `DiffSortNet`.)\n\n```python\nsorter = difftopk.DiffTopkNet(\n    sorting_network_type='bitonic',\n    size=16,                          # Number of inputs\n    k=5,                              # k\n    sparse=True,                      # whether to use a sparse implementation\n    device='cpu',                     # the device\n    steepness=10.0,                   # the inverse temperature\n    art_lambda=0.25,                  # the lambda for the ART used if `distribution='logistic_phi'`\n    distribution='cauchy'             # the distribution used for the differentiable relaxation\n)\n\n# Usage example for difftopk on a random input\nx = torch.randperm(16).unsqueeze(0).float() * 100.\nprint(x @ sorter(x)[1][0])\n```\n\n### `NeuralSort` / `SoftSort`\n\n```python\nsorter = difftopk.NeuralSort(\n    tau=2.,     # A temperature parameter\n)\nsorter = difftopk.SoftSort(tau=2.)\n```\n\n---\n\n## 🧪 Experiments\n\n### 🧫 ImageNet Fine-Tuning\n\nWe provide pre-computed embeddings for the ImageNet data set. ⚠️ These embedding files are very large (>10 GB each.)\nFeel free to also use the embeddings for other fine-tuning experiments.\n\n```shell\n# Resnext101 WSL ImageNet-1K (~11GB each)\nwget https://nyc3.digitaloceanspaces.com/publicdata1/ImageNet_embeddings_labels_train_test_IGAM_Resnext101_32x48d_320.p\nwget https://nyc3.digitaloceanspaces.com/publicdata1/ImageNet_embeddings_labels_train_test_IGAM_Resnext101_32x32d_320.p\nwget https://nyc3.digitaloceanspaces.com/publicdata1/ImageNet_embeddings_labels_train_test_IGAM_Resnext101_32x16d_320.p\nwget https://nyc3.digitaloceanspaces.com/publicdata1/ImageNet_embeddings_labels_train_test_IGAM_Resnext101_32x8d_320.p\n\n# Resnext101 WSL ImageNet-21K-P (~50GB)\nwget https://publicdata1.nyc3.digitaloceanspaces.com/ImageNet21K-P_embeddings_labels_train_test_IGAM_Resnext101_32x48d_224_float16.p\n\n# Noisy Student EfficientNet-L2 ImageNet-1K (~14GB)\nwget https://publicdata1.nyc3.digitaloceanspaces.com/ImageNet_embeddings_labels_train_test_tf_efficientnet_l2_ns_timm_transform_800_float16.p\n```\n\nThe following are the hyperparameter combinations for reproducing the tables in the paper.\nThe DiffSortNet entries in Table 5 can be reproduced using\n\n```shell\npython experiments/train_imagenet.py  -d ./ImageNet_embeddings_labels_train_test_IGAM_Resnext101_32x48d_320.p  --nloglr 4.5 \\\n    --p_k .2 .2 .2 .2 .2  --m 16  --method bitonic  --distribution logistic_phi  --inverse_temperature 1.  --art_lambda .5 \npython experiments/train_imagenet.py  -d ./ImageNet_embeddings_labels_train_test_IGAM_Resnext101_32x48d_320.p  --nloglr 4.5 \\\n    --p_k .2 .2 .2 .2 .2  --m 16  --method splitter_selection  --distribution logistic_phi  --inverse_temperature 1.  --art_lambda .5\n\npython experiments/train_imagenet.py  -d ./ImageNet_embeddings_labels_train_test_tf_efficientnet_l2_ns_timm_transform_800_float16.p  --nloglr 4.5 \\\n    --p_k .25 .0 .0 .0 .75  --m 16  --method bitonic  --distribution logistic  --inverse_temperature .5 \npython experiments/train_imagenet.py  -d ./ImageNet_embeddings_labels_train_test_tf_efficientnet_l2_ns_timm_transform_800_float16.p  --nloglr 4.5 \\\n    --p_k .25 .0 .0 .0 .75  --m 16  --method splitter_selection  --distribution logistic  --inverse_temperature .5 \n```\nand, for the remaining methods and tables, the hyperparameters are defined in the following:\n\n<details>\n  <summary>(<i>click to expand</i>)</summary>\n\n```shell\n# Tables 2+3 (1K):\npython experiments/train_imagenet.py -d ./ImageNet_embeddings_labels_train_test_IGAM_Resnext101_32x48d_320.p  --m 16  --nloglr 4.5\n\n# Tables 2+3 (21K):\npython experiments/train_imagenet.py -d ./ImageNet21K-P_embeddings_labels_train_test_IGAM_Resnext101_32x48d_224_float16.p  --m 50  --nloglr 4.  --n_epochs 40 \n\n# combined with one of each of the following\n\n--method softmax_cross_entropy\n--method bitonic             --distribution logistic_phi  --inverse_temperature 1.  --art_lambda .5\n--method splitter_selection  --distribution logistic_phi  --inverse_temperature 1.  --art_lambda .5\n--method neuralsort          --inverse_temperature 0.5\n--method softsort            --inverse_temperature 0.5\n--method smooth_hard_topk    --inverse_temperature 1. \n\n--p_k 1.  0. 0. 0. 0.\n--p_k 0.  0. 0. 0. 1.\n--p_k .5  0. 0. 0. .5\n--p_k .25 0. 0. 0. .75\n--p_k .1  0. 0. 0. .9\n--p_k .2  .2 .2 .2 .2\n```\n\n</details>\n\n### 🎆 CIFAR-100 Training from Scratch\n\nIn addition to ImageNet fine-tuning, we can also train a ResNet18 on CIFAR-100 from scratch. \n\n<details>\n  <summary>(<i>click to expand</i>)</summary>\n\n```shell\n# Tables 1+4:\npython experiments/train_cifar100.py\n\n--method softmax_cross_entropy\n--method bitonic             --distribution logistic_phi  --inverse_temperature .5  --art_lambda .5\n--method splitter_selection  --distribution logistic_phi  --inverse_temperature .5  --art_lambda .5\n--method neuralsort          --inverse_temperature 0.0625\n--method softsort            --inverse_temperature 0.0625\n--method smooth_hard_topk    --inverse_temperature 1.\n\n--p_k 1.  0. 0. 0. 0.\n--p_k 0.  0. 0. 0. 1.\n--p_k .5  0. 0. 0. .5\n--p_k .25 0. 0. 0. .75\n--p_k .1  0. 0. 0. .9\n--p_k .2  .2 .2 .2 .2\n  \n# Examples:\npython experiments/train_cifar100.py --method softmax_cross_entropy --p_k 1. 0. 0. 0. 0.\npython experiments/train_cifar100.py --method splitter_selection --distribution logistic_phi --inverse_temperature .5 --art_lambda .5 --p_k .2 .2 .2 .2 .2\n```\n\n</details>\n\n## 📖 Citing\n\n```bibtex\n@inproceedings{petersen2022difftopk,\n  title={{Differentiable Top-k Classification Learning}},\n  author={Petersen, Felix and Kuehne, Hilde and Borgelt, Christian and Deussen, Oliver},\n  booktitle={International Conference on Machine Learning (ICML)},\n  year={2022}\n}\n```\n\n## 📜 License\n\n`difftopk` is released under the MIT license. See [LICENSE](LICENSE) for additional details about it.\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Felix-Petersen/difftopk",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "difftopk",
    "package_url": "https://pypi.org/project/difftopk/",
    "platform": null,
    "project_url": "https://pypi.org/project/difftopk/",
    "project_urls": {
      "Homepage": "https://github.com/Felix-Petersen/difftopk"
    },
    "release_url": "https://pypi.org/project/difftopk/0.2.0/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Differentiable Sorting, Ranking, and Top-k.",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16289041,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d8935cee61e7b0dab390637883612bb0c3e880ff8433b18d1d74aabfe407984",
          "md5": "30b60d3c4381cf664a45073e93ff079b",
          "sha256": "bc152a046dae96f9f1fbadedeac559e6df2f7295e27f68d2eaead550fb672d28"
        },
        "downloads": -1,
        "filename": "difftopk-0.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "30b60d3c4381cf664a45073e93ff079b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 1291,
        "upload_time": "2022-04-21T11:15:10",
        "upload_time_iso_8601": "2022-04-21T11:15:10.921917Z",
        "url": "https://files.pythonhosted.org/packages/5d/89/35cee61e7b0dab390637883612bb0c3e880ff8433b18d1d74aabfe407984/difftopk-0.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9cc4c0f5bd05d8644579739633a35f0a5853b73862a3600dc60176f97470d8dd",
          "md5": "3108b84fca92d5b571798bc2b1f44e30",
          "sha256": "2ded2ffa587bad2613497c666a952e0b6fed54cfffee4522a66928293408e1ee"
        },
        "downloads": -1,
        "filename": "difftopk-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3108b84fca92d5b571798bc2b1f44e30",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 1439,
        "upload_time": "2022-04-21T11:15:12",
        "upload_time_iso_8601": "2022-04-21T11:15:12.480085Z",
        "url": "https://files.pythonhosted.org/packages/9c/c4/c0f5bd05d8644579739633a35f0a5853b73862a3600dc60176f97470d8dd/difftopk-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4d5af06dd12b5548147bd70d2547c8079fbaa797d88b60847d00e20093a734e3",
          "md5": "74caaa7c77042d740e939a89c44ba523",
          "sha256": "f9282a9e07bc2ff95e060af3c04d956b0d3ad9a8ef1c842bf5e535ebc880294d"
        },
        "downloads": -1,
        "filename": "difftopk-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "74caaa7c77042d740e939a89c44ba523",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 20173,
        "upload_time": "2023-01-03T12:00:43",
        "upload_time_iso_8601": "2023-01-03T12:00:43.562250Z",
        "url": "https://files.pythonhosted.org/packages/4d/5a/f06dd12b5548147bd70d2547c8079fbaa797d88b60847d00e20093a734e3/difftopk-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4d5af06dd12b5548147bd70d2547c8079fbaa797d88b60847d00e20093a734e3",
        "md5": "74caaa7c77042d740e939a89c44ba523",
        "sha256": "f9282a9e07bc2ff95e060af3c04d956b0d3ad9a8ef1c842bf5e535ebc880294d"
      },
      "downloads": -1,
      "filename": "difftopk-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "74caaa7c77042d740e939a89c44ba523",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 20173,
      "upload_time": "2023-01-03T12:00:43",
      "upload_time_iso_8601": "2023-01-03T12:00:43.562250Z",
      "url": "https://files.pythonhosted.org/packages/4d/5a/f06dd12b5548147bd70d2547c8079fbaa797d88b60847d00e20093a734e3/difftopk-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}