{
  "info": {
    "author": "Kun JIN",
    "author_email": "jin.kun@flykun.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Topic :: Software Development :: Build Tools"
    ],
    "description": "词频、互信息、信息熵发现中文新词\n================================\n\n**新词发现**\\ 任务是中文自然语言处理的重要步骤。\\ **新词**\\ 有“新”就有“旧”，属于一个相对个概念，在相对的领域（金融、医疗），在相对的时间（过去、现在）都存在新词。\\ `文本挖掘 <https://zh.wikipedia.org/wiki/文本挖掘>`__\\ 会先将文本\\ `分词 <https://zh.wikipedia.org/wiki/中文自动分词>`__\\ ，而通用分词器精度不过，通常需要添加\\ **自定义字典**\\ 补足精度，所以发现新词并加入字典，成为文本挖掘的一个重要工作。\n\n`单词 <https://zh.wikipedia.org/wiki/單詞>`__\\ 的定义，来自维基百科的定义如下：\n\n    在语言学中，\\ **单词**\\ （又称为词、词语、单字；英语对应用语为“word”）是能独立运用并含有语义内容或语用内容（即具有表面含义或实际含义）的最小单位。单词的集合称为词汇、术语，例如：所有中文单词统称为“中文词汇”，医学上专用的词统称为“医学术语”等。词典是为词语提供音韵、词义解释、例句、用法等等的工具书，有的词典只修录特殊领域的词汇。\n\n单从语义角度，“苹果“的法语是”pomme”，而“土豆”的法语是“pomme de\nterre”，若按上面的定义，“土豆”是要被拆的面目全非，但“pomme de\nterre”是却是表达“土豆”这个语义的最小单位；在机构名中，这类问题出现的更频繁，“Paris\n3”是“巴黎第三大学”的简称，如果“Paris”和“3”分别表示地名和数字，那这两个就无法表达“巴黎第三大学”的语义。而中文也有类似的例子，“北京大学”的”北京“和”大学“都可以作为一个最小单位来使用，分别表示”地方名“和“大学”，如果这样分词，那么就可以理解为“北京的大学”了，所以“北京大学”是一个表达语义的最小单位。前几年有部电影《夏洛特烦恼》，我们是要理解为“夏洛特\n烦恼“还是”夏洛 特 烦恼“，这就是很经典的分词问题。\n\n但是从语用角度，这些问题似乎能被解决，我们知道“pomme de\nterre”在日常生活中一般作为“土豆”而不是“土里的苹果”，在巴黎学习都知道“Paris\n3”，就像我们提到“北京大学”特指那所著名的高等学府一样。看过电影《夏洛特烦恼》的观众很容易的就能区分这个标题应该看为“夏洛\n特 烦恼”。\n\n发现新词的方法，《\\ `互联网时代的社会语言学：基于SNS的文本数据挖掘 <http://www.matrix67.com/blog/archives/5044%5D>`__\n》一文，里面提到的给每一个文本串计算\\ **文本片段**\\ 的\\ **凝固程度**\\ 和文本串对外的使用\\ **自由度**\\ ，通过设定阈值来将文本串分类为词和非词两类。原文给了十分通俗易懂的例子来解释凝固度和自动度。这里放上计算方法。这个方法还有许多地方需要优化，在之后的实践中慢慢调整了。\n\n环境\n----\n\n::\n\n    python >= 3.5\n\n安装\n----\n\n.. code:: bash\n\n    python setup.py install\n\n使用说明\n--------\n\n.. code:: python\n\n    import wordiscovery as wd\n\n    text = \"新词发现任务是中文自然语言处理的重要步骤。\n    新词有新就有旧，属于一个相对个概念，在相对的领域（金融、医疗），\n    在相对的时间（过去、现在）都存在新词。文本挖掘会先将文本分词，\n    而通用分词器精度不过，通常需要添加自定义字典补足精度，\n    所以发现新词并加入字典，成为文本挖掘的一个重要工作。\n    \"\n\n    f = wd.Wordiscovery()\n\n    # 解析过程默认参数, 根据文本自由调节这几个阈值\n    # 最小信息熵0.01\n    # 最小互信息4\n    # 最小词频2\n    f.parse(text)  # f.parse(text, 0.01, 4, 2)\n    # {'分词': (2, 5.18271944179699, 0.6931471805599453),\n    # '字典': (2, 6.2813317304651, 0.6931471805599453),\n    # '文本': (3, 4.895037369345209, 0.6365141682948128),\n    # '文本挖掘': (2, 5.588184549905154, 0.6931471805599453),\n    # '新词': (4, 4.371789225580661, 1.0397207708399179),\n    # '相对': (3, 4.3842117455792184, 0.6365141682948128),\n    # '精度': (2, 6.2813317304651, 0.6931471805599453),\n    # '通常': (2, 5.18271944179699, 0.6931471805599453),\n    # '重要': (2, 5.028568761969732, 0.6931471805599453),\n    # '需要': (2, 5.028568761969732, 0.6931471805599453),\n    # '领域': (2, 6.2813317304651, 0.6931471805599453)}\n\n详细说明\n--------\n\n`wordicovery解释 <https://github.com/Ushiao/wordiscovery/blob/master/docs/wordiscovery.ipynb>`__",
    "description_content_type": null,
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ushiao/wordiscovery",
    "keywords": "NLP,new word discorvery",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "wordiscovery",
    "package_url": "https://pypi.org/project/wordiscovery/",
    "platform": "",
    "project_url": "https://pypi.org/project/wordiscovery/",
    "project_urls": {
      "Homepage": "https://github.com/ushiao/wordiscovery"
    },
    "release_url": "https://pypi.org/project/wordiscovery/0.1.4.6/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A Chinese new word discovery",
    "version": "0.1.4.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 3413507,
  "releases": {
    "0.1.4.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a1ea3e3106e6c8ce96106fd231e863a007255e272b610c52b1a305f0efec15b1",
          "md5": "b3999a340746be349fe514cb0d85c4f2",
          "sha256": "ecd1a57d4c407bc8936cc33f86b1e2441f00850ad4be81edb27f343639e1ea2b"
        },
        "downloads": -1,
        "filename": "wordiscovery-0.1.4.6.tar.gz",
        "has_sig": false,
        "md5_digest": "b3999a340746be349fe514cb0d85c4f2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 7001,
        "upload_time": "2017-12-13T10:03:04",
        "upload_time_iso_8601": "2017-12-13T10:03:04.257479Z",
        "url": "https://files.pythonhosted.org/packages/a1/ea/3e3106e6c8ce96106fd231e863a007255e272b610c52b1a305f0efec15b1/wordiscovery-0.1.4.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a1ea3e3106e6c8ce96106fd231e863a007255e272b610c52b1a305f0efec15b1",
        "md5": "b3999a340746be349fe514cb0d85c4f2",
        "sha256": "ecd1a57d4c407bc8936cc33f86b1e2441f00850ad4be81edb27f343639e1ea2b"
      },
      "downloads": -1,
      "filename": "wordiscovery-0.1.4.6.tar.gz",
      "has_sig": false,
      "md5_digest": "b3999a340746be349fe514cb0d85c4f2",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 7001,
      "upload_time": "2017-12-13T10:03:04",
      "upload_time_iso_8601": "2017-12-13T10:03:04.257479Z",
      "url": "https://files.pythonhosted.org/packages/a1/ea/3e3106e6c8ce96106fd231e863a007255e272b610c52b1a305f0efec15b1/wordiscovery-0.1.4.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}