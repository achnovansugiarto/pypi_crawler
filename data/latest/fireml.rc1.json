{
  "info": {
    "author": "Anatoly Belikov",
    "author_email": "awbelikov@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# README #\n\nCaffe-like machine learning framework in python\n\n\n## layers types\n\n### ImageData\nlayer to read images. Possible sources: txt file with path labels on each line, cifar archive.\n\n**image_data_param**:\n\nsource: string - path to cifar archive or txt file\n\nbatch_size: int - how many images to process in each iteration\n\nshuffle: bool - images will be shuffled when sampling for a batch\n\nnew_height: int - new image height(can be same as original)\n\nnew_width: int - new image width(can be same as original)\n\nnew_labels: int - expected number of labels(txt file may contain multiple labels for each path)\n\nexample:\n\n     layer {\n       top: \"data\"\n       top: \"label\"\n       name: \"data\"\n       type: \"ImageData\"\n       image_data_param {\n         source: \"../cifar/cifar-10-python.tar.gz\"\n         source: \"data.txt.3\"  # use cifar or txt file\n         batch_size: 65\n         shuffle: true\n         new_height: 32\n         new_width: 32\n         n_labels: 10\n       }\n       transform_param {\n         mean_value: 126 # r\n         mean_value: 123 # g\n         mean_value: 114 # b\n         mirror: true\n         scale: 0.02728125\n         standard_params {\n             var_average: 5000\n             mean_average: 5000\n             mean_per_channel: false\n             var_per_channel: false\n         }\n       }\n       include: { phase: TRAIN }\n     }\n\n**transform_param**\n\nParameters for data preprocessing\n\n**standard_params**\n\nParameters for preprocessor for data standardization.\nTo achieve zero mean and unit variance the preprocessor will subtract iterative \nmean from each sample and divide the result by standard deviation.\n\n    standard_params {\n        var_average: 1\n        mean_average: 1\n        mean_per_channel: false\n        var_per_channel: true\n    }\n\nvar_average: int [default = 0] - use last var_average samples to compute variance and std  \ndisabled if var_average == 0  \nmean_average: int [default = 0]  - use last var_average samples to compute mean  \ndisabled if mean_average == 0  \nmean_per_channel: bool [default = false] - subtract from each channel mean for that channel  \nvar_per_channel: [default = false] - divide each channel by separate std value  \n\n### Convolution\n\nConvolution of 2-3 d images(matrices)\n\n**convolution_param**\n\nnum_output: int   number of filters(output feature maps)\n\nkernel_size:  int    size of receptive field of filters. Receptive field is kernel_size * kernel_size\n\nstride: int - filter will be applied after stride pixels\n\nweight_filler: see weight filler\n\nexample:\n\n    layer {\n      name: \"conv1\"\n      type: \"Convolution\"\n      bottom: \"data\"\n      top: \"conv1\"\n      convolution_param {\n        num_output: 40\n        kernel_size: 3\n        stride: 2\n        weight_filler {\n          type: \"xavier\"\n          variance_norm: AVERAGE\n        }\n      }\n    }\n\n\n### Pooling\n\nSubsampling layer for max or average pooling\n\n**pooling_param**\n\npool: MAX or AVE\n\nkernel_size: int \nsubsampling window size\n\nstride: int\nperform pooling each *stride* pixels\n\nexample:\n\n    layer {\n      name: \"pool1\"\n      type: \"Pooling\"\n      bottom: \"conv1\"\n      top: \"pool1\"\n      pooling_param {\n        pool: MAX\n        kernel_size: 3\n        stride: 2\n      }\n    }\n\n### Accuracy\n\nLayer for computing accuracy\nAccuracy of a classifier is defined as (true positive + true negative)/total  \nIn multilabel classification example counts as correctly classified iff **all** outputs\n are correct.  \nExample:   \n\n    layer {\n      name: \"accuracy\"\n      type: \"Accuracy\"\n      bottom: \"pool10\"\n      bottom: \"label\"\n      top: \"accuracy\"\n    }\n\n## weight_filler\n\nWeigth filler parameters are common for all layers with weights\n\ntype: string  \n\"xavier\", \"gaussian\", \"uniform\"\n\nmean: float  \nmean value for gaussian initialization\n\nstd: float  \nstandard deviation for gaussian initialization\n\nmin: float  \nlower bound for uniform initialization\n\nmax: float  \nupper bound for uniform initialization\n\n## Activation functions\n\n### SeLU\n\nSelf-regularized linear unit:\n\nexample:  \n    {\n      name: \"relu_conv1\"\n      type: \"SeLU\"\n      bottom: \"conv1\"\n      top: \"conv1\"\n    }\n\n## Loss layers\n\n### SigmoidCrossEntropyLoss\n\nLayer that applies sigmoid elementwise, followed by cross-entropy log loss\n-mean(sum(y * log(p(y))  + (1 - y) * log(1 - p(y))))\n\nwhere p(y) - sigmoid transformation of layer's input, that is vector of independent probabilities for each class.\n\n    example:  \n        layer { \n          name: \"loss\"\n          type: \"SigmoidCrossEntropyLoss\"\n          bottom: \"pool1\"\n          bottom: \"label\"\n          top: \"loss\"\n          include {\n              phase: TRAIN\n          }\n        }\n\n## Maxout layer\n\nApply max operator for each *size* channels  \n\nsize: int [default = 0] - take max over each *size* channels  \nlambda: int [default = 0.0] - apply probabilistic max if lambda != 0  \n\n    layer {\n      name: \"maxout_1\"\n      type: \"Maxout\"\n      maxout_param {\n        lambda: 1\n        size: 2\n      }\n      bottom: \"conv1\"\n      top: \"conv1\"\n    }",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://bitbucket.org/noSkill/fireml",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "fireml",
    "package_url": "https://pypi.org/project/fireml/",
    "platform": "",
    "project_url": "https://pypi.org/project/fireml/",
    "project_urls": {
      "Homepage": "https://bitbucket.org/noSkill/fireml"
    },
    "release_url": "https://pypi.org/project/fireml/0.1.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "fireml machine learning framework",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4380761,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b5097ec331593cb0bfc242d657695890f217eea7176837f04b25acb1645c3251",
          "md5": "59c141c27a63b196ded35149c0f963ae",
          "sha256": "730dee4422d0d12b73c265acca96aedbe090f30410e4303dd24493cbb79be7b5"
        },
        "downloads": -1,
        "filename": "fireml-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "59c141c27a63b196ded35149c0f963ae",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 93720,
        "upload_time": "2018-10-16T09:41:56",
        "upload_time_iso_8601": "2018-10-16T09:41:56.226507Z",
        "url": "https://files.pythonhosted.org/packages/b5/09/7ec331593cb0bfc242d657695890f217eea7176837f04b25acb1645c3251/fireml-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b5097ec331593cb0bfc242d657695890f217eea7176837f04b25acb1645c3251",
        "md5": "59c141c27a63b196ded35149c0f963ae",
        "sha256": "730dee4422d0d12b73c265acca96aedbe090f30410e4303dd24493cbb79be7b5"
      },
      "downloads": -1,
      "filename": "fireml-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "59c141c27a63b196ded35149c0f963ae",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 93720,
      "upload_time": "2018-10-16T09:41:56",
      "upload_time_iso_8601": "2018-10-16T09:41:56.226507Z",
      "url": "https://files.pythonhosted.org/packages/b5/09/7ec331593cb0bfc242d657695890f217eea7176837f04b25acb1645c3251/fireml-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}