{
  "info": {
    "author": "ken",
    "author_email": "kenbliky@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# kcrawler\n\n[![Build Status](https://travis-ci.org/kenblikylee/kcrawler.svg?branch=master)](https://travis-ci.org/kenblikylee/kcrawler)\n[![license](https://img.shields.io/github/license/kenblikylee/kcrawler)](https://github.com/kenblikylee/kcrawler/blob/master/LICENSE)\n\nA python crawler authored by Ken.\n\n\n## 1. 安装\n\n### 1.1 环境要求\n\n- python>=3.0\n- pip>=19.0\n\n``` sh\npython -V\npip install --upgrade pip\npip -V\n```\n\n### 1.2 查看最新版本\n\n``` sh\npip search kcrawler\n```\n\n### 1.3 初次安装\n\n``` sh\npip install kcrawler\n# or\npip install --index-url https://pypi.org/simple kcrawler\n```\n\n### 1.4 更新已有安装\n\n``` sh\npip install --upgrade kcrawler\n# or\npip install --upgrade --index-url https://pypi.org/simple kcrawler\n```\n\n### 1.5 卸载\n\n``` sh\npip uninstall -y kcrawler\n```\n\n## 2. 命令行调用\n\n### 2.1 使用方式\n\n使用 pip 安装成功后，会自动在系统搜索路径创建可执行程序：`kcrawler`, `kcanjuke`, `kcjuejin`。\n\n> 通常是 `python` 或 `conda` 安装目录下的 `bin` 子目录下，例如：`/anaconda3/bin/kcrawler`。windows 平台会创建 `.exe` 文件。\n\n`kcrawler` 是爬取所有网站应用的入口，命令执行格式如下：\n\n``` sh\nkcrawler <webapp> [webapp-data] [--options]\n```\n\n等效于：\n\n``` sh\nkc<webapp> [webapp-data] [--options]\n```\n\n例如：\n\n```\nkcrawler juejin books --url \"https://...\"\nkcjuejin books --url \"https://...\"\n```\n\n### 2.2 使用示例\n\n以 `kcrawler <webapp> [webapp-data] [--options]` 方式运行为例。\n\n#### 2.2.1 爬取掘金小册数据\n\n执行如下命令：\n\n``` sh\nkcrawler juejin book\n```\n\n命令执行成功，显示如下统计图表：\n\n![](http://cdn.kenblog.top/juejin_books_927.png)\n\n并将明细数据保存在当前目录下，同时保存 `.csv` 和 `.xls` 文件，文件名格式如下：\n\n`juejin_books_YYYY-MM-DD.csv` `juejin_books_YYYY-MM-DD.xls`\n\n#### 2.2.2 爬取掘金专栏阅读量\n\n格式：\n\n``` sh\nkcrawler juejin post --name <username> --limit 100 --url '<user_post_url>'\n```\n\n- name: 目标爬取用户的名称，可以自定义，仅仅用于区分不同用户，同时作为爬取数据保存的文件夹名称\n- limit: 限制爬取最新专栏数\n- url: 目标爬取用户的接口地址，这个参数真正决定了要爬取谁的专栏\n\nurl 获取方式如下：\n\n![](http://cdn.kenblog.top/juejin_post_url.png)\n\n为了快速体验爬取效果，也提供了 url 缺省情况下的支持，爬取用户 [ken](https://juejin.im/user/5bd2b8b25188252a784d19d7/posts) 的专栏：\n\n``` sh\nkcrawler juejin post --name ken --limit 100\n```\n\n爬取明细数据，会在 `ken` 目录下，以爬取日期和时间命名，同时保存 `.csv` 文件和 `.xls` 文件。\n\n#### 2.2.3 指定城市爬取安居客小区房价\n\n首先需要获取[网站](https://shenzhen.anjuke.com/community/p50/)的 `cookie` 。获取方式参考[《python 自动抓取分析房价数据——安居客版 》2.4 小节](https://juejin.im/post/5d7f021bf265da03cf7abed2#heading-9)。\n\n\n将 `<anjuke_cookie>` 替换成自己 `cookie`，运行如下命令：\n\n``` sh\nkcrawler anjuke --city shenzhen --limit 50 --cookie \"<anjuke_cookie>\"\n```\n\n也可以将 `cookie` 保存在当前目录下的 `anjuke_cookie` (无后缀)文件中，运行如下命令：\n\n``` sh\nkcrawler anjuke --city shenzhen --limit 50\n```\n\n![](http://cdn.kenblog.top/kcrawler_anjuke_shenzhen.gif)\n\n![](http://cdn.kenblog.top/sz_com_927.png)\n\n命令成功运行成功后，会显示房价平均值，最大值，最小值，并绘制房价分布直方图，关闭直方图后，明细数据将保存在当前目录下，形如：`anjuke_shenzhen_community_price_20xx-xx-xx.csv`。\n\n> 获取其他城市的房价，只需将 `city` 参数改成安居客网站覆盖的城市拼音。可打开页面 [https://www.anjuke.com/sy-city.html](https://www.anjuke.com/sy-city.html) ，点击需要获取的城市，复制浏览器地址栏中城市对应的二级域名，如 beijing.anjuke.com 只取 beijing 作为 city 参数。\n\n## 3. 导入 python 模块\n\n### 3.1 Boss 接口\n\n``` python\nfrom kcrawler import Boss\nboss  = Boss()\n\nboss_positions = boss.position()\nboss_cities = boss.city()\nboss_hotcities = boss.hotcity()\nboss_industries = boss.industry()\nboss_user_city = boss.userCity()\nboss_expects = boss.expect()\n\njobs = boss.job(0, 1)\ntencent_jobs = boss.queryjob(query='腾讯', city=101280600, industry=None, position=101301)\ntencent_jobs = boss.queryjobpage(query='腾讯', city=101280600, industry=None, position=101301, page=2)\n\njobcard = boss.jobcard('3c2016bbf8413f3b1XR63t-1FVI~', '505ee74b-504b-4aea-921c-a3dc2016be80.f1:common-155-GroupA--157-GroupA.15')\n```\n\n## Release history\n\n[https://pypi.org/project/kcrawler/#history](https://pypi.org/project/kcrawler/#history)\n\n## License\n\n[MIT](http://opensource.org/licenses/MIT)\n\nCopyright (c) 2019 kenblikylee\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/kenblikylee/kcrawler",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "kcrawler",
    "package_url": "https://pypi.org/project/kcrawler/",
    "platform": "",
    "project_url": "https://pypi.org/project/kcrawler/",
    "project_urls": {
      "Homepage": "https://github.com/kenblikylee/kcrawler"
    },
    "release_url": "https://pypi.org/project/kcrawler/1.1/",
    "requires_dist": [
      "requests (>=2.21.0)",
      "beautifulsoup4 (>=4.7.1)",
      "pandas (>=0.25.1)",
      "matplotlib (>=3.0.3)",
      "xlwt (>=1.3.0)",
      "Pillow (>=6.1.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "A python crawler authored by Ken.",
    "version": "1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6084356,
  "releases": {
    "0.0.18": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dcab209002fd165c2f9efe12ea4e0ee041e6fd29fb6a062f132066106e1743c9",
          "md5": "3e11881a20152bf633f65424e3bb73fb",
          "sha256": "900d5fadb25950cc701b5807c63c2b5b8a91abda7e693726e44a29162c559348"
        },
        "downloads": -1,
        "filename": "kcrawler-0.0.18-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3e11881a20152bf633f65424e3bb73fb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 12083,
        "upload_time": "2019-09-26T12:28:27",
        "upload_time_iso_8601": "2019-09-26T12:28:27.818843Z",
        "url": "https://files.pythonhosted.org/packages/dc/ab/209002fd165c2f9efe12ea4e0ee041e6fd29fb6a062f132066106e1743c9/kcrawler-0.0.18-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b3945a06fb71a66e66cb889c6982837a3cf0d86bc19447c0ae56bc3dbd599109",
          "md5": "c1ea57a6a9e05f39e43eb6a7160cd8c5",
          "sha256": "7f8278f0d9f9fe56cbf95457af72673f3fdec0aed01c29b3fcf30940ab10ce4f"
        },
        "downloads": -1,
        "filename": "kcrawler-0.0.18.tar.gz",
        "has_sig": false,
        "md5_digest": "c1ea57a6a9e05f39e43eb6a7160cd8c5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 7907,
        "upload_time": "2019-09-26T12:28:30",
        "upload_time_iso_8601": "2019-09-26T12:28:30.618779Z",
        "url": "https://files.pythonhosted.org/packages/b3/94/5a06fb71a66e66cb889c6982837a3cf0d86bc19447c0ae56bc3dbd599109/kcrawler-0.0.18.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3a78c1660d892caee48798400c86484f67d7a3553702f1fe09e4ece18009ef83",
          "md5": "00ae73fa33a8d1218a5f36b67641714c",
          "sha256": "51bba9a7fc32f71e43532ef96356fb19602c27bc314f372607687298b0d8e0e9"
        },
        "downloads": -1,
        "filename": "kcrawler-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "00ae73fa33a8d1218a5f36b67641714c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 12414,
        "upload_time": "2019-09-27T02:18:39",
        "upload_time_iso_8601": "2019-09-27T02:18:39.444325Z",
        "url": "https://files.pythonhosted.org/packages/3a/78/c1660d892caee48798400c86484f67d7a3553702f1fe09e4ece18009ef83/kcrawler-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "48be88b8f364a2e8dd8a8d90c3103bf1802046d0030d9b9d1ecc1d6a17637803",
          "md5": "0fcc6243b4d6df7d1eed2af781062cba",
          "sha256": "3377c5e4506bf7d6ef3e8aca60ed6ba4abffcf447f3f90de498ee613c8d4909e"
        },
        "downloads": -1,
        "filename": "kcrawler-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0fcc6243b4d6df7d1eed2af781062cba",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 8208,
        "upload_time": "2019-09-27T02:18:41",
        "upload_time_iso_8601": "2019-09-27T02:18:41.796179Z",
        "url": "https://files.pythonhosted.org/packages/48/be/88b8f364a2e8dd8a8d90c3103bf1802046d0030d9b9d1ecc1d6a17637803/kcrawler-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8a7fef2b98c6b951e729715903a95383b9cd4db3bc95ca079e1d8928c5e8d8ef",
          "md5": "794f7cc7bc345e6a8a63fdee94fda798",
          "sha256": "7d2aa13caf61cd168d8bb05007a055add22783740cb24cc21287b4173d42f05a"
        },
        "downloads": -1,
        "filename": "kcrawler-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "794f7cc7bc345e6a8a63fdee94fda798",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 17389,
        "upload_time": "2019-10-15T16:46:37",
        "upload_time_iso_8601": "2019-10-15T16:46:37.297227Z",
        "url": "https://files.pythonhosted.org/packages/8a/7f/ef2b98c6b951e729715903a95383b9cd4db3bc95ca079e1d8928c5e8d8ef/kcrawler-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4bc1a8484dd734c5e11ad56279dd5421c5f7746a7ee6fce71178d52b2d79731e",
          "md5": "1a25603f71d037e3efe16fe8ec84ecc3",
          "sha256": "638a3f62d5ca74158bfcfb0a6b3ee4da9c6f685ebe092f775eeb99bda8ec2169"
        },
        "downloads": -1,
        "filename": "kcrawler-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "1a25603f71d037e3efe16fe8ec84ecc3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 12899,
        "upload_time": "2019-10-15T16:46:39",
        "upload_time_iso_8601": "2019-10-15T16:46:39.577359Z",
        "url": "https://files.pythonhosted.org/packages/4b/c1/a8484dd734c5e11ad56279dd5421c5f7746a7ee6fce71178d52b2d79731e/kcrawler-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f4ab17fcf175afaa9bf72d9a7fec20fc80862264ea34be0c2b9596438998be8f",
          "md5": "28035da418ee5654a567b45118dcd9f0",
          "sha256": "ecce519c57617d7d627f774b98cc2d551fc4860709f314c20188dcc552f1cc2d"
        },
        "downloads": -1,
        "filename": "kcrawler-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "28035da418ee5654a567b45118dcd9f0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 17398,
        "upload_time": "2019-10-17T09:15:01",
        "upload_time_iso_8601": "2019-10-17T09:15:01.344137Z",
        "url": "https://files.pythonhosted.org/packages/f4/ab/17fcf175afaa9bf72d9a7fec20fc80862264ea34be0c2b9596438998be8f/kcrawler-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "478d2024688a20cc387001cf48cd528d31872c9cca5fca263afbea2a66ebb25f",
          "md5": "91c68b19e3200c531f7b5da6d7c5a468",
          "sha256": "b615cf500a7d0844441532a8d12fb6f8af6602a133f3b81174fbafe35853003e"
        },
        "downloads": -1,
        "filename": "kcrawler-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "91c68b19e3200c531f7b5da6d7c5a468",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 15270,
        "upload_time": "2019-10-17T09:15:03",
        "upload_time_iso_8601": "2019-10-17T09:15:03.494780Z",
        "url": "https://files.pythonhosted.org/packages/47/8d/2024688a20cc387001cf48cd528d31872c9cca5fca263afbea2a66ebb25f/kcrawler-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9bd137060caa40c4d78fd0ea0fbe8c680a73a6f131186ff318b7839a4407a284",
          "md5": "8dd316183db75805eed3e0ffd2938fbb",
          "sha256": "5f07579d961d0c5dd3abe981f90ea8a0ffc50c3e77c8c5a256b928263de754fd"
        },
        "downloads": -1,
        "filename": "kcrawler-1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8dd316183db75805eed3e0ffd2938fbb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 17360,
        "upload_time": "2019-11-06T02:27:02",
        "upload_time_iso_8601": "2019-11-06T02:27:02.957226Z",
        "url": "https://files.pythonhosted.org/packages/9b/d1/37060caa40c4d78fd0ea0fbe8c680a73a6f131186ff318b7839a4407a284/kcrawler-1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "07c7c5c0c2fe2232203e3b3bd60029f03b4b135c77dc5c0b80dd57b464b67ea8",
          "md5": "6888a1c1c75c78d42e2f4fabd1209eb4",
          "sha256": "f569987ce1e16f5e2a2f65a27ae84cf6183ac09e9869a2d67d75741a659247ed"
        },
        "downloads": -1,
        "filename": "kcrawler-1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "6888a1c1c75c78d42e2f4fabd1209eb4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 12904,
        "upload_time": "2019-11-06T02:27:05",
        "upload_time_iso_8601": "2019-11-06T02:27:05.055915Z",
        "url": "https://files.pythonhosted.org/packages/07/c7/c5c0c2fe2232203e3b3bd60029f03b4b135c77dc5c0b80dd57b464b67ea8/kcrawler-1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "9bd137060caa40c4d78fd0ea0fbe8c680a73a6f131186ff318b7839a4407a284",
        "md5": "8dd316183db75805eed3e0ffd2938fbb",
        "sha256": "5f07579d961d0c5dd3abe981f90ea8a0ffc50c3e77c8c5a256b928263de754fd"
      },
      "downloads": -1,
      "filename": "kcrawler-1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8dd316183db75805eed3e0ffd2938fbb",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 17360,
      "upload_time": "2019-11-06T02:27:02",
      "upload_time_iso_8601": "2019-11-06T02:27:02.957226Z",
      "url": "https://files.pythonhosted.org/packages/9b/d1/37060caa40c4d78fd0ea0fbe8c680a73a6f131186ff318b7839a4407a284/kcrawler-1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "07c7c5c0c2fe2232203e3b3bd60029f03b4b135c77dc5c0b80dd57b464b67ea8",
        "md5": "6888a1c1c75c78d42e2f4fabd1209eb4",
        "sha256": "f569987ce1e16f5e2a2f65a27ae84cf6183ac09e9869a2d67d75741a659247ed"
      },
      "downloads": -1,
      "filename": "kcrawler-1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "6888a1c1c75c78d42e2f4fabd1209eb4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 12904,
      "upload_time": "2019-11-06T02:27:05",
      "upload_time_iso_8601": "2019-11-06T02:27:05.055915Z",
      "url": "https://files.pythonhosted.org/packages/07/c7/c5c0c2fe2232203e3b3bd60029f03b4b135c77dc5c0b80dd57b464b67ea8/kcrawler-1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}