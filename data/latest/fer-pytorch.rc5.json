{
  "info": {
    "author": "Emin Tagiev",
    "author_email": "emin.tagiev@phystech.edu",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: Implementation :: PyPy",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "FER-pytorch\n===========\n\nFacial expression recognition package built on Pytorch and [FER+ dataset from Microsoft](https://github.com/microsoft/FERPlus).\n\n[![PyPI version](https://badge.fury.io/py/fer-pytorch.svg)](https://badge.fury.io/py/fer-pytorch)\n![CI](https://github.com/albumentations-team/albumentations/workflows/CI/badge.svg)\n[![Downloads](https://pepy.tech/badge/fer-pytorch)](https://pepy.tech/project/fer-pytorch)\n\n![dicaprio_result](https://user-images.githubusercontent.com/44554040/163209336-198e3db3-a84f-4d81-9156-19177bba7808.png)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1_sTDVvK-673CKyYQP7gsViCkO8eBa9jd?usp=sharing)\n\n# Installation\n`pip install fer-pytorch`\n\n# Training\nTraining is done using the synergy of [Pytorch Lightning](https://www.pytorchlightning.ai/) and\n[Hydra](https://hydra.cc/docs/intro/) packages for setting training loops and configs correspondingly.\nIn order to run training you should clone the repo and\n\n### Install dependencies\n```\npip install -r requirements/requirements-dev.txt\n```\n\n### Define environmental variables\nJust run `export PYTHONPATH=\"$PWD\"` from the root directory and it would be enough\nto run the code from the root. You can check the PYTHONPATH with `echo $PYTHONPATH` command.\n\n### Download data\n`bash get_data.sh`\n\nThis bash command will download the archive with the data and unpack it into the necessary directories in such a way\nthat everything is ready for training.\n\n### Define config\nThe config files are located in `fer-pytorch/conf` directory. To see all the parameters\nand their default values run\n\n`python fer_pytorch/run_trainer.py -h`\n\n### Run training\nTraining with default parameter values:\n\n`python fer_pytorch/run_trainer.py`\n\nThanks to Hydra all the parameters set in config files can be changed\ndirectly from the command line while running the script:\n\n* Example with change of model from resnet34 to resnet18:\n  * `python fer_pytorch/run_trainer.py model.model_name=\"resnet18\"`\n\n* Example with change of number of epochs:\n  * `python fer_pytorch/run_trainer.py trainer.trainer_params.max_epochs=100`\n\nBy default the output is saved to `output/` directory. If you wish to set\nthe path to output, run\n\n`python fer_pytorch/run_trainer.py hydra.run.dir=path_to_output`\n\n\n# Inference\n\n### Import inference class\n```\nimport cv2\nfrom fer_pytorch.fer import FER\n\nfer = FER()\n```\n\n### Initialize the model\nThere are 2 options:\n\n1. `fer.get_pretrained_model(model_name)`: download the ready-to-use pretrained on FER+ dataset weights from the github\npage of the package and initialize the model automatically. The list of available names are given in\n`fer_pytorch/pre_trained_models.py` file as the keys of `models` dictionary. For example,\n`fer.get_pretrained_model(\"resnet34\")`.\n2. `fer.load_user_weights(model_name, path_to_weights)`:  with this option you can load your own weights\nthat are stored locally.\n\n### Inference on an image\n* Basic prediction\n```\nimg = cv2.imread(\"tests/test_images/happy.jpg\")\nresult = fer.predict_image(img)\n```\n\nSample output:\n```\n[{'box': [295.90848, 87.36073, 463.75354, 296.00055],\n'emotions': {'neutral': 0.00033704843, 'happiness': 0.98931086, 'surprise': 0.00018355528, 'sadness': 0.0026534477, 'anger': 0.0054451805, 'disgust': 0.0019571118, 'fear': 0.000112833266}}]\n```\n\n* Get only top emotion:\n\n`result = fer.predict_image(frame, show_top=True)`\n\nSample output:\n\n`[{'box': [295.90848, 87.36073, 463.75354, 296.00055], 'top_emotion': {'happiness': 0.98931086}}]`\n\n* In order to save the output image, just set the output path:\n\n`result = fer.predict_image(frame, show_top=True, path_to_output=\"result.jpg\")`\n\n### Note\nIf there is more than one person on an image, this method will provide the results for all,\nwhose faces are detected by the MTCNN detector. However, this is not the case for the methods\ndescribed below (for list of images and video files). Instead, the inference methods for the list\nof images ad video files will return results for people with the largest bounding box around\ntheir faces. It is made for convenience and performance purposes since the mentioned methods\noutput the json file with the results, so it would be necessary to track the people to\nproduce the correct json files for corresponding detected people.\n\n### Inference on a list of images\nInference on a folder with images:\n```\nresult_json_list = fer.predict_list_images(\n    path_to_input=\"tests/test_images\",\n    path_to_output=\"tests/output_images\",\n    save_images=True\n)\n```\n\nOutputs the json with results and optionally the processed images in a separate directory.\n\nIt is also possible to read the result json with pandas leveraging FER class method:\n\n```\nresult_df = FER.json_to_pandas(\"tests/output_images/result.json\")\nprint(result_df.head())\n```\n\n### Inference on a video file\n```\nfer.analyze_video(\n    path_to_video=\"tests/test_videos/test_video.mp4\",\n    path_to_output=\"tests/test_video\",\n    save_video=True\n)\n```\n\nOutputs the json with results and optionally the processed video in a separate directory.\n\nJust like in the previous case it is also possible to read the result json with\npandas for further analysis leveraging FER class method:\n```\ndf = FER.json_to_pandas(\"tests/test_video/result.json\")\nprint(df.head())\n```\n\n### Inference on the test part of the FER dataset\nGet the dictionary with accuracy and f1 score for the test part of the FER+ dataset:\n```\nresult_dict = fer.test_fer(\n    path_to_dataset = \"fer_pytorch/dataset\",\n    path_to_csv = \"fer_pytorch/dataset/new_test.csv\",\n    batch_size = 32,\n    num_workers = 8,\n)\nprint(result_dict)\n```\n\nOutput:\n\n`{'accuracy': 0.83, 'f1': 0.83}`\n\n### Inference with the web camera\nTo run the model on the stream from the web camera and show the results in real-time just run\n\n`fer.run_webcam()`\n\n# Web application\nIn order to demonstrate the fer-pytorch package the web application has been developed\nusing [Streamlit](https://streamlit.io/) and deployed to [Heroku](heroku.com) via Docker image.\n\nLink to the github page of the app: https://github.com/Emilien-mipt/fer-webapp\n\nLink to the app: https://ferpytorch-webapp.herokuapp.com/\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Emilien-mipt/fer-pytorch",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "fer-pytorch",
    "package_url": "https://pypi.org/project/fer-pytorch/",
    "platform": null,
    "project_url": "https://pypi.org/project/fer-pytorch/",
    "project_urls": {
      "Homepage": "https://github.com/Emilien-mipt/fer-pytorch"
    },
    "release_url": "https://pypi.org/project/fer-pytorch/1.1.2/",
    "requires_dist": [
      "albumentations (<=1.0.0,>=0.5.2)",
      "facenet-pytorch (==2.5.2)",
      "hydra-core (==1.1.1)",
      "matplotlib (<=3.5.1,>=3.3.4)",
      "numpy (<=1.22,>=1.19.5)",
      "opencv-python (<=4.5.5.64,>=4.5.1.48)",
      "pandas (<=1.4.2,>=1.1.5)",
      "Pillow (<=8.4.0,>=8.1.2)",
      "pytorch-lightning (==1.5.10)",
      "scikit-learn (<=1.0,>=0.24.1)",
      "timm (<=0.5.4,>=0.3.2)",
      "torch (<=1.11,>=1.8.0)",
      "tqdm (<=4.63.0,>=4.59.0)"
    ],
    "requires_python": ">=3.6.0",
    "summary": "Facial Expression Recognition package implemented in Pytorch.",
    "version": "1.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13831547,
  "releases": {
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "771a02fb0dbeacb63e65b6202499bbda6cbba27e3347309d428d3db4f69434f0",
          "md5": "bae4effac78dd96d65ccec136c385bf7",
          "sha256": "6b0b952d129ea55777dc5e0d52b5902a6032901b5e97ccafc2d83ed30996fdb2"
        },
        "downloads": -1,
        "filename": "fer_pytorch-0.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bae4effac78dd96d65ccec136c385bf7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 13726,
        "upload_time": "2022-04-07T10:11:12",
        "upload_time_iso_8601": "2022-04-07T10:11:12.134179Z",
        "url": "https://files.pythonhosted.org/packages/77/1a/02fb0dbeacb63e65b6202499bbda6cbba27e3347309d428d3db4f69434f0/fer_pytorch-0.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "405a5816a01a08eafab61364484d6f06753be3d2b8b163a6161f551024ea9c92",
          "md5": "07d0a7276d5444fd98a23e21327cd99a",
          "sha256": "5bd10de685b2ba8f34eb4a0ad038f8eadc78d2695773c31f923a37b30d756b78"
        },
        "downloads": -1,
        "filename": "fer_pytorch-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "07d0a7276d5444fd98a23e21327cd99a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 15170,
        "upload_time": "2022-04-13T23:29:45",
        "upload_time_iso_8601": "2022-04-13T23:29:45.926789Z",
        "url": "https://files.pythonhosted.org/packages/40/5a/5816a01a08eafab61364484d6f06753be3d2b8b163a6161f551024ea9c92/fer_pytorch-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fd25636b6ef474a65ba8d21d1ffc5734c828f56c06fcec77ad14170b82fede66",
          "md5": "9e18faa83d950fc467fdc0c4317442c7",
          "sha256": "6ad09effb25dc1a2c2bf0a6c0a126544fc0ab530283a264dc49aa54c7036ecfb"
        },
        "downloads": -1,
        "filename": "fer_pytorch-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9e18faa83d950fc467fdc0c4317442c7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 15485,
        "upload_time": "2022-05-05T15:22:07",
        "upload_time_iso_8601": "2022-05-05T15:22:07.731515Z",
        "url": "https://files.pythonhosted.org/packages/fd/25/636b6ef474a65ba8d21d1ffc5734c828f56c06fcec77ad14170b82fede66/fer_pytorch-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c7d2e70faf3acb3c591456701314fde309c10949e04ed38c3a375949d0f83a41",
          "md5": "3e80b792776b924c2b397be9724807c2",
          "sha256": "ad0001201fc6349192d726478e84bfc001b58c6fc99bb1e4faee1e81f8ffddb8"
        },
        "downloads": -1,
        "filename": "fer_pytorch-1.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3e80b792776b924c2b397be9724807c2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 15737,
        "upload_time": "2022-05-13T14:45:53",
        "upload_time_iso_8601": "2022-05-13T14:45:53.485660Z",
        "url": "https://files.pythonhosted.org/packages/c7/d2/e70faf3acb3c591456701314fde309c10949e04ed38c3a375949d0f83a41/fer_pytorch-1.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ef54d34ff86994e39102b1b574ff2addd51ac15ff8bb6542d6031366af4111d5",
          "md5": "35b449a2c6c0c1daff653de5ff212c40",
          "sha256": "d3dfd9d28a92ed72599337a123247e0c5b472176e65894306dd257a183055412"
        },
        "downloads": -1,
        "filename": "fer_pytorch-1.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "35b449a2c6c0c1daff653de5ff212c40",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6.0",
        "size": 15765,
        "upload_time": "2022-05-16T16:14:43",
        "upload_time_iso_8601": "2022-05-16T16:14:43.126049Z",
        "url": "https://files.pythonhosted.org/packages/ef/54/d34ff86994e39102b1b574ff2addd51ac15ff8bb6542d6031366af4111d5/fer_pytorch-1.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ef54d34ff86994e39102b1b574ff2addd51ac15ff8bb6542d6031366af4111d5",
        "md5": "35b449a2c6c0c1daff653de5ff212c40",
        "sha256": "d3dfd9d28a92ed72599337a123247e0c5b472176e65894306dd257a183055412"
      },
      "downloads": -1,
      "filename": "fer_pytorch-1.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "35b449a2c6c0c1daff653de5ff212c40",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6.0",
      "size": 15765,
      "upload_time": "2022-05-16T16:14:43",
      "upload_time_iso_8601": "2022-05-16T16:14:43.126049Z",
      "url": "https://files.pythonhosted.org/packages/ef/54/d34ff86994e39102b1b574ff2addd51ac15ff8bb6542d6031366af4111d5/fer_pytorch-1.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}