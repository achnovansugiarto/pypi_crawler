{
  "info": {
    "author": "Felipe Maia Polo",
    "author_email": "felipemaiapolo@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# ***LegalNLP*** - Natural Language Processing Methods for the Brazilian Legal Language :balance_scale:\n\n### The library of Natural Language Processing for Brazilian legal language, *LegalNLP*, was born in a partnership between Brazilian researchers and the legal tech [Tikal Tech](https://www.tikal.tech) based in São Paulo, Brazil. Besides containing pre-trained language models for the Brazilian legal language, ***LegalNLP*** provides functions that can facilitate the manipulation of legal texts in Portuguese and demonstration/tutorials to help people in their own work.\n\n[![PyPI](https://img.shields.io/pypi/v/legalnlp.svg)](https://pypi.python.org/pypi/legalnlp)\n[![PyPI](https://img.shields.io/pypi/dm/legalnlp.svg)](https://pypi.python.org/pypi/legalnlp)\n\n\nYou can access our paper by clicking [**here**](https://arxiv.org/abs/2110.15709). \n\nIf you use our library in your academic work, please cite us in the following way\n\n    Polo, Felipe Maia, et al. \"LegalNLP-Natural Language Processing methods for the Brazilian Legal Language.\" Anais do XVIII Encontro Nacional de Inteligência Artificial e Computacional. SBC, 2021.\n    \n    @inproceedings{polo2021legalnlp,\n      title={LegalNLP-Natural Language Processing methods for the Brazilian Legal Language},\n      author={Polo, Felipe Maia and Mendon{\\c{c}}a, Gabriel Caiaffa Floriano and Parreira, Kau{\\^e} Capellato J and Gianvechio, Lucka and Cordeiro, Peterson and Ferreira, Jonathan Batista and de Lima, Leticia Maria Paz and do Amaral Maia, Ant{\\^o}nio Carlos and Vicente, Renato},\n      booktitle={Anais do XVIII Encontro Nacional de Intelig{\\^e}ncia Artificial e Computacional},\n      pages={763--774},\n      year={2021},\n      organization={SBC}\n    }\n\n--------------\n\n## Summary\n\n0. [Accessing the Language Models](#0)\n1. [ Introduction / Installing package](#1)\n2. [Fuctions ](#2)\n    1.  [ Text Cleaning Functions](#2.1)\n    2.  [Other Functions](#2.2)\n3. [ Language Models (Details / How to use)](#3)\n    1.  [ Phraser ](#3.1)\n    2.  [ Word2Vec/Doc2Vec ](#3.2)\n    3.  [ FastText ](#3.3)\n    4.  [ BERTikal ](#3.4)\n4. [ Demonstrations / Tutorials](#4)\n5. [ References](#5)\n\n--------------\n\n<a name=\"0\"></a>\n## 0\\. Accessing the Language Models\n\n\nAll our models can be found [here](https://drive.google.com/drive/folders/1tCccOXPLSEAEUQtcWXvED3YaNJi3p7la?usp=sharing).\n\nSome models can be download directly using our function `get_premodel` (more details in section [Other Functions](#2.2)).\n\n\nPlease contact *felipemaiapolo@gmail.com* if you have any problem accessing the language models. \n\n--------------\n\n<a name=\"1\"></a>\n## 1\\. Introduction / Installing package\n*LegalNLP* is promising given the scarcity of Natural Language Processing resources focused on the Brazilian legal language. It is worth mentioning that our library was made for Python, one of the most well-known programming languages for machine learning.\n\n\nYou can install our package running one of the following commands on the terminal\n\n``` :sh\n$ pip install legalnlp\n```\n\nor \n\n``` :sh\n$ pip install git+https://github.com/felipemaiapolo/legalnlp\n```\n\nYou can load all our functions running the following command\n\n```python\nfrom legalnlp.clean_functions import *\nfrom legalnlp.get_premodel import *\n```\n\n\n--------------\n\n<a name=\"2\"></a>\n## 2\\. Functions\n<a name=\"2.1\"></a>\n### 2.1\\.  Text Cleaning Functions\n\n\n<a name=\"2.1.1\"></a>\n#### 2.1.1\\. `clean(text, lower=True, return_masked=False)`\nFunction for cleaning texts to be used (optional) in conjunction with Doc2Vec, Word2Vec, and FastText models. We use RegEx to mask/extract information such as email addresses, URLs, dates, numbers, monetary values, etc.\n\n**input**:  \n\n- *text*, **str**;\n \n- *lower*, **bool**, default=**True**. If lower==True, function lower cases the whole text. Note that all the models (except BERT) were trained with lower cased texts;\n\n- *return_masked*, **bool**, default=**True**.  If return_masked == False, the function outputs a clean text. Otherwise, it returns a dictionary containing the clean text and the information extracted by RegEx;\n\n**output**:\n\n-  Clean text or dictionary, depending on the *return_masked* parameter;\n\n\n<a name=\"2.1.2\"></a>\n#### 2.1.2\\.`clean_bert(text)`\n\nFunction for cleaning the texts to be used (optional) in conjunction with the BERT model.\n\n**input:**  \n\n- *text*, **str**.\n\n**output:** \n\n-  **str** with clean text.\n\n<a name=\"2.2\"></a>\n### 2.2\\.  Other functions\n\n#### 2.2.2\\. `get_premodel(model)` \n\nFunction to download a pre-trained model in the same folder as the file that is being executed.\n\n**input:**  \n\n- *model*, **str**. Must contain the name of the pre-trained model that one wants to use. There are these options:  \n    - **model = \"bert\"**: Download a .zip file containing BERTikal model and unzip it.\n    - **model = \"wdoc\"**: Download Word2Vec and Do2vec pre-trained models in a.zip file and unzip it. It has 2 two files, one with an size 100 Doc2Vec Distributed Memory/ Word2Vec Continuous Bag-of-Words (CBOW) embeddings model and other with an size 100 Doc2Vec Distributed Bag-of-Words (DBOW)/ Word2Vec Skip-Gram (SG)  embeddings model.\n    - **model = \"fasttext\"**: Download a .zip file containing 100 sized FastText CBOW/SG models and unzip it.\n    - **model = \"phraser\"**: Download Phraser pre-trained model in a .zip file and unzip it. It has 2 two files with phraser1 and phreaser2. We explain how to use them in Section [ Phraser ](#3.1). \n    - **model = \"w2vnilc\"**: Download size 100 Word2Vec CBOW model trained by \"Núcleo Interinstitucional de Linguística Computacional - USP\" embeddings model in a .zip file and unzip it. [Click here for more details](http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc).\n    - **model = \"neuralmindbase\"**: Download a .zip file containing base BERT model (PyTorch), trained by NeuralMind and unzip it. For more informations about BERT models made by NeuralMind go to [their GitHub repo](https://github.com/neuralmind-ai/portuguese-bert).\n    - **model = \"neuralmindlarge\"**: Download a .zip file containing large BERT model (PyTorch), trained by NeuralMind and unzip it. For more informations about BERT models made by NeuralMind go to [their GitHub repo](https://github.com/neuralmind-ai/portuguese-bert).\n\n\n**output:** \n\n- True if download of some model was made and False otherwise.\n\n\n#### 2.2.1\\. `extract_features_bert(path_model, path_tokenizer, data, gpu=True)`\n\nFunction for extracting features with the BERT model (This function is not accessed through the package installation, but you can find it [here](https://github.com/felipemaiapolo/legalnlp/blob/main/demo/BERT/extract_features_bert.ipynb)).\n\n\n**Input:**  \n\n- *path_model*, **str**. Must contain the path of the pre-trained model;\n\n- *path_tokenizer*, **str**. Must contain the path of tokenizer;\n\n- *data*, **list**. Must contain a list of texts that will be extracted features;\n\n- *gpu*, **bool**, default=**True**. If gpu==False, the GPU will not be used in the model application (we recommend feature extraction to be done using Google Colab).\n\n\n**Output:** \n\n- **DataFrame** with features extracted by BERT model.\n\n\n<a name=\"3\"></a>\n## 3\\. Model Languages\n\n<a name=\"3.1\"></a>\n### 3.1\\. Phraser\n\nPhraser is a statistical method proposed in the natural language processing\nliterature [1] for identifying which words when they appear\ntogether, can be considered as unique tokens. This method application is able to\nidentify the relevance of the occurrence of a bigram against the occurrence of the\nwords that make it up separately. Thus, we can identify that a bigram like \"São\nPaulo\" should be treated as a single token, for example. If the method is applied\na second time in sequence, we can check which are the relevant trigrams and\nquadrigrams. Since the two applications should be done with different Phraser\nmodels, it can be the case that the second application identifies bigrams that were\nnot identified by the first model.\n\nThis model is compatible with the `clean` function, but it is not necessary to use it before. Remember to at least make all letters lowercase. Please check our paper or [Gensim page](https://radimrehurek.com/gensim_3.8.3/models/phrases.html) for more details. Preferably use Gensim version 3.8.3.\n\n#### Using *Phraser*\nInstalling Gensim\n\n\n```python\n!pip install gensim=='3.8.3' \n```\n\nImporting package and loading our two Phraser models.\n\n\n```python\n#Importing packages\nfrom gensim.models.phrases import Phraser \n\n#Loading two Phraser models\nphraser1=Phraser.load('models_phraser/phraser1')\nphraser2=Phraser.load('models_phraser/phraser2')\n```\n\n\nApplying Phraser once and twice to check output\n\n\n```python\ntxt='direito do consumidor origem : bangu regional xxix juizado especial civel ação : [processo] - - recte : fundo de investimento em direitos creditórios'\ntokens=txt.split()\n\nprint('Clean Text: \"'+' '.join(tokens)+'\"')\nprint('\\nApplying Phraser 1x: \"'+' '.join(phraser1[tokens])+'\"')\nprint('\\nApplying Phraser 2x: \"'+' '.join(phraser2[phraser1[tokens]])+'\"')\n```\n\n    Clean Text: \"direito do consumidor origem : bangu regional xxix juizado especial civel ação : [processo] - - recte : fundo de investimento em direitos creditórios\"\n    \n    Applying Phraser 1x: \"direito do consumidor origem : bangu regional xxix juizado_especial civel_ação : [processo] - - recte : fundo de investimento em direitos_creditórios\"\n    \n    Applying Phraser 2x: \"direito do consumidor origem : bangu_regional xxix juizado_especial_civel_ação : [processo] - - recte : fundo de investimento em direitos_creditórios\"\n\n<a name=\"3.2\"></a>\n### 3.2\\. Word2Vec/Doc2Vec\n\nOur first models for generating vector representation for tokens and\ntexts (embeddings) are variations of the Word2Vec [1,\n2] and Doc2Vec [3] methods. In short, the\nWord2Vec methods generate embeddings for tokens5 and that somehow capture\nthe meaning of the various textual elements, based on the contexts in which these\nelements appear. Doc2Vec methods are extensions/modifications of Word2Vec\nfor generating whole text representations.\n\nThe Word2Vec and Doc2Vec methods are presented together in this section because they were trained together using the Gensim package. Both models are compatible with the `clean` function, but it is not necessary to use it before. Remember to at least make all letters lowercase. Please check our paper or [Gensim page](https://radimrehurek.com/gensim_3.8.3/models/doc2vec.html) for more details. Preferably use Gensim version 3.8.3.\n\n\nBelow we have a summary table with some important information about the trained models:\n\n\n\n| Filenames       |  Doc2Vec | Word2Vec   | Size | Windows\n|:-------------------:|:--------------:|:--------------:|:--------------:|:--------------:|\n| ```w2v_d2v_dm*```     | Distributed Memory       (DM)             | Continuous Bag-of-Words (CBOW)          | 100, 200, 300 | 15 \n| ```w2v_d2v_dbow*``` | Distributed Bag-of-Words (DBOW)               | Skip-Gram (SG)                   | 100, 200, 300      | 15 \n\n\n\n\n\n#### Using *Word2Vec*\n\nInstalling Gensim\n\n\n```python\n!pip install gensim=='3.8.3' \n```\n\nLoading W2V (all the files for the specific model should be in the same folder)\n\n\n```python\nfrom gensim.models import KeyedVectors\n\n#Loading a W2V model\nw2v=KeyedVectors.load('models_w2v_d2v/w2v_d2v_dm_size_100_window_15_epochs_20')\nw2v=w2v.wv\n```\nViewing the first 10 entries of 'juiz' vector\n\n\n```python\nw2v['juiz'][:10]\n```\n\n\n\n\n    array([ 6.570131  , -1.262787  ,  5.156106  , -8.943866  , -5.884408  ,\n           -7.717058  ,  1.8819941 , -8.02803   , -0.66901577,  6.7223144 ],\n          dtype=float32)\n\n\n\n\nViewing closest tokens to 'juiz'\n\n```python\nw2v.most_similar('juiz')\n```\n\n\n\n\n    [('juíza', 0.8210258483886719),\n     ('juiza', 0.7306275367736816),\n     ('juíz', 0.691645085811615),\n     ('juízo', 0.6605231165885925),\n     ('magistrado', 0.6213295459747314),\n     ('mmª_juíza', 0.5510469675064087),\n     ('juizo', 0.5494943261146545),\n     ('desembargador', 0.5313084721565247),\n     ('mmjuiz', 0.5277603268623352),\n     ('fabíola_melo_feijão_juíza', 0.5043971538543701)]\n\n\n#### Using *Doc2Vec*\nInstalling Gensim\n\n\n```python\n!pip install gensim=='3.8.3' \n```\n\nLoading D2V (all the files for the specific model should be in the same folder)\n\n\n```python\nfrom gensim.models import Doc2Vec\n\n#Loading a D2V model\nd2v=Doc2Vec.load('models_w2v_d2v/w2v_d2v_dm_size_100_window_15_epochs_20')\n```\n\nInferring vector for a text\n\n\n```python\ntxt='direito do consumidor origem : bangu regional xxix juizado especial civel ação : [processo] - - recte : fundo de investimento em direitos creditórios'\ntokens=txt.split()\n\ntxt_vec=d2v.infer_vector(tokens, epochs=20)\ntxt_vec[:10]\n```\n\n\n\n\n    array([ 0.02626514, -0.3876521 , -0.24873355, -0.0318402 ,  0.3343679 ,\n           -0.21307918,  0.07193747,  0.02030687,  0.407305  ,  0.20065512],\n          dtype=float32)\n\n\n\n\n<a name=\"3.3\"></a>\n### 3.3\\. FastText\n\nThe FastText [4] methods, like Word2Vec, form a class of\nmodels for creating vector representations (embeddings) for tokens. Unlike\nWord2Vec, which disregards the morphology of the tokens and allocates a\ndifferent vector for each one of them, the FastText methods consider that each one\nof the tokens is formed by n-grams of characters or substrings. In this way, the\nrepresentation of tokens which do not appear in the training set can be inferred\nfrom the representation of substrings. Also, rare tokens can have more robust\nrepresentations than those returned by the Word2Vec methods.\n\nModels are compatible with the `clean` function, but it is not necessary to use it. Remember to at least make all letters lowercase. Please check our paper or the [Gensim page](https://radimrehurek.com/gensim/models/fasttext.html) for more details. Preferably use Gensim version 4.0.1.\n\nBelow we have a summary table with some important information about the trained models:\n| Filenames      | FastText   | Sizes | Windows\n|:-------------------:|:--------------:|:--------------:|:--------------:|\n| ```fasttext_cbow*```         | Continuous Bag-of-Words (CBOW)          | 100, 200, 300 | 15 \n| ```fasttext_sg*```             | Skip-Gram (SG)                   | 100, 200, 300      | 15 \n\n\n#### Using *FastText*\n\ninstalling Gensim\n\n\n```python\n!pip install gensim=='4.0.1' \n```\n\nLoading FastText (all the files for the specific model should be in the same folder)\n\n\n```python\nfrom gensim.models import FastText\n\n#Loading a FastText model\nfast=FastText.load('models_fasttext/fasttext_sg_size_100_window_15_epochs_20')\nfast=fast.wv\n```\n\nViewing the first 10 entries of 'juiz' vector\n\n\n\n```python\nfast['juiz'][:10]\n```\n\n\n\n\n    array([ 0.46769685,  0.62529474,  0.08549586,  0.09621219, -0.09998254,\n           -0.07897531,  0.32838237, -0.33229044, -0.05959201, -0.5865443 ],\n          dtype=float32)\n\n\n\nViewing the first 10 vector entries of a token that was not in our vocabulary\n\n\n```python\nfast['juizasjashdkjhaskda'][:10]\n```\n\n\n\n\n    array([ 0.02795791,  0.1361525 ,  0.1340836 , -0.36824217, -0.11549155,\n           -0.11167661,  0.32045627, -0.33701468, -0.05198409, -0.05513595],\n          dtype=float32)\n\n\n<a name=\"3.4\"></a>\n### 3.4\\. BERTikal\n\n\nWe call BERTikal our BERT-Base model   (cased) [5] for Brazilian legal language. BERT models are models based on neural network architectures called Transformers. BERT models are trained with large sets of texts using the self-supervised paradigm, which is basically solving unsupervised problems using supervised techniques. A pre-trained BERT model is capable of generating representations for entire texts and can be adapted for a supervised task, e.g., text classification or question answering, using the fine-tuning mechanism. \n\nBERTikal was trained using the Python package [Transformers](https://huggingface.co/transformers/})  in its 4.2.2 version and its checkpoint made available by us is compatible with [PyTorch](https://pytorch.org/) 1.9.0. Although we expose the versions of both packages, more current versions can be used in applications of the model, as long as there are no relevant version conflicts.\n\nOur model was trained from the checkpoint made available in [Neuralmind’s Github repository](https://github.com/neuralmind-ai/portuguese-bert) by the authors of recent research [6].\n\n#### Using *BERTikal*\n\nInstalling Torch e Transformers\n\n\n```python\n!pip install torch=='1.8.1' transformers=='4.2.2'\n```\n\nLoading BERT (all the files for the specific model should be in the same folder)\n\n\n```python\nfrom transformers import BertModel, BertTokenizer\n\nbert_tokenizer = BertTokenizer.from_pretrained('model_bertikal/', do_lower_case=False)\nbert_model = BertModel.from_pretrained('model_bertikal/')\n```\n\n--------------\n\n<a name=\"4\"></a>\n## 4\\. Demonstrations\n\nFor a better understanding of the application of these models, below are the links to notebooks where we apply them to a legal dataset using various classification models such as Logistic Regression and CatBoost:\n\n- **BERT notebook** : \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/felipemaiapolo/legalnlp/blob/main/demo/BERT/BERT_TUTORIAL.ipynb)\n \n- **Word2Vec notebook** : \n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/felipemaiapolo/legalnlp/blob/main/demo/Word2Vec/Word2Vec_TUTORIAL.ipynb)\n\n- **Doc2Vec notebook** :\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/felipemaiapolo/legalnlp/blob/main/demo/Doc2Vec/Doc2Vec_TUTORIAL.ipynb)\n\n\n\n--------------\n\n<a name=\"5\"></a>\n## 5\\. References\n\n[1] Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013b).\nDistributed representations of words and phrases and their compositionality.\nIn Advances in neural information processing systems, pages 3111–3119.\n\n[2] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013a). Efficient estimation of\nword representations in vector space. arXiv preprint arXiv:1301.3781.\n\n[3] Le, Q. and Mikolov, T. (2014). Distributed representations of sentences and\ndocuments. In International conference on machine learning, pages 1188–1196.\nPMLR.\n\n[4] Bojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). Enriching\nword vectors with subword information. Transactions of the Association for\nComputational Linguistics, 5:135–146.\n\n[5] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training\nof deep bidirectional transformers for language understanding. arXiv preprint\narXiv:1810.04805.\n\n[6] Souza, F., Nogueira, R., and Lotufo, R. (2020). BERTimbau: pretrained BERT\nmodels for Brazilian Portuguese. In 9th Brazilian Conference on Intelligent\nSystems, BRACIS, Rio Grande do Sul, Brazil, October 20-23",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/felipemaiapolo/legalnlp",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "legalnlp",
    "package_url": "https://pypi.org/project/legalnlp/",
    "platform": null,
    "project_url": "https://pypi.org/project/legalnlp/",
    "project_urls": {
      "Homepage": "https://github.com/felipemaiapolo/legalnlp"
    },
    "release_url": "https://pypi.org/project/legalnlp/1.0.3/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Pre-trained language models forthe  Brazilian  legal  language.",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14117093,
  "releases": {
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0b5f87cfa4c93006f9c523ed9933b915e0f1842af80ac5bcecdd857146cb2d25",
          "md5": "438c40a96da7aca3e841c8831e7761b8",
          "sha256": "5f3a49d73f3c20f24a1cde5e1788b9ea38776a6003b1ceeef3182e8d0d1cb854"
        },
        "downloads": -1,
        "filename": "legalnlp-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "438c40a96da7aca3e841c8831e7761b8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11277,
        "upload_time": "2022-06-13T16:15:37",
        "upload_time_iso_8601": "2022-06-13T16:15:37.762060Z",
        "url": "https://files.pythonhosted.org/packages/0b/5f/87cfa4c93006f9c523ed9933b915e0f1842af80ac5bcecdd857146cb2d25/legalnlp-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ec6bacd244eb03a143eb8efd119a27e1840b0605f267179b571d83f8d3b4ba41",
          "md5": "3e564d4a7001b5af744d4e79b103e91c",
          "sha256": "2726ae16ba3ddbccd68da0d006c9cbeccc49b2cdb74602d4564f269a19372a2a"
        },
        "downloads": -1,
        "filename": "legalnlp-1.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "3e564d4a7001b5af744d4e79b103e91c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11264,
        "upload_time": "2022-06-13T16:21:08",
        "upload_time_iso_8601": "2022-06-13T16:21:08.633488Z",
        "url": "https://files.pythonhosted.org/packages/ec/6b/acd244eb03a143eb8efd119a27e1840b0605f267179b571d83f8d3b4ba41/legalnlp-1.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ec6bacd244eb03a143eb8efd119a27e1840b0605f267179b571d83f8d3b4ba41",
        "md5": "3e564d4a7001b5af744d4e79b103e91c",
        "sha256": "2726ae16ba3ddbccd68da0d006c9cbeccc49b2cdb74602d4564f269a19372a2a"
      },
      "downloads": -1,
      "filename": "legalnlp-1.0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "3e564d4a7001b5af744d4e79b103e91c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 11264,
      "upload_time": "2022-06-13T16:21:08",
      "upload_time_iso_8601": "2022-06-13T16:21:08.633488Z",
      "url": "https://files.pythonhosted.org/packages/ec/6b/acd244eb03a143eb8efd119a27e1840b0605f267179b571d83f8d3b4ba41/legalnlp-1.0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}