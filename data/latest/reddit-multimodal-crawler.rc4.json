{
  "info": {
    "author": "Aneesh Aparajit G",
    "author_email": "aneeshaparajit.g2002@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Reddit Multimodal Crawler [![Downloads](https://static.pepy.tech/badge/reddit-multimodal-crawler)](https://pepy.tech/project/reddit-multimodal-crawler)\n\nThis is a wrapper to the `PRAW` package to scrape content from image in the form of `csv`, `json`, `tsv`, `sql` files.\n\nThis repository will help you scrape various subreddits, and will return to you multi-media attributes.\n\nYou can pip install this to integrate with some other application, or use it as an commandline application.\n\n- PyPI Link:  https://pypi.org/project/reddit-multimodal-crawler/\n\n```commandLine\npip install reddit-multimodal-crawler\n```\n\n## How to use the repository?\n\nBefore running the code, you should have registered with the Reddit API and create a sample project to run the code and obtain the `client_id`, `client_secret` and make a `user_agent`. Then pass them in the arguements.\n\nAlthough, the easier way is to use the `pip install reddit-multimodal-crawler`.\n\n## Functionalities\n\nThis will help you scrape multiple subreddits just like `PRAW` but, will also return and save datasets for the same. Will scrape the posts and the comments as well.\n\n### Sample Code\n\n```python\nimport nltk\nfrom reddit_multimodal_crawler.crawler import Crawler\nimport argparse\n\nnltk.download(\"vader_lexicon\")\n\nif __name__ == \"__main__\":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--subreddit_file_path\",\n        \"A path to the file which contains the subreddits to scrape from.\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--limit\", \"The limit to number of articles to scrape.\", type=int\n    )\n    parser.add_argument(\"--client_id\", \"The Client ID provided by Reddit.\", type=str)\n    parser.add_argument(\n        \"--client_secret\", \"The Secret ID provided by the Reddit.\", type=str\n    )\n    parser.add_argument(\n        \"--user_agent\",\n        \"The User Agent in the form of <APP_NAME> <VERSION> by /u/<REDDIT_USERNAME>\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--posts\", \"A boolean variable to parse through the posts or not.\", type=bool\n    )\n    parser.add_argument(\n        \"--comments\",\n        \"A boolean variable to parse through the comments of the top posts of subreddit\",\n        type=bool,\n    )\n\n    args = parser.parse_args()\n\n    client_id = args[\"client_id\"]\n    client_secret = args[\"client_secret\"]\n    user_agent = args[\"user_agent\"]\n    file_path = args[\"subreddit_file_path\"]\n    limit = args[\"limit\"]\n\n    r = Crawler(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n\n    subreddit_list = open(file_path, \"r\").readlines().split()\n\n    print(subreddit_list)\n\n    if args[\"posts\"]:\n        r.get_posts(subreddit_names=subreddit_list, sort_by=\"top\", limit=limit)\n\n    if args[\"comments\"]:\n        r.get_comments(subreddit_names=subreddit_list, sort_by=\"top\", limit=limit)\n\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/aneesh-aparajit/reddit-crawler",
    "keywords": "web-scraping,webscraper,reddit,multimodal,datascience",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "reddit-multimodal-crawler",
    "package_url": "https://pypi.org/project/reddit-multimodal-crawler/",
    "platform": null,
    "project_url": "https://pypi.org/project/reddit-multimodal-crawler/",
    "project_urls": {
      "Homepage": "https://github.com/aneesh-aparajit/reddit-crawler"
    },
    "release_url": "https://pypi.org/project/reddit-multimodal-crawler/1.3.2/",
    "requires_dist": [
      "praw",
      "pandas",
      "numpy",
      "requests",
      "bcrypt",
      "nltk"
    ],
    "requires_python": "",
    "summary": "A scraper which will scrape out multimedia data from reddit.",
    "version": "1.3.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16264362,
  "releases": {
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "94ac86b336c800ada23b40f8dfc5c175f5ec94a956bd7feb2fa5067ffeee81c7",
          "md5": "d511a07eb5031f06d1399e1d326f3f54",
          "sha256": "11e62ae5377eb7c55558ede45b30683f4e90b144842220d536511d77231be825"
        },
        "downloads": -1,
        "filename": "reddit_multimodal_crawler-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d511a07eb5031f06d1399e1d326f3f54",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5262,
        "upload_time": "2022-12-27T16:39:08",
        "upload_time_iso_8601": "2022-12-27T16:39:08.341011Z",
        "url": "https://files.pythonhosted.org/packages/94/ac/86b336c800ada23b40f8dfc5c175f5ec94a956bd7feb2fa5067ffeee81c7/reddit_multimodal_crawler-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "72fd83e203f0a0351ca42a43199ca1548610d55bc27d6e09c59e68510cf9906b",
          "md5": "121beff1418f2495f908072a960423b7",
          "sha256": "705cdf63a01aab10f497763e46398466368c36382973586b7e87e477c39e8869"
        },
        "downloads": -1,
        "filename": "reddit_multimodal_crawler-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "121beff1418f2495f908072a960423b7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4777,
        "upload_time": "2022-12-27T16:39:10",
        "upload_time_iso_8601": "2022-12-27T16:39:10.395258Z",
        "url": "https://files.pythonhosted.org/packages/72/fd/83e203f0a0351ca42a43199ca1548610d55bc27d6e09c59e68510cf9906b/reddit_multimodal_crawler-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6927a93f211084f2183e0db5efad20f80a0716f277afa3a932d1bd41ca3fce53",
          "md5": "0a3717b1fdb57f6b5fb027a2ce1245ac",
          "sha256": "f5563de474f862dad0ddfbe9877f5ca5528304a1394cc21b387f1fd168eb2cb2"
        },
        "downloads": -1,
        "filename": "reddit_multimodal_crawler-1.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0a3717b1fdb57f6b5fb027a2ce1245ac",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5294,
        "upload_time": "2022-12-27T16:42:17",
        "upload_time_iso_8601": "2022-12-27T16:42:17.579466Z",
        "url": "https://files.pythonhosted.org/packages/69/27/a93f211084f2183e0db5efad20f80a0716f277afa3a932d1bd41ca3fce53/reddit_multimodal_crawler-1.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "be2d1421e19602969be57ace8617357641c20b970124a042bb9daef43b0da5a1",
          "md5": "7d13651ac53ca10694caf3a61f224bc5",
          "sha256": "ad7885b5b37a41a0eeb5f713a67eb4fa75fed485bf94a0622d54a7f646359a06"
        },
        "downloads": -1,
        "filename": "reddit_multimodal_crawler-1.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7d13651ac53ca10694caf3a61f224bc5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4823,
        "upload_time": "2022-12-27T16:42:20",
        "upload_time_iso_8601": "2022-12-27T16:42:20.449553Z",
        "url": "https://files.pythonhosted.org/packages/be/2d/1421e19602969be57ace8617357641c20b970124a042bb9daef43b0da5a1/reddit_multimodal_crawler-1.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e8edb0f7dff406335345d68626100e1f85573e1cd539db6eec4edf2ef1606328",
          "md5": "ed20ab686d2c1222bf0dd2620cddc57c",
          "sha256": "41161cce549f97022540434a79f8a1c1b2a12a3e81ac8ba3d824039cdcef06a5"
        },
        "downloads": -1,
        "filename": "reddit_multimodal_crawler-1.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ed20ab686d2c1222bf0dd2620cddc57c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5400,
        "upload_time": "2022-12-30T13:43:48",
        "upload_time_iso_8601": "2022-12-30T13:43:48.546147Z",
        "url": "https://files.pythonhosted.org/packages/e8/ed/b0f7dff406335345d68626100e1f85573e1cd539db6eec4edf2ef1606328/reddit_multimodal_crawler-1.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6b3ba8a3d70e62530fafc2a7dc4cfb03da2aa1bca2e21e322b69caefd04d34c9",
          "md5": "a6d7d63dfe689001f38d2602e579cde4",
          "sha256": "a7dfe3ece16f9fe7acfc225426ecd5b03dd11df822aa9e62fe42258e3c12aaf1"
        },
        "downloads": -1,
        "filename": "reddit_multimodal_crawler-1.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a6d7d63dfe689001f38d2602e579cde4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4939,
        "upload_time": "2022-12-30T13:43:50",
        "upload_time_iso_8601": "2022-12-30T13:43:50.998881Z",
        "url": "https://files.pythonhosted.org/packages/6b/3b/a8a3d70e62530fafc2a7dc4cfb03da2aa1bca2e21e322b69caefd04d34c9/reddit_multimodal_crawler-1.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "de8f650efcb4ce8a2f2b0779e311af9a798c056dac0b73daf4f530c73fb1d4af",
          "md5": "94bd45fdee3610ced26e9b182adcb0cb",
          "sha256": "e627454c015000e79b10b20d3057bd4156e8e7af88dea11f4814487b50a7ce10"
        },
        "downloads": -1,
        "filename": "reddit_multimodal_crawler-1.3.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "94bd45fdee3610ced26e9b182adcb0cb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 5368,
        "upload_time": "2022-12-31T03:30:15",
        "upload_time_iso_8601": "2022-12-31T03:30:15.299414Z",
        "url": "https://files.pythonhosted.org/packages/de/8f/650efcb4ce8a2f2b0779e311af9a798c056dac0b73daf4f530c73fb1d4af/reddit_multimodal_crawler-1.3.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "541b552761ce29265fc0f53fed692d5414d9138c5ba5f9dcd9f9d9001cca9649",
          "md5": "48f3b2a959fd22ec25c66a9b488dfec2",
          "sha256": "96485a1a0aa7c111fbbe8165e97eeef80e7f8715c4eaa639c419d9311ea22882"
        },
        "downloads": -1,
        "filename": "reddit_multimodal_crawler-1.3.2.tar.gz",
        "has_sig": false,
        "md5_digest": "48f3b2a959fd22ec25c66a9b488dfec2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4910,
        "upload_time": "2022-12-31T03:30:17",
        "upload_time_iso_8601": "2022-12-31T03:30:17.353718Z",
        "url": "https://files.pythonhosted.org/packages/54/1b/552761ce29265fc0f53fed692d5414d9138c5ba5f9dcd9f9d9001cca9649/reddit_multimodal_crawler-1.3.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "de8f650efcb4ce8a2f2b0779e311af9a798c056dac0b73daf4f530c73fb1d4af",
        "md5": "94bd45fdee3610ced26e9b182adcb0cb",
        "sha256": "e627454c015000e79b10b20d3057bd4156e8e7af88dea11f4814487b50a7ce10"
      },
      "downloads": -1,
      "filename": "reddit_multimodal_crawler-1.3.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "94bd45fdee3610ced26e9b182adcb0cb",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 5368,
      "upload_time": "2022-12-31T03:30:15",
      "upload_time_iso_8601": "2022-12-31T03:30:15.299414Z",
      "url": "https://files.pythonhosted.org/packages/de/8f/650efcb4ce8a2f2b0779e311af9a798c056dac0b73daf4f530c73fb1d4af/reddit_multimodal_crawler-1.3.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "541b552761ce29265fc0f53fed692d5414d9138c5ba5f9dcd9f9d9001cca9649",
        "md5": "48f3b2a959fd22ec25c66a9b488dfec2",
        "sha256": "96485a1a0aa7c111fbbe8165e97eeef80e7f8715c4eaa639c419d9311ea22882"
      },
      "downloads": -1,
      "filename": "reddit_multimodal_crawler-1.3.2.tar.gz",
      "has_sig": false,
      "md5_digest": "48f3b2a959fd22ec25c66a9b488dfec2",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 4910,
      "upload_time": "2022-12-31T03:30:17",
      "upload_time_iso_8601": "2022-12-31T03:30:17.353718Z",
      "url": "https://files.pythonhosted.org/packages/54/1b/552761ce29265fc0f53fed692d5414d9138c5ba5f9dcd9f9d9001cca9649/reddit_multimodal_crawler-1.3.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}