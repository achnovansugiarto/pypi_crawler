{
  "info": {
    "author": "Germey",
    "author_email": "cqc@cuiqingcai.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "# Gerapy Pyppeteer\n\nThis is a package for supporting pyppeteer in Scrapy, also this\npackage is a module in [Gerapy](https://github.com/Gerapy/Gerapy).\n\n## Installation\n\n```shell script\npip3 install gerapy-pyppeteer\n```\n\n## Usage\n\nYou can use `PyppeteerRequest` to specify a request which uses pyppeteer to render.\n\nFor example:\n\n```python\nyield PyppeteerRequest(detail_url, callback=self.parse_detail)\n```\n\nAnd you also need to enable `PyppeteerMiddleware` in `DOWNLOADER_MIDDLEWARES`:\n\n```python\nDOWNLOADER_MIDDLEWARES = {\n    'gerapy_pyppeteer.downloadermiddlewares.PyppeteerMiddleware': 543,\n}\n```\n\nCongratulate, you've finished the all of the required configuration.\n\nIf you run the Spider again, Pyppeteer will be started to render every\nweb page which you configured the request as PyppeteerRequest.\n\n## Settings\n\nGerapyPyppeteer provides some optional settings.\n\n### Concurrency\n\nYou can directly use Scrapy's setting to set Concurrency of Pyppeteer,\nfor example:\n\n```python\nCONCURRENT_REQUESTS = 3\n```\n\n### Pretend as Real Browser\n\nSome website will detect WebDriver or Headless, GerapyPyppeteer can\npretend Chromium by inject scripts. This is enabled by default.\n\nYou can close it if website does not detect WebDriver to speed up:\n\n```python\nGERAPY_PYPPETEER_PRETEND = False\n```\n\nAlso you can use `pretend` attribute in `PyppeteerRequest` to overwrite this\nconfiguration.\n\n### Logging Level\n\nBy default, Pyppeteer will log all the debug messages, so GerapyPyppeteer\nconfigured the logging level of Pyppeteer to WARNING.\n\nIf you want to see more logs from Pyppeteer, you can change the this setting:\n\n```python\nimport logging\nGERAPY_PYPPETEER_LOGGING_LEVEL = logging.DEBUG\n```\n\n### Download Timeout\n\nPyppeteer may take some time to render the required web page, you can also change this setting, default is `30s`:\n\n```python\n# pyppeteer timeout\nGERAPY_PYPPETEER_DOWNLOAD_TIMEOUT = 30\n```\n\n### Headless\n\nBy default, Pyppeteer is running in `Headless` mode, you can also\nchange it to `False` as you need, default is `True`:\n\n```python\nGERAPY_PYPPETEER_HEADLESS = False\n```\n\n### Window Size\n\nYou can also set the width and height of Pyppeteer window:\n\n```python\nGERAPY_PYPPETEER_WINDOW_WIDTH = 1400\nGERAPY_PYPPETEER_WINDOW_HEIGHT = 700\n```\n\nDefault is 1400, 700.\n\n### Pyppeteer Args\n\nYou can also change the args of Pyppeteer, such as `dumpio`, `devtools`, etc.\n\nOptional settings and their default values:\n\n```python\nGERAPY_PYPPETEER_DUMPIO = False\nGERAPY_PYPPETEER_DEVTOOLS = False\nGERAPY_PYPPETEER_EXECUTABLE_PATH = None\nGERAPY_PYPPETEER_DISABLE_EXTENSIONS = True\nGERAPY_PYPPETEER_HIDE_SCROLLBARS = True\nGERAPY_PYPPETEER_MUTE_AUDIO = True\nGERAPY_PYPPETEER_NO_SANDBOX = True\nGERAPY_PYPPETEER_DISABLE_SETUID_SANDBOX = True\nGERAPY_PYPPETEER_DISABLE_GPU = True\n```\n\n### Disable loading of specific resource type\n\nYou can disable the loading of specific resource type to\ndecrease the loading time of web page. You can configure\nthe disabled resource types using `GERAPY_PYPPETEER_IGNORE_RESOURCE_TYPES`:\n\n```python\nGERAPY_PYPPETEER_IGNORE_RESOURCE_TYPES = []\n```\n\nFor example, if you want to disable the loading of css and javascript,\nyou can set as below:\n\n```python\nGERAPY_PYPPETEER_IGNORE_RESOURCE_TYPES = ['stylesheet', 'script']\n```\n\nAll of the optional resource type list:\n\n- document: the Original HTML document\n- stylesheet: CSS files\n- script: JavaScript files\n- image: Images\n- media: Media files such as audios or videos\n- font: Fonts files\n- texttrack: Text Track files\n- xhr: Ajax Requests\n- fetch: Fetch Requests\n- eventsource: Event Source\n- websocket: Websocket\n- manifest: Manifest files\n- other: Other files\n\n### Screenshot\n\nYou can get screenshot of loaded page, you can pass `screenshot` args to `PyppeteerRequest` as dict:\n\n- `type` (str): Specify screenshot type, can be either `jpeg` or `png`. Defaults to `png`.\n- `quality` (int): The quality of the image, between 0-100. Not applicable to `png` image.\n- `fullPage` (bool): When true, take a screenshot of the full scrollable page. Defaults to `False`.\n- `clip` (dict): An object which specifies clipping region of the page. This option should have the following fields:\n  - `x` (int): x-coordinate of top-left corner of clip area.\n  - `y` (int): y-coordinate of top-left corner of clip area.\n  - `width` (int): width of clipping area.\n  - `height` (int): height of clipping area.\n- `omitBackground` (bool): Hide default white background and allow capturing screenshot with transparency.\n- `encoding` (str): The encoding of the image, can be either `base64` or `binary`. Defaults to `binary`. If binary it will return `BytesIO` object.\n\nFor example:\n\n```python\nyield PyppeteerRequest(start_url, callback=self.parse_index, wait_for='.item .name', screenshot={\n            'type': 'png',\n            'fullPage': True\n        })\n```\n\nthen you can get screenshot result in `response.meta['screenshot']`:\n\nSimplest save it to file:\n\n```python\ndef parse_index(self, response):\n    with open('screenshot.png', 'wb') as f:\n        f.write(response.meta['screenshot'].getbuffer())\n```\n\nIf you want to enable screenshot for all requests, you can configure it by `GERAPY_PYPPETEER_SCREENSHOT`.\n\nFor example:\n\n```python\nGERAPY_PYPPETEER_SCREENSHOT = {\n    'type': 'png',\n    'fullPage': True\n}\n```\n\n## PyppeteerRequest\n\n`PyppeteerRequest` provide args which can override global settings above.\n\n- url: request url\n- callback: callback\n- one of \"load\", \"domcontentloaded\", \"networkidle0\", \"networkidle2\".\n  see https://miyakogi.github.io/pyppeteer/reference.html#pyppeteer.page.Page.goto, default is `domcontentloaded`\n- wait_for: wait for some element to load, also supports dict\n- script: script to execute\n- proxy: use proxy for this time, like `http://x.x.x.x:x`\n- sleep: time to sleep after loaded, override `GERAPY_PYPPETEER_SLEEP`\n- timeout: load timeout, override `GERAPY_PYPPETEER_DOWNLOAD_TIMEOUT`\n- ignore_resource_types: ignored resource types, override `GERAPY_PYPPETEER_IGNORE_RESOURCE_TYPES`\n- pretend: pretend as normal browser, override `GERAPY_PYPPETEER_PRETEND`\n- screenshot: ignored resource types, see\n  https://miyakogi.github.io/pyppeteer/_modules/pyppeteer/page.html#Page.screenshot,\n  override `GERAPY_PYPPETEER_SCREENSHOT`\n\nFor example, you can configure PyppeteerRequest as:\n\n```python\nfrom gerapy_pyppeteer import PyppeteerRequest\n\ndef parse(self, response):\n    yield PyppeteerRequest(url,\n        callback=self.parse_detail,\n        wait_until='domcontentloaded',\n        wait_for='title',\n        script='() => { return {name: \"Germey\"} }',\n        sleep=2)\n```\n\nThen Pyppeteer will:\n\n- wait for document to load\n- wait for title to load\n- execute `console.log(document)` script\n- sleep for 2s\n- return the rendered web page content, get from `response.meta['screenshot']`\n- return the script executed result, get from `response.meta['script_result']`\n\nFor waiting mechanism controlled by JavaScript, you can use await in `script`, for example:\n\n```python\njs = '''async () => {\n    await new Promise(resolve => setTimeout(resolve, 10000));\n    return {\n        'name': 'Germey'\n    }\n}\n'''\nyield PyppeteerRequest(url, callback=self.parse, script=js)\n```\n\nThen you can get the script result from `response.meta['script_result']`, result is `{'name': 'Germey'}`.\n\nIf you think the JavaScript is wired to write, you can use actions argument to define a function to execute `Python` based functions, for example:\n\n```python\nasync def execute_actions(page: Page):\n    await page.evaluate('() => { document.title = \"Hello World\"; }')\n    return 1\nyield PyppeteerRequest(url, callback=self.parse, actions=execute_actions)\n```\n\nThen you can get the actions result from `response.meta['actions_result']`, result is `1`.\n\n## Example\n\nFor more detail, please see [example](./example).\n\nAlso you can directly run with Docker:\n\n```\ndocker run germey/gerapy-pyppeteer-example\n```\n\nOutputs:\n\n```shell script\n2020-07-13 01:49:13 [scrapy.utils.log] INFO: Scrapy 2.2.0 started (bot: example)\n2020-07-13 01:49:13 [scrapy.utils.log] INFO: Versions: lxml 4.3.3.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.7.7 (default, May  6 2020, 04:59:01) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Darwin-19.4.0-x86_64-i386-64bit\n2020-07-13 01:49:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n2020-07-13 01:49:13 [scrapy.crawler] INFO: Overridden settings:\n{'BOT_NAME': 'example',\n 'CONCURRENT_REQUESTS': 3,\n 'NEWSPIDER_MODULE': 'example.spiders',\n 'RETRY_HTTP_CODES': [403, 500, 502, 503, 504],\n 'SPIDER_MODULES': ['example.spiders']}\n2020-07-13 01:49:13 [scrapy.extensions.telnet] INFO: Telnet Password: 83c276fb41754bd0\n2020-07-13 01:49:13 [scrapy.middleware] INFO: Enabled extensions:\n['scrapy.extensions.corestats.CoreStats',\n 'scrapy.extensions.telnet.TelnetConsole',\n 'scrapy.extensions.memusage.MemoryUsage',\n 'scrapy.extensions.logstats.LogStats']\n2020-07-13 01:49:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n 'gerapy_pyppeteer.downloadermiddlewares.PyppeteerMiddleware',\n 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n2020-07-13 01:49:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n2020-07-13 01:49:13 [scrapy.middleware] INFO: Enabled item pipelines:\n[]\n2020-07-13 01:49:13 [scrapy.core.engine] INFO: Spider opened\n2020-07-13 01:49:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2020-07-13 01:49:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n2020-07-13 01:49:13 [example.spiders.book] INFO: crawling https://dynamic5.scrape.center/page/1\n2020-07-13 01:49:13 [gerapy.pyppeteer] DEBUG: processing request <GET https://dynamic5.scrape.center/page/1>\n2020-07-13 01:49:13 [gerapy.pyppeteer] DEBUG: set options {'headless': True, 'dumpio': False, 'devtools': False, 'args': ['--window-size=1400,700', '--disable-extensions', '--hide-scrollbars', '--mute-audio', '--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu']}\n2020-07-13 01:49:14 [gerapy.pyppeteer] DEBUG: crawling https://dynamic5.scrape.center/page/1\n2020-07-13 01:49:19 [gerapy.pyppeteer] DEBUG: waiting for .item .name finished\n2020-07-13 01:49:20 [gerapy.pyppeteer] DEBUG: wait for .item .name finished\n2020-07-13 01:49:20 [gerapy.pyppeteer] DEBUG: close pyppeteer\n2020-07-13 01:49:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dynamic5.scrape.center/page/1> (referer: None)\n2020-07-13 01:49:20 [gerapy.pyppeteer] DEBUG: processing request <GET https://dynamic5.scrape.center/detail/26898909>\n2020-07-13 01:49:20 [gerapy.pyppeteer] DEBUG: processing request <GET https://dynamic5.scrape.center/detail/26861389>\n2020-07-13 01:49:20 [gerapy.pyppeteer] DEBUG: processing request <GET https://dynamic5.scrape.center/detail/26855315>\n2020-07-13 01:49:20 [gerapy.pyppeteer] DEBUG: set options {'headless': True, 'dumpio': False, 'devtools': False, 'args': ['--window-size=1400,700', '--disable-extensions', '--hide-scrollbars', '--mute-audio', '--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu']}\n2020-07-13 01:49:20 [gerapy.pyppeteer] DEBUG: set options {'headless': True, 'dumpio': False, 'devtools': False, 'args': ['--window-size=1400,700', '--disable-extensions', '--hide-scrollbars', '--mute-audio', '--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu']}\n2020-07-13 01:49:21 [gerapy.pyppeteer] DEBUG: set options {'headless': True, 'dumpio': False, 'devtools': False, 'args': ['--window-size=1400,700', '--disable-extensions', '--hide-scrollbars', '--mute-audio', '--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu']}\n2020-07-13 01:49:21 [gerapy.pyppeteer] DEBUG: crawling https://dynamic5.scrape.center/detail/26855315\n2020-07-13 01:49:21 [gerapy.pyppeteer] DEBUG: crawling https://dynamic5.scrape.center/detail/26861389\n2020-07-13 01:49:21 [gerapy.pyppeteer] DEBUG: crawling https://dynamic5.scrape.center/detail/26898909\n2020-07-13 01:49:24 [gerapy.pyppeteer] DEBUG: waiting for .item .name finished\n2020-07-13 01:49:24 [gerapy.pyppeteer] DEBUG: wait for .item .name finished\n2020-07-13 01:49:24 [gerapy.pyppeteer] DEBUG: close pyppeteer\n2020-07-13 01:49:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dynamic5.scrape.center/detail/26861389> (referer: https://dynamic5.scrape.center/page/1)\n2020-07-13 01:49:24 [gerapy.pyppeteer] DEBUG: processing request <GET https://dynamic5.scrape.center/page/2>\n2020-07-13 01:49:24 [gerapy.pyppeteer] DEBUG: set options {'headless': True, 'dumpio': False, 'devtools': False, 'args': ['--window-size=1400,700', '--disable-extensions', '--hide-scrollbars', '--mute-audio', '--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu']}\n2020-07-13 01:49:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dynamic5.scrape.center/detail/26861389>\n{'name': '壁穴ヘブンホール',\n 'score': '5.6',\n 'tags': ['BL漫画', '小基漫', 'BL', '『又腐又基』', 'BLコミック']}\n2020-07-13 01:49:25 [gerapy.pyppeteer] DEBUG: waiting for .item .name finished\n2020-07-13 01:49:25 [gerapy.pyppeteer] DEBUG: crawling https://dynamic5.scrape.center/page/2\n2020-07-13 01:49:26 [gerapy.pyppeteer] DEBUG: wait for .item .name finished\n2020-07-13 01:49:26 [gerapy.pyppeteer] DEBUG: close pyppeteer\n2020-07-13 01:49:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://dynamic5.scrape.center/detail/26855315> (referer: https://dynamic5.scrape.center/page/1)\n2020-07-13 01:49:26 [gerapy.pyppeteer] DEBUG: processing request <GET https://dynamic5.scrape.center/detail/27047626>\n2020-07-13 01:49:26 [gerapy.pyppeteer] DEBUG: set options {'headless': True, 'dumpio': False, 'devtools': False, 'args': ['--window-size=1400,700', '--disable-extensions', '--hide-scrollbars', '--mute-audio', '--no-sandbox', '--disable-setuid-sandbox', '--disable-gpu']}\n2020-07-13 01:49:26 [scrapy.core.scraper] DEBUG: Scraped from <200 https://dynamic5.scrape.center/detail/26855315>\n{'name': '冒险小虎队', 'score': '9.4', 'tags': ['冒险小虎队', '童年', '冒险', '推理', '小时候读的']}\n2020-07-13 01:49:26 [gerapy.pyppeteer] DEBUG: waiting for .item .name finished\n2020-07-13 01:49:26 [gerapy.pyppeteer] DEBUG: crawling https://dynamic5.scrape.center/detail/27047626\n2020-07-13 01:49:27 [gerapy.pyppeteer] DEBUG: wait for .item .name finished\n2020-07-13 01:49:27 [gerapy.pyppeteer] DEBUG: close pyppeteer\n...\n```\n\n## Trouble Shooting\n\n### Pyppeteer does not start properly\n\nChromium download speed is too slow, it can not be used normally.\n\nHere are two solutions:\n\n#### Solution 1 (Recommended)\n\nModify drive download source at `pyppeteer/chromium_downloader.py` line 22:\n\n```python\n# Default：\nDEFAULT_DOWNLOAD_HOST = 'https://storage.googleapis.com'\n# modify\nDEFAULT_DOWNLOAD_HOST = http://npm.taobao.org/mirror\n```\n\n#### Solution 2\n\nModify drive execution path at `pyppeteer/chromium_downloader.py` line 45:\n\n```python\n# Default：\nchromiumExecutable = {\n    'linux': DOWNLOADS_FOLDER / REVISION / 'chrome-linux' / 'chrome',\n    'mac': (DOWNLOADS_FOLDER / REVISION / 'chrome-mac' / 'Chromium.app' /\n            'Contents' / 'MacOS' / 'Chromium'),\n    'win32': DOWNLOADS_FOLDER / REVISION / windowsArchive / 'chrome.exe',\n    'win64': DOWNLOADS_FOLDER / REVISION / windowsArchive / 'chrome.exe',\n}\n```\n\nYou can find your own operating system, modify your chrome or chrome executable path.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Gerapy/GerapyPyppeteer",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapyer-gerapy-pyppeteer",
    "package_url": "https://pypi.org/project/scrapyer-gerapy-pyppeteer/",
    "platform": null,
    "project_url": "https://pypi.org/project/scrapyer-gerapy-pyppeteer/",
    "project_urls": {
      "Homepage": "https://github.com/Gerapy/GerapyPyppeteer"
    },
    "release_url": "https://pypi.org/project/scrapyer-gerapy-pyppeteer/0.1.4/",
    "requires_dist": null,
    "requires_python": ">=3.6.0",
    "summary": "Pyppeteer Components for Scrapy & Gerapy",
    "version": "0.1.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13731915,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7a1ae671f900d664f5a3ea32fa262161d65f1613a9cb7beadb43db2d84b7ba20",
          "md5": "0fdd96df5c27eaf3d4b373f78d599678",
          "sha256": "7aaa13cc2442a67f1515027a830a71ccecfe3aa64da6de03fba897c067485eb9"
        },
        "downloads": -1,
        "filename": "scrapyer-gerapy-pyppeteer-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0fdd96df5c27eaf3d4b373f78d599678",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 33144,
        "upload_time": "2021-11-19T09:14:42",
        "upload_time_iso_8601": "2021-11-19T09:14:42.641645Z",
        "url": "https://files.pythonhosted.org/packages/7a/1a/e671f900d664f5a3ea32fa262161d65f1613a9cb7beadb43db2d84b7ba20/scrapyer-gerapy-pyppeteer-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8e3278c27d9a01021ae211bbb70a5ba1a71e5f813bde94222ebe7de80cd192b8",
          "md5": "7c1ffae821d9bb094e9c785e8ba6073a",
          "sha256": "2c64b28667ca281556bc5d02179d5ff6f13dc4ce6e2ff99ad3fa2e6778640e56"
        },
        "downloads": -1,
        "filename": "scrapyer-gerapy-pyppeteer-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7c1ffae821d9bb094e9c785e8ba6073a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 32708,
        "upload_time": "2021-11-19T14:40:27",
        "upload_time_iso_8601": "2021-11-19T14:40:27.552889Z",
        "url": "https://files.pythonhosted.org/packages/8e/32/78c27d9a01021ae211bbb70a5ba1a71e5f813bde94222ebe7de80cd192b8/scrapyer-gerapy-pyppeteer-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "171a73b81c803608a1c432bc2c1c59334b0fbb9b53dba9bcda1d886a17b6be76",
          "md5": "6dc10d47494b6cfd31ca2b30cf340f6e",
          "sha256": "d33630432119c4ed4c37a391a8bd7214d5b3a531779a886692797c76079f642a"
        },
        "downloads": -1,
        "filename": "scrapyer-gerapy-pyppeteer-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "6dc10d47494b6cfd31ca2b30cf340f6e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 33193,
        "upload_time": "2022-05-06T06:23:31",
        "upload_time_iso_8601": "2022-05-06T06:23:31.963580Z",
        "url": "https://files.pythonhosted.org/packages/17/1a/73b81c803608a1c432bc2c1c59334b0fbb9b53dba9bcda1d886a17b6be76/scrapyer-gerapy-pyppeteer-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "67c4e04cc4bffba92258e9e973cd6622ab27890eb4da1079253a16291d0cf16b",
          "md5": "f5b2411e3a98ab87645e1f295fee193f",
          "sha256": "35d42d8b822586e11944650f200834ef0d96d6e15978e40a174f66f0abc2e8c2"
        },
        "downloads": -1,
        "filename": "scrapyer-gerapy-pyppeteer-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "f5b2411e3a98ab87645e1f295fee193f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 33200,
        "upload_time": "2022-05-06T09:16:54",
        "upload_time_iso_8601": "2022-05-06T09:16:54.925373Z",
        "url": "https://files.pythonhosted.org/packages/67/c4/e04cc4bffba92258e9e973cd6622ab27890eb4da1079253a16291d0cf16b/scrapyer-gerapy-pyppeteer-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d4e25b5c157a8063c2616a7eb1779ea78d9c8041b7171bb5fc8a663c51e6906",
          "md5": "05a2378285105819975651dbebad134f",
          "sha256": "da2f9b405b55239e4796dd1a4d63a44532ab69fafa87c19ecfcb375c50ad06c9"
        },
        "downloads": -1,
        "filename": "scrapyer-gerapy-pyppeteer-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "05a2378285105819975651dbebad134f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6.0",
        "size": 33193,
        "upload_time": "2022-05-06T09:19:42",
        "upload_time_iso_8601": "2022-05-06T09:19:42.017218Z",
        "url": "https://files.pythonhosted.org/packages/5d/4e/25b5c157a8063c2616a7eb1779ea78d9c8041b7171bb5fc8a663c51e6906/scrapyer-gerapy-pyppeteer-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5d4e25b5c157a8063c2616a7eb1779ea78d9c8041b7171bb5fc8a663c51e6906",
        "md5": "05a2378285105819975651dbebad134f",
        "sha256": "da2f9b405b55239e4796dd1a4d63a44532ab69fafa87c19ecfcb375c50ad06c9"
      },
      "downloads": -1,
      "filename": "scrapyer-gerapy-pyppeteer-0.1.4.tar.gz",
      "has_sig": false,
      "md5_digest": "05a2378285105819975651dbebad134f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6.0",
      "size": 33193,
      "upload_time": "2022-05-06T09:19:42",
      "upload_time_iso_8601": "2022-05-06T09:19:42.017218Z",
      "url": "https://files.pythonhosted.org/packages/5d/4e/25b5c157a8063c2616a7eb1779ea78d9c8041b7171bb5fc8a663c51e6906/scrapyer-gerapy-pyppeteer-0.1.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}