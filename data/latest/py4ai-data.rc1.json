{
  "info": {
    "author": "",
    "author_email": "Nicola Donelli <nicoladonelli87@gmail.com>, Enrico Deusebio <edeusebio85@gmail.com>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "py4ai data\n====\n\n[![PyPI](https://img.shields.io/pypi/v/py4ai-data.svg)](https://pypi.python.org/pypi/py4ai-data)\n[![Python version](https://img.shields.io/badge/python-3.8+-blue.svg)](https://pypi.python.org/pypi/py4ai-data)\n[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg)](https://py4ai.github.io/py4ai-data/)\n![Python package](https://github.com/NicolaDonelli/py4ai-data/workflows/CI%20-%20Build%20and%20Test/badge.svg)\n\n--------------------------------------------------------------------------------\n\n\nA Python library defining data structures optimized for machine learning pipelines \n\n\n## What is it ?\n**py4ai-data** is a Python package with modular design that provides powerful abstractions to build data \ningestion pipelines and run end to end machine learning pipelines. \nThe library offers lightweight object-oriented interface to MongoDB as well as Pandas based data structures. \nThe aim of the library is to provide extensive support for developing machine learning based applications \nwith a focus on practicing clean code and modular design. \nThe detailed documentation for all its features is available [here](https://nicoladonelli.github.io/py4ai-data/intro/readme.html).\n\n## Features\nSome cool features that we are proud to mention are: \n\n### [Data layers](https://nicoladonelli.github.io/py4ai-data/api/py4ai.data.layer.html) \nThe Data Layer abstractions are designed to decouple the business layers from \nthe detail of the persistence layer implementation.\nWe currently implemented a few data layers based on some common abstractions:\n1. Repository: class representing the access point to data. Specifies the methods to read/write data from/to persistence layer. It must be instantiated by passing it a Serializer, to specify how to read/write data from/to persistence layer to/from memory. \n2. Serializer: class that specifies how to read/write data from/to persistence layer to/from memory.\n3. Criteria: class implementing query method to filter data retrieved from repository.\nThe data layer currently implemented are:\n* file-system: access generic data stored in the file system as generic files, handling them in memory as generic indexed objects.\n* pandas: access tabular data stored in the file system as csv files, handling them in memory as pandas objects.\n* mongo: access data stored in MongoDB. Serializers for this layer should be implemented by the user according to its specific needs.\n* sqlalchemy: access data stored in a database handled with sqlalchemy ORM. Serializers for this layer should be implemented by the user according to its specific needs.\n\n### [Data Model](https://nicoladonelli.github.io/py4ai-data/api/py4ai.data.model.html)\nOffers the following data structures: \n1. Document : Data structure specifically designed to work with NLP applications that parses a json-like document \ninto a couple of uuid and dictionary of information.\n2. Sample : Data structure representing an observation (a.k.a. sample) as used in machine learning applications\n3. MultiFeatureSample : Data structure representing an observation defined by a nested list of arrays.\n4. Dataset : Data structure designed to be used specifically for machine learning applications representing a collection \nof samples.\n\n## Installation\nFrom pypi server\n```\npip install py4ai-data\n```\n\nFrom source\n```\ngit clone https://github.com/NicolaDonelli/py4ai-data\ncd py4ai-data\nmake install\n```\n\n## Tests \n```\nmake tests\n```\n\n## Checks \nTo run predefined checks (unit-tests, linting checks, formatting checks and static typing checks):\n```\nmake checks\n```\n\n## Examples \n\n### Data Layers\n\nThe Data Layer abstractions are designed to decouple the business layers from \nthe detail of the persistence layer implementation. The basic abstraction that will \nmake this possible is the `Repository`.\n\nAs an example, imagine to have a domain business object `Entity` that associates some string data to an integer id.\n\n```python\nfrom pydantic import BaseModel\n\nclass Entity(BaseModel):\n    my_id: int\n    my_data: str\n```\n\nTo start with, imagine we want to use csv files store on disk as a persistence \nlayer. To do so, we will use the `CsvRepository` that uses pandas DataFrames stored\nin memory and written to the disk as csv. Thus, we need to write the business logic\nto serialize the `Entity` into a row of the pandas DataFrame, i.e. a pandas Series:\n\n```python\nimport pandas as pd\nfrom py4ai.data.layer.common.serialiazer import DataSerializer\n\nclass EntitySerializer(DataSerializer[int, int, Entity, pd.Series]):\n    def to_object(self, entity: Entity) -> pd.Series:\n        return pd.Series(entity.dict())\n\n    def to_entity(self, document: pd.Series) -> Entity:\n        return Entity(**document)\n\n    def to_object_key(self, key: int) -> int:\n        return key\n\n    def get_key(self, entity: Entity) -> int:\n        return entity.my_id\n```\n\nWe can now instantiate the repository class that has all the methods for \nreading and writing objects from the persistence layer. \n\n```python\nfrom py4ai.data.layer.pandas.repository import CsvRepository\n\nrepo = CsvRepository(filename, EntitySerializer())\n\nentity = Entity(my_id=1234, my_data=\"Important data\")\n\n# This will create the entity in the persistence layer\nawait repo.create(entity)\n\n# Retrieving the entity\nretrieved = repo.retrieve(key=1234)\n\n# Retrieving all entities\nall_entities = repo.list()\n\n```\n\nImagine now that, given the data increase in size, we now would like to change the \npersistence layer with a proper backend into something more structured and scalable, such \nas a NoSQL document-based data platform, such as MongoDB. We only need to create \na new business logic to serialize/deserialize our class into a json (represented\nin python by a dictionary):\n\n```python\nfrom bson import ObjectId\nfrom py4ai.data.layer.mongo.serializer import create_mongo_id\nfrom py4ai.data.layer.common.serialiazer import DataSerializer\n\nclass MongoDataSerializer(DataSerializer[int, ObjectId, Entity, dict]):\n    def get_key(self, entity: Entity) -> int:\n        return entity.my_id\n\n    def to_object(self, entity: Entity) -> dict:\n        doc = entity.dict()\n        doc[\"_id\"] = self.to_object_key(self.get_key(entity))\n        return doc\n\n    def to_entity(self, document: dict) -> Entity:\n        return Entity(**document)\n\n    def to_object_key(self, key: int) -> ObjectId:\n        # This converts a string into an hash compatible with MongoDB format\n        return create_mongo_id(str(key))\n```\n\nA new repository based on the MongoDB persistence layer can now be created using\n\n```python\nfrom py4ai.data.layer.mongo.repository import MongoRepository\n\nrepo = MongoRepository(collection, MongoDataSerializer())\n```\n\nThis repository is compatible with the previous and can be used in place of the \nprevious one, having the same signatures. \n\n#### Abstracting Data Querying \n\nThe `Repository` abstraction also allow to retrieve data based on certain queries/filters:\n\n```python\nentities = repo.retrieve_by_query(criteria)\n```\n\nHowever, the format of the query also depends on the type of the persistence layer and \nmore specifically on how the data are organized. Therefore, in order to abstract\nand decouple the notion of the underlying persistence layer, we need to define a general \nclass containing the possible queries for a certain database:\n\n```python\nfrom typing import Generic\nfrom abc import ABC, abstractmethod\n\nfrom py4ai.data.layer.common.criteria import SearchCriteria\n\n\nclass EntityQueryFactory(ABC, Generic[Q]):\n    @abstractmethod\n    def by_id(self, id: int) -> SearchCriteria[Q]:\n        ...\n```\n\nWhen considering a particular persistence layer, the querying business logic \nneeds to be specified\n\n```python\nfrom py4ai.data.layer.mongo.criteria import MongoSearchCriteria\n\n\nclass MongoQueryFactory(EntityQueryFactory[Dict[str, Any]]):\n\n    def by_id(self, my_id: int) -> MongoSearchCriteria:\n        return MongoSearchCriteria({\"my_id\": my_id})\n\n\ncriteria = MongoQueryFactory()\n\nentities = repo.retrieve_by_query(criteria.by_id(1234))\n```\n\nNote that `SearchCriteria` can be also joined using logical operators:\n\n```python\n\nentities = repo.retrieve_by_query(\n    criteria.by_id(1234) & criteria.by_id(1235) \n)\n\nentities = repo.retrieve_by_query(\n    criteria.by_id(1234) | criteria.by_id(1235) \n)\n```\n\n\n### Data Model\n\nThe _data.model_ module contains data structures commonly used.\nThe most relevant structures defined in the _core_ submodule are `CachedIterable` and `LazyIterable` \nthat represent iterable objects that are either completely stored in memory or accessed lazily. In \nthe same submodule we also define `Range` and `CompositeRange` that represent time and numeric ranges, \neither continuous or not, with methods to compare two instances checking for overlaps or performing \nrange union by sum. \n\nThe _ml_ submodule on the other hand defines some data structures that are of common use in \nMachine Learning applications.\nThe basic structure we define is the `Sample`, which is a simple serializable object characterized\nby a set of features, an optional set of labels and an optional name identifier:\n```python\nclass Sample(DillSerialization, Generic[FeatType, LabType]):\n    \"\"\"Base class for representing a sample/observation.\"\"\"\n\n    def __init__(\n        self,\n        features: FeatType,\n        label: Optional[LabType] = None,\n        name: Optional[Union[int, str, Any]] = None,\n    ) -> None:\n        \"\"\"\n        Return an object representing a single sample of a training or test set.\n\n        :param features: features of the sample\n        :param label: labels of the sample (optional)\n        :param name: id of the sample (optional)\n        \"\"\"\n        self.features: FeatType = features\n        self.label: Optional[LabType] = label\n        self.name: Optional[Union[str, int, Any]] = name\n```\nWe also define a specialization of this class, called `MultiFeatureSample`,\nthought for Deep Learning applications, for which the features attribute is a list of numpy arrays.\nThe most used objects defined in this module are the datasets, which represents collections of Samples\nin various fashions. All the datasets share some common method allowing to retrieve the type of the Samples,\nto check the names of the samples, to retrieve features and labels in a form to be chosen among \n`pd.DataFrame`, `np.ndarray`, `dict`, `list`, or a generator of samples. In particular, we defined:\n* `LazyDataset`: a collection with lazy access to samples\n* `CachedDataset`: a collection of samples fully stored in memory\n* `PandasDataset`: a collection of samples, fully stored in memory, where features and labels are pd.DataFrames\n* `PandasTimeIndexedDataset`: a collection of samples, fully stored in memory, where features and labels are pd.DataFrames with time index\n\nCreating a PandasDataset object\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom py4ai.data.model.ml import PandasDataset\n\ndataset = PandasDataset(features=pd.concat([pd.Series([1, np.nan, 2, 3], name=\"feat1\"),\n                                            pd.Series([1, 2, 3, 4], name=\"feat2\")], axis=1),\n                        labels=pd.Series([0, 0, 0, 1], name=\"Label\"))\n\n# access features as a pandas dataframe \nprint(dataset.features.head())\n# access labels as pandas dataframe \nprint(dataset.labels.head())\n# access features as a python dictionary \ndataset.getFeaturesAs('dict')\n# access features as numpy array \ndataset.getFeaturesAs('array')\n\n# indexing operations \n# access features and labels at the given index as a pandas dataframe  \nprint(dataset.loc([2]).features.head())\nprint(dataset.loc([2]).labels.head())\n```\n\nCreating a PandasTimeIndexedDataset object\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom py4ai.data.model.ml import PandasTimeIndexedDataset\n\ndateStr = [str(x) for x in pd.date_range('2010-01-01', '2010-01-04')]\ndataset = PandasTimeIndexedDataset(\n    features=pd.concat([\n        pd.Series([1, np.nan, 2, 3], index=dateStr, name=\"feat1\"),\n        pd.Series([1, 2, 3, 4], index=dateStr, name=\"feat2\")\n    ], axis=1))\n```\n\n## How to contribute ? \n\nWe are very much willing to welcome any kind of contribution whether it is bug report, bug fixes, contributions to the \nexisting codebase or improving the documentation. \n\n### Where to start ? \nPlease look at the [Github issues tab](https://github.com/NicolaDonelli/py4ai-data/issues) to start working on open \nissues \n\n### Contributing to py4ai-data \nPlease make sure the general guidelines for contributing to the code base are respected\n1. [Fork](https://docs.github.com/en/get-started/quickstart/contributing-to-projects) the py4ai-data repository. \n2. Create/choose an issue to work on in the [Github issues page](https://github.com/NicolaDonelli/py4ai-data/issues). \n3. [Create a new branch](https://docs.github.com/en/get-started/quickstart/github-flow) to work on the issue. \n4. Commit your changes and run the tests to make sure the changes do not break any test. \n5. Open a Pull Request on Github referencing the issue.\n6. Once the PR is approved, the maintainers will merge it on the main branch.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "py4ai-data",
    "package_url": "https://pypi.org/project/py4ai-data/",
    "platform": null,
    "project_url": "https://pypi.org/project/py4ai-data/",
    "project_urls": {
      "Homepage": "https://github.com/NicolaDonelli/py4ai-data"
    },
    "release_url": "https://pypi.org/project/py4ai-data/0.0.1/",
    "requires_dist": [
      "py4ai-core (>=1.0.0)",
      "dill (>=0.3)",
      "motor (>=2.5)",
      "pymongo (>=4.0)",
      "setuptools (>=61.2)",
      "sqlalchemy (>=1.4.39)",
      "tomli (>=2.0.1)",
      "tornado (>=6.1)",
      "typeguard (>=2.13.0)",
      "typing-extensions (>=4.0)",
      "aiosqlite (>=0.17.0) ; extra == 'dev'",
      "bandit[toml] (>=1.7.4) ; extra == 'dev'",
      "black (>=21.12b0) ; extra == 'dev'",
      "darglint (>=1.8.1) ; extra == 'dev'",
      "flake8 (>=4.0.1) ; extra == 'dev'",
      "flake8-docstrings (>=1.6) ; extra == 'dev'",
      "flake8-rst-docstrings (>=0.2.5) ; extra == 'dev'",
      "Flake8-pyproject (>=1.1.0) ; extra == 'dev'",
      "greenlet (>=1.1.3) ; extra == 'dev'",
      "isort (>=5.10.1) ; extra == 'dev'",
      "licensecheck (>=2023) ; extra == 'dev'",
      "m2r2 (<0.3.3,>=0.3.2) ; extra == 'dev'",
      "mongomock (>=3.19) ; extra == 'dev'",
      "mongomock-motor (>=0.0.2) ; extra == 'dev'",
      "mypy (>=0.910) ; extra == 'dev'",
      "mypy-extensions (>=0.4.3) ; extra == 'dev'",
      "pip-tools (>=6.6.2) ; extra == 'dev'",
      "pytest (>=6.2) ; extra == 'dev'",
      "pytest-cov (>=3.0) ; extra == 'dev'",
      "recommonmark (>=0.7) ; extra == 'dev'",
      "scipy (>=1.7.3) ; extra == 'dev'",
      "Sphinx (<6.0.0,>4.1) ; extra == 'dev'",
      "sphinx-rtd-theme (>=0.5) ; extra == 'dev'",
      "twine (>=3.8) ; extra == 'dev'",
      "versioneer (>=0.21) ; extra == 'dev'"
    ],
    "requires_python": ">=3.8",
    "summary": "A Python library of data structures optimized for machine learning tasks",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16434514,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aac4871a979be353051c349874d884f7e4dcb0d28c8020437c5dc782d1112e78",
          "md5": "999190578b3360e8182fe09da9f95340",
          "sha256": "116f29d531e634e7132c0b8d9fb10803cf47ad24857f4899ca82238446d1cf95"
        },
        "downloads": -1,
        "filename": "py4ai_data-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "999190578b3360e8182fe09da9f95340",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 56952,
        "upload_time": "2023-01-15T10:02:58",
        "upload_time_iso_8601": "2023-01-15T10:02:58.583826Z",
        "url": "https://files.pythonhosted.org/packages/aa/c4/871a979be353051c349874d884f7e4dcb0d28c8020437c5dc782d1112e78/py4ai_data-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "489b2864a41fb245b4a3e97b5bb24a7d1ea4e1e414f6396720e6846a9073cbe5",
          "md5": "fc7f3cbf32ad1faacd83575fee0e04e7",
          "sha256": "5e304ff56440e6cc06c44be41c4b336973d350e563d9f43ab2b9efd5822f97f2"
        },
        "downloads": -1,
        "filename": "py4ai-data-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "fc7f3cbf32ad1faacd83575fee0e04e7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 67858,
        "upload_time": "2023-01-15T10:03:00",
        "upload_time_iso_8601": "2023-01-15T10:03:00.577685Z",
        "url": "https://files.pythonhosted.org/packages/48/9b/2864a41fb245b4a3e97b5bb24a7d1ea4e1e414f6396720e6846a9073cbe5/py4ai-data-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "aac4871a979be353051c349874d884f7e4dcb0d28c8020437c5dc782d1112e78",
        "md5": "999190578b3360e8182fe09da9f95340",
        "sha256": "116f29d531e634e7132c0b8d9fb10803cf47ad24857f4899ca82238446d1cf95"
      },
      "downloads": -1,
      "filename": "py4ai_data-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "999190578b3360e8182fe09da9f95340",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 56952,
      "upload_time": "2023-01-15T10:02:58",
      "upload_time_iso_8601": "2023-01-15T10:02:58.583826Z",
      "url": "https://files.pythonhosted.org/packages/aa/c4/871a979be353051c349874d884f7e4dcb0d28c8020437c5dc782d1112e78/py4ai_data-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "489b2864a41fb245b4a3e97b5bb24a7d1ea4e1e414f6396720e6846a9073cbe5",
        "md5": "fc7f3cbf32ad1faacd83575fee0e04e7",
        "sha256": "5e304ff56440e6cc06c44be41c4b336973d350e563d9f43ab2b9efd5822f97f2"
      },
      "downloads": -1,
      "filename": "py4ai-data-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "fc7f3cbf32ad1faacd83575fee0e04e7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 67858,
      "upload_time": "2023-01-15T10:03:00",
      "upload_time_iso_8601": "2023-01-15T10:03:00.577685Z",
      "url": "https://files.pythonhosted.org/packages/48/9b/2864a41fb245b4a3e97b5bb24a7d1ea4e1e414f6396720e6846a9073cbe5/py4ai-data-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}