{
  "info": {
    "author": "Alexandre Erwin Ittner",
    "author_email": "alexandre@ittner.com.br",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: End Users/Desktop",
      "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.8",
      "Topic :: Internet :: WWW/HTTP",
      "Topic :: Internet :: WWW/HTTP :: Browsers"
    ],
    "description": "# About\n\nnewslinkrss generates RSS feeds from websites that do not provide their own.\nThis is done by loading a given URL and collecting links that matches a\npattern, given as a regular expression, to gather the relevant information,\noptionally visiting them to get more details and even processing the target\npages with XPath and CSS Selectors if required. It basically works as a\npurpose specific crawler or scraper.\n\nThe results are printed as a RSS feed to stdout or, optionally, to a file. The\nsimplest way to use it is just configure your **local** feed reader, like\n[Liferea](https://lzone.de/liferea/) or [Newsboat](https://newsboat.org/), to\nuse a \"command\" source and pass the correct command line arguments to generate\na suitable feed -- this allows you to centralize the configuration in the\nreader itself and let it handle update times, etc.\n\nRun `newslinkrss --help` for the complete list of command line options.\n\n\n# Intended audience (and a rant)\n\nThis script is mostly intended to technically versed people using some kind\nof Unix operating system (e.g. Linux). I was planning to write a detailed\ndocumentation but I just gave up. There is no much hope of making it friendly\nto casual users when every use case requires complex command lines, abuse of\nregular expressions, strftime strings, XPath, CSS Selectors, etc.\n\n## The Rant\n\nEverything would be just easier if websites simply provided clean and complete\nRSS or Atom feeds, but these sites are becoming rarer every day. Most sites\njust assume we want to follow them through social media (and that we *use*\nsocial media!) while giving away privacy and submitting ourselves to tracking\nand personal data collection in exchange for timelines algorithmically\noptimized to improve \"engagement\" with advertisers.\n\nI'm still resisting and wrote lots of feed scrapers/filters in the last 18 or\nso years; newslinkrss is one that replaced several of these ad-hoc filters by\ncentralizing some very common pieces of code and it is polished enough to be\npublished.\n\n\n\n\n# Installation\n\n\n## Installing from PyPI\n\nThis is the simplest installation method and will deliver the latest **stable**\nversion of the program. To install in your `$HOME` directory, just type:\n\n    pip3 install newslinkrss\n\nThis assumes pip3 is installed and configured (which usually happens by\ndefault in Linux distributions intended for general use).\n\n\n\n## Installing from the Git repository\n\nThis installation process is recommended for developers and people wanting\nthe newest version. For that, just clone the Git repository and install or\nupdate the program with:\n\n    pip3 install -U .\n\nYou may want to do this in a virtual environment so it won't interfere with\nother user-level or system-wide components and make experimentation with\ndevelopment versions easier. In this case, just do:\n\n    python3 -m venv my-venv\n    . my-venv/bin/activate\n    pip install -U .\n\n\nnewslinkrss depends on a few libraries, this will ensure all them are also\ninstalled correctly.\n\n\n\n# Usage examples\n\nnewslinkrss is lacking a lot of documentation, but the following examples\ncan show a bit of what it can and can not do. A complete list of options\ncan be read by typing `newslinkrss --help`.\n\n\n### Simplest case\n\nThe simplest use case is just load a website, select all links matching a\npattern and expose a feed using the text of that link as description and\nsetting the publish date to the date and time that the command was run. For\nexample, to generate a feed from site https://www.jaraguadosul.sc.gov.br/noticias.php ,\ncollecting all links with the substring `/news/` in the URL, use:\n\n    newslinkrss -p '.+/news/.+' https://www.jaraguadosul.sc.gov.br/noticias.php\n\nIt won't generate a good feed (the limitation with the dates being the\nbiggest issue) but we will go back to this example later. It is already\nmore practical than checking this government website manually, however.\n\n\n\n### Following pages\n\nTo improve the situation exposed in the previous example, we may want to\nget information that it not is available from the URL or anchor text.\nOption `--follow` (shortcut `-f`) will make newslinkrss load the candidate\ntarget page and look for more data there. By default, it automatically\ncaptures the title of the page as the title of the feed entry, keeps the\nsummary from anchor text, and loads author information and the page\npublishing and update dates and times from the page metadata (**if** this\ninformation is available in some common format, like\n[Open Graph](https://ogp.me/) or Twitter cards.\n\nReuters [killed](https://news.ycombinator.com/item?id=23576022) its RSS feeds\nin mid 2020, so let's take them as an example and use newslinkrss to bring\nthe feed back to life and right into our news readers. Our criteria will be:\n\n- First we must find every link that appears as plain HTML on the front page:\n  https://www.reuters.com/ There is an infinite scroll and more links are\n  added periodically with JavaScript, but we can just ignore this and poll\n  the page more frequently, giving enough chance to capture them;\n\n- We want to be very selective with filtering so we only get the current news\n  and do not waste time downloading things like section listings, utility\n  pages, links for other domains, etc. By looking at the URLs of the news\n  pages, we can notice that all of them follow a pattern similar to:\n  https://www.reuters.com/world/europe/eu-proposes-create-solidarity-fund-ukraines-basic-needs-2022-03-18/\n  Notice they all are in domain \"https://www.reuters.com/\", have at least one\n  section (\"/world/europe/\") that is followed by part of the title and the\n  publish date. This format is really great, as it allows us to ignore\n  anything that does not look like a news article on the spot. So, a good\n  pattern will be `'https://www.reuters.com/.+/.+\\d{4}-\\d{2}-\\d{2}.+'`;\n\n- newslinkrss deduplicates URLs automatically, so we don't need to worry if we\n  end up capturing the same link twice;\n\n- Target pages have Open Graph meta tags, so by just following them we can\n  get accurate publish dates and times with no extra effort. Better yet, as\n  we know that **all** news pages that we want have them, we can also instruct\n  newslinkrss to ignore any page without a valid date, preventing any non-news\n  article, captured by accident, from appearing in our feed. This is done\n  with option `--require-dates`;\n\n- All page titles are something like \"The actual headline | Reuters\". This\n  format is nice for a website but not so for feed items: the \"| Reuters\"\n  part is not only redundant (the feed title already tells us about the\n  source of the article) but also noise, as it makes scanning through the\n  headlines harder. newslinkrss has an option `--title-regex` for exactly\n  this use case of cleaning up redundant text from titles. It accepts a\n  regular expression with a single capture group; if the expression matches,\n  the text from the group will be used as the title, otherwise the original\n  text will be used (so we don't loose titles if something changes at the\n  source, for example). For this case, a good choice would be\n  `--title-regex '(.+)\\s+\\|'` .\n\n- When following pages we must be **very careful** as we do not want to\n  abuse the website or get stuck downloading gigabytes of data. newslinkrss\n  has several options to prevent these problems, all with sensible default\n  values, but we should check if they work for every use case. At first we\n  must limit number of links to be followed with option `--max-links` (the\n  default value is 50, so it is OK for this so we can just omit the option\n  for now), then we may use option `--max-page-length` to only load the\n  first 512 kB of data from the every followed link, and stop processing a\n  page after after a few seconds with option `--http-timeout` (the default\n  value is 2 s, so we can omit this option for now too);\n\n- Cookies will be remembered among these requests, but they will only be\n  kept in memory and forgotten once the program finishes (there is an\n  option `--no-cookies` if this behavior becomes a problem for a particular\n  source).\n\n\nSo, our syntax for this will be:\n\n    newslinkrss \\\n        --follow \\\n        -p 'https://www.reuters.com/.+/.+\\d{4}-\\d{2}-\\d{2}.+' \\\n        --max-page-length 512 \\\n        --require-dates \\\n        --title-regex '(.+)\\s+\\|' \\\n        https://www.reuters.com/\n\n\n### Generating complete feeds\n\nComplete feeds are the ones which include the full text of the article with\nit, instead of just a summary and a link. They are really nice as we can\nread everything in the news aggregator itself. A good item body should be\nmostly static and clean HTML (so no scripts, interactive content, aggressive\nformatting, etc.) leaving everything else to the aggregator to handle.\n\nLet's extend the previous example from Reuters website: as we are already\nfollowing and downloading the links, there is no much extra work to\ngenerate the full feed from it. Option `--with-body` will copy the entire\ncontents of the \"body\" element from the page into the feed, just removing a\nfew obviously unwanted tags (scripts, forms, etc.).\n\nIncluding the entire body works for every case, but for this site we can\nfilter a bit more and pick only actual text of the news article, ignoring\nunwanted noise like menus, sidebars, links to other news, etc. Running a\nquick \"inspect element\" in Firefox shows us that there is a single \"article\"\nelement in the pages and that it has the text we want. newslinkrss allows\nusing both XPath expressions and CSS Selectors to pick particular elements\nfrom the DOM and, for this case, we choose XPath by using option\n`--body-xpath '//article'` — sometimes CSS Selectors are easier and\ncleaner, if it appears that you are struggling too much with a particular\nXPath expression, try using a CSS Selector with `--body-csss` instead.\n\nSo, the updated syntax will be:\n\n    newslinkrss \\\n        --follow \\\n        -p 'https://www.reuters.com/.+/.+\\d{4}-\\d{2}-\\d{2}.+' \\\n        --max-page-length 512 \\\n        --with-body \\\n        --body-xpath '//article' \\\n        --require-dates \\\n        --title-regex '(.+)\\s+\\|' \\\n        https://www.reuters.com/\n\nAnd now we have our feed!\n\nThe body for this example is very simple but the selectors (both XPath and\nCSSS) are surprisingly powerful. They can return any number of elements in\na given order, so you can create a readable item body from whatever exists\nin the source page by carefully picking elements in the right order (XPath\noperator \"|\" and CSSS \",\" are your friends!).\n\n\n### A single feed from multiple start URLs\n\nIt is possible to have several start URLs and make newslinkrss generate a\nsingle feed with content gathered from all of them, all other options\nremaining the same. Typically, this can be used to filter only the sections\nof interest from a site while keeping all articles in the same subscription\nin your newsreader.\n\nLet's continue with our Reuters example, but imagine we only want articles\nfrom sections \"World\" and \"Technology\". We noticed before that the section\nnames appears in the URL, so a first approach may be just hack the link\npattern to require `(world|technology)` in it. However this solution may fail\nto grab, for example, some news articles that appears in the \"technology\"\npage but have a different section in their URLs (as they can be related to\nboth) or skip articles that do not appear in the front page at all (as space\nthere is limited).\n\nThis can be solved with this command:\n\n    newslinkrss \\\n        --follow \\\n        -p 'https://www.reuters.com/.+/.+\\d{4}-\\d{2}-\\d{2}.+' \\\n        --max-page-length 512 \\\n        --with-body \\\n        --body-xpath '//article' \\\n        --require-dates \\\n        --title-regex '(.+)\\s+\\|' \\\n        --title \"Reuters (Technology and World only)\" \\\n        https://www.reuters.com/technology/ https://www.reuters.com/world/\n\nNotice that it is almost the same command line used in the previous example,\nexcept for the multiple URLs and option `--title`, which allow us to give an\nalternate title to the feed. This is welcome because newslinkrss picks the\ntitle from the first URL that has one, and it may be related to the first\nsection only (alternatively, you can rename the feed in your newsreader if\nit has an option for it).\n\nWhen using multiple start pages, a special attention is required to the\nparameter `--max-links`! If the limit is reached in a page, links loaded\nfrom later pages will be skipped.\n\nThis feature may be also used to capture more items from sites that split\nthem in very short pages but does still have stable pagination URLs, like:\n`https://news.example.org/page-1.html https://news.example.org/page-2.html\nhttps://news.example.org/page-3.html`.  If you are using newslinkrss in a\nshell script, you may avoid repeated URLs by using sequence expansions (in\nbash) or `seq -f` (in everything else).\n\n\n### Gathering information from insufficient metadata\n\nSome sites do not provide standard (not even quasi-standard) metadata that\nnewslinkrss can use automatically, so we must gather it from the pages\nwith site-specific approaches, following links and stitching information\nfrom several elements together. Assume we want to generate a feed from\nhttps://revistaquestaodeciencia.com.br/ , which provides no much facilities\nfor it. Looking into the site we find that:\n\n- URLs for news articles have a date on them (in format `YYYY/MM/DD`), so it\n  is possible to use this in the URL pattern (option `-p`) to limit which\n  links the script will look for. Some are prefixed by a section stub and all\n  are followed by a string generated from the title, so the regex must accept\n  the date anywhere in it. Anything could be a filter here, but as all\n  articles have a date on it we don't need to look anywhere else;\n\n- There is no standard, not even de-facto standard, representation for the\n  date an article was published, so the alternative is taking it from the URL\n  too. This is done with options `--date-from-url` (which requires regular\n  expression with a group capturing the substring that contains the date)\n  `--url-date-fmt` (which defines the format of the date);\n\n- Inconsistencies in the link formats prevent us from getting all articles\n  titles from the links in the front page, so the alternative is to\n  `--follow` every candidate link, downloading the target page and looking\n  for  the title there.\n\n- As we are already following, there is no much extra effort to also\n  generate a complete feed. The full text of the article is in an \"article\"\n  element, so we can use `--body-xpath \"//article\"` here too.\n\nThe resulting command line is:\n\n    newslinkrss -p '.+/\\d{4}/\\d{2}/\\d{2}/.+' \\\n        --date-from-url '.*/(\\d{4}/\\d{2}/\\d{2})/.*' \\\n        --url-date-fmt '%Y/%m/%d' \\\n        --follow \\\n        --with-body \\\n        --body-xpath \"//article\" \\\n        --max-links 50 \\\n        --max-page-length 512 \\\n        --http-timeout 4 \\\n        'https://revistaquestaodeciencia.com.br/'\n\nTo make understanding easier, this example uses the long, verbose, form of\nsome options even when abbreviations are available. For the same reason, some\nof the options are set to the default values and are not strictly required\nbut they are listed anyway. See `newslinkrss --help` for details.\n\n\n### Using complex XPath expressions\n\nSometimes we need to fight really hard to get the date that a particular item\nwas last updated. Taking GitHub issues as an example: while GH provides Atom\nfeeds for releases and commits (but always to specific branches), there is\nno equivalent for issues and pull requests. Of course, there is an API for\nthat but it requires authentication with a GitHub account, enables tracking,\nand requires writing a specific bridge to get the data as a feed. This makes\nthe scraping approach easier even with the very convoluted example that\nfollows.\n\nThe URLs for issues and PRs are pretty usable, we can already use them to\nlimit how many issues will be shown, their status, filter by date, etc. Just\nlook at the one used in the example.\n\nHowever, we need to get the date of the last comment on the issue and set it\nas the publishing date of the item, otherwise the reader won't show us that\nit was updated. A solution is to follow every issue page while using a XPath\nexpression to find the last occurrence of a \"relative-time\" tag that GitHub\nuses to mark the timestamp of a comment and parse the absolute date from\nattribute \"datetime\". This is done with options `--date-from-xpath` and\n`--xpath-date-fmt`.\n\nThe resulting command line is the following:\n\n    newslinkrss \\\n        --follow \\\n        --with-body \\\n        --http-timeout 5 \\\n        --max-links 30 \\\n        --max-page-length 1024 \\\n        -p '^https://github.com/.+/issues/\\d+$' \\\n        --date-from-xpath '(//h3/a/relative-time)[last()]/@datetime' \\\n        --xpath-date-fmt '%Y-%m-%dT%H:%M:%SZ' \\\n        'https://github.com/lwindolf/liferea/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc'\n\nDebugging XPath expressions is not a very easy task, the simplest way is just\nopen the target page in Firefox, launch web developer tools and use the $x()\nhelper function to get what the expression will return, for example:\n`$x('(//h3/a/relative-time)[last()]/@datetime')`.\n\n\n\n### The first example, revisited\n\nNow that we have a few extra tricks on our sleeves we can check back that\nvery first example and fix some of its limitations:\n\n- First, we need the correct publish dates; they are neither in the URL nor\n  in the anchor text, so we need to `--follow` the pages and get them from\n  there. The pages also have no metadata for that, but there is a publish\n  date intended for human readers there in this slice of HTML:\n  `<small class=\"text-muted\"><b>23/12/2022</b> - some random text</small>`\n  The important part if that we can use a XPath expression to select that\n  date and then parse it; a good (not optimal! It does not follow CSS rules)\n  would be `--date-from-xpath '//small[@class=\"text-muted\"]/b/text()'` and\n  it will capture the \"23/12/2022\" from the text node of the inner \"b\"\n  element, no need to filter it through `--xpath-date-regex` to remove\n  unwanted text, so we can then parse it with `--xpath-date-fmt '%d/%m/%Y'`;\n\n- Let's be extra careful with the URL patterns, so we don't follow a\n  random link going to another domain;\n\n- As we are already following the pages, let's also generate a full feed.\n  The relevant part of the articles are inside a \"div\" element with\n  attribute \"id\" set to \"area_impressao\" (that's Portuguese for\n  \"printing_area\" and one may imagine it is being used to format the page\n  for printing, however there is neither an alternate style sheet nor a\n  @media selector for it ... anyway, at least it helps us to select the\n  correct text). We can isolate this element with a XPath expression\n  `'//div[@id=\"area_impressao\"]'` but this is slightly more complex than\n  the equivalent CSS Selector `div#area_impressao` and, as we already used\n  XPath in several examples, let's use a CSSS this time;\n\n- That site can be a bit slow sometimes, so let's be extra tolerant and\n  increase the HTTP timeout to 10 s;\n\nAnd then we have our fixed command line:\n\n    newslinkrss \\\n        -p 'https://www.jaraguadosul.sc.gov.br/news/.+' \\\n        --http-timeout 10 \\\n        --follow \\\n        --with-body \\\n        --body-csss 'div#area_impressao' \\\n        --date-from-xpath '//small[@class=\"text-muted\"]/b/text()' \\\n        --xpath-date-fmt '%d/%m/%Y' \\\n        https://www.jaraguadosul.sc.gov.br/noticias.php\n\n... not perfect, but it gives us a very practical and usable feed!\n\n\n\n## More useful notes\n\n\n### Capturing publish dates\n\nThe date when a particular item was published is one of the most useful pieces\nof information from a feed, so newslinkrss has several ways of getting it\nfrom source pages, with availability depending on the way it is used (i.e.\nwith or without `--follow`) and the information avaialble in the HTML.\nRelated options are the following:\n\n- If explicitly used, options `--date-from-xpath`, `--xpath-date-regex`, and\n  `--xpath-date-fmt` will allow reading dates from any element in the target\n  page which can be found with a XPath expression. Naturally, the target page\n  which must be downloaded with `--follow`; The XPath expression must return\n  a string (from the inner text or the attributes of an element), the second\n  must optionally provide a regular expression with a single capture group\n  with the date, and the third should give the date format. If no regex is\n  given, all the string will be used and if no date format is given, the code\n  will try some common date formats;\n\n- If explicitly used, options `--date-from-csss`, `--csss-date-regex`, and\n  `--csss-date-fmt` work in a similar way as the previous ones, but using\n  CSS Selectors instead. They are not as powerful or flexible as XPath, but\n  simpler, cleaner, and more suitable for HTML;\n\n- If explicitly used,  options `--date-from-text` and `--text-date-fmt` allow\n  reading the date from the anchor text (i.e., the text inside tag `<a>`)\n  associated to a particular entry in the index page; no `--follow` is\n  necessary. The first option must provide a regular expression with a single\n  capture group for the date (which allows removing non-date parts of the\n  string) and the second option should give the date format. If the format is\n  not given, the code will try some common date formats;\n\n- If explicitly used, options `--date-from-url` and `--url-date-fmt` allow\n  reading the date from the item link; this is specially interesting for\n  sites that have URLs in format\n  \"https://example.org/posts/2020/09/22/happy-hobbit-day/\" but provide\n  no better source for a publish date. Again, no `--follow` is necessary. The\n  first option must provide a regular expression with a single capture group\n  with the date and the second option should give the date format. If the\n  format is not given, code will try some common date formats;\n\n- If no date was found yet and option `--follow` is used, dates will be read\n  automatically from standard metadata (Open Graph, etc.) if they exists in\n  the target page;\n\n- If no date was found yet, but the HTTP request returned a \"Last-Modified\"\n  header, assumes it as the actual last modification date (even if the\n  server may be lying to us here).\n\nThese options are tried in this order with the first valid date being picked.\nThis way, options explicitly included in command line have priority, with\nhigher precendence being given to the ones which may return a \"good\" date.\nIf no date options are listed, or if could not grab a date from the document,\nstandard metadata and HTTP headers will be used (in this order).\n\n\n\n### Ignoring URLs\n\nThe link pattern (-p) has a counterpart `--ignore-pattern` (shortcut `-i`)\nwhich also accepts a regular expression and makes `newslinkrss` ignore any\nmatching URL. Depending on the amount of information that the website puts on\nthe URLs, this can be used for excluding native advertisement, uninteresting\nsections, or other unwanted content from the feed, without support from the\nfeed reader and without counting to the total URL limit (`-n`). While it is\npossible to add this ignore rule to the link pattern itself, using `-i`\nprevents that regular expression from becoming excessively complex and makes\ndebugging easier.\n\n\n### Cleaning URL query strings\n\nSometimes the source page has URLs with unwanted query string parameters,\nlike the [UTM trackers](https://en.wikipedia.org/wiki/UTM_parameters), or\nmultiple URLs differing only by irrelevant query string parameters and\npointing to the same destination page (and therefore confusing the duplicate\nlink detection). Option `--qs-remove-param` (shortcut `-Q`) may be used to\nfix this. If the name of a parameter matches the regular expression given in\nthis option, that name/value pair will be removed from the URL query string.\nThis option may be repeated many times if necessary. Example: `-Q '^utm.+'`\n(notice the anchor to only match a prefix).\n\n\n\n### Excluding body elements\n\nWhen generating a complete feed we may sometimes end up including some\nunwanted elements from the source page into the item body, usually ads,\n\"related news\" boxes, section headers, random images, distracting formatting,\nunrelated links, etc. Some, but not all, of these noise sources can be\nremoved by carefully crafting XPath expressions or CSS selectors, but for the\ncases where this is not possible, demand too much effort, or result in a\nfragile solution, we can just remove the unwanted elements explicitly.\n\nnewslinkrss has tree command line options for this:\n\n- Option `--body-remove-tag` (shortcut `-R`) will remove all occurrences of\n  the given tag from the feed body and move their child elements to their\n  parents. This can be used to remove formatting while preserving the inner\n  text (e.g. `-R strong`) or to remove images from the body (with `-R  img`,\n  as \"img\" elements have no children);\n\n- Option `--body-remove-xpath` (shortcut `-X`) will remove the elements\n  given by a XPath expression **and** their children. This is a good way\n  to remove banners, divs, etc. from the generated feed;\n\n- Option `--body-remove-csss` (shortcut `-C`) will remove the elements given\n  by a CSS Selector **and** their children. This is another way to remove\n  banners, divs, etc. from the generated and more practical when selecting\n  element by their CSS classes.\n\nAll three options can be repeated in the command line how many times as\nnecessary to express all required rules.\n\n\n\n### Testing links\n\nnewslinkrss has an option `--test` that will skip the feed generation step\nand just print the links and titles that were captured for a particular set\nof options to stdout. That's a simple way to check if a pattern is working\nas intended.\n\n\n### Logging\n\nThings **will** go wrong when experimenting with ugly regexes and confusing\nXPath expressions or when fighting unexpected changes in some website's DOM.\nSimplest way to see how newslinkrss is reacting internally is to increase\noutput verbosity with option `--log`; default value is \"warning\", but \"info\"\nand \"debug\" will give more information like which element a XPath expression\nfound, if a regex matched or how some date was interpreted.\n\nThis information is always printed to stderr so it will not affect the feed\noutput written to stdout; when debugging in the terminal, remember to\nredirect the output to a file with the shell or command line option `-o`.\n\n\n### Error reporting\n\nBy default, newslinkrss writes exceptions and error messages to the feed\noutput itself. This is a very practical way to report errors to the user, as\nthis program is intended to work mostly on the background of the actual\nuser-facing application. It is possible to disable this behavior with option\n`--no-exception-feed` (shortcut `-E`).\n\nNotice that the program always return a non-zero status code on failures. Some\nnews readers (e.g. Liferea) won't process the output in these cases and discard\nthe feed entries with the error reports. You may need to override the status\ncode with something like `newslinkrss YOUR_OPTIONS; exit 0`.\n\n\n### Writing output to a file\n\nOption `-o` allows writing the output to an file; it is no much different\nthan redirecting stdout, but will ensure that only valid XML with the right\nencoding is written.\n\nWriting output to files and error reporting on the feed itself allows for\nsome unusual but interesting use patterns: for example, it is trivial for a\ncompany, group, or development team to have an internal \"feed server\", where\nfeeds are centrally downloaded by a cron job, saved to a web server public\ndirectory and then transparently provided to the end users. A setup with\nAlpine and nginx running in a LXD container is surprisingly small.\n\n\n\n\n## Caveats\n\nBe very careful with escape characters! In the shell, it is recommended to\nuse single quotes for regexes, so \"\\\\\" is not escaped. This becomes more\nconfusing if your feed reader also use escape sequences but with different\nor conflicting rules (e.g. Newsboat uses \"\\\\\" as escapes but does not follow\nthe same rules used by bash). When in doubt, run the command in the shell or\nuse a wrapper script to test for any unusual behavior introduced by the\nprogram.\n\nSome feed readers run commands in a synchronous (blocking) mode and their\ninterface will get stuck until the command terminates. Liferea had this\nbehavior [until version 1.13.7](https://github.com/lwindolf/liferea/commit/b03af3b0f6a4e42b17dfa49782faa6c044055738),\nfor example. A typical workaround is to create a script with all calls to\nnewslinkrss that saves the generated feeds to files (see option `-o`),\nschedule this script to run from cron and configure Liferea to load the\nfeed from the files. This solves the frozen interface problem, but requires\nconfiguring feeds in two different places.\n\n\n\n# Bugs\n\nYes :-)\n\n\n\n# Contributing\n\nFor this project I started an experiment: I'm using [sourcehut](https://sr.ht/)\nas the primary repository for code and other tools, leaving the usual Github\nand Gitlab as non-advertised mirrors. Sourcehut does everything by the\nold-school way and most features just work without an user account. Check the\n[project page](https://sr.ht/~ittner/newslinkrss/) there to see how it works.\n\nIf this sounds too strange, just clone the repository with an\n`git clone https://git.sr.ht/~ittner/newslinkrss`, publish your fork somewhere\nand send an email with the branches to be merged to `~ittner/newslinkrss@lists.sr.ht`\n(that's correct: tildes and slashes are valid characters for email addresses).\n\nIf you change the code, please run in through pyflakes for static analysis and\n[black](https://pypi.org/project/black/) to ensure a consistent formatting.\n\n\n\n\n# License\n\nCopyright (C) 2020  Alexandre Erwin Ittner <alexandre@ittner.com.br>\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <https://www.gnu.org/licenses/>.\n\n\n\n\n# Contact information\n\n- Author: Alexandre Erwin Ittner\n- Email: <alexandre@ittner.com.br>\n- Web: <https://www.ittner.com.br>\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://sr.ht/~ittner/newslinkrss/",
    "keywords": "",
    "license": "GPLv3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "newslinkrss",
    "package_url": "https://pypi.org/project/newslinkrss/",
    "platform": null,
    "project_url": "https://pypi.org/project/newslinkrss/",
    "project_urls": {
      "Homepage": "https://sr.ht/~ittner/newslinkrss/"
    },
    "release_url": "https://pypi.org/project/newslinkrss/0.8.0/",
    "requires_dist": [
      "PyRSS2Gen (>=1.1.0)",
      "cssselect (>=1.2.0)",
      "lxml (>=4.6.3)",
      "python-dateutil (>=2.6.1)",
      "requests (>=2.18.4)"
    ],
    "requires_python": ">=3.8",
    "summary": "Generate RSS feeds from generic sites",
    "version": "0.8.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16695346,
  "releases": {
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "35165f8f770ea3492a83f7aa0d1cbeceb896cbba9e8ba38fd12b72b8bed3ba88",
          "md5": "286025166753d3386557a355e912e37e",
          "sha256": "6e7cfa9d4008a924c2bd2536d8ee9a25c0f3685e58c4f3428b55b640a03cdc06"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "286025166753d3386557a355e912e37e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27176,
        "upload_time": "2021-12-29T00:05:30",
        "upload_time_iso_8601": "2021-12-29T00:05:30.030707Z",
        "url": "https://files.pythonhosted.org/packages/35/16/5f8f770ea3492a83f7aa0d1cbeceb896cbba9e8ba38fd12b72b8bed3ba88/newslinkrss-0.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dd0388d15f77da2446721187f910b0b5f61de36eae6956061586f44a7953ed6c",
          "md5": "c3f9acb6cc7e70fa50687c842e3dbe90",
          "sha256": "0611b90ea1a84e43ef00e9cf7dfdd8299010ca7b6ac4c82bdee45b85ccede73e"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c3f9acb6cc7e70fa50687c842e3dbe90",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 30536,
        "upload_time": "2021-12-29T00:12:45",
        "upload_time_iso_8601": "2021-12-29T00:12:45.136289Z",
        "url": "https://files.pythonhosted.org/packages/dd/03/88d15f77da2446721187f910b0b5f61de36eae6956061586f44a7953ed6c/newslinkrss-0.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e36c9f2f2237916a092f6e30ad1d21f3737468cca2a01eec9cecfc41cb9176ae",
          "md5": "51fcdbd27137200349d2729294ecf187",
          "sha256": "2cdb1ba33da779ded7c4bbac1641b6942831090cdd9776b015b5b16f534bc546"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.5.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "51fcdbd27137200349d2729294ecf187",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 27194,
        "upload_time": "2021-12-29T00:18:16",
        "upload_time_iso_8601": "2021-12-29T00:18:16.049366Z",
        "url": "https://files.pythonhosted.org/packages/e3/6c/9f2f2237916a092f6e30ad1d21f3737468cca2a01eec9cecfc41cb9176ae/newslinkrss-0.5.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fdf6d01355d98a6eee105b1f0a5fa08000d33d29c35535566301397b05a0ac88",
          "md5": "5d0b9f7db18d0c85b777edf99a516b9b",
          "sha256": "8c49046bcd04210daed11ea7ec8fa30b1e084728a25b2cc901ec43d3baeff4d3"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.5.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5d0b9f7db18d0c85b777edf99a516b9b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 30594,
        "upload_time": "2021-12-29T00:18:17",
        "upload_time_iso_8601": "2021-12-29T00:18:17.777606Z",
        "url": "https://files.pythonhosted.org/packages/fd/f6/d01355d98a6eee105b1f0a5fa08000d33d29c35535566301397b05a0ac88/newslinkrss-0.5.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4fbfd494f7d493216646993c0e2be71fad3244a5500a551d0a346e53358a3239",
          "md5": "051e524a5c203bb161053ccbdb056ca0",
          "sha256": "4127cca4466650a2ebd0391dcc324c48f055f0571599ebb3db4857901d276678"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.5.2-py3.6.egg",
        "has_sig": false,
        "md5_digest": "051e524a5c203bb161053ccbdb056ca0",
        "packagetype": "bdist_egg",
        "python_version": "0.5.2",
        "requires_python": ">=3.6",
        "size": 16423,
        "upload_time": "2022-05-09T01:58:09",
        "upload_time_iso_8601": "2022-05-09T01:58:09.289842Z",
        "url": "https://files.pythonhosted.org/packages/4f/bf/d494f7d493216646993c0e2be71fad3244a5500a551d0a346e53358a3239/newslinkrss-0.5.2-py3.6.egg",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08298ac85082f1737ef2d434396b235ca61a259bede08883fde3474f7f138f5a",
          "md5": "e99abaae6d66809e775bb8ae9ead10d7",
          "sha256": "ba21a6f97259a3cd2e94cd01a6c9ea607ea541f3c3ba72147b1c4c606da57e1f"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.5.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e99abaae6d66809e775bb8ae9ead10d7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 28795,
        "upload_time": "2022-05-09T01:58:07",
        "upload_time_iso_8601": "2022-05-09T01:58:07.573555Z",
        "url": "https://files.pythonhosted.org/packages/08/29/8ac85082f1737ef2d434396b235ca61a259bede08883fde3474f7f138f5a/newslinkrss-0.5.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cad3972009051aaf501afa8d2927c6e707485dbcd4b71ce443d5661c4ba90333",
          "md5": "bb0d67e52fd25f0cffc385e5243dd15c",
          "sha256": "26dabaf8f2ea97cccda13d1b6774cfba22f1e9364acb2e77dbad1f8606d4ddae"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.5.2.tar.gz",
        "has_sig": false,
        "md5_digest": "bb0d67e52fd25f0cffc385e5243dd15c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 17054,
        "upload_time": "2022-05-09T01:58:10",
        "upload_time_iso_8601": "2022-05-09T01:58:10.456896Z",
        "url": "https://files.pythonhosted.org/packages/ca/d3/972009051aaf501afa8d2927c6e707485dbcd4b71ce443d5661c4ba90333/newslinkrss-0.5.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "335bc8092dcb0560d9b030ef2e1ab01a62ccfd9202a35a8de1daeed7619db40c",
          "md5": "3d2f2222c72903df613b83088ef53232",
          "sha256": "02fe518d63e429ac3d2706a1db9d6ac40fcdb1626add0fa3cc8d32eed050af65"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.6.0-py3.8.egg",
        "has_sig": false,
        "md5_digest": "3d2f2222c72903df613b83088ef53232",
        "packagetype": "bdist_egg",
        "python_version": "0.6.0",
        "requires_python": ">=3.6",
        "size": 17536,
        "upload_time": "2022-09-25T01:12:53",
        "upload_time_iso_8601": "2022-09-25T01:12:53.152603Z",
        "url": "https://files.pythonhosted.org/packages/33/5b/c8092dcb0560d9b030ef2e1ab01a62ccfd9202a35a8de1daeed7619db40c/newslinkrss-0.6.0-py3.8.egg",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7ef966a894dd85928b744f1f70109d3687a1277accbf848582573a6d87cabf28",
          "md5": "8a0323e5f01c4db02fa5e8ba52a24743",
          "sha256": "35025e94a00d53e090e732fd823604c3913ea78fdb61b662b53066bc9742ee2d"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.6.0-py3-none-any.whl",
        "has_sig": true,
        "md5_digest": "8a0323e5f01c4db02fa5e8ba52a24743",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 29877,
        "upload_time": "2022-09-25T01:05:19",
        "upload_time_iso_8601": "2022-09-25T01:05:19.799472Z",
        "url": "https://files.pythonhosted.org/packages/7e/f9/66a894dd85928b744f1f70109d3687a1277accbf848582573a6d87cabf28/newslinkrss-0.6.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4780d505931e54f453f29eaa49af6167e6b8d1f6ecc9d421c19f09b539414696",
          "md5": "51ccf06ead7eae54d021e8c0f480ad03",
          "sha256": "2e53f7042a9966efa83825eb7b0e501cfb2f76dd1cff55c8a0b339d905638adb"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "51ccf06ead7eae54d021e8c0f480ad03",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 36715,
        "upload_time": "2022-09-25T01:10:07",
        "upload_time_iso_8601": "2022-09-25T01:10:07.774974Z",
        "url": "https://files.pythonhosted.org/packages/47/80/d505931e54f453f29eaa49af6167e6b8d1f6ecc9d421c19f09b539414696/newslinkrss-0.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.7.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fce6989d7a939fe497d1489719f2891c81ba18278576f2f8f3185a330781a39d",
          "md5": "876b6ea61709b934452a1249fff8c8df",
          "sha256": "962ede87dad08ddcef0997398c26b66f3dd04a2a769439c3ed6a3de093920201"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.7.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "876b6ea61709b934452a1249fff8c8df",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 35095,
        "upload_time": "2023-01-02T15:42:31",
        "upload_time_iso_8601": "2023-01-02T15:42:31.393489Z",
        "url": "https://files.pythonhosted.org/packages/fc/e6/989d7a939fe497d1489719f2891c81ba18278576f2f8f3185a330781a39d/newslinkrss-0.7.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c2fa37352adbca2c26d3ab4c93a323a0e6385fdd1db742152bb6ebe53875cf4e",
          "md5": "bfa63b91181899bf818cf6eaa22dff45",
          "sha256": "694de4e1f83210f05b156d890eedcba588a5b4e9dfa2207276bf06734a85532d"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.7.0.tar.gz",
        "has_sig": false,
        "md5_digest": "bfa63b91181899bf818cf6eaa22dff45",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 37008,
        "upload_time": "2023-01-02T15:42:32",
        "upload_time_iso_8601": "2023-01-02T15:42:32.848420Z",
        "url": "https://files.pythonhosted.org/packages/c2/fa/37352adbca2c26d3ab4c93a323a0e6385fdd1db742152bb6ebe53875cf4e/newslinkrss-0.7.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.8.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7da2aeabc46d25144544d9b0561b63e11c65fd9f5b7bb1d18fa6f3b53c9925ee",
          "md5": "525ea6c649b9ff05595f3cf37db77a76",
          "sha256": "5818eb6ff77cbc62c0f70bf03ef00c43dab6271d074c682d813c66d166c08bd7"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.8.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "525ea6c649b9ff05595f3cf37db77a76",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 38198,
        "upload_time": "2023-02-04T19:36:46",
        "upload_time_iso_8601": "2023-02-04T19:36:46.396726Z",
        "url": "https://files.pythonhosted.org/packages/7d/a2/aeabc46d25144544d9b0561b63e11c65fd9f5b7bb1d18fa6f3b53c9925ee/newslinkrss-0.8.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "30c1c4ea53f3d5693c145e6274a3d0428e3aa3ee7de87c1aabd1f086924e6781",
          "md5": "25a02bc40e7fc53658735825e8abe96e",
          "sha256": "4b3d8189eb7c25c7b2e76abbe64fbc64a496932edaa6a04c5852452452d7fdf3"
        },
        "downloads": -1,
        "filename": "newslinkrss-0.8.0.tar.gz",
        "has_sig": false,
        "md5_digest": "25a02bc40e7fc53658735825e8abe96e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 47596,
        "upload_time": "2023-02-04T19:36:48",
        "upload_time_iso_8601": "2023-02-04T19:36:48.915460Z",
        "url": "https://files.pythonhosted.org/packages/30/c1/c4ea53f3d5693c145e6274a3d0428e3aa3ee7de87c1aabd1f086924e6781/newslinkrss-0.8.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7da2aeabc46d25144544d9b0561b63e11c65fd9f5b7bb1d18fa6f3b53c9925ee",
        "md5": "525ea6c649b9ff05595f3cf37db77a76",
        "sha256": "5818eb6ff77cbc62c0f70bf03ef00c43dab6271d074c682d813c66d166c08bd7"
      },
      "downloads": -1,
      "filename": "newslinkrss-0.8.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "525ea6c649b9ff05595f3cf37db77a76",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 38198,
      "upload_time": "2023-02-04T19:36:46",
      "upload_time_iso_8601": "2023-02-04T19:36:46.396726Z",
      "url": "https://files.pythonhosted.org/packages/7d/a2/aeabc46d25144544d9b0561b63e11c65fd9f5b7bb1d18fa6f3b53c9925ee/newslinkrss-0.8.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "30c1c4ea53f3d5693c145e6274a3d0428e3aa3ee7de87c1aabd1f086924e6781",
        "md5": "25a02bc40e7fc53658735825e8abe96e",
        "sha256": "4b3d8189eb7c25c7b2e76abbe64fbc64a496932edaa6a04c5852452452d7fdf3"
      },
      "downloads": -1,
      "filename": "newslinkrss-0.8.0.tar.gz",
      "has_sig": false,
      "md5_digest": "25a02bc40e7fc53658735825e8abe96e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 47596,
      "upload_time": "2023-02-04T19:36:48",
      "upload_time_iso_8601": "2023-02-04T19:36:48.915460Z",
      "url": "https://files.pythonhosted.org/packages/30/c1/c4ea53f3d5693c145e6274a3d0428e3aa3ee7de87c1aabd1f086924e6781/newslinkrss-0.8.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}