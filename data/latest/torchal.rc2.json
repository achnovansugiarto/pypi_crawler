{
  "info": {
    "author": "Prateek Munjal",
    "author_email": "prateekmunjal31@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3"
    ],
    "description": "# TorchAL codebase\n\nSource code for our CVPR 2022 Paper: [Towards Robust and Reproducible Active Learning Using Neural Networks](https://arxiv.org/abs/2002.09564)\n\n<figure>\n\n<img src=\"https://raw.githubusercontent.com/PrateekMunjal/TorchAL/master/paper_images/cifar_five_lSet_statistics_latest_wo_cog_HR.png\" alt=\"cifar_five_lSet_statistics_latest_wo_cog\"/>\n<figcaption align = \"center\">Figure 1. Comparisons of AL methods on CIFAR10 (top) and CIFAR100 (bottom) for different initial labeled sets L0, L1, · · · , L4. The mean accuracy for the base model (at 10% labeled data) is noted at the bottom of each subplot. The model is trained 5 times for different random initialization seeds where for the first seed we use AutoML to tune hyper-parameters and re-use these hyper-parameters for the other 4 seeds.</figcaption>\n</figure>\n\n## Abstract\n\nActive learning (AL) is a promising ML paradigm that has the potential to parse through large unlabeled data \nand help reduce annotation cost in domains where labeling data can be prohibitive. Recently proposed neural \nnetwork based AL methods use different heuristics to accomplish this goal. In this study, we demonstrate that \nunder identical experimental settings, different types of AL algorithms (uncertainty based, diversity based, \nand committee based) produce an inconsistent gain over random sampling baseline. Through a variety of \nexperiments, controlling for sources of stochasticity, we show that variance in performance metrics achieved \nby AL algorithms can lead to results that are not consistent with the previously reported results. We also \nfound that under strong regularization, AL methods show marginal or no advantage over the random sampling \nbaseline under a variety of experimental conditions. Finally, we conclude with a set of recommendations on \nhow to assess the results using a new AL algorithm to ensure results are reproducible and robust under \nchanges in experimental conditions. We share our codes to facilitate AL evaluations. We believe our findings \nand recommendations will help advance reproducible research in AL using neural networks.\n\n## What is TorchAL?\n\nTL;DR: An Active Learning framework built on top of [pycls](https://github.com/facebookresearch/pycls).\n\nTorchAL is an evaluation toolkit with a motive to advance the **reproducible** research in deep active learning. We currently implement state-of-the-art \nactive learning (AL) algorithms. Our tookit extends the widely used [pycls](https://github.com/facebookresearch/pycls) codebase under AL settings.\n\n## Features of TorchAL\n\n* We report strong random baselines across widely used architectures and datasets.\n* Our baselines are well-trained using AutoML which helps in reducing the bias introduced by choosing sub-optimal hyper-parameters.\n* As we dream of reproducible results in AL, we release the training, validation index sets so that newer AL methods in future can use exact labeled set for training as we used to report our strong baselines.\n* For familiarity with the codebase, we recommend interested users to go through the [notebooks](https://github.com/PrateekMunjal/TorchAL/tree/master/notebooks). \n\n## AutoML in Active Learning\n\nDuring AL iterations we observed that labeled set changes and therefore does the class distribution too. \nThus in contrast to contemporary AL methods which fix the training hyper-parameters at the start of AL, we \ntune the training hyper-parameters using AutoML. To facilitate this we make use of [optuna](https://optuna.org/)\nto perform random search over 50 trials for each AL cycle.\n\n<!-- <img src=\"./paper_images/AL_cycles_anim.gif\" />-->\n\n<img src=\"https://raw.githubusercontent.com/PrateekMunjal/TorchAL/master/paper_images/AL_cycles_anim.gif\" />\n\n## Requirements\n* For creating a conda environment, kindly refer to [conda_env.yaml](conda_env.yml)\n* For installing dependencies via pip, kindly refer to [requirements.txt](requirements.txt)\n\nNOTE: In either case we have to download the dataset indexes and follow tools/train_al.py:\nDataset index sets\n```shell\nwget https://github.com/PrateekMunjal/torchal/blob/master/dataset_indexes.zip\n\n```\n\n## Installation\n\n### From source\n\n```\ngit clone https://github.com/PrateekMunjal/TorchAL\ncd TorchAL\npython setup.py install\n```\n\n### From pip\n```\npip install torchal\n```\n\n## Dataset indexes and Pretrained models\n\n* Dataset and active set indexes: [Click here to download](https://github.com/PrateekMunjal/TorchAL/blob/master/dataset_indexes.zip)\n* Pretrained CIFAR models trained on 10% data: [Click here to download](https://drive.google.com/drive/folders/102nZ5pCofBDncKDFQc9tnyvQ8__IcIOi)\n\nFor interested readers we recommend to checkout notebook ensuring reproducible active sets. [Link to Notebook](https://github.com/PrateekMunjal/TorchAL/blob/master/notebooks/Active_Sampling.ipynb)\n\n## AL algorithms implemented\n\n* Uncertainty\n* Coreset\n* BALD\n* DBAL\n* VAAL\n* QBC\n* Random Baseline\n\n## Experimental Settings\n\nNOTE: Below 5% meains 5 percent of the full training dataset. For example, for CIFAR10 - we have 50k datapoints as their official training split so with 5% we have 2500 datapoints in our initial labeled set.\n\n* Different budget sizes: 5%, 10% \n* Different validation set sizes: 2%, 5%, 10%\n* Effect of regularization techniques\n* We share [notebooks](https://github.com/PrateekMunjal/TorchAL/tree/master/notebooks) demonstrating such different experimental settings. \n* For interested readers we recommend to follow the summary of experiments presented [here](https://github.com/PrateekMunjal/TorchAL/tree/master/experiment_settings).\n\n## Examples\n\n### Run the random baseline without strong regularization\n\n<details>\n  <summary>Expand to see the full script</summary>\n\n```\npythonExec=$1\n\ncd /raid/shadab/prateek/newcode\n\n# script params\nport=5035\nsampling_fn=uncertainty\nlSet_partition=1\nbase_seed=1\nnum_GPU=2\nal_iterations=4 #7 #4\nnum_aml_trials=3 #50\nbudget_size=5000 #2500\n\ndataset=CIFAR10\ninit_partition=10\nstep_partition=10\nclf_epochs=5 #150\nnum_classes=10\n\nlog_iter=40\n\n#Data arguments\ntrain_dir=/raid/shadab/prateek/newcode/data/$dataset/train-$dataset/\ntest_dir=/raid/shadab/prateek/newcode/data/$dataset/test-$dataset/\nlSetPath=/raid/shadab/prateek/newcode/data/$dataset/partition_$lSet_partition/lSet_$dataset.npy\nuSetPath=/raid/shadab/prateek/newcode/data/$dataset/partition_$lSet_partition/uSet_$dataset.npy\nvalSetPath=/raid/shadab/prateek/newcode/data/$dataset/partition_$lSet_partition/valSet_$dataset.npy\n\n#for lSet 1\nout_dir=/raid/shadab/prateek/newcode/results \n\n# for other lSet Exps\n# out_dir=/raid/shadab/prateek/newcode/results_lSetPartitions\n\n#model_types: (i) wide_resnet_50 (ii) wide_resnet_28_10 (iii) wide_resnet_28_2\n\nmodel_style=vgg_style\nmodel_type=vgg #resnet_shake_shake\nmodel_depth=16 #26\n\nexport CUDA_VISIBLE_DEVICES=0,1\n\n$pythonExec tools/main_aml.py --n_GPU $num_GPU \\\n--port $port --sampling_fn $sampling_fn --lSet_partition $lSet_partition \\\n--seed_id $base_seed \\\n--init_partition $init_partition --step_partition $step_partition \\\n--dataset $dataset --budget_size $budget_size \\\n--out_dir $out_dir \\\n--num_aml_trials $num_aml_trials --num_classes $num_classes \\\n--al_max_iter $al_iterations \\\n--model_type $model_type --model_depth $model_depth \\\n--clf_epochs $clf_epochs \\\n--eval_period 1 --checkpoint_period 1 \\\n--lSetPath $lSetPath --uSetPath $uSetPath --valSetPath $valSetPath \\\n--train_dir $train_dir --test_dir $test_dir \\\n--dropout_iterations 25 \\\n--cfg configs/$dataset/$model_style/$model_type/R-18_4gpu_unreg.yaml \\\n--vaal_z_dim 32 --vaal_vae_bs 64 --vaal_epochs 15 \\\n--vaal_vae_lr 5e-4 --vaal_disc_lr 5e-4 --vaal_beta 1.0 --vaal_adv_param 1.0 \\\n\n```\n</details>\n\n<br>\nUsage: Assume above script is named as **run.sh**, then we can simply run it \n\n```\nsh run.sh `which python`\n```\n\n<br>\n\n### Run the random baseline with strong regularization\n\nIn the above script we only need to add few more switches to add strong-regularization.\n\n```\nswa_lr=5e-4\nswa_freq=50\nswa_epochs=5 #50\n\n...\n--rand_aug --swa_mode --swa_freq $swa_freq --swa_lr $swa_lr \\\n--swa_epochs $swa_epochs --swa_iter 0 \\\n\n```\n\n## Citing TorchAL\n\nIf you use TorchAL, please consider citing:\n\n    @inproceedings{Munjal2022TorchAL,\n        title={Towards Robust and Reproducible Active Learning Using Neural Networks}, \n        author={Prateek Munjal and Nasir Hayat and Munawar Hayat and Jamshid Sourati \n                and Shadab Khan},\n        booktitle={CVPR},\n        year={2022}\n    }\n\n## Acknowledgement \n\nThis repository is built using the following repositories. Thanks for their wonderful works. \n\n* [pycls](https://github.com/facebookresearch/pycls)\n* [Coreset](https://github.com/ozansener/active_learning_coreset)\n* [VAAL](https://github.com/sinhasam/vaal)\n* [CoreGCN](https://github.com/razvancaramalau/Sequential-GCN-for-Active-Learning)\n\n## Contact\n\nIf you have any question about this project, please feel free to contact prateekmunjal31@gmail.com or skhan.shadab@gmail.com.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/PrateekMunjal/TorchAL",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torchal",
    "package_url": "https://pypi.org/project/torchal/",
    "platform": null,
    "project_url": "https://pypi.org/project/torchal/",
    "project_urls": {
      "Homepage": "https://github.com/PrateekMunjal/TorchAL"
    },
    "release_url": "https://pypi.org/project/torchal/0.0.2/",
    "requires_dist": [
      "torch (==1.6.0)",
      "torchcontrib (==0.0.2)",
      "torchvision (==0.7.0)",
      "tqdm (==4.63.0)",
      "yacs (==0.1.8)",
      "simplejson (==3.17.6)",
      "optuna (==2.1.0)",
      "plotly (==5.6.0)",
      "matplotlib (==3.3.4)",
      "scipy (==1.5.2)",
      "scikit-learn (==0.24.2)",
      "opencv-python (==3.4.2.17)"
    ],
    "requires_python": "",
    "summary": "A codebase for active learning built on top of pycls.",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13575208,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b00ceac1c28b3c076b163a6a980c38ee64c0b1db1e95a2acc19329a9284b66b4",
          "md5": "b6702c28032934940e05624514612ec9",
          "sha256": "7cee5a1bc13bf38ebb750ecfec5ac42c2573d7dce8dbe5e7b557935f9b50afed"
        },
        "downloads": -1,
        "filename": "torchal-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b6702c28032934940e05624514612ec9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 125961,
        "upload_time": "2022-04-03T01:32:42",
        "upload_time_iso_8601": "2022-04-03T01:32:42.570180Z",
        "url": "https://files.pythonhosted.org/packages/b0/0c/eac1c28b3c076b163a6a980c38ee64c0b1db1e95a2acc19329a9284b66b4/torchal-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "45bf7e22b2dbabb73dd45288b0e0dfa854a637e56653b6acc43719f9c1a72815",
          "md5": "04f8182b42b6ed5b8d9de3dbc79fc13e",
          "sha256": "9e3946c9e112b835f7a4326d89fe29f8f7620b5b04ae56a55ab9fbf3e795583b"
        },
        "downloads": -1,
        "filename": "torchal-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "04f8182b42b6ed5b8d9de3dbc79fc13e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 94925,
        "upload_time": "2022-04-03T01:32:45",
        "upload_time_iso_8601": "2022-04-03T01:32:45.092994Z",
        "url": "https://files.pythonhosted.org/packages/45/bf/7e22b2dbabb73dd45288b0e0dfa854a637e56653b6acc43719f9c1a72815/torchal-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1d51e81fcc2e26ac05fc6f1c664198f5605da7c544803488c606b7a7fa6acf69",
          "md5": "b88e65dd5d8a437aa91c700dd39943ee",
          "sha256": "295aedcd20e34ef6e1b71f5b9cd54bdd110a9f7f4f751e2f598051bf91d1cd65"
        },
        "downloads": -1,
        "filename": "torchal-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b88e65dd5d8a437aa91c700dd39943ee",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 127070,
        "upload_time": "2022-04-21T05:19:23",
        "upload_time_iso_8601": "2022-04-21T05:19:23.477035Z",
        "url": "https://files.pythonhosted.org/packages/1d/51/e81fcc2e26ac05fc6f1c664198f5605da7c544803488c606b7a7fa6acf69/torchal-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "daa6c96527a009b14ae3c1bbeed3f6fbf3930e5ee5f2b62cfbbee2970e40fa76",
          "md5": "109db716f9501e5722b4ad833b9fd286",
          "sha256": "a4f30be2d5797bbb4610e34fe6f12e37966460befaa82daa4cf1e7df66418970"
        },
        "downloads": -1,
        "filename": "torchal-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "109db716f9501e5722b4ad833b9fd286",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 96156,
        "upload_time": "2022-04-21T05:19:26",
        "upload_time_iso_8601": "2022-04-21T05:19:26.037759Z",
        "url": "https://files.pythonhosted.org/packages/da/a6/c96527a009b14ae3c1bbeed3f6fbf3930e5ee5f2b62cfbbee2970e40fa76/torchal-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1d51e81fcc2e26ac05fc6f1c664198f5605da7c544803488c606b7a7fa6acf69",
        "md5": "b88e65dd5d8a437aa91c700dd39943ee",
        "sha256": "295aedcd20e34ef6e1b71f5b9cd54bdd110a9f7f4f751e2f598051bf91d1cd65"
      },
      "downloads": -1,
      "filename": "torchal-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b88e65dd5d8a437aa91c700dd39943ee",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 127070,
      "upload_time": "2022-04-21T05:19:23",
      "upload_time_iso_8601": "2022-04-21T05:19:23.477035Z",
      "url": "https://files.pythonhosted.org/packages/1d/51/e81fcc2e26ac05fc6f1c664198f5605da7c544803488c606b7a7fa6acf69/torchal-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "daa6c96527a009b14ae3c1bbeed3f6fbf3930e5ee5f2b62cfbbee2970e40fa76",
        "md5": "109db716f9501e5722b4ad833b9fd286",
        "sha256": "a4f30be2d5797bbb4610e34fe6f12e37966460befaa82daa4cf1e7df66418970"
      },
      "downloads": -1,
      "filename": "torchal-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "109db716f9501e5722b4ad833b9fd286",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 96156,
      "upload_time": "2022-04-21T05:19:26",
      "upload_time_iso_8601": "2022-04-21T05:19:26.037759Z",
      "url": "https://files.pythonhosted.org/packages/da/a6/c96527a009b14ae3c1bbeed3f6fbf3930e5ee5f2b62cfbbee2970e40fa76/torchal-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}