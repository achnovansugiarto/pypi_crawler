{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<br />\n<p align=\"center\">\n  <h1 align=\"center\">Multilingual-CLIP</h1>\n  <h3 align=\"center\">OpenAI CLIP text encoders for any language</h3>\n\n  <p align=\"center\">  \n    <a href=\"https://rom1504.github.io/clip-retrieval/?back=https%3A%2F%2Fknn5.laion.ai&index=laion_400m&useMclip=true\">Live Demo</a>\n    ·\n    <a href=\"https://huggingface.co/M-CLIP\">Pre-trained Models</a>\n    ·\n    <a href=\"https://github.com/FreddeFrallan/Contrastive-Tension/issues\">Report Bug</a>\n  </p>\n</p>\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FreddeFrallan/Multilingual-CLIP/blob/master/Multilingual_CLIP.ipynb)\n[![pypi](https://img.shields.io/pypi/v/multilingual-clip.svg)](https://pypi.python.org/pypi/multilingual-clip)\n\n\n<!-- ABOUT THE PROJECT -->\n## Overview\n![Alt text](Images/Multilingual-CLIP.png?raw=true \"Title\")\n\n[OpenAI](https://openai.com/) recently released the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) in which they present the CLIP (Contrastive Language–Image Pre-training) model. This model is trained to connect text and images, by matching their corresponding vector representations using a contrastive learning objective.\nCLIP consists of two separate models, a visual encoder and a text encoder. These were trained on a wooping 400 Million images and corresponding captions. \nOpenAI has since released a set of their smaller CLIP models, which can be found on the [official CLIP Github](https://github.com/openai/CLIP).\n\n## Demo\nA live demonstration of multilingual Text-Image retrieval using M-CLIP can be found [here!](https://rom1504.github.io/clip-retrieval/?back=https%3A%2F%2Fknn5.laion.ai&index=laion_400m&useMclip=true) This demo was created by [Rom1504](https://github.com/rom1504), and it allows you to search the LAION-400M dataset in various languages using M-CLIP.\n\n#### This repository contains\n* Pre-trained CLIP-Text encoders for multiple languages\n* Pytorch & Tensorflow inference code\n* Tensorflow training code\n\n### Requirements\nWhile it is possible that other versions works equally fine, we have worked with the following:\n\n* Python = 3.6.9\n* Transformers = 4.8.1\n\n## Install\n\n`pip install multilingual-clip torch`\n\nYou can also choose to `pip install tensorflow` instead of torch.\n\n\n## Inference Usage\n\nInference code for Tensorflow is also available in [inference_example.py](https://github.com/FreddeFrallan/Multilingual-CLIP/blob/main/inference_example.py)\n\n```python\nfrom multilingual_clip import pt_multilingual_clip\nimport transformers\n\ntexts = [\n    'Three blind horses listening to Mozart.',\n    'Älgen är skogens konung!',\n    'Wie leben Eisbären in der Antarktis?',\n    'Вы знали, что все белые медведи левши?'\n]\nmodel_name = 'M-CLIP/XLM-Roberta-Large-Vit-L-14'\n\n# Load Model & Tokenizer\nmodel = pt_multilingual_clip.MultilingualCLIP.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\nembeddings = model.forward(texts, tokenizer)\nprint(embeddings.shape)\n```\n\n## Install for development\n\nSetup a virtualenv:\n\n```\npython3 -m venv .env\nsource .env/bin/activate\npip install -e .\n```\n\n## Pre-trained Models\nEvery text encoder is a [Huggingface](https://huggingface.co/) available transformer, with an additional linear layer on top. For more information of a specific model, click the Model Name to see its model card.\n<br>\n<br>\n\n| Name |Model Base|Vision Model | Vision Dimensions | Pre-trained Languages | #Parameters|\n| ----------------------------------|:-----: |:-----: |:-----: |:-----: | :-----: |\n| [LABSE Vit-L/14](https://huggingface.co/M-CLIP/LABSE-Vit-L-14)| [LaBSE](https://huggingface.co/sentence-transformers/LaBSE)|  [OpenAI ViT-L/14](https://github.com/openai/CLIP) | 768 | [109 Languages](https://arxiv.org/pdf/2007.01852.pdf) | 110 M|\n| [XLM-R Large Vit-B/32](https://huggingface.co/M-CLIP/XLM-Roberta-Large-Vit-B-32)| [XLM-Roberta-Large](https://huggingface.co/xlm-roberta-large)|  [OpenAI ViT-B/32](https://github.com/openai/CLIP) | 512 | [100 Languages](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr#Introduction) | 344 M|\n| [XLM-R Large Vit-L/14](https://huggingface.co/M-CLIP/XLM-Roberta-Large-Vit-L-14)| [XLM-Roberta-Large](https://huggingface.co/xlm-roberta-large)|  [OpenAI ViT-L/14](https://github.com/openai/CLIP) | 768 | [100 Languages](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr#Introduction)|  344 M|\n| [XLM-R Large Vit-B/16+](https://huggingface.co/M-CLIP/XLM-Roberta-Large-Vit-B-16Plus)| [XLM-Roberta-Large](https://huggingface.co/xlm-roberta-large)|  [Open CLIP ViT-B-16-plus-240](https://github.com/mlfoundations/open_clip) | 640 | [100 Languages](https://github.com/facebookresearch/fairseq/tree/main/examples/xlmr#Introduction)| 344 M|\n\n### Validation & Training Curves\nFollowing is a table of the <b>Txt2Img @10-Recal</b> for the humanly tanslated [MS-COCO testset](https://arxiv.org/abs/2109.07622).\n\n| Name | En | De | Es | Fr | Zh | It | Pl | Ko | Ru | Tr | Jp |\n| ----------------------------------|:-----: |:-----: |:-----: |:-----: | :-----: |:-----: |:-----: |:-----: |:-----: |:-----: |:-----: |\n| [OpenAI CLIP Vit-B/32](https://github.com/openai/CLIP)| 90.3 | - | - | - | - | - | - | - | - | - | - |\n| [OpenAI CLIP Vit-L/14](https://github.com/openai/CLIP)| 91.8 | - | - | - | - | - | - | - | - | - | - |\n| [OpenCLIP ViT-B-16+-](https://github.com/openai/CLIP)| 94.3 | - | - | - | - | - | - | - | - | - | - |\n| [LABSE Vit-L/14](https://huggingface.co/M-CLIP/LABSE-Vit-L-14)| 91.6 | 89.6 | 89.5 | 89.9 | 88.9 | 90.1 | 89.8 | 80.8 | 85.5 | 89.8 | 73.9 |\n| [XLM-R Large Vit-B/32](https://huggingface.co/M-CLIP/XLM-Roberta-Large-Vit-B-32)| 91.8 | 88.7 | 89.1 | 89.4 | 89.3 | 89.8| 91.4 | 82.1 | 86.1 | 88.8 | 81.0 |\n| [XLM-R Vit-L/14](https://huggingface.co/M-CLIP/XLM-Roberta-Large-Vit-L-14)| 92.4 | 90.6 | 91.0 | 90.0 | 89.7 | 91.1 | 91.3 | 85.2 | 85.8 | 90.3 | 81.9 |\n| [XLM-R Large Vit-B/16+](https://huggingface.co/M-CLIP/XLM-Roberta-Large-Vit-B-16Plus)| <b>95.0</b> | <b>93.0</b> | <b>93.6</b> | <b>93.1</b> | <b>94.0</b> | <b>93.1</b> | <b>94.4</b> | <b>89.0</b> | <b>90.0</b> | <b>93.0</b> | <b>84.2</b> |\n\nThe training curves for these models are available at this [Weights and Biases Report](https://wandb.ai/freddefrallan/M-CLIP/reports/M-CLIP-2-6-2022--VmlldzoyMTE1MjU1/edit?firstReport&runsetFilter), the results for other non-succesfull and ongoing experiments can be found in the [Weights and Biases Project](https://wandb.ai/freddefrallan/M-CLIP?workspace=user-freddefrallan).\n\n## Legacy Usage and Models\nOlder versions of M-CLIP had the linear weights stored separately from Huggingface. Whilst the new models have them directly incorporated in the Huggingface repository. More information about these older models can be found in this section. \n\n<details>\n  <summary>Click for more information</summary>\n\n##### Download CLIP Model\n```bash\n$ conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n$ pip install ftfy regex tqdm\n$ pip install git+https://github.com/openai/CLIP.git\n```\nReplace `cudatoolkit=11.0` above with the appropriate CUDA version on your machine or `cpuonly` when installing on a machine without a GPU.\nFor more information please see the official [CLIP repostitory](https://github.com/openai/CLIP).\n##### Download Linear Weights\n```bash\n# Linear Model Weights\n$ bash legacy_get-weights.sh\n```\n\n### Inference\n```python\nfrom multilingual_clip import multilingual_clip\n\nprint(multilingual_clip.AVAILABLE_MODELS.keys())\n\nmodel = multilingual_clip.load_model('M-BERT-Distil-40')\n\nembeddings = model(['Älgen är skogens konung!', 'Wie leben Eisbären in der Antarktis?', 'Вы знали, что все белые медведи левши?'])\nprint(embeddings.shape)\n# Yields: torch.Size([3, 640])\n```\n\n<!--- For a more elaborative example see this [Google Colab](https://colab.research.google.com/github/FreddeFrallan/Multilingual-CLIP/blob/master/Multilingual_CLIP.ipynb). --->\n\nFor a more elaborate example, comparing the textual embeddings to the CLIP image embeddings see this [colab notebook](https://colab.research.google.com/github/FreddeFrallan/Multilingual-CLIP/blob/master/Multilingual_CLIP.ipynb).\n\n<!-- GETTING STARTED -->\n## Legacy Pre-trained Models\nEvery text encoder is a [Huggingface](https://huggingface.co/) available transformer, with an additional linear layer on top. Neither of the models have been extensively tested, but for more information and qualitative test results for a specific model, click the Model Name to see its model card.\n<br>\n<br>\n<b>*** Make sure to update to the most recent version of the repostitory when downloading a new model, and re-run the shell script to download the Linear Weights. *** </b>\n\n\n| Name |Model Base|Vision Model | Pre-trained Languages | Target Languages | #Parameters|\n| ----------------------------------|:-----: |:-----: |:-----: |:-----: |:-----: |\n|**Multilingual**    ||\n| [M-BERT Distil 40](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/M-BERT%20Distil%2040) | [M-BERT Distil](https://huggingface.co/bert-base-multilingual-uncased)|  RN50x4 | [101 Languages](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages) | [40 Languages](https://github.com/FreddeFrallan/Multilingual-CLIP/blob/main/Model%20Cards/M-BERT%20Distil%2040/Fine-Tune-Languages.md) | 66 M|\n| [M-BERT Base 69](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/M-BERT%20Base%2069) | [M-BERT Base](https://huggingface.co/bert-base-multilingual-uncased)|RN50x4 | [101 Languages](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages) | 68 Languages | 110 M|\n| [M-BERT Base ViT-B](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/M-BERT%20Base%20ViT-B) | [M-BERT Base](https://huggingface.co/bert-base-multilingual-uncased)|ViT-B/32 | [101 Languages](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages) | 68 Languages | 110 M|\n|**Monolingual**    ||\n|[Swe-CLIP 500k](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/Swe-CLIP%20500k)| [KB-BERT](https://huggingface.co/KB/bert-base-swedish-cased)|  RN50x4 | Swedish | Swedish | 110 M|\n|[Swe-CLIP 2M](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/Model%20Cards/Swe-CLIP%202M)| [KB-BERT](https://huggingface.co/KB/bert-base-swedish-cased)|  RN50x4 | Swedish | Swedish | 110 M|\n\n  </details>\n\n## Training a new model\n[This folder](https://github.com/FreddeFrallan/Multilingual-CLIP/tree/main/multilingual_clip/TeacherLearning) contains the code used for training the above models. If you wsh to train your own model you must do the following things:\n\n* Prepare a set of translated sentence pairs from English -> Your Language(s)\n* Compute regular CLIP-Text embeddings for the English sentences.\n* Edit [Training.py](https://github.com/FreddeFrallan/Multilingual-CLIP/blob/main/multilingual_clip/TeacherLearning/Training.py) to load your data.\n* Train a new CLIP-Text encoder via Teacher Learning \n\n### Pre-computed CLIP Embeddings & Translaton Data\n[This Google Drive folder](https://drive.google.com/drive/folders/1I9a7naSZubUATWzLFv61DQMWyFlF7wR5?usp=sharing) contains both pre-computed CLIP-Text Embeddings for a large porton of the the image captions of [GCC](https://ai.google.com/research/ConceptualCaptions/) + [MSCOCO](https://cocodataset.org/#home) + [VizWiz](https://vizwiz.org/tasks-and-datasets/image-captioning/).\n\nThe Google Drive folder also contains the translation data used to train the currently available models.\nGood Luck\n\n## Contribution\nIf you have trained a CLIP Text encoder specific to your language, or another model covering a language not supported here, Please feel free to contact us and we will either upload your model and credit you, or simply link to your already uploaded model.\n\n<!-- CONTACT -->\n## Contact\nIf you have questions regarding the code or otherwise related to this Github page, please open an [issue](https://github.com/FreddeFrallan/Contrastive-Tension/issues).\n\nFor other purposes, feel free to contact me directly at: Fredrik.Carlsson@ri.se\n\n<!-- ACKNOWLEDGEMENTS -->\n## Acknowledgements\n* [Stability.ai](https://stability.ai/) for providing much appreciated compute during training.\n* [CLIP](https://openai.com/blog/clip/)\n* [OpenAI](https://openai.com/)\n* [Huggingface](https://huggingface.co/)\n* [Best Readme Template](https://github.com/othneildrew/Best-README-Template)\n* [\"Two Cats\" Image by pl1602](https://search.creativecommons.org/photos/8dfd802b-58e5-4cc5-889d-96abba540de1)\n\n<!-- LICENSE -->\n## License\nDistributed under the MIT License. See `LICENSE` for more information.\n\n<!-- CITATION -->\n## Citing\nIf you found this repository useful, please consider citing:\n\n```bibtex\n@InProceedings{carlsson-EtAl:2022:LREC,\n  author    = {Carlsson, Fredrik  and  Eisen, Philipp  and  Rekathati, Faton  and  Sahlgren, Magnus},\n  title     = {Cross-lingual and Multilingual CLIP},\n  booktitle      = {Proceedings of the Language Resources and Evaluation Conference},\n  month          = {June},\n  year           = {2022},\n  address        = {Marseille, France},\n  publisher      = {European Language Resources Association},\n  pages     = {6848--6854},\n  abstract  = {The long-standing endeavor of relating the textual and the visual domain recently underwent a pivotal breakthrough, as OpenAI released CLIP. This model distinguishes how well an English text corresponds with a given image with unprecedented accuracy. Trained via a contrastive learning objective over a huge dataset of 400M of images and captions, it is a work that is not easily replicated, especially for low resource languages. Capitalizing on the modularization of the CLIP architecture, we propose to use cross-lingual teacher learning to re-train the textual encoder for various non-English languages. Our method requires no image data and relies entirely on machine translation which removes the need for data in the target language. We find that our method can efficiently train a new textual encoder with relatively low computational cost, whilst still outperforming previous baselines on multilingual image-text retrieval.},\n  url       = {https://aclanthology.org/2022.lrec-1.739}\n}\n```\n\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n[contributors-shield]: https://img.shields.io/github/contributors/othneildrew/Best-README-Template.svg?style=for-the-badge\n[contributors-url]: https://github.com/othneildrew/Best-README-Template/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/othneildrew/Best-README-Template.svg?style=for-the-badge\n[forks-url]: https://github.com/othneildrew/Best-README-Template/network/members\n[stars-shield]: https://img.shields.io/github/stars/othneildrew/Best-README-Template.svg?style=for-the-badge\n[stars-url]: https://github.com/othneildrew/Best-README-Template/stargazers\n[issues-shield]: https://img.shields.io/github/issues/othneildrew/Best-README-Template.svg?style=for-the-badge\n[issues-url]: https://github.com/othneildrew/Best-README-Template/issues\n[license-shield]: https://img.shields.io/github/license/othneildrew/Best-README-Template.svg?style=for-the-badge\n[license-url]: https://github.com/othneildrew/Best-README-Template/blob/master/LICENSE.txt\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/othneildrew\n[product-screenshot]: images/screenshot.png\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/nateagr/mclip",
    "keywords": "machine learning",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "mclip-anytorch",
    "package_url": "https://pypi.org/project/mclip-anytorch/",
    "platform": null,
    "project_url": "https://pypi.org/project/mclip-anytorch/",
    "project_urls": {
      "Homepage": "https://github.com/nateagr/mclip"
    },
    "release_url": "https://pypi.org/project/mclip-anytorch/1.0.13/",
    "requires_dist": [
      "transformers"
    ],
    "requires_python": "",
    "summary": "OpenAI CLIP text encoders for multiple languages!",
    "version": "1.0.13",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14972327,
  "releases": {
    "1.0.13": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c232e47d088846db0fcbb006f52fb1ee71aa68f1ea148491a4f9ae3470213da6",
          "md5": "dd437af4816edd2f3603a6a15ce49a21",
          "sha256": "25e600bf199753c595d468b510a3e6f6e644f22e28e589c7d3c9ef4848413137"
        },
        "downloads": -1,
        "filename": "mclip_anytorch-1.0.13-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dd437af4816edd2f3603a6a15ce49a21",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 22458,
        "upload_time": "2022-09-02T09:09:08",
        "upload_time_iso_8601": "2022-09-02T09:09:08.088277Z",
        "url": "https://files.pythonhosted.org/packages/c2/32/e47d088846db0fcbb006f52fb1ee71aa68f1ea148491a4f9ae3470213da6/mclip_anytorch-1.0.13-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d111eb55dbdeaaa172d2644f62f4e0bb83cd99dbe3a3a231aff0c58e9fa62a43",
          "md5": "8e7fa8b38d839b09c189e46a7c92db2b",
          "sha256": "233931055030829da1f425921c2a2641abe18747c340c7e61e98f4e4ee69d28b"
        },
        "downloads": -1,
        "filename": "mclip_anytorch-1.0.13.tar.gz",
        "has_sig": false,
        "md5_digest": "8e7fa8b38d839b09c189e46a7c92db2b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 15066,
        "upload_time": "2022-09-02T09:09:10",
        "upload_time_iso_8601": "2022-09-02T09:09:10.035562Z",
        "url": "https://files.pythonhosted.org/packages/d1/11/eb55dbdeaaa172d2644f62f4e0bb83cd99dbe3a3a231aff0c58e9fa62a43/mclip_anytorch-1.0.13.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c232e47d088846db0fcbb006f52fb1ee71aa68f1ea148491a4f9ae3470213da6",
        "md5": "dd437af4816edd2f3603a6a15ce49a21",
        "sha256": "25e600bf199753c595d468b510a3e6f6e644f22e28e589c7d3c9ef4848413137"
      },
      "downloads": -1,
      "filename": "mclip_anytorch-1.0.13-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "dd437af4816edd2f3603a6a15ce49a21",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 22458,
      "upload_time": "2022-09-02T09:09:08",
      "upload_time_iso_8601": "2022-09-02T09:09:08.088277Z",
      "url": "https://files.pythonhosted.org/packages/c2/32/e47d088846db0fcbb006f52fb1ee71aa68f1ea148491a4f9ae3470213da6/mclip_anytorch-1.0.13-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d111eb55dbdeaaa172d2644f62f4e0bb83cd99dbe3a3a231aff0c58e9fa62a43",
        "md5": "8e7fa8b38d839b09c189e46a7c92db2b",
        "sha256": "233931055030829da1f425921c2a2641abe18747c340c7e61e98f4e4ee69d28b"
      },
      "downloads": -1,
      "filename": "mclip_anytorch-1.0.13.tar.gz",
      "has_sig": false,
      "md5_digest": "8e7fa8b38d839b09c189e46a7c92db2b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 15066,
      "upload_time": "2022-09-02T09:09:10",
      "upload_time_iso_8601": "2022-09-02T09:09:10.035562Z",
      "url": "https://files.pythonhosted.org/packages/d1/11/eb55dbdeaaa172d2644f62f4e0bb83cd99dbe3a3a231aff0c58e9fa62a43/mclip_anytorch-1.0.13.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}