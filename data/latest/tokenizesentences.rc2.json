{
  "info": {
    "author": "Carlos A. Planch√≥n",
    "author_email": "bubbledoloresuruguay2@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Topic :: Software Development :: Build Tools"
    ],
    "description": "# tokenizesentences\n*Python3 module to tokenize english sentences.*\nBased on the answer of D Greenberg in StackOverflow:\nhttps://stackoverflow.com/questions/4576077/python-split-text-on-sentences\n\n## Installation\n### Install with pip\n```\npip3 install -U tokenizesentences\n```\n\n## Usage\n```\nIn [1]: import tokenizesentences\n\nIn [2]: m = tokenizesentences.SplitIntoSentences()\n\nIn [3]: m.split_into_sentences(\n    \"Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer. He also worked at craigslist.org as a business analyst.\"\n    )\n\nOut[3]: \n[\n    'Mr. John Johnson Jr. was born in the U.S.A but earned his Ph.D. in Israel before joining Nike Inc. as an engineer.',\n    'He also worked at craigslist.org as a business analyst.'\n]\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/carlosplanchon/tokenizesentences/archive/v0.2.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/carlosplanchon/tokenizesentences",
    "keywords": "tokenize,english,sentences",
    "license": "GPL3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tokenizesentences",
    "package_url": "https://pypi.org/project/tokenizesentences/",
    "platform": "",
    "project_url": "https://pypi.org/project/tokenizesentences/",
    "project_urls": {
      "Download": "https://github.com/carlosplanchon/tokenizesentences/archive/v0.2.tar.gz",
      "Homepage": "https://github.com/carlosplanchon/tokenizesentences"
    },
    "release_url": "https://pypi.org/project/tokenizesentences/0.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Python3 module to tokenize english sentences.",
    "version": "0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 5180288,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "635b61d586db39da9c12545c4c0a64a4370d306d40ae38fd9d895e0f759aede5",
          "md5": "36de88c1a2c1c2267eb4d80b10c1b5a6",
          "sha256": "7484a3068e86a9736fffad1dd212d241385aa4fe72139a88f369e277ebc6a8b6"
        },
        "downloads": -1,
        "filename": "tokenizesentences-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "36de88c1a2c1c2267eb4d80b10c1b5a6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 2408,
        "upload_time": "2019-04-22T19:52:01",
        "upload_time_iso_8601": "2019-04-22T19:52:01.483990Z",
        "url": "https://files.pythonhosted.org/packages/63/5b/61d586db39da9c12545c4c0a64a4370d306d40ae38fd9d895e0f759aede5/tokenizesentences-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "60779464717d1efd6441bf0d4978506cb164c71e5d0e13bd0e3d1f36bb50c6fc",
          "md5": "b5c7a6e82db5fa034d5ec02d90a832dc",
          "sha256": "a7863c7244782825a2f48145c634fea633806b703a6b9d3c78ed095bf88a5742"
        },
        "downloads": -1,
        "filename": "tokenizesentences-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "b5c7a6e82db5fa034d5ec02d90a832dc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 2438,
        "upload_time": "2019-04-24T04:38:48",
        "upload_time_iso_8601": "2019-04-24T04:38:48.508143Z",
        "url": "https://files.pythonhosted.org/packages/60/77/9464717d1efd6441bf0d4978506cb164c71e5d0e13bd0e3d1f36bb50c6fc/tokenizesentences-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "60779464717d1efd6441bf0d4978506cb164c71e5d0e13bd0e3d1f36bb50c6fc",
        "md5": "b5c7a6e82db5fa034d5ec02d90a832dc",
        "sha256": "a7863c7244782825a2f48145c634fea633806b703a6b9d3c78ed095bf88a5742"
      },
      "downloads": -1,
      "filename": "tokenizesentences-0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "b5c7a6e82db5fa034d5ec02d90a832dc",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 2438,
      "upload_time": "2019-04-24T04:38:48",
      "upload_time_iso_8601": "2019-04-24T04:38:48.508143Z",
      "url": "https://files.pythonhosted.org/packages/60/77/9464717d1efd6441bf0d4978506cb164c71e5d0e13bd0e3d1f36bb50c6fc/tokenizesentences-0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}