{
  "info": {
    "author": "Wilbert Berendsen",
    "author_email": "info@wilbertberendsen.nl",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)",
      "Programming Language :: Python :: 3.3",
      "Topic :: Text Processing"
    ],
    "description": "The livelex Python module\n=========================\n\nThis module parses text into tokens, and is able to reparse only modified parts\nof the text, using the earlier generated tokens. Tokenized text lives in a tree\nstructure with powerful quering methods for finding tokens and contexts.\n\nThe livelex module is designed to be fast, and can tokenize in a background\nthread, so that even when using very large documents, GUI applications that\nneed to be responsive do not grind to a halt.\n\nMain use case: syntax highlighting in text editors, but also understanding the\nmeaning of text to be able to provided context sensitive editing features.\n\nThe livelex module is written and maintained by Wilbert Berendsen.\n\nHomepage: https://github.com/wbsoft/livelex\nDownload: https://pypi.org/project/livelex/\n\nThe module is designed to parse text using rules, which are regular-expression\nbased. Rules are grouped into lexicons, and lexicons are grouped into a\nLanguage object. Every lexicon has its own set of rules that describe the text\nthat is expected in that context.\n\nA rule consists of three parts: a pattern, an action and a target.\n\n* The pattern is a either a regular expression string, or an object that\n  inherits Pattern. In that case its build() method is called to get the\n  pattern. If the pattern matches, a match object is created. If not,\n  the next rule is tried.\n\n* The action can be any object, and is streamed together with the matched part\n  of the text. It can be seen as a token. If the action is an instance of\n  DynamicAction, its filter_actions() method is called, which can yield zero or\n  more tokens.  The special `skip` action skips the matching text.\n\n* The target is a list of objects, which can be integer numbers or references\n  to a different lexicon. A positive number pushes the same lexicon on the\n  stack, while a negative number pops the current lexicon(s) off the stack, so\n  that lexing the text continues with a previous lexicon. It is also possible\n  to pop a lexicon and push a different one.\n\n  Instead of a list of objects, a DynamicTarget object can also be used, which\n  can change the target based on the match object.\n\nUsing a special rule, a lexicon may specify a default action, which is\nstreamed with text that is not recognized by any other rule in the lexicon.\nA lexicon may also specify a default target, which is chosen when no rule\nmatches the current text.\n\n\nParsing\n-------\n\nParsing (better: lexing) text always starts in a lexicon, which is called the\nroot lexicon. The rules in that lexicon are tried one by one. As soon as there\nis a match, a Token is generated with the matching text, the position of the\ntext and the action that was specified in the rule. And if a target was\nspecified, parsing continues in a different lexicon.\n\nThe tokens are put in a tree structure. Every active lexicon creates a Context\nlist that holds the tokens and child contexts. If a target pops back to a\nprevious lexicon, the previous context becomes the current one again.\n\nAll tokens and contexts point to their parents, so it is possible to manipulate\nand query the tree structure in various ways.\n\nThe structure of the tree is built by the TreeBuilder, see the `tree` and the\n`treebuilder` module. At the root is the Context carrying the root lexicon.\nThe root context contains Tokens and/or other Contexts.\n\nThe TreeBuilder is capable of tokenizing the text in a background thread and\nalso to rebuild just a changed part of the text, smartly reusing earlier\ngenerated tokens if possible.\n\n\nIterating and Querying\n----------------------\n\nBoth Token and Context have many methods for iterating over the tree, for\ngetting at the parent, child or sibling nodes. Context has various find()\nmethods to quickly find a token or context at a certain position in the text.\n\nUsing the Context.query property you can build XPath-like chains of filtering\nqueries to quickly find tokens or contexts based on text, action or lexicon.\nThis is described in the `query` module.\n\n\nExample\n-------\n\nHere are some examples of how to create a Language class and then use it:\n\n.. code:: python\n\n    import livelex\n\n    from livelex import (\n        Language, lexicon,\n        words, bygroup, bymatch, bytext,\n        default_action,\n        default_target,\n        skip,\n        tomatch,\n    )\n\n    class MyLang(Language):\n        \"\"\"A Language represents a set of Lexicons comprising a specific language.\n\n        A Language is never instantiated. The class itself serves as a namespace\n        and can be inherited from.\n\n\n\n        \"\"\"\n\n        @lexicon(re_flags=0)\n        def root(cls):\n            yield r'\"', \"string\", cls.string\n            yield r'\\(', \"paren\", cls.parenthesized\n            yield r'\\d+', \"number\"\n            yield r'%', \"comment\", cls.comment\n            yield r'[,.!?]', \"punctuation\"\n            yield r'\\w+', \"word\"\n\n        @lexicon\n        def string(cls):\n            yield r'\\\\[\\\\\"]', 'string escape'\n            yield r'\"', \"string\", -1\n            yield default_action, \"string\"\n\n        @lexicon(re_flags=re.MULTILINE)\n        def comment(cls):\n            yield r'$', \"comment\", -1\n            yield r'XXX|TODO', \"todo\"\n            yield default_action, \"comment\"\n\n        @lexicon\n        def parenthesized(cls):\n            yield r'\\)', \"paren\", -1\n            yield from cls.root()\n\n\n    s = r\"\"\"\n    This is (an example) text with 12 numbers\n    and \"a string with \\\" escaped characters\",\n    and a % comment that TODO lasts until the end\n    of the line.\n    \"\"\"\n\n\n    >>> import livelex\n    >>> tree = livelex.root(MyLang.root, s)\n    >>> tree.dump()\n    <Context MyLang.root at 1-144 (20 children)>\n     ├╴<Token 'This' at 1 (word)>\n     ├╴<Token 'is' at 6 (word)>\n     ├╴<Token '(' at 9 (paren)>\n     ├╴<Context MyLang.parenthesized at 10-21 (3 children)>\n     │  ├╴<Token 'an' at 10 (word)>\n     │  ├╴<Token 'example' at 13 (word)>\n     │  ╰╴<Token ')' at 20 (paren)>\n     ├╴<Token 'text' at 22 (word)>\n     ├╴<Token 'with' at 27 (word)>\n     ├╴<Token '12' at 32 (number)>\n     ├╴<Token 'numbers' at 35 (word)>\n     ├╴<Token 'and' at 43 (word)>\n     ├╴<Token '\"' at 47 (string)>\n     ├╴<Context MyLang.string at 48-84 (4 children)>\n     │  ├╴<Token 'a string with ' at 48 (string)>\n     │  ├╴<Token '\\\\\"' at 62 (string escape)>\n     │  ├╴<Token ' escaped characters' at 64 (string)>\n     │  ╰╴<Token '\"' at 83 (string)>\n     ├╴<Token ',' at 84 (punctuation)>\n     ├╴<Token 'and' at 86 (word)>\n     ├╴<Token 'a' at 90 (word)>\n     ├╴<Token '%' at 92 (comment)>\n     ├╴<Context MyLang.comment at 93-131 (3 children)>\n     │  ├╴<Token ' comment that ' at 93 (comment)>\n     │  ├╴<Token 'TODO' at 107 (todo)>\n     │  ╰╴<Token ' lasts until the end' at 111 (comment)>\n     ├╴<Token 'of' at 132 (word)>\n     ├╴<Token 'the' at 135 (word)>\n     ├╴<Token 'line' at 139 (word)>\n     ╰╴<Token '.' at 143 (punctuation)>\n    >>> tree.find_token(50)\n    <Token 'a string with ' at 48 (string)>\n    >>> tree.find_token(50).parent\n    <Context MyLang.string at 48-84 (4 children)>\n\n    >>> d = livelex.Document(MyLang.root, s)\n    >>> d\n    <Document '\\nThis is (an example) text w...'>\n\n    >>> print(d.text())\n\n    This is (an example) text with 12 numbers\n    and \"a string with \\\" escaped characters\",\n    and a % comment that TODO lasts until the end\n    of the line.\n\n    >>> d[50:56]\n    'string'\n    >>> with d:\n    ...  d[9:12] = '(a \"much longer'\n    ...  d[20:20] = '\"'\n    ...\n    >>> print(d.text())\n\n    This is (a \"much longer example\") text with 12 numbers\n    and \"a string with \\\" escaped characters\",\n    and a % comment that TODO lasts until the end\n    of the line.\n\n    >>> d.get_root()[3].dump()\n    <Context MyLang.parenthesized at 10-34 (4 children)>\n     ├╴<Token 'a' at 10 (word)>\n     ├╴<Token '\"' at 12 (string)>\n     ├╴<Context MyLang.string at 13-33 (2 children)>\n     │  ├╴<Token 'much longer example' at 13 (string)>\n     │  ╰╴<Token '\"' at 32 (string)>\n     ╰╴<Token ')' at 33 (paren)>\n\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/wbsoft/livelex",
    "keywords": "",
    "license": "GPL",
    "maintainer": "",
    "maintainer_email": "",
    "name": "livelex",
    "package_url": "https://pypi.org/project/livelex/",
    "platform": "",
    "project_url": "https://pypi.org/project/livelex/",
    "project_urls": {
      "Homepage": "https://github.com/wbsoft/livelex"
    },
    "release_url": "https://pypi.org/project/livelex/0.3.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "The livelex lexer",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6495237,
  "releases": {
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5bf2026003ca8bdba9d5b3f14595d4f86fbfdea980d450c722005351d845aa2d",
          "md5": "42362b5306717ace0e189fffcb7c81fa",
          "sha256": "d7e28196359a3398f215ccb43025d9734c79236681e101b52235751fed4ec549"
        },
        "downloads": -1,
        "filename": "livelex-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "42362b5306717ace0e189fffcb7c81fa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 8827,
        "upload_time": "2019-12-18T22:30:41",
        "upload_time_iso_8601": "2019-12-18T22:30:41.229457Z",
        "url": "https://files.pythonhosted.org/packages/5b/f2/026003ca8bdba9d5b3f14595d4f86fbfdea980d450c722005351d845aa2d/livelex-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a0318aafadc9e4b4828ae7498b5842cc25d369f9db649390952c2dae0a0edcd4",
          "md5": "bfbc26f13cd44fd4ec1b237f66d107d6",
          "sha256": "118ea2de72debe5d1a6de53b5e6d35f350b734dea4f8e46442895d3b512b6877"
        },
        "downloads": -1,
        "filename": "livelex-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "bfbc26f13cd44fd4ec1b237f66d107d6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9787,
        "upload_time": "2019-12-18T23:12:34",
        "upload_time_iso_8601": "2019-12-18T23:12:34.941166Z",
        "url": "https://files.pythonhosted.org/packages/a0/31/8aafadc9e4b4828ae7498b5842cc25d369f9db649390952c2dae0a0edcd4/livelex-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7fa4963c1775b5d8f082db8a2f09b8e904e9309e1aec3856c9030afa98124940",
          "md5": "051dd298841cb953204287a0eca602cb",
          "sha256": "c293877bf1ceeb1cf6bde18589d38b84b300f97cf4a494ad79cd5332a671c596"
        },
        "downloads": -1,
        "filename": "livelex-0.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "051dd298841cb953204287a0eca602cb",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 21602,
        "upload_time": "2020-01-02T08:21:09",
        "upload_time_iso_8601": "2020-01-02T08:21:09.774412Z",
        "url": "https://files.pythonhosted.org/packages/7f/a4/963c1775b5d8f082db8a2f09b8e904e9309e1aec3856c9030afa98124940/livelex-0.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4c2c4dadc842281662c549f74ccd6524d68a6468b23ec765e22300346c2816a1",
          "md5": "03f7ad49ba61a347884692779608e752",
          "sha256": "1a65b0d7c1503ff730da37e4248fc73b4d9b5b78ce248e59afa00fbab8ff6a2e"
        },
        "downloads": -1,
        "filename": "livelex-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "03f7ad49ba61a347884692779608e752",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 30332,
        "upload_time": "2020-01-08T07:17:07",
        "upload_time_iso_8601": "2020-01-08T07:17:07.550794Z",
        "url": "https://files.pythonhosted.org/packages/4c/2c/4dadc842281662c549f74ccd6524d68a6468b23ec765e22300346c2816a1/livelex-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9fa111e7826400743c4c70689f612496802ae43858b90ecb0a237648d495e4ea",
          "md5": "03742d9bc090ed24b545fdaa71f77204",
          "sha256": "300b7703c977351e00a4b2e2b428e1110e1d446716fa0bc4e48142a34a3e6555"
        },
        "downloads": -1,
        "filename": "livelex-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "03742d9bc090ed24b545fdaa71f77204",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 33191,
        "upload_time": "2020-01-15T07:50:23",
        "upload_time_iso_8601": "2020-01-15T07:50:23.996947Z",
        "url": "https://files.pythonhosted.org/packages/9f/a1/11e7826400743c4c70689f612496802ae43858b90ecb0a237648d495e4ea/livelex-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "95849720106a740aa1400b29417a62db715a60e1db19ce8f43a5793bb800f971",
          "md5": "0a77fc0a012c65b947d959f2e3e93530",
          "sha256": "d2a69fd1f6d60e94916f4067a66f92702f14c2ee967ea9a751f25e2a84e854c1"
        },
        "downloads": -1,
        "filename": "livelex-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0a77fc0a012c65b947d959f2e3e93530",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 48478,
        "upload_time": "2020-01-21T17:28:26",
        "upload_time_iso_8601": "2020-01-21T17:28:26.686535Z",
        "url": "https://files.pythonhosted.org/packages/95/84/9720106a740aa1400b29417a62db715a60e1db19ce8f43a5793bb800f971/livelex-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "95849720106a740aa1400b29417a62db715a60e1db19ce8f43a5793bb800f971",
        "md5": "0a77fc0a012c65b947d959f2e3e93530",
        "sha256": "d2a69fd1f6d60e94916f4067a66f92702f14c2ee967ea9a751f25e2a84e854c1"
      },
      "downloads": -1,
      "filename": "livelex-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "0a77fc0a012c65b947d959f2e3e93530",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 48478,
      "upload_time": "2020-01-21T17:28:26",
      "upload_time_iso_8601": "2020-01-21T17:28:26.686535Z",
      "url": "https://files.pythonhosted.org/packages/95/84/9720106a740aa1400b29417a62db715a60e1db19ce8f43a5793bb800f971/livelex-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}