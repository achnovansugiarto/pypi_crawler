{
  "info": {
    "author": "Luke",
    "author_email": "luke.melas@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "# TODO\n## Urgent\n- [x] Rewrite README\n- [x] Implement model\n- [x] Convert pretrained weights\n- [ ] Colab example\n- [ ] `pip`\n\n# ViT PyTorch\n\n### Quickstart\n\nInstall with `pip install vit_pytorch` and load a pretrained ViT with:\n```python\nfrom vit_pytorch import ViT\nmodel = ViT('B_16_imagenet1k', pretrained=True)\n```\n\nOr find a Google Colab example [here]().  \n<!-- TODO: Google Colab example -->\n\n### Overview\nThis repository contains an op-for-op PyTorch reimplementation of the [Visual Transformer](https://openreview.net/forum?id=YicbFdNTTy) architecture from [Google](https://github.com/google-research/vision_transformer), along with pre-trained models and examples.\n\nThe goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. \n\nAt the moment, you can easily:\n * Load pretrained ViT models\n * Evaluate on ImageNet or your own data\n\n_(Upcoming features)_ Coming soon: \n * Finetune ViT on your own dataset\n * Train ViT from scratch on ImageNet (1K)\n * Export to ONNX for efficient inference\n\n### Table of contents\n1. [About ViT](#about-vit)\n2. [About ViT-PyTorch](#about-vit-pytorch)\n3. [Installation](#installation)\n4. [Usage](#usage)\n    * [Load pretrained models](#loading-pretrained-models)\n    * [Example: Classify](#example-classification)\n    <!-- * [Example: Extract features](#example-feature-extraction) -->\n    <!-- * [Example: Export to ONNX](#example-export) -->\n6. [Contributing](#contributing)\n\n### About ViT\n\nVisual Transformers (ViT) are a straightforward application of the [transformer architecture](https://arxiv.org/abs/1706.03762) to image classification. Even in computer vision, it seems, attention is all you need. \n\nThe ViT architecture works as follows: (1) it considers an image as a 1-dimensional sequence of patches, (2) it prepends a classification token to the sequence, (3) it passes these patches through a transformer encoder (like [BERT](https://arxiv.org/abs/1810.04805)), (4) it passes the first token of the output of the transformer through a small MLP to obtain the classification logits. \nViT is trained on a large-scale dataset (ImageNet-21k) with a huge amount of compute. \n\n<div style=\"text-align: center; padding: 10px\">\n    <img src=\"https://raw.githubusercontent.com/google-research/vision_transformer/master/figure1.png\" width=\"100%\" style=\"max-width: 300px; margin: auto\"/>\n</div>\n\n\n### About ViT-PyTorch\n\nViT-PyTorch is a PyTorch re-implementation of EfficientNet. It is consistent with the [original Jax implementation](https://github.com/google-research/vision_transformer), so that it's easy to load Jax-pretrained weights.\n\nAt the same time, we aim to make our PyTorch implementation as simple, flexible, and extensible as possible.\n\n### Installation\n\nInstall with pip:\n```bash\npip install vit_pytorch\n```\n\nOr from source:\n```bash\ngit clone https://github.com/lukemelas/ViT-PyTorch\ncd ViT-Pytorch\npip install -e .\n```\n\n### Usage\n\n#### Loading pretrained models\n\nLoading a pretrained model is easy:\n```python\nfrom vit_pytorch import ViT\nmodel = ViT('B_16_imagenet1k', pretrained=True)\n```\n\nDetails about the models are below:\n\n|    *Name*         |* Pretrained on *|*Finetuned on*|*Available? *|\n|:-----------------:|:---------------:|:------------:|:-----------:|\n| `B_16`            |  ImageNet-21k   | -            |      ✓      |\n| `B_16_imagenet1k` |  ImageNet-21k   | ImageNet-1k  |      ✓      |\n| `B_32_imagenet1k` |  ImageNet-21k   | ImageNet-1k  |      ✓      |\n| `L_16_imagenet1k` |  ImageNet-21k   | ImageNet-1k  |      ✓      |\n| `L_32_imagenet1k` |  ImageNet-21k   | ImageNet-1k  |      ✓      |\n\n#### Custom ViT\n\nLoading custom configurations is just as easy: \n```python\nfrom vit_pytorch import ViT\n# The following is equivalent to ViT('B_16')\nconfig = dict(hidden_size=512, num_heads=8, num_layers=6)\nmodel = ViT.from_config(config)\n```\n\n#### Example: Classification\n\nBelow is a simple, complete example. It may also be found as a Jupyter notebook in `examples/simple` or as a [Colab Notebook]().  \n<!-- TODO: new Colab -->\n\n```python\nimport json\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\n\n# Load ViT\nfrom vit_pytorch import ViT\nmodel = ViT('B_16_imagenet1k', pretrained=True)\nmodel.eval()\n\n# Load image\n# NOTE: Assumes an image `img.jpg` exists in the current directory\nimg = transforms.Compose([\n    transforms.Resize((384, 384)), \n    transforms.ToTensor(),\n    transforms.Normalize(0.5, 0.5),\n])(Image.open('img.jpg')).unsqueeze(0)\nprint(img.shape) # torch.Size([1, 3, 384, 384])\n\n# Classify\nwith torch.no_grad():\n    outputs = model(img)\nprint(outputs.shape)  # (1, 1000)\n```\n\n<!-- #### Example: Feature Extraction\n\nYou can easily extract features with `model.extract_features`:\n```python\nfrom efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_pretrained('efficientnet-b0')\n\n# ... image preprocessing as in the classification example ...\nprint(img.shape) # torch.Size([1, 3, 384, 384])\n\nfeatures = model.extract_features(img)\nprint(features.shape) # torch.Size([1, 1280, 7, 7])\n``` -->\n\n<!-- #### Example: Export to ONNX\n\nExporting to ONNX for deploying to production is now simple:\n```python\nimport torch\nfrom efficientnet_pytorch import EfficientNet\n\nmodel = EfficientNet.from_pretrained('efficientnet-b1')\ndummy_input = torch.randn(10, 3, 240, 240)\n\nmodel.set_swish(memory_efficient=False)\ntorch.onnx.export(model, dummy_input, \"test-b1.onnx\", verbose=True)\n```\n\n[Here](https://colab.research.google.com/drive/1rOAEXeXHaA8uo3aG2YcFDHItlRJMV0VP) is a Colab example. -->\n\n\n#### ImageNet\n\nSee `examples/imagenet` for details about evaluating on ImageNet.\n\n#### Credit\n\nOther great repositories with this model include: \n - [Ross Wightman's repo](https://github.com/rwightman/pytorch-image-models)\n - [Phil Wang's repo](https://github.com/lucidrains/vit-pytorch)\n\n### Contributing\n\nIf you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.\n\nI look forward to seeing what the community does with these models!",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/lukemelas/ViT-PyTorch",
    "keywords": "",
    "license": "Apache",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pretrained-vit-pytorch",
    "package_url": "https://pypi.org/project/pretrained-vit-pytorch/",
    "platform": "",
    "project_url": "https://pypi.org/project/pretrained-vit-pytorch/",
    "project_urls": {
      "Homepage": "https://github.com/lukemelas/ViT-PyTorch"
    },
    "release_url": "https://pypi.org/project/pretrained-vit-pytorch/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.5.0",
    "summary": "Visual Transformers (ViT) in PyTorch.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8543121,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f076b7e9c9914ca39ba8fa4e06d813963bbad9e32608404a69d0886d3197adb0",
          "md5": "5d4d1f16dfb6769e2efc7620ea84c2ef",
          "sha256": "46aa24cd6877d1237e388ed9000ea5e882cae820e1243c0981d2c6c2834a9a1f"
        },
        "downloads": -1,
        "filename": "pretrained-vit-pytorch-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "5d4d1f16dfb6769e2efc7620ea84c2ef",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.5.0",
        "size": 9966,
        "upload_time": "2020-10-30T15:50:54",
        "upload_time_iso_8601": "2020-10-30T15:50:54.459425Z",
        "url": "https://files.pythonhosted.org/packages/f0/76/b7e9c9914ca39ba8fa4e06d813963bbad9e32608404a69d0886d3197adb0/pretrained-vit-pytorch-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f076b7e9c9914ca39ba8fa4e06d813963bbad9e32608404a69d0886d3197adb0",
        "md5": "5d4d1f16dfb6769e2efc7620ea84c2ef",
        "sha256": "46aa24cd6877d1237e388ed9000ea5e882cae820e1243c0981d2c6c2834a9a1f"
      },
      "downloads": -1,
      "filename": "pretrained-vit-pytorch-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "5d4d1f16dfb6769e2efc7620ea84c2ef",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.5.0",
      "size": 9966,
      "upload_time": "2020-10-30T15:50:54",
      "upload_time_iso_8601": "2020-10-30T15:50:54.459425Z",
      "url": "https://files.pythonhosted.org/packages/f0/76/b7e9c9914ca39ba8fa4e06d813963bbad9e32608404a69d0886d3197adb0/pretrained-vit-pytorch-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}