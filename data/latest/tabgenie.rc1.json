{
  "info": {
    "author": "Zdenek Kasner, Ekaterina Garanina, Ondrej Dusek",
    "author_email": "kasner@ufal.mff.cuni.cz",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development :: Libraries :: Python Modules",
      "Typing :: Typed"
    ],
    "description": "# üßû TabGenie: A Toolkit for Table-to-Text Generation \n\n**Demo üëâÔ∏è https://quest.ms.mff.cuni.cz/rel2text/tabgenie**\n\nTabGenie provides tools for working with data-to-text generation datasets in a unified tabular format. \n\n\nTabGenie allows you to:\n  - **explore** the content of the datasets\n  - **interact** with table-to-text generation models \n  - **load and preprocess** the datasets in a unified format\n  - **prepare spreadsheets** for error analysis\n  - **export tables** to various file formats\n\nTabGenie is equipped with user-friendly web interface, Python bindings and command-line processing tools.\n\n\n ### Frontend Preview\n![](https://raw.githubusercontent.com/kasnerz/tabgenie/main/img/preview.png)\n\n\n## Quickstart\n```\npip install tabgenie\ntabgenie run --host=127.0.0.1 --port 8890\nxdg-open http://127.0.0.1:8890\n```\n\nOr try the demo at:\n\n**üëâÔ∏è https://quest.ms.mff.cuni.cz/rel2text/tabgenie**\n\n\n## Datasets\n\nThe datasets are loaded from the [HuggingFace datasets](https://huggingface.co/datasets).\n\nInput data in each dataset is preprocessed into a tabular format:\n- each table contains M rows and N columns,\n- cells may span multiple columns or rows,\n- cells may be marked as headings (indicated by bold font),\n- cells may be highlighted (indicated by yellow background).\n\nAdditionally, each example may contain metadata (such as title, url, etc.) which are displayed next to the main table as *properties*.\n\n| Dataset                                                                              | Source                    | Data type      | # train | # dev  | # test | License     |\n| ------------------------------------------------------------------------------------ | ------------------------- | -------------- | ------- | ------ | ------ | ----------- |\n| **[CACAPO](https://huggingface.co/datasets/kasnerz/cacapo)**                         | van der Lee et al. (2020) | Key-value      | 15,290  | 1,831  | 3,028  | CC BY       |\n| **[DART](https://huggingface.co/datasets/GEM/dart)**                                 | Nan et al. (2021)         | Graph          | 62,659  | 2,768  | 5,097  | MIT         |\n| **[E2E](https://huggingface.co/datasets/GEM/e2e_nlg)**                               | Du≈°ek et al. (2019)       | Key-value      | 33,525  | 1,484  | 1,847  | CC BY-SA    |\n| **[EventNarrative](https://huggingface.co/datasets/kasnerz/eventnarrative)**         | Colas et al. (2021)       | Graph          | 179,544 | 22,442 | 22,442 | CC BY       |\n| **[HiTab](https://huggingface.co/datasets/kasnerz/hitab)**                           | Cheng et al. (2021)       | Table          | 7,417   | 1,671  | 1,584  | C-UDA       |\n| **[Chart-to-text](https://huggingface.co/datasets/kasnerz/charttotext-s)**           | Kantharaj et al. (2022)   | Chart          | 24,368  | 5,221  | 5,222  | GNU GPL     |\n| **[Logic2Text](https://huggingface.co/datasets/kasnerz/logic2text)**                 | Chen et al. (2020b)       | Table  + Logic | 8,566   | 1,095  | 1,092  | MIT         |\n| **[LogicNLG](https://huggingface.co/datasets/kasnerz/logicnlg)**                     | Chen et al. (2020a)       | Table          | 28,450  | 4,260  | 4,305  | MIT         |\n| **[NumericNLG](https://huggingface.co/datasets/kasnerz/numericnlg)**                 | Suadaa et al. (2021)      | Table          | 1,084   | 136    | 135    | CC BY-SA    |\n| **[SciGen](https://huggingface.co/datasets/kasnerz/scigen)**                         | Moosavi et al. (2021)     | Table          | 13,607  | 3,452  | 492    | CC BY-NC-SA |\n| **[SportSett:Basketball](https://huggingface.co/datasets/GEM/sportsett_basketball)** | Thomson et al. (2020)     | Table          | 3,690   | 1,230  | 1,230  | MIT         |\n| **[ToTTo](https://huggingface.co/datasets/totto)**                                   | Parikh et al. (2020)      | Table          | 121,153 | 7,700  | 7,700  | CC BY-SA    |\n| **[WebNLG](https://huggingface.co/datasets/GEM/web_nlg)**                            | Ferreira et al. (2020)    | Graph          | 35,425  | 1,666  | 1,778  | CC BY-NC    |\n| **[WikiBio](https://huggingface.co/datasets/wiki_bio)**                              | Lebret et al. (2016)      | Key-value      | 582,659 | 72,831 | 72,831 | CC BY-SA    |\n| **[WikiSQL](https://huggingface.co/datasets/wikisql)**                               | Zhong et al. (2017)       | Table + SQL    | 56,355  | 8,421  | 15,878 | BSD         |\n| **[WikiTableText](https://huggingface.co/datasets/kasnerz/wikitabletext)**           | Bao et al. (2018)         | Key-value      | 10,000  | 1,318  | 2,000  | CC BY       |\n\nSee `loaders/data.py` for an up-to-date list of available datasets.\n\n## Requirements\n- Python 3\n- Flask\n- HuggingFace datasets\n\nSee `setup.py` for the full list of requirements.\n\n## Installation\n- **pip**: `pip install tabgenie`\n- **development**: `pip install -e .[dev]`\n- **deployment**: `pip install -e .[deploy]`\n\n## Web interface\n- **local development**: `tabgenie [app parameters] run [--port=PORT] [--host=HOSTNAME]`\n- **deployment**: `gunicorn \"tabgenie.cli:create_app([app parameters])\"`\n\n## Command-line Interface\n### Export data\nExports individual tables to file.\n\nUsage:\n```\ntabgenie export \\\n  --dataset DATASET_NAME \\\n  --split SPLIT \\\n  --out_dir OUT_DIR \\\n  --export_format EXPORT_FORMAT\n```\nSupported formats: `json`, `csv`, `xlsx`, `html`, `txt`.\n\n### Generate a spreadsheet for error analysis\nGenerates a spreadsheet with system outputs and randomly selected examples for manual error analysis.\n\nUsage:\n```\ntabgenie sheet \\\n  --dataset DATASET  \\\n  --split SPLIT \\\n  --in_file IN_FILE  \\\n  --out_file OUT_FILE \\\n  --count EXAMPLE_COUNT\n```\n\n### Show dataset details\nDisplays information about the dataset in YAML format (or the list of available datasets if no argument is provided).\n\nUsage:\n```\ntabgenie info [-d DATASET]\n```\n\n## Python\n\nTabGenie can preprocess the datasets without dataset-specific preprocessing methods.\n\nSee the [examples](./examples) directory for a tutorial on using TabGenie for finetuning sequence-to-sequence models.\n\n\n\n## HuggingFace Integration\nThe datasets are stored to `HF_DATASETS_CACHE` directory which defaults to `~/.cache/huggingface/`. \n\n**Set the `HF_DATASETS_CACHE` environment variable before launching `tabgenie` if you want to store the (potentially very large) datasets in a different directory.** \n\n\nThe datasets are all loaded from [HuggingFace datasets](https://huggingface.co/datasets) instead of their original repositories which allows to use preprocessed datasets and a single unified loader.\n\nNote that there may be some minor changes in the data w.r.t. to the original datasets due to unification, such as adding \"subject\", \"predicate\" and \"object\" headings to RDF triple-to-text datasets.\n\n## Adding datasets\nFor adding a new dataset:\n- prepare the dataset\n  - [add the dataset to Huggingface Datasets](https://huggingface.co/docs/datasets/upload_dataset)\n  - OR download the dataset locally\n- create the dataset loader in `loaders`\n  - a subclass of `HFTabularDataset` for HF datasets\n  - a subclass of `TabularDataset` for local datasets\n- create a mapping between the dataset name and the class name in `loaders/__init__.py`\n- add the dataset name to `tabgenie/config.yml`.\n\nEach dataset should contain the `prepare_table(entry)` method which instantiates a `Table` object from the original `entry`.\n\nThe `Table` object is automatically exported to HTML and other formats (the methods may be overridden).\n\nIf a dataset is an instance of `HFTabularDataset` (i.e. is loaded from Huggingface Datasets), it should contain a `self.hf_id` attribute. The attribute is used to automatically load the dataset via `datasets` package.\n\n## Interactive mode\nPipelines are used for processing the tables and producing outputs.\n\nSee `processing/processing.py` for an up-to-date list of available pipelines.\n\nCurrently integrated:\n- **model_api** - a pipeline which generates a textual description of a table by calling a table-to-text generation model through API,\n- **graph** - a pipeline which creates a knowledge graph by extracting RDF triples from a table and visualizes the output using D3.js library,\n\n### Adding pipelines\nFor adding a new pipeline:\n- create a file in `processing/pipelines` containing the pipeline class,\n- create file(s) in `processing/processors` with processors needed for the pipeline,\n- add the mapping between pipeline name and class name to `get_pipeline_class_by_name()` in `processing/processing.py`. \n\nEach pipeline should define `self.processors` in the `__init__()` method, instantiating the processors needed for the pipeline.\n\nThe input to each pipeline is a `content` object containing several fields needed for table processing. This interface may be subject to change (see `__init.py_:run_pipeline()` for more details).\n\nThe processors serve as modules, i.e. existing processors can be combined to create new pipelines. The interface between the processors may vary, it is however expected that the last processor in the pipeline outputs HTML code which is displayed on the page.\n\n\n### Pipeline config\nThis is an example pipeline configuration in `tabgenie/config.yml`:\n```\nrdf_triples:\n  pipeline: graph\n  interactive: true\n  datasets:\n    - webnlg\n    - dart\n    - e2e\n```\nThe key `rdf_triples` is the name of the pipeline which will be displayed in the web interface. It should contain only letters of English alphabet, underscores `_` or dashes `-`.\n\nRequired arguments:\n- `pipeline` : `str` - the name of the pipeline as defined in `processing/processing.py`, will be mapped to pipeline class\n- `interactive`: `bool` - whether the pipeline will be displayed in the interactive mode in the web interface\n\nOptional arguments:\n- `datasets` : `list` - the list of datasets for which the pipeline will be active in the web interface (all datasets by default)\n- any other argument, will be passed to the pipeline in `pipeline_args`\n\n\n\n## Configuration\nThe global configuration is stored in the `tabgenie/config.yml` file.\n\n- `datasets` - datasets which will be available in the web interface,\n- `default_dataset` - the dataset which is loaded by default,\n- `host_prefix` - subdirectory on which the app is deployed (used for loading static files and sending POST requests),\n- `cache_dev_splits` - whether to preload all available dev sets after startup,\n- `generated_outputs_dir` - directory from which the generated outputs are loaded,\n- `pipelines` - pipelines which will be available in the web interface (see the *Interactive mode* section for more info).\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/kasnerz/tabgenie",
    "keywords": "",
    "license": "Apache-2.0 License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tabgenie",
    "package_url": "https://pypi.org/project/tabgenie/",
    "platform": null,
    "project_url": "https://pypi.org/project/tabgenie/",
    "project_urls": {
      "Homepage": "https://github.com/kasnerz/tabgenie"
    },
    "release_url": "https://pypi.org/project/tabgenie/0.0.1/",
    "requires_dist": [
      "Flask (>=2.2.2)",
      "datasets (>=2.9.0)",
      "requests",
      "lxml",
      "tinyhtml",
      "xlsxwriter",
      "coloredlogs",
      "gunicorn ; extra == 'deploy'",
      "wheel ; extra == 'dev'",
      "black ; extra == 'dev'"
    ],
    "requires_python": ">=3.8",
    "summary": "TabGenie: A toolkit for table-to-text generation.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16988826,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b2a2d18329edae869b948778cd3e0aa3493dd13ab4a47a30126b47cee07671e6",
          "md5": "3ee5a015287ef2b8da2beaf7da66500b",
          "sha256": "fde7fd6c53816f068303d4ae7a655e3bd6e72841b34d088120cc8a33dfce07a8"
        },
        "downloads": -1,
        "filename": "tabgenie-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3ee5a015287ef2b8da2beaf7da66500b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 934220,
        "upload_time": "2023-02-23T05:22:44",
        "upload_time_iso_8601": "2023-02-23T05:22:44.217014Z",
        "url": "https://files.pythonhosted.org/packages/b2/a2/d18329edae869b948778cd3e0aa3493dd13ab4a47a30126b47cee07671e6/tabgenie-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "31d7f1251e898805a666dbaed42125083258d544d8173799e98cbd3d86a90b94",
          "md5": "4a5adb97a3ac742f30561274a048cda1",
          "sha256": "fe2ae8f61d8e0e1d101c3f21a12205b6f787c543631f9892a917a262b6f96ff4"
        },
        "downloads": -1,
        "filename": "tabgenie-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4a5adb97a3ac742f30561274a048cda1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 906281,
        "upload_time": "2023-02-23T05:22:46",
        "upload_time_iso_8601": "2023-02-23T05:22:46.717647Z",
        "url": "https://files.pythonhosted.org/packages/31/d7/f1251e898805a666dbaed42125083258d544d8173799e98cbd3d86a90b94/tabgenie-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b2a2d18329edae869b948778cd3e0aa3493dd13ab4a47a30126b47cee07671e6",
        "md5": "3ee5a015287ef2b8da2beaf7da66500b",
        "sha256": "fde7fd6c53816f068303d4ae7a655e3bd6e72841b34d088120cc8a33dfce07a8"
      },
      "downloads": -1,
      "filename": "tabgenie-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3ee5a015287ef2b8da2beaf7da66500b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 934220,
      "upload_time": "2023-02-23T05:22:44",
      "upload_time_iso_8601": "2023-02-23T05:22:44.217014Z",
      "url": "https://files.pythonhosted.org/packages/b2/a2/d18329edae869b948778cd3e0aa3493dd13ab4a47a30126b47cee07671e6/tabgenie-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "31d7f1251e898805a666dbaed42125083258d544d8173799e98cbd3d86a90b94",
        "md5": "4a5adb97a3ac742f30561274a048cda1",
        "sha256": "fe2ae8f61d8e0e1d101c3f21a12205b6f787c543631f9892a917a262b6f96ff4"
      },
      "downloads": -1,
      "filename": "tabgenie-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "4a5adb97a3ac742f30561274a048cda1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 906281,
      "upload_time": "2023-02-23T05:22:46",
      "upload_time_iso_8601": "2023-02-23T05:22:46.717647Z",
      "url": "https://files.pythonhosted.org/packages/31/d7/f1251e898805a666dbaed42125083258d544d8173799e98cbd3d86a90b94/tabgenie-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}