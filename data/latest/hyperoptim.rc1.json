{
  "info": {
    "author": "Deepak Yadav",
    "author_email": "dky.united@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.0",
      "Topic :: Scientific/Engineering :: Mathematics"
    ],
    "description": "[![Downloads](https://pepy.tech/badge/complex)](https://pepy.tech/project/complex)\n[![Downloads](https://pepy.tech/badge/complex/month)](https://pepy.tech/project/complex/month)\n[![Downloads](https://pepy.tech/badge/complex/week)](https://pepy.tech/project/complex/week)\n\n# Hyperoptim\n> Hyperparameter Optimization Using Genetic Algorithm.\n\n## Installation\n\nOS X , Windows & Linux:\n\n```sh\npip install hyperoptim\n```\n## Usage example\nUse for find best hyperparameter\n\n```python\nfrom hyperoptim import GASearch, Hparams\nimport tensorflow as tf\nfrom tensorflow import keras\n\n(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()\n\n# define hyperparameter space\nht = Hparams()\nhp_units = ht.Int('units', min_value=32, max_value=512, step=32)\nhp_learning_rate = ht.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\nhp_activation = ht.Choice('activation', values=['relu', 'sigmoid', 'tanh'])\n\n# create the list of hyperparameter\nparams = [hp_units, hp_learning_rate, hp_activation]\n\n# define model \nparams = [hp_units, hp_learning_rate, hp_activation]\ndef model_builder(params):\n    model = keras.Sequential()\n    model.add(keras.layers.Flatten(input_shape=(28, 28)))\n    # here params[0] refer to hp_units and params[2] refer to hp_activation\n    model.add(keras.layers.Dense(units=params[0], activation=params[2]))\n    model.add(keras.layers.Dense(10))\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=params[1]),\n                    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                    metrics=['accuracy'])\n    return model\n\n# intialize the GASearch\ntuner = GASearch(model_builder=model_builder, params=params, objective='val_accuracy', weights=(1.0,), max_epochs=10, directory='my_dir', project_name='intro_to_kt')\n\n# run the search                  \ntuner.search(img_train, label_train, epochs=2, validation_split=0.2)\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters()[0]\n\n# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\nmodel = tuner.build(best_hps)\nhistory = model.fit(img_train, label_train, epochs=2, validation_split=0.2)\n\nval_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))\n\neval_result = model.evaluate(img_test, label_test)\nprint(\"[test loss, test accuracy]:\", eval_result)\n```\n\n## Development setup\nFor local development setup\n\n```sh\ngit clone https://github.com/deepak7376/hyperoptim\ncd hyperoptim\npip install -r requirements.txt\n```\n\n## Meta\nDeepak Yadav\n\nDistributed under the MIT license. See ``LICENSE`` for more information.\n[https://github.com/deepak7376/hypertune/blob/master/LICENSE](https://github.com/deepak7376)\n\n## References\nNone\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/deepak7376/hyperoptim",
    "keywords": "Hyperparameter optimization using Genetic Algorithms.",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "hyperoptim",
    "package_url": "https://pypi.org/project/hyperoptim/",
    "platform": null,
    "project_url": "https://pypi.org/project/hyperoptim/",
    "project_urls": {
      "Homepage": "https://github.com/deepak7376/hyperoptim"
    },
    "release_url": "https://pypi.org/project/hyperoptim/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3",
    "summary": "Hyperparameter Optimization using Genetic Algorithms.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15832527,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "02bd377dfda04800519b605c671a5b2eac27637ebeaaf467051235ca27853adf",
          "md5": "1392d3c612123f552adfb08813e519b0",
          "sha256": "14dc4248b33d36448523d53d715e724f33d21a7fb397149e10fca4d36aa7b9f8"
        },
        "downloads": -1,
        "filename": "hyperoptim-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1392d3c612123f552adfb08813e519b0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 4371,
        "upload_time": "2022-11-20T17:48:59",
        "upload_time_iso_8601": "2022-11-20T17:48:59.319493Z",
        "url": "https://files.pythonhosted.org/packages/02/bd/377dfda04800519b605c671a5b2eac27637ebeaaf467051235ca27853adf/hyperoptim-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "02bd377dfda04800519b605c671a5b2eac27637ebeaaf467051235ca27853adf",
        "md5": "1392d3c612123f552adfb08813e519b0",
        "sha256": "14dc4248b33d36448523d53d715e724f33d21a7fb397149e10fca4d36aa7b9f8"
      },
      "downloads": -1,
      "filename": "hyperoptim-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "1392d3c612123f552adfb08813e519b0",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3",
      "size": 4371,
      "upload_time": "2022-11-20T17:48:59",
      "upload_time_iso_8601": "2022-11-20T17:48:59.319493Z",
      "url": "https://files.pythonhosted.org/packages/02/bd/377dfda04800519b605c671a5b2eac27637ebeaaf467051235ca27853adf/hyperoptim-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}