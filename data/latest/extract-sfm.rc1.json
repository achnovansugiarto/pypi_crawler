{
  "info": {
    "author": "Yueen Ma",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Knowledge Graph Extraction\n\nWe designed a pipeline that can extract a special kind of knowledge graphs where a person's name will be recognized and his/her rank, role, title and organization will be related to him/her. It is not expected to perform perfectly so that all relevant persons will be recognized and all irrelevant persons will be excluded. Rather, it is seen as a first step to reduce the workload that is involved to manually extract such knowledge by combing through a large amount of documents.\n\nThis pipeline consists of two major components: Name Entity Recognition and Relation Extraction. Name Entity Recognition uses a BiLSTM-CNNs-CRF model. It recognizes names, ranks, roles, titles and organizations from raw text files. Then the Relation Extraction relates names to his/her corresponding rank, role, title or organization.\n\nExample:\n![Example](images/brat_stn.png)\n\n## Dependencies\nTensorflow 2.2.0 <br>\nTensorflow-addons <br>\nSpaCy <br>\nNumPy <br>\nDyNet <br>\nPathlib <br>\n\n## Install\nPackage: https://pypi.org/project/extract-sfm/\n```shell\n$ pip install extract_sfm\n```\n\n\n## Usage\n\n### Method 1\n\nCreate a python file and write:\n```python\nimport extract_sfm\n\nextract_sfm.extract(\"/PATH/TO/DIRECTORY/OF/INPUT/FILES\")\n```\nThen run the python file. This may take a while to finish.\n\n### Method 2\n\nDownload this Github repository\nUnder the project root directory, run the python script\n\n```shell\n$ python pipeline.py /PATH/TO/DIRECTORY/OF/INPUT/FILES\n```\n> Note: Use absolute path.\n\n\n## Website\n1. Copy NER_v2, RE, pipeline.py into the \"SERVER/KGE\" directory\n2. Install npm dependencies under the \"SERVER\" directory: express, path, multer\n```\n  $ npm install <package name>\n```\n3. Run the server by typing in:\n```\n  $ node server.js\n```\n\n![Example](images/website.jpeg)\n\n## Environment Setup\n```\ntensorflow 2.2.0\n  pip install tensorflow\n  pip install tensorflow-addons\n\nspaCy (macOS)\n  pip install -U spacy\n  python3 -m spacy download en_core_web_sm\n\nDyNet\n  pip install dynet\n\npathlib\n  pip install pathlib\n```\n\n\n## NER Documentation\n```\nTRAINING\n  Dataset:\n    1. SFM starter dataset: https://github.com/security-force-monitor/nlp_starter_dataset\n    2. CONLL2003: https://github.com/guillaumegenthial/tf_ner/tree/master/data/example\n    3. A set of known organizations from the starter dataset\n    Note: Title and role were collapsed into one class\n\n  Usage:\n    1) Prepare data\n      $ python process.py\n      $ cd SFM_STARTER\n      $ python build_vocab.py\n      $ python build_glove.py\n      $ cd ..\n\n    2) Train model\n      $ python train.py\n\n    3) Make predictions\n      $ python pred.py\n\n    4) Evaluate model\n      $ python eval.py\n      $ python eval_class.py\n\n  Files:\n    process.py: 1) preprocess dataset by recording info in dicts,\n                      which are saved in two pickle files: dataset_labels.pickle, dataset_sentences.pickle\n                2) convert SFM starter dataset to a format that can be used by the model,\n                      which are in files: {}.words.txt and {}.tags.txt where {} could be train, valid or test.\n    pred.py: generates predictions using the trained model\n    eval.py: evaluate the predctions made by model, which are generated by running pred.py\n    eval_class.py: get precision, recall and f1 score for each class\n\n    Other files are from https://github.com/guillaumegenthial/tf_ner\n      train.py, tf_metrics.py, SFM_STARTER/build_vocab.py, SFM_STARTER/build_glove.py\n\nPREDICTING\n  Usage:\n    $ python ner.py <doc_id>.txt\n\n  File:\n    ner.py: get BRAT format prediction for a text file.\n```\n\n## RE Documentation\n```\njPTDP:\n  Before running the following 3 methods, you need to run a dependency parser first, which some methods relies on.\n  Usage: Go to the jPTDP directory and run\n    $ python fast_parse.py <path_to_txt>.txt\n  The output will be put along side with the input text file in a directory whose name is same as the text file.\n\n\n\n--- METHOD 1: nearest person:\n    Assign the non-person name entities to the nearest person that is behind the name entities.\n\n    Usage:\n      1. To extraction relations in a single text file:\n        (extracted relations will be appended to the .ann file)\n        $ python relation_np.py <doc_id>.txt <doc_id>.ann\n      2. To generate annotations for a set of text file under <directory>\n        Set \"output_dir\" in pipeline.sh to <directory> and run:\n        $ source pipeline.sh\n\n\n\n--- METHOD 2: dependency parsing\n    Assign the non-person name entities to the closest person where distance is the length of the dependency path between the name entity and the person\n    Constraint: If we only choose from one of the two person that appear immediately on the left and the right side, the results could be improved but the drawbacks are also obvious\n\n    Usage:\n      1. To extraction relations in a single text file:\n        (extracted relations will be appended to the .ann file)\n        $ python relation_dep.py <jPTDP_buffer_path> <doc_id>.txt <doc_id>.ann\n      2. To generate annotations for a set of text file under <directory>\n        $ source pipeline.sh <directory>\n\n\n\n--- METHOD 3: neural networks\n    Use dependency path and its distance as features to predict which person in the sentence is the best option\n    The best model is saved in \"model_86.h5\"\n\n    Usage:\n      Predictions are made on files in \"pred_path\" and are written in place, \"pred_path\" can be set in config.py\n      $ python pred.py\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Panmani/KGE",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "extract-sfm",
    "package_url": "https://pypi.org/project/extract-sfm/",
    "platform": "",
    "project_url": "https://pypi.org/project/extract-sfm/",
    "project_urls": {
      "Homepage": "https://github.com/Panmani/KGE"
    },
    "release_url": "https://pypi.org/project/extract-sfm/2.0/",
    "requires_dist": [
      "numpy",
      "tensorflow",
      "tensorflow-addons",
      "spacy",
      "dynet",
      "pathlib"
    ],
    "requires_python": ">=3.6",
    "summary": "Knowledge Graph Extraction for SFM dataset",
    "version": "2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12572892,
  "releases": {
    "2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d2d751bbfaa10b9defac81e072ce320d24a9255f02222a7f20b2afbe1b0958e9",
          "md5": "5e68fc1705b14fa7a14a4516ee27e4a1",
          "sha256": "538611bb5e880944f46c2bec94537de04eab310aa0041780db39814ef3a69474"
        },
        "downloads": -1,
        "filename": "extract_sfm-2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5e68fc1705b14fa7a14a4516ee27e4a1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 129537164,
        "upload_time": "2020-08-04T17:57:37",
        "upload_time_iso_8601": "2020-08-04T17:57:37.559484Z",
        "url": "https://files.pythonhosted.org/packages/d2/d7/51bbfaa10b9defac81e072ce320d24a9255f02222a7f20b2afbe1b0958e9/extract_sfm-2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "19e08e9514910a2ca5ef596291cfa4ae7cfc08ce7e0809057ad2c259bfcf554e",
          "md5": "e5f15c78bdb0b9a93fb508ee6339b77e",
          "sha256": "b84fa13c4119f3dd9774cdf2d6a6e0fd851d949cccc50af4e64c6f96f3d5e3a8"
        },
        "downloads": -1,
        "filename": "extract_sfm-2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "e5f15c78bdb0b9a93fb508ee6339b77e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 31866,
        "upload_time": "2020-08-04T17:57:42",
        "upload_time_iso_8601": "2020-08-04T17:57:42.402169Z",
        "url": "https://files.pythonhosted.org/packages/19/e0/8e9514910a2ca5ef596291cfa4ae7cfc08ce7e0809057ad2c259bfcf554e/extract_sfm-2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d2d751bbfaa10b9defac81e072ce320d24a9255f02222a7f20b2afbe1b0958e9",
        "md5": "5e68fc1705b14fa7a14a4516ee27e4a1",
        "sha256": "538611bb5e880944f46c2bec94537de04eab310aa0041780db39814ef3a69474"
      },
      "downloads": -1,
      "filename": "extract_sfm-2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "5e68fc1705b14fa7a14a4516ee27e4a1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 129537164,
      "upload_time": "2020-08-04T17:57:37",
      "upload_time_iso_8601": "2020-08-04T17:57:37.559484Z",
      "url": "https://files.pythonhosted.org/packages/d2/d7/51bbfaa10b9defac81e072ce320d24a9255f02222a7f20b2afbe1b0958e9/extract_sfm-2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "19e08e9514910a2ca5ef596291cfa4ae7cfc08ce7e0809057ad2c259bfcf554e",
        "md5": "e5f15c78bdb0b9a93fb508ee6339b77e",
        "sha256": "b84fa13c4119f3dd9774cdf2d6a6e0fd851d949cccc50af4e64c6f96f3d5e3a8"
      },
      "downloads": -1,
      "filename": "extract_sfm-2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "e5f15c78bdb0b9a93fb508ee6339b77e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 31866,
      "upload_time": "2020-08-04T17:57:42",
      "upload_time_iso_8601": "2020-08-04T17:57:42.402169Z",
      "url": "https://files.pythonhosted.org/packages/19/e0/8e9514910a2ca5ef596291cfa4ae7cfc08ce7e0809057ad2c259bfcf554e/extract_sfm-2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}