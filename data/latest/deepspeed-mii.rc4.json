{
  "info": {
    "author": "DeepSpeed Team",
    "author_email": "deepspeed-mii@microsoft.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "<!-- [![Build Status](https://github.com/microsoft/deepspeed-mii/workflows/Build/badge.svg)](https://github.com/microsoft/DeepSpeed-MII/actions) -->\n[![Formatting](https://github.com/microsoft/DeepSpeed-MII/actions/workflows/formatting.yml/badge.svg)](https://github.com/microsoft/DeepSpeed-MII/actions/workflows/formatting.yml)\n[![License MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/Microsoft/DeepSpeed-MII/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed-mii.svg)](https://pypi.org/project/deepspeed-mii/)\n<!-- [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest) -->\n\n<div align=\"center\">\n <img src=\"docs/images/mii-white.svg#gh-light-mode-only\" width=\"400px\">\n <img src=\"docs/images/mii-dark.svg#gh-dark-mode-only\" width=\"400px\">\n</div>\n\n## Latest News\n\n* [2022/11] [Stable Diffusion Image Generation under 1 second w. DeepSpeed MII](examples/benchmark/txt2img)\n* [2022/10] [Announcing DeepSpeed Model Implementations for Inference (MII)](https://www.deepspeed.ai/2022/10/10/mii.html)\n\n# Contents\n\n<!-- toc -->\n\n- [DeepSpeed MII](#deepspeed-model-implementations-for-inference)\n- [How does MII work?](#how-does-mii-work)\n- [Supported Models and Tasks](#supported-models-and-tasks)\n- [MII-Public and MII-Azure](#mii-public-and-mii-azure)\n- [Getting started with MII](#getting-started-with-mii)\n- [Quantifying Latency and Cost Reduction](#quantifying-latency-and-cost-reduction)\n- [Community Tutorials](#community-tutorials)\n\n<!-- tocstop -->\n\n# DeepSpeed Model Implementations for Inference\n\n![hero dark](docs/images/hero-dark.png#gh-dark-mode-only)\n![hero light](docs/images/hero-transparent.png#gh-light-mode-only)\n\nThe Deep Learning (DL) open-source community has seen tremendous growth in the last few months. Incredibly powerful text generation models such as the Bloom 176B, or image generation model such as Stable Diffusion are now available to anyone with access to a handful or even a single GPU through platforms such as Hugging Face. While open sourcing has democratized access to AI capabilities, their application is still restricted by two critical factors: inference latency and cost.\n\nThere has been significant progress in system optimizations for DL model inference that can drastically reduce both latency and cost, but those are not easily accessible. A main reason for this limited accessibility is that the DL model inference landscape is diverse with models varying in size, architecture, system performance characteristics, hardware requirements, etc. Identifying the appropriate set of system optimizations applicable to a given model and applying them correctly is often beyond the scope of most data scientists, making low latency and low-cost inference mostly inaccessible.\n\nDeepSpeed-MII is a new open-source python library from DeepSpeed, aimed towards making low-latency, low-cost inference of powerful models not only feasible but also easily accessible.\n\n* MII offers access to highly optimized implementation of thousands of widely used DL models.\n* MII supported models achieve significantly lower latency and cost compared to their original implementation. For example, MII reduces the latency of Big-Science Bloom 176B model by 5.7x, while reducing the cost by over 40x. Similarly, it reduces the latency and cost of deploying Stable Diffusion by 1.9x. See more details for [an exhaustive latency and cost analysis of MII](#quantifying-latency-and-cost-reduction).\n* To enable low latency/cost inference, MII leverages an extensive set of optimizations from DeepSpeed-Inference such as deepfusion for transformers, automated tensor-slicing for multi-GPU inference, on-the-fly quantization with ZeroQuant, and several others (see our [blog post](https://www.deepspeed.ai/2022/10/10/mii.html) for more details).\n* With state-of-the-art performance, MII supports low-cost deployment of these models both on-premises and on Azure via AML with just a few lines of codes.\n\n# How does MII work?\n\n![Text Generation Models](docs/images/mii-arch.png)\n\n*Figure 1: MII Architecture, showing how MII automatically optimizes OSS models using DS-Inference before deploying them on-premises using GRPC, or on Microsoft Azure using AML Inference.*\n\nUnder-the-hood MII is powered by [DeepSpeed-Inference](https://arxiv.org/abs/2207.00032). Based on model type, model size, batch size, and available hardware resources, MII automatically applies the appropriate set of system optimizations from DeepSpeed-Inference to minimize latency and maximize throughput. It does so by using one of many pre-specified model injection policies, that allows MII and DeepSpeed-Inference to identify the underlying PyTorch model architecture and replace it with an optimized implementation (see *Figure A*). In doing so, MII makes the expansive set of optimizations in DeepSpeed-Inference automatically available for thousands of popular models that it supports.\n\n\n# Supported Models and Tasks\n\nMII currently supports over 30,000 models across a range of tasks such as text-generation, question-answering, text-classification. The models accelerated by MII are available through multiple open-sourced model repositories such as Hugging Face, FairSeq, EluetherAI, etc. We support dense models based on Bert, Roberta or GPT architectures ranging from few hundred million parameters to tens of billions of parameters in size. We continue to expand the list with support for massive hundred billion plus parameter dense and sparse models coming soon.\n\nMII model support will continue to grow over time, check back for updates! Currently we support the following Hugging Face Transformers model families:\n\nmodel family | size range | ~model count\n------ | ------ | ------\n[bloom](https://huggingface.co/models?other=bloom) | 0.3B - 176B | 198\n[stable-diffusion](https://huggingface.co/models?other=stable-diffusion) | 1.1B | 330\n[opt](https://huggingface.co/models?other=opt) | 0.1B - 66B | 170\n[gpt\\_neox](https://huggingface.co/models?other=gpt_neox) | 1.3B - 20B | 37\n[gptj](https://huggingface.co/models?other=gptj) | 1.4B - 6B | 140\n[gpt\\_neo](https://huggingface.co/models?other=gpt_neo) | 0.1B - 2.7B | 300\n[gpt2](https://huggingface.co/models?other=gpt2) | 0.3B - 1.5B | 7,888\n[xlm-roberta](https://huggingface.co/models?other=xlm-roberta) | 0.1B - 0.3B | 1,850\n[roberta](https://huggingface.co/models?other=roberta) | 0.1B - 0.3B | 5,190\n[bert](https://huggingface.co/models?other=bert) | 0.1B - 0.3B | 13,940\n\n<!--\nSD param count:\ntext_encoder: 123060480\nunet: 859520964\nvae: 83653863\n-->\n\n<!--For a full set of models and tasks supported by MII, please see here (TODO: add reference to specific model classes we support)-->\n\n# MII-Public and MII-Azure\n\nMII can work with two variations of DeepSpeed-Inference. The first, referred to as ds-public, contains most of the DeepSpeed-Inference optimizations discussed here,  is also available via our open-source DeepSpeed library. The second referred to as ds-azure, offers tighter integration with Azure, and is available via MII to all Microsoft Azure customers. We refer to MII running the two DeepSpeed-Inference variants as MII-Public and MII-Azure, respectively.\n\nWhile both variants offers significant latency and cost reduction over the open-sourced PyTorch baseline, the latter, offers additional performance advantage for generation based workloads. The full latency and cost advantage comparison with PyTorch baseline and across these two versions is available [here](#quantifying-latency-and-cost-reduction).\n\n# Getting Started with MII\n\n## Installation\n\nWe regularly push releases to [PyPI](https://pypi.org/project/deepspeed-mii/) and encourage users to install from there in most cases.\n\n```bash\npip install deepspeed-mii\n```\n\n## Deploying MII-Public\n\nMII-Public can be deployed on-premises or on any cloud offering with just a few lines of code. MII creates a lightweight GRPC server to support this form of deployment and provides a GRPC inference endpoint for queries.\n\nSeveral deployment and query examples can be found here: [examples/local](https://github.com/microsoft/DeepSpeed-MII/tree/main/examples/local)\n\nAs an example here is a deployment of the [bigscience/bloom-560m](https://huggingface.co/bigscience/bloom-560m) model from Hugging Face:\n\n**Deployment**\n```python\nimport mii\nmii_configs = {\"tensor_parallel\": 1, \"dtype\": \"fp16\"}\nmii.deploy(task=\"text-generation\",\n           model=\"bigscience/bloom-560m\",\n           deployment_name=\"bloom560m_deployment\",\n           mii_config=mii_configs)\n```\n\nThis will deploy the model onto a single GPU and start the GRPC server that can later be queried.\n\n**Query**\n```python\nimport mii\ngenerator = mii.mii_query_handle(\"bloom560m_deployment\")\nresult = generator.query({\"query\": [\"DeepSpeed is\", \"Seattle is\"]}, do_sample=True, max_new_tokens=30)\nprint(result)\n```\n\nThe only required key is `\"query\"`, all other items outside the dictionary will be passed to `generate` as kwargs. For Hugging Face provided models you can find all possible arguments in their [documentation for generate](https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate).\n\n**Shutdown Deployment**\n```python\nimport mii\nmii.terminate(\"bloom560m_deployment\")\n```\n\n## Deploying with MII-Azure\n\nMII supports deployment on Azure via AML Inference. To enable this, MII generates AML deployment assets for a given model that can be deployed using the Azure-CLI, as shown in the code below. Furthermore, deploying on Azure, allows MII to leverage DeepSpeed-Azure as its optimization backend, which offers better latency and cost reduction than DeepSpeed-Public.\n\nThis deployment process is very similar to local deployments and we will modify the code from the local deployment example with the [bigscience/bloom-560m](https://huggingface.co/bigscience/bloom-560m) model.\n\n---\n📌 **Note:**  MII-Azure has the benefit of supporting DeepSpeed-Azure for better latency and cost than DeepSpeed-Public for certain workloads. We are working to enable DeepSpeed-Azure automatically for all MII-Azure deployments in a near-term MII update. In the meantime, we are offering DeepSpeed-Azure as a preview release to MII-Azure users. If you have a MII-Azure deployment and would like to try DeepSpeed-Azure, please reach out to us at deepspeed-mii@microsoft.com to get access.\n\n---\n\nSeveral other AML deployment examples can be found here: [examples/aml](https://github.com/microsoft/DeepSpeed-MII/tree/main/examples/aml)\n\n**Setup**\n\nTo use MII on AML resources, you must have the Azure-CLI installed with an active login associated with your Azure resources. Follow the instructions below to get your local system ready for deploying on AML resources:\n\n1. Install Azure-CLI. Follow the official [installation instructions](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli#install).\n2. Run `az login` and follow the instructions to login to your Azure account. This account should be linked to the resources you plan to deploy on.\n3. Set the default subscription with `az account set --subscription <YOUR-SUBSCRIPTION-ID>`. You can find your subscription ID in the \"overview\" tab on your resource group page from the Azure web portal.\n4. Install the AML plugin for Azure-CLI with `az extension add --name ml`\n\n**Deployment**\n```python\nimport mii\nmii_configs = {\"tensor_parallel\": 1, \"dtype\": \"fp16\"}\nmii.deploy(task=\"text-generation\",\n           model=\"bigscience/bloom-560m\",\n           deployment_name=\"bloom560m-deployment\",\n           deployment_type=mii.constants.DeploymentType.AML,\n           mii_config=mii_configs)\n```\n\n---\n📌 **Note:** Running the `mii.deploy` with `deployment_type=mii.constants.DeploymentType.AML` will only generate the scripts to launch an AML deployment. You must also run the generated `deploy.sh` script to run on AML resources.\n\n---\n\nThis will generate the scripts and configuration files necessary to deploy the model on AML using a single GPU. You can find the generated output at `./bloom560m-deployment_aml/`\n\nWhen you are ready to run your deployment on AML resources, navigate to the newly created directory and run the deployment script:\n```bash\ncd ./bloom560m-deployment_aml/\nbash deploy.sh\n```\n\nThis script may take several minutes to run as it does the following:\n- Downloads the model locally\n- Creates a Docker Image with MII for your deployment\n- Creates an AML online-endpoint for running queries\n- Uploads and registers the model to AML\n- Starts your deployment\n\n---\n📌 **Note:** Large models (e.g., `bigscience/bloom`) may cause a timeout when trying to upload and register the model to AML. In these cases, it is required to manually upload models to Azure blob storage with [AzCopy](https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10). Instructions and automation of this step will be added soon.\n\n---\n\n**Query**\nOnce the deployment is running on AML, you can run queries by navigating to the online-endpoint that was created for this deployment (i.e., `bloom-560m-deployment-endpoint`) from the [AML web portal](https://ml.azure.com/endpoints). Select the \"Test\" tab at the top of the endpoint page and type your query into the text-box:\n```\n{\"query\": [\"DeepSpeed is\", \"Seattle is\"], \"do_sample\"=True, \"max_new_tokens\"=30}\n```\n\nThe only required key is `\"query\"`, all other items in the dictionary will be passed to `generate` as kwargs. For Hugging Face provided models you can find all possible arguments in their [documentation for generate](https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate).\n\n# Quantifying Latency and Cost Reduction\n\nInference workloads can be either latency critical, where the primary objective is to minimize latency, or cost sensitive, where the primary objective is to minimize cost. In this section, we quantify the benefits of using MII for both latency-critical and cost-sensitive scenarios.\n\n## Latency Critical Scenarios\n\nFor latency-critical scenarios, where a small batch size of 1 is often used, MII can reduce the latency by up to 6x for a wide range of open-source models, across multiple tasks. More specifically, we show model latency reduction of [^overhead_details]:\n\n1. Up to 5.7x for multi-GPU inference for text generation using massive models such as Big Science Bloom, Facebook OPT, and EluetherAI NeoX (*Figure 2 (left)*)\n\n2. Up to 1.9x for image generation tasks model using Stable Diffusion (*Figure 2 (right)*)\n\n3. Up to 3x for relatively smaller text generation models (up to 7B parameters) based on OPT, BLOOM, and GPT architectures, running on a single GPU (*Figures 3 and 4*)\n\n4. Up to 9x for various text representation tasks like fill-mask, text classification, question answering, and token classification using RoBERTa- and BERT- based models (*Figures 5 and 6*).\n\n[ ![multi gpu latency](/docs/images/llm-latency-sd-latency.png) ](/docs/images/llm-latency-sd-latency-zoom.png)\n*Figure 2: (Left) Best achievable latency for large models. MII-Azure (int8) offers 5.7X lower latency compared to Baseline for Bloom-176B. (Right) Stable Diffusion text to image generation latency comparison.*\n\n[ ![OPT and BLOOM Models](/docs/images/opt-bloom.png) ](/docs/images/opt-bloom.png)\n*Figure 3: Latency comparison for OPT and BLOOM models. MII-Azure is up to 2.8x faster than baseline.*\n\n[ ![GPT Models](/docs/images/gpt.png) ](/docs/images/mii/gpt.png)\n*Figure 4: Latency comparison for GPT models. MII-Azure is up to 3x faster than baseline.*\n\n[ ![Roberta Models](/docs/images/roberta.png) ](/docs/images/roberta.png)\n*Figure 5: Latency comparison for RoBERTa models. MII offers up to 9x lower model latency and up to 3x lower end-to-end latency than baseline on several tasks and RoBERTa variants [^overhead_details].*\n\n[ ![Bert Models](/docs/images/bert.png) ](/docs/images/bert.png)\n*Figure 6: Latency comparison for BERT models. MII offers up to 8.9x lower model latency and up to 4.5x end-to-end latency across several tasks and BERT variants[^overhead_details].*\n\n[^overhead_details]: The end-to-end latency of an inference workload is comprised of two components: i) actual model execution, and ii) pre-/post-processing before and after the model execution. MII optimizes the actual model execution but leaves the pre-/post-processing pipeline for future optimizations. We notice that text representation tasks have significant pre-/post-processing overhead (*Figures G and H*). We plan to address those in a future update.\n\n## Cost Sensitive Scenarios\n\nMII can significantly reduce the inference cost of very expensive language models like Bloom, OPT, etc. To get the lowest cost, we use a large batch size that maximizes throughput for both baseline and MII. Here we look at the cost reduction from MII using two different metrics: i) tokens generated per second per GPU, and ii) dollars per million tokens generated.\n\n*Figures 7 and 8* show that MII-Public offers over 10x throughput improvement and cost reduction compared to the baseline, respectively. Furthermore, MII-Azure offers over 30x improvement in throughput and cost compared to the baseline.\n\n[ ![tput large models](/docs/images/tput-llms.png) ](/docs/images/tput-llms.png)\n*Figure 7: Throughput comparison per A100-80GB GPU for large models. MII-Public offers over 15x throughput improvement while MII-Azure offers over 40x throughput improvement.*\n\n[ ![azure cost](/docs/images/azure-cost.png) ](/docs/images/azure-cost.png)\n*Figure 8: Cost of generating 1 million tokens on Azure with different model types. MII-Azure reduces the cost of generation by over 40x.*\n\n# Community Tutorials\n\n* [DeepSpeed Deep Dive — Model Implementations for Inference (MII) (Heiko Hotz)](https://towardsdatascience.com/deepspeed-deep-dive-model-implementations-for-inference-mii-b02aa5d5e7f7)\n\n# Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://deepspeed.ai",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "deepspeed-mii",
    "package_url": "https://pypi.org/project/deepspeed-mii/",
    "platform": null,
    "project_url": "https://pypi.org/project/deepspeed-mii/",
    "project_urls": {
      "Documentation": "https://github.com/microsoft/DeepSpeed-MII",
      "Homepage": "http://deepspeed.ai",
      "Source": "https://github.com/microsoft/DeepSpeed-MII"
    },
    "release_url": "https://pypi.org/project/deepspeed-mii/0.0.4/",
    "requires_dist": [
      "asyncio",
      "deepspeed (>=0.7.6)",
      "grpcio",
      "grpcio-tools",
      "pydantic",
      "torch",
      "transformers",
      "pre-commit ; extra == 'dev'",
      "pytest ; extra == 'dev'",
      "pytest-forked ; extra == 'dev'"
    ],
    "requires_python": "",
    "summary": "deepspeed mii",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15962753,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5bb6b2ac8ef66cb3c5213d485420e2723143dd0a071bae657ce5c18420e6bbd2",
          "md5": "eeebad08c9cfd81cd05e18b9d3a18706",
          "sha256": "5207c166b659a1db22ca30be34027b485eff7db341ad65364985d83089fb6e50"
        },
        "downloads": -1,
        "filename": "deepspeed_mii-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eeebad08c9cfd81cd05e18b9d3a18706",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 35781,
        "upload_time": "2022-10-24T19:39:37",
        "upload_time_iso_8601": "2022-10-24T19:39:37.362422Z",
        "url": "https://files.pythonhosted.org/packages/5b/b6/b2ac8ef66cb3c5213d485420e2723143dd0a071bae657ce5c18420e6bbd2/deepspeed_mii-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cbd94ceb951dfc47d9fd0949d196cb5a2596dddc53ab12b16470705a7788a171",
          "md5": "b6f284a4178eea09d608ab4265fb46aa",
          "sha256": "116f156d1839441a76bc06238e9ee1ffcfa81056b10e6450136c42c307b8f098"
        },
        "downloads": -1,
        "filename": "deepspeed_mii-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b6f284a4178eea09d608ab4265fb46aa",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 36778,
        "upload_time": "2022-10-24T19:47:11",
        "upload_time_iso_8601": "2022-10-24T19:47:11.853048Z",
        "url": "https://files.pythonhosted.org/packages/cb/d9/4ceb951dfc47d9fd0949d196cb5a2596dddc53ab12b16470705a7788a171/deepspeed_mii-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7a199d42afd2d2a4c038228289eb7c5cb1b1f0437f0ce424806125644e2e6156",
          "md5": "2d06b8786aebc8fb42051e6d40fe1a55",
          "sha256": "8d0f2319c5974dbeb8c9fde01d117699aeae727d20a4d6b94a7551a160d2fa5a"
        },
        "downloads": -1,
        "filename": "deepspeed_mii-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2d06b8786aebc8fb42051e6d40fe1a55",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 44112,
        "upload_time": "2022-11-14T17:55:47",
        "upload_time_iso_8601": "2022-11-14T17:55:47.049970Z",
        "url": "https://files.pythonhosted.org/packages/7a/19/9d42afd2d2a4c038228289eb7c5cb1b1f0437f0ce424806125644e2e6156/deepspeed_mii-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "af1ccef5266dab26e8adb1ab696b2f5498a2d9c79e944388057593075e7b4a4b",
          "md5": "33f7e33cbdad3f7764ff29c3cffe1639",
          "sha256": "8c8c8e3a01c140532ea9806fcc777be059735c225b1e186ec397d57b732ed788"
        },
        "downloads": -1,
        "filename": "deepspeed_mii-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "33f7e33cbdad3f7764ff29c3cffe1639",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 44942,
        "upload_time": "2022-12-01T23:54:59",
        "upload_time_iso_8601": "2022-12-01T23:54:59.084629Z",
        "url": "https://files.pythonhosted.org/packages/af/1c/cef5266dab26e8adb1ab696b2f5498a2d9c79e944388057593075e7b4a4b/deepspeed_mii-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "af1ccef5266dab26e8adb1ab696b2f5498a2d9c79e944388057593075e7b4a4b",
        "md5": "33f7e33cbdad3f7764ff29c3cffe1639",
        "sha256": "8c8c8e3a01c140532ea9806fcc777be059735c225b1e186ec397d57b732ed788"
      },
      "downloads": -1,
      "filename": "deepspeed_mii-0.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "33f7e33cbdad3f7764ff29c3cffe1639",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 44942,
      "upload_time": "2022-12-01T23:54:59",
      "upload_time_iso_8601": "2022-12-01T23:54:59.084629Z",
      "url": "https://files.pythonhosted.org/packages/af/1c/cef5266dab26e8adb1ab696b2f5498a2d9c79e944388057593075e7b4a4b/deepspeed_mii-0.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}