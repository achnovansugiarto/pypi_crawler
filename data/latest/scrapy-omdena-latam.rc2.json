{
  "info": {
    "author": "Martin Hadid",
    "author_email": "martinhadid@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Scrapy Tool for Omdena Latam LFR Challenge \nWeb Crawling application running *Scrapy* Tool, extracting official policies from the following sources:\n\n### Characteristics of the information sources\n\n### Chile (LeyChile)\n\n**Search type:** Exhaustive, through API , limited by the pages_num\n\n**Speed:** Fast\n\n**Amount of avaliable documents:** 10-100k\n\n**Document Type:** HTML\n\n\n### Mexico Distrito oficial de la Federaci√≥n \n**Search type:** Exhaustive, through scrapping (Xpath) , limited by years range.\n\n**Speed:**  Terrible slow and buggy when you change pipelines order\n\n**Amount of avaliable documents:** 10-100k\n\n**Document Type:** HTML\n\n### El Peruano\n**Search type:**\n\n**Speed:**\n\n**Amount of avaliable documents:**\n\n**Document Type:**\n\n\n\n\n\n# Setup Steps:\n## Recommendations:\nUse a virtual environment not your python system to run and also to install the dependencies.\n\n## Install dependencies\n\n```\npip install -r requirements.txt\n```\n\n\n\n## Scrapy settings.py\nhttps://drive.google.com/file/d/1bjbjYSXQqZQpJdwATRCLZFSRQciiULy-/view?usp=sharing\n### Warning!\nS3 upload pipeline and MySQL insert pipeline doesn't work together.\nUse:\n```\nITEM_PIPELINES = {\n    # 'scrapy.pipelines.files.FilesPipeline': 100,\n    'scrapy_official_newspapers.pipelines.ScrapyOfficialNewspapersMySQLPipeline': 200,\n        }\n```\n\nand \n\n```\nITEM_PIPELINES = {\n     'scrapy.pipelines.files.FilesPipeline': 100,\n    #'scrapy_official_newspapers.pipelines.ScrapyOfficialNewspapersMySQLPipeline': 200,\n        }\n```\nThe order doesn't mind.\n\n## Database access\nSetup the DB access inserting settings.json into *scrappy_official_newspapers*.\n\n```\n{\n  \"username\": \"username\",\n  \"password\": \"password\",\n  \"db_name\": \"db_name\",\n  \"aws_endpoint\": \"your_db_instance_access\"\n}\n```\n\n\n## S3 Access\nSetup scrapy *settings.py* located at *scrappy_official_newspapers*\n```\nAWS_ACCESS_KEY_ID = \"XXXXXXXXXXXXXXXXXXXX\"\nAWS_SECRET_ACCESS_KEY = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\nFILES_STORE = 's3://wri-latin-test/'\n```\n\n\n## Run\nfrom repository root:\n* cd scrapy_official_newspapers\n* scrapy crawl leychile\n* scrapy crawl MexicoDOF\n\n## Monitorization/Debug/Test\nThrough mysql table inspection, you can check how the information is being inserted.\n\nThrough https://console.aws.amazon.com/console/home after authenticating you can navigate to S3 Service, you can check the files uploaded and their properties.\n\n## Goal\nProvide a structured information system, from multiple variated sources formats containing raw official policy documents, keeping the reference to their attributes.\n\n## Attributes\ncountry\n\ngeo_code\n\nlevel\n\nsource\n\ntitle\n\nreference\n\nauthorship\n\nresume\n\npublication_date\n\nenforcement_date\nurl\n\ndoc_url\n\ndoc_name\n\ndoc_type\n\nfile_urls (Needed for Scrappy S3FilesPipeline, Not in DB Schema)\n\nNot implemented like that, but the idea is to also keep track of:\n* file_raw_S3_url\n* file_proccesed_task_1_S3_url \n\nSo we are able to keep track of the policies with their attributes and also have the capability of maintaining the relationships and results of different processes.\n\n## What is this information system:\nPolicies Documents Stored and indexed through the integration of a relational MySQL database and AWS S3 media database service (FTP would work too), with most attributes such as the title of the act, a resume of the document, the date of publication... \n\n\n## Documentation:\nhttps://docs.scrapy.org/en/2.2/\n\nhttps://docs.scrapy.org/en/2.2/topics/media-pipeline.html#enabling-your-media-pipeline\n\n## Recomendations:\nDo not make an extensive use of the tool, probably information will be thrown because it is still in development, you can collapse the target's resources and engraving amazon's bill through this.\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://pypi.org/project/scrapy_omdena_latam/",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/frapercan/scrapy_omdena_latam",
    "keywords": "scrapy,scraper,omdena",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-omdena-latam",
    "package_url": "https://pypi.org/project/scrapy-omdena-latam/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-omdena-latam/",
    "project_urls": {
      "Download": "https://pypi.org/project/scrapy_omdena_latam/",
      "Homepage": "https://github.com/frapercan/scrapy_omdena_latam"
    },
    "release_url": "https://pypi.org/project/scrapy-omdena-latam/0.0.2/",
    "requires_dist": [
      "attrs (==20.2.0)",
      "Automat (==20.2.0)",
      "boto3 (==1.15.16)",
      "botocore (==1.18.16)",
      "cffi (==1.14.3)",
      "constantly (==15.1.0)",
      "cryptography (==3.1.1)",
      "cssselect (==1.1.0)",
      "dateparser (==0.7.6)",
      "hyperlink (==20.0.1)",
      "idna (==2.10)",
      "incremental (==17.5.0)",
      "itemadapter (==0.1.1)",
      "itemloaders (==1.0.3)",
      "jmespath (==0.10.0)",
      "lxml (==4.5.2)",
      "numpy (==1.19.2)",
      "pandas (==1.1.3)",
      "parsel (==1.6.0)",
      "pkg-resources (==0.0.0)",
      "Protego (==0.1.16)",
      "pyasn1 (==0.4.8)",
      "pyasn1-modules (==0.2.8)",
      "pycparser (==2.20)",
      "PyDispatcher (==2.0.5)",
      "PyHamcrest (==2.0.2)",
      "PyMySQL (==0.10.1)",
      "pyOpenSSL (==19.1.0)",
      "python-dateutil (==2.8.1)",
      "pytz (==2020.1)",
      "queuelib (==1.5.0)",
      "regex (==2020.10.11)",
      "s3transfer (==0.3.3)",
      "Scrapy (==2.4.0)",
      "service-identity (==18.1.0)",
      "six (==1.15.0)",
      "SQLAlchemy (==1.3.19)",
      "Twisted (==20.3.0)",
      "tzlocal (==2.1)",
      "urllib3 (==1.25.10)",
      "w3lib (==1.22.0)",
      "zope.interface (==5.1.2)"
    ],
    "requires_python": "",
    "summary": "Web Crawling application running Scrapy Tool extracting official policies",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8417811,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "38efd9d32b185f421e1bfbc2b2312791df238ea62d8c04a95a29eea4cf6b1209",
          "md5": "7046a9d7d118466b4f91a6089286e63f",
          "sha256": "ed7bd31d5a816ba9dded4dfee341497744b4024ba626222ecbc9be77b15bfa0a"
        },
        "downloads": -1,
        "filename": "scrapy_omdena_latam-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7046a9d7d118466b4f91a6089286e63f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 24046,
        "upload_time": "2020-10-15T09:53:39",
        "upload_time_iso_8601": "2020-10-15T09:53:39.350553Z",
        "url": "https://files.pythonhosted.org/packages/38/ef/d9d32b185f421e1bfbc2b2312791df238ea62d8c04a95a29eea4cf6b1209/scrapy_omdena_latam-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dcaf307e6fc904c4814bb6d115be7fe45a0c9647acc7386c66a5c177d6d3e531",
          "md5": "8ac1284682fd7d66cc0b503ca017edc1",
          "sha256": "277cba6faee9b475a19c97a1a6d1f0ec9c33af9f9e7071e61ba37b4d64e6ed01"
        },
        "downloads": -1,
        "filename": "scrapy_omdena_latam-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "8ac1284682fd7d66cc0b503ca017edc1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9895,
        "upload_time": "2020-10-15T09:53:42",
        "upload_time_iso_8601": "2020-10-15T09:53:42.147939Z",
        "url": "https://files.pythonhosted.org/packages/dc/af/307e6fc904c4814bb6d115be7fe45a0c9647acc7386c66a5c177d6d3e531/scrapy_omdena_latam-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2c0711295b948c6a64de249c0313b8cf273afb128fd62ffb1e1d4956ed4d8efb",
          "md5": "3809abe7859a3c4c4ba0a2f1f6f38784",
          "sha256": "1060dda844bf8d1d3e97b5722f7ed188d678d08afe3414562740842631f9fdf3"
        },
        "downloads": -1,
        "filename": "scrapy_omdena_latam-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3809abe7859a3c4c4ba0a2f1f6f38784",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 24026,
        "upload_time": "2020-10-15T09:58:58",
        "upload_time_iso_8601": "2020-10-15T09:58:58.902341Z",
        "url": "https://files.pythonhosted.org/packages/2c/07/11295b948c6a64de249c0313b8cf273afb128fd62ffb1e1d4956ed4d8efb/scrapy_omdena_latam-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8b6525d4f44e82096334dd09f44812c3becea3009a7c6752b0453fcb2057842c",
          "md5": "9150ade6a2039691719ddce876d5ad02",
          "sha256": "547ad77b8e0764a7ef6e3f62ab7b59a69110d1a91fd5b69ce36121dcf31eccb4"
        },
        "downloads": -1,
        "filename": "scrapy_omdena_latam-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "9150ade6a2039691719ddce876d5ad02",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9879,
        "upload_time": "2020-10-15T09:59:00",
        "upload_time_iso_8601": "2020-10-15T09:59:00.275268Z",
        "url": "https://files.pythonhosted.org/packages/8b/65/25d4f44e82096334dd09f44812c3becea3009a7c6752b0453fcb2057842c/scrapy_omdena_latam-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2c0711295b948c6a64de249c0313b8cf273afb128fd62ffb1e1d4956ed4d8efb",
        "md5": "3809abe7859a3c4c4ba0a2f1f6f38784",
        "sha256": "1060dda844bf8d1d3e97b5722f7ed188d678d08afe3414562740842631f9fdf3"
      },
      "downloads": -1,
      "filename": "scrapy_omdena_latam-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3809abe7859a3c4c4ba0a2f1f6f38784",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 24026,
      "upload_time": "2020-10-15T09:58:58",
      "upload_time_iso_8601": "2020-10-15T09:58:58.902341Z",
      "url": "https://files.pythonhosted.org/packages/2c/07/11295b948c6a64de249c0313b8cf273afb128fd62ffb1e1d4956ed4d8efb/scrapy_omdena_latam-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8b6525d4f44e82096334dd09f44812c3becea3009a7c6752b0453fcb2057842c",
        "md5": "9150ade6a2039691719ddce876d5ad02",
        "sha256": "547ad77b8e0764a7ef6e3f62ab7b59a69110d1a91fd5b69ce36121dcf31eccb4"
      },
      "downloads": -1,
      "filename": "scrapy_omdena_latam-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "9150ade6a2039691719ddce876d5ad02",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 9879,
      "upload_time": "2020-10-15T09:59:00",
      "upload_time_iso_8601": "2020-10-15T09:59:00.275268Z",
      "url": "https://files.pythonhosted.org/packages/8b/65/25d4f44e82096334dd09f44812c3becea3009a7c6752b0453fcb2057842c/scrapy_omdena_latam-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}