{
  "info": {
    "author": "giganticode",
    "author_email": "hlibbabii@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Environment :: Console",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Operating System :: MacOS :: MacOS X",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3.7",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "## **langmodels**\n\n[![Build Status](https://travis-ci.org/giganticode/langmodels.svg?branch=master)](https://travis-ci.org/giganticode/langmodels)\n\nThis is a repository for **neural language models (LMs)** trained on a large corpus of source code \nand a toolkit to work with such models. \n\nYou could be interested in using this library if you want to:\n* Use existing pre-trained models for tasks such as autocompletion and bug prediction;\n* Use the pre-trained models for transfer transfer learning or further fine-tuning;\n* Train a model from scratch by choosing one of the wide range of corpus preprocessing choices, \n neural network (NN) architectures, and training options.\n\nThis project uses [fastai](https://www.fast.ai) and \n[pytorch](https://pytorch.org) libraries for NN training/inference. \nFor corpus preprocessing [giganticode-dataprep](https://github.com/giganticode/dataprep) is used.\n\n## Quick start\n\n### Prerequisites\n\n* Python version >= 3.6 required! \n\n### Installation\n\n```shell script\npip install giganticode-langmodels\n```\n\nOR to build from source:\n\n```\ngit clone https://github.com//giganticode/langmodels\ncd langmodels\npython -m venv langmodels-venv\nsource langmodels-venv/bin/activate\npip install -r requirements.txt\n```\n\n## Using existing pre-trained models\n### Loading a default pre-trained model\n```python\n>>> import langmodels.repository as repo\n>>> trained_model = repo.load_default_model()\n\n[langmodels.repository] INFO: Model is not found in cache. Downloading from https://www.inf.unibz.it/~hbabii/pretrained_models/langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-split_10k_2_1024_191022.141344 ...\n[langmodels.model] DEBUG: Loading model from: /home/hlib/.local/share/langmodels/0.0.1/modelzoo/langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-split_10k_2_1024_191022.141344/best.pth ...\n[langmodels.model] DEBUG: Using GPU for inference\n```\n\n### Other model loading options\n\n**To see which models are available, you can call `list_pretrained_models` function.**\n\nSet `cached` parameter to `True` (default is `False`) to display only cached LMs (e.g. if offline).\n```python\n>>> import langmodels.repository as repo\n>>> repo.list_pretrained_models(cached=False)\n\n  ID                                                                    BPE_MERGES  LAYERS_CONFIG  ARCH      BIN_ENTROPY    TRAINING_TIME_MINUTES_PER_EPOCH  N_EPOCHS  BEST_EPOCH  TAGS                 \n\n  langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-spl  10k         1024/2/1024    AWD_LSTM  2.1455788479   1429                             6         5           ['BEST', 'DEFAULT']  \n  it_10k_2_1024_191022.141344                                                                                                                                                                           \n  langmodel-large-split_10k_3_1024_191007.112257_-_langmodel-large-spl  10k         512/3/1024     AWD_LSTM  2.14730056622  1432                             6         5           []                   \n  it_10k_3_1024_191022.134822                                                                                                                                                                           \n  langmodel-large-split_10k_2_2048_191007.112249_-_langmodel-large-spl  10k         512/2/2048     GRU       2.19923468325  1429                             6         5           []                   \n  it_10k_2_2048_191022.141335                                                                                                                                                                           \n  langmodel-large-split_10k_1_512_190926.120146                         10k         512/1/512      AWD_LSTM  2.69019493253  479                              9         8           ['MEDIUM']           \n  langmodel-small-split_10k_1_512_190906.154943                         10k         512/1/512      AWD_LSTM  4.73768141172  4                                19        18          ['TINY']             \n  dev_10k_1_10_190923.132328                                            10k         10/1/10        AWD_LSTM  9.15688191092  0                                0         -1          ['RANDOM']\n```\n\nUse `query_all_models` method to get a list of `ModelDescription` objects\n```python\n>>> import langmodels.repository as repo\n>>> repo.query_all_models()[0]\nModelDescription(id='langmodel-large-split_10k_2_1024_191007.112241_-_langmodel-large-split_10k_2_1024_191022.141344', bpe_merges='10k', layers_config='1024/2/1024', arch='AWD_LSTM', bin_entropy=2.1455788479, training_time_minutes_per_epoch=1429, n_epochs=6, best_epoch=5, tags=['BEST', 'DEFAULT'])\n```\n\n**A model can be loaded by tag or by id.**\n\nYou can specify if you want to load a model to CPU despite having cuda-supported GPU with `force_use_cpu` parameter \n(defaults to `False`). If cuda-supported GPU is not available, this parameter is disregarded.\n```python\n>>> trained_model = repo.load_model_with_tag('BEST')\n\n>>> trained_model = repo.load_model_by_id('dev_10k_1_10_190923.132328_new', force_use_cpu=True)\n```\n\nAlso, you can use a lower-level API to load a model by path :\n```python\ntrained_model = repo.load_from_path('/home/hlib/.local/share/langmodels/0.0.1/modelzoo/dev_10k_1_10_190923.132328_new')\n```\n\n## Inference\n### Autocompletion\n\nExample\n\n```python\n>>> import langmodels.repository as repo\n>>> trained_model = repo.load_default_model()\n>>> trained_model.feed_text('public static main() { if', extension='java')\n\n# this does not change the state of the model:\n>>> predictions = trained_model.predict_next_full_token(n_suggestions=5)\n[('(', 0.9334765834402862), ('.', 0.01540983953864937), ('=', 0.008939018331858162), (',', 0.005372771784601065), ('the', 0.00309070517292041)]\n\n# adding more context:\n>>> trained_model.feed_text('(', extension='java')\n>>> trained_model.predict_next_full_token(n_suggestions=3)\n[('(', 0.14554535082422237), ('c', 0.018005003646104294), ('!', 0.01614662429123089)]\n\n\n# resetting the state of the model (make it forget the context)\n>>> trained_model.reset()\n>>> trained_model.predict_next_full_token(n_suggestions=5)\n[('/', 0.7209196484717589), ('package', 0.27093282656897594), ('import', 0.0007366385365522241), ('.', 0.0005714365190590807), ('public', 0.0003926736567296)]\n\n```\n\n\n### Bug prediction based on per-line entropies evaluation\n\nAn LM can be used to calculate cross-entropies for each line of a file. High values can give an idea about \nunusual/suspicious chunks of code [[1]](#1).\n\nCheck section [LM Evaluation](#lm-evaluation) section to learn how to calculate \ncross-entropy for a project/file/string,\n\nCheck our [vsc plugin](https://github.com/giganticode/vsc-extension) for highlighting suspicious code.\n\n## Fine-tuning and Transfer learning\n\n**TBD**\n\n## Training from scratch (Not supported on OSx)\n\n### Python API\n\n```python\n>>> from langmodels.training.training import train\n>>> from langmodels.lmconfig.datamodel import *\n\n>>> train(LMTrainingConfig(corpus=Corpus(path='/path/to/the/dataset')))\n```\n\nMore parameters to customize corpus pre-processing, NN architecture, and the training process can be specified:\n\n```python\n>>> from langmodels.training.training import train\n>>> from langmodels.lmconfig.datamodel import *\n\n>>> train(LMTrainingConfig(corpus=Corpus(path='/path/to/the/dataset'), \n                            prep_function=PrepFunction(options=PrepFunctionOptions(no_com=False, no_unicode=True)),\n                            arch=GRUArchj(n_layers=2),\n                            training=Training(weight_decay=5e-6)\n))\n```\n\nBelow you can see all the default parameters specified explicitly:\n\n```python\n>>> from langmodels.lmconfig.datamodel import *\n>>> from langmodels.training.training import train\n\n>>> train(LMTrainingConfig(base_model=None, \n                       bs=32, \n                       corpus=Corpus(path=os.path.join(HOME, 'dataset'), extensions=\"java\"), \n                       prep_function=PrepFunction(corpus_api.bpe, ['10k'], \n                                                  PrepFunctionOptions(no_com=False, no_unicode=True, \n                                                                    no_spaces=True, max_str_length=sys.maxsize)), \n                       arch=LstmArch(\n                           bidir=False, qrnn=False, emb_sz=1024, n_hid=1024, n_layers=3, \n                           drop=Dropouts(multiplier=0.5, oute=0.02, outi=0.25, outh=0.15, w=0.2, out=0.1), \n                           tie_weights=True, out_bias=True), \n                       bptt=200, \n                       training=Training(\n                            optimizer=Adam(betas=(0.9, 0.99)),\n                            files_per_epoch=50000,\n                            gradient_clip=0.3,\n                            activation_regularization=ActivationRegularization(alpha=2., beta=1.), \n                            schedule=RafaelsTrainingSchedule(init_lr=1e-4, mult_coeff=0.5, patience=0,\n                                                            max_epochs=50, max_lr_reduction_times=6), \n                            weight_decay=1e-6)\n                       )\n      )\n```\n\n### CLI API\n\nTraining can be run from command line as simple as running `train` command passing path to the config in json format \nas `--config` param. To override values in the json file (or default values if `--config` param is not specified), \nyou can use `--patch` param.\n```shell script\n>> langmodels train --config=\"/path/to/json/config.json\" --patch=\"bs=64,arch.drop.multiplier=3.0\"\n```\n\nIf neither `--config` nor `--patch` params are specified, the training will be running with the default parameters.\nThe json with the default parameters would look like follows:\n\n```json\n{'arch': {'bidir': False,\n          'drop': {'multiplier': 0.5,\n                   'out': 0.1,\n                   'oute': 0.02,\n                   'outh': 0.15,\n                   'outi': 0.25,\n                   'w': 0.2},\n          'emb_sz': 1024,\n          'n_hid': 1024,\n          'n_layers': 3,\n          'name': 'lstm',\n          'out_bias': True,\n          'qrnn': False,\n          'tie_weights': True},\n 'base_model': None,\n 'bptt': 200,\n 'bs': 32,\n 'config_version': '0.0.3-alpha.0',\n 'corpus': {'extensions': 'java', 'path': '/Users/hlib/dataset'},\n 'prep_function': {'callable': 'bpe',\n                   'options': {'max_str_length': 9223372036854775807,\n                               'no_com': False,\n                               'no_spaces': True,\n                               'no_str': False,\n                               'no_unicode': True},\n                   'params': ['10k']},\n 'training': {'activation_regularization': {'alpha': 2.0, 'beta': 1.0},\n              'files_per_epoch': 50000,\n              'gradient_clip': 0.3,\n              'optimizer': {'betas': [0.9, 0.99], 'name': 'Adam'},\n              'schedule': {'init_lr': 0.0001,\n                           'max_epochs': 50,\n                           'max_lr_reduction_times': 6,\n                           'mult_coeff': 0.5,\n                           'name': 'rafael',\n                           'patience': 0},\n              'weight_decay': 1e-06}}\n```\n\nMost probably, you would have to override at least the `corpus.path` value.\n\nFor more options, run:\n```shell script\n>> langmodels train --help\n```\n\n## LM Evaluation\n\nWhen training a language model, it is important to be able to evaluate LM's performance.\nIn this section we describe different ways to do this using `langmodels` library. \nYou can also use our [tool](https://github.com/giganticode/lm-powered) to visualize the evaluation.\n\n### Evaluation on a string / file\n\nFirst, a model can be evaluate on a string with `evaluate_model_on_string` method. Note that the result may differ a lot depending \non the state of the model. Use methods `reset` and `feed_text` to reset the model \nto initial state and change the context of the model respectively.\n\n```python\n\n>>> import langmodels.repository as repo \n>>> from langmodels.evaluation import evaluate_model_on_string    \n\n>>> model = repo.load_default_model()\n>>> evaluate_model_on_string(model, 'public class MyClass {')\n\n{full_token_entropy/ParsedToken: EvaluationResult(\n    tokens=['public</t>', 'class</t>', 'MyClass</t>', '{</t>'],\n    token_types=['KeyWord', 'KeyWord', 'SplitContainer', 'OpeningCurlyBracket'],\n    values=[1.8144783973693848, 3.668722629547119, 0.5620064437389374, 0.2571456730365753], \n    aggregated_value=1.5755882859230042\n)}\n\n```\n\nSimilarly, `evaluate_model_on_file` will return a list of `Evaluation` object (1 per each line)\n\n### Evaluation on a corpus\n\nEvaluation can be run on a set of files with `evaluate_model_on_path` method\n\n```python\n>>> import langmodels.repository as repo \n>>> from langmodels.evaluation import evaluate_model_on_path\n\n>>> model = repo.load_default_model()\n>>> evaluate_model_on_path(model, '/path/to/file')\n\n100%|████████████████████████████████████████████████████████████████████████████| 28/28 [00:11<00:00,  2.35it/s]\n{full_token_entropy/ParsedToken: (5.859160765187885, 5745)}\n```\n\nIn `full_token_entropy/ParsedToken`: `full_token_entropy` is a metric used to evaluate the performance; \n`ParsedToken` means that all the tokens were considered when evaluating (See the next section for more details).\nThus, the average full-token-entropy is ~ 5.85 evaluated on 5.7k tokens.\n\n### Specifying metrics\n\nYou can specify based on which metrics the model is to be evaluated.\n\n```python\n>>> import langmodels.repository as repo \n>>> from langmodels.evaluation import evaluate_model_on_path\n\n>>> model = repo.load_default_model()\n>>> evaluate_model_on_path(model, '/path/to/file', metrics={'full_token_entropy', 'mrr'})\n```\n\nPossible metric values are `full_token_entropy`, `subtoken_entropy`, `mrr`. Default metric set is `{full_token_entropy}`\n\n\n## Release Notes\n\n### 0.0.4-alpha.0 (NOT backward-compatible with 0.0.1-alpha.2)\n\n- Config datamodel improvements: \n    - Add possibility to specify SGD optimizer; \n    - Add patience param to training scedule;\n    - Add converters between versions of configs;\n- Training:\n    - Report binary entropy instead of log-base-e entropy;\n    - Save more model metrics (size on disk, trainable params, training time per epoch);\n    - Do not save model after every epoch by default;\n- Evaluation improvements:\n    - Return token types in `EvaluationResult`;\n    - Add possibility to specify token types to be considered when running evaluation;\n    - Trained_model.predict_next_token(): return 1 suggestion by default;\n- Add script for new models upload.\n\n### 0.0.1-alpha.2 (NOT backward-compatible with 0.0.1-alpha.1)\n\n- Make downloading model from the repository thread-safe\n- Force to specify the extension which corresponds to the type of the code fed into\nthe `TrainedModel`. **API change**: `trained_model.feed_text(text: str)` -> `trained_model.feed_text(text: str, extension: str)`\n\n### 0.0.1-alpha.1\n\nMake methods of `TrainedModel` that change underlying PyTorch model thread-safe\n\n### 0.0.1-alpha.0\n\nInitial PyPI release\n\n## References\n\n<a id=\"1\">[1]</a> Ray, B., Hellendoorn, V., Godhane, S., Tu, Z., Bacchelli, A., & Devanbu, P. (2016, May). \nOn the\" naturalness\" of buggy code. In 2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE) \n(pp. 428-439). IEEE.\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://github.com/giganticode/langmodels",
    "keywords": "big large data source code corpus machine learning nlp pytorch torch fastai language modeling",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "giganticode-langmodels",
    "package_url": "https://pypi.org/project/giganticode-langmodels/",
    "platform": "",
    "project_url": "https://pypi.org/project/giganticode-langmodels/",
    "project_urls": {
      "Homepage": "http://github.com/giganticode/langmodels"
    },
    "release_url": "https://pypi.org/project/giganticode-langmodels/0.0.4a0/",
    "requires_dist": [
      "fastai (==1.0.57)",
      "giganticode-dataprep (==1.0.0-alpha.12)",
      "future (==0.18.2)",
      "comet-ml (==3.0.2)",
      "flatdict (==3.4.0)",
      "retrying (==1.3.3)",
      "psutil (==5.6.7)",
      "tqdm (==4.39.0)",
      "jsons (==1.0.0)",
      "numpy (==1.17.4)",
      "appdirs (==1.4.3)",
      "Columnar (==1.3.1)",
      "requests (==2.22.0)",
      "pysftp (==0.2.9)",
      "semver (==2.9.0)",
      "jq (==0.1.6)"
    ],
    "requires_python": ">=3.6",
    "summary": "A toolkit for applying machine learning to large source code corpora",
    "version": "0.0.4a0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6488458,
  "releases": {
    "0.0.1a0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7e962fd3204d0165b7b4dab1953e677aaf160e4a11dbfe282089ce0f89f8b597",
          "md5": "76376ce958d1d307a91c4e79a91c42c9",
          "sha256": "3336c05a64c715a0537b97b38cb55a57b54f654dcd364b4ac63a58fe9bff0990"
        },
        "downloads": -1,
        "filename": "giganticode_langmodels-0.0.1a0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "76376ce958d1d307a91c4e79a91c42c9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 49341,
        "upload_time": "2019-12-02T13:14:21",
        "upload_time_iso_8601": "2019-12-02T13:14:21.463658Z",
        "url": "https://files.pythonhosted.org/packages/7e/96/2fd3204d0165b7b4dab1953e677aaf160e4a11dbfe282089ce0f89f8b597/giganticode_langmodels-0.0.1a0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a64840af4f56153451167b1f340693adab09fe73ebc37077824e6133405cca48",
          "md5": "49fb88180f4431682b54816cbcee1ba1",
          "sha256": "a8e40e20b10bcdf2684a7d50b046a5d8371da98cd7183585e9bf1be555ae8de0"
        },
        "downloads": -1,
        "filename": "giganticode-langmodels-0.0.1a0.tar.gz",
        "has_sig": false,
        "md5_digest": "49fb88180f4431682b54816cbcee1ba1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 46635,
        "upload_time": "2019-12-02T13:14:24",
        "upload_time_iso_8601": "2019-12-02T13:14:24.641028Z",
        "url": "https://files.pythonhosted.org/packages/a6/48/40af4f56153451167b1f340693adab09fe73ebc37077824e6133405cca48/giganticode-langmodels-0.0.1a0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1a1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7d9e6a872662a2883505bc06529035ff277ed5d1d2b2d5b21126ff88f27723fd",
          "md5": "f8b8a0a4c41426f00b70fed1b89118a8",
          "sha256": "0740f49611c6f4c95d807b36df99aff2c60bec4d1b4f36ed084fb42d9730bb2f"
        },
        "downloads": -1,
        "filename": "giganticode_langmodels-0.0.1a1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f8b8a0a4c41426f00b70fed1b89118a8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 49433,
        "upload_time": "2019-12-03T23:01:54",
        "upload_time_iso_8601": "2019-12-03T23:01:54.626744Z",
        "url": "https://files.pythonhosted.org/packages/7d/9e/6a872662a2883505bc06529035ff277ed5d1d2b2d5b21126ff88f27723fd/giganticode_langmodels-0.0.1a1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a66d315710625b73b86f6241df6251f97c0bea23ed3b57b78f8684583f672cac",
          "md5": "1d9c7c37cacf03e4f04aac46ad0da391",
          "sha256": "ec425d47f32a0f0bc85eaf41dadc5002997c22c5291cce33564ce7a3b3707faf"
        },
        "downloads": -1,
        "filename": "giganticode-langmodels-0.0.1a1.tar.gz",
        "has_sig": false,
        "md5_digest": "1d9c7c37cacf03e4f04aac46ad0da391",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 46795,
        "upload_time": "2019-12-03T23:01:56",
        "upload_time_iso_8601": "2019-12-03T23:01:56.860272Z",
        "url": "https://files.pythonhosted.org/packages/a6/6d/315710625b73b86f6241df6251f97c0bea23ed3b57b78f8684583f672cac/giganticode-langmodels-0.0.1a1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1a2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d76f4c20417fb11030a192313336ee27e163beb3de92f94615c6fa1d87b979ad",
          "md5": "cf559e6355b0d31d411f812f84b08a9c",
          "sha256": "c4d5f4966d5a19eb58fe85f00bb1c4ed066fe03bb557cdd7c1337c4b78b3bb08"
        },
        "downloads": -1,
        "filename": "giganticode_langmodels-0.0.1a2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cf559e6355b0d31d411f812f84b08a9c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 49640,
        "upload_time": "2019-12-04T09:50:45",
        "upload_time_iso_8601": "2019-12-04T09:50:45.575401Z",
        "url": "https://files.pythonhosted.org/packages/d7/6f/4c20417fb11030a192313336ee27e163beb3de92f94615c6fa1d87b979ad/giganticode_langmodels-0.0.1a2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9a77d4615817ee850f7e1c6572505e3d33b854b8334fc61c57430cb8e6d67154",
          "md5": "352e89e2e6c31eba1cd689542d432394",
          "sha256": "5c55ac81ea2a9afcfa3e72ffa1249efa4753d3af06e0e289e99e441cd7057e6a"
        },
        "downloads": -1,
        "filename": "giganticode-langmodels-0.0.1a2.tar.gz",
        "has_sig": false,
        "md5_digest": "352e89e2e6c31eba1cd689542d432394",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 47293,
        "upload_time": "2019-12-04T09:50:47",
        "upload_time_iso_8601": "2019-12-04T09:50:47.597189Z",
        "url": "https://files.pythonhosted.org/packages/9a/77/d4615817ee850f7e1c6572505e3d33b854b8334fc61c57430cb8e6d67154/giganticode-langmodels-0.0.1a2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3a0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "14798d2cb381a82e3bd284c0d6319c31cd0a7b4ec329377122a34635814d5f10",
          "md5": "567a49b9f9abf09da9e1a80a3efefac4",
          "sha256": "eb8f1e93aed514e3ee5b050e86af13e0470a91be427d47305ac2a170d0dfca2e"
        },
        "downloads": -1,
        "filename": "giganticode_langmodels-0.0.3a0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "567a49b9f9abf09da9e1a80a3efefac4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 57776,
        "upload_time": "2020-01-20T15:29:46",
        "upload_time_iso_8601": "2020-01-20T15:29:46.249582Z",
        "url": "https://files.pythonhosted.org/packages/14/79/8d2cb381a82e3bd284c0d6319c31cd0a7b4ec329377122a34635814d5f10/giganticode_langmodels-0.0.3a0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d1b1913cd31cef02408c85bbbfc00537fd9697fdcbcb4810031ac5d4114c28a2",
          "md5": "4b93e6353fdffdb3552014e42cf47bbd",
          "sha256": "ae55d63d0ff3aae5f969a8fc4e224cf0bb450528a9a67a01385f7f9025055318"
        },
        "downloads": -1,
        "filename": "giganticode-langmodels-0.0.3a0.tar.gz",
        "has_sig": false,
        "md5_digest": "4b93e6353fdffdb3552014e42cf47bbd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 52885,
        "upload_time": "2020-01-20T15:29:48",
        "upload_time_iso_8601": "2020-01-20T15:29:48.786954Z",
        "url": "https://files.pythonhosted.org/packages/d1/b1/913cd31cef02408c85bbbfc00537fd9697fdcbcb4810031ac5d4114c28a2/giganticode-langmodels-0.0.3a0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4a0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0744a063861a46f59bfd207a5d0baa8f75311a8d09ae60407f55bacdf602bc84",
          "md5": "4255d039fa2696171047b32f6b1adc85",
          "sha256": "a5a2d6c8c0f443a097452d2e4583848007e9f08299b8bf56fc6c9b22dbc75b44"
        },
        "downloads": -1,
        "filename": "giganticode_langmodels-0.0.4a0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4255d039fa2696171047b32f6b1adc85",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 57746,
        "upload_time": "2020-01-20T16:19:54",
        "upload_time_iso_8601": "2020-01-20T16:19:54.060088Z",
        "url": "https://files.pythonhosted.org/packages/07/44/a063861a46f59bfd207a5d0baa8f75311a8d09ae60407f55bacdf602bc84/giganticode_langmodels-0.0.4a0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e9212b9932a6623ca1e4e65cc6abdfdfde6e938dd940a2d24509dd6168165dfb",
          "md5": "56b7c6fb0a95b399a04a807364bb18ff",
          "sha256": "054b496cfd5a27766f0b1cfe6caa90374cbd2b40fa95184adcf9f57f091de7fe"
        },
        "downloads": -1,
        "filename": "giganticode-langmodels-0.0.4a0.tar.gz",
        "has_sig": false,
        "md5_digest": "56b7c6fb0a95b399a04a807364bb18ff",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 52847,
        "upload_time": "2020-01-20T16:19:56",
        "upload_time_iso_8601": "2020-01-20T16:19:56.386649Z",
        "url": "https://files.pythonhosted.org/packages/e9/21/2b9932a6623ca1e4e65cc6abdfdfde6e938dd940a2d24509dd6168165dfb/giganticode-langmodels-0.0.4a0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0744a063861a46f59bfd207a5d0baa8f75311a8d09ae60407f55bacdf602bc84",
        "md5": "4255d039fa2696171047b32f6b1adc85",
        "sha256": "a5a2d6c8c0f443a097452d2e4583848007e9f08299b8bf56fc6c9b22dbc75b44"
      },
      "downloads": -1,
      "filename": "giganticode_langmodels-0.0.4a0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4255d039fa2696171047b32f6b1adc85",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 57746,
      "upload_time": "2020-01-20T16:19:54",
      "upload_time_iso_8601": "2020-01-20T16:19:54.060088Z",
      "url": "https://files.pythonhosted.org/packages/07/44/a063861a46f59bfd207a5d0baa8f75311a8d09ae60407f55bacdf602bc84/giganticode_langmodels-0.0.4a0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e9212b9932a6623ca1e4e65cc6abdfdfde6e938dd940a2d24509dd6168165dfb",
        "md5": "56b7c6fb0a95b399a04a807364bb18ff",
        "sha256": "054b496cfd5a27766f0b1cfe6caa90374cbd2b40fa95184adcf9f57f091de7fe"
      },
      "downloads": -1,
      "filename": "giganticode-langmodels-0.0.4a0.tar.gz",
      "has_sig": false,
      "md5_digest": "56b7c6fb0a95b399a04a807364bb18ff",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 52847,
      "upload_time": "2020-01-20T16:19:56",
      "upload_time_iso_8601": "2020-01-20T16:19:56.386649Z",
      "url": "https://files.pythonhosted.org/packages/e9/21/2b9932a6623ca1e4e65cc6abdfdfde6e938dd940a2d24509dd6168165dfb/giganticode-langmodels-0.0.4a0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}