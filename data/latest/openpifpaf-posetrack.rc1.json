{
  "info": {
    "author": "Sven Kreiss",
    "author_email": "research@svenkreiss.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "[![Tests](https://github.com/vita-epfl/openpifpaf_posetrack/actions/workflows/tests.yml/badge.svg?branch=main)](https://github.com/vita-epfl/openpifpaf_posetrack/actions/workflows/tests.yml)\n\nThis is the tracking plugin for [OpenPifPaf](https://github.com/vita-epfl/openpifpaf).\n\n\n# Install\n\n```sh\npip install 'openpifpaf_posetrack[test,train]'\n\n# from source:\npip install --editable '.[test,train]'\n```\n\n\n# Prediction\n\nThe standard `openpifpaf.video` still works exactly the same way. With this\nplugin installed, you can use `--checkpoint=tshufflenetv2k16` (with a `t` at the\nbeginning). This model can be decoded in multiple ways and you should pick one\ndecoder. To get started, we recommend `--decoder=trackingpose:0`.\nPutting it all together, an example command to process a video stream from a\ncamera is:\n\n```sh\nMPLBACKEND=macosx python3 -m openpifpaf.video --show --long-edge=321 --checkpoint=tshufflenetv2k16 --decoder=trackingpose:0 --source 0 --horizontal-flip\n```\n\n\n# Posetrack Dataset\n\nData. Follow the Posetrack instructions to download and untar the images.\nLabels:\n\n```sh\nmkdir data-posetrack\ncd data-posetrack\nwget https://posetrack.net/posetrack18-data/posetrack18_v0.45_public_labels.tar.gz\ntar -xvf posetrack18_v0.45_public_labels.tar.gz\nmv posetrack_data/* .\nrm -r posetrack_data\n```\n\nGenerate PoseTrack2017 json data of the ground truth.\nUsage of `octave` instead of `matlab` is not documented, but this seems to work:\n\n```sh\ncd matlab\noctave --no-gui --eval \"addpath('./external/jsonlab'); mat2json('your_relative_path/data-posetrack2017/annotations/val/'); quit\"\n```\n\nThis takes a long time. It is faster on the test set:\n\n```sh\noctave --no-gui --eval \"addpath('./external/jsonlab'); mat2json('your_relative_path/data-posetrack2017/annotations/test/'); quit\"\n```\n\nThe Posetrack poses look like these:\n\n![poses](docs/skeleton_overview.png)\n\nCreated with `python -m openpifpaf_posetrack.draw_poses`.\n\n\n# Train posetrack2018-cocokpst\n\n```sh\n# 210226\n\n# first convert from single image to tracking model\npython3 -m openpifpaf_posetrack.imagetotracking --checkpoint shufflenetv2k30\n\n# train\ntime python3 -m torch.distributed.launch --nproc_per_node=4 \\\n  -m openpifpaf.train --ddp \\\n  --lr=0.0003 --momentum=0.95 --b-scale=10.0 \\\n  --epochs=50 --lr-decay 40 45 --lr-decay-epochs=5 \\\n  --batch-size=8 \\\n  --weight-decay=1e-5 \\\n  --dataset=posetrack2018-cocokpst --dataset-weights 1 1 --stride-apply=2 \\\n  --posetrack-upsample=2 \\\n  --cocokp-upsample=2 --cocokp-orientation-invariant=0.1 --cocokp-blur=0.1 \\\n  --checkpoint outputs/tshufflenetv2k30-210217-075056-cocokp-o10s-6f9daa84.pkl\n```\n\n```sh\nCUDA_VISIBLE_DEVICES=3 python -m openpifpaf.eval \\\n  --watch --checkpoint \"outputs/tshufflenetv2k??-20121?-*-posetrack2018-*.pkl.epoch??[0,5]\" \\\n  --dataset=posetrack2018 \\\n  --loader-workers=8 \\\n  --decoder=trackingpose:0 \\\n  --write-predictions\n```\n\nThe training script supports ``--train-annotations`` and ``--val-annotations``\nto restrict the used annotation files. This is useful for local testing.\n\nTo produce submissions to the 2018 test server:\n\n```sh\nCUDA_VISIBLE_DEVICES=0 python -m openpifpaf.eval \\\n  --checkpoint outputs/tshufflenetv2k30-210222-112623-posetrack2018-cocokpst-o10-123ec670.pkl \\\n  --dataset=posetrack2018 --posetrack2018-eval-annotations=\"data-posetrack2018/annotations/test/*.json\" \\\n  --loader-workers=8 \\\n  --decoder=trackingpose:0 \\\n  --write-predictions\n```\n\nFor the 2017 test server:\n\n```sh\nCUDA_VISIBLE_DEVICES=1 python -m openpifpaf.eval \\\n  --checkpoint outputs/tshufflenetv2k30-210222-112623-posetrack2018-cocokpst-o10-123ec670.pkl \\\n  --dataset=posetrack2017 --posetrack2017-eval-annotations=\"data-posetrack2017/annotations/test/*.json\" \\\n  --loader-workers=8 \\\n  --decoder=trackingpose:0 \\\n  --write-predictions\n```\n\n\n## Citation\n\n```\n... TODO ...\n\n@InProceedings{kreiss2019pifpaf,\n  author = {Kreiss, Sven and Bertoni, Lorenzo and Alahi, Alexandre},\n  title = {{PifPaf: Composite Fields for Human Pose Estimation}},\n  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n  month = {June},\n  year = {2019}\n}\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/vita-epfl/openpifpaf_posetrack",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "openpifpaf-posetrack",
    "package_url": "https://pypi.org/project/openpifpaf-posetrack/",
    "platform": "",
    "project_url": "https://pypi.org/project/openpifpaf-posetrack/",
    "project_urls": {
      "Homepage": "https://github.com/vita-epfl/openpifpaf_posetrack"
    },
    "release_url": "https://pypi.org/project/openpifpaf-posetrack/0.3.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "OpenPifPaf plugin for Posetrack",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9619519,
  "releases": {
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ce31860e2bfca3c62c26b0f9c91030c6cbc4d14fd5ead551c95d8d1bd1b161bc",
          "md5": "90d97fe35da1c9f466eff30bf6ab1821",
          "sha256": "f13f80b3b5cb7a691d1499ffd899c9180c8125a13c925cdd83c1aa776fd02e1d"
        },
        "downloads": -1,
        "filename": "openpifpaf_posetrack-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "90d97fe35da1c9f466eff30bf6ab1821",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 53259,
        "upload_time": "2021-03-03T15:13:18",
        "upload_time_iso_8601": "2021-03-03T15:13:18.210799Z",
        "url": "https://files.pythonhosted.org/packages/ce/31/860e2bfca3c62c26b0f9c91030c6cbc4d14fd5ead551c95d8d1bd1b161bc/openpifpaf_posetrack-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ce31860e2bfca3c62c26b0f9c91030c6cbc4d14fd5ead551c95d8d1bd1b161bc",
        "md5": "90d97fe35da1c9f466eff30bf6ab1821",
        "sha256": "f13f80b3b5cb7a691d1499ffd899c9180c8125a13c925cdd83c1aa776fd02e1d"
      },
      "downloads": -1,
      "filename": "openpifpaf_posetrack-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "90d97fe35da1c9f466eff30bf6ab1821",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 53259,
      "upload_time": "2021-03-03T15:13:18",
      "upload_time_iso_8601": "2021-03-03T15:13:18.210799Z",
      "url": "https://files.pythonhosted.org/packages/ce/31/860e2bfca3c62c26b0f9c91030c6cbc4d14fd5ead551c95d8d1bd1b161bc/openpifpaf_posetrack-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}