{
  "info": {
    "author": "Vaibhav Singh",
    "author_email": "vaibhav.singh.3001@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "## Attention mechanisms and Transformers\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/Attention-and-Transformers)](https://www.python.org/) [![TensorFlow](https://img.shields.io/badge/Tensorflow-2.10%20%7C%202.11-orange?logo=tensorflow)](https://github.com/tensorflow/tensorflow/releases/) [![PyPI version](https://badge.fury.io/py/Attention-and-Transformers.svg)](https://badge.fury.io/py/Attention-and-Transformers) [![TensorFlow](https://img.shields.io/badge/TensorFlow-%23FF6F00.svg?style=for-the-badge&logo=TensorFlow&logoColor=white)](https://www.tensorflow.org/)\n\n* This goal of this repository is to host basic architecture and model traning code associated with the different attention mechanisms and transformer architecture.\n* At the moment, I more interested in learning and recreating these new architectures from scratch than full-fledged training. For now, I'll just be training these models on small datasets.\n\n#### Installation\n\n* Using pip to install from [pypi](https://pypi.org/project/Attention-and-Transformers/)\n\n```bash\npip install Attention-and-Transformers\n```\n\n* Using pip to install latest version from github\n\n```bash\npip install git+https://github.com/veb-101/Attention-and-Transformers.git\n```\n\n* Local clone and install\n\n```bash\ngit clone https://github.com/veb-101/Attention-and-Transformers.git atf\ncd atf\npython setup.py install\n```\n\n**Example Use**\n\n```bash\npython load_test.py\n```\n\n**Attention Mechanisms**\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">\n<strong># No.</strong>\n</th>\n<th style=\"text-align:center\">\n<strong>Mechanism</strong>\n</th>\n<th style=\"text-align:center\">\n<strong>Paper</strong>\n</th>\n</tr>\n</thead>\n<tbody>\n\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/ViT/multihead_self_attention.py\">Multi-head Self Attention</a>\n</td>\n<td style=\"text-align:center\">\n<a href=\"https://arxiv.org/abs/1706.03762\">Attention is all you need</a>\n</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v1/multihead_self_attention_2D.py\">Multi-head Self Attention 2D</a>\n</td>\n<td style=\"text-align:center\">\n<a href=\"https://arxiv.org/abs/2110.02178\">MobileViT V1</a>\n</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v2/linear_attention.py\">Separable Self Attention</a>\n</td>\n<td style=\"text-align:center\">\n<a href=\"https://arxiv.org/abs/2206.02680\">MobileViT V2</a>\n</td>\n</tr>\n</tbody>\n</table>\n\n**Transformer Models**\n\n<table>\n<thead>\n<tr>\n<th style=\"text-align:center\">\n<strong># No.</strong>\n</th>\n<th style=\"text-align:center\">\n<strong>Models</strong>\n</th>\n<th style=\"text-align:center\">\n<strong>Paper</strong>\n</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:center\">1</td>\n<td style=\"text-align:center\">\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/ViT/vision_transformer.py\">Vision Transformer</a>\n</td>\n<td style=\"text-align:center\">\n<a href=\"https://arxiv.org/abs/2010.11929\">An Image is Worth 16x16 Words:</a>\n</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">2</td>\n<td style=\"text-align:center\">\n<a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v1/mobile_vit_v1.py\">MobileViT-V1</a>\n</td>\n<td style=\"text-align:center\">\n<a href=\"https://arxiv.org/abs/2110.02178\">MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</a>\n</td>\n</tr>\n<tr>\n<td style=\"text-align:center\">3</td>\n<td style=\"text-align:center\"><a href=\"https://github.com/veb-101/Attention-and-Transformers/blob/main/Attention_and_Transformers/MobileViT_v2/mobile_vit_v2.py\">MobileViT-V2</a></td>\n<td style=\"text-align:center\">\n<a href=\"https://arxiv.org/abs/2206.02680\">Separable Self-attention for Mobile Vision Transformers</a>\n</td>\n</tr>\n</tbody>\n</table>\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/veb-101/Attention-and-Transformers",
    "keywords": "tensorflow keras attention transformers",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "Attention-and-Transformers",
    "package_url": "https://pypi.org/project/Attention-and-Transformers/",
    "platform": null,
    "project_url": "https://pypi.org/project/Attention-and-Transformers/",
    "project_urls": {
      "Homepage": "https://github.com/veb-101/Attention-and-Transformers"
    },
    "release_url": "https://pypi.org/project/Attention-and-Transformers/0.0.15/",
    "requires_dist": [
      "tensorflow-datasets",
      "livelossplot",
      "Pillow",
      "opencv-contrib-python",
      "pandas",
      "scikit-learn",
      "matplotlib",
      "scikit-image",
      "tensorflow-addons ; platform_machine != \"aarch64\" and platform_machine != \"aarch32\"",
      "tensorflow (>=2.10.0) ; platform_system != \"Darwin\"",
      "tensorflow-macos ; platform_system == \"Darwin\""
    ],
    "requires_python": ">=3.7,<3.11.*",
    "summary": "Building attention mechanisms and Transformer models from scratch. Alias ATF.",
    "version": "0.0.15",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16136056,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8ff08046683eb8b6738c8fafa41c8f6ab317d2331000fa5303fd713134b87ee2",
          "md5": "c3613d59a4186f4f13fa17dfbb08999c",
          "sha256": "3035de547414eb7fc292a4e4e3dcb4bc4b57aa8bb544a1cd4e625eb647f23069"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c3613d59a4186f4f13fa17dfbb08999c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9,<3.11.*",
        "size": 8379,
        "upload_time": "2022-11-18T21:02:35",
        "upload_time_iso_8601": "2022-11-18T21:02:35.618551Z",
        "url": "https://files.pythonhosted.org/packages/8f/f0/8046683eb8b6738c8fafa41c8f6ab317d2331000fa5303fd713134b87ee2/Attention_and_Transformers-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f11abb921146cdd95edd31e8b51a35354a246b9f3e141ea614c8c71ffdc2a7d",
          "md5": "c8511ddbbafc8f9bd4819debf82c1bcf",
          "sha256": "34559a79d900db5e40f3056e34a1d733d95b249bbf4f2c3d9df8c4db3e259a56"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c8511ddbbafc8f9bd4819debf82c1bcf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9,<3.11.*",
        "size": 7613,
        "upload_time": "2022-11-18T21:02:37",
        "upload_time_iso_8601": "2022-11-18T21:02:37.262685Z",
        "url": "https://files.pythonhosted.org/packages/5f/11/abb921146cdd95edd31e8b51a35354a246b9f3e141ea614c8c71ffdc2a7d/Attention_and_Transformers-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.10": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8f05b1c5c4a9192fa46062e2ba9ba28ed018366d106914ba6fafcace033128d4",
          "md5": "56e1c802adee0d45897a6dd26dbbebc3",
          "sha256": "d2b07a813a567cf913654ac1bd6d3d5c542a5ce3d262bfbfdec1129a51139fa3"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.10-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "56e1c802adee0d45897a6dd26dbbebc3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<3.11.*",
        "size": 21294,
        "upload_time": "2022-12-16T17:58:18",
        "upload_time_iso_8601": "2022-12-16T17:58:18.699674Z",
        "url": "https://files.pythonhosted.org/packages/8f/05/b1c5c4a9192fa46062e2ba9ba28ed018366d106914ba6fafcace033128d4/Attention_and_Transformers-0.0.10-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9f7a1abf3baa59e239e973debf05df235b3835c012046fef0c1c473cb6cb16bf",
          "md5": "5613f36a8f3b8cacec42db9fd5b1b5c7",
          "sha256": "7b673233332d09e1de052409e4db5f574bdcbacc279b6c07505d9abb5e96244b"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.10.tar.gz",
        "has_sig": false,
        "md5_digest": "5613f36a8f3b8cacec42db9fd5b1b5c7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<3.11.*",
        "size": 13839,
        "upload_time": "2022-12-16T17:58:20",
        "upload_time_iso_8601": "2022-12-16T17:58:20.019680Z",
        "url": "https://files.pythonhosted.org/packages/9f/7a/1abf3baa59e239e973debf05df235b3835c012046fef0c1c473cb6cb16bf/Attention_and_Transformers-0.0.10.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "301fdc2cc3597f3e3b4014c79ec1a3e20cefd6dc0b47174da01333378be7132c",
          "md5": "9cb610c56dfae9eab23f6e6592bcaf99",
          "sha256": "82e988670beb8c8645d322857f2fc30a64f5f54beeea1fba269e6ca773083a31"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.11-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9cb610c56dfae9eab23f6e6592bcaf99",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<3.11.*",
        "size": 21283,
        "upload_time": "2022-12-16T18:00:43",
        "upload_time_iso_8601": "2022-12-16T18:00:43.414742Z",
        "url": "https://files.pythonhosted.org/packages/30/1f/dc2cc3597f3e3b4014c79ec1a3e20cefd6dc0b47174da01333378be7132c/Attention_and_Transformers-0.0.11-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7c103360d9076daca7e49e4107b622cf24d0c3f3397352c4dcf916ebb8a4e72d",
          "md5": "d4a1f8f6e425bd55e10753545c59f7e3",
          "sha256": "ceb3d7abe776eaed46ab66d0eab96c29f83483d5a60c8fbe8cbf8dd6ef30ba2d"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.11.tar.gz",
        "has_sig": false,
        "md5_digest": "d4a1f8f6e425bd55e10753545c59f7e3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<3.11.*",
        "size": 13864,
        "upload_time": "2022-12-16T18:00:44",
        "upload_time_iso_8601": "2022-12-16T18:00:44.667675Z",
        "url": "https://files.pythonhosted.org/packages/7c/10/3360d9076daca7e49e4107b622cf24d0c3f3397352c4dcf916ebb8a4e72d/Attention_and_Transformers-0.0.11.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.13": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5be99bf0c2a047aa23ac273b7475757d392edcb467f4a5990a624ccfb27bd063",
          "md5": "cea6d116e8a4f9e46ae110c42ec04a5e",
          "sha256": "f48381fee0b416b331498405213b27d9b76b7b5d8739fa4bb7f666b5d5a1b47e"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.13-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cea6d116e8a4f9e46ae110c42ec04a5e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<3.11.*",
        "size": 21287,
        "upload_time": "2022-12-16T18:05:58",
        "upload_time_iso_8601": "2022-12-16T18:05:58.881231Z",
        "url": "https://files.pythonhosted.org/packages/5b/e9/9bf0c2a047aa23ac273b7475757d392edcb467f4a5990a624ccfb27bd063/Attention_and_Transformers-0.0.13-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8298be8e4a64717d300409d12bb75c6768db3851960ac1b72732149dccfae29e",
          "md5": "97c440174e5f6e1e9c8264a5b6205498",
          "sha256": "159d38036aa1d465efcf01567ee068409b3b715caeabc3f1bfc232ccbb1d5536"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.13.tar.gz",
        "has_sig": false,
        "md5_digest": "97c440174e5f6e1e9c8264a5b6205498",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<3.11.*",
        "size": 13860,
        "upload_time": "2022-12-16T18:06:01",
        "upload_time_iso_8601": "2022-12-16T18:06:01.214724Z",
        "url": "https://files.pythonhosted.org/packages/82/98/be8e4a64717d300409d12bb75c6768db3851960ac1b72732149dccfae29e/Attention_and_Transformers-0.0.13.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.14": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "13da1c650d093d45772a2883d0465ea6a50bdb16d4142a85520517e1a52c0b6b",
          "md5": "86ae8b919eff9d6268637e9b93bb2001",
          "sha256": "dddce00a839ac0ba029af7aa3d6f761b31880ade165a6c37de7e26a9069111b4"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.14-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "86ae8b919eff9d6268637e9b93bb2001",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<3.11.*",
        "size": 21290,
        "upload_time": "2022-12-16T18:11:57",
        "upload_time_iso_8601": "2022-12-16T18:11:57.887688Z",
        "url": "https://files.pythonhosted.org/packages/13/da/1c650d093d45772a2883d0465ea6a50bdb16d4142a85520517e1a52c0b6b/Attention_and_Transformers-0.0.14-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5704b11bd785472a5ca2f7a1133c04fd8c9421cd7d1207f331c2d2f47598699a",
          "md5": "4a6d9d7feb0cbed48d3e00cc1c8be2aa",
          "sha256": "1c0d495d9576a727cca5b2abc2137a80138de2e10430a566e79cc65dfd96bad5"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.14.tar.gz",
        "has_sig": false,
        "md5_digest": "4a6d9d7feb0cbed48d3e00cc1c8be2aa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<3.11.*",
        "size": 13858,
        "upload_time": "2022-12-16T18:11:59",
        "upload_time_iso_8601": "2022-12-16T18:11:59.422573Z",
        "url": "https://files.pythonhosted.org/packages/57/04/b11bd785472a5ca2f7a1133c04fd8c9421cd7d1207f331c2d2f47598699a/Attention_and_Transformers-0.0.14.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.15": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "862c83acacb0fa37c7e47809d896287e2440ba66682f4f948e423148dcca8482",
          "md5": "e91cb98da61973197058849f34b4c2c8",
          "sha256": "a32c67a0fcb200627baad4f66e7bcec4edc96771f1faf67d7af1c669ce139ae3"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.15-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e91cb98da61973197058849f34b4c2c8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<3.11.*",
        "size": 24772,
        "upload_time": "2022-12-17T19:13:10",
        "upload_time_iso_8601": "2022-12-17T19:13:10.744046Z",
        "url": "https://files.pythonhosted.org/packages/86/2c/83acacb0fa37c7e47809d896287e2440ba66682f4f948e423148dcca8482/Attention_and_Transformers-0.0.15-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0c5d1b143ff86a9182751ac0ddabac126afce2f1c0e6ee91e812f46d0c9d2b3f",
          "md5": "c75f073989d43cbef4aea9d4950d427d",
          "sha256": "18de0593625a77b0dacff19e64ef77b6860e4e9a8d6f06e99f9448c127f0fd07"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.15.tar.gz",
        "has_sig": false,
        "md5_digest": "c75f073989d43cbef4aea9d4950d427d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<3.11.*",
        "size": 17527,
        "upload_time": "2022-12-17T19:13:12",
        "upload_time_iso_8601": "2022-12-17T19:13:12.190814Z",
        "url": "https://files.pythonhosted.org/packages/0c/5d/1b143ff86a9182751ac0ddabac126afce2f1c0e6ee91e812f46d0c9d2b3f/Attention_and_Transformers-0.0.15.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ada81aa838210bf384b8aeaf4009420e6c0a8f195decd8f59a411dee5ad26d67",
          "md5": "5955565e6c75118f26377d5a60f13f70",
          "sha256": "1898fb1e687bd72eea610fed3735cca6a2f4c9227a414dfb2932cc26dcdb8555"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5955565e6c75118f26377d5a60f13f70",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9,<3.11.*",
        "size": 21190,
        "upload_time": "2022-12-16T16:37:13",
        "upload_time_iso_8601": "2022-12-16T16:37:13.009138Z",
        "url": "https://files.pythonhosted.org/packages/ad/a8/1aa838210bf384b8aeaf4009420e6c0a8f195decd8f59a411dee5ad26d67/Attention_and_Transformers-0.0.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a8988406d632ac2a38e399fb04aeb4a01f332252b4a4f04e634beea3fa52be28",
          "md5": "221c1e0eb86cf19d518f497f6475f45f",
          "sha256": "ef774d1590623d449d452d79bf6b5a148b3608ad1647578cf227f97b353d6616"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "221c1e0eb86cf19d518f497f6475f45f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9,<3.11.*",
        "size": 13789,
        "upload_time": "2022-12-16T16:37:14",
        "upload_time_iso_8601": "2022-12-16T16:37:14.514837Z",
        "url": "https://files.pythonhosted.org/packages/a8/98/8406d632ac2a38e399fb04aeb4a01f332252b4a4f04e634beea3fa52be28/Attention_and_Transformers-0.0.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3980b4b9dfa15bca93f4a1053021758960593096a7a1dcc93c4606dcf4da7565",
          "md5": "6ff9cc62a6053719749f0e54fd6f5f13",
          "sha256": "4ab6a68901d8e4a4a2ea1d94f7d1159b16c16433e391fc9ed832cfb5bf8f1357"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6ff9cc62a6053719749f0e54fd6f5f13",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9,<3.11.*",
        "size": 21188,
        "upload_time": "2022-12-16T16:59:16",
        "upload_time_iso_8601": "2022-12-16T16:59:16.098612Z",
        "url": "https://files.pythonhosted.org/packages/39/80/b4b9dfa15bca93f4a1053021758960593096a7a1dcc93c4606dcf4da7565/Attention_and_Transformers-0.0.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f7329dae3f53230f3460af436b3ddabbdef2665a3bd399fc067537dbe49fd904",
          "md5": "7ad14ab184681b576b393a3af6c8bb65",
          "sha256": "1a3ed456ad6be647a299f0a6aacf618a64acdaaa8e445511b175137277287fb0"
        },
        "downloads": -1,
        "filename": "Attention_and_Transformers-0.0.9.tar.gz",
        "has_sig": false,
        "md5_digest": "7ad14ab184681b576b393a3af6c8bb65",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9,<3.11.*",
        "size": 13802,
        "upload_time": "2022-12-16T16:59:18",
        "upload_time_iso_8601": "2022-12-16T16:59:18.215511Z",
        "url": "https://files.pythonhosted.org/packages/f7/32/9dae3f53230f3460af436b3ddabbdef2665a3bd399fc067537dbe49fd904/Attention_and_Transformers-0.0.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "862c83acacb0fa37c7e47809d896287e2440ba66682f4f948e423148dcca8482",
        "md5": "e91cb98da61973197058849f34b4c2c8",
        "sha256": "a32c67a0fcb200627baad4f66e7bcec4edc96771f1faf67d7af1c669ce139ae3"
      },
      "downloads": -1,
      "filename": "Attention_and_Transformers-0.0.15-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e91cb98da61973197058849f34b4c2c8",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7,<3.11.*",
      "size": 24772,
      "upload_time": "2022-12-17T19:13:10",
      "upload_time_iso_8601": "2022-12-17T19:13:10.744046Z",
      "url": "https://files.pythonhosted.org/packages/86/2c/83acacb0fa37c7e47809d896287e2440ba66682f4f948e423148dcca8482/Attention_and_Transformers-0.0.15-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0c5d1b143ff86a9182751ac0ddabac126afce2f1c0e6ee91e812f46d0c9d2b3f",
        "md5": "c75f073989d43cbef4aea9d4950d427d",
        "sha256": "18de0593625a77b0dacff19e64ef77b6860e4e9a8d6f06e99f9448c127f0fd07"
      },
      "downloads": -1,
      "filename": "Attention_and_Transformers-0.0.15.tar.gz",
      "has_sig": false,
      "md5_digest": "c75f073989d43cbef4aea9d4950d427d",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7,<3.11.*",
      "size": 17527,
      "upload_time": "2022-12-17T19:13:12",
      "upload_time_iso_8601": "2022-12-17T19:13:12.190814Z",
      "url": "https://files.pythonhosted.org/packages/0c/5d/1b143ff86a9182751ac0ddabac126afce2f1c0e6ee91e812f46d0c9d2b3f/Attention_and_Transformers-0.0.15.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}