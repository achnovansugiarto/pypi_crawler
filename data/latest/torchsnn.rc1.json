{
  "info": {
    "author": "Federico A. Galatolo",
    "author_email": "galatolo.federico@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# torchsnn\npytorch implementation of the Stigmergic Neural Networks as presented in the paper [*Using stigmergy to incorporate the time into artificial neural networks*]().\n\nThis package was wrote with the intent to make as **easy** as possible to integrate the Stigmergic Neural Networks into the existing models.\n\nYou can safely **mix** native pytorch Modules with ours.  \nThe only **catch** is that you should use *StigmergicModule* (which extends pytorch's *Module*) as base class for your models in order to be able to *tick()* and *reset()* them.\n\nImplementing our [proposed architecture to solve MNIST]() becomes as easy as:\n```python\nimport torch\nimport torchsnn\n\nnet = torchsnn.Sequential(\n    torchsnn.SimpleLayer(28, 10),\n    torchsnn.FullLayer(10, 10),\n    torchsnn.TemporalAdapter(10, 28),\n    torch.nn.Linear(10*28, 10),\n    torch.nn.Sigmoid()\n)\n```\n\nYou can train a *StigmergicModule* as you would do with a pytorch's *Module*, but don't forget to *reset()* and *tick()* it!\n\n```python\noptimizer = torch.optim.Adam(net.parameters(), lr = 0.001)\n\nfor i in range(0,N):\n    for X,Y in zip(dataset_X, dataset_Y):\n        net.reset()\n        out = None\n        for xi in X:\n            out = net(torch.tensor(xi, dtype=torch.float32))\n            net.tick()\n        \n        loss = (Y-out)**2\n        \n        loss.backward()\n        optimizer.step()\n```\n\n### Does it support batch inputs?\n\nYes! if you forward into a StigmergicModule a batch of inputs it will return a batch of outputs\n\n```python\nfor t in range(0, num_ticks):\n    batch_out[0], batch_out[1], ... = net(torch.tensor([batch_in[0][t], batch_in[1][t], ...]))\n    net.tick()\n\n```\n### Can it run on CUDA?\n\nYes and as you will expect from a pytorch Module!  \nYou just need to call the *to(device)* method on a model to move it in the GPU memory\n\n```python\ndevice = torch.device(\"cuda\")\n\nnet = net.to(device)\n\nnet.forward(torch.tensor(..., device=device))\n```\n## Documentation\n\n### torchsnn.StigmergicModule\n\nBase class for a stigmergic network or layer.  \nIf you are writing your own *StigmergicModule* you have to implement the functions\n* tick()\n* reset()\n\nIf you are using others *StigmergicModule* it will propagate these calls in its subtree.  \nFor example if you want to build a network with a *Linear* and a *SimpleLayer* you can do something like:\n\n```python\nimport torch\nimport torchsnn\n\nclass TestNetwork(torchsnn.StigmergicModule):\n    def __init__(self):\n        torchsnn.StigmergicModule.__init__(self)\n        self.linear = torch.nn.Linear(2,5)\n        self.stigmergic = torchsnn.SimpleLayer(5,2)\n\n    def forward(self, input):\n        l1 = torch.sigmoid(self.linear(input))\n        l2 = self.stigmergic(l1)\n        return l2\n\nnet = TestNetwork()\n```\n\n### torchsnn.Sequential\n\nFunction with the same interface of *torch.nn.Sequential* for building sequential networks.  \nThe same network of the previous example can be built with:\n```python\nimport torch\nimport torchsnn\n\nnet = torchsnn.Sequential(\n    torch.nn.Linear(2,5),\n    torch.nn.Sigmoid(),\n    torchsnn.SimpleLayer(5,2)\n)\n```\n\n### torchsnn.SimpleLayer\n\nIt this layer only the thresholds are stigmergic variables and their *stimuli* are the output values.  \n\n![](https://latex.codecogs.com/gif.latex?x%28t%29%20%3D%20%5Ctext%7Binput%20at%20time%20t%7D)  \n\n![](https://latex.codecogs.com/gif.latex?y%28t%29%20%3D%20%5Ctext%7Boutput%20at%20time%20t%7D)  \n\n![](https://latex.codecogs.com/gif.latex?th%28t%29%20%3D%20%5Ctext%7Bthreshold%20at%20time%20t%7D)\n\n<br>\n\n![](https://latex.codecogs.com/gif.latex?y%28t%29%20%3D%20%5Csigma%28Wx%28t%29%20&plus;%20b%20-%20th%28t%29%29)  \n\n![](https://latex.codecogs.com/gif.latex?th%28t%29%20%3D%20C%28th%28t-1%29%20&plus;%20My%28t-1%29%20-%20%5Ctau%2C%20min%2C%20max%29) \n\n### torchsnn.FullLayer\n\nIn this layer both thresholds and weights are stigmergic variables and their *stimuli* are respectively the output values and the input ones.  \n\n![](https://latex.codecogs.com/gif.latex?x%28t%29%20%3D%20%5Ctext%7Binput%20at%20time%20t%7D)  \n\n![](https://latex.codecogs.com/gif.latex?y%28t%29%20%3D%20%5Ctext%7Boutput%20at%20time%20t%7D)  \n\n![](https://latex.codecogs.com/gif.latex?th%28t%29%20%3D%20%5Ctext%7Bthreshold%20at%20time%20t%7D) \n\n![](https://latex.codecogs.com/gif.latex?W%28t%29%20%3D%20%5Ctext%7Bweights%20at%20time%20t%7D)  \n\n<br>\n\n![](https://latex.codecogs.com/gif.latex?y%28t%29%20%3D%20%5Csigma%28Wx%28t%29%20&plus;%20b%20-%20th%28t%29%29)  \n\n![](https://latex.codecogs.com/gif.latex?th%28t%29%20%3D%20C%28th%28t-1%29%20&plus;%20My%28t-1%29%20-%20%5Ctau%2C%20min%2C%20max%29)  \n\n![](https://latex.codecogs.com/gif.latex?J%5Em%28v%29%20%3D%20%5Cbegin%7Bbmatrix%7D%20v_%7B0%7D%20%26%20v_%7B0%7D%20%26%20%5Cdots%20%26%20v_%7B0%7D%20%5C%5C%20v_%7B1%7D%20%26%20v_%7B1%7D%20%26%20%5Cdots%20%26%20v_%7B1%7D%20%5C%5C%20%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%20%5C%5C%20v_%7Bn-1%7D%20%26%20v_%7Bn-1%7D%20%26%20%5Cdots%20%26%20v_%7Bn-1%7D%20%5Cend%7Bbmatrix%7D)  \n\n![](https://latex.codecogs.com/gif.latex?W%28t%29%20%3D%20C%28W%28t-1%29%20&plus;%20J%5Em%28X%28t-1%29%29M_w%20-%20%5Ctau_w%2C%20min%2C%20max%29) \n\n## Citing\n\nWe can't wait to see what you will build with the Stigmergic Neural Networks!  \nWhen you will publish your work you can use this BibTex to cite us :)\n\n```\n@article{galatolo_snn\n,\tauthor\t= {Galatolo, Federico A and Cimino, Mario GCA and Vaglini, Gigliola}\n,\ttitle\t= {Using stigmergy to incorporate the time into artificial neural networks}\n,\tjournal\t= {MIKE 2018}\n,\tyear\t= {2018}\n}\n```\n\n## Contributing\n\nThis code is released under GNU/GPLv3 so feel free to fork it and submit your changes, every PR helps.  \nIf you need help using it or for any question please reach me at [galatolo.federico@gmail.com](mailto:galatolo.federico@gmail.com) or on Telegram  [@galatolo](https://t.me/galatolo)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/galatolofederico/torchsnn",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torchsnn",
    "package_url": "https://pypi.org/project/torchsnn/",
    "platform": "",
    "project_url": "https://pypi.org/project/torchsnn/",
    "project_urls": {
      "Homepage": "https://github.com/galatolofederico/torchsnn"
    },
    "release_url": "https://pypi.org/project/torchsnn/0.1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "pytorch implementation of Stigmergic Neural Netowrks",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 4417210,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f238d6852634b5d8c0805c6bb699f4423fa621443a277cf127356ad915640e44",
          "md5": "c102862853536338df022aaf16a32e51",
          "sha256": "40e78cece7bb1d11a0cce5e833e971653fa5f2752447ae79ad6e45bf018106d3"
        },
        "downloads": -1,
        "filename": "torchsnn-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c102862853536338df022aaf16a32e51",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5619,
        "upload_time": "2018-10-25T23:29:28",
        "upload_time_iso_8601": "2018-10-25T23:29:28.532329Z",
        "url": "https://files.pythonhosted.org/packages/f2/38/d6852634b5d8c0805c6bb699f4423fa621443a277cf127356ad915640e44/torchsnn-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f238d6852634b5d8c0805c6bb699f4423fa621443a277cf127356ad915640e44",
        "md5": "c102862853536338df022aaf16a32e51",
        "sha256": "40e78cece7bb1d11a0cce5e833e971653fa5f2752447ae79ad6e45bf018106d3"
      },
      "downloads": -1,
      "filename": "torchsnn-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "c102862853536338df022aaf16a32e51",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 5619,
      "upload_time": "2018-10-25T23:29:28",
      "upload_time_iso_8601": "2018-10-25T23:29:28.532329Z",
      "url": "https://files.pythonhosted.org/packages/f2/38/d6852634b5d8c0805c6bb699f4423fa621443a277cf127356ad915640e44/torchsnn-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}