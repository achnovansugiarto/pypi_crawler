{
  "info": {
    "author": "Joel Jang",
    "author_email": "wkddydpf@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "[![Build Status](https://github.com/microsoft/deepspeed/workflows/Build/badge.svg)](https://github.com/microsoft/DeepSpeed/actions)\n[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)\n[![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)\n[![License MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://github.com/Microsoft/DeepSpeed/blob/master/LICENSE)\n\n<div align=\"center\">\n <img src=\"docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only\" width=\"400px\">\n <img src=\"docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only\" width=\"400px\">\n</div>\n\n<!--\nRemove until pypi issue is resolved: https://status.python.org/incidents/2jj696st6yn5\n[![Downloads](https://pepy.tech/badge/deepspeed/month)](https://pepy.tech/project/deepspeed)\n-->\n## Latest News\n* [2022/03/21] [Supporting efficient large model training on AMD Instinct GPUs with DeepSpeed](https://cloudblogs.microsoft.com/opensource/2022/03/21/supporting-efficient-large-model-training-on-amd-instinct-gpus-with-deepspeed/)\n* [2022/03/07] [Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam](https://www.deepspeed.ai/tutorials/zero-one-adam/)\n* [2022/01/19] [DeepSpeed: Advancing MoE inference and training to power next-generation AI scale](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n    * [Mixture of Experts (MoE) for NLG tutorial](https://www.deepspeed.ai/tutorials/mixture-of-experts-nlg/).\n    * [Mixture of Experts (MoE) Inference tutorial](https://www.deepspeed.ai/tutorials/moe-inference-tutorial).\n* [2021/11/15] [Autotuning: Automatically discover the optimal DeepSpeed configuration that delivers good training speed](https://www.deepspeed.ai/news/2021/11/15/autotuning.html)\n* [2021/10/11] [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World’s Largest and Most Powerful Generative Language Model](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n  * Read more on how to [train large models with DeepSpeed](https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/)\n\n### DeepSpeed is hiring, [come join us!](https://careers.microsoft.com/us/en/search-results?keywords=http:%2F%2Fdeepspeed.ai)\n---\n\n[DeepSpeed](https://www.deepspeed.ai/) is a deep learning optimization\nlibrary that makes distributed training easy, efficient, and effective.\n\n<p align=\"center\"><i><b>10x Larger Models</b></i></p>\n<p align=\"center\"><i><b>10x Faster Training</b></i></p>\n<p align=\"center\"><i><b>Minimal Code Change</b></i></p>\n\nDeepSpeed delivers extreme-scale model training for everyone, from data scientists training on massive supercomputers to those training on low-end clusters or even on a single GPU:\n* Extreme scale: Using current generation of GPU clusters with hundreds of devices,  3D parallelism of DeepSpeed can efficiently train deep learning models with trillions of parameters.\n* Extremely memory efficient: With just a single GPU, ZeRO-Offload of DeepSpeed can train models with over 10B parameters, 10x bigger than the state of arts, democratizing multi-billion-parameter model training such that many deep learning scientists can explore bigger and better models.\n* Extremely long sequence length: Sparse attention of DeepSpeed powers an order-of-magnitude longer input sequence and obtains up to 6x faster execution comparing with dense transformers.\n* Extremely communication efficient: 3D parallelism improves communication efficiency allows users to train multi-billion-parameter models 2–7x faster on clusters with limited network bandwidth.  1-bit Adam, 0/1 Adam and 1-bit LAMB reduce communication volume by up to 26x while achieving similar convergence efficiency to Adam/LAMB, allowing for scaling to different types of GPU clusters and networks.\n\nEarly adopters of DeepSpeed have already produced\na language model (LM) with over 17B parameters called\n[Turing-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft),\nestablishing a new SOTA in the LM category.\n\nDeepSpeed is an important part of Microsoft’s new\n[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)\ninitiative to enable next-generation AI capabilities at scale, where you can find more\ninformation [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).\n\n**_For further documentation, tutorials, and technical deep-dives please see [deepspeed.ai](https://www.deepspeed.ai/)!_**\n\n# Table of Contents\n| Section                                 | Description                                 |\n| --------------------------------------- | ------------------------------------------- |\n| [Why DeepSpeed?](#why-deepspeed)        |  DeepSpeed overview                         |\n| [Install](#installation)                |  Installation details                       |\n| [Features](#features)                   |  Feature list and overview                  |\n| [Further Reading](#further-reading)     |  Documentation, tutorials, etc.             |\n| [Contributing](#contributing)           |  Instructions for contributing              |\n| [Publications](#publications)           |  Publications related to DeepSpeed          |\n| [Videos](#videos)                       |  Videos related to DeepSpeed                |\n\n# Why DeepSpeed?\nTraining advanced deep learning models is challenging. Beyond model design,\nmodel scientists also need to set up the state-of-the-art training techniques\nsuch as distributed training, mixed precision, gradient accumulation, and\ncheckpointing. Yet still, scientists may not achieve the desired system\nperformance and convergence rate. Large model sizes are even more challenging:\na large model easily runs out of memory with pure data parallelism and it is\ndifficult to use model parallelism. DeepSpeed addresses these challenges to\naccelerate model development *and* training.\n\n# Installation\n\nThe quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our 'ops'.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch's JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n**Note:** [PyTorch](https://pytorch.org/) must be installed _before_ installing\nDeepSpeed.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\nOn Windows you can build wheel with following steps, currently only inference mode is supported.\n1. Install pytorch, such as pytorch 1.8 + cuda 11.1\n2. Install visual cpp build tools, such as VS2019 C++ x64/x86 build tools\n3. Launch cmd console with Administrator privilege for creating required symlink folders\n4. Run `python setup.py bdist_wheel` to build wheel in `dist` folder\n\n# Features\nBelow we provide a brief feature list, see our detailed [feature\noverview](https://www.deepspeed.ai/features/) for descriptions and usage.\n\n* [Distributed Training with Mixed Precision](https://www.deepspeed.ai/features/#distributed-training-with-mixed-precision)\n  * 16-bit mixed precision\n  * Single-GPU/Multi-GPU/Multi-Node\n* [Model Parallelism](https://www.deepspeed.ai/features/#model-parallelism)\n  * Support for Custom Model Parallelism\n  * Integration with Megatron-LM\n* [Pipeline Parallelism](https://www.deepspeed.ai/tutorials/pipeline/)\n  * 3D Parallelism\n* [The Zero Redundancy Optimizer (ZeRO)](https://www.deepspeed.ai/tutorials/zero/)\n  * Optimizer State and Gradient Partitioning\n  * Activation Partitioning\n  * Constant Buffer Optimization\n  * Contiguous Memory Optimization\n* [ZeRO-Offload](https://www.deepspeed.ai/tutorials/zero-offload/)\n  * Leverage both CPU/GPU memory for model training\n  * Support 10B model training on a single GPU\n* [Ultra-fast dense transformer kernels](https://www.deepspeed.ai/2020/05/18/bert-record.html)\n* [Sparse attention](https://www.deepspeed.ai/2020/09/08/sparse-attention-news.html)\n  * Memory- and compute-efficient sparse kernels\n  * Support 10x longer sequences than dense\n  * Flexible support to different sparse structures\n* [1-bit Adam](https://www.deepspeed.ai/2020/09/08/onebit-adam-blog-post.html), [0/1 Adam](https://www.deepspeed.ai/tutorials/zero-one-adam/) and [1-bit LAMB](https://www.deepspeed.ai/tutorials/onebit-lamb/)\n  * Custom communication collective\n  * Up to 26x communication volume saving\n* [Additional Memory and Bandwidth Optimizations](https://www.deepspeed.ai/features/#additional-memory-and-bandwidth-optimizations)\n  * Smart Gradient Accumulation\n  * Communication/Computation Overlap\n* [Training Features](https://www.deepspeed.ai/features/#training-features)\n  * Simplified training API\n  * Gradient Clipping\n  * Automatic loss scaling with mixed precision\n* [Training Optimizers](https://www.deepspeed.ai/features/#training-optimizers)\n  * Fused Adam optimizer and arbitrary `torch.optim.Optimizer`\n  * Memory bandwidth optimized FP16 Optimizer\n  * Large Batch Training with LAMB Optimizer\n  * Memory efficient Training with ZeRO Optimizer\n  * CPU-Adam\n* [Training Agnostic Checkpointing](https://www.deepspeed.ai/features/#training-agnostic-checkpointing)\n* [Advanced Parameter Search](https://www.deepspeed.ai/features/#advanced-parameter-search)\n  * Learning Rate Range Test\n  * 1Cycle Learning Rate Schedule\n* [Simplified Data Loader](https://www.deepspeed.ai/features/#simplified-data-loader)\n* [Curriculum Learning](https://www.deepspeed.ai/tutorials/curriculum-learning/)\n  * A curriculum learning-based data pipeline that presents easier or simpler examples earlier during training\n  * Stable and 3.3x faster GPT-2 pre-training with 8x/4x larger batch size/learning rate while maintaining token-wise convergence speed\n  * Complementary to many other DeepSpeed features\n* [Performance Analysis and Debugging](https://www.deepspeed.ai/features/#performance-analysis-and-debugging)\n* [Mixture of Experts (MoE)](https://www.deepspeed.ai/tutorials/mixture-of-experts/)\n\n\n# Further Reading\n\nAll DeepSpeed documentation can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)\n\n\n| Article                                                                                        | Description                                  |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [DeepSpeed Features](https://www.deepspeed.ai/features/)                                       |  DeepSpeed features                          |\n| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                  |\n| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |\n| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |\n| [CIFAR-10 Tutorial](https://www.deepspeed.ai/tutorials/cifar-10)                               |  Getting started with CIFAR-10 and DeepSpeed |\n| [Megatron-LM Tutorial](https://www.deepspeed.ai/tutorials/megatron/)                           |  Train GPT2 with DeepSpeed and Megatron-LM   |\n| [BERT Pre-training Tutorial](https://www.deepspeed.ai/tutorials/bert-pretraining/)             |  Pre-train BERT with DeepSpeed               |\n| [Learning Rate Range Test Tutorial](https://www.deepspeed.ai/tutorials/lrrt/)                  |  Faster training with large learning rates   |\n| [1Cycle Tutorial](https://www.deepspeed.ai/tutorials/one-cycle/)                               |  SOTA learning schedule in DeepSpeed         |\n\n\n\n# Contributing\nDeepSpeed welcomes your contributions! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,\netc.\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Publications\n1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840).\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857).\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training. [arXiv:2108.06084](https://arxiv.org/abs/2108.06084).\n9. Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. [arXiv:2202.06009](https://arxiv.org/abs/2202.06009).\n10. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale [arXiv:2201.05596](https://arxiv.org/abs/2201.05596).\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer](https://www.youtube.com/watch?v=tC01FRB0M7w)",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://deepspeed.ai",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "deepspeed-moee",
    "package_url": "https://pypi.org/project/deepspeed-moee/",
    "platform": null,
    "project_url": "https://pypi.org/project/deepspeed-moee/",
    "project_urls": {
      "Documentation": "https://deepspeed.readthedocs.io",
      "Homepage": "http://deepspeed.ai",
      "Source": "https://github.com/joeljang/DeepSpeed"
    },
    "release_url": "https://pypi.org/project/deepspeed-moee/0.12/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "DeepSpeed library",
    "version": "0.12",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13393160,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a87b15305f279d69b0367be59dd907c8c6e71056c7a831c8d886623698d1d521",
          "md5": "fb52a314c63334184d58fdfc07aa0bc0",
          "sha256": "463fb31bf0184cb033b2bc937478a0fff2a9e02985cf731d059a26ef25fff65c"
        },
        "downloads": -1,
        "filename": "deepspeed_moee-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "fb52a314c63334184d58fdfc07aa0bc0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 530926,
        "upload_time": "2022-04-03T16:02:15",
        "upload_time_iso_8601": "2022-04-03T16:02:15.411030Z",
        "url": "https://files.pythonhosted.org/packages/a8/7b/15305f279d69b0367be59dd907c8c6e71056c7a831c8d886623698d1d521/deepspeed_moee-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "802f0d99ab8093e2dacd5489d58efed4789650fd80bd30b76ea92d07d9dffc51",
          "md5": "fa7c3c862d64de64dbf0f7b8a97678ba",
          "sha256": "0d084d10a74d4913217aa20d1b4dd5977b3180e1aa160cc6d26468305e949944"
        },
        "downloads": -1,
        "filename": "deepspeed_moee-0.11.tar.gz",
        "has_sig": false,
        "md5_digest": "fa7c3c862d64de64dbf0f7b8a97678ba",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 531033,
        "upload_time": "2022-04-03T16:23:14",
        "upload_time_iso_8601": "2022-04-03T16:23:14.132506Z",
        "url": "https://files.pythonhosted.org/packages/80/2f/0d99ab8093e2dacd5489d58efed4789650fd80bd30b76ea92d07d9dffc51/deepspeed_moee-0.11.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.12": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "81487bdb964c000e9a45a542d5ce674a906782749eeeaa92e4285134659fb1a7",
          "md5": "918e6ffbf7867e0a34439fe062530b60",
          "sha256": "fd53d2d83672482e4ba8ff6114969f43ba61ad6d11384d81059419119447211b"
        },
        "downloads": -1,
        "filename": "deepspeed_moee-0.12.tar.gz",
        "has_sig": false,
        "md5_digest": "918e6ffbf7867e0a34439fe062530b60",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 530947,
        "upload_time": "2022-04-03T16:26:25",
        "upload_time_iso_8601": "2022-04-03T16:26:25.206314Z",
        "url": "https://files.pythonhosted.org/packages/81/48/7bdb964c000e9a45a542d5ce674a906782749eeeaa92e4285134659fb1a7/deepspeed_moee-0.12.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "81487bdb964c000e9a45a542d5ce674a906782749eeeaa92e4285134659fb1a7",
        "md5": "918e6ffbf7867e0a34439fe062530b60",
        "sha256": "fd53d2d83672482e4ba8ff6114969f43ba61ad6d11384d81059419119447211b"
      },
      "downloads": -1,
      "filename": "deepspeed_moee-0.12.tar.gz",
      "has_sig": false,
      "md5_digest": "918e6ffbf7867e0a34439fe062530b60",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 530947,
      "upload_time": "2022-04-03T16:26:25",
      "upload_time_iso_8601": "2022-04-03T16:26:25.206314Z",
      "url": "https://files.pythonhosted.org/packages/81/48/7bdb964c000e9a45a542d5ce674a906782749eeeaa92e4285134659fb1a7/deepspeed_moee-0.12.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}