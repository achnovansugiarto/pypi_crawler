{
  "info": {
    "author": "X. Michael Wang, University of Toronto, Faculty of Kinesiology and Physical Education, AA Lab",
    "author_email": "michaelwxy.wang@utoronto.ca",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# TAT-HUM: Trajectory Analysis Toolkit for Human Movement\n\nFor more details on this toolkit, see the preprint [here](https://psyarxiv.com/4yrk7).\n\n## Abstract\nHuman movement trajectories can reveal useful insights regarding the underlying mechanisms of human behaviors. \nExtracting information from movement trajectories, however, could be challenging because of their complex and dynamic \nnature . The current paper presents a Python toolkit developed to help users analyze and extract useful information \nfrom the trajectories of discrete rap-id aiming movements  executed by humans. This toolkit utilizes various open-source \nPython libraries, such as NumPy and SciPy, and offers a collection of commonly used functions to analyze movement \ntrajectory data. To ensure flexibility and ease of use, this toolkit offers two approaches: an automated approach that \nprocesses raw data and generates relevant measures automatically, and a manual approach that allows users to \nselectively use different functions based on their specific needs. A behavioral experiment based on the spatial cueing \nparadigm was conducted to illustrate how one can use this toolkit in practice. Readers are encouraged to access the \npublically available data and relevant analysis scripts as an opportunity to learn about kinematic analysis for human \nmovements. \n\n## Installation\n\nSee PyPi project link: https://pypi.org/project/tathum/\n\n```\npython3 -m pip install tathum\npip install tathum\npip3 install tathum\n```\n\n## Sample Data\n\nTo facilitate learning, this toolkit also provides sample data, which can be downloaded through the OSF's online \nrepository [here](https://osf.io/24qvm/).\n\n## Quick Start\n\nTo quickly become familiar with TAT-HUM, users should refer to the following sample analysis code:\n\n1. ./demo/sample_data_analysis.py\n2. ./demo/sample_data_visualization.py\n\n## Implementation\n### Data Structure\nThe toolkit relies on NumPy’s array objects to store and process relevant data. The trajectory data should be stored as \na csv file with four columns: timestamps, x-, y-, and z- coordinates. Depending on the motion capture system’s \ncalibration setup, different coordinate axes could represent different directions in the capture space. Therefore, it \nis essentialy for users to maintain a consistent coordinate system across different calibrations for different sessions. \nBased on the coordinate system, users should identify the primary axis of the participants’ movement (i.e., the axis \nwith the most displacement) and the most relevant secondary movement axis using the calibrated coordinate system. \nAssuming this structure, users can simply read the raw data file and assign separate variables to extract information \nfrom each column:\n\n```python\nimport numpy as np\n\nraw_data = np.genfromtxt('raw_data.csv', delimiter=',')\n\ntimestamp = raw_data[:, 0]\nx = raw_data[:, 1]\ny = raw_data[:, 2]\nz = raw_data[:, 3]\n```\n\nIf using 2D data from a mouse or graphics tablet, the user could simply create a dummy column for one of the axes \n(e.g., an axis with all zeros) and only focus on the two non-zero axes in the subsequent analysis. \n\n### Preprocessing\n#### Missing Data\nWhen recording human movement using a motion capture system (e.g., Optotrak, Vicon, and OptiTrack), the researcher \naims to arrange the placement of the cameras and the participant to ensure that the motion tracking marker is visible \nto the cameras for the entire movement.  Despite best efforts, missing data (marker is not recorded by the motion \ncapture system) commonly occurs due to the occlusion of the markers from objects in the environement or from the \nparticipant moving in an unexpected way (e.g., lifts the finger outside of the capture volume or rotates the finger \ntoo much). These missing data are commonly coded using unique values, such as 0 or an exceedingly large number, to \ndifferentiate them from points of the actual trajectory. As Figure 1 (top panels) shows, the missing data were marked \nas 0 and could be easily identified. In general, one could consider eliminating all trials with missing data from the \nanalysis, and should eliminate trials that are missing data from more than 5%   of the trial.  However, for trials with \nless than 5% missing data, it is possible to recover the trial by interpolating the section of the trial that is \nmissing data.\n\n![missing_data_good](./img/missing_data_good.png) ![missing_data_bad](./img/missing_data_bad.png)\n\n*Figure 1. Two examples of missing data in the raw trajectory (top panels) and the trajectory with missing data filled \nin using linear interpolation (bottom panels). See text for explanations.*  \n\nIn the toolkit, the ```fill_missing_data``` function can automatically process the missing values in the trajectories. The \nusers need to supply the x, y, and z coordinates, along with the corresponding time stamps and the default missing data \nvalue. This function will identify the indices of the missing values and return the coordinates and timestamps with the \nmissing values interpolated using linear interpolation from SciPy (Virtanen et al., 2020; Figure 1 bottom panels). More \nimportantly, this function also returns a dictionary object  that contains relevant missing data information, including \nwhether there are any missing data, the number of missing data, and their indices. As illustrated in the subsequent \nparagraphs, users can use this information to determine whether they would keep the trial or not based on the number \nof locations of the missing values.\n\n```python\nfrom tathum.functions import fill_missing_data\n\nx, y, z, timestamp, missing_info = fill_missing_data(\n\t\tx, y, z, timestamp, missing_data_value=0.)\n\nprint(f'missing_info = {missing_info}')\n```\n```pycon\nmissing_info = {'contain_missing': True, 'n_missing': 3, 'missing_ind': array([346, 347, 348])}\n```\n\nIn the two examples provided above, the trajectories on the left contain only a few missing data points at around the \nend of the trajectories (the displacements appear to be plateaued at around the missing data), whereas the trials on the \nright contains several missing data segments, one of which occurred before the movement termination (there are missing \ndata before the displacements becoming stable and consistent). In the former case, it is unlikely for the linear \ninterpolation to introduce artifacts to the dynamic information of the movement during subsequent analysis because 1) \nthe interpolated portion may fall outside of the movement initiation and termination and, therefore, will not be \nconsidered in the analysis, and 2) the interpolated segment is short enough (n = 3) that linear interpolation could \nfill in the missing data without altering the dynamic information in the trajectory. The latter case, however, not only \ncontains missing data between movement initiation and termination, but the number of missing data for the segment is \nalso quite large (n = 34). Using linear interpolation to fill in the missing values may introduce artifacts to the \ntrajectory and affect the validity of the subsequent analysis. Therefore, situations like this may warrant the removal \nof the trial from further analysis. \n\nThus, although the present toolkit could automatically handle missing data, caution should still be taken to avoid \nintroducing artifacts to the trajectories. The decision between keeping the trial with interpolated values and \ndiscarding the trial due to missing data relies on two main factors: the locations of the missing data and the number \nof consecutive missing data. Because determining where the missing data are relative to the movement initiation and \ntermination requires first identifying where the movement has started and ended, evaluating the validity of the missing \ndata handling needs to happen after the movement boundaries have been identified, which, in turn, requires other \npreprocessing steps.\n\n#### Data smoothing\nData smoothing is a critical step in the preprocessing of human movement data. Although the current state-of-the-art \noptical motion capture systems allow precise motion tracking (e.g., for Optotrak, measurement error is commonly smaller \nthan 0.2 mm), slight deviations in the position data are still inevitable due to reasons such as marker movement and \nroom vibration. Despite the magnitude of these deviations may be relatively small and imperceptible through displacement \ndata (Figure 2, left panel, top row), such measurement variability would be amplified when computing the derivatives, \nsuch as velocity and acceleration (Figure 2, left panels). Therefore, it is essential to smooth the trajectories before \ncomputing their derivatives.\n\n![smoothing_good](./img/smoothing_demo_good.png) ![smoothing_bad](./img/smoothing_demo_bad.png)\n\n*Figure 2. Demonstration of the effect of slight measurement deviations (left) and trajectory smoothing using low-pass \nButterworth filter (right) on the second (i.e., velocity) and third (i.e., acceleration) order derivatives of position.* \n\nTrajectory smoothing is commonly performed using a low-pass Butterworth filter (Butterworth, 1930) with a cutoff \nfrequency of 10 Hz (Franks et al., 1990). The filter included in the present toolkit, ```low_butter```, is a wrapper function \nthat combines SciPy’s ```butter``` and ```filtfilt``` functions. It requires three input arguments, the signal, sampling frequency, \nand cutoff frequency, and can directly output the smoothed signal. As Figure 2 (right panel) shows, this filter helps \nto attenuate the frequencies in the signal that are higher than the cutoff frequency, reducing the noise in the signal.\n\n```python\nfrom tathum.functions import low_butter\n\nfs, fc = 250., 10.\n\nx_smooth = low_butter(x, fs, fc)\ny_smooth = low_butter(y, fs, fc)\nz_smooth = low_butter(z, fs, fc)\n```\n\n#### Spatial transformation\n\nAlthough movements are performed and recorded in a 3D space, the axes of interest should be contingent on the task and \nexperimental questions. For instance, for the movement-based spatial cueing paradigm (e.g., Neyedli & Welsh, 2012; Wang \net al., under review; Yoxon et al., 2019), trajectory analysis should focus on how the relative location between the \ncue and the target affects the spatial characteristics of the movement trajectory. In these studies, the movement’s \nstarting position and the locations of the cue and the target share the same two-dimensional (2D) plane and the spatial \ndeviations between them are along the longitudinal (away from the participant in depth) and lateral axes in the movement \nplane’s reference frame (the behavioral experiment presented in the current study used a similar experimental setup). \nTherefore, movement trajectories along the longitudinal and lateral axes should be of interest. Depending \non the calibration of the motion capture system, however, the movement plane’s reference frame may not coincide with \nthat reference frame of the trajectory data. \n\nAs Figure 3a demonstrates, the primary and secondary movement axes are longitudinal and lateral axes in the screen’s \nreference frame, which deviates from the reference frame in which the movement was recorded (the default reference \nframe, defined as the three unit axes). This spatial discrepancy could  impose challenges to the \nsubsequent analysis if one only wishes to focus on a subset of the movement axes, as the displacement data were \ncaptured in the default reference frame, not the screen’s reference frame. In other words, because the trajectory is \nconstrained by a slanted surface, all three axes are required to perform relevant spatial analysis. To address this \nissue, one can rotate the movement surface and the movement trajectories to be aligned with the default reference \nframe. This rotation only requires the movement surface normal, which can be derived using at least three points on the \nsurface. To identify these points, for instance, the experimenters can incorporate a simple screen calibration \nprocedure in an upper-limb aiming experiment that asks participants to aim different corners of the movement surface \n(e.g., a computer monitor) as reference points. Alternatively, one can also extract the start and end positions from \nthe original movement trajectories and use them as reference points.\n\n| a | ![trans1](./img/1_original_setup.png) | b | ![trans2](./img/2_norm_ground_projection.png) |\n|---|---------------------------------------|---|-----------------------------------------------|\n| c | ![trans3](./img/3_alignment.png)      | d | ![trans4](./img/4_final.png)                  |\n\n*Figure 3. Demonstration of spatial transformation with the movement plane (black rectangle) defined by the four corners \n(black points), its surface normal (magenta line), and the aiming trajectory (black line), where the red \n(x-axis), green (y-axis), and blue (z-axis) dashed lines represent the default reference frame. \n(a) The original spatial layout. (b) The movement plane’s normal is projected onto a horizontal (cyan) plane, where the \ndotted magenta line represents the surface normal’s projection. The angle formed between the projection and the positive \nz-axis (blue dashed line) can be used to align the primary directions between the movement plane and the default \nreference frame. (c) The aligned movement plane after rotation around the y-axis in (b). The angle between the rotated \nsurface normal and the positive ¬y-axis can be used to align the movement plane with the ground plane. (d) The final \nspatial layout after the movement plane was aligned to the primary direction and the ground plane.*\n\nIn the toolkit, the function ```compute_transformation``` computes the appropriate rotation to align the reference frames. \nThis function requires the x, y, and z coordinates of all reference points and the names of the axes based on which the \nrotation should be performed. Specifically, users need to specify the name of the axis that is perpendicular to the \nhorizontal plane (e.g., the ground plane), the primary movement axis (to go from Figure 3b to Figure 3c), and the \nsecondary movement axis (to go from Figure 3c to Figure 3d).\n\n```python\n# the reference points are formatted as an N x 3 matrix\nprint(reference_points)\n```\n```pycon\n[[ 250.5    78.57 -118.1 ]\n [-254.81   73.88 -119.74]\n [-247.97  -77.02  114.62]\n [ 252.29  -75.43  123.22]]\n```\n```pycon\n# the function requires separate x, y, and z input\nrotation, surface_center = compute_transformation(\n    reference_points[:, 0],\n    reference_points[:, 1],\n    reference_points[:, 2],\n    horizontal_norm_name='y',\n    primary_ax_name='z',\n    secondary_ax_name='x', )\n```\n\n#### Kinematic Analysis\n\nIn addition to position, velocity and acceleration of the movement could also provide useful information. Given \ndiscrete position measurement and its corresponding time vector, one can obtain the second- and third-order derivatives \nusing difference quotients.\n\nThis computation can be achieved using the function, ```cent_diff```, from the toolkit. This function takes two inputs, \na vector with timestamps and a vector with the corresponding signal (this could be position, velocity, or even \nacceleration). The algorithm then performs a two- (forward/backward difference) or three-point (central difference) \nnumerical differentiation, depending on where the data point is located. While the three-point central difference \nmethod provides further smoothing for the resulting derivatives (e.g., velocity and acceleration), the two-point \nmethods in the beginning and the end of the data vector ensures the derivatives to line up with the displacement data \nwith regard to the total number of samples.\n\n```python\nfrom tathum.functions import cent_diff\n\nx_vel = cent_diff(timestamp, x_smooth)\ny_vel = cent_diff(timestamp, y_smooth)\nz_vel = cent_diff(timestamp, z_smooth)\n```\n\n### Singular Markers\n#### Movement boundaries\n\nThe boundaries of the movement are marked by movement initiation and termination. Movement boundaries not only provide \nkey temporal markers that capture certain aspects of the movement (e.g., reaction time [RT] and movement time [MT]), \nbut also help to narrow down the trajectory segments that are of most interest in the trajectory analysis. Movement \nboundaries are typically identified using velocity-based criteria. The function, find_movement_bounds, uses velocity \nand a velocity threshold as inputs to identify the movement boundary, returning indices of the velocity vector that \nmarks movement initiation and termination. Movement initiation is defined as the point at which velocity exceeds the \nthreshold whereas movement termination is defined as the point at which velocity drops below the same threshold, which \ncan be set by the user during function call (Figure 4). \n\n![bound_good](./img/movement%20bound.png) ![bound_bad](./img/movement%20bound%20bad.png)\n\n*Figure 4. Demonstration of movement boundaries. The displacement and velocity are of a single dimension, plotted on the \nsame time scale. In this example, movement initiation (green dotted line) and termination (red dotted line) are defined \nas when the velocity exceeds and drops below 50 mm/s, respectively. The left column shows a single movement segment \nwhereas the right column shows two, where the second segment was due to unnecessary movement after the completion of \nthe task-related movement.* \n\nBecause there are three axes to the movement trajectory, the use of velocity could differ depending on the study and \nthe primary axis of the movement trajectory. For instance, the example provided in Figure 4 only used a single axis \n(e.g., x-axis) to determine the movement boundaries. In this example, the chosen axis is the principal movement axis \nof the task-relevant direction in which the participant performed the movement. In this case, the velocity input with \na single dimension is simply a 1D array:\n\n```python\nprint(coord_single)\n```\n```pycon\narray([220.58679039, 220.53076455, 220.45812056, 220.38970357,\n       220.29868951, 220.13652282, 219.65904993, 218.78207948,\n       217.66326255, 216.46275817, 215.23037902, 213.9994592 ,...]\n```\n\nAlternatively, multiple axes (two or three) could also be used to determine movement boundaries. In such situations, \nthe resultant velocity should be used. The resultant velocity can be computed as the Pythagorean of the axes of \ninterest. To use resultant velocity instead of velocity along a single dimension, the user can simply use a matrix with \nall the necessary dimensions as input instead of a vector. In other words, the input would just be a 2D array with each \nrow corresponds to each sample whereas the two columns represent the two axes based on which the resultant velocity will \nbe automatically calculated and used to identify the movement boundaries:\n\n```python\nprint(coord_double)\n```\n```pycon\narray([[220.58679039, 135.29505216],\n       [220.53076455, 135.28456359],\n       [220.45812056, 135.28753378],\n       [220.38970357, 135.27954967],\n       [220.29868951, 135.25909061], ...]\n```\n\nFinally, in some studies, the recording of movements occurs during a specific time interval and the participants may \nmake unnecessary movements before or after the task-relevant movement. For instance, Figure 4 (right) shows that the \nparticipants completed the required movement (first segment) and made some small adjustments to position afterward \n(second segment). To address this issue, the function ```find_movement_bounds``` can either automatically select the longest \nmovement segment as the task-relevant segment by default, or output movement initiation and termination indices for all \nsegments. To obtain indices of all segments, the user can simply set the optional boolean parameter, \n```allow_multiple_segments```, to ```True```. \n\n```python\nfrom tathum.functions import find_movement_bounds\n\nmovement_start_ind, mvoement_end_ind = find_movement_bounds(\n    x_vel, velocity_threshold=30., allow_multiple_segments=False)\n```\n\n#### Reaction time and movement time\n\nReaction time (RT) is defined as the time between stimulus onset and movement initiation. For experiments that require \nRT as a dependent measure, the movement recording should start immediately after the stimulus onset. Therefore, RT is \nthe time between the onset of data collection and movement initiation (Figure 4, green dotted lines). Movement time \n(MT) is defined as the time between movement initiation and termination. MT is the time interval between the green \nand red dotted lines in Figure 4. In practice, readers can first use the ```find_movement_bounds``` function to find the \nindices that specify the movement boundaries and use them to select the corresponding time stamps to derive RT and MT.\n\n```python\nfrom tathum.functions import find_start_end_pos\n\ntimestamp_start = timestamp[movement_start_ind]\ntimestamp_end = timestamp[movement_end_ind]\n\n# assuming the initiation of movement trajectory collection \n# coincides with stimulus onset\nrt = timestamp_start  \nmt = timestamp_end - timestamp_start\n```\n\n#### Movement start and end positions\n\nMovement start and end positions are, by definition, the positions of the trajectory before and after movement \ninitiation and termination, respectively. Given movement boundaries, it is rather straightforward to identify the \npositions. However, unlike the temporal aspects of the movement segment (i.e., RT and MT), the actual  position \nmeasurements of each individual movement may slightly fluctuate even though the participants remained stationary. As \nFigure 5 shows, the movement trajectory remained largely stable before the movement initiation. However, zooming in on \nthat portion of the trajectory reveals slight positional fluctuations before the movement initiation. To address this \npotential issue, when calling the `find_start_end_pos` function, users can optionally specify the number of coordinates \n(`ind_buffer`) to use before and after the movement initiation and termination, and use the average of these coordinates \nas the start and end positions. If the users still wish to use the start and end positions at their instantaneous \nlocations, they can set `ind_buffer` to 1. \n\n```python\nstart_pos, end_pos = find_start_end_pos(\n    \tx_smooth, y_smooth, z_smooth, \n    \tmovement_start_ind, movement_end_ind,\n    \tind_buffer=20)\n```\n\n![start_all](./img/start_pos_all.png) ![start_zoom](./img/start_pos_zoom.png)\n\n*Figure 5. Illustration of the instability of the starting position. Left: the entire movement trajectory where the \ngreen dotted line marks the movement initiation. Right: a zoom-in view of the movement trajectory around the movement \ninitiation as marked by the black bounding box in the left panel.*\n\n### Spatial Analysis Over Time\n#### Trajectory Parameterization\n\nAlthough the singular markers already offer valuable insights into the underlying dynamics of the movement trajectories, \nsometimes it is also crucial to examine the entire aiming trajectories to gain a more holistic understanding of the \nmovement. For instance, comparing the spatial deviations in the trajectories between different conditions can reveal \nthe effect of experimental manipulations on movement planning and execution. \n\nStatistically comparing different trajectories could be difficult. Trajectories from different trials contain different \nnumbers of samples at different time stamps. Because of the variability in the samples, averaging the trajectories is \nimpossible without normalization. Trajectory normalization is commonly achieved via resampling in the time domain, \nwhere an array of average positions/velocities/accelerations is computed at evenly spaced fractions of the MT. Although \ntime resampling produces an equal number of samples across different trials, as Gallivan and Chapman (2014) reasoned, \nthis approach does not preserve the temporal dynamics of the movement and may introduce artifacts in the subsequent \nanalysis. Specifically, the resulting samples from time resampling only correspond to the proportion of the MT, not \nin absolute time. If the experimental manipulation affects MT, evaluating the samples’ difference between conditions \nusing the proportion of the MT would introduce potential confounds in the analysis. \n\nAn alternative approach is to parameterize the movement trajectory as a function of time, which can be accomplished \nthrough a third-order B-spline (Gallivan & Chapman, 2014; Ramsay & Silverman, 2005). Using the B-spline function from \nSciPy (Virtanen et al., 2020), movement trajectories along each dimension are mapped onto each point’s corresponding \ntime stamp. Subsequently, the trajectories can be resampled using this parameterization and an equally spaced time \nvector measured in absolute time. Unlike the time resampling method, the B-spline approach preserves the temporal \ncharacteristics of the movement while providing temporal and spatial normalization. To use this function from the \ntoolkit, users simply need to call the function `b_spline_fit_1d` and use the time, position, and the number of resampled \ndata points as input. \n\n```python\nfrom tathum.functions import b_spline_fit_1d\n\nx_fit = b_spline_fit_1d(timestamp, x_smooth, n_fit=1000, smooth=0.)\nz_fit = b_spline_fit_1d(timestamp, z_smooth, n_fit=1000, smooth=0.)\ny_fit = b_spline_fit_1d(timestamp, y_smooth, n_fit=1000, smooth=0.)\n```\n\n### Automated Processing Pipeline\n\nThe trajectory processing functionalities illustrated above are commonly used in analyzing human movement data. Users \ncan independently select the functions to process movement data or adopt a more automated \napproach. This processing pipeline is a collection of all the functions mentioned above, organized in a sequential \norder that is suitable for most trajectory analyses. To use this pipeline, users simply need to instantiate a \n`Trajectory` class with the raw x, y, and z coordinates and an optional time vector, along with a series of other \noptional parameters that, for instance, specify points on the movement surface (for spatial transformation), the \ndata’s original sampling frequency, the low-pass Butterworth filter’s cutoff frequency, velocity thresholds.\n\n```python\nfrom tathum.trajectory import Trajectory\n\ntrajectory = Trajectory(x, y, z, time=timestamp,\n                        principal_dir=principal_ax,\n                        fs=250, fc=10,\n                        transform_end_points=end_points)\n```\n\nFigure 6 shows the processing order of the pipeline. With the raw data, the algorithm first identifies missing data in \nthe raw trajectory and recorded the indices for the missing for subsequent processing. Then, if the users supplied \npoints on the movement surface when instantiating the `Trajectory` class, the algorithm will automatically compute and \napply the relevant spatial transformation to the raw movement trajectory. Subsequently, the transformed trajectory is \nsmoothed with the low-pass Butterworth filter, and its second- and third-order temporal derivatives are computed and \nsmoothed using the same filter. This step yields the velocity and acceleration vectors along each dimension of the \nmovement. Based on the velocity profile, movement boundaries can be determined based on the velocity threshold \nspecified earlier, which in turn yields MT, RT, and the start and end positions. Finally, the movement boundaries also \nspecify the movement trajectory that is parameterized using the third-order B-spline. To access the final, normalized \ntrajectory, the users can simply refer to the fields `x_movement_fit`, `y_movement_fit`, and `z_movement_fit`. There is also \na series of other publically accessible fields to which the readers should refer to the documentation to find the \nrelevant ones specific to their purposes.\n\n![pipeline](./img/automated%20procedure.png)\n\n*Figure 6. The automated data processing procedure. See text for explanations.* \n\nAfter going through the data processing pipeline, users can manually check for the validity of the missing data handling \nusing the class function `debug_plots`, which creates a plot of displacement and velocity (Figure 7a):\n\n```python\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(2, 1)\ntrajectory.debug_plots(fig=fig, axs=axs)\n```\n\nAs illustrated in the figure caption, the debug plots provide relevant information about the movement, especially \nmovement onset and termination times as well as locations of missing data. As mentioned in the (Missing data)[#missing-data] subsection, \nit is important to examine the locations and extent of the missing data to determine whether the interpolated missing \nvalues would introduce artifacts to the aiming trajectory. Readers can refer to the sample analysis code for potential \nways to integrate the inspection component into their analysis.\n\n| a   | ![debug_single](./img/debug_plots_demo.png) | b   | ![debug_traj](./img/debug_trajectory_demo.png) |\n|-----|---------------------------------------------|-----|------------------------------------------------|\n\n*Figure 7. Demonstrations of the debug plots. (a) Sample plots are generated by the `debug_plots` function. Top: The \ndisplacement trajectories for the x- (red), y- (red), and z-axis (blue). The vertical cyan and magenta lines represent \nthe instances of movement onset and termination, respectively, whereas the horizontal cyan and magenta dotted lines \nrepresent the displacement values at movement onset and termination for the primary (x) and secondary (z) movement axes. \nThe narrow vertical dotted lines and the black segments indicate missing data due to occlusion. Bottom: The \ncorresponding smoothed velocity along each axis based on the same color scheme. (b) A sample plot generated by the \n`debug_plots_trajectory` method from the `TrajectoryMean` class. Individual trials for this condition were plotted as \nseparate lines marked with their corresponding indices, whereas the green thick line represents the mean trajectory. \nAs apparent from the figure, the participants were initially aiming in the wrong direction in trial 5 but corrected the \nmovement trajectory halfway through the aim. This trial could be considered an outlier and warrants removal.*\n\n#### Mean trajectories\n\nGiven the parameterized individual trajectories, it is also possible to compute the mean trajectories. The \n`TrajectoryMean` class was designed to streamline this process. For instance, users can instantiate this class for \neach participant and each unique combination of conditions. Then, they can just use the class method, `add_trajectory`, \nto store the instances of the `Trajectory` that contains data from a single trial. Once the `TrajectoryMean` object is \npopulated with all the trials, the user can optionally visualize the entire set with the class method \n`debug_plots_trajectory` (Figure 7b). As the caption explains, users can use this method to identify potential outliers \nbased on the aiming trajectory. To remove a trajectory from the `TrajectoryMean` object, users can simply call the \n`remove_trajectory` method and supply it with the indices of the trajectories to be removed. The sample analysis script \npresents an example of how users can take advantage of this functionality for data inspection.\n\nAfter setting up the `TrajectoryMean` object, the users can calculate mean trajectories with the \n`compute_mean_trajectory` method. Because the instances of the Trajectory class already have various trajectories \nparameterized using the B-spline method, deriving the mean trajectories simply entails taking the mean values at each \nnormalized time step. By default, the normalized movement trajectories (e.g., `x_movement_fit`, etc.) are used to \ncalculate the mean trajectories, which are simply saved as a public field of the class (e.g., x_mean and x_sd for \nthe mean and standard deviations, respectively). However, users can optionally calculate mean trajectories for \nother fields. For instance, the Trajectory class also contains normalized velocity trajectories within the movement \nrange (e.g., `x_vel_movement_fit`). To obtain the mean velocity trajectories, the users can set the method’s input \ntraj_means based on the variable of interest. The name of the public fields that store the resulting mean and standard \ndeviations always starts with the axis name (e.g., `x`), followed by an optional `post_script` (e.g., `_velocity`) and the \n`_mean` or `_sd` keywords (e.g., `x_velocity_mean`).\n\n```python\nfrom tathum.trajectory_mean import TrajectoryMean\n\n# initialize a TrajectoryMean object\ntrajectory_mean = TrajectoryMean()\n\n# store a Trajectory object from a single trial\ntrajectory_mean.add_trajectory(trajectory)\n\n# remove a Trajectory object based on its index\ntrajectory_mean.remove_trajectory(trajectory_ind)\n\n# compute the mean trajectory after all trial-based Trajectory objects are added\ntrajectory_mean.compute_mean_trajectory()\n\n# plot all the individual trajectories and the mean trajectory\nfig_traj, ax_traj = plt.subplots(1, 1)\ntrajectory_mean.debug_plots_trajectory(fig=fig_traj, ax=ax_traj)\n```\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/xywang01/TAT-HUM/archive/refs/tags/0.1.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/xywang01/TAT-HUM",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tathum",
    "package_url": "https://pypi.org/project/tathum/",
    "platform": null,
    "project_url": "https://pypi.org/project/tathum/",
    "project_urls": {
      "Download": "https://github.com/xywang01/TAT-HUM/archive/refs/tags/0.1.tar.gz",
      "Homepage": "https://github.com/xywang01/TAT-HUM"
    },
    "release_url": "https://pypi.org/project/tathum/0.3/",
    "requires_dist": [
      "numpy",
      "pandas",
      "scipy",
      "scikit-spatial",
      "vg",
      "pytransform3d",
      "matplotlib",
      "seaborn",
      "jupyter"
    ],
    "requires_python": "",
    "summary": "TAT-HUM: Trajectory Analysis Toolkit for Human Movement",
    "version": "0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17276378,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "53afdf22f2fa07f151c3ef8180c8712c4a600e28856df145f1b094f00e4acf0c",
          "md5": "8ec005ae1e1ccde1d186979c2063c02f",
          "sha256": "8eb5d20524fe5e858f323c174260a36d6192a9b9ff46a6c067d07dfe4b64e053"
        },
        "downloads": -1,
        "filename": "tathum-0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8ec005ae1e1ccde1d186979c2063c02f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16189,
        "upload_time": "2023-03-07T16:13:10",
        "upload_time_iso_8601": "2023-03-07T16:13:10.163334Z",
        "url": "https://files.pythonhosted.org/packages/53/af/df22f2fa07f151c3ef8180c8712c4a600e28856df145f1b094f00e4acf0c/tathum-0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f95cfec5bc0d3da10ea155d86197f4c25d8bad885dfe6490e0f7eaac28bbe70d",
          "md5": "9d531ad6cdbed63bc5352179ccfc9414",
          "sha256": "11ac497ac2ae36d28e9eddd63c6b5e81e707c794b38bbae8cc725c52ef158c48"
        },
        "downloads": -1,
        "filename": "tathum-0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9d531ad6cdbed63bc5352179ccfc9414",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16191,
        "upload_time": "2023-03-07T16:17:23",
        "upload_time_iso_8601": "2023-03-07T16:17:23.854468Z",
        "url": "https://files.pythonhosted.org/packages/f9/5c/fec5bc0d3da10ea155d86197f4c25d8bad885dfe6490e0f7eaac28bbe70d/tathum-0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "80e0c6e91a6a2b5e8001163357cf328a860bf41edddedb25721af3cb5e81e509",
          "md5": "1a9e569c14342ea733244d7b5cffddf5",
          "sha256": "5e6e3fd95a74544aad79c32b4cc61f1c4b1a74753b007bf0fa74c5b4eb197b5a"
        },
        "downloads": -1,
        "filename": "tathum-0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1a9e569c14342ea733244d7b5cffddf5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14148,
        "upload_time": "2023-03-07T16:17:44",
        "upload_time_iso_8601": "2023-03-07T16:17:44.097301Z",
        "url": "https://files.pythonhosted.org/packages/80/e0/c6e91a6a2b5e8001163357cf328a860bf41edddedb25721af3cb5e81e509/tathum-0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "43eb1be8c43ea9daa63b50c5d5d2ed6c5f5cc2a54847f0e9b110feac12cb7a82",
          "md5": "4adced090c56360fb17229a5fcd75e2d",
          "sha256": "28e96b985828854cb30eccbeaca0ac1f9b22531a682d80e76f53ba56ee24274c"
        },
        "downloads": -1,
        "filename": "tathum-0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4adced090c56360fb17229a5fcd75e2d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 27101,
        "upload_time": "2023-03-13T20:12:05",
        "upload_time_iso_8601": "2023-03-13T20:12:05.506453Z",
        "url": "https://files.pythonhosted.org/packages/43/eb/1be8c43ea9daa63b50c5d5d2ed6c5f5cc2a54847f0e9b110feac12cb7a82/tathum-0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "42373c20aa4066bfd85412688d07fd8cc8f15cb01624c5371761d71253cd8c73",
          "md5": "a2be89169c69dc6d0b7f9dca461a471c",
          "sha256": "ed4200c8f19aea599967a377e9b854f92d55b7165091235d24906f7e8fb12c34"
        },
        "downloads": -1,
        "filename": "tathum-0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "a2be89169c69dc6d0b7f9dca461a471c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 45501,
        "upload_time": "2023-03-13T20:12:09",
        "upload_time_iso_8601": "2023-03-13T20:12:09.306881Z",
        "url": "https://files.pythonhosted.org/packages/42/37/3c20aa4066bfd85412688d07fd8cc8f15cb01624c5371761d71253cd8c73/tathum-0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "43eb1be8c43ea9daa63b50c5d5d2ed6c5f5cc2a54847f0e9b110feac12cb7a82",
        "md5": "4adced090c56360fb17229a5fcd75e2d",
        "sha256": "28e96b985828854cb30eccbeaca0ac1f9b22531a682d80e76f53ba56ee24274c"
      },
      "downloads": -1,
      "filename": "tathum-0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "4adced090c56360fb17229a5fcd75e2d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 27101,
      "upload_time": "2023-03-13T20:12:05",
      "upload_time_iso_8601": "2023-03-13T20:12:05.506453Z",
      "url": "https://files.pythonhosted.org/packages/43/eb/1be8c43ea9daa63b50c5d5d2ed6c5f5cc2a54847f0e9b110feac12cb7a82/tathum-0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "42373c20aa4066bfd85412688d07fd8cc8f15cb01624c5371761d71253cd8c73",
        "md5": "a2be89169c69dc6d0b7f9dca461a471c",
        "sha256": "ed4200c8f19aea599967a377e9b854f92d55b7165091235d24906f7e8fb12c34"
      },
      "downloads": -1,
      "filename": "tathum-0.3.tar.gz",
      "has_sig": false,
      "md5_digest": "a2be89169c69dc6d0b7f9dca461a471c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 45501,
      "upload_time": "2023-03-13T20:12:09",
      "upload_time_iso_8601": "2023-03-13T20:12:09.306881Z",
      "url": "https://files.pythonhosted.org/packages/42/37/3c20aa4066bfd85412688d07fd8cc8f15cb01624c5371761d71253cd8c73/tathum-0.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}