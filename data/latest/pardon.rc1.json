{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# PARDON\n\nPardon is a Python library developed as a machine learning (ML) and data transformation accelerator, to rapidly prepare data as well as test and deploy supervised ML models, by massively reducing the amount of code required during development.\n\nThe library can be used to perform data cleansing and transformation, as well as model and feature selection, data visualisations, and prediction auditing. Data can be quickly prepared for use with ML algorithms, and all data transformations are recorded in a 'script'. This 'script' is then automatically applied to data prior to predictions, which ensures data consistency, transparency, and removes the need for multi-stage data engineering while reducing the chances of error.\n\nThe intention of this library is not to replace or automate the work of a data scientist, but to enable integration work to begin early by rapidly deploying a working machine learning model. This then allows the data science work to be completed in parallel, with the final model being deployed on completion.\n\nThe pardon library functionality includes:\n•\tData transformation\n•\tData cleansing\n•\tFeature selection\n•\tFeature engineering\n•\tModel selection\n•\tClass balancing\n•\tPrediction auditing\n•\tPrincipal Component Analysis\n•\tData scaling\n•\tData encoding\n•\tSentiment analysis\n•\tHyperparameter tuning\n•\tModel “explainability”\n•\tModel performance metrics\n•\tModel training, testing and validation\n•\tIdentifying data outliers\n•\tTesting model output in a REST API\n•\tData visualisation\n•\tUse unsupervised learning to cluster data\n\n\n## Installation\n\nThe easiest way to install and use pardon is to pip install using pip or the latest pardon package whl file.\n```bash\npip install pardon\n```\n```bash\npip install pardon-1.0.0-py3-none-any.whl\n```\n\n## Usage\n\n```python\nimport pardon\n\n# create a new ML Data object. The minimum requirements are a data file or data stream, and the name of the column you're trying to predict.\npaml = pardon.Pardon(data='\\\\your\\\\specified\\\\directory\\\\data_file.csv', target='column_name_to_predict')\n\n# rapid ml will perform the following steps by default:\n# -> 1. Perform the minimum required data cleansing and transformation to prepare your data for model training.\n# -> 2. Trains multiple models using the data and selects the best performing model.\n# -> 3. Where applicable, display a summary of the model performance.\n# -> 4. Save the model as a .pkl file to the location specified. You must ensure the filename is a .pkl file.\n# -> 5. Display the output scores of all the models tested.\npaml.rapid_ml(model_fullpath='\\\\your\\\\specified\\\\directory\\\\your_model_name.pkl')\n\n# you can run a test on the localhost to see the API output\npardon.TestAPI(model_fullpath='\\\\your\\\\specified\\\\directory\\\\your_model_name.pkl')\n```\n\n## Contributing\nFor changes, please contact pardonaccelerator@gmail.com to discuss your amendments or requirements.\n\nPlease make sure to update tests as appropriate.\n\n## License\n[MIT](https://choosealicense.com/licenses/mit/)\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pardon",
    "package_url": "https://pypi.org/project/pardon/",
    "platform": null,
    "project_url": "https://pypi.org/project/pardon/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/pardon/2.0.5/",
    "requires_dist": [
      "category-encoders (==2.3.0)",
      "click (==7.1.2)",
      "cloudpickle (==2.0.0)",
      "colorama (==0.4.4)",
      "cycler (==0.11.0)",
      "dill (==0.3.4)",
      "Flask (==1.1.4)",
      "fonttools (==4.33.3)",
      "imbalanced-learn (==0.9.0)",
      "imblearn (==0.0)",
      "itsdangerous (==1.1.0)",
      "Jinja2 (==2.11.3)",
      "joblib (==1.1.0)",
      "kiwisolver (==1.4.2)",
      "llvmlite (==0.39.1)",
      "MarkupSafe (==2.0.1)",
      "matplotlib (==3.5.1)",
      "neattext (==0.1.3)",
      "nltk (==3.7)",
      "numba (==0.56.4)",
      "numpy (==1.23.4)",
      "openpyxl (==3.0.10)",
      "packaging (==21.3)",
      "pandas (==1.4.3)",
      "patsy (==0.5.2)",
      "Pillow (==9.1.0)",
      "plotly (==5.6.0)",
      "PyAutoGUI (==0.9.53)",
      "pyparsing (==3.0.9)",
      "python-dateutil (==2.8.2)",
      "pytz (==2022.1)",
      "PyYAML (==6.0)",
      "regex (==2022.4.24)",
      "schemdraw (==0.14)",
      "scikit-learn (==1.0.2)",
      "scipy (==1.8.0)",
      "seaborn (==0.11.2)",
      "shap (==0.41.0)",
      "six (==1.16.0)",
      "slicer (==0.0.7)",
      "statsmodels (==0.13.2)",
      "streamlit (==1.11.0)",
      "streamlit-aggrid (==0.2.3.post2)",
      "tenacity (==8.0.1)",
      "threadpoolctl (==3.1.0)",
      "tqdm (==4.64.0)",
      "varname (==0.10.0)",
      "Werkzeug (==1.0.1)",
      "xgboost (==1.5.2)"
    ],
    "requires_python": ">=3.8.1, <3.11",
    "summary": "The Data Transformation and Machine Learning Accelerator",
    "version": "2.0.5",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16358323,
  "releases": {
    "2.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c66b981a505d14f41129d9ed6241c2b97315e3f313f14c9f877031214cbcb4f0",
          "md5": "5f4753057c9b9ed0a55a69ec9d0f4494",
          "sha256": "6a2913a8e384678376ca3c193f670aa2e491ac72d87df4d190f44107c22ba3e4"
        },
        "downloads": -1,
        "filename": "pardon-2.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5f4753057c9b9ed0a55a69ec9d0f4494",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8.1, <3.11",
        "size": 125880,
        "upload_time": "2022-12-09T21:38:36",
        "upload_time_iso_8601": "2022-12-09T21:38:36.933778Z",
        "url": "https://files.pythonhosted.org/packages/c6/6b/981a505d14f41129d9ed6241c2b97315e3f313f14c9f877031214cbcb4f0/pardon-2.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c66b981a505d14f41129d9ed6241c2b97315e3f313f14c9f877031214cbcb4f0",
        "md5": "5f4753057c9b9ed0a55a69ec9d0f4494",
        "sha256": "6a2913a8e384678376ca3c193f670aa2e491ac72d87df4d190f44107c22ba3e4"
      },
      "downloads": -1,
      "filename": "pardon-2.0.5-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "5f4753057c9b9ed0a55a69ec9d0f4494",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8.1, <3.11",
      "size": 125880,
      "upload_time": "2022-12-09T21:38:36",
      "upload_time_iso_8601": "2022-12-09T21:38:36.933778Z",
      "url": "https://files.pythonhosted.org/packages/c6/6b/981a505d14f41129d9ed6241c2b97315e3f313f14c9f877031214cbcb4f0/pardon-2.0.5-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}