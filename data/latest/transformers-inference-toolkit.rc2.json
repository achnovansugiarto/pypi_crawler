{
  "info": {
    "author": "Alexey Burlakov",
    "author_email": "feraturdev@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# Transformers Inference Toolkit\n[![PyPI](https://img.shields.io/pypi/v/transformers-inference-toolkit)](https://pypi.org/project/transformers-inference-toolkit/)\n[![](https://img.shields.io/badge/%E2%96%BA-%20Changelog-blue)](https://github.com/feratur/transformers-inference-toolkit/blob/main/CHANGELOG.md)\n\nðŸ¤— [Transformers](https://github.com/huggingface/transformers) library provides great API for manipulating pre-trained NLP (as well as CV and Audio-related) models. However, preparing ðŸ¤— Transformers models for use in production usually requires additional effort. The purpose of `transformers-inference-toolkit` is to get rid of boilerplate code and to simplify automatic optimization and inference process of Huggingface Transformers models.\n\n## Installation\nUsing `pip`:\n```bash\npip install transformers-inference-toolkit\n```\n\n## Optimization\nThe original ðŸ¤— Transformers library includes `transformers.onnx` package, which can be used to convert PyTorch or TensorFlow models into [ONNX](https://onnx.ai/) format. This Toolkit extends this functionality by giving the user an opportunity to automatically [optimize ONNX model graph](https://onnxruntime.ai/docs/performance/graph-optimizations.html) - this is similar to what ðŸ¤— [Optimum](https://github.com/huggingface/optimum) library does, but ðŸ¤— Optimum currently has limited support for locally stored pre-trained models as well as for models of less popular architectures (for example, [MPNet](https://github.com/microsoft/MPNet)).\n\nAside from ONNX conversion the Toolkit also supports resaving PyTorch models with half-precision and setting up [DeepSpeed Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/).\n\n### Prerequisite\nThe Toolkit expects your pretrained model (in PyTorch format) and tokenizer to be saved (using `save_pretrained()` method) inside a common parent directory in `model` and `tokenizer` folders respectively. This is how a file structure of `toxic-bert` model should look like:\n```bash\ntoxic-bert\nâ”œâ”€â”€ model\nâ”‚Â Â  â”œâ”€â”€ config.json\nâ”‚Â Â  â””â”€â”€ pytorch_model.bin\nâ””â”€â”€ tokenizer\n    â”œâ”€â”€ merges.txt\n    â”œâ”€â”€ special_tokens_map.json\n    â”œâ”€â”€ tokenizer_config.json\n    â””â”€â”€ vocab.json\n```\n\n### How to use\nMost of the popular Transformer model architectures (like BERT and its variations) can be converted with a single command:\n```python\nfrom transformers_inference_toolkit import (\n    Feature,\n    OnnxModelType,\n    OnnxOptimizationLevel,\n    optimizer,\n)\n\noptimizer.pack_onnx(\n    input_path=\"toxic-bert\",\n    output_path=\"toxic-bert-optimized\",\n    feature=Feature.SEQUENCE_CLASSIFICATION,\n    for_gpu=True,\n    fp16=True,\n    optimization_level=OnnxOptimizationLevel.FULL,\n)\n```\nIf your model architecture is not supported out-of-the-box (described [here](https://huggingface.co/docs/transformers/serialization)) you can try writing a custom OnnxConfig class:\n```python\nfrom collections import OrderedDict\nfrom transformers.onnx import OnnxConfig\n\nclass MPNetOnnxConfig(OnnxConfig):\n    @property\n    def default_onnx_opset(self):\n        return 14\n\n    @property\n    def inputs(self):\n        dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n        return OrderedDict(\n            [\n                (\"input_ids\", dynamic_axis),\n                (\"attention_mask\", dynamic_axis),\n            ]\n        )\n\noptimizer.pack_onnx(\n    input_path=\"all-mpnet-base-v2\",\n    output_path=\"all-mpnet-base-v2-optimized\",\n    feature=Feature.DEFAULT,\n    custom_onnx_config_cls=MPNetOnnxConfig,\n)\n```\nONNX is not the only option, it is also possible to resave the model for future inference simply using PyTorch (`optimizer.pack_transformers()` method, `force_fp16` argument to save in half-precision) or [DeepSpeed Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/) (`optimizer.pack_deepspeed()` method):\n```python\noptimizer.pack_deepspeed(\n    input_path=\"gpt-neo\",\n    output_path=\"gpt-neo-optimized\",\n    feature=Feature.CAUSAL_LM,\n    replace_with_kernel_inject=True,\n    mp_size=1,\n)\n```\nAfter calling `optimizer` methods the model and tokenizer would be saved at `output_path`. The output directory will also contain `metadata.json` file that is necessary for the `Predictor` object (described below) to correctly load the model:\n```bash\ntoxic-bert-optimized\nâ”œâ”€â”€ metadata.json\nâ”œâ”€â”€ model\nâ”‚Â Â  â”œâ”€â”€ config.json\nâ”‚Â Â  â””â”€â”€ model.onnx\nâ””â”€â”€ tokenizer\n    â”œâ”€â”€ special_tokens_map.json\n    â”œâ”€â”€ tokenizer.json\n    â””â”€â”€ tokenizer_config.json\n```\n## Prediction\nAfter model and tokenizer are packaged using one of the `optimizer` methods, it is possible to initialize a `Predictor` object:\n```python\n>>> from transformers_inference_toolkit import Predictor\n>>> \n>>> predictor = Predictor(\"toxic-bert-optimized\", cuda=True)\n>>> print(predictor(\"I hate this!\"))\n{'logits': array([[ 0.02940369, -7.0195312 , -4.7890625 , -6.0664062 , -5.625     ,\n        -6.09375   ]], dtype=float32)}\n```\nThe `Predictor` object can be simply called with tokenizer arguments (similar to ðŸ¤— Transformers `pipeline`s, `return_tensors` argument can be omitted, `padding` and `truncation` are `True` by default). For text generation tasks `Predictor.generate()` method (with [generation arguments](https://huggingface.co/docs/transformers/main_classes/text_generation)) can be used:\n```python\n>>> predictor = Predictor(\"gpt-neo-optimized\", cuda=True)\n>>> predictor.generate(\n...     \"Tommy: Hi Mark!\",\n...     do_sample=True,\n...     top_p=0.9,\n...     num_return_sequences=3,\n...     max_new_tokens=5,\n... )\n['Tommy: Hi Mark!\\nMadelyn: Hello', 'Tommy: Hi Mark! Itâ€™s so', 'Tommy: Hi Mark! How are you?\\n']\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/feratur/transformers-inference-toolkit",
    "keywords": "",
    "license": "Apache-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "transformers-inference-toolkit",
    "package_url": "https://pypi.org/project/transformers-inference-toolkit/",
    "platform": null,
    "project_url": "https://pypi.org/project/transformers-inference-toolkit/",
    "project_urls": {
      "Homepage": "https://github.com/feratur/transformers-inference-toolkit",
      "Repository": "https://github.com/feratur/transformers-inference-toolkit"
    },
    "release_url": "https://pypi.org/project/transformers-inference-toolkit/0.1.1/",
    "requires_dist": [
      "transformers (>=4.24.0,<5.0.0)",
      "torch (>=1.12.1,<2.0.0)",
      "accelerate (>=0.14.0,<0.15.0)",
      "onnxruntime-gpu (>=1.13.1,<2.0.0)",
      "onnx (>=1.12.0,<2.0.0)",
      "deepspeed (>=0.7.4,<0.8.0)"
    ],
    "requires_python": ">=3.7,<3.11",
    "summary": "A collection of helper methods to simplify optimization and inference of Huggingface Transformers-based models",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16137305,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2d163d203bd527b18673c42faaae297adb955d3659e3eb021b6ab27908f77f16",
          "md5": "91883904e890d62f6ca8d42e28f5c9eb",
          "sha256": "bfc85c1d3d5f0c4ba990b09370addb3b9cd7288b94bad39f482de2f2239f822d"
        },
        "downloads": -1,
        "filename": "transformers_inference_toolkit-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "91883904e890d62f6ca8d42e28f5c9eb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<3.11",
        "size": 14043,
        "upload_time": "2022-11-29T20:43:19",
        "upload_time_iso_8601": "2022-11-29T20:43:19.734916Z",
        "url": "https://files.pythonhosted.org/packages/2d/16/3d203bd527b18673c42faaae297adb955d3659e3eb021b6ab27908f77f16/transformers_inference_toolkit-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "20a695e4e8be7456fb6e24b4b9f724b274d7fcd8aae6fb4cb01c0018d3ce5109",
          "md5": "6e03bd18983a09980cc7be80122ede21",
          "sha256": "8faf860cc76f7322b5b86f288bc96dc7d7b0900bc8e287604ca748df9bd421fe"
        },
        "downloads": -1,
        "filename": "transformers_inference_toolkit-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "6e03bd18983a09980cc7be80122ede21",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<3.11",
        "size": 13795,
        "upload_time": "2022-11-29T20:43:21",
        "upload_time_iso_8601": "2022-11-29T20:43:21.039468Z",
        "url": "https://files.pythonhosted.org/packages/20/a6/95e4e8be7456fb6e24b4b9f724b274d7fcd8aae6fb4cb01c0018d3ce5109/transformers_inference_toolkit-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6eb683fca844d24d8203de77844f55d028de57a3b40f996b7c3cc0e252a8ddd0",
          "md5": "f3a258118236fb6dd01b32b9800653d6",
          "sha256": "a68bd2e19154a7a587c23dc3858b9ce53b9b9c411fa350b6ee349e101d93540e"
        },
        "downloads": -1,
        "filename": "transformers_inference_toolkit-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f3a258118236fb6dd01b32b9800653d6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<3.11",
        "size": 15548,
        "upload_time": "2022-12-17T23:52:26",
        "upload_time_iso_8601": "2022-12-17T23:52:26.880951Z",
        "url": "https://files.pythonhosted.org/packages/6e/b6/83fca844d24d8203de77844f55d028de57a3b40f996b7c3cc0e252a8ddd0/transformers_inference_toolkit-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dc1200f97d1672cfb7b4f835a04afc9ec78150866e0d891fe68ac2a0916cc282",
          "md5": "7f4ebed09ccd98fd04eafbe5dee32cc1",
          "sha256": "50a491efffe49a1fc04d1adaaf8b34be6de094aa733cad4378b87c66b98630a9"
        },
        "downloads": -1,
        "filename": "transformers_inference_toolkit-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7f4ebed09ccd98fd04eafbe5dee32cc1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<3.11",
        "size": 15078,
        "upload_time": "2022-12-17T23:52:28",
        "upload_time_iso_8601": "2022-12-17T23:52:28.252056Z",
        "url": "https://files.pythonhosted.org/packages/dc/12/00f97d1672cfb7b4f835a04afc9ec78150866e0d891fe68ac2a0916cc282/transformers_inference_toolkit-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6eb683fca844d24d8203de77844f55d028de57a3b40f996b7c3cc0e252a8ddd0",
        "md5": "f3a258118236fb6dd01b32b9800653d6",
        "sha256": "a68bd2e19154a7a587c23dc3858b9ce53b9b9c411fa350b6ee349e101d93540e"
      },
      "downloads": -1,
      "filename": "transformers_inference_toolkit-0.1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f3a258118236fb6dd01b32b9800653d6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7,<3.11",
      "size": 15548,
      "upload_time": "2022-12-17T23:52:26",
      "upload_time_iso_8601": "2022-12-17T23:52:26.880951Z",
      "url": "https://files.pythonhosted.org/packages/6e/b6/83fca844d24d8203de77844f55d028de57a3b40f996b7c3cc0e252a8ddd0/transformers_inference_toolkit-0.1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dc1200f97d1672cfb7b4f835a04afc9ec78150866e0d891fe68ac2a0916cc282",
        "md5": "7f4ebed09ccd98fd04eafbe5dee32cc1",
        "sha256": "50a491efffe49a1fc04d1adaaf8b34be6de094aa733cad4378b87c66b98630a9"
      },
      "downloads": -1,
      "filename": "transformers_inference_toolkit-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "7f4ebed09ccd98fd04eafbe5dee32cc1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7,<3.11",
      "size": 15078,
      "upload_time": "2022-12-17T23:52:28",
      "upload_time_iso_8601": "2022-12-17T23:52:28.252056Z",
      "url": "https://files.pythonhosted.org/packages/dc/12/00f97d1672cfb7b4f835a04afc9ec78150866e0d891fe68ac2a0916cc282/transformers_inference_toolkit-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}