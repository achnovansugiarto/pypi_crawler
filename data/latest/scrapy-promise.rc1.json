{
  "info": {
    "author": "Tony Wu",
    "author_email": "tony(dot)wu(at)nyu(dot)edu@inval.id",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Framework :: Scrapy",
      "Intended Audience :: Developers",
      "Intended Audience :: End Users/Desktop",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Software Development :: Libraries"
    ],
    "description": "# scrapy-promise\n\nPromise API for making Scrapy requests.\n\n## Usage & Examples\n\n```python\nfrom scrapy_promise import fetch\n```\n\nThe `Promise` here works like Promise in JavaScript. If you are new to Promise, a great starting point would be MDN's\n[Promise API reference](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise)\nand the guide to [Using Promises](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Using_promises).\n\n### Creating and making requests\n\n`fetch()` accepts all arguments that `scrapy.http.Request` accepts, except for `callback` and `errback`\n\n```python\n>>> fetch('https://example.org/login', method='POST', meta={'username': 'admin'})\n```\n\n`fetch()` returns a `Promise` object, which is an iterator/generator. You can return it directly in `start_requests`,\nor `yield from` it in an existing callback.\n\n### Adding handlers\n\nIf you only call `fetch()` and yield from it, then all it does is storing the response once the request is finished:\n\n```python\nrequest = fetch('https://httpbin.org/ip')\nyield from request\n# When the request is done\n>>> request.is_fulfilled\nTrue\n>>> request.get()\n<200 https://httpbin.org/ip>\n```\n\n`fetch()` returns a `Promise` object. Call its `.then()` method and provide a callable, and `Promise` will call it once there is a response.\n\n`.then()` returns another `Promise` that you can `yield from`:\n\n```python\ndef on_fulfill(response: TextResponse):\n    # You can yield items from your handler\n    # just like you would in a Scrapy callback\n    yield Item(response)\n\n>>> yield from fetch(...).then(on_fulfill)\n```\n\nYou can also attach an error handler with `.catch()`, which will receive either a Twisted\n[`Failure`](https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html) or an Exception:\n\n```python\ndef on_reject(exc: Union[Failure, Exception]):\n    if isinstance(exc, Failure):\n        exc = exc.value\n    ...\n\n>>> yield from fetch(...).then(on_fulfill).catch(on_reject)\n# will catch both exceptions during the request\n# and exceptions raised in on_fulfill\n```\n\n### Branching and chaining\n\nBecause `.then()` and `.catch()` return another Promise, you can chain additional handlers.\n\nSubsequent handlers\nwill receive the **return** value of the previous handler. _This is different from an ordinary Scrapy callback,_\nwhere returning a value has no effect:\n\n```python\nyield from (\n    fetch('https://httpbin.org/ip')\n    .then(parse_json)   # returns dict\n    .then(create_item)  # will be passed the dict from the previous handler\n    .catch(lambda exc: logging.getLogger().error(exc)))\n```\n\n**Dynamic chaining**: If you return another `fetch()` request in your handler, that request will be scheduled,\nand the next handler will be called with the `Response` of this new request. This lets you schedule multiple\nrequests in order. \n\n```python\nyield from (\n    fetch('https://httpbin.org/ip')\n    # A second Request is created from the response of the first one and is scheduled.\n    .then(lambda response: fetch(json.loads(response.text)['origin']))\n    .then(lambda response: (yield Item(response)))\n    .catch(lambda exc: logging.getLogger().error(exc)))\n```\n\nNote that only the request you _returned_ will be connected to subsequent handlers, `Request`s that are yielded\nin the middle of the handler will be scheduled directly by Scrapy.\n\nYou can also attach multiple handlers to one request, and they will be evaluated in the order they were\ndeclared:\n\n```python\nresource = fetch(...)\nresource.then(save_token)\nresource.then(parse_html).catch(log_error)\nresource.then(next_page).catch(stop_spider)\nyield from resource  # Evaluating any Promise in a chain/branch causes\n                     # the entire Promise tree to be evaluated.\n```\n\n### Promise aggregation functions\n\n`Promise` provides several aggregation functions that let you better control the how the requests are scheduled.\n\n```python\nfrom notcallback import Promise  # dependency\n```\n\n**`Promise.all()`** will only fulfill when all requests are made successfully, and will reject as soon as one of\nthe requests failed. If all the requests succeed, the handler will receive a list of Responses:\n\n```python\ndef parse_pages(responses: Tuple[TextResponse]):\n    for r in responses:\n        ...\n\nyield from Promise.all(*[fetch(url) for url in urls]).then(parse_pages)\n```\n\n**`Promise.race()`** will fulfill as soon as one of the requests is fulfilled/rejected.\n\n```python\ndef select_fastest_cdn():\n    yield from (\n        Promise.race(*[fetch(url, method='HEAD') for url in cdn_list])\n        .then(crawl_server))\n```\n\n**`Promise.all_settled()`** always fulfills when all requests are finished, regardless of whether or not\nthey are successful. The handler will receive a list of `Promise`s whose value (the response) can be accessed\nwith the `.get()` method:\n\n```python\ndef report(promises: Tuple[Promise]):\n    for promise in promises:\n        result = promise.get()\n        if isinstance(result, Response):\n            log.info(f'Crawled {result.url}')\n        else:\n            log.warn(f'Encountered error {result}')\n\nyield from Promise.all_settled(*[fetch(u) for u in urls]).then(report)\n```\n\n**`Promise.any()`** fulfills with the first request that fulfills, and rejects if no request is successful:\n\n```python\ndef download(response):\n    ...\n\nyield from (\n    Promise.any(*[fetch(u) for u in urls])\n    .then(download)\n    .catch(lambda exc: log.warn('No valid URL!')))\n```\n\nFor more info on the `Promise API`, see [notcallback](https://github.com/monotony113/notcallback)\n\n## See also\n\nOther ways to schedule requests within a callback:\n- [`twisted.internet.defer.inlineCallbacks(f)`](https://twistedmatrix.com/documents/current/api/twisted.internet.defer.inlineCallbacks.html)\n- [scrapy-inline-requests](https://github.com/rmax/scrapy-inline-requests)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/monotony113/scrapy-promise",
    "keywords": "promise scrapy",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-promise",
    "package_url": "https://pypi.org/project/scrapy-promise/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-promise/",
    "project_urls": {
      "Homepage": "https://github.com/monotony113/scrapy-promise"
    },
    "release_url": "https://pypi.org/project/scrapy-promise/0.0.6/",
    "requires_dist": [
      "notcallback (>=0.0.8)"
    ],
    "requires_python": ">=3.6",
    "summary": "Promise-style workflow for Scrapy",
    "version": "0.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8160962,
  "releases": {
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "16a6e3a86fc65368e20a3f29d668cd270289cd3a1f03db053a0ff443acc13e99",
          "md5": "8c3d1d49e5073fa702d1b5800bbecb93",
          "sha256": "4a302fc307e822f420326bb97d4c27e885f66cfb49658d529ba50e1b8f009996"
        },
        "downloads": -1,
        "filename": "scrapy_promise-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8c3d1d49e5073fa702d1b5800bbecb93",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 7314,
        "upload_time": "2020-09-11T02:06:04",
        "upload_time_iso_8601": "2020-09-11T02:06:04.204011Z",
        "url": "https://files.pythonhosted.org/packages/16/a6/e3a86fc65368e20a3f29d668cd270289cd3a1f03db053a0ff443acc13e99/scrapy_promise-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "84def11bceaae2c8884da578748d6f86532c5141a469d38ce78835e39e787955",
          "md5": "a05eb936d17e63fb29631d08384a80e7",
          "sha256": "527780c89d82ac033d091d3da61462d5c062b9123319565d08aa7cc766136ddb"
        },
        "downloads": -1,
        "filename": "scrapy_promise-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "a05eb936d17e63fb29631d08384a80e7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 6483,
        "upload_time": "2020-09-11T02:06:05",
        "upload_time_iso_8601": "2020-09-11T02:06:05.312878Z",
        "url": "https://files.pythonhosted.org/packages/84/de/f11bceaae2c8884da578748d6f86532c5141a469d38ce78835e39e787955/scrapy_promise-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "16a6e3a86fc65368e20a3f29d668cd270289cd3a1f03db053a0ff443acc13e99",
        "md5": "8c3d1d49e5073fa702d1b5800bbecb93",
        "sha256": "4a302fc307e822f420326bb97d4c27e885f66cfb49658d529ba50e1b8f009996"
      },
      "downloads": -1,
      "filename": "scrapy_promise-0.0.6-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8c3d1d49e5073fa702d1b5800bbecb93",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 7314,
      "upload_time": "2020-09-11T02:06:04",
      "upload_time_iso_8601": "2020-09-11T02:06:04.204011Z",
      "url": "https://files.pythonhosted.org/packages/16/a6/e3a86fc65368e20a3f29d668cd270289cd3a1f03db053a0ff443acc13e99/scrapy_promise-0.0.6-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "84def11bceaae2c8884da578748d6f86532c5141a469d38ce78835e39e787955",
        "md5": "a05eb936d17e63fb29631d08384a80e7",
        "sha256": "527780c89d82ac033d091d3da61462d5c062b9123319565d08aa7cc766136ddb"
      },
      "downloads": -1,
      "filename": "scrapy_promise-0.0.6.tar.gz",
      "has_sig": false,
      "md5_digest": "a05eb936d17e63fb29631d08384a80e7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 6483,
      "upload_time": "2020-09-11T02:06:05",
      "upload_time_iso_8601": "2020-09-11T02:06:05.312878Z",
      "url": "https://files.pythonhosted.org/packages/84/de/f11bceaae2c8884da578748d6f86532c5141a469d38ce78835e39e787955/scrapy_promise-0.0.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}