{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering"
    ],
    "description": "Fast Transformers\n=================\n\nTransformers are very successful models that achieve state of the art\nperformance in many natural language tasks. However, it is very difficult to\nscale them to long sequences due to the quadratic scaling of self-attention.\n\nThis library was developed for our research on fast attention for transformers.\nYou can find a list of our papers `in the docs\n<https://fast-transformers.github.io>`_ as well as related papers and papers\nthat we have implemented.\n\nQuick-start\n-----------\n\nThe following code builds a transformer with softmax attention and one with\nlinear attention and compares the time required by each to encode a sequence\nwith 1000 elements.\n\n.. code:: python\n\n    import torch\n    from fast_transformers.builders import TransformerEncoderBuilder\n\n    # Create the builder for our transformers\n    builder = TransformerEncoderBuilder.from_kwargs(\n        n_layers=8,\n        n_heads=8,\n        query_dimensions=64,\n        value_dimensions=64,\n        feed_forward_dimensions=1024\n    )\n\n    # Build a transformer with softmax attention\n    builder.attention_type = \"full\"\n    softmax_model = builder.get()\n\n    # Build a transformer with linear attention\n    builder.attention_type = \"linear\"\n    linear_model = builder.get()\n\n    # Construct the dummy input\n    X = torch.rand(10, 1000, 8*64)\n\n    # Prepare everythin for CUDA\n    X = X.cuda()\n    softmax_model.cuda()\n    softmax_model.eval()\n    linear_model.cuda()\n    linear_model.eval()\n\n    # Warmup the GPU\n    with torch.no_grad():\n        softmax_model(X)\n        linear_model(X)\n    torch.cuda.synchronize()\n\n    # Measure the execution time\n    softmax_start = torch.cuda.Event(enable_timing=True)\n    softmax_end = torch.cuda.Event(enable_timing=True)\n    linear_start = torch.cuda.Event(enable_timing=True)\n    linear_end = torch.cuda.Event(enable_timing=True)\n\n    with torch.no_grad():\n        softmax_start.record()\n        y = softmax_model(X)\n        softmax_end.record()\n        torch.cuda.synchronize()\n        print(\"Softmax: \", softmax_start.elapsed_time(softmax_end), \"ms\")\n        # Softmax: 144 ms (on a GTX1080Ti)\n\n    with torch.no_grad():\n        linear_start.record()\n        y = linear_model(X)\n        linear_end.record()\n        torch.cuda.synchronize()\n        print(\"Linear: \", linear_start.elapsed_time(linear_end), \"ms\")\n        # Linear: 68 ms (on a GTX1080Ti)\n\nDependencies & Installation\n---------------------------\n\nThe fast transformers library has the following dependencies:\n\n* PyTorch\n* C++ toolchain\n* CUDA toolchain (if you want to compile for GPUs)\n\nFor most machines installation should be as simple as:\n\n.. code:: bash\n\n    pip install --user pytorch-fast-transformers\n\nNote: macOS users should ensure they have `llvm` and `libomp` installed.\nUsing the `homebrew <https://brew.sh>`_ package manager, this can be\naccomplished by running `brew install llvm libomp`.\n\nDocumentation\n-------------\n\nThere exists a dedicated `documentation site\n<https://fast-transformers.github.io/>`_ but you are also encouraged to read\nthe `source code <https://github.com/idiap/fast-transformers>`_.\n\nResearch\n--------\n\nOurs\n~~~~\n\nTo read about the theory behind some attention implementations in this library\nwe encourage you to follow our research.\n\n* Transformers are RNNs: Fast Autoregressive Transformers with\n  Linear Attention (`2006.16236 <https://arxiv.org/abs/2006.16236>`_)\n* Fast Transformers with Clustered Attention\n  (`2007.04825 <https://arxiv.org/abs/2007.04825>`_)\n\nIf you found our research helpful or influential please consider citing\n\n.. code::\n\n    @inproceedings{katharopoulos_et_al_2020,\n        author = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},\n        title = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},\n        booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},\n        year = {2020}\n    }\n\n    @article{vyas_et_al_2020,\n        author={Vyas, A. and Katharopoulos, A. and Fleuret, F.},\n        title={Fast Transformers with Clustered Attention},\n        booktitle = {Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS)},\n        year={2020}\n    }\n\nBy others\n~~~~~~~~~\n\n* Efficient Attention: Attention with Linear Complexities (`1812.01243\n  <https://arxiv.org/abs/1812.01243>`_)\n* Linformer: Self-Attention with Linear Complexity (`2006.04768\n  <https://arxiv.org/abs/2006.04768>`_)\n* Reformer: The Efficient Transformer (`2001.04451\n  <https://arxiv.org/abs/2001.04451>`_)\n\nSupport, License and Copyright\n------------------------------\n\nThis software is distributed with the **MIT** license which pretty much means that\nyou can use it however you want and for whatever reason you want. All the\ninformation regarding support, copyright and the license can be found in the\n`LICENSE <https://github.com/idiap/fast-transformers/blob/master/LICENSE>`_\nfile in the repository.",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/idiap/fast-transformers",
    "keywords": "",
    "license": "MIT",
    "maintainer": "Angelos Katharopoulos, Apoorv Vyas",
    "maintainer_email": "angelos.katharopoulos@idiap.ch, avyas@idiap.ch",
    "name": "pytorch-fast-transformers",
    "package_url": "https://pypi.org/project/pytorch-fast-transformers/",
    "platform": "",
    "project_url": "https://pypi.org/project/pytorch-fast-transformers/",
    "project_urls": {
      "Homepage": "https://github.com/idiap/fast-transformers"
    },
    "release_url": "https://pypi.org/project/pytorch-fast-transformers/0.4.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Provide a library with fast transformer implementations.",
    "version": "0.4.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10073854,
  "releases": {
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2595f9b2bdee5bc35e2c3b44cb2847f7bc77ddecd89aae74c4f86f8a9451c78f",
          "md5": "1b4a84e8f1aaea3a1b41bb5cd312088d",
          "sha256": "c2823a900eb5275f7ec2f35681dccd7a6adf853206dbcb70973ada4cee098296"
        },
        "downloads": -1,
        "filename": "pytorch-fast-transformers-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1b4a84e8f1aaea3a1b41bb5cd312088d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 45417,
        "upload_time": "2020-07-01T18:11:02",
        "upload_time_iso_8601": "2020-07-01T18:11:02.121719Z",
        "url": "https://files.pythonhosted.org/packages/25/95/f9b2bdee5bc35e2c3b44cb2847f7bc77ddecd89aae74c4f86f8a9451c78f/pytorch-fast-transformers-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "004026426232fa1697bfd681cf5ab3c784f13d72028b6e132d84e2453fab948b",
          "md5": "b872dd69ce97779ab16b542382b7d79e",
          "sha256": "15b9a48d204e54e1eb413126c19556c766df9a1bf64dd0c3a1f09c30ffdd564e"
        },
        "downloads": -1,
        "filename": "pytorch-fast-transformers-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "b872dd69ce97779ab16b542382b7d79e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 45423,
        "upload_time": "2020-07-01T18:26:55",
        "upload_time_iso_8601": "2020-07-01T18:26:55.982775Z",
        "url": "https://files.pythonhosted.org/packages/00/40/26426232fa1697bfd681cf5ab3c784f13d72028b6e132d84e2453fab948b/pytorch-fast-transformers-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4de97c352eb727b87ea71b23f00a52de80d40dc0398937a82044692eb26a2fb7",
          "md5": "76601193f20011f24b5ed688471ff961",
          "sha256": "cf89b628bb4defc016a8fbc109b94d87a877f449a9d1ae2bb6b3a0fd1fb08c4c"
        },
        "downloads": -1,
        "filename": "pytorch-fast-transformers-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "76601193f20011f24b5ed688471ff961",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 56662,
        "upload_time": "2020-07-02T00:30:12",
        "upload_time_iso_8601": "2020-07-02T00:30:12.280949Z",
        "url": "https://files.pythonhosted.org/packages/4d/e9/7c352eb727b87ea71b23f00a52de80d40dc0398937a82044692eb26a2fb7/pytorch-fast-transformers-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "127d9899e7add42cd7852b765580bee1a9c31d40775833dccfc800d2cc33aa7e",
          "md5": "f515ca9f57fc7bb1f69a81b5af8a371e",
          "sha256": "202f9b10f42ab006848c7f58d2f35cbc6e33ec90fff3b82b3750f52fb6a05bb0"
        },
        "downloads": -1,
        "filename": "pytorch-fast-transformers-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f515ca9f57fc7bb1f69a81b5af8a371e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 64071,
        "upload_time": "2020-08-26T17:01:33",
        "upload_time_iso_8601": "2020-08-26T17:01:33.506349Z",
        "url": "https://files.pythonhosted.org/packages/12/7d/9899e7add42cd7852b765580bee1a9c31d40775833dccfc800d2cc33aa7e/pytorch-fast-transformers-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d312998209a95d523bdd42beff04e04b7bdad9b336e91a192fabe67c233a165f",
          "md5": "94b5c347f4cad4edcd5971272b155416",
          "sha256": "976fd2198a6bc1ddbc5f730212f9d41b6ebfa7f50b87ab10a492603369db2944"
        },
        "downloads": -1,
        "filename": "pytorch-fast-transformers-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "94b5c347f4cad4edcd5971272b155416",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 64082,
        "upload_time": "2020-08-26T22:04:53",
        "upload_time_iso_8601": "2020-08-26T22:04:53.413121Z",
        "url": "https://files.pythonhosted.org/packages/d3/12/998209a95d523bdd42beff04e04b7bdad9b336e91a192fabe67c233a165f/pytorch-fast-transformers-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "01b64f046de9852b88c98ededc7905d01cb25a945337068a03e3213009c843bb",
          "md5": "ff6ba8eabdb46aef4bb4987382f56b16",
          "sha256": "1a97f52ac293de7cd96ddcc0daa6bf5b0c98f1811fbb43429571bee7deef7f97"
        },
        "downloads": -1,
        "filename": "pytorch-fast-transformers-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "ff6ba8eabdb46aef4bb4987382f56b16",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 64131,
        "upload_time": "2020-08-27T14:45:20",
        "upload_time_iso_8601": "2020-08-27T14:45:20.706572Z",
        "url": "https://files.pythonhosted.org/packages/01/b6/4f046de9852b88c98ededc7905d01cb25a945337068a03e3213009c843bb/pytorch-fast-transformers-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "039b38905999695b381a1e239b91afce219892a23614248fc024e04558f36237",
          "md5": "a28f865e3618ca8d5cdc27803b9cde1f",
          "sha256": "6811900db71babd232b0ccf01cd199cc788d288789efb592280f1190da2dd41a"
        },
        "downloads": -1,
        "filename": "pytorch-fast-transformers-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a28f865e3618ca8d5cdc27803b9cde1f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 78727,
        "upload_time": "2020-10-14T20:14:35",
        "upload_time_iso_8601": "2020-10-14T20:14:35.253985Z",
        "url": "https://files.pythonhosted.org/packages/03/9b/38905999695b381a1e239b91afce219892a23614248fc024e04558f36237/pytorch-fast-transformers-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eabc00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962",
          "md5": "332493c8711a225f4eb5358f43d03ad4",
          "sha256": "d1826bc31b9dfbcd018998b897667e89fc6566bd3f8c424cda9f0943544f7e90"
        },
        "downloads": -1,
        "filename": "pytorch-fast-transformers-0.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "332493c8711a225f4eb5358f43d03ad4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 93616,
        "upload_time": "2021-04-15T13:17:57",
        "upload_time_iso_8601": "2021-04-15T13:17:57.755729Z",
        "url": "https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eabc00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962",
        "md5": "332493c8711a225f4eb5358f43d03ad4",
        "sha256": "d1826bc31b9dfbcd018998b897667e89fc6566bd3f8c424cda9f0943544f7e90"
      },
      "downloads": -1,
      "filename": "pytorch-fast-transformers-0.4.0.tar.gz",
      "has_sig": false,
      "md5_digest": "332493c8711a225f4eb5358f43d03ad4",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 93616,
      "upload_time": "2021-04-15T13:17:57",
      "upload_time_iso_8601": "2021-04-15T13:17:57.755729Z",
      "url": "https://files.pythonhosted.org/packages/ea/bc/00f597fefeab6341114c41045c1b232c38436738e0e8eac1bc9d5e9d5962/pytorch-fast-transformers-0.4.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}