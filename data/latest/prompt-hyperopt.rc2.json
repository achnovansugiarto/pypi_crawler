{
  "info": {
    "author": "",
    "author_email": "Mavenoid <info@mavenoid.com>, Cenny Wenner <cwenner@gmail.com>",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Text Processing :: Linguistic",
      "Topic :: Utilities"
    ],
    "description": "![logo](assets/logo.png)\n\n# prompt-hyperopt\n\nMore reliable prompt crafting though templates, hyperparameter optimization from few examples, and calibration of and across language models.\n\n![tests status](https://github.com/mavenoid/prompt-hyperopt/actions/workflows/tests.yml/badge.svg)\n\n## Why\n\nPrompt optimization offers a stuctured way to produce more reliable generation.\n\nManually crafting reliable prompts can be a difficult and time-consuming task. It can seem like one experiment succeeds only to find the next example going on a seemingly random tangent. Adapting the prompt to fix one such misbehaving example often breaks a previously-tried example, and revalidating all examples manually is a time consuming.\n\n## The solution\n\nInstead, humans should focus on generating ideas for the different ways a task prompt can be expressed. For example, maybe you could use emojis 🙂/😞 instead of Positive/Negative for sentiment analysis? Maybe you could put quotes around the paragraph to make it clear that it is separate from the task description? Maybe you want a preamble that explains what the task is about or what the generation should not do? When you see a failed generation, you can usually think of a few ways to adjust prompts to steer it in the right direction and so iteratively add more prompt options.\n\nThe task on the human is provide these variants, and the library is responsible for finding the alternative that performs the best with respect to the examples. The library provides intuitive curly-brace templates to express variants more compactly.\n\nBy using token probabilities and tuning parameters such as `temperature`, an informative evaluation can be made even with a few examples; and by using hyperparameter-optimization approaches, the number of evaluations to find the best prompts can be kept to a minimum. Evaluations can also be cached to prevent unnecessary reruns. Long hyperparameter runs can initially also be done on smaller (i.e. faster and cheaper) models and then adapted to and tuned further on larger models. Finally, the temperature can be chosen automatically to calibrate confidence levels.\n\nThe new workflow to engineer prompts becomes:\n1. Come up with an initial prompt and a few sensible variations.\n2. Come up with a few examples of expected results.\n3. Run the optimization.\n4. Inspect the examples that fail and add a few new examples or variations to hopefully fix them.\n5. Repeat 3-4 until happy with the performance.\n\n## Details\n\nThis library provides convenient methods to express prompt alternatives for a task and use hyperparameter-optimization techniques to find the best one. This approach has the benefit that it works without access to gradients (such as with GPT3), significant time and resource budgets for optimization, and it generalizes well also for small datasets (e.g. 3-30 examples). For projects where these limitations are not factors, one should expect to see better results fine proper fine tuning or [prompt tuning](https://arxiv.org/pdf/2104.08691.pdf).\n\nPrompt hyperparameter optimization should perform better than traditional few-shot learning prompts and worse than proper fine tuning. Even if a project can afford to fine tune however, it may be advantageous to prototype and iterate more quickly with this library.\n\n## Features\n\n* Seemlessly switch between API language-models such as GPT3 and locally-run models such as GPT2, GPT-J and Flan-T5.\n* Optimize `temperature` and `top_p` rather than guessing.\n* Calibrate token biases to get results similar to [neutral-prompt calibration](https://arxiv.org/pdf/2102.09690.pdf).\n* Optimize for prompts which generate parseable results rather than going off script.\n* Find prompts that most reliably produce expected outputs.\n* Initiate optimization with smaller language models and recalibrate prompts for larger models.\n* Minimize unproductive evaluations using Hyperband Bayesian Optimization via [hpbandster](https://automl.github.io).\n\n## Installation\n\nFor the full installation, simply `pip install prompt_hyperopt`. Note that this\npresently also includes heavy dependencies like `torch` and `transformers`.\n\nTo execute tests, use `pip install prompt_hyperopt[dev]` or `pip install -e .[dev]` if checked out.\n\n## Getting started\n\n### Expressing what you want\n\nAfter installing, the first step to use the library is to make a handful\nexamples of what you expect the prompt to generate. Each input can have\none or more input values and should have exactly one output.\n\nFor example, if you want to optimize story generation, you could write a \nfew examples with an input of a preceding paragraph, and the output\nshould be the next generated sentence.\n\n```python\nfrom prompt_hyperopt import PromptHyperopt\n\nexamples = [\n    {\n        \"story\": \"While out on a walk, I found a $20 bill. As I was picking it up, I saw a homeless man watching me.\",\n        \"continuation\": \"I handed him the money and he smiled.\",\n    },\n    {\n        \"story\": \"The dog was hungry.\",\n        \"continuation\": \"So she ate the food.\",\n    },\n]\n```\n\nThe way this optimization works, you must also provide a number of answer\noptions, including examples continuations that you would not like to see.\n\n```python\n\npossible_continuations = [\n    examples[0][\"continuation\"],\n    examples[1][\"continuation\"],\n    \"I set fire to the bill.\",\n    \"So she ordered a pizza.\",\n    \"Once upon a time...\",\n    \"\\n\\n\",\n]\n```\n\n### Defining the prompt options\n\nNext come up with a few different ways to express the prompt. This is\ndone using curly-brace templates option lists. You should not provide\noptions for the inputs and outputs of the examples.\n\n```python\ntrompt = TemplatedPrompt(\n    \"\"\"{preamble}\n\n{story}{separation}{continuation}\"\"\",\n    options=dict(\n        preamble=[\n            \"Write a continuation to the following story:\",\n            \"This is the most amazing story ever:\",\n            \"A paragraph from a best-selling novel:\",\n        ],\n        separation=[\n            \"\\n\\n\", \"\\n\", \" \",\n            \"\\n\\nStory continuation: \",\n        ],\n    ),\n)\n```\n\n### Finding the best prompt\n\nWith the template and examples, we can now optimize the prompt. The\n`TemplatedPrompt` class provides a convenient interface to do this.\nIn order to do the optimization, the library needs to know the\nrelation between the examples and the template, which is done through\nthe field `targets_field_mapping`. This describes what fields in the\ntemplate should be prediceted vs filled statically. For more complex\ndatasets, additional fields may need to be specified.\n\nNote also that if your template is complex, this will need to\ngenerate a lot of examples, and depending on the engine you use, this\nmay both take a while and be costly.\n\n```python\n\nengine = \"text-curie-001\"\n\ntrompt.optimize(\n    engine,\n    examples,\n    targets_field_mapping={\n        \"continuation\": \"continuation\",\n    },\n)\n```\n\nThen print the best prompt:\n\n```python\nprint(trompt(story=\"<story>, continuation=\"<continuation>\"))\n```\n\n\n## Interesting findings\n\nOut of the box, prompts which seem to provide mostly accurate predictions for one language model do not necessarily perform well for others, even bordering on random answers. Notably this can be observed for GPT3-curie 6.7B vs GPT3-davinci 175B. What we have found however is that by recalibrating token biases for the two models, then often high-performing prompts for one model are also high-performing for the other. This can be done by running the optimize method without options.\n\n## Examples\n\n### Sentiment analysis\n\n```\nfrom prompt_hyperopt import TemplatedPrompt\n\ntrompt = TemplatedPrompt(\n    prompt=\"\"\"{{preamble}}\n\nStatement: {{sentence}}\nSentiment: {{sentiment}}\n\"\"\",\n    options=dict(\n        answer_positive=[\"Positive\", \"happy\", \"Positive sentiment\", \"🙂\", \"😀\"],\n        answer_negative=[\"Negative\", \"sad\", \"Negative sentiment\", \"☹\", \"😡\", \"😞\"],\n        preamble=[\"Sentiment analysis.\", \"Predict whether the sentence is Positive or Negative\"]\n    ),\n)\n\nexamples = [\n    dict(sentence=\"I am happy.\", sentiment=\"Positive\"),\n    dict(sentence=\"I am sad.\", sentiment=\"Negative\"),\n    dict(sentence=\"I am\", sentiment=\"Neutral\"),\n]\n\ntrompt.optimize(\n    \"gpt2\",\n    examples,\n    features_field_mapping={\"sentence\": \"sentence\"},\n    targets_field_mapping={\"sentiment\": \"sentiment\"},\n    targets_value_mapping={\n        \"sentiment\": {\n            \"Positive\": \"{{answer_positive}}\",\n            \"Negative\": \"{{answer_negative}}\",\n        }\n    }\n)\n\nprint(\"Optimized prompt:\")\nprint(trompt(sentence=\"Coffee is good\"))\n\nprint(\"Prediction:\")\nprint(trompt.predict(sentence=\"Coffee is good\"))\n```\n\n## Results\n\nOn [Bool-Q](https://paperswithcode.com/sota/question-answering-on-boolq), prompt hyperoptimization produces an accuracy of 81.3 % with 32 examples for GPT-3 davinci, in contrast to previous results of GPT-3 few-shot on 32 examples. TODO confirm. TODO fill in more.\n\nBest found prompt for Bool-Q (with 1-shot examples):\n\n```\nAnswers are helpful and accurate.\n--\n\n--\nIn 1907 the Peking to Paris automobile race had inspired an even bolder test of these new machines. The following year the course would be from New York City, USA, to Paris, France with a 150-mile (240 km) ship passage from Nome, Alaska, across the Bering Strait to East Cape, Siberia, this at a time when ``the motor car is the most fragile and capricious thing on earth.''\n\nQ: 「can you drive to paris from new york」 Yes or No?\nAnswer. 「No」\n\nPersian (/ˈpɜːrʒən, -ʃən/), also known by its endonym Farsi (فارسی fārsi (fɒːɾˈsiː) ( listen)), is one of the Western Iranian languages within the Indo-Iranian branch of the Indo-European language family. It is primarily spoken in Iran, Afghanistan (officially known as Dari since 1958), and Tajikistan (officially known as Tajiki since the Soviet era), and some other regions which historically were Persianate societies and considered part of Greater Iran. It is written in the Persian alphabet, a modified variant of the Arabic script, which itself evolved from the Aramaic alphabet.\n\nQ: 「do iran and afghanistan speak the same language」 Yes or No?\nAnswer. 「Yes」\n```\n\n### License\n\nThis package has been developed by [Mavenoid](https://www.mavenoid.com) and is released under MIT License.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Mavenoid/prompt-hyperopt",
    "keywords": "algorithm,configuration,hyperparameter optimization,machine learning,natural language processing",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "prompt-hyperopt",
    "package_url": "https://pypi.org/project/prompt-hyperopt/",
    "platform": null,
    "project_url": "https://pypi.org/project/prompt-hyperopt/",
    "project_urls": {
      "Funding": "https://www.mavenoid.com",
      "Homepage": "https://github.com/Mavenoid/prompt-hyperopt",
      "Source": "https://github.com/Mavenoid/prompt-hyperopt",
      "Tracker": "https://github.com/Mavenoid/prompt-hyperopt/issues"
    },
    "release_url": "https://pypi.org/project/prompt-hyperopt/0.1.1/",
    "requires_dist": [
      "datasets (>=2.6)",
      "transformers (>=4.10)",
      "configspace (>=0.6)",
      "scipy (>=1.9)",
      "openai (>=0.24)",
      "jinja2 (>=1.2)",
      "torch (>=1.13)",
      "torchvision",
      "torchaudio",
      "black (>=22.10.0) ; extra == 'dev'",
      "build ; extra == 'dev'",
      "check-manifest ; extra == 'dev'",
      "pytest (>=3.0) ; extra == 'dev'",
      "setuptools ; extra == 'dev'",
      "tox (>=3.27) ; extra == 'dev'",
      "tox-gh-actions (>=2.10) ; extra == 'dev'",
      "twine ; extra == 'dev'",
      "flake8 (>=5.0) ; extra == 'dev'",
      "flake8-bugbear (>=22.10) ; extra == 'dev'",
      "wheel ; extra == 'dev'"
    ],
    "requires_python": "",
    "summary": "Reliable language-model prompts through templates, calibration, and hyperparameter optimization",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15654345,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "11f774b683cbc49722c198e1ee02a8adfa35a0891927453f01185005949514a9",
          "md5": "ea97b3e71e7a78fba50ff58d53391911",
          "sha256": "4bae7348e3ae6776a218ed0870e22a19b8d112c3cef04a30081743e99e431398"
        },
        "downloads": -1,
        "filename": "prompt_hyperopt-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ea97b3e71e7a78fba50ff58d53391911",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 20561,
        "upload_time": "2022-11-02T21:55:44",
        "upload_time_iso_8601": "2022-11-02T21:55:44.887202Z",
        "url": "https://files.pythonhosted.org/packages/11/f7/74b683cbc49722c198e1ee02a8adfa35a0891927453f01185005949514a9/prompt_hyperopt-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fb00194334aea948d923f70fc80344b2856726dbe91ba18482cc721bd5048922",
          "md5": "479f90afafd6df48bcf27b94b4d8cb9a",
          "sha256": "4d5953713d86281cebec64eb28db945ace98e0b006e4b29960309ba82866fc57"
        },
        "downloads": -1,
        "filename": "prompt-hyperopt-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "479f90afafd6df48bcf27b94b4d8cb9a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23523,
        "upload_time": "2022-11-02T21:55:46",
        "upload_time_iso_8601": "2022-11-02T21:55:46.537107Z",
        "url": "https://files.pythonhosted.org/packages/fb/00/194334aea948d923f70fc80344b2856726dbe91ba18482cc721bd5048922/prompt-hyperopt-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "658f0211576d12208a111789b10681d3e8907293331fafdfad6258daf4b70cb3",
          "md5": "7603b23f98ac7d0a1b18de2174bd1ad1",
          "sha256": "82e8730ce47319141cd636ec4494e8e0004fc4ac404195909714776673604c38"
        },
        "downloads": -1,
        "filename": "prompt_hyperopt-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7603b23f98ac7d0a1b18de2174bd1ad1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 20621,
        "upload_time": "2022-11-04T11:57:55",
        "upload_time_iso_8601": "2022-11-04T11:57:55.285927Z",
        "url": "https://files.pythonhosted.org/packages/65/8f/0211576d12208a111789b10681d3e8907293331fafdfad6258daf4b70cb3/prompt_hyperopt-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "829c63322ccfab3389f181a3d257c2e2e2c9f279a200e19c7d8a5e0c48c8a9be",
          "md5": "bcb99ab1d0fb0dd9e925438f95d8f265",
          "sha256": "ffd7c5434e893f60927d9f0ccb080babc6dbf17bc19f5721940aa889adafd5f9"
        },
        "downloads": -1,
        "filename": "prompt-hyperopt-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "bcb99ab1d0fb0dd9e925438f95d8f265",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 60715,
        "upload_time": "2022-11-04T11:57:58",
        "upload_time_iso_8601": "2022-11-04T11:57:58.320879Z",
        "url": "https://files.pythonhosted.org/packages/82/9c/63322ccfab3389f181a3d257c2e2e2c9f279a200e19c7d8a5e0c48c8a9be/prompt-hyperopt-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "658f0211576d12208a111789b10681d3e8907293331fafdfad6258daf4b70cb3",
        "md5": "7603b23f98ac7d0a1b18de2174bd1ad1",
        "sha256": "82e8730ce47319141cd636ec4494e8e0004fc4ac404195909714776673604c38"
      },
      "downloads": -1,
      "filename": "prompt_hyperopt-0.1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "7603b23f98ac7d0a1b18de2174bd1ad1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 20621,
      "upload_time": "2022-11-04T11:57:55",
      "upload_time_iso_8601": "2022-11-04T11:57:55.285927Z",
      "url": "https://files.pythonhosted.org/packages/65/8f/0211576d12208a111789b10681d3e8907293331fafdfad6258daf4b70cb3/prompt_hyperopt-0.1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "829c63322ccfab3389f181a3d257c2e2e2c9f279a200e19c7d8a5e0c48c8a9be",
        "md5": "bcb99ab1d0fb0dd9e925438f95d8f265",
        "sha256": "ffd7c5434e893f60927d9f0ccb080babc6dbf17bc19f5721940aa889adafd5f9"
      },
      "downloads": -1,
      "filename": "prompt-hyperopt-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "bcb99ab1d0fb0dd9e925438f95d8f265",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 60715,
      "upload_time": "2022-11-04T11:57:58",
      "upload_time_iso_8601": "2022-11-04T11:57:58.320879Z",
      "url": "https://files.pythonhosted.org/packages/82/9c/63322ccfab3389f181a3d257c2e2e2c9f279a200e19c7d8a5e0c48c8a9be/prompt-hyperopt-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}