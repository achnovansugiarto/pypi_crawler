{
  "info": {
    "author": "Nikolay",
    "author_email": "kutuzov.nv@phystech.edu",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# TorchClippedOptimizers\r\n\r\n`torch-clip` a library to improve optimization methods by clipping off heavy-tailed gradient. This makes it possible to increase the accuracy and speed of convergence during the training of neural networks on a specific number of tasks.\r\n\r\n### Installation\r\nyou can install our library using pip:  \r\n`pip install torch-clip`  \r\n```requirements.txt\r\nnumpy~=1.20.0\r\ntorch~=1.11.0+cu113\r\nmatplotlib~=3.4.3\r\ntqdm~=4.62.3\r\n```\r\n\r\n### What do you need us for?\r\nIn the last few years, for various neural network training models (for example, BERT + CoLA), it has been found that in the case of \"large stochastic gradients\", it is advantageous to use special clipping (clipping/normalization) of the batched gradient. Since all modern machine learning, one way or another, ultimately boils down to stochastic optimization problems, the question of exactly how to \"clip\" large values of batched gradients plays a key role in the development of effective numerical training methods for a large class of models. This repository implements optimizers for the pytorch library with different clipping methods.\r\n\r\n### Our repository\r\nThe source code and research results can be found at the link:\r\nhttps://github.com/EugGolovanov/TorchClippedOptimizers\r\n\r\n### Use example  \r\nYou can use our optimizers as well as all the standard optimizers from the pytorch library  \r\n```python\r\nfrom torch_clip.optimizers import  ClippedSGD\r\noptimizer = ClippedSGD(model.parameters(), lr=5e-2, momentum=0.9, clipping_type=\"layer_wise\", clipping_level=1)\r\n\r\nloss = my_loss_function\r\nfor epoch in range(EPOCHS):\r\n    for i, data in enumerate(train_loader, 0):\r\n        outputs = net(inputs)\r\n        loss = criterion(outputs, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n```\r\n\r\n<br>\r\n<br>\r\n\r\n### Use example (with restarts)\r\n\r\n```python\r\nfrom torch_clip.optimizers import ClippedSGD\r\nfrom torch_clip.restarter import Restarter\r\nfrom torch_clip.optimizers_collector import OptimizerProperties, ModelProperties, RestartProperties\r\n\r\nloss = my_loss_function\r\nmodel = my_model_object\r\n\r\noptimizer_props = OptimizerProperties(ClippedSGD, lr=5e-2, momentum=0.9, \r\n                                      clipping_type=\"layer_wise\", clipping_level=1)\r\nrestarter = Restarter(optimizer_properties=optimizer_props, first_restart_steps_cnt=50,\r\n                      restart_coeff=1.25, max_steps_cnt=2000)\r\noptimizer = optimizer_props.optimizer_class(model.parameters(), **optimizer_props.optimizer_kwargs)\r\n\r\nfor epoch in range(EPOCHS):\r\n    for i, data in enumerate(train_loader, 0):\r\n        outputs = model(inputs)\r\n        loss = criterion(outputs, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n        optimizer.zero_grad()\r\n        restarter.add_coords(model.parameters())\r\n        optimizers = restarter.make_restart(net, optimizer)\r\n```\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "python,torch,clipping",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torch-clip",
    "package_url": "https://pypi.org/project/torch-clip/",
    "platform": null,
    "project_url": "https://pypi.org/project/torch-clip/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/torch-clip/0.0.6/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "torch-clip a library to improve optimization methods by clipping off heavy-tailed gradient. This makes it possible to increase the accuracy and speed of convergence during the training of neural networks on a specific number of tasks.",
    "version": "0.0.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14516470,
  "releases": {
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "595fc10d9df2ce115a106b0a879e43d2ad40e67b34b1552b108031d873f2407d",
          "md5": "2b2e9958ea2e35bcedb595af28add25c",
          "sha256": "34ab255b33e4c5371bebe6b582b89be6633097e3ec47d1781c4ea121ca8d1ef7"
        },
        "downloads": -1,
        "filename": "torch_clip-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "2b2e9958ea2e35bcedb595af28add25c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 12138,
        "upload_time": "2022-07-22T13:34:08",
        "upload_time_iso_8601": "2022-07-22T13:34:08.670175Z",
        "url": "https://files.pythonhosted.org/packages/59/5f/c10d9df2ce115a106b0a879e43d2ad40e67b34b1552b108031d873f2407d/torch_clip-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "595fc10d9df2ce115a106b0a879e43d2ad40e67b34b1552b108031d873f2407d",
        "md5": "2b2e9958ea2e35bcedb595af28add25c",
        "sha256": "34ab255b33e4c5371bebe6b582b89be6633097e3ec47d1781c4ea121ca8d1ef7"
      },
      "downloads": -1,
      "filename": "torch_clip-0.0.6.tar.gz",
      "has_sig": false,
      "md5_digest": "2b2e9958ea2e35bcedb595af28add25c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 12138,
      "upload_time": "2022-07-22T13:34:08",
      "upload_time_iso_8601": "2022-07-22T13:34:08.670175Z",
      "url": "https://files.pythonhosted.org/packages/59/5f/c10d9df2ce115a106b0a879e43d2ad40e67b34b1552b108031d873f2407d/torch_clip-0.0.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}