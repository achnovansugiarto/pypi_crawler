{
  "info": {
    "author": "Andrei Mutu",
    "author_email": "mutuandrei02@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "DeepCrawl GraphQL Wrapper 1.0.0 documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to DeepCrawl GraphQL Wrapper’s documentation!¶\n======================================================\n\n\n\n```\npip install deepcrawl_graphql\n\n```\n\n\n\n\nAuthentication¶\n===============\n\n\nTo authenticate with the DeepCrawl API you have to first create a connection.\n\n\n\n\n\nConnection¶\n-----------\n\n\n\n\n*class* deepcrawl_graphql.api.DeepCrawlConnection(*user_key_id=None*, *secret=None*, *token=None*)¶\n\nDeepCrawlConnection class\n\nCreates a connection instance used for sending GraphQL queries to DeepCrawl.\n\n\n\n```\n>>> from deepcrawl_graphql.api import DeepCrawlConnection\n\n```\n\n\n\n```\n>>> DeepCrawlConnection(\"user_key_id\", \"secret\")\nDeepCrawlConnection instance\n\n```\n\n\n\n```\n>>> DeepCrawlConnection(token=\"token\")\nDeepCrawlConnection instance\n\n```\n\n\n\nParameters:\n* **user_key_id** (*int* *or* *str*) – user key, used together with secret for authentication.\n* **secret** (*str*) – secret key, used together with user_key_id for authentication.\n* **token** (*str*) – authentication token, if used ignores the user_key_id and secret\n\n\n\n\n\n\nrun_query(*query*, *variables=None*, *var=None*, ***kwargs*)¶\nRuns a query.\n\n\n\n```\n>>> conn = DeepCrawlConnection(\"user_key_id\", \"secret\")\n\n```\n\n\nThere are 3 possible ways you can run a query\n\n\nUsing a query string. You can use the DeepCrawl explorer to construct the string https://graph-docs.deepcrawl.com/graphql/explorer\n\n\n\n```\n>>> query = 'query MyQuery {version}'\n>>> conn.run_query(query)\n{'version': '1.91.1-next.0-en'}\n\n```\n\n\nUsing the gql package to construct a dynamic query.\n\n\n\n```\n>>> from gql.dsl import DSLQuery\n>>> query = DSLQuery(conn.ds.Query.me.select(conn.ds.User.id, conn.ds.User.username))\n>>> conn.run_query(query)\n{\n 'me': {\n 'id': 'id',\n 'username': 'email@example.com'\n }\n}\n\n```\n\n\n\n```\n>>> from gql.dsl import DSLVariableDefinitions\n>>> from gql.dsl import DSLQuery\n>>> var = DSLVariableDefinitions()\n>>> query = conn.ds.Query.me.select(\n conn.ds.User.accounts.args(first=var.first, after=var.after)\n .select(\n conn.ds.AccountConnection.nodes.select(\n conn.ds.Account.id,\n conn.ds.Account.name,\n )\n )\n .select(\n conn.ds.AccountConnection.pageInfo.select(\n conn.ds.PageInfo.startCursor,\n conn.ds.PageInfo.endCursor,\n conn.ds.PageInfo.hasNextPage,\n conn.ds.PageInfo.hasPreviousPage,\n )\n )\n )\n>>> conn.run_query(query, variables={\"first\": 1, \"after\": \"MQ\"}, var=var)\n{\n \"me\": {\n \"accounts\": {\n \"nodes\": [\n {\n \"id\": \"id\",\n \"name\": \"name\"\n }\n ],\n \"pageInfo\": {\n \"startCursor\": \"Mg\",\n \"endCursor\": \"Mg\",\n \"hasNextPage\": false,\n \"hasPreviousPage\": true\n }\n }\n }\n}\n# For more information about constructing queries with dsl\n# see https://gql.readthedocs.io/en/stable/advanced/dsl_module.html\n\n```\n\n\nImport a query from the deepcrawl_graphql package and use it’s prebuild queries.\n\n\n\n```\n>>> from deepcrawl_graphql.me.me import MeQuery\n>>> me_query = MeQuery(conn)\n>>> me_query.select_me()\n>>> conn.run_query(me_query)\n{\n 'me': {\n 'id': 'id',\n 'username': 'email@example.com',\n 'email': 'email@example.com',\n 'firstName': 'FirstName',\n 'lastName': 'LastName',\n 'createdAt': '2019-10-27T17:11:17.000Z',\n 'updatedAt': '2022-01-15T10:10:38.000Z',\n 'jobTitle': None,\n 'overallLimitLevelsMax': 1000,\n 'overallLimitPagesMax': 10000000,\n 'ssoClientId': None,\n 'termsAgreed': True,\n 'rawID': 'id',\n 'permissions': []\n }\n}\n\n```\n\n\n\nParameters:\n* **query** (*Query* *or* *DSLField* *or* *DSLQuery* *or* *str*) – query object\n* **variables** (*dict* *or* *None*) – variables to use in the query.\nrun_query(… variables={“first”: 1, “after”: “MQ”})\n* **var** (*DSLVariableDefinitions*) – gql DSLVariableDefinitions instance to be used in the query.\nWorks together with variables\n* **kwargs** – variables to use in the query.\nIf variables is not used and the user prefers to send variables as function arguments\nrun_query(… first=1, after=”MQ”)\n\n\n\n\n\n\n\nget_by_base64_id(*base64_id*, *object_type*, *fields*)¶\nGet object by Base64 id\n\n\n\n```\n>>> conn.get_by_base64_id(\n \"TjAxNFJlcG9ydERvd25sb2FkNTk2Njg1MzE\",\n \"ReportDownload\",\n fields=(conn.ds.ReportDownload.id, conn.ds.ReportDownload.fileURL),\n )\n\n```\n\n\n\nParameters:\n* **base64_id** (*str*) – Object’s Base64 id\n* **object_type** (*str*) – Object’s meta type name\n* **fields** (*List**(**DSLField**)*) – Fields to select\n\n\n\n\n\n\n\nrun_mutation(*mutation*, *variables=None*, *var=None*, ***kwargs*)¶\nRuns a mutation.\n\n\n\n```\n>>> conn = DeepCrawlConnection(\"user_key_id\", \"secret\")\n\n```\n\n\nThere are multiple ways you can run a mutation\n\n\nImport the Mutation class from deepcrawl_graphql package and use it’s prebuild methods.\n\n\n\n```\n>>> from deepcrawl_graphql.mutation import Mutation\n>>> mutation = Mutation(conn)\n>>> project_input = {\"accountId\": \"acount_id\", \"name\": \"Create-Project-0\", \"primaryDomain\": \"domain\"}\n>>> mutation.create_project(project_input)  # Automatically runs this method\n{\n \"createProject\": {\n \"project\": {\n \"id\": \"id\",\n \"name\": \"Create-Project-0\",\n \"primaryDomain\": \"http://python.deepcrawl.com/\",\n ...\n }\n }\n}\n\n```\n\n\n\nParameters:\n* **mutation** (*DSLField* *or* *DSLMutation* *or* *str*) – mutation object\n* **variables** (*dict* *or* *None*) – variables to use in the query.\nrun_query(… variables={“input”: {“name”: “name”, …}})\n* **var** (*DSLVariableDefinitions*) – gql DSLVariableDefinitions instance to be used in the query.\nWorks tougether with variables\n* **kwargs** – variables to use in the query.\nIf variables is not used and the user prefers to send variables as function arguments\nrun_query(… input={“name”: “name”, …})\n\n\n\n\n\n\n\n\n\nSpecial Arguments¶\n==================\n\n\n\n\nPagination¶\n-----------\n\n\nThere are some optional arguments which can be used while running the query: first, last, after, before\n\n\n* first - Number of records to fetch from start\n* last - Number of records to fetch from end\n* after - Fetch after cursor\n* before - Fetch before cursor\n\n\n\n```\n>>> conn = DeepCrawlConnection(\"user_key_id\", \"secret\")\n\n```\n\n\nBy default first 100 or less objects are retrieved\n\n\n\n```\n>>> account_query = AccountQuery(conn, \"74910\")\n>>> account_query.select_projects()\n>>> conn.run_query(account_query)\n\n```\n\n\n\n```\n{\n    \"getAccount\": {\n        \"projects\": {\n            \"nodes\": [\n                {\"id\": \"id-1\", \"name\": \"name-1\", ...},\n                {\"id\": \"id-2\", \"name\": \"name-2\", ...},\n                {\"id\": \"id-3\", \"name\": \"name-3\", ...},\n                {\"id\": \"id-4\", \"name\": \"name-4\", ...},\n            ],\n            \"pageInfo\": {\n                \"startCursor\": \"MQ\",\n                \"endCursor\": \"NA\",\n                \"hasNextPage\": False,\n                \"hasPreviousPage\": False\n            }\n        }\n    }\n}\n\n```\n\n\nBy using the **first** argument you can choose how many object to retrieve per page.\n\n\n\n```\n>>> conn.run_query(account_query, variables={\"first\": 2})\nOR\n>>> conn.run_query(account_query, first=2)\n\n```\n\n\n\n```\n{\n    \"me\": {\n        \"accounts\": {\n            \"nodes\": [\n                {\"id\": \"id-1\", \"name\": \"name-1\", ...},\n                {\"id\": \"id-2\", \"name\": \"name-2\", ...},\n            ],\n            \"pageInfo\": {\n                \"startCursor\": \"MQ\",\n                \"endCursor\": \"Mg\",\n                \"hasNextPage\": True,\n                \"hasPreviousPage\": False\n            }\n        }\n    }\n}\n\n```\n\n\nBy using the **first** argument combined with the **after** argument you can choose how many object to retrieve per page after the selected cursor.\n\n\n\n```\n>>> conn.run_query(account_query, variables={\"first\": 2, \"after\": \"Mg\"})\nOR\n>>> conn.run_query(account_query, first=2, after=\"Mg\")\n\n```\n\n\n\n```\n{\n    \"me\": {\n        \"accounts\": {\n            \"nodes\": [\n                {\"id\": \"id-3\", \"name\": \"name-3\", ...},\n                {\"id\": \"id-4\", \"name\": \"name-4\", ...},\n            ],\n            \"pageInfo\": {\n                \"startCursor\": \"Mw\",\n                \"endCursor\": \"NA\",\n                \"hasNextPage\": False,\n                \"hasPreviousPage\": True\n            }\n        }\n    }\n}\n\n```\n\n\nBy using the **first** argument combined with the **before** argument you can choose how many object to retrieve per page before the selected cursor.\n\n\n\n```\n>>> conn.run_query(account_query, variables={\"first\": 2, \"before\": \"Mg\"})\nOR\n>>> conn.run_query(account_query, first=2, before=\"Mg\")\n\n```\n\n\n\n```\n{\n    \"me\": {\n        \"accounts\": {\n            \"nodes\": [\n                {\"id\": \"id-1\", \"name\": \"name-1\", ...},\n                {\"id\": \"id-2\", \"name\": \"name-2\", ...},\n            ],\n            \"pageInfo\": {\n                \"startCursor\": \"MQ\",\n                \"endCursor\": \"Mg\",\n                \"hasNextPage\": True,\n                \"hasPreviousPage\": False\n            }\n        }\n    }\n}\n\n```\n\n\n\n\nOrdering¶\n---------\n\n\nThere is an optional argument which can be used while running the query: order_by\n\n\nThe order argument can be used to sort the dataset.\n\n\nIt accepts a dictionary with two keys: direction (ASC or DESC) and a field (to sort against)\n\n\n\n```\n>>> conn = DeepCrawlConnection(\"user_key_id\", \"secret\")\n\n```\n\n\n\n```\n>>> account_query = AccountQuery(conn, \"74910\")\n>>> account_query.select_projects()\n\n```\n\n\nDefault result:\n\n\n\n```\n>>> conn.run_query(account_query)\n\n```\n\n\n\n```\n{\n    \"getAccount\": {\n        \"projects\": {\n            \"nodes\": [\n                {\"id\": \"id-1\", \"name\": \"name-1\", ...},\n                {\"id\": \"id-2\", \"name\": \"name-2\", ...},\n                {\"id\": \"id-3\", \"name\": \"name-3\", ...},\n                {\"id\": \"id-4\", \"name\": \"name-4\", ...},\n            ],\n            \"pageInfo\": {\n                \"startCursor\": \"MQ\",\n                \"endCursor\": \"NA\",\n                \"hasNextPage\": False,\n                \"hasPreviousPage\": False\n            }\n        }\n    }\n}\n\n```\n\n\nUsing order_by result:\n\n\n\n```\n>>> conn.run_query(account_query, order_by={\"direction\": \"DESC\", \"field\": \"id\"})\n\n```\n\n\n\n```\n{\n    \"getAccount\": {\n        \"projects\": {\n            \"nodes\": [\n                {\"id\": \"id-4\", \"name\": \"name-4\", ...},\n                {\"id\": \"id-3\", \"name\": \"name-3\", ...},\n                {\"id\": \"id-2\", \"name\": \"name-2\", ...},\n                {\"id\": \"id-1\", \"name\": \"name-1\", ...},\n            ],\n            \"pageInfo\": {\n                \"startCursor\": \"MQ\",\n                \"endCursor\": \"NA\",\n                \"hasNextPage\": False,\n                \"hasPreviousPage\": False\n            }\n        }\n    }\n}\n\n```\n\n\n\n\nSelect Specific Fields¶\n-----------------------\n\n\nIn some cases, you may not need all the returned fields. Maybe you are interested only in a list of project ids and names.\n\n\nFor this kind of scenario, you can use the fields argument implemented for all the select methods.\n\n\n\n```\n>>> conn = DeepCrawlConnection(\"user_key_id\", \"secret\")\n>>> account_query = AccountQuery(conn, \"74910\")\n>>> account_query.select_projects(fields=(conn.ds.Project.id, conn.ds.Project.name))\n>>> conn.run_query(account_query)\n\n```\n\n\n\n```\n{\n    \"getAccount\": {\n        \"projects\": {\n            \"nodes\": [\n                {\"id\": \"id-1\", \"name\": \"name-1\"},\n                {\"id\": \"id-2\", \"name\": \"name-2\"},\n            ],\n            \"pageInfo\": {\n                \"startCursor\": \"MQ\",\n                \"endCursor\": \"NA\",\n                \"hasNextPage\": False,\n                \"hasPreviousPage\": False\n            }\n        }\n    }\n}\n\n```\n\n\nAlthough this is very useful you have to have a pretty good understanding of the schema for using this feature.\n\n\n\n\n\n\nMe¶\n===\n\n\n\n\nMeQuery¶\n--------\n\n\n\n\n*class* deepcrawl_graphql.me.me.MeQuery(*conn: DeepCrawlConnection*)¶\n\nMeQuery class\n\nCreates a me query instance. “Me” being the authenticated user.\nThe instance will be passed to the run_query method in order to execute the query.\n\n\n\n```\n>>> from deepcrawl_graphql.me.me import MeQuery\n\n```\n\n\n\n```\n>>> me_query = MeQuery(conn)\n>>> me_query.select_me()\n>>> me_query.select_accounts()\n>>> me = conn.run_query(me_query)\n\n```\n\n\n\nParameters:\n**conn** (*DeepCrawlConnection*) – Connection.\n\n\n\n\n\n\nselect_me(*fields=None*)¶\nSelects user fields.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_accounts(*fields=None*)¶\nSelects users accounts.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\n\n\n\nAccounts¶\n=========\n\n\n\n\nAccountQuery¶\n-------------\n\n\n\n\n*class* deepcrawl_graphql.accounts.account.AccountQuery(*conn: DeepCrawlConnection*, *account_id*)¶\n\nAccountQuery class\n\nCreates an accout query instance.\nThe instance will be passed to the run_query method in order to execute the query.\n\n\n\n```\n>>> from deepcrawl_graphql.accounts.account import AccountQuery\n\n```\n\n\n\n```\n>>> account_query = AccountQuery(conn, \"id\")\n>>> account_query.select_account()\n>>> account_query.select_settings()\n>>> account_query.select_callback_headers()\n>>> account_query.select_feature_flags()\n>>> account_query.select_locations()\n>>> account_query.select_package()\n>>> account_query.select_subscription()\n>>> account_query.select_projects()\n>>> account_query.select_project(\"project_id\")\n>>> account = conn.run_query(account_query)\n\n```\n\n\n\nParameters:\n* **conn** (*DeepCrawlConnection*) – Connection.\n* **account_id** (*int* *or* *str*) – account id.\n\n\n\n\n\n\nselect_account(*fields=None*)¶\nSelects account fields.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_settings(*fields=None*)¶\nSelects account accountSettings.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_callback_headers(*fields=None*)¶\nSelects account apiCallbackHeaders.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_feature_flags(*fields=None*)¶\nSelects account featureFlags.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_locations(*fields=None*)¶\nSelects account locations.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_package(*fields=None*)¶\nSelects account primaryAccountPackage.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_subscription(*include_addons=False*, *integration_type=None*, *fields=None*)¶\nSelects account subscription.\n\n\n\nParameters:\n* **include_addons** (*bool*) – If true includes the addons available.\n* **integration_type** (*str*) – Selects an addon by integration type\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_projects(*fields=None*)¶\nSelects account projects.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_project(*project_id*, *fields=None*)¶\nSelects account project by id.\n\n\n\nParameters:\n* **project_id** (*bool*) – Project id.\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\n\n\n\nProjects¶\n=========\n\n\n\n\nProjectQuery¶\n-------------\n\n\n\n\n*class* deepcrawl_graphql.projects.project.ProjectQuery(*conn: DeepCrawlConnection*, *project_id*)¶\n\nProjectQuery class\n\nCreates a project query instance.\nThe instance will be passed to the run_query method in order to execute the query.\n\n\n\n```\n>>> from deepcrawl_graphql.projects.project import ProjectQuery\n\n```\n\n\n\n```\n>>> project_query = ProjectQuery(conn, \"project_id\")\n>>> project_query.select_project()\n>>> project_query.select_sitemaps()\n>>> project_query.select_advanced_crawl_rate()\n>>> project_query.select_majestic_configuration()\n>>> project_query.select_location()\n>>> project_query.select_google_search_configuration()\n>>> project_query.select_custom_extraction_settings()\n>>> project_query.select_account()\n>>> project_query.select_crawls()\n>>> project = conn.run_query(project_query)\n\n```\n\n\n\nParameters:\n* **conn** (*DeepCrawlConnection*) – Connection.\n* **project_id** (*int* *or* *str*) – project id.\n\n\n\n\n\n\nselect_project(*fields=None*)¶\nSelects project fields.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_sitemaps(*fields=None*)¶\nSelects project sitemaps.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_advanced_crawl_rate(*fields=None*)¶\nSelects project maximumCrawlRateAdvanced.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_majestic_configuration(*fields=None*)¶\nSelects project majesticConfiguration.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_location(*fields=None*)¶\nSelects project location.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_last_finished_crawl(*fields=None*)¶\nSelects project lastFinishedCrawl.\n\n\nNot implemented yet.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_google_search_configuration(*fields=None*)¶\nSelects project googleSearchConsoleConfiguration.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_google_analytics_project_view(*fields=None*)¶\nSelects project googleAnalyticsProjectView.\n\n\nNot implemented yet.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_custom_extraction_settings(*fields=None*)¶\nSelects project customExtractions.\n\n\n\n\n\nselect_account(*fields=None*)¶\nSelects project account.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_crawls(*fields=None*)¶\nSelects project crawls.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\n\n\n\nCrawls¶\n=======\n\n\n\n\nCrawlQuery¶\n-----------\n\n\n\n\n*class* deepcrawl_graphql.crawls.crawl.CrawlQuery(*conn: DeepCrawlConnection*, *crawl_id*)¶\n\nCrawlQuery class\n\nCreates a crawl query instance.\nThe instance will be passed to the run_query method in order to execute the query.\n\n\n\n```\n>>> from deepcrawl_graphql.crawls.crawl import CrawlQuery\n\n```\n\n\n\n```\n>>> crawl_query = CrawlQuery(conn, \"crawl_id\")\n>>> crawl_query.select_crawl()\n>>> crawl_query.select_parquet_files(\"datasource_name\")\n>>> crawl_query.select_compared_to()\n>>> crawl = conn.run_query(crawl_query)\n\n```\n\n\n\nParameters:\n* **conn** (*DeepCrawlConnection*) – Connection.\n* **crawl_id** (*int* *or* *str*) – crawl id.\n\n\n\n\n\n\nselect_crawl(*fields=None*)¶\nSelects crawl fields.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_parquet_files(*datasource_name*, *fields=None*)¶\nSelects crawl parquetFiles.\n\n\n\nParameters:\n* **datasource_name** (*str*) – Datasource name.\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_crawl_type_counts(*crawl_types*, *segment_id=None*, *fields=None*)¶\nSelects crawl fields.\n\n\nNot implemented yet.\n\n\n\nParameters:\n* **crawl_types** (*str*) – Crawl type.\n* **segment_id** (*int* *or* *str*) – Segment id.\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_crawl_settings(*fields=None*)¶\nSelects crawl crawlSetting.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_compared_to(*fields=None*)¶\nSelects crawl comparedTo.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_reports(*fields=None*)¶\nSelects crawl reports.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_report_downloads(*fields=None*)¶\nSelects reports downloads.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\n\n\n\nReports¶\n========\n\n\n\n\nReportQuery¶\n------------\n\n\n\n\n*class* deepcrawl_graphql.reports.report.ReportQuery(*conn: DeepCrawlConnection*, *crawl_id*, *report_tamplate_code*, *report_type_code*, *segment_id=None*)¶\n\nReportQuery class\n\nCreates a report query instance.\nThe instance will be passed to the run_query method in order to execute the query.\n\n\n\n```\n>>> from deepcrawl_graphql.reports.report import ReportQuery\n\n```\n\n\n\n```\n>>> report_query = ReportQuery(conn, \"crawl_id\", \"report_tamplate_code\", \"report_type_code\")\n>>> report_query.select_report()\n>>> report_query.select_datasource()\n>>> report_query.select_type()\n>>> report_query.select_trend()\n>>> report_query.select_segment()\n>>> report_query.select_report_template()\n>>> conn.run_query(report_query)\n\n```\n\n\n\nParameters:\n* **conn** (*DeepCrawlConnection*) – Connection.\n* **crawl_id** (*int* *or* *str*) – crawl id.\n* **report_tamplate_code** (*str*) – report template code.\n* **report_type_code** (*str*) – report type code.\n* **segment_id** (*int* *or* *str*) – segment id.\n\n\n\n\n\n\nselect_report(*fields=None*)¶\nSelects report fields.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_raw_trends(*fields=None*)¶\nSelects report rawTrends.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_datasource(*fields=None*)¶\nSelects report datasources.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_type(*fields=None*)¶\nSelects report type.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_trend(*fields=None*)¶\nSelects report trend.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_segment(*fields=None*)¶\nSelects report segment.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_report_template(*fields=None*)¶\nSelects report reportTemplate.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nselect_report_downloads(*fields=None*)¶\nSelects report reportDownloads.\n\n\n\nParameters:\n**fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\n\n\n\nMutations¶\n==========\n\n\n\n\nMutations¶\n----------\n\n\n\n\n*class* deepcrawl_graphql.mutations.Mutation(*conn: DeepCrawlConnection*)¶\nMutation class\n\n\n\n\ncancel_crawling(*crawl_id*, *fields=None*)¶\nCancel a running Crawl\n\n\n\nParameters:\n* **crawl_id** (*int* *or* *str*) – Crawl id\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\ncreate_project(*project_input*, *fields=None*)¶\nCreates a Project\n\n\n\n\nGraphQL Input Example\n\n\n\n\nIt has to be converted to dict\n\n\n\n```\ninput CreateProjectInput {\n  accountId: ObjectID!\n  alertEmails: [String!]\n  alertSettingCode: AlertSettingCode! = Always\n  apiCallbackHeaders: [APICallbackHeaderInput!]! = []\n  apiCallbackUrl: String = null\n  autoFinalizeOnCrawlLimits: Boolean! = false\n  compareToCrawl: CompareToCrawlType! = LastCrawl\n  crawlDisallowedUrls1stLevel: Boolean! = false\n  crawlHyperlinksExternal: Boolean! = false\n  crawlHyperlinksInternal: Boolean! = true\n  crawlImagesExternal: Boolean! = false\n  crawlImagesInternal: Boolean! = true\n  crawlNofollowHyperlinks: Boolean! = true\n  crawlNonHtml: Boolean! = false\n  crawlNotIncluded1stLevel: Boolean! = false\n  crawlRedirectsExternal: Boolean! = true\n  crawlRedirectsInternal: Boolean! = true\n  crawlRelAmphtmlExternal: Boolean! = false\n  crawlRelAmphtmlInternal: Boolean! = true\n  crawlRelCanonicalsExternal: Boolean! = false\n  crawlRelCanonicalsInternal: Boolean! = true\n  crawlRelHreflangsExternal: Boolean! = false\n  crawlRelHreflangsInternal: Boolean! = true\n  crawlRelMobileExternal: Boolean! = false\n  crawlRelMobileInternal: Boolean! = true\n  crawlRelNextPrevExternal: Boolean! = false\n  crawlRelNextPrevInternal: Boolean! = true\n  crawlRobotsTxtNoindex: Boolean! = true\n  crawlScriptsExternal: Boolean! = false\n  crawlScriptsInternal: Boolean! = true\n  crawlStylesheetsExternal: Boolean! = false\n  crawlStylesheetsInternal: Boolean! = true\n  crawlTestSite: Boolean! = false\n  crawlTypes: [CrawlType!]! = [Web]\n  customDns: [CustomDnsSettingInput!]! = []\n  customExtractions: [CustomExtractionSettingInput!]! = []\n  customRequestHeaders: [CustomRequestHeaderInput!]! = []\n  discoverSitemapsInRobotsTxt: Boolean! = true\n  duplicatePrecision: Float! = 3\n  emptyPageThreshold: Int! = 512\n  excludeUrlPatterns: [String!]! = []\n  gaDateRange: Int! = 30\n  ignoreInvalidSSLCertificate: Boolean! = false\n  includeHttpAndHttps: Boolean! = false\n  includeSubdomains: Boolean! = false\n  includeUrlPatterns: [String!]! = []\n  industryCode: String\n  limitLevelsMax: Int\n  limitUrlsMax: Int\n  locationCode: LocationCode! = Default\n  logSummaryRequestsHigh: Int! = 100\n  logSummaryRequestsLow: Int! = 10\n  maxBodyContentLength: Int! = 51200\n  maxDescriptionLength: Int! = 230\n  maxFollowedExternalLinks: Int! = 10\n  maxHtmlSize: Int! = 204800\n  maxLinks: Int! = 250\n  maxLoadTime: Float! = 3\n  maxRedirections: Int! = 4\n  maxTitleWidth: Int! = 600\n  maxUrlLength: Int! = 1024\n  maximumCrawlRate: Float! = 3\n  maximumCrawlRateAdvanced: [AdvancedCrawlRateInput!]! = []\n  minDescriptionLength: Int! = 50\n  minTitleLength: Int! = 10\n  minVisits: Int! = 2\n  mobileHomepageUrl: String = null\n  mobileUrlPattern: String = null\n  mobileUserAgentCode: String! = \"googlebot-smartphone\"\n  name: String!\n  primaryDomain: String!\n  rendererBlockAds: Boolean! = true\n  rendererBlockAnalytics: Boolean! = true\n  rendererBlockCustom: [String!]! = []\n  rendererJsString: String = null\n  rendererJsUrls: [String!]! = []\n  robotsOverwrite: String = null\n  secondaryDomains: [String!]! = []\n  startUrls: [String!]! = []\n  testSiteDomain: String = null\n  testSitePassword: String = null\n  testSiteUsername: String = null\n  thinPageThreshold: Int! = 3072\n  urlRewriteQueryParameters: [String!]! = []\n  urlRewriteRules: [UrlRewriteRuleInput!]! = []\n  urlRewriteStripFragment: Boolean! = true\n  urlSampling: [UrlSamplingInput!]! = []\n  useMobileSettings: Boolean! = false\n  useRenderer: Boolean! = false\n  useRobotsOverwrite: Boolean! = false\n  useStealthMode: Boolean! = false\n  useUrlRewriteRules: Boolean! = false\n  userAgentCode: String! = \"googlebot-smartphone\"\n  userAgentString: String = null\n  userAgentStringMobile: String = null\n  userAgentToken: String = null\n  userAgentTokenMobile: String = null\n}\n\n```\n\n\n\n\nParameters:\n* **project_input** (*dict*) – Project input\n* **fields** (*List**(**DSLField**)*) – Select specific fields\n\n\n\n\n\n\n\ncreate_report_download(*report_download_input*, *fields=None*)¶\nCreates a report download.\n\n\n\n\nGraphQL Input Example\n\n\n\n\nIt has to be converted to dict\n\n\n\n```\ninput CreateReportDownloadInput {\n  crawlDuplicateUrlFilter: CrawlDuplicateUrlConnectionFilterInput\n  crawlHreflangsFilter: CrawlHreflangsConnectionFilterInput\n  crawlId: ObjectID\n  crawlLinkFilter: CrawlLinkConnectionFilterInput\n  crawlLinkedDomainFilter: CrawlLinkedDomainConnectionFilterInput\n  crawlSitemapFilter: CrawlSitemapConnectionFilterInput\n  crawlUncrawledUrlFilter: CrawlUncrawledUrlConnectionFilterInput\n  crawlUniqueLinkFilter: CrawlUniqueLinkConnectionFilterInput\n  crawlUrlFilter: CrawlUrlConnectionFilterInput\n  crawlWebCrawlDepthFilter: CrawlWebCrawlDepthConnectionFilterInput\n  fileName: String\n  filter: JSONObject\n  outputType: ReportDownloadOutputType! = CsvZip\n  reportId: ObjectID\n  reportTemplateCode: String\n  reportTypeCode: ReportTypeCode\n  segmentId: ObjectID\n  selectedMetrics: [String!]\n}\n\n```\n\n\n\n\nParameters:\n* **report_download_input** (*dict*) – Report Download input.\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\ndelete_crawl(*crawl_id*, *fields=None*)¶\nDeletes a Crawl\n\n\n\nParameters:\n* **crawl_id** (*int* *or* *str*) – Crawl id\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\ndelete_project(*project_id*, *fields=None*)¶\nDeletes a Project\n\n\n\nParameters:\n* **project_id** (*int* *or* *str*) – Project id\n* **fields** (*List**(**DSLField**)*) – Select specific fields\n\n\n\n\n\n\n\ndelete_report_download(*report_download_id*, *fields=None*)¶\nDeletes a report download.\n\n\n\nParameters:\n* **report_download_id** (*int* *or* *str*) – Report Download id.\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\npause_crawling(*crawl_id*, *fields=None*)¶\nPause a running Crawl\n\n\n\nParameters:\n* **crawl_id** (*int* *or* *str*) – Crawl id\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nresume_crawling(*crawl_id*, *fields=None*)¶\nResume a Crawl\n\n\n\nParameters:\n* **crawl_id** (*int* *or* *str*) – Crawl id\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nrun_crawl_for_project(*project_id*, *fields=None*)¶\nRun a Crawl for a Project\n\n\n\nParameters:\n* **project_id** (*int* *or* *str*) – Project id\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nunarchive_crawl(*crawl_id*, *fields=None*)¶\nUnarchive a Crawl\n\n\n\nParameters:\n* **crawl_id** (*int* *or* *str*) – Crawl id\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nupdate_account(*account_input*, *fields=None*)¶\nUpdate Account.\n\n\n\n\nGraphQL Input Example\n\n\n\n\nIt has to be converted to dict\n\n\n\n```\ninput UpdateAccountInput {\n    accountId: ObjectID!\n    apiCallbackHeaders: [APICallbackHeaderInput!]\n    apiCallbackUrl: String\n    country: String\n    customLogo: Upload\n    customProxy: String\n}\n\n```\n\n\n\n\nParameters:\n* **account_input** (*dict*) – Account input\n* **fields** (*List**(**DSLField**)*) – Select specific fields.\n\n\n\n\n\n\n\nupdate_project(*project_input*, *fields=None*)¶\nUpdates a Project\n\n\n\n\nGraphQL Input Example\n\n\n\n\nIt has to be converted to dict\n\n\n\n```\ninput UpdateProjectInput {\n  alertEmails: [String!]\n  alertSettingCode: AlertSettingCode\n  apiCallbackHeaders: [APICallbackHeaderInput!]\n  apiCallbackUrl: String\n  autoFinalizeOnCrawlLimits: Boolean\n  compareToCrawl: CompareToCrawl\n  crawlDisallowedUrls1stLevel: Boolean\n  crawlHyperlinksExternal: Boolean\n  crawlHyperlinksInternal: Boolean\n  crawlImagesExternal: Boolean\n  crawlImagesInternal: Boolean\n  crawlNofollowHyperlinks: Boolean\n  crawlNonHtml: Boolean\n  crawlNotIncluded1stLevel: Boolean\n  crawlRedirectsExternal: Boolean\n  crawlRedirectsInternal: Boolean\n  crawlRelAmphtmlExternal: Boolean\n  crawlRelAmphtmlInternal: Boolean\n  crawlRelCanonicalsExternal: Boolean\n  crawlRelCanonicalsInternal: Boolean\n  crawlRelHreflangsExternal: Boolean\n  crawlRelHreflangsInternal: Boolean\n  crawlRelMobileExternal: Boolean\n  crawlRelMobileInternal: Boolean\n  crawlRelNextPrevExternal: Boolean\n  crawlRelNextPrevInternal: Boolean\n  crawlRobotsTxtNoindex: Boolean\n  crawlScriptsExternal: Boolean\n  crawlScriptsInternal: Boolean\n  crawlStylesheetsExternal: Boolean\n  crawlStylesheetsInternal: Boolean\n  crawlTestSite: Boolean\n  crawlTypes: [CrawlType!]\n  customDns: [CustomDnsSettingInput!]\n  customExtractions: [CustomExtractionSettingInput!]\n  customRequestHeaders: [CustomRequestHeaderInput!]\n  discoverSitemapsInRobotsTxt: Boolean\n  duplicatePrecision: Float\n  emptyPageThreshold: Int\n  excludeUrlPatterns: [String!]\n  gaDateRange: Int\n  ignoreInvalidSSLCertificate: Boolean\n  includeHttpAndHttps: Boolean\n  includeSubdomains: Boolean\n  includeUrlPatterns: [String!]\n  industryCode: String\n  limitLevelsMax: Int\n  limitUrlsMax: Int\n  locationCode: LocationCode\n  logSummaryRequestsHigh: Int\n  logSummaryRequestsLow: Int\n  maxBodyContentLength: Int\n  maxDescriptionLength: Int\n  maxFollowedExternalLinks: Int\n  maxHtmlSize: Int\n  maxLinks: Int\n  maxLoadTime: Float\n  maxRedirections: Int\n  maxTitleWidth: Int\n  maxUrlLength: Int\n  maximumCrawlRate: Float\n  maximumCrawlRateAdvanced: [AdvancedCrawlRateInput!]\n  minDescriptionLength: Int\n  minTitleLength: Int\n  minVisits: Int\n  mobileHomepageUrl: String\n  mobileUrlPattern: String\n  mobileUserAgentCode: String\n  name: String\n  primaryDomain: String\n  projectId: ObjectID!\n  rendererBlockAds: Boolean\n  rendererBlockAnalytics: Boolean\n  rendererBlockCustom: [String!]\n  rendererJsString: String\n  rendererJsUrls: [String!]\n  robotsOverwrite: String\n  secondaryDomains: [String!]\n  startUrls: [String!]\n  testSiteDomain: String\n  testSitePassword: String\n  testSiteUsername: String\n  thinPageThreshold: Int\n  urlRewriteQueryParameters: [String!]\n  urlRewriteRules: [UrlRewriteRuleInput!]\n  urlRewriteStripFragment: Boolean\n  urlSampling: [UrlSamplingInput!]\n  useMobileSettings: Boolean\n  useRenderer: Boolean\n  useRobotsOverwrite: Boolean\n  useStealthMode: Boolean\n  useUrlRewriteRules: Boolean\n  userAgentCode: String\n  userAgentString: String\n  userAgentStringMobile: String\n  userAgentToken: String\n  userAgentTokenMobile: String\n}\n\n```\n\n\n\n\nParameters:\n* **project_input** (*dict*) – Project input\n* **fields** (*List**(**DSLField**)*) – Select specific fields\n\n\n\n\n\n\n\n\n\n\nIndices and tables¶\n===================\n\n\n* Index\n* Module Index\n* Search Page\n\n\n\n\n\n\n\n\nDeepCrawl GraphQL Wrapper\n=========================\n\n\n### Navigation\n\n\n* Pagination\n* Ordering\n* Select Specific Fields\n\n\n* MeQuery\n\n\n* AccountQuery\n\n\n* ProjectQuery\n\n\n* CrawlQuery\n\n\n* ReportQuery\n\n\n* Mutations\n\n\n\n### Related Topics\n\n\n* Documentation overview\n\n\n\n\n\n\n\n\n ©2022, Andrei Mutu.\n \n |\n Powered by Sphinx 5.0.2\n & Alabaster 0.7.12",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/DeepCrawlSEO/dc_graphql_wrapper",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "deepcrawl-graphql",
    "package_url": "https://pypi.org/project/deepcrawl-graphql/",
    "platform": null,
    "project_url": "https://pypi.org/project/deepcrawl-graphql/",
    "project_urls": {
      "Homepage": "https://github.com/DeepCrawlSEO/dc_graphql_wrapper"
    },
    "release_url": "https://pypi.org/project/deepcrawl-graphql/0.2.3/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "A package to simplify usage of the DeepCrawl GraphQL",
    "version": "0.2.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15585516,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6c636877fc5d9dd0dcb31fa21a1e8f95c670460de15ffd331450d2444fbe6fd8",
          "md5": "0d97f5729fb336d272a0549ed8b9f892",
          "sha256": "55ceab4d529d729a488bcf247922b4edef9afb4d5a0cec251434446fe1b3eb7e"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0d97f5729fb336d272a0549ed8b9f892",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 53250,
        "upload_time": "2022-06-27T19:37:49",
        "upload_time_iso_8601": "2022-06-27T19:37:49.040642Z",
        "url": "https://files.pythonhosted.org/packages/6c/63/6877fc5d9dd0dcb31fa21a1e8f95c670460de15ffd331450d2444fbe6fd8/deepcrawl_graphql-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d5ce32772f2f5aa5c6d3f99d5144bdb6406c49944924ce32c92c132d488c4cfa",
          "md5": "0e6dc8a6e62e15e2fe12a843cded02c5",
          "sha256": "30fa49873e55afcfd998df22eed86f39b4c40922ea45cd84717fea554e456ffa"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0e6dc8a6e62e15e2fe12a843cded02c5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 53257,
        "upload_time": "2022-06-28T19:36:10",
        "upload_time_iso_8601": "2022-06-28T19:36:10.635732Z",
        "url": "https://files.pythonhosted.org/packages/d5/ce/32772f2f5aa5c6d3f99d5144bdb6406c49944924ce32c92c132d488c4cfa/deepcrawl_graphql-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eaed2019f0167b3fc4636c402324c10d9ce92e32cd3dcc2a2b6d0e17e65bc20d",
          "md5": "e7a452baf489c047df1539f4df7a6b54",
          "sha256": "1a82082ff5a66dedf59bcc572483fd6c5ea450f11cb64e7fb7e38be76a2179da"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "e7a452baf489c047df1539f4df7a6b54",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 53293,
        "upload_time": "2022-06-28T19:44:32",
        "upload_time_iso_8601": "2022-06-28T19:44:32.878470Z",
        "url": "https://files.pythonhosted.org/packages/ea/ed/2019f0167b3fc4636c402324c10d9ce92e32cd3dcc2a2b6d0e17e65bc20d/deepcrawl_graphql-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "894fbabf357373c15f2d1a33b4bd124494f361da0ef156f71fd21aff6986f12a",
          "md5": "91af9d74aa35c0075a46683f3299a56c",
          "sha256": "f844cb2c097fcdb5692c1ceee0aedb9bd1d41c53305203eb9f03616b5eb60201"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "91af9d74aa35c0075a46683f3299a56c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 55346,
        "upload_time": "2022-07-09T12:05:42",
        "upload_time_iso_8601": "2022-07-09T12:05:42.382985Z",
        "url": "https://files.pythonhosted.org/packages/89/4f/babf357373c15f2d1a33b4bd124494f361da0ef156f71fd21aff6986f12a/deepcrawl_graphql-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9f38be534d8dc90f6dd02def86cdc5a4a7040dfdede950379f9c057b3ff0be5c",
          "md5": "b072844063e8a41c48c24eac05125e45",
          "sha256": "0f80a76c9d2d7194922acb203672ddfdef611685a15288db3509b0000e45ca1c"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "b072844063e8a41c48c24eac05125e45",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 56854,
        "upload_time": "2022-07-10T12:37:04",
        "upload_time_iso_8601": "2022-07-10T12:37:04.163317Z",
        "url": "https://files.pythonhosted.org/packages/9f/38/be534d8dc90f6dd02def86cdc5a4a7040dfdede950379f9c057b3ff0be5c/deepcrawl_graphql-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e8fd62d20e027d85012aab4d1be87b6d36b8106b6eb069eae494f06f1d0537df",
          "md5": "8adfb120c1a25977ff5e712d95130db3",
          "sha256": "1ff44214852414089b225bbf6fb5b52a1af1fb8e9e551f8a5f47bede9f55a433"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "8adfb120c1a25977ff5e712d95130db3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 62474,
        "upload_time": "2022-07-18T09:30:31",
        "upload_time_iso_8601": "2022-07-18T09:30:31.149953Z",
        "url": "https://files.pythonhosted.org/packages/e8/fd/62d20e027d85012aab4d1be87b6d36b8106b6eb069eae494f06f1d0537df/deepcrawl_graphql-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4f08e037f6e3e390d708b722de6cb8d57e84acfe1acaf6ff4b0359596d889b3b",
          "md5": "212d30871e7c185c875e80d7d9590b97",
          "sha256": "efe4fc573029f746d3fc2edc441a7fd57716aa475f7ba2ff12a9801dbba60de8"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "212d30871e7c185c875e80d7d9590b97",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 73672,
        "upload_time": "2022-08-11T21:17:16",
        "upload_time_iso_8601": "2022-08-11T21:17:16.912672Z",
        "url": "https://files.pythonhosted.org/packages/4f/08/e037f6e3e390d708b722de6cb8d57e84acfe1acaf6ff4b0359596d889b3b/deepcrawl_graphql-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "296b0c95dce9ac989b9df1bd5c60ad15a94ecc8c3f26c3592718acb419d7415b",
          "md5": "548e3927db6af6119abef94b8fd8fa55",
          "sha256": "3c6d924e6f49e478a70e2c03f11563d43c8a539f022e14981d76258ae1c50e4d"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "548e3927db6af6119abef94b8fd8fa55",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 73693,
        "upload_time": "2022-09-17T14:10:43",
        "upload_time_iso_8601": "2022-09-17T14:10:43.631712Z",
        "url": "https://files.pythonhosted.org/packages/29/6b/0c95dce9ac989b9df1bd5c60ad15a94ecc8c3f26c3592718acb419d7415b/deepcrawl_graphql-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c89427cc7f86c3c0dae132ce035f0ed10932e42652f66e8dae44a7fa9f37de0a",
          "md5": "d24b203e4c06ee4183cea1687e9675aa",
          "sha256": "855f5d250af3abd82abf1198d142d8fb8ab48c559f45f0e2443b6a3e223f91de"
        },
        "downloads": -1,
        "filename": "deepcrawl_graphql-0.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "d24b203e4c06ee4183cea1687e9675aa",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 75030,
        "upload_time": "2022-10-30T10:09:40",
        "upload_time_iso_8601": "2022-10-30T10:09:40.206720Z",
        "url": "https://files.pythonhosted.org/packages/c8/94/27cc7f86c3c0dae132ce035f0ed10932e42652f66e8dae44a7fa9f37de0a/deepcrawl_graphql-0.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c89427cc7f86c3c0dae132ce035f0ed10932e42652f66e8dae44a7fa9f37de0a",
        "md5": "d24b203e4c06ee4183cea1687e9675aa",
        "sha256": "855f5d250af3abd82abf1198d142d8fb8ab48c559f45f0e2443b6a3e223f91de"
      },
      "downloads": -1,
      "filename": "deepcrawl_graphql-0.2.3.tar.gz",
      "has_sig": false,
      "md5_digest": "d24b203e4c06ee4183cea1687e9675aa",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 75030,
      "upload_time": "2022-10-30T10:09:40",
      "upload_time_iso_8601": "2022-10-30T10:09:40.206720Z",
      "url": "https://files.pythonhosted.org/packages/c8/94/27cc7f86c3c0dae132ce035f0ed10932e42652f66e8dae44a7fa9f37de0a/deepcrawl_graphql-0.2.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}