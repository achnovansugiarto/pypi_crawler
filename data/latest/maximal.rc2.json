{
  "info": {
    "author": "Ivan Bongiorni",
    "author_email": "ivanbongiorni@protonmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering"
    ],
    "description": "# maximal\n\nSee the [Official Documentation site](https://ivanbongiorni.github.io/maximal/)\n\nCurrent version: **1.0**\n\nA TensorFlow-compatible Python library that provides models and layers to implement custom Transformer neural networks.\n\nBuilt on TensorFlow 2.\n\n# Installation\nIts installation is straightforward:\n\n```\npip install maximal\n```\n\n# How to use it?\n`maximal` is commonly called as:\n\n```\nimport maximal\nfrom maximal.layers import TransformerLayer, GPTLayer\n```\n\nand can be used in a `tf.keras` model as any common layer.\n\n\n# Documentation\nAn [Official Website](https://ivanbongiorni.github.io/maximal/) is now available with documentation and tutorials.\n\n\n# Elements\n\nIn `layers.py`:\n- `SelfAttention`: `keras.Layer`, computes *Scaled Dot-Product Attention*.\n\n- `MultiHeadSelfAttention`: `keras.Layer`, it is a concatenation of `SelfAttention` layers, resized back to original input shape through linear transformation.\n\n- `PositionalEmbedding`: `keras.Layer`, implements double Embedding layers used in Transformers literature, for tokens and positions. Positional encoding is learned through a `tf.keras.layers.Embedding()` layer, instead of deterministic positional encoding in the original paper.\n\n- `TransformerLayer`: `keras.Layer` single Transformer Encoder piece. It can be used inside any `Sequential()` model in Keras.\n\n- `GPTLayer`: `keras.Layer` GPT block. Similar to `TransformerLayer` but with causal Attention mechanism. It can be used inside any `Sequential()` model in Keras.\n\n\nIn `schedules.py`:\n- `OriginalTransformerSchedule`: `keras.Layer` implements the learning rate schedule of the original Transformer paper. It is taken from this [official TensorFlow tutorial](https://www.tensorflow.org/text/tutorials/transformer).\n\n# Requirements\n```\nnumpy\ntensorflow >= 2.0\n```\n\n# Author\nIvan Bongiorni. [LinkedIn](https://www.linkedin.com/in/ivan-bongiorni-b8a583164/)\n\n# License\n2020 Ivan Bongiorni\n\nThis repository is licensed under the MIT license. See [LICENCE.txt]() for further details.\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/IvanBongiorni/maximal",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "maximal",
    "package_url": "https://pypi.org/project/maximal/",
    "platform": null,
    "project_url": "https://pypi.org/project/maximal/",
    "project_urls": {
      "Homepage": "https://github.com/IvanBongiorni/maximal"
    },
    "release_url": "https://pypi.org/project/maximal/1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "TensorFlow-compatible Transformer layers and models.",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16696881,
  "releases": {
    "0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "318b3a55ba1ab8000b7d446999a4267f6e66af85d1efc7edb4560d4eab281120",
          "md5": "1414b333e37b7817b1dba25fcaea853f",
          "sha256": "2a52ce95ad98eb0197d7988b3c503ba0cf35dd9569e09773a04b043c112636a3"
        },
        "downloads": -1,
        "filename": "maximal-0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "1414b333e37b7817b1dba25fcaea853f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 4572,
        "upload_time": "2022-09-22T20:31:53",
        "upload_time_iso_8601": "2022-09-22T20:31:53.677482Z",
        "url": "https://files.pythonhosted.org/packages/31/8b/3a55ba1ab8000b7d446999a4267f6e66af85d1efc7edb4560d4eab281120/maximal-0.3.tar.gz",
        "yanked": true,
        "yanked_reason": "I uploaded a newer release"
      }
    ],
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "948d21eefb0f74d9221816c0710a52d6875166289de67e28efcc08c3e03bb146",
          "md5": "76e3ee0028eabc4e81d0d5ca854d09f7",
          "sha256": "fc8b77a133a5700229989c18c2073acfed3b9da9c9ce05e591cdd94aa1da0e83"
        },
        "downloads": -1,
        "filename": "maximal-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "76e3ee0028eabc4e81d0d5ca854d09f7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6297,
        "upload_time": "2023-02-04T23:48:28",
        "upload_time_iso_8601": "2023-02-04T23:48:28.719712Z",
        "url": "https://files.pythonhosted.org/packages/94/8d/21eefb0f74d9221816c0710a52d6875166289de67e28efcc08c3e03bb146/maximal-1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "948d21eefb0f74d9221816c0710a52d6875166289de67e28efcc08c3e03bb146",
        "md5": "76e3ee0028eabc4e81d0d5ca854d09f7",
        "sha256": "fc8b77a133a5700229989c18c2073acfed3b9da9c9ce05e591cdd94aa1da0e83"
      },
      "downloads": -1,
      "filename": "maximal-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "76e3ee0028eabc4e81d0d5ca854d09f7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 6297,
      "upload_time": "2023-02-04T23:48:28",
      "upload_time_iso_8601": "2023-02-04T23:48:28.719712Z",
      "url": "https://files.pythonhosted.org/packages/94/8d/21eefb0f74d9221816c0710a52d6875166289de67e28efcc08c3e03bb146/maximal-1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}