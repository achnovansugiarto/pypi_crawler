{
  "info": {
    "author": "Allen WU",
    "author_email": "allen.wu18621039969@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# toolkit-bert-ner\nBase Google pre-training model(BERT), then add BiLSTM layer and crf layer, train a Chinese named entity recognition model.\n\n## Download project and install  \nYou can install this project by:  \n```\npip install -i https://test.pypi.org/simple/ toolkit-bert-ner==1.0.0\n```\n\nOR\n```\ngit clone http://git.huimeimt.net:8008/ds/toolkit-bert-ner.git\ncd toolkit-bert-ner/\npython3 setup.py install\n```\n\nif you do not want to install, you just need clone this project and reference the file of <run.py> to train the model or start the service.   \n\n## Train model:\nYou can use -help to view the relevant parameters of the training named entity recognition model, where data_dir, bert_config_file, output_dir, init_checkpoint, vocab_file must be specified.\n```angular2html\ntoolkit-bert-ner-train -help\n```\n\ntrain/dev/test dataset is like this:\n```\n海 O\n钓 O\n比 O\n赛 O\n地 O\n点 O\n在 O\n厦 B-LOC\n门 I-LOC\n与 O\n金 B-LOC\n门 I-LOC\n之 O\n间 O\n的 O\n海 O\n域 O\n。 O\n```\nThe first one of each line is a token, the second is token's label, and the line is divided by a blank line. The maximum length of each sentence is [max_seq_length] params.  \nYou can get training data from above two git repos  \nYou can training ner model by running below command:  \n```\ntoolkit_bert_ner_training \\\n    -data_dir {your dataset dir}\\\n    -output_dir {training output dir}\\\n    -init_checkpoint {Google BERT model dir}\\\n    -bert_config_file {bert_config.json under the Google BERT model dir} \\\n    -vocab_file {vocab.txt under the Google BERT model dir}\n```\nlike my init_checkpoint: \n```\ninit_checkpoint = {$HOME}/pre-trained-models/chinese_L-12_H-768_A-12/bert_model.ckpt\n```\nyou can special labels using -label_list params, the project get labels from training data.  \n```\n# using , split\n-labels 'B-LOC, I-LOC ...'\nOR save label in a file like labels.txt, one line one label\n-labels labels.txt\n```\n\nAfter training model, the NER model will be saved in {output_dir} which you special above cmd line.  \n##### My Training environment：Tesla P40 24G mem  \n\n## As Service\n```\ntoolkit-bert-ner-serving-start -help\n```\n\nand than you can using below cmd start ner service:\n```angular2html\ntoolkit_bert_ner_serving \\\n    -model_dir C:\\workspace\\python\\BERT_Base\\output\\ner2 \\\n    -bert_model_dir F:\\chinese_L-12_H-768_A-12\n    -model_pb_dir C:\\workspace\\python\\BERT_Base\\model_pb_dir\n    -mode NER\n```\n\nyou can using below code test client:  \n#### 1. NER Client\n```angular2html\nimport time\nfrom bert_base.client import BertClient\n\nwith BertClient(show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n    start_t = time.perf_counter()\n    str = '1月24日，新华社对外发布了中央对雄安新区的指导意见，洋洋洒洒1.2万多字，17次提到北京，4次提到天津，信息量很大，其实也回答了人们关心的很多问题。'\n    rst = bc.encode([str, str])\n    print('rst:', rst)\n    print(time.perf_counter() - start_t)\n```\n```angular2html\nrst = bc.encode([list(str), list(str)], is_tokenized=True)\n```  \n\n## License\nMIT.  \n\n## How to train\n#### 1. Download BERT chinese model:  \n ```\n wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip  \n ```\n#### 2. Put BERT chinese model to $HOME/pre-trained-models/:  \n ```\nmkdir $HOME/pre-trained-models/\nunzip chinese_L-12_H-768_A-12.zip $HOME/pre-trained-models/\n ```\n\n#### 3. Train model\n\n##### first method \n```\n  python3 bert_lstm_ner.py   \\\n                  --task_name=\"NER\"  \\ \n                  --do_train=True   \\\n                  --do_eval=True   \\\n                  --do_predict=True\n                  --data_dir=NERdata   \\\n                  --vocab_file=checkpoint/vocab.txt  \\ \n                  --bert_config_file=checkpoint/bert_config.json \\  \n                  --init_checkpoint=checkpoint/bert_model.ckpt   \\\n                  --max_seq_length=128   \\\n                  --train_batch_size=32   \\\n                  --learning_rate=2e-5   \\\n                  --num_train_epochs=3.0   \\\n                  --output_dir=./output \\\n ```       \n ##### OR replace the BERT path and project path in bert_lstm_ner.py\n ```\n if os.name == 'nt': #windows path config\n    bert_path = '{your BERT model path}'\n    root_path = '{project path}'\nelse: # linux path config\n    bert_path = '{your BERT model path}'\n    root_path = '{project path}'\n ```\n Than Run:\n ```angular2html\npython3 bert_lstm_ner.py\n```\n\n### USING BLSTM-CRF OR ONLY CRF FOR DECODE!\nJust alter bert_lstm_ner.py line of 450, the params of the function of add_blstm_crf_layer: crf_only=True or False  \n\nONLY CRF output layer:\n```\n    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=FLAGS.lstm_size, cell_type=FLAGS.cell, num_layers=FLAGS.num_layers,\n                          dropout_rate=FLAGS.droupout_rate, initializers=initializers, num_labels=num_labels,\n                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n    rst = blstm_crf.add_blstm_crf_layer(crf_only=True)\n```\n\n\nBiLSTM with CRF output layer\n```\n    blstm_crf = BLSTM_CRF(embedded_chars=embedding, hidden_unit=FLAGS.lstm_size, cell_type=FLAGS.cell, num_layers=FLAGS.num_layers,\n                          dropout_rate=FLAGS.droupout_rate, initializers=initializers, num_labels=num_labels,\n                          seq_length=max_seq_length, labels=labels, lengths=lengths, is_training=is_training)\n    rst = blstm_crf.add_blstm_crf_layer(crf_only=False)\n```\n\n## ONLINE PREDICT\nIf model is train finished, just run\n```angular2html\npython3 terminal_predict.py\n```\n\n ## Using NER as Service\n\n#### Service \nUsing NER as Service is simple, you just need to run the python script below in the project root path:\n```angular2html\npython3 runs.py \\ \n    -mode NER\n    -bert_model_dir /home/macan/ml/data/chinese_L-12_H-768_A-12 \\\n    -ner_model_dir /home/macan/ml/data/bert_ner \\\n    -model_pd_dir /home/macan/ml/workspace/BERT_Base/output/predict_optimizer \\\n    -num_worker 8\n```\n\n\n#### Client\nThe client using methods can reference client_test.py script\n```angular2html\nimport time\nfrom client.client import BertClient\n\nner_model_dir = 'C:\\workspace\\python\\BERT_Base\\output\\predict_ner'\nwith BertClient( ner_model_dir=ner_model_dir, show_server_config=False, check_version=False, check_length=False, mode='NER') as bc:\n    start_t = time.perf_counter()\n    str = '1月24日，新华社对外发布了中央对雄安新区的指导意见，洋洋洒洒1.2万多字，17次提到北京，4次提到天津，信息量很大，其实也回答了人们关心的很多问题。'\n    rst = bc.encode([str])\n    print('rst:', rst)\n    print(time.perf_counter() - start_t)\n```\n\n\nNOTE: input format you can sometime reference bert as service project.    \nWelcome to provide more client language code like java or others.  \n ## Using yourself data to train\n if you want to use yourself data to train ner model,you just modify  the get_labes func.\n ```angular2html\ndef get_labels(self):\n        return [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n```\nNOTE: \"X\", “[CLS]”, “[SEP]” These three are necessary, you just replace your data label to this return list.  \nOr you can use last code lets the program automatically get the label from training data\n```angular2html\ndef get_labels(self):\n        # 通过读取train文件获取标签的方法会出现一定的风险。\n        if os.path.exists(os.path.join(FLAGS.output_dir, 'label_list.pkl')):\n            with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'rb') as rf:\n                self.labels = pickle.load(rf)\n        else:\n            if len(self.labels) > 0:\n                self.labels = self.labels.union(set([\"X\", \"[CLS]\", \"[SEP]\"]))\n                with codecs.open(os.path.join(FLAGS.output_dir, 'label_list.pkl'), 'wb') as rf:\n                    pickle.dump(self.labels, rf)\n            else:\n                self.labels = [\"O\", 'B-TIM', 'I-TIM', \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]\n        return self.labels\n\n```\n\n\n## Reference: \n+ The evaluation codes come from:https://github.com/guillaumegenthial/tf_metrics/blob/master/tf_metrics/__init__.py\n\n+ [https://github.com/google-research/bert](https://github.com/google-research/bert)\n\n+ [https://github.com/kyzhouhzau/BERT-NER](https://github.com/kyzhouhzau/BERT-NER)\n\n+ [https://github.com/zjy-ucas/ChineseNER](https://github.com/zjy-ucas/ChineseNER)\n\n+ [https://github.com/hanxiao/bert-as-service](https://github.com/hanxiao/bert-as-service)\n\n+ [https://github.com/macanv/BERT-BiLSTM-CRF-NER](https://github.com/macanv/BERT-BiLSTM-CRF-NER)\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/wxl18039675170",
    "keywords": "toolkit_bert_ner nlp ner NER named entity recognition bilstm crf tensorflow machine learning sentence encoding embedding serving",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "toolkit-bert-ner",
    "package_url": "https://pypi.org/project/toolkit-bert-ner/",
    "platform": "",
    "project_url": "https://pypi.org/project/toolkit-bert-ner/",
    "project_urls": {
      "Homepage": "https://github.com/wxl18039675170"
    },
    "release_url": "https://pypi.org/project/toolkit-bert-ner/1.0.2/",
    "requires_dist": [
      "numpy",
      "six",
      "pyzmq (>=16.0.0)",
      "GPUtil (>=1.3.0)",
      "termcolor (>=1.1)",
      "tensorflow (>=1.10.0) ; extra == 'cpu'",
      "tensorflow-gpu (>=1.10.0) ; extra == 'gpu'",
      "flask ; extra == 'http'",
      "flask-compress ; extra == 'http'",
      "flask-cors ; extra == 'http'",
      "flask-json ; extra == 'http'"
    ],
    "requires_python": "",
    "summary": "Use Google's BERT for Chinese natural language processing tasks such as named entity recognition and provide server services",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6456098,
  "releases": {
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "98c8b907a8d3933b4ce3dd71be063f8cf266165531d6a425a79eddf9ea5e8b43",
          "md5": "1c2f309e080313beaa01df8007dd7c49",
          "sha256": "3ca0298f2384bab62a7f226fa59e1954d98cd22ab8ef4d87fa83e17c1189a714"
        },
        "downloads": -1,
        "filename": "toolkit_bert_ner-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1c2f309e080313beaa01df8007dd7c49",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 106272,
        "upload_time": "2020-01-15T03:30:36",
        "upload_time_iso_8601": "2020-01-15T03:30:36.175393Z",
        "url": "https://files.pythonhosted.org/packages/98/c8/b907a8d3933b4ce3dd71be063f8cf266165531d6a425a79eddf9ea5e8b43/toolkit_bert_ner-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "938c7964762d65d37b8646665bee0153bfb4b7746e5f357de2eabc29bec2fea7",
          "md5": "3a31b5d6d95653fa885a8945167812f0",
          "sha256": "fb7f6f67fdafc400ce0b822f9503e24aaf24e6200a645175c52b52a985502c85"
        },
        "downloads": -1,
        "filename": "toolkit_bert_ner-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3a31b5d6d95653fa885a8945167812f0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 91210,
        "upload_time": "2020-01-15T03:30:40",
        "upload_time_iso_8601": "2020-01-15T03:30:40.268815Z",
        "url": "https://files.pythonhosted.org/packages/93/8c/7964762d65d37b8646665bee0153bfb4b7746e5f357de2eabc29bec2fea7/toolkit_bert_ner-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "98c8b907a8d3933b4ce3dd71be063f8cf266165531d6a425a79eddf9ea5e8b43",
        "md5": "1c2f309e080313beaa01df8007dd7c49",
        "sha256": "3ca0298f2384bab62a7f226fa59e1954d98cd22ab8ef4d87fa83e17c1189a714"
      },
      "downloads": -1,
      "filename": "toolkit_bert_ner-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1c2f309e080313beaa01df8007dd7c49",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 106272,
      "upload_time": "2020-01-15T03:30:36",
      "upload_time_iso_8601": "2020-01-15T03:30:36.175393Z",
      "url": "https://files.pythonhosted.org/packages/98/c8/b907a8d3933b4ce3dd71be063f8cf266165531d6a425a79eddf9ea5e8b43/toolkit_bert_ner-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "938c7964762d65d37b8646665bee0153bfb4b7746e5f357de2eabc29bec2fea7",
        "md5": "3a31b5d6d95653fa885a8945167812f0",
        "sha256": "fb7f6f67fdafc400ce0b822f9503e24aaf24e6200a645175c52b52a985502c85"
      },
      "downloads": -1,
      "filename": "toolkit_bert_ner-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "3a31b5d6d95653fa885a8945167812f0",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 91210,
      "upload_time": "2020-01-15T03:30:40",
      "upload_time_iso_8601": "2020-01-15T03:30:40.268815Z",
      "url": "https://files.pythonhosted.org/packages/93/8c/7964762d65d37b8646665bee0153bfb4b7746e5f357de2eabc29bec2fea7/toolkit_bert_ner-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}