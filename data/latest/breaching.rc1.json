{
  "info": {
    "author": "Jonas Geiping",
    "author_email": "jonas.geiping@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Topic :: Security"
    ],
    "description": "# Breaching - A Framework for Attacks against Privacy in Federated Learning\n\nThis PyTorch framework implements a number of gradient inversion attacks that *breach* privacy in federated learning scenarios,\ncovering examples with small and large aggregation sizes and examples both vision and text domains.\n\n\n![](examples/teaser_breaching.png)\n\nThis includes implementations of recent work such as:\n* Malicious-model attacks as described in \"Robbing The Fed\" https://openreview.net/forum?id=fwzUgo0FM9v\n* Attacks against transformer architectures described in \"Decepticons\" https://arxiv.org/abs/2201.12675\n* Fishing attacks that breach arbitrary aggregations described in https://arxiv.org/abs/2202.00580\n\nBut also a range of implementations of other attacks from optimization attacks (such as \"Inverting Gradients\" and \"See through Gradients\") to recent analytic and recursive attacks. Jupyter notebook examples for these attacks can be found in the `examples/` folder.\n\n## Overview:\nThis repository implements two main components. A list of modular attacks under `breaching.attacks` and a list of relevant use cases (including server threat model, user setup, model architecture and dataset) under `breaching.cases`. All attacks and scenarios are highly modular and can be customized and extended through the configuration at `breaching/config`.\n\n### Installation\nEither download this repository (including notebooks and examples) directly using `git clone` or install the python package via `pip install breaching` for easy access to key functionality.\n\nBecause this framework covers several use cases across vision and language, it also accumulates a kitchen-sink of dependencies. The full list of all dependencies can be found at `environment.yml` (and installed with conda by calling `conda env create --file environment.yml `), but the full list of dependencies not installed by default. Install these as necessary (for example install huggingface packages only if you are interested in language applications).\n\nYou can verify your installation by running `python simulate_breach.py dryrun=True`. This tests the simplest reconstruction setting with a single iteration.\n\n\n### Usage\nYou can load any use case by\n```\ncfg_case = breaching.get_case_config(case=\"1_single_imagenet\")\nuser, server, model, loss = breaching.cases.construct_case(cfg_case)\n```\nand load any attack by\n```\ncfg_attack = breaching.get_attack_config(attack=\"invertinggradients\")\nattacker = breaching.attacks.prepare_attack(model, loss, cfg_attack)\n```\n\nThis is a good spot to print out an overview over the loaded threat model and setting, maybe you would want to change some settings?\n```\nbreaching.utils.overview(server, user, attacker)\n```\n\nTo evaluate the attack, you can then simulate an FL exchange:\n```\nshared_user_data, payloads, true_user_data = server.run_protocol(user)\n```\nAnd then run the attack (which consumes only the user update and the server state):\n```\nreconstructed_user_data, stats = attacker.reconstruct(payloads, shared_user_data)\n```\n\nFor more details, have a look at the notebooks in the `examples/` folder, the cmd-line script `simulate_breach.py` or the minimal examples in `minimal_example.py` and `minimal_example_robbing_the_fed.py`.\n\n### What is this framework?\nThis framework is modular collections of attacks against federated learning that breach privacy by recovering user data from their updates sent to a central server. The framework covers gradient updates as well as updates from multiple local training steps and evaluates datasets and models in language and vision. Requirements and variations in the threat model for each attack (such as the existence of labels or number of data points) are made explicit. Modern initializations and label recovery strategies are also included.\n\nWe especially focus on clarifying the threat model of each attack and constraining the `attacker` to only act based on the `shared_user_data` objects generated by the user. All attacks should be as use-case agnostic as possible based only on these limited transmissions of data and implementing a new attack should require no knowledge of any use case. Likewise implementing a new use case should be entirely separate from the attack portion. Everything is highly configurable through `hydra` configuration syntax.\n\n### What does this framework not do?\nThis framework focuses only on attacks, implementing no defense aside from user-level differential privacy and aggregation. We wanted to focus only on attack evaluations and investigate the questions \"where do these attacks work currently\", and \"where are the limits\". Accordingly, the FL simulation is \"shallow\". No model is actually trained here and we investigate fixed checkpoints (which can be generated somewhere else). Other great repositories, such as https://github.com/Princeton-SysML/GradAttack focus on defenses and their performance during a full simulation of a FL protocol.\n\n\n### Attacks\nA list of all included attacks with references to their original publications can be found at `examples/README.md`.\n\n### Datasets\nMany examples for vision attacks show `ImageNet` examples. For this to work, you need to download the *ImageNet ILSVRC2012* dataset **manually**. However, almost all attacks require only the small validation set, which can be easily downloaded onto a laptop and do not look for the whole training set. If this is not an option for you, then the `Birdsnap` dataset is a reasonably drop-in replacement for ImageNet. By default, we further only show examples from `ImageNetAnimals`, which are the first 397 classes of the ImageNet dataset. This reduces the number of weird pictures of actual people substantially. Of course `CIFAR10` and `CIFAR100` are also around.\nFor these vision datasets there are several options in the literature on how to partition them for a FL simulation. We implement a range of such partitions with `data.partition`, ranging from `random` (but replicable and with no repetitions of data across users), over `balanced` (separate classes equally across users) to `unique-class` (every user owns data from a single class). When changing the partition you might also have to adjust the number of expected clients `data.default_clients` (for example, for `unique_class` there can be only `len(classes)` many users).\n\nFor language data, you can load `wikitext` which we split into separate users on a per-article basis, or the `stackoverflow` and `shakespeare` FL datasets from tensorflow federated, which are already split into users (installing `tensorflow-cpu` is required for these tensorflow-federated datasets).\n\nFurther, nothing stops you from skipping the `breaching.cases` sub-module and using your own code to load a model and dataset. An example can be found in `minimal_example.py`.\n\n## Metrics\n\nWe implement a range of metrics which can be queried through `breaching.analysis.report`. Several metrics (such as CW-SSIM and R-PSNR) require additional packages to be installed - they will warn about this. For language data we hook into a range of huggingface metrics. Overall though, we note that most of these metrics give only a partial picture of the actual severity of a breach of privacy, and are best handled with care.\n\n## Additional Topics\n\n### Benchmarking\nA script to benchmark attacks is included as `benchmark_breaches.py`. This script will iterate over the first valid `num_trials` users, attack each separately and average the resulting metrics. This can be useful for quantitative analysis of these attacks. The default case takes about a day to benchmark on a single GTX2080 GPU for optimization-based attacks, and less than 30 minutes for analytic attacks.\nUsing the default scripts for benchmarking and cmd-line executes also includes a bunch of convenience based mostly on `hydra`. This entails the creation of separate sub-folders for each experiment in `outputs/`. These folders contain logs, metrics and optionally recovered data for each run. Summary tables are written to `tables/`.\n\n### System Requirements\nAll attacks can be run on both CPU/GPU (any `torch.device` actually). However, the optimization-based attacks are very compute intensive and using a GPU is highly advised. The other attacks are cheap enough to be run on CPUs (The Decepticon attack for example does most of the heavy lifting in assignment problems on CPU anyway, for example).\n\n### Options\nIt is probably best to have a look into `breaching/config` to see all possible options.\n\n### Citation\nFor now, please cite the respective publications for each attack and use case.\n\n\n### License\nWe integrate several snippets of code from other repositories and refer to the licenses included in those files for more info.\nWe're especially thankful for related projects such as https://www.tensorflow.org/federated, https://github.com/NVlabs/DeepInversion, https://github.com/JunyiZhu-AI/R-GAP, https://github.com/facebookresearch/functorch, https://github.com/ildoonet/pytorch-gradual-warmup-lr and https://github.com/nadavbh12/VQ-VAE from which we incorporate components.\n\nFor the license of our code, refer to `LICENCE.md`.\n\n### Authors\nThis framework was built by me ([Jonas Geiping](https://github.com/JonasGeiping)), [Liam Fowl](https://github.com/lhfowl) and [Yuxin Wen](https://github.com/YuxinWenRick) while working at the University of Maryland, College Park.\n\n### Contributing\nIf you have an attack that you are interested in implementing in this framework, or a use case that is interesting to you, don't hesitate to contact us or open a pull-request.\n\n### Contact\nIf you have any questions, also don't hesitate to open an issue here on github or write us an email.\n\nMIT License\n\nCopyright (c) 2022 Jonas Geiping, Yuxin Wen, Liam Fowl\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/JonasGeiping/breaching",
    "keywords": "Federated Learning,Privacy,PyTorch,Machine Learning,Security Audit,Research",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "breaching",
    "package_url": "https://pypi.org/project/breaching/",
    "platform": "any",
    "project_url": "https://pypi.org/project/breaching/",
    "project_urls": {
      "Homepage": "https://github.com/JonasGeiping/breaching"
    },
    "release_url": "https://pypi.org/project/breaching/0.1.2/",
    "requires_dist": [
      "torch (>=1.10)",
      "torchvision",
      "hydra-core (>=1.1)",
      "scipy",
      "lpips"
    ],
    "requires_python": ">=3.7",
    "summary": "Framework for Attacks against Privacy in Federated Learning",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12903144,
  "releases": {
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "73b1f2fd688d00fa4784cb70ff88797c9c823ff569b67a4dbb1f36d52277c64b",
          "md5": "ac234240e4741f3cad27bfc6df4b28dd",
          "sha256": "a838348beb9981bc680ff6d922d38e854a7e7fabe7afd1409befb4da5dbc06e9"
        },
        "downloads": -1,
        "filename": "breaching-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ac234240e4741f3cad27bfc6df4b28dd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 181413,
        "upload_time": "2022-02-15T23:53:31",
        "upload_time_iso_8601": "2022-02-15T23:53:31.552066Z",
        "url": "https://files.pythonhosted.org/packages/73/b1/f2fd688d00fa4784cb70ff88797c9c823ff569b67a4dbb1f36d52277c64b/breaching-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c9f3606b3dd9904aeef8da6e2f558e051b19b00ef2b3afc7785016fa142fb1ab",
          "md5": "1536d7016c9eb30a9d40f3cfbc806da9",
          "sha256": "ab546297141128ed897479199a104258a93c35eea40762fa9068559bde5a7e25"
        },
        "downloads": -1,
        "filename": "breaching-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1536d7016c9eb30a9d40f3cfbc806da9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 144607,
        "upload_time": "2022-02-15T23:53:33",
        "upload_time_iso_8601": "2022-02-15T23:53:33.271700Z",
        "url": "https://files.pythonhosted.org/packages/c9/f3/606b3dd9904aeef8da6e2f558e051b19b00ef2b3afc7785016fa142fb1ab/breaching-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "73b1f2fd688d00fa4784cb70ff88797c9c823ff569b67a4dbb1f36d52277c64b",
        "md5": "ac234240e4741f3cad27bfc6df4b28dd",
        "sha256": "a838348beb9981bc680ff6d922d38e854a7e7fabe7afd1409befb4da5dbc06e9"
      },
      "downloads": -1,
      "filename": "breaching-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ac234240e4741f3cad27bfc6df4b28dd",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 181413,
      "upload_time": "2022-02-15T23:53:31",
      "upload_time_iso_8601": "2022-02-15T23:53:31.552066Z",
      "url": "https://files.pythonhosted.org/packages/73/b1/f2fd688d00fa4784cb70ff88797c9c823ff569b67a4dbb1f36d52277c64b/breaching-0.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c9f3606b3dd9904aeef8da6e2f558e051b19b00ef2b3afc7785016fa142fb1ab",
        "md5": "1536d7016c9eb30a9d40f3cfbc806da9",
        "sha256": "ab546297141128ed897479199a104258a93c35eea40762fa9068559bde5a7e25"
      },
      "downloads": -1,
      "filename": "breaching-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "1536d7016c9eb30a9d40f3cfbc806da9",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 144607,
      "upload_time": "2022-02-15T23:53:33",
      "upload_time_iso_8601": "2022-02-15T23:53:33.271700Z",
      "url": "https://files.pythonhosted.org/packages/c9/f3/606b3dd9904aeef8da6e2f558e051b19b00ef2b3afc7785016fa142fb1ab/breaching-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}