{
  "info": {
    "author": "Andrew Chen Wang",
    "author_email": "acwangpython@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Framework :: Django",
      "Intended Audience :: Developers",
      "License :: CC0 1.0 Universal (CC0 1.0) Public Domain Dedication",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Internet :: WWW/HTTP"
    ],
    "description": "# Hear-Ye/congress\n\nThis repository is a port from \n[unitedstates/congress](https://github.com/unitedstates/congress). We required a \nversioning policy which the `unitedstates` organization does not provide, so we made \nthis repository. Most changes here are still coming from the main repository,\nand any changes we make here will most likely end up in the main repository\nas well via PRs.\n\nAdditionally, we needed to store older data in bulk for our open source developers,\nso we decided to utilize GitHub Actions and its cron job. We update our releases\nof bulk data on a daily cycle. This can be found at:\n[Hear-Ye/congress-data](https://github.com/Hear-Ye/congress-data)\n\n## New Installation\n\nRun `pip install congress-crawler`\n\n### Notes!\n\nAfter reviewing much of this repository, lots of code is just missing and not updated.\nBe careful before using either this port or the main repository. Additionally, we want\nto maintain the same license as the main repository in the spirit of open source and\nbeing public domain. We firmly believe tools are the main gears of our society, and thus\nthis tool in particular should remain free.\n\nThe following is the documentation (mostly intact with more features and docs from our \nrepository) from [unitedstates/congress](https://github.com/unitedstates/congress).\n\n---\n\n## unitedstates/congress\n\nPublic domain code that collects data about the bills, amendments, roll call votes, and other core data about the U.S. Congress.\n\nIncludes:\n\n* A data importing script for the [official bulk bill status data](https://github.com/usgpo/bill-status) from Congress, the official source of information on the life and times of legislation.\n\n* Scrapers for House and Senate roll call votes.\n\n* A document fetcher for GovInfo.gov, which holds bill text, bill status, and other official documents.\n\n* A defunct THOMAS scraper for presidential nominations in Congress.\n\nRead about the contents and schema in the [documentation](https://github.com/unitedstates/congress/wiki) in the github project wiki.\n\nFor background on how this repository came to be, see [Eric's blog post](https://sunlightfoundation.com/blog/2013/08/20/a-modern-approach-to-open-data/).\n\n### Setting Up\n\nThis project supports Python 3.6+.\n\n**System dependencies**\n\nOn Ubuntu, you'll need `wget`, `pip`, and some support packages:\n\n```bash\nsudo apt-get install git python3-dev libxml2-dev libxslt1-dev libz-dev python3-pip python3-venv\n```\n\nOn OS X, you'll need developer tools installed ([XCode](https://developer.apple.com/xcode/)), and `wget`.\n\n```bash\nbrew install wget\n```\n\n**Python dependencies**\n\nIt's recommended you use a `virtualenv` (virtual environment) for development. Create a virtualenv for this project:\n\n```bash\npython3 -m venv congress\nsource congress/bin/activate\n```\nFinally, with your virtual environment activated, install Python packages:\n\n```bash\npip3 install -r requirements.txt\n```\n\n### Collecting the data\n\nThe general form to start the scraping process is:\n\n    ./run <data-type> [--force] [other options]\n\nwhere data-type is one of:\n\n* `bills` (see [Bills](https://github.com/unitedstates/congress/wiki/bills)) and [Amendments](https://github.com/unitedstates/congress/wiki/amendments))\n* `votes` (see [Votes](https://github.com/unitedstates/congress/wiki/votes))\n* `nominations` (see [Nominations](https://github.com/unitedstates/congress/wiki/nominations))\n* `committee_meetings` (see [Committee Meetings](https://github.com/unitedstates/congress/wiki/committee-meetings))\n* `govinfo` (see [Bill Text](https://github.com/unitedstates/congress/wiki/bill-text))\n* `statutes` (see [Bills](https://github.com/unitedstates/congress/wiki/bills) and [Bill Text](https://github.com/unitedstates/congress/wiki/bill-text))\n\nTo get data for bills, resolutions, and amendments, run:\n\n```bash\n./run govinfo --bulkdata=BILLSTATUS\n./run bills\n```\n\nThe bills script will output bulk data into a top-level `data` directory, then organized by Congress number, bill type, and bill number. Two data output files will be generated for each bill: a JSON version (data.json) and an XML version (data.xml).\n\n### Common options\n\nDebugging messages are hidden by default. To include them, run with --log=info or --debug. To hide even warnings, run with --log=error.\n\nTo get emailed with errors, copy config.yml.example to config.yml and fill in the SMTP options. The script will automatically use the details when a parsing or execution error occurs.\n\nThe --force flag applies to all data types and supresses use of a cache for network-retreived resources.\n\n### Data Output\n\nThe script will cache downloaded pages in a top-level `cache` directory, and output bulk data in a top-level `data` directory.\n\nTwo bulk data output files will be generated for each object: a JSON version (data.json) and an XML version (data.xml). The XML version attempts to maintain backwards compatibility with the XML bulk data that [GovTrack.us](https://www.govtrack.us) has provided for years. Add the --govtrack flag to get fully backward-compatible output using GovTrack IDs (otherwise the source IDs used for legislators is used).\n\nSee the [project wiki](https://github.com/unitedstates/congress/wiki) for documentation on the output format.\n\n### Contributing\n\nPull requests with patches are awesome. Unit tests are strongly encouraged ([example tests](https://github.com/unitedstates/congress/blob/master/test/test_bill_actions.py)).\n\nThe best way to file a bug is to [open a ticket](https://github.com/unitedstates/congress/issues).\n\n\n### Running tests\n\nTo run this project's unit tests:\n\n```bash\n./test/run\n```\n\n### Who's Using This Data\n\nThe [Sunlight Foundation](https://sunlightfoundation.com) and [GovTrack.us](https://www.govtrack.us) are the two principal maintainers of this project.\n\nBoth Sunlight and GovTrack operate APIs where you can get much of this data delivered over HTTP:\n\n* [GovTrack.us API](https://www.govtrack.us/developers/api)\n* [Sunlight Congress API](https://sunlightlabs.github.io/congress/)\n\n## Public domain\n\nThis project is [dedicated to the public domain](LICENSE). As spelled out in [CONTRIBUTING](CONTRIBUTING.md):\n\n> The project is in the public domain within the United States, and copyright and related rights in the work worldwide are waived through the [CC0 1.0 Universal public domain dedication](https://creativecommons.org/publicdomain/zero/1.0/).\n\n> All contributions to this project will be released under the CC0 dedication. By submitting a pull request, you are agreeing to comply with this waiver of copyright interest.\n\n[![Build Status](https://travis-ci.org/unitedstates/congress.svg?branch=master)](https://travis-ci.org/unitedstates/congress)\n\n\n",
    "description_content_type": "",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Hear-Ye/congress",
    "keywords": "",
    "license": "CC0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "congress-crawler",
    "package_url": "https://pypi.org/project/congress-crawler/",
    "platform": "",
    "project_url": "https://pypi.org/project/congress-crawler/",
    "project_urls": {
      "Homepage": "https://github.com/Hear-Ye/congress"
    },
    "release_url": "https://pypi.org/project/congress-crawler/0.0.2/",
    "requires_dist": [
      "pyyaml",
      "python-dateutil",
      "lxml (>=2.2)",
      "pytz",
      "cssselect",
      "scrapelib",
      "mechanize",
      "BeautifulSoup4",
      "xmltodict",
      "rtyaml",
      "requests"
    ],
    "requires_python": "",
    "summary": "Gathers data on the U.S. Congress.",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10843767,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3ec2813315b74115e3251b81c0965467f5095fb5bd1dd8f414d274c213664916",
          "md5": "5eab674f1c97d9c0ea7d918e0d188121",
          "sha256": "1e3bbe7a12c4f3aa7934eae7bedc497f0b67f2bdf0e3f482aec9ece104a40829"
        },
        "downloads": -1,
        "filename": "congress_crawler-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5eab674f1c97d9c0ea7d918e0d188121",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 96699,
        "upload_time": "2021-07-05T23:31:20",
        "upload_time_iso_8601": "2021-07-05T23:31:20.846116Z",
        "url": "https://files.pythonhosted.org/packages/3e/c2/813315b74115e3251b81c0965467f5095fb5bd1dd8f414d274c213664916/congress_crawler-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "77aad42f8fb6bf2ac7aaf3c0ec148d3837d58679fbf8d6c15f3e3af35e6bc0ab",
          "md5": "e16ad6e3a1c2a75823997423f035c281",
          "sha256": "16cb5b3d8c78db4c7c2352c5d7487c7ad0066eb39431ed2a6097f47eb4675cc5"
        },
        "downloads": -1,
        "filename": "congress-crawler-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e16ad6e3a1c2a75823997423f035c281",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 97334,
        "upload_time": "2021-07-05T23:31:21",
        "upload_time_iso_8601": "2021-07-05T23:31:21.938564Z",
        "url": "https://files.pythonhosted.org/packages/77/aa/d42f8fb6bf2ac7aaf3c0ec148d3837d58679fbf8d6c15f3e3af35e6bc0ab/congress-crawler-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7fbd4fd2a2a8cb298c99b8681a1b8c9a64504a3fbebce0c81ddc664afad2fb10",
          "md5": "af32b98c9f2039e5eb1bbc0eeb75cc30",
          "sha256": "72dbce43d0355c2303837730f5d6fd0df5e8e210198ce3ef1230b04276a6c998"
        },
        "downloads": -1,
        "filename": "congress_crawler-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "af32b98c9f2039e5eb1bbc0eeb75cc30",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 96705,
        "upload_time": "2021-07-07T09:11:16",
        "upload_time_iso_8601": "2021-07-07T09:11:16.916224Z",
        "url": "https://files.pythonhosted.org/packages/7f/bd/4fd2a2a8cb298c99b8681a1b8c9a64504a3fbebce0c81ddc664afad2fb10/congress_crawler-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a6956687a0d6421f52e6f021482da86be12e2897af4a6fdf020caa1baa091263",
          "md5": "3f91c699347b06ba88055b9a6277af97",
          "sha256": "5e3e5bc730e40076a0a4764fe2531f7209012c7565627ffd48c4397a42ad928c"
        },
        "downloads": -1,
        "filename": "congress-crawler-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3f91c699347b06ba88055b9a6277af97",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 97479,
        "upload_time": "2021-07-07T09:11:18",
        "upload_time_iso_8601": "2021-07-07T09:11:18.260759Z",
        "url": "https://files.pythonhosted.org/packages/a6/95/6687a0d6421f52e6f021482da86be12e2897af4a6fdf020caa1baa091263/congress-crawler-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7fbd4fd2a2a8cb298c99b8681a1b8c9a64504a3fbebce0c81ddc664afad2fb10",
        "md5": "af32b98c9f2039e5eb1bbc0eeb75cc30",
        "sha256": "72dbce43d0355c2303837730f5d6fd0df5e8e210198ce3ef1230b04276a6c998"
      },
      "downloads": -1,
      "filename": "congress_crawler-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "af32b98c9f2039e5eb1bbc0eeb75cc30",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 96705,
      "upload_time": "2021-07-07T09:11:16",
      "upload_time_iso_8601": "2021-07-07T09:11:16.916224Z",
      "url": "https://files.pythonhosted.org/packages/7f/bd/4fd2a2a8cb298c99b8681a1b8c9a64504a3fbebce0c81ddc664afad2fb10/congress_crawler-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a6956687a0d6421f52e6f021482da86be12e2897af4a6fdf020caa1baa091263",
        "md5": "3f91c699347b06ba88055b9a6277af97",
        "sha256": "5e3e5bc730e40076a0a4764fe2531f7209012c7565627ffd48c4397a42ad928c"
      },
      "downloads": -1,
      "filename": "congress-crawler-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "3f91c699347b06ba88055b9a6277af97",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 97479,
      "upload_time": "2021-07-07T09:11:18",
      "upload_time_iso_8601": "2021-07-07T09:11:18.260759Z",
      "url": "https://files.pythonhosted.org/packages/a6/95/6687a0d6421f52e6f021482da86be12e2897af4a6fdf020caa1baa091263/congress-crawler-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}