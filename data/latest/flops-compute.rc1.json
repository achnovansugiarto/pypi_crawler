{
  "info": {
    "author": "junxi",
    "author_email": "843525908@qq.com",
    "bugtrack_url": null,
    "classifiers": [
      "Programming Language :: Python :: 3"
    ],
    "description": "# Flops counter for convolutional networks in pytorch framework\n[![Pypi version](https://img.shields.io/pypi/v/ptflops.svg)](https://pypi.org/project/ptflops/)\n[![Build Status](https://travis-ci.com/sovrasov/flops-counter.pytorch.svg?branch=master)](https://travis-ci.com/sovrasov/flops-counter.pytorch)\n\nThis script is designed to compute the theoretical amount of multiply-add operations\nin convolutional neural networks. It also can compute the number of parameters and\nprint per-layer computational cost of a given network.\n\nSupported layers:\n- Conv1d/2d/3d (including grouping)\n- ConvTranspose2d (including grouping)\n- BatchNorm1d/2d/3d\n- Activations (ReLU, PReLU, ELU, ReLU6, LeakyReLU)\n- Linear\n- Upsample\n- Poolings (AvgPool1d/2d/3d, MaxPool1d/2d/3d and adaptive ones)\n\nExperimental support:\n- RNN, LSTM, GRU (NLH layout is assumed).\n- RNNCell, LSTMCell, GRUCell.\n\nRequirements: Pytorch >= 0.4.1, torchvision >= 0.2.1\n\nThanks to @warmspringwinds for the initial version of script.\n\n## Usage tips\n\n- This script doesn't take into account `torch.nn.functional.*` operations. For an instance, if one have a semantic segmentation model and use `torch.nn.functional.interpolate` to upscale features, these operations won't contribute to overall amount of flops. To avoid that one can use `torch.nn.Upsample` instead of `torch.nn.functional.interpolate`.\n- `ptflops` launches a given model on a random tensor and estimates amount of computations during inference. Complicated models can have several inputs, some of them could be optional. To construct non-trivial input one can use the `input_constructor` argument of the `get_model_complexity_info`. `input_constructor` is a function that takes the input spatial resolution as a tuple and returns a dict with named input arguments of the model. Next this dict would be passed to the model as keyworded arguments.\n- `verbose` parameter allows to get information about modules that don't contribute to the final numbers.\n- `ignore_modules` option forces `ptflops` to ignore the listed modules. This can be useful\nfor research purposes. For an instance, one can drop all convolutuions from the counting process\nspecifying `ignore_modules=[torch.nn.Conv2d]`.\n\n## Install the latest version\nFrom PyPI:\n```bash\npip install ptflops\n```\n\nFrom this repository:\n```bash\npip install --upgrade git+https://github.com/sovrasov/flops-counter.pytorch.git\n```\n\n## Example\n```python\nimport torchvision.models as models\nimport torch\nfrom ptflops import get_model_complexity_info\n\nwith torch.cuda.device(0):\n  net = models.densenet161()\n  macs, params = get_model_complexity_info(net, (3, 224, 224), as_strings=True,\n                                           print_per_layer_stat=True, verbose=True)\n  print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n  print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n```\n\n## Benchmark\n\n### [torchvision](https://pytorch.org/docs/1.0.0/torchvision/models.html)\n\nModel         | Input Resolution | Params(M) | MACs(G) | Top-1 error | Top-5 error\n---           |---               |---        |---      |---          |---\nalexnet       |224x224           | 61.1      | 0.72    | 43.45       | 20.91\nvgg11         |224x224           | 132.86    | 7.63    | 30.98       | 11.37\nvgg13         |224x224           | 133.05    | 11.34   | 30.07       | 10.75\nvgg16         |224x224           | 138.36    | 15.5    | 28.41       | 9.62\nvgg19         |224x224           | 143.67    | 19.67   | 27.62       | 9.12\nvgg11_bn      |224x224           | 132.87    | 7.64    | 29.62       | 10.19\nvgg13_bn      |224x224           | 133.05    | 11.36   | 28.45       | 9.63\nvgg16_bn      |224x224           | 138.37    | 15.53   | 26.63       | 8.50\nvgg19_bn      |224x224           | 143.68    | 19.7    | 25.76       | 8.15\nresnet18      |224x224           | 11.69     | 1.82    | 30.24       | 10.92\nresnet34      |224x224           | 21.8      | 3.68    | 26.70       | 8.58\nresnet50      |224x224           | 25.56     | 4.12    | 23.85       | 7.13\nresnet101     |224x224           | 44.55     | 7.85    | 22.63       | 6.44\nresnet152     |224x224           | 60.19     | 11.58   | 21.69       | 5.94\nsqueezenet1_0 |224x224           | 1.25      | 0.83    | 41.90       | 19.58\nsqueezenet1_1 |224x224           | 1.24      | 0.36    | 41.81       | 19.38\ndensenet121   |224x224           | 7.98      | 2.88    | 25.35       | 7.83\ndensenet169   |224x224           | 14.15     | 3.42    | 24.00       | 7.00\ndensenet201   |224x224           | 20.01     | 4.37    | 22.80       | 6.43\ndensenet161   |224x224           | 28.68     | 7.82    | 22.35       | 6.20\ninception_v3  |224x224           | 27.16     | 2.85    | 22.55       | 6.44\n\n* Top-1 error - ImageNet single-crop top-1 error (224x224)\n* Top-5 error - ImageNet single-crop top-5 error (224x224)\n\n### [Cadene/pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch)\n\nModel               | Input Resolution | Params(M) | MACs(G)     | Acc@1       | Acc@5\n---                 |---               |---        |---          |---          |---\nalexnet             | 224x224          | 61.1      | 0.72        | 56.432      | 79.194\nbninception         | 224x224          | 11.3      | 2.05        | 73.524      | 91.562\ncafferesnet101      | 224x224          | 44.55     | 7.62        | 76.2        | 92.766\ndensenet121         | 224x224          | 7.98      | 2.88        | 74.646      | 92.136\ndensenet161         | 224x224          | 28.68     | 7.82        | 77.56       | 93.798\ndensenet169         | 224x224          | 14.15     | 3.42        | 76.026      | 92.992\ndensenet201         | 224x224          | 20.01     | 4.37        | 77.152      | 93.548\ndpn107              | 224x224          | 86.92     | 18.42       | 79.746      | 94.684\ndpn131              | 224x224          | 79.25     | 16.13       | 79.432      | 94.574\ndpn68               | 224x224          | 12.61     | 2.36        | 75.868      | 92.774\ndpn68b              | 224x224          | 12.61     | 2.36        | 77.034      | 93.59\ndpn92               | 224x224          | 37.67     | 6.56        | 79.4        | 94.62\ndpn98               | 224x224          | 61.57     | 11.76       | 79.224      | 94.488\nfbresnet152         | 224x224          | 60.27     | 11.6        | 77.386      | 93.594\ninceptionresnetv2   | 299x299          | 55.84     | 13.22       | 80.17       | 95.234\ninceptionv3         | 299x299          | 27.16     | 5.73        | 77.294      | 93.454\ninceptionv4         | 299x299          | 42.68     | 12.31       | 80.062      | 94.926\nnasnetalarge        | 331x331          | 88.75     | 24.04       | 82.566      | 96.086\nnasnetamobile       | 224x224          | 5.29      | 0.59        | 74.08       | 91.74\npnasnet5large       | 331x331          | 86.06     | 25.21       | 82.736      | 95.992\npolynet             | 331x331          | 95.37     | 34.9        | 81.002      | 95.624\nresnet101           | 224x224          | 44.55     | 7.85        | 77.438      | 93.672\nresnet152           | 224x224          | 60.19     | 11.58       | 78.428      | 94.11\nresnet18            | 224x224          | 11.69     | 1.82        | 70.142      | 89.274\nresnet34            | 224x224          | 21.8      | 3.68        | 73.554      | 91.456\nresnet50            | 224x224          | 25.56     | 4.12        | 76.002      | 92.98\nresnext101_32x4d    | 224x224          | 44.18     | 8.03        | 78.188      | 93.886\nresnext101_64x4d    | 224x224          | 83.46     | 15.55       | 78.956      | 94.252\nse_resnet101        | 224x224          | 49.33     | 7.63        | 78.396      | 94.258\nse_resnet152        | 224x224          | 66.82     | 11.37       | 78.658      | 94.374\nse_resnet50         | 224x224          | 28.09     | 3.9         | 77.636      | 93.752\nse_resnext101_32x4d | 224x224          | 48.96     | 8.05        | 80.236      | 95.028\nse_resnext50_32x4d  | 224x224          | 27.56     | 4.28        | 79.076      | 94.434\nsenet154            | 224x224          | 115.09    | 20.82       | 81.304      | 95.498\nsqueezenet1_0       | 224x224          | 1.25      | 0.83        | 58.108      | 80.428\nsqueezenet1_1       | 224x224          | 1.24      | 0.36        | 58.25       | 80.8\nvgg11               | 224x224          | 132.86    | 7.63        | 68.97       | 88.746\nvgg11_bn            | 224x224          | 132.87    | 7.64        | 70.452      | 89.818\nvgg13               | 224x224          | 133.05    | 11.34       | 69.662      | 89.264\nvgg13_bn            | 224x224          | 133.05    | 11.36       | 71.508      | 90.494\nvgg16               | 224x224          | 138.36    | 15.5        | 71.636      | 90.354\nvgg16_bn            | 224x224          | 138.37    | 15.53       | 73.518      | 91.608\nvgg19               | 224x224          | 143.67    | 19.67       | 72.08       | 90.822\nvgg19_bn            | 224x224          | 143.68    | 19.7        | 74.266      | 92.066\nxception            | 299x299          | 22.86     | 8.42        | 78.888      | 94.292\n\n* Acc@1 - ImageNet single-crop top-1 accuracy on validation images of the same size used during the training process.\n* Acc@5 - ImageNet single-crop top-5 accuracy on validation images of the same size used during the training process.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "flops-compute",
    "package_url": "https://pypi.org/project/flops-compute/",
    "platform": "",
    "project_url": "https://pypi.org/project/flops-compute/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/flops-compute/0.6.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Flops counter for convolutional networks inpytorch framework",
    "version": "0.6.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8611550,
  "releases": {
    "0.6.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "14184f91ecdc6db4a414ef989e83db7f494d831dc72e118904b8c937786ef3a1",
          "md5": "3bce5d051df7e4ec81318a09764ae006",
          "sha256": "61d31db8bc577e5a02be09bfad8346582a0ebf4e3641b8f685c9c9791a29ae01"
        },
        "downloads": -1,
        "filename": "flops_compute-0.6.2.tar.gz",
        "has_sig": false,
        "md5_digest": "3bce5d051df7e4ec81318a09764ae006",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 8746,
        "upload_time": "2020-11-09T06:20:26",
        "upload_time_iso_8601": "2020-11-09T06:20:26.301861Z",
        "url": "https://files.pythonhosted.org/packages/14/18/4f91ecdc6db4a414ef989e83db7f494d831dc72e118904b8c937786ef3a1/flops_compute-0.6.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "14184f91ecdc6db4a414ef989e83db7f494d831dc72e118904b8c937786ef3a1",
        "md5": "3bce5d051df7e4ec81318a09764ae006",
        "sha256": "61d31db8bc577e5a02be09bfad8346582a0ebf4e3641b8f685c9c9791a29ae01"
      },
      "downloads": -1,
      "filename": "flops_compute-0.6.2.tar.gz",
      "has_sig": false,
      "md5_digest": "3bce5d051df7e4ec81318a09764ae006",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 8746,
      "upload_time": "2020-11-09T06:20:26",
      "upload_time_iso_8601": "2020-11-09T06:20:26.301861Z",
      "url": "https://files.pythonhosted.org/packages/14/18/4f91ecdc6db4a414ef989e83db7f494d831dc72e118904b8c937786ef3a1/flops_compute-0.6.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}