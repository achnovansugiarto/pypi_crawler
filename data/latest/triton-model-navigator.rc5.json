{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "Operating System :: MacOS",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Software Development"
    ],
    "description": "<!--\nCopyright (c) 2021-2023, NVIDIA CORPORATION. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Project description\n\nThe [Triton Model Navigator](https://github.com/triton-inference-server/model_navigator) automates\nthe process of moving model from source to deployment on\n[Triton Inference Server](https://github.com/triton-inference-server/server). The tool validate possible\nexport and conversion paths to serializable formats like [TensorRT](https://github.com/NVIDIA/TensorRT) and\nselect the most promising format for production deployment.\n\nThe Triton Model Navigator is designed to provide a single entrypoint for each supported framework. The usage is\nsimple as call a dedicated `optimize` function to start the process of searching for the best\npossible deployment by going through a broad spectrum of model conversions.\n\nThe `optimize` internally it performs model export, conversion, correctness testing, performance profiling,\nand saves all generated artifacts in the `navigator_workspace`, which is represented by a returned `package` object.\nThe result of `optimize` process can be saved as a portable Navigator Package with the `save` function.\nSaved packages only contain the base model formats along with the best selected format based on latency and throughput.\nThe package can be reused to recreate the process on same or different hardware. The configuration and execution status\nis saved in the `status.yaml` file located inside the workspace and the `Navigator Package`.\n\nFinally, the `Navigator Package` can be used for model deployment\non [NVIDIA Triton Inference Server](https://github.com/triton-inference-server/server). Dedicated API helps with obtaining all\nnecessary parameters and creating `model_repository` or receive the optimized model for inference in Python environment.\n\n# Installing\n\nThe package can be installed from `pypi.org` using:\n\n<!--pytest.mark.skip-->\n\n```shell\npip install -U triton-model-navigator[<extras,>]\n```\n\nExtras:\n\n- tensorflow - Model Navigator for TensorFlow2\n- jax - Model Navigator for JAX\n\n# Quick Start\n\nThe quick start presents how to optimize Python model for deployment on Triton Inference Server. In the\nexample we are using a simple TensorFlow 2 model.\n\n## Export and optimize model\n\nTo use Triton Model Navigator you must prepare model and dataloader. We recommend to create following helper\nfunctions:\n\n- `get_model` - return model object\n- `get_dataloader` - generate samples required for export and conversion\n- `get_verify_func` (optional) - validate the correctness of models based on implemented metric\n\nNext you can use Triton Model Navigator `optimize` function with provided model, dataloader and verify function\nto export and convert model to all supported formats.\n\nSee the below example of optimizing a simple TensorFlow model.\n\n```python\nimport logging\n\nimport numpy as np\nimport tensorflow as tf\n\nimport model_navigator as nav\n\n# enable tensorflow memory growth to avoid allocating all GPU memory\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n\nLOGGER = logging.getLogger(__name__)\n\n\n# dataloader is used for inference and finding input shapes of the model.\n# If you do not have dataloader, create one with samples with min and max shapes.\ndef get_dataloader():\n    return [np.random.rand(1, 224, 224, 3).astype(\"float32\") for _ in range(10)]\n\n\ndef get_verify_function():\n    def verify_func(ys_runner, ys_expected):\n        for a, b in zip(ys_runner, ys_expected):\n            if not (np.isclose(a[\"output__0\"], b[\"output__0\"], atol=0.01)).all():\n                return False\n\n        return True\n\n    return verify_func\n\n\n# Model inputs must be a Tensor to support deployment on Triton Inference Server.\ndef get_model():\n    inp = tf.keras.layers.Input((224, 224, 3))\n    layer_output = tf.keras.layers.Lambda(lambda x: x)(inp)\n    layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\n    layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\n    layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\n    layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\n    model_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)\n    return tf.keras.Model(inp, model_output)\n\n# Check documentation for more details about Profiler Configuration options.\ndef get_profiler_config():\n    return nav.ProfilerConfig()\n\n\nmodel = get_model()\ndataloader = get_dataloader()\nverify_func = get_verify_function()\nprofiler_config = get_profiler_config()\n\n# Model Navigator optimize starts export, optimization and testing process.\n# The resulting package represents all artifacts produced by Model Navigator.\npackage = nav.tensorflow.optimize(\n    model=model,\n    profiler_config=profiler_config,\n    target_formats=(nav.Format.ONNX,),\n    dataloader=dataloader,\n    verify_func=verify_func,\n)\n\n# Save nav package that can be used for Triton Inference Server deployment or obtaining model runner later.\n# The package contains base format checkpoints that can be used for all other conversions.\n# Models with minimal latency and maximal throughput are added to the package.\nnav.package.save(package=package, path=\"mlp.nav\")\n```\n\nYou can customize behavior of export and conversion steps\npassing [CustomConfig][model_navigator.api.config.CustomConfig]\nto `optimize` function.\n\n## NVIDIA Triton Inference Server deployment\n\nIf you prefer the standalone [NVIDIA Triton Inference Server](https://github.com/triton-inference-server) you can create\nand use `model_repository`.\n\n```python\nimport logging\nimport pathlib\n\nfrom model_navigator.exceptions import (\n    ModelNavigatorEmptyPackageError,\n    ModelNavigatorError,\n    ModelNavigatorWrongParameterError\n)\nimport model_navigator as nav\n\nLOGGER = logging.getLogger(__name__)\n\npackage = nav.package.load(\"mlp.nav\", \"load_workspace\")\n\n# Create model_repository for standalone Triton deployment\ntry:\n    nav.triton.model_repository.add_model_from_package(\n        model_repository_path=pathlib.Path(\"model_repository\"), model_name=\"dummy_model\", package=package\n    )\nexcept (ModelNavigatorWrongParameterError, ModelNavigatorEmptyPackageError, ModelNavigatorError) as e:\n    LOGGER.warning(f\"Model repository cannot be created.\\n{str(e)}\")\n```\n\nUse command to start server with provided `model_repository`:\n\n```shell\n$ docker run --gpus=1 --rm \\\n  -p8000:8000 \\\n  -p8001:8001 \\\n  -p8002:8002 \\\n  -v ${PWD}/model_repository:/models \\\n  nvcr.io/nvidia/tritonserver:23.01-py3 \\\n  tritonserver --model-repository=/models\n```\n\n# Examples\n\nWe provide simple examples how to use Triton Model Navigator to optimize the PyTorch, TensorFlow2, JAX and ONNX models\nfor deployment on Triton Inference Server.\n\n## Optimize for various frameworks\n\n- `PyTorch`:\n  * [Linear Model](https://github.com/triton-inference-server/model_navigator/examples/torch/linear)\n  * [ResNet50](https://github.com/triton-inference-server/model_navigator/examples/torch/resnet50)\n  * [BERT](https://github.com/triton-inference-server/model_navigator/examples/torch/bert)\n\n- `TensorFlow`:\n  * [Linear Model](https://github.com/triton-inference-server/model_navigator/examples/tensorflow/linear)\n  * [EfficientNet](https://github.com/triton-inference-server/model_navigator/examples/tensorflow/efficientnet)\n  * [BERT](https://github.com/triton-inference-server/model_navigator/examples/tensorflow/bert)\n\n- `JAX`:\n  * [Linear Model](https://github.com/triton-inference-server/model_navigator/examples/jax/linear)\n  * [GPT-2](https://github.com/triton-inference-server/model_navigator/examples/jax/gpt2)\n\n- `ONNX`:\n  * [Identity Model](https://github.com/triton-inference-server/model_navigator/examples/onnx/identity)\n\n## Optimize Navigator Package\n\nThe Navigator Package can be reused for optimize e.g. on the new hardware or with newer libraries.\nThe example code can be found in [examples/package](https://github.com/triton-inference-server/model_navigator/examples/package).\n\n## Using model on Triton Inference Server\n\nThe optimized model by Triton Model Navigator can be used for serving inference through Triton Inference Server. The\nexample code can be found in [examples/triton](https://github.com/triton-inference-server/model_navigator/examples/triton).\n\n\n# Links\n\n* Documentation: https://triton-inference-server.github.io/model_navigator\n* Source: https://github.com/triton-inference-server/model_navigator\n* Issues: https://github.com/triton-inference-server/model_navigator/issues\n* Changelog: https://github.com/triton-inference-server/model_navigator/CHANGELOG.md\n* Known Issues: https://github.com/triton-inference-server/model_navigator/docs/known_issues.md\n* Contributing: https://github.com/triton-inference-server/model_navigator/CONTRIBUTING.md\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "triton,inference,server,service,nvidia,tensorrt,onnx,tensorflow,pytorch,jax",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "triton-model-navigator",
    "package_url": "https://pypi.org/project/triton-model-navigator/",
    "platform": null,
    "project_url": "https://pypi.org/project/triton-model-navigator/",
    "project_urls": {
      "Documentation": "https://triton-inference-server.github.io/model_navigator",
      "Source": "https://github.com/triton-inference-server/model_navigator",
      "Tracker": "https://github.com/triton-inference-server/model_navigator/issues"
    },
    "release_url": "https://pypi.org/project/triton-model-navigator/0.5.1/",
    "requires_dist": [
      "polygraphy (>=0.33.1)",
      "coloredlogs (>=15.0.0)",
      "dacite (>=1.6.0)",
      "fire (>=0.4.0)",
      "jsonlines (>=3.1.0)",
      "mpmath (<1.0.0)",
      "numpy (<1.24,~=1.21)",
      "onnx (>=1.8.1)",
      "onnxruntime-gpu (<1.14,~=1.13)",
      "onnx-graphsurgeon (>=0.3.14)",
      "protobuf (<3.21.0,>=3.18.3)",
      "psutil (>=5.7)",
      "py-cpuinfo (>=8.0)",
      "pyyaml (~=6.0)",
      "tabulate (>=0.8)",
      "tritonclient[all]",
      "triton-model-navigator[test] ; extra == 'dev'",
      "triton-model-navigator[doc] ; extra == 'dev'",
      "black (>=22.8) ; extra == 'dev'",
      "build (>=0.8) ; extra == 'dev'",
      "ipython (>=7.16) ; extra == 'dev'",
      "isort (>=5.10) ; extra == 'dev'",
      "pdbpp (>=0.10) ; extra == 'dev'",
      "pip (>=21.1) ; extra == 'dev'",
      "pre-commit (>=2.20.0) ; extra == 'dev'",
      "psutil (~=5.1) ; extra == 'dev'",
      "GitPython (>=3.1.30) ; extra == 'doc'",
      "mike (>=1.1.0) ; extra == 'doc'",
      "mkdocs-htmlproofer-plugin (>=0.8.0) ; extra == 'doc'",
      "mkdocs-material (>=8.5.6) ; extra == 'doc'",
      "mkdocstrings[python] (>=0.19.0) ; extra == 'doc'",
      "tf2onnx (>=1.9.3) ; extra == 'jax'",
      "tf2onnx (>=1.9.3) ; extra == 'tensorflow'",
      "GitPython (>=3.1.30) ; extra == 'test'",
      "pytest (>=5.2) ; extra == 'test'",
      "pytest-mock (>=3.8.2) ; extra == 'test'",
      "pytype (!=2021.11.18,!=2022.2.17) ; extra == 'test'",
      "pre-commit (>=2.20.0) ; extra == 'test'",
      "pytest-unordered (~=0.5) ; extra == 'test'",
      "tox (>=3.23.1) ; extra == 'test'",
      "tqdm (>=4.64.1) ; extra == 'test'"
    ],
    "requires_python": "<4,>=3.8",
    "summary": "Triton Model Navigator provides tools supporting to create Deep Learning production ready inference models",
    "version": "0.5.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17506452,
  "releases": {
    "0.4.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b18c35d2454fa4f73b75729729947e1f7ae35aafc8e3e7176747db242c856407",
          "md5": "bb33e81f7e714a94fae0af455122a2a0",
          "sha256": "4db5a590974ad163b20c059360d4cc89f10b23f1067c0eadfbb5b2165a35f26c"
        },
        "downloads": -1,
        "filename": "triton_model_navigator-0.4.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bb33e81f7e714a94fae0af455122a2a0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.8",
        "size": 203414,
        "upload_time": "2023-03-29T19:55:32",
        "upload_time_iso_8601": "2023-03-29T19:55:32.779466Z",
        "url": "https://files.pythonhosted.org/packages/b1/8c/35d2454fa4f73b75729729947e1f7ae35aafc8e3e7176747db242c856407/triton_model_navigator-0.4.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a527bc445a81e545a95d51b2b5da1992b93b9563534d871d659b92f8e14a9a86",
          "md5": "2eadc3e9b0996b86fa7b75df0d989a8e",
          "sha256": "3057dfdd5660d8bfc37873d49e8db72f6d0c8c4501fbe405ff4cec255dfa9cb5"
        },
        "downloads": -1,
        "filename": "triton_model_navigator-0.4.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2eadc3e9b0996b86fa7b75df0d989a8e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.8",
        "size": 203441,
        "upload_time": "2023-03-30T07:35:07",
        "upload_time_iso_8601": "2023-03-30T07:35:07.216258Z",
        "url": "https://files.pythonhosted.org/packages/a5/27/bc445a81e545a95d51b2b5da1992b93b9563534d871d659b92f8e14a9a86/triton_model_navigator-0.4.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c09d84409549ba60adbfde4dfb618a2d82d4074c7ad04bfce3bef697ba002e57",
          "md5": "b950686ce15c7e873505bf3d49a681f3",
          "sha256": "5242d94c688152efccf465e2fd997f6fd4a124aea622dd4581d038646c9811f4"
        },
        "downloads": -1,
        "filename": "triton_model_navigator-0.4.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b950686ce15c7e873505bf3d49a681f3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.8",
        "size": 203457,
        "upload_time": "2023-03-30T07:31:24",
        "upload_time_iso_8601": "2023-03-30T07:31:24.856145Z",
        "url": "https://files.pythonhosted.org/packages/c0/9d/84409549ba60adbfde4dfb618a2d82d4074c7ad04bfce3bef697ba002e57/triton_model_navigator-0.4.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3858ed5b33fe674e0485c42c6fab244656161642c01dbd221ed9dc3328d452c6",
          "md5": "edb14808b57bfe3a3388e758b49e1abc",
          "sha256": "5e2e27b7d9410b8622b207447c314c7de5385945f48412768de4c8bd115e6837"
        },
        "downloads": -1,
        "filename": "triton_model_navigator-0.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "edb14808b57bfe3a3388e758b49e1abc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.8",
        "size": 212579,
        "upload_time": "2023-03-30T07:38:38",
        "upload_time_iso_8601": "2023-03-30T07:38:38.568425Z",
        "url": "https://files.pythonhosted.org/packages/38/58/ed5b33fe674e0485c42c6fab244656161642c01dbd221ed9dc3328d452c6/triton_model_navigator-0.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7b826e189443ca8cbb086455adbad37ef586cb2064f34a8d4e5c776db30baf8f",
          "md5": "b02636c51ea48c6c659de1580f59d77c",
          "sha256": "effea40448679b3078a500330a79f14e211af142a9f99d9ecd07eb3c23ca3683"
        },
        "downloads": -1,
        "filename": "triton_model_navigator-0.5.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b02636c51ea48c6c659de1580f59d77c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "<4,>=3.8",
        "size": 212734,
        "upload_time": "2023-03-30T07:49:19",
        "upload_time_iso_8601": "2023-03-30T07:49:19.060979Z",
        "url": "https://files.pythonhosted.org/packages/7b/82/6e189443ca8cbb086455adbad37ef586cb2064f34a8d4e5c776db30baf8f/triton_model_navigator-0.5.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7b826e189443ca8cbb086455adbad37ef586cb2064f34a8d4e5c776db30baf8f",
        "md5": "b02636c51ea48c6c659de1580f59d77c",
        "sha256": "effea40448679b3078a500330a79f14e211af142a9f99d9ecd07eb3c23ca3683"
      },
      "downloads": -1,
      "filename": "triton_model_navigator-0.5.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b02636c51ea48c6c659de1580f59d77c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "<4,>=3.8",
      "size": 212734,
      "upload_time": "2023-03-30T07:49:19",
      "upload_time_iso_8601": "2023-03-30T07:49:19.060979Z",
      "url": "https://files.pythonhosted.org/packages/7b/82/6e189443ca8cbb086455adbad37ef586cb2064f34a8d4e5c776db30baf8f/triton_model_navigator-0.5.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}