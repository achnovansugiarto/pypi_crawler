{
  "info": {
    "author": "duyvnc",
    "author_email": "duyvnc@fpt.com.vn",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Install package\n```bash\npip install spark-sdk --upgrade\n```\n\n# Get Spark\n```python\nimport spark_sdk as ss\nspark = ss.PySpark(yarn=False, num_executors=4, driver_memory='8G').spark\n\n# Yarn mode\nspark = ss.PySpark(yarn=True, driver_memory='2G', num_executors=4, executor_memory='4G').spark\n\n# Add more spark config\nspark = ss.PySpark(yarn=False, driver_memory='8G', num_executors=4, executor_memory='4G',\n                  add_on_config1=(\"spark.sql.catalog.spark_catalog\",\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"),\n                  add_on_config2=('spark.databricks.delta.retentionDurationCheck.enabled', 'false')\n                  ).spark\n```\n# Store data to dwh\n```python\n\n# step 1 read data\nELT_DATE = '2021-12-01'\nELT_STR = ELT_DATE[:7]\nimport pandas as pd\ndf1 = pd.read_csv('./data.csv', sep='\\t')\n\n\nimport spark_sdk as ss\n\n# function to store to dwh\ndf1.to_dwh(    \n    hdfs_path=\"path/to/data/test1.parquet/\", # path hdfs\n    partition_by=\"m\", # column time want to partition\n    partition_date=ELT_DATE, # partition date\n    database=\"database_name\", # database name\n    table_name=\"test1\", # table name\n    repartition=True\n)\n\n\n# function to read from dwh\n# method 1: using pandas\nimport spark_sdk as ss\nimport pandas as pd\n\npandas_df = pd.read_dwh('database.table_name', filters=\"\"\"m='2022-04-01'\"\"\")\n\n# method 2: sql\nsparkDF = ss.sql(\"\"\"\nSELECT *\nFROM database_name.table1\nWHERE m='2022-04-01'\n\"\"\")\n\ndf = sparkDF.toPandas()\n\n\n# method 3: read_table\nsparkDF = ss.read_table('database_name.table')\nsparkDF = sparkDF.filter(\"m == '2022-04-01'\")\n\ndf = sparkDF.toPandas()\n\n\n# IF got error timestamp out of range\nsparkDF = ss.limit_timestamp(sparkDF).toPandas()\n\n# IF YOU WANT TO DELETE DATA\n# Link hdfs_file to table\n# When to drop it also delete data\nss.drop_table_and_delete_data('database_name.test1')\n\n# IF JUST WANT TO DELETE TABLE NOT DELETE DATA\nss.drop_table('database_name.test1')\n\n```\n# Delta format\n# function to store to dwh\n```python\ndf1.to_dwh(\n    # just end path with delta then table will be store in delta format\n    hdfs_path=\"path/to/data/test1.delta/\", \n    partition_by=\"m\", # column time want to partition\n    partition_date=ELT_DATE, # partition date\n    database=\"database_name\", # database name\n    table_name=\"test1\",\n    repartition=True# table name\n)\n\n# read table not read file\nsparkDF = ss.sql(\"\"\"\nSELECT *\nFROM database_name.table1\nWHERE m='2022-04-01'\n\"\"\")\n\n```\n\n\n# Decrypt Data\n\n## method 1: sql\nUsing Spark SQL\n```python\nimport spark_sdk as ps\n\nkey = '1234' # contact Data Owner to get key\n\ndf = ps.sql(f\"\"\"\nselect fdecrypt(column_name, \"{key}\") column_name\nfrom database_name.table_name\nlimit 50000\n\"\"\").toPandas()\n```\n\n\n## method 2: using pandas\nUsing pandas\n```python\nimport spark_sdk as ss\ndf['new_column'] = df['column'].decrypt_column(key)\n\n# function will return dataframe with column_decrypted\n```\n\n# method 3\nUsing pandas apply function\n```python\nfrom spark_sdk import decrypt\ndf['decrypted'] = df['encrypted'].apply(decrypt,args=(\"YOUR_KEY\",))\n```\n\n# Encrypt data\n```python\nimport spark_sdk as ss\n\n\n# function to store to dwh\ndf.to_dwh(    \n    hdfs_path=\"path/to/data/test1.parquet/\", # path hdfs\n    partition_by=\"m\", # column time want to partition\n    partition_date=ELT_DATE, # partition date\n    database=\"database_name\", # database name\n    table_name=\"test1\", # table name\n    encrypt_columns=['column1','column2'], # list column name need to encrypt\n    keys_path = '/path/to/store/keys.json' # if you add encrypt_columns, you need to have keys_path to store keys\n)\n\nss.PySpark().stop()\n```\n\n# Create Yaml File Mapping data from CSV to DWH\n```python\nfrom spark_sdk import CreateYamlDWH\ncreate_yaml = CreateYamlDWH(\ncsv_file = 'data.csv',\nhdfs_path = '/path/to/data/table_name.parquet',\nsep = '\\t',\ndatabase_name = 'database_name',\ntable_name = 'table_name',\nyaml_path = '/path/to/yaml/file/'\n)\n\ncreate_yaml.generate_yaml_file()\n```\n\n# Store spark.sql.DataFrame\n```python\nfrom spark_sdk.src import spark_sdk as ss\nsparkDF = ss.sql(\"\"\"select * from database.table_name limit 1000\"\"\")\n\nELT_DATE = '2022-06-10'\nsparkDF.to_dwh(\n    hdfs_path=\"path/to/data/test6.parquet/\", # path hdfs\n    partition_by=\"m\", # column time want to partition\n    partition_date=ELT_DATE, # partition date\n    database=\"database_name\", # database name\n    table_name=\"test6\", # table name\n    repartition=True,\n    driver_memory='4G', executor_memory='4g', num_executors='1', port='4090', yarn=True\n)\n```\n\n# Working with hdfs\n```python\nmkdir, cat, exists, info, open\n\n# list file\nss.ls('/path/data')\n\n# create new path \nss.mkdir('/path/create/')\n\n# create new path \nss.exists('/check/path/exists') # return True if exists\n\n# print info\nss.info('/path/info/')\n\n# open file like normal file\nimport json\nwith ss.open('/path/to/file.json', mode='rb') as file:\n    data = json.load(file)\n\nimport json\nfrom spark_sdk import Utf8Encoder\ndata = {'user': 1, 'code': '456'}\nwith ss.open('/path/to/file.json', mode='wb') as file:\n    json.dump(data, Utf8Encoder(file), indent=4)\n\n\njson__file = ss.read_json('/path/to/file.json')\nss.write_json(data, '/path/to_file.json')\n\nsparkDF = ss.read_csv(\"file:///path/to/data.csv\", sep='\\t') # with local file\nsparkDF = ss.read_csv(\"/path/to/data.csv\", sep='\\t') # with hdfs file\n\n# mode in ['rb', 'wb', 'ab']\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "http://git.bigdata.local/data-engineers/sdk/utilities",
    "keywords": "",
    "license": "duyvnc",
    "maintainer": "",
    "maintainer_email": "",
    "name": "spark-sdk",
    "package_url": "https://pypi.org/project/spark-sdk/",
    "platform": null,
    "project_url": "https://pypi.org/project/spark-sdk/",
    "project_urls": {
      "Bug Tracker": "https://github.com/pypa/sampleproject/issues",
      "Homepage": "http://git.bigdata.local/data-engineers/sdk/utilities"
    },
    "release_url": "https://pypi.org/project/spark-sdk/0.4.19/",
    "requires_dist": [
      "pyspark (>=3.0.2)",
      "pyarrow (>=8.0.0)",
      "pycryptodome",
      "pandas (>=1.0.5)",
      "thrift",
      "hmsclient",
      "sqlglot",
      "findspark"
    ],
    "requires_python": ">=3.6",
    "summary": "Function to help Data Scientist work more effectively with DWH",
    "version": "0.4.19",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17491704,
  "releases": {
    "0.3.26": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3db5827ca106f825888de0277c75c98e37eba0cd12eb0dd6d157c0b36fc0870b",
          "md5": "65f7b5dbd049c8812d2d72fe09dd539c",
          "sha256": "e6f1eec0b2624172b5eead6b877b70e99764db65dd186d926ddaac277965afa6"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.3.26-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "65f7b5dbd049c8812d2d72fe09dd539c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 19475,
        "upload_time": "2022-07-19T07:59:00",
        "upload_time_iso_8601": "2022-07-19T07:59:00.179289Z",
        "url": "https://files.pythonhosted.org/packages/3d/b5/827ca106f825888de0277c75c98e37eba0cd12eb0dd6d157c0b36fc0870b/spark_sdk-0.3.26-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.27": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "849cd0f001bc748ecdee159c634c928039fd031057f59ad701e0190b29c0e496",
          "md5": "3c5a826a8c814bbb72875a23ca5914fd",
          "sha256": "340ee330356749963754ce0885d2cb06320463b1c952a3dccbb15afd1bf66f85"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.3.27-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3c5a826a8c814bbb72875a23ca5914fd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 19494,
        "upload_time": "2022-07-19T08:38:28",
        "upload_time_iso_8601": "2022-07-19T08:38:28.526991Z",
        "url": "https://files.pythonhosted.org/packages/84/9c/d0f001bc748ecdee159c634c928039fd031057f59ad701e0190b29c0e496/spark_sdk-0.3.27-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.28": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "be5a2e0f356d7ff206a1f91e57d9e828a5124bddc2592ef8a459086b9292c732",
          "md5": "ae99f66de5bde5e315d90e97476fd404",
          "sha256": "3676ee57bf34d51d0ad879dd1725adc442dc1102fc869c845eda9c7a96f5d53f"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.3.28-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ae99f66de5bde5e315d90e97476fd404",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 19491,
        "upload_time": "2022-07-22T06:52:13",
        "upload_time_iso_8601": "2022-07-22T06:52:13.733253Z",
        "url": "https://files.pythonhosted.org/packages/be/5a/2e0f356d7ff206a1f91e57d9e828a5124bddc2592ef8a459086b9292c732/spark_sdk-0.3.28-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.29": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63d3288b6a8c61af93b2b43c777d0c879b24d5ea080db1e89dc04d69625a5080",
          "md5": "a9ed8fcb251bb137ff22e3c70a5ba288",
          "sha256": "ee7fa9b0f15378871c0b8335e0224226ef64cce49837a139cb95fe348adb760b"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.3.29-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a9ed8fcb251bb137ff22e3c70a5ba288",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 19863,
        "upload_time": "2022-08-05T06:50:54",
        "upload_time_iso_8601": "2022-08-05T06:50:54.678597Z",
        "url": "https://files.pythonhosted.org/packages/63/d3/288b6a8c61af93b2b43c777d0c879b24d5ea080db1e89dc04d69625a5080/spark_sdk-0.3.29-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.35": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e7ce3ea49c788ca485a007782c49a8c0a0f4967617329760390d4cdd4aa3eda4",
          "md5": "4e26556b35f2ef725d238e8048e540ba",
          "sha256": "f45836d0b0f6a8c8484d6ca66167c7212a31939dac6559e279548cfd80f06f67"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.3.35-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4e26556b35f2ef725d238e8048e540ba",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 20117,
        "upload_time": "2022-08-18T16:03:57",
        "upload_time_iso_8601": "2022-08-18T16:03:57.371651Z",
        "url": "https://files.pythonhosted.org/packages/e7/ce/3ea49c788ca485a007782c49a8c0a0f4967617329760390d4cdd4aa3eda4/spark_sdk-0.3.35-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.38": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "04143bf2ffdd42c127c549cbea5dfa71de25fd4a671736e9a4104edf0da25739",
          "md5": "59f6b911a36708a7bff3aba160b54a3b",
          "sha256": "c2fcacaa77c84958eb8bb59ef95528c77903d161517e2c9b86051fe2d4d471ff"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.3.38-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "59f6b911a36708a7bff3aba160b54a3b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 20438,
        "upload_time": "2022-09-27T01:47:36",
        "upload_time_iso_8601": "2022-09-27T01:47:36.294021Z",
        "url": "https://files.pythonhosted.org/packages/04/14/3bf2ffdd42c127c549cbea5dfa71de25fd4a671736e9a4104edf0da25739/spark_sdk-0.3.38-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.10": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1f4b0ede9db643a2f0f718b0bb078b57baf1b9ee7f17f67ae10dbe1b5cd8d013",
          "md5": "9411c42660b20a1692f41492e7fb812b",
          "sha256": "ce6a3258fffa2f4a3e141759d6dbde1b5a77282791bd2f946d40b779623c8914"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.10-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9411c42660b20a1692f41492e7fb812b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32238,
        "upload_time": "2022-11-15T07:08:10",
        "upload_time_iso_8601": "2022-11-15T07:08:10.379754Z",
        "url": "https://files.pythonhosted.org/packages/1f/4b/0ede9db643a2f0f718b0bb078b57baf1b9ee7f17f67ae10dbe1b5cd8d013/spark_sdk-0.4.10-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1218c60c374442110d50720945f7711a9b05f399c63dab0069c6455a79caed98",
          "md5": "012e7d440e87b49012cb947e14f35849",
          "sha256": "f71832f887908685cdd2ad95bce739b89c4e8157ad713fbe3f7695a6ef60aad2"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.11-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "012e7d440e87b49012cb947e14f35849",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32254,
        "upload_time": "2022-11-24T02:11:53",
        "upload_time_iso_8601": "2022-11-24T02:11:53.938688Z",
        "url": "https://files.pythonhosted.org/packages/12/18/c60c374442110d50720945f7711a9b05f399c63dab0069c6455a79caed98/spark_sdk-0.4.11-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.12": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "327ee3a718f9d96ddbebdd7bcbdecd84abbba9869310df0391b232eee597545c",
          "md5": "2e09e8c8b1749b395b51bf8d613f4a3b",
          "sha256": "b02732bd6bba07b05c511fbfe971f359c739f93ca721603275f3691fc344624e"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.12-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2e09e8c8b1749b395b51bf8d613f4a3b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32273,
        "upload_time": "2022-12-01T07:37:11",
        "upload_time_iso_8601": "2022-12-01T07:37:11.484167Z",
        "url": "https://files.pythonhosted.org/packages/32/7e/e3a718f9d96ddbebdd7bcbdecd84abbba9869310df0391b232eee597545c/spark_sdk-0.4.12-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.13": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "724c62c5bfedd53bd92b5463e457943d077c61f4ab8713efec8f3d90d3e3c30b",
          "md5": "2c2962354199a88a14c96edfdbce29fd",
          "sha256": "c62c776bca2353d81b3ed05d2607bd5931bce79df44b42539d9761ab0a9289a5"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.13-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2c2962354199a88a14c96edfdbce29fd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32690,
        "upload_time": "2022-12-13T09:49:40",
        "upload_time_iso_8601": "2022-12-13T09:49:40.138429Z",
        "url": "https://files.pythonhosted.org/packages/72/4c/62c5bfedd53bd92b5463e457943d077c61f4ab8713efec8f3d90d3e3c30b/spark_sdk-0.4.13-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.14": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9ed11a27cb60ffd125c2ca145369acb0c503d45065af6361f5c21687c4fd0bfe",
          "md5": "6e58893c80896ca63e856e0031cb7276",
          "sha256": "78cc4128fd815876de860f90f399cc7952bbce4255c2eeb0daf4266fb8582380"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.14-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6e58893c80896ca63e856e0031cb7276",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32747,
        "upload_time": "2022-12-22T16:47:47",
        "upload_time_iso_8601": "2022-12-22T16:47:47.848019Z",
        "url": "https://files.pythonhosted.org/packages/9e/d1/1a27cb60ffd125c2ca145369acb0c503d45065af6361f5c21687c4fd0bfe/spark_sdk-0.4.14-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.15": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a9430abacbcff59762779e0640a96f7c0540f203076a6346fd06f391e2d5b5ea",
          "md5": "e704730e8d110b04c5ee62b8fc6266ba",
          "sha256": "5a319d0d4f30a944ae3ceb30e9d7bfb97f2d232508c8b8daf92390e9ea4ad839"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.15-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e704730e8d110b04c5ee62b8fc6266ba",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 33048,
        "upload_time": "2022-12-27T02:46:39",
        "upload_time_iso_8601": "2022-12-27T02:46:39.474253Z",
        "url": "https://files.pythonhosted.org/packages/a9/43/0abacbcff59762779e0640a96f7c0540f203076a6346fd06f391e2d5b5ea/spark_sdk-0.4.15-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.19": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4d85d414d9683366120c9551eb22c3687cb0b8688b18105ced5354cfb9ec082e",
          "md5": "ef42e44965269b50dc5c56cca57bea08",
          "sha256": "642af40c85b8190decb0bbdafe4235bd927e8d20e865cbd253a30a35b818b4bc"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.19-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ef42e44965269b50dc5c56cca57bea08",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 34290,
        "upload_time": "2023-02-15T01:23:55",
        "upload_time_iso_8601": "2023-02-15T01:23:55.777583Z",
        "url": "https://files.pythonhosted.org/packages/4d/85/d414d9683366120c9551eb22c3687cb0b8688b18105ced5354cfb9ec082e/spark_sdk-0.4.19-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.20rc7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f64e6d4d59576255158b6cfaf0e791d52f5f7020a99c54ebd0b8a798bd09cdc2",
          "md5": "1a3406a1d487358aa9739c47d2b3eaae",
          "sha256": "d197060776e4a0d1822b19e9194bdb5709ed3555f9d424268ab160dfc15bb3e3"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.20rc7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1a3406a1d487358aa9739c47d2b3eaae",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 56169,
        "upload_time": "2023-03-29T09:19:16",
        "upload_time_iso_8601": "2023-03-29T09:19:16.553774Z",
        "url": "https://files.pythonhosted.org/packages/f6/4e/6d4d59576255158b6cfaf0e791d52f5f7020a99c54ebd0b8a798bd09cdc2/spark_sdk-0.4.20rc7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.4.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "78efa94fc9e735951687c8ae5c09971b47a197f864afe848bd3c4ada1591b88a",
          "md5": "11706122d665351b49ffbe5c7ef72758",
          "sha256": "162531dcc53166395e655baaaa64beb5439b008d537513f809a073f2f6655f53"
        },
        "downloads": -1,
        "filename": "spark_sdk-0.4.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "11706122d665351b49ffbe5c7ef72758",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32225,
        "upload_time": "2022-11-01T06:43:26",
        "upload_time_iso_8601": "2022-11-01T06:43:26.386463Z",
        "url": "https://files.pythonhosted.org/packages/78/ef/a94fc9e735951687c8ae5c09971b47a197f864afe848bd3c4ada1591b88a/spark_sdk-0.4.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4d85d414d9683366120c9551eb22c3687cb0b8688b18105ced5354cfb9ec082e",
        "md5": "ef42e44965269b50dc5c56cca57bea08",
        "sha256": "642af40c85b8190decb0bbdafe4235bd927e8d20e865cbd253a30a35b818b4bc"
      },
      "downloads": -1,
      "filename": "spark_sdk-0.4.19-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ef42e44965269b50dc5c56cca57bea08",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 34290,
      "upload_time": "2023-02-15T01:23:55",
      "upload_time_iso_8601": "2023-02-15T01:23:55.777583Z",
      "url": "https://files.pythonhosted.org/packages/4d/85/d414d9683366120c9551eb22c3687cb0b8688b18105ced5354cfb9ec082e/spark_sdk-0.4.19-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}