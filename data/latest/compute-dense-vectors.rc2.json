{
  "info": {
    "author": "Guillaume Bernard",
    "author_email": "contact@guillaume-bernard.fr",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
      "Operating System :: POSIX :: Linux",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# Compute dense representation of texts\n\nThis software is used to compute dense vectorisations (sentence embeddings) of sequences of sentences of natural text. It is able to handle multilingual documents until the model used is a multilingual one. This relies on the S-BERT architecture, software and models (https://www.sbert.net/). It computes dense vector representations for tokens, lemmas, entities, etc. of your datasets.\n\nThe idea of computing dense representation of documents is inspired by some previous works:\n\n```text\nReimers, Nils, et Iryna Gurevych. 2019. ’Sentence-BERT: Sentence Embeddings \nUsing Siamese BERT-Networks’. In Proceedings of the 2019 Conference on \nEmpirical Methods in Natural Language Processing and the 9th International \nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), 3982‑92. \nHong Kong, China: Association for Computational Linguistics. \nhttps://doi.org/10.18653/v1/D19-1410.\n```\n\n```text\nLinger, Mathis, et Mhamed Hajaiej. 2020. ’Batch Clustering for Multilingual \nNews Streaming’. In Proceedings of Text2Story - Third Workshop on Narrative\nExtraction From Texts Co-Located with 42nd European Conference on \nInformation Retrieval, 2593:55‑61. CEUR Workshop Proceedings. Lisbon, \nPortugal. http://ceur-ws.org/Vol-2593/paper7.pdf.\n```\n\n```text\nStaykovski, Todor, Alberto Barron-Cedeno, Giovanni da San Martino, et Preslav \nNakov. 2019. ‘Dense vs. Sparse Representations for News Stream Clustering’. \nIn Proceedings of Text2Story - 2nd Workshop on Narrative Extraction From \nTexts, Co-Located with the 41st European Conference on Information, \n2342:47‑52. Cologne, Germany: CEUR-WS.org. \nhttps://ceur-ws.org/Vol-2342/paper6.pdf.\n```\n\n## Installation\n\n```bash\npip install compute_dense_vectors\n```\n\n## Pre-requisites\n\n### Dependencies\n\nThis project relies on two other packages: [`document_tracking_resources`](https://gitlab.univ-lr.fr/cross-lingual-event-tracking/developpement/from-documents-to-events/documents_tracking_resources). This code needs to have access to this packages. It relies on [`sentence-transformers`](https://www.sbert.net/) to compute dense representation of documents.\n\n## Transformers Models\n\nTo compute dense representation of documents, we use the [`sentence-transformers`](https://www.sbert.net/) package is used with two multilingual models: `paraphrase-multilingual-mpnet-base-v2` and `distiluse-base-multilingual-cased-v1`. According to [the documentation](https://www.sbert.net/docs/pretrained_models.html), and at the time of writing, they are the two models to give the best results in multilingual semantic similarity.\n\n## The corpus to process\n\nThe script can process two different types of Corpus from `document_tracking_resources`. The one for News (`NewsCorpusWithSparseFeatures`), the other one for Tweets (`TwitterCorpusWithSparseFeatures`). The datafiles should be loaded by `document_tracking_resources` in order to have this project to work.\n\nFor instance, below an example of a `TwitterCorpusWithSparseFeatures`:\n\n```text\n                                         date lang                                text               source  cluster\n1218234203361480704 2020-01-17 18:10:42+00:00  eng  Q: What is a novel #coronavirus...      Twitter Web App   100141\n1218234642186297346 2020-01-17 18:12:27+00:00  eng  Q: What is a novel #coronavirus...                IFTTT   100141\n1219635764536889344 2020-01-21 15:00:00+00:00  eng  A new type of #coronavirus     ...            TweetDeck   100141\n...                                       ...  ...                                 ...                  ...      ...\n1298960028897079297 2020-08-27 12:26:19+00:00  eng  So you come in here WITHOUT A M...   Twitter for iPhone   100338\n1310823421014573056 2020-09-29 06:07:12+00:00  eng  Vitamin and mineral supplements...            TweetDeck   100338\n1310862653749952512 2020-09-29 08:43:05+00:00  eng  FACT: Vitamin and mineral suppl...  Twitter for Android   100338\n```\n\nAnd an example of a `NewsCorpusWithSparseFeatures`:\n```text\n                              date lang                     title               text                     source  cluster\n24290965 2014-11-02 20:09:00+00:00  spa  Ponta gana la prim   ...  Las encuestas...                    Publico     1433\n24289622 2014-11-02 20:24:00+00:00  spa  La cantante Katie Mel...  La cantante b...          La Voz de Galicia      962\n24290606 2014-11-02 20:42:00+00:00  spa  Los sondeos dan ganad...  El Tribunal  ...                    RTVE.es     1433\n...                            ...  ...                       ...               ...                        ...      ...\n47374787 2015-08-27 12:32:00+00:00  deu  Microsoft-Betriebssys...  San Francisco...               Handelsblatt      170\n47375011 2015-08-27 12:44:00+00:00  deu  Microsoft-Betriebssy ...  San Francisco...               WiWo Gründer      170\n47394969 2015-08-27 20:35:00+00:00  deu  Windows 10: Mehr als ...  In zwei Tagn ...                  gamona.de      170\n```\n\n## Command line arguments\n\nOnce installed, the command `compute_dense_vectors` can be used directly, as registered in your PATH.\n\n\n```text\nusage: compute_dense_vectors [-h] --corpus CORPUS --dataset-type {twitter,news} [--model-name MODEL_NAME] --output-corpus OUTPUT_CORPUS\n\nTake a document corpus (in pickle format) and compute dense vectors for every feature\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --corpus CORPUS       Path to the pickle file containing the corpus to process.\n  --dataset-type {twitter,news}\n                        The kind of dataset to process. ‘twitter’ will use the ’TwitterCorpus’ class, the ‘Corpus’ class otherwise\n  --model-name MODEL_NAME\n                        The name of the model that can be used to encode sentences using the S-BERT library\n  --output-corpus OUTPUT_CORPUS\n                        Path where to export the new corpus with computed TF-IDF vectors.\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.univ-lr.fr/cross-lingual-event-tracking/datasets/dataset_manipulation_tools/compute_dense_vectors",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "compute-dense-vectors",
    "package_url": "https://pypi.org/project/compute-dense-vectors/",
    "platform": null,
    "project_url": "https://pypi.org/project/compute-dense-vectors/",
    "project_urls": {
      "Bug Tracker": "https://gitlab.univ-lr.fr/cross-lingual-event-tracking/datasets/dataset_manipulation_tools/compute_dense_vectors/-/issues",
      "Homepage": "https://gitlab.univ-lr.fr/cross-lingual-event-tracking/datasets/dataset_manipulation_tools/compute_dense_vectors"
    },
    "release_url": "https://pypi.org/project/compute-dense-vectors/1.0.3/",
    "requires_dist": [
      "document-tracking-resources (~=1.0.0)",
      "pandas (~=1.3.5)",
      "sentence-transformers (~=2.1.0)",
      "tqdm (~=4.62.3)"
    ],
    "requires_python": ">=3.9",
    "summary": "Utility to compute dense vector representation for dataset in the document_tracking_resources format base on dense transformers models.",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14946425,
  "releases": {
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "36881dadc43ff5689a02a58896cff66e9e30b5cc30fb79c695180c02df423c8d",
          "md5": "ea85942086b1f96b8db8639ed96a0f2c",
          "sha256": "85593718986ed5028811da88ee6408b9583fdb783f773a4bb8c463a69b38f13f"
        },
        "downloads": -1,
        "filename": "compute_dense_vectors-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ea85942086b1f96b8db8639ed96a0f2c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 18511,
        "upload_time": "2022-03-29T20:10:14",
        "upload_time_iso_8601": "2022-03-29T20:10:14.151865Z",
        "url": "https://files.pythonhosted.org/packages/36/88/1dadc43ff5689a02a58896cff66e9e30b5cc30fb79c695180c02df423c8d/compute_dense_vectors-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d718e93775492cfc233f061341d53823bd0efdfa8fc73d5b815878c1aa054f15",
          "md5": "419a0d423fd74d606bf96532687ee00e",
          "sha256": "5758f4589f41bb21b33f3db5d97ed9d325992cb3d20a006a09dac73890bae84c"
        },
        "downloads": -1,
        "filename": "compute_dense_vectors-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "419a0d423fd74d606bf96532687ee00e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 17868,
        "upload_time": "2022-03-29T20:10:16",
        "upload_time_iso_8601": "2022-03-29T20:10:16.276884Z",
        "url": "https://files.pythonhosted.org/packages/d7/18/e93775492cfc233f061341d53823bd0efdfa8fc73d5b815878c1aa054f15/compute_dense_vectors-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b18eaf675724c329be573d3d5abee8ce350a1ebe68713219e159ad48bf60e843",
          "md5": "188d41bc30c3bda50b55d7cac9d3dce0",
          "sha256": "dc44434d3f3f20d568bc10b1fe93740bb35d632737bac46cd2b04ff5bd48579c"
        },
        "downloads": -1,
        "filename": "compute_dense_vectors-1.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "188d41bc30c3bda50b55d7cac9d3dce0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 18903,
        "upload_time": "2022-08-31T08:28:43",
        "upload_time_iso_8601": "2022-08-31T08:28:43.899211Z",
        "url": "https://files.pythonhosted.org/packages/b1/8e/af675724c329be573d3d5abee8ce350a1ebe68713219e159ad48bf60e843/compute_dense_vectors-1.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b18eaf675724c329be573d3d5abee8ce350a1ebe68713219e159ad48bf60e843",
        "md5": "188d41bc30c3bda50b55d7cac9d3dce0",
        "sha256": "dc44434d3f3f20d568bc10b1fe93740bb35d632737bac46cd2b04ff5bd48579c"
      },
      "downloads": -1,
      "filename": "compute_dense_vectors-1.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "188d41bc30c3bda50b55d7cac9d3dce0",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.9",
      "size": 18903,
      "upload_time": "2022-08-31T08:28:43",
      "upload_time_iso_8601": "2022-08-31T08:28:43.899211Z",
      "url": "https://files.pythonhosted.org/packages/b1/8e/af675724c329be573d3d5abee8ce350a1ebe68713219e159ad48bf60e843/compute_dense_vectors-1.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}