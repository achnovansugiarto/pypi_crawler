{
  "info": {
    "author": "CSIRO",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "![](/assets/image.png)\n\n***Aims of this library:***\n\n - To be simple and easy to understand so that the focus is on the data science.\n - To reduce the time taken from implementation to results.\n - To promote rapid innovation of models via configuration files, class composition and/or class inheritance.\n - Reduce boilerplate code (sections of code that are repeated in multiple places with little to no variation).\n - To simplify cluster management and distributed computing with High Performance Computing (HPC).\n - Be able to easily accommodate multiple research avenues simultaneously.\n - To cooperatively improve the functionality and documentation of this repository to make it better!\n\n***Features:***\n - The [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) `LightningModule` and `Trainer` are used to implement, train, and test models. It allows for many of the above aims to be accomplished, such as simplified distributed computing and a reduction of boilerplate code. It also allows us to simply use class inheritance and composition, allowing for rapid innovation.\n - The [Compose API](https://hydra.cc/docs/advanced/compose_api/) of [Hydra](https://hydra.cc/) is used to create a hierarchical configuration, allowing for rapid innovation.\n - [Neptune.ai](https://neptune.ai/) is used to track experiments; metric scores are automatically uploaded to [Neptune.ai](https://neptune.ai/), allowing you to easily track your experiments from your browser.\n - Scripts for submission to a cluster manager, such as [SLURM](https://slurm.schedmd.com/documentation.html) are written for you. Also, cluster manager jobs are automatically resubmitted and resumed if they haven't finished before the time-limit.\n\n# Installation\n\nThe Deep Learning and HPC starter pack is available on PyPI:\n```shell\npip install dlhpcstarter\n```\n\n# Table of Contents\n\n[//]: # (- [How to structure your project]&#40;#how-to-structure-your-project&#41;)\n- [Package map](#package-map)\n- [Tasks](#tasks)\n- [Models](#models)\n- [Innovate via Model Composition and Inheritance](#innovate-via-model-composition-and-inheritance)\n- [Configuration YAML files and argparse](#configuration-yaml-files-and-argparse)\n- [Innovate via Configuration Files](#innovate-via-configuration-files)\n- [Next level: Configuration composition via Hydra](#next-level-configuration-composition-via-hydra)\n- [Stages and Trainer](#stages-and-trainer)\n- [Tying it all together: `main.py`](#tying-it-all-together-mainpy)\n- [Cluster manager and distributed computing](#cluster-manager-and-distributed-computing)\n- [Monitoring using Neptune.ai](#monitoring-using-neptuneai)\n- [Where all the outputs go: `exp_dir`](#where-all-the-outputs-go-exp_dir)\n- [Repository Wish List](#repository-wish-list)\n\n[//]: # (# How to structure your project)\n\n[//]: # ()\n[//]: # (---)\n\n[//]: # (There will be a `task` directory containing each of your tasks, e.g., `cifar10`. For each task, you will have a set of configurations and models, which are stored in the `config` and `models` directories, respectively. Each task will also have a `stages` module for each stage of model development.)\n\n[//]: # (```)\n\n[//]: # (├──  task  )\n\n[//]: # (│    │)\n\n[//]: # (│    └── TASK_NAME     - name of the task, e.g., cifar10.)\n\n[//]: # (│        └── config    - .yaml configuration files for a model.)\n\n[//]: # (│        └── models    - .py modules that contain pytorch_lightning.LightningModule definitions that represent models.)\n\n[//]: # (│        └── stages.py - training and testing stages for a task.)\n\n[//]: # (```)\n\n# Package map\n\n---\n\nThe package is structured as follows:\n\n```\n├──  dlhpcstarter\n│    │\n│    ├── tools                     - for all other modules; tools that are repeadetly used.\n│    ├──  __main__.py - __main__.py does the following:\n│    │               1. Reads command line arguments using argparse.\n│    │               2. Imports the 'stages' function for the task from task/TASK_NAME/stages.py.\n│    │               3. Loads the specified configuration .yaml for the job from task/TASK_NAME/config.\n│    │               4. Submits the job (the configuration + 'stages') to the cluster manager (or runs it locally if 'submit' is false).\n│    └── cluster.py                - contains the cluster management object.\n│    └── command_line_arguments.py - argparse for reading command line arguments.\n│    └── trainer.py                - contains a wrapper for pytorch_lightning.Trainer.\n│    └── utils.py                  - small utility definitions.\n\n```\n\n# Tasks\n\n---\n\n\n***Tasks are named based on the data and the type of prediction or inference being made***. For example:\n - Two tasks have the same data but require different names due to differing predictions, e.g., **MS-COCO Detection** and **MS-COCO Caption**.\n - Two tasks may have similar predictions but require different names due to differing data, e.g., **MNIST** and **Chinese MNIST**.\n\n***Some publicly available tasks include***:\n- Image classification tasks, e.g., [MNIST](http://yann.lecun.com/exdb/mnist/), [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html), [CIFAR100](https://www.cs.toronto.edu/~kriz/cifar.html), [ImageNet](https://www.image-net.org/). \n- Object detection tasks, e.g., [MS-COCO Detection](https://cocodataset.org/#detection-2020).\n- Image captioning detection tasks, e.g., [MS-COCO Caption](https://cocodataset.org/#captions-2015).\n- Speech recognition tasks, e.g., [LibriSpeech](https://www.openslr.org/12).\n- Chest X-Ray report generation, e.g., [MIMIC-CXR](https://physionet.org/content/mimic-cxr/2.0.0/).\n\n***How to add a task:***\n\nAdding a task is as simple as creating a directory with the name of the task in `task`. For example, if we choose CIFAR10 as the task, with the task name `cifar10`, then we would create the directory `task/cifar10`. The task directory will then house everything necessary for that task, for example, the models, the configurations for the models, the data pipeline, and the stages of development (training and testing).\n\n# Models\n\n---\n\n\n***Please familiarise yourself with the [`pytorch_lightning.LightningModule`](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#) in order to correctly implement a model:*** https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n\nOnce we have created our task directory (e.g., `task/cifar10`), we now want to create a model using a [`pytorch_lightning.LightningModule`](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#). Everything we need for the model can be placed in the `LightningModule`, in including commonly used libraries and objects, for example:\n\n - [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module): base class for all neural networks in `PyTorch`. \n - [transformers](https://huggingface.co/docs/transformers/index): a library containing pre-trained Transformer models.\n - [torchvision](https://pytorch.org/vision/stable/index.html): a library for image pre-processing and pre-trained computer vision models.\n - [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset): an object that processes each instance of a dataset.\n - [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader): an object that samples mini-batches from a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset).\n\n***Note:*** \n\n- The data pipeline could be implemented within the `LightningModule` or seperately from a model using a [pytorch_lightning.LightningDataModule](https://pytorch-lightning.readthedocs.io/en/stable/extensions/datamodules.html). The `LightningModule` instance would then have to be given separately to the `pytorch_lightning.Trainer`. \n\n***Example:***\n\n - An example model for `cifar10` is in [task/cifar10/model/baseline.py](https://github.com/csiro-mlai/dl_hpc_starter_pack/blob/main/task/cifar10/model/baseline.py). \n\n# Innovate via Model Composition and Inheritance\n\n---\nTo promote rapid innovation of models, we recommend using class composition and/or inheritance. ***For example, we may have a baseline that not only includes a basic model, but also the data pipeline:***\n\n```python\nfrom pytorch_lightning import LightningModule\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision\nimport torch\n\nclass Baseline(LightningModule):\n    def __init__(self, lr, ..., **kwargs):\n        super(Baseline, self).__init__()\n        self.save_hyperparameters()\n        self.lr = lr\n        self.model = torchvision.models.resnet18(...)\n\n    def setup(self, stage=None):\n        if stage == 'fit' or stage is None:\n            train_set = torchvision.datasets.CIFAR10(...)\n            self.train_set, self.val_set = random_split(train_set, [45000, 5000])\n\n        if stage == 'test' or stage is None:\n            self.test_set = torchvision.datasets.CIFAR10(...)\n\n    def train_dataloader(self, shuffle=True):\n        return DataLoader(self.train_set, ...)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_set, ...)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, ...)\n\n    def configure_optimizers(self):     \n        optimiser = {'optimizer': torch.optim.SGD(self.parameters(), lr=self.lr, momentum=0.9)}\n        return optimiser\n\n    def forward(self, images):\n        return self.model(images)\n\n    def training_step(self, batch, batch_idx):\n        images, labels = batch\n        y_hat = self(images)\n        loss = self.loss(y_hat, labels)\n        self.log_dict({'train_loss': loss}, ...)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, labels = batch\n        y_hat = self(images)\n        loss = self.loss(y_hat, labels)\n        self.val_accuracy(torch.argmax(y_hat['logits'], dim=1), labels)\n        self.log_dict({'val_acc': self.val_accuracy, 'val_loss': loss}, ...)\n\n    def test_step(self, batch, batch_idx):\n        images, labels = batch\n        y_hat = self(images)\n        self.test_accuracy(torch.argmax(y_hat['logits'], dim=1), labels)\n        self.log_dict({'test_acc': self.test_accuracy}, ...)\n```\n\nAfter training and testing the baseline, we may want to improve upon its performance. For example, if we wanted to make the following modifications:\n \n - Use a DenseNet instead of a ResNet.\n - Use the `AdamW` optimiser.\n - Use a warmup learning rate scheduler. \n\n***All we would need to do is inherit the baseline and make our modifications:***\n\n```python\nfrom transformers import get_constant_schedule_with_warmup\n\nclass Inheritance(Baseline):\n\n    def __init__(self, num_warmup_steps, **kwargs):\n        super(Inheritance, self).__init__(**kwargs)\n        self.save_hyperparameters()\n        self.num_warmup_steps = num_warmup_steps\n        self.model = torchvision.models.densenet121(...)\n\n    def configure_optimizers(self):\n        optimiser = {'optimizer': torch.optim.AdamW(self.parameters(), lr=self.lr)}\n        optimiser['scheduler'] = {\n                'scheduler': get_constant_schedule_with_warmup(optimiser['optimizer'], self.num_warmup_steps),\n                'interval': 'step',\n                'frequency': 1,\n            }\n        return optimiser\n```\nWe could also construct a model that is the combination of the two via composition. For example, we may want to use everything from `Baseline`, but the optimiser from `Inheritance`:\n\n```python\nfrom pytorch_lightning import LightningModule\n\nclass Composite(LightningModule):\n    def __init__(self, **kwargs):\n        self.baseline = Baseline(self, **kwargs)\n\n    def setup(self, stage=None):\n        self.baseline.setup(stage)\n\n    def train_dataloader(self, shuffle=True):\n        return self.baseline.train_dataloader(shuffle)\n\n    def val_dataloader(self):\n        return self.baseline.val_dataloader()\n\n    def test_dataloader(self):\n        return self.baseline.test_dataloader()\n\n    def configure_optimizers(self):     \n        return Inheritance.configure_optimizers(self)  # Use configure_optimizers() from Inheritance.\n\n    def forward(self, images):\n        return self.baseline.forward(images)\n\n    def training_step(self, batch, batch_idx):\n        return self.baseline.training_step(batch, batch_idx)\n\n    def validation_step(self, batch, batch_idx):\n        return self.baseline.validation_step(batch, batch_idx)\n\n    def test_step(self, batch, batch_idx):\n        return self.baseline.test_step(batch, batch_idx)\n```\n\n# Configuration YAML files and argparse\n\n---\n\n\nCurrently, there are two methods for giving arguments:\n\n1. **Via command line arguments using the [`argparse` module](https://docs.python.org/3/library/argparse.html)**. `argparse` mainly handles paths, development stage flags (e.g., training and testing flags), and cluster manager arguments.\n2. **Via a configuration file stored in [`YAML` format](https://www.cloudbees.com/blog/yaml-tutorial-everything-you-need-get-started)**. Can handle all the arguments defined by the `argparse` plus more, including hyperparameters for the model.\n\n***The mandatory arguments include:***\n1. `task`, the name of the task.\n2. `config`, relative or absolute path to the configuration file (can handle with or without extension).\n3. `module`, the module that the model definition is housed.\n4. `definition`, the class representing the model.\n5. `exp_dir`, the experiment directory, i.e., where all outputs, including model checkpoints will be saved.\n6. `monitor`, metric to monitor for [ModelCheckpoint](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.ModelCheckpoint.html) and [EarlyStopping](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.callbacks.EarlyStopping.html?highlight=earlystoppin#earlystopping) (optional), as well as test checkpoint loading (e.g., 'val_loss').\n7. `monitor_mode`, whether the monitored metric is to be maximised or minimised ('max' or 'min').\n\n***`task` and `config` must be given as command line arguments for `argparse`:***\n\n```shell\ndlhpcstarter --config task/cifar10/config/baseline --task cifar10\n```\n\n***`module`, `definition`, and `exp_dir` can be given either as command line arguments, or be placed in the configuration file.***\n\nFor each model of a task, we define a configuration. Hyperparameters, paths, as well as the device configuration can be stored in a configuration file. Configurations are in [`YAML` format](https://www.cloudbees.com/blog/yaml-tutorial-everything-you-need-get-started), e.g., `task/cifar10/config/baseline.yaml`.\n\n# Innovate via Configuration Files\n\n---\n\nIf we have the following configuration file for the aforementioned CIFAR10  `Baseline` model, `task/cifar10/config/baseline.yaml`:\n\n```yaml\ntrain: True\ntest: True\nmodule: task.cifar10.model.baseline\ndefinition: Baseline\nmonitor: 'val_acc'\nmonitor_mode: 'max'\nlr: 1e-3\nmax_epochs: 32\nmbatch_size: 32\nnum_workers: 5\nexp_dir: /my/experiment/directory\ndataset_dir: /my/datasets/directory\n```\n\nAnother way we can improve upon the baseline model, i.e., the baseline configuration, is by modifying its hyperparameters. For example, we can still use `Baseline`, but alter the learning rate in `task/cifar10/config/baseline_rev_a.yaml`:\n\n```yaml\ntrain: True\ntest: True\nmodule: task.cifar10.model.baseline\ndefinition: Baseline\nmonitor: 'val_acc'\nmonitor_mode: 'max'\nlr: 1e-4  # modify this.\nmax_epochs: 32\nmbatch_size: 32\nnum_workers: 5\nexp_dir: /my/experiment/directory\ndataset_dir: /my/datasets/directory\n```\n\n```shell\ndlhpcstarter --config task/cifar10/config/baseline_rev_a --task cifar10\n```\n\n# Next level: Configuration composition via Hydra\n\n---\n\nIf your new configuration only modifies a few arguments of another configuration file, you can take advantage of the composition feature of [Hydra](https://hydra.cc/). This makes creating `task/cifar10/config/baseline_rev_a.yaml` from the previous section easy. We simply add the arguments from `task/cifar10/config/baseline.yaml` by adding its name to the `defaults` list:\n\n```yaml\ndefaults:\n  - baseline\n  - _self_\n\nlr: 1e-4\n```\n***Note that other configuration files are imported with reference to the current configuration path (not the working directory).***\n\n\nPlease note that groups are not being used, and packages should be placed using `@_global_` if the configurations being used for composition are not in the same directory. ***For example, the following would not work with this repository as the arguments in `hpc_paths` will be grouped under `paths`:***\n\n```yaml\ndefaults:\n  - paths/hpc_paths\n  - _self_\n\ntrain: True\ntest: True\nresumable: True\nmodule: task.cifar10.model.baseline\ndefinition: Baseline\nmonitor: 'val_acc'\nmonitor_mode: 'max'\nlr: 1e-3\nmax_epochs: 3\nmbatch_size: 32\nnum_workers: 5\n```\n\nTo get around this, simply place `@_global_` to remove the grouping:\n\n```yaml\ndefaults:\n  - paths/hpc_paths@_global_  # changed here to remove \"paths\" grouping.\n  - _self_\n\ntrain: True\ntest: True\nresumable: True\nmodule: task.cifar10.model.baseline\ndefinition: Baseline\nmonitor: 'val_acc'\nmonitor_mode: 'max'\nlr: 1e-3\nmax_epochs: 3\nmbatch_size: 32\nnum_workers: 5\n```\n\nThis also allows us to organise configurations easily. For example, if we have the following directory structure:\n```\n├── task\n│   └──  cifar10          \n│        └── config  \n│            ├── cluster\n│            │    ├── 2hr.yaml\n│            │    └── 24hr.yaml\n│            │\n│            ├── distributed\n│            │    ├── 1gpu.yaml\n│            │    ├── 4gpu.yaml\n│            │    └── 4gpu4node.yaml\n│            │\n│            ├── paths\n│            │    ├── local.yaml\n│            │    └── hpc.yaml\n│            │\n│            └── baseline.yaml\n```\nWith `task/cifar10/config/baseline.yaml` as:\n```yaml\ndefaults:\n  - cluster/2hr@_global_\n  - distributed/4gpu@_global_\n  - paths/hpc_paths@_global_\n  - _self_\n\ntrain: True\ntest: True\nresumable: True\nmodule: task.cifar10.model.baseline\ndefinition: Baseline\nmonitor: 'val_acc'\nmonitor_mode: 'max'\nlr: 1e-3\nmax_epochs: 3\nmbatch_size: 32\nnum_workers: 5\n```\n\nWhere `task/cifar10/config/baseline.yaml` will now include arguments from the following example sub-configurations:\n\n- `task/cifar10/config/cluster/2hr.yaml`:\n   ```yaml\n   memory: 32GB\n   time_limit: '02:00:00'\n   venv_path: /path/to/my/venv/bin/activate\n   ```\n - `task/cifar10/config/distributed/4gpu.yaml`:\n   ```yaml\n   num_gpus: 2\n   strategy: ddp\n   ```\n - `task/cifar10/config/paths/hpc.yaml`:\n   ```yaml\n   exp_dir: /path/to/my/experiments\n   dataset_dir: /path/to/my/dataset\n   ```\n\nSee the following documentation for more information:\n - https://hydra.cc/docs/1.2/tutorials/basic/your_first_app/defaults/\n - https://hydra.cc/docs/1.2/advanced/defaults_list/#composition-order\n - https://hydra.cc/docs/1.2/advanced/overriding_packages/\n\n# Stages and Trainer\n\n---\n\n\nIn each task directory is a Python module called `stages.py`, which contains the `stages` definition. This definition takes an object as input that houses the configuration for a job.  \n\nTypically, the following things happen in `stages()`:\n\n - The `LightningModule` model is imported via the `model` argument, e.g.,\n    ```python\n    from src import importer\n   \n    Model = importer(definition=args.definition, module=args.module)\n    model = Model(**vars(args))\n   ```\n    See `src.utils.importer` for a handy function that imports based on strings.\n - A `pytorch_lightning.Trainer` instance is created, e.g., `trainer = pytorch_lightning.Trainer(...)`.\n - The model is trained using trainer: `trainer.fit(model)`.\n - The model is tested using trainer: `trainer.test(model)`.\n\nIt handles the training and testing of a model for a task by using a [`pytorch_lightning.Trainer`](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html).\n\n***A helpful wrapper at `src/trainer.py` exists that passes frequently used and useful `callbacks`, `loggers`, and `plugins` to a `pytorch_lightning.Trainer` instance:***\n\n```python\nfrom src.dlhpcstarter.trainer import trainer_instance\n\ntrainer = trainer_instance(**vars(args))\n```\nPlace any of the parameters for the trainer detailed at \nhttps://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api in your configuration file, and they will be passed to the `pytorch_lightning.Trainer` instance.\n\n# Tying it all together: `dlhpcstarter`\n\n---\n\n***This is an overview of what occurs when the entrypoint `dlhpcstarter` is executed, this is not necessary to understand to use the package.***\n\n\n\n`dlhpcstarter` does the following:\n\n - Gets the command line arguments using `argparse`, e.g., arguments like this:\n    ```shell\n    dlhpcstarter --config task.cifar10.config.baseline --task cifar10\n    ```\n - Imports the `stages` definition for the task using `src.utils.importer`.\n - Reads the configuration `.yaml` and combines it with the command line arguments.\n - Submits `stages` to the cluster manager if `args.submit = True` or runs `stages` locally. The command line arguments and the configuration arguments are passed to `stages` in both cases.\n\n# Cluster manager and distributed computing\n\n---\n\nThe following arguments are used for distributed computing:\n\n| Argument      | Description                                                 | Default |\n|---------------|-------------------------------------------------------------|---------|\n| `num_workers` | No. of workers per DataLoader & GPU.                        | `1`     |\n| `num_gpus`    | Number of GPUs per node.                                    | `None`  |\n| `num_nodes`   | Number of nodes (should only be used with `submit = True`). | `1`     |\n\nThe following arguments are used to configure a job for a cluster manager (the default cluster manager is SLURM):\n\n| Argument     | Description                                                    | Default      |\n|--------------|----------------------------------------------------------------|--------------|\n| `memory`     | Amount of memory per node.                                     | `'16GB'`     |\n| `time_limit` | Job time limit.                                                | `'02:00:00'` |\n| `submit`     | Submit job to the cluster manager.                             | `None`       |\n| `resumable`  | Resumable training; Automatic resubmission to cluster manager. | `None`       |\n| `qos`        | Quality of service.                                            | `None`       |\n| `begin`      | When to begin the Slurm job, e.g. `now+1hour`.                 | `None`       |\n| `email`      | Email for cluster manager notifications.                       | `None`       |\n| `venv_path`  | Path to ''bin/activate'' of a venv.                            | `None`       |\n\n***These can be given as command line arguments:***\n\n ```shell\ndlhpcstarter --config task/cifar10/config/baseline --task cifar10 --submit 1 --num-gpus 4 --num-workers 5 --memory 32GB\n ```\n\n***Or they can be placed in the configuration `.yaml` file:***\n\n```yaml\nnum_gpus: 4  # Added.\nnum_workers: 5  # Added.\nmemory: '32GB'  # Added.\n\ntrain: True\ntest: True\nmodule: task.cifar10.model.baseline\ndefinition: Baseline\nmonitor: 'val_acc'\nmonitor_mode: 'max'\nlr: 1e-3\nmax_epochs: 32\nmbatch_size: 32\nnum_workers: 5\nexp_dir: /my/experiment/directory\ndataset_dir: /my/datasets/directory\n```\nAnd executed with:\n```shell\ndlhpcstarter --config task/cifar10/config/baseline --task cifar10 --submit True\n ```\n\nIf using a cluster manager, add the path to the `bin/activate` of your virtual environment:\n```yaml\n...\nvenv_path: /my/env/name/bin/activate\n...\n```\n\n# Monitoring using Neptune.ai\n\nSimply sign up at https://neptune.ai/ and add your username and API token to your configuration file:\n\n```yaml\n...\nneptune_username: my_username\nneptune_api_key: df987y94y2q9hoiusadhc9wy9tr82uq408rjw98ch987qwhtr093q4jfi9uwehc987wqhc9qw4uf9w3q4h897324th\n...\n```\nThe [PyTorch Lightning Trainer](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html) will then automatically upload metrics using the [Neptune Logger](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loggers.neptune.html) to [Neptune.ai](https://neptune.ai/). Once logged in to https://neptune.ai/, you will be able to monitor your task. See here for information about using the online UI: https://docs.neptune.ai/you-should-know/displaying-metadata.\n\n# Where all the outputs go: `exp_dir`\n\n---\n\nThe experiments directory is where all your outputs will be saved, including model checkpoints, metric scores. This is also where the cluster manager script, as well as where stderr and stdout are saved.\n\nNote: the trial number also sets the seed number for your experiment.\n\n***Description to be finished.\n\n\n# Repository Wish List\n\n---\n - Transfer cluster management over to submitit: https://ai.facebook.com/blog/open-sourcing-submitit-a-lightweight-tool-for-slurm-cluster-computation/\n - Add description about how to use https://neptune.ai/.\n - Use https://hydra.cc/ instead of argparse (or have the option to use either).\n - https://docs.ray.io/en/latest/tune/index.html for hyperparameter optimisation.\n - Notebook examples.\n\n",
    "description_content_type": "text/markdown; charset=UTF-8",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/csiro-mlai/dl_hpc_starter_pack",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dlhpcstarter",
    "package_url": "https://pypi.org/project/dlhpcstarter/",
    "platform": null,
    "project_url": "https://pypi.org/project/dlhpcstarter/",
    "project_urls": {
      "Homepage": "https://github.com/csiro-mlai/dl_hpc_starter_pack"
    },
    "release_url": "https://pypi.org/project/dlhpcstarter/0.0.8/",
    "requires_dist": [
      "Bottleneck (>=1.3)",
      "GPUtil (>=1.4)",
      "hydra-core (>=1.2)",
      "neptune-client (>=0.16)",
      "numpy (>=1.21)",
      "pytorch-lightning (>=1.8.6)",
      "rich (>=12.5)",
      "scipy (>=1.7)",
      "torchmetrics (>=0.9)",
      "torchvision (>=0.11.1)",
      "pytest ; extra == 'test'"
    ],
    "requires_python": ">=3.7",
    "summary": "Deep Learning and HPC Starter Pack",
    "version": "0.0.8",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17295187,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9f0c19e8b25f86d21d7a73037519afd478b3d996755e3830b13133ba23b565cd",
          "md5": "2dddbca085b085b54b764d25437f7fb4",
          "sha256": "7e2cffada2d3ea13cc9ad5d07e22eaee55e48e12e967afe416752639d9129fd4"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2dddbca085b085b54b764d25437f7fb4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 36391,
        "upload_time": "2022-09-08T09:59:34",
        "upload_time_iso_8601": "2022-09-08T09:59:34.180038Z",
        "url": "https://files.pythonhosted.org/packages/9f/0c/19e8b25f86d21d7a73037519afd478b3d996755e3830b13133ba23b565cd/dlhpcstarter-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "57fd1540003a371dd896e8226de1278754b2041f975c54b4bcc6616787988578",
          "md5": "3e677db26cfe01e67f3c0e99b9e3836a",
          "sha256": "01092a57065fb7749e27899d99f8195b021e28fffef70749b196c5b28e7272a7"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3e677db26cfe01e67f3c0e99b9e3836a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 36328,
        "upload_time": "2022-09-12T00:37:53",
        "upload_time_iso_8601": "2022-09-12T00:37:53.999489Z",
        "url": "https://files.pythonhosted.org/packages/57/fd/1540003a371dd896e8226de1278754b2041f975c54b4bcc6616787988578/dlhpcstarter-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "21c8836acd7a1efd44552368d7e942f5d72c66dffcc4ca2d945fc98d8fbd5728",
          "md5": "bcabb090dcd2442ce0e8d90ede814a07",
          "sha256": "687b6bd61fff6a17f71a8967d816f2896bd1949821ab0c74f65fd1a6c2ceef21"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "bcabb090dcd2442ce0e8d90ede814a07",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 36375,
        "upload_time": "2022-09-27T01:56:15",
        "upload_time_iso_8601": "2022-09-27T01:56:15.511480Z",
        "url": "https://files.pythonhosted.org/packages/21/c8/836acd7a1efd44552368d7e942f5d72c66dffcc4ca2d945fc98d8fbd5728/dlhpcstarter-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9981f7d212271ef3de9ecaf6207011f34f63921e8150ad2c080d2e5c27ef5037",
          "md5": "d0c4088e07e3308fc7b2f8b1b8819c00",
          "sha256": "7909c4da99de2f7f7de3bb1628e9217bad17c62982165230dcf0abdde2621a15"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "d0c4088e07e3308fc7b2f8b1b8819c00",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 35503,
        "upload_time": "2022-11-21T01:29:33",
        "upload_time_iso_8601": "2022-11-21T01:29:33.245337Z",
        "url": "https://files.pythonhosted.org/packages/99/81/f7d212271ef3de9ecaf6207011f34f63921e8150ad2c080d2e5c27ef5037/dlhpcstarter-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "35ab1dd4df813e0f72051ced4503460a2143b191d055169daab7aa7893726151",
          "md5": "31360af4110acaeddbb33cf64853b875",
          "sha256": "6df7e139d7b1dfebfa50d08b5b7e80746c9ee2c92594044968c7b68d29b3b5b8"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "31360af4110acaeddbb33cf64853b875",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 30696,
        "upload_time": "2023-01-13T05:24:41",
        "upload_time_iso_8601": "2023-01-13T05:24:41.029626Z",
        "url": "https://files.pythonhosted.org/packages/35/ab/1dd4df813e0f72051ced4503460a2143b191d055169daab7aa7893726151/dlhpcstarter-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "930a295dd9b8cef9c6ebf2d365ed59128124f5e54faca1ca50ea874a76ae92c9",
          "md5": "aa9935ed7d116be3e2100fa2eae901f7",
          "sha256": "84eaada96e8c90f3bbd286b117b6bc029795edbef80a0c2bfdb7e4714492e90d"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "aa9935ed7d116be3e2100fa2eae901f7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 36138,
        "upload_time": "2023-01-13T05:24:43",
        "upload_time_iso_8601": "2023-01-13T05:24:43.851481Z",
        "url": "https://files.pythonhosted.org/packages/93/0a/295dd9b8cef9c6ebf2d365ed59128124f5e54faca1ca50ea874a76ae92c9/dlhpcstarter-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8f9df2e80884656c6f1a9a696013d7c92571fdd7c9718380383be118bae0b7c2",
          "md5": "3265f7845e0b30cef16095b026cb0388",
          "sha256": "2df9e2aae4fff7b21c5df061ca34ed5c42e650a885e449f71c59c48d96c956fe"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3265f7845e0b30cef16095b026cb0388",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 30725,
        "upload_time": "2023-01-18T00:30:02",
        "upload_time_iso_8601": "2023-01-18T00:30:02.783708Z",
        "url": "https://files.pythonhosted.org/packages/8f/9d/f2e80884656c6f1a9a696013d7c92571fdd7c9718380383be118bae0b7c2/dlhpcstarter-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "735f44e265459d0f4ffa3a48b4741866ad3a789cdbad59f3e301d7125f235189",
          "md5": "92091ead2738c43d1f84bad20ce05358",
          "sha256": "33a1ea07b2c5971bb35ed3f07c4ff26bf542cabc12a40bbfcf1aac1377310df6"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "92091ead2738c43d1f84bad20ce05358",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 36105,
        "upload_time": "2023-01-18T00:30:05",
        "upload_time_iso_8601": "2023-01-18T00:30:05.828718Z",
        "url": "https://files.pythonhosted.org/packages/73/5f/44e265459d0f4ffa3a48b4741866ad3a789cdbad59f3e301d7125f235189/dlhpcstarter-0.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "da02ca109f0dece5ff29186c8264e23b1ecfa0e8bd7aea47d1d2f0b59bff59a0",
          "md5": "aa2e80b298d5c76419262f58b2840649",
          "sha256": "c303dd182ddd7511f14b94627a79e9a03f9281d07f6e367bc1fa6df8e64450b4"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "aa2e80b298d5c76419262f58b2840649",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 30765,
        "upload_time": "2023-02-12T10:23:48",
        "upload_time_iso_8601": "2023-02-12T10:23:48.487471Z",
        "url": "https://files.pythonhosted.org/packages/da/02/ca109f0dece5ff29186c8264e23b1ecfa0e8bd7aea47d1d2f0b59bff59a0/dlhpcstarter-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6213d8f65f3241e410cf5225357e5eec71aed2b1174f1d638554337f150e87dd",
          "md5": "5a4abc8bd7c57ad1e8c97c8196256c3b",
          "sha256": "07e1650f441117f6d8c6170a685c1152900f5cae539821f204040e21f334e8c2"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "5a4abc8bd7c57ad1e8c97c8196256c3b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 36168,
        "upload_time": "2023-02-12T10:23:52",
        "upload_time_iso_8601": "2023-02-12T10:23:52.607460Z",
        "url": "https://files.pythonhosted.org/packages/62/13/d8f65f3241e410cf5225357e5eec71aed2b1174f1d638554337f150e87dd/dlhpcstarter-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "47ae635d8fb0fc35989e27f7f93732043ce19946e439a2767b2bbbcf129f9a84",
          "md5": "93f792eb992e55ab6e3b78d725dcfdb1",
          "sha256": "5a1def3dc93fa51d67e3da075a21a110069c1807dcfe65528a464db8f29646b1"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "93f792eb992e55ab6e3b78d725dcfdb1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 30795,
        "upload_time": "2023-02-17T00:58:32",
        "upload_time_iso_8601": "2023-02-17T00:58:32.818916Z",
        "url": "https://files.pythonhosted.org/packages/47/ae/635d8fb0fc35989e27f7f93732043ce19946e439a2767b2bbbcf129f9a84/dlhpcstarter-0.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7487888f157cc06c3f3bee690d52d6962967cb1022572b4b543c832fc9b84096",
          "md5": "7de8dc2aa99afabe98f4f282459d941c",
          "sha256": "f06708045cb31a6f56cf581dd56aed01a7ac8fff7f25c6657222606a61b4f46b"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "7de8dc2aa99afabe98f4f282459d941c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 36213,
        "upload_time": "2023-02-17T00:58:38",
        "upload_time_iso_8601": "2023-02-17T00:58:38.858148Z",
        "url": "https://files.pythonhosted.org/packages/74/87/888f157cc06c3f3bee690d52d6962967cb1022572b4b543c832fc9b84096/dlhpcstarter-0.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "95b52ebd2d90f6d5f165839d115a1936d2d841202b11613ae70c2da90347cbc5",
          "md5": "d890e6e98e6ed9937398ad809949d10b",
          "sha256": "6cf153d41130957a30dcd720da7d5d2bebd1129f0c273f54626a41605ccd9c66"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d890e6e98e6ed9937398ad809949d10b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 30886,
        "upload_time": "2023-03-14T22:50:04",
        "upload_time_iso_8601": "2023-03-14T22:50:04.480427Z",
        "url": "https://files.pythonhosted.org/packages/95/b5/2ebd2d90f6d5f165839d115a1936d2d841202b11613ae70c2da90347cbc5/dlhpcstarter-0.0.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dcd5eae29122a23493961d13b2e8101732b5a49f9daf0f3eaad84616a8ae423c",
          "md5": "2068111b29dd4fb0fbe204cfa9e04575",
          "sha256": "d24f077e34c2c0d33a3e11e437afd3cc0dacccf5bc3967b190c80a660872ebbc"
        },
        "downloads": -1,
        "filename": "dlhpcstarter-0.0.8.tar.gz",
        "has_sig": false,
        "md5_digest": "2068111b29dd4fb0fbe204cfa9e04575",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 36420,
        "upload_time": "2023-03-14T22:50:12",
        "upload_time_iso_8601": "2023-03-14T22:50:12.575665Z",
        "url": "https://files.pythonhosted.org/packages/dc/d5/eae29122a23493961d13b2e8101732b5a49f9daf0f3eaad84616a8ae423c/dlhpcstarter-0.0.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "95b52ebd2d90f6d5f165839d115a1936d2d841202b11613ae70c2da90347cbc5",
        "md5": "d890e6e98e6ed9937398ad809949d10b",
        "sha256": "6cf153d41130957a30dcd720da7d5d2bebd1129f0c273f54626a41605ccd9c66"
      },
      "downloads": -1,
      "filename": "dlhpcstarter-0.0.8-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d890e6e98e6ed9937398ad809949d10b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 30886,
      "upload_time": "2023-03-14T22:50:04",
      "upload_time_iso_8601": "2023-03-14T22:50:04.480427Z",
      "url": "https://files.pythonhosted.org/packages/95/b5/2ebd2d90f6d5f165839d115a1936d2d841202b11613ae70c2da90347cbc5/dlhpcstarter-0.0.8-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "dcd5eae29122a23493961d13b2e8101732b5a49f9daf0f3eaad84616a8ae423c",
        "md5": "2068111b29dd4fb0fbe204cfa9e04575",
        "sha256": "d24f077e34c2c0d33a3e11e437afd3cc0dacccf5bc3967b190c80a660872ebbc"
      },
      "downloads": -1,
      "filename": "dlhpcstarter-0.0.8.tar.gz",
      "has_sig": false,
      "md5_digest": "2068111b29dd4fb0fbe204cfa9e04575",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 36420,
      "upload_time": "2023-03-14T22:50:12",
      "upload_time_iso_8601": "2023-03-14T22:50:12.575665Z",
      "url": "https://files.pythonhosted.org/packages/dc/d5/eae29122a23493961d13b2e8101732b5a49f9daf0f3eaad84616a8ae423c/dlhpcstarter-0.0.8.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}