{
  "info": {
    "author": "Randy Chng",
    "author_email": "chngyuanlong@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "# Contents\n1. [Name of Candidate](#name-of-candidate)\n2. [Overview of folder structure](#overview-of-folder-structure)\n3. [Running instructions](#running-instructions)\n4. [Description of logical steps/ flow of pipeline](#description-of-logical-steps-flow-of-pipeline)\n5. [Overview of Key findings in EDA and Pipeline, Feature Engineering Choices](#overview-of-key-findings-in-eda-and-pipeline-feature-engineering-choices)\n6. [Model choices](#model-choices)\n7. [Evaluation choices](#evaluation-choices)\n8. [Other Considerations](#other-considerations)\n9. [Parting Words](#parting-words)\n------------------------------\n## Name of Candidate\n------------------------------\n[Back to content page](#contents)\n\nHi! My name is :\n\n>Chng Yuan Long, Randy\n\nEmail:\n\n>chngyuanlong@gmail.com\n\n------------------------------\n##  Overview of folder structure\n------------------------------\n\n[Back to content page](#contents)\n\n### Folder structure:\n```\nAIAP\n│  README.md\n│  requirements.txt    \n│  test-requirements.txt\n|  run.sh\n|  Dockerfile\n|  eda.ipynb\n|  tox.ini\n|\n└──data\n|  survive.db\n|  sample_df.csv\n|\n└──src\n   |  main.py\n   │\n   └──config\n   |     config.py\n   |   \n   └──preprocessing\n   |     datamanager.py\n   |\n   └──tests\n   |     test-datamanager.py\n   |     test-predict.py\n   |     test-train_pipeline.py\n   |     test-pipeline.py\n   |     test_bound_outliers.py\n   |     test_load_from_database.py\n   |     test_pipeline.py\n   |     test_predict.py\n   |     test_preprocess_data.py\n   |     test_preprocess_input.py\n   |\n   └──model\n         pipeline.pkl\n         pipeline.py\n         predict.py\n         train_pipeline.py\n```\n### File Summary:\n> Format: File (folder)\n> - Usage\n\nmain.py (src)\n- runs application\n\nConfig.py (src/config)\n- Tweak variables in config/config files\n   - File paths\n   - model specific objects (CV, test ratio, random seed, params for cross validation)\n   - Column names\n   - Data related like Column names, default values on streamlit UI\n\nDatamanager.py (src/Preprocessing)\n- loads pipeline, data\n- preprocesses input from application, data from database after reading\n\npython files (src/tests)\n- tests functions in the respective python files\n\nTrain-Pipeline.py (src/model)\n- trains and scores the pipeline with the data in data folder\n- outputs pipeline.pkl and a log on training outcome in the same folder\n\nPipeline.py (src/model)\n- contains pipeline to transform data\n\npredict.py (src/model)\n- predicts inputs using pipeline trained on data in data folder\n\n------------------------------\n## Running instructions\n------------------------------\n\n[Back to content page](#contents)\n\nYou can run the application straight with either the bash script or from docker.\nOptionally, you may run tests or train the pipeline on data in the data folder or run lint tools on the code with Tox.\nA trained pipeline named pipeline.pkl should already be included in the src/model folder.\n\nDefault values are present on the application itself so that you can click on predict button at the end. If prediction is 0, message 'Please see a doctor!' will appear, otherwise it will appear as 'Please keep up the healthy habits'. Along with the message the predict class and the probability will appear as well. \n\nThe instructions below assumes a Windows OS with Python version 3.10.0\n\n### Tox\n\nI have 3 environments in tox (train_pipeline, pytest, lint) with each for a specific function.\nYou may run tox command like so in the root directory to run all 3 back to back\n\n> tox\n\nOr you can run a specific environment like so\n\n> tox -e pytest\n\n\n### Running Main Application\n\n1. Bash Script: \n\nRun bash script (run.sh) by double clicking it. Streamlit application should appear in your browser. \n\n2. Docker:\n\nPlease pull image by running command in terminal with docker running\n\n   >docker pull hashketh/aiap\n\nOnce retrieved, please run command \n\n   >docker run -p 8501:8501 hashketh/aiap\n\nThe streamlit should be available in your browser via \n\n   >localhost:8501\n\n------------------------------\n## Description of logical steps flow of pipeline\n------------------------------\n\n[Back to content page](#contents)\n\n### Test\n\nI imagine the user would like to test the application first to make sure that everything is working. After that they might want to train the model on the data, or they may wish to use linting tools to assist in cleaning or spotting issues with the code. They may do all using the package tox. \n\nFor the testing, I tried to test as much of the functions in each python file as I can. I used a sample of the database to replicate the loading and preprocessing of the pipeline. This sample is saved in the data file under sample_df.csv. All of the test files are included in the test folder.\n\n### Configuration\n\nThis deployment is done in streamlit and all of the variables are stored in config.py in the config folder save for the default values. This goes with all of the other variables like pathing on so on. If they have any configuration to be done they can tweak them in the config.py file.\n\n### Training Data\n\n> Train -> Ingest Data -> Preprocessing -> Train Pipeline -> Score -> Output Results\n\nSay they train the pipeline, the pipeline.py will call on config.py for value of variables, pipeline.py for the loading of pipeline, datamanager.py for loading and preprocessing of the data. \n\nThe preprocessing phase will include all of the transformation that was done from the eda jupyter notebook. This includes imputation of missing values, bounding of the outliers, replacement of the invalid values from smoke, ejection fraction and other features. It will also add the BMI feature.\n\nThe pipeline.py will train the pipeline on the data, score it and generate a txt file for the user to view the results. The resulting pipeline will be saved as a pickle file. User can either run the train_pipeline.py file directly or call it from tox.\n\n### Run application\n\n> Run application -> load pipeline -> consume inputs -> preprocess inputs -> predict -> display results\n\nAfter that they can run the application. The main.py contains the Streamlit UI for it and its filled with the default values provided by config.py. If the user clicks on the predict button, main.py will call predict.py which in turn will call on pipeline.py to load the pipeline and datamanager.py to preprocess the input. Predict.py will generate both the prediction and the probability of the outcome. This will be displayed on the page. \n\n------------------------------\n## Overview of Key findings in EDA and Pipeline Feature Engineering Choices\n------------------------------\n\n[Back to content page](#contents)\n\nThe dataset contains a moderate amount of features with 150K observations. Numerical features are typically tail heavy with some features require cleaning or imputing. Likewise the categorical features require some cleaning as well. All numerical features do not correlate with each other. \n\nThe pipeline included median imputation of possible null values , bounding outliers within the distribution and the usual scaling or numerical features and one-hot encoding of categorical features. \n\nAs I think that domain knowledge is useful in feature engineering and I do not have any medical knowledge, the only feature introduced is BMI which revealed to be a rather terrible feature. Through feature importance of both the random forest classifier and light Gradient boosting machine, I discovered that 5 features have a higher weight in determining the outcome. They are CK, Smoke, Gender, Diabetes and Age. \n\n------------------------------\n## Model choices\n------------------------------\n\n[Back to content page](#contents)\n\nI used the following models\n- Logistic Regression (LOGREG)\n- Support Vector Machines (SVM)\n- K-Nearest Neighbours (KNN)\n- Random Forest (RF)\n- Light Gradient Boosting Machine (LGBM)\n\nThe models used to train were chosen based on how complex the models are, whether they are ensemble models or not and where it is instance or model based. I originally intended to compare the models on the validation data and then choose 1 to perform hyperparamter tunning to achieve better results. However the models happen to give me good results with the default values that I dont need to tune hyperparameters. \n\nAnother selection criterion is also whether if there is any indication of overfitting on the data. Based on the training and test cross validation scores provided, I can see if a model is prone to overfit or not. If there is overfitting I can regularise the model or choose a less complex model. If there is underfitting I will choose a more complex model\n\nOn the second iteration, I chose to focus on 5 features with the highest weightage but I am unable to achieve the same score. Although it was a very comparable performance I think in terms of the severity of a false negative in context of the problem I am still comfortable with a perfect score with more features. Furthermore, training time is negligible at this point. Either one of the ensemble model would be fine but I settled on the random forest.\n\nLOGREG is the simplest of them all being a linear model. A simple model has its usefulness however it is unable to fit well onto the data using the default values.\n\nSVM is a very flexible model that allows me to reach both linear or polynominal solutions with its kernel methods and its hyper-parameters. But it requires a bit of knowledge in order to tune the hyperparameters properly.\n\nKNN is an instance based model that does not have any algorithim but predicts based on distance of the training data to the new instances for predictions.\n\nRF is an ensemble model of decision trees but it is prone to overfitting. Typically I will fit using default and then prune (regularise) the tree later. I like that the interpretation of the Tree is easy to understand.\n\nLGBM is an ensemble model that improves on every iteration by adjusting to the residual error of the previous iteration. My understanding is that the LGBM is a variant of XGBoost that is faster. XGBoost is itself a more regularised variant of Gradient Boosting machine.   \n\nI intitally chose to use LGBM as it provided the highest score on all metrics with accuracy taking precedence. It is also the fastest to train. However when I was building the application I have some issues running the LGBM model so I used random forest instead as it is the runner up with the same scoring on all metrics just a tad bit slower when training.\n\n------------------------------\n## Evaluation choices\n------------------------------\n\n[Back to content page](#contents)\n\nAs this is a classification problem, scores like recall, precision, accuracy, F1 score and the ROC AUC score are relevant. I got most of the metrics through sklearn's classification report.\n\nI think its important to know before hand which metric should take priority before the problem is modelled. The problem is about predicting the surival of a patient suffering from heart artery disease and I think between choosing a low false negative rate or a low false positive rate, a low false negative rate will take priority since the outcome of a false positive (predicted death when it is survive) is less disastrous than a false negative (predicted survive when it is death). The model should have high recall\n\nBeyond that, accuracy measures the true positive and true negative rates and is great to know the absolute performance of the model. F1 is good to see as a weighted measure of both recall and precision. F1 is a harmonic mean that is just another measurement of the mean similar to arithmetic mean or the geometric mean (where harmonic mean ≤ geometric mean ≤ arithmetic mean). F1 is penalised by low value and F1 will be high only when all components have high values. \n\nROC AUC tells us how much is the model able to distinguish between the positive and the negative class, with 0.5 being an equal chance of the model to label the positive as negative and vice versa. \n\nAll scores are bounded between 0 and 1 inclusive with higher values being better.\n\n------------------------------\n## Other Considerations\n------------------------------\n\n[Back to content page](#contents)\n\nThis deployment is built with ease of use and maintenance in mind. \n\nA couple of design choices are made to this end:\nTox allows me to run a couple of virtual environments and commands in a easy manner. With Tox, I can use pytest, run lint packages on the code and train the model on the training data with one command regardless of what virtual environment the user is in.\n\nPytest allows me and any other users to ensure that the code is working properly. I have written pre-train and post-train test cases so that I can cover both the data and the functions in the model and the expected behavior of the model.\n\nLint tools like black, isort and flake8 formats and flags out inconsistencies with the code, the docstrings and the imports in accordance with PEP8. I hope this improves readability and ease of use for other people using the application.\n\nThe model is also containerised in docker so we can avoid the \"it only runs on my machine\" problem. This is done in the event that the bash script fails to run the application for some reason.\n\n------------------------------\n## Parting Words\n------------------------------\n\n[Back to content page](#contents)\n\nThank you for reading all the way to the end of the README! I hope that everything is according to your expectations.\n\nI had fun  practising what I have learnt especially the software engineering aspects of it. Many tutorials or courses on data science stops after you score the model! Thank you for allowing me to participate!\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ChngYuanLongRandy/AIAP",
    "keywords": "",
    "license": "BSD-3",
    "maintainer": "",
    "maintainer_email": "",
    "name": "aiap-model",
    "package_url": "https://pypi.org/project/aiap-model/",
    "platform": "",
    "project_url": "https://pypi.org/project/aiap-model/",
    "project_urls": {
      "Homepage": "https://github.com/ChngYuanLongRandy/AIAP"
    },
    "release_url": "https://pypi.org/project/aiap-model/0.0.0/",
    "requires_dist": [
      "numpy (==1.22.0)",
      "pandas (==1.3.5)",
      "sklearn (==0.0)",
      "tox (==3.24.5)",
      "virtualenv (==20.13.0)",
      "streamlit (==1.3.1)"
    ],
    "requires_python": "==3.10.0",
    "summary": "Example classification model package from AIAP assessment.",
    "version": "0.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12905972,
  "releases": {
    "0.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cedb3f4625bf0b063d7b80cd4a0140f8cffa24f092aac8e32a5f8c2570afeb78",
          "md5": "7f869f492add738afcc5ef493c364985",
          "sha256": "b868efb624fc0684ecb996e62cebf96a013ca4f4ed8b729b68e829dc91e304e5"
        },
        "downloads": -1,
        "filename": "aiap_model-0.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7f869f492add738afcc5ef493c364985",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": "==3.10.0",
        "size": 17460,
        "upload_time": "2022-02-16T06:59:47",
        "upload_time_iso_8601": "2022-02-16T06:59:47.850191Z",
        "url": "https://files.pythonhosted.org/packages/ce/db/3f4625bf0b063d7b80cd4a0140f8cffa24f092aac8e32a5f8c2570afeb78/aiap_model-0.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0823a8ed56134a712680da718707be779b65726b05dd43c822dc218b8008b10b",
          "md5": "db72957d9279a140d03f9e24ebacf3d2",
          "sha256": "a214dfcf180c916e15b842850bda217682c426031d9d0eaa53ae753f63870c87"
        },
        "downloads": -1,
        "filename": "aiap-model-0.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "db72957d9279a140d03f9e24ebacf3d2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": "==3.10.0",
        "size": 15667,
        "upload_time": "2022-02-16T06:59:49",
        "upload_time_iso_8601": "2022-02-16T06:59:49.290683Z",
        "url": "https://files.pythonhosted.org/packages/08/23/a8ed56134a712680da718707be779b65726b05dd43c822dc218b8008b10b/aiap-model-0.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cedb3f4625bf0b063d7b80cd4a0140f8cffa24f092aac8e32a5f8c2570afeb78",
        "md5": "7f869f492add738afcc5ef493c364985",
        "sha256": "b868efb624fc0684ecb996e62cebf96a013ca4f4ed8b729b68e829dc91e304e5"
      },
      "downloads": -1,
      "filename": "aiap_model-0.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "7f869f492add738afcc5ef493c364985",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": "==3.10.0",
      "size": 17460,
      "upload_time": "2022-02-16T06:59:47",
      "upload_time_iso_8601": "2022-02-16T06:59:47.850191Z",
      "url": "https://files.pythonhosted.org/packages/ce/db/3f4625bf0b063d7b80cd4a0140f8cffa24f092aac8e32a5f8c2570afeb78/aiap_model-0.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0823a8ed56134a712680da718707be779b65726b05dd43c822dc218b8008b10b",
        "md5": "db72957d9279a140d03f9e24ebacf3d2",
        "sha256": "a214dfcf180c916e15b842850bda217682c426031d9d0eaa53ae753f63870c87"
      },
      "downloads": -1,
      "filename": "aiap-model-0.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "db72957d9279a140d03f9e24ebacf3d2",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": "==3.10.0",
      "size": 15667,
      "upload_time": "2022-02-16T06:59:49",
      "upload_time_iso_8601": "2022-02-16T06:59:49.290683Z",
      "url": "https://files.pythonhosted.org/packages/08/23/a8ed56134a712680da718707be779b65726b05dd43c822dc218b8008b10b/aiap-model-0.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}