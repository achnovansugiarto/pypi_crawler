{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Education",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: Apache Software License",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "# OpenCLIP\n\n[[Paper]](https://arxiv.org/abs/2212.07143) [[Clip Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb) [[Coca Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)\n[![pypi](https://img.shields.io/pypi/v/open_clip_torch.svg)](https://pypi.python.org/pypi/open_clip_torch)\n\nWelcome to an open source implementation of OpenAI's [CLIP](https://arxiv.org/abs/2103.00020) (Contrastive Language-Image Pre-training).\n\nThe goal of this repository is to enable training models with contrastive image-text supervision, and to investigate their properties such as robustness to distribution shift. Our starting point is an implementation of CLIP that matches the accuracy of the original CLIP models when trained on the same dataset.\nSpecifically, a ResNet-50 model trained with our codebase on OpenAI's [15 million image subset of YFCC](https://github.com/openai/CLIP/blob/main/data/yfcc100m.md) achieves **32.7%** top-1 accuracy on ImageNet. OpenAI's CLIP model reaches **31.3%** when trained on the same subset of YFCC. For ease of experimentation, we also provide code for training on the 3 million images in the [Conceptual Captions](https://ai.google.com/research/ConceptualCaptions/download) dataset, where a ResNet-50x4 trained with our codebase reaches 22.2% top-1 ImageNet accuracy.\n\nWe further this with a replication study on a dataset of comparable size to OpenAI's, [LAION-400M](https://arxiv.org/abs/2111.02114), and with the larger [LAION-2B](https://laion.ai/blog/laion-5b/) superset. In addition, we study scaling behavior in a paper on [reproducible scaling laws for contrastive language-image learning](https://arxiv.org/abs/2212.07143).\n\nWe have trained the following ViT CLIP models:\n  * ViT-B/32 on LAION-400M with a accuracy of **62.9%**, comparable to OpenAI's **63.2%**, zero-shot top-1 on ImageNet-1k\n  * ViT-B/32 on LAION-2B with a accuracy of **66.6%**.\n  * ViT-B/16 on LAION-400M achieving an accuracy of **67.1%**, lower than OpenAI's **68.3%** (as measured here, 68.6% in paper)\n  * ViT-B/16+ 240x240 (~50% more FLOPS than B/16 224x224) on LAION-400M achieving an accuracy of **69.2%**\n  * ViT-B/16 on LAION-2B with a accuracy of **70.2%**.\n  * ViT-L/14 on LAION-400M with an accuracy of **72.77%**, vs OpenAI's **75.5%** (as measured here, 75.3% in paper)\n  * ViT-L/14 on LAION-2B with an accuracy of **75.3%**, vs OpenAI's **75.5%** (as measured here, 75.3% in paper)\n  * CoCa ViT-L/14 on LAION-2B with an accuracy of **75.5%** (currently only 13B samples seen) vs. CLIP ViT-L/14 73.1% (on the same dataset and samples seen)\n  * ViT-H/14 on LAION-2B with an accuracy of **78.0**. The second best in1k zero-shot for released, open-source weights thus far.\n  * ViT-g/14 on LAION-2B with an accuracy of **76.6**. This was trained on reduced 12B samples seen schedule, same samples seen as 400M models.\n  * ViT-g/14 on LAION-2B with an accuracy of **78.5**. Full 34B samples seen schedule.\n  * ViT-G/14 on LAION-2B with an accuracy of **80.1**. The best in1k zero-shot for released, open-source weights thus far.\n\nAnd the following ConvNeXt CLIP models:\n  * ConvNext-Base @ 224x224 on LAION-400M with an ImageNet-1k zero-shot top-1 of **66.3%**\n  * ConvNext-Base (W) @ 256x256 on LAION-2B with an ImageNet-1k zero-shot top-1 of **70.8%**\n  * ConvNext-Base (W) @ 256x256 /w augreg (extra augmentation + regularization) on LAION-2B with a top-1 of **71.5%**\n  * ConvNext-Base (W) @ 256x256 on LAION-A (900M sample aesthetic subset of 2B) with a top-1 of **71.0%**\n  * ConvNext-Base (W) @ 320x320 on LAION-A with a top-1 of **71.7%** (eval at 384x384 is **71.0**)\n  * ConvNext-Base (W) @ 320x320 /w augreg on LAION-A with a top-1 of **71.3%** (eval at 384x384 is **72.2%**)\n  * ConvNext-Large (D) @ 256x256 /w augreg on LAION-2B with a top-1 of **75.9%**\n  * ConvNext-Large (D) @ 320x320 fine-tune of 256x256 weights above for ~2.5B more samples on LAION-2B, top-1 of **76.6%**\n  * ConvNext-Large (D) @ 320x320 soup of 3 fine-tunes of 256x256 weights above on LAION-2B, top-1 of **76.9%**\n  * ConvNext-XXLarge @ 256x256 original run **79.1%** \n  * ConvNext-XXLarge @ 256x256 rewind of last 10% **79.3%**\n  * ConvNext-XXLarge @ 256x256 soup of original + rewind **79.4%**\n\nModel cards w/ additional model specific details can be found on the Hugging Face Hub under the OpenCLIP library tag: https://huggingface.co/models?library=open_clip\n\nAs we describe in more detail [below](#why-are-low-accuracy-clip-models-interesting), CLIP models in a medium accuracy regime already allow us to draw conclusions about the robustness of larger CLIP models since the models follow [reliable scaling laws](https://arxiv.org/abs/2107.04649).\n\nThis codebase is work in progress, and we invite all to contribute in making it more accessible and useful. In the future, we plan to add support for TPU training and release larger models. We hope this codebase facilitates and promotes further research in contrastive image-text learning. Please submit an issue or send an email if you have any other requests or suggestions.\n\nNote that portions of `src/open_clip/` modelling and tokenizer code are adaptations of OpenAI's official [repository](https://github.com/openai/CLIP).\n\n## Approach\n\n| ![CLIP](https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png) |\n|:--:|\n| Image Credit: https://github.com/openai/CLIP |\n\n## Usage\n\n```\npip install open_clip_torch\n```\n\n```python\nimport torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\ntokenizer = open_clip.get_tokenizer('ViT-B-32-quickgelu')\n\nimage = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0)\ntext = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]\n```\nSee also this [[Clip Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb)\n\nTo compute billions of embeddings efficiently, you can use [clip-retrieval](https://github.com/rom1504/clip-retrieval) which has openclip support.\n\n## Fine-tuning on classification tasks\n\nThis repository is focused on training CLIP models. To fine-tune a *trained* zero-shot model on a downstream classification task such as ImageNet, please see [our other repository: WiSE-FT](https://github.com/mlfoundations/wise-ft). The [WiSE-FT repository](https://github.com/mlfoundations/wise-ft) contains code for our paper on [Robust Fine-tuning of Zero-shot Models](https://arxiv.org/abs/2109.01903), in which we introduce a technique for fine-tuning zero-shot models while preserving robustness under distribution shift.\n\n## Data\n\nTo download datasets as webdataset, we recommend [img2dataset](https://github.com/rom1504/img2dataset)\n\n### Conceptual Captions\n\nSee [cc3m img2dataset example](https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md)\n\n### YFCC and other datasets\n\nIn addition to specifying the training data via CSV files as mentioned above, our codebase also supports [webdataset](https://github.com/webdataset/webdataset), which is recommended for larger scale datasets. The expected format is a series of `.tar` files. Each of these `.tar` files should contain two files for each training example, one for the image and one for the corresponding text. Both files should have the same name but different extensions. For instance, `shard_001.tar` could contain files such as `abc.jpg` and `abc.txt`. You can learn more about `webdataset` at [https://github.com/webdataset/webdataset](https://github.com/webdataset/webdataset). We use `.tar` files with 1,000 data points each, which we create using [tarp](https://github.com/webdataset/tarp).\n\nYou can download the YFCC dataset from [Multimedia Commons](http://mmcommons.org/).\nSimilar to OpenAI, we used a subset of YFCC to reach the aforementioned accuracy numbers.\nThe indices of images in this subset are in [OpenAI's CLIP repository](https://github.com/openai/CLIP/blob/main/data/yfcc100m.md).\n\n\n## Training CLIP\n\n### Install\n\nWe advise you first create a virtual environment with:\n\n```\npython3 -m venv .env\nsource .env/bin/activate\npip install -U pip\n```\n\nYou can then install openclip for training with `pip install 'open_clip_torch[training]'`.\n\n#### Development\n\nIf you want to make changes to contribute code, you can close openclip then run `make install` in openclip folder (after creating a virtualenv)\n\nInstall pip PyTorch as per https://pytorch.org/get-started/locally/\n\nYou may run `make install-training` to install training deps\n\n#### Testing\n\nTest can be run with `make install-test` then `make test`\n\n`python -m pytest -x -s -v tests -k \"training\"` to run a specific test\n\nRunning regression tests against a specific git revision or tag:\n1. Generate testing data\n    ```sh\n    python tests/util_test.py --model RN50 RN101 --save_model_list models.txt --git_revision 9d31b2ec4df6d8228f370ff20c8267ec6ba39383\n    ```\n    **_WARNING_: This will invoke git and modify your working tree, but will reset it to the current state after data has been generated! \\\n    Don't modify your working tree while test data is being generated this way.**\n\n2. Run regression tests\n    ```sh\n    OPEN_CLIP_TEST_REG_MODELS=models.txt python -m pytest -x -s -v -m regression_test\n    ```\n\n### Sample single-process running code:\n\n```bash\npython -m training.main \\\n    --save-frequency 1 \\\n    --zeroshot-frequency 1 \\\n    --report-to tensorboard \\\n    --train-data=\"/path/to/train_data.csv\"  \\\n    --val-data=\"/path/to/validation_data.csv\"  \\\n    --csv-img-key filepath \\\n    --csv-caption-key title \\\n    --imagenet-val=/path/to/imagenet/root/val/ \\\n    --warmup 10000 \\\n    --batch-size=128 \\\n    --lr=1e-3 \\\n    --wd=0.1 \\\n    --epochs=30 \\\n    --workers=8 \\\n    --model RN50\n```\n\nNote: `imagenet-val` is the path to the *validation* set of ImageNet for zero-shot evaluation, not the training set!\nYou can remove this argument if you do not want to perform zero-shot evaluation on ImageNet throughout training. Note that the `val` folder should contain subfolders. If it doest not, please use [this script](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh).\n\n### Multi-GPU and Beyond\n\nThis code has been battle tested up to 1024 A100s and offers a variety of solutions\nfor distributed training. We include native support for SLURM clusters.\n\nAs the number of devices used to train increases, so does the space complexity of\nthe the logit matrix. Using a na誰ve all-gather scheme, space complexity will be\n`O(n^2)`. Instead, complexity may become effectively linear if the flags\n`--gather-with-grad` and `--local-loss` are used. This alteration results in one-to-one\nnumerical results as the na誰ve method.\n\n#### Epochs\n\nFor larger datasets (eg Laion2B), we recommend setting --train-num-samples to a lower value than the full epoch, for example `--train-num-samples 135646078` to 1/16 of an epoch in conjunction with --dataset-resampled to do sampling with replacement. This allows having frequent checkpoints to evaluate more often.\n\n#### Patch Dropout\n\n<a href=\"https://arxiv.org/abs/2212.00794\">Recent research</a> has shown that one can dropout half to three-quarters of the visual tokens, leading to up to 2-3x training speeds without loss of accuracy.\n\nYou can set this on your visual transformer config with the key `patch_dropout`.\n\nIn the paper, they also finetuned without the patch dropout at the end. You can do this with the command-line argument `--force-patch-dropout 0.`\n\n#### Multiple data sources\n\nOpenCLIP supports using multiple data sources, by separating different data paths with `::`.\nFor instance, to train on CC12M and on LAION, one might use `--train-data '/data/cc12m/cc12m-train-{0000..2175}.tar'::/data/LAION-400M/{00000..41455}.tar\"`.\nUsing `--dataset-resampled` is recommended for these cases.\n\nBy default, on expectation the amount of times the model will see a sample from each source is proportional to the size of the source.\nFor instance, when training on one data source with size 400M and one with size 10M, samples from the first source are 40x more likely to be seen in expectation.\n\nWe also support different weighting of the data sources, by using the `--train-data-upsampling-factors` flag.\nFor instance, using `--train-data-upsampling-factors=1::1` in the above scenario is equivalent to not using the flag, and `--train-data-upsampling-factors=1::2` is equivalent to upsampling the second data source twice.\nIf you want to sample from data sources with the same frequency, the upsampling factors should be inversely proportional to the sizes of the data sources.\nFor instance, if dataset `A` has 1000 samples and dataset `B` has 100 samples, you can use `--train-data-upsampling-factors=0.001::0.01` (or analogously, `--train-data-upsampling-factors=1::10`).\n\n#### Single-Node\n\nWe make use of `torchrun` to launch distributed jobs. The following launches a\na job on a node of 4 GPUs:\n\n```bash\ncd open_clip/src\ntorchrun --nproc_per_node 4 -m training.main \\\n    --train-data '/data/cc12m/cc12m-train-{0000..2175}.tar' \\\n    --train-num-samples 10968539 \\\n    --dataset-type webdataset \\\n    --batch-size 320 \\\n    --precision amp \\\n    --workers 4 \\\n    --imagenet-val /data/imagenet/validation/\n```\n\n#### Multi-Node\n\nThe same script above works, so long as users include information about the number\nof nodes and host node.\n\n```bash\ncd open_clip/src\ntorchrun --nproc_per_node=4 \\\n    --rdzv_endpoint=$HOSTE_NODE_ADDR \\\n    -m training.main \\\n    --train-data '/data/cc12m/cc12m-train-{0000..2175}.tar' \\\n    --train-num-samples 10968539 \\\n    --dataset-type webdataset \\\n    --batch-size 320 \\\n    --precision amp \\\n    --workers 4 \\\n    --imagenet-val /data/imagenet/validation/\n```\n\n#### SLURM\n\nThis is likely the easiest solution to utilize. The following script was used to\ntrain our largest models:\n\n```bash\n#!/bin/bash -x\n#SBATCH --nodes=32\n#SBATCH --gres=gpu:4\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=6\n#SBATCH --wait-all-nodes=1\n#SBATCH --job-name=open_clip\n#SBATCH --account=ACCOUNT_NAME\n#SBATCH --partition PARTITION_NAME\n\neval \"$(/path/to/conda/bin/conda shell.bash hook)\" # init conda\nconda activate open_clip\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nexport MASTER_PORT=12802\n\nmaster_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\nexport MASTER_ADDR=$master_addr\n\ncd /shared/open_clip\nexport PYTHONPATH=\"$PYTHONPATH:$PWD/src\"\nsrun --cpu_bind=v --accel-bind=gn python -u src/training/main.py \\\n    --save-frequency 1 \\\n    --report-to tensorboard \\\n    --train-data=\"/data/LAION-400M/{00000..41455}.tar\" \\\n    --warmup 2000 \\\n    --batch-size=256 \\\n    --epochs=32 \\\n    --workers=8 \\\n    --model ViT-B-32 \\\n    --name \"ViT-B-32-Vanilla\" \\\n    --seed 0 \\\n    --local-loss \\\n    --gather-with-grad\n```\n\n### Resuming from a checkpoint:\n\n```bash\npython -m training.main \\\n    --train-data=\"/path/to/train_data.csv\" \\\n    --val-data=\"/path/to/validation_data.csv\"  \\\n    --resume /path/to/checkpoints/epoch_K.pt\n```\n\n### Training CoCa:\nTraining [CoCa](https://arxiv.org/abs/2205.01917) models is enabled through specifying a CoCa config using the ```--model``` parameter of the training script. Currently available configs are \"coca_base\", \"coca_ViT-B-32\", and \"coca_roberta-ViT-B-32\" (which uses RoBERTa as the text encoder). CoCa configs are different from CLIP configs because they have an additional \"multimodal_cfg\" component which specifies parameters for the multimodal text decoder. Here's an example from the coca_ViT-B-32 config:\n```json\n\"multimodal_cfg\": {\n\t\"context_length\": 76,\n\t\"vocab_size\": 49408,\n\t\"width\": 512,\n\t\"heads\": 8,\n\t\"layers\": 12,\n\t\"latent_dim\": 512,\n\t\"attn_pooler_heads\": 8\n}\n```\nCredit to [lucidrains](https://github.com/lucidrains) for [initial code](https://github.com/lucidrains/CoCa-pytorch), [gpucce](https://github.com/gpucce) for adapting the code to open_clip, and [iejMac](https://github.com/iejMac) for training the models.\n\n### Generating text with CoCa\n\n```python\nimport open_clip\nimport torch\nfrom PIL import Image\n\nmodel, _, transform = open_clip.create_model_and_transforms(\n  model_name=\"coca_ViT-L-14\",\n  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n)\n\nim = Image.open(\"cat.jpg\").convert(\"RGB\")\nim = transform(im).unsqueeze(0)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n  generated = model.generate(im)\n\nprint(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))\n```\n\nSee also this [[Coca Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)\n\n### Fine Tuning CoCa\n\nTo fine-tune coca on mscoco, first create the dataset, one way is using a csvdataset and perhaps the simplest way to do it is using [CLIP_benchmark](https://github.com/LAION-AI/CLIP_benchmark) which in turn uses [pycocotools](https://github.com/cocodataset/cocoapi) (that can be used also by itself).\n\n```python\nfrom clip_benchmark.datasets.builder import build_dataset\nimport pandas as pd\nimport os\n\nroot_path = \"path/to/data/dir\" # set this to smth meaningful\nds = build_dataset(\"mscoco_captions\", root=root_path, split=\"train\") # this downloads the dataset if it is not there already\ncoco = ds.coco\nimgs = coco.loadImgs(coco.getImgIds())\nfuture_df = {\"filepath\":[], \"title\":[]}\nfor img in imgs:\n    caps = coco.imgToAnns[img[\"id\"]]\n    for cap in caps:\n        future_df[\"filepath\"].append(img[\"file_name\"])\n        future_df[\"title\"].append(cap[\"caption\"])\npd.DataFrame.from_dict(future_df).to_csv(\n  os.path.join(root_path, \"train2014.csv\"), index=False, sep=\"\\t\"\n)\n```\nThis should create a csv dataset that one can use to fine-tune coca with open_clip\n```bash\npython -m training.main \\\n    --dataset-type \"csv\" \\\n    --train-data \"path/to/data/dir/train2014.csv\" \\\n    --warmup 1000 \\\n    --batch-size 128 \\\n    --lr 1e-5 \\\n    --wd 0.1 \\\n    --epochs 1 \\\n    --workers 3 \\\n    --model \"coca_ViT-L-14\" \\\n    --report-to \"wandb\" \\\n    --coca-contrastive-loss-weight 0 \\\n    --coca-caption-loss-weight 1 \\\n    --log-every-n-steps 100\n```\n\nThis is a general setting, open_clip has very parameters that can be set, ```python -m training.main --help``` should show them. The only relevant change compared to pre-training are the two arguments\n\n```bash\n--coca-contrastive-loss-weight 0\n--coca-caption-loss-weight 1\n```\nwhich make the model only train the generative side.\n\n### Training with pre-trained language models as text encoder:\n\nIf you wish to use different language models as the text encoder for CLIP you can do so by using one of the Hugging Face model configs in ```src/open_clip/model_configs``` and passing in it's tokenizer as the ```--model``` and ```--hf-tokenizer-name``` parameters respectively. Currently we only support RoBERTa (\"test-roberta\" config), however adding new models should be trivial. You can also determine how many layers, from the end, to leave unfrozen with the ```--lock-text-unlocked-layers``` parameter. Here's an example command to train CLIP with the RoBERTa LM that has it's last 10 layers unfrozen:\n```bash\npython -m training.main \\\n         --train-data=\"pipe:aws s3 cp s3://s-mas/cc3m/{00000..00329}.tar -\" \\\n         --train-num-samples 3000000 \\\n         --val-data=\"pipe:aws s3 cp s3://s-mas/cc3m/{00330..00331}.tar -\" \\\n         --val-num-samples 10000 \\\n         --dataset-type webdataset \\\n         --batch-size 256 \\\n         --warmup 2000 \\\n         --epochs 10 \\\n         --lr 5e-4 \\\n         --precision amp \\\n         --workers 6 \\\n         --model \"roberta-ViT-B-32\" \\\n         --lock-text \\\n         --lock-text-unlocked-layers 10 \\\n         --name \"10_unfrozen\" \\\n         --report-to \"tensorboard\" \\\n```\n\n### Loss Curves\n\nWhen run on a machine with 8 GPUs the command should produce the following training curve for Conceptual Captions:\n\n![CLIP zero shot training curve](https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png)\n\nMore detailed curves for Conceptual Captions are given at [/docs/clip_conceptual_captions.md](/docs/clip_conceptual_captions.md).\n\nWhen training a RN50 on YFCC the same hyperparameters as above are used, with the exception of `lr=5e-4` and `epochs=32`.\n\nNote that to use another model, like `ViT-B/32` or `RN50x4` or `RN50x16` or `ViT-B/16`, specify with `--model RN50x4`.\n\n### Launch tensorboard:\n```bash\ntensorboard --logdir=logs/tensorboard/ --port=7777\n```\n\n## Evaluation / Zero-Shot\n\nWe recommend https://github.com/LAION-AI/CLIP_benchmark#how-to-use for systematic evaluation on 40 datasets.\n\n### Evaluating local checkpoint:\n\n```bash\npython -m training.main \\\n    --val-data=\"/path/to/validation_data.csv\"  \\\n    --model RN101 \\\n    --pretrained /path/to/checkpoints/epoch_K.pt\n```\n\n### Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:\n\n```bash\npython -m training.main \\\n    --imagenet-val /path/to/imagenet/validation \\\n    --model ViT-B-32-quickgelu \\\n    --pretrained laion400m_e32\n```\n\n## Pretrained model details\n\n### LAION-400M - https://laion.ai/laion-400-open-dataset\n\nWe are working on reproducing OpenAI's ViT results with the comparably sized (and open) LAION-400M dataset. Trained\nweights may be found in release [v0.2](https://github.com/mlfoundations/open_clip/releases/tag/v0.2-weights).\n\nThe LAION400M weights have been trained on the JUWELS supercomputer (see acknowledgements section below).\n\n#### ViT-B/32 224x224\n\nWe replicate OpenAI's results on ViT-B/32, reaching a top-1 ImageNet-1k zero-shot accuracy of 62.96%.\n\n<img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot.png\" width=\"700\">\n\n__Zero-shot comparison (courtesy of Andreas F端rst)__\n<img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_openai_compare_b32.jpg\" width=\"700\">\n\nViT-B/32 was trained with 128 A100 (40 GB) GPUs for ~36 hours, 4600 GPU-hours. The per-GPU batch size was 256 for a global batch size of 32768. 256 is much lower than it could have been (~320-384) due to being sized initially before moving to 'local' contrastive loss.\n\n#### ViT-B/16 224x224\n\nThe B/16 LAION400M training reached a top-1 ImageNet-1k zero-shot validation score of 67.07.\n\n<img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_b16.png\" width=\"700\">\n\nThis was the first major train session using the updated webdataset 0.2.x code. A bug was found that prevented shards from being shuffled properly between nodes/workers each epoch. This was fixed part way through training (epoch 26) but likely had an impact.\n\nViT-B/16 was trained with 176 A100 (40 GB) GPUS for ~61 hours, 10700 GPU-hours. Batch size per GPU was 192 for a global batch size of 33792.\n\n#### ViT-B/16+ 240x240\n\nThe B/16+ 240x240 LAION400M training reached a top-1 ImageNet-1k zero-shot validation score of 69.21.\n\nThis model is the same depth as the B/16, but increases the\n  * vision width from 768 -> 896\n  * text width from 512 -> 640\n  * the resolution 224x224 -> 240x240 (196 -> 225 tokens)\n\n<img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_b16_plus_240.png\" width=\"700\">\n\nUnlike the B/16 run above, this model was a clean run with no dataset shuffling issues.\n\nViT-B/16+ was trained with 224 A100 (40 GB) GPUS for ~61 hours, 13620 GPU-hours. Batch size per GPU was 160 for a global batch size of 35840.\n\n#### ViT-L/14 224x224\n\nThe L/14 LAION-400M training reached a top-1 ImageNet-1k zero-shot validation score of 72.77.\n\n<img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion_clip_zeroshot_l14.png\" width=\"700\">\n\nViT-L/14 was trained with 400 A100 (40 GB) GPUS for ~127 hours, 50800 GPU-hours. Batch size per GPU was 96 for a global batch size of 38400. Grad checkpointing was enabled.\n\n### LAION-2B (en) - https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/\n\nA ~2B sample subset of LAION-5B with english captions (https://huggingface.co/datasets/laion/laion2B-en)\n\n#### ViT-B/32 224x224\nA ViT-B/32 trained on LAION-2B, reaching a top-1 ImageNet-1k zero-shot accuracy of 65.62%.\n\n<img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/laion2b_clip_zeroshot_b32.png\" width=\"700\">\n\nViT-B/32 was trained with 112 A100 (40 GB) GPUs. The per-GPU batch size was 416 for a global batch size of 46592. Compute generously provided by [stability.ai](https://stability.ai/).\n\nA second iteration of B/32 was trained on stability.ai cluster with a larger global batch size and learning rate, hitting 66.6% top-1. See https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K\n\n#### ViT-L/14 224x224\n\nA ViT-L/14 with a 75.3% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here https://huggingface.co/laion/CLIP-ViT-L-14-laion2B-s32B-b82K\n\nThese weights use a different dataset mean and std than others. Instead of using the OpenAI mean & std, inception style normalization `[-1, 1]` is used via a mean and std of `[0.5, 0.5, 0.5]`. This is handled automatically if using `open_clip.create_model_and_transforms` from pretrained weights.\n\n#### ViT-H/14 224x224\n\nA ViT-H/14 with a 78.0% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K\n\n#### ViT-g/14 224x224\n\nA ViT-g/14 with a 76.6% top-1 ImageNet-1k zero-shot was trained on JUWELS Booster. See model details here https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K\n\nThis model was trained with a shorted schedule than other LAION-2B models with 12B samples seen instead of 32+B. It matches LAION-400M training in samples seen. Many zero-shot results are lower as a result, but despite this it performs very well in some OOD zero-shot and retrieval tasks.\n\n\n#### ViT-B/32 roberta base\n\nA ViT-B/32 with roberta base encoder with a 61.7% top-1 ImageNet-1k zero-shot was trained on stability. See model details here https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k\nThis is the first openclip model using a HF text tower. It has better performance on a range of tasks compared to the standard text encoder, see [metrics](https://huggingface.co/laion/CLIP-ViT-B-32-roberta-base-laion2B-s12B-b32k/blob/main/unknown.png)\n\n#### ViT-B/32 xlm roberta base\n\nA ViT-B/32 with xlm roberta base encoder with a 62.33% top-1 ImageNet-1k zero-shot was trained on stability. See model details here https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k\nThis is the first openclip model trained on the full laion5B dataset; hence the first multilingual clip trained with openclip. It has better performance on a range of tasks compared to the standard text encoder, see [metrics](https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k/blob/main/metrics.png)\nA preliminary multilingual evaluation was run: 43% on imagenet1k italian (vs 21% for english B/32), 37% for imagenet1k japanese (vs 1% for english B/32 and 50% for B/16 clip japanese). It shows the multilingual property is indeed there as expected. Larger models will get even better performance.\n\n#### ViT-H/14 xlm roberta large\n\nA ViT-H/14 with xlm roberta large encoder with a 77.0% (vs 78% for the english equivalent) top-1 ImageNet-1k zero-shot was trained on stability. See model details here https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k\n\nThis model was trained following the [LiT](https://arxiv.org/abs/2111.07991) methodology: the image tower was frozen (initialized from english openclip ViT-H/14), the text tower was initialized from [xlm roberta large](https://huggingface.co/xlm-roberta-large) and unfrozen. This reduced training cost by a 3x factor.\n\nSee full english [metrics](https://huggingface.co/laion/CLIP-ViT-H-14-frozen-xlm-roberta-large-laion5B-s13B-b90k/resolve/main/results_xlm_roberta_large.png)\n\nOn zero shot classification on imagenet with translated prompts this model reaches:\n\n* 56% in italian (vs 21% for https://github.com/clip-italian/clip-italian)\n* 53% in japanese (vs 54.6% for https://github.com/rinnakk/japanese-clip)\n* 55.7% in chinese (to be compared with https://github.com/OFA-Sys/Chinese-CLIP)\n\n\n#### YFCC-15M\n\nBelow are checkpoints of models trained on YFCC-15M, along with their zero-shot top-1 accuracies on ImageNet and ImageNetV2. These models were trained using 8 GPUs and the same hyperparameters described in the \"Sample running code\" section, with the exception of `lr=5e-4` and `epochs=32`.\n\n* [ResNet-50](https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-yfcc15m-455df137.pt) (32.7% / 27.9%)\n* [ResNet-101](https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn101-quickgelu-yfcc15m-3e04b30e.pt) (34.8% / 30.0%)\n\n#### CC12M - https://github.com/google-research-datasets/conceptual-12m\n\n* [ResNet-50](https://github.com/mlfoundations/open_clip/releases/download/v0.2-weights/rn50-quickgelu-cc12m-f000538c.pt) (36.45%)\n\n### Pretrained Model Interface\n\nWe offer a simple model interface to instantiate both pre-trained and untrained models.\n\nNOTE: Many existing checkpoints use the QuickGELU activation from the original OpenAI models. This activation is actually less efficient than native torch.nn.GELU in recent versions of PyTorch. The model defaults are now nn.GELU, so one should use model definitions with `-quickgelu` postfix for the OpenCLIP pretrained weights. All OpenAI pretrained weights will always default to QuickGELU. One can also use the non `-quickgelu` model definitions with pretrained weights using QuickGELU but there will be an accuracy drop, for fine-tune that will likely vanish for longer runs.\n\nFuture trained models will use nn.GELU.\n\n```python\n>>> import open_clip\n>>> open_clip.list_pretrained()\n[('RN50', 'openai'),\n ('RN50', 'yfcc15m'),\n ('RN50', 'cc12m'),\n ('RN50-quickgelu', 'openai'),\n ('RN50-quickgelu', 'yfcc15m'),\n ('RN50-quickgelu', 'cc12m'),\n ('RN101', 'openai'),\n ('RN101', 'yfcc15m'),\n ('RN101-quickgelu', 'openai'),\n ('RN101-quickgelu', 'yfcc15m'),\n ('RN50x4', 'openai'),\n ('RN50x16', 'openai'),\n ('RN50x64', 'openai'),\n ('ViT-B-32', 'openai'),\n ('ViT-B-32', 'laion400m_e31'),\n ('ViT-B-32', 'laion400m_e32'),\n ('ViT-B-32', 'laion2b_e16'),\n ('ViT-B-32', 'laion2b_s34b_b79k'),\n ('ViT-B-32-quickgelu', 'openai'),\n ('ViT-B-32-quickgelu', 'laion400m_e31'),\n ('ViT-B-32-quickgelu', 'laion400m_e32'),\n ('ViT-B-16', 'openai'),\n ('ViT-B-16', 'laion400m_e31'),\n ('ViT-B-16', 'laion400m_e32'),\n ('ViT-B-16-plus-240', 'laion400m_e31'),\n ('ViT-B-16-plus-240', 'laion400m_e32'),\n ('ViT-L-14', 'openai'),\n ('ViT-L-14', 'laion400m_e31'),\n ('ViT-L-14', 'laion400m_e32'),\n ('ViT-L-14', 'laion2b_s32b_b82k'),\n ('ViT-L-14-336', 'openai'),\n ('ViT-H-14', 'laion2b_s32b_b79k'),\n ('ViT-g-14', 'laion2b_s12b_b42k'),\n ('ViT-bigG-14', 'laion2b_s39b_b160k'),\n ('roberta-ViT-B-32', 'laion2b_s12b_b32k'),\n ('xlm-roberta-base-ViT-B-32', 'laion5b_s13b_b90k'),\n ('xlm-roberta-large-ViT-H-14', 'frozen_laion5b_s13b_b90k'),\n ('coca_ViT-B-32', 'laion2B-s13B-b90k'),\n ('coca_ViT-B-32', 'mscoco_finetuned_laion2B-s13B-b90k'), # finetuned models lose contrastive capabilities\n ('coca_ViT-L-14', 'laion2B-s13B-b90k'),\n ('coca_ViT-L-14', 'mscoco_finetuned_laion2B-s13B-b90k'),] # finetuned models lose contrastive capabilities\n\n>>> model, train_transform, eval_transform = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n```\n### Model distillation\n\nYou can distill from a pre-trained by using `--distill-model` and `--distill-pretrained` to specify the model you'd like to distill from.\nFor instance, to distill from OpenAI ViT-L/14 use `--distill-model ViT-L-14 --distill-pretrained openai`.\n\n### Gradient accumulation\n\nTo simulate larger batches use `--accum-freq k`. If per gpu batch size, `--batch-size`, is `m`, then the effective batch size will be `k * m * num_gpus`.\n\nWhen increasing `--accum-freq` from its default of 1, samples/s will remain approximately constant (batch size will double, as will time-per-batch). It is recommended to use other features to reduce batch size such as `--grad-checkpointing --local-loss --gather-with-grad` before increasing `--accum-freq`. `--accum-freq` can be used in addition to these features.\n\nInstead of 1 forward pass per example, there are now 2 forward passes per-example. However, the first is done with `torch.no_grad`.\n\nThere is some additional GPU memory required --- the features and data from all `m` batches are stored in memory.\n\nThere are also `m` loss computations instead of the usual 1.\n\nFor more information see Cui et al. (https://arxiv.org/abs/2112.09331) or Pham et al. (https://arxiv.org/abs/2111.10050).\n\n### Support for remote loading/training\n\nIt is always possible to resume directly from a remote file, e.g., a file in an s3 bucket. Just set `--resume s3://<path-to-checkpoint> `.\nThis will work with any filesystem supported by `fsspec`.\n\nIt is also possible to train `open_clip` models while continuously backing up to s3. This can help to avoid slow local file systems.\n\nSay that your node has a local ssd `/scratch`, an s3 bucket `s3://<path-to-bucket>`.\n\nIn that case, set `--logs /scratch` and `--remote-sync s3://<path-to-bucket>`. Then, a background process will sync `/scratch/<run-name>` to `s3://<path-to-bucket>/<run-name>`. After syncing, the background process will sleep for `--remote-sync-frequency` seconds, which defaults to 5 minutes.\n\nThere is also experimental support for syncing to other remote file systems, not just s3. To do so, specify `--remote-sync-protocol fsspec`. However, this is currently very slow and not recommended.\n\nAlso, to optionally avoid saving too many checkpoints locally when using these features, you can use `--delete-previous-checkpoint` which deletes the previous checkpoint after saving a new one.\n\nNote: if you are using this feature with `--resume latest`, there are a few warnings. First, use with `--save-most-recent` is not supported. Second, only `s3` is supported. Finally, since the sync happens in the background, it is possible that the most recent checkpoint may not be finished syncing to the remote.\n\n### Pushing Models to Hugging Face Hub\n\nThe module `open_clip.push_to_hf_hub` includes helpers for pushing models /w weights and config to the HF Hub.\n\nThe tool can be run from command line, ex:\n`pytorch -m open_clip.push_to_hf_hub --model convnext_large_d_320 --pretrained /train/checkpoints/epoch_12.pt --repo-id laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft`\n\n## Scaling trends\n\nThe plot below shows how zero-shot performance of CLIP models varies as we scale the number of samples used for training. Zero-shot performance increases steadily for both ImageNet and [ImageNetV2](https://arxiv.org/abs/1902.10811), and is far from saturated at ~15M samples.\n\n<img src=\"https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/scaling.png\" width=\"700\">\n\n## Why are low-accuracy CLIP models interesting?\n\n**TL;DR:** CLIP models have high effective robustness, even at small scales.\n\nCLIP models are particularly intriguing because they are more robust to natural distribution shifts (see Section 3.3 in the [CLIP paper](https://arxiv.org/abs/2103.00020)).\nThis phenomena is illustrated by the figure below, with ImageNet accuracy on the x-axis\nand [ImageNetV2](https://arxiv.org/abs/1902.10811) (a reproduction of the ImageNet validation set with distribution shift) accuracy on the y-axis.\nStandard training denotes training on the ImageNet train set and the CLIP zero-shot models\nare shown as stars.\n\n![CLIP scatter plot](https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/effective_robustness.png)\n\nAs observed by [Taori et al., 2020](https://arxiv.org/abs/2007.00644) and [Miller et al., 2021](https://arxiv.org/abs/2107.04649), the in-distribution\nand out-of-distribution accuracies of models trained on ImageNet follow a predictable linear trend (the red line in the above plot). *Effective robustness*\nquantifies robustness as accuracy beyond this baseline, i.e., how far a model lies above the red line. Ideally a model would not suffer from distribution shift and fall on the y = x line ([trained human labelers are within a percentage point of the y = x line](http://proceedings.mlr.press/v119/shankar20c.html)).\n\nEven though the CLIP models trained with\nthis codebase achieve much lower accuracy than those trained by OpenAI, our models still lie on the same\ntrend of improved effective robustness (the purple line). Therefore, we can study what makes\nCLIP robust without requiring industrial-scale compute.\n\nFor more information on effective robustness, please see:\n\n- [Recht et al., 2019](https://arxiv.org/abs/1902.10811).\n- [Taori et al., 2020](https://arxiv.org/abs/2007.00644).\n- [Miller et al., 2021](https://arxiv.org/abs/2107.04649).\n\nTo know more about the factors that contribute to CLIP's robustness refer to [Fang et al., 2022](https://arxiv.org/abs/2205.01397).\n\n## Acknowledgments\n\nWe gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J端lich Supercomputing Centre (JSC).\n\n## The Team\n\nCurrent development of this repository is led by [Ross Wightman](https://rwightman.com/), [Cade Gordon](http://cadegordon.io/), and [Vaishaal Shankar](http://vaishaal.com/).\n\nThe original version of this repository is from a group of researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley.\n\n[Gabriel Ilharco*](http://gabrielilharco.com/), [Mitchell Wortsman*](https://mitchellnw.github.io/), [Nicholas Carlini](https://nicholas.carlini.com/), [Rohan Taori](https://www.rohantaori.com/), [Achal Dave](http://www.achaldave.com/), [Vaishaal Shankar](http://vaishaal.com/), [John Miller](https://people.eecs.berkeley.edu/~miller_john/), [Hongseok Namkoong](https://hsnamkoong.github.io/), [Hannaneh Hajishirzi](https://homes.cs.washington.edu/~hannaneh/), [Ali Farhadi](https://homes.cs.washington.edu/~ali/), [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/)\n\nSpecial thanks to [Jong Wook Kim](https://jongwook.kim/) and [Alec Radford](https://github.com/Newmu) for help with reproducing CLIP!\n\n## Citing\n\nIf you found this repository useful, please consider citing:\n```bibtex\n@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}\n```\n\n```bibtex\n@inproceedings{Radford2021LearningTV,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n  booktitle={ICML},\n  year={2021}\n}\n```\n\n```bibtex\n@inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}\n```\n\n[![DOI](https://zenodo.org/badge/390536799.svg)](https://zenodo.org/badge/latestdoi/390536799)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/mlfoundations/open_clip",
    "keywords": "CLIP pretrained",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "open-clip-torch",
    "package_url": "https://pypi.org/project/open-clip-torch/",
    "platform": null,
    "project_url": "https://pypi.org/project/open-clip-torch/",
    "project_urls": {
      "Homepage": "https://github.com/mlfoundations/open_clip"
    },
    "release_url": "https://pypi.org/project/open-clip-torch/2.16.0/",
    "requires_dist": [
      "torch (>=1.9.0)",
      "torchvision",
      "regex",
      "ftfy",
      "tqdm",
      "huggingface-hub",
      "sentencepiece",
      "protobuf (<4)",
      "timm",
      "torch (>=1.9.0) ; extra == 'training'",
      "torchvision ; extra == 'training'",
      "webdataset (>=0.2.5) ; extra == 'training'",
      "regex ; extra == 'training'",
      "ftfy ; extra == 'training'",
      "tqdm ; extra == 'training'",
      "pandas ; extra == 'training'",
      "braceexpand ; extra == 'training'",
      "huggingface-hub ; extra == 'training'",
      "transformers ; extra == 'training'",
      "timm ; extra == 'training'",
      "fsspec ; extra == 'training'"
    ],
    "requires_python": ">=3.7",
    "summary": "OpenCLIP",
    "version": "2.16.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17168985,
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f4e75e79cef9cbabd81bbcd793482cbd179f4296fc3a4b9d3f047266af4e5e0",
          "md5": "7abc1694a34f5ba63c84c9e4fb96e088",
          "sha256": "98774c9042c6a017eabf26ad3260fcf52f5fe59a30a55678d592218392161f80"
        },
        "downloads": -1,
        "filename": "open_clip_torch-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7abc1694a34f5ba63c84c9e4fb96e088",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1388911,
        "upload_time": "2022-04-04T21:25:48",
        "upload_time_iso_8601": "2022-04-04T21:25:48.587610Z",
        "url": "https://files.pythonhosted.org/packages/5f/4e/75e79cef9cbabd81bbcd793482cbd179f4296fc3a4b9d3f047266af4e5e0/open_clip_torch-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e8dac5b1ca82ff8a862de11ce9289faf5ba25d9156f736d0b442f010ebab31f4",
          "md5": "c9e8cbedc9252f2f3a467f65ca19cff7",
          "sha256": "5a811752b8ecf96261d862b1820829d0ed9b376bff5ffc604c1a94cd6ab736c8"
        },
        "downloads": -1,
        "filename": "open_clip_torch-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c9e8cbedc9252f2f3a467f65ca19cff7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1390711,
        "upload_time": "2022-04-04T21:25:51",
        "upload_time_iso_8601": "2022-04-04T21:25:51.522646Z",
        "url": "https://files.pythonhosted.org/packages/e8/da/c5b1ca82ff8a862de11ce9289faf5ba25d9156f736d0b442f010ebab31f4/open_clip_torch-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7286980598c6c3382b2e21ef320139049f719b47893cc6b474f5ca6f13aa718e",
          "md5": "32b1f8e70f4df9cc71329a75311436d3",
          "sha256": "e258588ac1fd604ca7d08589575345dd9f58ffa5acb32b1530a272ff4ac77f5d"
        },
        "downloads": -1,
        "filename": "open_clip_torch-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "32b1f8e70f4df9cc71329a75311436d3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1392057,
        "upload_time": "2022-04-08T16:39:42",
        "upload_time_iso_8601": "2022-04-08T16:39:42.539423Z",
        "url": "https://files.pythonhosted.org/packages/72/86/980598c6c3382b2e21ef320139049f719b47893cc6b474f5ca6f13aa718e/open_clip_torch-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fa2d1074d2b599841a28ac1ddca8daf6b656f757a959a8f5915fe00033ffd44b",
          "md5": "ea0d73a7812ce7dc9961f627f42fb0d9",
          "sha256": "34b7f86cd92227813f765c738fb8f17354e59d3606b898bf2f6cc4fdd2777464"
        },
        "downloads": -1,
        "filename": "open_clip_torch-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ea0d73a7812ce7dc9961f627f42fb0d9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1393888,
        "upload_time": "2022-04-08T16:39:45",
        "upload_time_iso_8601": "2022-04-08T16:39:45.436621Z",
        "url": "https://files.pythonhosted.org/packages/fa/2d/1074d2b599841a28ac1ddca8daf6b656f757a959a8f5915fe00033ffd44b/open_clip_torch-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f2d989d29db756a5b248e6b0c580540c4191de4da72269c28a56f4858586a260",
          "md5": "e54ecc0832fef6b4525a22874366c7bd",
          "sha256": "59e41e5f6a2e57c7223fc905ae2dbaa7395ceb72c9212bf2e82d2053465f8e0f"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e54ecc0832fef6b4525a22874366c7bd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1394116,
        "upload_time": "2022-04-26T23:29:11",
        "upload_time_iso_8601": "2022-04-26T23:29:11.158948Z",
        "url": "https://files.pythonhosted.org/packages/f2/d9/89d29db756a5b248e6b0c580540c4191de4da72269c28a56f4858586a260/open_clip_torch-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b2c6a78dd7685cdd0ab875ac9918a689dfbb0125f20fa3c089db2fc59ea09b8d",
          "md5": "2bcf96dd308882ba564a21c9778d30f2",
          "sha256": "8f9f239f0cb8f57c9f74346005b5388759cd09cdf15934f733d6af18a5ec74b3"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "2bcf96dd308882ba564a21c9778d30f2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1396540,
        "upload_time": "2022-04-26T23:29:13",
        "upload_time_iso_8601": "2022-04-26T23:29:13.084306Z",
        "url": "https://files.pythonhosted.org/packages/b2/c6/a78dd7685cdd0ab875ac9918a689dfbb0125f20fa3c089db2fc59ea09b8d/open_clip_torch-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "00204ac11ee98c1e5dba3e3e1bd7c7c41880d2aadc3786127c73ae698b3ac34b",
          "md5": "e6e80ffe951656a2e525f2e52a90ae91",
          "sha256": "fad72c1ffe7bba0a138ed9e6370d43c3e83476d26a5ab0c1bb7ad8c19adee48f"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e6e80ffe951656a2e525f2e52a90ae91",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1396456,
        "upload_time": "2022-05-15T21:12:14",
        "upload_time_iso_8601": "2022-05-15T21:12:14.227901Z",
        "url": "https://files.pythonhosted.org/packages/00/20/4ac11ee98c1e5dba3e3e1bd7c7c41880d2aadc3786127c73ae698b3ac34b/open_clip_torch-1.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "92583733fd0b91551877245077e7a42a2e00f1999b75c994fae5ddf5e855487b",
          "md5": "7968cfd3e2eeeb3bd4824e25445faf9a",
          "sha256": "3281bd74cd9ac1b4c0d49de609e74d8ffa3551839a7b1ee621bbb42ed710e398"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7968cfd3e2eeeb3bd4824e25445faf9a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1397874,
        "upload_time": "2022-05-15T21:12:15",
        "upload_time_iso_8601": "2022-05-15T21:12:15.996197Z",
        "url": "https://files.pythonhosted.org/packages/92/58/3733fd0b91551877245077e7a42a2e00f1999b75c994fae5ddf5e855487b/open_clip_torch-1.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4eb631eb171cab7a9d2627de304227c92a1b35e883af385ad5f0415f384dbedf",
          "md5": "6d3e1b748e12802e16b7212961d1b15f",
          "sha256": "a0fe8cf11136bf54f101aef586a248b6b38ce939a1baae86f525194d0a68ab5d"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6d3e1b748e12802e16b7212961d1b15f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1396665,
        "upload_time": "2022-05-20T22:27:05",
        "upload_time_iso_8601": "2022-05-20T22:27:05.275737Z",
        "url": "https://files.pythonhosted.org/packages/4e/b6/31eb171cab7a9d2627de304227c92a1b35e883af385ad5f0415f384dbedf/open_clip_torch-1.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d2d7489f2aa7a99a9c01f8c082c7f1e0f50ee6dca766ced210e4897950ce3051",
          "md5": "7d4a1448f076a4834ad2368702a8b4cf",
          "sha256": "b49f8e0ed7b23f5407cfae2648a1a7ab361def98046310c065e439902e37944e"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "7d4a1448f076a4834ad2368702a8b4cf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1398298,
        "upload_time": "2022-05-20T22:27:06",
        "upload_time_iso_8601": "2022-05-20T22:27:06.926513Z",
        "url": "https://files.pythonhosted.org/packages/d2/d7/489f2aa7a99a9c01f8c082c7f1e0f50ee6dca766ced210e4897950ce3051/open_clip_torch-1.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3f8762237ba3696909f4c4b2e5069f526b47e09bba4619054631b5dbaa0f26d2",
          "md5": "92ebbf59b17ee6b795504efc8417beb1",
          "sha256": "39a84ffb4ed849440cb5fc898c2e91eef2e59b2cf9c76103b60a14ef7ceb84c3"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "92ebbf59b17ee6b795504efc8417beb1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1396676,
        "upload_time": "2022-05-21T21:35:47",
        "upload_time_iso_8601": "2022-05-21T21:35:47.516623Z",
        "url": "https://files.pythonhosted.org/packages/3f/87/62237ba3696909f4c4b2e5069f526b47e09bba4619054631b5dbaa0f26d2/open_clip_torch-1.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f4b0cda76f746ad543722e5932de24949cdb079cc2e28cc3dc27aa5007f77bd6",
          "md5": "1e80c885c644093e717bd24d6c1bcc75",
          "sha256": "8771c6b063f3dac77ae35cd6c365b2b72d0bdf5de58c3a5481918218242c2fa8"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "1e80c885c644093e717bd24d6c1bcc75",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1398309,
        "upload_time": "2022-05-21T21:35:49",
        "upload_time_iso_8601": "2022-05-21T21:35:49.496968Z",
        "url": "https://files.pythonhosted.org/packages/f4/b0/cda76f746ad543722e5932de24949cdb079cc2e28cc3dc27aa5007f77bd6/open_clip_torch-1.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1249fcc716b260c267b7298048566dda0e09589655bb1cbf8700c549a29bc5a9",
          "md5": "9301f00005b45074ed379093801265b0",
          "sha256": "3a516571694ef887994e77e75ad960c06d48f58a9cb9711db1516a428d2a17ab"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9301f00005b45074ed379093801265b0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1397155,
        "upload_time": "2022-06-03T23:08:58",
        "upload_time_iso_8601": "2022-06-03T23:08:58.715669Z",
        "url": "https://files.pythonhosted.org/packages/12/49/fcc716b260c267b7298048566dda0e09589655bb1cbf8700c549a29bc5a9/open_clip_torch-1.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "66351294d9e57fb0729ad96680c7dafaa3334df88a5b27ad4baf440cff637979",
          "md5": "41489c84e69aa3d514aeca3d82ace49d",
          "sha256": "09ae90ddc019baf191c2f48868ce7b9701660f5c297787493d76b671df8ada6f"
        },
        "downloads": -1,
        "filename": "open_clip_torch-1.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "41489c84e69aa3d514aeca3d82ace49d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1398606,
        "upload_time": "2022-06-03T23:09:00",
        "upload_time_iso_8601": "2022-06-03T23:09:00.603156Z",
        "url": "https://files.pythonhosted.org/packages/66/35/1294d9e57fb0729ad96680c7dafaa3334df88a5b27ad4baf440cff637979/open_clip_torch-1.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9536097d8e7f28d778d1d711817aac1d0400300a4b93a965684bb75a10349a18",
          "md5": "0ab36e3b47dc872f7cc0073f2db16cd4",
          "sha256": "1c5711286cd6f201ce37040a74fb3f6d88442385742aec817aa2e26128056c6f"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0ab36e3b47dc872f7cc0073f2db16cd4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1401436,
        "upload_time": "2022-09-15T19:04:12",
        "upload_time_iso_8601": "2022-09-15T19:04:12.005452Z",
        "url": "https://files.pythonhosted.org/packages/95/36/097d8e7f28d778d1d711817aac1d0400300a4b93a965684bb75a10349a18/open_clip_torch-2.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3e30f5d150b49e736447046da10910205d11a48fbb8c457d65d161727ad6fe44",
          "md5": "3c49d6c7f4cde5092c3c8cf97030324f",
          "sha256": "7499effa5ae6268333b8355e43485f9223e1dc705e968c3ed7dcd4f215a3dbb1"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3c49d6c7f4cde5092c3c8cf97030324f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1403383,
        "upload_time": "2022-09-15T19:04:13",
        "upload_time_iso_8601": "2022-09-15T19:04:13.693240Z",
        "url": "https://files.pythonhosted.org/packages/3e/30/f5d150b49e736447046da10910205d11a48fbb8c457d65d161727ad6fe44/open_clip_torch-2.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "54fcd778c36335a082fe2599fc61bbc0293a50008cfad904218bbc3392f66871",
          "md5": "1698c9194fc43025874ff6b13d2887e5",
          "sha256": "5744b063b16d80609ce2de6a322dc207fe18dd663b3ca9dce28e92b5c2160be8"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1698c9194fc43025874ff6b13d2887e5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1401536,
        "upload_time": "2022-09-15T23:53:30",
        "upload_time_iso_8601": "2022-09-15T23:53:30.452632Z",
        "url": "https://files.pythonhosted.org/packages/54/fc/d778c36335a082fe2599fc61bbc0293a50008cfad904218bbc3392f66871/open_clip_torch-2.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ca19288d9203fb4817af2cc69e7328efbfecd81510d260b82d62efe4f60d9766",
          "md5": "81db879251dba8cd614256519fde6e9c",
          "sha256": "c93e686e0fb4d49f085cfed4e437aefe389bc28ae4131fa9b694a3b042e10059"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "81db879251dba8cd614256519fde6e9c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1403541,
        "upload_time": "2022-09-15T23:53:32",
        "upload_time_iso_8601": "2022-09-15T23:53:32.608423Z",
        "url": "https://files.pythonhosted.org/packages/ca/19/288d9203fb4817af2cc69e7328efbfecd81510d260b82d62efe4f60d9766/open_clip_torch-2.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3ad8f4bdff87f2b2e89f6e46b6537e4f43029cf46119d196308290032468a074",
          "md5": "a2a58d26e27db8fefbd5fab28f47d780",
          "sha256": "a7ec80a3e9e9ee1e3e78c686f3fe842699b7b22fca41439a8eb7e11e710d047f"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a2a58d26e27db8fefbd5fab28f47d780",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1401592,
        "upload_time": "2022-09-16T15:08:09",
        "upload_time_iso_8601": "2022-09-16T15:08:09.605906Z",
        "url": "https://files.pythonhosted.org/packages/3a/d8/f4bdff87f2b2e89f6e46b6537e4f43029cf46119d196308290032468a074/open_clip_torch-2.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e87ed4c824cb36e7bdbd30459196efe1e8c3a3f0805db6096e256f856c01e6f",
          "md5": "29d1f9ecfe274045dd45a3694f06def7",
          "sha256": "77a2de79f5b3b7672586d0555b882c1ec6025f71016fdd4037c5a0c2376e8a40"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "29d1f9ecfe274045dd45a3694f06def7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1403590,
        "upload_time": "2022-09-16T15:08:11",
        "upload_time_iso_8601": "2022-09-16T15:08:11.315212Z",
        "url": "https://files.pythonhosted.org/packages/5e/87/ed4c824cb36e7bdbd30459196efe1e8c3a3f0805db6096e256f856c01e6f/open_clip_torch-2.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.10.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9cfad51b31e42d1c620e647e33d5d66213428621510721c918321c759cdb7eca",
          "md5": "5b10db93cf877c65d6480178cec2e3db",
          "sha256": "6043eaac289f6267160e2121ee6b62fc05496546bd72debce455b25c1dc2f7ea"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.10.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5b10db93cf877c65d6480178cec2e3db",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1452555,
        "upload_time": "2023-01-23T23:57:08",
        "upload_time_iso_8601": "2023-01-23T23:57:08.094280Z",
        "url": "https://files.pythonhosted.org/packages/9c/fa/d51b31e42d1c620e647e33d5d66213428621510721c918321c759cdb7eca/open_clip_torch-2.10.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fb2f5e88d06f3e7183db94990405cd8838dd6f9f18a482b5b8c720119117eb04",
          "md5": "927bed630b74a90e7798ea1f6a060202",
          "sha256": "62cb1014d747d8516dc7a0d56f754b10195a41a14846f9e4fdbb1874d6aef7cb"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.10.0.tar.gz",
        "has_sig": false,
        "md5_digest": "927bed630b74a90e7798ea1f6a060202",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1454419,
        "upload_time": "2023-01-23T23:57:09",
        "upload_time_iso_8601": "2023-01-23T23:57:09.866532Z",
        "url": "https://files.pythonhosted.org/packages/fb/2f/5e88d06f3e7183db94990405cd8838dd6f9f18a482b5b8c720119117eb04/open_clip_torch-2.10.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.10.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b99840e7fa0ce72d833e2a71618096101ed9be85d3ef0d0f1e0d2738cd0758b9",
          "md5": "a9d9002ac0f0324b3850d5775630e17f",
          "sha256": "77bf358c17eb21b140387905a5a07814d577ddf2366e2d6f0856612e850097b0"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.10.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a9d9002ac0f0324b3850d5775630e17f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1452759,
        "upload_time": "2023-01-24T01:18:18",
        "upload_time_iso_8601": "2023-01-24T01:18:18.184035Z",
        "url": "https://files.pythonhosted.org/packages/b9/98/40e7fa0ce72d833e2a71618096101ed9be85d3ef0d0f1e0d2738cd0758b9/open_clip_torch-2.10.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "41daf129f15e51e41048c6926a108c095db16ee79dc322f674560da7db1e42fc",
          "md5": "4703a94998183fa478f6e154a0ac59ab",
          "sha256": "a8e17e1346cb02f2575e467c5555941f3b7855b1d9ae339b3cbb7b1b8e4f20fa"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.10.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4703a94998183fa478f6e154a0ac59ab",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1454682,
        "upload_time": "2023-01-24T01:18:20",
        "upload_time_iso_8601": "2023-01-24T01:18:20.645300Z",
        "url": "https://files.pythonhosted.org/packages/41/da/f129f15e51e41048c6926a108c095db16ee79dc322f674560da7db1e42fc/open_clip_torch-2.10.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.11.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e5e4867953eb1360141ca355498e06f1fdcaddb886fc47a9b00ac2a9751ffe50",
          "md5": "584038c85ef149010285178730beb098",
          "sha256": "78274d6cceba6a99f0faa3930eef23973a6fa9cd77e449ec790a63fcc9cc1200"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.11.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "584038c85ef149010285178730beb098",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1463175,
        "upload_time": "2023-02-03T19:15:42",
        "upload_time_iso_8601": "2023-02-03T19:15:42.469481Z",
        "url": "https://files.pythonhosted.org/packages/e5/e4/867953eb1360141ca355498e06f1fdcaddb886fc47a9b00ac2a9751ffe50/open_clip_torch-2.11.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5c7b50e5ef95c901cbeaab9beb7df9238e2d35b488a97fc2a34c2adfcec993ae",
          "md5": "95293a1ccd0a45e5a5e0358cb8691f41",
          "sha256": "b90d414f2d6e226eea45a867edf9bcf939813e6985f1c054f925bd7cb50ba643"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.11.0.tar.gz",
        "has_sig": false,
        "md5_digest": "95293a1ccd0a45e5a5e0358cb8691f41",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1465493,
        "upload_time": "2023-02-03T19:15:44",
        "upload_time_iso_8601": "2023-02-03T19:15:44.353669Z",
        "url": "https://files.pythonhosted.org/packages/5c/7b/50e5ef95c901cbeaab9beb7df9238e2d35b488a97fc2a34c2adfcec993ae/open_clip_torch-2.11.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.11.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9bdf037a2070bd9cc11cc4500806239b8503951b3cc413283fcf69d845e29ab1",
          "md5": "a76d28a4081adf0df438e69a61fce238",
          "sha256": "395db67ea33d578e7812a6f0316322a213542bd335f6bf4c8dd9bba6f2050de6"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.11.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a76d28a4081adf0df438e69a61fce238",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1463297,
        "upload_time": "2023-02-04T00:09:08",
        "upload_time_iso_8601": "2023-02-04T00:09:08.946189Z",
        "url": "https://files.pythonhosted.org/packages/9b/df/037a2070bd9cc11cc4500806239b8503951b3cc413283fcf69d845e29ab1/open_clip_torch-2.11.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "59b45e3e7de3761c4dd2f2c53125738c7bc045c453b4992a2c6c9472034e28b5",
          "md5": "7381250cf96e6f5477b2b00acffd68cd",
          "sha256": "91834035fce19c44cc3b3d8b53efd99445fd63466b0bfe643868894666b6613e"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.11.1.tar.gz",
        "has_sig": false,
        "md5_digest": "7381250cf96e6f5477b2b00acffd68cd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1465734,
        "upload_time": "2023-02-04T00:09:10",
        "upload_time_iso_8601": "2023-02-04T00:09:10.796258Z",
        "url": "https://files.pythonhosted.org/packages/59/b4/5e3e7de3761c4dd2f2c53125738c7bc045c453b4992a2c6c9472034e28b5/open_clip_torch-2.11.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.12.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0d363174879b693436d10fe8a58d0aad23f4d1ce47c609dd7e2069a4f2e0f969",
          "md5": "8041e2b5bd0d357d67abd762c115147e",
          "sha256": "29be115f04c67b776fb4a36c9e999426023ac11e2e4a4d67ba9f9e81a33d5ff6"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.12.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8041e2b5bd0d357d67abd762c115147e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1467930,
        "upload_time": "2023-02-11T10:15:19",
        "upload_time_iso_8601": "2023-02-11T10:15:19.725311Z",
        "url": "https://files.pythonhosted.org/packages/0d/36/3174879b693436d10fe8a58d0aad23f4d1ce47c609dd7e2069a4f2e0f969/open_clip_torch-2.12.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9c5db94909aadf29a5eab8a7bc76147d21d6132ec515a0cad8af395dea7df19c",
          "md5": "4cd9afdb7c908314fdb7a0cd5a329385",
          "sha256": "ba8c068c2e3fb7b459f8abcfbd4de771e591b3c061bda0c166876a8bc875ec31"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.12.0.tar.gz",
        "has_sig": false,
        "md5_digest": "4cd9afdb7c908314fdb7a0cd5a329385",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1471666,
        "upload_time": "2023-02-11T10:15:21",
        "upload_time_iso_8601": "2023-02-11T10:15:21.857493Z",
        "url": "https://files.pythonhosted.org/packages/9c/5d/b94909aadf29a5eab8a7bc76147d21d6132ec515a0cad8af395dea7df19c/open_clip_torch-2.12.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.13.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1e0f51238c3f45f4cea79cbce92fd9e0ccb72f0b1ed4489e5300b9eb63f3f0a6",
          "md5": "19ed34e5433378b3c3666a9ac02cd26f",
          "sha256": "a4d4a63a91efd5324cc1a597e0c42bd660c2bdf06e844c2ce7eda4220b30f3fd"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.13.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "19ed34e5433378b3c3666a9ac02cd26f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1468754,
        "upload_time": "2023-02-12T22:47:10",
        "upload_time_iso_8601": "2023-02-12T22:47:10.523463Z",
        "url": "https://files.pythonhosted.org/packages/1e/0f/51238c3f45f4cea79cbce92fd9e0ccb72f0b1ed4489e5300b9eb63f3f0a6/open_clip_torch-2.13.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f68606daeb24291ef90342ac1edc511d23344a3936c7625d64ce975fa6f28522",
          "md5": "a41ef021955c7293e8f6821aad53614f",
          "sha256": "8e8f856a8749fc9010965507708423ae6fe29e8dd80cab72524115f52b29a0e8"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.13.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a41ef021955c7293e8f6821aad53614f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1473133,
        "upload_time": "2023-02-12T22:47:12",
        "upload_time_iso_8601": "2023-02-12T22:47:12.999623Z",
        "url": "https://files.pythonhosted.org/packages/f6/86/06daeb24291ef90342ac1edc511d23344a3936c7625d64ce975fa6f28522/open_clip_torch-2.13.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.14.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "db9c7e17b7f4e4767de1f8eb3f96a8a2546aa50b7333d7d691ebc05252cbaeb9",
          "md5": "bd725ad079fdd434cf8631766d3d40c2",
          "sha256": "1d4a9e9e53301a07f3996879a434785e603756fd35f59912b4111f0e217015e4"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.14.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bd725ad079fdd434cf8631766d3d40c2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1468993,
        "upload_time": "2023-02-16T17:29:15",
        "upload_time_iso_8601": "2023-02-16T17:29:15.838559Z",
        "url": "https://files.pythonhosted.org/packages/db/9c/7e17b7f4e4767de1f8eb3f96a8a2546aa50b7333d7d691ebc05252cbaeb9/open_clip_torch-2.14.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "29d69697f31c5a7b3d5dc254cccb433498ad231fe5531f2f4a9a1aa16c8d1b6c",
          "md5": "4c2c82b8d54cda13c27c805322ad1faf",
          "sha256": "1bcc93652f0972756e1e91301c9745290ae42f5b4e81a7988cf356e2ca6bf301"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.14.0.tar.gz",
        "has_sig": false,
        "md5_digest": "4c2c82b8d54cda13c27c805322ad1faf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1473554,
        "upload_time": "2023-02-16T17:29:17",
        "upload_time_iso_8601": "2023-02-16T17:29:17.287477Z",
        "url": "https://files.pythonhosted.org/packages/29/d6/9697f31c5a7b3d5dc254cccb433498ad231fe5531f2f4a9a1aa16c8d1b6c/open_clip_torch-2.14.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.15.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3d837df144e8ad9627790f9bd21bb82fff3ee926718253476b9bc270a18cf8e1",
          "md5": "ffee22385116829f3b350877f3848f39",
          "sha256": "19734448af323fc4d3dc59ef1a29f0cae4653cae3ab8fcf418f790aa323a175b"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.15.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ffee22385116829f3b350877f3848f39",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1469126,
        "upload_time": "2023-02-27T01:23:18",
        "upload_time_iso_8601": "2023-02-27T01:23:18.921516Z",
        "url": "https://files.pythonhosted.org/packages/3d/83/7df144e8ad9627790f9bd21bb82fff3ee926718253476b9bc270a18cf8e1/open_clip_torch-2.15.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "df6d31da085a7fbb7762a6265a116adec457fbc73f5678ee1982d197f9cfd058",
          "md5": "92d7a69ad3a26c95e8f94da6032695f7",
          "sha256": "e255377a62f7164ccc0f9f6e025fcd7150bac09ab7b5260cc143edc17275c1a1"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.15.0.tar.gz",
        "has_sig": false,
        "md5_digest": "92d7a69ad3a26c95e8f94da6032695f7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1473796,
        "upload_time": "2023-02-27T01:23:21",
        "upload_time_iso_8601": "2023-02-27T01:23:21.187883Z",
        "url": "https://files.pythonhosted.org/packages/df/6d/31da085a7fbb7762a6265a116adec457fbc73f5678ee1982d197f9cfd058/open_clip_torch-2.15.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.16.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e778bcd317fab28c8bd524c921dd176cb59f38a29a96628908749402486c9cb8",
          "md5": "ac2dfa8dc745afdd4546bfbcf2c5c578",
          "sha256": "a95b76fe7fa2becbf08ad1d9e6ca477f4e2c0ec4592f49b64881b4282a6540f6"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.16.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ac2dfa8dc745afdd4546bfbcf2c5c578",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1469149,
        "upload_time": "2023-03-06T01:13:45",
        "upload_time_iso_8601": "2023-03-06T01:13:45.646327Z",
        "url": "https://files.pythonhosted.org/packages/e7/78/bcd317fab28c8bd524c921dd176cb59f38a29a96628908749402486c9cb8/open_clip_torch-2.16.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "31752e7080f12675bc4e58d9d9498b90a0c26c9b0d97b4bb760d175340dab1c2",
          "md5": "3a790057a5e5d1a442c7542b007d73ac",
          "sha256": "83c51046e3dc05616ded51169e61c7063c2731712f1efc8ab629509fc8182cc8"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.16.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3a790057a5e5d1a442c7542b007d73ac",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1473845,
        "upload_time": "2023-03-06T01:13:48",
        "upload_time_iso_8601": "2023-03-06T01:13:48.597585Z",
        "url": "https://files.pythonhosted.org/packages/31/75/2e7080f12675bc4e58d9d9498b90a0c26c9b0d97b4bb760d175340dab1c2/open_clip_torch-2.16.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8a8ba39084010e83984145cfc16926fff0b36c165b586993b566d96f6d09bd8c",
          "md5": "2aa99c205fe8414dae30233e610fbad7",
          "sha256": "1538fc4dfbdcb5b917bef7298e232cda64854f4c2a6846dede644b7003b8ab76"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2aa99c205fe8414dae30233e610fbad7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1408710,
        "upload_time": "2022-11-07T18:08:11",
        "upload_time_iso_8601": "2022-11-07T18:08:11.853062Z",
        "url": "https://files.pythonhosted.org/packages/8a/8b/a39084010e83984145cfc16926fff0b36c165b586993b566d96f6d09bd8c/open_clip_torch-2.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "25ecc6b7c97af475ca25a7de0fa9a47d2e435540f91687f19e85a5514779406b",
          "md5": "d3c5a543990c77bfca891ba141887c6b",
          "sha256": "88eb3b5867f15f142a9567a3faf30f62bab6e24e398a8100b3b1896570c6ea05"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "d3c5a543990c77bfca891ba141887c6b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1406891,
        "upload_time": "2022-11-07T18:08:13",
        "upload_time_iso_8601": "2022-11-07T18:08:13.715317Z",
        "url": "https://files.pythonhosted.org/packages/25/ec/c6b7c97af475ca25a7de0fa9a47d2e435540f91687f19e85a5514779406b/open_clip_torch-2.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f87b166a7a0d95c7a7c9d8078806d524535537fdab15fda4dadf4cd75729536",
          "md5": "a4d3ec9ffe0123a37b0ccfb8bc35f7ee",
          "sha256": "24c88be0f4ab4a4ce8b31055883676d55683721a7ce5e80de2556263bc42d894"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a4d3ec9ffe0123a37b0ccfb8bc35f7ee",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1412008,
        "upload_time": "2022-11-07T19:48:59",
        "upload_time_iso_8601": "2022-11-07T19:48:59.869553Z",
        "url": "https://files.pythonhosted.org/packages/5f/87/b166a7a0d95c7a7c9d8078806d524535537fdab15fda4dadf4cd75729536/open_clip_torch-2.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a4c6a8945a44f677535db9065cff15be1fedebcfa61db3a995567949f2137046",
          "md5": "9ce872471e20435c036c168ea2a42b85",
          "sha256": "8019d95aed82c5afb4c84f0c043e5e5473eb5753a3426134332195301e06af82"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "9ce872471e20435c036c168ea2a42b85",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1412511,
        "upload_time": "2022-11-07T19:49:01",
        "upload_time_iso_8601": "2022-11-07T19:49:01.956621Z",
        "url": "https://files.pythonhosted.org/packages/a4/c6/a8945a44f677535db9065cff15be1fedebcfa61db3a995567949f2137046/open_clip_torch-2.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "123a9c975003a39009a7c4dad36b3005dc8bad7984771edf157de5d97766d100",
          "md5": "051570bb1483738e3b6ccb1025609eeb",
          "sha256": "f1ae1bb572dda7db54ad9a76790128e688393945936fb5406d1b63c7aa693b4e"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "051570bb1483738e3b6ccb1025609eeb",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1412145,
        "upload_time": "2022-11-07T20:52:39",
        "upload_time_iso_8601": "2022-11-07T20:52:39.982873Z",
        "url": "https://files.pythonhosted.org/packages/12/3a/9c975003a39009a7c4dad36b3005dc8bad7984771edf157de5d97766d100/open_clip_torch-2.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a8ee7bb21fdc90f2753f58af29425fd298ea3264949281ed6c7f0d1a4ab3243d",
          "md5": "94b0eef9d218e67343e39062cb4022b1",
          "sha256": "f04c02029189f8e2c68282f16c334ccd959b16d14058848f2909027e33ecdc59"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "94b0eef9d218e67343e39062cb4022b1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1412631,
        "upload_time": "2022-11-07T20:52:42",
        "upload_time_iso_8601": "2022-11-07T20:52:42.994052Z",
        "url": "https://files.pythonhosted.org/packages/a8/ee/7bb21fdc90f2753f58af29425fd298ea3264949281ed6c7f0d1a4ab3243d/open_clip_torch-2.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "109fba8af962bc85278c49c171ff7893694131f48a77f1debe505b209c6e96bf",
          "md5": "4f1b4c09cf93c7cc641339d8e97ba3a1",
          "sha256": "a6eefa57d15dcb769cfbd41c4726fbb5abbfb1252498b2af66d6d5e60175ed35"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.4.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "4f1b4c09cf93c7cc641339d8e97ba3a1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1412597,
        "upload_time": "2022-11-10T10:25:46",
        "upload_time_iso_8601": "2022-11-10T10:25:46.471241Z",
        "url": "https://files.pythonhosted.org/packages/10/9f/ba8af962bc85278c49c171ff7893694131f48a77f1debe505b209c6e96bf/open_clip_torch-2.4.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "34d5ff80f20f4be2669047334d77c18a82a0b73a17d2817d9eabb99e7e8bc368",
          "md5": "605bc0f648412d01ef51c20af652f8f8",
          "sha256": "54113b674b046f2297a7d981ae6f2d264f261aed3355c601bc92efa9bff10b37"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "605bc0f648412d01ef51c20af652f8f8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1412775,
        "upload_time": "2022-11-10T10:25:48",
        "upload_time_iso_8601": "2022-11-10T10:25:48.691397Z",
        "url": "https://files.pythonhosted.org/packages/34/d5/ff80f20f4be2669047334d77c18a82a0b73a17d2817d9eabb99e7e8bc368/open_clip_torch-2.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.4.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ca05edcd7a121577d2f0ed7944dcbe7cc4f2c49af6adfd8715ca8e8b1fdbf8a9",
          "md5": "5161eb0d45e2c3a8f8e0b5f91d807fd3",
          "sha256": "f6082f0b97f11608d15e9d1db0348f74d6b1157504118913fbbc22981d39e866"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.4.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5161eb0d45e2c3a8f8e0b5f91d807fd3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1412604,
        "upload_time": "2022-11-10T11:23:04",
        "upload_time_iso_8601": "2022-11-10T11:23:04.017759Z",
        "url": "https://files.pythonhosted.org/packages/ca/05/edcd7a121577d2f0ed7944dcbe7cc4f2c49af6adfd8715ca8e8b1fdbf8a9/open_clip_torch-2.4.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8f0ffea2aaea3f850ae78e5af01cee0a379b823ecdb08a9a6ae3d07aea5a0176",
          "md5": "bb6b0715c56787bed75c3d989f541b62",
          "sha256": "9904d382c92ff408a336a867250c9451a2fd4a48a379f2976a2e7bb360437dc6"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.4.1.tar.gz",
        "has_sig": false,
        "md5_digest": "bb6b0715c56787bed75c3d989f541b62",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1412784,
        "upload_time": "2022-11-10T11:23:09",
        "upload_time_iso_8601": "2022-11-10T11:23:09.020955Z",
        "url": "https://files.pythonhosted.org/packages/8f/0f/fea2aaea3f850ae78e5af01cee0a379b823ecdb08a9a6ae3d07aea5a0176/open_clip_torch-2.4.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5aab643e2ee2cce7c0bcc43afdeb8635550fb87d89f38641c4384a7460cd9986",
          "md5": "60ad80c4a6ce2eefd8ea71cdb3d05353",
          "sha256": "2dfd9b7c3e93402752c29e8d1d458829ecb16e1cae0d156aedd32b54bbf63824"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "60ad80c4a6ce2eefd8ea71cdb3d05353",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1413562,
        "upload_time": "2022-11-14T16:21:04",
        "upload_time_iso_8601": "2022-11-14T16:21:04.974583Z",
        "url": "https://files.pythonhosted.org/packages/5a/ab/643e2ee2cce7c0bcc43afdeb8635550fb87d89f38641c4384a7460cd9986/open_clip_torch-2.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "37964a7ee230d7bd4a9cf1ea5d64b96ee7b2231c6fff8d91b29600bba1d094a3",
          "md5": "741f5de13916527063b82c39ab4f5324",
          "sha256": "4fd93c0c3c773e8ecb9e200c6920901512262929618786d632d8d049d54de9e7"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "741f5de13916527063b82c39ab4f5324",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1415484,
        "upload_time": "2022-11-14T16:21:07",
        "upload_time_iso_8601": "2022-11-14T16:21:07.556176Z",
        "url": "https://files.pythonhosted.org/packages/37/96/4a7ee230d7bd4a9cf1ea5d64b96ee7b2231c6fff8d91b29600bba1d094a3/open_clip_torch-2.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.6.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e5bc7a4938e040a99ff8ee423d2edced538e00f703b6ab5d19321caaa7fb0cb2",
          "md5": "79ce8940cce91052834cd7f00beac3f9",
          "sha256": "974efb361326df9bc83f5bead8a32e3e6f08bb490ce39c4778e7d8ae1352cb3d"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.6.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "79ce8940cce91052834cd7f00beac3f9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1441387,
        "upload_time": "2022-11-17T20:58:03",
        "upload_time_iso_8601": "2022-11-17T20:58:03.847744Z",
        "url": "https://files.pythonhosted.org/packages/e5/bc/7a4938e040a99ff8ee423d2edced538e00f703b6ab5d19321caaa7fb0cb2/open_clip_torch-2.6.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "54a33ce195af266d227e0d33915ee76553eeb5bc753429b04cae168f286f57cd",
          "md5": "3063d9bbfcc02821ffef27ad4cf8c065",
          "sha256": "e257d8b02eefeb731276926ce748b9b03192909960bde4ad7bb1a408ec355430"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.6.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3063d9bbfcc02821ffef27ad4cf8c065",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1440307,
        "upload_time": "2022-11-17T20:58:05",
        "upload_time_iso_8601": "2022-11-17T20:58:05.590421Z",
        "url": "https://files.pythonhosted.org/packages/54/a3/3ce195af266d227e0d33915ee76553eeb5bc753429b04cae168f286f57cd/open_clip_torch-2.6.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.7.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7f8077b44d0a3c08e2c64fed22f4f0b9049bdf6fddcadbce57453c706c549d65",
          "md5": "1a919cf880d06bac81dbc4344701cc99",
          "sha256": "a86abf63a9fbe70fdd8f251598a0df952e33cefc2fa077434d26ce9c10d905d0"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.7.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1a919cf880d06bac81dbc4344701cc99",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1441736,
        "upload_time": "2022-11-18T21:25:54",
        "upload_time_iso_8601": "2022-11-18T21:25:54.686743Z",
        "url": "https://files.pythonhosted.org/packages/7f/80/77b44d0a3c08e2c64fed22f4f0b9049bdf6fddcadbce57453c706c549d65/open_clip_torch-2.7.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ebe51f5469f53ea8b09661984fedf41cd55b7585e2e0c7a2d245255ab0fb385e",
          "md5": "675927895782f6276ba89387443b4ed1",
          "sha256": "fc0075dd4614494d0af928d817220c1faff5ee96d430d8ef9589571beb4657dd"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.7.0.tar.gz",
        "has_sig": false,
        "md5_digest": "675927895782f6276ba89387443b4ed1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1442503,
        "upload_time": "2022-11-18T21:25:56",
        "upload_time_iso_8601": "2022-11-18T21:25:56.555213Z",
        "url": "https://files.pythonhosted.org/packages/eb/e5/1f5469f53ea8b09661984fedf41cd55b7585e2e0c7a2d245255ab0fb385e/open_clip_torch-2.7.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.8.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "efebb36692ddb2b52437f24d6fcfb91b8e69a03ae0e0293e106e8634b3e869b7",
          "md5": "9aaf2af05f3e1ba374023a524b5399c8",
          "sha256": "c0500589c361f9ec3f0eaf09c1bfea49d768a3f36f2e0a6d013aeabc72d57a65"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.8.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9aaf2af05f3e1ba374023a524b5399c8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1446442,
        "upload_time": "2022-12-14T21:09:52",
        "upload_time_iso_8601": "2022-12-14T21:09:52.683568Z",
        "url": "https://files.pythonhosted.org/packages/ef/eb/b36692ddb2b52437f24d6fcfb91b8e69a03ae0e0293e106e8634b3e869b7/open_clip_torch-2.8.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "55e0ac232794af6e310e32d6585bc1a4094fc465080bf7b136286c7df77431b4",
          "md5": "849013472d3df1c5a80fd893b2066d3a",
          "sha256": "375dca29fed2956c8f828e13af0c6a95ad012a82efd3f3afce73f5474d42660d"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.8.0.tar.gz",
        "has_sig": false,
        "md5_digest": "849013472d3df1c5a80fd893b2066d3a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1448245,
        "upload_time": "2022-12-14T21:09:54",
        "upload_time_iso_8601": "2022-12-14T21:09:54.083372Z",
        "url": "https://files.pythonhosted.org/packages/55/e0/ac232794af6e310e32d6585bc1a4094fc465080bf7b136286c7df77431b4/open_clip_torch-2.8.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.8.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "36518af27293eb683ca9b8d75e611b57c0858e6bb96f97b37ddeb6102e58b2bf",
          "md5": "eebbd587a464ead969d4ab678273d2bc",
          "sha256": "924d47963a917fa693c5c4493c75ca5ed9503a09fca97a7766b960e533b5c8fd"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.8.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eebbd587a464ead969d4ab678273d2bc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1446495,
        "upload_time": "2022-12-15T17:50:29",
        "upload_time_iso_8601": "2022-12-15T17:50:29.682955Z",
        "url": "https://files.pythonhosted.org/packages/36/51/8af27293eb683ca9b8d75e611b57c0858e6bb96f97b37ddeb6102e58b2bf/open_clip_torch-2.8.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8955bf3e0d9134b3bd38feeb779ac4934b0eeb8907e91e8996f9321dece3645b",
          "md5": "f4f7bd217eac12f2f510cf27a3167af4",
          "sha256": "b28c6cb7469cea747550b8b15de14e3b9f8559810e7b2f1d93c3065075c66abf"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.8.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f4f7bd217eac12f2f510cf27a3167af4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1448392,
        "upload_time": "2022-12-15T17:50:31",
        "upload_time_iso_8601": "2022-12-15T17:50:31.439417Z",
        "url": "https://files.pythonhosted.org/packages/89/55/bf3e0d9134b3bd38feeb779ac4934b0eeb8907e91e8996f9321dece3645b/open_clip_torch-2.8.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.8.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a8f6a30f10401f80b6b70d402c0bf474369e887fc7d0a398c6125325b722dbcd",
          "md5": "b1d49ab9a6401835017175c815506b72",
          "sha256": "e045bd0edc220a67b70bdb3de6f08b2ae3554225abff0acdf04e3851fcb46dec"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.8.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b1d49ab9a6401835017175c815506b72",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1446497,
        "upload_time": "2022-12-17T00:00:21",
        "upload_time_iso_8601": "2022-12-17T00:00:21.493410Z",
        "url": "https://files.pythonhosted.org/packages/a8/f6/a30f10401f80b6b70d402c0bf474369e887fc7d0a398c6125325b722dbcd/open_clip_torch-2.8.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6bcab990636c2051b38ca2411e06a5ff19148c898da0e7eb0d5b67268142e6fa",
          "md5": "51d4deae3db8704a5687e7cb211ab5ef",
          "sha256": "a5a6d64408f73183e327d79ff1e3b75aaeb02cc4550eb6a5553d6621ac7bcd37"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.8.2.tar.gz",
        "has_sig": false,
        "md5_digest": "51d4deae3db8704a5687e7cb211ab5ef",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1448417,
        "upload_time": "2022-12-17T00:00:23",
        "upload_time_iso_8601": "2022-12-17T00:00:23.457632Z",
        "url": "https://files.pythonhosted.org/packages/6b/ca/b990636c2051b38ca2411e06a5ff19148c898da0e7eb0d5b67268142e6fa/open_clip_torch-2.8.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.9.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b80cc41975c8fb8b4d43c270dd206ed9df8c2b24920674fffe5790b31fdae1ca",
          "md5": "9f4cd232a067d44c626dd9a6ce945707",
          "sha256": "d007b327e8e7f3883969dffa5dcda50836ed187199eb37cba994da497a5eb88d"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.9.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9f4cd232a067d44c626dd9a6ce945707",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1448220,
        "upload_time": "2022-12-29T22:21:47",
        "upload_time_iso_8601": "2022-12-29T22:21:47.084172Z",
        "url": "https://files.pythonhosted.org/packages/b8/0c/c41975c8fb8b4d43c270dd206ed9df8c2b24920674fffe5790b31fdae1ca/open_clip_torch-2.9.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4a5424429171e520573cf4d38daa990201608f69706b470cae385fe0812138c8",
          "md5": "c52ab0e2c6120651778db54a68b8c8d9",
          "sha256": "f5b3139ae7ed39eb81194e3d0281a574ecd59dc2fb6b9cf3ceffe942363c98ec"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.9.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c52ab0e2c6120651778db54a68b8c8d9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1449388,
        "upload_time": "2022-12-29T22:21:48",
        "upload_time_iso_8601": "2022-12-29T22:21:48.907606Z",
        "url": "https://files.pythonhosted.org/packages/4a/54/24429171e520573cf4d38daa990201608f69706b470cae385fe0812138c8/open_clip_torch-2.9.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.9.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "af7d24cebce30f4b20cfad03111e97cf21bc517494b5c1ce1aa1257bce304364",
          "md5": "a426c8bf7cd7b7efdaa60acaa5a50316",
          "sha256": "ae053bdc5cbb11f6540ebba31a1148645038f2286d5d51ff37d0d172b85ded82"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.9.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a426c8bf7cd7b7efdaa60acaa5a50316",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1448253,
        "upload_time": "2023-01-05T14:35:27",
        "upload_time_iso_8601": "2023-01-05T14:35:27.151987Z",
        "url": "https://files.pythonhosted.org/packages/af/7d/24cebce30f4b20cfad03111e97cf21bc517494b5c1ce1aa1257bce304364/open_clip_torch-2.9.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "94d570315f8f54e2a2b723fb206893bc308a9585a314883c7d07d7b97611d5cd",
          "md5": "bff1e1fbc8bfd8b65faf0259f8fde5e9",
          "sha256": "14ca5fc3f44e91f2f012f5adf16bf016c87045bcaa364a4169ffe46d3e479d5b"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.9.2.tar.gz",
        "has_sig": false,
        "md5_digest": "bff1e1fbc8bfd8b65faf0259f8fde5e9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1449491,
        "upload_time": "2023-01-05T14:35:28",
        "upload_time_iso_8601": "2023-01-05T14:35:28.552487Z",
        "url": "https://files.pythonhosted.org/packages/94/d5/70315f8f54e2a2b723fb206893bc308a9585a314883c7d07d7b97611d5cd/open_clip_torch-2.9.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.9.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "042954d496b431cbc2da20d6eb18f327ed80280faeacfd5576e5e8cdffacbded",
          "md5": "6bb3ae97e2cf0709d07fe517bfe659cc",
          "sha256": "a9e4881b33c01cb8d26e6b5a3c3fa64405b81fe24dbb8a99c1ceb4707c2e9787"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.9.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6bb3ae97e2cf0709d07fe517bfe659cc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1448261,
        "upload_time": "2023-01-09T22:01:40",
        "upload_time_iso_8601": "2023-01-09T22:01:40.341448Z",
        "url": "https://files.pythonhosted.org/packages/04/29/54d496b431cbc2da20d6eb18f327ed80280faeacfd5576e5e8cdffacbded/open_clip_torch-2.9.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d484d83e98f44a1d9fdb89ab45ab9ca8337f067e917dcffce449bc02732dedb",
          "md5": "3b90dba4210040dbba60516d5567f9c1",
          "sha256": "46bb98c93cedc0ea5b5fd9ebeb5e4b07375959f627056d432b6bb5774eb89877"
        },
        "downloads": -1,
        "filename": "open_clip_torch-2.9.3.tar.gz",
        "has_sig": false,
        "md5_digest": "3b90dba4210040dbba60516d5567f9c1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1449489,
        "upload_time": "2023-01-09T22:01:42",
        "upload_time_iso_8601": "2023-01-09T22:01:42.023765Z",
        "url": "https://files.pythonhosted.org/packages/5d/48/4d83e98f44a1d9fdb89ab45ab9ca8337f067e917dcffce449bc02732dedb/open_clip_torch-2.9.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e778bcd317fab28c8bd524c921dd176cb59f38a29a96628908749402486c9cb8",
        "md5": "ac2dfa8dc745afdd4546bfbcf2c5c578",
        "sha256": "a95b76fe7fa2becbf08ad1d9e6ca477f4e2c0ec4592f49b64881b4282a6540f6"
      },
      "downloads": -1,
      "filename": "open_clip_torch-2.16.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ac2dfa8dc745afdd4546bfbcf2c5c578",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 1469149,
      "upload_time": "2023-03-06T01:13:45",
      "upload_time_iso_8601": "2023-03-06T01:13:45.646327Z",
      "url": "https://files.pythonhosted.org/packages/e7/78/bcd317fab28c8bd524c921dd176cb59f38a29a96628908749402486c9cb8/open_clip_torch-2.16.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "31752e7080f12675bc4e58d9d9498b90a0c26c9b0d97b4bb760d175340dab1c2",
        "md5": "3a790057a5e5d1a442c7542b007d73ac",
        "sha256": "83c51046e3dc05616ded51169e61c7063c2731712f1efc8ab629509fc8182cc8"
      },
      "downloads": -1,
      "filename": "open_clip_torch-2.16.0.tar.gz",
      "has_sig": false,
      "md5_digest": "3a790057a5e5d1a442c7542b007d73ac",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 1473845,
      "upload_time": "2023-03-06T01:13:48",
      "upload_time_iso_8601": "2023-03-06T01:13:48.597585Z",
      "url": "https://files.pythonhosted.org/packages/31/75/2e7080f12675bc4e58d9d9498b90a0c26c9b0d97b4bb760d175340dab1c2/open_clip_torch-2.16.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}