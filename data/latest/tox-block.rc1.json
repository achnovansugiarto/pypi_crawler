{
  "info": {
    "author": "Pascal Bliem",
    "author_email": "pascal@bliem.de",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: Implementation :: CPython",
      "Programming Language :: Python :: Implementation :: PyPy"
    ],
    "description": "\n# ToxBlock\n\nToxBlock is a Python machine learning application for recognizing toxic language in text. It can potentially be employed for automatically screening text in articles, posts, and comments on social media, digital news, online forums etc. and blocking it or flagging it for further review by human intelligence.\n\nIt can predict probabilities for classifying English text into six categories of verbal toxicity: toxic, severe toxic, obscene, threat, insult, and identity hate. \n\nToxBlock currently (`version 0.1.2`) uses a bidirectional LSTM recurrent neural network with a word embedding layer (pre-trained with the [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings) and has been trained on the Toxic Comment Data Set ([CC0](https://creativecommons.org/share-your-work/public-domain/cc0/) license) provided by Conversation AI / Jigsaw in a [Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview) in 2018.\n\n*Disclaimer: the data set used for training, test examples in this repo, as well as the usage examples presented here contain toxic language that may be considered profane, vulgar, or offensive. If you do not wish to be exposed to toxic language, DO NOT proceed to read any further.*\n\n## Usage\n\nThe methods for prediction are contained in the module `tox_block.prediction`.\n\nPredictions for single strings of text can me made via `tox_block.prediction.make_single_prediction`:\n\n```python\nfrom tox_block.prediction import make_single_prediction\n\nmake_single_prediction(\"I will kill you, you fucking idiot!\", rescale=True)\n```\nIt will return a dictionary with the original text and the predicted probabilities for each category of toxicity:\n```python\n{'text': 'I will kill you, you fucking idiot!',\n 'toxic': 0.9998680353164673,\n 'severe_toxic': 0.7870364189147949,\n 'obscene': 0.9885633587837219,\n 'threat': 0.8483908176422119,\n 'insult': 0.9883397221565247,\n 'identity_hate': 0.1710592657327652}\n```\n\nTo make bulk predictions for several texts, they can be passed as a list of strings into `tox_block.prediction.make_predictions`:\n```python\nfrom tox_block.prediction import make_predictions\n\nmake_predictions([\"Good morning my friend, I hope you're having a fantastic day!\",\n                  \"I will kill you, you fucking idiot!\",\n                  \"I do strongly disagree with the fascist views of this joke that calls itself a political party.\"], rescale=True)\n```\nIt will return a dictionary of dictionaries of which each contains the original text and the predicted probabilities for each category of toxicity:\n```python\n{\n0: {'text': \"Good morning my friend, I hope you're having a fantastic day!\",\n  'toxic': 0.05347811430692673,\n  'severe_toxic': 0.0006274021579883993,\n  'obscene': 0.004466842859983444,\n  'threat': 0.009578478522598743,\n  'insult': 0.00757843442261219,\n  'identity_hate': 0.002106667961925268},\n 1: {'text': 'I will kill you, you fucking idiot!',\n  'toxic': 0.9998679757118225,\n  'severe_toxic': 0.7870362997055054,\n  'obscene': 0.9885633587837219,\n  'threat': 0.8483908176422119,\n  'insult': 0.9883397221565247,\n  'identity_hate': 0.171059250831604},\n 2: {'text': 'I do strongly disagree with the fascist views of this joke that calls itself a political party.',\n  'toxic': 0.026190076023340225,\n  'severe_toxic': 7.185135473264381e-05,\n  'obscene': 0.0009493605466559529,\n  'threat': 0.00012321282702032477,\n  'insult': 0.0029190618079155684,\n  'identity_hate': 0.0022098885383456945}\n}\n```\nThe boolean parameter `rescale` specifies if the predicted probabilities should be min-max-scaled to be on a similar range. If `rescale=False`, the raw probabilities will be returned.\n\n## Training ToxBlock on your own data\n\nIn case you want to extend ToxBlock with data that you collected yourself or got from other sources, follow these steps: First, clone this repo so that you have it on your local machine. \n\nThe data should be saved in a CSV file in the same format as the data from the Toxic Comment [Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data), which means it should have a column called `comment_text`, which contains the texts that will be used for training, and six columns corresponding to the six categories `toxic`, `severe_toxic`,`obscene`, `threat`, `insult`, and `identity_hate`, which should contain a `1` if the text has been labeled as belonging to the respective category, and `0` otherwise. Either save the file under the path `data/merged.csv` or set the file path you want as value for the variable `tox_block.config.config.MERGED_DATA_FILE`.\n\nDownload the [GloVe embedding files](http://nlp.stanford.edu/data/glove.6B.zip), unzip them, and place the `glove.6B.50d.txt` file in the `data/` folder. \nYou can then start training the model by:\n```python\nfrom tox_block.training_pipeline import run_training\n\nrun_training()\n```\nYou can specify any training or model parameters you want to change in the configuration `tox_block.config.config`. After retraining the model, you may also want to update the parameters for min-max-scaling in `tox_block.config.config.RESCALE_PROBA`, which are used for rescaling the predicted probabilities if `rescale=True` in the prediction methods.\n\n## Acknowledgements\n\nThanks a lot to the organizers of the Toxic Comment [Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview) for providing the data on which this model is trained, as well as to all the participants of the competition who shared their ideas on how to build great classification models.\n\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Pascal-Bliem/tox-block",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tox-block",
    "package_url": "https://pypi.org/project/tox-block/",
    "platform": "",
    "project_url": "https://pypi.org/project/tox-block/",
    "project_urls": {
      "Homepage": "https://github.com/Pascal-Bliem/tox-block"
    },
    "release_url": "https://pypi.org/project/tox-block/0.1.2/",
    "requires_dist": [
      "joblib (>=0.15.1)",
      "numpy (>=1.18.1)",
      "pandas (>=1.0.3)",
      "scikit-learn (>=0.22.1)",
      "tensorflow (>=2.1.0)"
    ],
    "requires_python": ">=3.7.0",
    "summary": "This is a python package for classifying text in 6 categories of verbal toxicity using deep learning.",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7527030,
  "releases": {
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "642ce22f66bd06526124ac93d3f248e4d37c43f8f709617ad4ededb1142b6aa0",
          "md5": "929cdb49dfcc81750307e1a164fbfd4d",
          "sha256": "ed3a934f77d456f5324a6ae1b55bd9a3d404f6ebdb30aedad73dea5a2f01b7ed"
        },
        "downloads": -1,
        "filename": "tox_block-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "929cdb49dfcc81750307e1a164fbfd4d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7.0",
        "size": 17967946,
        "upload_time": "2020-06-21T10:09:46",
        "upload_time_iso_8601": "2020-06-21T10:09:46.576878Z",
        "url": "https://files.pythonhosted.org/packages/64/2c/e22f66bd06526124ac93d3f248e4d37c43f8f709617ad4ededb1142b6aa0/tox_block-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "da0a34eba902d38f02323eae239677b288f51875f7503beb85fdda9510e26ceb",
          "md5": "2e0a437036c9a7415feaa65ab36a9bea",
          "sha256": "4860f2d268de836245f1cd7475ffdcb290ee2d6e44387e3eadf73618f8f6dae2"
        },
        "downloads": -1,
        "filename": "tox_block-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "2e0a437036c9a7415feaa65ab36a9bea",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7.0",
        "size": 18009422,
        "upload_time": "2020-06-21T10:10:24",
        "upload_time_iso_8601": "2020-06-21T10:10:24.858472Z",
        "url": "https://files.pythonhosted.org/packages/da/0a/34eba902d38f02323eae239677b288f51875f7503beb85fdda9510e26ceb/tox_block-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "642ce22f66bd06526124ac93d3f248e4d37c43f8f709617ad4ededb1142b6aa0",
        "md5": "929cdb49dfcc81750307e1a164fbfd4d",
        "sha256": "ed3a934f77d456f5324a6ae1b55bd9a3d404f6ebdb30aedad73dea5a2f01b7ed"
      },
      "downloads": -1,
      "filename": "tox_block-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "929cdb49dfcc81750307e1a164fbfd4d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7.0",
      "size": 17967946,
      "upload_time": "2020-06-21T10:09:46",
      "upload_time_iso_8601": "2020-06-21T10:09:46.576878Z",
      "url": "https://files.pythonhosted.org/packages/64/2c/e22f66bd06526124ac93d3f248e4d37c43f8f709617ad4ededb1142b6aa0/tox_block-0.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "da0a34eba902d38f02323eae239677b288f51875f7503beb85fdda9510e26ceb",
        "md5": "2e0a437036c9a7415feaa65ab36a9bea",
        "sha256": "4860f2d268de836245f1cd7475ffdcb290ee2d6e44387e3eadf73618f8f6dae2"
      },
      "downloads": -1,
      "filename": "tox_block-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "2e0a437036c9a7415feaa65ab36a9bea",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7.0",
      "size": 18009422,
      "upload_time": "2020-06-21T10:10:24",
      "upload_time_iso_8601": "2020-06-21T10:10:24.858472Z",
      "url": "https://files.pythonhosted.org/packages/da/0a/34eba902d38f02323eae239677b288f51875f7503beb85fdda9510e26ceb/tox_block-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}