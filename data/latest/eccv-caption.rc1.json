{
  "info": {
    "author": "Sanghyuk Chun",
    "author_email": "sanghyuk.chun@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Extended COCO Validation (ECCV) Caption dataset\n\nOfficial Python implementation of ECCV Caption | [Paper](https://arxiv.org/abs/2204.03359)\n\n[Sanghyuk Chun](https://sanghyukchun.github.io/home/), [Wonjae Kim](https://wonjae.kim/), [Song Park](https://8uos.github.io/), [Minsuk Chang](https://minsukchang.com/), [Seong Joon Oh](https://coallaoh.github.io/)\n\n[NAVER AI Lab](https://naver-career.gitbook.io/en/teams/clova-cic)\n\nECCV Caption contains x8.47 positive images and x3.58 positive captions compared to the original COCO Caption. The positives are verified by machines (five state-of-the-art image-text matching models) and humans. This library provides an unified interface to measure various COCO Caption retrieval metrics, such as COCO 1k Recall@K, COCO 5k Recall@K, [CxC](https://github.com/google-research-datasets/Crisscrossed-Captions) Recall@K, [PMRP](https://github.com/naver-ai/pcme), and ECCV Caption Recall@K, R-Precision and mAP@R.\n\nFor more details, please read our paper:\n\n[ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO](https://arxiv.org/abs/2204.03359)\n\n### Abstract\n\nImage-Test matching (ITM) is a common task for evaluating the quality of Vision and Language (VL) models. However, existing ITM benchmarks have a significant limitation. They have many missing correspondences, originating from the data construction process itself. For example, a caption is only matched with one image although the caption can be matched with other similar images, and vice versa. To correct the massive false negatives, we construct the Extended COCO Validation (ECCV) Caption dataset by supplying the missing associations with machine and human annotators. We employ five state-of-the-art ITM models with diverse properties for our annotation process. Our dataset provides x3.6 positive image-to-caption associations and x8.5 caption-to-image associations compared to the original MS-COCO. We also propose to use an informative ranking-based metric, rather than the popular Recall@K(R@K). We re-evaluate the existing 25 VL models on existing and proposed benchmarks. Our findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K R@K, CxC R@1 are highly correlated with each other, while the rankings change when we shift to the ECCV mAP. Lastly, we delve into the effect of the bias introduced by the choice of machine annotator. Source code and dataset are available in [https://github.com/naver-ai/eccv-caption](https://github.com/naver-ai/eccv-caption)\n\n\n### Dataset statistics\n\nECCV Caption dataset is an extended version of the COCO Caption test split by [karpathy/neuraltalk2](https://github.com/karpathy/neuraltalk2). We annotate positives the subset of the COCO Caption test set (1,332 query images, 1,261 query captions). We show the number of positive items for the subset of the COCO Caption test split.\n\n| Dataset                  | # positive images | # positive captions |\n|--------------------------|-------------------|---------------------|\n| Original MS-COCO Caption | 1,332             | 6,305 (=1,261×5)   |\n| [CxC](https://github.com/google-research-datasets/Crisscrossed-Captions)                    | 1,895 (×1.42) | 8,906 (×1.41) |\n| Human-verified positives | 10,814 (×8.12)   | 16,990 (×2.69)     |\n| ECCV Caption             | 11,279 (×8.47)   | 22,550 (×3.58)     |\n\n## Updates\n\n- 8 Apr, 2022: Initial upload.\n\n## Getting Started\n\n### Installation\n\n```\npip3 install eccv_caption\n```\n\n### Requirements\n\n```\nnumpy\nujson\ntqdm\n```\n\n`ujson` and `tqdm` are not neccessary.\n\n### Usage\n\n```python\nfrom eccv_caption import Metrics\n\nmetric = Metrics()\n\n# Get i2t, t2i retrived items from your own model\n# i2t = {query_image_id: [sorted_caption_id_by_similarity]}\n# t2i = {query_caption_id: [sorted_image_id_by_similarity]}\n# See the example code for how to compute i2t / t2i from your model\n\nscores = metric.compute_all_metrics(\n\ti2t, t2i,\n\ttarget_metrics=('eccv_r1', 'eccv_map_at_r', 'eccv_rprecision',\n                   'coco_1k_recalls', 'coco_5k_recalls', 'cxc_recalls'),\n\tKs=(1, 5, 10),\n\tverbose=False\n)\nprint(scores)\n```\n\nIt will return a score map, for example:\n```\n{\n    'coco_1k_r1': {'i2t': 0.7425999999999999, 't2i': 0.5544},\n    'coco_1k_r5': {'i2t': 0.9282, 't2i': 0.82324},\n    'coco_1k_r10': {'i2t': 0.9658, 't2i': 0.90152},\n    'coco_5k_r1': {'i2t': 0.5636, 't2i': 0.3658},\n    'coco_5k_r5': {'i2t': 0.7928, 't2i': 0.61048},\n    'coco_5k_r10': {'i2t': 0.867, 't2i': 0.71148},\n    'cxc_r1': {'i2t': 0.5804, 't2i': 0.38314912702226495},\n    'cxc_r5': {'i2t': 0.8142, 't2i': 0.6385151369533878},\n    'cxc_r10': {'i2t': 0.8862, 't2i': 0.7425116130065673},\n    'eccv_map_at_r': {'i2t': 0.23988490345859761, 't2i': 0.3195708452398554},\n    'eccv_rprecision': {'i2t': 0.3381634023972558, 't2i': 0.4177097641300428},\n    'eccv_r1': {'i2t': 0.7129262490087233, 't2i': 0.7297297297297297}\n}\n```\n\n`cxc` denotes CrissCrossed Captions. You can find the details of the CxC dataset from [google-research-datasets/Crisscrossed-Captions](https://github.com/google-research-datasets/Crisscrossed-Captions)\n\nYou can find an example code from [here](./examples/clip_example_run.py)\n\n### Arguments\n\n- `eccv_caption.Metrics`\n\t- `extra_file_dir`: str (default: `None`): directory where `pm_image_to_caption.json` and `pm_caption_to_image.json` are downloaded. You don't need to specify `extra_file_dir` if you are not going to measure `pmrp`.\n\t- `verbose`: bool (default: False), `tqdm` verbose option\n\nAll metrics can be computed by the following method:\n\n- `eccv_caption.Metrics.compute_all_metrics`\n\nThe details of the method arguments are as the follows:\n\n```python\ndef compute_all_metrics(self, i2t_retrieved_items: Dict[int, List[int]] = None,\n                        t2i_retrieved_items: Dict[int, List[int]] = None,\n                        target_metrics: List[str] = ('coco_1k_r1', 'coco_5k_r1', 'cxc_r1', 'eccv_r1', 'eccv_map_at_r'),\n                        Ks: List[int] = (1, 5, 10),\n                        verbose: bool = None):\n    \"\"\" Compute various evaluation metrics for the given image-to-text & text-to-image retrieved items.\n\n    Parameters\n    ----------\n    i2t_retrived_items: dict (key: image id (int), value: retrived caption ids sorted by similarity),\n        (default: `self.i2t_retrived_items` set by `self.set_i2t_retrieved_items`)\n        dictionary of target retrived caption ids to evaluate.\n    t2i_retrived_items: dict (key: caption id (int), value: retrived image ids sorted by similarity),\n        (default: `self.t2i_retrived_items` set by `self.set_t2i_retrieved_items`)\n        dictionary of target retrived image ids to evaluate.\n    target_metrics: list, the list of target metrics.\n        (default: ['coco_1k_r1', 'coco_5k_r1', 'cxc_r1', 'eccv_r1', 'eccv_map_at_r'])\n        Valid metrics: {'coco_1k_r1', 'coco_5k_r1', 'cxc_r1',\n                        'coco_1k_recalls', 'coco_5k_recalls', 'cxc_recalls',\n                        'eccv_r1', 'eccv_rprecision', 'eccv_map_at_r',\n                        'eccv_recalls', 'pmrp'}\n        Note1: `{datasetname}_recalls` returns R@K, where K is given by `Ks`.\n        Note2: to use pmrp, you have to download PM GTs first. Please refer the official document for the details.\n    Ks: list, the list of target Ks for R@K.\n        (default: [1, 5, 10])\n        `target_metrics` should contain {datasetname}_recalls\n    verbose: bool (default: self.verbose), tqdm verbose option\n\n    Returns\n    -------\n    scores (dict): key: the name of metrics, value: {'i2t': score (float), 't2i': score (float)}\n```\n\n### Download the full data\n\nYou can find the full data from the following gdrive link, including `pm_image_to_caption.json` and `pm_caption_to_image.json` for computing `pmrp`.\n\nhttps://drive.google.com/drive/folders/1Sam8_Hpm4uWKB_Ehk9C_JcGNpYH7jZD2?usp=sharing\n\nThe gdrive link also includes the full data, such as the raw human annotated data (`mturk_parsed.csv`) and filtered items (`wrong_captions.txt`, `invalid_images.txt`, `invalid_captions.txt`, `verified_iids.npy`, `verified_cids.npy`). We describe the details of how the dataset is built in the next section.\n\n## Dataset construction\n\nDue to the nature of the dataset annotation process, widely-used Image-Text aligned datasets, such as MS-COCO, have many false negatives. We extend the MS-COCO Caption `test` split by using machine and human annotators.\n\nOur annotation is built upon five state-of-the-art image-text matching models, such as [VSRN (ICCV'19)](https://arxiv.org/abs/1909.02701), [PVSE (CVPR'19)](https://arxiv.org/abs/1906.04402), [ViLT (ICML'21)](https://arxiv.org/abs/2102.03334), [CLIP (ICML'21)](https://arxiv.org/abs/2103.00020), [PCME (CVPR'21)](https://arxiv.org/abs/2101.05068).\n\nWe first collect top-25 retrived items for the subset of MS-COCO Caption by the five SOTA models. We verify whether the top-25 items are positive or negative by Amazon Mechanical Turk (MTurk). The full MTurk results can be found in [gdrive](https://drive.google.com/drive/folders/1Sam8_Hpm4uWKB_Ehk9C_JcGNpYH7jZD2?usp=sharing) `mturk_parsed.csv`.\n\nWe measure precision and recall of the existing benchmarks. Precision (Prec) and Recall of previous datasets measured by human verified positive pairs are shown in the followin table. A low Prec means that many positives are actually negatives, and a low Recall means that there exist many missing positives. The table shows that the existing benchmarks, such as MS-COCO Caption have many missing positive correspondences.\n\n| Dataset                  | I2T Prec | I2T Recall | T2I Prec | T2I Recall |\n|--------------------------|----------|------------|----------|------------|\n| Original MS-COCO Caption | 47.3     | 20.0       | 89.4     | 12.8       |\n| [CxC](https://github.com/google-research-datasets/Crisscrossed-Captions) | 39.6     | 22.0       | 81.4     | 15.0       |\n| [Plausible Match](https://github.com/naver-ai/pcme) | 8.3      | 74.6       | 10.5     | 69.0       |\n\nWe combine our MTurk results and CxC results to build the final ECCV Caption dataset (by treating CxC results as the sixth machine annotator).\n\nFor more details, please read our [paper](https://arxiv.org/abs/2204.03359).\n\n## License\n\n```\nCopyright (c) 2022-present NAVER Corp.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n```\n\n## How to cite\n\n```\n@article{chun2022eccv_caption,\n    title={ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO}, \n    author={Chun, Sanghyuk and Kim, Wonjae and Park, Song and Chang, Minsuk Chang and Oh, Seong Joon},\n    journal={arXiv preprint arXiv:2204.03359},\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/naver-ai/eccv-caption",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "eccv-caption",
    "package_url": "https://pypi.org/project/eccv-caption/",
    "platform": null,
    "project_url": "https://pypi.org/project/eccv-caption/",
    "project_urls": {
      "Homepage": "https://github.com/naver-ai/eccv-caption"
    },
    "release_url": "https://pypi.org/project/eccv-caption/0.1.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A PyThon wrapper for Extended COCO Validation (ECCV) Caption dataset",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13449372,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f8ad24b55f380db9a03ed1d72c9341eddde10823892ce6b497b14f2ee7e3aace",
          "md5": "154ae697dbe09fe61f2b9cdd7e6b93ed",
          "sha256": "c043cb27df5bca26d1870bba0363fe722ef8eaddf72590c1b7598a6d24eb1351"
        },
        "downloads": -1,
        "filename": "eccv_caption-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "154ae697dbe09fe61f2b9cdd7e6b93ed",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 785596,
        "upload_time": "2022-04-08T02:22:20",
        "upload_time_iso_8601": "2022-04-08T02:22:20.565304Z",
        "url": "https://files.pythonhosted.org/packages/f8/ad/24b55f380db9a03ed1d72c9341eddde10823892ce6b497b14f2ee7e3aace/eccv_caption-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0790095ab7f584a2e678c0affd862e76afc164db0139650a3cba0191d6ff746f",
          "md5": "59205888b1a1d0268fdd00927df6e0e7",
          "sha256": "0fd7d23a8dc9819cefcc322a7c7ff7e0c3d533741f2acd5fdd38d22d0659a6d2"
        },
        "downloads": -1,
        "filename": "eccv_caption-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "59205888b1a1d0268fdd00927df6e0e7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 784222,
        "upload_time": "2022-04-08T02:22:24",
        "upload_time_iso_8601": "2022-04-08T02:22:24.660104Z",
        "url": "https://files.pythonhosted.org/packages/07/90/095ab7f584a2e678c0affd862e76afc164db0139650a3cba0191d6ff746f/eccv_caption-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f8ad24b55f380db9a03ed1d72c9341eddde10823892ce6b497b14f2ee7e3aace",
        "md5": "154ae697dbe09fe61f2b9cdd7e6b93ed",
        "sha256": "c043cb27df5bca26d1870bba0363fe722ef8eaddf72590c1b7598a6d24eb1351"
      },
      "downloads": -1,
      "filename": "eccv_caption-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "154ae697dbe09fe61f2b9cdd7e6b93ed",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 785596,
      "upload_time": "2022-04-08T02:22:20",
      "upload_time_iso_8601": "2022-04-08T02:22:20.565304Z",
      "url": "https://files.pythonhosted.org/packages/f8/ad/24b55f380db9a03ed1d72c9341eddde10823892ce6b497b14f2ee7e3aace/eccv_caption-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "0790095ab7f584a2e678c0affd862e76afc164db0139650a3cba0191d6ff746f",
        "md5": "59205888b1a1d0268fdd00927df6e0e7",
        "sha256": "0fd7d23a8dc9819cefcc322a7c7ff7e0c3d533741f2acd5fdd38d22d0659a6d2"
      },
      "downloads": -1,
      "filename": "eccv_caption-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "59205888b1a1d0268fdd00927df6e0e7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 784222,
      "upload_time": "2022-04-08T02:22:24",
      "upload_time_iso_8601": "2022-04-08T02:22:24.660104Z",
      "url": "https://files.pythonhosted.org/packages/07/90/095ab7f584a2e678c0affd862e76afc164db0139650a3cba0191d6ff746f/eccv_caption-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}