{
  "info": {
    "author": "Stiliyan Dragnev",
    "author_email": "stiliyan_dragnev@yahoo.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# neural-image-caption\n\nA simple Python API built on top of [TensorFlow](https://www.tensorflow.org/) for neural image captioning with [MSCOCO](https://cocodataset.org/) data.  \n\n## Table of contents\n\n * [Description](#description)\n * [Installation](#installation)\n * [MSCOCO API](#mscoco-api)\n * [NIC Model](#nic-model)\n * [Training on Google Colab](#training-on-colab)\n * [References](#references)\n\n<a name=\"description\"></a>\n\n## Description\n\nThe **nic** API has two main purposes:  \n\n* working with the MSCOCO dataset  \n  The data can be downloaded, preprocessed and then loaded into Python objects as expected by TensorFlow.  \n* training a neural network model for image captioning  \n  A deep neural network model with sequence-to-sequence architecture can be easily defined, trained on the dataset and then used to caption images.  \n\nThese are discussed in more detail in the following sections.  \n\n<a name=\"installation\"></a>\n\n## Installation\n\nThe API is available on PYPI and can be istalled with pip:  \n`pip install nic`  \n\n<a name=\"mscoco-api\"></a>\n\n## MSCOCO API\n\nThe MSCOCO dataset consists of more than 100 000 captioned images. Each image is \"paired\" with a few descriptions (in English) of what can be seen on it.  \n\nThe **nic** API makes it possible to download the dataset, preprocess the data and load it into Python objects used to train neural networks. We'll look into each of these next.  \n\nNote that the dataset is very big so downloading and preprocessing it will take up a lot of space. At the time of writing this, **an archive file** of the dataset is between 10 and 20 GB. This is why getting rid of the original data might be a good idea once it is preprocessed.  \n\nFirst we need to import the API (and TensorFlow).  \n\n```python\nimport tensorflow as tf\n\nimport nic\n```\n\n### Downloading\n\nThen we can download the dataset (from [here](http://images.cocodataset.org)).  \n\n```python\nmscoco_dir = r\"mscoco\"\nversion = \"2017\"\nnic.dp.download_mscoco(mscoco_dir, version)\n```\n\nThe dataset has train and validation splits so we will create a test split from the train data. Usually 20% of the samples are used for testing:  \n\n```python\nnic.dp.split_out_test_data(mscoco_dir,\n                           split=0.2,\n                           version=version,\n                           verbose=True)\n```\n\n------\n\nA note for those who may want to use the original MSCOCO data for something else too:   \nThe train images (randomly) selected for testing are moved from *mscoco/train2017* to a separate directory named *mscoco/test2017*. Their annotations are extracted from *annotations/captions_train2017.json* to *annotations/captions_test2017.json* but this extraction simply removes the annotations from the 'annotations' list in the first file and creates the second file which only contains the extracted annotations like so: `{\"annotations\": <annotations>}`.  \nA copy of the original train captions file is created as back up so the original structure of the dataset can be restored by moving the images back to *train2017*, deleting the *captions_test2017.json* file and restoring the backup file with train captions.  \n\n------\n\n### Preprocessing\n\nNext, we preprocess the dataset by calling the `preprocess_data` function. We provide this function with the path of the MSCOCO directory, the path where to store the preprocessed data, the meta tokens to be used when preprocessing captions, the maximum number of words (if needed) to include in the dictionary extracted from the captions, and some image options.  \n\nThe image options describe the way in which images are preprocessed. Image preprocessing involves 'preparing' images for a specific CNN encoder and optionally extracting features for images by running them through the encoder. The second part is useful when doing transfer learning with the CNN encoder module of the model being frozen. Extracting the features once and reusing them to train the other model layers is much more efficient than the alternative.  \n\nThe image options are as follows:  \n\n- model_name  \n  The name of the CNN encoder to preprocess the images for. This model is looked up in `tf.keras.applications` and its `preprocess_input` method is called on batches of images\n- target_size  \n  The spatial size of the image, as expected by the chosen CNN encoder\n- feature_extractor  \n  A callable taking and returning a `tf.Tensor`. If provided, it will extract features for batches of preprocessed images\n- batch_size  \n  The batch size to use when preprocessing (and extracting features for) the images\n\n------\n\nAs we will see in a moment, the API provides a function that loads preprocessed data into a `tf.data.Dataset`. For those interested, here is how the preprocessed data looks on disk:  \n\n - the data is stored in a directory `D` which has three subdirectories - *train*, *test* and *val*\n - each of the subdirectories has a subdirectory named *images* which stores preprocessed images and optionally a subdirectory named *features* which stores features extracted for the images. Preprocessed images and image features are pickled `tf.Tensor`s, the file names are simply `<image_id>`*.pcl*\n - each of the subdirectories also contains a file named *captions.pcl*. It contains a pickled dictionary mapping image ids (int) to a list of str captions (the original captions enclosed with the start and end meta tokens)\n - the *train* subdirectory has another file - *tokenizer.json*. This is the JSON representation of a `tf.keras.preprocessing.text.Tokenizer` created from the train captions\n\n------\n\nIn this example we will preprocess the data for [Inception ResNet v2](https://arxiv.org/abs/1602.07261).  \n\n```python\ndata_dir = \"data\"\n\nencoder = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n    include_top=False,\n    weights=\"imagenet\",\n    pooling=\"max\"\n)\nencoder = tf.keras.Model(encoder.input,\n                         encoder.layers[-1].output,\n                         name=\"inception-resnet-v2\")\n\nimage_options = nic.dp.ImageOptions(\n    model_name=\"inception_resnet_v2\",\n    target_size=(299, 299),\n    feature_extractor=encoder,\n    batch_size=16\n)\nmeta_tokens = nic.dp.MetaTokens(\n    start=\"<start>\",\n    end=\"<end>\",\n    unknown=\"<unk>\",\n    padding=\"<pad>\",\n)\nmax_words = None\nnic.dp.preprocess_data(source_dir=mscoco_dir,\n                       target_dir=data_dir,\n                       version=version,\n                       image_options=image_options,\n                       meta_tokens=meta_tokens,\n                       max_words=max_words,\n                       verbose=True)\n```\n\n### Loading preprocessed data\n\nPreprocessed data can be loaded with the `load_data` function. It takes the path of the directory where preprocessed data is stored, the type of data to load (`'train'`, `'val'` or `'test'`) and a boolean value indicating whether to load features or preprocessed images:  \n\n```python\ntrain_data = nic.dp.load_data(data_dir, type=\"train\", load_as_features=True)\ntest_data = nic.dp.load_data(data_dir, type=\"test\", load_as_features=False)\n```\n\nThe data is loaded into a `tf.data.Dataset` which yields 3-tuples whose components are `tf.Tensor`s:  \n* the 3D image tensor or features vector (if `load_as_features` is set to `True`)\n* an integer vector which represents a caption for the image, without the end meta token at the end\n* an integer vector which represents the same caption but this time without the start meta token in front\n\nThe shape of the caption vectors is `(max_caption_length,)` and shorter captions are post-padded with 0 (the index of the padding meta token). The shape of the image or features tensor depends on the chosen CNN encoder.  \n\n------\n\nKeras models are typically trained with `tf.data.Dataset` objects which yield elements with a different structure (not  3-tuples). In order to train a model with the datasets returned by `load_data`, we'd need to customise `fit`, as explained [here](https://keras.io/guides/customizing_what_happens_in_fit/).  \n\nAs described [later](#nic-model), the **nic** API can also be used to define and train a model with the MSCOCO dataset. The `CustomModel` class defined within **nic** can be used as an example for customizing `fit` to work with the datasets returned by `load_data`.  \n\n------\n\nThere are a few more API functions that work with preprocessed data. The [tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) can be loaded like this:  \n\n```python\ntokenizer = nic.dp.load_tokenizer(data_dir)\n```\n\nCaptions can be loaded into a dictionary mapping integers (image ids) to lists of strings (the original captions enclosed with the start and end meta tokens):  \n\n```python\nval_captions = nic.dp.load_captions(data_dir, type=\"val\")\n```\n\nImages (preprocessed for the chosen CNN encoder) or their corresponding features can be loaded into a `tf.data.Dataset` which yields pairs of the images/features and the image id:  \n\n```python\ntest_images, count = nic.dp.load_images(data_dir, type=\"test\", load_as_features=False)\n```\n\nVocabulary and features sizes can also be obtained:  \n\n```python\nvocabulary_size = nic.dp.vocabulary_size(data_dir)\nfeatures_size = nic.dp.features_size(data_dir)\n```\n\n<a name=\"nic-model\"></a>\n\n## NIC Model\n\nThe other main part of the **nic** API is a neural network model that can be easily defined, trained on the MSCOCO dataset and then used to caption images.  \n\nThe model has a Seq2Seq architecture which is depicted below.  \n![model_architecture](docs/architecture.png)\n\nImages are represented as 3D tensors which are fed into a CNN. The resulting feature vectors are transformed and fed into an RNN, as the initial hidden state vectors.  \n\nCaptions are tokenized and each token is represented as a vector from a word embedding. The word embeddings are fed into the RNN as inputs.  \n\nThe hidden state vectors at each time point (caption length) are transformed and projected over the vocabulary words/terms.  \n\nDuring training, the word projections are used to calculate the loss (categorical cross entropy). During inference, the projections are used to generate a word distribution that is used to select the next word in the caption.  \n\nThe CNN image encoder is typically a pretrained model, like Inception ResNet v2. The rest of the model, visualised with `tf.keras.utils.plot_model`, looks like this (the RNN's hidden size is 512 in this case):  \n![decoder_functional](docs/decoder.png)\n\nThe model is largely similar to the one described [here](https://ieeexplore.ieee.org/document/7505636?denied=).  \n\n### Defining the model\n\nFirst we need to import the API:  \n\n```python\nimport nic\n```\n\nThe CNN encoder module can be any model (built with TensorFlow 2) that transforms an image (3D tensor) into a vector. Remember that the encoder is important when preprocessing data too, as mentioned in the [MSCOCO](#mscoco-api) section. The **nic** API makes it easy to use Inception ResNet v2 via the following function call:  \n\n```python\npooling = \"max\"\nencoder = nic.define_encoder_model(pooling)\n```\n\nThis returns the [Inception ResNet v2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_resnet_v2/InceptionResNetV2) model trained on ImageNet with the top layer removed and global pooling applied to the last convolutional layer so that the output is a vector. `pooling` can be `\"max\"` or `\"avg\"`.  \n\nThe rest of the model (the RNN, word embeddings and so on) is referred to as 'decoder' below for simplicity (even though that is not what is typically called a decoder).  \n\nThe decoder can be defined with the `define_decoder_model` function. It needs to be passed the features size, vocabulary size, embedding size and some options for the RNN module. The first two can be obtained via the API from preprocessed data; docs are available for the `RNNOptions` (as well as for every public object from **nic**), use `help(nic.RNNOptions)` in an interpreter. The embedding size defaults to the RNN's hidden size.  \n\n```python\ndata_dir = r\"data\"\nrnn_options = nic.RNNOptions(size=256)\nembedding_size = None\ndecoder = nic.define_decoder_model(\n    nic.dp.features_size(data_dir),\n    nic.dp.vocabulary_size(data_dir),\n    rnn_options,\n    embedding_size,\n    name=\"nic-decoder\"\n)\n```\n\nThe two modules can be connected into a single model:  \n\n```python\nmodel = nic.connect(\n    decoder,\n    image_shape=(299, 299, 3),\n    encoder=encoder,\n    name=\"nic\"\n)\n```\n\nThe `image_shape` argument must be a three-tuple of integers - the shape of the input images, as expected by the encoder.  \n\nIf the encoder module is going to be the default one - Inception ResNet v2, the model can be defined like so:  \n\n```python\nmodel = nic.define_model(nic.dp.vocabulary_size(data_dir),\n                         rnn_options,\n                         embedding_size,\n                         pooling)\n```\n\n### Training the model\n\nThe model, or the decoder module only, can be trained on preprocessed MSCOCO data. First, the model (or decoder) needs to be compiled:  \n\n```python\ncompiled_model = nic.compile_model(\n    model,\n    learning_rate=0.0001\n)\n```\n\nA compiled model can be trained with `train_model`:  \n\n```python\ncheckpoint_dir = r\"training/checkpoints\"\ntensor_board_dir = r\"training/tensor_board\"\n\nhistory, test_metrics = nic.train_model(\n    model=compiled_model,\n    path_to_data=data_dir,\n    is_decoder_only=False,\n    batch_size=32,\n    buffer_size=1024,\n    tensor_board_dir=tensor_board_dir,\n    tensor_board_update_freq=\"epoch\",\n    checkpoint_dir=checkpoint_dir,\n    checkpoint_freq=\"epoch\",\n    learning_rate_decay=0.9,\n    decay_patience=3,\n    perplexity_delta=0.001,\n    min_learning_rate=0.,\n    early_stop_patience=3,\n    max_epochs=10,\n    shuffle_for_each_epoch=False,\n    initial_epoch=0\n)\n```\n\nThis function trains the compiled model for at most `max_epochs` epochs, possibly shuffling the train data prior to each epoch (`shuffle_for_each_epoch`).  Resuming a training process is as easy as setting `initial_epoch` to the number of the last completed epoch and **increasing** `max_epochs`.   \n\nThe initial learning rate is the compiled model's learning rate, if the process is started from scratch; restored models (mention in a bit) come with their optimizers which include the latest learning rate. If the validation perplexity does not improve with at least `perplexity_delta` for `decay_patience` epochs in a row, the learning rate is reduced my multiplying it with `learning_rate_decay` ($lr = decay * lr$​​). If `early_stop_patience` learning rate changes still lead to no perplexity improvement (or the loss becomes NaN), the training process is terminated.  \n\nTensorBoard logs go to `tensor_board_dir` with `tensor_board_update_freq` frequency. Checkpoints (`SavedModel`s) go to `checkpoint_dir` with `checkpoint_freq` frequency.  \n\n`buffer_size` is the size of the buffer used to shuffle the train data before training is started.  \n\nMore details are available in the function's docs (`help(nic.train_model)`).  \n\nA model checkpoint can be restored like so:  \n\n```python\ncompiled_model = nic.restore_model(checkpoint_dir, restore_best=False)\n```\n\nSetting `restore_best` to `True` would restore the model with the best validation perplexity. Otherwise, the latest checkpoint is loaded.  \n\n### Evaluating the model\n\nA compiled model (which can also be the decoder module only) can be evaluated by computing its [BLEU-4](https://aclanthology.org/P02-1040.pdf) score:  \n\n```python\nmeta_tokens = nic.dp.MetaTokens()\nbleu_score = nic.bleu_score_of(\n    compiled_model,\n    is_decoder_only=False,\n    path_to_data=data_dir,\n    batch_size=32,\n    data_type=\"test\",\n    meta_tokens=meta_tokens,\n    caption_limit=100,\n    verbose=True\n)\n```\n\n`data_type` can also be `\"val\"` or even `\"train\"`. `meta_tokens` should be the meta tokens used when preprocessing data, which are typically the default ones so this can be omitted. `caption_limit` is a limit for the captions generated from `data_type` images. Omitting it means that there is no limit which is not a good idea for models that have not been trained for much time (as the captions are still pretty random and can be very long).  \n\n### Generating captions\n\nCaptions can be generated by using `nic.CaptionGenerator`. It can be created from the entire model or the decoder module only.  \n\nCreating it from the decoder is restrictive as this means it will need to be fed image features, as returned by the encoder. This is useful when training the decoder as we can evaluate it without needing the images (which take up a lot of space).      \n\nCreating it from the entire model is useful when evaluating the model or at inference time, as we will need to process images (not image features). Here's an example:  \n\n```python\nimage_options = nic.dp.ImageOptions()\ngenerator = nic.CaptionGenerator(\n    compiled_model,\n    meta_tokens=meta_tokens,\n    tokenizer=nic.dp.load_tokenizer(data_dir),\n    is_decoder_only=False,\n    image_options=image_options\n)\n```\n\nAgain, `image_options` should be the image options used when preprocessing the data (same for `meta_tokens`).  \n\nA `nic.CaptionGenerator` instance generates captions for batches of images. A batch is represented as a `tf.Tensor` of:  \n\n* image paths or 3D image tensors, when the instance is created from the entire model\n* features vectors, when the instance is created from the decoder module only\n\nFor example, we can call the above generator on a batch of image paths like this:  \n\n```python\nimport tensorflow as tf\n\nimage_paths = [\n    \"images/cat.jpg\",\n    \"images/car.jpg\",\n]\ncaptions = generator(tf.constant(image_paths), limit=None)\n```\n\nA list of captions (lists of `str` tokens) is returned. To limit the length of the captions, we can set `limit`.   \n\nThe following example shows a batch of 3D image tensors being passed to the generator:  \n\n```python\nimages, count = nic.dp.load_images(data_dir, type=\"val\")\nimages = images.batch(10)\nimages_batch, ids_batch = next(iter(images))\ncaptions = generator(images_batch, limit=100)\n```\n\nSimilarly, we could create the generator from the decoder module:  \n\n```python\ngenerator = nic.CaptionGenerator(\n    decoder,\n    meta_tokens=meta_tokens,\n    tokenizer=nic.dp.load_tokenizer(data_dir),\n    is_decoder_only=True\n)\n```\n\nThere's no need for image options as this generator will be working with image features only:  \n\n```python\nimages, count = nic.dp.load_images(data_dir, type=\"val\", load_as_features=True)\nimages = images.batch(10)\nimages_batch, ids_batch = next(iter(images))\ncaptions = generator(images_batch, limit=100)\n```\n\nOnce we have an iterable of batches, we can create a Python generator that calls the caption generator on each batch:  \n\n```python\nfor captions_batch in nic.generate_captions(images, generator, limit=100):\n    pass\n```\n\nThere's also a convenience function that creates the caption generator and then returns a Python generator that calls the caption generator on each batch of tensors in an iterable:  \n\n```python\nbatches_of_captions = list(nic.generate_captions_from_tensors(\n    images,\n    decoder,\n    meta_tokens=meta_tokens,\n    tokenizer=nic.dp.load_tokenizer(data_dir),\n    is_decoder_only=True,\n    caption_limit=100,\n))\n```\n\nFinally, there's a high-level function that generates captions when given image paths. We also need to give it the entire model, the path to the preprocessed data, meta tokens and image options (if they are not the default ones), as well as the batch size to use and a caption limit:  \n\n```python\nimages_paths = [\n    \"images/cat.jpg\",\n    \"images/car.jpg\",\n    \"images/nature.jpg\",\n]\ncaptions = nic.generate_captions_from_paths(\n    images_paths,\n    compiled_model,\n    path_to_data=data_dir,\n    batch_size=32,\n    meta_tokens=meta_tokens,\n    image_options=image_options,\n    caption_limit=100\n)\nimage_captions = dict(zip(image_paths, captions))\n```\n\nThe returned value is a Python generator which yields `str`s - the captions generated for the given images, in the same order. In the example above we pair each of the paths with the corresponding caption and create a mapping from the pairs.  \n\n<a name=\"training-on-colab\"></a>\n\n## Training on Google Colab\n\n[Google Colab](https://colab.research.google.com/) offers a Python environment with preinstalled packages like TensorFlow. It is also possible to request a GPU for a user allocated runtime. The runtimes have limited resources and even though Google Drive can be mounted, it most definitely wouldn't fit the entire MSCOCO dataset (the images in particular).  \n\nTo take advantage of Colab, we can:  \n\n* preprocess the dataset on our machines once\n* create an archive file containing image features\n* upload it to Google Drive\n* extract the features into the runtime\n* train and evaluate a model using a GPU\n\nIn fact, the *data* directory contains preprocessed MSCOCO data with image features extracted with Inception ResNet v2. Each of the two archives - *max.zip* and *avg.zip* - is a separate instance of preprocessed data; the name indicates the global pooling applied to the output of the last convolutional block when extracting image features. The [neural_image_caption](neural_image_caption.ipynb) notebook can be used with any of the archive files to train and evaluate the decoder module of a model on Google Colab.  \n\n<a name=\"References\"></a>\n\n## References\n\n[O. Vinyals, A. Toshev, S. Bengio and D. Erhan](https://ieeexplore.ieee.org/document/7505636?denied=), \"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 652-663, 1 April 2017, doi: 10.1109/TPAMI.2016.2587640.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/StiliyanDr/neural-image-caption",
    "keywords": "neural,image,caption,mscoco,coco",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "nic",
    "package_url": "https://pypi.org/project/nic/",
    "platform": "",
    "project_url": "https://pypi.org/project/nic/",
    "project_urls": {
      "Bug Tracker": "https://github.com/StiliyanDr/neural-image-caption/issues",
      "Homepage": "https://github.com/StiliyanDr/neural-image-caption"
    },
    "release_url": "https://pypi.org/project/nic/0.0.1/",
    "requires_dist": [
      "nltk",
      "numpy",
      "tensorflow",
      "tqdm"
    ],
    "requires_python": ">=3.6",
    "summary": "Simple API for neural image captioning with MSCOCO data",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11370155,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d23b2001d46ce7b6acca9be02384c473ad094b29efe831cb62afe8f6bba3295a",
          "md5": "aeee6064491d41368efcc93ccbaf6448",
          "sha256": "4a3623136adf3df98f9cea6a092bf5d4a979c752c8e187821ec8f0697e89204b"
        },
        "downloads": -1,
        "filename": "nic-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "aeee6064491d41368efcc93ccbaf6448",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 30321,
        "upload_time": "2021-09-05T15:15:09",
        "upload_time_iso_8601": "2021-09-05T15:15:09.096004Z",
        "url": "https://files.pythonhosted.org/packages/d2/3b/2001d46ce7b6acca9be02384c473ad094b29efe831cb62afe8f6bba3295a/nic-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8003801f0ab9f84a19cb2819a5245f25d1def1ca6911041a912736be0cfb8825",
          "md5": "b501a62699bb1062601c7a4d336ac234",
          "sha256": "7f48a720a0ca868509f795f7f88c10d8bb2982530d387e1dbb994820a5d64f0d"
        },
        "downloads": -1,
        "filename": "nic-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b501a62699bb1062601c7a4d336ac234",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 25419,
        "upload_time": "2021-09-05T15:15:10",
        "upload_time_iso_8601": "2021-09-05T15:15:10.880805Z",
        "url": "https://files.pythonhosted.org/packages/80/03/801f0ab9f84a19cb2819a5245f25d1def1ca6911041a912736be0cfb8825/nic-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d23b2001d46ce7b6acca9be02384c473ad094b29efe831cb62afe8f6bba3295a",
        "md5": "aeee6064491d41368efcc93ccbaf6448",
        "sha256": "4a3623136adf3df98f9cea6a092bf5d4a979c752c8e187821ec8f0697e89204b"
      },
      "downloads": -1,
      "filename": "nic-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "aeee6064491d41368efcc93ccbaf6448",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 30321,
      "upload_time": "2021-09-05T15:15:09",
      "upload_time_iso_8601": "2021-09-05T15:15:09.096004Z",
      "url": "https://files.pythonhosted.org/packages/d2/3b/2001d46ce7b6acca9be02384c473ad094b29efe831cb62afe8f6bba3295a/nic-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8003801f0ab9f84a19cb2819a5245f25d1def1ca6911041a912736be0cfb8825",
        "md5": "b501a62699bb1062601c7a4d336ac234",
        "sha256": "7f48a720a0ca868509f795f7f88c10d8bb2982530d387e1dbb994820a5d64f0d"
      },
      "downloads": -1,
      "filename": "nic-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "b501a62699bb1062601c7a4d336ac234",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 25419,
      "upload_time": "2021-09-05T15:15:10",
      "upload_time_iso_8601": "2021-09-05T15:15:10.880805Z",
      "url": "https://files.pythonhosted.org/packages/80/03/801f0ab9f84a19cb2819a5245f25d1def1ca6911041a912736be0cfb8825/nic-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}