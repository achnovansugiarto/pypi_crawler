{
  "info": {
    "author": "ZhouYang Luo",
    "author_email": "zhouyang.luo@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# rapidnlp-datasets\n\n![Python package](https://github.com/naivenlp/rapidnlp-datasets/workflows/Python%20package/badge.svg)\n[![PyPI version](https://badge.fury.io/py/rapidnlp-datasets.svg)](https://badge.fury.io/py/rapidnlp-datasets)\n[![Python](https://img.shields.io/pypi/pyversions/rapidnlp-datasets.svg?style=plastic)](https://badge.fury.io/py/rapidnlp-datasets)\n\n\nData pipelines for both **TensorFlow** and **PyTorch** !\n\nIf you want to load public datasets, try:\n\n* [tensorflow/datasets](https://github.com/tensorflow/datasets)\n* [huggingface/datasets](https://github.com/huggingface/datasets)\n\nIf you want to load local, personal dataset with minimized boilerplate, use **rapidnlp-datasets**!\n\n## installation\n\n```bash\npip install -U rapidnlp-datasets\n```\n\n## Usage\n\nHere are few examples to show you how to use this library.\n\n* [QuickStart: Sequence Classification Task](#sequence-classification-quickstart)\n* [QuickStart: Question Answering Task](#question-answering-quickstart)\n* [QuickStart: Token Classification Task](#token-classification-quickstart)\n* [QuickStart: Masked Language Model Task](#masked-language-model-quickstart)\n* [QuickStart: SimCSE(Sentence Embedding)](#simcse-quickstart)\n\n### sequence-classification-quickstart\n\n```python\nimport torch\nfrom tokenizers import BertWordPieceTokenizer\nfrom rapidnlp_datasets import DatasetForSequenceClassification\nfrom rapidnlp_datasets.tf import TFDatasetForSequenceClassifiation\n\n# build dataset\ntokenizer = BertWordPieceTokenizer.from_file(\"testdata/vocab.txt\")\ndataset = DatasetForSequenceClassification(tokenizer)\ndataset.add_jsonl_files(input_files=[\"testdata/sequence_classification.jsonl\"])\n\n# convert to tf.data.Dataset\ntf_dataset = dataset.to_tf_dataset(batch_size=32)\nfor idx, batch in enumerate(iter(tf_dataset)):\n    print(\"No.{} batch: \\n{}\".format(idx, batch))\n\n# save tfrecord\ndataset.save_tfrecord(\"testdata/sequence_classification.tfrecord\")\n# build dataset from tfrecord files\ndataset = TFDatasetForSequenceClassifiation.from_tfrecord_files(\"testdata/sequence_classification.tfrecord\")\nfor idx, batch in enumerate(iter(dataset)):\n    print(\"No.{} batch: \\n{}\".format(idx, batch))\n\n# convert to torch.utils.data.Dataset\npt_dataset = dataset.to_pt_dataset()\ndataloader = torch.utils.data.DataLoader(\n    pt_dataset, num_workers=2, shuffle=True, batch_size=32, collate_fn=pt_dataset.batch_padding_collator\n)\nfor idx, batch in enumerate(dataloader):\n    print(\"No.{} batch: \\n{}\".format(idx, batch))\n```\n\n### question-answering-quickstart\n\n```python\nimport torch\nfrom tokenizers import BertWordPieceTokenizer\nfrom rapidnlp_datasets import DatasetForQuestionAnswering\nfrom rapidnlp_datasets.tf import TFDatasetForQuestionAnswering\n\n# build dataset\ntokenizer = BertWordPieceTokenizer.from_file(\"testdata/vocab.txt\")\ndataset = DatasetForQuestionAnswering(tokenizer)\ndataset.add_jsonl_files(input_files=\"testdata/qa.jsonl\")\n\n# convert to tf.data.Dataset\ntf_dataset = dataset.to_tf_dataset()\nfor idx, batch in enumerate(iter(tf_dataset)):\n    print(\"NO.{} batch: \\n{}\".format(idx, batch))\n\n# save to tfrecord\ndataset.save_tfrecord(\"testdata/qa.tfrecord\")\n\n# build dataset from tfrecord files\ntf_dataset = TFDatasetForQuestionAnswering.from_tfrecord_files(\n    \"testdata/qa.tfrecord\", \n    batch_size=32, \n    padding=\"batch\"\n)\nfor idx, batch in enumerate(iter(tf_dataset)):\n    print()\n    print(\"No.{} batch: \\n{}\".format(idx, batch))\n\n# convert to torch.utils.data.Dataset\npt_dataset = dataset.to_pt_dataset()\ndataloader = torch.utils.data.DataLoader(\n    pt_dataset,\n    batch_size=32,\n    collate_fn=pt_dataset.batch_padding_collator,\n)\nfor idx, batch in enumerate(dataloader):\n    print(\"No.{} batch: \\n{}\".format(idx, batch))\n```\n\n\n### token-classification-quickstart\n\n```python\nimport torch\nfrom tokenizers import BertWordPieceTokenizer\nfrom rapidnlp_datasets import DatasetForTokenClassification\nfrom rapidnlp_datasets.tf import TFDatasetForTokenClassification\n\n# build dataset\ntokenizer = BertCharLevelTokenizer.from_file(\"testdata/vocab.txt\")\ndataset = DatasetForTokenClassification(tokenizer)\ndataset.add_jsonl_files(\"testdata/token_classification.jsonl\", label2id=_label_to_id)\n\n# conver to tf.data.Dataset\ntf_dataset = dataset.to_tf_dataset()\nfor idx, batch in enumerate(iter(tf_dataset)):\n    print(\"No.{} batch:\\n{}\".format(idx, batch))\n\n# save dataset to tfrecord\ndataset.save_tfrecord(\"testdata/token_classification.tfrecord\")\n# build dataset from tfrecord files\ntf_dataset = TFDatasetForTokenClassification.from_tfrecord_files(\n    input_files=\"testdata/token_classification.tfrecord\",\n    batch_size=4,\n)\nfor idx, batch in enumerate(iter(tf_dataset)):\n    print(\"No.{} batch:\\n{}\".format(idx, batch))\n\n# convert to torch.utils.data.Dataset\npt_dataset = dataset.to_pt_dataset()\ndataloader = torch.utils.data.DataLoader(\n    pt_dataset, num_workers=1, batch_size=4, collate_fn=pt_dataset.batch_padding_collator\n)\nfor idx, batch in enumerate(dataloader):\n    print(\"No.{} batch:\\n{}\".format(idx, batch))\n```\n\n### masked-language-models-quickstart\n\n```python\nimport torch\nfrom tokenizers import BertWordPieceTokenizer\nfrom rapidnlp_datasets import DatasetForMaskedLanguageModel\nfrom rapidnlp_datasets.tf import TFDatasetForMaksedLanguageModel\n\n# build dataset\ntokenizer = BertWordPieceTokenizer.from_file(\"testdata/vocab.txt\")\ndataset = DatasetForMaskedLanguageModel(tokenizer)\ndataset.add_jsonl_files(input_files=[\"testdata/mlm.jsonl\"])\ndataset.add_text_files(input_files=[\"/path/to/text/files\"])\n\n# convert to tf.data.Dataset\ntf_dataset = dataset.to_tf_dataset(batch_size=4)\nfor idx, batch in enumerate(iter(tf_dataset)):\n    print(\"No.{} batch:\\n{}\".format(idx, batch))\n\n# save dataset as tfrecord\ndataset.save_tfrecord(\"testdata/mlm.tfrecord\")\n# load tf.data.Dataset from tfrecord files\ndataset = TFDatasetForMaksedLanguageModel.from_tfrecord_files(input_files=\"testdata/mlm.tfrecord\", batch_size=4)\nfor idx, batch in enumerate(iter(dataset)):\n    print(\"No.{} batch:\\n{}\".format(idx, batch))\n\n# convert to torch.utils.data.Dataset\npt_dataset = dataset.to_pt_dataset()\n# build dataloader\ndataloader = torch.utils.data.DataLoader(\n    pt_dataset, batch_size=4, num_workers=1, collate_fn=pt_dataset.batch_padding_collator\n)\nfor idx, batch in enumerate(dataloader):\n    print(\"No.{} batch:\\n{}\".format(idx, batch))\n\n````\n\n### simcse-quickstart\n\n```python\nimport torch\nfrom tokenizers import BertWordPieceTokenizer\nfrom rapidnlp_datasets import DatasetForSimCSE\nfrom rapidnlp_datasets.tf import TFDatasetForSimCSE\n\n# build dataset\ndataset = DatasetForSimCSE(\n    tokenizer=BertWordPieceTokenizer.from_file(\"testdata/vocab.txt\"),\n    with_positive_sequence=False,\n    with_negative_sequence=False,\n)\ndataset.add_jsonl_files(\"testdata/simcse.jsonl\")\n\n# convert to tf.data.Dataset\ntf_dataset = dataset.to_tf_dataset()\nfor idx, batch in enumerate(iter(tf_dataset)):\n    print()\n    print(\"No.{} batch: \\n{}\".format(idx, batch))\n\n# save to tfrecord\ndataset.save_tfrecord(\"testdata/simcse.tfrecord\")\n# build dataset from tfrecord files\ntf_dataset = TFDatasetForSimCSE.from_tfrecord_files(\n    \"testdata/simcse.tfrecord\",\n    with_positive_sequence=False,\n    with_negative_sequence=False,\n)\nfor idx, batch in enumerate(iter(tf_dataset)):\n    print(\"No.{} batch: \\n{}\".format(idx, batch))\n\n# convert to torch.utils.data.Dataset\npt_dataset = dataset.to_pt_dataset()\ndataloader = torch.utils.data.DataLoader(\n    pt_dataset, num_workers=2, shuffle=True, batch_size=32, collate_fn=pt_dataset.batch_padding_collator\n)\nfor idx, batch in enumerate(dataloader):\n    print(\"No.{} batch: \\n{}\".format(idx, batch))\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/naivenlp/rapidnlp-datasets",
    "keywords": "",
    "license": "Apache Software License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "rapidnlp-datasets",
    "package_url": "https://pypi.org/project/rapidnlp-datasets/",
    "platform": "",
    "project_url": "https://pypi.org/project/rapidnlp-datasets/",
    "project_urls": {
      "Homepage": "https://github.com/naivenlp/rapidnlp-datasets"
    },
    "release_url": "https://pypi.org/project/rapidnlp-datasets/0.2.0/",
    "requires_dist": [
      "tokenizers",
      "torch ; extra == 'pt'",
      "torchaudio ; extra == 'pt'",
      "torchvision ; extra == 'pt'",
      "tensorflow ; extra == 'tf'"
    ],
    "requires_python": "",
    "summary": "Data pipelines for TensorFlow and PyTorch.",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12753884,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f73724aeb2fd295c6b86552d2f6febfcd5823b6718a481f49f5b36ec27784eb8",
          "md5": "1f71a08b64fd3bf5d801735e1224e3d1",
          "sha256": "97ef1d6ac5fd568d4e15b604c01972ab7266400ba43be405f97a9023f0d0fe03"
        },
        "downloads": -1,
        "filename": "rapidnlp_datasets-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1f71a08b64fd3bf5d801735e1224e3d1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 41285,
        "upload_time": "2021-12-08T07:24:34",
        "upload_time_iso_8601": "2021-12-08T07:24:34.375747Z",
        "url": "https://files.pythonhosted.org/packages/f7/37/24aeb2fd295c6b86552d2f6febfcd5823b6718a481f49f5b36ec27784eb8/rapidnlp_datasets-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a0b017a6af35bbaba84f928fbed9264748e79a487ef2ea23e3bd2b9e52174aec",
          "md5": "12adb74382825cb3139a2f212f3b2349",
          "sha256": "47bd08f73f40685443a4326ce0f3aa7fb2737dd884a7ea6c3f996ecb2417f84e"
        },
        "downloads": -1,
        "filename": "rapidnlp-datasets-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "12adb74382825cb3139a2f212f3b2349",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 22608,
        "upload_time": "2021-12-08T07:24:36",
        "upload_time_iso_8601": "2021-12-08T07:24:36.305073Z",
        "url": "https://files.pythonhosted.org/packages/a0/b0/17a6af35bbaba84f928fbed9264748e79a487ef2ea23e3bd2b9e52174aec/rapidnlp-datasets-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2d1cbd87d573448173f6e7566d74baaa51a69e472b1288ff685795b404ca15a9",
          "md5": "bef4b7b0858dd8ca02b0842f7ee4c38e",
          "sha256": "95cf8fcb5b2cda5945f1eea50b9a9eb976c5ff21b5f81e244c5b6846d95b8ac5"
        },
        "downloads": -1,
        "filename": "rapidnlp_datasets-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bef4b7b0858dd8ca02b0842f7ee4c38e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 44948,
        "upload_time": "2022-02-01T11:09:09",
        "upload_time_iso_8601": "2022-02-01T11:09:09.616929Z",
        "url": "https://files.pythonhosted.org/packages/2d/1c/bd87d573448173f6e7566d74baaa51a69e472b1288ff685795b404ca15a9/rapidnlp_datasets-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "51ca359c799445c9d6e67b8e64f213242bc44588a0d189882ac6711f9aa76503",
          "md5": "3598118ae2650c281bc98bcdc5e7fd05",
          "sha256": "4ae0e042a8f508281af2ff0c450a5fb16658d753c7d7b3486585cfcf5dcf2054"
        },
        "downloads": -1,
        "filename": "rapidnlp-datasets-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3598118ae2650c281bc98bcdc5e7fd05",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 27305,
        "upload_time": "2022-02-01T11:09:11",
        "upload_time_iso_8601": "2022-02-01T11:09:11.234896Z",
        "url": "https://files.pythonhosted.org/packages/51/ca/359c799445c9d6e67b8e64f213242bc44588a0d189882ac6711f9aa76503/rapidnlp-datasets-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2d1cbd87d573448173f6e7566d74baaa51a69e472b1288ff685795b404ca15a9",
        "md5": "bef4b7b0858dd8ca02b0842f7ee4c38e",
        "sha256": "95cf8fcb5b2cda5945f1eea50b9a9eb976c5ff21b5f81e244c5b6846d95b8ac5"
      },
      "downloads": -1,
      "filename": "rapidnlp_datasets-0.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "bef4b7b0858dd8ca02b0842f7ee4c38e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 44948,
      "upload_time": "2022-02-01T11:09:09",
      "upload_time_iso_8601": "2022-02-01T11:09:09.616929Z",
      "url": "https://files.pythonhosted.org/packages/2d/1c/bd87d573448173f6e7566d74baaa51a69e472b1288ff685795b404ca15a9/rapidnlp_datasets-0.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "51ca359c799445c9d6e67b8e64f213242bc44588a0d189882ac6711f9aa76503",
        "md5": "3598118ae2650c281bc98bcdc5e7fd05",
        "sha256": "4ae0e042a8f508281af2ff0c450a5fb16658d753c7d7b3486585cfcf5dcf2054"
      },
      "downloads": -1,
      "filename": "rapidnlp-datasets-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "3598118ae2650c281bc98bcdc5e7fd05",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 27305,
      "upload_time": "2022-02-01T11:09:11",
      "upload_time_iso_8601": "2022-02-01T11:09:11.234896Z",
      "url": "https://files.pythonhosted.org/packages/51/ca/359c799445c9d6e67b8e64f213242bc44588a0d189882ac6711f9aa76503/rapidnlp-datasets-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}