{
  "info": {
    "author": "Patrick El Hage",
    "author_email": "patrickelhageuniv@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# Table of contents\n \n- [Overview](#overview)\n- [Flow configuration documentation](#flow-configuration-documentation)\n- [Implementer configuration documentation](#implementer-configuration-documentation)\n- [API documentation](#api-documentation)\n- [Processing functions](#processing-functions)\n\n# Overview\n\n## Install\n\nIt is highly advised to install the ADF framework in a virtual env running `python3.7`, as this is (currently) the only supported python version for the AWS implementer. In addition, make sure to properly set your `PYSPARK_PYTHON` path for full spark support :\n\n```\nmkvirtualenv adf -p `which python3.7`\nexport PYSPARK_PYTHON=`which python3`\npip install adf\n```\n\n## ADF in a nutshell\n\n**Abstract Data Flows (ADF)** is a framework that provides data platform automation without infrastructure commitment. Data processing flows are defined in an infrastructure agnostic manner, and are then plugged into any **implementer** configuration of your choice. This provides all the major benefits of automation (namely, instantly deployable production ready infrastructure) while avoiding its major pitfall : being tied to your choice of infrastructure.\n\n## Getting started\n\nFor an easy-to-follow tutorial, please refer to the accompanying [adf_app](https://github.com/ticowiko/adf_app) sister repository and its associated README.\n\n# Flow configuration documentation\n\n- [Global configuration](#global-configuration)\n- [Modules](#modules)\n- [Flows](#flows)\n  - [Starting steps](#starting-steps)\n    - [Landing step](#landing-step)\n    - [Combination step](#combination-step)\n    - [Reception step](#reception-step)\n  - [Non-starting step](#non-starting-step)\n  - [Metadata configuration](#metadata-configuration)\n\nEach flow configuration file defines an **ADF collection**. The configuration can be broken down into 3 categories of parameters.\n\n## Global configuration\n\n| Parameter               | Obligation                          | Description                          |\n|-------------------------|-------------------------------------|--------------------------------------|\n| `name`                  | **REQUIRED**                        | The name for the ADF collection.     |\n| `BATCH_ID_COLUMN_NAME`  | **OPTIONAL**, advised not to change | Column name to store batch IDs.      |\n| `SQL_PK_COLUMN_NAME`    | **OPTIONAL**, advised not to change | Column name to store a PK if needed. |\n| `TIMESTAMP_COLUMN_NAME` | **OPTIONAL**, advised not to change | Column name to store timestamps.     |\n\nFor example :\n\n```yaml\nname: collection-name\nBATCH_ID_COLUMN_NAME: modified-batch-id-column-name\nSQL_PK_COLUMN_NAME: modified-sql-pk-column-name\nTIMESTAMP_COLUMN_NAME: modified-timestamp-column-name\n```\n\n## Modules\n\nModules are listed under the `modules` parameter. Each module must define the following parameters :\n\n| Parameter     | Obligation   | Description                    |\n|---------------|--------------|--------------------------------|\n| `name`        | **REQUIRED** | Alias to refer to this module. |\n| `import_path` | **REQUIRED** | Valid python import path.      |\n\nFor example :\n\n```yaml\nmodules:\n  - name: module-alias\n    import_path: package.module\n```\n\n## Flows\n\nThe actual data flows are defined under the `flows` parameter, as a list of named flows, each containing a list of named steps.\n\n| Parameter | Obligation   | Description        |\n|-----------|--------------|--------------------|\n| `name`    | **REQUIRED** | Unique Identifier. |\n| `steps`   | **REQUIRED** | List of steps.     |\n\nFor example :\n\n```yaml\ncollection: collection-name\nmodules:\n  - [...]\n  - [...]\nflows:\n  name: flow-name\n  steps:\n    - [...] # Step config\n    - [...] # Step config\n    - [...] # Step config\n```\n\n### Starting steps\n\nThe first step in a flow is known as a starting step. There are 3 types of starting steps.\n\n#### Landing step\n\nThere are passive steps that define where input data is expected to be received. As such, they cannot define a processing function, metadata, or any custom flow control mechanisms.\n\n| Parameter          | Obligation   | Description                            |\n|--------------------|--------------|----------------------------------------|\n| `start`            | **REQUIRED** | Must be set to `landing`.              |\n| `layer`            | **REQUIRED** | Data layer name.                       |\n| `name`             | **REQUIRED** | Identifier unique within this flow.    |\n| `version`          | **OPTIONAL** | Data version ID.                       |\n| `func`             | **BANNED**   | Cannot define a processing function.   |\n| `func_kwargs`      | **BANNED**   | Cannot define function keywords.       |\n| `meta`             | **BANNED**   | Cannot define custom metadata.         |\n| `sequencer`        | **BANNED**   | Cannot define a custom sequencer.      |\n| `data_loader`      | **BANNED**   | Cannot define a custom data loader.    |\n| `batch_dependency` | **BANNED**   | Cannot define custom batch dependency. |\n\nFor example :\n\n```yaml\nname: collection-name\nflows:\n  - name: flow-name\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n        version: source-data-version-name\n```\n\n#### Combination step\n\nThese steps define the start of a flow that takes as input multiple previous steps.\n\n| Parameter          | Obligation   | Description                               |\n|--------------------|--------------|-------------------------------------------|\n| `start`            | **REQUIRED** | Must be set to `combination`.             |\n| `layer`            | **REQUIRED** | Data layer name.                          |\n| `name`             | **REQUIRED** | Identifier unique within this flow.       |\n| `input_steps`      | **REQUIRED** | Input steps to combine.                   |\n| `version`          | **OPTIONAL** | Data version ID.                          |\n| `func`             | **OPTIONAL** | The processing function.                  |\n| `func_kwargs`      | **OPTIONAL** | Extra kwargs to pass to the function.     |\n| `meta`             | **OPTIONAL** | Metadata constraints on output.           |\n| `sequencer`        | **OPTIONAL** | Defined for the step itself.              |\n| `data_loader`      | **OPTIONAL** | Defined for the step and the input steps. |\n| `batch_dependency` | **OPTIONAL** | Defined only for the input steps.         |\n\nA minimal working example :\n\n```yaml\nname: collection-name\nmodules:\n  - name: module-alias\n    import_path: package.module\nflows:\n  - name: flow-name-0\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n  - name: flow-name-1\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n  - name: combination-flow\n    steps:\n      - start: combination\n        layer: layer-name\n        name: combination-step-name\n        input_steps:\n          - flow_name: flow-name-0\n            step_name: landing-step\n          - flow_name: flow-name-1\n            step_name: landing-step\n        func: # REQUIRED if there are more than one input steps\n          load_as: module\n          params:\n            module: module-alias\n            name: processing_function_name\n```\n\nAn example with all optional configurations :\n\n```yaml\nname: collection-name\nmodules:\n  - name: module-alias\n    import_path: package.module\nflows:\n  - name: flow-name-0\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n  - name: flow-name-1\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n  - name: combination-flow\n    steps:\n      - start: combination\n        layer: layer-name\n        name: combination-step-name\n        version: version-name\n        input_steps:\n          - flow_name: flow-name-0\n            step_name: landing-step\n            data_loader:\n              module: module-alias\n              class_name: DataLoaderClassName\n              params: [...] # class init params\n            batch_dependency:\n              module: module-alias\n              class_name: BatchDependencyClassName\n              params: [...] # class init params\n          - flow_name: flow-name-1\n            step_name: landing-step\n            data_loader:\n              module: module-alias\n              class_name: DataLoaderClassName\n              params: [...] # class init params\n            batch_dependency:\n              module: module-alias\n              class_name: BatchDependencyClassName\n              params: [...] # class init params\n        func:\n          load_as: module\n          params:\n            module: module-alias\n            name: processing_function_name\n        func_kwargs: [...] # kwargs dictionary\n        meta: [...] # metadata for output\n        sequencer:\n          module: module-alias\n          class_name: SequencerClassName\n          params: [...] # class init params\n        data_loader:\n          module: module-alias\n          class_name: DataLoaderClassName\n          params: [...] # class init params\n```\n\n#### Reception step\n\nA reception step is used when we want a processing step to output more than one data structure. When a processing step is hooked to reception steps, no data will actually be saved at the processing step itself. Instead, the reception steps will serve as storage steps. As a result, much like landing steps, they cannot define a processing function or any custom flow control mechanisms, but they can define metadata.\n\n| Parameter          | Obligation   | Description                            |\n|--------------------|--------------|----------------------------------------|\n| `start`            | **REQUIRED** | Must be set to `reception`.            |\n| `layer`            | **REQUIRED** | Data layer name.                       |\n| `name`             | **REQUIRED** | Identifier unique within this flow.    |\n| `key`              | **REQUIRED** | Keyword by which to specify this step. |\n| `input_steps`      | **REQUIRED** | Upstream steps to store results of.    |\n| `meta`             | **OPTIONAL** | Metadata constraints on incoming data. |\n| `version`          | **BANNED**   | Uses input step version.               |\n| `func`             | **BANNED**   | Cannot define a processing function.   |\n| `func_kwargs`      | **BANNED**   | Cannot define function keywords.       |\n| `sequencer`        | **BANNED**   | Cannot define a custom sequencer.      |\n| `data_loader`      | **BANNED**   | Cannot define a custom data loader.    |\n| `batch_dependency` | **BANNED**   | Cannot define custom batch dependency. |\n\nFor example :\n\n```yaml\nname: collection-name\nmodules:\n  - name: module-alias\n    import_path: package.module\nflows:\n  - name: flow-name\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n      - layer: layer-name\n        name: processing-step\n        func: # see below for example function\n          load_as: module\n          params:\n            module: module-alias\n            name: multiple_outputs\n  - name: reception-flow-0\n    steps:\n      - start: reception\n        layer: layer-name\n        name: reception-step-name\n        key: reception-key-0\n        input_steps:\n          - flow_name: flow-name\n            step_name: processing-step\n  - name: reception-flow-1\n    steps:\n      - start: reception\n        layer: layer-name\n        name: reception-step-name\n        key: reception-key-1\n        input_steps:\n          - flow_name: flow-name\n            step_name: processing-step\n```\n\nHooking a processing step into a reception step changes the expected output signature of the processing function. Instead of returning a single ADS, it must now return a dictionary whose keys correspond to the reception step keys. In our case :\n\n```python\ndef multiple_outputs(\n    ads: AbstractDataStructure,\n) -> Dict[str, AbstractDataStructure]:\n    return {\n        \"reception-key-0\": ads[ads[\"col_0\"] == 0],\n        \"reception-key-1\": ads[ads[\"col_0\"] != 0],\n    }\n```\n\n### Non-starting step\n\nA non-starting step is any step that is not the first step in a flow. It can customize any and all flow control mechanisms, as well as define a processing function and define metadata.\n\n| Parameter          | Obligation   | Description                           |\n|--------------------|--------------|---------------------------------------|\n| `layer`            | **REQUIRED** | Data layer name.                      |\n| `name`             | **REQUIRED** | Identifier unique within this flow.   |\n| `version`          | **OPTIONAL** | Data version ID.                      |\n| `func`             | **OPTIONAL** | The processing function.              |\n| `func_kwargs`      | **OPTIONAL** | Extra kwargs to pass to the function. |\n| `meta`             | **OPTIONAL** | Metadata constraints on output.       |\n| `sequencer`        | **OPTIONAL** | Defines batch sequencing.             |\n| `data_loader`      | **OPTIONAL** | Defines data loading.                 |\n| `batch_dependency` | **OPTIONAL** | Defines batch dependency.             |\n| `start`            | **BANNED**   | Must not be set.                      |\n\nA minimal working example :\n\n```yaml\nname: collection-name\nflows:\n  - name: flow-name-0\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n      - layer: layer-name\n        name: processing-step-name\n        func: # if not set, the input data is merely copied\n          load_as: eval\n          params:\n            expr: 'lambda ads: ads[ads[\"col_0\" == 0]]'\n```\n\nAn example with all optional configurations :\n\n```yaml\nname: collection-name\nmodules:\n  - name: module-alias\n    import_path: package.module\nflows:\n  - name: flow-name-0\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n      - layer: layer-name\n        name: processing-step-name\n        version: version-name\n        func:\n          load_as: eval\n          params:\n            expr: 'lambda ads: ads[ads[\"col_0\" == 0]]'\n        func_kwargs: [...] # kwargs dictionary\n        meta: [...] # metadata for output\n        sequencer:\n          module: module-alias\n          class_name: SequencerClassName\n          params: [...] # class init params\n        data_loader:\n          module: module-alias\n          class_name: DataLoaderClassName\n          params: [...] # class init params\n        batch_dependency:\n          module: module-alias\n          class_name: BatchDependencyClassName\n          params: [...] # class init params\n```\n\n### Metadata configuration\n\nMetadata is configured by specifying column names, data types, and requested behavior when missing. You can also set the default missing column behavior, as well as what to do with extra columns. All metadata parameters except for the column name are optional.\n\n| Parameter          | Values                                                      | Description                     |\n|--------------------|-------------------------------------------------------------|---------------------------------|\n| column.name        | Any                                                         | Name of the column              |\n| column.cast        | `str`,`int`,`float`,`complex`,`datetime`,`date`,`timedelta` | Data type                       |\n| column.on_missing  | `ignore`, `fail`, `fill`                                    | Missing column behavior         |\n| column.fill_val    | Any, defaults to `None`                                     | Value to fill column if missing |\n| in_partition       | `true`, `false`                                             | Whether to use in partition     |\n| on_missing_default | `ignore`, `fail`, `fill`                                    | Default missing column behavior |\n| on_extra           | `ignore`, `fail`, `cut`                                     | What to do with extra columns   |\n\nFor example :\n\n```yaml\nname: collection-name\nflows:\n  - name: flow-name\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step\n      - layer: layer-name\n        name: meta-step\n        meta:\n          columns:\n            - name: essential_column\n              cast: str\n              on_missing: fail\n              in_partition: true\n            - name: integer_column\n              cast: int\n              on_missing: fill\n              fill_val: \"FILL_VALUE\"\n            - name: weakly_defined_column\n          on_missing_default: ignore\n          on_extra: cut\n```\n\n# Implementer configuration documentation\n\n- [Local Implementer](#local-implementer)\n- [AWS Implementer](#aws-implementer)\n  - [Managed infrastructure AWS Implementer](#managed-infrastructure-aws-implementer)\n  - [Prebuilt infrastructure AWS Implementer](#prebuilt-infrastructure-aws-implementer)\n\nWhile implementer configurations are allowed to vary freely, there is one parameter they must all contain to actually specify which implementer they are destined for. Its value must be a valid python import path.\n\n| Parameter         | Description                                    |\n|-------------------|------------------------------------------------|\n| `implementer_class` | Module path followed by implementer class name |\n\nFor example, if the implementer class is defined in the module `package.module`, and the implementer class name is `ImplementerClass`, the corresponding configuration would be :\n\n```yaml\nimplementer_class: package.module.ImplementerClass\n```\n\n## Local Implementer\n\nThe local implementer requires a root path, as well as a list of layer names to associate to each layer type. The available layer types are :\n\n- 3 file based CSV data layers, each of which manipulates data differently to perform computation :\n  - `list_of_dicts` : Loads data as a list of dictionaries to perform computation.\n  - `pandas` : Loads data as pandas DataFrame to perform computation.\n  - `spark` : Loads data as a pyspark DataFrame to perform computation.\n- 2 database backed data layers, each of which uses a different database engine to perform storage and computation :\n  - `sqlite` : Uses an sqlite database to store and process data.\n  - `postgres` : Uses a postgresql database to store and process data.\n\nIf at least one `postgres` layer is used, then connection information must also be passed. Admin credentials may also be passed if one wishes for the implementer to create the database and technical user in question.\n\n| Parameter              | Obligation   | Description                                                  |\n|------------------------|--------------|--------------------------------------------------------------|\n| `implementer_class`    | **REQUIRED** | Module path followed by implementer class name               |\n| `root_path`            | **REQUIRED** | Root path to store data and state handler                    |\n| `extra_packages`       | **OPTIONAL** | List of local paths to any packages required                 |\n| `layers.list_of_dicts` | **OPTIONAL** | List of list of dict based layers                            |\n| `layers.pandas`        | **OPTIONAL** | List of pandas based layers                                  |\n| `layers.spark`         | **OPTIONAL** | List of pyspark based layers                                 |\n| `layers.sqlite`        | **OPTIONAL** | List of sqlite based layers                                  |\n| `layers.postgres`      | **OPTIONAL** | List of postgres based layers                                |\n| `postgres_config`      | **OPTIONAL** | `host`, `port`, `db`, `user`, `pw`, `admin_user`, `admin_pw` |\n\nFor example, to configure a local implementer without any postgres based layers :\n\n```yaml\nimplementer_class: ADF.components.implementers.MultiLayerLocalImplementer\nextra_packages: [.]\nroot_path: path/to/data\nlayers:\n  pandas:\n    - pandas-layer-name-0\n    - pandas-layer-name-1\n  spark:\n    - spark-layer-name-0\n    - spark-layer-name-1\n  sqlite:\n    - sqlite-layer-name-0\n    - sqlite-layer-name-1\n```\n\nTo be able to include postgres based layers, one must add connection information :\n\n```yaml\nimplementer_class: ADF.components.implementers.MultiLayerLocalImplementer\nextra_packages: [.]\nroot_path: path/to/data\npostgres_config:\n  db: adf_db           # Required\n  user: adf_user       # Required\n  pw: pw               # Required\n  host: localhost      # Optional, defaults to localhost\n  port: 5432           # Optional, defaults to 5432\n  admin_user: postgres # Optional, will be used to create db and user if needed\n  admin_pw: postgres   # Optional, will be used to create db and user if needed\nlayers:\n  pandas:\n    - pandas-layer-name-0\n    - pandas-layer-name-1\n  spark:\n    - spark-layer-name-0\n    - spark-layer-name-1\n  postgres:\n    - postgres-layer-name-0\n    - postgres-layer-name-1\n```\n\n## AWS Implementer\n\n### Managed infrastructure AWS Implementer\n\nThe AWS implementer configuration file is similar in structure to that of the local implementer. When ADF is given free rein over infrastructure deployment, individual layers carry sizing information. In addition, the state handler configuration and sizing must also be specified.\n\n| Parameter               | Obligation   | Description                                                                                                                                                                                                       |\n|-------------------------|--------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `implementer_class`     | **REQUIRED** | Module path followed by implementer class name                                                                                                                                                                    |\n| `mode`                  | **REQUIRED** | Set to `managed` to tell your implementer to handle infrastructure deployment                                                                                                                                     |\n| `name`                  | **REQUIRED** | An identifier for the implementer                                                                                                                                                                                 |\n| `log_folder`            | **REQUIRED** | A local folder in which to store subcommand logs                                                                                                                                                                  |\n| `bucket`                | **REQUIRED** | The S3 bucket used for data storage                                                                                                                                                                               |\n| `s3_prefix`             | **REQUIRED** | S3 prefix for all data and uploaded configuration                                                                                                                                                                 |\n| `state_handler`         | **REQUIRED** | `engine`, `db_name`, `db_instance_class`, `allocated_storage`                                                                                                                                                     |\n| `extra_packages`        | **OPTIONAL** | List of local paths to any additional required packages                                                                                                                                                           |\n| `lambda_layers`         | **OPTIONAL** | `sep`, `timeout`, `memory`                                                                                                                                                                                        |\n| `emr_layers`            | **OPTIONAL** | `master_instance_type`, `slave_instance_type`, `instance_count`, `step_concurrency`, `format`, `landing_format`                                                                                                   |\n| `emr_serverless_layers` | **OPTIONAL** | `initial_driver_worker_count`, `initial_driver_cpu`, `initial_driver_memory`, `initial_executor_worker_count`, `initial_executor_cpu`, `initial_executor_memory`, `max_cpu`, `max_memory`, `idle_timeout_minutes` |\n| `redshift_layers`       | **OPTIONAL** | `db_name`, `node_type`, `number_of_nodes`                                                                                                                                                                         |\n| `athena_layer`          | **OPTIONAL** | `landing_format`                                                                                                                                                                                                  |\n\nFor example :\n\n```yaml\nimplementer_class: ADF.components.implementers.AWSImplementer\nextra_packages: [.]\nmode: managed # ADF will handle infrastructure deployment\nname: implementer-name\nlog_folder: local/path/to/logs\nbucket: YOUR-BUCKET-NAME-HERE\ns3_prefix: YOUR_S3_PREFIX/\nstate_handler:\n  engine: postgres # only postgres is currently supported\n  db_name: ADF_STATE_HANDLER\n  db_instance_class: db.t3.micro\n  allocated_storage: 20\nlambda_layers:\n  lambda-layer-name:\n    sep: \",\" # separator for CSVs\n    timeout: 60\n    memory: 1024\nemr_layers:\n  heavy:\n    master_instance_type: m5.xlarge\n    slave_instance_type: m5.xlarge\n    instance_count: 1\n    step_concurrency: 5\n    format: parquet     # the format in which to store data\n    landing_format: csv # the format in which to expect data in landing steps\nemr_serverless_layers:\n  serverless:\n    initial_driver_worker_count: 1\n    initial_driver_cpu: \"1vCPU\"\n    initial_driver_memory: \"8GB\"\n    initial_executor_worker_count: 1\n    initial_executor_cpu: \"1vCPU\"\n    initial_executor_memory: \"8GB\"\n    max_cpu: \"32vCPU\",\n    max_memory: \"256GB\",\n    idle_timeout_minutes: 15,\nredshift_layers:\n  expose:\n    db_name: expose\n    number_of_nodes: 1\n    node_type: ds2.xlarge\nathena_layers:\n  dump:\n    landing_format: csv # the format in which to expect data in landing steps\n```\n\n### Prebuilt infrastructure AWS Implementer\n\nIf you wish to connect your AWS implementer to pre-existing infrastructure, you can do this by changing the implementer mode to `prebuilt`. Once the implementer setup is run, it is possible to output a `prebuilt` configuration and use it moving forward. This is the recommended usage, as `prebuilt` mode requires fewer permissions to run, as well as fewer API calls to determine the current state of the infrastructure. Unlike in `managed` mode, no sizing information is provided. Instead, we pass endpoints and various configurations that define the data layer.\n\n| Parameter               | Obligation   | Description                                                                                                                                                    |\n|-------------------------|--------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `implementer_class`     | **REQUIRED** | Module path followed by implementer class name                                                                                                                 |\n| `mode`                  | **REQUIRED** | Set to `managed` to tell your implementer to handle infrastructure deployment                                                                                  |\n| `name`                  | **REQUIRED** | An identifier for the implementer                                                                                                                              |\n| `log_folder`            | **REQUIRED** | A local folder in which to store subcommand logs                                                                                                               |\n| `bucket`                | **REQUIRED** | The S3 bucket used for data storage                                                                                                                            |\n| `s3_prefix`             | **REQUIRED** | S3 prefix for all data and uploaded configuration                                                                                                              |\n| `state_handler_url`     | **REQUIRED** | URL to state handler DB                                                                                                                                        |\n| `extra_packages`        | **OPTIONAL** | List of local paths to any additional required packages                                                                                                        |\n| `lambda_layers`         | **OPTIONAL** | `lambda_arn`, `lambda_name`, `s3_fcp_template`, `s3_icp`, `sep`, `sqs_arn`, `sqs_name`, `sqs_url`                                                              |\n| `emr_layers`            | **OPTIONAL** | `bucket`, `s3_prefix`, `cluster_id`, `cluster_arn`, `name`, `public_dns`, `log_uri`, `format`, `landing_format`                                                |\n| `emr_serverless_layers` | **OPTIONAL** | `application_id`, `bucket`, `environ`, `format`, `landing_format`, `role_arn`, `s3_fcp_template`, `s3_icp`, `s3_launcher_key`, `s3_prefix`, `venv_package_key` |\n| `redshift_layers`       | **OPTIONAL** | `table_prefix`, `endpoint`, `port`, `db_name`, `user`, `role_arn`                                                                                              |\n| `athena_layers`         | **OPTIONAL** | `bucket`, `db_name`, `landing_format`, `s3_prefix`, `table_prefix`                                                                                             |\n\nFor example :\n\n```yaml\nimplementer_class: ADF.components.implementers.AWSImplementer\nextra_packages: [.]\nmode: prebuilt # ADF will plug into pre-existing infrastructure\nname: implementer-name\nlog_folder: local/path/to/logs\nbucket: YOUR-BUCKET-NAME-HERE\ns3_prefix: YOUR_S3_PREFIX/\nstate_handler_url: postgresql://username:password@state.handler.db.url:5432/DB_NAME\nlambda_layers:\n  light:\n    lambda_arn: LAMBDA_ARN\n    lambda_name: LAMBDA_FUNCTION_NAME\n    s3_fcp_template: s3://TEMPLATE/TO/FCP/PATH/fcp.{collection_name}.yaml\n    s3_icp: s3://ICP/PATH/icp.yaml\n    sep: ',' # separator for CSVs\n    sqs_arn: SQS_ARN\n    sqs_name: SQS_QUEUE_NAME\n    sqs_url: https://url.to/sqs/queue\nemr_layers:\n  heavy:\n    bucket: YOUR-BUCKET-NAME-HERE\n    s3_prefix: S3/PREFIX/ # where to store data in the bucket\n    cluster_id: EMR_CLUSTER_ID\n    cluster_arn: EMR_CLUSTER_ARN\n    name: EMR_CLUSTER_NAME\n    public_dns: https://url.to.emr.cluster\n    log_uri: s3://PATH/TO/LOGS/\n    format: parquet     # the format in which to store data\n    landing_format: csv # the format in which to expect data in landing steps\nemr_serverless_layers:\n  serverless:\n    application_id: app-id\n    bucket: YOUR-BUCKET-NAME-HERE\n    environ:\n      AWS_DEFAULT_REGION: aws-region\n      RDS_PW: RDS_STATE_HANDLER_PASSWORD\n      REDSHIFT_PW: REDSHIFT_PASSWORD\n    format: parquet     # the format in which to store data\n    landing_format: csv # the format in which to expect data in landing steps\n    role_arn: EXECUTION_ROLE_ARN\n    s3_fcp_template: s3://TEMPLATE/TO/FCP/PATH/fcp.{collection_name}.yaml\n    s3_icp: s3://ICP/PATH/icp.yaml\n    s3_launcher_key: KEY/TO/ADF/LAUNCHER/adf-launcher.py\n    s3_prefix: S3/PREFIX/ # where to store data in the bucket\n    venv_package_key: S3/PREFIX/venv_package.tar.gz\nredshift_layers:\n  expose:\n    table_prefix: TABLE_PREFIX\n    endpoint: https://url.to.db\n    port: PORT_NUMBER\n    db_name: DB_NAME\n    user: DB_USERNAME\n    role_arn: EXECUTION_ROLE_ARN\nathena_layers:\n  dump:\n    bucket: YOUR-BUCKET-NAME-HERE\n    db_name: ATHENA_DB_NAME\n    landing_format: csv # the format in which to expect data in landing steps\n    s3_prefix: S3/PREFIX/TO/DATA/ # where to store data in the bucket\n    table_prefix: 'expose_'\n```\n\n# API documentation\n\n- [AbstractDataStructure](#abstractdatastructure)\n  - [Column manipulation methods](#column-manipulation-methods)\n  - [Data access](#data-access)\n  - [Aggregation methods](#aggregation-methods)\n- [AbstractDataColumn](#abstractdatacolumn)\n  - [Column operations](#column-operations)\n  - [Column aggregations](#column-aggregations)\n  - [Operators](#operators)\n- [AbstractDataInterface](#abstractdatainterface)\n- [AbstractStateHandler](#abstractstatehandler)\n- [ADFSequencer and ADFCombinationSequencer](#adfsequencer-and-adfcombinationsequencer)\n  - [ADFSequencer](#adfsequencer)\n  - [ADFCombinationSequencer](#adfcombinationsequencer)\n- [ADFDataLoader](#adfdataloader)\n- [ADFBatchDependencyHandler](#adfbatchdependencyhandler)\n\n## AbstractDataStructure\n\nAn **Abstract Data Structure** (ADS) provides a dataframe like API for data manipulation. This is the native input and output format for your processing functions, barring concretization. The actual execution details of the below methods will depend on which type of ADS the underlying data layer has provided us with (Pandas based, Spark based, SQL based etc.).\n\n### Column manipulation methods\n\n-----\n\n```python\ndef list_columns(self) -> List[str]\n```\n\nLists columns currently in the ADS.\n\n-----\n\n```python\ndef col_exists(self, col_name: str) -> bool\n```\n\nCheck if column `col_name` exists.\n\n-----\n\n```python\ndef prune_tech_cols(self) -> \"AbstractDataStructure\"\n```\n\nRemoves technical columns from the ADS.\n\n-----\n\n```python\ndef rename(self, names: Dict[str, str]) -> \"AbstractDataStructure\"\n```\n\nRenames columns from the keys of the `names` dictionary to the values of the `names` dictionary.\n\n-----\n\n### Data access\n\n-----\n\n```python\ndef __getitem__(\n    self, key: Union[str, AbstractDataColumn, List[str]]\n) -> Union[\"AbstractDataStructure\", AbstractDataColumn]\n```\n\n- If `key` is a string, returns the corresponding column : `ads[\"col_name\"]`\n- If `key` is an `AbstractDataColumn`, return an ADS filtered based on the truth value of the column : `ads[ads[\"col_name\"] == \"filter_value\"]`\n- If `key` is a list of strings, returns an ADS containing only the subset of columns specified in `key` : `ads[[\"col_0\", \"col_1\"]]`\n\n-----\n\n```python\ndef __setitem__(\n    self,\n    key: Union[str, Tuple[str, AbstractDataColumn]],\n    value: Union[Any, AbstractDataColumn],\n) -> None\n```\n\n- If `value` is an `AbstractDataColumn`, use it to set the specified entries in the ADS : `ads[\"col_name\"] = ads[\"col_name\"]*2`\n- If `value` is any other type, fill every specified entry with its value : `ads[\"col_name\"] = \"col_value\"`\n- If `key` is a string, set the values of the corresponding column. Creates the column if it does not already exist.\n- If `key` is a `(str, AbstractDataColumn)` type tuple, set the values of the column `key[0]` only for rows filtered by `key[1]`. Note that `key[0]` must necessarily already exist as a column. Can set using either a constant value or another column. For example:\n\n```python\nads[\"col_0\", ads[\"col_1\"] == \"FILTER_VAL\"] = \"SOME_VAL\"\nads[\"col_0\", ads[\"col_1\"] == \"FILTER_VAL\"] = ads[\"col_2\"]\n```\n\n-----\n\n```python\ndef to_list_of_dicts(self) -> List[Dict]\n```\n\nReturns a list of dictionaries, each of which corresponds to a single row in the ADS.\n\n-----\n\n### Aggregation methods\n\n-----\n\n```python\ndef __len__(self) -> int\n```\n\nReturns the number of rows in the ADS.\n\n-----\n\n```python\ndef __bool__(self) -> bool\n```\n\nReturn `False` if the ADS has 0 rows, `True` otherwise.\n\n-----\n\n```python\ndef join(\n    self,\n    other: \"AbstractDataStructure\",\n    left_on: List[str],\n    right_on: List[str],\n    how: Literal[\"left\", \"right\", \"outer\", \"inner\", \"cross\"] = \"inner\",\n    l_modifier: Callable[[str], str] = lambda x: x,\n    r_modifier: Callable[[str], str] = lambda x: x,\n    modify_on: bool = True,\n) -> \"AbstractDataStructure\"\n```\n\nJoins 2 ADS objects together.\n\n- `other` : The right-hand ADS in the join.\n- `left_on` : The left-hand columns on which to join.\n- `right_on` : The right-hand columns on which to join.\n- `how`: The join type.\n- `l_modifier`: A function that modifies column names for the left-hand ADS.\n- `r_modifier`: A function that modifies column names for the right-hand ADS.\n- `modify_on`: Specify whether the column name modification functions should apply to the join columns.\n\n-----\n\n```python\ndef group_by(\n    self,\n    keys: List[str],\n    outputs: Dict[\n        str,\n        Tuple[\n            Callable[[\"AbstractDataStructure\"], Any],\n            Type,\n        ],\n    ],\n) -> \"AbstractDataStructure\"\n```\n\nPerforms a group by operation on a given ADS.\n\n- `keys` : List of columns on which to group.\n- `outputs` : Dictionary defining aggregations to perform. The dict key is the output column name. The dict value is a 2-tuple whose first entry is a callable defining the aggregation, and whose second entry is the output type.\n\nFor example, to group on columns `col_0` and `col_1`, and compute the integer maximum and minimum values of column `col_2` for each group, one would write :\n\n```python\nads.group_by(\n    keys=[\"col_0\", \"col_1\"],\n    outputs={\n        \"min_col_2\": (lambda ads: ads[\"col_2\"].min(), int),\n        \"max_col_2\": (lambda ads: ads[\"col_2\"].max(), int),\n    },\n)\n```\n\n-----\n\n```python\ndef union(\n    self, *others: \"AbstractDataStructure\", all: bool = True\n) -> \"AbstractDataStructure\"\n```\n\nPerforms a union with all given input ADSs.\n\n- `others` : A varargs list of ADSs.\n- `all` : If `False`, deduplicate results.\n\n-----\n\n```python\ndef distinct(self, keys: Optional[List[str]] = None) -> \"AbstractDataStructure\"\n```\n\nDeduplicate entries. Can optionally deduplicate only on a subset of columns by specifying the `keys` arguments.\n\n-----\n\n```python\ndef apply(\n    self, output_column: str, func: Callable[[Dict], Any], cast: Type\n) -> \"AbstractDataStructure\"\n```\n\nApply a User Defined Function (UDF) on the ADS.\n\n- `output_column` : Name of the output column that will contain the result of the UDF.\n- `func` : The UDF in question. Takes a dict as input that corresponds to a given row of the ADS.\n- `cast` : The output data type.\n\nFor example :\n\n```python\nads.apply(\"output_col\", lambda x: str(x[\"col_0\"]).upper(), str)\n```\n\n-----\n\n```python\ndef sort(\n    self, *cols: str, asc: Union[bool, List[bool]] = True\n) -> \"AbstractDataStructure\"\n```\n\nSort the ADS along the given columns. Set if ascending or descending order using `asc`.\n\n-----\n\n```python\ndef limit(self, n: int) -> \"AbstractDataStructure\"\n```\n\nOutput a subset of the ADS based on the given number of rows.\n\n-----\n\n## AbstractDataColumn\n\nAn **Abstract Data Column** is a column of an ADS. Much like with an ADS, specific execution details vary based on the ADS it originated from (Pandas based, Spark based, SQL based etc.).\n\n### Column operations\n\n-----\n\n```python\ndef as_type(self, t: Type, **kwargs) -> \"AbstractDataColumn\"\n```\n\nCast a column to the requested type. Acceptable types are :\n\n- `str`\n- `int`\n- `float`\n- `complex`\n- `bool`\n- `datetime.datetime` : default conversion options may be overridden by specifying kwargs `auto_convert`, `as_timestamp`, and `datetime_format`.\n- `datetime.date`\n- `datetime.timedelta`\n\n-----\n\n```python\ndef isin(self, comp: List) -> \"AbstractDataColumn\"\n```\n\nReturns a boolean column where rows are set to `True` when entries are in the given `comp` list, and `False` otherwise.\n\n### Column aggregations\n\n```python\ndef min(self) -> Any\n```\n\nReturns minimum value of column.\n\n-----\n\n```python\ndef max(self) -> Any\n```\n\nReturns maximum value of column.\n\n-----\n\n```python\ndef sum(self) -> Any\n```\n\nReturns sum of all column entries.\n\n-----\n\n```python\ndef mean(self) -> Any\n```\n\nReturns average value of column entries.\n\n-----\n\n```python\ndef count(self) -> int\ndef __len__(self) -> int\n```\n\nReturns number of rows in column.\n\n-----\n\n```python\ndef __bool__(self) -> bool\n```\n\nReturn `False` if the column has 0 rows, `True` otherwise.\n\n-----\n\n### Operators\n\nAll binary and unary operators are supported. For example :\n\n```python\nads[\"col_0\"] * 2\n2 - ads[\"col_0\"]\nads[\"col_0\"] / ads[\"col_1\"]\n~(ads[\"col_0\"] == ads[\"col_1\"])\n```\n\n-----\n\n## AbstractDataInterface\n\nAn **Abstract Data Interface** handles all matters related to persisting data. Abstract Data Interfaces correspond either directly to a given data layer, or to a transition between 2 data layers. Much like with an ADS, execution details depend on the underlying persistance details (file based, database based, cloud based etc.).\n\n-----\n\n```python\ndef read_batch_data(self, step: ADFStep, batch_id: str) -> AbstractDataStructure\n```\n\nFor a given step and batch ID, return the corresponding ADS.\n\n-----\n\n```python\ndef read_full_data(self, step: ADFStep) -> AbstractDataStructure\n```\n\nFor a given step return all available data.\n\n-----\n\n```python\ndef read_batches_data(\n    self, step: ADFStep, batch_ids: List[str]\n) -> Optional[AbstractDataStructure]\n```\n\nFor a given step and a list of batch IDs, return the corresponding ADS. Returns `None` if the input batch ID list is empty.\n\n-----\n\n```python\ndef write_batch_data(\n    self, ads: AbstractDataStructure, step: ADFStep, batch_id: str\n) -> None\n```\n\nGiven an ADS and a target step and batch ID, persist the ADS.\n\n-----\n\n```python\ndef delete_step(self, step: ADFStep) -> None\n```\n\nDelete all data in the given step.\n\n-----\n\n```python\ndef delete_batch(self, step: ADFStep, batch_id: str) -> None\n```\n\nDelete data corresponding to a specific batch ID for a given step.\n\n-----\n\n## AbstractStateHandler\n\nAn **Abstract State Handler** contains all information related to the current processing state. In particular, it can list all batch IDs and their current state.\n\n-----\n\n```python\ndef to_ads(self) -> AbstractDataStructure\n```\n\nReturns an ADS describing all batches. The output ADS will always have the following columns :\n\n- `collection_name`\n- `flow_name`\n- `step_name`\n- `version`\n- `layer`\n- `batch_id`\n- `status`\n- `datetime`\n- `msg`\n\nThis method gives you complete read capabilities on the state handler, allowing you to extract any information you need from it. All following methods are merely shortcuts built on top of this one.\n\n-----\n\n```python\ndef to_step_ads(self, step: ADFStep) -> AbstractDataStructure\n```\n\nReturns an ADS containing the processing state of a given ADF step.\n\n-----\n\n```python\ndef get_entries(\n    self,\n    collection_name: Optional[str] = None,\n    flow_name: Optional[str] = None,\n    step_name: Optional[str] = None,\n    version: Optional[str] = None,\n    layer: Optional[str] = None,\n    batch_id: Optional[str] = None,\n    status: Optional[str] = None,\n) -> List[Dict]\n```\n\nReturns a list of batch IDs corresponding to the given filters.\n\n-----\n\n```python\ndef get_step_submitted(self, step: ADFStep) -> List[str]\ndef get_step_running(self, step: ADFStep) -> List[str]\ndef get_step_deleting(self, step: ADFStep) -> List[str]\ndef get_step_failed(self, step: ADFStep) -> List[str]\ndef get_step_success(self, step: ADFStep) -> List[str]\n```\n\nFor a given ADF step, return all batch IDs in the given state (`submitted`, `running`, `deleting`, `failed`, or `success`).\n\n-----\n\n```python\ndef get_step_all(self, step) -> List[str]\n```\n\nFor a given ADF step, return all batch IDs.\n\n-----\n\n```python\ndef get_batch_info(self, step: ADFStep, batch_id: str) -> Dict\n```\n\nFor a given batch ID of a given ADF step, return a dictionary containing all batch information.\n\n-----\n\n```python\ndef get_batch_status(self, step: ADFStep, batch_id: str) -> str\n```\n\nFor a given ADF step, returns the status of a given batch ID. Raises an error if the batch ID is unknown to the state handler.\n\n-----\n\n## ADFSequencer and ADFCombinationSequencer\n\nAn **ADF Sequencer** defines the batches to be processed by a given step at any given time. To define your own ADF Sequencer, you must inherit from either the **ADFSequencer** or **ADFCombinationSequencer** base class (the latter should only be used for combination steps). In both cases, there are 2 abstract methods that require defining.\n\n### ADFSequencer\n\n-----\n\n```python\ndef from_config(cls, config: Dict) -> \"ADFSequencer\"\n```\n\nGiven configuration parameters, return an `ADFSequencer` instance. If you define a custom `__init__` for this, make sure it calls `super().__init__()`.\n\n-----\n\n```python\ndef get_to_process(\n    self,\n    state_handler: AbstractStateHandler,\n    step_in: ADFStep,\n    step_out: ADFStep,\n) -> List[str]\n```\n\nInput arguments :\n\n- `state_handler` : Contains the current processing state.\n- `step_in` : The input step.\n- `step_out` : The output step.\n\nHow the output shapes the flow of data :\n\n- The return value is the list of batch IDs the output step is expected to create.\n- By default, previously submitted batches are ignored, there is no need for your method to check for them.\n- If you want your sequencer to resubmit such batches, you have to explicitly set `redo` to `True` in your constructor by calling `super().__init__(redo=True)`.\n\n-----\n\n### ADFCombinationSequencer\n\n-----\n\n```python\ndef from_config(cls, config: Dict) -> \"ADFCombinationSequencer\"\n```\n\nGiven configuration parameters, return an `ADFCombinationSequencer` instance. If you define a custom `__init__` for this, make sure it calls `super().__init__()`.\n\n-----\n\n```python\ndef get_to_process(\n    self,\n    state_handler: AbstractStateHandler,\n    combination_step: ADFCombinationStep,\n) -> List[Tuple[List[str], str]]\n```\n\nInput arguments :\n\n- `state_handler` : Contains the current processing state.\n- `combination_step` : The combination step in question.\n\nHow the output shapes the flow of data :\n\n- The return value is a list of 2-tuples :\n  - The first entry is a list of batch IDs corresponding to the input steps.\n  - The second entry is the corresponding batch ID output by the combination step.\n- By default, previously submitted batches are ignored, there is no need for your method to check for them.\n- If you want your sequencer to resubmit such batches, you have to explicitly set `redo` to `True` in your constructor by calling `super().__init__(redo=True)`.\n\n-----\n\n## ADFDataLoader\n\nAn **ADF Data Loader** defines what data is passed to your processing function at each step. To define your own ADF Data Loader, you must inherit from the `ADFDataLoader` base class. There are 2 abstract methods that then require defining.\n\n-----\n\n```python\ndef from_config(cls, config: Dict) -> \"ADFDataLoader\"\n```\n\nGiven configuration parameters, return an `ADFDataLoader` instance. If you define a custom `__init__` for this, make sure it calls `super().__init__()`.\n\n-----\n\n```python\ndef get_ads_args(\n    self,\n    data_interface: AbstractDataInterface,\n    step_in: ADFStep,\n    step_out: ADFStep,\n    batch_id: str,\n    state_handler: AbstractStateHandler,\n) -> Tuple[\n    List[AbstractDataStructure],\n    Dict[str, AbstractDataStructure],\n]\n```\n\nInput arguments :\n\n- `data_interface` : The data persistance interface from which to load data.\n- `step_in` : The input step.\n- `step_out` : The output step.\n- `batch_id` : The output batch ID.\n- `state_handler` : Contains the current processing state.\n\nHow the output is passed to your data processing functions :\n\n- The first output is a list of ADSs. These are passed as positional arguments.\n- The second output is a dict, for which each entry is passed as a keyword argument.\n\nSimply put, if your `get_ads_args` method returns `args, kwargs`, then these are passed to your processing function `func` simply as :\n\n```python\nfunc(*args, **kwargs)\n```\n\n-----\n\n## ADFBatchDependencyHandler\n\nAn **ADF Batch Dependency Handler** defines which batches are defined as being _downstream_ of batches in previous steps. This is mainly used to define which downstream batches are also deleted when deleting a particular batch of data. To define your own ADF Batch Dependency Handler, you must inherit from the `ADFBatchDependencyHandler` base class. There are 2 abstract methods that then require defining.\n\n-----\n\n```python\ndef from_config(cls, config: Dict) -> \"ADFBatchDependencyHandler\"\n```\n\nGiven configuration parameters, return an `ADFBatchDependencyHandler` instance. If you define a custom `__init__` for this, make sure it calls `super().__init__()`.\n\n-----\n\n```python\ndef get_dependent_batches(\n    self,\n    state_handler: AbstractStateHandler,\n    step_in: ADFStep,\n    step_out: ADFStep,\n    batch_id: str,\n) -> List[str]\n```\n\nInput arguments :\n\n- `state_handler` : Contains the current processing state.\n- `step_in` : The input step.\n- `step_out` : The output step.\n- `batch_id` : The input batch ID.\n\nThe return value represents the list of batch IDs in the output step that are considered dependent on the given batch ID in the input step.\n\n-----\n\n# Processing functions\n\n- [Input signature](#input-signature)\n  - [Non-starting step](#non-starting-step)\n  - [Combination step](#combination-step)\n- [Output signature](#output-signature)\n- [Concretization](#concretization)\n\nThe processing function signature depends on the specific step configuration. In particular, **data loaders** will modify the expected input arguments, and the presence of downstream reception steps will modify the expected output.\n\n## Input signature\n\n### Non-starting step\n\nFor non-starting steps, the input arguments will depend on the output `args` and `kwargs` of the step data loader, in addition to any `func_kwargs` defined in the step configuration. For example, consider the builtin `KwargDataLoader`, that merely passes the current batch of data as a keyword argument, meaning `args` is an empty list and `kwargs` is a single entry dictionary whose key is user defined :\n\n```yaml\nname: collection-name\nmodules:\n  - name: flow_config\n    import_path: ADF.components.flow_config\n  - name: module-alias\n    import_path: package.module\nflows:\n  - name: flow-name-0\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n      - layer: layer-name\n        name: processing-step-name\n        version: version-name\n        func:\n          load_as: module\n          params:\n            module: module-alias\n            name: foo\n        func_kwargs:\n          extra_kwarg: kwarg_val\n        data_loader:\n          module: flow_config\n          class_name: KwargDataLoader\n          params:\n            kwarg_name: custom_kwarg_name\n```\n\nIn this case, the expected function input signature is :\n\n```python\ndef foo(custom_kwarg_name: AbstractDataStructure, extra_kwarg: str)\n```\n\nThe `custom_kwarg_name` argument will contain the ADS passed by our data loader, and the `extra_kwarg` argument will contain the value `kwarg_val` as defined in our flow configuration.\n\n### Combination step\n\nFor a combination step, the `args` and `kwargs` are the combination of the outputs of the data loaders of all input steps, plus the data loader of the combination step itself. Again, let's use the `KwargDataLoader` to illustrate this, as well as the `FullDataLoader` which loads all available data for a given step.\n\n```yaml\nname: collection-name\nmodules:\n  - name: flow_config\n    import_path: ADF.components.flow_config\n  - name: module-alias\n    import_path: package.module\nflows:\n  - name: flow-name-0\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n  - name: flow-name-1\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n  - name: combination-flow\n    steps:\n      - start: combination\n        layer: layer-name\n        name: combination-step-name\n        version: version-name\n        input_steps:\n          - flow_name: flow-name-0\n            step_name: landing-step\n            data_loader:\n              module: flow_config\n              class_name: KwargDataLoader\n              params:\n                kwarg_name: custom_kwarg_name_0\n          - flow_name: flow-name-1\n            step_name: landing-step\n            data_loader:\n              module: flow_config\n              class_name: KwargDataLoader\n              params:\n                kwarg_name: custom_kwarg_name_0\n        func:\n          load_as: module\n          params:\n            module: module-alias\n            name: foo\n        func_kwargs:\n          extra_kwarg: kwarg_val\n        data_loader:\n          module: flow_config\n          class_name: FullDataLoader\n          params: {}\n```\n\nEach input step data loader will add an entry to the input `kwargs`. The data loader of the combination step itself will load the full data of that same step as the sole entry of our output `args`. Finally, there is also the user defined `extra_kwarg`, defined directly in our flow configuration, that will enrich the input `kwargs`. Putting all of this together, we get the following input signature :\n\n```python\ndef foo(\n    full_data: AbstractDataStructure,\n    custom_kwarg_name_0: AbstractDataStructure,\n    custom_kwarg_name_1: AbstractDataStructure,\n    extra_kwarg: str,\n)\n```\n\n## Output signature\n\nBy default, a processing function must output a single ADS :\n\n```python\ndef foo(ads: AbstractDataStructure) -> AbstractDataStructure:\n    return ads[ads[\"some_col\"] == \"some_val\"]\n```\n\nHowever, hooking a processing step into a reception step changes the expected output signature of the processing function. Instead of returning a single ADS, it must now return a dictionary whose keys correspond to the reception step keys. Take the following flow configuration as an example :\n\n```yaml\nname: collection-name\nmodules:\n  - name: module-alias\n    import_path: package.module\nflows:\n  - name: flow-name\n    steps:\n      - start: landing\n        layer: layer-name\n        name: landing-step-name\n      - layer: layer-name\n        name: processing-step\n        func:\n          load_as: module\n          params:\n            module: module-alias\n            name: multiple_outputs\n  - name: reception-flow-0\n    steps:\n      - start: reception\n        layer: layer-name\n        name: reception-step-name\n        key: reception-key-0\n        input_steps:\n          - flow_name: flow-name\n            step_name: processing-step\n  - name: reception-flow-1\n    steps:\n      - start: reception\n        layer: layer-name\n        name: reception-step-name\n        key: reception-key-1\n        input_steps:\n          - flow_name: flow-name\n            step_name: processing-step\n```\n\nA valid corresponding processing function would then be :\n\n```python\ndef multiple_outputs(\n    ads: AbstractDataStructure,\n) -> Dict[str, AbstractDataStructure]:\n    return {\n        \"reception-key-0\": ads[ads[\"col_0\"] == 0],\n        \"reception-key-1\": ads[ads[\"col_0\"] != 0],\n    }\n```\n\n## Concretization\n\nIt is possible to define processing functions using familiar \"concrete\" APIs (such as Pandas, PySpark, raw SQL etc.) using a procedure known as **concretization**. However, this comes at the cost that when the corresponding step is mapped to a layer, that layer must support the chosen concretization. For example, concretizing to a PySpark dataframe will fail for an SQL based layer. Our data flows remain abstract, but they become somewhat constrained in their eventual layer mapping.\n\nTo define a processing function as concrete, you may use the `concretize` decorator, which takes as input a concretization type. The decorator will transform all input ADSs into the requested type. It will do so in a nested manner, also transforming ADSs within input lists, tuples, or dictionaries. It also expects that type as the function output type. :\n\n```python\nfrom pandas import DataFrame\nfrom ADF.utils import concretize\n\n@concretize(DataFrame)\ndef foo(df: DataFrame) -> DataFrame:\n    return df.drop_duplicates()\n```\n\nThere are 2 main use cases for concretization that may be worth the slight trade-off of layer constraint :\n\n- Migrating a non ADF pipeline to ADF, allowing reuse of business logic code.\n- Exploiting API specific optimizations, such as `broadcast` for a PySpark dataframe.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "adf",
    "package_url": "https://pypi.org/project/adf/",
    "platform": null,
    "project_url": "https://pypi.org/project/adf/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/adf/0.1.2/",
    "requires_dist": [
      "Cython (>=0.29.32)",
      "numpy (>=1.21.2)",
      "pandas (<1.3.0,>=1.2.4)",
      "pyarrow (>=4.0.1)",
      "pyspark (>=3.1.1)",
      "SQLAlchemy (<2.0.0,>=1.4.17)",
      "sqlalchemy-redshift (>=0.8.5)",
      "pyathena (>=2.14.0)",
      "PyYAML (>=6.0)",
      "psycopg2-binary (>=2.9.1)",
      "boto3 (>=1.24.66)",
      "croniter (>=1.3.4)",
      "venv-pack (>=0.2.0)",
      "boto3-stubs (>=1.24.66)",
      "ipython (>=7.27.0)",
      "black (>=22.10)",
      "twine (>=4.0)"
    ],
    "requires_python": "",
    "summary": "Create infrastructure agnostic data processing pipelines",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15581298,
  "releases": {
    "0.0.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "989fe3f7da5115d255f154e749c5be0fc4acc7b7859a78a7eeb21ba29a82151e",
          "md5": "19f95f5d8bb95557659dc3080c487f98",
          "sha256": "d1c4a514dca88bfe7668725a24e932607250c4ff6b880c4473c3fde82ee00481"
        },
        "downloads": -1,
        "filename": "adf-0.0.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "19f95f5d8bb95557659dc3080c487f98",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 126083,
        "upload_time": "2022-06-12T21:51:09",
        "upload_time_iso_8601": "2022-06-12T21:51:09.006578Z",
        "url": "https://files.pythonhosted.org/packages/98/9f/e3f7da5115d255f154e749c5be0fc4acc7b7859a78a7eeb21ba29a82151e/adf-0.0.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "314eb67506f4e9a3f67d3f8f73005a508e872d2407ec3bcf1d2dae810490ecd6",
          "md5": "cef4439ede7141b0b0febab845c7681c",
          "sha256": "f1364985135039a942b8b53f40162a94e7de55e46ed743577354e4460763826e"
        },
        "downloads": -1,
        "filename": "adf-0.0.6.tar.gz",
        "has_sig": false,
        "md5_digest": "cef4439ede7141b0b0febab845c7681c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 96954,
        "upload_time": "2022-06-12T21:51:11",
        "upload_time_iso_8601": "2022-06-12T21:51:11.317994Z",
        "url": "https://files.pythonhosted.org/packages/31/4e/b67506f4e9a3f67d3f8f73005a508e872d2407ec3bcf1d2dae810490ecd6/adf-0.0.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d98a810bc5df948e3988554a197f50347d910ea4602c46fad7711760ef7bebe0",
          "md5": "481ff6934eae35496ed199fb6e6f4f89",
          "sha256": "884d91c02e03ec4cc935c221a1a11d53c31ad72f12d66a754d7c064852462806"
        },
        "downloads": -1,
        "filename": "adf-0.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "481ff6934eae35496ed199fb6e6f4f89",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 136841,
        "upload_time": "2022-10-10T19:22:52",
        "upload_time_iso_8601": "2022-10-10T19:22:52.899767Z",
        "url": "https://files.pythonhosted.org/packages/d9/8a/810bc5df948e3988554a197f50347d910ea4602c46fad7711760ef7bebe0/adf-0.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "799f5158e886671a2a4812a4aecbc750c06f7643ff888417ce37b278c296e3f9",
          "md5": "60a4812709decfe7f57b8c7b9991a9ec",
          "sha256": "125ac3c1b8b2a022dd3219e636de219dc47b4b3e6d050b237b281fd9921b69e2"
        },
        "downloads": -1,
        "filename": "adf-0.0.7.tar.gz",
        "has_sig": false,
        "md5_digest": "60a4812709decfe7f57b8c7b9991a9ec",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 106288,
        "upload_time": "2022-10-10T19:22:55",
        "upload_time_iso_8601": "2022-10-10T19:22:55.074185Z",
        "url": "https://files.pythonhosted.org/packages/79/9f/5158e886671a2a4812a4aecbc750c06f7643ff888417ce37b278c296e3f9/adf-0.0.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9601f060f89b4d577f4d86117e172b671a635bc5a0e7a6018ed2e7dcaf915144",
          "md5": "7ca886ef2e6a26f2be032b9a0e5de3f3",
          "sha256": "14a36f24a6b1220f74245ea7567b5033ce836bb6768567b173646261e51c3b8a"
        },
        "downloads": -1,
        "filename": "adf-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7ca886ef2e6a26f2be032b9a0e5de3f3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 149146,
        "upload_time": "2022-10-11T19:00:14",
        "upload_time_iso_8601": "2022-10-11T19:00:14.328755Z",
        "url": "https://files.pythonhosted.org/packages/96/01/f060f89b4d577f4d86117e172b671a635bc5a0e7a6018ed2e7dcaf915144/adf-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9f69b5156c00e8906b9c88b7670a35a14bb9814dbfc874787d624a0277ee69ac",
          "md5": "51c552af1c3d275bee314992e6957f98",
          "sha256": "397194f327c71ebeeb0e7d8c39a5c618ac3ba1f44030e3b4c7bd973afeafa6be"
        },
        "downloads": -1,
        "filename": "adf-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "51c552af1c3d275bee314992e6957f98",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 132317,
        "upload_time": "2022-10-11T19:00:17",
        "upload_time_iso_8601": "2022-10-11T19:00:17.463453Z",
        "url": "https://files.pythonhosted.org/packages/9f/69/b5156c00e8906b9c88b7670a35a14bb9814dbfc874787d624a0277ee69ac/adf-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b4d124947e14445c672dba347a04439a69c52def931996098486cacbff4067a7",
          "md5": "750a5eba779de27fdce66d05e1d1609f",
          "sha256": "02baf67dd8b0a94471ed42b07cf03fb169c3c51be530c0952c92697958bf84f2"
        },
        "downloads": -1,
        "filename": "adf-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "750a5eba779de27fdce66d05e1d1609f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 149367,
        "upload_time": "2022-10-20T19:16:57",
        "upload_time_iso_8601": "2022-10-20T19:16:57.109627Z",
        "url": "https://files.pythonhosted.org/packages/b4/d1/24947e14445c672dba347a04439a69c52def931996098486cacbff4067a7/adf-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e9cc31b2c0c90f5d512d56f624f1b538c72061a2eafbd7d14c9d4d14dd312d40",
          "md5": "ea8ee0d13c5d2661c73680a472868db0",
          "sha256": "9c3c07a191d885b4ad55047986e7e73cee46b09a615635f68b40f6c295ac9c2a"
        },
        "downloads": -1,
        "filename": "adf-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ea8ee0d13c5d2661c73680a472868db0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 132607,
        "upload_time": "2022-10-20T19:16:59",
        "upload_time_iso_8601": "2022-10-20T19:16:59.721344Z",
        "url": "https://files.pythonhosted.org/packages/e9/cc/31b2c0c90f5d512d56f624f1b538c72061a2eafbd7d14c9d4d14dd312d40/adf-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ea147ffe1ca8eb7c93787a0903539178725d2a2a702e16a6819cba2dd646cfa2",
          "md5": "c8fd6cb341f2eccb5d12d99e0da65a3e",
          "sha256": "4889dae49a7cac66fc432f06344e2d39e470bdfa64a44ac333432f8222b50b30"
        },
        "downloads": -1,
        "filename": "adf-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c8fd6cb341f2eccb5d12d99e0da65a3e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 149434,
        "upload_time": "2022-10-29T21:19:22",
        "upload_time_iso_8601": "2022-10-29T21:19:22.075091Z",
        "url": "https://files.pythonhosted.org/packages/ea/14/7ffe1ca8eb7c93787a0903539178725d2a2a702e16a6819cba2dd646cfa2/adf-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7f8391910b90a5d39afea1773920a66736ef28dd8536a213ebcfdf6f111da194",
          "md5": "01882ca9e284cccdfc76470975b1acea",
          "sha256": "07ad45a190eb8d84a7997aedcee0a9cc5a6a43d0d86457827b3e976dec24873a"
        },
        "downloads": -1,
        "filename": "adf-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "01882ca9e284cccdfc76470975b1acea",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 131303,
        "upload_time": "2022-10-29T21:19:25",
        "upload_time_iso_8601": "2022-10-29T21:19:25.101601Z",
        "url": "https://files.pythonhosted.org/packages/7f/83/91910b90a5d39afea1773920a66736ef28dd8536a213ebcfdf6f111da194/adf-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ea147ffe1ca8eb7c93787a0903539178725d2a2a702e16a6819cba2dd646cfa2",
        "md5": "c8fd6cb341f2eccb5d12d99e0da65a3e",
        "sha256": "4889dae49a7cac66fc432f06344e2d39e470bdfa64a44ac333432f8222b50b30"
      },
      "downloads": -1,
      "filename": "adf-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c8fd6cb341f2eccb5d12d99e0da65a3e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 149434,
      "upload_time": "2022-10-29T21:19:22",
      "upload_time_iso_8601": "2022-10-29T21:19:22.075091Z",
      "url": "https://files.pythonhosted.org/packages/ea/14/7ffe1ca8eb7c93787a0903539178725d2a2a702e16a6819cba2dd646cfa2/adf-0.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7f8391910b90a5d39afea1773920a66736ef28dd8536a213ebcfdf6f111da194",
        "md5": "01882ca9e284cccdfc76470975b1acea",
        "sha256": "07ad45a190eb8d84a7997aedcee0a9cc5a6a43d0d86457827b3e976dec24873a"
      },
      "downloads": -1,
      "filename": "adf-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "01882ca9e284cccdfc76470975b1acea",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 131303,
      "upload_time": "2022-10-29T21:19:25",
      "upload_time_iso_8601": "2022-10-29T21:19:25.101601Z",
      "url": "https://files.pythonhosted.org/packages/7f/83/91910b90a5d39afea1773920a66736ef28dd8536a213ebcfdf6f111da194/adf-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}