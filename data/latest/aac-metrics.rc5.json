{
  "info": {
    "author": "Etienne Labbé (Labbeti)",
    "author_email": "labbeti.pub@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "<!-- # -*- coding: utf-8 -*- -->\n\n<div align=\"center\">\n\n# Audio Captioning metrics (aac-metrics)\n\n<a href=\"https://www.python.org/\"><img alt=\"Python\" src=\"https://img.shields.io/badge/-Python 3.9+-blue?style=for-the-badge&logo=python&logoColor=white\"></a>\n<a href=\"https://pytorch.org/get-started/locally/\"><img alt=\"PyTorch\" src=\"https://img.shields.io/badge/-PyTorch 1.10.1+-ee4c2c?style=for-the-badge&logo=pytorch&logoColor=white\"></a>\n<a href=\"https://black.readthedocs.io/en/stable/\"><img alt=\"Code style: black\" src=\"https://img.shields.io/badge/code%20style-black-black.svg?style=for-the-badge&labelColor=gray\"></a>\n<a href=\"https://github.com/Labbeti/aac-metrics/actions\"><img alt=\"Build\" src=\"https://img.shields.io/github/actions/workflow/status/Labbeti/aac-metrics/python-package-pip.yaml?branch=main&style=for-the-badge&logo=github\"></a>\n<a href='https://aac-metrics.readthedocs.io/en/stable/?badge=stable'>\n    <img src='https://readthedocs.org/projects/aac-metrics/badge/?version=stable&style=for-the-badge' alt='Documentation Status' />\n</a>\n\nMetrics for evaluating Automated Audio Captioning systems, designed for PyTorch.\n\n</div>\n\n## Why using this package?\n- **Easy installation and download**\n- **Same results than [caption-evaluation-tools](https://github.com/audio-captioning/caption-evaluation-tools) and [fense](https://github.com/blmoistawinde/fense) repositories**\n- **Provides the following metrics:**\n    - BLEU [[1]](#bleu)\n    - ROUGE-L [[2]](#rouge-l)\n    - METEOR [[3]](#meteor)\n    - CIDEr-D [[4]](#cider)\n    - SPICE [[5]](#spice)\n    - SPIDEr [[6]](#spider)\n    - SPIDEr-max [[7]](#spider-max)\n    - SBERT [[8]](#fense)\n    - FluencyError [[8]](#fense)\n    - FENSE [[8]](#fense)\n    - SPIDErErr\n\n## Installation\nInstall the pip package:\n```bash\npip install aac-metrics\n```\n\nDownload the external code and models needed for METEOR, SPICE, PTBTokenizer and FENSE:\n```bash\naac-metrics-download\n```\n\nNotes:\n- The external code for SPICE, METEOR and PTBTokenizer is stored in `$HOME/.cache/aac-metrics`.\n- The weights of the FENSE fluency error detector and the the SBERT model are respectively stored by default in `$HOME/.cache/torch/hub/fense_data` and `$HOME/.cache/torch/sentence_transformers`.\n\n## Usage\n### Evaluate default AAC metrics\nThe full evaluation process to compute AAC metrics can be done with `aac_metrics.aac_evaluate` function.\n\n```python\nfrom aac_metrics import aac_evaluate\n\ncandidates: list[str] = [\"a man is speaking\"]\nmult_references: list[list[str]] = [[\"a man speaks.\", \"someone speaks.\", \"a man is speaking while a bird is chirping in the background\"]]\n\ncorpus_scores, _ = aac_evaluate(candidates, mult_references)\nprint(corpus_scores)\n# dict containing the score of each aac metric: \"bleu_1\", \"bleu_2\", \"bleu_3\", \"bleu_4\", \"rouge_l\", \"meteor\", \"cider_d\", \"spice\", \"spider\"\n# {\"bleu_1\": tensor(0.7), \"bleu_2\": ..., ...}\n```\n\n### Evaluate a specific metric\nEvaluate a specific metric can be done using the `aac_metrics.functional.<metric_name>.<metric_name>` function or the `aac_metrics.classes.<metric_name>.<metric_name>` class. Unlike `aac_evaluate`, the tokenization with PTBTokenizer is not done with these functions, but you can do it manually with `preprocess_mono_sents` and `preprocess_mult_sents` functions.\n\n```python\nfrom aac_metrics.functional import cider_d\nfrom aac_metrics.utils.tokenization import preprocess_mono_sents, preprocess_mult_sents\n\ncandidates: list[str] = [\"a man is speaking\"]\nmult_references: list[list[str]] = [[\"a man speaks.\", \"someone speaks.\", \"a man is speaking while a bird is chirping in the background\"]]\n\ncandidates = preprocess_mono_sents(candidates)\nmult_references = preprocess_mult_sents(mult_references)\n\ncorpus_scores, sents_scores = cider_d(candidates, mult_references)\nprint(corpus_scores)\n# {\"cider_d\": tensor(0.1)}\nprint(sents_scores)\n# {\"cider_d\": tensor([0.9, ...])}\n```\n\nEach metrics also exists as a python class version, like `aac_metrics.classes.cider_d.CIDErD`.\n\n## Metrics\n### Default AAC metrics\n| Metric | Python Class | Origin | Range | Short description |\n|:---|:---|:---|:---|:---|\n| BLEU [[1]](#bleu) | `BLEU` | machine translation | [0, 1] | Precision of n-grams |\n| ROUGE-L [[2]](#rouge-l) | `ROUGEL` | machine translation | [0, 1] | FScore of the longest common subsequence |\n| METEOR [[3]](#meteor) | `METEOR` | machine translation | [0, 1] | Cosine-similarity of frequencies with synonyms matching |\n| CIDEr-D [[4]](#cider) | `CIDErD` | image captioning | [0, 10] | Cosine-similarity of TF-IDF computed on n-grams |\n| SPICE [[5]](#spice) | `SPICE` | image captioning | [0, 1] | FScore of semantic graph |\n| SPIDEr [[6]](#spider) | `SPIDEr` | image captioning | [0, 5.5] | Mean of CIDEr-D and SPICE |\n\n### Other metrics\n| Metric name | Python Class | Origin | Range | Short description |\n|:---|:---|:---|:---|:---|\n| SPIDEr-max [[7]](#spider-max) | `SPIDErMax` | audio captioning | [0, 5.5] | Max of SPIDEr scores for multiples candidates |\n| SBERT [[7]](#spider-max) | `SBERT` | audio captioning | [-1, 1] | Cosine-similarity of **Sentence-BERT embeddings** |\n| FluencyError [[7]](#spider-max) | `FluencyError` | audio captioning | [0, 1] | Use pretrained model to detect fluency errors in sentences |\n| FENSE [[8]](#fense) | `FENSE` | audio captioning | [-1, 1] | Combines `SBERT` and `FluencyError` |\n| SPIDErErr | `SPIDErErr` | audio captioning | [0, 5.5] | Combines `SPIDEr` and `FluencyError` |\n\n## SPIDEr-max metric\nSPIDEr-max [[7]](#spider-max)  is a metric based on SPIDEr that takes into account multiple candidates for the same audio. It computes the maximum of the SPIDEr scores for each candidate to balance the high sensitivity to the frequency of the words generated by the model.\n\n### SPIDEr-max: why ?\nThe SPIDEr metric used in audio captioning is highly sensitive to the frequencies of the words used.\n\nHere is 2 examples with the 5 candidates generated by the beam search algorithm, their corresponding SPIDEr scores and the associated references:\n\n<div align=\"center\">\n\n| Beam search candidates | SPIDEr |\n|:---|:---:|\n| heavy rain is falling on a roof | 0.562 |\n| heavy rain is falling on **a tin roof** | **0.930** |\n| a heavy rain is falling on a roof | 0.594 |\n| a heavy rain is falling on the ground | 0.335 |\n| a heavy rain is falling on the roof | 0.594 |\n\n| References |\n|:---|\n| heavy rain falls loudly onto a structure with a thin roof |\n| heavy rainfall falling onto a thin structure with a thin roof |\n| it is raining hard and the rain hits **a tin roof** |\n| rain that is pouring down very hard outside |\n| the hard rain is noisy as it hits **a tin roof** |\n\n_(Candidates and references for the Clotho development-testing file named \"rain.wav\")_\n\n| Beam search candidates | SPIDEr |\n|:---|:---:|\n| a woman speaks and a sheep bleats | 0.190 |\n| a woman **speaks and a goat bleats** | **1.259** |\n| a man speaks and a sheep bleats | 0.344 |\n| an adult male speaks and a sheep bleats | 0.231 |\n| an adult male is speaking and a sheep bleats | 0.189 |\n\n| References |\n|:---|\n| a man speaking and laughing followed by a goat bleat |\n| a man is speaking in high tone while a goat is bleating one time |\n| a man speaks followed by a goat bleat |\n| a person **speaks and a goat bleats** |\n| a man is talking and snickering followed by a goat bleating |\n\n_(Candidates and references for an AudioCaps testing file with the id \"jid4t-FzUn0\")_\n</div>\n\nEven with very similar candidates, the SPIDEr scores varies drastically. To adress this issue, we proposed a SPIDEr-max metric which take the maximum value of several candidates for the same audio. SPIDEr-max demonstrate that SPIDEr can exceed state-of-the-art scores on AudioCaps and Clotho and even human scores on AudioCaps [[7]](#spider-max).\n\n### SPIDEr-max: usage\nThis usage is very similar to other captioning metrics, with the main difference of take a multiple candidates list as input.\n\n```python\nfrom aac_metrics.functional import spider_max\nfrom aac_metrics.utils.tokenization import preprocess_mult_sents\n\nmult_candidates: list[list[str]] = [[\"a man is speaking\", \"maybe someone speaking\"]]\nmult_references: list[list[str]] = [[\"a man speaks.\", \"someone speaks.\", \"a man is speaking while a bird is chirping in the background\"]]\n\nmult_candidates = preprocess_mult_sents(mult_candidates)\nmult_references = preprocess_mult_sents(mult_references)\n\ncorpus_scores, sents_scores = spider_max(mult_candidates, mult_references)\nprint(corpus_scores)\n# {\"spider\": tensor(0.1), ...}\nprint(sents_scores)\n# {\"spider\": tensor([0.9, ...]), ...}\n```\n\n## Requirements\n### Python packages\n\nThe pip requirements are automatically installed when using `pip install` on this repository.\n```\ntorch >= 1.10.1\nnumpy >= 1.21.2\npyyaml >= 6.0\ntqdm >= 4.64.0\nsentence-transformers>=2.2.2\n```\n\n### External requirements\n- `java` >= 1.8 is required to compute METEOR, SPICE and use the PTBTokenizer.\nMost of these functions can specify a java executable path with `java_path` argument.\n\n- `unzip` command to extract SPICE zipped files.\n\n\n## Additional notes\n### CIDEr or CIDEr-D ?\nThe CIDEr metric differs from CIDEr-D because it applies a stemmer to each word before computing the n-grams of the sentences. In AAC, only the CIDEr-D is reported and used for SPIDEr in [caption-evaluation-tools](https://github.com/audio-captioning/caption-evaluation-tools), but some papers called it \"CIDEr\".\n\n### Does metrics work on multi-GPU ?\nNo. Most of these metrics use numpy or external java programs to run, which prevents multi-GPU testing for now.\n\n### Is torchmetrics needed for this package ?\nNo. But if torchmetrics is installed, all metrics classes will inherit from the base class `torchmetrics.Metric`.\nIt is because most of the metrics does not use PyTorch tensors to compute scores and numpy and strings cannot be added to states of `torchmetrics.Metric`.\n\n## References\n#### BLEU\n[1] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “BLEU: a\nmethod for automatic evaluation of machine translation,” in Proceed-\nings of the 40th Annual Meeting on Association for Computational\nLinguistics - ACL ’02. Philadelphia, Pennsylvania: Association\nfor Computational Linguistics, 2001, p. 311. [Online]. Available:\nhttp://portal.acm.org/citation.cfm?doid=1073083.1073135\n\n#### ROUGE-L\n[2] C.-Y. Lin, “ROUGE: A package for automatic evaluation of summaries,”\nin Text Summarization Branches Out. Barcelona, Spain: Association\nfor Computational Linguistics, Jul. 2004, pp. 74–81. [Online]. Available:\nhttps://aclanthology.org/W04-1013\n\n#### METEOR\n[3] M. Denkowski and A. Lavie, “Meteor Universal: Language Specific\nTranslation Evaluation for Any Target Language,” in Proceedings of the\nNinth Workshop on Statistical Machine Translation. Baltimore, Maryland,\nUSA: Association for Computational Linguistics, 2014, pp. 376–380.\n[Online]. Available: http://aclweb.org/anthology/W14-3348\n\n#### CIDEr\n[4] R. Vedantam, C. L. Zitnick, and D. Parikh, “CIDEr: Consensus-based\nImage Description Evaluation,” arXiv:1411.5726 [cs], Jun. 2015, arXiv:\n1411.5726. [Online]. Available: http://arxiv.org/abs/1411.5726\n\n#### SPICE\n[5] P. Anderson, B. Fernando, M. Johnson, and S. Gould, “SPICE: Semantic\nPropositional Image Caption Evaluation,” arXiv:1607.08822 [cs], Jul. 2016,\narXiv: 1607.08822. [Online]. Available: http://arxiv.org/abs/1607.08822\n\n#### SPIDEr\n[6] S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy, “Improved Image\nCaptioning via Policy Gradient optimization of SPIDEr,” 2017 IEEE Inter-\nnational Conference on Computer Vision (ICCV), pp. 873–881, Oct. 2017,\narXiv: 1612.00370. [Online]. Available: http://arxiv.org/abs/1612.00370\n\n#### SPIDEr-max\n[7] E. Labbé, T. Pellegrini, and J. Pinquier, “Is my automatic audio captioning system so bad? spider-max: a metric to consider several caption candidates,” Nov. 2022. [Online]. Available: https://hal.archives-ouvertes.fr/hal-03810396\n\n#### FENSE\n[8] Z. Zhou, Z. Zhang, X. Xu, Z. Xie, M. Wu, and K. Q. Zhu, Can Audio Captions Be Evaluated with Image Caption Metrics? arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2110.04684 \n\n## Citation\nIf you use **SPIDEr-max**, you can cite the following paper using BibTex:\n```\n@inproceedings{labbe:hal-03810396,\n    TITLE = {{Is my automatic audio captioning system so bad? spider-max: a metric to consider several caption candidates}},\n    AUTHOR = {Labb{\\'e}, Etienne and Pellegrini, Thomas and Pinquier, Julien},\n    URL = {https://hal.archives-ouvertes.fr/hal-03810396},\n    BOOKTITLE = {{Workshop DCASE}},\n    ADDRESS = {Nancy, France},\n    YEAR = {2022},\n    MONTH = Nov,\n    KEYWORDS = {audio captioning ; evaluation metric ; beam search ; multiple candidates},\n    PDF = {https://hal.archives-ouvertes.fr/hal-03810396/file/Labbe_DCASE2022.pdf},\n    HAL_ID = {hal-03810396},\n    HAL_VERSION = {v1},\n}\n```\n\n## Contact\nMaintainer:\n- Etienne Labbé \"Labbeti\": labbeti.pub@gmail.com\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Labbeti/aac-metrics",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "aac-metrics",
    "package_url": "https://pypi.org/project/aac-metrics/",
    "platform": null,
    "project_url": "https://pypi.org/project/aac-metrics/",
    "project_urls": {
      "Documentation": "https://aac-metrics.readthedocs.io/",
      "Homepage": "https://github.com/Labbeti/aac-metrics",
      "PyPI": "https://pypi.org/project/aac-metrics/",
      "Source": "https://github.com/Labbeti/aac-metrics"
    },
    "release_url": "https://pypi.org/project/aac-metrics/0.3.0/",
    "requires_dist": null,
    "requires_python": ">=3.9",
    "summary": "Metrics for evaluating Automated Audio Captioning systems, designed for PyTorch.",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17064022,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9a6f0a87a06ce96b54a12d555e1069a3c854c05df83f7e2db594106e6bdfae19",
          "md5": "c66c521f66ab58d259dbedb38286fb03",
          "sha256": "d9bfb93d9cb0fd25fc2d493bd0feca9844ade837b240243a0d26fba5143c3825"
        },
        "downloads": -1,
        "filename": "aac-metrics-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c66c521f66ab58d259dbedb38286fb03",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 45785,
        "upload_time": "2022-09-28T09:50:57",
        "upload_time_iso_8601": "2022-09-28T09:50:57.118457Z",
        "url": "https://files.pythonhosted.org/packages/9a/6f/0a87a06ce96b54a12d555e1069a3c854c05df83f7e2db594106e6bdfae19/aac-metrics-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c785d922b3e0f7619425a87329a8a0706e071aa44e5f3dbbfb25d8e1dd919dfd",
          "md5": "867633600eeb74455e9e27991872a929",
          "sha256": "d7e4f3d2d5a36f1d3167131aff6750b6f3f4590f2136500ba5d071911294ba03"
        },
        "downloads": -1,
        "filename": "aac-metrics-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "867633600eeb74455e9e27991872a929",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 149948,
        "upload_time": "2022-09-30T12:11:02",
        "upload_time_iso_8601": "2022-09-30T12:11:02.754115Z",
        "url": "https://files.pythonhosted.org/packages/c7/85/d922b3e0f7619425a87329a8a0706e071aa44e5f3dbbfb25d8e1dd919dfd/aac-metrics-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4615b878d961babb79a94271c22e9fdb40ee1726d317b706a766e9b893463809",
          "md5": "03a86cdfeaf46b2e61111982f33c1f83",
          "sha256": "a6bc9809580589815b1408882b13463a16a40602a48816d6a598c90ecc012148"
        },
        "downloads": -1,
        "filename": "aac-metrics-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "03a86cdfeaf46b2e61111982f33c1f83",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 151347,
        "upload_time": "2022-10-31T09:52:08",
        "upload_time_iso_8601": "2022-10-31T09:52:08.973757Z",
        "url": "https://files.pythonhosted.org/packages/46/15/b878d961babb79a94271c22e9fdb40ee1726d317b706a766e9b893463809/aac-metrics-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "43ae53d1e7dd1a2116b3992281dae8fbf6b8f744c6a48a8f5ff6ed4181b81818",
          "md5": "3be4bdc39177afd19d045224f2be26ab",
          "sha256": "c047a9e2870a25b1bf3e6deb81983348aead0400ff9e8ea50d80490059115d55"
        },
        "downloads": -1,
        "filename": "aac-metrics-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "3be4bdc39177afd19d045224f2be26ab",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 162472,
        "upload_time": "2022-12-14T13:17:52",
        "upload_time_iso_8601": "2022-12-14T13:17:52.189788Z",
        "url": "https://files.pythonhosted.org/packages/43/ae/53d1e7dd1a2116b3992281dae8fbf6b8f744c6a48a8f5ff6ed4181b81818/aac-metrics-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "34789e1cd5886d43249581781ee580fbc789153c946ec0b59fe0e51f5396a2d1",
          "md5": "0950cf0fb3bcf09446d9db4c47e42fe1",
          "sha256": "c716f09f792b7ee96352d41cf96dd5677e9e237618cc88f6607de4d432c459c8"
        },
        "downloads": -1,
        "filename": "aac-metrics-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "0950cf0fb3bcf09446d9db4c47e42fe1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 162268,
        "upload_time": "2023-02-27T13:11:34",
        "upload_time_iso_8601": "2023-02-27T13:11:34.599828Z",
        "url": "https://files.pythonhosted.org/packages/34/78/9e1cd5886d43249581781ee580fbc789153c946ec0b59fe0e51f5396a2d1/aac-metrics-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "34789e1cd5886d43249581781ee580fbc789153c946ec0b59fe0e51f5396a2d1",
        "md5": "0950cf0fb3bcf09446d9db4c47e42fe1",
        "sha256": "c716f09f792b7ee96352d41cf96dd5677e9e237618cc88f6607de4d432c459c8"
      },
      "downloads": -1,
      "filename": "aac-metrics-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "0950cf0fb3bcf09446d9db4c47e42fe1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.9",
      "size": 162268,
      "upload_time": "2023-02-27T13:11:34",
      "upload_time_iso_8601": "2023-02-27T13:11:34.599828Z",
      "url": "https://files.pythonhosted.org/packages/34/78/9e1cd5886d43249581781ee580fbc789153c946ec0b59fe0e51f5396a2d1/aac-metrics-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}