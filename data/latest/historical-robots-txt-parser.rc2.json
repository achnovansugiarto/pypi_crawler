{
  "info": {
    "author": "Alex Litel",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "[![PyPI version shields.io](https://img.shields.io/pypi/v/historical-robots-txt-parser.svg)](https://pypi.python.org/pypi/historical-robots-txt-parser/) [![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)\n\n\n\n# Historical Robots.txt Parser\n\nThis is a small Python package that parses the historical robots.txt files from the Internet Archive's Wayback Machine and coerces the data into a CSV file for tracking addition and removal of `Allow` and `Disallow` rules by timestamp of addition, path, user-agent, rule type (optional). It's a fairly narrow use case but may be helpful to researchers or journalists.\n\nIt also includes a parser to coerce a robots.txt file into a dictionary.\n\n## Requirements\n* Python 3.7 or later\n\n## Installation\n#### Install with Python\n```\npip3 install historical-robots-txt-parser\n```\n\n#### Install with Git\nThis package was developed using [Poetry](https://github.com/python-poetry/poetry), which greatly simplifies the experience of dealing with dependencies and everything. Using Poetry is strongly recommended.\n\n```\ngit clone https://github.com/alexlitel/historical-robots-txt-parser\ncd historical-robots-txt-parser\npoetry install\n```\n\nThere is a `requirements.txt` file included here, so you can also use `pip3 install -r requirements.txt` if you don't want to use Poetry.\n\n## Usage\nThere are two functions included in the package: `parse_robots_txt` and `historical_scraper`. `historical_scraper` scrapes the historical files for a domain from the Wayback Machine and exports to a CSV. `parse_robots_txt` makes a request to a robots.txt file, parses and coerces it to a dictionary. If you clone the repo, there's a file `app.py` which takes command line arguments for domains to scrape.\n\n\n### historical_scraper\n#### Usage\n```\nfrom historical_robots import historical_scraper\n\nhistorical_scraper('website.com', 'website.csv', <optional arguments>)\n```\n\n#### Parameters\n| parameter | type | required | default value | description |\n|----------------------|------------|----------|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n| domain | string | true |  | The domain to scrape records from. Only should be hostname without `www`. |\n| file_path | string | true |  | Path of CSV file to export to |\n| accept_allow | boolean | false | False | Whether to allow parser to parse `Allow` rules and include those in dataset. Adds a new column to CSV for `Rule` to note `Disallow` or `Allow` rule. By default, function only checks `Disallow` rules.\n| skip_scrape_interval | boolean | false | False | Whether to skip the default sleep interval between each historical robots.txt request.  `True` value may cause errors. |\n| sleep_interval | number | false | 0.05 | Number of seconds to sleep in between robots.txt requests.  Ignored if `skip_scrape_interval` is `True` |\n| params | dictionary | false | {} | Key value pairs representing [valid URL params for the Wayback CDX API](https://github.com/internetarchive/wayback/tree/master/wayback-cdx-server) |\n\n\n\n### parse_robots_txt\n#### Usage\n```\nfrom historical_robots import parse_robots_text\n\nparse_robots_txt('https://www.website.com/robots.txt', False)\n```\n\n#### Parameters\n| parameter | type | required | default value | description |\n|----------------------|------------|----------|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n| URL | string | true |  | The URL to request robots.txt file from. |\n| accept_allow | boolean | false | False | Whether to parse `Allow` rules. By default, function only checks `Disallow` rules.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/alexlitel/historical-robots-txt-parser",
    "keywords": "robots.txt,wayback machine,historical data,historical",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "historical-robots-txt-parser",
    "package_url": "https://pypi.org/project/historical-robots-txt-parser/",
    "platform": "",
    "project_url": "https://pypi.org/project/historical-robots-txt-parser/",
    "project_urls": {
      "Homepage": "https://github.com/alexlitel/historical-robots-txt-parser",
      "Repository": "https://github.com/alexlitel/historical-robots-txt-parser"
    },
    "release_url": "https://pypi.org/project/historical-robots-txt-parser/0.1.2/",
    "requires_dist": [
      "requests (>=2.23.0,<3.0.0)"
    ],
    "requires_python": ">=3.7,<4.0",
    "summary": "Parses historical robots.txt files from Wayback Machine",
    "version": "0.1.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7206531,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e7220eb457422c2254a1dfe5ae83bd2a4c3945ccdf204a3b3067f24777c453de",
          "md5": "72a7a673fcd2728247f379f685c0205b",
          "sha256": "bc1db0e984ff440f91889bfe358abb492b9e614a2fbb179d27bb81293f5d55af"
        },
        "downloads": -1,
        "filename": "historical_robots_txt_parser-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "72a7a673fcd2728247f379f685c0205b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<4.0",
        "size": 4800,
        "upload_time": "2020-05-04T02:59:06",
        "upload_time_iso_8601": "2020-05-04T02:59:06.756350Z",
        "url": "https://files.pythonhosted.org/packages/e7/22/0eb457422c2254a1dfe5ae83bd2a4c3945ccdf204a3b3067f24777c453de/historical_robots_txt_parser-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4ae7368cf44f91a8ecdec04e82e1acd1a6deb4b8f6c1457de16e893f3bf0593c",
          "md5": "c5384ab61cd3977acbaa64af2e0eef0e",
          "sha256": "c5790dcff336a65dae53c737543d6f27883a1648044da2f58ca6efc3419efb58"
        },
        "downloads": -1,
        "filename": "historical-robots-txt-parser-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "c5384ab61cd3977acbaa64af2e0eef0e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<4.0",
        "size": 4442,
        "upload_time": "2020-05-04T02:59:04",
        "upload_time_iso_8601": "2020-05-04T02:59:04.694510Z",
        "url": "https://files.pythonhosted.org/packages/4a/e7/368cf44f91a8ecdec04e82e1acd1a6deb4b8f6c1457de16e893f3bf0593c/historical-robots-txt-parser-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5a6ef5d2f45718ba5a9cf19ebb93d43fbdd36287995c9fbbcbbf4bd018567d3c",
          "md5": "c16479bd07badd0c845a75dfdd4b17c6",
          "sha256": "492f60ae770dbbc9608b6ed87aa1b49b2ebe2e5cb84f2ff8d3befcf4101a1fdc"
        },
        "downloads": -1,
        "filename": "historical_robots_txt_parser-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c16479bd07badd0c845a75dfdd4b17c6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7,<4.0",
        "size": 5760,
        "upload_time": "2020-05-10T02:39:40",
        "upload_time_iso_8601": "2020-05-10T02:39:40.393201Z",
        "url": "https://files.pythonhosted.org/packages/5a/6e/f5d2f45718ba5a9cf19ebb93d43fbdd36287995c9fbbcbbf4bd018567d3c/historical_robots_txt_parser-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "06a236bebf92b4b54b46f1b7e5dbf6444da46c3b66c92be1e6639f86e3d423e8",
          "md5": "60d5e4273400987890720dad394a0a79",
          "sha256": "db52bfb1a3a7fd42455956df2e63979e45d55a115f041e13a64b53537cc74c8d"
        },
        "downloads": -1,
        "filename": "historical-robots-txt-parser-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "60d5e4273400987890720dad394a0a79",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7,<4.0",
        "size": 5214,
        "upload_time": "2020-05-10T02:39:39",
        "upload_time_iso_8601": "2020-05-10T02:39:39.693322Z",
        "url": "https://files.pythonhosted.org/packages/06/a2/36bebf92b4b54b46f1b7e5dbf6444da46c3b66c92be1e6639f86e3d423e8/historical-robots-txt-parser-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5a6ef5d2f45718ba5a9cf19ebb93d43fbdd36287995c9fbbcbbf4bd018567d3c",
        "md5": "c16479bd07badd0c845a75dfdd4b17c6",
        "sha256": "492f60ae770dbbc9608b6ed87aa1b49b2ebe2e5cb84f2ff8d3befcf4101a1fdc"
      },
      "downloads": -1,
      "filename": "historical_robots_txt_parser-0.1.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c16479bd07badd0c845a75dfdd4b17c6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7,<4.0",
      "size": 5760,
      "upload_time": "2020-05-10T02:39:40",
      "upload_time_iso_8601": "2020-05-10T02:39:40.393201Z",
      "url": "https://files.pythonhosted.org/packages/5a/6e/f5d2f45718ba5a9cf19ebb93d43fbdd36287995c9fbbcbbf4bd018567d3c/historical_robots_txt_parser-0.1.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "06a236bebf92b4b54b46f1b7e5dbf6444da46c3b66c92be1e6639f86e3d423e8",
        "md5": "60d5e4273400987890720dad394a0a79",
        "sha256": "db52bfb1a3a7fd42455956df2e63979e45d55a115f041e13a64b53537cc74c8d"
      },
      "downloads": -1,
      "filename": "historical-robots-txt-parser-0.1.2.tar.gz",
      "has_sig": false,
      "md5_digest": "60d5e4273400987890720dad394a0a79",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7,<4.0",
      "size": 5214,
      "upload_time": "2020-05-10T02:39:39",
      "upload_time_iso_8601": "2020-05-10T02:39:39.693322Z",
      "url": "https://files.pythonhosted.org/packages/06/a2/36bebf92b4b54b46f1b7e5dbf6444da46c3b66c92be1e6639f86e3d423e8/historical-robots-txt-parser-0.1.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}