{
  "info": {
    "author": "Guillaume Klein",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.11",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "[![CI](https://github.com/guillaumekln/faster-whisper/workflows/CI/badge.svg)](https://github.com/guillaumekln/faster-whisper/actions?query=workflow%3ACI) [![PyPI version](https://badge.fury.io/py/faster-whisper.svg)](https://badge.fury.io/py/faster-whisper)\n\n# Faster Whisper transcription with CTranslate2\n\n**faster-whisper** is a reimplementation of OpenAI's Whisper model using [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models.\n\nThis implementation is up to 4 times faster than [openai/whisper](https://github.com/openai/whisper) for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.\n\n## Benchmark\n\nFor reference, here's the time and memory usage that are required to transcribe [**13 minutes**](https://www.youtube.com/watch?v=0u7tTptBo9I) of audio using different implementations:\n\n* [openai/whisper](https://github.com/openai/whisper)@[6dea21fd](https://github.com/openai/whisper/commit/6dea21fd7f7253bfe450f1e2512a0fe47ee2d258)\n* [whisper.cpp](https://github.com/ggerganov/whisper.cpp)@[3b010f9](https://github.com/ggerganov/whisper.cpp/commit/3b010f9bed9a6068609e9faf52383aea792b0362)\n* [faster-whisper](https://github.com/guillaumekln/faster-whisper)@[cce6b53e](https://github.com/guillaumekln/faster-whisper/commit/cce6b53e4554f71172dad188c45f10fb100f6e3e)\n\n### Large-v2 model on GPU\n\n| Implementation | Precision | Beam size | Time | Max. GPU memory | Max. CPU memory |\n| --- | --- | --- | --- | --- | --- |\n| openai/whisper | fp16 | 5 | 4m30s | 11325MB | 9439MB |\n| faster-whisper | fp16 | 5 | 54s | 4755MB | 3244MB |\n| faster-whisper | int8 | 5 | 59s | 3091MB | 3117MB |\n\n*Executed with CUDA 11.7.1 on a NVIDIA Tesla V100S.*\n\n### Small model on CPU\n\n| Implementation | Precision | Beam size | Time | Max. memory |\n| --- | --- | --- | --- | --- |\n| openai/whisper | fp32 | 5 | 10m31s | 3101MB |\n| whisper.cpp | fp32 | 5 | 17m42s | 1581MB |\n| whisper.cpp | fp16 | 5 | 12m39s | 873MB |\n| faster-whisper | fp32 | 5 | 2m44s | 1675MB |\n| faster-whisper | int8 | 5 | 2m04s | 995MB |\n\n*Executed with 8 threads on a Intel(R) Xeon(R) Gold 6226R.*\n\n## Installation\n\nThe module can be installed from [PyPI](https://pypi.org/project/faster-whisper/):\n\n```bash\npip install faster-whisper\n```\n\n**Other installation methods:**\n\n```bash\n# Install the master branch:\npip install --force-reinstall \"faster-whisper @ https://github.com/guillaumekln/faster-whisper/archive/refs/heads/master.tar.gz\"\n\n# Install a specific commit:\npip install --force-reinstall \"faster-whisper @ https://github.com/guillaumekln/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz\"\n\n# Install for development:\ngit clone https://github.com/guillaumekln/faster-whisper.git\npip install -e faster-whisper/\n```\n\n### GPU support\n\nGPU execution requires the NVIDIA libraries cuBLAS 11.x and cuDNN 8.x to be installed on the system. Please refer to the [CTranslate2 documentation](https://opennmt.net/CTranslate2/installation.html).\n\n## Usage\n\n### Transcription\n\n```python\nfrom faster_whisper import WhisperModel\n\nmodel_size = \"large-v2\"\n\n# Run on GPU with FP16\nmodel = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n\n# or run on GPU with INT8\n# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n# or run on CPU with INT8\n# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n\nsegments, info = model.transcribe(\"audio.mp3\", beam_size=5)\n\nprint(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n\nfor segment in segments:\n    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n```\n\n#### Word-level timestamps\n\n```python\nsegments, _ = model.transcribe(\"audio.mp3\", word_timestamps=True)\n\nfor segment in segments:\n    for word in segment.words:\n        print(\"[%.2fs -> %.2fs] %s\" % (word.start, word.end, word.word))\n```\n\nSee more model and transcription options in the [`WhisperModel`](https://github.com/guillaumekln/faster-whisper/blob/master/faster_whisper/transcribe.py) class implementation.\n\n## Model conversion\n\nWhen loading a model from its size such as `WhisperModel(\"large-v2\")`, the correspondig CTranslate2 model is automatically downloaded from the [Hugging Face Hub](https://huggingface.co/guillaumekln).\n\nWe also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.\n\nFor example the command below converts the [original \"large-v2\" Whisper model](https://huggingface.co/openai/whisper-large-v2) and saves the weights in FP16:\n\n```bash\npip install transformers[torch]>=4.23\n\nct2-transformers-converter --model openai/whisper-large-v2 --output_dir whisper-large-v2-ct2 \\\n    --copy_files tokenizer.json --quantization float16\n```\n\n* The option `--model` accepts a model name on the Hub or a path to a model directory.\n* If the option `--copy_files tokenizer.json` is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.\n\nModels can also be converted from the code. See the [conversion API](https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html).\n\n## Comparing performance against other implementations\n\nIf you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:\n\n* Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, `model.transcribe` uses a default beam size of 1 but here we use a default beam size of 5.\n* When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable `OMP_NUM_THREADS`, which can be set when running your script:\n\n```bash\nOMP_NUM_THREADS=4 python3 my_script.py\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/guillaumekln/faster-whisper",
    "keywords": "openai whisper speech ctranslate2 inference quantization transformer",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "faster-whisper",
    "package_url": "https://pypi.org/project/faster-whisper/",
    "platform": null,
    "project_url": "https://pypi.org/project/faster-whisper/",
    "project_urls": {
      "Homepage": "https://github.com/guillaumekln/faster-whisper"
    },
    "release_url": "https://pypi.org/project/faster-whisper/0.3.0/",
    "requires_dist": [
      "av (==10.*)",
      "ctranslate2 (<4,>=3.10)",
      "huggingface-hub (>=0.13)",
      "tokenizers (==0.13.*)",
      "transformers[torch] (>=4.23) ; extra == 'conversion'",
      "black (==23.*) ; extra == 'dev'",
      "flake8 (==6.*) ; extra == 'dev'",
      "isort (==5.*) ; extra == 'dev'",
      "pytest (==7.*) ; extra == 'dev'"
    ],
    "requires_python": ">=3.8",
    "summary": "Faster Whisper transcription with CTranslate2",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17429417,
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2de45118b4cff04b993c3ce12f5f3cd82e7135ddd9c3fd8fe77f6b1d287c18ce",
          "md5": "249b535159ce4d3c1a4951ca9b735607",
          "sha256": "380b2e17c30f60cdacf0a816cf9265ffb56fffdd099a13c442a6341efabe75e6"
        },
        "downloads": -1,
        "filename": "faster_whisper-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "249b535159ce4d3c1a4951ca9b735607",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 16414,
        "upload_time": "2023-03-22T19:57:38",
        "upload_time_iso_8601": "2023-03-22T19:57:38.104008Z",
        "url": "https://files.pythonhosted.org/packages/2d/e4/5118b4cff04b993c3ce12f5f3cd82e7135ddd9c3fd8fe77f6b1d287c18ce/faster_whisper-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2eda1876106e4b0d9705e4864870eb351d7949d09d273c6f6582ba15c7157e5b",
          "md5": "654786fa060023c0621c592f3ed94f58",
          "sha256": "609ce86b521c762cb204552b6de4121ec6c1c297e7d46a90db29280ac1dd4cd5"
        },
        "downloads": -1,
        "filename": "faster-whisper-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "654786fa060023c0621c592f3ed94f58",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 17240,
        "upload_time": "2023-03-22T19:57:40",
        "upload_time_iso_8601": "2023-03-22T19:57:40.307798Z",
        "url": "https://files.pythonhosted.org/packages/2e/da/1876106e4b0d9705e4864870eb351d7949d09d273c6f6582ba15c7157e5b/faster-whisper-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "34cae9d4de171b0ff00be8c3d0ec6274d2b8d0f03eb943cd8228b4819156da39",
          "md5": "849420029696ffccabea3a7e8fb45d11",
          "sha256": "76c4ac83c65ec214fbd8f8c837a3de666c8585d6892c3468e88ed34374b01641"
        },
        "downloads": -1,
        "filename": "faster_whisper-0.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "849420029696ffccabea3a7e8fb45d11",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 17545,
        "upload_time": "2023-03-24T09:58:23",
        "upload_time_iso_8601": "2023-03-24T09:58:23.873574Z",
        "url": "https://files.pythonhosted.org/packages/34/ca/e9d4de171b0ff00be8c3d0ec6274d2b8d0f03eb943cd8228b4819156da39/faster_whisper-0.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4a92a3e75c399c0ba7f3a3c4d554744bacae45390fde65726ad84adee0f16a43",
          "md5": "07a26d6bdb48172c71a7224b0eecb243",
          "sha256": "bb5d688a370300ff09bb409e57457d0826383977406ee392ed6638474d75af53"
        },
        "downloads": -1,
        "filename": "faster-whisper-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "07a26d6bdb48172c71a7224b0eecb243",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 18602,
        "upload_time": "2023-03-24T09:58:25",
        "upload_time_iso_8601": "2023-03-24T09:58:25.074622Z",
        "url": "https://files.pythonhosted.org/packages/4a/92/a3e75c399c0ba7f3a3c4d554744bacae45390fde65726ad84adee0f16a43/faster-whisper-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "34cae9d4de171b0ff00be8c3d0ec6274d2b8d0f03eb943cd8228b4819156da39",
        "md5": "849420029696ffccabea3a7e8fb45d11",
        "sha256": "76c4ac83c65ec214fbd8f8c837a3de666c8585d6892c3468e88ed34374b01641"
      },
      "downloads": -1,
      "filename": "faster_whisper-0.3.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "849420029696ffccabea3a7e8fb45d11",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 17545,
      "upload_time": "2023-03-24T09:58:23",
      "upload_time_iso_8601": "2023-03-24T09:58:23.873574Z",
      "url": "https://files.pythonhosted.org/packages/34/ca/e9d4de171b0ff00be8c3d0ec6274d2b8d0f03eb943cd8228b4819156da39/faster_whisper-0.3.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4a92a3e75c399c0ba7f3a3c4d554744bacae45390fde65726ad84adee0f16a43",
        "md5": "07a26d6bdb48172c71a7224b0eecb243",
        "sha256": "bb5d688a370300ff09bb409e57457d0826383977406ee392ed6638474d75af53"
      },
      "downloads": -1,
      "filename": "faster-whisper-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "07a26d6bdb48172c71a7224b0eecb243",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 18602,
      "upload_time": "2023-03-24T09:58:25",
      "upload_time_iso_8601": "2023-03-24T09:58:25.074622Z",
      "url": "https://files.pythonhosted.org/packages/4a/92/a3e75c399c0ba7f3a3c4d554744bacae45390fde65726ad84adee0f16a43/faster-whisper-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}