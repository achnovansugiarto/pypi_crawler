{
  "info": {
    "author": "palak_agrawal",
    "author_email": "palakagrawaljk@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# PY_4\n\nThis is a website statistics/benchmarking tool which helps to crawl data from websites and generate reports in YAML, HTML, JSON, CSV format. It helps to inmplement a command line utility which gathers statistics of a website.\n\nThis utility will also gather a list of broken links, a list of links leading to external websites, and loading time for each web page on the website. The command-line utility will crawl the entire website to collect the statistics and also store them into a local database. It will also provide an option to generate a report on a terminal standard output or an HTML file for the statistics collected.\n\nThis utility includes the following modules:\n1. Website Crawler : crawler.py\n2. Statistics Data Model : storage.py\n3. Report Generator : representation.py\n4. Command Line Parser : command_parser.py\n\n# Website Crawler \nIt crawls each page in websites concurrently and handles all the errors and exceptions.\n\n# Statistics Data Model\nIt stores the crawling statistics into an organised format in the database. It provides an abstraction for storing statistics and extracting them for reports.\n\n# Report Generator\nIt helps to generate reports on standard output or to a file. Formats accessible are YAML(default), HTML. It also has a provision to add new report generator components to generate report in JSON formats, CSV formats, etc. as a plugin without affecting other components of the application.\n\n# Command Line Parser\nIt is responsible for parsing command line arguments and generating web response data formats and utility helpers.\n\n\nIt can run as an application, by running the command './website-stats.py' with command-line arguments in windows / mac terminal.\nIt is also be possible to load it as a module within python program using the package website_stats.\n\n# Requirements:\n\npip install pyyaml\nTo generate yaml report\n\n# Package\nThis final build can be used as a standalone installable package or as a developer library that can be loaded as a module and extended in another python application. \n\nYou just need to run this command on terminal:\npip install pycrawler\n\nYou can use this library in your module using the below line:\nfrom PY_4 import website_stats\n\nInclude this in your main:\nif __name__ == '__main__':\n     website_stats.run()\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://git.corp.adobe.com/yanand/PY_4",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pycrawler-04",
    "package_url": "https://pypi.org/project/pycrawler-04/",
    "platform": "",
    "project_url": "https://pypi.org/project/pycrawler-04/",
    "project_urls": {
      "Homepage": "https://git.corp.adobe.com/yanand/PY_4"
    },
    "release_url": "https://pypi.org/project/pycrawler-04/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "A crawler",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8289804,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b1c8f50e907178716cced0ce8bdfffd73bf4f19f8bd7cae2d8cf7a7fabea5e7e",
          "md5": "238f2c768cd0a84b2ca909e7e8825529",
          "sha256": "66aa0d94f213a3907888fb40d1da667fda0f715d36394ca4c02567dda4546cc9"
        },
        "downloads": -1,
        "filename": "pycrawler_04-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "238f2c768cd0a84b2ca909e7e8825529",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 10811,
        "upload_time": "2020-09-28T14:21:25",
        "upload_time_iso_8601": "2020-09-28T14:21:25.906783Z",
        "url": "https://files.pythonhosted.org/packages/b1/c8/f50e907178716cced0ce8bdfffd73bf4f19f8bd7cae2d8cf7a7fabea5e7e/pycrawler_04-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "09d2e3d4bb93253021a862541a0d4352a797bfc44f8b81d9af5f0f833e6c1161",
          "md5": "14ca75367e2441e1471edf45e71a1594",
          "sha256": "b170fb62c3b927ca7a28fe429d58a56a0dee19cf3d469ccf5fa11c1cedab1989"
        },
        "downloads": -1,
        "filename": "pycrawler_04-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "14ca75367e2441e1471edf45e71a1594",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 9620,
        "upload_time": "2020-09-28T14:21:29",
        "upload_time_iso_8601": "2020-09-28T14:21:29.082774Z",
        "url": "https://files.pythonhosted.org/packages/09/d2/e3d4bb93253021a862541a0d4352a797bfc44f8b81d9af5f0f833e6c1161/pycrawler_04-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "b1c8f50e907178716cced0ce8bdfffd73bf4f19f8bd7cae2d8cf7a7fabea5e7e",
        "md5": "238f2c768cd0a84b2ca909e7e8825529",
        "sha256": "66aa0d94f213a3907888fb40d1da667fda0f715d36394ca4c02567dda4546cc9"
      },
      "downloads": -1,
      "filename": "pycrawler_04-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "238f2c768cd0a84b2ca909e7e8825529",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 10811,
      "upload_time": "2020-09-28T14:21:25",
      "upload_time_iso_8601": "2020-09-28T14:21:25.906783Z",
      "url": "https://files.pythonhosted.org/packages/b1/c8/f50e907178716cced0ce8bdfffd73bf4f19f8bd7cae2d8cf7a7fabea5e7e/pycrawler_04-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "09d2e3d4bb93253021a862541a0d4352a797bfc44f8b81d9af5f0f833e6c1161",
        "md5": "14ca75367e2441e1471edf45e71a1594",
        "sha256": "b170fb62c3b927ca7a28fe429d58a56a0dee19cf3d469ccf5fa11c1cedab1989"
      },
      "downloads": -1,
      "filename": "pycrawler_04-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "14ca75367e2441e1471edf45e71a1594",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 9620,
      "upload_time": "2020-09-28T14:21:29",
      "upload_time_iso_8601": "2020-09-28T14:21:29.082774Z",
      "url": "https://files.pythonhosted.org/packages/09/d2/e3d4bb93253021a862541a0d4352a797bfc44f8b81d9af5f0f833e6c1161/pycrawler_04-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}