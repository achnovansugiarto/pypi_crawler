{
  "info": {
    "author": "Kevin Yang",
    "author_email": "yang.kevin@microsoft.com",
    "bugtrack_url": null,
    "classifiers": [
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "Pytorch modules and utilities for modeling biological sequence data.\n\nHere we will demonstrate the application of several tools we hope will help with modeling biological sequences.\n\n## Installation\n\n```\n$ pip install sequence-models\n$ pip install git+https://github.com/microsoft/protein-sequence-models.git  # bleeding edge, current repo main branch\n\n```\n\n## Loading pretrained models\n\nModels require PyTorch. We tested on `v1.9.0`, `v1.11.0`,and `1.12`. If you installed into a clean conda environment, you may also need to install pandas, scipy, and wget. \n\nTo load a model:\n\n```\nfrom sequence_models.pretrained import load_model_and_alphabet\n\nmodel, collater = load_model_and_alphabet('carp_640M')\n```\n\nAvailable models are\n\n- `carp_600k`\n- `carp_38M`\n- `carp_76M`\n- `carp_640M`\n- `mif`\n- `mifst`\n- `bigcarp_esm1bfinetune`\n- `bigcarp_esm1bfrozen`\n- `bigcarp_random`\n\n\n## Convolutional autoencoding representations of proteins (CARP)\n\nWe make available pretrained CNN protein sequence masked language models of various sizes. All of these have a ByteNet encoder architecture and are pretrained on the March 2020 release of UniRef50 using the same masked language modeling task as in BERT and ESM-1b.\n\nCARP is described in this [preprint](https://doi.org/10.1101/2022.05.19.492714).\n\nYou can also download the weights manually from [Zenodo](https://doi.org/10.5281/zenodo.6368483). \n\nTo encode a batch of sequences: \n\n```\nseqs = [['MDREQ'], ['MGTRRLLP']]\nx = collater(seqs)[0]  # (n, max_len)\nrep = model(x)  # (n, max_len, d_model)\n```\n\nCARP also supports computing representations from arbitrary layers and the final logits.\n\n```\nrep = model(x, repr_layers=[0, 2, 32], logits=True)\n```\n\n### Compute embeddings in bulk from FASTA\n\nWe provide a script that efficiently extracts embeddings in bulk from a FASTA file. A cuda device is optional and will be auto-detected. The following command extracts the final-layer embedding for a FASTA file from the `CARP_640M` model:\n\n```\n$ python scripts/extract.py carp_640M examples/some_proteins.fasta \\\n    examples/results/some_proteins_emb_carp_640M/ \\\n    --repr_layers 0 32 33 logits --include mean per_tok\n```\nDirectory `examples/results/some_proteins_emb_carp_640M/` now contains one `.pt` file per extracted embedding; use `torch.load()` to load them. `scripts/extract.py` has flags that determine what .pt files are included:\n\n`--repr-layers` (default: final only) selects which layers to include embeddings from. `0` is the input embedding. `logits` is the per-token logits.\n\n`--include` specifies what embeddings to save. You can use the following:\n\n- `per_tok` includes the full sequence, with an embedding per amino acid (seq_len x hidden_dim).\n- `mean` includes the embeddings averaged over the full sequence, per layer (only valid for representations).\n- `logp` computes the average log probability per sequence and stores it in a csv (only valid for logits).\n\n`scripts/extract.py` also has `--batchsize` and `--device` flags. For example, to use GPU 2 on a multi-GPU machine, pass `--device cuda:2`. The default is to use a batchsize of 1 and `cpu` if cuda is not detected or `cuda:0` if cuda is detected. \n\n## Masked Inverse Folding (MIF) and Masked Inverse Folding with Sequence Transfer (MIF-ST)\n\nWe make available pretrained masked inverse folding models with and without sequence pretraining transfer from CARP-640M.\n\nMIF and MIF-ST are described in this [preprint](https://doi.org/10.1101/2022.05.25.493516)\n\nYou can also download the weights manually from [Zenodo](https://zenodo.org/record/6573779#.YqjXT-zMI-Q). \n\nTo encode a sequence with its structure: \n\n```\nfrom sequence_models.pdb_utils import parse_PDB, process_coords\ncoords, wt, _ = parse_PDB('examples/gb1_a60fb_unrelaxed_rank_1_model_5.pdb')\ncoords = {\n        'N': coords[:, 0],\n        'CA': coords[:, 1],\n        'C': coords[:, 2]\n    }\ndist, omega, theta, phi = process_coords(coords)\nbatch = [[wt, torch.tensor(dist, dtype=torch.float),\n          torch.tensor(omega, dtype=torch.float),\n          torch.tensor(theta, dtype=torch.float), torch.tensor(phi, dtype=torch.float)]]\nsrc, nodes, edges, connections, edge_mask = collater(batch)\n# can use result='repr' or result='logits'. Default is 'repr'.\nrep = model(src, nodes, edges, connections, edge_mask)  \n```\n\n### Compute embeddings in bulk from csv\n\nWe provide a script that efficiently extracts embeddings in bulk from a csv file. A cuda device is optional and will be auto-detected. The following command extracts the final-layer embedding for a FASTA file from the `mifst` model:\n\n```\n$ python scripts/extract_mif.py mifst examples/gb1s.csv \\\n    examples/ \\\n    examples/results/some_proteins_mifst/ \\\n    repr --include mean per_tok\n```\nDirectory `examples/results/some_proteins_mifst/` now contains one `.pt` file per extracted embedding; use `torch.load()` to load them. `scripts/extract_mif.py` has flags that determine what .pt files are included:\n\nThe syntax is:\n```\n$ python script/extract_mif.py <model> <csv_fpath> <pdb_dir> <out_dir> <result> --include <pooling options>\n```\n\nThe input csv should have columns for `name`, `sequence`, and `pdb`. The script looks in `pdb_dir` for the filenames in the `pdb` column. \n\nThe options for `result` are `repr` or `logits`.\n\n`--include` specifies what embeddings to save. You can use the following:\n\n- `per_tok` includes the full sequence, with an embedding per amino acid (seq_len x hidden_dim).\n- `mean` includes the embeddings averaged over the full sequence, per layer (only valid for representations).\n- `logp` computes the average log probability per sequence and stores it in a csv (only valid for logits).\n\n\n`scripts/extract.py` also has a `--device` flags. For example, to use GPU 2 on a multi-GPU machine, pass `--device cuda:2`. The default is to use `cpu` if cuda is not detected or `cuda:0` if cuda is detected. \n\n\n## Biosynthetic gene cluster CARP (BiGCARP)\n\nWe make available pretrained CNN Pfam domain masked language models of BGCs. All of these have a ByteNet encoder architecture and are pretrained on antiSMASH using the same masked language modeling task as in BERT and ESM-1b.\n\nBiGCARP is described in this [preprint](https://doi.org/10.1101/2022.07.22.500861). Training code is available [here](https://github.com/microsoft/protein-sequence-models).\n\nYou can also download the weights and datasets manually from [Zenodo](https://doi.org/10.5281/zenodo.6857704). \n\nTo encode a batch of sequences: \n\n```\nbgc = [['#;PF07690;PF06609;PF00083;PF00975;PF12697;PF00550;PF14765'],\n       ['t3pks;PF07690;PF06609;PF00083;PF00975;PF12697;PF00550;PF14765;PF00698']]\nmodel, collater = load_model_and_alphabet('bigcarp_esm1bfinetune')\nx = collater(bgc)[0]\nrep = model(x)\n```\n\n\n### Sequence Datasets and Dataloaders\nIn ```sampler.py```, you will find two Pytorch sampler classes: ```SortishSampler```, a sampler to sort similarly length \n sample sequences into length-defined buckets; and ```ApproxBatchSampler```, a batch sampler which grabs sequences \n from length-defined buckets until the batch has the set approximate max number of tokens or max number of tokens squared.\n\n```\nfrom sequence_models.samplers import SortishSampler, ApproxBatchSampler\n\n# grab datasets\nds = dataset # your sequence dataset\n\n# build dataloaders\nlen_ds = np.array([len(i[0]) for i in ds]) # list of lengths of the sequence in dataset (in order)\nbucket_size = 1000 # number of length-defined buckets\nmax_tokens = 8000 # max number of tokens per batch\nmax_batch_size = 100 # max number of samples per batch\nsortish_sampler = SortishSampler(len_ds, bucket_size)\nbatch_sampler = ApproxBatchSampler(sortish_sampler, max_tokens, max_batch_size, len_ds)\ncollater = collater # your collater function\ndl = DataLoader(ds_train, collate_fn=collater, batch_sampler=batch_sampler, num_workers=16)\n``` \n\n### Pre-implemented Models\n* Struct2SeqDecoder (GNN)\n\nThe ```Struct2SeqDecoder``` model was adapted from \n[Ingraham et al.](https://papers.nips.cc/paper/2019/file/f3a4ff4839c56a5f460c88cce3666a2b-Paper.pdf). This model uses protein structural information \nencoded as a graph nodes and edges representing the structural information of each amino acid residue and their \nrelations to each other, respectively.\n\nIf you already have node features, edge features, connections between nodes, encoded sequences (src), \nand edge mask (edge_mask); you can directly use the the ```Struct2SeqDecoder``` as demonstrated below:\n\n```\nfrom sequence_models.constants import trR_ALPHABET\nfrom sequence_models.gnn import Struct2SeqDecoder\n\nnum_letters = len(trR_ALPHABET) # length of your amino acid alphabet  \nnode_features = 10 # number of node features\nedge_features = 11 # number of edge features\nhidden_dim =  128 # your choice of hidden layer dimension\nnum_decoder_layers = 3 # your choice of number of decoder layers to use\ndropout = 0.1 # dropout used by decoder layer\nuse_mpnn = False # if True, use MPNN layer, else use Transformer layer for decoder \ndirection = 'bidirectional' # direction of information flow/masking: forward, backward or bidirectional \n\nmodel = Struct2SeqDecoder(num_letters, node_features, edge_features, hidden_dim,\n            num_decoder_layers, dropout, use_mpnn, direction)\nout = model(nodes, edges, connections, src, edge_mask)\n```\n\nIf you do not have prepared inputs, but have 2d maps representing the distance between residues (dist) and the dihedral\nangles between residues (omega, theta, and phi), you can use our preprocessing functions to generate nodes, edges, and \nconnections as demonstrated below:\n\n```\nfrom sequence_models.gnn import get_node_features, get_k_neighbors, get_edge_features, \\\n    get_mask, replace_nan\n\n# process features\nnode = get_node_features(omega, theta, phi) # generate nodes\ndist = dist.fill_diagonal_(np.nan) # if the diagonal of dist tensor is not already filled with nans, it should \n                                    # to prevent selecting self when getting k nearest residues in the next step \nconnections = get_k_neighbors(dist, n_connections) # get connections\nedge = get_edge_features(dist, omega, theta, phi, connections) # generate edge\nedge_mask = get_mask(edge) # get edge mask (in the scenario where there is missing edge features between neighbors)\nedge = replace_nan(edge) # replace nans with 0s \nnode = replace_nan(node) \n```\n\nAlternatively, we have also prepared ```StructureCollater```, a collater function \nfound in ```collaters.py``` that also performs this task:\n\n```\nfrom sequence_models.collaters import StructureCollater\n\nn_connections = 20 # number of connections per amino acid residue  \ncollater = StructureCollater(n_connections=n_connections)\nds = dataset # Dataset must return sequences, dists, omegas, thetas, phis \ndl = Dataloader(ds, collate_fn=collater)\n```\n\n* ByteNet\n\nThe ```ByteNet``` model was adapted from [Kalchbrenner et al.](https://arxiv.org/abs/1610.10099). ByteNet uses stacked\nconvolutional encoder and decoder layers to preserve temporal resolution of \nsequential data. \n\n```\nfrom sequence_models.convolutional import ByteNet\nfrom sequence_models.constants import trR_ALPHABET\n\nn_tokens = len(trR_ALPHABET) # number of tokens in token dictionary\nd_embedding = 128 # dimension of embedding\nd_model = 128 # dimension to use within ByteNet model, //2 every layer\nn_layers = 3 # number of layers of ByteNet block\nkernel_size = 3 # the kernel width\nr = ??? # used to calculate dilation factor\npadding_idx = trR_ALPHABET.index('-') # location of padding token in ordered alphabet\ncausal = True # if True, chooses MaskedCausalConv1d() over MaskedConv1d()\ndropout = 0.1 \n\nx = torch.randn(32, 128) # input (n samples, len of seqs) \ninput_mask = torch.ones(32, 128, 1) # mask (n samples, len of seqs, 1)\nmodel = ByteNet(n_tokens, d_embedding, d_model, n_layers, kernel_size, r, \n            padding_idx=padding_idx, causal=causal, dropout=dropout)\nout = model(x, input_mask) \n```\n\nWe have also an implemented versions of ```ByteNet``` to be able to use 2d inputs (```ByteNet2d```) \nand as a language model (```ByteNetLM```): \n\n```\nfrom sequence_models.convolutional import ByteNet2d, ByteNetLM\n\nx = torch.randn(32, 128, 128, 64) # input (n samples, len of seqs, len of seqs, feature dimension)\ninput_mask = torch.ones # (n samples, len of seqs, len of seqs, 1), optional\nmodel = ByteNet2d(d_in, d_model, n_layers, kernel_size, r, dropout=0.0)\nout = model(x, input_mask)\n\nx = torch.randn(32, 128) # input (n samples, len of seqs) \ninput_mask = torch.ones(32, 128, 1) # mask (n samples, len of seqs, 1)\nmodel = ByteNetLM(n_tokens, d_embedding, d_model, n_layers, kernel_size, r,\n                    padding_idx=None, causal=False, dropout=0.0)\nout = model(x, input_mask)\n```\n\n* trRosetta\nThe ```trRosetta``` model was implemented according to [Yang et al.](https://www.pnas.org/content/117/3/1496). In this model, multiple sequence\nalignments (MSAs) are used to predict distances between amino acid residues as well as their dihedral \nangles (omega, theta, phi). Predictions are in the format of bins. Omega, theta and phi angle are binned into 24, 24, and 12 bins, respectively \nwith 15 degrees segments and one no-contact bin. [Yang et al.](https://www.pnas.org/content/117/3/1496) has pretrained five models (model ids: 'a', 'b', 'c', 'd', 'e'). To run\na single model: \n\n```\nfrom sequence_models.trRosetta_utils import trRosettaPreprocessing, parse_a3m\nfrom sequence_models.trRosetta import trRosetta\nfrom sequence_models.constants import trR_ALPHABET\n\nmsas = parse_a3m(path_to_msa) # load in msas in a3m format\nalphabet = trR_ALPHABET # load your alphabet order\ntr_preprocessing = trRosettaPreprocessing(alphabet) # setup preprocessor for msa\nmsas_processed = tr_preprocessing.process(msas)\n\nn2d_layers = 61 # keep at 61 if you want to use pretrained version\nmodel_id = 'a' # choose your pretrained model id\ndecoder = True # if True, return 2d structure maps, else returns hidden layer\np_dropout = 0.0\nmodel = trRosetta(n2d_layers, model_id, decoder, p_dropout)\nout = model(msas_processed) # returns dist_probs, theta_probs, phi_probs, omega_probs\n```\n\nTo run an ensemble of models: \n```\nfrom sequence_models.trRosetta_utils import trRosettaPreprocessing, parse_a3m\nfrom sequence_models.trRosetta import trRosetta, trRosettaEnsemble\nfrom sequence_models.constants import trR_ALPHABET\n\nmsas = parse_a3m(path_to_msa) # load in msas in a3m format\nalphabet = trR_ALPHABET # load your alphabet order\ntr_preprocessing = trRosettaPreprocessing(alphabet) # setup preprocessor for msa\nmsas_processed = tr_preprocessing.process(msas)\n\nn2d_layers = 61 # keep at 61 if you want to use pretrained version\nmodel_ids = 'abcde' # choose your pretrained model id\ndecoder = True # if True, return 2d structure maps, else returns hidden layer\np_dropout = 0.0\nbase_model = trRosetta\nmodel = trRosettaEnsemble(base_model, n2d_layers, model_ids)\nout = model(msas_processed)\n```\n\nIf you would like to convert bin prediction into actual values, use ```probs2value```. \nHere is an example of converting distance bin predictions into values: \n\n```\nfrom sequence_models.trRosetta_utils import probs2value\n\ndist_probs, theta_probs, phi_probs, omega_probs  = model(x) # structure predictions (batch, # of bins, len of seq, len of seq)\npreperty = 'dist' # choose between 'dist', 'theta', 'phi', or 'omega'\nmask = mask # your 2d mask (batch, len of seq, len of seq)\ndist_values = probs2value(dist, property, mask):\n\n```\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/microsoft/protein-sequence-models",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "sequence-models",
    "package_url": "https://pypi.org/project/sequence-models/",
    "platform": null,
    "project_url": "https://pypi.org/project/sequence-models/",
    "project_urls": {
      "Homepage": "https://github.com/microsoft/protein-sequence-models"
    },
    "release_url": "https://pypi.org/project/sequence-models/1.6.0/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Machine learning for sequences.",
    "version": "1.6.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14720336,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "58d926f7636fcda07a16c14de47a83b043810961c43f281cd07fd91855e6e644",
          "md5": "41461a0bbe0213cac59169aef2ab740f",
          "sha256": "6fa57920367635d993966ecdcfd6d46dc9cbbe5769cc27ffa2cea0e20dc44e99"
        },
        "downloads": -1,
        "filename": "sequence_models-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "41461a0bbe0213cac59169aef2ab740f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 58223,
        "upload_time": "2022-03-18T18:46:44",
        "upload_time_iso_8601": "2022-03-18T18:46:44.753602Z",
        "url": "https://files.pythonhosted.org/packages/58/d9/26f7636fcda07a16c14de47a83b043810961c43f281cd07fd91855e6e644/sequence_models-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5958da002f528242116c87ddf20bd64867d9a9a4faf6ba8f712effdeacaec523",
          "md5": "634ff5aad32dbe7ba7fd80efaa852a06",
          "sha256": "59019057da140311b9a43894c09971d36184f88acabaf891d9c634bece67cd41"
        },
        "downloads": -1,
        "filename": "sequence-models-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "634ff5aad32dbe7ba7fd80efaa852a06",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 49336,
        "upload_time": "2022-03-18T18:46:46",
        "upload_time_iso_8601": "2022-03-18T18:46:46.333813Z",
        "url": "https://files.pythonhosted.org/packages/59/58/da002f528242116c87ddf20bd64867d9a9a4faf6ba8f712effdeacaec523/sequence-models-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f232e4bda34b3330ae381677c661e8166116dea03cc2bdccf51eec8e0be22872",
          "md5": "a1b99eea530d11122adc0ea8684e6a0b",
          "sha256": "9ced2cb9c2204bf986a028adb31ea91dfcdaf0ea09fb0661b552edb268a50427"
        },
        "downloads": -1,
        "filename": "sequence_models-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a1b99eea530d11122adc0ea8684e6a0b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 57745,
        "upload_time": "2022-05-26T14:46:28",
        "upload_time_iso_8601": "2022-05-26T14:46:28.548890Z",
        "url": "https://files.pythonhosted.org/packages/f2/32/e4bda34b3330ae381677c661e8166116dea03cc2bdccf51eec8e0be22872/sequence_models-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "56967a0e30857ee7b1953f54bfd10565630857ad6d9cc71371c3d72af1f523a1",
          "md5": "f25ec941c6d7019e9f1286b53ed9edc9",
          "sha256": "4dcd053905e6fa02f21e9e1697d269833688d75d9dcca5d1e30c25ac98d90c14"
        },
        "downloads": -1,
        "filename": "sequence-models-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f25ec941c6d7019e9f1286b53ed9edc9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 52303,
        "upload_time": "2022-05-26T14:46:32",
        "upload_time_iso_8601": "2022-05-26T14:46:32.208895Z",
        "url": "https://files.pythonhosted.org/packages/56/96/7a0e30857ee7b1953f54bfd10565630857ad6d9cc71371c3d72af1f523a1/sequence-models-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "45501a2555dee416d8f6fa5025474b378967a250cfd52a10873907dde931ef06",
          "md5": "1613c24bf0beb19e863f325b4102e21f",
          "sha256": "b9ddb66cecc9ee50b1d3e155139133173f3c8f7539e880d650c21ad5d4c1a9ba"
        },
        "downloads": -1,
        "filename": "sequence_models-1.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1613c24bf0beb19e863f325b4102e21f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 57738,
        "upload_time": "2022-06-02T14:18:53",
        "upload_time_iso_8601": "2022-06-02T14:18:53.084105Z",
        "url": "https://files.pythonhosted.org/packages/45/50/1a2555dee416d8f6fa5025474b378967a250cfd52a10873907dde931ef06/sequence_models-1.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "282415bcbef786ac99dc30ffceb6056cde5423dd9dc1f310803a329eaea948a9",
          "md5": "389bb94eaa3828a8208d1f76f9c4d7eb",
          "sha256": "f887916fcdd117c61e2beee840f7f40a61bc0c7a8194c37ad95f8feada487a91"
        },
        "downloads": -1,
        "filename": "sequence-models-1.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "389bb94eaa3828a8208d1f76f9c4d7eb",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 52266,
        "upload_time": "2022-06-02T14:18:56",
        "upload_time_iso_8601": "2022-06-02T14:18:56.978789Z",
        "url": "https://files.pythonhosted.org/packages/28/24/15bcbef786ac99dc30ffceb6056cde5423dd9dc1f310803a329eaea948a9/sequence-models-1.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ff04de3e6b7afeeb48762c03dc799ab2aecc7644a054b4b1160115f9aedd9657",
          "md5": "f01cc287270f418da36a979b89be5067",
          "sha256": "6bcec88a32d3871c7844ced763116dd1c183c23f4674e6774c01db9deb3782ed"
        },
        "downloads": -1,
        "filename": "sequence_models-1.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f01cc287270f418da36a979b89be5067",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 58582,
        "upload_time": "2022-06-27T19:45:30",
        "upload_time_iso_8601": "2022-06-27T19:45:30.495058Z",
        "url": "https://files.pythonhosted.org/packages/ff/04/de3e6b7afeeb48762c03dc799ab2aecc7644a054b4b1160115f9aedd9657/sequence_models-1.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f0d98c4941d92ba19322f27b4b34aad6957e5d0c87950e88c7023d85a8828332",
          "md5": "449580e4eac6266b6b3da016bf6bc468",
          "sha256": "5d8f94aeecac71b0d66a9a9906c45b70551bba5df4dee41939a6fc5e4e532e85"
        },
        "downloads": -1,
        "filename": "sequence-models-1.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "449580e4eac6266b6b3da016bf6bc468",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 52834,
        "upload_time": "2022-06-27T19:45:34",
        "upload_time_iso_8601": "2022-06-27T19:45:34.636415Z",
        "url": "https://files.pythonhosted.org/packages/f0/d9/8c4941d92ba19322f27b4b34aad6957e5d0c87950e88c7023d85a8828332/sequence-models-1.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.4.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5f4c5014fd0dee011ba363c308729f3bba92c93109f7f7dabf378a7773c762e9",
          "md5": "a6c07ce8ad6356ef3a12c6e71f4db5e2",
          "sha256": "eab96955021fc9b5bba757e7a8eff43e20335b98d8cefc521f3a2ce631efa369"
        },
        "downloads": -1,
        "filename": "sequence_models-1.4.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a6c07ce8ad6356ef3a12c6e71f4db5e2",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 58907,
        "upload_time": "2022-07-19T15:30:15",
        "upload_time_iso_8601": "2022-07-19T15:30:15.379782Z",
        "url": "https://files.pythonhosted.org/packages/5f/4c/5014fd0dee011ba363c308729f3bba92c93109f7f7dabf378a7773c762e9/sequence_models-1.4.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "11039a69e4fac7b24e93dad2fc044f6e394889ffa9d572e1e161c2baf8d0e66f",
          "md5": "ab4c366b0e426312abe3d734bc3aabc5",
          "sha256": "25477fef2a56ea13a34787c25ff7e642f262dbb7b1cda77c631c7c9c09f2818c"
        },
        "downloads": -1,
        "filename": "sequence-models-1.4.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ab4c366b0e426312abe3d734bc3aabc5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 53491,
        "upload_time": "2022-07-19T15:30:19",
        "upload_time_iso_8601": "2022-07-19T15:30:19.896525Z",
        "url": "https://files.pythonhosted.org/packages/11/03/9a69e4fac7b24e93dad2fc044f6e394889ffa9d572e1e161c2baf8d0e66f/sequence-models-1.4.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8098e799c7b6946fef4f8bbc8190e1d60d5390f77087cf21008ccb102ace5e00",
          "md5": "d6235ff7835ef33e7ae37c3f1bebbe64",
          "sha256": "dfc242633fac2e265f02dd29a90054bcf1c378f90fe9e494dd12f608a0a15349"
        },
        "downloads": -1,
        "filename": "sequence_models-1.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d6235ff7835ef33e7ae37c3f1bebbe64",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 59740,
        "upload_time": "2022-07-27T19:57:05",
        "upload_time_iso_8601": "2022-07-27T19:57:05.606814Z",
        "url": "https://files.pythonhosted.org/packages/80/98/e799c7b6946fef4f8bbc8190e1d60d5390f77087cf21008ccb102ace5e00/sequence_models-1.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f6aa9e7ad79efc95be1de18aad2c0bf79550eb7ebfe3e5860472076e2e8d9b12",
          "md5": "ee12b8ef6f82f21196f54e18bbb84e93",
          "sha256": "068d91699a708e349148fe6e5edf25525335aa5821d0200a244fc2de2d4c8441"
        },
        "downloads": -1,
        "filename": "sequence-models-1.5.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ee12b8ef6f82f21196f54e18bbb84e93",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 55137,
        "upload_time": "2022-07-27T19:57:17",
        "upload_time_iso_8601": "2022-07-27T19:57:17.145157Z",
        "url": "https://files.pythonhosted.org/packages/f6/aa/9e7ad79efc95be1de18aad2c0bf79550eb7ebfe3e5860472076e2e8d9b12/sequence-models-1.5.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.6.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "efec5724b393112ea6857b1cc8e6d8c06938a73d9f7bbff9ba39f34b8706e178",
          "md5": "ee214db2712c1cca1f0c217064c40fd1",
          "sha256": "7f6360020c044ec6d7e4d27f4adcf7587f659ffda4bd16b75b2792476ffd495b"
        },
        "downloads": -1,
        "filename": "sequence_models-1.6.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ee214db2712c1cca1f0c217064c40fd1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 60493,
        "upload_time": "2022-08-10T18:11:50",
        "upload_time_iso_8601": "2022-08-10T18:11:50.062334Z",
        "url": "https://files.pythonhosted.org/packages/ef/ec/5724b393112ea6857b1cc8e6d8c06938a73d9f7bbff9ba39f34b8706e178/sequence_models-1.6.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63f6cdd1f67f571da87592c7538e2814622cfa06ba3eca68fedee85edf9f8b1b",
          "md5": "97fca11bfb434e50c4338d3b42976cb8",
          "sha256": "832468c66532065625ee36a38b2b070dd729cf2ff470446d60e646575c80e364"
        },
        "downloads": -1,
        "filename": "sequence-models-1.6.0.tar.gz",
        "has_sig": false,
        "md5_digest": "97fca11bfb434e50c4338d3b42976cb8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 56076,
        "upload_time": "2022-08-10T18:12:03",
        "upload_time_iso_8601": "2022-08-10T18:12:03.977104Z",
        "url": "https://files.pythonhosted.org/packages/63/f6/cdd1f67f571da87592c7538e2814622cfa06ba3eca68fedee85edf9f8b1b/sequence-models-1.6.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "efec5724b393112ea6857b1cc8e6d8c06938a73d9f7bbff9ba39f34b8706e178",
        "md5": "ee214db2712c1cca1f0c217064c40fd1",
        "sha256": "7f6360020c044ec6d7e4d27f4adcf7587f659ffda4bd16b75b2792476ffd495b"
      },
      "downloads": -1,
      "filename": "sequence_models-1.6.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ee214db2712c1cca1f0c217064c40fd1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 60493,
      "upload_time": "2022-08-10T18:11:50",
      "upload_time_iso_8601": "2022-08-10T18:11:50.062334Z",
      "url": "https://files.pythonhosted.org/packages/ef/ec/5724b393112ea6857b1cc8e6d8c06938a73d9f7bbff9ba39f34b8706e178/sequence_models-1.6.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "63f6cdd1f67f571da87592c7538e2814622cfa06ba3eca68fedee85edf9f8b1b",
        "md5": "97fca11bfb434e50c4338d3b42976cb8",
        "sha256": "832468c66532065625ee36a38b2b070dd729cf2ff470446d60e646575c80e364"
      },
      "downloads": -1,
      "filename": "sequence-models-1.6.0.tar.gz",
      "has_sig": false,
      "md5_digest": "97fca11bfb434e50c4338d3b42976cb8",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 56076,
      "upload_time": "2022-08-10T18:12:03",
      "upload_time_iso_8601": "2022-08-10T18:12:03.977104Z",
      "url": "https://files.pythonhosted.org/packages/63/f6/cdd1f67f571da87592c7538e2814622cfa06ba3eca68fedee85edf9f8b1b/sequence-models-1.6.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}