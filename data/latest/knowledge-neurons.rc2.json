{
  "info": {
    "author": "Sid Black",
    "author_email": "sdtblck@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# knowledge-neurons\n\nAn open source repository replicating the 2021 paper *[Knowledge Neurons in Pretrained Transformers](https://arxiv.org/abs/2104.08696)* by Dai et al., and extending the technique to autoregressive models, as well as MLMs.\n\nThe Huggingface Transformers library is used as the backend, so any model you want to probe must be implemented there. \n\nCurrently integrated models:\n```python\nBERT_MODELS = [\"bert-base-uncased\", \"bert-base-multilingual-uncased\"]\nGPT2_MODELS = [\"gpt2\"]\nGPT_NEO_MODELS = [\n    \"EleutherAI/gpt-neo-125M\",\n    \"EleutherAI/gpt-neo-1.3B\",\n    \"EleutherAI/gpt-neo-2.7B\",\n]\n```\n\nThe technique from Dai et al. has been used to locate knowledge neurons in `bert-base-uncased` for all the head/relation/tail entities in the PARAREL dataset. Both the neurons, and more detailed results of the experiment are published at `bert_base_uncased_neurons/*.json` and can be replicated by running `pararel_evaluate.py`. More details in the `Evaluations on the PARAREL dataset` section. \n\n# Setup\n\nEither clone the github, and run scripts from there:\n\n```bash\ngit clone knowledge-neurons\ncd knowledge-neurons\n```\n\nOr install as a pip package:\n\n```\npip install knowledge-neurons\n```\n\n# Usage & Examples\n\nAn example using bert-base-uncased:\n\n```python\nfrom knowledge_neurons import KnowledgeNeurons, initialize_model_and_tokenizer, model_type\nimport random\n\n# first initialize some hyperparameters\nMODEL_NAME = \"bert-base-uncased\"\n\n# to find the knowledge neurons, we need the same 'facts' expressed in multiple different ways, and a ground truth\nTEXTS = [\n    \"Sarah was visiting [MASK], the capital of france\",\n    \"The capital of france is [MASK]\",\n    \"[MASK] is the capital of france\",\n    \"France's capital [MASK] is a hotspot for romantic vacations\",\n    \"The eiffel tower is situated in [MASK]\",\n    \"[MASK] is the most populous city in france\",\n    \"[MASK], france's capital, is one of the most popular tourist destinations in the world\",\n]\nTEXT = TEXTS[0]\nGROUND_TRUTH = \"paris\"\n\n# these are some hyperparameters for the integrated gradients step\nBATCH_SIZE = 20\nSTEPS = 20 # number of steps in the integrated grad calculation\nADAPTIVE_THRESHOLD = 0.3 # in the paper, they find the threshold value `t` by multiplying the max attribution score by some float - this is that float.\nP = 0.5 # the threshold for the sharing percentage\n\n# setup model & tokenizer\nmodel, tokenizer = initialize_model_and_tokenizer(MODEL_NAME)\n\n# initialize the knowledge neuron wrapper with your model, tokenizer and a string expressing the type of your model ('gpt2' / 'gpt_neo' / 'bert')\nkn = KnowledgeNeurons(model, tokenizer, model_type=model_type(MODEL_NAME))\n\n# use the integrated gradients technique to find some refined neurons for your set of prompts\nrefined_neurons = kn.get_refined_neurons(\n    TEXTS,\n    GROUND_TRUTH,\n    p=P,\n    batch_size=BATCH_SIZE,\n    steps=STEPS,\n    coarse_adaptive_threshold=ADAPTIVE_THRESHOLD,\n)\n\n# suppress the activations at the refined neurons + test the effect on a relevant prompt\n# 'results_dict' is a dictionary containing the probability of the ground truth being generated before + after modification, as well as other info\n# 'unpatch_fn' is a function you can use to undo the activation suppression in the model. \n# By default, the suppression is removed at the end of any function that applies a patch, but you can set 'undo_modification=False', \n# run your own experiments with the activations / weights still modified, then run 'unpatch_fn' to undo the modifications\nresults_dict, unpatch_fn = kn.suppress_knowledge(\n    TEXT, GROUND_TRUTH, refined_neurons\n)\n\n# suppress the activations at the refined neurons + test the effect on an unrelated prompt\nresults_dict, unpatch_fn = kn.suppress_knowledge(\n    \"[MASK] is the official language of the solomon islands\",\n    \"english\",\n    refined_neurons,\n)\n\n# enhance the activations at the refined neurons + test the effect on a relevant prompt\nresults_dict, unpatch_fn = kn.enhance_knowledge(TEXT, GROUND_TRUTH, refined_neurons)\n\n# erase the weights of the output ff layer at the refined neurons (replacing them with zeros) + test the effect\nresults_dict, unpatch_fn = kn.erase_knowledge(\n    TEXT, refined_neurons, target=GROUND_TRUTH, erase_value=\"zero\"\n)\n\n# erase the weights of the output ff layer at the refined neurons (replacing them with an unk token) + test the effect\nresults_dict, unpatch_fn = kn.erase_knowledge(\n    TEXT, refined_neurons, target=GROUND_TRUTH, erase_value=\"unk\"\n)\n\n# edit the weights of the output ff layer at the refined neurons (replacing them with the word embedding of 'target') + test the effect\n# we can make the model think the capital of france is London!\nresults_dict, unpatch_fn = kn.edit_knowledge(\n    TEXT, target=\"london\", neurons=refined_neurons\n)\n```\n\nfor bert models, the position where the `\"[MASK]\"` token is located is used to evaluate the knowledge neurons, (and the ground truth should be what the mask token is expected to be), but due to the nature of GPT models, the last position in the prompt is used by default, and the ground truth is expected to immediately follow.\n\nIn GPT models, due to the subword tokenization, the integrated gradients are taken n times, where n is the length of the expected ground truth in tokens, and the mean of the integrated gradients at each step is taken.\n\nfor bert models, the ground truth is currently expected to be a single token. Multi-token ground truths are on the todo list.\n\n# Evaluations on the PARAREL dataset\n\nTODO: plotting script isn't quite finished, this won't work yet\nTo find the knowledge neurons in bert-base-uncased for the PARAREL dataset, and replicate figures 3. and 4. from the paper, you can run\n```bash\n# find knowledge neurons + test suppression / enhancement (this will take a day or so on a decent gpu)\npython -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE pararel_evaluate.py\n# plot results\npython plot_pararel_results.py\n```\n\n# TODO:\n- [ ] Better documentation\n- [ ] Publish PARAREL results for bert-base-uncased\n- [ ] Publish PARAREL results for bert-base-multilingual-uncased\n- [ ] Publish PARAREL results for bert-large-uncased\n- [ ] Publish PARAREL results for bert-large-multilingual-uncased\n- [ ] Multiple masked tokens for bert models\n- [ ] Find good dataset for GPT-like models to evaluate knowledge neurons (PARAREL isn't applicable since the tail entities aren't always at the end of the sentence)\n- [ ] Add negative examples for getting refined neurons (i.e expressing a different fact in the same way)\n- [ ] Look into different attribution methods (cf. https://arxiv.org/pdf/2010.02695.pdf)\n\n\n# Citations\n```\n@article{Dai2021KnowledgeNI,\n  title={Knowledge Neurons in Pretrained Transformers},\n  author={Damai Dai and Li Dong and Y. Hao and Zhifang Sui and Furu Wei},\n  journal={ArXiv},\n  year={2021},\n  volume={abs/2104.08696}\n}\n```# knowledge-neurons",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/EleutherAI/knowledge-neurons",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "knowledge-neurons",
    "package_url": "https://pypi.org/project/knowledge-neurons/",
    "platform": "",
    "project_url": "https://pypi.org/project/knowledge-neurons/",
    "project_urls": {
      "Homepage": "https://github.com/EleutherAI/knowledge-neurons"
    },
    "release_url": "https://pypi.org/project/knowledge-neurons/0.0.2/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "A library for finding knowledge neurons in pretrained transformer models",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11062138,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3c179a2ee3d1877eac2ee4f7b5876e78b660f2ea5598d2cbc0d1f5fccf65726e",
          "md5": "ac0d0c18ac880dd58d30af42f1f63800",
          "sha256": "c30a99a81fa60c79bac03463c7a65a6a9773e0f9165a0cada3907970568e0c96"
        },
        "downloads": -1,
        "filename": "knowledge-neurons-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ac0d0c18ac880dd58d30af42f1f63800",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16487,
        "upload_time": "2021-08-01T11:49:41",
        "upload_time_iso_8601": "2021-08-01T11:49:41.938177Z",
        "url": "https://files.pythonhosted.org/packages/3c/17/9a2ee3d1877eac2ee4f7b5876e78b660f2ea5598d2cbc0d1f5fccf65726e/knowledge-neurons-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a7f8cb12f1f35308d3c90504171a43f0fadc83f5cfc0cf38309494912134e9c1",
          "md5": "c247ed36e0b17454d0d817fd161469f7",
          "sha256": "0025a89385dae0caff58cb5f2f4977de1af448c9adc597ab12bb9eefd051994e"
        },
        "downloads": -1,
        "filename": "knowledge-neurons-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "c247ed36e0b17454d0d817fd161469f7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 16511,
        "upload_time": "2021-08-01T11:52:47",
        "upload_time_iso_8601": "2021-08-01T11:52:47.912839Z",
        "url": "https://files.pythonhosted.org/packages/a7/f8/cb12f1f35308d3c90504171a43f0fadc83f5cfc0cf38309494912134e9c1/knowledge-neurons-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a7f8cb12f1f35308d3c90504171a43f0fadc83f5cfc0cf38309494912134e9c1",
        "md5": "c247ed36e0b17454d0d817fd161469f7",
        "sha256": "0025a89385dae0caff58cb5f2f4977de1af448c9adc597ab12bb9eefd051994e"
      },
      "downloads": -1,
      "filename": "knowledge-neurons-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "c247ed36e0b17454d0d817fd161469f7",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 16511,
      "upload_time": "2021-08-01T11:52:47",
      "upload_time_iso_8601": "2021-08-01T11:52:47.912839Z",
      "url": "https://files.pythonhosted.org/packages/a7/f8/cb12f1f35308d3c90504171a43f0fadc83f5cfc0cf38309494912134e9c1/knowledge-neurons-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}