{
  "info": {
    "author": "Palash Shah",
    "author_email": "ps9cmk@virginia.edu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "## INITIAL BUILD RELEASE DATE: JULY 16TH.\nin progress....this README.md is out of date. Documentation site coming soon. \n\n![image](tools/data/gh_images/verve-logo.png)\n\n# verve: Deep Learning fluent in one-liners\n\nverve is a deep learning API that allows users to use machine learning in their workflows in fluent one-liners. It is written in Python and TensorFlow and makes training neural networks as simple as a one line function call. It was written to make deep learning as simple as possible to every user. \n*** \n\n## Guiding Principles ## \n  * **Beginner Friendly.** verve is an API designed to be used by developers with no deep learning experience whatsoever. It is built so that users with no knowledge in preprocessing, modeling, or tuning can build high-performance models with ease without worrying about the details of implementation.\n\n  * **Quick Integration.** With the recent rise of machine learning on the cloud, the developer community has failed to make easy-to-use platforms that exist locally and integrate directly into workflows. verve allows users to develop models directly in programs with hundreds of API endpoints without having to worry about the transition to the cloud.\n\n  * **Automation.** End-to-end pipelines containing hundreds of processes are automatically run for the user. The developer only has to consider what they want to accomplish from the task and the location of their initial dataset.\n\n  * **Easy Extensibility.** Queries are split into standalone modules. Under the dev-pipeline module you can pipeline both different and new modules and integrate them into the workflow directly. This allows newly developed features to be easily tested before integrating them into the main program. \n\n***\n\nTable of Contents\n=================\n\n* [Prediction Queries: building blocks](#queries)\n   * [Regression Neural Network](#regression-neural-network)\n   * [Classification Neural Network](#classification-neural-network)\n   * [Convolutional Neural Network](#convolutional-neural-network)\n   * [K-Means Clustering](#k-means-clustering)\n   * [Nearest Neighbors](#nearest-neighbors)\n   * [Support Vector Machines](#support-vector-machine)\n   * [Decision Tree](#decision-tree)\n* [Image Generation](#image-generation)\n   * [Class Wise Image Generation](#class-wise-image-generation)\n   * [Generate Dataset & Convolutional Neural Network](#generate-dataset-and-convolutional-neural-network)\n* [Model Information](#model-modifications)\n   * [Model Tuning](#model-tuning)\n   * [Plotting](#plotting)\n   * [Dataset Information](#dataset-information)\n* [Dimensionality Reduction](#dimensionality-reduction)\n   * [Reduction Pipeliner](#reduction-pipeliner)\n   * [Principle Component Analysis](#principle-component-analysis)\n   * [Feature Importances via Random Forest Regressor](feature-importances-via-random-forest-regressor)\n   * [Independent Component Analysis](#indepedent-component-analysis)\n* [Process Logger](#process-logger)\n* [Pipelining for Contributors](#pipelining-for-contributors)\n* [Providing Instructions](#instructions)\n* [Example Projects](#example-projects)\n\n## Queries ##\n\nGenerally, all queries have the same structure. You should always be passing an English instruction to the query. The information that you generate from the query will always be stored in the `client`class in the model's dictionary. When you call a query on the `client` object, an instruction will be passed. Any format will be decoded, but avoiding more complex sentence structures will yield better results. If you already know the exact target class label name, you can also provide it. \n\n### Regression Neural Network ###\n\nLet's start with the most basic query. This will build a feed-forward network for a continuous label that you specify.\n\n```python\nimport verve\n\nnewClient = client('dataset')\nnewClient.regression_query_ann('Model the median house value')\n```\nNo preprocessing is neccesary. All plots, losses, and models are stored in the models field in the client class.\n\nBasic tuning with the number of layers is done when you call this query. If you'd like to tune more in depth you can call: \n\n```python\nnewClient.tune('regression', inplace = False)\n```\nTo specify which model to tune, you must pass the type of model that you'd like to perform tuning on. \n\nThis function tunes hyperparameters like node count, layer count, learning rate, and other features. This will return the best network and if ```inplace = True``` it will replace the old model it in the client class under ```regression_ANN```. \n\n### Classification Neural Network ###\n\nThis query will build a feed-forward neural network for a classification task. As such, your label must be a discrete variable. \n\n```python\nnewClient = client('dataset')\nnewClient.classification_query_ann('Predict building name')\n```\nThis creates a neural network to predict building names given your dataset. Any number of classes will work for this query. By default, ```categorical_crossentropy``` and an `adam` optimizer are used. \n\n### Convolutional Neural Network ###\nCreating a convolutional neural network for a dataset you already have created is as simple as: \n\n```python\nnewClient = client()\nnewClient.convolutional_query('path_to_class1', 'path_to_class2', 'path_to_class3')\n```\nFor this query, no initial shallow tuning is performed is done because of how memory intensive CNN's can be. User specified parameters for this query are currently being implemented. The defaults can be found in the `prediction_queries.py` file.\n\n### K-means Clustering ###\n\nThis query will create a k-means clustering algorithm trained on your processed dataset. \n\n```python\nnewClient = client('dataset')\nnewClient.kmeans_clustering_query()\n```\n\nIt continues to grow the number of clusters until the value of ``inertia`` stops decreasing by at least 1000 units. This is a threshold determined based on several papers, and extensive testing. This can also be changed by specifying ```threshold = new_threshold_num```. If you'd like to specify the number of clusters you'd like it to use you can do ``clusters = number_of_clusters``. \n\n\n### Nearest-neighbors ###\n\nThis query will use scikit-learn's nearest-neighbor function to return the best nearest neighbor model on the dataset.\n\n```python\nnewClient = client('dataset')\nnewClient.nearest_neighbor_query()\n```\n\nYou can specify the ```min_neighbors, max_neighbors``` as keyword arguments to the function. Values are stored under the ```nearest_neighbor``` field in the model dictionary. \n\n### Support Vector Machine ###\n\nThis will use scikit-learn's SVM function to return the best support vector machine on the dataset.\n\n```python\nnewClient = client('dataset')\nnewClient.svm_query('Model the value of houses')\n```\n\nValues are stored under the ```svm``` field in the model dictionary. \n\nNOTE: A linear kernel is used as the default, this can be modified by specifying your new kernel name as a keyword argument: ```kernel = 'rbf_kernel'```. \n\n\n### Decision Tree ###\n\nThis will use scikit's learns decision tree function to return the best decision tree on the dataset.\n\n```python\nnewClient = client('dataset')\nnewClient.decision_tree_query()\n```\n\nThis will use scikit's learns Decision Tree function to return the best decision tree on the dataset. Values are stored under the ```decision_tree``` field in the model dictionary. \n\nYou can specify these hyperparameters by passing them as keyword arguments to the query: ```max_depth = num, min_samples_split = num, max_samples_split = num, min_samples_leaf = num, max_samples_leaf= num)```\n***\n\n## Image Generation ##\n\n### Class wise image generation ### \nIf you want to generate an image dataset to use in one of your models you can do:\n\n```python\ngenerate_set('apples', 'oranges', 'bananas', 'pineapples')\n```\n\nThis will create separate folders in your directory with each of these names with ~100 images for each class. An updated version of Google Chrome is required for this feature; if you'd like to use it with an older version of Chrome please install the appropriate chromedriver. \n\n### Generate Dataset and Convolutional Neural Network ###\nIf you'd like to generate images and fit it automatically to a Convolutional Neural Network you can use this command:\n\n```python\nnewClient.generate_fit_cnn('apples', 'oranges')\n```\nThis particular will generate a dataset of apples and oranges by parsing Google Images, preprocess the dataset appropriately and then fit it to a convolutional neural network. All images are reduced to a standard (224, 224, 3) size using a traditional OpenCV resizing algorithm. Default size is the number of images in one Google Images page *before* having to hit more images, which is generally around 80-100 images. \n\nThe infrastructure to generate more images is currently being worked on. \n\nNote: all images will be resized to (224, 224, 3). Properties are maintained by using a geometric image transformation explained here: [OpenCV Transformation](https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html).\n\n***\n\n## Model Modifications ## \n\n### Model Tuning ###\n\nIn order to further tune your neural network models, you can call: \n\n```python\nnewClient.tune('convolutional neural network')\n```\nThis will tune:\n  1. Number of Layers\n  2. Number of Nodes in every layer\n  3. Learning Rate\n  4. Activation Functions\n\nIn order to ensure that the tuned models accuracy is robust, every model is run multiple times and the accuracies are averaged. This ensures that the model configuration is optimal.\n\nYou can just specify what type of network you want to tune — it will identify your target model from the `models` dictionary using another instruction algorithm. \n\nNOTE: Tuning for CNN's is **very** memory intensive, and should not be done frequently. \n\n### Plotting ###\nAll plots are stored during runtime. This function plots all generated graphs for your current client object on one pane. \n\n```python\nnewClient.plot_all('regression')\n```\n\nIf you'd like to extract a single plot, you can do:\n\n```python\nnewClient.show_plots('regression')\n``` \n\nand then \n\n```python\nnewClient.getModels()['regression']['plots']['trainlossvstestloss']\n```\nNo other plot retrieval technique is currently implemented. While indexing nested dictionaries might seem tedious, this was allowed for fluency. \n\n### Dataset Information ###\nIn depth metrics about your dataset and similarity information can be generated by calling:\n\n```python\nnewClient.stat_analysis()\n```\nA information graph as well as a similarity spectrum shown below will be generated:\n\n![Image description](data/similarity.png)\n\nThis represents 5 columns that have the smallest cosine distance; you might need to remove these columns because they're too similar to each other and will just act as noise. You can specify whether you want to remove them with ```inplace = True```. Information on cosine similarity can be found [here](https://www.sciencedirect.com/topics/computer-science/cosine-similarity).\n\nIf you'd like information on just one column you can do: \n\n```python\n newClient.stat_analysis(dataset[columnname])\n```\n***\n\n## Dimensionality Reduction ##\n\n### Reduction Pipeliner ###\n\nIf you'd like to get the best pipeline for dimensionality reduction you can call:\n\n```python\n dimensionality_reduc(\"I want to estimate number of crime\", path_to_dataset) \n\n```\nInstructions like \"I want to model x\" are provided in the dimensionality reduction pipeline because it identifies which prediction objective you would like to maximize the accuracy for. Providing this instruction helps verve provide users with the best modification pipeline. \n\nverve current supports feature importance identification using random forest regressor, indepedent component analysis, and principle component analysis. The output of the dimensionalityReduc() function should look something like this: \n\n```\nBaseline Accuracy: 0.9752906976744186\n----------------------------\nPermutation --> ('RF',) | Final Accuracy --> 0.9791666666666666\nPermutation --> ('PCA',) | Final Accuracy --> 0.8015988372093024\nPermutation --> ('ICA',) | Final Accuracy --> 0.8827519379844961\nPermutation --> ('RF', 'PCA') | Final Accuracy --> 0.3316375968992248\nPermutation --> ('RF', 'ICA') | Final Accuracy --> 0.31419573643410853\nPermutation --> ('PCA', 'RF') | Final Accuracy --> 0.7996608527131783\nPermutation --> ('PCA', 'ICA') | Final Accuracy --> 0.8832364341085271\nPermutation --> ('ICA', 'RF') | Final Accuracy --> 0.8873546511627907\nPermutation --> ('ICA', 'PCA') | Final Accuracy --> 0.7737403100775194\nPermutation --> ('RF', 'PCA', 'ICA') | Final Accuracy --> 0.32630813953488375\nPermutation --> ('RF', 'ICA', 'PCA') | Final Accuracy --> 0.30886627906976744\nPermutation --> ('PCA', 'RF', 'ICA') | Final Accuracy --> 0.311531007751938\nPermutation --> ('PCA', 'ICA', 'RF') | Final Accuracy --> 0.8924418604651163\nPermutation --> ('ICA', 'RF', 'PCA') | Final Accuracy --> 0.34205426356589147\nPermutation --> ('ICA', 'PCA', 'RF') | Final Accuracy --> 0.9970639534883721\n\nBest Accuracies\n----------------------------\n[\"Permutation --> ('ICA', 'PCA', 'RF) | Final Accuracy --> 0.9970639534883721\"]\n\n```\nThe baseline accuracy represents the accuracy acheived without any dimensionality reduction techniques. Then, each possible reduction technique permutation is displayed with its respective accuracy. At the bottom is the pipeline which resulted in the highest accuracy. You can also specify which of the reduction techniques you'd like to try by passing ```reducers= ['ICA', 'RF']``` to the function.\n\nIf you'd like to replace the dataset with one that replaces it with the best reduced one, you can just specify ```inplace=True```.\n\n### Principle Component Analysis ###\n\nPerforming Principle Component is as simple as: \n\n```python \ndimensionality_PCA(\"Estimating median house value\", path_to_dataset)\n```\n\nNOTE: this will select the optimal number of principal components to keep. The default search space is up to the number of columns in your dataset. If you'd like to specify the number of components you can just do ```n_components = number_of_components```.  \n\n### Feature Importances via Random Forest Regressor ###\nUsing the random forest regressor to identify feature importances is as easy as calling: \n\n```python\ndimensionality_RF(\"Estimating median house value\", path_to_dataset)\n```\nThis will find the optimal number of features to use and will return the dataset with the best accuracy. If you'd like to manually set the number of feature you can do ```n_features = number of features```. \n\n### Indepedent Component Analysis ###\n\nPerforming Indepedent Component Analysis can be done by calling:\n\n```python \ndimensionality_ICA(\"Estimating median house value\", path_to_dataset)\n```\n\nIf this does not converge a message will be displayed for users to warn them by default.  \n\n***\n\n## Process Logger ##\n\nverve will automatically output the current process running in a hierarchial format like this:\n\n```\nloading dataset...\n  |\n  |- getting most similar column from instruction...\n    |\n    |- generating dimensionality permutations...\n      |\n      |- running each possible permutation...\n        |\n        |- realigning tensors...\n          |\n          |- getting best accuracies...\n ```\n\nA quiet mode feature is currently being implemented. \n\n***\n\n## Pipelining for Contributors ##\n\nIn order to help make verve extensible, a process pipeliner has been implemented to help contributors easily test their newly-developed modules. \n\nLet's say you've developed a different preprocesser for data that you want to test before integrating it into verve's primary workflow. This is the process to test it out:\n\nFirst, you want to initialize your base parameters, which are your instructions, the path to your dataset, and any other information your new function might require.\n\n```\ninit_params = {\n    'instruction': \"Predict median house value\",\n    'path_to_set': './data/housing.csv',\n}\n```\n\nYou can then modify the main pipeline: \n\n<pre>\nsingle_regression_pipeline = [initializer,\n                <b>your_own_preprocessor</b>, #is originally just preprocessor\n                instruction_identifier,\n                set_splitter,\n                modeler,\n                plotter]\n</pre>\n\nThese pipelines can be found under the ``dev-pipeliner`` folder. Currently, this format is only supported for the single regression pipeline. Complete integration of pipelining into the main framework is currently being implemented. \n\nFinally, you can run your pipeline by using: \n\n```\n[func(init_params) for func in reg_pipeline] \n\n```\n\nAll model information should be stored in ```init_params```. If you'd like to modify smaller details, you can copy over the module and modify the smaller detail; this split was not done to maintain ease of use of the pipeline. \n\n***\n## Instructions ##\n\nverve uses intelligent natural language processing to analyze user instructions and match it with a column in user datasets. \n  1. [Textblob](https://textblob.readthedocs.io/en/dev/), a part of speech recognition algorithm, is used to identify parts of speech.\n  2. A self-developed part-of-speech deciphering algorithm is used to extract relevant parts of a sentence. \n  3. Masks are generated to represent all words as tensors in order for easy comparison\n  4. Levenshentein distances are used to match relevant parts of the sentence to a column name.\n  5. Target column selected based on lowest levenshentein distance and is returned.\n\n## Example Projects ##\n\nTo get started, take a look at some of these examples of data science projects analyzing datasets using verve. \n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Palashio/verve",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "verve",
    "package_url": "https://pypi.org/project/verve/",
    "platform": "",
    "project_url": "https://pypi.org/project/verve/",
    "project_urls": {
      "Homepage": "https://github.com/Palashio/verve"
    },
    "release_url": "https://pypi.org/project/verve/0.0.1/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Fully automated machine learning in one-liners.",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7630799,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3a26b0a42ecd9175f3382a245d17da2df373f744a137b75616a15f64b93cf0b4",
          "md5": "f2d9e175bc326e86a935b6f08c9efbc4",
          "sha256": "975a5edef5db750e2fe4d710401c2ca04994c09c62b702a9e28f210339c2b6e7"
        },
        "downloads": -1,
        "filename": "verve-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f2d9e175bc326e86a935b6f08c9efbc4",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 67267,
        "upload_time": "2020-07-04T21:40:05",
        "upload_time_iso_8601": "2020-07-04T21:40:05.535585Z",
        "url": "https://files.pythonhosted.org/packages/3a/26/b0a42ecd9175f3382a245d17da2df373f744a137b75616a15f64b93cf0b4/verve-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "36bb0dee4d5c05323eacf4bef1dd25f184afca737f494b5bde75167e69f70875",
          "md5": "4a86eb1ebc90ce7dec15bc94b75c92b1",
          "sha256": "7647c0218347434c767bea289f204bd639c709bece34ad3a4b60ab416f1a9df6"
        },
        "downloads": -1,
        "filename": "verve-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "4a86eb1ebc90ce7dec15bc94b75c92b1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 62460,
        "upload_time": "2020-07-04T21:40:07",
        "upload_time_iso_8601": "2020-07-04T21:40:07.868610Z",
        "url": "https://files.pythonhosted.org/packages/36/bb/0dee4d5c05323eacf4bef1dd25f184afca737f494b5bde75167e69f70875/verve-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3a26b0a42ecd9175f3382a245d17da2df373f744a137b75616a15f64b93cf0b4",
        "md5": "f2d9e175bc326e86a935b6f08c9efbc4",
        "sha256": "975a5edef5db750e2fe4d710401c2ca04994c09c62b702a9e28f210339c2b6e7"
      },
      "downloads": -1,
      "filename": "verve-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f2d9e175bc326e86a935b6f08c9efbc4",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 67267,
      "upload_time": "2020-07-04T21:40:05",
      "upload_time_iso_8601": "2020-07-04T21:40:05.535585Z",
      "url": "https://files.pythonhosted.org/packages/3a/26/b0a42ecd9175f3382a245d17da2df373f744a137b75616a15f64b93cf0b4/verve-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "36bb0dee4d5c05323eacf4bef1dd25f184afca737f494b5bde75167e69f70875",
        "md5": "4a86eb1ebc90ce7dec15bc94b75c92b1",
        "sha256": "7647c0218347434c767bea289f204bd639c709bece34ad3a4b60ab416f1a9df6"
      },
      "downloads": -1,
      "filename": "verve-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "4a86eb1ebc90ce7dec15bc94b75c92b1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 62460,
      "upload_time": "2020-07-04T21:40:07",
      "upload_time_iso_8601": "2020-07-04T21:40:07.868610Z",
      "url": "https://files.pythonhosted.org/packages/36/bb/0dee4d5c05323eacf4bef1dd25f184afca737f494b5bde75167e69f70875/verve-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}