{
  "info": {
    "author": "Anders Elton",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Description\nThis repository contains a package to read from SQL server and store data in Google cloud storage and possibilities to\nload the data to google BigQuery.\n\nThe package tries to be effective when reading from sql server and has basic cache capabilities.\n\n# Usage example\nYou need to have the google default credentials set by for example setting GOOGLE_APPLICATION_CREDENTIALS to the path\nof a valid service account.\n\n```python\nfrom database_to_bigquery.sql_server import SqlServerToCsv, SqlServerToBigquery\nsql_server_to_csv = SqlServerToCsv(username=\"scott\",\n                                   password=\"t1ger\",\n                                   host=\"127.0.0.1//optionalinstance_name\",\n                                   database=\"thedb\",\n                                   destination=f\"gs://gcsbucketname\")\n\nbigquery = SqlServerToBigquery(sql_server_to_csv=sql_server_to_csv)\n\nresult = bigquery.ingest_table(sql_server_table=\"table_to_read\",\n                               sql_server_schema=\"dbo\",\n                               bigquery_destination_project=\"bigqueryproject\",\n                               bigquery_destination_dataset=\"bigquerydataset\")\nprint(result.full_str())\n```\n\n# Options\n## Environment variables\n- DB_PORT - override default sql server port\n- DB_DRIVER - override default pyodbc driver (ODBC Driver 17 for SQL Server), for example with (FreeTDS)\n\nThis originally came packaged as we dockerfile for usage in k8s environment and I created a package for convinience, \nso please look at the github repo for updated description https://github.com/ael-computas/sqlserver-to-bigquery\n\n## The split concept\nThe package tries to split the Sql server data into chunks if the table contains more than 1m rows.  You can override\nthis in the ingest_table function.\n\nIf the sql server tables contains columns that sql server CRC does not work on it will fail.\n\nLets take a simple table as example\n\n| ID  (PK)      | data          |\n| ------------- |:-------------:|\n| 1             | test1         |\n| 2             | test2         |\n| 3             | test3         |\n| 4             | test4         |\n\nLet us use a split size of 2, meaning 2 rows in every split.\nThe PK always has a predictable sort, so we will sort by PK, even if it might not be the most effective in all cases\n\nThe split here will be\n\nSplit 1\n\n| ID  (PK)      | data          |\n| ------------- |:-------------:|\n| 1             | test1         |\n| 2             | test2         |\n\nSplit 2\n\n| ID  (PK)      | data          |\n| ------------- |:-------------:|\n| 3             | test3         |\n| 4             | test4         |\n\nTo know how to split it this way, we will first do a\n\n````\nWITH splits AS (SELECT (ROW_NUMBER() OVER(ORDER BY ID) / 2 +1 as internal_split, ALL_FIELDS, from table)\nselect 2 AS split_size\ninternal_split\ncount(*) as cnt\nmin(ID) as min_id, max(id) as max_id\nCHECKSUM_AGG(CHECKSUM(*)) as crc \nfrom splits group by internal_split\n ````\n\nWill produce the following\n\n| split_size    | internal_split|     cnt       |      min_id   |    max_id     |      crc      |\n| ------------- |:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n| 2             | 1             | 2             |      1        |      2        | 1234567       |\n| 2             | 2             | 2             |      3        |      4        | -23456789     |\n\nNote: This is not from an actual db, so CRC is faked.  mostly to show concept.\nThe Entire ROW will act as a crc in GCS.  \n\nFrom this result its quite easy to select the data belonging to a split.\n\n````\nselect WITH splits AS (SELECT (ROW_NUMBER() OVER(ORDER BY ID) / 2 +1 as internal_split, ALL_FIELDS from table)\nSELECT ALL_FIELDS... from splits where internal_split=1\n ````\n\nThen the payload can be streamed to a file.  The CRC is stored to GCS.\n\nWhat happens if we now update the database to this?\n\n| ID  (PK)      | data          |\n| ------------- |:-------------:|\n| 1             | test1         |\n| 2             | test2         |\n| 3             | test3         |\n| 4             | THIS_CHANGED  |\n\nRow number 2 in the aggregate will change..\n\n| split_size    | internal_split|     cnt       |      min_id   |    max_id     |      crc      |\n| ------------- |:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n| 2             | 1             | 2             |      1        |      2        | 1234567       |\n| 2             | 2             | 2             |      3        |      4        | 1243535341434 |\n\nAnd we can now just read internal_split=2, since internal_split=1 did not change.\n\n## FAQ\n\n### What about CDC?\nChange data capture is better and more scalable.  Look into debezium, for instance, to get you started.\n\nThis program can work for a while, but a true CDC solution is better.\n\n### Scalability\nThis has only been tested on small databases with 100m rows or so in the biggest tables.  Im not sure how this solution\ncopes with bigger datasets.\n\n### What happens if the data changes between split group by and while reading data?\n\nThe data you read might be inconsistent, in regards to PK/FK and CRC with actual data. However, the next time you run \nthe prgram, it will recognize that the crc no longer matches, and it will rewrite the payloads\n\n### What happens if CRC is same for some datasets?\n\nThis can happen. Please see SQL server docs.\n\nThis process can handle this if you have an updated_date field, for example, then it will use that as additional min/max fields\nto mitigate this rare issue.\n\n### Some SQL Server types are unsupported\nYes, this is built solving real customer scenarios, so I guess that case was never encountered.  fork repo and do a pull request!\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/ael-computas/sqlserver-to-bigquery",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ael-computas/sqlserver-to-bigquery",
    "keywords": "gcp,BigQuery,SQL Server,integration,copy",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "database-to-bigquery",
    "package_url": "https://pypi.org/project/database-to-bigquery/",
    "platform": null,
    "project_url": "https://pypi.org/project/database-to-bigquery/",
    "project_urls": {
      "Download": "https://github.com/ael-computas/sqlserver-to-bigquery",
      "Homepage": "https://github.com/ael-computas/sqlserver-to-bigquery"
    },
    "release_url": "https://pypi.org/project/database-to-bigquery/1.0.2/",
    "requires_dist": [
      "python-dateutil (>=2.8.2)",
      "google-cloud-storage (>=1.37.1)",
      "google-cloud-bigquery (>=2.13.1)",
      "smart-open[gcs] (>=5.0.0)",
      "pyodbc (>=4.0.30)",
      "sqlalchemy (>=1.4.10)",
      "backoff (>=1.10.0)",
      "google-cloud-secret-manager (>=2.4.0)",
      "PyYAML (>=5.4.1)"
    ],
    "requires_python": "",
    "summary": "Read from SQL server and load to Google BigQuery",
    "version": "1.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13412535,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "df5e45c7d9d7173f688241fab1e13e25543fadc8fe03088a097eea7599524138",
          "md5": "8f759bd86e403b57fa9f2786594cbbc1",
          "sha256": "c26bf6c72ccc23e8487a89cbc31b9204093e12137a93f5543c2030789e72a419"
        },
        "downloads": -1,
        "filename": "database_to_bigquery-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8f759bd86e403b57fa9f2786594cbbc1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 14276,
        "upload_time": "2021-04-30T19:33:53",
        "upload_time_iso_8601": "2021-04-30T19:33:53.181857Z",
        "url": "https://files.pythonhosted.org/packages/df/5e/45c7d9d7173f688241fab1e13e25543fadc8fe03088a097eea7599524138/database_to_bigquery-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b6d3ebeb0d5084396b98c9c01a294bd62368980fa521aa51314e37fc164167ef",
          "md5": "0c8f82778f52ee23d3be809845187b29",
          "sha256": "6abbb683097487bb128a88aabf463e7d59e146ac11db93211f5544edbcbc7a3f"
        },
        "downloads": -1,
        "filename": "database-to-bigquery-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "0c8f82778f52ee23d3be809845187b29",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13246,
        "upload_time": "2021-04-30T19:33:55",
        "upload_time_iso_8601": "2021-04-30T19:33:55.402780Z",
        "url": "https://files.pythonhosted.org/packages/b6/d3/ebeb0d5084396b98c9c01a294bd62368980fa521aa51314e37fc164167ef/database-to-bigquery-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1f862c8bc11a824635388a975f86b6109ab388d26c46626f9ff6466c51e09738",
          "md5": "04a6df69532a2f447d46e41629b45b62",
          "sha256": "04c5b864a0df7a59a84819fdacf52dcba5a96925b0cff556d99b088d8a7491bf"
        },
        "downloads": -1,
        "filename": "database_to_bigquery-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "04a6df69532a2f447d46e41629b45b62",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 13727,
        "upload_time": "2021-04-30T20:10:50",
        "upload_time_iso_8601": "2021-04-30T20:10:50.746414Z",
        "url": "https://files.pythonhosted.org/packages/1f/86/2c8bc11a824635388a975f86b6109ab388d26c46626f9ff6466c51e09738/database_to_bigquery-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7d9b97a38b3ab549ce877189608b6e6ab4152222b5f119dfe8349614e6369374",
          "md5": "f56dcf31a29f3e5a79f0163cca6d1a77",
          "sha256": "6eeea72c97abc0eb7b1db98c8909583e3c391957b7345ab66b2b21b04b16c8ed"
        },
        "downloads": -1,
        "filename": "database-to-bigquery-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f56dcf31a29f3e5a79f0163cca6d1a77",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14092,
        "upload_time": "2021-04-30T20:10:53",
        "upload_time_iso_8601": "2021-04-30T20:10:53.350886Z",
        "url": "https://files.pythonhosted.org/packages/7d/9b/97a38b3ab549ce877189608b6e6ab4152222b5f119dfe8349614e6369374/database-to-bigquery-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c518b95ad7fef200fcb40117189717ae6a09d940c25827de831bcb2cecff26d2",
          "md5": "e283aeaaa56d8d92dbdd28be3d95e222",
          "sha256": "e8183ec6e7d43343ba693a7542d39fd2c90d03dd2cc76dac2c2473eaf74cbca9"
        },
        "downloads": -1,
        "filename": "database_to_bigquery-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e283aeaaa56d8d92dbdd28be3d95e222",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 13899,
        "upload_time": "2021-05-03T06:26:02",
        "upload_time_iso_8601": "2021-05-03T06:26:02.124211Z",
        "url": "https://files.pythonhosted.org/packages/c5/18/b95ad7fef200fcb40117189717ae6a09d940c25827de831bcb2cecff26d2/database_to_bigquery-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5c1426f6affa23b6c4170be4a61b6305071b5d017f32bd20b8cb9c800c8b0111",
          "md5": "654a67a9e0759b9ac3b0c5cec4baf343",
          "sha256": "dc52b877b62491e78dc6d73158501931d621f62b983badd55e9a16cce3c630f3"
        },
        "downloads": -1,
        "filename": "database-to-bigquery-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "654a67a9e0759b9ac3b0c5cec4baf343",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14282,
        "upload_time": "2021-05-03T06:26:03",
        "upload_time_iso_8601": "2021-05-03T06:26:03.630350Z",
        "url": "https://files.pythonhosted.org/packages/5c/14/26f6affa23b6c4170be4a61b6305071b5d017f32bd20b8cb9c800c8b0111/database-to-bigquery-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c7725c0382a584c22195b07210e8dce2163b750c7a7604aa7f11aaab0d17d3e6",
          "md5": "d7ed54407e9f0ec3f8a68b95d97b283e",
          "sha256": "571e4700f469477f33fd4382512d8aecceefc591e6fd2f15d648662e907ac1b1"
        },
        "downloads": -1,
        "filename": "database_to_bigquery-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d7ed54407e9f0ec3f8a68b95d97b283e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 13927,
        "upload_time": "2022-04-05T10:09:34",
        "upload_time_iso_8601": "2022-04-05T10:09:34.806517Z",
        "url": "https://files.pythonhosted.org/packages/c7/72/5c0382a584c22195b07210e8dce2163b750c7a7604aa7f11aaab0d17d3e6/database_to_bigquery-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "00129e05a9c309b2a0f4025b58cbce882aa19d6daa089c72cd13cc5e09d9848f",
          "md5": "472d8fc2c46ed3e96ea9b2152b5cb10c",
          "sha256": "7959212ecfb8d4b3c023070b546096c649e72d8e8014cd4ba8911f70eb4b9fdd"
        },
        "downloads": -1,
        "filename": "database-to-bigquery-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "472d8fc2c46ed3e96ea9b2152b5cb10c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14873,
        "upload_time": "2022-04-05T10:09:36",
        "upload_time_iso_8601": "2022-04-05T10:09:36.265028Z",
        "url": "https://files.pythonhosted.org/packages/00/12/9e05a9c309b2a0f4025b58cbce882aa19d6daa089c72cd13cc5e09d9848f/database-to-bigquery-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c7725c0382a584c22195b07210e8dce2163b750c7a7604aa7f11aaab0d17d3e6",
        "md5": "d7ed54407e9f0ec3f8a68b95d97b283e",
        "sha256": "571e4700f469477f33fd4382512d8aecceefc591e6fd2f15d648662e907ac1b1"
      },
      "downloads": -1,
      "filename": "database_to_bigquery-1.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d7ed54407e9f0ec3f8a68b95d97b283e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 13927,
      "upload_time": "2022-04-05T10:09:34",
      "upload_time_iso_8601": "2022-04-05T10:09:34.806517Z",
      "url": "https://files.pythonhosted.org/packages/c7/72/5c0382a584c22195b07210e8dce2163b750c7a7604aa7f11aaab0d17d3e6/database_to_bigquery-1.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "00129e05a9c309b2a0f4025b58cbce882aa19d6daa089c72cd13cc5e09d9848f",
        "md5": "472d8fc2c46ed3e96ea9b2152b5cb10c",
        "sha256": "7959212ecfb8d4b3c023070b546096c649e72d8e8014cd4ba8911f70eb4b9fdd"
      },
      "downloads": -1,
      "filename": "database-to-bigquery-1.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "472d8fc2c46ed3e96ea9b2152b5cb10c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 14873,
      "upload_time": "2022-04-05T10:09:36",
      "upload_time_iso_8601": "2022-04-05T10:09:36.265028Z",
      "url": "https://files.pythonhosted.org/packages/00/12/9e05a9c309b2a0f4025b58cbce882aa19d6daa089c72cd13cc5e09d9848f/database-to-bigquery-1.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}