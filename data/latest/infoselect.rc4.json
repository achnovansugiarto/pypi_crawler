{
  "info": {
    "author": "Felipe Maia Polo & Felipe Leno da Silva",
    "author_email": "felipemaiapolo@gmail.com, f.leno@usp.br",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "\n# ***InfoSelect*** - Mutual Information Based Feature Selection in Python\n\n\n\n\n### *Felipe Maia Polo (felipemaiapolo), Felipe Leno da Silva (f-leno)*\n\n[![PyPI](https://img.shields.io/pypi/v/infoselect.svg)](https://pypi.python.org/pypi/infoselect)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/felipemaiapolo/infoselect/blob/master/InfoSelect.ipynb)\n\nIn case you have any question or suggestion, please get in touch sending us an e-mail in *felipemaiapolo@gmail.com*.\n\n--------------\n## Contents\n1. [ Introduction ](#1)\n2. [ Installing *InfoSelect*  ](#2)\n3. [ Main functionalities of *InfoSelect* ](#3)\n4. [ Examples of *InfoSelect* use ](#4)\n5. [ References ](#5)\n\n--------------\n\n<a name=\"1\"></a>\n## 1\\. Introduction \n\nIn this package we implement the ideas proposed by [1, 2] in order to make variable/feature selection prior to regression and classification tasks using Gaussian Mixture Models (GMMs) to estimate the Mutual Information between labels and features. This is an efficient and well-performing alternative and was used in a recent work [3] by one of us:\n\n    @article{maia2022effective,\n      title={Effective sample size, dimensionality, and generalization in covariate shift adaptation},\n      author={Maia Polo, Felipe and Vicente, Renato},\n      journal={Neural Computing and Applications},\n      pages={1--13},\n      year={2022},\n      publisher={Springer}\n    }\n    \n    \n    @misc{polo2020infoselect,\n      title={InfoSelect - Mutual Information Based Feature Selection in Python},\n      author={Polo, Felipe Maia and Da Silva, Felipe Leno},\n      journal={GitHub: github.com/felipemaiapolo/infoselect},\n      year={2020}\n    }\n    \n    \n\n\n--------------\n\n<a name=\"2\"></a>\n## 2\\. Installing *InfoSelect* \n\nYou can install the package from\n[PyPI](https://pypi.org/project/infoselect/)\n\n``` :sh\n$ pip install infoselect\n```\n\nAlso, you can install the package from\n[GitHub](https://github.com/felipemaiapolo/infosel).\n\n``` :sh\n$ pip install git+https://github.com/felipemaiapolo/infoselect.git#egg=infoselect\n```\n\n--------------------\n\n<a name=\"3\"></a>\n## 3\\. Main functionalities of *InfoSelect* \n\n<a name=\"3.1\"></a>\n### 3.1\\. Main Class `SelectVars`\n\nThis class is used to order features/variables according to their importance and making the selection itself. Next we detail its methods:\n\n1. `__init__(self, gmm, selection_mode = 'forward')`\n    - **gmm**: \n        - If <img src=\"https://render.githubusercontent.com/render/math?math=Y\"> is *non*-categorical: a [Scikit-Learn GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) fitted in (y,X) - y should always be in the first column;\n        - If <img src=\"https://render.githubusercontent.com/render/math?math=Y\"> is categorical: a Python dictionary containing one [Scikit-Learn GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) fitted in X conditional on each category - something like X[y==c,:]. Format `{0:gmm0, 1:gmm1, ..., C:gmmC}`;\n        - Please use auxiliary function `get_gmm` below, especially if you want to use `covariance_type` other than 'full'.\n    - **selection_mode**: `forward`/`backward` algorithms.\n        - `forward` selection: we start with an empty set of features and then select the feature that has the largest estimated mutual information with the target variable and. At each subsequent step, we select the feature that marginally maximizes the estimated mutual information of the target and all the chosen features so far. We stop when we have selected/ordered all the features;\n        - `backward` elimination: we start with the full set of features and then at each step, we eliminate the feature that marginally maximizes the estimated mutual information of the target and all the remaining features. We stop when we have no more features to eliminate;\n\n2. `fit(self, X, y, verbose=True, eps=0)`\n    - **X**: numpy array of features; \n    - **y**: numpy array of labels;\n    - **verbose**: print or not to print!?\n    - **eps**: small value so we can avoid taking log of zero in some cases .\n\n3. `get_info(self)`: \n    - This function creates and outputs a Pandas DataFrame with the history of feature selection/elimination. The `mi_mean` column gives the estimated Mutual Information while `mi_error` gives the standard error of that estimate. On the other hand, the `delta` column gives us the percentual information loss/gain in that round, relatively to the latter;\n    \n4. `plot_delta(self)`: \n    - This function plots the history of percentual changes in the mutual information.\n    \n5. `plot_mi(self)`: \n    - This function plots the history of the mutual information.\n    \n6. `transform(self, X, rd)`: \n    - This function takes **X** and transforms it in **X_new**, maintaining the features of Round `rd`; \n \n<a name=\"3.2\"></a>\n### 3.2\\. Auxiliary Function `get_gmm`\n\n1. `get_gmm(X, y, y_cat=False, num_comps=[2,5,10,15,20,30,40,50], val_size=0.33, reg_covar=1e-06, covariance_type=\"full\", random_state=42)`: \n\n    - Firstly, this function validate the number of GMM components, for each model it will train, in a holdout set using the mean log likelihood of samples in that set. If Y is non-categorical, it returns a [Scikit-Learn GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) fitted in (y,X) model (in this order). On the other hand, if Y is categorical it returns a Python dictionary containing one [Scikit-Learn GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) fitted in X conditional on each category - something like X[y==c,:]. Format `{0:gmm0, 1:gmm1, ..., C:gmmC}`.\n\n        - **X**: numpy array of features; \n        - **y**: numpy array of labels;\n        - **y_cat**: if we should consider Y as categorical;\n        - **num_comps**: numbers of GMM components to be validated;\n        - **val_size**: size of holdout set used to validate the GMMs numbers of components;\n        - **reg_covar**: non-negative regularization added to the diagonal of covariance. Ensures the covariance matrices are non-singular.\n        - **covariance_type**: one of the following options:'full','tied','diag','spherical'. See [Scikit-Learn GMM](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html). Thanks to Pritha Gupta for her suggestion on this point.\n        - **random_state**: seed.\n\n--------------------\n\n<a name=\"4\"></a>\n## 4\\. Examples of *InfoSelect* use\n\nLoading Packages:\n\n\n```python\nimport infoselect as inf\nimport numpy as np  \nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n\n<a name=\"4.1\"></a>\n### 4.1\\. Dataset\n\nWe generate a dataset <img src=\"https://render.githubusercontent.com/render/math?math=D\"> sampled from <img src=\"https://render.githubusercontent.com/render/math?math=\\mathcal{D}=\\{(X_{0,i},...,X_{6,i},Y_i)\\}_{i=1}^{n}\"> similar to the one in [here](https://www.cs.toronto.edu/~delve/data/add10/desc.html), in which <img src=\"https://render.githubusercontent.com/render/math?math=Y_i\"> is given by\n\n<br>\n<img src=\"https://render.githubusercontent.com/render/math?math=%5Cbegin%7Balign*%7D%0AY_i%20%26%3D%20f(X_%7B0%2Ci%7D%2C...%2CX_%7B6%2Ci%7D)%20%2B%20%5Cepsilon_i%5C%5C%0A%20%20%20%20%26%3D10%5Ccdot%20%5Csin(%5Cpi%20X_%7B0%2Ci%7D%20%20X_%7B1%2Ci%7D)%20%2B%2020%20(X_%7B2%2Ci%7D-0.5)%5E2%20%2B%2010%20X_%7B3%2Ci%7D%20%2B%205%20X_%7B4%2Ci%7D%20%2B%20%5Cepsilon_i%0A%5Cend%7Balign*%7D\">\n<br>\n\nWhere <img src=\"https://render.githubusercontent.com/render/math?math=X_{0,i},...,X_{6,i} \\overset{iid}{\\sim} U[0,1]\"> and <img src=\"https://render.githubusercontent.com/render/math?math=\\epsilon_i \\sim N(0,1)\"> independent from all the other random variables for all <img src=\"https://render.githubusercontent.com/render/math?math=i\\in [n]\">. See that our target variable does not depende on the last two features. In the following we set `n=10000`:\n\n\n```python\ndef f(X,e): return 10*np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2]-.5)**2 + 10*X[:,3] + 5*X[:,4] + e\n```\n\n\n```python\nn=10000\nd=7\n\nX = np.random.uniform(0,1,d*n).reshape((n,d))\ne = np.random.normal(0,1,n)\ny = f(X,e)\n\nX.shape, y.shape\n```\n\n\n\n\n    ((10000, 7), (10000,))\n\n\n<a name=\"4.2\"></a>\n### 4.2\\. Selecting Features for a Regression Task\n\nTraining (and validating) GMM:\n\n\n```python\n%%time\n\ngmm = inf.get_gmm(X, y)\n```\n\n    Wall time: 8.43 s\n    \n\nOrdering features by their importances using the *Backward Elimination* algorithm:\n\n\n```python\nselect = inf.SelectVars(gmm, selection_mode = 'backward')\nselect.fit(X, y, verbose=True)    \n```\n\n    Let's start...\n    \n    Round =   0   |   Î =  1.36   |   Δ%Î =  0.00   |   Features=[0, 1, 2, 3, 4, 5, 6]\n    Round =   1   |   Î =  1.36   |   Δ%Î = -0.00   |   Features=[0, 1, 2, 3, 4, 5]\n    Round =   2   |   Î =  1.36   |   Δ%Î = -0.00   |   Features=[0, 1, 2, 3, 4]\n    Round =   3   |   Î =  0.97   |   Δ%Î = -0.29   |   Features=[0, 1, 3, 4]\n    Round =   4   |   Î =  0.73   |   Δ%Î = -0.24   |   Features=[0, 1, 3]\n    Round =   5   |   Î =  0.40   |   Δ%Î = -0.46   |   Features=[0, 3]\n    Round =   6   |   Î =  0.21   |   Δ%Î = -0.48   |   Features=[3]\n    \n\nChecking history:\n\n\n```python\nselect.get_info()\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rounds</th>\n      <th>mi_mean</th>\n      <th>mi_error</th>\n      <th>delta</th>\n      <th>num_feat</th>\n      <th>features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.358832</td>\n      <td>0.008771</td>\n      <td>0.000000</td>\n      <td>7</td>\n      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1.358090</td>\n      <td>0.008757</td>\n      <td>-0.000546</td>\n      <td>6</td>\n      <td>[0, 1, 2, 3, 4, 5]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.356661</td>\n      <td>0.008753</td>\n      <td>-0.001053</td>\n      <td>5</td>\n      <td>[0, 1, 2, 3, 4]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.969897</td>\n      <td>0.007843</td>\n      <td>-0.285085</td>\n      <td>4</td>\n      <td>[0, 1, 3, 4]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.734578</td>\n      <td>0.007396</td>\n      <td>-0.242622</td>\n      <td>3</td>\n      <td>[0, 1, 3]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.400070</td>\n      <td>0.007192</td>\n      <td>-0.455375</td>\n      <td>2</td>\n      <td>[0, 3]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.209808</td>\n      <td>0.005429</td>\n      <td>-0.475571</td>\n      <td>1</td>\n      <td>[3]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nIt is possible to see that the estimated mutual information is untouched until Round 2, when it varies around -30%.\n\nSince there is a 'break' in Round 2, we should choose to stop the algorithm at theta round. This will be clear in the Mutual Information history plot that follows:\n\n\n```python\nselect.plot_mi()\n```\n\n<img src=\"https://raw.githubusercontent.com/felipemaiapolo/imgs_infoselect/main/output_17_0.png\">\n\n\nPlotting the percentual variations of the mutual information between rounds:\n\n\n```python\nselect.plot_delta()\n```\n\n\n![png](https://raw.githubusercontent.com/felipemaiapolo/imgs_infoselect/main/output_19_0.png)\n\n\nMaking the selection choosing to stop at Round 2:\n\n\n```python\nX_new = select.transform(X, rd=2)\n\nX_new.shape\n```\n\n\n\n\n    (10000, 5)\n\n\n<a name=\"4.3\"></a>\n### 4.3\\. Selecting Features for a Classification Task\n\nCategorizing <img src=\"https://render.githubusercontent.com/render/math?math=Y\">:\n\n\n```python\nind0 = (y<np.percentile(y, 33))\nind1 = (np.percentile(y, 33)<=y) & (y<np.percentile(y, 66))\nind2 = (np.percentile(y, 66)<=y)\n\ny[ind0] = 0\ny[ind1] = 1\ny[ind2] = 2\n\ny=y.astype(int)\n```\n\n\n```python\ny[:15]\n```\n\n\n\n\n    array([2, 0, 1, 2, 0, 2, 0, 0, 0, 1, 2, 1, 0, 0, 2])\n\n\n\nTraining (and validating) GMMs:\n\n\n```python\n%%time \n\ngmm=inf.get_gmm(X, y, y_cat=True)\n```\n\n    Wall time: 6.7 s\n    \n\nOrdering features by their importances using the *Forward Selection* algorithm:\n\n\n```python\nselect=inf.SelectVars(gmm, selection_mode='forward')\nselect.fit(X, y, verbose=True)    \n```\n\n    Let's start...\n    \n    Round =   0   |   Î =  0.00   |   Δ%Î =  0.00   |   Features=[]\n    Round =   1   |   Î =  0.14   |   Δ%Î =  0.00   |   Features=[3]\n    Round =   2   |   Î =  0.28   |   Δ%Î =  1.01   |   Features=[3, 0]\n    Round =   3   |   Î =  0.50   |   Δ%Î =  0.79   |   Features=[3, 0, 1]\n    Round =   4   |   Î =  0.62   |   Δ%Î =  0.22   |   Features=[3, 0, 1, 4]\n    Round =   5   |   Î =  0.75   |   Δ%Î =  0.21   |   Features=[3, 0, 1, 4, 2]\n    Round =   6   |   Î =  0.75   |   Δ%Î = -0.00   |   Features=[3, 0, 1, 4, 2, 5]\n    Round =   7   |   Î =  0.74   |   Δ%Î = -0.01   |   Features=[3, 0, 1, 4, 2, 5, 6]\n    \n\nChecking history:\n\n\n```python\nselect.get_info()\n```\n\n\n\n\n<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rounds</th>\n      <th>mi_mean</th>\n      <th>mi_error</th>\n      <th>delta</th>\n      <th>num_feat</th>\n      <th>features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.139542</td>\n      <td>0.005217</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>[3]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.280835</td>\n      <td>0.006377</td>\n      <td>1.012542</td>\n      <td>2</td>\n      <td>[3, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0.503872</td>\n      <td>0.006499</td>\n      <td>0.794196</td>\n      <td>3</td>\n      <td>[3, 0, 1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0.617048</td>\n      <td>0.006322</td>\n      <td>0.224612</td>\n      <td>4</td>\n      <td>[3, 0, 1, 4]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>0.745933</td>\n      <td>0.005135</td>\n      <td>0.208874</td>\n      <td>5</td>\n      <td>[3, 0, 1, 4, 2]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>0.745549</td>\n      <td>0.005202</td>\n      <td>-0.000515</td>\n      <td>6</td>\n      <td>[3, 0, 1, 4, 2, 5]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>0.740968</td>\n      <td>0.005457</td>\n      <td>-0.006144</td>\n      <td>7</td>\n      <td>[3, 0, 1, 4, 2, 5, 6]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nIt is possible to see that the estimated mutual information is untouched from Round 6 onwards.\n\nSince there is a 'break' in Round 5, we should choose to stop the algorithm at theta round. This will be clear in the Mutual Information history plot that follows:\n\n\n```python\nselect.plot_mi()\n```\n\n\n![png](https://raw.githubusercontent.com/felipemaiapolo/imgs_infoselect/main/output_33_0.png)\n\n\nPlotting the percentual variations of the mutual information between rounds:\n\n\n```python\nselect.plot_delta()\n```\n\n![png](https://raw.githubusercontent.com/felipemaiapolo/imgs_infoselect/main/output_35_0.png)\n\n\nMaking the selection choosing to stop at Round 5:\n\n\n```python\nX_new = select.transform(X, rd=5)\n\nX_new.shape\n```\n\n\n\n\n    (10000, 5)\n    \n--------------\n\n<a name=\"5\"></a>\n## 5\\. References\n\n[1] Eirola, E., Lendasse, A., & Karhunen, J. (2014, July). Variable selection for regression problems using Gaussian mixture models to estimate mutual information. In 2014 International Joint Conference on Neural Networks (IJCNN) (pp. 1606-1613). IEEE.\n\n[2] Lan, T., Erdogmus, D., Ozertem, U., & Huang, Y. (2006, July). Estimating mutual information using gaussian mixture model for feature ranking and selection. In The 2006 IEEE International Joint Conference on Neural Network Proceedings (pp. 5034-5039). IEEE.\n\n[3] Maia Polo, F., & Vicente, R. (2022). Effective sample size, dimensionality, and generalization in covariate shift adaptation. Neural Computing and Applications, 1-13.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/felipemaiapolo/infoselect",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "infoselect",
    "package_url": "https://pypi.org/project/infoselect/",
    "platform": null,
    "project_url": "https://pypi.org/project/infoselect/",
    "project_urls": {
      "Homepage": "https://github.com/felipemaiapolo/infoselect"
    },
    "release_url": "https://pypi.org/project/infoselect/1.0.3/",
    "requires_dist": [
      "scipy",
      "numpy",
      "pandas",
      "sklearn",
      "matplotlib"
    ],
    "requires_python": "",
    "summary": "Mutual Information Based Feature Selection in Python.",
    "version": "1.0.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17503973,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "375808f80b189ed66fec1e1f9c929ace58d6c376a08ca5f42e7bfe542f626730",
          "md5": "848e8e7a790b8a734a949b6afd1f7a1c",
          "sha256": "386e854330c78ef77830d824e44ff5ab4f27f9229bebf15f51e644af59e2d9b3"
        },
        "downloads": -1,
        "filename": "infoselect-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "848e8e7a790b8a734a949b6afd1f7a1c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14216,
        "upload_time": "2020-11-06T12:55:35",
        "upload_time_iso_8601": "2020-11-06T12:55:35.058782Z",
        "url": "https://files.pythonhosted.org/packages/37/58/08f80b189ed66fec1e1f9c929ace58d6c376a08ca5f42e7bfe542f626730/infoselect-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "91f186690070d140d7fb7ea56934f80fadddc65a750ddd362034e8c63230dac1",
          "md5": "866bb84a5e4caec4993102fb63d9f2cf",
          "sha256": "4fe4d350e231957513d1fae0b1c744387f10a4991e424c9ac009bfe6bdea23c9"
        },
        "downloads": -1,
        "filename": "infoselect-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "866bb84a5e4caec4993102fb63d9f2cf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14620,
        "upload_time": "2020-11-06T20:19:05",
        "upload_time_iso_8601": "2020-11-06T20:19:05.167803Z",
        "url": "https://files.pythonhosted.org/packages/91/f1/86690070d140d7fb7ea56934f80fadddc65a750ddd362034e8c63230dac1/infoselect-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e2b708c71cd9bc212385e338e2e8ebde807c4512d5d2584e225e6b63b572f7ed",
          "md5": "e064f87278653cf846a5130eab24a50c",
          "sha256": "f4608baf60d9fbd892536541dee5104cfa92c6e68a765f07e75360685632fc01"
        },
        "downloads": -1,
        "filename": "infoselect-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e064f87278653cf846a5130eab24a50c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12285,
        "upload_time": "2023-03-30T01:50:47",
        "upload_time_iso_8601": "2023-03-30T01:50:47.196657Z",
        "url": "https://files.pythonhosted.org/packages/e2/b7/08c71cd9bc212385e338e2e8ebde807c4512d5d2584e225e6b63b572f7ed/infoselect-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d02720be364763e961a0f9e3a87f3761e7bb0ba486a9d75231120ea2faa2e6c6",
          "md5": "e246ae059e2cd45b359200d38386da3e",
          "sha256": "ff4bc7bdc2dbf080389db77d6684fcfed7c9fbea724e93a6fd78559bbe9eab39"
        },
        "downloads": -1,
        "filename": "infoselect-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "e246ae059e2cd45b359200d38386da3e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 10764,
        "upload_time": "2020-11-06T20:53:25",
        "upload_time_iso_8601": "2020-11-06T20:53:25.935768Z",
        "url": "https://files.pythonhosted.org/packages/d0/27/20be364763e961a0f9e3a87f3761e7bb0ba486a9d75231120ea2faa2e6c6/infoselect-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "24193287571b379b6dd89c72b6bf26270daba0a8ca0be19fd3403826da7f503a",
          "md5": "f9bf5a3bc1e8cc94b74838a4c0b34a1b",
          "sha256": "2796d081c3fa8940e7aafc4aa19080b766042b887375be71f4e304a2f17021d9"
        },
        "downloads": -1,
        "filename": "infoselect-1.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f9bf5a3bc1e8cc94b74838a4c0b34a1b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 12285,
        "upload_time": "2023-03-30T01:50:49",
        "upload_time_iso_8601": "2023-03-30T01:50:49.593959Z",
        "url": "https://files.pythonhosted.org/packages/24/19/3287571b379b6dd89c72b6bf26270daba0a8ca0be19fd3403826da7f503a/infoselect-1.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "24193287571b379b6dd89c72b6bf26270daba0a8ca0be19fd3403826da7f503a",
        "md5": "f9bf5a3bc1e8cc94b74838a4c0b34a1b",
        "sha256": "2796d081c3fa8940e7aafc4aa19080b766042b887375be71f4e304a2f17021d9"
      },
      "downloads": -1,
      "filename": "infoselect-1.0.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f9bf5a3bc1e8cc94b74838a4c0b34a1b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 12285,
      "upload_time": "2023-03-30T01:50:49",
      "upload_time_iso_8601": "2023-03-30T01:50:49.593959Z",
      "url": "https://files.pythonhosted.org/packages/24/19/3287571b379b6dd89c72b6bf26270daba0a8ca0be19fd3403826da7f503a/infoselect-1.0.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}