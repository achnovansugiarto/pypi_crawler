{
  "info": {
    "author": "Adam Wiemerslage, Kyle Gorman, Travis M. Bartley",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Environment :: Console",
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.9",
      "Topic :: Text Processing :: Linguistic"
    ],
    "description": "# Yoyodyne ðŸª€\n\n[![CircleCI](https://circleci.com/gh/CUNY-CL/yoyodyne/tree/master.svg?style=svg&circle-token=37883deeb03d32c8a7b2aa7c34e5143bf514acdd)](https://circleci.com/gh/CUNY-CL/yoyodyne/tree/master)\n\nYoyodyne provides neural models for small-vocabulary sequence-to-sequence\ngeneration with and without feature conditioning.\n\nThese models are implemented using PyTorch and PyTorch Lightning.\n\nWhile we provide classic `lstm` and `transformer` models, some of the provided\nmodels are particularly well-suited for problems where the source-target\nalignments are roughly monotonic (e.g., `transducer`) and/or where source and\ntarget vocabularies are not disjoint and substrings of the source are copied\ninto the target (e.g., `pointer_generator_lstm`).\n\n## Philosophy\n\nYoyodyne is inspired by [FairSeq](https://github.com/facebookresearch/fairseq)\nbut differs on several key points of design:\n\n-   It is for small-vocabulary sequence-to-sequence generation, and therefore\n    includes no affordances for machine translation or language modeling.\n    Because of this:\n    -  It has no plugin interface and the architectures provided are intended\n       to be reasonably exhaustive.\n    -  There is little need for data preprocessing; it works with TSV files.\n-   It has support for using features to condition decoding, with\n    architecture-specific code to handle this feature information.\n-   ðŸš§ UNDER CONSTRUCTION ðŸš§: It has exhaustive test suites.\n-   ðŸš§ UNDER CONSTRUCTION ðŸš§: It has performance benchmark.\n-   ðŸš§ UNDER CONSTRUCTION ðŸš§: Releases are made regularly.\n\n## Install\n\nFirst install dependencies:\n\n    pip install -r requirements.txt\n\nThen install:\n\n    python setup.py install\n\nOr:\n\n    python setup.py develop\n\nThe latter creates a Python module in your environment that updates as you\nupdate the code. It can then be imported like a regular Python module:\n\n```python\nimport yoyodyne\n```\n\n## Usage\n\nFor examples, see [`experiments`](experiments). See\n[`train.py`](yoyodyne/train.py) and [`predict.py`](yoyodyne/predict.py) for all\nmodel options.\n\n## Architectures\n\nThe user specifies the model using the `--arch` flag (and in some cases\nadditional flags).\n\n-   `feature_invariant_transformer`: This is a variant of the `transformer`\n    which uses a learned embedding to distinguish input symbols from features.\n    It may be superior to the vanilla transformer when using features.\n-   `lstm`: This is an LSTM encoder-decoder, with the initial hidden state\n    treated as a learned parameter. By default, the encoder is connected to the\n    decoder by an attention mechanism; one can disable this (with `--no-attn`),\n    in which case the last non-padding hidden state of the encoder is\n    concatenated with the decoder hidden state.\n-   `pointer_generator_lstm`: This is an attentive pointer-generator with an\n    LSTM backend. Since this model contains a copy mechanism, it may be superior\n    to the `lstm` when the input and output vocabularies overlap significantly.\n-   `transducer`: This is a transducer with an LSTM backend. On model creation,\n    expectation maximization is used to learn a sequence of edit operations, and\n    imitation learning is used to train the model to implement the oracle\n    policy, with roll-in controlled by the `--oracle-factor` flag (default: 1).\n    Since this model assumes monotonic alignment, it may be superior to\n    attentive models when the alignment between input and output is roughly\n    monotonic and when input and output vocabularies overlap significantly.\n-   `transformer`: This is a transformer encoder-decoder with positional\n    encoding and layer normalization. The user may wish to specify the number of\n    attention heads (with `--nheads`; default: 4).\n\nFor all models, the user may also wish to specify:\n\n-   `--dec-layers` (default: 1): number of decoder layers\n-   `--embedding` (default: 128): embedding size\n-   `--enc-layers` (default: 1): number of encoder layers\n-   `--hidden-size` (default: 256): hidden layer size\n\nBy default, the `lstm`, `pointer_generator_lstm`, and `transducer` models use an\nLSTM bidirectional encoder. One can disable this with the `--no-bidirectional`\nflag.\n\n## Training options\n\n-   `--batch-size` (default: 16)\n-   `--beta1` (default: .9): $\\beta_1$ hyperparameter for the Adam optimizer\n    (`--optimizer adam`)\n-   `--beta2` (default: .99): $\\beta_2$ hyperparameter for the Adam optimizer\n    (`--optimizer adam`)\n-   `--dropout` (default: .1): dropout probability\n-   `--epochs` (default: 20)\n-   `--gradient-clip` (default: 0.0)\n-   `--label-smoothing` (default: not enabled)\n-   `--learning-rate` (required)\n-   `--lr-scheduler` (default: not enabled)\n-   `--optimizer` (default: \"adadelta\")\n-   `--patience` (default: not enabled)\n-   `--wandb` (default: False): enables [Weights &\n    Biases](https://wandb.ai/site) tracking\n-   `--warmup-steps` (default: 1): warm-up parameter for a linear warm-up\n    followed by inverse square root decay schedule (only valid with\n    `--lr-scheduler warmupinvsq`)\n\n## Data format\n\nThe default data format is based on the SIGMORPHON 2017 shared tasks:\n\n    source   target    feat1;feat2;...\n\nThat is, the first column is the source (a lemma), the second is the target (the\ninflection), and the third contains semi-colon delimited feature strings.\n\nFor the SIGMORPHON 2016 shared task data format:\n\n    source   feat1,feat2,...    target\n\none instead specifies `--target-col 3 --features-col 2 --features-sep ,`\n\nFinally, to perform transductions without features (whether or not a feature\ncolumn exists in the data), one specifies `--features-col 0`.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "computational linguistics,morphology,natural language processing,language",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "yoyodyne",
    "package_url": "https://pypi.org/project/yoyodyne/",
    "platform": null,
    "project_url": "https://pypi.org/project/yoyodyne/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/yoyodyne/0.2.1/",
    "requires_dist": null,
    "requires_python": ">=3.9",
    "summary": "Small-vocabulary neural sequence-to-sequence models",
    "version": "0.2.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15996577,
  "releases": {
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3c1d4fd9cc0765cb44648ca44b94f29aa1c143c6787c1fc45c0ab24c937b7c79",
          "md5": "c5a0e81b621352783e0c6d521e7423e1",
          "sha256": "68593d5993d570fb4e6bb1e38a67bc41bc6462b7c9fc4bd43e7b8ce2281a4453"
        },
        "downloads": -1,
        "filename": "yoyodyne-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "c5a0e81b621352783e0c6d521e7423e1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 44771,
        "upload_time": "2022-12-05T14:57:03",
        "upload_time_iso_8601": "2022-12-05T14:57:03.996785Z",
        "url": "https://files.pythonhosted.org/packages/3c/1d/4fd9cc0765cb44648ca44b94f29aa1c143c6787c1fc45c0ab24c937b7c79/yoyodyne-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3c1d4fd9cc0765cb44648ca44b94f29aa1c143c6787c1fc45c0ab24c937b7c79",
        "md5": "c5a0e81b621352783e0c6d521e7423e1",
        "sha256": "68593d5993d570fb4e6bb1e38a67bc41bc6462b7c9fc4bd43e7b8ce2281a4453"
      },
      "downloads": -1,
      "filename": "yoyodyne-0.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "c5a0e81b621352783e0c6d521e7423e1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.9",
      "size": 44771,
      "upload_time": "2022-12-05T14:57:03",
      "upload_time_iso_8601": "2022-12-05T14:57:03.996785Z",
      "url": "https://files.pythonhosted.org/packages/3c/1d/4fd9cc0765cb44648ca44b94f29aa1c143c6787c1fc45c0ab24c937b7c79/yoyodyne-0.2.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}