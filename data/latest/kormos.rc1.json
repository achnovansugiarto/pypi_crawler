{
  "info": {
    "author": "Michael B Hynes",
    "author_email": "mike.hynes.rhymes@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Scientific/Engineering :: Information Analysis"
    ],
    "description": ".. image:: https://github.com/mbhynes/kormos/workflows/Tests/badge.svg\n  :target: https://github.com/mbhynes/kormos/actions\n\n.. image:: https://readthedocs.org/projects/kormos/badge/?version=latest\n  :target: https://kormos.readthedocs.io\n\n.. image:: https://img.shields.io/pypi/v/kormos\n  :target: https://pypi.org/project/kormos\n\n\nKormos\n=================================\n\nThe `kormos` package provides an interface between `scipy.optimize.minimize <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`_ and `Keras <https://keras.io>`_ for training models with deterministic minimization algorithms like L-BFGS.\n\nIt provides `Keras` users with:\n\n- ``keras.Model`` subclasses that may be optimized without changes in the API---a model may be trained using *either* the built-in stochastic mini-batch algorithms *or* the deterministic batch algorithms from ``scipy.optimize``\n- Out-of-the-box interoperability with the usual `Keras` utilities and workflows; e.g.:\n\n  - ``fit()`` still returns a the history object with optimization metadata and validation metrics from each iteration of the solver and is usable by `KerasTuner <https://keras.io/keras_tuner/>`_\n  - Support for distributed training strategies (at least in principle---this has admittedly not been integration tested)\n- The ability to use *second order* optimization methods from `scipy` by evaluating Hessian-vector-products if you have a very specific need for this (spoiler: you almost certainly do not)\n\n\nMotivation\n-----------\nWhy would anyone want to go full batch in this day and age?\n\n`Keras` is a powerful tool for developing predictive models and optimizing their parameters in a high-level language.\nWhile its primary use case is large-scale deep learning, Keras's auto-differentiation utilities (from `Tensorflow`) that enable rapid prototyping and optimization with gradient-based minimization algorithms are great for other use cases in mathematical modelling and numerical optimization.\n\nIf you are working with models or datasets in which training data can reasonably fit in memory on a single machine, you may have situations in which deterministic algorithms like `L-BFGS <https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb>`_ or `Newton-CG <https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html#optimize-minimize-newtoncg>`_ are complementary or viable alternatives to the *stochastic* `optimizers <https://keras.io/api/optimizers/>`_ available in `Keras`, since:\n\n- deterministic algorithms don't require additional hyperparamter tuning to ensure convergence; if you're just prototyping something small and having trouble tuning learning rates, you may just want to crank out L-BFGS for a few minutes as a sanity check that your model can in fact be optimized\n- these algorithms may have faster convergence to accurate solutions of the optimization problem if the dataset is small enough that full batch gradient and Hessian-vector-product computation times aren't prohibitive\n\nSo TL;DR: because Luddites exist even in the field of numerical optimization.\n\n.. Luddites or Wackos lol? https://www.youtube.com/watch?v=0C4yBk6syOE#t=1m48s\n\nWhy The Name Kormos?\n--------------------\n\nBecause `Keras` is a powerful and useful tool, and is named after the Greek word *κέρας*, which means *horn*.\n\n`Kormos` is related to `Keras`, but it's not very powerful or useful.\nIt's named after the Greek word *κορμός*, which means *stump*.\n\nLicense \n=======\nThis project is released under the MIT license, and contains adaptations of other codes released under the Apache and MIT licenses.\nPlease see the header in each source file for the applicable license and copyright notices therein. \n\nSetup\n=====\n\nRequirements\n------------\n\nThe `kormos` package is built for:\n\n- Python 3+ \n- Tensorflow 2+ (and the respective ``tensorflow.keras`` module with it)\n- Scipy 0.1+ (any version really, since the ``scipy.optimize.minimize`` signature is stable)\n\nInstallation\n------------\n\nInstall via the PyPI package [kormos](https://pypi.org/project/kormos/) using `pip`:\n\n.. code-block:: python\n\n  pip3 install kormos\n\nAlternatively, if you like your releases bloody rare you may install from `git` directly:\n\n.. code-block:: python\n\n  pip3 install git+https://github.com/mbhynes/kormos\n\nUsage Examples\n==============\n\nA `kormos` model is drag-and-drop replaceable with any `Keras` model.\nBelow we provide some toy code examples, including Collaborative Filtering and MNIST classification examples adapted from the *Code Examples* section of `keras.io <https://keras.io/examples/>`_. \n\nExample: Linear Regression with Sequential API\n----------------------------------------------\n\n.. code-block:: python\n\n  import numpy as np\n  from tensorflow import keras\n\n  import kormos\n\n  rank = 50\n\n  # Define the model using the keras.model.Model Sequential API\n  model = kormos.models.BatchOptimizedSequentialModel()\n  model.add(\n      keras.layers.Dense(\n          units=1,\n          input_shape=(rank,),\n          activation=None,\n          use_bias=False,\n          kernel_regularizer=keras.regularizers.L2(1e-3),\n          kernel_initializer=\"ones\",\n      )\n  )\n  loss = keras.losses.MeanSquaredError()\n  model.compile(loss=loss, optimizer='l-bfgs-b', metrics=['mean_absolute_error'])\n\n  # Generate samples of normally distributed random data\n  np.random.seed(1)\n  w = np.random.normal(size=rank)\n  X = np.random.normal(size=(1000, rank))\n  y = np.expand_dims(X.dot(w), axis=1)\n\n  Xval = np.random.normal(size=(1000, rank))\n  yval = np.expand_dims(Xval.dot(w), axis=1)\n\n  # Fit the model\n  history = model.fit(\n      x=X,\n      y=y,\n      epochs=10,\n      validation_data=(Xval, yval),\n      options={\"maxcors\": 3}, # can pass options payload if so desired\n  )\n  best_fit_weights = np.reshape(model.trainable_weights[0].numpy(), (1, -1))\n  assert np.allclose(best_fit_weights, w, 1e-2)\n\nWe can now inspect the optimization metris traced in the ``history`` object returned from ``fit()``.\nThe training metrics captured by `kormos` include the:\n\n- training loss function value (including regularization terms)\n- 2-norm of the batch gradient\n- number of evaluations of the loss/gradient function (equivalent to an *epoch* for a stochastic optimizer)\n- number of evaluations of the Hessian-vector-product function, if applicable (equivalent to an *epoch* for a stochastic optimizer)\n\n.. code-block:: python\n\n  >>> import pandas as pd; pd.DataFrame(history.history)\n          loss       grad  fg_evals  hessp_evals   val_loss  val_mean_absolute_error\n  0  79.121972  17.946233         2            0  78.418121                 7.137860\n  1   0.192005   0.713242         3            0   0.232164                 0.344657\n  2   0.056429   0.186013         4            0   0.059140                 0.088700\n  3   0.047397   0.042760         5            0   0.047348                 0.015531\n  4   0.047006   0.008019         6            0   0.047006                 0.006401\n  5   0.046991   0.001854         7            0   0.046994                 0.005846\n  6   0.046990   0.000350         8            0   0.046992                 0.005675\n  7   0.046990   0.000073         9            0   0.046992                 0.005642\n  8   0.046990   0.000051        11            0   0.046992                 0.005642\n\nWe can now also *recompile* the model to use a stochastic optimizer; let's refit the model using ADAM:\n\n.. code-block:: python\n\n  # Recompile the model to use a different optimizer (this doesn't change its weights)\n  model.compile(loss=model.loss, optimizer='adam', metrics=['mean_absolute_error'])\n\n  # Reset the weights\n  model.set_weights([np.random.random(size=(rank, 1))])\n\n  # Fit the model using ADAM\n  history = model.fit(\n      x=X,\n      y=y,\n      epochs=150,\n      validation_data=(Xval, yval),\n  )\n\nThis is a somewhat contrived example in modern machine learning (small dataset and simple model with very few parameters), but it's the kind of classical use case in which a deterministic algorithm will converge faster than a stochastic algorithm. If you were interested in `Keras` primarily for the nice `tensorflow` API and autodifferentiation routines, but had unsexy, non-deep modelling goals, this bud's for you:\n\n.. code-block:: python\n\n  >>> import pandas as pd; pd.DataFrame(history.history)\n            loss  mean_absolute_error   val_loss  val_mean_absolute_error\n  0    59.751369             6.218111  52.518566                 5.756832\n  1    50.042812             5.688218  45.344589                 5.346300\n  2    43.674156             5.308869  40.368832                 5.043641\n  3    39.074280             5.021304  36.492527                 4.795147\n  4    35.389912             4.781666  33.423710                 4.588754\n  ..         ...                  ...        ...                      ...\n  145   0.047031             0.008966   0.047031                 0.009047\n  146   0.047023             0.008606   0.047025                 0.008718\n  147   0.047017             0.008268   0.047019                 0.008344\n  148   0.047012             0.007934   0.047013                 0.007977\n  149   0.047008             0.007655   0.047009                 0.007717\n\n  [150 rows x 4 columns]\n    \n\nExample: Linear Regression using the Functional API\n---------------------------------------------------\n\nThe same linear regression model as above may be expressed equivalently by the functional API.\nHere we specify a different `scipy` solver, the Newton-CG algorithm that uses Hessian-vector-products:\n\n.. code-block:: python\n\n  # Define the model using the keras.model.Model functional API\n  model_input = keras.Input(shape=(rank,), name=\"input\")\n  model_output = keras.layers.Dense(\n      units=1,\n      input_shape=(rank,),\n      activation=None,\n      use_bias=False,\n      kernel_regularizer=keras.regularizers.L2(1e-3),\n      kernel_initializer=\"ones\",\n  )(model_input)\n  model = kormos.models.BatchOptimizedModel(\n      inputs=model_input,\n      outputs=model_output,\n  )\n  loss = keras.losses.MeanSquaredError()\n  model.compile(loss=loss, optimizer='newton-cg', metrics=['mean_absolute_error'])\n\n  # Fit the model on the same data as previously\n  history = model.fit(\n      x=X,\n      y=y,\n      epochs=10,\n      validation_data=(Xval, yval),\n  )\n  best_fit_weights = np.reshape(model.trainable_weights[0].numpy(), (1, -1))\n  assert np.allclose(best_fit_weights, w, 1e-2)\n\nThe Newton-CG algorithm has second order convergence, so we should find that the gradient norm has decreased by several orders of magnitude more than with the L-BFGS-B algorithm.\n(Of course, practically speaking this is a moot point in the world of approximate parameter estimation due to the limitations of both imperfect models and sampling bias that exists in training datasets: the numerical error in the solution is orders of magnitude smaller than other errors...)\n\nExample: Collaborative Filtering for Item Recommendation\n--------------------------------------------------------\n\nWe present a simple linear matrix factorization model for building a recommender system using the MovieLens dataset, and use the same preprocessing steps as in the `Keras` example, `Collaborative Filtering for Movie Recommendations <https://keras.io/examples/structured_data/collaborative_filtering_movielens/>`_.\n\n**Define the Model**\n\nWe define a simple matrix factorization model for factorizing the ratings matrix into the product of 2 latent feature matrices, represented by *user* and *item* embeddings: \n\n.. code-block:: python\n\n  import tensorflow as tf\n  from tensorflow import keras\n  import kormos\n\n  def build_model(rank, num_users, num_items, **kwargs):\n      inputs = [\n          keras.Input(shape=(1,), name=\"user\", dtype=tf.int32),\n          keras.Input(shape=(1,), name=\"item\", dtype=tf.int32),\n      ] \n      user_embedding = keras.layers.Embedding(\n          input_dim=(num_users + 1),\n          output_dim=rank,\n          mask_zero=True,\n          embeddings_initializer=\"normal\",\n          embeddings_regularizer=keras.regularizers.L2(1e-5),\n          name=\"user_embedding\",\n      )\n      item_embedding = keras.layers.Embedding(\n          input_dim=(num_items + 1),\n          output_dim=rank,\n          mask_zero=True,\n          embeddings_initializer=\"normal\",\n          embeddings_regularizer=keras.regularizers.L2(1e-5),\n          name=\"item_embedding\",\n      )\n      features = [\n          user_embedding(inputs[0]),\n          item_embedding(inputs[1]),\n      ]\n      output = keras.layers.Dot(axes=2, normalize=False)(features)\n      model = kormos.models.BatchOptimizedModel(\n          inputs=inputs,\n          outputs=output,\n          **kwargs\n      )\n      return model\n\n**Prepare the Data**\n\nWe run the same pre-processing steps as in the `Keras` example above.\n(Please be aware that there are methodological errors in these steps that we have left unchanged: (1) it is not correct to split the training and testing data uniformly randomly, since some movies have only 1 rating and hence should not be members of the testing set, and (2) it is not possible to construct a factorization model that represents each user/item by a vector of rank ``k`` if ``k`` is *greater* than the number of observations (ratings) that that user/item has in the training data---such a system is `overdetermined <https://en.wikipedia.org/wiki/Overdetermined_system>`_).\n\n.. code-block:: python\n\n  import pandas as pd\n  import numpy as np\n  from zipfile import ZipFile\n  import tensorflow as tf\n  from tensorflow import keras\n  from tensorflow.keras import layers\n  from pathlib import Path\n\n  # Download the data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n  # Use the ratings.csv file\n  movielens_data_file_url = (\n      \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n  )\n  movielens_zipped_file = keras.utils.get_file(\n      \"ml-latest-small.zip\", movielens_data_file_url, extract=False\n  )\n  keras_datasets_path = Path(movielens_zipped_file).parents[0]\n  movielens_dir = keras_datasets_path / \"ml-latest-small\"\n\n  # Only extract the data the first time the script is run.\n  if not movielens_dir.exists():\n      with ZipFile(movielens_zipped_file, \"r\") as zip:\n          # Extract files\n          print(\"Extracting all the files now...\")\n          zip.extractall(path=keras_datasets_path)\n          print(\"Done!\")\n\n  ratings_file = movielens_dir / \"ratings.csv\"\n  df = pd.read_csv(ratings_file)\n\n  user_ids = df[\"userId\"].unique().tolist()\n  user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n  userencoded2user = {i: x for i, x in enumerate(user_ids)}\n  movie_ids = df[\"movieId\"].unique().tolist()\n  movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n  movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n  df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n  df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n\n  num_users = len(user2user_encoded)\n  num_movies = len(movie_encoded2movie)\n  df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n  # min and max ratings will be used to normalize the ratings later\n  min_rating = min(df[\"rating\"])\n  max_rating = max(df[\"rating\"])\n\n  print(\n      \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n          num_users, num_movies, min_rating, max_rating\n      )\n  )\n\n  df = df.sample(frac=1, random_state=42)\n  x = df[[\"user\", \"movie\"]].values\n  # Normalize the targets between 0 and 1. Makes it easy to train.\n  y = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n  # Assuming training on 90% of the data and validating on 10%.\n  train_indices = int(0.9 * df.shape[0])\n  x_train, x_val, y_train, y_val = (\n      x[:train_indices],\n      x[train_indices:],\n      y[:train_indices],\n      y[train_indices:],\n  )\n\n**Train the Model**\n\nWe may now train our factorization model:\n\n.. code-block:: python\n\n  rank = 5\n  model = build_model(rank, num_users, num_movies)\n  model.compile(\n      loss=tf.keras.losses.MeanSquaredError(),\n      optimizer=\"l-bfgs-b\",\n  )\n\n  history = model.fit(\n    x=(x_train[:, 0], x_train[:, 1]),\n    y=y_train,\n    batch_size=2**14,\n    epochs=10,\n    verbose=1\n    validation_data=((x_val[:, 0], x_val[:, 1]), y_val),\n  )\n\n.. code-block:: python\n\n  >>> import pandas as pd; pd.DataFrame(history.history)\n          loss      grad  fg_evals  hessp_evals  val_loss\n  0   0.499431  0.001055         2            0  0.497424\n  1   0.492091  0.010318         5            0  0.496749\n  2   0.491067  0.015367         7            0  0.499127\n  3   0.461140  0.012731         9            0  0.472772\n  4   0.271020  0.017515        12            0  0.327173\n  5   0.228658  0.021585        14            0  0.298120\n  6   0.156481  0.012698        16            0  0.226349\n  7   0.125350  0.007833        17            0  0.193145\n  8   0.101411  0.007957        18            0  0.169513\n  9   0.093375  0.013233        19            0  0.162208\n  10  0.082876  0.005307        20            0  0.152423\n  11  0.077789  0.004717        21            0  0.149731\n  12  0.072867  0.004420        22            0  0.144979\n  13  0.066927  0.006463        23            0  0.137852\n  14  0.063850  0.004983        24            0  0.136306\n  15  0.061897  0.002353        25            0  0.133633\n  16  0.060514  0.001867        26            0  0.132471\n  17  0.058629  0.002211        27            0  0.131402\n  18  0.057408  0.003710        28            0  0.130704\n  19  0.056111  0.001484        29            0  0.129850\n \n\nExample: MNIST convnet\n----------------------\n\nAs a more realistic example of using `kormos` on a canonical dataset, we adapt the sample classification problem from the `MNIST convnet <https://keras.io/examples/vision/mnist_convnet/>`_ example.\nPlease note that this convolutional network model has a large number of highly correlated parameters to optimize, and stochastic algorithms like ADAM will generally perform better and provide better results.\nHowever we provide it as an example of how both stochastic and deterministic algorithms may be combined by *recompiling* a `kormos` model.\n\n**Prepare the Data**\n\n.. code-block:: python\n\n  import numpy as np\n\n  from tensorflow import keras \n  from keras import layers\n\n  # Model / data parameters\n  num_classes = 10\n  input_shape = (28, 28, 1)\n\n  # Load the data and split it between train and test sets\n  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n  # Scale images to the [0, 1] range\n  x_train = x_train.astype(\"float32\") / 255\n  x_test = x_test.astype(\"float32\") / 255\n  # Make sure images have shape (28, 28, 1)\n  x_train = np.expand_dims(x_train, -1)\n  x_test = np.expand_dims(x_test, -1)\n  print(\"x_train shape:\", x_train.shape)\n  print(x_train.shape[0], \"train samples\")\n  print(x_test.shape[0], \"test samples\")\n\n  # convert class vectors to binary class matrices\n  y_train = keras.utils.to_categorical(y_train, num_classes)\n  y_test = keras.utils.to_categorical(y_test, num_classes)\n\n**Build the Model**\n\n.. code-block:: python\n  \n  from kormos.models import BatchOptimizedSequentialModel\n\n  def build_model():\n      model = BatchOptimizedSequentialModel(\n          [\n              keras.Input(shape=input_shape),\n              layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n              layers.MaxPooling2D(pool_size=(2, 2)),\n              layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n              layers.MaxPooling2D(pool_size=(2, 2)),\n              layers.Flatten(),\n              layers.Dropout(0.5),\n              layers.Dense(num_classes, activation=\"softmax\"),\n          ]\n      )\n      return model\n\n  model = build_model()\n  model.summary()\n\n.. code-block::\n\n  Model: \"batch_optimized_sequential_model\"\n  _________________________________________________________________\n   Layer (type)                   Output Shape              Param #\n  =================================================================\n   conv2d (Conv2D)                (None, 26, 26, 32)        320\n\n   max_pooling2d (MaxPooling2D)   (None, 13, 13, 32)        0\n\n   conv2d_1 (Conv2D)              (None, 11, 11, 64)        18496\n\n   max_pooling2d_1 (MaxPooling2D) (None, 5, 5, 64)          0\n\n   flatten (Flatten)              (None, 1600)              0\n\n   dropout (Dropout)              (None, 1600)              0\n\n   dense (Dense)                  (None, 10)                16010\n\n  =================================================================\n  Total params: 34,826\n  Trainable params: 34,826\n  Non-trainable params: 0\n  _________________________________________________________________\n\n**Train the Model**\n\nWe use this example train the model by running a combination of different algorithms.\nWe start by running ADAM for 1 epoch, and then using this solution as a warm start initial guess for a batch solver by *recompiling* the model:\n\n.. code-block:: python\n\n  loss = keras.losses.CategoricalCrossentropy()\n  # Train a model with ADAM\n  model = build_model()\n  model.compile(loss=loss, optimizer=\"adam\", metrics=[\"accuracy\"])\n  hist1 = model.fit(x_train, y_train, batch_size=2**5, epochs=1, validation_data=(x_test, y_test))\n\n  # Continue training the model with a batch algorithm.\n  # We can instantiate the optimizer as well instead of a string identifier\n  optimizer = kormos.optimizers.ScipyBatchOptimizer()\n  model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n\n  # We can specify the method and any options for it in fit as keyword wargs\n  hist2 = model.fit(\n      x_train,\n      y_train,\n      batch_size=2**14, # this is much larger than for stochastic solvers!\n      epochs=3,\n      validation_data=(x_test, y_test),\n      method='bfgs',\n  )\n\nImplementation Details\n======================\n\nThe `kormos` package implements an interface for batch optimization and wraps ``scipy.optimize.minimize`` in that interface in the following steps:\n\n- We create a subclass of ``keras.Model``, ``BatchOptimizedModel`` (and ``BatchOptimizedSequentialModel`` to extend the `Sequential` API).\n\n- The subclass provides a ``fit_batch()`` method with nearly identical signature to the parent ``fit()``, but does not perform stochastic mini-batch optimization. Instead, this method offloads all optimization to the the model's ``optimizer`` attribute, which must implement the method ``minimize()`` to perform training by minimizing the the loss function provided during model compilation.\n\n- When a ``BatchOptimizedModel`` is compiled with a ``BatchOptimzer`` (or string identifier for one) as its `optimizer` argument, the ``fit()`` method inherited from ``keras.Model`` is overriden with a pointer to ``fit_batch()`` (such that a ``BatchOptimizedModel`` may be trained with either stochastic or deterministic solvers, depending on how it's compiled).\n\n- The ``ScipyBatchOptimizer`` class extends the ``BatchOptimizer`` interface and uses the ``scipy.optimize.minimize`` routine to fit the model.\n\nAt first face this is more complicated than the *recommended* way of extending `Keras` to perform custom training (i.e. by overriding ``keras.Model.train_step()`` such as in the article `Customizing what happens in fit() <https://keras.io/guides/customizing_what_happens_in_fit/>`_).\nHowever, unfortunately we found extending ``train_step()`` to be awkward or infeasible for implementing a batch optimization algorithm while still making use of the standard `Keras` utilities for computing *validation metrics* at each iteration end (epoch).\nOverriding the model ``train_step()`` (and putting the call to `scipy.optimize.minimize` inside it) would mean that from the `Keras` model's perspective only a single *epoch* would be performed, such that validation metrics would only be computed at the very end of the optimzation routine.\n\nAcknowledgements & Related Work\n================================\n\nThis package has adapted code from the following sources:\n\n- `Pi-Yueh Chuang's <https://pychao.com/contact-us-and-pgp-key/>`_ MIT-licensed `scipy.optimize.minimize_lbfgs` wrapper on `github here <https://gist.github.com/piyueh/712ec7d4540489aad2dcfb80f9a54993>`_.\n- `Allen Lavoie's <https://github.com/allenlavoie>`_ Hessian-vector-product routines from `tensorflow`, available in the `following commit <https://github.com/tensorflow/tensorflow/commit/5b37e7ed14eb7dddae8a0e87435595347a315bb7>`_ under the Apache License version 2.\n\nThere is also a related project `keras-opt <https://github.com/pedro-r-marques/keras-opt>`_ with the same goal but different implementation and API.\nThe `kormos` package is recommended over `keras-opt` because its implementation is faster and more robust when training models with large memory requirements, it exposes all of the arguments to ``scipy.optimize.minimize`` if you wish to solve a constrained optimization problem, and is a little bit more seemless to use as part of the native `Keras` workflow.\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/mbhynes/kormos",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "kormos",
    "package_url": "https://pypi.org/project/kormos/",
    "platform": null,
    "project_url": "https://pypi.org/project/kormos/",
    "project_urls": {
      "Homepage": "https://github.com/mbhynes/kormos"
    },
    "release_url": "https://pypi.org/project/kormos/0.1.4/",
    "requires_dist": [
      "tensorflow (>=2)",
      "numpy",
      "scipy"
    ],
    "requires_python": ">=3",
    "summary": "An interface to scipy.optimize.minimize for training Keras models with batch optimization algorithms like L-BFGS.",
    "version": "0.1.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14824693,
  "releases": {
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4b66749a974a51a6e0b5783a34946d6a019e184b7719c3e0be41bc92f77447a7",
          "md5": "417e102188fbbaad8028499e62e8baef",
          "sha256": "c751921d3d4bea99d617bd19fecc251fff97150c1472aa5b92a3347b61cb08f0"
        },
        "downloads": -1,
        "filename": "kormos-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "417e102188fbbaad8028499e62e8baef",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 33790,
        "upload_time": "2022-08-21T02:03:56",
        "upload_time_iso_8601": "2022-08-21T02:03:56.917006Z",
        "url": "https://files.pythonhosted.org/packages/4b/66/749a974a51a6e0b5783a34946d6a019e184b7719c3e0be41bc92f77447a7/kormos-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "92ac6938f9eb79ea4fa43f0bffd42efa584bbf3daf7fa7068303f7d5c57a09c1",
          "md5": "3b59e4699485048ac18bee558a9bb566",
          "sha256": "ef7bbd3b4ac3adba6a462adc213027bd0bf8fcda9d43af58634ff9bdab95185f"
        },
        "downloads": -1,
        "filename": "kormos-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "3b59e4699485048ac18bee558a9bb566",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 37373,
        "upload_time": "2022-08-21T02:03:59",
        "upload_time_iso_8601": "2022-08-21T02:03:59.835994Z",
        "url": "https://files.pythonhosted.org/packages/92/ac/6938f9eb79ea4fa43f0bffd42efa584bbf3daf7fa7068303f7d5c57a09c1/kormos-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4b66749a974a51a6e0b5783a34946d6a019e184b7719c3e0be41bc92f77447a7",
        "md5": "417e102188fbbaad8028499e62e8baef",
        "sha256": "c751921d3d4bea99d617bd19fecc251fff97150c1472aa5b92a3347b61cb08f0"
      },
      "downloads": -1,
      "filename": "kormos-0.1.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "417e102188fbbaad8028499e62e8baef",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3",
      "size": 33790,
      "upload_time": "2022-08-21T02:03:56",
      "upload_time_iso_8601": "2022-08-21T02:03:56.917006Z",
      "url": "https://files.pythonhosted.org/packages/4b/66/749a974a51a6e0b5783a34946d6a019e184b7719c3e0be41bc92f77447a7/kormos-0.1.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "92ac6938f9eb79ea4fa43f0bffd42efa584bbf3daf7fa7068303f7d5c57a09c1",
        "md5": "3b59e4699485048ac18bee558a9bb566",
        "sha256": "ef7bbd3b4ac3adba6a462adc213027bd0bf8fcda9d43af58634ff9bdab95185f"
      },
      "downloads": -1,
      "filename": "kormos-0.1.4.tar.gz",
      "has_sig": false,
      "md5_digest": "3b59e4699485048ac18bee558a9bb566",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3",
      "size": 37373,
      "upload_time": "2022-08-21T02:03:59",
      "upload_time_iso_8601": "2022-08-21T02:03:59.835994Z",
      "url": "https://files.pythonhosted.org/packages/92/ac/6938f9eb79ea4fa43f0bffd42efa584bbf3daf7fa7068303f7d5c57a09c1/kormos-0.1.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}