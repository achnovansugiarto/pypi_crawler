{
  "info": {
    "author": "David MacÃªdo",
    "author_email": "dlm@cin.ufpe.br",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: Apache Software License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<img align=\"center\" src=\"assets/pme.png\" width=\"750\">\n\n# The Robust Deep Learning Library\n\n>> **TRAIN OR FINE-TUNE**\n\nTrain your model from scratch or fine-tune a pretrained model using the losses provided in this library to improve out-of-distribution detection and uncertainty estimation performances.\n\n>> **CALIBRATE**\n\nCalibrate your model to produce enhanced uncertainy estimations.\n\n>> **DETECT**\n\nDetect out-of-distribution data using the defined score type and threshold. \n\n## Features\n\n- **Model Independent:**\n\nUse models from timm library or whatever you want.\n\n- **Data Independent:**\n\nMost cases work for any type of media (e.g., image, text, audio, and others).\n\n- **Large-Scale Models and Data:**\n\nTrain using large-scale models and data (e.g., ImageNet).\n\n- **Efficient Inferences:**\n\nThe trained models are as efficient as the ones trained using the cross-entropy loss. \n\n- **Hyperparameter-Free:**\n\nThere is no hyperparameter to tune. \"You only train once\" (YOTO).\n\n- **Standard Interface:**\n\nUse the same API to train models with improved robustness using different losses.\n\n- **No Need for Additional Data:**\n\nThe losses used in this library do not require collecting or using additional data. \n\n- **Temperature Calibration:**\n\nCalculate the Uncertainty Estimation and update the temperature of the output last layer. \n\n- **Scalability: More data, Bigger Models, Better Results!**\n\nEntropic losses perform better and better as the size of the data and model increase. \n\n- **Threshold Computation:**\n\nCompute the threshold for deciding regarding out-of-distribution examples. \n\n- **Scores Computation:**\n\nCompute the scores opting from a set of many different types available. \n\n- **Detect Out-of-Distribution:**\n\nDetect out-of-distribution examples using the computed scores. \n\n- **State-of-the-art:**\n\nSOTA results for out-of-distribution detection and uncertainty estimation.\n\n## Results\n\n### Model=ResNet18, Dataset=ImageNet, Near OOD=ImageNet-O \n\n| Loss [Score] | Class (ACC) | Near OOD (AUROC) |\n|:---|:---:|:---:|\n| Cross-Entropy [MPS] | 69.9 | 52.4 |\n| DisMax [MMLES] | 69.6 | 75.8 |\n\nMore ImageNet results comming soon...\n\nFor results regarding CIFAR10 and CIFAR100, please see the DisMax paper:\n\nhttps://arxiv.org/abs/2205.05874\n\n## Installation\n\n```bash\npip install robust-deep-learning\n```\n\n## Usage\n\n```python\n# Import the robust deep learning library as rdl\nimport robust_deep_learning as rdl\n\n#####################################################################################################\n#####################################################################################################\n# Training or Fine-tuning\n#####################################################################################################\n#####################################################################################################\n\n#########################################################\n# Option 1: Creating a custom model and defining the loss\n#########################################################\n\n# Create from a model definition file. \n# For example, import a class \"Model\" from a model definition file.\nmodel = Model()\n# Load a pretrained model if fine-tuning rather than training from scratch (random weights).\n# model.load_state_dict(torch.load(pre_trained_file_name, map_location=\"cuda:\" + str(args.gpu)))\n\n# Chance the output last layer of the model with the desired loss first part.\n# If the name of the output last layer of the model is unknown, print it with \"print(model)\".\n# For example, if the output last layer is called \"classifier\":\n# \"model.classifier = nn.Linear(num_features, num_classes)\"\n# Then, replace this output layer (usually a linear layer) by adding the following line: \nmodel.classifier = rdl.DisMaxLossFirstPart(num_features, num_classes)\n\n# Replace the Cross-Entropy Loss.\n# The first argument is the name of the output last layer of the model used above.\n# In case of training using a not too constrained model on image data, pass an add-on.\n# Currently, only DisMax has one add-on called \"FPR\".\n# Otherwise, do not pass add-on or simple pass \"add_on=None\". \ncriterion = rdl.DisMaxLossSecondPart(model.classifier, add_on=\"FPR\", gpu=None)\n\n#######################################################\n# Option 2: Creating a timm model and defining the loss\n#######################################################\n\n# For using a model from timm lib, use \"create_model\" functionality.\n# It is possible to start from a pretrained model to fine-tune using the desired loss.\nmodel = timm.create_model('resnet18', pretrained=False)\n\n# Chance the output last layer of the model with the desired loss first part.\n# If the name of the output last layer of the model is unknown, print it with \"print(model)\".\n# For example, if the output last layer is called \"fc\":\n# \"model.fc = nn.Linear(num_features, num_classes)\"\n# Then, replace this output layer (usually a linear layer) by adding the following line: \nmodel.fc = rdl.DisMaxLossFirstPart(model.get_classifier().in_features, num_classes)\n\n# Replace the Cross-Entropy Loss.\n# The first argument is the name of the output last layer of the model used above.\n# In case of training using a not too constrained model on image data, pass an add-on.\n# Currently, only DisMax has one add-on called \"FPR\".\n# Otherwise, do not pass add-on or simple pass \"add_on=None\". \ncriterion = rdl.DisMaxLossSecondPart(model.fc, add_on=\"FPR\", gpu=None)\n\n############################\n# Checking the training loop\n############################\n\nfor epoch in epochs:\n\n    # Training loop\n    for inputs, targets in in_data_train_loader:\n\n        # In the training loop, add the line below for preprocessing before forwarding.\n        # This is only required if using add_on other than None. Otherwise, this line is not needed.\n        inputs, targets = criterion.preprocess(inputs, targets) \n\n        # The three below lines are usually found in the training loop!\n        # These lines must not be changed!\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n\n#####################################################################################################\n#####################################################################################################\n# Uncertainty Estimation\n#####################################################################################################\n#####################################################################################################\n\n#############\n# Calibrating\n#############\n\n# Get outputs, labels, accuracy, expected calibration error (ECE), and negative log-likelihood (NLL).\nresults = rdl.get_outputs_labels_and_metrics(model, in_data_val_loader, gpu=None)\n\n# Calculate probabilities and verify the values returned above. \nprobabilities = torch.nn.Softmax(dim=1)(results[\"outputs\"])\nprint(probabilities)\nprint(results[\"acc\"], results[\"ece\"], results[\"nll\"])\n\n# Calibrate the temperature passing the output last layer, the model, and the validation set.\n# Choose the metric to optimize. For example, \"ECE\" (the only option for now).\nrdl.calibrate_temperature(model.classifier, model, in_data_val_loader, optimize=\"ECE\", gpu=None)\n#rdl.calibrate_temperature(model.fc, model, in_data_val_loader, optimize=\"ECE\")\n\n# Verify the novel value of the temperature of the output last layer.\nprint(model.classifier.temperature)\n\n######################\n# Verifing calibration\n######################\n\n# Get the results again using the calibrated model.\nresults = rdl.get_outputs_labels_and_metrics(model, in_data_val_loader, gpu=None)\n\n# Verifiy the probabilities, ECE, and NLL are improved regarding the now calibrated model.\nprobabilities = torch.nn.Softmax(dim=1)(results[\"outputs\"])\nprint(probabilities)\nprint(results[\"acc\"], results[\"ece\"],= results[\"nll\"])\n\n####################################################################################################\n####################################################################################################\n# Out-of-Distribution Detection\n####################################################################################################\n####################################################################################################\n\n########################\n# Estimating performance\n########################\n\n# Define a score type. Typically, the best/recommended option for the loss you are using.\nscore_type = \"MMLES\"\n\n# Evaluate the out-of-distribution detection performance passing loaders.\nood_metrics = rdl.get_ood_metrics(\n    model, in_data_val_loader, out_data_loader, score_type, fpr=0.05, gpu=None)\n\n# Optionally, first get in-data scores:\nresults = rdl.get_outputs_labels_and_metrics(model, in_data_val_loader, gpu=None)\nin_data_scores = rdl.get_scores(results[\"outputs\"], score_type)\n\n# Second, get out-data scores:\nresults = rdl.get_outputs_labels_and_metrics(model, out_data_loader, gpu=None)\nout_data_scores = rdl.get_scores(results[\"outputs\"], score_type)\n\n# Then, finally, evaluate the out-of-distribution detection performance passing scores.\nood_metrics = rdl.get_ood_metrics_from_scores(in_data_scores, out_data_scores, fpr=0.05)\n\n######################################\n# Detecting (still testing this part))\n######################################\n\n# Before detecting, it is necessary to compute the thresholds.\nthresholds = rdl.get_thresholds(model, in_data_val_loader, score_type, gpu=None)\n\n# Optionaly, it is possible to compute the threshold using in-data scores.\n#thresholds = rdl.get_thresholds(results[\"outputs\"], score_types=\"MMLES\")\nthresholds = rdl.get_thresholds_from_scores(in_data_scores) # guarder \n\n# After calculating thresholds, detection may be performed.\n# Some test input may be obtained using: input = next(iter(in_data_val_loader))\nood_detections = rdl.get_ood_detections(model, inputs, thresholds, fpr=\"0.05\", gpu=None)\n```\n\n## Losses and Scores\n\nThe following losses are implemented:\n\n- Isotropy Maximization Loss [arXiv](https://arxiv.org/abs/2006.04005) [conference version](https://ieeexplore.ieee.org/document/9533899) [journal version](https://ieeexplore.ieee.org/document/9556483)\n- Enhanced Isotropy Maximization Loss [arXiv](https://arxiv.org/abs/2105.14399)\n- Distiction Maximization Loss [arXiv](https://arxiv.org/abs/2205.05874)\n\nThe following scores are implemented:\n\n- Maximum Probability Score\n- Entropic Score [arXiv](https://arxiv.org/abs/2006.04005) [conference version](https://ieeexplore.ieee.org/document/9533899) [journal version](https://ieeexplore.ieee.org/document/9556483)\n- Minimum Distance Score [arXiv](https://arxiv.org/abs/2105.14399)\n- Maximum Mean Logit Entropy Score [arXiv](https://arxiv.org/abs/2205.05874)\n\n## Reproducibility\n\nPlease, move to the `data` directory and run all the prepare data bash scripts:\n\n```bash\n# Download and prepare out-of-distrbution data for CIFAR10 and CIFAR100 datasets.\n./prepare-cifar.sh\n# Download and prepare out-of-distrbution data for ImageNet.\n./prepare-imagenet.sh\n```\n\n```bash\n./run_cifar100_densenetbc100.sh*\n./run_cifar100_resnet34.sh*\n./run_cifar100_wideresnet2810.sh*\n./run_cifar10_densenetbc100.sh*\n./run_cifar10_resnet34.sh*\n./run_cifar10_wideresnet2810.sh*\n./run_imagenet1k_resnet18.sh*\n```\n\n```bash\n./analize.sh\n```\n\n## Citing\n\n### BibTeX\n\nPlease, cite our papers if you use our losses in your work:\n\n```bibtex\n@INPROCEEDINGS{9533899,\n  author={MacÃªdo, David and Ren, Tsang Ing and Zanchettin, Cleber and Oliveira, \n  Adriano L. I. and Ludermir, Teresa},\n  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, \n  title={Entropic Out-of-Distribution Detection}, \n  year={2021},\n  pages={1-8},\n  doi={10.1109/IJCNN52387.2021.9533899}}\n```\n\n```bibtex\n@ARTICLE{9556483,\n  author={MacÃªdo, David and Ren, Tsang Ing and Zanchettin, Cleber and Oliveira, \n  Adriano L. I. and Ludermir, Teresa},\n  journal={IEEE Transactions on Neural Networks and Learning Systems}, \n  title={Entropic Out-of-Distribution Detection:\n  Seamless Detection of Unknown Examples}, \n  year={2022},\n  volume={33},\n  number={6},\n  pages={2350-2364},\n  doi={10.1109/TNNLS.2021.3112897}}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-2105-14399,\n  author    = {David Mac{\\^{e}}do and\n               Teresa Bernarda Ludermir},\n  title     = {Enhanced Isotropy Maximization Loss: \n  Seamless and High-Performance Out-of-Distribution Detection\n  Simply Replacing the SoftMax Loss},\n  journal   = {CoRR},\n  volume    = {abs/2105.14399},\n  year      = {2021},\n  url       = {https://arxiv.org/abs/2105.14399},\n  eprinttype = {arXiv},\n  eprint    = {2105.14399},\n  timestamp = {Wed, 02 Jun 2021 11:46:42 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-14399.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-2205-05874,\n  author    = {David Mac{\\^{e}}do and\n               Cleber Zanchettin and\n               Teresa Bernarda Ludermir},\n  title     = {Distinction Maximization Loss:\n  Efficiently Improving Out-of-Distribution Detection and Uncertainty Estimation\n  Simply Replacing the Loss and Calibrating},\n  journal   = {CoRR},\n  volume    = {abs/2205.05874},\n  year      = {2022},\n  url       = {https://doi.org/10.48550/arXiv.2205.05874},\n  doi       = {10.48550/arXiv.2205.05874},\n  eprinttype = {arXiv},\n  eprint    = {2205.05874},\n  timestamp = {Tue, 17 May 2022 17:31:03 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2205-05874.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n```bibtex\n@article{DBLP:journals/corr/abs-2208-03566,\n  author    = {David Mac{\\^{e}}do},\n  title     = {Towards Robust Deep Learning using Entropic Losses},\n  journal   = {CoRR},\n  volume    = {abs/2208.03566},\n  year      = {2022},\n  url       = {https://doi.org/10.48550/arXiv.2208.03566},\n  doi       = {10.48550/arXiv.2208.03566},\n  eprinttype = {arXiv},\n  eprint    = {2208.03566},\n  timestamp = {Wed, 10 Aug 2022 14:49:54 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2208-03566.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/dlmacedo/robust-deep-learning",
    "keywords": "pytorch isomax isomax+ dismax",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "robust-deep-learning",
    "package_url": "https://pypi.org/project/robust-deep-learning/",
    "platform": null,
    "project_url": "https://pypi.org/project/robust-deep-learning/",
    "project_urls": {
      "Homepage": "https://github.com/dlmacedo/robust-deep-learning"
    },
    "release_url": "https://pypi.org/project/robust-deep-learning/0.5.0/",
    "requires_dist": [
      "torch (>=1.10)",
      "torchvision",
      "torchmetrics",
      "torchnet",
      "numpy",
      "pandas",
      "scipy",
      "scikit-learn",
      "matplotlib",
      "seaborn",
      "tqdm",
      "timm"
    ],
    "requires_python": ">=3.6",
    "summary": "The Robust Deep Learning Library",
    "version": "0.5.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14914893,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "992850aceabcfb7a921a4f29b6c308f291e1a982124aadb64ab1da99d60f2965",
          "md5": "8a04edfedfc4a0c7a9373e7a5dcb6a38",
          "sha256": "4970fffe8bbbc0626ba242d6c09a36640ec3884474c250bdd0f98340ed471e5d"
        },
        "downloads": -1,
        "filename": "robust_deep_learning-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8a04edfedfc4a0c7a9373e7a5dcb6a38",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 25237,
        "upload_time": "2022-08-20T04:23:07",
        "upload_time_iso_8601": "2022-08-20T04:23:07.108705Z",
        "url": "https://files.pythonhosted.org/packages/99/28/50aceabcfb7a921a4f29b6c308f291e1a982124aadb64ab1da99d60f2965/robust_deep_learning-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fa2bfaac2cf6dac6febd908a8999327cdfe384763859972c48d918bf369850ad",
          "md5": "f366938bbd2329a7c81661a146e63419",
          "sha256": "d7ee30c0c20ad73be225318178ff47438d173e7de7c21a125e76bf71147d2a66"
        },
        "downloads": -1,
        "filename": "robust_deep_learning-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f366938bbd2329a7c81661a146e63419",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 25237,
        "upload_time": "2022-08-20T04:29:17",
        "upload_time_iso_8601": "2022-08-20T04:29:17.862315Z",
        "url": "https://files.pythonhosted.org/packages/fa/2b/faac2cf6dac6febd908a8999327cdfe384763859972c48d918bf369850ad/robust_deep_learning-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b4e9be3d82f1dfadee9fc427141f4a59098763a159138ae15d67efe2454a2838",
          "md5": "80f1615a08a86e9d9f931419d00ec6ed",
          "sha256": "ebe5445708ddd0ea6aed6a277faccc7cdb43c5cceb915f8a2d571a05074b9604"
        },
        "downloads": -1,
        "filename": "robust_deep_learning-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "80f1615a08a86e9d9f931419d00ec6ed",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 24617,
        "upload_time": "2022-08-20T11:45:19",
        "upload_time_iso_8601": "2022-08-20T11:45:19.454934Z",
        "url": "https://files.pythonhosted.org/packages/b4/e9/be3d82f1dfadee9fc427141f4a59098763a159138ae15d67efe2454a2838/robust_deep_learning-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dd33591540bf7d4c759796e2238db0ed908816025d8afad5ba6413a269598290",
          "md5": "dd571d981b3a12a0243ba8bd445e67c8",
          "sha256": "fbc30aa76195b42f876bba5fdbc807221d2354e602dc9f040652324d9ed655a5"
        },
        "downloads": -1,
        "filename": "robust_deep_learning-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "dd571d981b3a12a0243ba8bd445e67c8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 19692,
        "upload_time": "2022-08-27T16:35:28",
        "upload_time_iso_8601": "2022-08-27T16:35:28.144947Z",
        "url": "https://files.pythonhosted.org/packages/dd/33/591540bf7d4c759796e2238db0ed908816025d8afad5ba6413a269598290/robust_deep_learning-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6efcac0c5c7a3b19d5bc506793df68a2ef409d12950e213b83f213434adf39d0",
          "md5": "a7cb994a749932da8eb51e66cded51ad",
          "sha256": "97f527d120342b455e62e399469a87edc6752d9655db11f650aad6c9ee4b0385"
        },
        "downloads": -1,
        "filename": "robust_deep_learning-0.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a7cb994a749932da8eb51e66cded51ad",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 19686,
        "upload_time": "2022-08-27T17:17:11",
        "upload_time_iso_8601": "2022-08-27T17:17:11.068164Z",
        "url": "https://files.pythonhosted.org/packages/6e/fc/ac0c5c7a3b19d5bc506793df68a2ef409d12950e213b83f213434adf39d0/robust_deep_learning-0.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.5.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a2d83376f73ae491f0ea05202534d456aadb5e8de1f57a2dcac4125297d65600",
          "md5": "424a89d3650aaf2df220a301628c317d",
          "sha256": "70b40a3d522723757e9696c33d8ee1553190abcd8eb4abd09c2a4b64c0d0fa40"
        },
        "downloads": -1,
        "filename": "robust_deep_learning-0.5.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "424a89d3650aaf2df220a301628c317d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 19684,
        "upload_time": "2022-08-28T13:39:47",
        "upload_time_iso_8601": "2022-08-28T13:39:47.069079Z",
        "url": "https://files.pythonhosted.org/packages/a2/d8/3376f73ae491f0ea05202534d456aadb5e8de1f57a2dcac4125297d65600/robust_deep_learning-0.5.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a2d83376f73ae491f0ea05202534d456aadb5e8de1f57a2dcac4125297d65600",
        "md5": "424a89d3650aaf2df220a301628c317d",
        "sha256": "70b40a3d522723757e9696c33d8ee1553190abcd8eb4abd09c2a4b64c0d0fa40"
      },
      "downloads": -1,
      "filename": "robust_deep_learning-0.5.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "424a89d3650aaf2df220a301628c317d",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 19684,
      "upload_time": "2022-08-28T13:39:47",
      "upload_time_iso_8601": "2022-08-28T13:39:47.069079Z",
      "url": "https://files.pythonhosted.org/packages/a2/d8/3376f73ae491f0ea05202534d456aadb5e8de1f57a2dcac4125297d65600/robust_deep_learning-0.5.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}