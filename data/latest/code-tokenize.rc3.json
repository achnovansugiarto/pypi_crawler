{
  "info": {
    "author": "Cedric Richter",
    "author_email": "cedricr.upb@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Software Development :: Build Tools"
    ],
    "description": "<p align=\"center\">\n  <img height=\"150\" src=\"https://github.com/cedricrupb/ptokenizers/raw/main/resources/code_tokenize.svg\" />\n</p>\n\n------------------------------------------------\n> Fast tokenization and structural analysis of\nany programming language in Python\n\nProgramming Language Processing (PLP) brings the capabilities of modern NLP systems to the world of programming languages. \nTo achieve high performance PLP systems, existing methods often take advantage of the fully defined nature of programming languages. Especially the syntactical structure can be exploited to gain knowledge about programs.\n\n**code.tokenize** provides easy access to the syntactic structure of a program. The tokenizer converts a program into a sequence of program tokens ready for further end-to-end processing.\nBy relating each token to an AST node, it is possible to extend the program representation easily with further syntactic information.\n\n## Installation\nThe package is tested under Python 3. It can be installed via:\n```\npip install code-tokenize\n```\n\n## Usage\ncode.tokenize can tokenize nearly any program code in a few lines of code:\n```python\nimport code_tokenize as ctok\n\n# Python\nctok.tokenize(\n    '''\n        def my_func():\n            print(\"Hello World\")\n    ''',\nlang = \"python\")\n\n# Output: [def, my_func, (, ), :, #NEWLINE#, ...]\n\n# Java\nctok.tokenize(\n    '''\n        public static void main(String[] args){\n          System.out.println(\"Hello World\");\n        }\n    ''',\nlang = \"java\", \nsyntax_error = \"ignore\")\n\n# Output: [public, static, void, main, (, String, [, ], args), {, System, ...]\n\n# JavaScript\nctok.tokenize(\n    '''\n        alert(\"Hello World\");\n    ''',\nlang = \"javascript\", \nsyntax_error = \"ignore\")\n\n# Output: [alert, (, \"Hello World\", ), ;]\n\n\n```\n\n## Supported languages\ncode.tokenize employs [tree-sitter](https://tree-sitter.github.io/tree-sitter/) as a backend. Therefore, in principal, any language supported by tree-sitter is also\nsupported by a tokenizer in code.tokenize.\n\nFor some languages, this library supports additional\nfeatures that are not directly supported by tree-sitter.\nTherefore, we distinguish between three language classes\nand support the following language identifier:\n\n- `native`: python\n- `advanced`: java\n- `basic`: javascript, go, ruby, cpp, c, swift, rust, ...\n\nLanguages in the `native` class support all features \nof this library and are extensively tested. `advanced` languages are tested but do not support the full feature set. Languages of the `basic` class are not tested and\nonly support the feature set of the backend. They can still be used for tokenization and AST parsing.\n\n## How to contribute\n**Your language is not natively supported by code.tokenize or the tokenization seems to be incorrect?** Then change it!\n\nWhile code.tokenize is developed mainly as an helper library for internal research projects, we welcome pull requests of any sorts (if it is a new feature or a bug fix). \n\n**Want to help to test more languages?**\nOur goal is to support as many languages as possible at a `native` level. However, languages on `basic` level are completly untested. You can help by testing `basic` languages and reporting issues in the tokenization process!\n\n## Release history\n* 0.2.0\n    * Major API redesign!\n    * CHANGE: AST parsing is now done by an external library: [code_ast](https://github.com/cedricrupb/code_ast)\n    * CHANGE: Visitor pattern instead of custom tokenizer\n    * CHANGE: Custom visitors for language dependent tokenization\n* 0.1.0\n    * The first proper release\n    * CHANGE: Language specific tokenizer configuration\n    * CHANGE: Basic analyses of the program structure and token role\n    * CHANGE: Documentation\n* 0.0.1\n    * Work in progress\n\n## Project Info\nThe goal of this project is to provide developer in the\nprogramming language processing community with easy\naccess to program tokenization and AST parsing. This is currently developed as a helper library for internal research projects. Therefore, it will only be updated\nas needed.\n\nFeel free to open an issue if anything unexpected\nhappens. \n\nDistributed under the MIT license. See ``LICENSE`` for more information.\n\nThis project was developed as part of our research related to:\n```bibtex\n@inproceedings{richter2022tssb,\n  title={TSSB-3M: Mining single statement bugs at massive scale},\n  author={Cedric Richter, Heike Wehrheim},\n  booktitle={MSR},\n  year={2022}\n}\n```\n\nWe thank the developer of [tree-sitter](https://tree-sitter.github.io/tree-sitter/) library. Without tree-sitter this project would not be possible. \n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/cedricrupb/code_tokenize/archive/refs/tags/v0.2.0.tar.gz",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/cedricrupb/code_tokenize",
    "keywords": "code,tokenization,tokenize,program,language processing",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "code-tokenize",
    "package_url": "https://pypi.org/project/code-tokenize/",
    "platform": null,
    "project_url": "https://pypi.org/project/code-tokenize/",
    "project_urls": {
      "Download": "https://github.com/cedricrupb/code_tokenize/archive/refs/tags/v0.2.0.tar.gz",
      "Homepage": "https://github.com/cedricrupb/code_tokenize"
    },
    "release_url": "https://pypi.org/project/code-tokenize/0.2.0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Fast program tokenization and structural analysis in Python",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14271760,
  "releases": {
    "0.0.1.post1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "807c85b9a8f0a5506dc020497aec0316a35fb9755b956026664fc8bf87399259",
          "md5": "10444dc6bc2545279150bf0320c601ec",
          "sha256": "2fc3cccce55026c10a09eae867f883f813d75508cf4bc61ddef75e03e64de9d6"
        },
        "downloads": -1,
        "filename": "code_tokenize-0.0.1.post1.tar.gz",
        "has_sig": false,
        "md5_digest": "10444dc6bc2545279150bf0320c601ec",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9754,
        "upload_time": "2021-11-01T17:20:28",
        "upload_time_iso_8601": "2021-11-01T17:20:28.863624Z",
        "url": "https://files.pythonhosted.org/packages/80/7c/85b9a8f0a5506dc020497aec0316a35fb9755b956026664fc8bf87399259/code_tokenize-0.0.1.post1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d2713487bf4440b4b9ad0af0974b7bca0c644442fd7c9cc2700c1ffa283444c",
          "md5": "a9fc32a52a05495440d1bbe0e32114b0",
          "sha256": "815c59b0da537f20d781e786956132d0c3b975ac62ef223a852354286719959e"
        },
        "downloads": -1,
        "filename": "code_tokenize-0.1.0-py3.8.egg",
        "has_sig": false,
        "md5_digest": "a9fc32a52a05495440d1bbe0e32114b0",
        "packagetype": "bdist_egg",
        "python_version": "0.1.0",
        "requires_python": null,
        "size": 25626,
        "upload_time": "2022-01-19T18:55:09",
        "upload_time_iso_8601": "2022-01-19T18:55:09.760139Z",
        "url": "https://files.pythonhosted.org/packages/5d/27/13487bf4440b4b9ad0af0974b7bca0c644442fd7c9cc2700c1ffa283444c/code_tokenize-0.1.0-py3.8.egg",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d94131136883de20f3730e1ffacd1e91c08de339efee31bb018a3a68b7bab32d",
          "md5": "f03cf8c7868c081902e24c20ef3cbfc0",
          "sha256": "308c03ad830939f3a883ac825e800a33b84c6ca9f43fe64f35bb081d8cb61075"
        },
        "downloads": -1,
        "filename": "code_tokenize-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f03cf8c7868c081902e24c20ef3cbfc0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 13018,
        "upload_time": "2022-01-19T18:55:11",
        "upload_time_iso_8601": "2022-01-19T18:55:11.287395Z",
        "url": "https://files.pythonhosted.org/packages/d9/41/31136883de20f3730e1ffacd1e91c08de339efee31bb018a3a68b7bab32d/code_tokenize-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3177269655dd34ac9e8946183fc385c5a08668e89cf195fbf825ee8d39ea3447",
          "md5": "24d57d09df2d8e0c44d786b60b4fc96f",
          "sha256": "5b8af8979f569a63bc7aabdb09b542bbf0b21ca50194935f321263b861f9899f"
        },
        "downloads": -1,
        "filename": "code_tokenize-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "24d57d09df2d8e0c44d786b60b4fc96f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 12649,
        "upload_time": "2022-06-28T11:17:39",
        "upload_time_iso_8601": "2022-06-28T11:17:39.029497Z",
        "url": "https://files.pythonhosted.org/packages/31/77/269655dd34ac9e8946183fc385c5a08668e89cf195fbf825ee8d39ea3447/code_tokenize-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3177269655dd34ac9e8946183fc385c5a08668e89cf195fbf825ee8d39ea3447",
        "md5": "24d57d09df2d8e0c44d786b60b4fc96f",
        "sha256": "5b8af8979f569a63bc7aabdb09b542bbf0b21ca50194935f321263b861f9899f"
      },
      "downloads": -1,
      "filename": "code_tokenize-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "24d57d09df2d8e0c44d786b60b4fc96f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 12649,
      "upload_time": "2022-06-28T11:17:39",
      "upload_time_iso_8601": "2022-06-28T11:17:39.029497Z",
      "url": "https://files.pythonhosted.org/packages/31/77/269655dd34ac9e8946183fc385c5a08668e89cf195fbf825ee8d39ea3447/code_tokenize-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}