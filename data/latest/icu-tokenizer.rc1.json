{
  "info": {
    "author": "Wang Ming Rui",
    "author_email": "mingruimingrui@hotmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering",
      "Topic :: Software Development",
      "Topic :: Software Development :: Libraries",
      "Topic :: Software Development :: Libraries :: Python Modules",
      "Topic :: Software Development :: Localization"
    ],
    "description": "**ICU-tokenizer** is a python package used to perform universal language\nnormalization and tokenization using the International Components for\nUnicode.\n\n- [Install](#install)\n- [Usage (Python)](#usage-python)\n  - [Sentence splitter](#sentence-splitter)\n  - [Normalizer](#normalizer)\n  - [Tokenizer](#tokenizer)\n\n## Install\n\nSee [./INSTALL.md](./INSTALL.md)\n\n## Usage (Python)\n\n### Sentence splitter\n\n```py\n# To split a paragraph into multiple sentences\n>>> from icu_tokenizer import SentSplitter\n>>> splitter = SentSplitter('zh')\n\n>>> paragraph = \"\"\"\nÁæéÂõΩÊúÄÈ´òÊ≥ïÈô¢ÔºàËã±ËØ≠ÔºöSupreme Court of the United StatesÔºâÔºå‰∏ÄËà¨ÊòØÊåáÁæéÂõΩËÅîÈÇ¶ÊúÄÈ´òÊ≥ïÈô¢ÔºåÊòØÁæéÂõΩÊúÄÈ´òÁ∫ßÂà´ÁöÑËÅîÈÇ¶Ê≥ïÈô¢Ôºå‰∏∫ÁæéÂõΩ‰∏âÊùÉÁªßÊÄªÁªü„ÄÅÂõΩ‰ºöÂêéÊúÄ‰∏∫ÈáçË¶ÅÁöÑ‰∏ÄÁéØ„ÄÇÊ†πÊçÆ1789Âπ¥„ÄäÁæéÂõΩÂÆ™Ê≥ïÁ¨¨‰∏âÊù°„ÄãÁöÑËßÑÂÆöÔºåÊúÄÈ´òÊ≥ïÈô¢ÂØπÊâÄÊúâËÅîÈÇ¶Ê≥ïÈô¢„ÄÅÂ∑ûÊ≥ïÈô¢ÂíåÊ∂âÂèäËÅîÈÇ¶Ê≥ïÂæãÈóÆÈ¢òÁöÑËØâËÆºÊ°à‰ª∂ÂÖ∑ÊúâÊúÄÁªàÔºàÂπ∂‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÊúâÊñüÈÖåÂÜ≥ÂÆöÊùÉÁöÑÔºâ‰∏äËØâÁÆ°ËæñÊùÉÔºå‰ª•ÂèäÂØπÂ∞èËåÉÂõ¥Ê°à‰ª∂ÁöÑÂÖ∑ÊúâÂàùÂÆ°ÁÆ°ËæñÊùÉ„ÄÇÂú®ÁæéÂõΩÁöÑÊ≥ïÂæãÂà∂Â∫¶‰∏≠ÔºåÊúÄÈ´òÊ≥ïÈô¢ÈÄöÂ∏∏ÊòØÂåÖÊã¨„ÄäÁæéÂõΩÂÆ™Ê≥ï„ÄãÂú®ÂÜÖÁöÑËÅîÈÇ¶Ê≥ïÂæãÁöÑÊúÄÁªàËß£ÈáäËÄÖÔºå‰ΩÜ‰ªÖÂú®ÂÖ∑ÊúâÁÆ°ËæñÊùÉÁöÑÊ°à‰ª∂ËåÉÂõ¥ÂÜÖ„ÄÇÊ≥ïÈô¢‰∏ç‰∫´ÊúâÂà§ÂÆöÊîøÊ≤ªÈóÆÈ¢òÁöÑÊùÉÂäõÔºõÊîøÊ≤ªÈóÆÈ¢òÁöÑÊâßÊ≥ïÊú∫ÂÖ≥ÊòØË°åÊîøÊú∫ÂÖ≥ÔºåËÄå‰∏çÊòØÊîøÂ∫úÁöÑÂè∏Ê≥ïÈÉ®Èó®„ÄÇ\n\"\"\"\n>>> splitter.split(paragraph)\n[\n    'ÁæéÂõΩÊúÄÈ´òÊ≥ïÈô¢ÔºàËã±ËØ≠ÔºöSupreme Court of the United StatesÔºâÔºå‰∏ÄËà¨ÊòØÊåáÁæéÂõΩËÅîÈÇ¶ÊúÄÈ´òÊ≥ïÈô¢ÔºåÊòØÁæéÂõΩÊúÄÈ´òÁ∫ßÂà´ÁöÑËÅîÈÇ¶Ê≥ïÈô¢Ôºå‰∏∫ÁæéÂõΩ‰∏âÊùÉÁªßÊÄªÁªü„ÄÅÂõΩ‰ºöÂêéÊúÄ‰∏∫ÈáçË¶ÅÁöÑ‰∏ÄÁéØ„ÄÇ',\n    'Ê†πÊçÆ1789Âπ¥„ÄäÁæéÂõΩÂÆ™Ê≥ïÁ¨¨‰∏âÊù°„ÄãÁöÑËßÑÂÆöÔºåÊúÄÈ´òÊ≥ïÈô¢ÂØπÊâÄÊúâËÅîÈÇ¶Ê≥ïÈô¢„ÄÅÂ∑ûÊ≥ïÈô¢ÂíåÊ∂âÂèäËÅîÈÇ¶Ê≥ïÂæãÈóÆÈ¢òÁöÑËØâËÆºÊ°à‰ª∂ÂÖ∑ÊúâÊúÄÁªàÔºàÂπ∂‰∏îÂú®ÂæàÂ§ßÁ®ãÂ∫¶‰∏äÊòØÊúâÊñüÈÖåÂÜ≥ÂÆöÊùÉÁöÑÔºâ‰∏äËØâÁÆ°ËæñÊùÉÔºå‰ª•ÂèäÂØπÂ∞èËåÉÂõ¥Ê°à‰ª∂ÁöÑÂÖ∑ÊúâÂàùÂÆ°ÁÆ°ËæñÊùÉ„ÄÇ',\n    'Âú®ÁæéÂõΩÁöÑÊ≥ïÂæãÂà∂Â∫¶‰∏≠ÔºåÊúÄÈ´òÊ≥ïÈô¢ÈÄöÂ∏∏ÊòØÂåÖÊã¨„ÄäÁæéÂõΩÂÆ™Ê≥ï„ÄãÂú®ÂÜÖÁöÑËÅîÈÇ¶Ê≥ïÂæãÁöÑÊúÄÁªàËß£ÈáäËÄÖÔºå‰ΩÜ‰ªÖÂú®ÂÖ∑ÊúâÁÆ°ËæñÊùÉÁöÑÊ°à‰ª∂ËåÉÂõ¥ÂÜÖ„ÄÇ',\n    'Ê≥ïÈô¢‰∏ç‰∫´ÊúâÂà§ÂÆöÊîøÊ≤ªÈóÆÈ¢òÁöÑÊùÉÂäõÔºõÊîøÊ≤ªÈóÆÈ¢òÁöÑÊâßÊ≥ïÊú∫ÂÖ≥ÊòØË°åÊîøÊú∫ÂÖ≥ÔºåËÄå‰∏çÊòØÊîøÂ∫úÁöÑÂè∏Ê≥ïÈÉ®Èó®„ÄÇ'\n]\n```\n\n### Normalizer\n\n```py\n# To normalize text\n>>> from icu_tokenizer import Normalizer\n>>> normalizer = Normalizer(lang='en', norm_puncts=True)\n\n>>> text = \"ùëªùíâùíÜ ùíëùíìùíêùíÖùíñùíÑùíïùíî ùíöùíêùíñ ùíêùíìùíÖùíÜùíìùíÜùíÖ ùíòùíäùíçùíç ùíÉùíÜ ùíîùíâùíäùíëùíëùíÜùíÖ ùíÖùíäùíìùíÜùíÑùíïùíçùíö ùíáùíìùíêùíé ùë≤ùíêùíìùíÜùíÇ.\"\n>>> normalizer.normalize(text)\n\"The products you ordered will be shipped directly from Korea.\"\n\n>>> text = \"„Äê„ÄëÔºàÔºâ\"\n>>> normalizer.normalize(text)\n\"[]()\"\n```\n\n### Tokenizer\n\n```py\n>>> from icu_tokenizer import Tokenizer\n>>> tokenizer = Tokenizer(lang='th')\n\n>>> text = \"‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÄ‡∏ä‡πà‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏≤‡∏à‡∏µ‡∏ô ‡πÅ‡∏•‡∏∞‡∏≠‡∏≠‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏≥‡∏ï‡πà‡∏≠‡∏Ñ‡∏≥\"\n>>> tokenizer.tokenize(text)\n['‡∏†‡∏≤‡∏©‡∏≤', '‡πÑ‡∏ó‡∏¢', '‡πÄ‡∏õ‡πá‡∏ô', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏ó‡∏µ‡πà', '‡∏°‡∏µ', '‡∏£‡∏∞‡∏î‡∏±‡∏ö', '‡πÄ‡∏™‡∏µ‡∏¢‡∏á', '‡∏Ç‡∏≠‡∏á', '‡∏Ñ‡∏≥', '‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô', '‡∏´‡∏£‡∏∑‡∏≠', '‡∏ß‡∏£‡∏£‡∏ì‡∏¢‡∏∏‡∏Å‡∏ï‡πå', '‡πÄ‡∏ä‡πà‡∏ô', '‡πÄ‡∏î‡∏µ‡∏¢‡∏ß', '‡∏Å‡∏±‡∏ö', '‡∏†‡∏≤‡∏©‡∏≤', '‡∏à‡∏µ‡∏ô', '‡πÅ‡∏•‡∏∞', '‡∏≠‡∏≠‡∏Å', '‡πÄ‡∏™‡∏µ‡∏¢‡∏á', '‡πÅ‡∏¢‡∏Å', '‡∏Ñ‡∏≥', '‡∏ï‡πà‡∏≠', '‡∏Ñ‡∏≥']\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/mingruimingrui/ICU-tokenizer",
    "keywords": "",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "icu-tokenizer",
    "package_url": "https://pypi.org/project/icu-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/icu-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/mingruimingrui/ICU-tokenizer"
    },
    "release_url": "https://pypi.org/project/icu-tokenizer/0.0.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "ICU based universal language tokenizer",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7507904,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2636d4fdf94a2b58135d4df4b9c12c9602e18a6e1f0267e60802c3bf455e530d",
          "md5": "ae6f7955f673c7313e67fe205fdeba39",
          "sha256": "0430f5191697904168769938fe3533ddd12e8afd5d84d43e852e9aee4a8b2447"
        },
        "downloads": -1,
        "filename": "icu_tokenizer-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ae6f7955f673c7313e67fe205fdeba39",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 11153,
        "upload_time": "2020-06-18T17:41:54",
        "upload_time_iso_8601": "2020-06-18T17:41:54.254901Z",
        "url": "https://files.pythonhosted.org/packages/26/36/d4fdf94a2b58135d4df4b9c12c9602e18a6e1f0267e60802c3bf455e530d/icu_tokenizer-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2636d4fdf94a2b58135d4df4b9c12c9602e18a6e1f0267e60802c3bf455e530d",
        "md5": "ae6f7955f673c7313e67fe205fdeba39",
        "sha256": "0430f5191697904168769938fe3533ddd12e8afd5d84d43e852e9aee4a8b2447"
      },
      "downloads": -1,
      "filename": "icu_tokenizer-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "ae6f7955f673c7313e67fe205fdeba39",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 11153,
      "upload_time": "2020-06-18T17:41:54",
      "upload_time_iso_8601": "2020-06-18T17:41:54.254901Z",
      "url": "https://files.pythonhosted.org/packages/26/36/d4fdf94a2b58135d4df4b9c12c9602e18a6e1f0267e60802c3bf455e530d/icu_tokenizer-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}