{
  "info": {
    "author": "Søren Vedel",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "![greattunes](https://raw.githubusercontent.com/svedel/greattunes/main/figs/greattunes.png)\n\n[![Tests](https://github.com/svedel/greattunes/actions/workflows/testing.yml/badge.svg)](https://github.com/svedel/greattunes/actions/workflows/testing.yml)\n[![Automatic publish to PyPI](https://github.com/svedel/greattunes/actions/workflows/prod.workflow.yml/badge.svg)](https://github.com/svedel/greattunes/actions/workflows/prod.workflow.yml)\n\n**Easy-to-use Bayesian optimization library** made available for either closed-loop or user-driven (manual) optimization of either \nknown or unknown objective functions. Drawing on `PyTorch` (`GPyTorch`), `BOTorch` and with proprietary extensions.\n\nA short primer on Bayesian optimization is provided in [this section](#a-primer-on-bayesian-optimization).  \n\n## Features\n\n* Handles **continuous**, **integer** and **categorical** covariates.\n* Optimization of either *known* or *unknown* functions. The allows for optimization of e.g. real-world experiments \n  without specifically requiring a model of the system be defined a priori.\n* Simple interface with focus on ease of use: only few lines of code required for full Bayesian optimization.\n* Erroneous observations of either covariates or response can be overwritten during optimization. \n* Well-documented code with detailed end-to-end examples of use, see [examples](#examples).\n* Optimization can start from scratch or repurpose existing data.\n\n\n### Design decisions\n\n* **Multivariate covariates, univariate system response:** It is assumed that input covariates (the independent \n  variables) can be either multivariate or univariate, while the system response (the dependent variable) is only \n  univariate.\n* **Optimizing across continuous, integer and categorical covariates:** Problems can depend on any of these types of \n  variables, in any combination. Special attention is given to implementation of integer and categorical variables\n  which are handled via the method of Garrido-Merchán and Hernandéz-Lobato (E.C. Garrido-Merchán and D. Hernandéz-Lobato, Neurocomputing, see [References](#references)).\n* **System-generated or manual input:** Observations of covariates and responses during optimization can be provided \n  both programmatically or manually via prompt input.\n* **Optimizes known and unknown response functions:** Both cases where the response function can be formulated \n  mathematically and cases where the response can only be measured (e.g. a real-life experiment) can be \n  optimized.  \n* **Observed covariates can vary from the proposed covariates:** The optimization routine at each iteration proposes \n  new covariate data points to investigate, but there is no requirement that this is also the observed data point.\n  At each iteration step, proposed covariates, observed covariates and observed response are 3 separate entities. That\n  that noisy or unexpected measurement points will be fully useful (no introduce any errors), even if they vary a lot \n  from the proposed covariate data points.\n* **Data stored in class instance:** Data for *proposed covariate data points*, *observed covariates* and *observed \n  responses* is stored in the instantiated class object.\n* **Data format and type validation:** Input data is validated at each iteration.\n* **Observations of covariates and response can be overridden during execution:** If an observation of either covariates \n  or response seems incorrect, the framework allows overriding the previous observation.  \n* **Consistency in number of covariates and observations:** It is assumed that there is consistency in the number of \n  observations of covariates and responses: at each step a new covariate data point is proposed, before observations\n  of covariates and response *for this iteration* are reported (specifically the number of proposed data points cannot \n  exceed the number of observed covariates by more than 1, and the number of observed covariates also cannot exceed the\n  number of observed responses by more than 1). If additional data is provided for either observed covariates or \n  observed response, this will override the last provided data. \n\n\n## Installation\n\n### Via `pip`\n\nThe library is available on [https://pypi.org/](https://pypi.org/), so to install simply run\n```python\npip install greattunes\n```\n\n### From source\n\nYou can also download the library source code and install it from there.\n\n#### First install `torch` dependencies\n\n**Installing `torch` dependencies is not always a requirement.** Unfortunately, in some cases `torch`-libraries have to \nbe installed outside normal bulk `pip install -r requirements.txt`. **First try to install directly via steps 1-3 in\n[Install library](#Install-library) below, and only install `torch` libraries manually if direct installation fails.**\n\nTo find the right installation command for `torch`, use [this link](https://pytorch.org/get-started/locally/)\nto determine the details and add as a separate command in the `github` actions yaml. As an example, the following is the \ninstall command on my local system (an `Ubuntu`-based system with `pip` and without `CUDA` access)\n```python\npip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n### Install library\n\nCurrently the code is not available on any repo servers except the private GitHub account. The best way to install the\ncode (after adding `torch` and `torchvision`) is follow this series of steps.\n\n1. Upgrade local versions of packaging libraries\n```python\npip install --upgrade setuptools wheel\n```\n2. Clone this repo\n3. Do local installation\n```python\npython -m pip install https://github.com/svedel/greattunes/\n```\n\nStep 3 will install by running `greattunes/setup.py` locally and installing. This step can also be broken into two, \nwhich might improve debugging\n```python\npython3 https://github.com/svedel/greattunes/ setup.py bdist_wheel\npython -m pip install https://github.com/svedel/greattunes/dist/greattunes-<version>-py3-none-any.whl\n```\nwhere `<version>` is the latest version in normal `python` format of `MAJOR.MINOR[.MICRO]` \n(check `/dist`-folder to see which one to pick).\n\n## Using the framework\n\nAll capabilities of the framework are described below.\n\nFor readers wanting to skip directly to working with the framework, a number of examples of how to use the framework end-to-end are included as Jupyter notebooks in [examples](#examples).  \n\n### Solving a problem\n\nSolving an optimization problem consists of two steps in this framework:\n1. Define the input variables (covariates), the surrogate model type and the acquisition function. Also define the\n   response function if this is known\n2. Optimize based on closed-loop or iterative interface\n\nHere's a simple illustration of how to do this for a known function `f`.\n\n#### Step 1: Define the problem\n\nThe critical things to define in this step are\n* The number of covariates. Upper and lower limits must be provided for each covariate to constraint the search space, \n  and initial guess for each to be provided as well. Works for both univariate and multivariate covariate structures.\n* The type of surrogate model. The model will be fitted at each step of the optimization.\n* The type of acquisitition function. This will also be fitted at each step of the optimization.\n\n```python\n# import library\nfrom greattunes import TuneSession\n\n# === Step 1: define the input ===\n\n# specify covariate. For each covariate of the model provide best guess of starting point together with upper and lower\n# limit \nx_start = 0.5  # initial guess\nx_min = 0  # lower limit\nx_max = 1  # upper limit\ncovars = [(x_start, x_min, x_max)]\n\n# initialize the class\ncls = TuneSession(covars=covars, model=\"SingleTaskGP\", acq_func=\"ExpectedImprovement\")\n```\n\n#### Step 2: Solve the problem\n\nIn order to optimize, we must first describe *which* function we want to do this for. The framework works both when this\nfunction can be formulated mathematically and when it can only be sampled (e.g. through examples) but cannot be\nformulated. For an illustrate of the latter see Example 2 under [examples](#examples).\n\nHere we will work with a known objective function to optimize\n```python\n# === Step 2: solve the problem ===\n\n# univariate function to optimize\nimport numpy as np\n\ndef f(x):\n    return -(6 * x - 2) ** 2 * np.sin(12 * x - 4)\n```\nBeware that the number of covariates (including their range) specified by `covars` under Step 1 must comply with the\nfunctional dependence of the objective function (`x` in the case above).\n\nWe are now ready to solve the problem. We will run for `max_iter`=20 iterations.\n```python\n# run the auto-method\ncls.auto(response_samp_func=f, max_iter=max_iter)\n```\n\nHad we worked with an objective function `f` which could not be formulated explicitly, the right entrypoint would have\nbeen to use the `.ask`-`.tell` methods instead of `.auto`.\n\n### Key attributes and methods\n\n#### User-facing attributes/methods: easily-accessible covariates and response\nThe following key attributes are stored for each optimization as part of the instantiated class. These primary\ndata structures for users are stored in `pandas` dataframes in pretty format. `current_best` and `best_predicted` are \nmethods which print their output to the prompt.\n\n| Attribute/method | Comments |\n| --------- | -------- |\n| `x_data` | All *observed* covariates with dimensions, one row per observation. If no names have been added to the covariates they will take the naems \"covar0\", \"covar1\", ... . Dimensions `num_observations` X `num_covariates`. |\n| `y_data` | All *observed* responses corresponding to the covariate points (rows) in `x_data`. Dimensions `num_observations` X 1. |\n| `best_response` | Best *observed* response value during optimization run, including current iteration. Dimensions `num_observations` X 1. |\n| `covars_best_response` | *Observed* covariates for best response value during optimization run, i.e. each row in `covars_best_response` generated the same row in `best_response`. Dimensions `num_observations` X `num_covariates`. |\n| `current_best()` | Returns the best *observed* response of the objective up to the current iteration. |\n| `best_predicted()` | Best *predicted* response from the surrogate model. Calculates for the mean model as well as for the lower confidence region (e.g. mean minus one standard deviation) of the full model. For both cases also returns the covariates resulting in the maximum. |\n\n#### Backend attributes\nIn the backend the framework makes use of different data structures based on the `tensor` structure from `torch` which \nalso handles one-hot encoding of categorical variables. The key backend attributes are listed in the table below.\n\n| Attribute | Comments |\n| --------- | -------- |\n| `train_X` | All *observed* covariates with dimensions `num_observations` X `num_covariates`. Backend equivalent to `x_data`. |\n| `proposed_X` | All *proposed* covariate datapoints to investigate, with dimensions `num_observations` X `num_covariates`. |\n| `train_Y` | All *observed* responses corresponding to the covariate points in `train_X`. Dimensions `num_observations` X 1. Backend equivalent to `y_data`. |\n| `best_response_value` | Best *observed* response value during optimization run, including current iteration. Dimensions `num_observations` X 1. Backend equivalent to `best_response`.|\n| `covars_best_response_value` | *Observed* covariates for best response value during optimization run, i.e. each row in `covars_best_response_value` generated the same row in `best_response_value`. Dimensions `num_observations` X `num_covariates`. Backend equivalent to `covars_best_response`. |    \n\n### Covariates: the free parameters which are adjusted by the framework during optimization\nThe user must detail which covariates the framework can adjust in order to optimize (maximize/minimize) the\nresponse. This is a mandatory part of class initialization and set via `covars` input variable; without any knowledge \nof the covariates, the framework cannot proceed to optimization. Here's an example for a problem with two covariates \n```python\ncovars = [(0.5, 0, 1), (2,1,4)]  # each tuple defines one covariate; the tuple entries are (initial guess, min, max)\n\n# initialize the class\ncls = TuneSession(covars=covars, ...)\n``` \nThis is also illustrated for a single-variable situation in [Step 1: Define the problem](#Step-1:-Define-the-problem) \nabove.\n\n#### Supported types: Handling continuous, integer and categorical covariates\nThe following three types of covariates are supported.\n* **Continuous**: Variables which can take any numerical value, i.e. can take values which include decimals. The data \n  type of a continuous variable will be among `float` types. Typical examples of continuous covariates will be weights \n  in a model and time thresholds (imagine a case where total runtime was a parameter). \n* **Integer**: Variables which can only take integer values; the data types of these variables will be among `int` types.\n  Special consideration must be taken during optimization because these variables only can update in discrete steps, \n  resulting in step changes of the response. Examples of integer covariates include number of layers in a neural network\n  and number of eggs in a recipe.\n* **Categorical**: Variables that can take different discrete values, which, contrary to integers do not even have any\n  internal relation in terms of size. An example is a variable which can take the values {`green`,`blue`,`red`} where\n  there clearly is no direct numerical relationship between the potential values; in contrast, a numerical relationship\n  does exist for integer variables (e.g. 5 is bigger than 2). In addition to the color example above, another example of \n  a categorical variable can be one which determines the make of a car (e.g. take values `volvo`, `lincoln`, `fiat` etc)\n\nThe framework follows the method of Garrido-Merchán and Hernandéz-Lobato (see [References](#References)) to integrate \nthe different types of covariates and bring them to a form that is consistent with using continuous Gaussian \nprocesses to drive the optimization. Briefly, the method relies on adding a transformation of variables in the \ncorrelation (kernel) function of the Gaussian processes with the following properties: integer covariates are rounded to\nnearest integer and categorical variables are one-hot encoded and only the one with highest numerical value is carried\nforward in each round by adjusting the value of its associated one-hot encoded variable to 1 and setting all other\none-hot encoded variables to 0.\n\n#### Two approaches to defining covariates in framework: working with named covariates and setting data types\nTwo ways are offered to provide covariate details to the framework: the simple way which assigns names to covariates \nand infers their data types from the provided data in `covars` (used so far), and an elaborate way which allows for \nnaming covariates and gives more control to specify data types. In either case, the information is given to the\nframework via the `covars` input variable.\n\n##### Simple approach: faster, but no control over covariate names and data types \nEach covariate is defined by a tuple, and the order of the tuples defines the order of the covariates. The same order\nmust be used later if covariates are manually reported via the `.tell`-method.\n\n###### Covariate data types\nCovariate data type is critical because it impacts how to handle the covariate during the optimization. In this simple\napproach, data types are inferred from the provided data in `covars` as indicated by the table below.\n\n| Data type | How report | Example | Comments |\n| --------- | ---------- | ------- | -------- |\n| Integer   | (`<initial_guess>`,`<parameter_minimum>`, `<parameter_maximum>`) | `(2, 0, 5)` | All tuple entries must be of data type `int` for covariate to be taken as integer |\n| Continuous | (`<initial_guess>`,`<parameter_minimum>`, `<parameter_maximum>`) | `(2.0, -1.2, 2.5)` | Only one tuple entry has to be a `float` for the covariate to be set to continuous |\n| Categorical | (`<initial_guess>`,`<option_1>`, `<option_2>`, ...) | `(volvo, fiat, aston martin, ford, toyota)` | Covariate is taken as categorical if any entry has data type `str`. There must be at least one other option than `<initial_guess>`, but otherwise no limit to the number of entries. | \n\nHere's an example of how to use the simple approach to define the `covars`-variable to communicate covariates of \ndifferent data types. This `covars` could be used to initialize a class instantiation\n```python\ncovars = [\n            (1, 0, 2),  # will be taken as INTEGER (type: int)\n            (1.0, 0.0, 2.0),  # will be taken as CONTINUOUS (type: float)\n            (1, 0, 2.0),  # will be taken as CONTINUOUS (type: float)\n            (\"red\", \"green\", \"blue\", \"yellow\"),  # will be taken as CATEGORICAL (type: str)\n            (\"volvo\", \"chevrolet\", \"ford\"),  # will be taken as CATEGORICAL (type: str)\n            (\"sunny\", \"cloudy\"),  # will be taken as CATEGORICAL (type: str)\n        ]\n```\n\n###### Covariate names \nCovariates are assigned names behind the scenes of the type `covar1`, `covar2` etc. with numbers added in the order in \nwhich the variable is processed from the `covars` list of tuples during class initialization (beware that this order may\nnot be preserved). Covariate names are visible as the column names in the `x_data` attribute. \n\n##### Elaborate approach: allows for specifying names and data types of covariates\nThis approach requires a bit more details to be provided, but also offers much more flexibility.\n\nIn this approach, all covariates are defined in a dictionary which is fed via the `covars` parameter, and each covariate is defined by their own dictionary \nnested within the outer dictionary specifying all covariates. An example, which will be elaborated further in the\nfollowing, is given below for 3 covariates to make this concrete\n```python\ncovars = {\n            'variable1':  # type: integer\n                {\n                    'guess': 1,\n                    'min': 0,\n                    'max': 2,\n                    'type': int,\n                },\n            'variable2':  # type: continuous (float)\n                {\n                    'guess': 12.2,\n                    'min': -3.4,\n                    'max': 30.8,\n                    'type': float,\n                },\n            'variable3':  # type: categorical (str)\n                {\n                    'guess': 'red',\n                    'options': {'red', 'blue', 'green'},\n                    'type': str,\n                }\n        }\n```\n\nEach nested dictionary gives the details of an individual covariate, and the name of these nested dictionaries are used\nto name the covariate. \n\n**Covariate names**: Anything that's permissable as a `python` string is a valid covariate name. These names are used\nthroughout the framework (will be inherited into `x_data`).\n\n**Specifying data type**: The variable `type` indicates the type of the covariate. The framework uses the following types\n* `int`: integer covariate\n* `float`: continuous covariate\n* `str`: categorical covariate\nBeware that the data type (and not a string) is used to define the type (i.e. use e.g. `str` not `'str'` to indicate a \n  categorical variable).\n\n**Required information for each covariate**: Requirements vary with the covariate data type. The following is required \nfor each type of covariate\n* Integer (`'type': int`): Required fields are `guess`, `min` and `max` (all single entries of type: `int`), as well as \n  `type` (must be `int` to specify categorical).\n* Continuous (`'type': float`): Required fields are `guess`, `min` and `max` (all single entries of types `int` or \n  `float`), as well as `type` (must be `float` to specify categorical).\n* Categorical (`'type': str`): Required fields are `guess` (a single entry, type: `str`), `options` (dictionary of `str`, one \n  for each option the covariate can take. Must also include the element in `guess`) and `type` (must be `str` to specify\n  categorical).\n\nThe example above shows 3 covariates but the framework can handle any number of covariates. Simply adjust the number of\nnested dictionaries to meet the need (and use appropriate naming and covariate specification for your application).\n\n#### Multivariate covariates\nMultivariate covariates are set via the (mandatory) `covars` parameter during class initialization. Each covariate is \ngiven as a 3-tuple of parameters (`<initial_guess>`,`<parameter_minimum>`, `<parameter_maximum>`) (the order matters!), with `covars` being a\nlist of these tuples. As an example, for a cases with 3 covariates, the `covars` parameter would be\n\n```python\ncovars = [(1, 0, 4.4), (5.2, 1.5, 7.0), (4, 2.2, 5.1)]\n```\n\nThe order of the covariates matters since framework does not work with named covariates. Hence, the parameter defined \nby the first tuple in `covars` will always have to be reported as the first covariate when iterating during \noptimization, the second covariate will be initialized by the second tuple in `covars` etc.  \n\nObservations of multivariate covariates are specified as columns in the `train_X` attribute (format: `torch.tensor`), \nwith observations added as rows. As an example, the initial guess for the three covariates defined by `covars` above\nwould be\n```python\ntrain_X = torch.tensor([[1, 5.2, 4]], dtype=torch.double)\n```\n\n### Initialization options\n\n#### Starting with historical data\n\nIf historical data for pairs of covariates and response is available for your system, this can be added during\ninitialization. In this case the optimization framework will have a better starting position and will likely converge\nmore quickly.\n\nHistorical data is added during class initialization. The number of observations (rows) of covariates and response must\nmatch. Historical training data is added during class instantiation via arguments `train_X=<>` and `train_Y=<>` as\nillustrated below for the following cases\n1. Multiple observations of multivariate system\n2. Single observation of univariate system\n3. Single observation of multivariate system\n\n```python\n# import\nimport torch\nfrom greattunes import TuneSession\n\n### ------ Case 1 - multiple observations (multivariate) ------ ###\n\n# set range of data\ncovars = [(1, 0, 4.4), (5.2, 1.5, 7.0), (4, 2.2, 5.1)]\n\n# define initial data\nX = torch.tensor([[1, 2, 3],[3, 4.4, 5]], dtype=torch.double)\nY = torch.tensor([[33],[37.8]], dtype=torch.double)\n\n# initialize class\ncls = TuneSession(covars=covars,train_X=X, train_Y=Y)\n\n### ------ Case 2 - single observation (univariate) ------ ###\n\n# set range of data\ncovars = [(1, 0, 4.4)]\n\n# define initial data\nX = torch.tensor([[1]], dtype=torch.double)\nY = torch.tensor([[33]], dtype=torch.double)\n\n# initialize class\ncls = TuneSession(covars=covars,train_X=X, train_Y=Y)\n\n### ------ Case 3 - single observation (multivariate) ------ ###\n\n# set range of data\ncovars = [(1, 0, 4.4), (5.2, 1.5, 7.0), (4, 2.2, 5.1)]\n\n# define initial data\nX = torch.tensor([[1, 2, 3]], dtype=torch.double)\nY = torch.tensor([[33]], dtype=torch.double)\n\n# initialize class\ncls = TuneSession(covars=covars,train_X=X, train_Y=Y)\n```\n\n#### Random initialization\n\nStarting from a few randomly sampled datapoints typically increases the convergence of the optimization because it \nmakes it less likely that the algorithm locks onto a local maximum without consideration for an unknown global one. \nFurthermore, in the absence of historical data, random sampling is the best option is to start.\n\nRandom initialization is enabled via the parameter `random_start` during initialization and can be applied both in case \nhistorical data has been added or not (default is `random_start = True`).\n\n```python\n\n# import\nimport torch\nfrom greattunes import TuneSession\n\n### ------ Case 1 - No historical data ------ ###\n\n# set range of data\ncovars = [(1, 0, 4.4), (5.2, 1.5, 7.0), (4, 2.2, 5.1)]\n\n# define initial data\nX = torch.tensor([[1, 2, 3],[3, 4.4, 5]], dtype=torch.double)\nY = torch.tensor([[33],[37.8]], dtype=torch.double)\n\n# initialize class\ncls = TuneSession(covars=covars, random_start=True)\n\n### ------ Case 2 - With historical data ------ ###\n\n# set range of data\ncovars = [(1, 0, 4.4), (5.2, 1.5, 7.0), (4, 2.2, 5.1)]\n\n# define initial data\nX = torch.tensor([[1, 2, 3],[3, 4.4, 5]], dtype=torch.double)\nY = torch.tensor([[33],[37.8]], dtype=torch.double)\n\n# initialize class\ncls = TuneSession(covars=covars,train_X=X, train_Y=Y, random_start=True)\n```\n\n##### Parameters for random start \n\n**Number of random datapoints:** The number of random datapoints to be sampled is set via the kwarg `num_initial_random` during initialization. This defaults to the closest integer to $\\sqrt{d}$ for a problem with $d$ covariates unless a value is provided.\n\n**Sampling method:** Two sampling methods are available: \n* `random`: Fully random sampling within the whole hypercube specified by `covars`.\n* `latin_hcs`: [Latin hypercube sampling](https://en.wikipedia.org/wiki/Latin_hypercube_sampling) within the hypercube specified by `covars`. \nThe sampling method is determined by the kwarg `random_sampling_method` during class initialization.\n\n#### Improved convergence: adding randomly sampled points during optimization\n\nJust like random initialization helps with convergence, best practice also prescribes adding randomly sampled points \nduring the optimization run.\n\nThis is easily done within this framework. The parameter `random_step_cadence` determines the cadence between randomly \nsampled datapoints (in between points sampled via Bayesian optimization). \n\n#### Kernels for Gaussian process surrogate model\n\nThe following kernels for Gaussian process surrogate model are implemented. Model type and listed parameters are \nprovided as input to class initialization, i.e. during initialization of `TuneSession`\n\n| Model name | Parameters | Comments |\n| ---------- | ---------- | -------- |\n| `\"SingleTaskGP\"` | N/A | A single-task exact kernel for Gaussian process regression. Follow this link for [more details](https://botorch.org/api/models.html#module-botorch.models.gp_regression). |\n| `\"FixedNoiseGP\"` | `train_Yvar` | A single-task exact kernel for Gaussian process regression assuming a fixed noise level. Follow this link for [more details](https://botorch.org/api/models.html#module-botorch.models.gp_regression). |\n| `\"HeteroskedasticSingleTaskGP\"` | `train_Yvar` | A single-task exact kernel for Gaussian process regression using a heteroskedastic noise model. Follow this link for [more details](https://botorch.org/api/models.html#module-botorch.models.gp_regression). |\n| `\"SimpleCustomMaternGP\"` | `nu` | A custom Matérn kernel with parameter `nu` (a float). For more details on Matérn kernels see [wiki page](https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function), and see the source code for the model in [`greattunes\\custom_models`](greattunes/custom_models). |\n\n#### Acquisition functions\n\nThese acquisition functions are currently available. Parameters (if any) are provided during initialization of the \n`TuneSession` class instance.\n\n| Acquisition function name | Parameter | Comments |\n| ------------------------- | --------- | -------- |\n| `\"ExpectedImprovement\"` | N/A | Expected improvement acquisition function. This is the default for `greattunes`. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic) or Section 2 [in this paper](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf). |\n| `\"NoisyExpectedImprovement\"` | `num_fantasies` (default: 20) | Expected improvement acquisition averaged over `num_fantasies` realizations of a single but noisy model. Requires that the Gaussian process model is of the type `FixedNoiseGP`. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic). |\n| `\"qExpectedImprovement\"` | `sampler` (default: `botorch.sampling.SobolQMCNormalSampler`) | Monte Carlo-based expected improvement function. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic). |\n| `\"qNoisyExpectedImprovement\"` | `sampler` (default: `botorch.sampling.SobolQMCNormalSampler`) | Monte Carlo-based noisy expected improvement function. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic). |\n| `\"PosteriorMean\"` | N/A | Posterior mean. Requires the surrogate (Gaussian process) model to have a mean property (all implemented models do). For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic). |\n| `\"ProbabilityOfImprovement\"` | N/A | Probability of improvement over the current best observed value, computed using the analytic formula under a Normal posterior distribution. Requires the outcome to be Gaussian. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic). |\n| `\"qProbabilityOfImprovement\"` | `sampler` (default: `botorch.sampling.SobolQMCNormalSampler`) | Monte Carlo based probability of improvement method. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic). |\n| `\"qSimpleRegret\"` | `sampler` (default: `botorch.sampling.SobolQMCNormalSampler`) | Monte Carlo method for simple regret. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic). |\n| `\"UpperConfidenceBound\"` | `beta` (default: 0.2) | Analytic upper confidence bound that comprises of the posterior mean plus an additional term: the posterior standard deviation weighted by a trade-off parameter, `beta`. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic). |\n| `\"qUpperConfidenceBound\"` | `beta` (default: 0.2), `sampler` (default: `botorch.sampling.SobolQMCNormalSampler`) | Monte carlo based Upper Confidence Bound method. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic) or [here](https://arxiv.org/abs/1712.00424). |\n| `\"qKnowledgeGradient\"` | `num_fantasies` (default: 20) | Computes the Knowledge Gradient using realizations (\"fantasies\") for the outer expectation and either the model posterior mean or MC-sampling for the inner expectation. For a fixed number of realizations (\"fantasies\"), optimizes in a “one-shot” fashion. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic) or [here](https://epubs.siam.org/doi/10.1137/070693424?mobileUi=0&). |\n| `\"qMaxValueEntropy\"` | N/A | Uses max-value entropy search. This acquisition function computes the mutual information of max values and a candidate point. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic) or [here](https://arxiv.org/abs/1703.01968). |\n| `\"qMultiFidelityMaxValueEntropy\"` | N/A | Multi-fidelity max-value entropy search. For more details [see here](https://botorch.org/api/acquisition.html#module-botorch.acquisition.analytic) or [here](http://proceedings.mlr.press/v119/takeno20a.html). |\n\n### Closed-loop: the `.auto` method\n\nClosed-loop optimization refers to situations where the function is known and therefore can iterate itself to \noptimality. These are addressed via the `.auto` method, which takes a function handle `response_samp_func` as well as a \nmaximum number of iterations `max_iter` as input parameters. See the [example above](#Step-2:-Solve-the-problem) as \nillustration of how to use the method.\n\n#### Stopping based on relative improvement in best observed response: `rel_tol` and `rel_tol_steps`\n\nThe optimization can be stopped before `max_iter` steps have been taken by specifying the limit on the relative \nimprovement in best observed response value (`best_response_value`). This is invoked by providing the parameter \n`rel_tol` to the `.auto` method. \n\n```python\n# some function to optimize\ndef f(x):\n  ...\n\n# parameters\nmax_iter = 100\nrel_tol = 1e-10\n\n# run the auto-method\ncls.auto(response_samp_func=f, max_iter=max_iter, rel_tol=rel_tol)\n```\n\nIn most cases the best results are found by requiring the `rel_tol` limit to be satisfied for multiple consecutive\niterations. This can be achieved by also providing the number of consecutive steps required `rel_tol_steps`. If \n`rel_tol_steps` is not provided, the limit on relative improvement only needs to be reached once for convergence.\n\n```python\n# some function to optimize\ndef f(x):\n  ...\n\n# parameters\nmax_iter = 100\nrel_tol = 1e-10\nrel_tol_steps = 5\n\n# run the auto-method\ncls.auto(response_samp_func=f, max_iter=max_iter, rel_tol=rel_tol, rel_tol_steps=rel_tol_steps)\n```\n\nBest practises on using `rel_tol` and `rel_tol_steps` are provided in Example 5 in [examples](examples).\n\n### Iterative: the `.ask` and `.tell` methods\n\nThe true value of Bayesian optimization is its ability to optimize problems which cannot be formulated mathematically.\nThe mathematical method can work as long as a response can be generated, and in fact makes no assumptions on the \nnature of the problem (except that a maximum is present). Thus, whether the response is generated as a measurement from\nan experiment, the feedback from users or the output of a defined mathematical function does not matter; all can be\noptimized via the framework.\n\nOptimization of unknown functions is handled by the methods `.ask` and `.tell`.\n* `.ask` provides a best guess of the next covariate data point to sample, given the history of previously sampled points for the \n  problem (that is, `.ask` provides the output of the acquisition function)\n* `.tell` is the method to report the observed covariate data point and the associated response\nOne call to `.ask` followed by a call to `.tell` performs one iteration of `.auto` from the point of view of the \nBayesian optimization; the difference is only in how to interface with it. Examples 2 and 3 in [examples](examples)\nshows how to use `.ask`-`.tell` to solve problems end-to-end.  \n\nTo solve a problem, apply these problems iteratively: in each iteration start by calling `.ask`, then use the proposed \nnew data point to sample the system response and provide both this value and the actually sampled covariate values (can \nbe different from proposed values) back via `.tell`.\n\n```python\n# in below, \"cc\" is an instantiated version of TuneSession class (identical initialization as when using .auto method) \nmax_iter = 20\n\nfor i in range(max_iter):\n  \n    # generate candidate\n    cls.ask()  # new candidate is last row in cc.proposed_X\n\n    # sample response (beware results must be formulated as torch tensors)\n    observed_covars = <from measurement or from cc.proposed_X>\n    observed_response = <from measurement or from specified objective function>\n\n    # report response\n    cls.tell(covars=observed_covars, response=observed_response)\n```\n\n#### Providing input via prompt\n\nObservations of covariates and response can be provided manually to `.tell`. To do so, simply call `.tell` without any \narguments at each iteration (all book keeping will be handled on backend)\n```python\n# in below, \"cc\" is an instantiated version of TuneSession class (identical initialization as when using .auto method) \nmax_iter = 20\n\nfor i in range(max_iter):\n  \n    # generate candidate\n    cls.ask()  # new candidate is last row in cc.proposed_X\n\n    # report response\n    cls.tell()\n```\n\nIn this case, the user will be prompted to provide input manually. There will be 3 attempts to provide covariates \n(another 3 for response), and the method will stop if not successful within these attempts. Provided input data will be\nvalidated for number of variables and data type as part of these cycles.\n\nAny of `covars` and `response` not provided as (named) parameter to `.tell` the user will be requested to provide via \nmanual input in prompt. It is thus possible to get e.g. covariates automatically but manually read off response values\nfrom an instrument.\n\n#### Overriding reported values of covariates or response \n\nObserved covariates and observed responses are sometimes off. To override the latest datapoint for either, simply \nprovide it again in the same iteration. This will automatically override the latest reported value \n```python\n# in below, \"cc\" is an instantiated version of TuneSession class (identical initialization as when using .auto method) \n# further assumes that at least on full iteration has been taken\n\n# define a response\ndef f(x):\n  ...\n\n# generate candidate\ncls.ask()  # new candidate is last row in cc.proposed_X\n\n# first result\nobserved_results = torch.tensor([[it.item() for it in cc.proposed_X[-1]]], dtype=torch.double)\nobserved_response = torch.tensor([[f(cc.proposed_X[-1]).item()]], dtype=torch.double)\n\n# report first response\ncls.tell(covars=observed_results, response=observed_response)\n\n# second result\nobserved_response_second = observed_response + 1\n\n# update response\ncls.tell(covars=observed_results, response=observed_response_second)\n```\n\n### Plotting and results presentation\n\nSome standard plots and standard methods for presenting the results have been included.\n\n#### Pre-defined plots\n* `plot_1d_latest()`: plots the latest retrained surrogate model (mean and variance), including all sampled data points.\n* `plot_convergence()`: plots the relative error between consecutive iterations.\n* `plot_best_objective()`: plots the best recorded value of the objective function as a function of the number of iterations.\n\n#### Result summaries\nThese methods print their results to the prompt.\n\n* `current_best()`: returns the largest *observed* response value (observed in either previous or current iteration). Also returns the corresponding values of the covariates.\n* `best_predicted()`: returns the largest response *predicted* from the surrogate model trained on all available data. Two values are returned: the largest mean and the largest of the lower confidence region, i.e. the largest value of the mean minus the first standard deviation (note: heteroskedacticity is allowed, so the standard deviation will vary across different covariates). Also returns the corresponding covariate values. Uses the [Nelder-Mead method](https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method), a multivariate equivalent to bisection, to find maximum value of surrogate model.\n\n## Contributing\nWe are happy if you would like to invest time in this project! Details are given in [CONTRIBUTING.md](CONTRIBUTING.md) \non how to get started.\n\n## Examples \n\nA number of examples showing how to use the framework in `jupyter` notebooks is available in the [examples](examples) \nfolder. This includes both closed-loop and iterative usages, as well as a few real-world examples (latter to come!)\n\n## References\n\n* [E.C. Garrido-Merchán and D. Hernandéz-Lobato: Dealing with categorical and integer-valued variables in Bayesian\nOptimization with Gaussian processes, Neurocomputing vol. 380, 7 March 2020, pp. 20-35](https://www.sciencedirect.com/science/article/abs/pii/S0925231219315619), [ArXiv preprint](https://arxiv.org/pdf/1805.03463.pdf)\n\n## A primer on Bayesian optimization\n\nA number of good resources are available for Bayesian optimization, so below follows only a short primer. Interested\nreaders are referred to the references listed below for more information.\n\n### Basics of Bayesian optimization\n\nBriefly and heuristically, Bayesian optimization works as follows. \n1. Define a *objective function*. The goal of the optimization is to maximize this function.\n2. Define a *surrogate model*. This is an approximation of the actual functional dependencies underlying the objective\nfunction. Because Bayesian optimization builds its own model there is no requirement that the objective function can be\n   written as a mathematical expression.\n3. Define an *acquisition function*. This function is applied to the surrogate model to identify the next datapoint to\nsample (as such, the acquisition function is actually a functional)\n4. Iterate:\n    * Use the acquisition function to identify the next data point to sample.\n    * Observe the response of the objective function at the proposed point  \n    * Based on all observed covariates and responses of the objective function, update the surrogate model via Bayes \n      theorem and repeat. \n\n### Surrogate models\n\nA typical choice of surrogate model class is the [Gaussian process](https://en.wikipedia.org/wiki/Gaussian_process), \nbut this is not a strict requirement. Examples exist in which both random forest and various types of neural networks \nhave been used. \n\nFormally, Bayesian optimization considers the function to be optimized as unknown and instead places a Bayesian prior\ndistribution over it. This is the initial surrogate model. Upon observing the response, the prior model is updated to \nobtain the posterior distribution of functions.\n\nThe benefit of Gaussian process models is their explicit modeling of the uncertainty and ease of obtaining the posterior.\n\n### Acquisition functions\n\nAcquisition functions (functionals) propose the best point to sample for a particular problem, given the\nprior distribution of the surrogate model.\n\nA number of different functions exist, with some typical ones provided in Peter Frazier's \n[Tutorial on Bayesian Optimization](https://arxiv.org/pdf/1807.02811.pdf). They typically balance exploration and \nexploitation in different ways.\n\n### References\nA list of Bayesian optimization references for later use\n* [Wikipedia entry on Bayesian optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)\n* [`BoTorch` introduction to Bayesian optimization](https://botorch.org/docs/overview)\n* [borealis.ai](https://www.borealisai.com/en/blog/tutorial-8-bayesian-optimization/)\n* [bayesopt, SigOpt page](http://bayesopt.github.io/)\n* [Towards Data Science](https://towardsdatascience.com/quick-start-to-gaussian-process-regression-36d838810319)\n* [Gaussian processes for dummies](https://katbailey.github.io/post/gaussian-processes-for-dummies/)\n* [Peter Frazier, Cornell, Bayesian Optimization expert](https://people.orie.cornell.edu/pfrazier/)\n* [Tutorial on Bayesian Optimization](https://arxiv.org/pdf/1807.02811.pdf)\n* [Bayesian Optimization, Martin Krasser's blog](http://krasserm.github.io/2018/03/21/bayesian-optimization/)\n* [Bayesian Optimization with inequality constraints](https://stat.columbia.edu/~cunningham/pdf/GardnerICML2014.pdf)\n* [Bayesian deep learning](https://towardsdatascience.com/bayesian-deep-learning-with-fastai-how-not-to-be-uncertain-about-your-uncertainty-6a99d1aa686e)\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/svedel/greattunes",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "greattunes",
    "package_url": "https://pypi.org/project/greattunes/",
    "platform": "",
    "project_url": "https://pypi.org/project/greattunes/",
    "project_urls": {
      "Homepage": "https://github.com/svedel/greattunes"
    },
    "release_url": "https://pypi.org/project/greattunes/0.0.7/",
    "requires_dist": [
      "botorch (==0.2.1)",
      "gpytorch (==1.1.1)",
      "matplotlib (==3.3.2)",
      "numpy (==1.19.1)",
      "pandas (==1.2.3)",
      "pytz (==2021.1)"
    ],
    "requires_python": ">=3.7",
    "summary": "Toolset for easy execution of Bayesian optimization for either step-by-step or closed-loop needs.",
    "version": "0.0.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11379788,
  "releases": {
    "0.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2fcc0db2af28ea4e304f17f22d5ebc3fc497d17049ee0146ea018db928481bee",
          "md5": "35d1a94e9da8e9eb19c8871c11758dd8",
          "sha256": "c16b292d23f088891df1d93460b3382ed19fae528b4997ebaabf26a215acb9a7"
        },
        "downloads": -1,
        "filename": "greattunes-0.0.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "35d1a94e9da8e9eb19c8871c11758dd8",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 53487,
        "upload_time": "2021-07-15T09:37:00",
        "upload_time_iso_8601": "2021-07-15T09:37:00.127162Z",
        "url": "https://files.pythonhosted.org/packages/2f/cc/0db2af28ea4e304f17f22d5ebc3fc497d17049ee0146ea018db928481bee/greattunes-0.0.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2dc4559a1af02c4b84203f877659a3e53f4940deec5aa5f4bb547cde8917077e",
          "md5": "96b578b1c4eb7d1cb0e947aa80eb6332",
          "sha256": "4f6a6bd4acd202a57d5225de796a48555c860cb6fd0c9c408b47296d1a57073a"
        },
        "downloads": -1,
        "filename": "greattunes-0.0.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "96b578b1c4eb7d1cb0e947aa80eb6332",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 60522,
        "upload_time": "2021-09-06T21:14:36",
        "upload_time_iso_8601": "2021-09-06T21:14:36.178865Z",
        "url": "https://files.pythonhosted.org/packages/2d/c4/559a1af02c4b84203f877659a3e53f4940deec5aa5f4bb547cde8917077e/greattunes-0.0.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2dc4559a1af02c4b84203f877659a3e53f4940deec5aa5f4bb547cde8917077e",
        "md5": "96b578b1c4eb7d1cb0e947aa80eb6332",
        "sha256": "4f6a6bd4acd202a57d5225de796a48555c860cb6fd0c9c408b47296d1a57073a"
      },
      "downloads": -1,
      "filename": "greattunes-0.0.7-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "96b578b1c4eb7d1cb0e947aa80eb6332",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 60522,
      "upload_time": "2021-09-06T21:14:36",
      "upload_time_iso_8601": "2021-09-06T21:14:36.178865Z",
      "url": "https://files.pythonhosted.org/packages/2d/c4/559a1af02c4b84203f877659a3e53f4940deec5aa5f4bb547cde8917077e/greattunes-0.0.7-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}