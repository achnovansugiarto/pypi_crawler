{
  "info": {
    "author": "Martin Molnar",
    "author_email": "martin.molnar07@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6"
    ],
    "description": "This is a package for a concept about a tokenization algorithm that is a neural network layer, training as part of a model trying to solve some NLP task, to make tokens that are best for the task. You can find more information on the [GitHub repository](https://github.com/martinm07/tokenization-layer).\n\n#\n\n<img src=\"https://imgur.com/gxxJtjz.png\">\n\n#\n\nThis package mainly consists of the `TokenizationLayer`, which is a `tf.keras` layer doing the described above. However it also contains initializers for the layer's parameter, a function for one-hot encoding a string as letters, and an Embedding layer, that doesn't have to be the first layer in the network unlike the official keras version.\n\nDocumentation for this package is [here](https://martin-github07.gitbook.io/tokenization-layer/).\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/martinm07/tokenization-layer",
    "keywords": "natural-language-processing,research,neural-networks,tokenization,tensorflow2",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "tokenization-layer",
    "package_url": "https://pypi.org/project/tokenization-layer/",
    "platform": "",
    "project_url": "https://pypi.org/project/tokenization-layer/",
    "project_urls": {
      "Bug Tracker": "https://github.com/martinm07/tokenization-layer/issues",
      "Homepage": "https://github.com/martinm07/tokenization-layer",
      "Package Documentation": "https://martin-github07.gitbook.io/tokenization-layer/",
      "Project Discussion": "https://github.com/martinm07/tokenization-layer/discussions"
    },
    "release_url": "https://pypi.org/project/tokenization-layer/0.0.2/",
    "requires_dist": [
      "tensorflow (>=2.0.0)",
      "numpy (>=1.15.0)",
      "pandas (>=1.0)",
      "colorama"
    ],
    "requires_python": ">=3.6",
    "summary": "An NLP tokenization algorithm that is a trainable layer for neural networks.",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11122923,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "72d6d26fb3035c2a39369774f5fb7b3b8a8e6a8fff9285f5d881ae423ac0af7f",
          "md5": "8e49cb5543cf812fb62eeb6170cc287e",
          "sha256": "228c8a774b248dec9790081edda7b001e263b74282ba00da0de8a295f7498c3c"
        },
        "downloads": -1,
        "filename": "tokenization_layer-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8e49cb5543cf812fb62eeb6170cc287e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 8425,
        "upload_time": "2021-08-07T21:29:30",
        "upload_time_iso_8601": "2021-08-07T21:29:30.951938Z",
        "url": "https://files.pythonhosted.org/packages/72/d6/d26fb3035c2a39369774f5fb7b3b8a8e6a8fff9285f5d881ae423ac0af7f/tokenization_layer-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "88e8c7dbd06d45f0e96d34a6bf7a57b665fbf9c2a532d15ddfb01a372ee41238",
          "md5": "88f1532f509bdffc3c68dc77398f3a99",
          "sha256": "23171f0bfaaf1fea82cd1ad2c4e6572eb206ebc2fd7feafb93261866d4e83cc9"
        },
        "downloads": -1,
        "filename": "tokenization-layer-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "88f1532f509bdffc3c68dc77398f3a99",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 8775,
        "upload_time": "2021-08-07T21:29:32",
        "upload_time_iso_8601": "2021-08-07T21:29:32.829008Z",
        "url": "https://files.pythonhosted.org/packages/88/e8/c7dbd06d45f0e96d34a6bf7a57b665fbf9c2a532d15ddfb01a372ee41238/tokenization-layer-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4b588765654f541831ba3887f6f2256e52b655b5eb5cd8039e8661c1d0db9eb1",
          "md5": "e0cb7da9cf67e4a617cf3b5c6e586d6e",
          "sha256": "e609e944804dc979d48da7d65039dbf02a0e32e481eb6ac01b194e625e377251"
        },
        "downloads": -1,
        "filename": "tokenization_layer-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e0cb7da9cf67e4a617cf3b5c6e586d6e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9234,
        "upload_time": "2021-08-08T14:48:01",
        "upload_time_iso_8601": "2021-08-08T14:48:01.895351Z",
        "url": "https://files.pythonhosted.org/packages/4b/58/8765654f541831ba3887f6f2256e52b655b5eb5cd8039e8661c1d0db9eb1/tokenization_layer-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "72e1e9ce38fcd73793398694c911b7db78814723f086ebc8525a12c43e9290fe",
          "md5": "1d5fd9b26a265e93e702b8d5b8fd3ad6",
          "sha256": "94e7ddd3caafadca2d1cd4c3bd91d6f31be4831f60f42dcc1b6aff55d99b47fd"
        },
        "downloads": -1,
        "filename": "tokenization-layer-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1d5fd9b26a265e93e702b8d5b8fd3ad6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 8107,
        "upload_time": "2021-08-08T14:48:03",
        "upload_time_iso_8601": "2021-08-08T14:48:03.420526Z",
        "url": "https://files.pythonhosted.org/packages/72/e1/e9ce38fcd73793398694c911b7db78814723f086ebc8525a12c43e9290fe/tokenization-layer-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "4b588765654f541831ba3887f6f2256e52b655b5eb5cd8039e8661c1d0db9eb1",
        "md5": "e0cb7da9cf67e4a617cf3b5c6e586d6e",
        "sha256": "e609e944804dc979d48da7d65039dbf02a0e32e481eb6ac01b194e625e377251"
      },
      "downloads": -1,
      "filename": "tokenization_layer-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "e0cb7da9cf67e4a617cf3b5c6e586d6e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 9234,
      "upload_time": "2021-08-08T14:48:01",
      "upload_time_iso_8601": "2021-08-08T14:48:01.895351Z",
      "url": "https://files.pythonhosted.org/packages/4b/58/8765654f541831ba3887f6f2256e52b655b5eb5cd8039e8661c1d0db9eb1/tokenization_layer-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "72e1e9ce38fcd73793398694c911b7db78814723f086ebc8525a12c43e9290fe",
        "md5": "1d5fd9b26a265e93e702b8d5b8fd3ad6",
        "sha256": "94e7ddd3caafadca2d1cd4c3bd91d6f31be4831f60f42dcc1b6aff55d99b47fd"
      },
      "downloads": -1,
      "filename": "tokenization-layer-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "1d5fd9b26a265e93e702b8d5b8fd3ad6",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 8107,
      "upload_time": "2021-08-08T14:48:03",
      "upload_time_iso_8601": "2021-08-08T14:48:03.420526Z",
      "url": "https://files.pythonhosted.org/packages/72/e1/e9ce38fcd73793398694c911b7db78814723f086ebc8525a12c43e9290fe/tokenization-layer-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}