{
  "info": {
    "author": "Mikołaj Małkiński",
    "author_email": "mikolaj.malkinski@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.6",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "![image](area_attention.png)\n\n# Area Attention\nPyTorch implementation of Area Attention [1].\nThis module allows to attend to areas of the memory, where each area contains a group of items that are either spatially or temporally adjacent.\nTensorFlow implementation can be found [here](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/area_attention.py).\n\n## Setup\n```bash\n$ pip install area_attention\n```\n\n## Usage\nSingle-head Area Attention:\n```python\nimport torch\n\nfrom area_attention import AreaAttention\n\narea_attention = AreaAttention(\n    key_query_size=32,\n    area_key_mode='max',\n    area_value_mode='mean',\n    max_area_height=2,\n    max_area_width=2,\n    memory_height=4,\n    memory_width=4,\n    dropout_rate=0.2,\n    top_k_areas=0\n)\nq = torch.rand(4, 8, 32)\nk = torch.rand(4, 16, 32)\nv = torch.rand(4, 16, 64)\nx = area_attention(q, k, v)\nx  # torch.Tensor with shape (8, 64)\n```\n\nMulti-head Area Attention:\n```python\nimport torch\n\nfrom area_attention import AreaAttention, MultiHeadAreaAttention\n\narea_attention = AreaAttention(\n    key_query_size=32,\n    area_key_mode='max',\n    area_value_mode='mean',\n    max_area_height=2,\n    max_area_width=2,\n    memory_height=4,\n    memory_width=4,\n    dropout_rate=0.2,\n    top_k_areas=0\n)\nmulti_head_area_attention = MultiHeadAreaAttention(\n    area_attention=area_attention,\n    num_heads=2,\n    key_query_size=32,\n    key_query_size_hidden=32,\n    value_size=64,\n    value_size_hidden=64\n)\nq = torch.rand(4, 8, 32)\nk = torch.rand(4, 16, 32)\nv = torch.rand(4, 16, 64)\nx = multi_head_area_attention(q, k, v)\nx  # torch.Tensor with shape (8, 64)\n```\n\n## Unit tests\n```bash\n$ python -m pytest tests\n```\n\n## Bibliography\n[1] Li, Yang, et al. \"Area attention.\" International Conference on Machine Learning. PMLR, 2019.\n\n## Citations\n```bibtex\n@inproceedings{li2019area,\n  title={Area attention},\n  author={Li, Yang and Kaiser, Lukasz and Bengio, Samy and Si, Si},\n  booktitle={International Conference on Machine Learning},\n  pages={3846--3855},\n  year={2019},\n  organization={PMLR}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/mikomel/area-attention",
    "keywords": "artificial intelligence,area attention",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "area-attention",
    "package_url": "https://pypi.org/project/area-attention/",
    "platform": "",
    "project_url": "https://pypi.org/project/area-attention/",
    "project_urls": {
      "Homepage": "https://github.com/mikomel/area-attention"
    },
    "release_url": "https://pypi.org/project/area-attention/0.1.0/",
    "requires_dist": [
      "torch (>=1.5)"
    ],
    "requires_python": ">=3.6",
    "summary": "PyTorch implementation of Area Attention",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8722380,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1c4095824f37771903f3a593ca3d844485d92fb8a18b93aa63c45fe88b61ef86",
          "md5": "8bb9d39e2dbb8caa44a2b4a7f16927c0",
          "sha256": "6ae5457f229c2f28f6053016f65ea6b718f0c2dbbfded71d6df833c1383517ce"
        },
        "downloads": -1,
        "filename": "area_attention-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8bb9d39e2dbb8caa44a2b4a7f16927c0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 6884,
        "upload_time": "2020-11-22T21:51:46",
        "upload_time_iso_8601": "2020-11-22T21:51:46.415671Z",
        "url": "https://files.pythonhosted.org/packages/1c/40/95824f37771903f3a593ca3d844485d92fb8a18b93aa63c45fe88b61ef86/area_attention-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "91d699c8bd3ba753a55adcfc234100adf0f4515a74414af5135f2c9c7ec93bf6",
          "md5": "8a3c8c59b0e6e21b5df9ea6581bc564b",
          "sha256": "af249a0ec673338efbe4eecdfa7d131cd46f55fecd4ba8930e259d60f01b7ccb"
        },
        "downloads": -1,
        "filename": "area_attention-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "8a3c8c59b0e6e21b5df9ea6581bc564b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 5989,
        "upload_time": "2020-11-22T21:51:47",
        "upload_time_iso_8601": "2020-11-22T21:51:47.764525Z",
        "url": "https://files.pythonhosted.org/packages/91/d6/99c8bd3ba753a55adcfc234100adf0f4515a74414af5135f2c9c7ec93bf6/area_attention-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1c4095824f37771903f3a593ca3d844485d92fb8a18b93aa63c45fe88b61ef86",
        "md5": "8bb9d39e2dbb8caa44a2b4a7f16927c0",
        "sha256": "6ae5457f229c2f28f6053016f65ea6b718f0c2dbbfded71d6df833c1383517ce"
      },
      "downloads": -1,
      "filename": "area_attention-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "8bb9d39e2dbb8caa44a2b4a7f16927c0",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 6884,
      "upload_time": "2020-11-22T21:51:46",
      "upload_time_iso_8601": "2020-11-22T21:51:46.415671Z",
      "url": "https://files.pythonhosted.org/packages/1c/40/95824f37771903f3a593ca3d844485d92fb8a18b93aa63c45fe88b61ef86/area_attention-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "91d699c8bd3ba753a55adcfc234100adf0f4515a74414af5135f2c9c7ec93bf6",
        "md5": "8a3c8c59b0e6e21b5df9ea6581bc564b",
        "sha256": "af249a0ec673338efbe4eecdfa7d131cd46f55fecd4ba8930e259d60f01b7ccb"
      },
      "downloads": -1,
      "filename": "area_attention-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "8a3c8c59b0e6e21b5df9ea6581bc564b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 5989,
      "upload_time": "2020-11-22T21:51:47",
      "upload_time_iso_8601": "2020-11-22T21:51:47.764525Z",
      "url": "https://files.pythonhosted.org/packages/91/d6/99c8bd3ba753a55adcfc234100adf0f4515a74414af5135f2c9c7ec93bf6/area_attention-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}