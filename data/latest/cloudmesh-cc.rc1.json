{
  "info": {
    "author": "Gregor von Laszewski",
    "author_email": "laszewski@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: Web Environment",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.10",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# Hybrid Multi-Cloud Analytics Services Framework\n\n**Cloudmesh Controlled Computing through Workflows**\n\nGregor von Laszewski (laszewski@gmail.com)$^*$,\nJacques Fleischer\n\n$^*$ Corresponding author\n\n\n## Background\n\nHigh-performance computing (HPC) is for decades a very important tool\nfor science. Scientific tasks can be leveraging the processing power\nof a supercomputer so they can run at previously unobtainable high\nspeeds or utilize specialized hardware for acceleration that otherwise\nare not available to the user. HPC can be used for analytic programs\nthat leverage machine learning applied to large data sets to, for\nexample, predict future values or to model current states. For such\nhigh-complexity projects, there are often multiple complex programs\nthat may be running repeatedly in either competition or\ncooperation. Leveraging for example computational GPUs leads to\nseveral times higher performance when applied to deep learning\nalgorithms. With such projects, program execution is submitted as a\njob to a typically remote HPC center, where time is billed as node\nhours. Such projects must have a service that lets the user manage and\nexecute without supervision. We have created a service that lets the\nuser run jobs across multiple platforms in a dynamic queue with\nvisualization and data storage.\n\nSee @fig:fastapi-service.\n\n![OpenAPI Description of the REST Interface to the Workflow](images/fastapi-service.png){#fig:fastapi-service width=50%}\n\n\n## Workflow Controlled Computing\n\nThis software was developed end enhancing Cloudmesh, a suite of\nsoftware to make using cloud and HPC resources easier. Specifically,\nwe have added a library called Cloudmesh Controlled Computing\n(cloudmesh-cc) that adds workflow features to control the execution of\njobs on remote compute resources.\n\nThe goal is to provide numerous methods of specifying the workflows on\na local computer and running them on remote services such as HPC and\ncloud computing resources. This includes REST services and command\nline tools. The software developed is freely available and can easily\nbe installed with standard Python tools so integration in the Python\necosystem using virtualenv's and Anaconda is simple.\n\n\n## Workflow Functionality\n\nA hybrid multi-cloud analytics service framework was created to manage\nheterogeneous and remote workflows, queues, and jobs. It was designed\nfor access through both the command line and REST services\nto simplify the coordination of tasks on remote computers. In\naddition, this service supports multiple operating systems like macOS,\nLinux, and Windows 10 and 11, on various hosts: the computer's\nlocalhost, remote computers, and the Linux-based virtual image WSL.\nJobs can be visualized and saved as a YAML and SVG data file. This\nworkflow was extensively tested for functionality and reproducibility.\n\n## Quickstart\n\nTo test the workflow program, prepare a cm directory in your home\ndirectory by executing the following commands in a terminal:\n\n```bash\ncd ~\nmkdir cm\ncd cm\npip install cloudmesh-installer -U\ncloudmesh-installer get cc\ngit clone https://github.com/cloudmesh/cloudmesh-cc\ncd cloudmesh-cc\npip install -e .\npip install -r requirements.txt\npytest -v -x --capture=no tests/test_080_workflow_clean.py\n```\n\nThis test runs three jobs within a singular workflow: the first job\nruns a local shell script, the second runs a local Python script, and\nthe third runs a local Jupyter notebook.\n\n## Application demonstration using MNIST\n\nThe Modified National Institute of Standards and Technology Database\nis a machine learning database based on image processing Various MNIST\nfiles involving different machine learning cases were modified and\ntested on various local and remote machines These cases include\nMultilayer Perceptron, LSTM (Long short-term memory), Auto-Encoder,\nConvolutional, and Recurrent Neural Networks, Distributed Training,\nand PyTorch training.\n\nSee @fig:workflow-uml.\n\n![Design for the workflow.](images/workflow-uml.png){#fig:workflow-uml}\n\n## Design\n\nThe hybrid multi-cloud analytics service framework was created to\nensure running jobs across many platforms. We designed a small and\nstreamlined number of abstractions so that jobs and workflows can be\nrepresented easily. The design is flexible and can be expanded as each\njob can contain arbitrary arguments. This made it possible to custom\ndesign for each target type a specific job type so that execution on\nlocal and remote compute resources including batch operating systems\ncan be achieved. The job types supported include: local job on Linux,\nmacOS, Windows 10, and Windows 11, jobs running in WSL on Windows\ncomputers, remote jobs using ssh, and batch jobs using Slurm.\n\n\n\nIn addition, we leveraged the existing Networkx Graph framework to\nallow dependencies between jobs. This greatly reduced the complexity\nof the implementation while being able to leverage graphical displays\nof the workflow, as well as using scheduling jobs with for example\ntopological sort available in Networkx. Custom schedulers can be\ndesigned easily based on the dependencies and job types managed\nthrough this straightforward interface. The status of the jobs is\nstored in a database that can be monitored during program\nexecution. The creation of the jobs is done on the fly, e.g. when the\njob is needed to be determined on the dependencies when all its\nparents are resolved. This is especially important as it allows\ndynamic workflow patterns to be implemented while results from\nprevious calculations can be used in later stages of the workflow.\n\nWe have developed a simple-to-use API for this so programs can be\nformulated using the API in python. However, we embedded this API also\nin a prototype REST service to showcase that integration into\nlanguage-independent frameworks is possible. The obvious functions to\nmanage workflows are supported including graph specification through\nconfiguration files, upload of workflows, export, adding jobs and\ndependencies, and visualizing the workflow during the execution. An\nimportant feature that we added is the monitoring of the jobs while\nusing progress reports through automated log file mining. This way\neach job reports the progress during the execution. This is especially\nof importance when we run very complex and long-running jobs.\n\n\nThe REST service was implemented in FastAPI to leverage a small but\nfast service that features a much smaller footprint for implementation\nand setup in contrast to other similar REST service frameworks using\npython.\n\nThis architectural component building this framework is depicted\n@fig:workflow-uml.  The code is available in this repository and\nmanual pages are provided on how to install it:\n[cloudmesh-cc](https://github.com/cloudmesh/cloudmesh-cc).\n\n## Summary\n\nThe main interaction with the workflow is through the command line.\nWith the framework, researchers and scientists should be able to\ncreate jobs on their own, place them in the workflow, and run them on\nvarious types of computers.\n\nIn addition, developers and users can utilize the built-in OpenAPI \ngraphical user interface to manage\nworkflows between jobs. They can be uploaded as YAML files or individually \nadded through the build-in debug framework.\n\nImprovements to this project will include code cleanup and manual development.\n\n## References\n\nA poster based on a pre-alpha version of this code is available as ppt\nand PDF file. However, that version is no longer valid and is\nsuperseded by much improved efforts. The code summarized in the\npre-alpha version was mainly used to teach a number of students Python\nand how to work in a team\n\n* [Poster Presentation (PPTX)](https://github.com/cloudmesh/cloudmesh-cc/raw/main/documents/analytics-service.pptx)\n* [Poster Presentation (PDF)](https://github.com/cloudmesh/cloudmesh-cc/raw/main/documents/analytics-service.pdf)\n\nPlease note also that the poster contains inaccurate statements and\ndescriptions and should not be used as a reference to this work.\n\n## Acknowledgments\n\nContinued work was in part funded by the NSF CyberTraining: CIC:\nCyberTraining for Students and Technologies from Generation Z with the\naward numbers 1829704 and 2200409.\nWe like to thank the following contributors for their help and evaluation in a \npre-alpha version of the code: Jackson Miskill, Alex Beck, Alison Lu.\nWe are excited that this effort contributed significantly to their\nincreased understanding of Python and how to develop in a team using\nthe Python ecosystem.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/cloudmesh/cloudmesh-cc",
    "keywords": "",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cloudmesh-cc",
    "package_url": "https://pypi.org/project/cloudmesh-cc/",
    "platform": null,
    "project_url": "https://pypi.org/project/cloudmesh-cc/",
    "project_urls": {
      "Homepage": "https://github.com/cloudmesh/cloudmesh-cc"
    },
    "release_url": "https://pypi.org/project/cloudmesh-cc/4.3.7/",
    "requires_dist": [
      "cloudmesh-cmd5",
      "cloudmesh-sys",
      "cloudmesh-inventory",
      "cloudmesh-configuration",
      "cloudmesh-progress",
      "psutil",
      "yamldb",
      "docker-compose",
      "networkx[default]",
      "pydot",
      "graphviz",
      "fastapi[all]",
      "httpx",
      "trio",
      "papermill",
      "pandas",
      "markdown",
      "sse-starlette",
      "ipython",
      "ipykernel"
    ],
    "requires_python": "",
    "summary": "A command called cc and foo for the cloudmesh shell",
    "version": "4.3.7",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15290983,
  "releases": {
    "4.3.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "63329227e96a1f5a3eac747fefcd057ce5954865f54a48b9760146a048d1b08d",
          "md5": "fb738ca5911f314db324127759cea41f",
          "sha256": "99c4f34a74cd8421b6b36e845da94bf6a03007e608a8d0ff88e2005ecc62cafa"
        },
        "downloads": -1,
        "filename": "cloudmesh_cc-4.3.7-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fb738ca5911f314db324127759cea41f",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 72432,
        "upload_time": "2022-10-03T15:07:33",
        "upload_time_iso_8601": "2022-10-03T15:07:33.842383Z",
        "url": "https://files.pythonhosted.org/packages/63/32/9227e96a1f5a3eac747fefcd057ce5954865f54a48b9760146a048d1b08d/cloudmesh_cc-4.3.7-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3cf00d66e1134cc7e978d93cb9b87d7813f6e92d22c87db1d1e2ee9d6f488a01",
          "md5": "ab437ee087e7efe083adb2bc0446e66e",
          "sha256": "3be24aca4917907bb21260ff6989050495b4b75f498d966f1bcc887770474361"
        },
        "downloads": -1,
        "filename": "cloudmesh-cc-4.3.7.tar.gz",
        "has_sig": false,
        "md5_digest": "ab437ee087e7efe083adb2bc0446e66e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 62305,
        "upload_time": "2022-10-03T15:07:35",
        "upload_time_iso_8601": "2022-10-03T15:07:35.492569Z",
        "url": "https://files.pythonhosted.org/packages/3c/f0/0d66e1134cc7e978d93cb9b87d7813f6e92d22c87db1d1e2ee9d6f488a01/cloudmesh-cc-4.3.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "63329227e96a1f5a3eac747fefcd057ce5954865f54a48b9760146a048d1b08d",
        "md5": "fb738ca5911f314db324127759cea41f",
        "sha256": "99c4f34a74cd8421b6b36e845da94bf6a03007e608a8d0ff88e2005ecc62cafa"
      },
      "downloads": -1,
      "filename": "cloudmesh_cc-4.3.7-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fb738ca5911f314db324127759cea41f",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 72432,
      "upload_time": "2022-10-03T15:07:33",
      "upload_time_iso_8601": "2022-10-03T15:07:33.842383Z",
      "url": "https://files.pythonhosted.org/packages/63/32/9227e96a1f5a3eac747fefcd057ce5954865f54a48b9760146a048d1b08d/cloudmesh_cc-4.3.7-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3cf00d66e1134cc7e978d93cb9b87d7813f6e92d22c87db1d1e2ee9d6f488a01",
        "md5": "ab437ee087e7efe083adb2bc0446e66e",
        "sha256": "3be24aca4917907bb21260ff6989050495b4b75f498d966f1bcc887770474361"
      },
      "downloads": -1,
      "filename": "cloudmesh-cc-4.3.7.tar.gz",
      "has_sig": false,
      "md5_digest": "ab437ee087e7efe083adb2bc0446e66e",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 62305,
      "upload_time": "2022-10-03T15:07:35",
      "upload_time_iso_8601": "2022-10-03T15:07:35.492569Z",
      "url": "https://files.pythonhosted.org/packages/3c/f0/0d66e1134cc7e978d93cb9b87d7813f6e92d22c87db1d1e2ee9d6f488a01/cloudmesh-cc-4.3.7.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}