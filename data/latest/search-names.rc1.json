{
  "info": {
    "author": "Suriyan Laohaprapanon, Gaurav Sood",
    "author_email": "suriyant@gmail.com, gsood07@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Topic :: Scientific/Engineering :: Information Analysis",
      "Topic :: Software Development :: Libraries :: Python Modules",
      "Topic :: Utilities"
    ],
    "description": "Search Names: Search a long list of names in a large text corpus\n-----------------------------------------------------------------\n\n.. image:: https://travis-ci.org/appeler/search_names.svg?branch=master\n    :target: https://travis-ci.org/appeler/search_names\n.. image:: https://ci.appveyor.com/api/projects/status/v3ao00u6uccnpi0n?svg=true\n    :target: https://ci.appveyor.com/project/soodoku/search-names\n.. image:: https://img.shields.io/pypi/v/search-names.svg\n    :target: https://pypi.python.org/pypi/search-names\n.. image:: https://readthedocs.org/projects/search-names/badge/?version=latest\n    :target: http://search-names.readthedocs.io/en/latest/?badge=latest\n.. image:: https://pepy.tech/badge/search-names\n    :target: https://pepy.tech/project/search-names\n\n|\n\nThere are seven kinds of challenges in searching a long list of names in a large text corpus:\n\n1. Names may not be in a standard format, e.g., the first name may not always be followed by the last name, etc.\n\n2. Searching FirstName LastName may not be enough. References to the person may take the form of Prefix LastName, etc. For instance, President Clinton.\n\n3. Names may be misspelled.\n\n4. Text may refer to people by their diminutive name (hypocorism), or by their middle name, or diminutive form of their middle name, etc. For instance, citations to Bill Clinton are liable to be much more common than William Clinton.\n\n5. Names on the list may overlap with names not on the list, especially names of other famous people. For instance, searching for `Maryland politician <https://en.wikipedia.org/wiki/Michael_A._Jackson_(politician)>`__ Michael Jackson may yield lots of false positives.\n\n6. Names on the list may match other names on the list (duplicates). \n\n7. Searching is computationally expensive. And searching for a long list over a large corpus is a double whammy.\n\nWe address each of the problems.\n\nThe Workflow\n~~~~~~~~~~~~\n\nBefore anything else, use `clean_names`_ to standardize the names on the list. The script appends separate columns for prefix, first\\_name, last\\_name, etc. Some human curation will likely still be needed. Do it before going further. After that, use `merge supplementary data`_ to append other potential prefixes, diminutive norms of the first name, and other names by which the person is known by to the output of `clean_names`_. Next,\n`preprocess`_ the search list. In particular, the script does three things:\n\n1. **Converts the data from wide to long**: The script creates a\n   separate row for each pattern we want to search for. For instance, if\n   we add 'Bill' as a diminutive name for William, and in the\n   configuration file, say, we want to only search for 'FirstName\n   LastName', the script creates a separate row for 'William Clinton'\n   and 'Bill Clinton', copying all other information across rows. And\n   appends a column called 'search\\_pattern.'\n\n2. **Deduplicates**: it removes any 'pattern', say 'Prefix LastName'\n   that is duplicated and hence cannot be easily disambiguated in\n   search. (This can be turned off.) and\n\n3. **Removes an ad hoc list of patterns**: For instance, patterns\n   matching famous people not on the list, e.g. we can remove 'Michael\n   Jackson' and it won't remove 'Congressman Jackson.'\n\nLastly, the `search`_ script searches patterns in the list in\na multi-threaded, parallelized way.\n\nInstallation\n~~~~~~~~~~~~\n\nWe strongly recommend installing ``search-names`` inside a Python virtual environment (see `venv documentation <https://docs.python.org/3/library/venv.html#creating-virtual-environments>`__)\n\n::\n\n    pip install search_names\n\n\n.. _`clean_names`: `Clean the name on the list`_\n\nClean the name on the list\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``clean_names``: The script is a modified version of `Clean Names <http://github.com/appeler/clean-names>`__.\n\nThe script takes a csv file with column 'Name' containing 'dirty names'--- names with all different formats: lastname firstname, firstname lastname, middlename lastname firstname etc. (see `sample input file <https://github.com/appeler/search_names/blob/master/examples/clean_names/sample_input.csv>`__\\ ) and produces a csv file that has all the columns of the original csv file and the following columns: 'uniqid', 'FirstName', 'MiddleInitial/Name', 'LastName', 'RomanNumeral', 'Title', 'Suffix' (see `sample output file <https://github.com/appeler/search_names/blob/master/examples/clean_names/sample_output.csv>`__\\ ).\n\nUsage\n^^^^^\n\n::\n\n   usage: clean_names [-h] [-o OUTFILE] [-c COLUMN] [-a] input\n\n   Clean name\n\n   positional arguments:\n   input                 Input file name\n\n   optional arguments:\n   -h, --help            show this help message and exit\n   -o OUTFILE, --out OUTFILE\n                           Output file in CSV (default: clean_names.csv)\n   -c COLUMN, --column COLUMN\n                           Column name file in CSV contains Name list (default:\n                           Name)\n   -a, --all             Export all names (not take duplicate names out)\n                           (default: False)\n\nExample\n^^^^^^^\n::\n\n    clean_names -a sample_input.csv\n\n\nMerge Supplementary Data\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe script takes output from `clean_names`_ (see `sample input file <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data/sample_in.csv>`__\\ ) and appends supplementary data (prefixes, nicknames) to the file (see `sample output file <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data/augmented_clean_names.csv>`__\\ ). In particular, the script merges two supplementary data files:\n\n   **Prefixes:** Generally the same set of prefixes will be used for a group of names. For instance, if you have a long list of politicians, state governors with no previous legislative experience will only have prefixes Governor, Mr., Mrs., Ms. etc., and not prefixes like Congressman or Congresswoman. We require a column in the input file that captures information about which 'prefix group' a particular name belongs to. We use that column to merge prefix data. The prefix file itself needs two columns: 1) A column to look up prefixes for groups of names depending on the value. The name of the column must be the same as the column name specified by the argument ``-p/--prefix`` (default is ``seat``\\ ), and 2) a column of prefixes (multiple prefixes separated by semi-colon). The default name of the prefix data file is ``prefixes.csv``. See `sample prefixes data file <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data/prefixes.csv>`__.\n\n   **Nicknames:**  Nicknames are merged using first names in the input data file. The nicknames file is a plain text file. Each line contains single or list of first names on left side of the '-' and one or multiple nicknames on the right hand side. List of first names and nicknames must be separated by comma. Default name of the nicknames data file is ``nick_names.txt``. See `sample nicknames file <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data/nick_names.txt>`__.\n\nUsage\n^^^^^\n\n::\n\n   usage: merge_supp [-h] [-o OUTFILE] [-n NAME] [-p PREFIX]\n                     [--prefix-file PREFIX_FILE] [--nick-name-file NICKNAME_FILE]\n                     input\n\n   Merge supplement data\n\n   positional arguments:\n   input                 Input file name\n\n   optional arguments:\n   -h, --help            show this help message and exit\n   -o OUTFILE, --out OUTFILE\n                           Output file in CSV (default:\n                           augmented_clean_names.csv)\n   -n NAME, --name NAME  Name of column use for nick name look up (default:\n                           FirstName)\n   -p PREFIX, --prefix PREFIX\n                           Name of column use for prefix look up (default: seat)\n   --prefix-file PREFIX_FILE\n                           CSV File contains list of prefixes (default:\n                           prefixes.csv)\n   --nick-name-file NICKNAME_FILE\n                           Text File contains list of nick names (default:\n                           nick_names.txt)\n\nExample\n^^^^^^^\n\n::\n\n   merge_supp sample_in.csv\n\nThe script takes `sample_in.csv <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data/sample_in.csv>`__\\ , `prefixes.csv <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data/prefixes.csv>`__\\ , and `nick_names.txt <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data/nick_names.txt>`__ and produces `augmented_clean_names.csv <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data/augmented_clean_names.csv>`__. The output file has two additional columns:\n\n\n* ``prefixes`` - List of prefixes (separated by semi-colon)\n* ``nick_names`` - List of nick names (separated by semi-colon)\n\n.. _`preprocess`: `Preprocess Search List`_\n\nPreprocess Search List\n~~~~~~~~~~~~~~~~~~~~~~~\n\nThe script takes the output from `merge supp. data <https://github.com/appeler/search_names/blob/master/examples/merge_supp_data>`__ (\\ `sample input file <https://github.com/appeler/search_names/blob/master/examples/preprocess/augmented_clean_names.csv>`__\\ ), list of patterns we want to search for, an ad hoc list of patterns we want to drop (\\ `sample drop patterns file <https://github.com/appeler/search_names/blob/master/examples/preprocess/drop_patterns.txt>`__\\ , and relative edit distance (based on the length of the pattern we are searching for) for approximate matching and does three things: a) creates a row for each pattern we want to search for (duplicating all the supplementary information), b) drops the ad hoc list of patterns we want to drop and c) de-duplicates based on edit distance and patterns we want to search for. See `sample output file <https://github.com/appeler/search_names/blob/master/examples/preprocess/deduped_augmented_clean_names.csv>`__.\n\nThe script also takes arguments that define the patterns to search for, name of the file containing patterns we want to drop, and edit distance.\n\n1) search\n\n   An argument ``--patterns`` contains patterns---combination of field names---we want to search for. For instance ``--patterns \"FirstName LastName\" \"NickName LastName\" \"Prefix LastName\"`` means that we want to search for combination of \"FirstName LastName\" \"NickName LastName\" and \"Prefix LastName\" respectively.\n\n2) drop\n\n   An argument ``--drop-patterns``  points to the text file containing list of people to be dropped. Usually, this file is an ad hoc list of patterns that we want removed. For instance, patterns matching famous people not on the list.\n\n3) editlength\n\n   An argument ``--editlength`` contains minimum name length for the specific string length. For instance, ``--editlength 10 15`` means that for patterns of length 10 or more, match within edit distance of 1 and patterns of length 15 or more, match within edit distance of 2.\n\n   If you want to disable `fuzzy` matching, just don't pass the argument ``--editlength``.\n\n\nUsage\n^^^^^\n\n::\n\n   usage: preprocess [-h] [-o OUTFILE] [-d DROP_PATTERNS_FILE]\n                     [-p PATTERNS [PATTERNS ...]]\n                     [-e EDITLENGTH [EDITLENGTH ...]]\n                     input\n\n   Preprocess Search List\n\n   positional arguments:\n   input                 Input file name\n\n   optional arguments:\n   -h, --help            show this help message and exit\n   -o OUTFILE, --out OUTFILE\n                           Output file in CSV (default:\n                           deduped_augmented_clean_names.csv)\n   -d DROP_PATTERNS_FILE, --drop-patterns DROP_PATTERNS_FILE\n                           File with Default Patterns (default:\n                           drop_patterns.txt)\n   -p PATTERNS [PATTERNS ...], --patterns PATTERNS [PATTERNS ...]\n                           List of Default Patterns (default: ['FirstName\n                           LastName', 'NickName LastName', 'Prefix LastName'])\n   -e EDITLENGTH [EDITLENGTH ...], --editlength EDITLENGTH [EDITLENGTH ...]\n                           List of Edit Lengths (default: [])\n\n\nExample\n^^^^^^^\n\n::\n\n   preprocess augmented_clean_names.csv\n\nBy default, the output will be saved as ``deduped_augmented_clean_names.csv``. The script adds a new column, ``search_name`` for unique search key.\n\n\nSearch\n~~~~~~~\n\nWe implement poor man's parallelization---scripts for splitting the corpus and merging the results back---along with multi-threading to quickly search through a large text corpus. We also provide the option to reduce the amount of searching by reducing the size of the text corpus by preprocessing it --- removing stop words etc.\n\nThere are three scripts --- to be run sequentially --- for the purpose:\n\n\nSplit text corpus into smaller chunks\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThis script splits large text corpora into multiple smaller chunks that can be run on multiple servers.\n\nUsage\n~~~~~\n\n::\n\n   usage: split_text_corpus [-h] [-o OUTFILE] [-s SIZE] input\n\n   Split large text corpus into smaller chunks\n\n   positional arguments:\n   input                 CSV input file name\n\n   optional arguments:\n   -h, --help            show this help message and exit\n   -o OUTFILE, --out OUTFILE\n                           Output file in CSV (default:\n                           chunk_{chunk_id:02d}/{basename}.csv)\n   -s SIZE, --size SIZE  Number of row in each chunk (default: 1000)\n\nExample\n~~~~~~~\n\n::\n\n   split_text_corpus -s 1000 text_corpus.csv\n\nThe script will split `text_corpus.csv <https://github.com/appeler/search_names/blob/master/examples/search/text_corpus.csv>`__ into multiple ``chunk_*`` directories.\n\nIn this case ``chunk_00, chunk_01, ... chunk_09`` directory will be created along with ``text_corpus.csv`` which will have 1000 rows in it.\n\nThe output location and file name convention can be specified by the ``-o / --out`` command line option. Actually, it is a Python format string where ``chunk_id`` will replace chunk number starting from 0, and ``basename`` is input file's name (without path and extension).\n\nSearch for names\n^^^^^^^^^^^^^^^^\n\nThis is the script to search names in the text corpus. The input file must contain at least two columns ``uniqid`` and ``text``.\n\nUsage\n~~~~~\n\n::\n\n   usage: search_names [-h] [-m MAX_NAME] [-p PROCESSES] [-o OUTFILE] [-t TEXT]\n                     [-i INPUT_COLS [INPUT_COLS ...]]\n                     [-c SEARCH_COLS [SEARCH_COLS ...]] [--overwritten]\n                     [-e EDITLENGTH [EDITLENGTH ...]] [-f NAMEFILE]\n                     [-u NAME_ID] [-s NAME_SEARCH] [-d] [--clean]\n                     input\n\n   Search names in text corpus\n\n   positional arguments:\n   input                 CSV input file name\n\n   optional arguments:\n   -h, --help            show this help message and exit\n   -m MAX_NAME, --max-name MAX_NAME\n                           Maximum name in search results (default: 20)\n   -p PROCESSES, --processes PROCESSES\n                           Number of processes to run (default: 4)\n   -o OUTFILE, --out OUTFILE\n                           Search results in CSV (default: search_results.csv)\n   -t TEXT, --text TEXT  Column name with text (default: text)\n   -i INPUT_COLS [INPUT_COLS ...], --input-cols INPUT_COLS [INPUT_COLS ...]\n                           List of column name from input file to include in the\n                           output (default: ['uniqid', 'text'])\n   -c SEARCH_COLS [SEARCH_COLS ...], --search-cols SEARCH_COLS [SEARCH_COLS ...]\n                           List of column name from search output (default:\n                           ['uniqid', 'n', 'match', 'start', 'end', 'count'])\n   --overwritten         Overwritten if output file is exists\n   -e EDITLENGTH [EDITLENGTH ...], --editlength EDITLENGTH [EDITLENGTH ...]\n                           List of Edit Lengths (default: [])\n   -f NAMEFILE, --file NAMEFILE\n                           CSV file contains unique ID and Name want to search\n                           for (default: deduped_augmented_clean_names.csv)\n   -u NAME_ID, --uniqid NAME_ID\n                           Column of unique ID in name want to search for\n                           (default: uniqid)\n   -s NAME_SEARCH, --search NAME_SEARCH\n                           Colunm of name want to search for (default:\n                           search_name)\n   -d, --debug           Enable debug message\n   --clean               Clean text column before search\n\nArguments\n~~~~~~~~~\n\n- ``--search-cols`` that lists the columns from search file to be included in the output\n- ``--input-cols`` that lists columns from the file containing the text data to be included in the output.\n- ``--file`` which you can use to specify a CSV file where ``id`` and ``search`` refer to uniqid and keywords to be searched in that file respectively. In this case ``id`` and ``search`` are set to ``uniqid`` and ``search_name``\\ , the de-duped output generated by `preprocess`_.\n- ``--editlength`` specifies the list of minimum string length for that edit distance. For instance ``--editlength 10 15`` first value (``10``) means edit distance of 1 is allowed if string longer than 10 characters and the 2nd value (``15``) means that edit distance of 2 is allowed if the string is longer than 15 characters. We must use the same ``editlength`` as setting used in `preprocess`_ to avoid getting ambiguous search results. Once again, if you want to disable `fuzzy` matching, just omitted ``editlength``.\n- ``--text`` specifies the name of the column that contains the text data to be searched.\n- ``-m / --max-name`` is used to limit maximum search results.\n- ``--overwritten`` is used to overwrite the output file if it exists; it is disabled by default.\n- ``--clean`` option is provided to clean the ``text`` column (remove stop words, special characters etc.) before search.\n\nExample\n~~~~~~~\n\n::\n\n   search_names text_corpus.csv\n\nBy default, the script forks 4 processes (specify by ``-p / --processes``\\ ) and searches for the names specified by ``--file``, ``--search``.\n\nThe output file (specify by ``-o / --out``\\ ) will contains all columns from the input file (except ``text`` column will be replaced by cleaned text if ``--clean`` is specify) along with the search result columns that are:\n\n::\n\n   `nameX.uniqid` - uniqid number from name file\n   `nameX.n` - occurrences of name found\n   `nameX.match` - name found (separated by semi-colon `;` if multiple matches)\n   `nameX.start` - start index of name found\n   `nameX.end` - end index of name found\n   `count` - total occurrences of name found\n\n\nwhere ``X`` is result numbering start from 1 to maximum search results\n\nPlease note that row sequence in the output file will not be same as the input file as the script gets results from multi-threaded searching.\n\nMerge Search Results\n^^^^^^^^^^^^^^^^^^^^\n\nMerge search results back from multiple files to a single file.\n\nUsage\n~~~~~\n\n::\n\n   usage: merge_results [-h] [-o OUTFILE] [inputs [inputs ...]]\n\n   Merge search results from multiple chunks\n\n   positional arguments:\n   inputs                CSV input file(s) name\n\n   optional arguments:\n   -h, --help            show this help message and exit\n   -o OUTFILE, --out OUTFILE\n                           Output file in CSV (default:\n                           merged_search_results.csv)\n\n\nExample\n~~~~~~~\n\n::\n\n   merge_results chunk_00/search_results.csv chunk_01/search_results.csv chunk_02/search_results.csv\n\nAbove script will merge 3 search results into a single output file. The default is ``merged_results.csv``\n\nDocumentation\n-------------\n\nFor more information, please see `project documentation <http://search-names.readthedocs.io/en/latest/>`__.\n\nAuthors\n-------\n\nSuriyan Laohaprapanon and Gaurav Sood\n\nContributor Code of Conduct\n---------------------------\n\nThe project welcomes contributions from everyone! In fact, it depends on\nit. To maintain this welcoming atmosphere, and to collaborate in a fun\nand productive way, we expect contributors to the project to abide by\nthe `Contributor Code of\nConduct <https://www.contributor-covenant.org/version/2/0/code_of_conduct/>`__.\n\nLicense\n-------\n\nThe package is released under the `MIT\nLicense <https://opensource.org/licenses/MIT>`__.\n\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/appeler/search_names",
    "keywords": "search names",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "search-names",
    "package_url": "https://pypi.org/project/search-names/",
    "platform": "",
    "project_url": "https://pypi.org/project/search-names/",
    "project_urls": {
      "Homepage": "https://github.com/appeler/search_names"
    },
    "release_url": "https://pypi.org/project/search-names/0.2.0/",
    "requires_dist": [
      "nameparser",
      "python-Levenshtein",
      "regex",
      "nltk",
      "six",
      "check-manifest ; extra == 'dev'",
      "coverage ; extra == 'test'"
    ],
    "requires_python": "",
    "summary": "Search a long list of names (patterns) in a large text corpus systematically and quickly",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13777833,
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c9cad7f4d5022df0142ea30f292bbf945161a7bf8bd4c88630a27698a72d793c",
          "md5": "0930c8a76f68b6bdbbabfb7ddd67aec1",
          "sha256": "047c0d59c6cb626815dc1b43a61a559e40487405cc4a499dc92f63b741f6e2f2"
        },
        "downloads": -1,
        "filename": "search_names-0.2.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0930c8a76f68b6bdbbabfb7ddd67aec1",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 26870,
        "upload_time": "2021-02-07T00:50:19",
        "upload_time_iso_8601": "2021-02-07T00:50:19.129614Z",
        "url": "https://files.pythonhosted.org/packages/c9/ca/d7f4d5022df0142ea30f292bbf945161a7bf8bd4c88630a27698a72d793c/search_names-0.2.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "601a9c00c538e51753d85a11cc5f60cf892e1368b3b1dffa4bc09832cb6029e2",
          "md5": "33f2698d4972f680eeb0226a9b782744",
          "sha256": "e880664d86518b8eb807c56c0569d3c50b0428d332f32dc987beb55665f82333"
        },
        "downloads": -1,
        "filename": "search_names-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "33f2698d4972f680eeb0226a9b782744",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 28189,
        "upload_time": "2021-02-07T00:50:20",
        "upload_time_iso_8601": "2021-02-07T00:50:20.403334Z",
        "url": "https://files.pythonhosted.org/packages/60/1a/9c00c538e51753d85a11cc5f60cf892e1368b3b1dffa4bc09832cb6029e2/search_names-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "c9cad7f4d5022df0142ea30f292bbf945161a7bf8bd4c88630a27698a72d793c",
        "md5": "0930c8a76f68b6bdbbabfb7ddd67aec1",
        "sha256": "047c0d59c6cb626815dc1b43a61a559e40487405cc4a499dc92f63b741f6e2f2"
      },
      "downloads": -1,
      "filename": "search_names-0.2.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0930c8a76f68b6bdbbabfb7ddd67aec1",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 26870,
      "upload_time": "2021-02-07T00:50:19",
      "upload_time_iso_8601": "2021-02-07T00:50:19.129614Z",
      "url": "https://files.pythonhosted.org/packages/c9/ca/d7f4d5022df0142ea30f292bbf945161a7bf8bd4c88630a27698a72d793c/search_names-0.2.0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "601a9c00c538e51753d85a11cc5f60cf892e1368b3b1dffa4bc09832cb6029e2",
        "md5": "33f2698d4972f680eeb0226a9b782744",
        "sha256": "e880664d86518b8eb807c56c0569d3c50b0428d332f32dc987beb55665f82333"
      },
      "downloads": -1,
      "filename": "search_names-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "33f2698d4972f680eeb0226a9b782744",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 28189,
      "upload_time": "2021-02-07T00:50:20",
      "upload_time_iso_8601": "2021-02-07T00:50:20.403334Z",
      "url": "https://files.pythonhosted.org/packages/60/1a/9c00c538e51753d85a11cc5f60cf892e1368b3b1dffa4bc09832cb6029e2/search_names-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}