{
  "info": {
    "author": "Jim Clauwaert",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "<div align=\"center\">\n<h1>transcript_transformer</h1> \n\nDeep learning utility functions for processing and annotating transcript genome data.\n\n[![PyPi Version](https://img.shields.io/pypi/v/transcript-transformer.svg)](https://pypi.python.org/pypi/transcript-transformer/)\n[![GitHub license](https://img.shields.io/github/license/jdcla/transcript_transformer)](https://github.com/jdcla/transcript_transformer/blob/main/LICENSE.md)\n[![GitHub issues](https://img.shields.io/github/issues/jdcla/transcript_transformer)](https://github.com/jdcla/transcript_transformer/issues)\n[![GitHub stars](https://img.shields.io/github/stars/jdcla/transcript_transformer)](https://github.com/jdcla/transcript_transformer/stargazers)\n</div>\n\n\n`transcript_transformer`  is constructed in concordance with the creation of TIS Transformer, ([paper](https://www.biorxiv.org/content/10.1101/2021.11.18.468957v2), [repository](https://github.com/jdcla/TIS_transformer)) and RIBO-former (to be released). `transcript_transformer` makes use of the [Performer](https://arxiv.org/abs/2009.14794) architecture to allow for the annotations and processing of transcripts at single nucleotide resolution. The package makes use of `h5py` for data loading and `pytorch-lightning` as a high-level interface for training and evaluation for deep learning models. `transcript_transformer` is designed to allow a high degree of modularity, but has not been tested for every combination of arguments, and can therefore return errors.\n\n## 🔗 Installation\n`pytorch` needs to be separately [installed by the user](https://pytorch.org/get-started/locally/). \n\nNext, the package can be installed running \n```bash\npip install transcript-transformer\n```\n\n## 📖 User guide <a name=\"code\"></a>\n\nThe library features a tool that can be called directly by the command `transcript_transformer`, featuring three main functions: `pretrain`, `train` and `predict`. Data loading is achieved using the `h5` format, handled by the `h5py` package.\n\n### Data loading\n Information is separated by transcript and information type. Information belonging to a single transcript is mapped according to the index they populate within each `h5py.dataset`, used to store different types of information. [Variable length arrays](https://docs.h5py.org/en/stable/special.html#arbitrary-vlen-data) are used to store the sequences and annotations of all transcripts under a single data set. \nSequences are stored using integer arrays following: `{A:0, T:1, C:2, G:3, N:4}`\nAn example `data.h5` has the following structure:\n\n\n```\ndata.h5                                     (h5py.file)\n    transcript                              (h5py.group)\n    ├── tis                                 (h5py.dataset, dtype=vlen(int))\n    ├── contig                              (h5py.dataset, dtype=str)\n    ├── id                                  (h5py.dataset, dtype=str)\n    ├── seq                                 (h5py.dataset, dtype=vlen(int))\n    ├── ribo                                (h5py.group)\n    │   ├── SRR0000001                      (h5py.group)\n    │   │   ├── 5                           (h5py.group)\n    │   │   │   ├── data                    (h5py.dataset, dtype=vlen(int))\n    │   │   │   ├── indices                 (h5py.dataset, dtype=vlen(int))\n    │   │   │   ├── indptr                  (h5py.dataset, dtype=vlen(int))\n    │   │   │   ├── shape                   (h5py.dataset, dtype=vlen(int))\n    │   ├── ...\n    │   ....\n    \n```\n\nRibosome profiling data is saved by reads mapped to each transcript position. Mapped reads are furthermore separated by their read lengths. As ribosome profiling data is often sparse, we made use of `scipy.sparse` to save data within the `h5` format. This allows us to save space and store matrix objects as separate arrays. Saving and loading of the data is achieved using the [h5max](https://github.com/jdcla/h5max) package.\n\n<div align=\"center\">\n<img src=\"https://github.com/jdcla/h5max/raw/main/h5max.png\" width=\"600\">\n</div>\n\nDictionary `.json` files are used to specify the application of data to `transcript_transformer`. When no sequence information or ribosome profiling data is used, either entry `seq` or `ribo` is set to `false`. For each ribosome profiling dataset, custom [P-site offsets](https://plastid.readthedocs.io/en/latest/glossary.html#term-P-site-offset) can be set per read length. \n\n```json\n{\n  \"h5_path\":\"data.h5\",\n  \"exp_path\":\"transcript\",\n  \"y_path\":\"tis\",\n  \"chrom_path\":\"contig\",\n  \"id_path\":\"id\",\n  \"seq\":\"seq\",\n  \"ribo\": {\n    \"SRR000001/5\": {\n      \"25\": 7,\n      \"26\": 7,\n      \"27\": 8,\n      \"28\": 10,\n      \"29\": 10,\n      \"30\": 11,\n      \"31\": 11,\n      \"32\": 7,\n      \"33\": 7,\n      \"34\": 9,\n      \"35\": 9,\n    }\n  }\n}\n```\n\n\n\n### pretrain\n\n\nConform with transformers trained for natural language processing objectives, models can first be trained following a self-supervised learning objective. Using a masked language modelling approach, models are tasked to predict the classes of the masked input tokens. As such, a model is trained the 'semantics' of transcript sequences. The approach is similar to the one described by [Zaheer et al. ](https://arxiv.org/abs/2007.14062). This approach has not been using ribosome profiling data.\n\n\n```\ntranscript_transformer pretrain -h\n\npositional arguments:\n    input_data            path to json file specifying input data (see README.md)\n    val                   list of chromosomes used for the validation set\n    test                  list of chromosomes used for the test set\n  --mask_frac float       fraction of input positions that are masked (default: 0.85)\n  --rand_frac float       fraction of masked inputs that are randomized (default: 0.1)\n```\n\nExample\n\n```\ntranscript_transformer pretrain input_data.json --val 1 13 --test 2 14 --max_epochs 70 --gpu 1 \n```\n\n</details>\n\n### train\nThe package supports training the models architectures listed under `transcript_transformer/models.py`. The function expects a `.json` file containing the input data info (see [data loading](https://github.com/jdcla/transcript_transformer#data-loading)). It is possible to start training upon pre-trained models using the `--transfer_checkpoint` functionality.\n\n\n```\ntranscript_transformer train -h\n    \npositional arguments:\n  dict_path               dictionary (json) path containing input data file info\n\noptions:\n  --val                   contigs in data_path folder used for validation (default: None)\n  --test                  contigs in data_path folder used for testing (default: None)\n  --ribo_offset           offset mapped ribosome reads by read length (default: False)\n  --name                  name of the model (default: )\n  --log_dir               log dir (default: lightning_logs)\n  --transfer_checkpoint   Path to checkpoint pretrained model (default: None)\n```\n\nExample\n\n```\ntranscript_transformer train input_data.json --val 1 13 --test 2 14 --max_epochs 70 --transfer_checkpoint lightning_logs/mlm_model/version_0/ --name experiment_1 --gpu 1 \n```\n\n### predict\n\nThe predict function returns probabilities for all nucleotide positions on the transcript and can be saved using the `.npy` or `.h5` format. In addition to reading from `.h5` files, the function supports the use of a single RNA sequence as input or a path to a `.fa` file. Note that `.fa` and `.npy` formats are only supported for models that only apply transcript nucleotide information.\n\n```\ntranscript_transformer predict -h\n\npositional arguments:\n  input_data            path to JSON dict (h5) or fasta file, or RNA sequence\n  input_type            type of input\n  checkpoint            path to checkpoint of trained model\n\noptions:\n  -h, --help            show this help message and exit\n  --test                contigs to predict on (h5 input format only) (default: None)\n  --ribo_offset         offset mapped ribosome reads by read length (default: False)\n  --output_type         file type of output predictions (default: npy)\n  --save_path           save file path (default: results)\n```\n\n\nExample\n\n```\ntranscript_transformer predict AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACGGT RNA --output_type npy models/example_model.ckpt\ntranscript_transformer predict data/example_data.fa fa --output_type npy models/example_model.ckpt\n```\n\n### Output data\n\nThe model returns predictions for every nucleotide on the transcripts. For each transcript, the array lists the transcript label and model outputs. The tool can output predictions using both the `npy` or `h5` format. \n\n```python\n>>> results = np.load('results.npy', allow_pickle=True)\n>>> results[0]\narray(['>ENST00000410304',\n       array([2.3891837e-09, 7.0824785e-07, 8.3791534e-09, 4.3269135e-09,\n              4.9220684e-08, 1.5315813e-10, 7.0196869e-08, 2.4103475e-10,\n              4.5873511e-10, 1.4299616e-10, 6.1071654e-09, 1.9664975e-08,\n              2.9255699e-07, 4.7719610e-08, 7.7600065e-10, 9.2305236e-10,\n              3.3297397e-07, 3.5771163e-07, 4.1942007e-05, 4.5123262e-08,\n              1.0270607e-11, 1.1841109e-09, 7.9038587e-10, 6.5511790e-10,\n              6.0892291e-13, 1.6157842e-11, 6.9130129e-10, 4.5778301e-11,\n              2.1682500e-03, 2.3315516e-09, 2.2578116e-11], dtype=float32)],\n      dtype=object)\n\n```\n\n### Other function flags\nVarious other function flags dictate the properties of the dataloader, model architecture and training procedure.\n\n<details><summary>Dataloader</summary>\n\n```\ndata loader arguments\n\n  --min_seq_len int     minimum sequence length of transcripts (default: 0)\n  --max_seq_len int     maximum sequence length of transcripts (default: 30000)\n  --leaky_frac float    fraction of samples that escape conditions (default: 0.05)\n  --num_workers int     number of data loader workers (default: 12)\n  --max_transcripts_per_batch int\n                        maximum of transcripts per batch (default: 400)\n```\n\n</details>\n\n<details><summary>Model architecture</summary>\n\n```\nModel:\n  Transformer arguments\n\n  --transfer_checkpoint str\n                        Path to checkpoint pretrained model (default: None)\n  --lr float            learning rate (default: 0.001)\n  --decay_rate float    linearly decays learning rate for every epoch (default: 0.95)\n  --num_tokens int      number of unique input tokens (default: 7)\n  --dim int             dimension of the hidden states (default: 30)\n  --depth int           number of layers (default: 6)\n  --heads int           number of attention heads in every layer (default: 6)\n  --dim_head int        dimension of the attention head matrices (default: 16)\n  --nb_features int     number of random features, if not set, will default to (d * log(d)),where d is the dimension\n                        of each head (default: 80)\n                        of each head (default: 80)\n  --feature_redraw_interval int\n                        how frequently to redraw the projection matrix (default: 100)\n  --generalized_attention boolean\n                        applies generalized attention functions (default: True)\n  --kernel_fn boolean   generalized attention function to apply (if generalized attention) (default: ReLU())\n  --reversible boolean  reversible layers, from Reformer paper (default: True)\n  --ff_chunks int       chunk feedforward layer, from Reformer paper (default: 10)\n  --use_scalenorm boolean\n                        use scale norm, from 'Transformers without Tears' paper (default: False)\n  --use_rezero boolean  use rezero, from 'Rezero is all you need' paper (default: False)\n  --ff_glu boolean      use GLU variant for feedforward (default: True)\n  --emb_dropout float   embedding dropout (default: 0.1)\n  --ff_dropout float    feedforward dropout (default: 0.1)\n  --attn_dropout float  post-attn dropout (default: 0.1)\n  --local_attn_heads int\n                        the amount of heads used for local attention (default: 4)\n  --local_window_size int\n                        window size of local attention (default: 256)\n```\n\n</details>\n\n<details><summary>Pytorch lightning trainer</summary>\n\n```\npl.Trainer:\n  --logger [str_to_bool]\n                        Logger (or iterable collection of loggers) for experiment tracking. A ``True`` value uses\n                        the default ``TensorBoardLogger``. ``False`` will disable logging. (default: True)\n  --checkpoint_callback [str_to_bool]\n                        If ``True``, enable checkpointing. It will configure a default ModelCheckpoint callback if\n                        there is no user-defined ModelCheckpoint in\n                        :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`. (default: True)\n  --default_root_dir str\n                        Default path for logs and weights when no logger/ckpt_callback passed. Default:\n                        ``os.getcwd()``. Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n                        (default: None)\n  --gradient_clip_val float\n                        0 means don't clip. (default: 0.0)\n  --gradient_clip_algorithm str\n                        'value' means clip_by_value, 'norm' means clip_by_norm. Default: 'norm' (default: norm)\n  --process_position int\n                        orders the progress bar when running multiple models on same machine. (default: 0)\n  --num_nodes int       number of GPU nodes for distributed training. (default: 1)\n  --num_processes int   number of processes for distributed training with distributed_backend=\"ddp_cpu\" (default: 1)\n  --gpus _gpus_allowed_type\n                        number of gpus to train on (int) or which GPUs to train on (list or str) applied per node\n                        (default: None)\n  --auto_select_gpus [str_to_bool]\n                        If enabled and `gpus` is an integer, pick available gpus automatically. This is especially\n                        useful when GPUs are configured to be in \"exclusive mode\", such that only one process at a\n                        time can access them. (default: False)\n  --tpu_cores _gpus_allowed_type\n                        How many TPU cores to train on (1 or 8) / Single TPU to train on [1] (default: None)\n  --log_gpu_memory str  None, 'min_max', 'all'. Might slow performance (default: None)\n  --progress_bar_refresh_rate int\n                        How often to refresh progress bar (in steps). Value ``0`` disables progress bar. Ignored\n                        when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means\n                        a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).\n                        (default: None)\n  --overfit_batches _int_or_float_type\n                        Overfit a fraction of training data (float) or a set number of batches (int). (default: 0.0)\n  --track_grad_norm float\n                        -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm. (default:\n                        -1)\n  --check_val_every_n_epoch int\n                        Check val every n train epochs. (default: 1)\n  --fast_dev_run [str_to_bool_or_int]\n                        runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es) of train, val and test to\n                        find any bugs (ie: a sort of unit test). (default: False)\n  --accumulate_grad_batches int\n                        Accumulates grads every k batches or as set up in the dict. (default: 1)\n  --max_epochs int      Stop training once this number of epochs is reached. Disabled by default (None). If both\n                        max_epochs and max_steps are not specified, defaults to ``max_epochs`` = 1000. (default:\n                        None)\n  --min_epochs int      Force training for at least these many epochs. Disabled by default (None). If both\n                        min_epochs and min_steps are not specified, defaults to ``min_epochs`` = 1. (default: None)\n  --max_steps int       Stop training after this number of steps. Disabled by default (None). (default: None)\n  --min_steps int       Force training for at least these number of steps. Disabled by default (None). (default:\n                        None)\n  --max_time str        Stop training after this amount of time has passed. Disabled by default (None). The time\n                        duration can be specified in the format DD:HH:MM:SS (days, hours, minutes seconds), as a\n                        :class:`datetime.timedelta`, or a dictionary with keys that will be passed to\n                        :class:`datetime.timedelta`. (default: None)\n  --limit_train_batches _int_or_float_type\n                        How much of training dataset to check (float = fraction, int = num_batches) (default: 1.0)\n  --limit_val_batches _int_or_float_type\n                        How much of validation dataset to check (float = fraction, int = num_batches) (default: 1.0)\n  --limit_test_batches _int_or_float_type\n                        How much of test dataset to check (float = fraction, int = num_batches) (default: 1.0)\n  --limit_predict_batches _int_or_float_type\n                        How much of prediction dataset to check (float = fraction, int = num_batches) (default: 1.0)\n  --val_check_interval _int_or_float_type\n                        How often to check the validation set. Use float to check within a training epoch, use int\n                        to check every n steps (batches). (default: 1.0)\n  --flush_logs_every_n_steps int\n                        How often to flush logs to disk (defaults to every 100 steps). (default: 100)\n  --log_every_n_steps int\n                        How often to log within steps (defaults to every 50 steps). (default: 50)\n  --accelerator str     Previously known as distributed_backend (dp, ddp, ddp2, etc...). Can also take in an\n                        accelerator object for custom hardware. (default: None)\n  --sync_batchnorm [str_to_bool]\n                        Synchronize batch norm layers between process groups/whole world. (default: False)\n  --precision int       Double precision (64), full precision (32) or half precision (16). Can be used on CPU, GPU\n                        or TPUs. (default: 32)\n  --weights_summary str\n                        Prints a summary of the weights when training begins. (default: top)\n  --weights_save_path str\n                        Where to save weights if specified. Will override default_root_dir for checkpoints only. Use\n                        this if for whatever reason you need the checkpoints stored in a different place than the\n                        logs written in `default_root_dir`. Can be remote file paths such as `s3://mybucket/path` or\n                        'hdfs://path/' Defaults to `default_root_dir`. (default: None)\n  --num_sanity_val_steps int\n                        Sanity check runs n validation batches before starting the training routine. Set it to `-1`\n                        to run all batches in all validation dataloaders. (default: 2)\n  --truncated_bptt_steps int\n                        Deprecated in v1.3 to be removed in 1.5. Please use\n                        :paramref:`~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps` instead.\n                        (default: None)\n  --resume_from_checkpoint str\n                        Path/URL of the checkpoint from which training is resumed. If there is no checkpoint file at\n                        the path, start from scratch. If resuming from mid-epoch checkpoint, training will start\n                        from the beginning of the next epoch. (default: None)\n  --profiler str        To profile individual steps during training and assist in identifying bottlenecks. (default:\n                        None)\n  --benchmark [str_to_bool]\n                        If true enables cudnn.benchmark. (default: False)\n  --deterministic [str_to_bool]\n                        If true enables cudnn.deterministic. (default: False)\n  --reload_dataloaders_every_epoch [str_to_bool]\n                        Set to True to reload dataloaders every epoch. (default: False)\n  --auto_lr_find [str_to_bool_or_str]\n                        If set to True, will make trainer.tune() run a learning rate finder, trying to optimize\n                        initial learning for faster convergence. trainer.tune() method will set the suggested\n                        learning rate in self.lr or self.learning_rate in the LightningModule. To use a different\n                        key set a string instead of True with the key name. (default: False)\n  --replace_sampler_ddp [str_to_bool]\n                        Explicitly enables or disables sampler replacement. If not specified this will toggled\n                        automatically when DDP is used. By default it will add ``shuffle=True`` for train sampler\n                        and ``shuffle=False`` for val/test sampler. If you want to customize it, you can set\n                        ``replace_sampler_ddp=False`` and add your own distributed sampler. (default: True)\n  --terminate_on_nan [str_to_bool]\n                        If set to True, will terminate training (by raising a `ValueError`) at the end of each\n                        training batch, if any of the parameters or the loss are NaN or +/-inf. (default: False)\n  --auto_scale_batch_size [str_to_bool_or_str]\n                        If set to True, will `initially` run a batch size finder trying to find the largest batch\n                        size that fits into memory. The result will be stored in self.batch_size in the\n                        LightningModule. Additionally, can be set to either `power` that estimates the batch size\n                        through a power search or `binsearch` that estimates the batch size through a binary search.\n                        (default: False)\n  --prepare_data_per_node [str_to_bool]\n                        If True, each LOCAL_RANK=0 will call prepare data. Otherwise only NODE_RANK=0, LOCAL_RANK=0\n                        will prepare data (default: True)\n  --plugins str         Plugins allow modification of core behavior like ddp and amp, and enable custom lightning\n                        plugins. (default: None)\n  --amp_backend str     The mixed precision backend to use (\"native\" or \"apex\") (default: native)\n  --amp_level str       The optimization level to use (O1, O2, etc...). (default: O2)\n  --distributed_backend str\n                        deprecated. Please use 'accelerator' (default: None)\n  --move_metrics_to_cpu [str_to_bool]\n                        Whether to force internal logged metrics to be moved to cpu. This can save some gpu memory,\n                        but can make training slower. Use with attention. (default: False)\n  --multiple_trainloader_mode str\n                        How to loop over the datasets when there are multiple train loaders. In 'max_size_cycle'\n                        mode, the trainer ends one epoch when the largest dataset is traversed, and smaller datasets\n                        reload when running out of their data. In 'min_size' mode, all the datasets reload when\n                        reaching the minimum length of datasets. (default: max_size_cycle)\n  --stochastic_weight_avg [str_to_bool]\n                        Whether to use `Stochastic Weight Averaging (SWA)(default: False)\n```\n\n</details>\n\n\n## ✔️ Package features\n\n- [ ] creation of `h5` file from genome assemblies and ribosome profiling datasets\n- [x] bucket sampling\n- [x] pre-training functionality\n- [x] data loading for sequence and ribosome data\n- [x] custom target labels\n- [ ] function hooks for custom data loading and pre-processing\n- [x] model architectures\n- [x] application of trained networks\n- [ ] test scripts\n\n## 🖊️ Citation <a name=\"citation\"></a>\n       \n```latex\n@article {Clauwaert2021.11.18.468957,\n\tauthor = {Clauwaert, Jim and McVey, Zahra and Gupta, Ramneek and Menschaert, Gerben},\n\ttitle = {TIS Transformer: Re-annotation of the human proteome using deep learning},\n\telocation-id = {2021.11.18.468957},\n\tyear = {2021},\n\tdoi = {10.1101/2021.11.18.468957},\n\tURL = {https://www.biorxiv.org/content/early/2021/11/19/2021.11.18.468957},\n\teprint = {https://www.biorxiv.org/content/early/2021/11/19/2021.11.18.468957.full.pdf},\n\tjournal = {bioRxiv}\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/jdcla/transcript_transformer",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "transcript-transformer",
    "package_url": "https://pypi.org/project/transcript-transformer/",
    "platform": null,
    "project_url": "https://pypi.org/project/transcript-transformer/",
    "project_urls": {
      "Homepage": "https://github.com/jdcla/transcript_transformer"
    },
    "release_url": "https://pypi.org/project/transcript-transformer/0.1.4/",
    "requires_dist": [
      "pytorch-lightning (==1.7.7)",
      "torchmetrics (==0.10.0)",
      "performer-pytorch (>=1.1.4)",
      "h5py (>=3.6.0)",
      "numpy (>=1.21.0)",
      "scipy (>=1.7.3)",
      "h5max (>=0.3.0)"
    ],
    "requires_python": ">=3.9",
    "summary": "Transformers for Transcripts",
    "version": "0.1.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17242829,
  "releases": {
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2aa651eb32e49773ce533a9d855918d59394a2c256509762632d724afaaef0fc",
          "md5": "772f4c3d5490b8e5c4c9fb419d4d4f31",
          "sha256": "dba10843fb4746b002e6363749303a199b0a3094ec569f6a4bd1fdaf225bca64"
        },
        "downloads": -1,
        "filename": "transcript_transformer-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "772f4c3d5490b8e5c4c9fb419d4d4f31",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 21499,
        "upload_time": "2022-10-19T10:52:30",
        "upload_time_iso_8601": "2022-10-19T10:52:30.205572Z",
        "url": "https://files.pythonhosted.org/packages/2a/a6/51eb32e49773ce533a9d855918d59394a2c256509762632d724afaaef0fc/transcript_transformer-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "91a1145961ba395d044df2464788d7c1538a64e15de620791b50c14c5e1a4b06",
          "md5": "683d223b315e857e12749dea0fb4cbcf",
          "sha256": "9dc6d3e3f919c32e62b2d0094406d27c8291a18e933041d54de440b0d1659b02"
        },
        "downloads": -1,
        "filename": "transcript_transformer-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "683d223b315e857e12749dea0fb4cbcf",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 21455,
        "upload_time": "2022-10-19T11:30:12",
        "upload_time_iso_8601": "2022-10-19T11:30:12.169230Z",
        "url": "https://files.pythonhosted.org/packages/91/a1/145961ba395d044df2464788d7c1538a64e15de620791b50c14c5e1a4b06/transcript_transformer-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "011c46b03b53d151a62a7b833e8bc395681aae79b24edf28c6c94d59bc840ff8",
          "md5": "789c51f9aa4a60a0b2e196d1d4431fb7",
          "sha256": "71b4b4e451471800b77b4e54a858c1e4a45268260445fe3d9f68c361b10fac1e"
        },
        "downloads": -1,
        "filename": "transcript_transformer-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "789c51f9aa4a60a0b2e196d1d4431fb7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 20512,
        "upload_time": "2022-11-30T16:49:37",
        "upload_time_iso_8601": "2022-11-30T16:49:37.641464Z",
        "url": "https://files.pythonhosted.org/packages/01/1c/46b03b53d151a62a7b833e8bc395681aae79b24edf28c6c94d59bc840ff8/transcript_transformer-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "51a827e95ea28bfe7ae1747af39e91de2e6574d11f82115a5971b0e907747915",
          "md5": "9606542a0d940fe1d3004545b43c8034",
          "sha256": "d80a0f96ff632f5242cbbbfca8951421e388aabb597478c75dac89af639bbae9"
        },
        "downloads": -1,
        "filename": "transcript transformer-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "9606542a0d940fe1d3004545b43c8034",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 11135741,
        "upload_time": "2022-11-30T16:49:39",
        "upload_time_iso_8601": "2022-11-30T16:49:39.859242Z",
        "url": "https://files.pythonhosted.org/packages/51/a8/27e95ea28bfe7ae1747af39e91de2e6574d11f82115a5971b0e907747915/transcript%20transformer-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f814d4e3ec86916ff16a2cf8ad8970e84633a069881c026f8113b8087e523d72",
          "md5": "9b720fa9f1b61eeb01426da99553a05c",
          "sha256": "1b9c8987f81c726aa69af062ee5ad0b16799c99ed606a267f9119a6f102a6c6b"
        },
        "downloads": -1,
        "filename": "transcript_transformer-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9b720fa9f1b61eeb01426da99553a05c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 20846,
        "upload_time": "2023-03-10T17:01:48",
        "upload_time_iso_8601": "2023-03-10T17:01:48.921933Z",
        "url": "https://files.pythonhosted.org/packages/f8/14/d4e3ec86916ff16a2cf8ad8970e84633a069881c026f8113b8087e523d72/transcript_transformer-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "219b37c7e2ca6eab4c832f1f330b821fe812b1c160153adee837db90db135159",
          "md5": "fa28de111c7ee5ef9c38d241dfc4d39b",
          "sha256": "66ee60a2f503c22d9e9735729ff1927ab9b5709128a8b9b7ddfc55411ca717d7"
        },
        "downloads": -1,
        "filename": "transcript transformer-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "fa28de111c7ee5ef9c38d241dfc4d39b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 11135493,
        "upload_time": "2023-03-10T17:01:51",
        "upload_time_iso_8601": "2023-03-10T17:01:51.131376Z",
        "url": "https://files.pythonhosted.org/packages/21/9b/37c7e2ca6eab4c832f1f330b821fe812b1c160153adee837db90db135159/transcript%20transformer-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eb307e5f8f436b29f6cb37982a5c164edea3ac34501b774036735aa161604911",
          "md5": "d025b12c9d46e8155715cf9e0184d6d6",
          "sha256": "4a6e5a2bd558a488332cc3478919cd64efa9bd5f71fc8edb9045cc4686dab003"
        },
        "downloads": -1,
        "filename": "transcript_transformer-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d025b12c9d46e8155715cf9e0184d6d6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.9",
        "size": 20844,
        "upload_time": "2023-03-10T17:08:22",
        "upload_time_iso_8601": "2023-03-10T17:08:22.756931Z",
        "url": "https://files.pythonhosted.org/packages/eb/30/7e5f8f436b29f6cb37982a5c164edea3ac34501b774036735aa161604911/transcript_transformer-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6ee3541dbbc000d688b89fae7c6a421b4a9eb85a4577f24d8910e6842879e85f",
          "md5": "121f6f3d258ee0b646a69a3fe08082c5",
          "sha256": "50d8b742ab4db79efff701adfd3d8fddceff7207b1571713cb799227c078129d"
        },
        "downloads": -1,
        "filename": "transcript transformer-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "121f6f3d258ee0b646a69a3fe08082c5",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.9",
        "size": 11135509,
        "upload_time": "2023-03-10T17:08:24",
        "upload_time_iso_8601": "2023-03-10T17:08:24.751258Z",
        "url": "https://files.pythonhosted.org/packages/6e/e3/541dbbc000d688b89fae7c6a421b4a9eb85a4577f24d8910e6842879e85f/transcript%20transformer-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eb307e5f8f436b29f6cb37982a5c164edea3ac34501b774036735aa161604911",
        "md5": "d025b12c9d46e8155715cf9e0184d6d6",
        "sha256": "4a6e5a2bd558a488332cc3478919cd64efa9bd5f71fc8edb9045cc4686dab003"
      },
      "downloads": -1,
      "filename": "transcript_transformer-0.1.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d025b12c9d46e8155715cf9e0184d6d6",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.9",
      "size": 20844,
      "upload_time": "2023-03-10T17:08:22",
      "upload_time_iso_8601": "2023-03-10T17:08:22.756931Z",
      "url": "https://files.pythonhosted.org/packages/eb/30/7e5f8f436b29f6cb37982a5c164edea3ac34501b774036735aa161604911/transcript_transformer-0.1.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6ee3541dbbc000d688b89fae7c6a421b4a9eb85a4577f24d8910e6842879e85f",
        "md5": "121f6f3d258ee0b646a69a3fe08082c5",
        "sha256": "50d8b742ab4db79efff701adfd3d8fddceff7207b1571713cb799227c078129d"
      },
      "downloads": -1,
      "filename": "transcript transformer-0.1.4.tar.gz",
      "has_sig": false,
      "md5_digest": "121f6f3d258ee0b646a69a3fe08082c5",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.9",
      "size": 11135509,
      "upload_time": "2023-03-10T17:08:24",
      "upload_time_iso_8601": "2023-03-10T17:08:24.751258Z",
      "url": "https://files.pythonhosted.org/packages/6e/e3/541dbbc000d688b89fae7c6a421b4a9eb85a4577f24d8910e6842879e85f/transcript%20transformer-0.1.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}