{
  "info": {
    "author": "Lucas Liu",
    "author_email": "llychinalz@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 2 - Pre-Alpha",
      "Intended Audience :: Developers",
      "Natural Language :: English",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "\n![GitHub](https://img.shields.io/github/license/Microsoft/Admin-Torch) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/very-deep-transformers-for-neural-machine/machine-translation-on-wmt2014-english-french)](https://paperswithcode.com/sota/machine-translation-on-wmt2014-english-french?p=very-deep-transformers-for-neural-machine)[![Maintenance](https://img.shields.io/badge/doc-yes-green.svg)](https://microsoft.github.io/admin-torch/)\n\n<h2 align=\"center\">Admin-Torch</h2>\n<h5 align=\"center\">Transformers Training **Stabilized**</h5>\n\n<p align=\"center\">\n  <a href=\"#whats-new\">What's New?</a> •\n  <a href=\"#key-idea\">Key Idea</a> •\n  <a href=\"#how-to-use\">How To Use</a> •\n  <a href=\"https://microsoft.github.io/admin-torch/\">Docs</a> •\n  <a href=\"https://github.com/microsoft/admin-torch/tree/main/example\">Examples</a> •\n  <a href=\"#citation\">Citation</a> •\n  <a href=\"https://github.com/microsoft/admin-torch/tree/main/LICENSE\">License</a>\n</p>\n\nHere, we provide a plug-in-and-play implementation of [Admin](https://arxiv.org/abs/2004.08249),\nwhich stabilizes previously-diverged Transformer training and achieves better performance, \n**without introducing additional hyper-parameters**. The design of Admin is half-precision \nfriendly and can be **reparameterized into the original Transformer**. \n\n______________________________________________________________________\n## What's New?\n\nBeyond the [original admin implementation](https://github.com/LiyuanLucasLiu/Transformer-Clinic):\n1.  `admin-torch` removed the profilling stage and is **plug-in-and-play**. \n2.  `admin-torch`'s implementation is **more robust** (see below).\n\nComparison w. the [DeepNet Init](https://arxiv.org/abs/2203.00555) and the [Original Admin Init](https://github.com/LiyuanLucasLiu/Transformer-Clinic) \n(on WMT'17).\n\n|               | Regular batch size (8x4096) |  Huge batch size (128x4096) |\n|---------------|--------------------|------------------|\n| [Original Admin](https://github.com/LiyuanLucasLiu/Transformer-Clinic)| ✅ | ❌ |\n| [DeepNet](https://arxiv.org/abs/2203.00555) | ❌ | ✅ |\n| `admin-torch` | ✅ | ✅ |\n\nMore details can be found in [our example](https://github.com/microsoft/admin-torch/tree/main/example).\n\n## Key Idea\n<h5 align=\"center\"><i>What complicates Transformer training?</i></h5>\n\nFor Transformer f, input x, randomly initialized weight w, we describe its stability (``output_change_scale``) as \n\n<p align=\"center\">\n<!-- $E[|f(x, w) - f(x, w + \\delta)|_2^2]$ --> <img style=\"transform: translateY(0.1em); background: white;\" src=\"https://render.githubusercontent.com/render/math?math=E%5B%7Cf(x%2C%20w)%20-%20f(x%2C%20w%20%2B%20%5Cdelta)%7C_2%5E2%5D\">\n</p>\n\nIn [our study](https://arxiv.org/abs/2004.08249), we show that, an original N-layer Transformer's \n``output_change_scale`` is ``O(n)``, which unstabilizes its training. Admin stabilize Transformer's\ntraining by regulating this scale to ``O(logn)`` or ``O(1)``. \n\n<p align=\"center\"><img width=\"60%\" src=\"doc/source/_static/output_change.png\"/></p>\n \nMore details can be found in our [paper](https://arxiv.org/abs/2004.08249).\n\n\n## How to use?\n\n### install \n```\npip install admin-torch\n```\n\n### import\n```\nimport admin-torch\n```\n\n### enjoy\n\n```diff\ndef __init__(self, ...):\n...\n+(residual = admin-torch.as_module(self, self.number_of_sub_layers))+\n...\n\ndef forward(self, ):\n...\n-!x = x + f(x)!-\n+(x = residual(x, f(x)))+\nx = self.LN(x)\n...\n```\n\nAn elaborated example can be found at [our doc](https://liyuanlucasliu.github.io/Admin/), and a real working example can be found at [LiyuanLucasLiu/fairseq](https://github.com/LiyuanLucasLiu/fairseq/commit/33ad76ae5dc927bc32b9594f9728a367c45680bb) (training recipe is available at [our example](https://github.com/microsoft/admin-torch/tree/main/example)).\n\n## Citation\nPlease cite the following papers if you found our model useful. Thanks!\n\n>Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han (2020). Understanding the Difficulty of Training Transformers. Proc. 2020 Conf. on Empirical Methods in Natural Language Processing (EMNLP'20).\n```\n@inproceedings{liu2020admin,\n  title={Understanding the Difficulty of Training Transformers},\n  author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},\n  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)},\n  year={2020}\n}\n```\n> Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng Gao (2020). Very Deep Transformers for Neural Machine Translation. arXiv preprint arXiv:2008.07772 (2020).\n```\n@inproceedings{liu_deep_2020,\n author = {Liu, Xiaodong and Duh, Kevin and Liu, Liyuan and Gao, Jianfeng},\n booktitle = {arXiv:2008.07772 [cs]},\n title = {Very Deep Transformers for Neural Machine Translation},\n year = {2020}\n}\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/microsoft/admin-torch",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "admin-torch",
    "package_url": "https://pypi.org/project/admin-torch/",
    "platform": null,
    "project_url": "https://pypi.org/project/admin-torch/",
    "project_urls": {
      "Homepage": "https://github.com/microsoft/admin-torch"
    },
    "release_url": "https://pypi.org/project/admin-torch/0.1.0/",
    "requires_dist": [
      "torch"
    ],
    "requires_python": "",
    "summary": "Plug-in-and-Play Toolbox for Stablizing Transformer Training",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13494773,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8ff8a637a2448682e641efb852821084906ff07e6548d75c2ef322f35c342763",
          "md5": "cde297e48c2df8115ddba75ce12ffb29",
          "sha256": "fd5696ff43a699b97bee2f58b476653dcf393c0320f43d9a22eee9a6a71ddc70"
        },
        "downloads": -1,
        "filename": "admin_torch-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cde297e48c2df8115ddba75ce12ffb29",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 6482,
        "upload_time": "2022-04-12T20:44:36",
        "upload_time_iso_8601": "2022-04-12T20:44:36.197131Z",
        "url": "https://files.pythonhosted.org/packages/8f/f8/a637a2448682e641efb852821084906ff07e6548d75c2ef322f35c342763/admin_torch-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e56f9b420533c0f9f09536d88f17fe5b79e046c225b414d6f9b872de17978189",
          "md5": "10ac9b8de35b6f3a0f71457bf05b215b",
          "sha256": "fc7fce3fafed83d719e0a0594a0201bee54e53807d6f9f0715391bfb89803db4"
        },
        "downloads": -1,
        "filename": "admin_torch-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "10ac9b8de35b6f3a0f71457bf05b215b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 1239786,
        "upload_time": "2022-04-12T20:44:38",
        "upload_time_iso_8601": "2022-04-12T20:44:38.363932Z",
        "url": "https://files.pythonhosted.org/packages/e5/6f/9b420533c0f9f09536d88f17fe5b79e046c225b414d6f9b872de17978189/admin_torch-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8ff8a637a2448682e641efb852821084906ff07e6548d75c2ef322f35c342763",
        "md5": "cde297e48c2df8115ddba75ce12ffb29",
        "sha256": "fd5696ff43a699b97bee2f58b476653dcf393c0320f43d9a22eee9a6a71ddc70"
      },
      "downloads": -1,
      "filename": "admin_torch-0.1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "cde297e48c2df8115ddba75ce12ffb29",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 6482,
      "upload_time": "2022-04-12T20:44:36",
      "upload_time_iso_8601": "2022-04-12T20:44:36.197131Z",
      "url": "https://files.pythonhosted.org/packages/8f/f8/a637a2448682e641efb852821084906ff07e6548d75c2ef322f35c342763/admin_torch-0.1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e56f9b420533c0f9f09536d88f17fe5b79e046c225b414d6f9b872de17978189",
        "md5": "10ac9b8de35b6f3a0f71457bf05b215b",
        "sha256": "fc7fce3fafed83d719e0a0594a0201bee54e53807d6f9f0715391bfb89803db4"
      },
      "downloads": -1,
      "filename": "admin_torch-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "10ac9b8de35b6f3a0f71457bf05b215b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 1239786,
      "upload_time": "2022-04-12T20:44:38",
      "upload_time_iso_8601": "2022-04-12T20:44:38.363932Z",
      "url": "https://files.pythonhosted.org/packages/e5/6f/9b420533c0f9f09536d88f17fe5b79e046c225b414d6f9b872de17978189/admin_torch-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}