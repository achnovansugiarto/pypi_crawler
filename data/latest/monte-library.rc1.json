{
  "info": {
    "author": "draktr",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Education",
      "Intended Audience :: Financial and Insurance Industry",
      "Intended Audience :: Healthcare Industry",
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Topic :: Scientific/Engineering",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Scientific/Engineering :: Bio-Informatics",
      "Topic :: Scientific/Engineering :: Chemistry",
      "Topic :: Scientific/Engineering :: Mathematics",
      "Topic :: Scientific/Engineering :: Physics"
    ],
    "description": "# Monte Library\r\n\r\nmonte-library is a set of Monte Carlo methods in Python. The package is written to be flexible, clear to understand and encompass variety of Monte Carlo methods.\r\n\r\n* Free software: MIT license\r\n* Documentation: <https://monte-library.readthedocs.io/en/latest/>\r\n\r\n## Installation\r\n\r\nPreferred method to install `monte-library` is through Python's package installer pip. To install `monte-library`, run this command in your terminal\r\n\r\n```shell\r\npip install monte-library\r\n```\r\n\r\nAlternatively, you can install the package directly from GitHub:\r\n\r\n```shell\r\ngit clone -b development https://github.com/draktr/monte-library.git\r\ncd monte-library\r\npython setup.py install\r\n```\r\n\r\n## Features\r\n\r\n### Base module\r\n\r\n* saving samples and log probability values as `.csv` file\r\n* posterior mean, standard deviation and quantiles\r\n* diagnostic tools: effective sample size (ESS), autocorrelation plots, ergodic mean plots, acceptance rate\r\n* visualizations: histograms, kernel density estimation plots, traceplots\r\n\r\n### General Monte Carlo Methods\r\n\r\n* multidimensional Monte Carlo integration\r\n* multidimensional rejection sampling\r\n* multidimensional importance sampling\r\n\r\n### Markov Chain Monte Carlo Modelling Methods\r\n\r\n* symmetric proposal Metropolis algorithm\r\n* Metropolis-Hastings algorithm\r\n* Gibbs sampler\r\n* vanilla Hamiltonian Monte Carlo\r\n\r\n## Advantages\r\n\r\n* **FLEXIBLE** - the package allows users to use various existing Monte Carlo methods for their needs without needing to write the whole algorithm. At the same time,  `monte-library` allows users to specify their own hyperparameters, posterior and proposal distributions as needed. Furthermore `BaseSampler` class can be used as parent class for any proprietary Monte Carlo algorithm thus utilizing its features for visualizations, posterior analysis and convergence checks\r\n\r\n* **SIMPLE AND CLEAR CODE BASE** - code was intentionally kept simple to be understandable to those with limited exposure to Statistical Computing. `monte-library` is a great tool to supplement learning as it generally matches mathematical formulations of algorithms and simple syntax helps focus on the algorithm itself.\r\n\r\n* **COMPREHENSIVE** - includes Monte Calor methods for various applications. Bayesian modelling methods include both classical methods (Metropolis algorithm) as well as more advanced methods such as Hamiltonian Monte Carlo.\r\n\r\n## Usage\r\n\r\nPackage contains variety of Monte Carlo methods that can be applied to problems ranging from integration to modelling. Importantly, code is both simple and generalized as to match the respective mathematical formulations of algorithms. As such it can be a great supplement when learning these topics. Finally, the package is flexible and `BaseSampler` class can be used as a parent class to any user-defined sampler. Furthermore, it is easy to modify existing algorithms with proprietary improvements.\r\n\r\n### Example 1: Monte Carlo Integration\r\n\r\nThe following example is a simple Monte Carlo implementation to solve the following integral:\r\n$$ \\int_{-3}^{3} \\int_{-3}^{3} x^2 + y^3 dxdy $$\r\n\r\n```python\r\nfrom monte import integrator\r\n\r\ndef integrand(args):\r\n        return args[0] ** 2 + args[1] ** 3\r\n\r\nresult = integrator(integrand, lower_bounds=[-3, -3], upper_bounds=[3, 3], n=10000000)\r\nresult\r\n\r\n```\r\n\r\n### Example 2: Bayesian Linear Regression with Metropolis Algorithm\r\n\r\nExample 2 is using Metropolis algorithm (with Gaussian proposal) to estimate parameters of a multivariate linear regression.\r\n\r\n```python\r\nimport numpy as np\r\nfrom scipy import stats\r\nfrom monte import GaussianMetropolis\r\n\r\n# First, we create data\r\ntrue_theta = np.array([5, 10, 2, 2, 4])\r\nn = 1000\r\nx = np.zeros((n, 4))\r\nx[:, 0] = np.repeat(1, n)\r\nx[:, 1:4] = stats.norm(loc=0, scale=1).rvs(size=(n, 3))\r\n\r\nmu = np.matmul(x, true_theta[0:-1])\r\ny = stats.norm(loc=mu, scale=true_theta[-1]).rvs(size=n)\r\n\r\n# Define the posterior\r\ndef posterior(theta, x, y):\r\n\r\n    beta_prior = stats.multivariate_normal(\r\n        mean=np.repeat(0, len(theta[0:-1])),\r\n        cov=np.diag(np.repeat(30, len(theta[0:-1]))),\r\n    ).logpdf(theta[0:-1])\r\n    sd_prior = stats.uniform(loc=0, scale=30).logpdf(theta[-1])\r\n\r\n    mu = np.matmul(x, theta[0:-1])\r\n    likelihood = np.sum(stats.norm(loc=mu, scale=theta[-1]).logpdf(y))\r\n\r\n    return beta_prior + sd_prior + likelihood\r\n\r\n# Lastly, we sample\r\ngaussian_sampler = GaussianMetropolis(posterior)\r\ngaussian_sampler.sample(\r\n    iter=10000,\r\n    warmup=5000,\r\n    theta=np.array([0, 0, 0, 0, 1]),\r\n    step_size=1,\r\n    lag=1,\r\n    x=x,\r\n    y=y,\r\n    )\r\n\r\n```\r\n\r\nUsing methods from the `BaseSampler` class we can perform posterior analytics. These are some of the analytics methods:\r\n\r\n```python\r\n# Checking parameter estimates and their credible intervals\r\ngaussian_sampler.mean()\r\ngaussian_sampler.credible_interval()\r\n\r\n# Checking Metropolis acceptance rate\r\ngaussian_sampler.acceptance_rate()\r\n\r\n# Plotting KDE plot with histogram\r\ngaussian_sampler.parameter_kde()\r\n\r\n# Plotting traceplots and ergodic means, and calculating effective sample sizes as convergence diagnostics\r\ngaussian_sampler.traceplots()\r\ngaussian_sampler.plot_ergodic_mean()\r\n```\r\n\r\n### Example 3: Sampling from a Multivariate Distribution using Hamiltonian Monte Carlo\r\n\r\nIn the following example we use Hamiltonian Monte Carlo (HMC) algorithm to sample from a distribution. Note that this is a toy example, and HMC is more appropriate to be used for higher-dimensional model parameter estimation. Also note that analytical gradient is not necessary.\r\n\r\n```python\r\nimport numpy as np\r\nfrom monte import HamiltonianMC\r\n\r\n# Defining the distribution that we are going to sample from...\r\ndef posterior(theta):\r\n    return -0.5 * np.sum(theta**2)\r\n\r\n# ... and its gradient\r\ndef posterior_gradient(theta):\r\n    return -theta\r\n\r\n# Sampling\r\nsampler = HamiltonianMC(posterior, posterior_gradient)\r\nsampler.sample(\r\n    iter=10000,\r\n    warmup=10,\r\n    theta=np.array([8.0, -3.0]),\r\n    epsilon=0.01,\r\n    l=10,\r\n    metric=None,\r\n    lag=1,\r\n    )\r\n```\r\n\r\n## Alternatives and Complements\r\n\r\nThere are more sophisticated and computationally efficient implementations of Monte Carlo methods for off-the-shelf solutions\r\n\r\n* [ArviZ](https://www.arviz.org/en/latest/) - independent library for exploratory analysis of Bayesian models\r\n* [vegas](https://github.com/gplepage/vegas) - uses improved version of the adaptive Monte Carlo vegas algorithm\r\n* [OpenBUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs/openbugs/) - open source implementation of BUGS language utilizing Gibbs sampler\r\n* [JAGS](https://mcmc-jags.sourceforge.io/) - cross-platform and more extensible implementation of BUGS language\r\n* [WinBUGS](https://www.mrc-bsu.cam.ac.uk/software/bugs/the-bugs-project-winbugs/) - software for Bayesian analysis utilizing Gibbs sampler (available, but discontinued in favour of OpenBUGS)\r\n* [Stan](https://mc-stan.org/) - state-of-the-art probabilistic language implementing advanced version of No-U-Turn Sampler\r\n* [PyMC](https://github.com/pymc-devs/pymc) - supports HMC and Metropolis-Hastings algorithms, as well as Sequential Monte Carlo methods\r\n\r\n## Project Principles\r\n\r\n* Easy to be understood and used by non-mathematicians\r\n* Potential to be used as pedagogical tool\r\n* Easy to modify algorithms with proprietary improvements\r\n* Flexibility and simplicity over computational efficiency\r\n* Tested\r\n* Dedicated documentation\r\n* Formatting deferred to [Black](https://github.com/psf/black)\r\n\r\n## Future Development\r\n\r\nFeel free to reach out through Issues forum if you wish to add features or help in any other way. If there are any issues, bugs or improvement recommendations, please raise them in the forum. Especially reach out if you want to contribute with any of the possible features listed below.\r\n\r\n### Possible Future Features\r\n\r\n#### In `BaseSampler` Class\r\n\r\n* Sampling trace animation\r\n* ECDF plot\r\n* Forrest plot of parameter estimates with credible intervals\r\n* Monte Carlo Error\r\n* Support for multiple chains\r\n* Convergence iterations required (Raftery-Lewis 1995)\r\n* Rhat (Gelman-Rubin 1992)\r\n* Means equality test (Geweke 1992)\r\n\r\n#### Monte Carlo Methods\r\n\r\n* Slice sampling\r\n* Annealed importance sampling\r\n* Component-wise Metropolis\r\n* Independent Metropolis\r\n* Wang-Landau algorithm\r\n* Monte Carlo tree search\r\n* Direct Sampling Monte Carlo\r\n* Monte Carlo statistical distribution test\r\n\r\n## Further Reading\r\n\r\nThe following is the non-exhaustive list of useful sources for learning more about Monte Carlo methods. Some of the code in `monte` has been written based on mathematical formulae from some of these sources.\r\n\r\n### General\r\n\r\n<a id=\"1\">[1]</a> Ntzoufras, I. (2009). Bayesian Modelling Using WinBUGS. Wiley. \\\r\n<a id=\"2\">[2]</a> Metropolis, N., & Ulam, S. (1949). The Monte Carlo Method. Journal of the American Statistical Association, 44(247), 335â€“341. <https://doi.org/10.1080/01621459.1949.10483310>\r\n\r\n### Metropolis\r\n\r\n<a id=\"3\">[3]</a> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (2004). Equation of State Calculations by Fast Computing Machines. The Journal of Chemical Physics, 21(6), 1087. <https://doi.org/10.1063/1.1699114> \\\r\n<a id=\"4\">[4]</a> Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications. Biometrika, 57(1), 97â€“109. <https://doi.org/10.1093/BIOMET/57.1.97> \\\r\n<a id=\"5\">[5]</a> Hartig, F. (n.d.). A simple Metropolis-Hastings MCMC in R | theoretical ecology. Retrieved February 15, 2023, from <https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/> \\\r\n<a id=\"6\">[6]</a> Dirty Quant @YouTube. (n.d.). The Metropolis-Hastings Algorithm (MCMC in Python) - YouTube. Retrieved February 15, 2023, from <https://www.youtube.com/watch?v=MxI78mpq_44> \\\r\n<a id=\"7\">[7]</a> TWEAG Software Innovation Lab. (n.d.). Markov chain Monte Carlo (MCMC) Sampling, Part 1: The Basics - Tweag. Retrieved February 15, 2023, from <https://www.tweag.io/blog/2019-10-25-mcmc-intro1/> \\\r\n<a id=\"8\">[8]</a> Urbanevych, V. (n.d.). VU | Bayesian linear regression and Metropolis-Hastings sampler. Retrieved February 15, 2023, from <https://vitaliiur.github.io/blog/2021/linreg/>\r\n\r\n### Gibbs Sampler\r\n\r\n<a id=\"9\">[9]</a> Campbell, K. R. (n.d.). Gibbs sampling for Bayesian linear regression in Python | Kieran R Campbell - blog. Retrieved February 15, 2023, from <https://kieranrcampbell.github.io/blog/2016/05/15/gibbs-sampling-bayesian-linear-regression.html>\r\n\r\n### Hamiltonian Monte Carlo\r\n\r\n<a id=\"10\">[10]</a> Betancourt, M. (2017). A Conceptual Introduction to Hamiltonian Monte Carlo. <https://doi.org/10.48550/arxiv.1701.02434> \\\r\n<a id=\"11\">[11]</a> Neal, R. M. (2012). MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte Carlo, 1â€“592. <https://doi.org/10.1201/b10905> \\\r\n<a id=\"12\">[12]</a> Stan. (n.d.). 15.1 Hamiltonian Monte Carlo | Stan Reference Manual. Retrieved February 15, 2023, from <https://mc-stan.org/docs/reference-manual/hamiltonian-monte-carlo.html> \\\r\n<a id=\"13\">[13]</a> Clark, M. (n.d.). Hamiltonian Monte Carlo | Model Estimation by Example. Retrieved February 15, 2023, from <https://m-clark.github.io/models-by-example/hamiltonian-monte-carlo.html> \\\r\n<a id=\"14\">[14]</a> Richard. (n.d.). Markov Chains: Why Walk When You Can Flow? | Elements of Evolutionary Anthropology. Retrieved February 15, 2023, from <http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/>\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/draktr/monte-library",
    "keywords": "montecarlo monte carlo,optimization,integration,sampling,mcmc,hmc,simulation,modelling",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "monte-library",
    "package_url": "https://pypi.org/project/monte-library/",
    "platform": null,
    "project_url": "https://pypi.org/project/monte-library/",
    "project_urls": {
      "Documentation": "https://monte-library.readthedocs.io/en/latest/",
      "Homepage": "https://github.com/draktr/monte-library",
      "Issues": "https://github.com/draktr/monte-library/issues"
    },
    "release_url": "https://pypi.org/project/monte-library/0.1.0/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "monte-library is a set of Monte Carlo methods in Python. The package is written to be flexible, clear to understand and encompass variety of Monte Carlo methods.",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17364488,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cd3837aa7819f7ae20b71c385e02d3ac9535bdd863755124e82b136fe4626fae",
          "md5": "d51685b21e284153ee8f6619a17c776b",
          "sha256": "79b3d542bef28f37b559bbc6987f91172689de82a43186d1788e441c1c53ecb1"
        },
        "downloads": -1,
        "filename": "monte-library-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "d51685b21e284153ee8f6619a17c776b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 27608,
        "upload_time": "2023-03-20T14:26:37",
        "upload_time_iso_8601": "2023-03-20T14:26:37.329444Z",
        "url": "https://files.pythonhosted.org/packages/cd/38/37aa7819f7ae20b71c385e02d3ac9535bdd863755124e82b136fe4626fae/monte-library-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "cd3837aa7819f7ae20b71c385e02d3ac9535bdd863755124e82b136fe4626fae",
        "md5": "d51685b21e284153ee8f6619a17c776b",
        "sha256": "79b3d542bef28f37b559bbc6987f91172689de82a43186d1788e441c1c53ecb1"
      },
      "downloads": -1,
      "filename": "monte-library-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "d51685b21e284153ee8f6619a17c776b",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 27608,
      "upload_time": "2023-03-20T14:26:37",
      "upload_time_iso_8601": "2023-03-20T14:26:37.329444Z",
      "url": "https://files.pythonhosted.org/packages/cd/38/37aa7819f7ae20b71c385e02d3ac9535bdd863755124e82b136fe4626fae/monte-library-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}