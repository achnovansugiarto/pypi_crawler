{
  "info": {
    "author": "Soumil Nitin Shah",
    "author_email": "shahsoumil519@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# ClickUp to Data Lake (S3) Migration and backup All Data Scripts\n\n[![license](https://img.shields.io/github/license/mashape/apistatus.svg?maxAge=2592000)]\n\n* My coworkers and I created this free and open-source library. Anyone can now transfer entire clicks folders, tickets, and comments to AWS S3. When we switched from Clickup to JIRA software, this task was essentially done to backup the entire Clickup system. This threaded script employs the maximum number of threads to use ClickUP APIS and migrate all data into Data Lake S3 so that users or teams can use Athena to query using regular SQL if necessary.\n\n### Authors \n* Soumil Nitin Shah\n* April Love Ituhat  \n* Divyansh Patel\n\n#### NOTE:\n* This is migration script which will move all the data from ClickUp we will add  more features and methods in case if you want to just backup a given workspace or Folde. But For now this will Back up Everything feel free to add more functionality and submit Merge Request so other people can use the functionality. Note this is more than 500 Lines of code I will cleanup and add some amazing functionality during my free time\n\n\n----------------------------------------------------------------------------\n\n<img width=\"941\" alt=\"c3\" src=\"https://user-images.githubusercontent.com/39345855/197051580-514532f5-7161-4007-bdb1-b109dc3b355c.PNG\">\n\n\nclickup_to_s3_migration\n\n## Installation\n\n```bash\npip install clickup_to_s3_migration\n```\n## Usage\n\n```python\nimport sys\nfrom ClickUptoS3Migration import ClickUptoS3Migration\n\n\ndef main():\n    helper = ClickUptoS3Migration(\n        aws_access_key_id=\"<ACCESS KEY>\",\n        aws_secret_access_key=\"<SECRET KEY GOES HERE>\",\n        region_name=\"<AWS REGION>\",\n        bucket=\"<BUCKET NAME >\",\n        clickup_api_token=\"<CLICKUP API KEY>\"\n    )\n    ressponse = helper.run()\n\n\nmain()\n\n```\nItâ€™s really that easy this will iterate over all Workspaces For Each Workspaces it will call all Spaces and For Each Space it will get all Folders and For Each Folder it will Call List and For Each list call Tickets and Each Tickets call Comments\n\n------------------------------------------------------------------\n\n#### Explanation on the code works and Flow\n* Company can have several WorkSpace. The Code calls API and get all workspaces. Each work spaces can have several spaces and each spaces can have several folders and Each folder has many Lists and Each list has many tickets and Each tickets can have several Comments\n  ![image](https://user-images.githubusercontent.com/39345855/197052204-fa7a5509-e65e-4173-bc17-2b3db2ba5894.png)\n\n##### Figure Shows the structure how click up stores data and shows how we need to iterate and get all data\n\n# End Goal \n![image](https://user-images.githubusercontent.com/39345855/197052634-f4d0e059-9fa2-4477-b2b7-feed655ff498.png)\n\n* Once the Script is complete running which might take 1 or 2 days depending upon how much data you have \n* you can Then create glue crawler and then  Query the data using Athena (SQL Query)\n\n\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/soumilshah1995/clickup_to_s3_migration",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "clickup-to-s3-migration",
    "package_url": "https://pypi.org/project/clickup-to-s3-migration/",
    "platform": null,
    "project_url": "https://pypi.org/project/clickup-to-s3-migration/",
    "project_urls": {
      "Homepage": "https://github.com/soumilshah1995/clickup_to_s3_migration"
    },
    "release_url": "https://pypi.org/project/clickup-to-s3-migration/1.1.1/",
    "requires_dist": [
      "boto3",
      "tqdm"
    ],
    "requires_python": "",
    "summary": "My coworkers and I created this free and open-source library. Anyone can now transfer entire clicks folders, tickets, and comments to AWS S3. When we switched from Clickup to JIRA software, this task was essentially done to backup the entire Clickup system. This threaded script employs the maximum number of threads to use ClickUP APIS and migrate all data into Data Lake S3 so that users or teams can use Athena to query using regular SQL if necessary.",
    "version": "1.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15479830,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4d3a5d906640b55b3bf859292b4cbbb94db2cfb3a6f23941861390143b281b9a",
          "md5": "2f20ef195946c1d7a89db58a1e412164",
          "sha256": "835a00a7ad4b0bce53eb2e253f1ca7aa67e05f5332e37cdeb456b23197b96666"
        },
        "downloads": -1,
        "filename": "clickup_to_s3_migration-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2f20ef195946c1d7a89db58a1e412164",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18697,
        "upload_time": "2022-10-20T20:49:51",
        "upload_time_iso_8601": "2022-10-20T20:49:51.802010Z",
        "url": "https://files.pythonhosted.org/packages/4d/3a/5d906640b55b3bf859292b4cbbb94db2cfb3a6f23941861390143b281b9a/clickup_to_s3_migration-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "21d584e760439894453d51080c10a75399ebecdd5636b724cf77723b78866c4d",
          "md5": "fe6112a9f39ed90ac4d2a1093bb8ebb7",
          "sha256": "03293d40ccae969f85345209cfcd524d636e8637a9c94c740cfe002dbeb68cbf"
        },
        "downloads": -1,
        "filename": "clickup_to_s3_migration-1.1.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fe6112a9f39ed90ac4d2a1093bb8ebb7",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 18793,
        "upload_time": "2022-10-20T21:32:50",
        "upload_time_iso_8601": "2022-10-20T21:32:50.869950Z",
        "url": "https://files.pythonhosted.org/packages/21/d5/84e760439894453d51080c10a75399ebecdd5636b724cf77723b78866c4d/clickup_to_s3_migration-1.1.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "21d584e760439894453d51080c10a75399ebecdd5636b724cf77723b78866c4d",
        "md5": "fe6112a9f39ed90ac4d2a1093bb8ebb7",
        "sha256": "03293d40ccae969f85345209cfcd524d636e8637a9c94c740cfe002dbeb68cbf"
      },
      "downloads": -1,
      "filename": "clickup_to_s3_migration-1.1.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "fe6112a9f39ed90ac4d2a1093bb8ebb7",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 18793,
      "upload_time": "2022-10-20T21:32:50",
      "upload_time_iso_8601": "2022-10-20T21:32:50.869950Z",
      "url": "https://files.pythonhosted.org/packages/21/d5/84e760439894453d51080c10a75399ebecdd5636b724cf77723b78866c4d/clickup_to_s3_migration-1.1.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}