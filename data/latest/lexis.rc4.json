{
  "info": {
    "author": "Thor Whalen",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# lexis\nWordnet wrapper - Easy access to words and their relationships\n\nTo install:\t```pip install lexis```\n\nThe key-value (i.e. dict-list) wrapper to nltk.corpus.wordnet.\n\nYour no fuss gateway to (English) words.\n\nThe easiest way to get nltk.corpus.wordnet is\n\nThe `nltk` dependency is installed for you when installing \n`lexis`, but the wordnet data is not downloaded automatically.\nTo do so (only once), go to a python console and do:\n```\n>>> import nltk; nltk.download('wordnet')  # doctest: +SKIP\n```\n\nIf you don't like that way, [see here](https://www.nltk.org/install.html) \nfor other ways to get wordnet.\n\nThe central construct of this module is the Synset \n(a set of synonyms that share a common meaning).\nTo see a few things you can do with Synsets, naked, \n[see here](https://www.nltk.org/howto/wordnet.html).\n\nHere we put a py2store wrapper around this stuff.\n\nWhat is WordNet? https://wordnet.princeton.edu/\n\n\n# A little peek at Lemmas\n\n\n```python\nfrom lexis import Lemmas\n\nlm = Lemmas()\nlen(lm)\n```\n\n\n\n\n    147306\n\n\n\n`lm` is a `Mapping` (think \"acts like a (read-only) dict\")\n\n\n```python\nfrom typing import Mapping\n\nisinstance(lm, Mapping)\n```\n\n\n\n\n    True\n\n\n\nLet's have a look at a few keys\n\n\n```python\nlist(lm)[44630:44635]\n```\n\n\n\n\n    ['blond', 'kaunda', 'peacetime', 'intolerantly', \"'hood\"]\n\n\n\nAnd the value of a `lm` item?\n\n\n```python\nlm['blond']\n```\n\n\n\n\n    {'blond.n.01': WordnetElement('blond.n.01'),\n     'blond.n.02': WordnetElement('blond.n.02'),\n     'blond.a.01': WordnetElement('blond.a.01')}\n\n\n\nOkay, it looks like it's different meanings of \"blond\". The middle letter tells us its grammatical role it's a noun (`n`) or an adjective (`a`). More on that later. \n\nAnd what's a `WordnetElement`?\n\nWell, it's another Mapping, apparently:\n\n\n```python\nisinstance(lm['blond']['blond.n.01'], Mapping)\n```\n\n\n\n\n    True\n\n\n\n\n```python\nlist(lm['blond']['blond.n.01'])\n```\n\n\n\n\n    ['also_sees',\n     'instance_hypernyms',\n     'verb_groups',\n     'entailments',\n     'region_domains',\n     'substance_holonyms',\n     'part_holonyms',\n     'examples',\n     'part_meronyms',\n     'hyponyms',\n     'member_meronyms',\n     'offset',\n     'causes',\n     'definition',\n     'lemma_names',\n     'lexname',\n     'member_holonyms',\n     'in_topic_domains',\n     'lemmas',\n     'topic_domains',\n     'max_depth',\n     'hypernym_distances',\n     'name',\n     'attributes',\n     'hypernyms',\n     'min_depth',\n     'usage_domains',\n     'in_region_domains',\n     'instance_hyponyms',\n     'in_usage_domains',\n     'similar_tos',\n     'root_hypernyms',\n     'pos',\n     'frame_ids',\n     'hypernym_paths',\n     'substance_meronyms']\n\n\n\nWow! That's a lot of information! \n\nLet's look at what the definition of `'blond.n.01'` is:\n\n\n```python\nprint(lm['blond']['blond.n.01']['definition'])\n```\n\n    a person with fair skin and hair\n\n\n... actually, let's just poke at all of them (at least those that are non-empty)\n\n\n```python\nmeaning = 'blond.n.01'\nprint(f\"Values for meaning: {meaning}\")\nfor k, v in lm['blond'][meaning].items():\n    if v:\n        print(f\"- {k}: {v}\")\n```\n\n    Values for meaning: blond.n.01\n    - hyponyms: [WordnetElement('peroxide_blond.n.01'), WordnetElement('platinum_blond.n.01'), WordnetElement('towhead.n.01')]\n    - offset: 9860506\n    - definition: a person with fair skin and hair\n    - lemma_names: ['blond', 'blonde']\n    - lexname: noun.person\n    - lemmas: [KvLemma('blond.n.01.blond'), KvLemma('blond.n.01.blonde')]\n    - max_depth: 7\n    - hypernym_distances: {(WordnetElement('physical_entity.n.01'), 6), (WordnetElement('entity.n.01'), 7), (WordnetElement('physical_entity.n.01'), 3), (WordnetElement('entity.n.01'), 4), (WordnetElement('living_thing.n.01'), 3), (WordnetElement('object.n.01'), 5), (WordnetElement('blond.n.01'), 0), (WordnetElement('organism.n.01'), 2), (WordnetElement('causal_agent.n.01'), 2), (WordnetElement('whole.n.02'), 4), (WordnetElement('person.n.01'), 1)}\n    - name: blond.n.01\n    - hypernyms: [WordnetElement('person.n.01')]\n    - min_depth: 4\n    - root_hypernyms: [Synset('entity.n.01')]\n    - pos: n\n    - hypernym_paths: [[WordnetElement('entity.n.01'), WordnetElement('physical_entity.n.01'), WordnetElement('causal_agent.n.01'), WordnetElement('person.n.01'), WordnetElement('blond.n.01')], [WordnetElement('entity.n.01'), WordnetElement('physical_entity.n.01'), WordnetElement('object.n.01'), WordnetElement('whole.n.02'), WordnetElement('living_thing.n.01'), WordnetElement('organism.n.01'), WordnetElement('person.n.01'), WordnetElement('blond.n.01')]]\n\n\n## You can get meaning information directly\n\nWhat if you made a list of these strings like `'blond.n.01'`, `'blond.a.01'`... and you wanted to access the `WordnetElement` instances with all that cool information about those specifics meanings?\n\nYou could do `lm['blond']['blond.n.01']`, `lm['blond']['blond.a.01']`... But then you'd have to remember the full references `('blond', 'blond.n.01')`, `('blond', 'blond.a.01')`... \n\nYou don't need to go through `lm['blond']` to get to the `WordnetElement` instance that gives you access to the meaning information -- you can use the `Synsets` store (i.e. Mapping). \n\nNote: \"synset\" is what Wordnet calls this. We'll just call is meaning for simplicity. I hope the purists won't mind.\n\n\n\n```python\nfrom lexis import Synsets\n```\n\n\n```python\nmeanings = Synsets()\nmeanings['blond.n.01']\n```\n\n\n\n\n    WordnetElement('blond.n.01')\n\n\n\nWe saw earlier that we had `147306` lemmas (i.e. \"words\" or more precisely \"terms\"... but really precisely, \"lemmas\"). \n\nWell, we have `117659` synsets (i.e. \"meanings\") in the `Synsets` instance.\n\n\n```python\nlen(meanings)\n```\n\n\n\n\n    117659\n\n\n\n## Multiple lemma names\n\n`'lemma_names'` are different ways that the same meaning can be written. \n\n\n```python\nlm['blond']['blond.n.01']['lemma_names']\n```\n\n\n\n\n    ['blond', 'blonde']\n\n\n\nIndeed, `lm['blond']` and `lm['blonde']` really point to the same thing.\n\n\n```python\nlm['blond']\n```\n\n\n\n\n    {'blond.n.01': WordnetElement('blond.n.01'),\n     'blond.n.02': WordnetElement('blond.n.02'),\n     'blond.a.01': WordnetElement('blond.a.01')}\n\n\n\n\n```python\nlm['blonde']\n```\n\n\n\n\n    {'blond.n.01': WordnetElement('blond.n.01'),\n     'blond.n.02': WordnetElement('blond.n.02'),\n     'blond.a.01': WordnetElement('blond.a.01')}\n\n\n\n## Grammatical roles\n\nWhat are the different grammatical roles that are used in the meaning identifiers (aka synset keys) of our lemmas?\n\n\n```python\nfrom collections import Counter\nimport re\nfrom lexis import Lemmas\n\nlm = Lemmas()\n\np_middle_of_dot_path = re.compile('(?P<first>[^\\.]+)\\.(?P<middle>\\w+)\\.(?P<last>[^\\.]+)')\n\ndef extract_grammatical_role_from_meaning(meaning):\n    m = p_middle_of_dot_path.match(meaning)\n    if m:\n        return m.groupdict().get('middle', None) \n    else:\n        return None\n    \n\nc = Counter()\nfor meanings in lm.values():\n    for meaning in meanings:\n        c.update(extract_grammatical_role_from_meaning(meaning))\n        \nc.most_common()\n```\n\n\n\n\n    [('n', 148478),\n     ('v', 42751),\n     ('s', 20895),\n     ('a', 9846),\n     ('r', 5619),\n     ('_', 29),\n     ('e', 28),\n     ('u', 17),\n     ('g', 17),\n     ('i', 15),\n     ('t', 14),\n     ('p', 8),\n     ('b', 7),\n     ('o', 7),\n     ('l', 6),\n     ('d', 4),\n     ('c', 2),\n     ('m', 1),\n     ('k', 1)]\n\n\n\n\n# Miscellaneous explorations\n\n```python\nfrom py2store import filt_iter, cached_keys, add_ipython_key_completions\nfrom py2store import kvhead\nfrom lexis import Lemmas\n```\n\n\n```python\nlm = Lemmas()\n\ndef print_definitions(words):\n    for word in words:\n        print(f\"- {word}\")\n        for k, v in lm[word].items():\n            print(f\"    {'.'.join(k.split('.')[1:])}: {v['definition']}\")\n\n```\n\n## Find words containing some substring\n\n\n```python\nfrom lexis import print_word_definitions\n```\n\n\n```python\nsubstr = 'iep'\nwords = list(filter(lambda w: substr in w, lm))\nlen(words)\n```\n\n\n    12\n\n\n```python\nprint_definitions(words)\n```\n\n    - hemiepiphyte\n        n.01: a plant that is an epiphyte for part of its life\n    - antiepileptic\n        n.01: a drug used to treat or prevent convulsions (as in epilepsy)\n    - pieplant\n        n.01: long pinkish sour leafstalks usually eaten cooked and sweetened\n    - liepaja\n        n.01: a city of southwestern Latvia on the Baltic Sea\n    - semiepiphyte\n        n.01: a plant that is an epiphyte for part of its life\n    - archiepiscopal\n        a.01: of or associated with an archbishop\n    - tiepin\n        n.01: a pin used to hold the tie in place\n    - giovanni_battista_tiepolo\n        n.01: Italian painter (1696-1770)\n    - tiepolo\n        n.01: Italian painter (1696-1770)\n    - antiepileptic_drug\n        n.01: a drug used to treat or prevent convulsions (as in epilepsy)\n    - dnieper\n        n.01: a river that rises in Russia near Smolensk and flowing south through Belarus and Ukraine to empty into the Black Sea\n    - dnieper_river\n        n.01: a river that rises in Russia near Smolensk and flowing south through Belarus and Ukraine to empty into the Black Sea\n\n## Find palindrome\n\n```python\nimport re\nfrom lexis import Lemmas\n\nlm = Lemmas()\n\nis_palendrome_with_at_least_3_letters = lambda w: len(w) >= 3 and w == w[::-1]\nprint(*filter(is_palendrome_with_at_least_3_letters, lm), sep=', ')\n```\n\n    ono, waw, tot, kkk, ldl, anna, tenet, mom, igigi, sus, hallah, sls, pcp, mam, ofo, ene, alula, oto, civic, cfc, 101, tet, kazak, sss, ctc, aba, tevet, ara, wnw, mum, siris, tebet, tut-tut, ccc, naan, xix, tnt, peep, tut, kook, xanax, ala, eve, level, xxx, dud, aaa, dad, tdt, odo, pip, tibit, iii, sas, wow, radar, madam, yay, dmd, poop, ana, sos, bib, pop, isi, eye, gag, gig, cdc, dod, nun, pep, mym, bob, malayalam, sis, www, utu, non, ewe, aga, akka, noon, ese, rotor, ded, ppp, kayak, pap, wsw, pup, minim, nan, tat, ada, boob, mem, deed, nauruan, ma'am, succus, seles, cbc, tit, dvd, refer, toot\n\n\nWait a minute... Where's racecar?!? Isn't that a palindrome?\n```python\n# \nassert 'racecar' not in lm\nassert 'race_car' in lm\n```\n\n### Which of these are (or rather \"can be\") a verb?\n\nWhat are the keys of the lemmas? \n\nAnswer: Synset keys -- that is, an id that references a unit of meaning\n\n\n```python\n# what do are the values of the lemmas?\nlist(lm['eat'])\n```\n\n\n\n\n    ['eat.v.01',\n     'eat.v.02',\n     'feed.v.06',\n     'eat.v.04',\n     'consume.v.05',\n     'corrode.v.01']\n\n\n\nThat little `v` seems to be indicating that the meaning is... verbal?\n\nLet's make a function to grab that middle part of the dot path and use it to make a `is_a_verb` (more like \"can be a verb\"). \n\n\n```python\nfrom collections import Counter\nimport re\nfrom lexis import Lemmas\n\nlm = Lemmas()\n\np_middle_of_dot_path = re.compile('(?P<first>[^\\.]+)\\.(?P<middle>\\w+)\\.(?P<last>[^\\.]+)')\n\ndef _extract_middle(string):\n    m = p_middle_of_dot_path.match(string)\n    if m:\n        return m.groupdict().get('middle', None) \n    else:\n        return None\n    \ndef grammatical_roles(lemma):\n    return Counter(map(_extract_middle, lm[lemma]))\n    \n\nassert grammatical_roles('go') == Counter({'n': 4, 'v': 30, 'a': 1})  # the lemma \"go\" can be a verb, noun, or adjective\n\ndef is_a_verb(lemma):\n    return 'v' in grammatical_roles(lemma)\n    \nassert is_a_verb('go')\nassert not is_a_verb('chess')  # unlike go, chess cannot be used as a verb, apparently\n```\n\nPalendromes that are verbs\n\n\n```python\nlist(filter(lambda x: is_a_verb(x) and is_palendrome_with_at_least_3_letters(x), lm))\n```\n\n\n\n\n    ['tot',\n     'tut-tut',\n     'peep',\n     'tut',\n     'level',\n     'pip',\n     'wow',\n     'bib',\n     'pop',\n     'eye',\n     'gag',\n     'bob',\n     'kayak',\n     'pup',\n     'tat',\n     'boob',\n     'refer',\n     'toot']\n\n\n\n\n## Only p, q, b, d, and vowels\n\n\n```python\nimport re\nfrom lexis import Lemmas\n\nlm = Lemmas()\n\nconsonants = 'pqbd'\nvowels = 'aeiou'  # 'aeiouy'\nfilt = re.compile(f'[{vowels}{consonants}]+$').match  # the pattern\n\nwords = list(filter(lambda w: 2 <= len(w)  <= 7, # number of letters constraing\n                    filter(filt, lm)))  # filter for iep pattern\nlen(words)\n```\n\n\n\n\n    249\n\n\n\n\n```python\nprint(*words, sep='\\t')\n```\n\n    bod\taaa\tadd\tpoa\tpop\tbeaded\taqua\tpib\tedda\tdoob\tboa\tdoi\tpadded\tiodide\tbop\tedo\tbide\teb\tbai\tquid\tde\tade\tdaba\tpid\tbaba\tpaba\tbi\tabb\tbebop\tpa\tpoop\tpb\tdea\todo\tpope\tdad\tpup\tbode\tquad\tbb\tbe\tea\tepee\tbid\tpu\tpique\tiii\tpod\tbee\tpub\tddi\tid\tbaobab\tequid\tpadua\tpipidae\topaque\tpappa\tuppp\tuub\tqepiq\tbibbed\tadp\tada\tpied\taoudad\tqed\tpupa\tbedaub\tbd\tdba\tbopeep\toboe\tado\teq\tbpi\taid\tbud\tdodo\tabo\tqaeda\taa\tpad\tpapua\tbaa\tabode\tbad\tadad\tadapid\tpapaia\tdb\tbede\tai\tpo\tdoei\tpep\teib\tdubai\tepi\tboob\tuuq\tio\tbeep\tquip\tad\tbabu\tded\tia\tdud\tda\tqi\tpaid\tpeep\tdoodad\tbeda\teddo\tboo\tpadda\tipidae\tdeep\tdope\tied\tdoped\tdopa\tii\tiaea\tuda\tdd\tbaud\tdido\tebb\tepa\tbodied\tpap\ted\tpeba\tbed\taudio\tdeed\tidea\tapoidea\tbeau\tup\tpda\tiud\tip\tdiode\tbida\tpi\tapidae\tbead\todd\tod\tdia\tbaddie\tiaa\tape\tipo\tdod\tidp\tee\tie\tdaub\tduo\tboidae\tpoe\tabed\tadobe\tpea\tdude\tdo\taided\tobi\tido\tpipe\tpe\tdoe\taiai\tpd\tbaboo\tquipu\tpood\tpapio\tequidae\tiop\tqadi\tab\tdado\tdub\tadobo\tbap\tpei\tbaeda\tequip\tdupe\taqaba\tbob\tba\tdead\tdada\tadapa\tpee\topepe\tpob\tiou\tduad\tdoodia\tdab\taide\tpip\tdipped\tbubo\tpipa\tpoi\tapia\tode\tupupa\tiq\taba\tabbe\tedp\tedd\tpia\tdue\tpud\tob\taudad\tdp\tdeb\tpie\toed\tdie\tppp\tqueue\tpapa\tadieu\tbiped\tbabe\tida\tdubuque\tdip\tuup\teu\tipod\tbade\tau\tabide\tbib\tbedded\n\n\n\n```python\nprint_definitions(['adapa'])\n```\n\n    - adapa\n        n.01: a Babylonian demigod or first man (sometimes identified with Adam)\n\n\n### Containing i, e, p in that order, with other letters in between\n\n\n```python\nfilt = re.compile('\\w{0,2}i\\w{0,2}e\\w{0,2}p\\w{0,2}$').match  # The *i*e*p* pattern\n\nwords = list(filter(lambda w: len(w) <= 6, # no more than 6 letters\n                    filter(filt, lm)))  # filter for iep pattern\nprint_definitions(words)\n```\n\n\n\n\n    9\n\n\n\n\n```python\nprint(*words, sep=', ')\n```\n\n    tie_up, ginep, lineup, inept, pileup, tiepin, biceps, icecap, ice_up\n\n\n\n```python\nprint_definitions(words)\n```\n\n    - tie_up\n        v.01: secure with or as if with ropes\n        v.02: invest so as to make unavailable for other purposes\n        v.03: restrain from moving or operating normally\n        v.01: secure in or as if in a berth or dock\n        v.05: finish the last row\n    - ginep\n        n.01: tropical American tree bearing a small edible fruit with green leathery skin and sweet juicy translucent pulp\n    - lineup\n        n.01: (baseball) a list of batters in the order in which they will bat\n        n.02: a line of persons arranged by police for inspection or identification\n    - inept\n        s.04: not elegant or graceful in expression\n        s.02: generally incompetent and ineffectual\n        s.03: revealing lack of perceptiveness or judgment or finesse\n    - pileup\n        n.01: multiple collisions of vehicles\n    - tiepin\n        n.01: a pin used to hold the tie in place\n    - biceps\n        n.01: any skeletal muscle having two origins (but especially the muscle that flexes the forearm)\n    - icecap\n        n.01: a mass of ice and snow that permanently covers a large area of land (e.g., the polar regions or a mountain peak)\n    - ice_up\n        v.01: become covered with a layer of ice; of a surface such as a window\n\n\n\n## S-words\n\n\nWords that start with `s` but if you remove `s`, it's still a word.\n\n```python\nfrom lexis import Lemmas  # pip install py2store\nlm = Lemmas()\nswords = list(filter(lambda x: x.startswith('s') and x[1:] in lm, lm))  # one line!\n```\n\n\n```python\nprint(len(t))\nprint(*t[:40], sep=', ')\n```\n\n    711\n    softener, spock, scent, spark, sbe, stickweed, screaky, salt, salp, sec, strap, sliver, slack, swish, sebs, sarawak, scuttle, stripping, swell, stole, spine, space, scar, sass, sewer, spitting, serving, sew, stalk, smite, sniffy, stripe, slake, stone, slit, sea, shoe, sweeper, swear_off, swan\n\n\n\n```python\nfrom py2store import filt_iter, wrap_kvs, KvReader\nfrom lexis import Lemmas  # pip install py2store\nlm = Lemmas()\n\n@filt_iter(filt=lambda x: x.startswith('s') and x[1:] in lm)\nclass Swords(Lemmas):\n    def __getitem__(self, k):\n        v = super().__getitem__(k)\n        for kk, vv in v.items():\n            yield f\"    {'.'.join(kk.split('.')[1:])}: {vv['definition']}\"\n            \ns = Swords()\nlen(s)\n```\n\n\n\n\n    711\n\n\n\n\n```python\nk, v = s.head()\nlist(v)\n```\n\n\n\n\n    ['    n.01: a substance added to another to make it less hard']\n\n\n\n\n```python\nfrom itertools import islice\nfor k, v in islice(s.items(), 5):\n    print(f\"------------ {k} -------------\")\n    print(*v, sep='\\n')\n\n```\n\n    ------------ softener -------------\n        n.01: a substance added to another to make it less hard\n    ------------ spock -------------\n        n.01: United States pediatrician whose many books on child care influenced the upbringing of children around the world (1903-1998)\n    ------------ scent -------------\n        n.02: a distinctive odor that is pleasant\n        n.02: an odor left in passing by which a person or animal can be traced\n        n.01: any property detected by the olfactory system\n        v.01: cause to smell or be smelly\n        v.02: catch the scent of; get wind of\n        v.02: apply perfume to\n    ------------ spark -------------\n        n.01: a momentary flash of light\n        n.01: merriment expressed by a brightness or gleam or animation of countenance\n        n.05: electrical conduction through a gas in an applied electric field\n        n.04: a small but noticeable trace of some quality that might become stronger\n        n.05: Scottish writer of satirical novels (born in 1918)\n        n.06: a small fragment of a burning substance thrown out by burning material or by friction\n        v.04: put in motion or move to act\n        v.02: emit or produce sparks\n    ------------ sbe -------------\n        n.01: the compass point that is one point east of due south",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/thorwhalen/lexis",
    "keywords": "words,definitions,lexicon,wordnet,NLP,Natural Language Processing,text mining",
    "license": "apache-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "lexis",
    "package_url": "https://pypi.org/project/lexis/",
    "platform": "any",
    "project_url": "https://pypi.org/project/lexis/",
    "project_urls": {
      "Homepage": "https://github.com/thorwhalen/lexis"
    },
    "release_url": "https://pypi.org/project/lexis/0.1.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Wordnet wrapper - Easy access to words and their relationships",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15538190,
  "releases": {
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3b98826c3d1909c8bb916d528b6df082ec1cce55c9ece7149b412d8c1c230716",
          "md5": "e02f4be5fb5f786003e0a3ef54ad0e18",
          "sha256": "46c106ec99539d34aebd2de8bcec0cb45da3af61623e66ae71b529c6f2de5f54"
        },
        "downloads": -1,
        "filename": "lexis-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "e02f4be5fb5f786003e0a3ef54ad0e18",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10127,
        "upload_time": "2021-08-05T15:53:04",
        "upload_time_iso_8601": "2021-08-05T15:53:04.999680Z",
        "url": "https://files.pythonhosted.org/packages/3b/98/826c3d1909c8bb916d528b6df082ec1cce55c9ece7149b412d8c1c230716/lexis-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1b9579be5224f0738cd4db35c8ad361b1e2d9afe8559d00ec9908ec566cbb32d",
          "md5": "46d0882a00c332d102157f6d2a4ed506",
          "sha256": "9472f1d6fad0681fe612555722d2f49826aa27cacfa7da0397b2ab9ed1e022c8"
        },
        "downloads": -1,
        "filename": "lexis-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "46d0882a00c332d102157f6d2a4ed506",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 6046,
        "upload_time": "2021-08-05T15:53:06",
        "upload_time_iso_8601": "2021-08-05T15:53:06.248187Z",
        "url": "https://files.pythonhosted.org/packages/1b/95/79be5224f0738cd4db35c8ad361b1e2d9afe8559d00ec9908ec566cbb32d/lexis-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f4f3bb82aafbe29e9e1bb39c0f0b1ad417cd68dddca83594926fba735154cf6f",
          "md5": "5c40bdeea412712657909eec719d1715",
          "sha256": "e051f1b3af2fca4c9e277be4789a269cee9b3f60123ee809d136ca31bd6a46f7"
        },
        "downloads": -1,
        "filename": "lexis-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5c40bdeea412712657909eec719d1715",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 16669,
        "upload_time": "2021-08-07T15:34:55",
        "upload_time_iso_8601": "2021-08-07T15:34:55.900027Z",
        "url": "https://files.pythonhosted.org/packages/f4/f3/bb82aafbe29e9e1bb39c0f0b1ad417cd68dddca83594926fba735154cf6f/lexis-0.0.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3aa72c320113ab4f78636914ad2e7677ede028b224bfd812118aef9a542c287d",
          "md5": "ed313274abbd857f7e8f0bab87afd390",
          "sha256": "19aa600fbd04b2ba21e14cdec89a9b55694b2b5da2d150704edd90d48c55de5e"
        },
        "downloads": -1,
        "filename": "lexis-0.0.3.tar.gz",
        "has_sig": false,
        "md5_digest": "ed313274abbd857f7e8f0bab87afd390",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 20173,
        "upload_time": "2021-08-07T15:34:57",
        "upload_time_iso_8601": "2021-08-07T15:34:57.468670Z",
        "url": "https://files.pythonhosted.org/packages/3a/a7/2c320113ab4f78636914ad2e7677ede028b224bfd812118aef9a542c287d/lexis-0.0.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "773ea23a6be5dbc3a45ed61262072d7d6ccb9750605e44910e74b4e38caa6753",
          "md5": "12a39fa79923eab8f48cacb7ca1c7007",
          "sha256": "8a576716b38855a2ba8ffdec0f67df4cc8f1b011319e2d1aa9b54c40bfcd1ede"
        },
        "downloads": -1,
        "filename": "lexis-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "12a39fa79923eab8f48cacb7ca1c7007",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23973,
        "upload_time": "2022-09-27T12:17:48",
        "upload_time_iso_8601": "2022-09-27T12:17:48.357844Z",
        "url": "https://files.pythonhosted.org/packages/77/3e/a23a6be5dbc3a45ed61262072d7d6ccb9750605e44910e74b4e38caa6753/lexis-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f8fd0b3c45aafc16c9d96c64b1cc65316700666ed35656894a520042e1885d72",
          "md5": "3ad32f0ad2924bc7c1e5b4c33d387083",
          "sha256": "d97c012074f974c47e2717b7331813002a40af270f9f0c47b11dfcf7499728bc"
        },
        "downloads": -1,
        "filename": "lexis-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3ad32f0ad2924bc7c1e5b4c33d387083",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 23973,
        "upload_time": "2022-10-26T14:21:43",
        "upload_time_iso_8601": "2022-10-26T14:21:43.454048Z",
        "url": "https://files.pythonhosted.org/packages/f8/fd/0b3c45aafc16c9d96c64b1cc65316700666ed35656894a520042e1885d72/lexis-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f8fd0b3c45aafc16c9d96c64b1cc65316700666ed35656894a520042e1885d72",
        "md5": "3ad32f0ad2924bc7c1e5b4c33d387083",
        "sha256": "d97c012074f974c47e2717b7331813002a40af270f9f0c47b11dfcf7499728bc"
      },
      "downloads": -1,
      "filename": "lexis-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "3ad32f0ad2924bc7c1e5b4c33d387083",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 23973,
      "upload_time": "2022-10-26T14:21:43",
      "upload_time_iso_8601": "2022-10-26T14:21:43.454048Z",
      "url": "https://files.pythonhosted.org/packages/f8/fd/0b3c45aafc16c9d96c64b1cc65316700666ed35656894a520042e1885d72/lexis-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}