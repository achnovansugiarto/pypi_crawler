{
  "info": {
    "author": "Gaetan De Waele",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "<div align=\"center\">\n<h1>bio-attention</h1>\n\nSimple implementations of attention modules adapted for the biological data domain.\n\n\n[![PyPi Version](https://img.shields.io/pypi/v/bio-attention.svg)](https://pypi.python.org/pypi/bio-attention/)\n[![GitHub license](https://img.shields.io/github/license/gdewael/bio-attention)](https://github.com/gdewael/bio-attention/blob/main/LICENSE)\n\n</div>\n\n\n\n## Install\nSince PyTorch is a dependency of `h5torch`, we recommend [installing PyTorch](https://pytorch.org/get-started/locally/) independently first, as your system may require a specific version (e.g. CUDA drivers).\n\nAfter PyTorch installation, `h5torch` can be installed using `pip`\n```bash\npip install bio-attention\n```\n\n## Usage\n\n## Package roadmap\n- [x] Implement typing\n\n\n# LEGACY documentation\n## THIS REPO USED TO BE A 2D SLIDING WINDOW ATTENTION REPO\n\n### 2D Sliding Window Attention\n\n<img src=\"./bio-attention/img/2Dslidingwindow-attention.png\" width=\"750\">\n\nStand-alone PyTorch implementation of 2D sliding window attention. Introduced by and part of CpG Transformer located at this [repo](https://github.com/gdewael/cpg-transformer) and detailed in our [preprint paper](https://www.biorxiv.org/content/10.1101/2021.06.08.447547v1).\n\n### Contents\n\n`sliding_window_attn.py` contains three PyTorch modules: `RelPositionalWindowEmbedding`, `MultiDimWindowAttention`, and `MultiDimWindowTransformerLayer`. The modules have been programmed in a way so that they can be used to do 1D sliding window attention, as well as >= 2-dimensional sliding window attention. In the multidimensional case, sliding window attention is applied over the first dimension following the batch dimension and full self-attention is applied over all the others.\n\nSliding windows are efficiently obtained using the `unfold` operation.\n\nPositional embeddings are relative sinusoidal ones as described in [Transformer-XL](https://arxiv.org/abs/1901.02860). Note that positional encodings are applied for the dimension in which sliding windows are applied. To inform the model of position in other dimensions, this should be encoded in the input itself.\n\n### Usage\n\n```python\n\nfrom sliding_window_attn import MultiDimWindowTransformerLayer\n\n# one layer:\nlayer = MultiDimWindowTransformerLayer(\n    hidden_dim=64,     # number of input & output hidden dimensions (int)\n    head_dim=8,        # hidden dimensionality of each SA head (int)\n    n_head=8,          # number of SA heads (int)\n    ff_dim=256,        # number of feed-forward hidden dimensions (int)\n    window=21,         # window size of sliding window, should be odd. (int) (default=21)\n    dropout=0.20,      # dropout rate on the self-attention matrix (float) (default=0.20)\n    activation='relu', # activation used in feed-forward, either 'relu' or 'gelu' (str) (default='relu')\n    layernorm=True     # whether to apply layernorm after attn+res and ff+res (bool) (default=True)\n)\n\n# model consisting of 4 layers:\nmodel = nn.Sequential([MultiDimWindowTransformerLayer(64, 8, 8, 256),\n                       MultiDimWindowTransformerLayer(64, 8, 8, 256),\n                       MultiDimWindowTransformerLayer(64, 8, 8, 256),\n                       MultiDimWindowTransformerLayer(64, 8, 8, 256)])\n\n\n\n# 2D sequence input:\n# batch size = 1\n# sequence dim1 length = 512 (sliding window SA)\n# sequence dim2 length = 4 (full SA)\n# hidden = 64\nx = torch.randn(1, 512, 4, 64)\npos = torch.cumsum(torch.randint(1, 7, (1, 512)), 1)\n# if all positional indices follow on eachother by one: pos = torch.arange(512).unsqueeze(0)\n\nx, pos = model((x, pos))\n```\n\nThe same model can also be used for 1D sequence inputs:\n```python\n# batch size = 1\n# sequence dim1 length = 512 (sliding window SA)\n# hidden = 64\nx = torch.randn(1, 512, 64)\npos = torch.cumsum(torch.randint(1, 7, (1, 512)), 1)\n\nx, pos = model((x, pos))\n```\n\n\nOr even 3D (or more) sequence input:\n```python\n# batch size = 1\n# sequence dim1 length = 512 (sliding window SA)\n# sequence dim2 length = 4 (full SA)\n# sequence dim3 length = 3 (full SA)\n# hidden = 64\nx = torch.randn(1, 512, 4, 3, 64)\npos = torch.cumsum(torch.randint(1, 7, (1, 512)), 1)\n\nx, pos = model((x, pos))\n\n```\n\nNote that computational complexity will scale quadratically with each added dimension.\nFor example: the attention matrix (per head) for the above 1D example is: `512 * 21`.\nFor the 2D example this becomes: `(512*4) * (21*4)`.\nAnd for the 3D example: `(512*4*3) * (21*4*3)`.\n\n## Citation\n\nIf you find this repository useful in your research, please cite our [paper](https://www.biorxiv.org/content/10.1101/2021.06.08.447547v1).\n```bibtex\n@article{dewaele2021cpg,\n\tauthor = {Gaetan De Waele and Jim Clauwaert and Gerben Menschaert and Willem Waegeman},\n\ttitle = {CpG Transformer for imputation of single-cell methylomes},\n\tyear = {2021},\n\tdoi = {10.1101/2021.06.08.447547},\n\tURL = {https://www.biorxiv.org/content/early/2021/06/09/2021.06.08.447547}\n}\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/gdewael/bio-attention",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "bio-attention",
    "package_url": "https://pypi.org/project/bio-attention/",
    "platform": null,
    "project_url": "https://pypi.org/project/bio-attention/",
    "project_urls": {
      "Homepage": "https://github.com/gdewael/bio-attention"
    },
    "release_url": "https://pypi.org/project/bio-attention/0.0.1/",
    "requires_dist": [
      "torch"
    ],
    "requires_python": ">=3.8",
    "summary": "Simple implementations of attention modules adapted for the biological data domain",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16009128,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ba6c0392a572e5ab0cc99dee1b0a43f4451df328aa42f9f2b5240b922999309b",
          "md5": "ebdb5437d4f21da49995ccf995bcb80e",
          "sha256": "3a5a9735b66395b90f0007e00a2b6317fbb643a3c5801e99b9436f45057f64d1"
        },
        "downloads": -1,
        "filename": "bio_attention-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ebdb5437d4f21da49995ccf995bcb80e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 7344,
        "upload_time": "2022-12-06T14:51:38",
        "upload_time_iso_8601": "2022-12-06T14:51:38.778485Z",
        "url": "https://files.pythonhosted.org/packages/ba/6c/0392a572e5ab0cc99dee1b0a43f4451df328aa42f9f2b5240b922999309b/bio_attention-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3aa46bee18dbacf63084de05baed42ca930ac4c71306a3e811d5d9e4b0e88766",
          "md5": "d1a149ce1eb3295aba4000324d0d6286",
          "sha256": "220d11cf8ebca1a37ae45b14a970c46d2eebd23465a2c2775aa436797a6cf6de"
        },
        "downloads": -1,
        "filename": "bio-attention-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "d1a149ce1eb3295aba4000324d0d6286",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 2201993,
        "upload_time": "2022-12-06T14:51:40",
        "upload_time_iso_8601": "2022-12-06T14:51:40.733542Z",
        "url": "https://files.pythonhosted.org/packages/3a/a4/6bee18dbacf63084de05baed42ca930ac4c71306a3e811d5d9e4b0e88766/bio-attention-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ba6c0392a572e5ab0cc99dee1b0a43f4451df328aa42f9f2b5240b922999309b",
        "md5": "ebdb5437d4f21da49995ccf995bcb80e",
        "sha256": "3a5a9735b66395b90f0007e00a2b6317fbb643a3c5801e99b9436f45057f64d1"
      },
      "downloads": -1,
      "filename": "bio_attention-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ebdb5437d4f21da49995ccf995bcb80e",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 7344,
      "upload_time": "2022-12-06T14:51:38",
      "upload_time_iso_8601": "2022-12-06T14:51:38.778485Z",
      "url": "https://files.pythonhosted.org/packages/ba/6c/0392a572e5ab0cc99dee1b0a43f4451df328aa42f9f2b5240b922999309b/bio_attention-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3aa46bee18dbacf63084de05baed42ca930ac4c71306a3e811d5d9e4b0e88766",
        "md5": "d1a149ce1eb3295aba4000324d0d6286",
        "sha256": "220d11cf8ebca1a37ae45b14a970c46d2eebd23465a2c2775aa436797a6cf6de"
      },
      "downloads": -1,
      "filename": "bio-attention-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "d1a149ce1eb3295aba4000324d0d6286",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 2201993,
      "upload_time": "2022-12-06T14:51:40",
      "upload_time_iso_8601": "2022-12-06T14:51:40.733542Z",
      "url": "https://files.pythonhosted.org/packages/3a/a4/6bee18dbacf63084de05baed42ca930ac4c71306a3e811d5d9e4b0e88766/bio-attention-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}