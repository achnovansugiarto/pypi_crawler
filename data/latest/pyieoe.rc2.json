{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence"
    ],
    "description": "# pyIEOE: Towards An Interpretable Evaluation for Offline Evaluation\n\n## Background\nIn the midst of growing interest in off-policy evaluation (OPE), the research community has produced a number of OPE estimators. One emerging challenge with this trend is that there is a need for practitioners to select and tune appropriate hyper-parameters of OPE estimators for their specific application. However, most OPE papers evaluate the estimator's performance for a single set of arbitrary hyper-parameters and a single arbitrary evaluation policy. Therefore, it is difficult for practitioners to evaluate the estimator's sensitivity to hyper-parameter choices or evaluation policy choices, which is critical in real-world scenarios. Consequently, this type of evaluation procedure fails to answer the following question: *which OPE estimator provides an accurate and reliable off-policy evaluation without environment specific hyper-parameter tuning?*\n\n## Overview\nTowards a reliable offline evaluation, we develop a **interpretable evaluation procedure for OPE methods** that quantifies their sensitivity to hyper-parameter choices and/or evaluation policy choices. Our proposed evaluation procedure is summarized in the figure below:\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/sony/pyIEOE/master/images/algorithm1.png\" width=\"450\"/></div>\n\nTo evaluate the performance of a single OPE estimator, we prepare a set of candidates for hyper-parameters, evaluation policies, and random seeds, respectively. For each random seed, we (uniformly) randomly choose hyper-parameters and an evaluation policy. Furthermore, we randomly sample log data with replacement via the bootstrap method. Then, we conduct OPE using the specified OPE estimator and obtain a performance measure. We can use the aggregated performance measure to evaluate the performance of the OPE estimator. In particular, we can estimate the cumulative distribution function (CDF) of the performance measure. By repeating this process for each OPE estimator, we can evaluate and compare the performance of the OPE estimators.\n\n## Installation\nYou can install pyIEOE using Python's package manager pip.\n```bash\npip install pyieoe\n```\n\nYou can also install pyIEOE from source.\n```bash\ngit clone https://github.com/sony/pyIEOE\ncd pyIEOE\npython setup.py install\n```\n\n## Usage\nWe provide a Python package that allows practitioners to easily execute the proposed robust and interpretable evaluation procedure for OPE methods. For example with actual code, please look at [`./examples/`](./examples/). (Note: this package is built with the intention of being used along with the [Open Bandit Pipeline (obp)](https://github.com/st-tech/zr-obp))\n\n### (1) Prepare Dataset and Evaluation Policies\nBefore we use pyIEOE, we need to prepare logged bandit feedback data and evaluation policies (action distributions and ground truth policy value for each action distribution). This can be done by using the dataset module of obp as follows.\n```python\n# here we show an example using SyntheticBanditDataset from obp\nfrom obp.dataset import (\n    SyntheticBanditDataset,\n    logistic_reward_function,\n    linear_behavior_policy\n)\n\n# initialize SyntheticBanditDataset class\ndataset = SyntheticBanditDataset(\n    n_actions=10,\n    dim_context=5,\n    reward_type=\"binary\", # \"binary\" or \"continuous\"\n    reward_function=logistic_reward_function,\n    behavior_policy_function=linear_behavior_policy,\n    random_state=12345,\n)\n# obtain synthetic logged bandit feedback data\nn_rounds = 10000\nbandit_feedback = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds)\n\n# prepare action distribution and ground truth policy value for each evaluation policy\naction_dist_a = #...\nground_truth_a = #...\naction_dist_b = #...\nground_truth_b = #...\n```\n\n### (2) Define Hyperparameter Spaces\nThen, we define hyperparameter spaces for OPE estimators and regression models. (Note: we can also set this after initializing the InterpretableOPEEvaluator class.)\n```python\n# hyperparameter space for ope estimators\nlambda_ = {\n    \"lower\": 1e-3,\n    \"upper\": 1e2,\n    \"log\": True,\n    \"type\": float\n}\nK = {\n    \"lower\": 1,\n    \"upper\": 5,\n    \"log\": False,\n    \"type\": int\n}\ndros_param = {\"lambda_\": lambda_, \"K\": K}\nsndr_param = {\"K\": K}\n\n# hyperparameter space for regression model\nC = {\n    \"lower\": 1e-3,\n    \"upper\": 1e2,\n    \"log\": True,\n    \"type\": float\n}\nn_estimators = {\n    \"lower\": 20,\n    \"upper\": 200,\n    \"log\": True,\n    \"type\": int\n}\nlr_hp = {\"C\": C}\nrf_hp = {\"n_estimators\": n_estimators}\n```\n\n### (3) Interpretable OPE Evaluation\nThen, we initialize the `InterpretableOPEEvaluator` class.\n```python\n# import InterpretableOPEEvaluator\nfrom pyieoe.evaluator import InterpretableOPEEvaluator\n\n# import other necessary packages\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier as RandomForest\nfrom obp.ope import (\n    DoublyRobustWithShrinkage,\n    SelfNormalizedDoublyRobust,\n)\n\n# initialize InterpretableOPEEvaluator class\nevaluator = InterpretableOPEEvaluator(\n    random_states=np.arange(1000),\n    bandit_feedbacks=[bandit_feedback],\n    evaluation_policies=[\n        (ground_truth_a, action_dist_a),\n        (ground_truth_b, action_dist_b),\n    ],\n    ope_estimators=[\n        DoublyRobustWithShrinkage(),\n        SelfNormalizedDoublyRobust(),\n    ],\n    regression_models=[\n        LogisticRegression,\n        RandomForest,\n    ],\n    regression_model_hyperparams={\n        LogisticRegression: lr_hp,\n        RandomForest: rf_hp,\n    },\n    ope_estimator_hyperparams={\n        DoublyRobustWithShrinkage.estimator_name: dros_param,\n        SelfNormalizedDoublyRobust.estimator_name: sndr_param,\n    }\n)\n```\n\nWe can now perform the interpretable OPE evaluation by calling built in methods.\n```bash\n# estimate policy values\npolicy_value = evaluator.estimate_policy_value()\n\n# compute squared errors\nse = evaluator.calculate_squared_error()\n```\nWe can visualize the results as well.\n```bash\n# visualize CDF of squared errors for each OPE estimator\nevaluator.visualize_cdf(fig_dir=\"figures\", fig_name=\"cdf.png\")\n```\n\nAfter calling `InterpretableOPEEvaluator().visualize_cdf()`, we can expect to obtain figures similar to the ones shown below:\n\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/sony/pyIEOE/master/images/sndr_cdf.png\" alt=\"IPW CDF\" width=\"350\"/><img src=\"https://raw.githubusercontent.com/sony/pyIEOE/master/images/dr-os_cdf.png\" alt=\"IPW CDF\" width=\"350\"/></div>\n\nIn this example, we evaluate and compare Self Normalized Doubly Robust (SNDR) and Doubly Robust with Shrinkage (DRos). Each of these figures visualizes the CDF of the squared error for an OPE estimator. The y-axis represents the cumulative probability, and the x-axis represents the squared error. For instance, if we look at the figure for SNDR, we can tell that with a probability of 80%, the squared error of the SNDR estimator is less than 0.0002. On the other hand, if we look at the figure for DRos, we can tell that with a probability of 80%, the squared error of the DRos estimator is less than 0.0003.\n\nTo compare the estimators, we can run the following:\n```bash\n# visualize CDF of squared errors for each OPE estimator\nevaluator.visualize_cdf_aggregate(fig_dir=\"figures\", fig_name=\"cdf.png\")\n```\nAfter calling `InterpretableOPEEvaluator().visualize_cdf_aggregate()`, we can expect to obtain figures similar to the one shown below:\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/sony/pyIEOE/master/images/cdf.png\" width=\"350\"/></div>\n\nThis figure plots the CDFs of all OPE estimators. **This allows for an easy comparison between the OPE estimators. In particular, these figures suggests that the SNDR estimator outperforms the DRos estimator.**\n\n## Citation\nIf you use our package in your work, please cite our paper:\n\nBibtex:\n```\n@inproceedings{saito2021evaluating,\n  title={Evaluating the Robustness of Off-Policy Evaluation},\n  author={Saito, Yuta and Udagawa, Takuma and Kiyohara, Haruka and Mogi, Kazuki and Narita, Yusuke and Tateno, Kei},\n  booktitle = {Proceedings of the 15th ACM Conference on Recommender Systems},\n  pages={xxx},\n  year={2021}\n}\n```\n\n## Contributors\n- Kazuki Mogi (Stanford University / Hanjuku-kaso Co., Ltd.)\n- [Haruka Kiyohara](https://sites.google.com/view/harukakiyohara) (Tokyo Institute of Technology / Hanjuku-kaso Co., Ltd.)\n- [Yuta Saito](https://usaito.github.io/) (Cornell University / Hanjuku-kaso Co., Ltd.)\n\n## Project Team\n- Yuta Saito (Cornell University / Hanjuku-kaso Co., Ltd.)\n- Takuma Udagawa (Sony Group Corporation)\n- Haruka Kiyohara (Tokyo Institute of Technology / Hanjuku-kaso Co., Ltd.)\n- Kazuki Mogi (Stanford University / Hanjuku-kaso Co., Ltd.)\n- Yusuke Narita (Hanjuku-kaso Co., Ltd. / Yale University)\n- Tateno Kei (Sony Group Corporation)\n\n## Contact\nFor any question about the paper and package, feel free to contact: saito@hanjuku-kaso.com\n\n\n## Licence\nThis software is released under the MIT License, see LICENSE for the detail.",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/sony/pyIEOE",
    "keywords": "off-policy evaluation",
    "license": "MIT License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "pyieoe",
    "package_url": "https://pypi.org/project/pyieoe/",
    "platform": "",
    "project_url": "https://pypi.org/project/pyieoe/",
    "project_urls": {
      "Homepage": "https://github.com/sony/pyIEOE"
    },
    "release_url": "https://pypi.org/project/pyieoe/0.1.1/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "pyIEOE: a Python package to facilitate interpretable OPE evaluation",
    "version": "0.1.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11331013,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c0f87633cfde5f48a53fec55078bb0c13f19ff414a0f6d348d4e3fc9ffbe9e52",
          "md5": "57e1618b246b3791fb7942ed90de9ed1",
          "sha256": "a9f2f4b88c80b6dd9bfada8d9b79b250e33ceabb0fe713c385958a65962a5559"
        },
        "downloads": -1,
        "filename": "pyieoe-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "57e1618b246b3791fb7942ed90de9ed1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14721,
        "upload_time": "2021-08-20T05:00:38",
        "upload_time_iso_8601": "2021-08-20T05:00:38.833988Z",
        "url": "https://files.pythonhosted.org/packages/c0/f8/7633cfde5f48a53fec55078bb0c13f19ff414a0f6d348d4e3fc9ffbe9e52/pyieoe-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ee5e6789c133597ed370ebf870db1f6aa798fd712eba617e3898d0f99f590b98",
          "md5": "92109d6930d08b62c9f86dfbbd8fbc85",
          "sha256": "679bee6e437cbba7bae9462a5d52d7d1a71bc7568aa4f80eec0add1dc3126b78"
        },
        "downloads": -1,
        "filename": "pyieoe-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "92109d6930d08b62c9f86dfbbd8fbc85",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 15499,
        "upload_time": "2021-09-01T00:52:57",
        "upload_time_iso_8601": "2021-09-01T00:52:57.162548Z",
        "url": "https://files.pythonhosted.org/packages/ee/5e/6789c133597ed370ebf870db1f6aa798fd712eba617e3898d0f99f590b98/pyieoe-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ee5e6789c133597ed370ebf870db1f6aa798fd712eba617e3898d0f99f590b98",
        "md5": "92109d6930d08b62c9f86dfbbd8fbc85",
        "sha256": "679bee6e437cbba7bae9462a5d52d7d1a71bc7568aa4f80eec0add1dc3126b78"
      },
      "downloads": -1,
      "filename": "pyieoe-0.1.1.tar.gz",
      "has_sig": false,
      "md5_digest": "92109d6930d08b62c9f86dfbbd8fbc85",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 15499,
      "upload_time": "2021-09-01T00:52:57",
      "upload_time_iso_8601": "2021-09-01T00:52:57.162548Z",
      "url": "https://files.pythonhosted.org/packages/ee/5e/6789c133597ed370ebf870db1f6aa798fd712eba617e3898d0f99f590b98/pyieoe-0.1.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}