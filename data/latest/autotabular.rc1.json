{
  "info": {
    "author": "Jianzh Nie",
    "author_email": "jianzhnie@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Environment :: Console",
      "Intended Audience :: Developers",
      "Intended Audience :: Education",
      "Intended Audience :: Information Technology",
      "Intended Audience :: Science/Research",
      "Natural Language :: English",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Topic :: Scientific/Engineering :: Artificial Intelligence",
      "Topic :: Scientific/Engineering :: Information Analysis"
    ],
    "description": "# AutoTabular\n\n[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://www.nature.com/articles/nature14539)\n[![Conference](http://img.shields.io/badge/NeurIPS-2019-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)\n[![Conference](http://img.shields.io/badge/ICLR-2019-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)\n[![Conference](http://img.shields.io/badge/AnyConference-year-4b44ce.svg)](https://papers.nips.cc/book/advances-in-neural-information-processing-systems-31-2018)\n\n\nAutoTabular automates machine learning tasks enabling you to easily achieve strong predictive performance in your applications.  With just a few lines of code, you can train and deploy high-accuracy machine learning and deep learning models tabular data.\n\n![autotabular](./docs/autotabular.png)\n\n[Toc]\n## What's good in it?\n\n- It is using the RAPIDS as back-end support, gives you the ability to execute end-to-end data science and analytics pipelines entirely on GPUs.\n- It Supports many anomaly detection models: ,\n- It using meta learning to accelerate  model selection and parameter tuning.\n- It is using many Deep Learning models for tabular data: `Wide&Deep`,  `DCN(Deep & Cross Network)`, `FM`, `DeepFM`, `PNN` ...\n- It is using many machine learning algorithms: `Baseline`, `Linear`, `Random Forest`, `Extra Trees`, `LightGBM`, `Xgboost`, `CatBoost`, and `Nearest Neighbors`.\n- It can compute Ensemble based on greedy algorithm from [Caruana paper](http://www.cs.cornell.edu/~alexn/papers/shotgun.icml04.revised.rev2.pdf).\n- It can stack models to build level 2 ensemble (available in `Compete` mode or after setting `stack_models` parameter).\n- It can do features preprocessing, like: missing values imputation and converting categoricals. What is more, it can also handle target values preprocessing.\n- It can do advanced features engineering, like: [Golden Features](https://supervised.mljar.com/features/golden_features/), [Features Selection](https://supervised.mljar.com/features/features_selection/), Text and Time Transformations.\n- It can tune hyper-parameters with `not-so-random-search` algorithm (random-search over defined set of values) and hill climbing to fine-tune final models.\n\n##  Installation\n\nThe sources for AutoTabular can be downloaded from the `Github repo`.\n\nYou can either clone the public repository:\n\n```bash\n# clone project\ngit clone https://apulis-gitlab.apulis.cn/apulis/AutoTabular/autotabular.git\n# First, install dependencies\npip install -r requirements.txt\n```\n\nOnce you have a copy of the source, you can install it with:\n\n```bash\npython setup.py install\n```\n## Example\nNext, navigate to any file and run it.\n```bash\n# module folder\ncd example\n\n# run module (example: mnist as your main contribution)\npython binary_classifier_Titanic.py\n```\n\n### Auto Feature generate & Selection\n```python\nTODO\n```\n### Deep Feature Synthesis\n```python\nimport featuretools as ft\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# Load data and put into dataframe\niris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['species'] = iris.target\ndf['species'] = df['species'].map({\n    0: 'setosa',\n    1: 'versicolor',\n    2: 'virginica'\n})\n# Make an entityset and add the entity\nes = ft.EntitySet()\nes.add_dataframe(\n    dataframe_name='data', dataframe=df, make_index=True, index='index')\n# Run deep feature synthesis with transformation primitives\nfeature_matrix, feature_defs = ft.dfs(\n    entityset=es,\n    max_depth=3,\n    target_dataframe_name='data',\n    agg_primitives=['mode', 'mean', 'max', 'count'],\n    trans_primitives=[\n        'add_numeric', 'multiply_numeric', 'cum_min', 'cum_mean', 'cum_max'\n    ],\n    groupby_trans_primitives=['cum_sum'])\n\nprint(feature_defs)\nprint(feature_matrix.head())\nprint(feature_matrix.ww)\n```\n### GBDT Feature Generate\n```python\nfrom autofe.feature_engineering.gbdt_feature import CatboostFeatureTransformer, GBDTFeatureTransformer, LightGBMFeatureTransformer, XGBoostFeatureTransformer\n\ntitanic = pd.read_csv('autotabular/datasets/data/Titanic.csv')\n# 'Embarked' is stored as letters, so fit a label encoder to the train set to use in the loop\nembarked_encoder = LabelEncoder()\nembarked_encoder.fit(titanic['Embarked'].fillna('Null'))\n# Record anyone travelling alone\ntitanic['Alone'] = (titanic['SibSp'] == 0) & (titanic['Parch'] == 0)\n# Transform 'Embarked'\ntitanic['Embarked'].fillna('Null', inplace=True)\ntitanic['Embarked'] = embarked_encoder.transform(titanic['Embarked'])\n# Transform 'Sex'\ntitanic.loc[titanic['Sex'] == 'female', 'Sex'] = 0\ntitanic.loc[titanic['Sex'] == 'male', 'Sex'] = 1\ntitanic['Sex'] = titanic['Sex'].astype('int8')\n# Drop features that seem unusable. Save passenger ids if test\ntitanic.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n\ntrainMeans = titanic.groupby(['Pclass', 'Sex'])['Age'].mean()\n\ndef f(x):\n    if not np.isnan(x['Age']):  # not NaN\n        return x['Age']\n    return trainMeans[x['Pclass'], x['Sex']]\n\ntitanic['Age'] = titanic.apply(f, axis=1)\nrows = titanic.shape[0]\nn_train = int(rows * 0.77)\ntrain_data = titanic[:n_train, :]\ntest_data = titanic[n_train:, :]\n\nX_train = titanic.drop(['Survived'], axis=1)\ny_train = titanic['Survived']\n\nclf = XGBoostFeatureTransformer(task='classification')\nclf.fit(X_train, y_train)\nresult = clf.concate_transform(X_train)\nprint(result)\n\nclf = LightGBMFeatureTransformer(task='classification')\nclf.fit(X_train, y_train)\nresult = clf.concate_transform(X_train)\nprint(result)\n\nclf = GBDTFeatureTransformer(task='classification')\nclf.fit(X_train, y_train)\nresult = clf.concate_transform(X_train)\nprint(result)\n\nclf = CatboostFeatureTransformer(task='classification')\nclf.fit(X_train, y_train)\nresult = clf.concate_transform(X_train)\nprint(result)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nlr = LogisticRegression()\nx_train_gb, x_test_gb, y_train_gb, y_test_gb = train_test_split(\n    result, y_train)\nx_train, x_test, y_train, y_test = train_test_split(X_train, y_train)\n\nlr.fit(x_train, y_train)\nscore = roc_auc_score(y_test, lr.predict(x_test))\nprint('LR with GBDT apply data, train data shape : {0}  auc: {1}'.format(\n    x_train.shape, score))\n\nlr = LogisticRegression()\nlr.fit(x_train_gb, y_train_gb)\nscore = roc_auc_score(y_test_gb, lr.predict(x_test_gb))\nprint('LR with GBDT apply data, train data shape : {0}  auc: {1}'.format(\n    x_train_gb.shape, score))\n```\n### Golden Feature Generate\n```python\nfrom autofe import GoldenFeatureTransform\n\ntitanic = pd.read_csv('autotabular/datasets/data/Titanic.csv')\nembarked_encoder = LabelEncoder()\nembarked_encoder.fit(titanic['Embarked'].fillna('Null'))\n# Record anyone travelling alone\ntitanic['Alone'] = (titanic['SibSp'] == 0) & (titanic['Parch'] == 0)\n# Transform 'Embarked'\ntitanic['Embarked'].fillna('Null', inplace=True)\ntitanic['Embarked'] = embarked_encoder.transform(titanic['Embarked'])\n# Transform 'Sex'\ntitanic.loc[titanic['Sex'] == 'female', 'Sex'] = 0\ntitanic.loc[titanic['Sex'] == 'male', 'Sex'] = 1\ntitanic['Sex'] = titanic['Sex'].astype('int8')\n# Drop features that seem unusable. Save passenger ids if test\ntitanic.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n\ntrainMeans = titanic.groupby(['Pclass', 'Sex'])['Age'].mean()\n\ndef f(x):\n    if not np.isnan(x['Age']):  # not NaN\n        return x['Age']\n    return trainMeans[x['Pclass'], x['Sex']]\n\ntitanic['Age'] = titanic.apply(f, axis=1)\n\nX_train = titanic.drop(['Survived'], axis=1)\ny_train = titanic['Survived']\nprint(X_train)\ngbdt_model = GoldenFeatureTransform(\n    results_path='./', ml_task='BINARY_CLASSIFICATION')\ngbdt_model.fit(X_train, y_train)\nresults = gbdt_model.transform(X_train)\nprint(results)\n```\n### Neural Network Embeddings\n```python\n# data url\n\"\"\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques.\"\"\"\ndata_dir = '/media/robin/DATA/datatsets/structure_data/house_price/train.csv'\ndata = pd.read_csv(\n    data_dir,\n    usecols=[\n        'SalePrice', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea',\n        'Street', 'YearBuilt', 'LotShape', '1stFlrSF', '2ndFlrSF'\n    ]).dropna()\n\ncategorical_features = [\n    'MSSubClass', 'MSZoning', 'Street', 'LotShape', 'YearBuilt'\n]\noutput_feature = 'SalePrice'\nlabel_encoders = {}\nfor cat_col in categorical_features:\n    label_encoders[cat_col] = LabelEncoder()\n    data[cat_col] = label_encoders[cat_col].fit_transform(data[cat_col])\n\ndataset = TabularDataset(\n    data=data, cat_cols=categorical_features, output_col=output_feature)\n\nbatchsize = 64\ndataloader = DataLoader(dataset, batchsize, shuffle=True, num_workers=1)\n\ncat_dims = [int(data[col].nunique()) for col in categorical_features]\nemb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = FeedForwardNN(\n    emb_dims,\n    no_of_cont=4,\n    lin_layer_sizes=[50, 100],\n    output_size=1,\n    emb_dropout=0.04,\n    lin_layer_dropouts=[0.001, 0.01]).to(device)\nprint(model)\nnum_epochs = 100\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\nfor epoch in range(num_epochs):\n    for y, cont_x, cat_x in dataloader:\n        cat_x = cat_x.to(device)\n        cont_x = cont_x.to(device)\n        y = y.to(device)\n        # Forward Pass\n        preds = model(cont_x, cat_x)\n        loss = criterion(preds, y)\n        # Backward Pass and Optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print('loss:', loss)\n```\n\n## Citation\nIf you use AutoTabular in a scientific publication, please cite the following paper:\n\nRobin, et al. [\"AutoTabular: Robust and Accurate AutoML for Structured Data.\"](https://arxiv.org/abs/2003.06505) arXiv preprint arXiv:2003.06505 (2021).\n\nBibTeX entry:\n\n```bibtex\n@article{agtabular,\n  title={AutoTabular: Robust and Accurate AutoML for Structured Data},\n  author={JianZheng, WenQi},\n  journal={arXiv preprint arXiv:2003.06505},\n  year={2021}\n}\n```\n\n## License\n\nThis library is licensed under the Apache 2.0 License.\n\n## Contributing to AutoTabular\n\nWe are actively accepting code contributions to the AutoTabular project. If you are interested in contributing to AutoTabular, please contact me.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/jianzhnie/AutoTabular",
    "keywords": "automated machine learning,automl,machine learning,data science,data mining,autotabular",
    "license": "Apache License",
    "maintainer": "",
    "maintainer_email": "",
    "name": "autotabular",
    "package_url": "https://pypi.org/project/autotabular/",
    "platform": "Linux",
    "project_url": "https://pypi.org/project/autotabular/",
    "project_urls": {
      "Homepage": "https://github.com/jianzhnie/AutoTabular"
    },
    "release_url": "https://pypi.org/project/autotabular/0.12.0/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "Automatic machine learning for tabular data.",
    "version": "0.12.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11580513,
  "releases": {
    "0.12.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "46d5d754729ae242bf0dbf5ae7d9415ad388a24e02f7e276c0a0b7f88aebd02d",
          "md5": "9b563fabb78063b88095cc01d2b3c2dc",
          "sha256": "b45fff32c8e93aa6eca4e519675a44437eaf16acf0030ce6e72b2b7ca024b704"
        },
        "downloads": -1,
        "filename": "autotabular-0.12.0.tar.gz",
        "has_sig": false,
        "md5_digest": "9b563fabb78063b88095cc01d2b3c2dc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 9722985,
        "upload_time": "2021-09-29T10:06:27",
        "upload_time_iso_8601": "2021-09-29T10:06:27.556894Z",
        "url": "https://files.pythonhosted.org/packages/46/d5/d754729ae242bf0dbf5ae7d9415ad388a24e02f7e276c0a0b7f88aebd02d/autotabular-0.12.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "46d5d754729ae242bf0dbf5ae7d9415ad388a24e02f7e276c0a0b7f88aebd02d",
        "md5": "9b563fabb78063b88095cc01d2b3c2dc",
        "sha256": "b45fff32c8e93aa6eca4e519675a44437eaf16acf0030ce6e72b2b7ca024b704"
      },
      "downloads": -1,
      "filename": "autotabular-0.12.0.tar.gz",
      "has_sig": false,
      "md5_digest": "9b563fabb78063b88095cc01d2b3c2dc",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 9722985,
      "upload_time": "2021-09-29T10:06:27",
      "upload_time_iso_8601": "2021-09-29T10:06:27.556894Z",
      "url": "https://files.pythonhosted.org/packages/46/d5/d754729ae242bf0dbf5ae7d9415ad388a24e02f7e276c0a0b7f88aebd02d/autotabular-0.12.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}