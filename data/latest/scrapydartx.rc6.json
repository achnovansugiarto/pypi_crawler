{
  "info": {
    "author": "Scrapy developers",
    "author_email": "zhling2012@live.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: Console",
      "Environment :: No Input/Output (Daemon)",
      "License :: OSI Approved :: BSD License",
      "Operating System :: OS Independent",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Topic :: Internet :: WWW/HTTP"
    ],
    "description": "本项目fork自ScrapydArt: https://github.com/dequinns/ScrapydArt\n=====================================================================================================\n\n### 建议使用anaconda环境，请前往[主站](https://www.anaconda.com/distribution/#download-section)或者[清华站点](https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/?C=M&O=D)下载并安装\n\n```\n$ bash Anaconda3-2019.03-Linux-x86_64.sh\n```\n\n#### pip 依赖文件为 req.txt, 直接安装:\n\n```\n$ pip install -r req.txt\n```\n#### 即可解决环境问题\n\npip 安装： pip install scrapydartx\n----------------------------------\n\n功能扩展说明：\n--------------------\n\n## 一. 集成了动态调度功能\n#### scrapydartx 现在可以在设置文件里(default_scrapyd.conf)设置调度数据库为mysql 或者是sqlite:\n```\n...\ndatabase_type = sqlite\n...\n```\n#### 若设置为mysql 则需要填写mysql 的配置信息, 可以是运行环境中的数据库或者在线数据库:\n```\n...\nmysql_host = 127.0.0.1\nmysql_port = 3306\nmysql_user = root\nmysql_password = mysql\nmysql_db = scrapydartTest\n...\n```\n\n#### 其中mysql_db是指使用 mysql 的哪一个数据库，若database_type设置为 sqlite 则不需要设置这些\n\n#### 配置文件的其它设置：\n\n```\n...\nclear_up_database_when_start = yes  // yes/no, 每次运行前清理观察数据\nobservation_times = 20      // >0的整数，设置异常监控的观察次数\nstrict_mode = no           // yes/no，设置是否严格模式\nstrict_degree = 4         // >0的任意数，设置严格模式的严格程度，越低越严格\n...\n```\n\n#### 在程序正确安装后，每次运行时都会有配置文件修改提示（包含以上部分配置选项），您可以手动修改这些配置，所以不用担心配置文件的位置问题，也不用担心程序配置错误了，直接重启程序再配置就行了，程序会保留您最后一次修改的信息。\n\n#### 平台被安装并且启动后，会自动在调度数据库查找需要调度的爬虫，并且根据调度字典计算下次运行时间。\n#### 需要注意的是，加入到调度数据库里的爬虫的项目名称（project）是必须的，并且不同的项目建议使用不同的项目名称\n\n#### 下面介绍将爬虫加入到调度数据库的方法。\n\n#### 扩展平台加入了数个api以方便操控，包括调度数据库的增删改查都可以通过向端口发送请求完成。\n\n### 1. 增加调度爬虫：\n\n```\n$ curl http://localhost:6800/scheduletodb.json -d project=MyProject -d spider=my_spider -d schedule='{\"year\":\"2019\", \"month\": \"08\", \"day\": \"28\", \"hour\": \"09\", \"minute\": \"19\", \"second\": \"00\"}' -d spider_args='{}' -d status='1'\n```\n\n#### 这样显得太长而且复用性太差，不建议这样，建议直接使用 python 的 requests 方法发送请求，效果是一样的，如下面的简单例子：\n\n```\nimport requests\n\npost_data = {\n    'project': 'my_project', # str\n    'spider': 'my_sdpier', # str\n    'schedule': '{\"day\": \"*/1\", \"hour\": \"19\"}', # str dict\n    'spider_args': '{\"times\": 1, \"method\": \"auto_increment\"}', # str dict\n    'status': 1, # int\n}\nresult = requests.post(url='http://localhost:6800/scheduletodb.json', data=post_data)\n```\n\n#### 上面post_data变量中的 “spider_args” 如有需要请传入一个字典，里面的键和值将自动传递给相应的爬虫（需要提前在爬虫内接收），此项为可选项，可不传。\n\n#### spider_args 支持动态变量模式, 需要指定其内部的 \"method\" 字段, 如果不给定 \"method\", 或者 \"method\" 的值为\"normal\", 则每次自动调度都会使用同样的参数;\n\n#### 如果指定 \"method\" 为 \"auto_increment\", 则args参数中是数值类型的参数会在每次调度时自动增加整数1, 其他非数值类型参数不变, 而且 \"auto_increment\" 方法仅支持整数变化;\n\n#### 自定义动态传参:\n\n - \"method\" 的值设置为一个字典, 字典的键为 \"expression\" 或者 \"function\", 意思是表达式或者方法, 在调度器内部, \"expression\"会被执行 eval() 方法, \"function\" 会被执行 exec()方法\n - \"expression\" 或者 \"function\" 的值设置为表达式或者方法的字符串, 例如:\n```\n\"method\": { \"expression\": \"{k: v+1 for k, v in args.items()}\" }\n```\n - 表达式生成的值会被调度器内的变量 \"next_args\" 所接收, 用于下一次调度的传参\n - \"function\" 的值会直接用exec()运行, 所以 \"function\" 的值的运行结果需要对变量 \"next_args\" 作出改变, 例如:\n```\n\"method\": { \"function\":\n\"def func(dic):\n    next_dic = {}\n    for k, v in dic.items():\n      next_dic[k] = v + 1\n    return next_dic\nnext_args = func(args)\" }\n```\n - 即可以将参数变量\"args\"通过你自己编写的方法变为下一次的变量\"next_args\"\n - 注意, 参数 \"args\" 和下次参数 \"next_args\" 是的名称在程序内部是固定的, 不可更改否则不起效果, 甚至报错\n\n#### “status” 参数，数据类型为 int ，值为 0 或者1 或者2 或者3\n - 0 ： 不启用此条调度\n - 1 ： 正常启用\n - 2 ： 高级模式\n - 3 ： 最高级模式\n\n#### 设定为1的时候，爬虫会被系统资源限制运行，即当平台检测到系统cpu或者内存严重不足时，延缓数秒到一个周期执行，普通情况下请设置为1，同时如果运行时间异常，将会被平台异常管理机制自动终结。\n#### 设定为2的时候，爬虫将忽略系统资源情况，不管系统资源占用多少都会按时执行，但如果运行时间异常，也将会被平台异常管理机制自动终结。\n#### 设定为3的时候，爬虫为超级权限模式，不会被异常管理机制终止，同时忽略系统资源状况。\n\n#### “schedule” 的值需要一个字典，或者逗号分割的日期字符串，字典的值类似:\n```\n{\"second\": \"*/30\"}\n```\n#### 意思是每30秒运行一次，也可以设置为具体的某个时间点，例如：\n\n```\n...\n'schedule': {\n    'year': 2019,\n    'month': 6,\n    'day': 4,\n    'hour': 10,\n    'minute': 50,\n    'second': 50\n  }\n```\n\n#### 这样的设置是单次任务模式，爬虫只在设定的时间点\n  ```\n  （2019-06-04 10:50:50）\n  ```\n#### 运行一次\n#### 也可以是只有分钟，或者只有秒，或者只有小时、天、月、年，平台将自动计算下次到达设定的时间的时间间隔，\n#### 例如只给定 {'hour': 10}，则爬虫会在从明天开始的每一天的10点的当前分秒数启动，例如当发送了 \"schedule\" 参数为：\n```\n\"schedule\": {\"hour\": 10}\n```\n#### 而此时系统时间为 16:37:12，则爬虫将在明天，以及接下来的每一天的上午的 10:37:12 自动发送任务到运行平台并启动。\n#### 若只设置为 {'day': 4}，则意思是从下个月开始的每个月的4号的当前时分秒启动，以此类推。\n#### 需要注意的是，设定的月为1-12， 日小于等于月最大天数，小时为0-23， 分钟数0-59，秒数0-59。\n#### 此外，我想你应该已经完全了解在参数中加 “*/” 或 不加 “*/” 的作用了，若仍觉得不清楚，直接使用它你会发现其中的含义。\n#### 此外还可以设定星期数，例如：\n  ```\n  {'week': '*/2'}\n  ```\n#### 意思是每周的周三执行，对的，星期的范围为0~6。\n#### 星期可以配合年、月、时、分、秒同时设定，但是不可同时设定星期数和天，否则将只按照天来计算，并忽略星期的设定。\n\n#### 若 schedule 的值为字符串，则需要这样的格式：\n```\n'2019,9,13,16,30,0'     # 2019年9月13日16点30分00秒\n```\n#### 注意全日期是必须的，若中间缺失某个值将无法正常部署。\n#### 这样设置，方法也是只会被执行一次\n\n#### 若入库成功，则返回的result如下：\n```\n {\"node_name\": \"name of node\", \"status\": \"ok\"}\n```\n#### 入库成功后，等待几秒钟平台会发现它并且自动计算下次运行时间，后台将显示类似如下语句的日志：\n```\n  [- Scheduler -#info] job default-xinhua is waiting, countdown 540s\n                               ↑     ↑                           ↑\n                            项目名   爬虫名                  距下次运行秒数\n```\n\n### 2. 查看调度爬虫：\n```\n    $ curl http://localhost:6800/listdbschedule.json -d projects=\"['project_name1', 'project_name2']\" -d spiders=\"['spider1', 'spider2']\"\n```\n\n#### 注意，参数projects与spiders需要传入字符串类型的列表，也可以什么都不传，将返回所有在数据库中的待调度爬虫和调度参数。\n\n#### requests 方法：\n```\nimport requests\nurl = 'http://127.0.0.1:6800/listdbschedule.json?un=&pwd='\ndata = {\n    'projects': str(['project1', 'project2']),\n    'spiders': str(['spider1', 'spider2']),\n        }\nres = json.loads(requests.post(url=url, data=data).content)\nprint(res)\n```\n\n#### 正确返回：\n```\n{\"node_name\": \"name of node\", \"status\": \"ok\", \"database_schedules\":\n                                        [{'id': schedule_id,\n                                          'project': project_name,\n                                          'spider': spider_name,\n                                          'schedule': spider_schedule,\n                                          'args': spider_args,\n                                          'status': spider_status\n                                          },\n                                          {... ...},\n                                          ...\n                                          ] }\n```\n\n### 3. 更改调度爬虫：\n```\n    $ curl http://localhost:6800/updatedbschedule.json -d id=update_id -d schedule=new_schedule -d args=new_args -d status=new_status\n```\n\n#### 参数 id 是需要修改的那一条数据的id值，根据查看调度爬虫 listdbschedule.json 得知。\n#### requests 方法：\n```\nimport requests\npost_data = {\n          'id': update_id,\n          'schedule': new_schedule,\n          'args': new_args,\n          'status': new_status,\n          }\nresult = requests.post(url='http://localhost:6800/updatedbschedule.json', data=post_data)\n```\n\n#### 操作成功则返回：\n```\n    {\"node_name\": \"name of node\", 'update': \"ok\"}\n```\n\n### 4. 删除调度爬虫：\n```\n    $ curl http://localhost:6800/rmschedulefromdb.json -d id=delete_id\n```\n#### delete_id 根据查询调度爬虫得知。\n#### requests 方法：\n```\nimport requests\n\npost_data = {\n            'id': delete_id,\n            }\nresult = requests.post(url='http://localhost:6800/rmschedulefromdb.json', data=post_data)\n```\n\n#### 正确返回:\n```\n  {\"node_name\": \"name of node\", 'delete': \"ok\"}\n```\n\n#### 若在配置中设置了auth_username 和 auth_password，则需要在每次post请求的data中加入 {'un': user_name, 'pwd': password}， get 请求中加入 ?un=user_name&pwd=password。\n\n## 2. 异常爬虫管理功能\n\n### 此项功能默认在平台启动的时候自动启动，也可以在配置文件内设置为不启用 (默认 activate)。\n```\n...\nTerminator = deactivate\n...\n```\n#### 此功能会每隔数秒巡视一遍所有正在运行的爬虫，并根据之前的运行时间计算方差数与均值，推算判断本次运行的时间是否合理，若不合理，则会在几秒钟内终结此任务, 不需要担心爬虫任务永远被杀死了，除非你设置的是单次定时任务，否则爬虫将在下一个周期继续正常启动, 若设置为 Terminator 设置为 \"activate\"，将会在平台启动时在后台显示日志：\n```\n[- TERMINATOR -#info] Terminator Started\n```\n#### Terminator 在每次巡视后在后台打印日志：\n```\n[- TERMINATOR -#warn] Scan completed\n```\n#### 若本次巡视发现异常爬虫，且被终结，则巡视日志为：\n```\n[- TERMINATOR -#warn] Scan completed, Terminated target: '[default-xinhua-6796090a940b11e98d6854e1adc0a997, default-fifa-6796090a940b11e98d6854e1adc0a997]'\n```\n#### 日志中 “Terminated target” 为被终结的爬虫的列表字符串，格式为:\n```\n  [project-spider-jobid, ...]\n```\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/GuardianGH/scrapydartx",
    "keywords": "",
    "license": "",
    "maintainer": "Scrapy developers",
    "maintainer_email": "info@scrapy.org",
    "name": "scrapydartx",
    "package_url": "https://pypi.org/project/scrapydartx/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapydartx/",
    "project_urls": {
      "Homepage": "https://github.com/GuardianGH/scrapydartx"
    },
    "release_url": "https://pypi.org/project/scrapydartx/1.3.2/",
    "requires_dist": [
      "attrs (==19.3.0)",
      "Automat (==0.8.0)",
      "bleach (==3.1.0)",
      "certifi (==2019.11.28)",
      "cffi (==1.13.2)",
      "chardet (==3.0.4)",
      "constantly (==15.1.0)",
      "cryptography (==2.8)",
      "cssselect (==1.1.0)",
      "docutils (==0.15.2)",
      "enum-compat (==0.0.3)",
      "hyperlink (==19.0.0)",
      "idna (==2.8)",
      "importlib-metadata (==1.3.0)",
      "incremental (==17.5.0)",
      "jeepney (==0.4.1)",
      "keyring (==20.0.0)",
      "lxml (==4.4.2)",
      "more-itertools (==8.0.2)",
      "numpy (==1.17.4)",
      "parsel (==1.5.2)",
      "pkginfo (==1.5.0.1)",
      "Protego (==0.1.16)",
      "psutil (==5.6.7)",
      "pyasn1 (==0.4.8)",
      "pyasn1-modules (==0.2.7)",
      "pycparser (==2.19)",
      "PyDispatcher (==2.0.5)",
      "Pygments (==2.5.2)",
      "PyHamcrest (==1.9.0)",
      "PyMySQL (==0.9.3)",
      "pyOpenSSL (==19.1.0)",
      "queuelib (==1.5.0)",
      "readme-renderer (==24.0)",
      "requests (==2.22.0)",
      "requests-toolbelt (==0.9.1)",
      "Scrapy (==1.8.0)",
      "SecretStorage (==3.1.1)",
      "service-identity (==18.1.0)",
      "six (==1.13.0)",
      "SQLAlchemy (==1.3.12)",
      "tqdm (==4.40.2)",
      "twine (==3.1.0)",
      "Twisted (==18.9.0)",
      "urllib3 (==1.25.7)",
      "w3lib (==1.21.0)",
      "webencodings (==0.5.1)",
      "zipp (==0.6.0)",
      "zope.interface (==4.7.1)"
    ],
    "requires_python": "",
    "summary": "a extension from ScrapydArt",
    "version": "1.3.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 6329344,
  "releases": {
    "1.2.0.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4ad18951beae966d09d156132be24853568a43108c16bbd0518685c58961af81",
          "md5": "995d87c05a5abadca97b82acd3c0b7e1",
          "sha256": "8613996abf12174d8884be928c178269fecb4b2c55e7f2338c4cfad1ded5a657"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.2.0.5.tar.gz",
        "has_sig": false,
        "md5_digest": "995d87c05a5abadca97b82acd3c0b7e1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 105617,
        "upload_time": "2019-08-12T10:14:39",
        "upload_time_iso_8601": "2019-08-12T10:14:39.568835Z",
        "url": "https://files.pythonhosted.org/packages/4a/d1/8951beae966d09d156132be24853568a43108c16bbd0518685c58961af81/scrapydartx-1.2.0.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "12f587d093976076c0c46971d9353e7347839c223bc146e926586167c7fe5006",
          "md5": "7b070d9bd5ea6e7c025a8f8a8216de58",
          "sha256": "659894e89c3cecfde71be36641797ef097bdf2f6c6b1a3c463e5bfbb5ee2976c"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.2.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "7b070d9bd5ea6e7c025a8f8a8216de58",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 216368,
        "upload_time": "2019-09-04T09:17:52",
        "upload_time_iso_8601": "2019-09-04T09:17:52.399672Z",
        "url": "https://files.pythonhosted.org/packages/12/f5/87d093976076c0c46971d9353e7347839c223bc146e926586167c7fe5006/scrapydartx-1.2.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bbe624668df2b5d6e328e5d47ca0b514615af6137d49315024e7f566a589876a",
          "md5": "11e67ae048c6d792aad243a08db5a2ec",
          "sha256": "070b6892464b2e9e5dcd0929da207cdc816ffda797d1ca5918441f39bda7b0e1"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.2.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "11e67ae048c6d792aad243a08db5a2ec",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 112561,
        "upload_time": "2019-12-18T03:21:34",
        "upload_time_iso_8601": "2019-12-18T03:21:34.200700Z",
        "url": "https://files.pythonhosted.org/packages/bb/e6/24668df2b5d6e328e5d47ca0b514615af6137d49315024e7f566a589876a/scrapydartx-1.2.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "909ae4123fe19e9a11329b4b4407e2f5c08c2d643dd8fbc83f20e2951195db52",
          "md5": "f9650258e03086deeba729ab82acefd0",
          "sha256": "e1debcbefbd339e09e0d1406e8d88fa2dea4bf9b2a3fa812a353885520e02edb"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.3.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f9650258e03086deeba729ab82acefd0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 217335,
        "upload_time": "2019-12-18T10:11:53",
        "upload_time_iso_8601": "2019-12-18T10:11:53.476357Z",
        "url": "https://files.pythonhosted.org/packages/90/9a/e4123fe19e9a11329b4b4407e2f5c08c2d643dd8fbc83f20e2951195db52/scrapydartx-1.3.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ca7105583b09c801cacaa7c6c70f5030be65feb5ee24d9d7ce03c88427c95217",
          "md5": "f663340fc819fca0319e35c363258031",
          "sha256": "2f7c03e73b3484548b6ea5ce9a83923e96cd4a880eb629fedece3970d3b799eb"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f663340fc819fca0319e35c363258031",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 113431,
        "upload_time": "2019-12-18T10:11:56",
        "upload_time_iso_8601": "2019-12-18T10:11:56.046780Z",
        "url": "https://files.pythonhosted.org/packages/ca/71/05583b09c801cacaa7c6c70f5030be65feb5ee24d9d7ce03c88427c95217/scrapydartx-1.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2f328a9493ea0e7596589b63a4c5ac0604971e0edf1f0dd4ef4b490961d6a71f",
          "md5": "c8ba2923b59979dbb09bc28ab18f7efc",
          "sha256": "16d6b3ee9a20f6835960ac470f3addfe6e74e30629c2b144c5e31108e3b8800b"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.3.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c8ba2923b59979dbb09bc28ab18f7efc",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 217571,
        "upload_time": "2019-12-19T02:17:01",
        "upload_time_iso_8601": "2019-12-19T02:17:01.721786Z",
        "url": "https://files.pythonhosted.org/packages/2f/32/8a9493ea0e7596589b63a4c5ac0604971e0edf1f0dd4ef4b490961d6a71f/scrapydartx-1.3.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "30daea1cfedf0c628e759b3e0c6d1028704b5070ec74cc526a7c57c837f85b28",
          "md5": "ed26a359c0b48017ace133a0145cf88d",
          "sha256": "caec4f9bb352c76113bd2886f7264762a738997f55c36632e180b2bbc1e0cb59"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.3.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ed26a359c0b48017ace133a0145cf88d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 113679,
        "upload_time": "2019-12-19T02:17:04",
        "upload_time_iso_8601": "2019-12-19T02:17:04.311276Z",
        "url": "https://files.pythonhosted.org/packages/30/da/ea1cfedf0c628e759b3e0c6d1028704b5070ec74cc526a7c57c837f85b28/scrapydartx-1.3.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.3.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "206f5a83fc7c6292238eb1b1b6c04521d40091228ed837933b29f3facb6a8113",
          "md5": "92e65f12f71ac5aee69fb9a9db18fb20",
          "sha256": "af9d68f5e69c53cae90eb2eab31f165903c7706c2642a3bad2072d75bf567a4d"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.3.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "92e65f12f71ac5aee69fb9a9db18fb20",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 218061,
        "upload_time": "2019-12-19T02:32:10",
        "upload_time_iso_8601": "2019-12-19T02:32:10.936269Z",
        "url": "https://files.pythonhosted.org/packages/20/6f/5a83fc7c6292238eb1b1b6c04521d40091228ed837933b29f3facb6a8113/scrapydartx-1.3.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "05a39401a5eb462103c5d609f6552581e3ba417e92e997dd6c40f2e98d886e4e",
          "md5": "1c3d5fbc5ce3f2397072e4c245a9652f",
          "sha256": "22882787a860fb73c008864a81ad2ef52655a47a8f2c905f5c78ac43b204b1a2"
        },
        "downloads": -1,
        "filename": "scrapydartx-1.3.2.tar.gz",
        "has_sig": false,
        "md5_digest": "1c3d5fbc5ce3f2397072e4c245a9652f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 114514,
        "upload_time": "2019-12-19T02:32:13",
        "upload_time_iso_8601": "2019-12-19T02:32:13.589963Z",
        "url": "https://files.pythonhosted.org/packages/05/a3/9401a5eb462103c5d609f6552581e3ba417e92e997dd6c40f2e98d886e4e/scrapydartx-1.3.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "206f5a83fc7c6292238eb1b1b6c04521d40091228ed837933b29f3facb6a8113",
        "md5": "92e65f12f71ac5aee69fb9a9db18fb20",
        "sha256": "af9d68f5e69c53cae90eb2eab31f165903c7706c2642a3bad2072d75bf567a4d"
      },
      "downloads": -1,
      "filename": "scrapydartx-1.3.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "92e65f12f71ac5aee69fb9a9db18fb20",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 218061,
      "upload_time": "2019-12-19T02:32:10",
      "upload_time_iso_8601": "2019-12-19T02:32:10.936269Z",
      "url": "https://files.pythonhosted.org/packages/20/6f/5a83fc7c6292238eb1b1b6c04521d40091228ed837933b29f3facb6a8113/scrapydartx-1.3.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "05a39401a5eb462103c5d609f6552581e3ba417e92e997dd6c40f2e98d886e4e",
        "md5": "1c3d5fbc5ce3f2397072e4c245a9652f",
        "sha256": "22882787a860fb73c008864a81ad2ef52655a47a8f2c905f5c78ac43b204b1a2"
      },
      "downloads": -1,
      "filename": "scrapydartx-1.3.2.tar.gz",
      "has_sig": false,
      "md5_digest": "1c3d5fbc5ce3f2397072e4c245a9652f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 114514,
      "upload_time": "2019-12-19T02:32:13",
      "upload_time_iso_8601": "2019-12-19T02:32:13.589963Z",
      "url": "https://files.pythonhosted.org/packages/05/a3/9401a5eb462103c5d609f6552581e3ba417e92e997dd6c40f2e98d886e4e/scrapydartx-1.3.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}