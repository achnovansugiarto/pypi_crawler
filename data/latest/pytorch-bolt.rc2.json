{
  "info": {
    "author": "Yi Zhang",
    "author_email": "yizhang.dev@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.8"
    ],
    "description": "# PyTorch Bolt\n\nPyTorch Bolt is\n\n* a simple PyTorch wrapper making multi-node multi-GPU training much easier on Slurm\n\n\n\nPyTorch Bolt supports to\n\n* use single-node single-GPU training on a specified GPU device\n\n* use multi-node (or single-node) multi-GPU [`DistributedDataParallel`](https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html) (DDP) training\n  - with [`torch.distributed.launch`](https://pytorch.org/docs/stable/distributed.html#launch-utility) module\n  - with [Slurm](https://slurm.schedmd.com/quickstart.html) cluster workload manager\n\n\n\n## Quickstart\n\n### All-in-One Template Using PyTorch Bolt\n\n#### Recommended Structure\n\n```\n.\n├── data\n│   ├── __init__.py\n│   └── customized_datamodule.py\n├── model\n│   ├── __init__.py\n│   └── customized_model.py\n├── main.py\n├── main.sbatch\n└── requirements.txt\n```\n\n#### Demo\n\n[MNIST classification using PyTorch Bolt](https://github.com/yzhang-dev/PyTorch-with-Slurm/tree/main/Tutorials/All-in-One-Template-Using-PyTorch-Bolt) (you might need to go through the relevant [tutorials](https://github.com/yzhang-dev/PyTorch-with-Slurm) step by step).\n\n\n\n### Dependencies and Installation\n\n#### Package Dependencies\n\n`pip install -r requirements.txt` can handle all package dependencies.\n\n\n\n#### Install Pytorch Bolt\n\n```bash\n$ pip install pytorch-bolt\n```\n\n\n\n## Documentation\n\n### Module `DataModule`\n\n```python\nclass pytorch_bolt.DataModule(data_dir='data', num_splits=10, batch_size=1, num_workers=0, pin_memory=False, drop_last=False)\n```\n\n#### `use_dist_sampler()`\n\nCan be called to trigger `DistributedSampler` when using `DistributedDataParallel` (DDP).\n\n#### `train_dataloader()`\n\nReturns `Dataloader` for trainset.\n\n#### `val_dataloader()`\n\nReturns  `Dataloader` for valset.\n\n#### `test_dataloader()`\n\nReturns  `Dataloader` for testset.\n\n#### `add_argparse_args(parent_parser)`\n\nReturns `argparse` parser. (**Staticmethod**)\n\n\n\n**Practical template**:\n\n```python\nimport pytorch_bolt\n\nclass MyDataModule(pytorch_bolt.DataModule):\n\n    def __init__(self, args):\n        super().__init__(args)\n        # arguments for customized dataset\n\n    # optional helper function can be used\n    def _prepare_data(self):\n        pass  \n\n    def _setup_dataset(self):\n        # trainset and valset for fit stage\n        # `self.num_splits` can be used for splitting trainset and valset \n        # testset for test stage\n        return trainset, valset, testset\n\n    @staticmethod\n    def add_argparse_args(parent_parser):\n        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n        parser = pytorch_bolt.DataModule.add_argparse_args(parser)\n        # TODO\n        return parser\n\n    @classmethod\n    def from_argparse_args(cls, args):\n        return cls(args)\n```\n\n\n\n### Module `Module`\n\n```python\nclass pytorch_bolt.Module()\n```\n\n#### `parameters_to_update()`\n\nReturns model parameters that have `requires_grad=True`.\n\n#### `configure_criterion()`\n\nReturns criterion.\n\n#### `configure_metric()`\n\nReturns metric.\n\n#### `configure_optimizer()`\n\nReturns optimizer (and learning rate scheduler).\n\n\n\n**Practical template**:\n\n```python\nimport pytorch_bolt\n\nclass MyModel(pytorch_bolt.Module):\n\n    def __init__(self, args):\n        super().__init__()\n        # hyperparameters for model\n        self.model = self._setup_model()\n        # hyperparameters for criterion, metric, optimizer and lr_scheduler\n\n    def _setup_model(self):\n        # TODO\n        return model\n\n    def forward(self, inputs):\n        return self.model(inputs)\n\n    # return parameters that have requires_grad=True\n    # `parameters_to_update` can be useful for transfer learning\n    def parameters_to_update(self):\n        return\n\n    # return criterion\n    def configure_criterion(self):\n        return\n\n    # return metric\n    def configure_metric(self):\n        return\n\n    # return optimizer (and lr_scheduler)\n    def configure_optimizer(self):\n        return\n\n    @staticmethod\n    def add_argparse_args(parent_parser):\n        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n        # TODO     \n        return parser\n\n    @classmethod\n    def from_argparse_args(cls, args):\n        return cls(args)\n```\n\n\n\n### Module `Loggers`\n\n```python\nclass pytorch_bolt.Loggers(logs_dir='logs', loggerfmt='%(asctime)s | %(levelname)-5s | %(name)s - %(message)s', datefmt=None, tracker_keys=None (Required), tracker_reduction='mean')\n```\n\n#### `configure_root_logger(root)`\n\nReturns `root` logger.\n\n#### `configure_child_logger(child)`\n\nReturns `root.child` logger.\n\n#### `configure_tracker()`\n\nReturns tracker for tracking forward propagation step outputs and statistics.\n\n#### `configure_progressbar()`\n\nReturns progress bar for showing forward propagation step progress and details. \n\n#### `configure_writer()`\n\nReturns Tensorboard writer for visualizing forward propagation epoch outputs.\n\n#### `add_argparse_args(parent_parser)`\n\nReturns `argparse` parser. (**Staticmethod**)\n\n#### `from_argparse_args(args)`\n\n`Loggers` constructor.\n\n\n\n### Module `Trainer`\n\n```python\nclass pytorch_bolt.Trainer(loggers=None (Required), device=None, distributed=False, use_slurm=False, dist_backend='nccl', master_addr='localhost', master_port='29500', world_size=1, rank=0, local_rank=0, datamodule=None (Required), model=None (Required), max_epochs=5, verbose=False)\n```\n\n#### `get_rank()`\n\nGets rank of current process. (**Staticmethod**)\n\n#### `fit()`\n\nFits the model on trainset, validating each epoch on valset.\n\n#### `validate()`\n\nValidates trained model by running one epoch on valset.\n\n#### `test()`\n\nTests trained model by running one epoch on testset.\n\n#### `destroy()`\n\nDestroys trainer..\n\n#### `add_argparse_args(parent_parser)`\n\nReturns `argparse` parser. (**Staticmethod**)\n\n#### `from_argparse_args(args)`\n\n`Trainer` constructor.\n\n\n\n**Practical template for customized trainer**: \n\n```python\nimport pytorch_bolt\n\nclass MyTrainer(pytorch_bolt.Trainer):\n\n    def _training_step(self, batch_idx, batch):\n        return\n\n    def _training_step_end(self, batch_idx, batch, step_outs):\n        return\n\n    # if return\n    # return dict, containing at least 2 keys: \"loss\", \"score\"\n    def _training_epoch_end(self):        \n        return\n```\n\n\n\n## Related Projects\n\n* Inspired by [Pytorch Lightning](https://www.pytorchlightning.ai/)\n\n\n\n## Appendix\n\n### Environment Variable Mapping\n\n**WORLD_SIZE** | **SLURM_NTASKS** (and **SLURM_NPROCS** for backwards compatibility)\n\n> Same as **-n, --ntasks**\n\n**RANK** | **SLURM_PROCID**\n\n> The MPI rank (or relative process ID) of the current process\n\n**LOCAL_RANK** | **SLURM_LOCALID**\n\n> Node local task ID for the process within a job.\n\n**MASTER_ADDR** | **SLURM_SUBMIT_HOST**\n\n> The hostname of the machine from which **sbatch** was invoked.\n\n**NPROC_PER_NODE** | **SLURM_NTASKS_PER_NODE**\n\n> Number of tasks requested per node. Only set if the **--ntasks-per-node** option is specified.\n\n**NNODES** | **SLURM_JOB_NUM_NODES** (and **SLURM_NNODES** for backwards compatibility)\n\n> Total number of nodes in the job's resource allocation.\n\n**NODE_RANK** | **SLURM_NODEID**\n\n> ID of the nodes allocated.\n\n**SLURM_JOB_NODELIST** (and **SLURM_NODELIST** for backwards compatibility)\n\n> List of nodes allocated to the job.\n\n\n\n### Reference\n\n* [`sbatch` Output Environment Variables](https://slurm.schedmd.com/sbatch.html#lbAK)\n\n* [`torch.distributed` TCP Initialization Environment Variables](https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization)\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "https://github.com/yzhang-dev/PyTorch-Bolt",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/yzhang-dev/PyTorch-Bolt",
    "keywords": "pytorch,pytorch-template,ddp,distributed,slurm",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "PyTorch-Bolt",
    "package_url": "https://pypi.org/project/PyTorch-Bolt/",
    "platform": "",
    "project_url": "https://pypi.org/project/PyTorch-Bolt/",
    "project_urls": {
      "Download": "https://github.com/yzhang-dev/PyTorch-Bolt",
      "Homepage": "https://github.com/yzhang-dev/PyTorch-Bolt"
    },
    "release_url": "https://pypi.org/project/PyTorch-Bolt/0.0.2/",
    "requires_dist": [
      "torch (==1.8.1)",
      "torchvision (==0.9.1)",
      "tensorboard (==2.4.1)"
    ],
    "requires_python": ">=3.8",
    "summary": "A simple PyTorch wrapper making multi-node multi-GPU training much easier on Slurm",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10326033,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b9e3c1c384a0f4fed2a1adba1a353e2375c56663b13d48537c2c10759421a71f",
          "md5": "6d3f4a993f28f4fc9a5508292870e388",
          "sha256": "255ab404d8b2a32ba7b859e80a087a61b173adc9fffee00f2ece169e81dfa7d2"
        },
        "downloads": -1,
        "filename": "PyTorch_Bolt-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6d3f4a993f28f4fc9a5508292870e388",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 14036,
        "upload_time": "2021-05-11T04:06:23",
        "upload_time_iso_8601": "2021-05-11T04:06:23.019869Z",
        "url": "https://files.pythonhosted.org/packages/b9/e3/c1c384a0f4fed2a1adba1a353e2375c56663b13d48537c2c10759421a71f/PyTorch_Bolt-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "684029a49e8071bc9b5e5eb7e95a9965bf0d1499437832472b2d81610b63547e",
          "md5": "a0a55f3db5e84ca421628408d9259807",
          "sha256": "99c4383c837a60190fa584e41ee64841ac655297215edd3e19217db06fa41a7b"
        },
        "downloads": -1,
        "filename": "PyTorch-Bolt-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a0a55f3db5e84ca421628408d9259807",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 9907,
        "upload_time": "2021-05-11T04:06:24",
        "upload_time_iso_8601": "2021-05-11T04:06:24.954947Z",
        "url": "https://files.pythonhosted.org/packages/68/40/29a49e8071bc9b5e5eb7e95a9965bf0d1499437832472b2d81610b63547e/PyTorch-Bolt-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d8f15b4b8a01a0170906c06a0db5218b854ec5780449aa7cc0da5328b9025ea3",
          "md5": "f506faae1c478200a2c10615361ddd87",
          "sha256": "ef32525cd2bb43fda5be0e8a077da8782fe3b2fcabbcf801f25aabb4d7cdfc8f"
        },
        "downloads": -1,
        "filename": "PyTorch_Bolt-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f506faae1c478200a2c10615361ddd87",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8",
        "size": 15092,
        "upload_time": "2021-05-11T20:13:44",
        "upload_time_iso_8601": "2021-05-11T20:13:44.837093Z",
        "url": "https://files.pythonhosted.org/packages/d8/f1/5b4b8a01a0170906c06a0db5218b854ec5780449aa7cc0da5328b9025ea3/PyTorch_Bolt-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "83bad277001d39724840405abb2fa45a2be365cd332deefa75d2736b5f6cb575",
          "md5": "e8d5f4ba898e1de4dae3afde0042efe0",
          "sha256": "c47b222c4def8f0733655044d86109162321c8f21e37881a0c240086b5ca3b83"
        },
        "downloads": -1,
        "filename": "PyTorch-Bolt-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "e8d5f4ba898e1de4dae3afde0042efe0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 10865,
        "upload_time": "2021-05-11T20:13:46",
        "upload_time_iso_8601": "2021-05-11T20:13:46.629290Z",
        "url": "https://files.pythonhosted.org/packages/83/ba/d277001d39724840405abb2fa45a2be365cd332deefa75d2736b5f6cb575/PyTorch-Bolt-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d8f15b4b8a01a0170906c06a0db5218b854ec5780449aa7cc0da5328b9025ea3",
        "md5": "f506faae1c478200a2c10615361ddd87",
        "sha256": "ef32525cd2bb43fda5be0e8a077da8782fe3b2fcabbcf801f25aabb4d7cdfc8f"
      },
      "downloads": -1,
      "filename": "PyTorch_Bolt-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "f506faae1c478200a2c10615361ddd87",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8",
      "size": 15092,
      "upload_time": "2021-05-11T20:13:44",
      "upload_time_iso_8601": "2021-05-11T20:13:44.837093Z",
      "url": "https://files.pythonhosted.org/packages/d8/f1/5b4b8a01a0170906c06a0db5218b854ec5780449aa7cc0da5328b9025ea3/PyTorch_Bolt-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "83bad277001d39724840405abb2fa45a2be365cd332deefa75d2736b5f6cb575",
        "md5": "e8d5f4ba898e1de4dae3afde0042efe0",
        "sha256": "c47b222c4def8f0733655044d86109162321c8f21e37881a0c240086b5ca3b83"
      },
      "downloads": -1,
      "filename": "PyTorch-Bolt-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "e8d5f4ba898e1de4dae3afde0042efe0",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 10865,
      "upload_time": "2021-05-11T20:13:46",
      "upload_time_iso_8601": "2021-05-11T20:13:46.629290Z",
      "url": "https://files.pythonhosted.org/packages/83/ba/d277001d39724840405abb2fa45a2be365cd332deefa75d2736b5f6cb575/PyTorch-Bolt-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}