{
  "info": {
    "author": "Hugo Flores Garcia",
    "author_email": "hf01049@georgiasouthern.edu",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License"
    ],
    "description": "# audacitorch\n\nThis package contains utilities for prepping PyTorch audio models for use in Audacity. More specifically, it provides abstract classes for you to wrap your waveform-to-waveform and waveform-to-labels models (see the [Deep Learning for Audacity](https://interactiveaudiolab.github.io/project/audacity) website to learn more about deep learning models for audacity).   \n\n## Table of Contents\n- [Downloading Audacity with Deep Learning](#download)\n- [Installing](#pip)\n- [Contributing Models to Audacity](#contrib)\n- [Choosing an Effect Type ](#effect-types)\n    - [Waveform to Waveform models](#wav2wav)\n    - [Waveform to Labels models](#wav2labels)\n- [Model Metadata](#metadata)\n- [Making Your Model Built-In to Audacity](#builtin)\n- [Debugging Your Model in Audacity](#debugging)\n- [Example - Waveform-to-Waveform](#example-wav2wav)\n    - [Making sure your model is compatible with `torchscript`](#compat)\n- [Example - Exporting a Pretrained Asteroid Model](#example-asteroid)\n\n--- \n\n![img](./assets/whole-manager.png)\n\n<a name=\"download\"/>\n\n## Download Audacity with Deep Learning\n\nOur work has not yet been merged to the main build of Audacity, though it will be soon. You can keep track of its progress by viewing our [pull request](https://github.com/audacity/audacity/pull/1384). In the meantime, you can download an alpha version of [Audacity + Deep Learning here](https://interactiveaudiolab.github.io/project/audacity.html).\n\n## Installing\n\n<a name=\"pip\"/>\n\nYou can install `audacitorch` using pip: \n\n```\npip install -e \"git+https://github.com/hugofloresgarcia/audacitorch.git#egg=audacitorch\"\n```\n\n<a name=\"contrib\"/>\n\n## Contributing Models to Audacity\n\n### Supported Torch versions\n\n`audacitorch` requires for your model to be able to run in **Torch 1.9.0**, as that's what the Audacity torchscript interpreter uses. \n\n### Deep Learning Effect and Analyzer\n\nAudacity is equipped with a wrapper framework for deep learning models written in PyTorch. Audacity contains two deep learning tools: `Deep Learning Effect` and `Deep Learning Analyzer`.  \n`Deep Learning Effect` performs waveform to waveform processing, and is useful for audio-in-audio-out tasks (such as source separation, voice conversion, style transfer, amplifier emulation, etc.), while `Deep Learning Analyzer` performs waveform to labels processing, and is useful for annotation tasks (such as sound event detection, musical instrument recognition, automatic speech recognition, etc.).\n`audacitorch` contains two abstract classes for serializing two types of models: waveform-to-waveform and waveform-to-labels. The classes are `WaveformToWaveformBase`, and `WaveformToLabelsBase`, respectively. \n\n<a name=\"effect-types\"/> \n\n## Choosing an Effect Type \n\n<a name=\"effect-diagram\"/> \n\n![](./assets/tensor-flow.png)\n\n<a name=\"wav2wav\"/> \n\n### Waveform to Waveform models\n\nAs shown in the [effect diagram](#effect-diagram), Waveform-to-waveform models receive a single multichannel audio track as input, and may write to a variable number of new audio tracks as output.\n\nExample models for waveform-to-waveform effects include source separation, neural upsampling, guitar amplifier emulation, generative models, etc. Output tensors for waveform-to-waveform models must be multichannel waveform tensors with shape `(num_output_channels, num_samples)`. For every audio waveform in the output tensor, a new audio track is created in the Audacity project. \n\n<a name=\"wav2labels\"/> \n\n### Waveform to Labels models\n\nAs shown in the [effect diagram](#effect-diagram), Waveform-to-labels models receive a single multichannel audio track as input, and may write to an output label track as output. The waveform-to-labels effect can be used for many audio analysis applications, such as voice activity detection, sound event detection, musical instrument recognition, automatic speech recognition, etc. The output for waveform-to-labels models must be a tuple of two tensors. The first tensor corresponds to the class indexes for each label present in the waveform, shape `(num_timesteps,)`. The second tensor must contain timestamps with start and stop times for each label, shape `(num_timesteps, 2)`.  \n\n### What If My Model Uses a Spectrogram as Input/Output?\n\nIf your model uses a spectrogram as input/output, you'll need to wrap your forward pass with some torchscript-compatible preprocessing/postprocessing. We recommend using [torchaudio](https://pytorch.org/audio/stable/index.html), writing your own preprocessing transforms in their own `nn.Module`, or writing your PyTorch-only preprocessing and placing it in `WaveformToWaveform.do_forward_pass` or `WaveformToLabels.do_forward_pass`. See the [compatibility](#compat) section for more info.  \n\n<a name=\"metadata\"/>\n\n## Model Metadata\n\nCertain details about the model, such as its sample rate, tool type (e.g. waveform-to-waveform or waveform-to-labels), list of labels, etc. must be provided by the model contributor in a separate `metadata.json` file. In order to help users choose the correct model for their required task, model contributors are asked to provide a short and long description of the model, the target domain of the model (e.g. speech, music, environmental, etc.), as well as a list of tags or keywords as part of the metadata. See [here](#creating-metadata) for an example metadata dictionary. \n\n#### Metadata Spec\n\nrequired fields:\n\n- `sample_rate` (`int`)\n    - range `(0, 396000)`\n    - Model sample rate. Input tracks will be resampled to this value. \n- `domains` (`List[str]`)\n    - List of data domains for the model. The list should contain any of the following strings (any others will be ignored): `[\"music\", \"speech\", \"environmental\", \"other\"]`\n- `short_description`(`str`)\n    -  max 60 chars\n    -  short description of the model. should contain a brief message with the model's purpose, e.g. \"Use me for separating vocals from the background!\". \n-  `long_description` (`str`)\n    -  max 280 chars\n    -  long description of the model. Shown in the detailed view of the model UI.\n-  `tags` (`List[str]`)\n    -  list of tags (to be shown in the detailed view)\n    -  each tag should be 15 characters max\n    -  max 5 tags per model. \n-  `labels` (`List[str`)\n    -  output labels for the model. Depending on the effect type, this field means different things\n    -  **waveform-to-waveform**\n        -  name of each output source (e.g. `drums`, `bass`, `vocal`). To create the track name for each output source, each one of the labels will be appended to the mixture track's name.\n    -  **waveform-to-labels**:\n        -  This should be classlist for model. The class indexes output by the model during a forward pass will be used to index into this classlist.  \n-  `effect_type` (`str`)\n    -  Target effect for this model. Must be one of `[\"waveform-to-waveform\", \"waveform-to-labels\"]`. \n-  `multichannel` (`bool`)\n    -  If `multichannel` is set to `true`, stereo tracks are passed to the model as multichannel audio tensors, with shape `(2, n)`. Note that this means that the input could either be a mono track with shape `(1, n)` or stereo track with shape `(2, n)`.\n    -  If `multichannel` is set to `false`, stereo tracks are downmixed, meaning that the input audio tensor will always be shape `(1, n)`.\n\n---\n\n<a name=\"builtin\"/>\n\n## Making Your Model Built-In To Audacity\n\nBy default, users have to click on the `Add From HuggingFace` button on the Audacity Model Manager and enter the desired repo's ID to install a community contributed model. If you, instead, would like your community contributed model to show up in Audacity's Model Manager by default, please open a request [here](https://github.com/hugofloresgarcia/audacitorch/issues/new?assignees=hugofloresgarcia&labels=model-contrib&template=built-in-model-request.md&title=Built-in+Model+Request%3A+MODEL_NAME). \n\n<a name=\"example-wav2wav\"/>\n\n## Example - Waveform-to-Waveform model\n\nHere's a minimal example for a model that simply boosts volume by multiplying the incoming audio by a factor of 2. \n\nWe can sum up the whole process into 4 steps:\n\n1. [Developing your model](#developing)\n2. [Wrapping your model using `audacitorch`](#wrapping)\n3. [Creating a metadata document](#creating-metadata) \n4. [Exporting to HuggingFace](#exporting)\n\n<a name=\"developing\"/>\n\n### Developing your model\n\nFirst, we create our model. There are no internal constraints on what the internal model architecture should be, as long as you can use `torch.jit.script` or `torch.jit.trace` to serialize it, and it is able to meet the input-output constraints specified in waveform-to-waveform and waveform-to-labels models. \n\n```python\nimport torch\nimport torch.nn as nn\n\nclass MyVolumeModel(nn.Module):\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # do the neural net magic!\n        x = x * 2\n\n        return x\n```\n\n<a name=\"compat\"/>\n\n#### Making sure your model is compatible with `torchscript`\nPyTorch makes it really easy to deploy your Python models in C++ by using `torchscript`, an intermediate representation format for torch models that can be called in C++. Many of Python's built-in functions are supported by torchscript. However, not all Python operations are supported by the torchscript environment, meaning that you are only allowed to use a subset of Python operations in your model code. See [the torch.jit docs](https://pytorch.org/docs/master/jit.html#python-functions-and-modules) to learn more about writing torchscript-compatible code. \n\nIf your model computes spectrograms (or requires any kind of preprocessing/postprocessing), make sure those operations are compatible with torchscript, like [torchaudio](https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html#feature-extractions)'s operation set. \n\nUseful links:\n- [Torchscript reference](https://pytorch.org/docs/master/jit.html)\n- [Pytorch's tutorial on torchscript models](https://pytorch.org/tutorials/advanced/cpp_export.html#step-1-converting-your-pytorch-model-to-torch-script)\n- [A 1:1 mapping of the features in python to their support in torchscript](https://pytorch.org/docs/master/jit_python_reference.html#python-language-reference)\n- (**recommended**) [Mastering Torchscript: Tracing vs Scripting, Device Pinning, Direct Graph Modification](https://paulbridger.com/posts/mastering-torchscript/)\n\n<a name=\"wrapping\"/>\n\n### Wrapping your model using `audacitorch`\n\nNow, we create a wrapper class for our model. Because our model returns an audio waveform as output, we'll use `WaveformToWaveformBase` as our parent class. For both `WaveformToWaveformBase` and `WaveformToLabelsBase`, we need to implement the `do_forward_pass` method with our processing code. See the [docstrings](/audacitorch/core.py) for more details. \n\n```python\nfrom audacitorch import WaveformToWaveformBase\n\nclass MyVolumeModelWrapper(WaveformToWaveformBase):\n    \n    def do_forward_pass(self, x: torch.Tensor) -> torch.Tensor:\n        \n        # do any preprocessing here! \n        # expect x to be a waveform tensor with shape (n_channels, n_samples)\n\n        output = self.model(x)\n\n        # do any postprocessing here!\n        # the return value should be a multichannel waveform tensor with shape (n_channels, n_samples)\n    \n        return output\n```\n\n<a name=\"creating-metadata\"/>\n\n### Creating a metadata document\n\nAudacity models need a metadata file. See the metadata [spec](#metadata-spec) to learn about the required fields. \n\n```python\nmetadata = {\n    'sample_rate': 48000, \n    'domain_tags': ['music', 'speech', 'environmental'],\n    'short_description': 'Use me to boost volume by 3dB :).',\n    'long_description':  'This description can be a max of 280 characters aaaaaaaaaaaaaaaaaaaa.',\n    'tags': ['volume boost'],\n    'labels': ['boosted'],\n    'effect_type': 'waveform-to-waveform',\n    'multichannel': False,\n}\n\n```\n\nAll set! We can now proceed to serialize the model to torchscript and save the model, along with its metadata.\n\n```python\nfrom pathlib import Path\nfrom audacitorch.utils import save_model, validate_metadata, \\\n                              get_example_inputs, test_run\n\n# create a root dir for our model\nroot = Path('booster-net')\nroot.mkdir(exist_ok=True, parents=True)\n\n# get our model\nmodel = MyVolumeModel()\n\n# wrap it\nwrapper = MyVolumeModelWrapper(model)\n\n# serialize it using torch.jit.script, torch.jit.trace,\n# or a combination of both. \n\n# option 1: torch.jit.script \n# using torch.jit.script is preferred for most cases, \n# but may require changing a lot of source code\nserialized_model = torch.jit.script(wrapper)\n\n# option 2: torch.jit.trace\n# using torch.jit.trace is typically easier, but you\n# need to be extra careful that your serialized model behaves \n# properly after tracing\nexample_inputs = get_example_inputs()\nserialized_model = torch.jit.trace(wrapper, example_inputs[0], \n                                    check_inputs=example_inputs)\n\n# take your model for a test run!\ntest_run(serialized_model)\n\n# check that we created our metadata correctly\nsuccess, msg = validate_metadata(metadata)\nassert success\n\n# save!\nsave_model(serialized_model, metadata, root)\n```\n\n<a name=\"exporting\"/>\n\n### Exporting to HuggingFace\n\nYou should now have a directory structure that looks like this: \n\n```\n/booster-net/\n/booster-net/model.pt\n/booster-net/metadata.json\n```\n\nThis will be the repository for your audacity model. Make sure to add a readme with the `audacity` tag in the YAML metadata, so it show up on the explore tab of Audacity's Deep Learning Tools. \n\nCreate a `README.md` inside `booster-net/`, and add the following header:\n\n\nin README.md\n```\n---\ntags: audacity\n---\n```\n\nAwesome! It's time to push to HuggingFace. See their [documentation](https://huggingface.co/docs/hub/adding-a-model) for adding a model to the HuggingFace model hub. \n\n<a name=\"debugging\"/>\n\n## Debugging Your Model in Audacity\n\nAfter serializing, you may need to debug your model inside Audacity, to make sure that it handles inputs correctly, doesn't crash while processing, and produces the correct output. \nWhile debugging, make sure your model isn't available through other users through the `Explore HuggingFace` button by temporarily removing the `audacity` tag from your README file.\nIf your model fails internally while processing audio, you may see something like this:\n\n<img src=\"/assets/error.png\" width=300>\n\nTo debug, you can access the error logs through the Help menu, in `Help->Diagnostics->Show Log...`. Any torchscript errors that may occur during the forward pass will be redirected here. \n\n\n<a name=\"example-asteroid\"/>\n\n## Example - Exporting a Pretrained [Asteroid](https://github.com/asteroid-team/asteroid) model\n\nSee this [example notebook](notebooks/example.ipynb), where we serialize a pretrained ConvTasNet model for speech separation using the [Asteroid](https://github.com/asteroid-team/asteroid) source separation library.\n\n\n## Example - Exporting a Pretrained [S2T](https://huggingface.co/facebook/s2t-medium-librispeech-asr) model\n\nSee this [example notebook](notebooks/labeler-example.ipynb), where we serialize a pretrained speech to text transformer from Facebook.\n\n---\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/hugofloresgarcia/audacitorch",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "audacitorch",
    "package_url": "https://pypi.org/project/audacitorch/",
    "platform": "",
    "project_url": "https://pypi.org/project/audacitorch/",
    "project_urls": {
      "Homepage": "https://github.com/hugofloresgarcia/audacitorch"
    },
    "release_url": "https://pypi.org/project/audacitorch/0.0.4/",
    "requires_dist": [
      "torch (==1.9.0)",
      "jsonschema"
    ],
    "requires_python": "",
    "summary": "",
    "version": "0.0.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 12760338,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a7b9526b3a05c31378d153fb597e768aff6d0968e0c705a15e840d61042883e5",
          "md5": "1faf07ea13763237d1d9b36bd308d91a",
          "sha256": "e25585dc67c439d556b46cc28b74a8083625daff5e86829cb9fbedf828a84615"
        },
        "downloads": -1,
        "filename": "audacitorch-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1faf07ea13763237d1d9b36bd308d91a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10324,
        "upload_time": "2021-10-27T15:52:52",
        "upload_time_iso_8601": "2021-10-27T15:52:52.825540Z",
        "url": "https://files.pythonhosted.org/packages/a7/b9/526b3a05c31378d153fb597e768aff6d0968e0c705a15e840d61042883e5/audacitorch-0.0.1-py3-none-any.whl",
        "yanked": true,
        "yanked_reason": "dep. issues"
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3b74c000ea29206ce320d752faf6e58673e0411834b138e6d0496ec79517956e",
          "md5": "a4db32dfc89ff603c6c53b9849eaa2bc",
          "sha256": "e1c80917928bdb11dae3e9d972f830d4406fc2559de009ed5b95c259ed15adee"
        },
        "downloads": -1,
        "filename": "audacitorch-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "a4db32dfc89ff603c6c53b9849eaa2bc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 9671,
        "upload_time": "2021-10-27T15:52:54",
        "upload_time_iso_8601": "2021-10-27T15:52:54.569974Z",
        "url": "https://files.pythonhosted.org/packages/3b/74/c000ea29206ce320d752faf6e58673e0411834b138e6d0496ec79517956e/audacitorch-0.0.1.tar.gz",
        "yanked": true,
        "yanked_reason": "dep. issues"
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0e6ffbdeca013d3a226572e6de6e8fb0a5f1bb9dd71d0f7bfc80f2dc1b7b497b",
          "md5": "fc79cbd2b02cb0d01633a2cd8a13f99f",
          "sha256": "669bbdfb83bab670a7dc55b559bcaaf4a20786293e157d2aef56e43326288531"
        },
        "downloads": -1,
        "filename": "audacitorch-0.0.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "fc79cbd2b02cb0d01633a2cd8a13f99f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10514,
        "upload_time": "2022-01-19T21:45:37",
        "upload_time_iso_8601": "2022-01-19T21:45:37.405973Z",
        "url": "https://files.pythonhosted.org/packages/0e/6f/fbdeca013d3a226572e6de6e8fb0a5f1bb9dd71d0f7bfc80f2dc1b7b497b/audacitorch-0.0.3-py3-none-any.whl",
        "yanked": true,
        "yanked_reason": "dep. issues"
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "28f1a60e569c1f01cb6d19ca3bf75fe4e21d8fe2612b71eb2ef5de3424f96a0c",
          "md5": "b6631adf9d0ef11dd7b0e2c580972054",
          "sha256": "8e1d998f3d356cabbce1e842bdc48e6320a9f6941bd547d5a552a0b3198c21da"
        },
        "downloads": -1,
        "filename": "audacitorch-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b6631adf9d0ef11dd7b0e2c580972054",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10456,
        "upload_time": "2021-10-29T01:00:08",
        "upload_time_iso_8601": "2021-10-29T01:00:08.669065Z",
        "url": "https://files.pythonhosted.org/packages/28/f1/a60e569c1f01cb6d19ca3bf75fe4e21d8fe2612b71eb2ef5de3424f96a0c/audacitorch-0.0.2-py3-none-any.whl",
        "yanked": true,
        "yanked_reason": "issue with waveform to labels model"
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c45a64fa4a0a7ab62e8ba17281cf29c03edf72363e09900d70d19e056c04b482",
          "md5": "f27ef053598c5c7a317db90e5e4416cf",
          "sha256": "a9d4ba9a0685cbc0b8c05b435c51f83fdab5c1d906e5f5aa07773e5279fd4bf4"
        },
        "downloads": -1,
        "filename": "audacitorch-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "f27ef053598c5c7a317db90e5e4416cf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14558,
        "upload_time": "2021-10-29T01:00:09",
        "upload_time_iso_8601": "2021-10-29T01:00:09.835648Z",
        "url": "https://files.pythonhosted.org/packages/c4/5a/64fa4a0a7ab62e8ba17281cf29c03edf72363e09900d70d19e056c04b482/audacitorch-0.0.2.tar.gz",
        "yanked": true,
        "yanked_reason": "issue with waveform to labels model"
      }
    ],
    "0.0.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "55d0a3c616c8fb994de5c272d332098c2eff81d1cd48522d2db054b950d8eaae",
          "md5": "9ea9fc4a3eb7f5fffb1aafc3c9e613c9",
          "sha256": "1cff173604b8617ca6e7c7913933c33085cf1fd1947729c9cc749823a4350dcd"
        },
        "downloads": -1,
        "filename": "audacitorch-0.0.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9ea9fc4a3eb7f5fffb1aafc3c9e613c9",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 10514,
        "upload_time": "2022-02-02T00:32:22",
        "upload_time_iso_8601": "2022-02-02T00:32:22.829048Z",
        "url": "https://files.pythonhosted.org/packages/55/d0/a3c616c8fb994de5c272d332098c2eff81d1cd48522d2db054b950d8eaae/audacitorch-0.0.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "413826bb72718f95461535445064b74331e423c26c47484afd1522fad24338e5",
          "md5": "e006b44f6594261014df038224e8bd7f",
          "sha256": "fdc0d5576a13f8e9f8647623daf736e74cc97350c8be6ea6ef2da8952cce3c24"
        },
        "downloads": -1,
        "filename": "audacitorch-0.0.4.tar.gz",
        "has_sig": false,
        "md5_digest": "e006b44f6594261014df038224e8bd7f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14680,
        "upload_time": "2022-02-02T00:32:24",
        "upload_time_iso_8601": "2022-02-02T00:32:24.298429Z",
        "url": "https://files.pythonhosted.org/packages/41/38/26bb72718f95461535445064b74331e423c26c47484afd1522fad24338e5/audacitorch-0.0.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "55d0a3c616c8fb994de5c272d332098c2eff81d1cd48522d2db054b950d8eaae",
        "md5": "9ea9fc4a3eb7f5fffb1aafc3c9e613c9",
        "sha256": "1cff173604b8617ca6e7c7913933c33085cf1fd1947729c9cc749823a4350dcd"
      },
      "downloads": -1,
      "filename": "audacitorch-0.0.4-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "9ea9fc4a3eb7f5fffb1aafc3c9e613c9",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 10514,
      "upload_time": "2022-02-02T00:32:22",
      "upload_time_iso_8601": "2022-02-02T00:32:22.829048Z",
      "url": "https://files.pythonhosted.org/packages/55/d0/a3c616c8fb994de5c272d332098c2eff81d1cd48522d2db054b950d8eaae/audacitorch-0.0.4-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "413826bb72718f95461535445064b74331e423c26c47484afd1522fad24338e5",
        "md5": "e006b44f6594261014df038224e8bd7f",
        "sha256": "fdc0d5576a13f8e9f8647623daf736e74cc97350c8be6ea6ef2da8952cce3c24"
      },
      "downloads": -1,
      "filename": "audacitorch-0.0.4.tar.gz",
      "has_sig": false,
      "md5_digest": "e006b44f6594261014df038224e8bd7f",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 14680,
      "upload_time": "2022-02-02T00:32:24",
      "upload_time_iso_8601": "2022-02-02T00:32:24.298429Z",
      "url": "https://files.pythonhosted.org/packages/41/38/26bb72718f95461535445064b74331e423c26c47484afd1522fad24338e5/audacitorch-0.0.4.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}