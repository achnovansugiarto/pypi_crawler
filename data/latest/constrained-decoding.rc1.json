{
  "info": {
    "author": "Ayal Klein",
    "author_email": "ayal.s.klein@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# seq2seq_constrained_decoding\n\nThis project includes constrained-decoding utilities for *structured text generation* using Huggingface seq2seq models.\n\n## Requirements\n`pip install torch transformers`\n\nThe package is tested with `transformers==4.15.0`, might break for past or future versions.\n\n# Use cases\n\nSeveral use-cases leverage pretrained sequence-to-sequence models, such as BART or T5, \nfor generating a (maybe partially) structured text sequence. \nFor example, you may want to finetine the model to generate a set of key-points summarizing an input paragraph, or to produce a text sequence that follows some strict regularities. \n\nBelow we detail about the use-cases supported by this project. \n\n## Word lists\n\nThe `word_list_decoding.py` module suggests simple `LogitsProcessor` classes which enforce allowed / forbidden words during text generation (see the `WhiteListLogitsProcessor` and `BlackListLogitsProcessor` classes respectively).\n\nExample usage:\n\n```python\nfrom transformers import LogitsProcessorList, AutoTokenizer, AutoModel\nfrom constrained_decoding.word_list_decoding import BlackListLogitsProcessor\ntokenizer = AutoTokenizer.from_pretrained('t5-small')\nmodel = AutoModel.from_pretrained('t5-small')\n\nbad_words = \"here are words that should not occur in the generated text\"\nbad_word_ids = tokenizer.encode(bad_words)\nblack_list_processor = BlackListLogitsProcessor(bad_word_ids)\ngood_words = \"only these words can occur in the generated text\"\ngood_word_ids = tokenizer.encode(good_words)\nwhite_list_processor = WhiteListLogitsProcessor(good_word_ids)\n\ninput_seq = \"here are the input words to condition generated text upon\"\ninput_ids = tokenizer.encode(input_seq, return_tensors='pt')\nout = model.generate(input_ids, num_beams=10)\nprint(tokenizer.batch_decode(out))\n# ['<pad> Hier are the input words to condition generated text upon</s>']\nout = model.generate(input_ids, num_beams=10, logits_processor=[black_list_processor])\nprint(tokenizer.batch_decode(out))\n# ['<pad> Voici voici les input mots to condition a condition a condition a condition a']\nout = model.generate(input_ids, num_beams=10, logits_processor=[white_list_processor])\nprint(tokenizer.batch_decode(out))\n# ['<pad> in the words in the words in the words in the words in the words in the words in']\n```\n\n## Set decoding\n\nIn some scenarios, you would like to regard the output sequence as expressing a set of elements comprised of sub-sequences. For example, you might finetune your Seq2Seq model on a multi-label document classification task (e.g. generating the set of relation types occuring in the input document). \n\nThe `set_decoding.SetDecodingLogitProcessor` class can gurantee that no subsequence (e.g. a relation type) would occur more than once. Output subsequences are assumed to be defined using a special separator token.\n\n\n## DFA-based constrained decoding\n\nThe most powerful and generic constrained decoding algorithm we propose is using a *Deterministic Finite Automata* (DFA).\nYou can instanciate a DFA with the `dfa.DFA` class, defined over a dictionary of dictionaries. \nFor example, the following represents an automaton that accepts only binary numbers that are multiples of 3 (see [illustration](https://en.wikipedia.org/wiki/Deterministic_finite_automaton#/media/File:DFA_example_multiplies_of_3.svg) in the [Wikipedia article on DFA](https://en.wikipedia.org/wiki/Deterministic_finite_automaton)): \n```python\nfrom dfa import DFA\ntransitions = {0:{'0':0, '1':1},\n               1:{'0':2, '1':0},\n               2:{'0':1, '1':2}} \ndfa = DFA(transitions, s0=0, accept_states=[0])\n```\n\nFor defining constrained decoding using a DFA, the automaton's alphabet should correspond to tokens in the model's vocabulry.\nThe `DFA` class supports translating a `dfa` that uses regular words or phrases as alphabet into a tokenizer-adjusted dfa - \n\n```python\ntransitions = {0:{'John':1, 'Mike':1, 'Dan':1},\n               1:{'went':2, 'ran':2, 'jogged':2},\n               2:{'to':3, 'in':3},\n               3:{'the':4, 'a':4},\n               4:{'park':5}} \nwords_dfa = DFA(transitions, s0=0, accept_states=[5])\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\ntokens_dfa = words_dfa.adjust_for_tokenizer(tokenizer)\n```\n\nEventually, generation constraining is achieved by replacing the model's `beam_search` method with an adapted version (in `dfa_constrained_beam_search.py`) that\nenforces every beam to follow the automaton transitions. The probability of vocabulary entries that are not accessible according to the dfa will be set to minus infinity.\n\nFor example:\n\n```python\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained(\"t5-small\")\nfrom dfa_constrained_beam_search import set_model_beam_search_to_dfa_constrained\nset_model_beam_search_to_dfa_constrained(model, tokens_dfa)\n\n#   The previous two steps can equivalently be done in one call:\n# set_model_beam_search_to_dfa_constrained(model, words_dfa, tokenizer) \n```\n\nOther supported utility functions within the `DFA` class include:\n* `DFA.from_slots` - for constructing a linear DFA out of a list of \"slots\", where each slot is represented by a list of allowed words / phrases.\n* `DFA.concat_two` and `DFA.concat` - for concatenating two or more (linear) DFAs into a long DFA.\n* `as_cyclic` - for converting a linear DFA into a cyclic one, by merging or connecting some end-states with the initial state. \n\n\n### QA-SRL\n\nOur motivational use-case is [seq2seq-based QA-SRL parsing](https://huggingface.co/kleinay/qanom-seq2seq-model-joint). In that project, we finetune BART/T5 on the [Question-Answer driven Semantic Role Labeling](https://qasrl.org/) task. Given a verb or nominalization in context, the task is to generate Question-Answer pairs capturing the participants of the verbal event. \n\nTo model the task using a seq2seq paradigm, the QAs are linearized into a single target sequence, using separators between QA pairs, between question and answer, and between multiple answers. Furthermore, QASRL questions must adhere a slot-based template, while answers could only be continuous spans copied from the input sentence. \n\nCheck out the `qasrl.py` module to see how we leverage the `DFA` utilities for enforcing a valid QASRL output sequence.\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/kleinay/seq2seq_constrained_decoding",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "constrained-decoding",
    "package_url": "https://pypi.org/project/constrained-decoding/",
    "platform": null,
    "project_url": "https://pypi.org/project/constrained-decoding/",
    "project_urls": {
      "Homepage": "https://github.com/kleinay/seq2seq_constrained_decoding"
    },
    "release_url": "https://pypi.org/project/constrained-decoding/0.1.0/",
    "requires_dist": null,
    "requires_python": ">=3.7",
    "summary": "package for constrained text generation during decoding",
    "version": "0.1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16270768,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d36aced55867be6daae8ff1483d73e46365889313f28a5ae8ee220ac2a5f1958",
          "md5": "ebf4d475d7e00387dcfa7374b6bbdf88",
          "sha256": "6e617e74d0c54a844ea4f104c0cdb4211b16dc9c8609af07afc3055f721a08f7"
        },
        "downloads": -1,
        "filename": "constrained_decoding-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ebf4d475d7e00387dcfa7374b6bbdf88",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 58542,
        "upload_time": "2023-01-01T07:44:19",
        "upload_time_iso_8601": "2023-01-01T07:44:19.912974Z",
        "url": "https://files.pythonhosted.org/packages/d3/6a/ced55867be6daae8ff1483d73e46365889313f28a5ae8ee220ac2a5f1958/constrained_decoding-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d36aced55867be6daae8ff1483d73e46365889313f28a5ae8ee220ac2a5f1958",
        "md5": "ebf4d475d7e00387dcfa7374b6bbdf88",
        "sha256": "6e617e74d0c54a844ea4f104c0cdb4211b16dc9c8609af07afc3055f721a08f7"
      },
      "downloads": -1,
      "filename": "constrained_decoding-0.1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ebf4d475d7e00387dcfa7374b6bbdf88",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 58542,
      "upload_time": "2023-01-01T07:44:19",
      "upload_time_iso_8601": "2023-01-01T07:44:19.912974Z",
      "url": "https://files.pythonhosted.org/packages/d3/6a/ced55867be6daae8ff1483d73e46365889313f28a5ae8ee220ac2a5f1958/constrained_decoding-0.1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}