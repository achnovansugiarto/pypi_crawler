{
  "info": {
    "author": "Peng Zhang",
    "author_email": "p.zhang@zoho.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "Kedro to Dataiku\n## Convert Kedro project to Dataiku project in minutes\n This is a tool to enable one to deploy a Kedro (>=0.16.5) project on Dataiku DSS instance without modifying the original Kedro project at all.  \n\n- Automatic\n- Fast\n- Flexible\n\n## Features\n- Create Dataiku datasets automatically based on Kedro dataset catalog\n- Convert Kedro nodes into Dataiku recipes \n- Convert Kedro pipelines into Dataiku flow\n- Create flow zones in Dataiku project based on Kedro pipeline segmentation \n- Load all raw input data for the Kedro project into corresponding datasets in Dataiku proeject \n- Support PySpark through PySpark recipes in Dataiku \n- Support source code clone from git repository\n- Enable code editing via Dataiku project library\n\n## Adaption of Kedro project\n- As Dataiku flow is basically pandas dataset based, and every single Kedro node will be converted to Dataiku recipe, it is recommended to make inputs and ouputs of Kedro nodes in dataframe format. Pandas DataFrame, PySpark DataFrame, dictionary of Pandas DataFrame will be saeved into Dataiku datasets which can be previewed, while other types of inputs/outputs (array, string, dictionary, etc.) will be saved in managed folders as pickle object. \n- Nodes in Kedro must have distinct function names.\n- As Dataiku recipe must have at least one output, it is recommened to make sure that each Kedro node has at least one output too. However, just in case some nodes in Kedro do not explicitly have any output, this tool will automatically create dummy dataset outputs which are actually not meaningful.  \n- If there are local data files, it is required to define the file paths in data catalog configuration either in static format or in templated format (https://kedro.readthedocs.io/en/stable/kedro.config.TemplatedConfigLoader.html) with a macro named as ${data_prefix}. Example:\n    ```sh\n    ### data location\n    data\n      --01_raw\n        --sales.csv\n        \n    ### In conf/base/catalog.yml:\n    sales:\n      <<: *csv_tab\n      filepath: ${data_prefix}/01_raw/sales.csv\n    \n    ### in conf/local/globals.yml:\n    data_prefix: data/\n\n    ### or simply \n    sales:\n      <<: *csv_tab\n      filepath: data/01_raw/sales.csv\t\t\n    ```\n- In order to create Dataiku zones in flow automatically, there must be pipepline segmentation defined in context.pipelines. The keys of context.pipelines can be used to define zones. For example, if there are pipelines defined as the following, then we can use  [\"int\",\"primary\",\"master\",\"modeling\"] to define the zones.\n\n    ```sh\n    return {\n        \"int\": int,\n        \"primary\": primary,\n        \"master\": master,\n        \"modeling\": modeling,\n        \"__default__\": (\n                    int\n                    + primary\n                    + master\n                    + modeling\n                ),\n    }\n    ```\n## Installation\n\nAs the package depends on dataiku which is internal module in DSS instance, it is recommneded to install and use this package inside Dataiku DSS. \n\nInstall it in Dataiku DSS code enviroment like any other pip packages, or install in Jupyter notebook by\n```sh\n%pip install kedro_to_dataiku\n```\n\nThe required packages \"dataiku\" and \"kedro\" (>=0.16.5) will be the ones already exist in the Dataiku DSS environment. \n\nInstead of installing the package, one can also upload the kernel file https://github.com/ppvastar/kedro_to_dataiku/blob/main/kedro_to_dataiku/kedro_to_dataiku.py\nto Dataiku project library\n```sh\nlib/python\n```\nso that one can \n```sh\nfrom kedro_to_dataiku import *\n```\nin project code.\n\n## Usage\n1. Create a managed folder in Dataiku project. Let us suppose it to be \"workspace\".\n2. Compress (into zip) and upload the whole Kedro project root folder (containing subfolders like data, conf, src, etc) into the managed folder, and uncompress it there.\n3. Open Jupyter notebook in Dataiku, follow the following steps:\n\n* Initial set up\n    ```sh\n    import dataiku\n    from kedro_to_dataiku import *\n    \n    ### the absolut path to the Kedro project root folder in Dataiku DSS filesystem.\n    kedro_project_path=dataiku.Folder(\"workspace\").get_path()+\"[relative path of the kedro project root folder]\"\n    ### package_name: name of the folder in \"[kedro project root folder]/src/\" which contains \"nodes\" and \"pipelines\" subfolders\n    package_name=\"[Kedro project package name]\"\n    ### set dataset connection (location). Or any other established connections (like S3) in Dataiku DSS.\n    connection=\"filesystem_managed\" [or any other established connections (like S3) in Dataiku DSS]\n    ### data foramt in Dataiku dataset\n    format_type=\"csv\"\n    ### define recipe type. Or use \"pyspark\" if want to create pyspark recipes. \n    recipe_type=\"python\" \n    ### use source code residing in kedro_project_path+\"/src\". Otherwise, if True, will use source code imported as Dataiku python library -- this option will enable us to edit the soruce code residing in library.\n    src_in_lib=False \n    ### a list of zones to be created. They are from the keys of context.pipelines in the Kedro project. Example: [\"int\",\"primary\",\"master\",\"master_ds\",\"modeling\"]. Or just keep it as None so that no zones will be created automatically.\n    zone_list=None\n    ### if want to load the raw input data to Dataiku datasets. \n    load_data=False\n    ### if some inputs/outputs of Kedro projects are not Pandas dataframe/Spark dataframe/dictionary of Pandas dataframe format, they will be saved in managed folders instead of Dataiku datasets. This is critical to clarity.\n    folder_list=None \n    ```\n* Fast creation and clean\n    ```sh\n    ## fast creation and clean\n    ### one command to create the projects\n    create_all(kedro_project_path, package_name, connection, recipe_type,folder_list,zone_list,load_data,format_type,src_in_lib)\n    ### one command to clean the projects. Make sure not to delete the managed folder hosting the Kedro project.\n    delete_all(excluded=[\"workspace\"])\n    ``\n* Create the project step by step\n    ```sh  \n    ### update data catalog configuration\n    update_catalog_conf(kedro_project_path)\n    ### create datasets\n    input_list,dataset_list=create_datasets(kedro_project_path, package_name,connection,folder_list,format_type,src_in_lib)\n    ### create recipes\n    create_recipes(kedro_project_path, package_name,folder_list,recipe_type,src_in_lib)\n    ### create zones\n    create_zones(zone_list,folder_list,kedro_project_path, package_name,src_in_lib)\n    ### load raw input datasets\n    load_input_datasets(input_list,kedro_project_path, package_name,src_in_lib)\n    ```\n\n* Try some other tools:\n    ```sh\n    ### list all datastes\n    act_on_project(target=\"dataset\",cmd=\"list\")\n    ### clear data in all datastes\n    act_on_project(target=\"dataset\",cmd=\"clear\",excluded=None)\n    ### delete all datastes\n    act_on_project(target=\"dataset\",cmd=\"delete\",excluded=None)\n    \n    ### return all recipes\n    act_on_project(target=\"recipe\",cmd=\"list\")\n    ### delete all recipes\n    act_on_project(target=\"recipe\",cmd=\"delete\",excluded=None)\n    \n    ### return all zones\n    act_on_project(target=\"zone\",cmd=\"list\")\n    ### delete all zones except the \"Default\". Caution: do not delete this Default zone otherwise the project flow will corrupt.\n    act_on_project(target=\"zone\",cmd=\"delete\")\n\n    ### return all folders\n    act_on_project(target=\"folder\",cmd=\"list\")\n    ### delete all folders\n    act_on_project(target=\"folder\",cmd=\"delete\",excluded=['workspace'])\n\n    ```\n\n4. In Dataiku, the src code in managed folder is not editable. If one want to do simple and fast edit on code within dataiku after deployment, one can import the source code to project library (https://doc.dataiku.com/dss/latest/python/reusing-code.html) which is editable. To do this, just load (one can use git) the folder in \"[kedro project root folder]/src/\" which contains \"nodes\" and \"pipelines\" subfolders into the lib/python path (keep the module name as the kedro package name), and then set \n    ```sh\n    src_in_lib=True \n    ```\n    in previouly mentioned steps.\n    \n    By doing so, the soruce code (nodes, pipelines, etc) in this library \"lib/python/[package_name]\" will be used instead of the orgin one under \"[kedro project root folder]/src/[package_name]\" \n    \n    \n5. One can also clone Kedro project from git repository to the managed folder we created previously. \n    ```sh\n    git_url=\"[git repository URL]\"\n    kedro_project_path_in_git=\"[relative path of Kedro project root folder on git repository]\"\n    ### Keep existing data folder under the kedro_project_path in Dataiku managed folder\n    clone_from_git(kedro_project_path,git_url,kedro_project_path_in_git)\n    ```\n6. When code has been edited under path lib/python/[package_name], one may want to copy it back to the managed folder (for further download or other operations). To do this, one can use copy_lib function:\n    ```sh\n    ### overwrite=False, module lib/python/[package_name] will be copied to a new folder: [kedro_project_path]/src/[package_name]_lib\n    copy_lib(kedro_project_path,package_name,overwrite=False)\n    ### overwrite=True, [kedro_project_path]/src/[package_name] in managed folder will just be overwritten with the lib/python/[package_name]\n    copy_lib(kedro_project_path,package_name,overwrite=True)\n    ```\n## Example\nTaking the Kedro 0.17.0 IRIS starter project as an example (https://github.com/ppvastar/kedro_example_iris)\n\nOne can create a \"workspace\" managed folder on dataiku, and then upload the project root folder \"iris\" into the \"workspace\".\n\n![Alt text](image/iris_workspace.png?raw=true)\n\nThen run code like the following in Dataiku project jupyter notebook:\n\n```sh\nfrom kedro_to_dataiku import *\nimport dataiku\n\nkedro_project_path=dataiku.Folder(\"workspace\").get_path()+\"/iris\"\npackage_name=\"iris\"\n## change the connection according to actual situation\nconnection=\"S3_DSS\" \nrecipe_type=\"python\"\nsrc_in_lib=False\nload_data=True\nformat_type=\"csv\"\nfolder_list=[\"example_model\",\"example_predictions\"]\nzone_list=[\"ds\",\"de\"]\n\ncreate_all(kedro_project_path, package_name, connection, recipe_type,folder_list,zone_list,load_data,format_type,src_in_lib)\n```\n\nAs a magic, the Dataiku flow is created and raw input data is loaded in seconds:\n\n![Alt text](image/iris_flow.png?raw=true)\n\n\nThe flow is already ready for execution. Note that this simple Iris example is not a typical example, as at the final end there is actually no output (except some log information to report the model accuracy). As a resullt, the end of the Dataiku low is a dummy dataset called as \"report_accurary_dummy_output\" which will not be utilized. \n\nTo clear everything just now generated,\n    \n```sh\ndelete_all(excluded=[\"workspace\"])\n```",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ppvastar/kedro_to_dataiku",
    "keywords": "data science,pipeline,flowdataiku,kedro",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "kedro-to-dataiku",
    "package_url": "https://pypi.org/project/kedro-to-dataiku/",
    "platform": "",
    "project_url": "https://pypi.org/project/kedro-to-dataiku/",
    "project_urls": {
      "Homepage": "https://github.com/ppvastar/kedro_to_dataiku"
    },
    "release_url": "https://pypi.org/project/kedro-to-dataiku/0.3.6/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "Deploy Kedro project to Dataiku",
    "version": "0.3.6",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9860666,
  "releases": {
    "0.3.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "fb72014ba982f6939d174c1c91cca6c9c35a92109fb7780edd06e66cc4779e1e",
          "md5": "474065a89d0cdebcf506977ad4d92ee3",
          "sha256": "1a99845845edac1406b857ef2a0b58f5ecf7231864292b6a28a2c322342d5ab7"
        },
        "downloads": -1,
        "filename": "kedro_to_dataiku-0.3.6.tar.gz",
        "has_sig": false,
        "md5_digest": "474065a89d0cdebcf506977ad4d92ee3",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 14000,
        "upload_time": "2021-03-24T00:45:50",
        "upload_time_iso_8601": "2021-03-24T00:45:50.898062Z",
        "url": "https://files.pythonhosted.org/packages/fb/72/014ba982f6939d174c1c91cca6c9c35a92109fb7780edd06e66cc4779e1e/kedro_to_dataiku-0.3.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "fb72014ba982f6939d174c1c91cca6c9c35a92109fb7780edd06e66cc4779e1e",
        "md5": "474065a89d0cdebcf506977ad4d92ee3",
        "sha256": "1a99845845edac1406b857ef2a0b58f5ecf7231864292b6a28a2c322342d5ab7"
      },
      "downloads": -1,
      "filename": "kedro_to_dataiku-0.3.6.tar.gz",
      "has_sig": false,
      "md5_digest": "474065a89d0cdebcf506977ad4d92ee3",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 14000,
      "upload_time": "2021-03-24T00:45:50",
      "upload_time_iso_8601": "2021-03-24T00:45:50.898062Z",
      "url": "https://files.pythonhosted.org/packages/fb/72/014ba982f6939d174c1c91cca6c9c35a92109fb7780edd06e66cc4779e1e/kedro_to_dataiku-0.3.6.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}