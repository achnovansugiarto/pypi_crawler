{
  "info": {
    "author": "Chris Wood",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: Mozilla Public License 2.0 (MPL 2.0)",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: POSIX",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Topic :: Software Development :: Testing"
    ],
    "description": "# Python Testing Crawler   :snake: :stethoscope: :spider:\n[![PyPI version](https://badge.fury.io/py/python-testing-crawler.svg)](https://badge.fury.io/py/python-testing-crawler)\n[![PyPI Supported Python Versions](https://img.shields.io/pypi/pyversions/python-testing-crawler.svg)](https://pypi.python.org/pypi/python-testing-crawler/)\n[![GitHub license](https://img.shields.io/github/license/python-testing-crawler/python-testing-crawler)](https://github.com/python-testing-crawler/python-testing-crawler/blob/master/LICENSE.txt)\n[![GitHub Actions (Tests)](https://github.com/python-testing-crawler/python-testing-crawler/workflows/Tests/badge.svg)](https://github.com/python-testing-crawler/python-testing-crawler)\n\n_A crawler for automated functional testing of a web application_\n\nCrawling a server-side-rendered web application is a _low cost_ way to get _low quality_ test coverage of your JavaScript-light web application.\n\nIf you have only partial test coverage of your routes, but still want to protect against silly mistakes, then this is for you. \n\nFeatures:\n\n* Selectively spider pages and resources, or just request them\n* Submit forms, and control what values to send\n* Ignore links by source using CSS selectors\n* Fail fast or collect many errors\n* Configurable using straightforward rules\n\nWorks with the test clients for [Flask](https://flask.palletsprojects.com/en/1.1.x/testing/) (inc [Flask-WebTest](https://flask-webtest.readthedocs.io/en/latest/)), [Django](https://docs.djangoproject.com/en/3.0/topics/testing/tools/) and [WebTest](https://docs.pylonsproject.org/projects/webtest/en/latest/).\n\n## Why should I use this?\n\nHere's an example: [_Flaskr_](https://flask.palletsprojects.com/en/1.1.x/tutorial/), the Flask tutorial application has [166 lines of test code](https://github.com/pallets/flask/tree/master/examples/tutorial/tests) to achieve 100% test coverage.\n\n[Using Python Testing Crawler](https://github.com/python-testing-crawler/flaskr/blob/master/tests/test_crawl.py) in a similar way to the Usage example below, we can hit 73% with very little effort. Disclaimer: Of course! It's not the same quality or utility of testing! But it is better than no tests, a complement to hand-written unit or functional tests and a useful stopgap.\n\n## Installation\n\n```\n$ pip install python-testing-crawler\n```\n\n## Usage\n\nCreate a crawler using your framework's existing test client, tell it where to start and what rules to obey, then set it off:\n\n```python\nfrom python_testing_crawler import Crawler\nfrom python_testing_crawler import Rule, Request, Ignore, Allow\n\ndef test_crawl_all():\n    client = ## ... existing testing client\n    ## ... any setup ...\n    crawler = Crawler(\n        client=my_testing_client,\n        initial_paths=['/'],\n        rules=[\n            Rule(\"a\", '/.*', \"GET\", Request()),\n        ]\n    )\n    crawler.crawl()\n```\n\nThis will crawl all anchor links to relative addresses beginning \"/\". Any exceptions encountered will be collected and presented at the end of the crawl. For **more power** see the Rules section below.\n\nIf you need to authorise the client's session, e.g. login, then you should that before creating the Crawler.\n\nIt is also a good idea to create enough data, via fixtures or otherwise, to expose enough endpoints.\n\n### How do I setup a test client?\n\nIt depends on your framework:\n\n* Flask: https://flask.palletsprojects.com/en/1.1.x/testing/\n* Django: https://docs.djangoproject.com/en/3.0/topics/testing/tools/\n\n## Crawler Options\n\n| Param | Description |\n| --- | --- |\n| `initial_paths` |  list of paths/URLs to start from\n| `rules` | list of Rules to control the crawler; see below\n| `path_attrs` | list of attribute names to extract paths/URLs from; defaults to \"href\" -- include \"src\" if you want to check e.g. `<link>`, `<script>` or even `<img>`\n| `ignore_css_selectors` | any elements matching this list of CSS selectors will be ignored when extracting links\n| `ignore_form_fields` | list of form input names to ignore when determining the identity/uniqueness of a form. Include CSRF token field names here.\n| `max_requests` | Crawler will raise an exception if this limit is exceeded\n| `capture_exceptions` | upon encountering an exception, keep going and fail at the end of the crawl instead of during (default `True`)\n| `output_summary` | print summary statistics and any captured exceptions and tracebacks at the end of the crawl (default `True`)\n| `should_process_handlers` | list of \"should process\" handlers; see Handlers section\n| `check_response_handlers` | list of \"check response\" handlers; see Handlers section\n\n## Rules\n\nThe crawler has to be told what URLs to follow, what forms to post and what to ignore, using Rules.\n\nRules are made of four parameters:\n\n```Rule(<source element regex>, <target URL/path regex>, <HTTP method>, <action to take>)```\n\nThese are matched against every HTML element that the crawler encounters, with the last matching rule winning.\n\nActions must be one of the following objects:\n\n1. `Request(only=False, params=None)` -- follow a link or submit a form\n    - `only=True` will retrieve a page/resource but _not_ spider its links.\n    -  the dict `params` allows you to specify _overrides_ for a form's default values\n1. `Ignore()` -- do nothing / skip\n1. `Allow(status_codes)` -- allow a HTTP status in the supplied list, i.e. do not consider it an error.\n\n\n### Example Rules\n\n#### Follow all local/relative links\n\n```python\nHYPERLINKS_ONLY_RULE_SET = [\n    Rule('a', '/.*', 'GET', Request()),\n    Rule('area', '/.*', 'GET', Request()),\n]\n```\n\n#### Request but do not spider all links\n\n```python\nREQUEST_ONLY_EXTERNAL_RULE_SET = [\n    Rule('a', '.*', 'GET', Request(only=True)),\n    Rule('area', '.*', 'GET', Request(only=True)),\n]\n```\n\nThis is useful for finding broken links.  You can also check `<link>` tags from the `<head>` if you include the following rule _plus_ set the Crawler's `path_attrs` to `(\"HREF\", \"SRC\")`.\n\n```Rule('link', '.*', 'GET', Request())```\n\n#### Submit forms with GET or POST\n\n```python\nSUBMIT_GET_FORMS_RULE_SET = [\n    Rule('form', '.*', 'GET', Request())\n]\n\nSUBMIT_POST_FORMS_RULE_SET = [\n    Rule('form', '.*', 'POST', Request())\n]\n```\n\nForms are submitted with their default values, unless overridden using `Request(params={...})` for a specific form target or excluded using (globally) using the `ignore_form_fields` parameter to `Crawler` (necessary for e.g. CSRF token fields).\n\n#### Allow some routes to fail\n\n```python\nPERMISSIVE_RULE_SET = [\n    Rule('.*', '.*', 'GET', Allow([*range(400, 600)])),\n    Rule('.*', '.*', 'POST', Allow([*range(400, 600)]))\n]\n```\n\nIf any HTTP error (400-599) is encountered for any request, allow it; do not error.\n\n## Crawl Graph\n\nThe crawler builds up a graph of your web application. It can be interrogated via `crawler.graph` when the crawl is finished.\n\nSee [the graph module](python_testing_crawler/graph.py) for the defintion of `Node` objects.\n\n## Handlers\n\nTwo hooks points are provided. These operate on `Node` objects (see above).\n\n### Whether to process a Node\n\nUsing `should_process_handlers`, you can register functions that take a `Node` and return a `bool` of whether the Crawler should \"process\" -- follow a link or submit a form -- or not.\n\n### Whether a response is acceptable\n\nUsing `check_response_handlers`, you can register functions that take a `Node` and response object (specific to your test client) and return a bool of whether the response should constitute an error.\n\nIf your function returns `True`, the Crawler with throw an exception.\n\n## Examples\n\nThere are currently Flask and Django examples in [the tests](tests/).\n\nSee https://github.com/python-testing-crawler/flaskr for an example of integrating into an existing application, using Flaskr, the Flask tutorial application.\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "python testing crawler",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "python-testing-crawler",
    "package_url": "https://pypi.org/project/python-testing-crawler/",
    "platform": "",
    "project_url": "https://pypi.org/project/python-testing-crawler/",
    "project_urls": null,
    "release_url": "https://pypi.org/project/python-testing-crawler/0.2.2/",
    "requires_dist": [
      "bs4",
      "soupsieve",
      "dataclasses ; python_version < \"3.7\""
    ],
    "requires_python": ">=3.6",
    "summary": "Python Test Crawler",
    "version": "0.2.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10663276,
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b79725c8f37d2bd9566770285d37ae7e4dc766c29d714a0b579891f05410cd9f",
          "md5": "19d1f6977363ef1c8f8ba98af2bb6806",
          "sha256": "1a338da6faa4b5ff857810ea6fe575e020594d064e93a16ad3d996fb8943fecf"
        },
        "downloads": -1,
        "filename": "python_testing_crawler-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "19d1f6977363ef1c8f8ba98af2bb6806",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 31690,
        "upload_time": "2020-05-20T14:58:43",
        "upload_time_iso_8601": "2020-05-20T14:58:43.914934Z",
        "url": "https://files.pythonhosted.org/packages/b7/97/25c8f37d2bd9566770285d37ae7e4dc766c29d714a0b579891f05410cd9f/python_testing_crawler-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4bdb7fa528333bda1af2a08f64c78ee68ffbe0d256db1b0f7beca081b96390cf",
          "md5": "767b6799dc919a693a9c2e9b6465262a",
          "sha256": "3220d089d9de8c289867f280765454dfaf899cf2e1622c3e7dae3aece26731c9"
        },
        "downloads": -1,
        "filename": "python-testing-crawler-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "767b6799dc919a693a9c2e9b6465262a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 17817,
        "upload_time": "2020-05-20T14:58:45",
        "upload_time_iso_8601": "2020-05-20T14:58:45.801181Z",
        "url": "https://files.pythonhosted.org/packages/4b/db/7fa528333bda1af2a08f64c78ee68ffbe0d256db1b0f7beca081b96390cf/python-testing-crawler-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2b1f03a4417c1694ee00e835b0bf31cb5dcd82db5c2c03b461fa776844f9ee12",
          "md5": "0cff4ec32976331ea7bcc0dfe32f834d",
          "sha256": "8c296b8d7983581eeaa3ff6edf714ddd0c42391a86edcaf0a90c43ca5c15f7cf"
        },
        "downloads": -1,
        "filename": "python_testing_crawler-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0cff4ec32976331ea7bcc0dfe32f834d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32736,
        "upload_time": "2020-05-29T14:14:24",
        "upload_time_iso_8601": "2020-05-29T14:14:24.556199Z",
        "url": "https://files.pythonhosted.org/packages/2b/1f/03a4417c1694ee00e835b0bf31cb5dcd82db5c2c03b461fa776844f9ee12/python_testing_crawler-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e07d36ce54c4c3ada969632f5043271dbc2079d1d73fe8d15185d2e4710e2179",
          "md5": "e4722cf2bf9d6faf66346f35f66780be",
          "sha256": "0b2b7fa34911c984d55b90a26e0d5700c74fce1672616d0c830fda1af98f6efd"
        },
        "downloads": -1,
        "filename": "python-testing-crawler-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e4722cf2bf9d6faf66346f35f66780be",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 19990,
        "upload_time": "2020-05-29T14:14:26",
        "upload_time_iso_8601": "2020-05-29T14:14:26.383145Z",
        "url": "https://files.pythonhosted.org/packages/e0/7d/36ce54c4c3ada969632f5043271dbc2079d1d73fe8d15185d2e4710e2179/python-testing-crawler-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a8adc80c72bfadf7eee7796e1da65ec64feb85ccabf8322786643af3cc63a575",
          "md5": "0e9d9b81d00f8bb639d35e22ea3a278a",
          "sha256": "415fdb534b533e8bff6339a38c6d3870c363e446d412a8cf3242a648db116fa8"
        },
        "downloads": -1,
        "filename": "python_testing_crawler-0.2.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0e9d9b81d00f8bb639d35e22ea3a278a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 32754,
        "upload_time": "2021-06-16T13:43:09",
        "upload_time_iso_8601": "2021-06-16T13:43:09.641746Z",
        "url": "https://files.pythonhosted.org/packages/a8/ad/c80c72bfadf7eee7796e1da65ec64feb85ccabf8322786643af3cc63a575/python_testing_crawler-0.2.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a6629ad384236794e6f35bc43dfbdfffbb22e7013ec2dcd7d8c1df906dee60fd",
          "md5": "2a760a762e7f9b8afb676fea5cc90afc",
          "sha256": "fe6cf00e18da4e1e54023f741fab0e32943627f1d55b545c16fb32c256d59786"
        },
        "downloads": -1,
        "filename": "python-testing-crawler-0.2.2.tar.gz",
        "has_sig": false,
        "md5_digest": "2a760a762e7f9b8afb676fea5cc90afc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 21316,
        "upload_time": "2021-06-16T13:43:11",
        "upload_time_iso_8601": "2021-06-16T13:43:11.403127Z",
        "url": "https://files.pythonhosted.org/packages/a6/62/9ad384236794e6f35bc43dfbdfffbb22e7013ec2dcd7d8c1df906dee60fd/python-testing-crawler-0.2.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a8adc80c72bfadf7eee7796e1da65ec64feb85ccabf8322786643af3cc63a575",
        "md5": "0e9d9b81d00f8bb639d35e22ea3a278a",
        "sha256": "415fdb534b533e8bff6339a38c6d3870c363e446d412a8cf3242a648db116fa8"
      },
      "downloads": -1,
      "filename": "python_testing_crawler-0.2.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0e9d9b81d00f8bb639d35e22ea3a278a",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 32754,
      "upload_time": "2021-06-16T13:43:09",
      "upload_time_iso_8601": "2021-06-16T13:43:09.641746Z",
      "url": "https://files.pythonhosted.org/packages/a8/ad/c80c72bfadf7eee7796e1da65ec64feb85ccabf8322786643af3cc63a575/python_testing_crawler-0.2.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a6629ad384236794e6f35bc43dfbdfffbb22e7013ec2dcd7d8c1df906dee60fd",
        "md5": "2a760a762e7f9b8afb676fea5cc90afc",
        "sha256": "fe6cf00e18da4e1e54023f741fab0e32943627f1d55b545c16fb32c256d59786"
      },
      "downloads": -1,
      "filename": "python-testing-crawler-0.2.2.tar.gz",
      "has_sig": false,
      "md5_digest": "2a760a762e7f9b8afb676fea5cc90afc",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 21316,
      "upload_time": "2021-06-16T13:43:11",
      "upload_time_iso_8601": "2021-06-16T13:43:11.403127Z",
      "url": "https://files.pythonhosted.org/packages/a6/62/9ad384236794e6f35bc43dfbdfffbb22e7013ec2dcd7d8c1df906dee60fd/python-testing-crawler-0.2.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}