{
  "info": {
    "author": "Tom Charnock",
    "author_email": "tom@charnock.fr",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "######################################\nInformation maximising neural networks\n######################################\n\nOptimising a neural network to maximise the Fisher information provides us with a function able to massively compress data without losing information about parameters of interest. This function can then be used for likelihood-free inference.\n\nThe module here provides both the routines for fitting a neural network by maximising the Fisher information as well as a few methods for performing likelihood-free inference and approximate Bayesian computation.\n\nSpecifically, the neural network takes some data, |bf_d|, and maps it to a compressed summary, |f_bf_d_to_bf_x|, where |bf_x| can have the same dimensionality as that of the parameter space, rather than the data space, potentially without losing any information. To do so we maximise the Fisher information of the summary statistics provided by the neural network, and in doing so, find a functional form of the optimal compression.\n\nTo train the neural network a batch of simulations |bf_d^textrm_fid_| created at a fiducial parameter value |boldsymbol_theta| for training (and another for validation). These simulations are compressed by the neural network to obtain some statistic |bf_x^textrm_fid_|, i.e. the output of the neural network. We can use these to calculate the covariance, |bf_C_f|, of the compressed summaries. The sensitivity to model parameters uses the derivative of the simulation. This can be provided analytically or numercially using  |bf_d^textrm_fid_-2| created above the fiducial parameter value |boldsymbol_theta-2| and |bf_d^textrm_fid_-1| created below the fiducial parameter value |boldsymbol_theta-1| The simulations are compressed using the network and used to find mean of the summaries\n\n.. image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/frac_partial_bol.png\n\nIf the derivative of the simulations with respect to the parameters can be calculated analytically (or via autograd, etc.) then that can be used directly using the chain rule since the derivative of the network outputs with respect to the network input can be calculated easily\n\n.. image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/frac_partial_bol-1.png\n\nWe then use |bf_C_f| and |partial_boldsymb| to calculate the Fisher information\n\n.. image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/bf_F_alpha_beta_.png\n\nSince any linear rescaling of the summaries is also a summary, when maximising the Fisher information we set their scale using\n\n.. image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/Lambda_%3D_-_ln%7C_b.png\n\nwhere\n\n.. image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/Lambda_2_%3D_%7C%7C_bf.png\n\nis a regularisation term whose strength is dictated by\n\n.. image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/r(_Lambda_2)=_fr.png\n\nwith |lambda| as a strength and |alpha| as a rate parameter which can be determined from a closeness condition on the Frobenius norm of the difference between the covariance (and inverse covariance) from the identity matrix.\n\nWhen using this code please cite\n\n  Tom Charnock, Guilhem Lavaux and Benjamin D. Wandelt\n\n  Automatic physical inference with information maximizing neural networks.\n\n  Physical Review D. 97 083004 (2018)\n\n  doi:10.1103/PhysRevD.97.083004\n\n  arXiv:1802.03537\n\nThe code in the paper can be downloaded as v1 or v1.1 of the code kept on zenodo:\n\n  https://doi.org/10.5281/zenodo.1175196\n\nThe code can be installed using::\n\n  pip install imnn-tf\n\nor::\n\n  git clone https://bitbucket.org/tomcharnock/imnn-tf.git\n  cd imnn-tf\n  python3 setup.py install\n\n*******\nModules\n*******\n\nAvailable are modules for fitting the IMNN in ``IMNN`` and for doing likelihood-free inference in ``LFI``. Examples of how to use these modules are available in the ``examples`` directory.\n\n====\nIMNN\n====\n\nThe basic call for the IMNN is\n\n.. code-block:: python\n\n  imnn = IMNN.IMNN(\n      n_s, n_d, n_params, n_summaries, θ_fid, δθ, input_shape,\n      fiducial, derivative, validation_fiducial, validation_derivative,\n      {model}, {optimiser}, {save}, {load}, {weights}, {directory},\n      {filename}, {at_once}, {map_fn}, {check_shape}, {verbose},\n      {dtype}, {itype})\n\nwhere\n\n - ``n_s`` - number of simulations to calculate covariance of fiducial simulations\n - ``n_d`` - number of simulations to calculate mean of the derivative simulations\n - ``n_params`` - number of parameters in the model\n - ``n_summaries`` - number of summaries to compress the data to\n - ``θ_fid`` - fiducial parameter values\n - ``δθ`` - parameter differences for numerical derivative\n - ``input_shape`` - shape of a single simulation\n - ``fiducial`` - a numpy array of fiducial simulations, generative function of individual fiducial simulations, or list of TFRecord filenames\n - ``derivative`` -  a numpy array of derivative simulations, generative function of individual derivative simulations, or list of TFRecord filenames\n - ``validation_fiducial`` - a numpy array of fiducial simulations, generative function of individual fiducial simulations, or list of TFRecord filenames\n - ``validation_derivative`` -  a numpy array of derivative simulations, generative function of individual derivative simulations, or list of TFRecord filenames\n - ``model`` - a ``keras``-like model (optional if loading)\n - ``optimiser`` - a ``keras``-like optimiser (optional if loading)\n - ``save`` - boolean describing whether to save or not (need ``directory`` and ``filename`` if `save=True`)\n - ``load`` - boolean describing whether to load model or not (need ``directory``, ```filename`` and optionally ``weights`` if ``load=True``)\n - ``weights`` - string with name of file of saved weights\n - ``directory`` - string with directory to load or save model\n - ``filename`` - string with filename to load or save model\n - ``at_once`` - number of simulations to process with model at once (should be ``n_s`` if memory is large enough)\n - ``map_fn`` - function to preprocess data (``None`` if no preprocessing)\n - ``check_shape`` - boolean describing whether to check shape of simulation on initialisation\n - ``verbose`` - boolean to turn on and off descriptive write out\n - ``dtype`` - TensorFlow float type (default ``tf.float32``)\n - ``itype`` - TensorFlow int type (default ``tf.int32``)\n\n---\nFit\n---\n\n.. code-block:: python\n\n  imnn.fit(\n    {n_iterations}, {λ}, {ϵ}, {reset}, {patience}, {min_iterations},\n    {checkpoint}, {tqdm_notebook}, {weight_file})\n\nwhere\n\n - ``n_iterations`` - number of iterations to run the fitting for (can be ``None`` when using patience)\n - ``λ`` - strength of the regularisation\n - ``ϵ`` - distance of covariance (and inverse covariance) from the identity\n - ``reset`` - boolean describing whether to reset weights and start training from scratch\n - ``patience`` - number of iterations of decreasing Fisher information of the validation set before stopping\n - ``min_iterations`` - number of iterations before early stopping turns on\n - ``checkpoint`` - number of iterations between model saving (default turned off)\n - ``tqdm_notebook`` - ``True`` if using a Jupyter notebook and False otherwise #TODO - make automatic (might already be implemented)\n - ``weight_file`` - string with filename to save model weights to\n\nOnce trained, statistics are saved in a history dictionary attribute ``imnn.history``\n\n - ``\"det_F\"`` - determinant of the Fisher information of the training set\n - ``\"det_C\"`` - determinant of the covariance matrix of the training set\n - ``\"det_Cinv\"`` - determinant of the inverse covariance matrix of the training set\n - ``\"dμ_dθ\"`` - derivative of the mean of the training set summaries\n - ``\"reg\"`` - value of the regularisation\n - ``\"r\"`` - value of the dynamic strength of the regularisation\n - ``\"val_det_F\"`` - determinant of the Fisher information of the validation set\n - ``\"val_det_C\"`` - determinant of the covariance matrix of the validation set\n - ``\"val_det_Cinv\"`` - determinant of the inverse covariance matrix of the validation set\n - ``\"val_dμ_dθ\"`` - derivative of the mean of the validation set summaries\n\n----\nPlot\n----\n\n.. code-block:: python\n\n  imnn.plot(\n      {regulariser}, {known_det_fisher}, {figsize})\n\nwhere\n\n - ``regulariser`` - boolean describing whether to plot the regularisation history\n - ``known_det_fisher`` - value of the determinant of the target Fisher information if already known\n - ``figsize`` - tuple with the size of the figure if not default\n\n-------------------\nEstimate parameters\n-------------------\n\nGaussian estimates of the parameter values can be obtained from the network by running\n\n.. code-block:: python\n\n  imnn.get_estimate(input_data)\n\nwhere ```input_data`` is data input to the network (shape ``None + input_shape``). Note that if you want to make estimates without initialising the IMNN (once trained), the model can be loaded, along with the saved data during fit. For an IMNN saved with ``directory=\"model\"`` and ``filename=model`` then an estimator can be made using\n\n.. code-block:: python\n\n  estimator_parameters = np.load(\"model/model/estimator.npz\")\n  Finv = estimator_parameters[\"Finv\"]\n  θ_fid = estimator_parameters[\"θ_fid\"]\n  dμ_dθ = estimator_parameters[\"dμ_dθ\"]\n  Cinv = estimator_parameters[\"Cinv\"]\n  μ = estimator_parameters[\"μ\"]\n\n  @tf.function:\n  def estimator(data):\n      return tf.add(\n          θ_fid,\n          tf.einsum(\n              \"ij,jk,kl,ml->mi\",\n              Finv,\n              dμ_dθ,\n              Cinv,\n              model(data) - μ))\n\nor\n\n.. code-block:: python\n\n  def estimator(data):\n      return θ_fid + np.einsum(\n          \"ij,jk,kl,ml->mi\",\n          Finv,\n          dμ_dθ,\n          Cinv,\n          model(data) - μ)\n\n-----------------------------------\nTraining and validation data format\n-----------------------------------\n\nThe data must have the correct shape. For a single simulation with shape ``input_shape`` then a fiducial data array must have a shape of\n\n.. code-block:: python\n\n  fiducial.shape = (n_s,) + input_shape\n\nThe derivatives need to have a shape of\n\n.. code-block:: python\n\n  derivative.shape = (n_d, 2, n_params) + input_shape\n\nwhere ``derivative[:, 0, ...]`` is the lower part of the numerical derivative and ``derivative[:, 1, ...]`` is the upper part of the numerical derivative and ``derivative[:, :, i, ...]`` labels the ``i`` th parameter.\n\nIf the data won't fit in memory then we can load data via a generative function\n\n.. code-block:: python\n\n  def fiducial_loader(seed):\n      yield fiducial[seed], seed\n\n  def derivative_loader(seed, derivative, parameter):\n      yield derivative[seed, derivative, parameter] (seed, derivative, parameter)\n\nThe function yields a single simulation at for each call labelled with the seed index (``seed`` in range ``0`` to ``n_s``) for the fiducial loader. The derivative loader yields a single simulation at a given seed, given upper or lower derivative and given parameter index (``seed`` in range ``0`` to ``n_d``, ``derivative`` in range ``0`` to ``1``, and ``parameter`` in range ``0`` to ``n_params``). In the above functions, ``fiducial`` and ``derivative`` are some way of grabbing the data - it could be reading from file or from memory, etc. This has quite a bit of overhead and so it would be preferred to save the data as a TFRecord format. Instructions on how to do this for ingestion by the IMNN is available in the ``examples/TFRecords.ipynb`` and ``examples/IMNN - TFRecords.ipynb`` tutorials.\n\n---------------------------\nNetwork model and optimiser\n---------------------------\n\nThe IMNN is based on ``keras``-like network and optimisers, so an example could be\n\n.. code-block:: python\n\n  model = tf.keras.Sequential(\n      [tf.keras.Input(shape=input_shape),\n       tf.keras.layers.Dense(128),\n       LeakyReLU(0.01),\n       tf.keras.layers.Dense(128),\n       LeakyReLU(0.01),\n       tf.keras.layers.Dense(n_summaries),\n      ])\n  opt = tf.keras.optimizers.Adam()\n\nMake sure to choose this network sensibly so that it best pulls the information from the data.\n\n***\nLFI\n***\n\nThe LFI module provides a Gaussian approximation to the posterior, a simple approximation Bayesian computation (ABC) implementation and a population Monte Carlo (PMC). These work with any estimator and not just with the IMNN.\n\n=======================================\nGaussian approximation of the posterior\n=======================================\n\nThe Gaussian approximation takes the inverse Fisher information as the variance of a Gaussian posterior (as implied by the Cramer-Rao bound) whose mean is at the estimate value.\n\n.. code-block:: python\n\n  GA = LFI.GaussianApproximation(\n    target_data, prior, Fisher, get_estimate, {labels})\n\nwhere\n\n - ``target_data`` - as many pieces of data to be inferred (``target_data.shape = (None,) + input_shape``)\n - ``prior`` - the prior distribution which can be sampled from and whose probability can be evaluated with an ``event_shape`` of at least ``[1]`` (suggested to use a TensorFlow Probability distribution)\n - ``Fisher`` - Fisher information matrix (``imnn.F`` or otherwise for non-IMNN)\n - ``get_estimate`` - function providing estimate of the ``n_params`` model parameters from the data (``imnn.get_estimate`` or otherwise for non-IMNN)\n - ``labels`` - list of strings for labelling plots\n\n-----------------------\nPlot Fisher information\n-----------------------\n\nThe inverse Fisher information can plotted using\n\n.. code-block:: python\n\n  GA.plot_Fisher({figsize})\n\n------------------------------------------------------\nGaussian approximation to the likelihood and posterior\n------------------------------------------------------\n\nThe Gaussian approximation to the likelihood (``prob``) and the posterior (and their logarithms) can be obtained using\n\n.. code-block:: python\n\n  GA.log_prob({grid}, {gridsize})\n  GA.prob({grid}, {gridsize})\n  GA.log_posterior({grid}, {gridsize})\n  GA.posterior({grid}, {gridsize})\n\nwhere\n\n - ``grid`` - a set of parameters or an array of parameter or a meshgrid of parameter to evaluate the likelihood or posterior at (if ``None`` gridsize takes over)\n - ``gridsize`` - a tuple of length ``n_params`` with the size of the meshgrid to make #TODO might crash if ``GA.prior.low=-np.inf`` for any parameter or ``GA.prior.high=np.inf`` for any parameter. This defaults to ``20`` for every parameter if ``grid=None`` and ``gridsize`` is not provided\n\n------------------\nPlotting posterior\n------------------\n\nThe posterior can be plotted using\n\n.. code-block:: python\n\n  GA.plot({grid}, {gridsize}, **kwargs)\n\nwhere ``**kwargs`` are a variety of ``matplotlib`` arguments.\n\n======================================\nApproximate Bayesian computation (ABC)\n======================================\n\nThe ABC draws parameter values from the prior and makes simulations at these points. These simulations are then summarised and then the distance between these estimates and the estimate of the target data can be calculated. Estimates within some small ϵ-ball around the target estimate are approximately samples from the posterior. Note that the larger the value of ϵ, the worse the approximation to the posterior.\n\nNote that a simulator of the data is needed. The simulator must be a function\n\n.. code-block:: python\n\n\n  def simulator(parameters, seed, simulator_args):\n      return simulation\n\nwhere ``seed`` is a random number generator and ``simulator_args`` is a dict of arguments. The ``seed`` and ``simulator_args`` are only for setting up the simulator - the function used in the ABC (and PMC) call must only take an array of parameters and return an array of simulations made at those parameter values. The function can call external codes, submit jobs on a cluster, etc. as long as the simulations are returned in the same order as the passed parameter array.\n\nThe ABC can be initialised using\n\n.. code-block:: python\n\n  ABC = LFI.ApproximateBayesianComputation(\n      target_data, prior, Fisher, get_estimate, simulator, {labels})\n\nwhere\n\n - ``target_data`` - as many pieces of data to be inferred (``target_data.shape = (None,) + input_shape``)\n - ``prior`` - the prior distribution which can be sampled from and whose probability can be evaluated with an ``event_shape`` of at least ``[1]`` (suggested to use a TensorFlow Probability distribution)\n - ``Fisher`` - Fisher information matrix (``imnn.F`` or otherwise for non-IMNN)\n - ``get_estimate`` - function providing estimate of the ``n_params`` model parameters from the data (``imnn.get_estimate`` or otherwise for non-IMNN)\n - ``simulator`` - function taking array of parameter values and returning simulations made at those values\n - ``labels`` - list of strings for labelling plots\n\n-----------------\nObtaining samples\n-----------------\n\nThe ABC can be run using\n\n.. code-block:: python\n\n  ABC(draws, {at_once}, {save_sims})\n\nor\n\n.. code-block:: python\n\n  ABC.ABC(draws, {at_once}, {save_sims}, {PMC}, {update})\n\nwhere\n\n - ``draws`` - the number of simulations to make (or parameter values to make the simulations if ``PMC=True``)\n - ``at_once`` - boolean describing whether to process (and make) all simulations at once or not\n - ``save_sims`` - string with the filename to save the sims (as a ``.npy``) if provided\n - ``PMC`` - boolean describing whether ``draws`` is a number of simulations or ``draws`` is an array of parameter values to make simulations at\n - ``update`` - boolean describing whether to update the ABC attributes onces the ABC is run or not\n\nOnce this is run the parameters, estimates, differences from the estimate and the target and the distance from the target are found as\n\n - ``ABC.parameters``\n - ``ABC.estimates``\n - ``ABC.differences``\n - ``ABC.distances``\n\n----------------------------------\nAcception and rejection of samples\n----------------------------------\n\n``ABC`` only runs the simulations and calculates the estimate distances but doesn't do the accept and reject step within the ϵ-ball. This is done using\n\n.. code-block:: python\n\n  ABC.accept_reject(ϵ)\n\nwhere\n\n - ``ϵ``` - a float describing the radius of the ϵ-ball\n\nOnce this is run more attributes are filled\n\n - ``ABC.num_accepted`` - number of accepted samples\n - ``ABC.num_rejected`` - number of rejected samples\n - ``ABC.num_draws`` - total number of samples done\n - ``ABC.accepted_parameters``\n - ``ABC.accepted_differences``\n - ``ABC.accepted_estimates``\n - ``ABC.accepted_distances``\n - ``ABC.rejected_parameters``\n - ``ABC.rejected_differences``\n - ``ABC.rejected_estimates``\n - ``ABC.rejected_distances``\n\n---------------------------\nAutomatic rejection sampler\n---------------------------\n\nTo get a certain number of draws within a chosen ϵ-ball one can run\n\n.. code-block:: python\n\n  ABC.get_min_accepted(\n      ϵ, accepted, {min_draws}, {at_once}, {save_sims}, {tqdm_notebook})\n\nwhere\n\n - ``ϵ`` - a float describing the radius of the ϵ-ball\n - ``accepted`` - the number of samples to be accepted within the ϵ-ball\n - ``min_draws`` - how many simulations to do at a time iteratively until enough simulations are accepted\n - ``at_once`` - boolean describing whether to process (and make) all simulations at once or not\n - ``save_sims`` - string with the filename to save the sims (as a ``.npy``) if provided\n - ``tqdm_notebook`` - True if using a Jupyter notebook and False otherwise #TODO - make automatic (might already be implemented)\n\n----------------------\nHistogrammed posterior\n----------------------\n\nThe posterior is approximated by histogramming the accepted samples from the ABC (and acception/rejection) and can be calculated using\n\n.. code-block:: python\n\n  ABC.posterior(\n      {bins}, {ranges}, {ϵ}, {draws}, {accepted},\n      {at_once}, {save_sims}, {tqdm_notebook})\n\nwhere\n\n - ``bins`` - number of bins in the histogram defining the posterior\n - ``ranges`` - minimum and maximum values for each parameter in the histogram\n\nOptionally any of the parameters for ``ABC.ABC(...)``, ``ABC.accept_reject(...)``, and/or ``ABC.get_min_accepted(...)`` can be passed to ``ABC.posterior(...)`` to run the ABC when calling posterior rather than calling the sampling step first.\n\n---------------\nPlot plosterior\n---------------\n\nThe posterior can be plotted using\n\n.. code-block:: python\n\n  ABC.plot(\n      {smoothing}, {bins}, {ranges}, {ϵ}, {draws}, {accepted},\n      {at_once}, {save_sims}, {tqdm_notebook}, **kwargs)\n\nwhere\n\n - ``smoothing`` - the pixel range of a Gaussian smoothing of the histogram for plotting (smoothing causes inflation of the posterior)\n\nOptionally any of the parameters for ``ABC.ABC(...)``, ``ABC.accept_reject(...)``, and/or ``ABC.get_min_accepted(...)`` can be passed to ``ABC.plot(...)`` to run the ABC when making the plot rather than calling the sampling step first. ``matplotlib`` parameters can also be passed for the plotting routine.\n\n------------\nPlot samples\n------------\n\nThe samples can also be plotted using\n\n.. code-block:: python\n\n  ABC.scatter_plot(\n      {axes}, {rejected}, {ϵ}, {draws}, {accepted},\n      {at_once}, {save_sims}, {tqdm_notebook}, **kwargs)\n\nwhere\n\n - ``axes`` - either ``\"parameter_estimate\"``, ``\"parameter_parameter\"``, or ``\"estimate_estimate\"`` for plotting the estimates against the parameters, or the parameters against the parameters or the estimates against the estimates (the last two are good for diagnostics such as the completeness of the sampling from the prior and the shape and correlation of the estimation function)\n - ``rejected`` - a number between ``0`` and ``1`` describing the fraction of the rejected samples to plot (there are often orders of magnitude more samples rejected and so it makes sense to plot fewer, if they are to be plotted at all)\n\nOptionally any of the parameters for ``ABC.ABC(...)``, ``ABC.accept_reject(...)``, and/or ``ABC.get_min_accepted(...)`` can be passed to ``ABC.scatter_plot(...)`` to run the ABC when making the plot rather than calling the sampling step first. ``matplotlib`` parameters can also be passed for the plotting routine.\n\n------------------------------\nRunning with saved simulations\n------------------------------\n\nIf simulations have already been run and we want to perform a simple ABC on them then we can set the simulator to return the saved simulations, ``saved_simulations -> (?) + input_shape``, and pass the corresponding saved parameters, ``saved_parameters -> (?, n_params)```, used to make the simulation.\n\n.. code-block:: python\n\n  simulator = lambda _ : saved_simulations\n  ABC = LFI.ApproximateBayesianComputation(\n      target_data, prior, Fisher, get_estimate, simulator, {labels})\n  ABC(draws=saved_parameters, predrawn=True, save_sims=None, {at_once})\n  ABC.accept_reject(ϵ)\n\n============================\nPopulation Monte Carlo (PMC)\n============================\n\nWhilst we can obtain approximate posteriors using ABC, the rejection rate is very high because we sample always from the prior. Population Monte Carlo (PMC) uses statistics of the population of samples to propose new parameter values, so each new simulation is more likely to be accepted. This prevents us needing to define an ϵ parameter to define the acceptance distance. Instead we start with a population from the prior and iteratively move samples inwards. Once it becomes difficult to move the population any more, i.e. the number of attempts to accept a parameter becomes very large, then the distribution is seen to be a stable approximation to the posterior.\n\nThe whole module works very similarly to ``ABC`` with a few changes in arguments.\n\n.. code-block:: python\n\n  PMC = LFI.PopulationMonteCarlo(\n      target_data, prior, Fisher, get_estimate, simulator, {labels})\n\nwhere\n\n - ``target_data`` - as many pieces of data to be inferred (``target_data.shape = (None,) + input_shape``)\n - ``prior`` - the prior distribution which can be sampled from and whose probability can be evaluated with an ``event_shape`` of at least ``[1]`` (suggested to use a TensorFlow Probability distribution)\n - ``Fisher`` - Fisher information matrix (``imnn.F`` or otherwise for non-IMNN)\n - ``get_estimate`` - function providing estimate of the `n_params` model parameters from the data (``imnn.get_estimate`` or otherwise for non-IMNN)\n - ``simulator`` - function taking array of parameter values and returning simulations made at those values\n - ``labels`` - list of strings for labelling plots\n\n--------------------------\nObtaining accepted samples\n--------------------------\n\nThe PMC can be run by calling\n\n.. code-block:: python\n\n  PMC(draws, initial_draws, criterion, {percentile},\n      {at_once}, {save_sims}, {tqdm_notebook})\n\nor\n\n.. code-block:: python\n\n  PMC.PMC(\n      draws, initial_draws, criterion, {percentile},\n      {at_once}, {save_sims}, {tqdm_notebook})\n\nwhere\n\n - ``draws`` - number of samples from the posterior\n - ``initial_draws`` - number of samples from the prior to start the PMC (must be equal to or greater than the number of draws from the posterior\n - ``criterion`` - the stopping condition, the fraction of times samples are accepted in any one iteration of the PMC (when this is small then many samples are not accepted into the population, suggesting a stationary distribution)\n - ``percentile`` - the percentage of points which are considered the in the main sample (making this small moves more samples at once, but with reduced statistics from the population, default set to 75%, it takes longer to run (but may be cheaper in number of simulations) if set to a high value or ``None``)\n - ``at_once`` - boolean describing whether to process (and make) all simulations at once or not\n - ``save_sims`` - string with the filename to save the sims (as a ``.npy``) if provided\n - ``tqdm_notebook`` - ``True`` if using a Jupyter notebook and False otherwise #TODO - make automatic (might already be implemented)\n\n----------------------\nHistogrammed posterior\n----------------------\n\nThe posterior is approximated by histogramming the accepted samples from the PMC and can be calculated using\n\n.. code-block:: python\n\n  PMC.posterior(\n      {bins}, {ranges}, {draws}, {initial_draws}, {criterion}, {percentile},\n      {at_once}, {save_sims}, {tqdm_notebook})\n\nwhere\n\n - ``bins`` - number of bins in the histogram defining the posterior\n - ``ranges`` - minimum and maximum values for each parameter in the histogram\n\nOptionally any of the parameters for ``PMC.PMC(...)`` can be passed to ``PMC.posterior(...)``` to run the PMC when calling posterior rather than calling the sampling step first.\n\n---------------\nPlot posterior\n---------------\n\nThe posterior can be plotted using\n\n.. code-block:: python\n\n  PMC.plot(\n      {smoothing}, {bins}, {ranges}, {draws}, {initial_draws}, {criterion},\n      {percentile}, {at_once}, {save_sims}, {tqdm_notebook}, **kwargs)\n\nwhere\n\n - ``smoothing`` - the pixel range of a Gaussian smoothing of the histogram for plotting (smoothing causes inflation of the posterior)\n\nOptionally any of the parameters for ``PMC.PMC(...)`` can be passed to ``PMC.plot(...)`` to run the PMC when making the plot rather than calling the sampling step first. ``matplotlib`` parameters can also be passed for the plotting routine.\n\n------------\nPlot samples\n------------\n\nThe samples can also be plotted using\n\n.. code-block:: python\n\n  PMC.scatter_plot(\n      {axes}, {draws}, {initial_draws}, {criterion}, {percentile},\n      {at_once}, {save_sims}, {tqdm_notebook}, **kwargs)\n\nwhere\n\n - ``axes`` - either ``\"parameter_estimate\"``, ``\"parameter_parameter\"``, or ``\"estimate_estimate\"`` for plotting the estimates against the parameters, or the parameters against the parameters or the estimates against the estimates (the last two are good for diagnostics such as the completeness of the sampling from the prior and the shape and correlation of the estimation function)\n\nOptionally any of the parameters for ``PMC.PMC(...)`` can be passed to ``PMC.scatter_plot(...)`` to run the PMC when making the plot rather than calling the sampling step first. ``matplotlib`` parameters can also be passed for the plotting routine.\n\n****\nTODO\n****\n\nThe module is under constant development, and progress can be checked in the ``dev`` branch. Current additions to the IMNN include\n\n- Put back summary support\n  - Previous versions of the IMNN had the ability to pass arbitrary summaries along with network summaries. This is useful because it can be a suggestion of how much information is gained over other summarising functions (such as the two point statistics, etc.)\n  - Need to accept array, generative function and TFRecords with summaries and split covariance between summaries and network outputs for regularisation\n\n- JAX implementation of all routines\n  - This is under private development currently\n\n- Docstrings written for LFI\n\n- Write unit tests\n\n.. |bf_d| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/bf_d.png\n.. |f_bf_d_to_bf_x| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/f_bf_d_to_bf_x.png\n.. |bf_x| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/bf_x.png\n.. |bf_d^textrm_fid_| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/bf_d^textrm_fid_.png\n.. |boldsymbol_theta| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/boldsymbol_theta.png\n.. |bf_x^textrm_fid_| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/bf_x^textrm_fid_.png\n.. |bf_C_f| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/bf_C_f.png\n.. |bf_d^textrm_fid_-1| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/bf_d^textrm_fid_-1.png\n.. |boldsymbol_theta-1| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/boldsymbol_theta-1.png\n.. |bf_d^textrm_fid_-2| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/bf_d^textrm_fid_-2.png\n.. |boldsymbol_theta-2| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/boldsymbol_theta-2.png\n.. |partial_boldsymb| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/partial_boldsymb.png\n.. |lambda| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/lambda.png\n.. |alpha| image:: https://bitbucket.org/tomcharnock/imnn-tf/raw/master/eq/png/alpha.png\n\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://bitbucket.org/tomcharnock/imnn-tf",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "imnn-tf",
    "package_url": "https://pypi.org/project/imnn-tf/",
    "platform": "",
    "project_url": "https://pypi.org/project/imnn-tf/",
    "project_urls": {
      "Homepage": "https://bitbucket.org/tomcharnock/imnn-tf"
    },
    "release_url": "https://pypi.org/project/imnn-tf/0.2.0/",
    "requires_dist": [
      "tensorflow (>=2.1.0)",
      "tqdm (>=4.31.0)",
      "numpy (>=1.16.0)",
      "scipy (>=1.4.1)",
      "matplotlib"
    ],
    "requires_python": ">=3",
    "summary": "Using neural networks to extract sufficient statistics from         data by maximising the Fisher information",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13057461,
  "releases": {
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8d1b60886e2e5cb5bac8cb068302fb70d6967d8a5153ee383b1ef8568d13c7d7",
          "md5": "3fc1cc6da7e403ffd85129c5993fe178",
          "sha256": "c7469a02b3b3a3a239e988c3a20140671a22eacd98d343113b2da15153ea309a"
        },
        "downloads": -1,
        "filename": "imnn_tf-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3fc1cc6da7e403ffd85129c5993fe178",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3",
        "size": 34375,
        "upload_time": "2021-04-20T18:00:39",
        "upload_time_iso_8601": "2021-04-20T18:00:39.796495Z",
        "url": "https://files.pythonhosted.org/packages/8d/1b/60886e2e5cb5bac8cb068302fb70d6967d8a5153ee383b1ef8568d13c7d7/imnn_tf-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "98c9ee76cedaaebe59ce1aeabc5e08e6491679c92ab0530247a403bdef3842c4",
          "md5": "020acdf01145644783f0c65ee108299a",
          "sha256": "ff2eee441ff2b10c20f1a00431c7b4a2c9ac6984a6420e501ef3c1e9bde62da0"
        },
        "downloads": -1,
        "filename": "imnn-tf-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "020acdf01145644783f0c65ee108299a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3",
        "size": 2978946,
        "upload_time": "2021-04-20T18:00:49",
        "upload_time_iso_8601": "2021-04-20T18:00:49.786184Z",
        "url": "https://files.pythonhosted.org/packages/98/c9/ee76cedaaebe59ce1aeabc5e08e6491679c92ab0530247a403bdef3842c4/imnn-tf-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8d1b60886e2e5cb5bac8cb068302fb70d6967d8a5153ee383b1ef8568d13c7d7",
        "md5": "3fc1cc6da7e403ffd85129c5993fe178",
        "sha256": "c7469a02b3b3a3a239e988c3a20140671a22eacd98d343113b2da15153ea309a"
      },
      "downloads": -1,
      "filename": "imnn_tf-0.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "3fc1cc6da7e403ffd85129c5993fe178",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3",
      "size": 34375,
      "upload_time": "2021-04-20T18:00:39",
      "upload_time_iso_8601": "2021-04-20T18:00:39.796495Z",
      "url": "https://files.pythonhosted.org/packages/8d/1b/60886e2e5cb5bac8cb068302fb70d6967d8a5153ee383b1ef8568d13c7d7/imnn_tf-0.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "98c9ee76cedaaebe59ce1aeabc5e08e6491679c92ab0530247a403bdef3842c4",
        "md5": "020acdf01145644783f0c65ee108299a",
        "sha256": "ff2eee441ff2b10c20f1a00431c7b4a2c9ac6984a6420e501ef3c1e9bde62da0"
      },
      "downloads": -1,
      "filename": "imnn-tf-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "020acdf01145644783f0c65ee108299a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3",
      "size": 2978946,
      "upload_time": "2021-04-20T18:00:49",
      "upload_time_iso_8601": "2021-04-20T18:00:49.786184Z",
      "url": "https://files.pythonhosted.org/packages/98/c9/ee76cedaaebe59ce1aeabc5e08e6491679c92ab0530247a403bdef3842c4/imnn-tf-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}