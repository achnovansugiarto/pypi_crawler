{
  "info": {
    "author": "Prajwal Kailas, Max Homilius, Shinichi Goto",
    "author_email": "prajwal967@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Programming Language :: Python"
    ],
    "description": "![De-Identification Logo](./deid.png)\n\n# Robust DeID: De-Identification of Notes - Removal of Private Information from Text.\n\n**Named Entity Recognition models to identify and remove/replace protected health information (PHI) in text.**\n\n*Version 1.0.0 / 01 September 2022*\n\n*We're working on expanding the documentation and readme. While we work on that, taking a look at the source code may help answer some questions.*\n\n*Most elements of the readme are referenced and used from [BLOOM](https://huggingface.co/bigscience/bloom). We'd like to thank the BLOOM authors for their extensive model card that inspired and helped us develop ours!*\n\n*Comments, feedback and improvements are welcome and encouraged!*\n\n---\n\n# Project Details \n\n*This section provides information about the project. This includes information about installation, project features, and a general project overview.*\n*It is useful for getting a brief understanding of the project and submitting any questions to the project team*\n\n<details>\n<summary>Click to expand</summary>\n\n* This repository was used to train and evaluate various de-identification models and strategies. \n* The models and strategies are extensible and can be used on other datasets as well.\n* The medical notes from the i2b2 2014 cohort [[Stubbs and Uzuner, 2015]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978170/) and the Mass General Brigham network (medical notes from 11 institutes) were used to train and test the models.\n* The medical notes within the Mass General Brigham network contain private information. To make the models trained on this set of notes available to the public, we replaced all the private information (PHI) in these notes with named-entities generated using the [Faker](https://faker.readthedocs.io/en/stable/) library (e.g. names were replaced with names generated from [Faker](https://faker.readthedocs.io/en/stable/)).\n* Trained models are published on huggingface under the [OBI organization](https://huggingface.co/obi). The models trained as part of this project are hosted on HuggingFace with the names: [obi/deberta_deid_i2b2_mgb](https://huggingface.co/obi/deberta_deid_i2b2_mgb), [obi/deberta_deid_i2b2](https://huggingface.co/obi/deberta_deid_i2b2), [obi/deberta_deid_no_augment_i2b2](https://huggingface.co/obi/deberta_deid_no_augment_i2b2)\n* The models are token classification models built to identify tokens that contain private information (named entity recognition).\n* A token classification model along with a NER notation (e.g. BIO, BILOU etc.) can be used to identify spans from token labels. Spans are a collection of tokens that represent some protected health information (PHI) entity.\n* The 11 private entities/spans the models identify are defined by HIPAA and more details about these PHI entities can be found here: [Annotation guidelines](data/AnnotationGuidelines.md)\n\n> *We'd like to thank the authors of [i2b2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978170/), [Faker](https://faker.readthedocs.io/en/stable/), [HuggingFace](https://huggingface.cohttps://huggingface.co) and the other libraries that made this project possible.*\n\n\n## Overview\n*This section provides information about the project version, license, funders, release date, developers, and contact information.*\n*It is useful for anyone who wants to reference the project.*\n\n<details>\n<summary>Click to expand</summary>\n\n*All collaborators are either volunteers or have an agreement with their employer.*\n  \n**Developed by:** One Brave Idea ([website](https://www.onebraveidea.org))\n\n**Authors:** [Prajwal Kailas](https://github.com/prajwal967), [Max Homilius](https://github.com/mxhm), [Shinichi Goto](https://github.com/sgoto1), [Rahul Deo](https://github.com/rahulcdeo)\n\n**Version:** 1.0.0\n\n**Languages:** English\n\n**License:** MIT\n\n**Release Date:** Thursday, 01 September 2022\n\n**Send Questions to:** *(Further information forthcoming.)*\n\n**Cite as:** *(Further information forthcoming.)*\n\n**Funded by:** \n    \n* *(Further information forthcoming.)*\n\n</details>\n\n## Features\n*This section provides the key features of this project. It provides a quick overview of the project.*\n\n<details>\n<summary>Click to expand</summary>\n\n1. **Transformer models:** Any transformer model from the [HuggingFace](https://huggingface.co/models) library can be used for training. \n2. **Public Models:** We make available three [DeBERTa](https://arxiv.org/abs/2006.03654) based de-identification models. The models are hosted on HuggingFace with the names: [obi/deberta_deid_i2b2_mgb](https://huggingface.co/obi/deberta_deid_i2b2_mgb), [obi/deberta_deid_i2b2](https://huggingface.co/obi/deberta_deid_i2b2), [obi/deberta_deid_no_augment_i2b2](https://huggingface.co/obi/deberta_deid_no_augment_i2b2)\n3. **Recall biased thresholding:** Use a classification bias to aggressively remove PHI from documents. This is a safer and more robust option when working with sensitive data like medical notes.\n4. **Augmentations:** Replacing private information with randomly generated information (names, locations etc.) using the [Faker](https://faker.readthedocs.io/en/stable/) library. This opens up the possibility to release models trained on sensitive data to the public.\n4. **Context enhancement:** Extract a sentence from the note and add tokens (from the sentences adjacent to the extracted sentence) on either side of the extracted sentence until we have a sequence of 512 sub-tokens. The reason for including context tokens was to provide additional context, especially for peripheral tokens in a given sequence.\n5. **Custom clinical tokenizer:** Includes medically relevant regular expressions and abbreviations based on the structure and information generally found in medical notes. This tokenizer resolves common typographical errors and missing spaces that occur in clinical notes.\n\nSince de-identification is a sequence labeling task, this project can be used for other sequence labeling tasks.\nMore details on how to use the project, the format of data and other useful information is presented in future sections.\n\n</details>\n\n## Installation\n*This section provides information on how to install dependencies and set up the project.*\n\n<details>\n<summary>Click to expand</summary>\n\n### Source\n\n* Git clone the repository\n* Install the dependencies using conda or pip.\n* We developed this project using the conda environment specified in [deid.yml](./deid.yml). You can create the environment using this file, and it will install the required dependencies.\n\n```shell\ngit clone git@github.com:obi-ml-public/ehr_deidentification.git\nconda env create -f deid.yml\nconda activate deid\n```\n\n### Pip\n\n* You can install the **robust_deid** package to use the tools and models for de-identification of text.\n\n#### Option 1:\n- Update conda environment file ([deid.yml](./deid.yml)): Add *robust_deid* to the environment file under the pip section.\n- Create and activate conda environment\n- This will install the dependencies and the package in the conda environment\n\n```shell\nvi deid.yml # Update env file\nconda env create -f deid.yml\nconda activate deid\n```\n\n#### Option 2:\n- Using only pip.\n- Install the *robust_deid* package and the dependencies using pip install along with the [requirments.txt](./requirements.txt) file\n\n```shell\npip install robust_deid -r requirments.txt\n```\n\n#### Option 3:\n- Install the dependencies using the conda environment ([deid.yml](./deid.yml)) described in the previous section.\n- pip install the package\n\n```shell\nconda env create -f deid.yml\nconda activate deid\npip install robust_deid\n```\n    \n\n</details>\n\n</details>\n\n---\n\n# Dataset Creation\n*This section provides information on how the datasets were annotated, the size of the datasets and the distribution of PHI in the datasets.*\n\n<details>\n<summary>Click to expand</summary>\n\n## Annotation guidelines\n* The guidelines for the dataset annotation and prodigy setup can be found here: \n[Annotation Guidelines](data/AnnotationGuidelines.md)\n  \n## Dataset Information\n  \n* Information about the distribution of PHI and size of the datasets can be found here: [Datasets](data/Datasets.md)\n\n</details>\n\n---\n\n# Training\n*This section provides information about the training data, the speed and size of training elements.*\n*It is useful for people who want to learn more about the model inputs, objective, architecture, development, training/evaluation/prediction procedure and compute infrastructure.*\n\n<details>\n<summary>Click to expand</summary>\n\n## Training Data\n*This section provides a high-level overview of the training data. It is relevant for anyone who wants to know the basics of what the model is learning.*\n\n<details>\n<summary>Click to expand</summary>\n\n### Overview\n\n-   Language: English\n-   Data: Medical notes from the i2b2 2014 cohort [[Stubbs and Uzuner, 2015]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4978170/) and the Mass General Brigham Network (notes from 11 institutes).\n- Detailed description of the data, and the distribution of entities can be found here: [Datasets](data/Datasets.md)\n-   4 training datasets (one model trianed on each of these datasets):\n    1. i2b2 dataset (public dataset - private information has been replaced by the authors/annotators).\n    2. Augmented i2b2 dataset.\n    3. i2b2 + MGB dataset (contains private data, hence model cannot be released to the public).\n    4. Augmented i2b2 + MGB dataset (private data is replaced, hence model can be released to the public).\n- To create the augmented version of a dataset we replaced the annotated private information in the dataset with entities generated using [Faker](https://faker.readthedocs.io/en/stable/).\n\n### Dataset Splits\n\n* We used the [dataset_splitter.py](src/robust_deid/dataset_splitter.py) script to create the train and validation datasets.\n* We used the script to create the dataset splits. The parameters that we ran the script with can be found here: [train_val_splits.sh](./run/dataset/train_val_splits.sh)\n\n</details>\n\n## Preprocessing\n*This section talks about the preprocessing steps.*\n*Relevant for those that want to understand how the notes were split and tokenized to fit into the model*\n\n<details>\n<summary>Click to expand</summary>\n\n### Data Format\n\n* The data is in the json format, where we store the notes in a jsonl file. Each line in this file is a json object that refers to one sequence.\n* Since we had datasets that contained medical notes that would not fit into the model without truncation, we created chunks of size 500 subword tokens that would fit into the model.\n* Essentially we are training the de-identification model on sequence chunks from the note as opposed to using the entire note (entire notes would need truncation). However, we can still de-identify the entire note at test time by aggregating the predictions on the chunked sequences back to the note level.\n* The format after splitting into chunks was like this:\n```json\n{ \n  \"sentence_text\": \"Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982 Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928) ...\", \n  \"current_chunk_start\": 20,\n  \"current_chunk_end\": 192, \n  \"global_start\": 6,\n  \"spans\": [{\"id\":\"0\", \"start\": 40, \"end\": 50, \"label\": \"DATE\"}, {\"id\":\"1\", \"start\": 67, \"end\": 77, \"label\": \"DATE\"}, {\"id\":\"3\", \"start\": 98, \"end\": 110, \"label\": \"PATIENT\"}, {\"id\":\"3\", \"start\": 112, \"end\": 114, \"label\": \"AGE\"}, {\"...\": \"...\"}]\n}\n```\n* As long as the input data to the sequence tagger is in this format, the sequence tagger code should work without any errors.\n* The global start field is needed to reconstruct the original note from the chunks. This is used when running predictions. We don't need this field for training and evaluation.\n* The [sequence_dataset.py](src/robust_deid/sequence_dataset.py) script can be used to create this chunked dataset, given a collection of notes/documents.\n\n### Chunking\n\n* As mentioned, we had datasets that contained medical notes that would not fit into the model without truncation, we chunked the data.\n* To chunk the data we first split the note into sentences, word tokenized these sentences and finally subword tokenized these sentences.\n* To create the chunk, we took a sentence and took all the adjacent tokens around it until we reached a sequence of 500 sub-word tokens. This was done for every sentence.\n* Based on the start and end positions of these chunks, we get the text of the sequence, and the spans associated with the chunked sequence.\n* Essentially we are training the de-identification model on sequence chunks from the note as opposed to using the entire note (entire notes would need truncation). However, we can still de-identify the entire note at test time by aggregating the predictions on the chunked sequences back to the note level.\n* For more information, refer to the following scripts: [sequence_dataset.py](src/robust_deid/sequence_dataset.py) and [sequence_chunker.py](src/robust_deid/sequence_datasets/sequences/sequence_chunker.py)\n* During evaluation however, we only evaluate on the tokens in the current sentence and not on the added on tokens. The added on tokens are present in the sequence and are used as additional context only.\n* We don't evaluate on the added on contextual tokens because we care about note level performance as opposed to sequence level performance. By evaluating only on the current sentence in the note we are calculating metrics for each token just once, effectively evaluating at the note level. \n* Similar approach during predict, we only consider predictions on the current sentence, so that we have one single prediction for a given token in the note. Then we can aggregate these predictions back to the note level.\n* We've explained chunking during evaluation and predict in the future sections.\n* Example:\n    - Note: 20 tokens\n    - Sentences: 3 sentences\n        - Sentence 1: 5 tokens\n        - Sentence 2: 7 tokens\n        - Sentence 3: 8 tokens\n    - Max tokens (Sequence length): 10 tokens\n\n    - Based on these configurations we get:\n        - Sequence 1: 5 tokens from sentence 1 and 5 tokens from sentence 2\n        - Sequence 2: 7 tokens from sentence 1, 1 tokens from sentence 1 and 2 tokens from sentence 2\n        - Sequence 3: 8 tokens from sentence 3 and 2 tokens from sentence 2\n\n    - The datasets now consists of 3 sequences.\n    - Training: Train on all tokens in all 3 sequences\n        - Sequence 1: Trained on all 10 tokens\n        - Sequence 2: Trained on all 10 tokens\n        - Sequence 3: Trained on all 10 tokens\n    - Evaluation/Predict: Evaluate/Predict only on the current sentence:\n        - Sequence 1: Evaluate/Predict only on the 5 tokens from sentence 1\n        - Sequence 2: Evaluate/Predict only on the 7 tokens from sentence 2\n        - Sequence 3: Evaluate/Predict only on the 8 tokens from sentence 3\n        - We've evaluated/predicted on the 20 tokens (evaluated/predicted each token only once)\n> *This step is optional and depends on the size of your data/sequences. If the data contains sequences that do not exceed the model max length, you can skip this step.*\n\n### Sentencizer\nThe medical notes are split into chunks of 500 sub-word tokens, and the model is trained on these chunks. \nSince DeBERTa can handle a maximum sequence length of 512 tokens, we split the notes into chunks (< 512). \nIn the simplest case, a chunk is a sentence from the note. \nWe took this approach of extracting a sentence and packing it with surrounding tokens for additional context.\n\n- Sentencizer: The dataset was sentencized with the en_core_sci_sm sentencizer from [Scispacy](https://allenai.github.io/scispacy/).\n- 500 Length Chunks: Extract a sentence from the note and add tokens (from the sentences adjacent to the extracted sentence) on either side of the extracted sentence until we have a sequence of 500 sub-tokens.\n- You can think of this as a sliding window approach where the window (stride) is dynamic, i.e. we move the window from one sentence to the next.\n- *This step is used only when chunking the dataset*\n\n### Tokenization\n\nTwo-step process. Word tokenization followed by sub-word tokenization.\n\n- Word-level tokenization: Custom spacy tokenizer (additional medically relevant regexes and abbreviations) built on top of the en_core_sci_sm tokenizer from [Scispacy](https://allenai.github.io/scispacy/).\n- The custom tokenizer can be found here: [tokenizer-0.0.1](./tokenizer-0.0.1) and the code to build this tokenizer can be found here: [create_tokenizer.py](./create_tokenizer.py) and [custom_tokenizer](./custom_tokenizer).\n- Labels are assigned to word level tokens and private information is at the token level.\n- Sub-word tokenization: Apply the byte-level Byte Pair Encoding (BPE) algorithm defined by DeBERTa on the split word level tokens.\n- We do not train on all subword tokens, we only use the first subword of the word level token to update the model weights.\n- Tokenization can be applied on the fly if the user wishes.\n- This step is used when chunking the dataset and also when feeding the chunked dataset to the model for training/evaluation/testing.\n- The input the model is always in text format (format is shown in the earlier section), so we use this tokenizer to word tokenize and sub-word tokenize the text before feeding it into the model.\n\n### Augmentation\n\n* We built a pipeline to augment any private information, i.e. replace any private information in the data with randomly generated entities.\n* This was using the [Faker](https://faker.readthedocs.io/en/stable/) library. We also used the [PHICON](https://github.com/betterzhou/PHICON) repository to generate hospital and company names.\n* These libraries were used to generate fake names, locations, dates, id etc (fake PHI). By doing so, we are able to remove replace private information in the notes with generated entities.\n* Augmentation lets us release models that have been trained on private data to the public since the private information has been replaced.\n* The augmentation pipeline and rules can be found in the scripts present in the following folder: [Augmentations](src/robust_deid/sequence_datasets/text_transformations/augmentations/phi).\n* The augmentation rules vary for each type of protected health information (PHI).\n* Augmentations can be applied on the fly. An advantage of applying augmentations on the fly is that a note with a set of PHI is only ever seen once, since we replace a given note with different entities in every epoch of the training process. This gives an illusion of the presence of a large collection of notes and thus we can train the models for longer.\n\n</details>\n\n## Trained Models\n*This section talks about the models that were trained and how they were trained.*\n*It is useful for anyone who wants to know what models were trained, and how they can train, evaluate and test (how to de-identify notes with a trained model) their own models.*\n\n<details>\n<summary>Click to expand</summary>\n\n### Overview\n\n* We trained a total of 4 models as part of this project. All 4 models are DeBERTa (see [paper](https://arxiv.org/abs/2006.03654)) based models.\n* DeBERTa Encoder-Decoder architecture with a token classification head. The token classification head is a linear classification layer that outputs scores for each named entity/class (protected health information entities).\n* **Objective Function:** Cross Entropy with mean reduction (see [API documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)).\n* The models are token classification model built to identify tokens that contain private information (named entity recognition).\n* A token classification model along with a NER notation (e.g. BIO, BILOU etc.) can be used to identify spans from token labels. Spans are a collection of tokens that represent some protected health information (PHI) entity.\n* Each model was trained with a different training dataset.\n    1. **Model 1 ([obi/deberta_deid_no_augment_i2b2](https://huggingface.co/obi/deberta_deid_no_augment_i2b2))**: DeBERTa based de-identification model trained with the i2b2 dataset.\n    2. **Model 2 ([obi/deberta_deid_i2b2](https://huggingface.co/obi/deberta_deid_i2b2))**: DeBERTa based de-identification model trained with the augmented i2b2 dataset.\n    3. **Model 3**: DeBERTa based de-identification model trained with the MGB dataset (contains private data and cannot be released to the public).\n    4. **Model 4 ([obi/deberta_deid_i2b2_mgb](https://huggingface.co/obi/deberta_deid_i2b2_mgb))**: DeBERTa based de-identification model trained with the augmented MGB dataset (private data is replaced and can be released to the public).\n* The [sequence_tagger.py](src/robust_deid/sequence_tagger.py) script was used to train the models. The following shell script contains all the training calls: [train.sh](./run/model/train.sh)\n* While we update our documentation and readme, this script should serve as a good starting point to train, evaluate and test your own models/datasets.\n* The config files for each of these models can be found in the [train config_files](./config_files/custom/train) folder. The config files contain information about the learning rate, warmup, batch size etc.\n\n> *More details about the models can be found in the model cards on HuggingFace (click on model links).*\n\n### Model 1\n* The model is available on HuggingFace: [obi/deberta_deid_no_augment_i2b2](https://huggingface.co/obi/deberta_deid_no_augment_i2b2)\n* Trained with 713 i2b2 notes which are chunked into 29587 sequences.\n* The PHI tokens are not augmented.\n\n### Model 2\n* The model is available on HuggingFace: [obi/deberta_deid_i2b2](https://huggingface.co/obi/deberta_deid_i2b2)\n* Trained with 713 i2b2 notes which are chunked into 29587 sequences. \n* The PHI tokens are augmented. The augmentations are applied on the fly (the sequences are different every epoch).\n\n### Model 3\n* The model has not been made available since it has been trained on private data.\n* Trained with 713 i2b2 notes and 1150 MGB notes which are chunked into 82917 sequences. \n* The PHI tokens are not augmented. \n\n### Model 4\n* The model is available on HuggingFace: [obi/deberta_deid_i2b2_mgb](https://huggingface.co/obi/deberta_deid_i2b2_mgb)\n* Trained with 713 i2b2 notes and 1150 MGB notes which are chunked into 82917 sequences. \n* The PHI tokens are augmented, and the private data has been replaced. The augmentations are applied on the fly (the sequences are different every epoch).\n\n> *If you wish to train your own models, you can do so by following the steps in this notebook: [Train](./Train.ipynb)*\n\n> *If you wish to evaluate a trained model, you can do so by following the steps in this notebook: [Eval](./Eval.ipynb)*\n\n> *If you wish to de-identify notes using a trained model, you can do so by following the steps in this notebook: [Predict](./Predict.ipynb)*\n\n</details>\n\n## Technical Specifications\n\n*This section includes details about compute infrastructure.*\n\n<details>\n<summary>Click to expand</summary>\n\n### Compute infrastructure\n\n#### Hardware\n\n* 8 Tesla V100-SXM2-32GB GPUs (1 Node)\n    \n* 8 GPUs using NVLink\n\n* CPU: Intel\n\n* CPU memory: 512GB per node\n\n* GPU memory: 256GB per node\n\n#### Software\n\n* PyTorch (pytorch-1.11.0 w/ CUDA-11.3.1; see [Github link](https://github.com/pytorch/pytorch))\n\n* NVIDIA Apex ([Github link](https://github.com/NVIDIA/apex))\n    \n</details>\n\n</details>\n\n---\n\n# Evaluation\n*This section describes the evaluation protocols and provides the results of the trained models.*\n\n<details>\n<summary>Click to expand</summary>\n\n## Metrics \n*This section describes the different ways performance is calculated and why.*\n\n<details>\n<summary>Click to expand</summary>\n\nIncludes:\n\n| Metric             | Why chosen                                                         |\n|--------------------|--------------------------------------------------------------------|\n| [F1](https://en.wikipedia.org/wiki/F-score) | Quantifies the amount of PHI being captured and the amount of entities falsely captured as PHI |\n| [Recall](https://en.wikipedia.org/wiki/Precision_and_recall) | Standard objective for computing how much PHI has been identified|\n| [Precision](https://en.wikipedia.org/wiki/Precision_and_recall) | Standard objective for computing how much non-PHI has been identified as PHI |\n\nMultiple metrics are computed. F1, precision, recall scores are computed for each entity individually (i.e. we have scores for names, ages etc.). Micro, macro and Binary (PHI v/s Non-PHI) averaged scores are also computed.\n\n</details>\n\n## Datasets\n*This section contains information about the test datasets*\n\n<details>\n<summary>Click to expand</summary>\n\n* Detailed description of the test data, and the distribution of entities can be found here: [Datasets](data/Datasets.md)\n\n</details>\n\n##  Results\n*This section contains the results on the test datasets*\n\n<details>\n<summary>Click to expand</summary>\n\n**De-Identification Results:**\n\n*TODO: Forthcoming*\n\n**Downstream Results:**\n\n*TODO: Forthcoming*\n\n</details>\n\n</details>\n\n---\n\n# Uses\n\n*This section addresses questions around how the models/scripts is intended to be used, discusses the foreseeable users of the model (including those affected by it), and describes uses that are considered out of scope.*\n*It is useful for anyone who is considering using the models/scripts or who is affected by the project.*\n\n<details>\n<summary>Click to expand</summary>\n    \n## How to use\n*This section goes through how to use and deploy this project. It is usefel for anyone who wants to know how to train and evaluate the model, and how to use the model to de-identify documents/notes.*\n\n<details>\n<summary>Click to expand</summary>\n\n### Training\n\n* Follow the steps in this notebook to train your own models: [Train](./Train.ipynb)\n\n### Evaluation\n\n* Follow the steps in this notebook to evaluate a trained model: [Eval](./Eval.ipynb)\n\n### Predict\n\n* Follow the steps in this notebook to de-identify notes using a trained model: [Predict](./Predict.ipynb)\n* You can also take a look at our HuggingFace space: [Medical Note Deidentification](https://huggingface.co/spaces/obi/Medical-Note-Deidentification) to see how the model is used.\n\n</details>\n\n## Intended Use\n\n*This section addresses how we intend the model to be used and what use cases are considered out-of-scope.*\n\n<details>\n<summary>Click to expand</summary>\n\nThis project was created in order to de-identify medical notes (i.e. remove protected health information) to facilitate research. De-identification of notes can enable institutes to share notes amongst other institutions while protecting the privacy of their patients. Learning from a larger collection of notes can be more beneficial as compared to learning from a limited set of medical text from a single institution.\n\n### Direct Use\n\n-   De-identification of medical notes. Removing private information (PHI) from medical notes.\n\n### Downstream Use\n\n-   The de-identification pipeline can be the first step in a downstream task (e.g. disease classification) where the de-identification model is used to remove private information from the notes being used in the downstream task.\n-   Removing private information from the downstream tasks will allow researchers to publish their models to the public.\n\n### Out-of-scope Uses\n\n* There is no guarantee that the de-identification pipeline will identify all the protected health information.\n* If the pipeline is being used in a high stakes setting, additional care needs to be taken to ensure patient data is not being leaked.\n* Additional care can be taken, by adding handcrafted rules to identify the PHI or varying classification thresholds to increase recall  \n* Intentionally using the model for harm, violating human rights, or other kinds of malicious activities, is a misuse of this model.\n\n### Intended Users\n\n-   General Public\n\n-   Healthcare Institutes\n\n-   Researchers\n\n-   Students\n\n-   Educators\n\n-   Engineers/developers\n\n</details>\n\n</details>\n\n---\n\n# Limitations\n\n*This section addresses some current limitations of the project.*\n*While we continue to address and overcome some of these limitations, we are always welcome to suggestions and collaboration.*\n*We encourage the community to give us feedback and help address some of the limitations. Thanks!*\n\n<details>\n<summary>Click to expand</summary>\n\n* Model may:\n\n    -   Not identify all the protected health information\n\n    -   Contain stereotypes\n  \n    -   Contain personal information\n\n    -   Make errors, missing private information or tagging non-private data as PHI\n    \n* Model has not been tested on a truly external dataset (e.g. healthcare provider outside Boston or outside the US) due to lack of availability.\n\n* Integrating the model with additional handcrafted rules to increase recall (prevent the leak of PHI).\n\n* Always room for improvement in terms of speed and memory efficiency. There are a few steps that are redundantly applied on-the-fly instead of being applied just once. We hope to remove this bottleneck.\n\n* Speed up the text transformations (tokenization, label alignment etc.) \n  \n* Augmented Models:\n  \n    * Augmentations may not cover every possible edge case. Improving augmentations may improve performance of the augmented models.\n    * Training with augmentations, lets us train the model for longer (since the PHI is always being replaced with new data), but there could be a possibility the model may start to over-fit on non-phi text. Running augmentations (e.g. replacing words with their synonyms) on non-phi text may help resolve this. Refer [PHICON](https://github.com/betterzhou/PHICON).\n    * Augmentations currently don't maintain consistency (e.g. a given note can have different patient names for the same patient), while this doesn't seem to affect performance, it would be nice to have augmentations that have some consistency. More so for model predictions, than for training. By consistency, we refer to having the augmentation pipeline generate the same patient name for a given note, dates that are in the right timeline or ages that correspond the dates/patient.\n    * While Faker has a good diversity of augmentations, we can always try to improve the diversity of our augmentations.\n    \n\n</details>\n\n---\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/obi-ml-public/ehr_deidentification",
    "keywords": "ehr deidentification medical phi transformers nlp ner",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "robust-deid",
    "package_url": "https://pypi.org/project/robust-deid/",
    "platform": "any",
    "project_url": "https://pypi.org/project/robust-deid/",
    "project_urls": {
      "Bug Tracker": "https://github.com/obi-ml-public/ehr_deidentification/issues",
      "DeBERTa De-ID I2B2": "https://huggingface.co/obi/deberta_deid_i2b2",
      "DeBERTa De-ID I2B2 MGB": "https://huggingface.co/obi/deberta_deid_i2b2_mgb",
      "DeBERTa De-ID I2B2 No Augment": "https://huggingface.co/obi/deberta_deid_no_augment_i2b2",
      "Demo": "https://huggingface.co/spaces/obi/Medical-Note-Deidentification",
      "Homepage": "https://github.com/obi-ml-public/ehr_deidentification"
    },
    "release_url": "https://pypi.org/project/robust-deid/0.3.0/",
    "requires_dist": null,
    "requires_python": ">=3.8",
    "summary": "Package with tools and named entity recognition models to identify and remove/replace protected health information (PHI) in text.",
    "version": "0.3.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 14954474,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "be4ae4661c4c9718e81be63d741b3c09db7c3c2d4c98a5b50c35bc7f1dda3df7",
          "md5": "f9582f09e07b1f412be266c489d5624d",
          "sha256": "767b4cce0f04c67c91522bfa90c473cb33e619b46b62d61bf28773382f3901f7"
        },
        "downloads": -1,
        "filename": "robust_deid-0.1.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f9582f09e07b1f412be266c489d5624d",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.7",
        "size": 112357,
        "upload_time": "2022-02-15T20:21:39",
        "upload_time_iso_8601": "2022-02-15T20:21:39.847771Z",
        "url": "https://files.pythonhosted.org/packages/be/4a/e4661c4c9718e81be63d741b3c09db7c3c2d4c98a5b50c35bc7f1dda3df7/robust_deid-0.1.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "0a3f391ce504dd105e03c790708cabcbea6d15a136b7fb258923bf01bfdd366e",
          "md5": "2842193a4835cd4f757ae9114d93f8f7",
          "sha256": "7c37907bc1f7dc6b825436167e27203c6766529b4ce2bd9d67c68e037806e0ca"
        },
        "downloads": -1,
        "filename": "robust_deid-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "2842193a4835cd4f757ae9114d93f8f7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 85187,
        "upload_time": "2022-02-15T20:21:41",
        "upload_time_iso_8601": "2022-02-15T20:21:41.400782Z",
        "url": "https://files.pythonhosted.org/packages/0a/3f/391ce504dd105e03c790708cabcbea6d15a136b7fb258923bf01bfdd366e/robust_deid-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e44386e4961adb39cf20e14add1c224f8027a2e4ad4fbb544a9e588403cda281",
          "md5": "2b841fa4173650910d825ca69d2892ac",
          "sha256": "d4a5fc4e64642d8e0258f6eff1a878b8ca4e68082dd310e823b204fecd2d5906"
        },
        "downloads": -1,
        "filename": "robust_deid-0.1.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2b841fa4173650910d825ca69d2892ac",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.7",
        "size": 112397,
        "upload_time": "2022-02-15T20:40:12",
        "upload_time_iso_8601": "2022-02-15T20:40:12.685650Z",
        "url": "https://files.pythonhosted.org/packages/e4/43/86e4961adb39cf20e14add1c224f8027a2e4ad4fbb544a9e588403cda281/robust_deid-0.1.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "77e7c4a0f40860719dcf5fe2fcf3a6dbba71dbdaf3252b9462a026b387343334",
          "md5": "e553c9cd04f2576d943175b29860db47",
          "sha256": "73a98c237a1e621737a48d8fe65d87cd643e3af4c92ee3676dc28c556cdaa2d5"
        },
        "downloads": -1,
        "filename": "robust_deid-0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "e553c9cd04f2576d943175b29860db47",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 85268,
        "upload_time": "2022-02-15T20:40:14",
        "upload_time_iso_8601": "2022-02-15T20:40:14.325791Z",
        "url": "https://files.pythonhosted.org/packages/77/e7/c4a0f40860719dcf5fe2fcf3a6dbba71dbdaf3252b9462a026b387343334/robust_deid-0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "abcff0adb02cb63c81007bf244c9662d348999605831682b1ca7e29a9c95f34b",
          "md5": "36b8536d2df737a63eb441d2a6868b0d",
          "sha256": "bb03413e9377ddbc4710580f083bc6769cd71f4f6307f961dbd54e0fde1efa85"
        },
        "downloads": -1,
        "filename": "robust_deid-0.2.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "36b8536d2df737a63eb441d2a6868b0d",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.8",
        "size": 170886,
        "upload_time": "2022-08-30T18:36:56",
        "upload_time_iso_8601": "2022-08-30T18:36:56.906439Z",
        "url": "https://files.pythonhosted.org/packages/ab/cf/f0adb02cb63c81007bf244c9662d348999605831682b1ca7e29a9c95f34b/robust_deid-0.2.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f7709a0f68ca63b81f388c188a48fe2d99496765048fbbbaf79a93aee9dd158c",
          "md5": "a816fb907955bd3181e2bd9ccc9003c8",
          "sha256": "24c7b6d3e550cbfac1491f36280c34c9261c96b5bd7fce04e6ce16f3bc7840d1"
        },
        "downloads": -1,
        "filename": "robust_deid-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a816fb907955bd3181e2bd9ccc9003c8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 154302,
        "upload_time": "2022-08-30T18:37:00",
        "upload_time_iso_8601": "2022-08-30T18:37:00.418766Z",
        "url": "https://files.pythonhosted.org/packages/f7/70/9a0f68ca63b81f388c188a48fe2d99496765048fbbbaf79a93aee9dd158c/robust_deid-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.3.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5d7deaf2f9b9ad4d467106401c4febf41cdef042291269d20a5fdbd0e71e088a",
          "md5": "993d36099436bf128e0df56942cb38ea",
          "sha256": "d911f0ad4e8224aa8118d9e71e8623ff6c8a70de58076667abde447a50ef4f82"
        },
        "downloads": -1,
        "filename": "robust_deid-0.3.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "993d36099436bf128e0df56942cb38ea",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=3.8",
        "size": 170733,
        "upload_time": "2022-08-31T21:07:30",
        "upload_time_iso_8601": "2022-08-31T21:07:30.253642Z",
        "url": "https://files.pythonhosted.org/packages/5d/7d/eaf2f9b9ad4d467106401c4febf41cdef042291269d20a5fdbd0e71e088a/robust_deid-0.3.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2d0d36dca4e26842a7b023ca9126da129d173b5ead56d992fb37a8e1f3b78030",
          "md5": "5f26cdf36bf8762e416a585177942925",
          "sha256": "71d1139ce25768c8849a2b0b3d89653a8c9a2486ad5a2641bd4bf6b912191c0e"
        },
        "downloads": -1,
        "filename": "robust_deid-0.3.0.tar.gz",
        "has_sig": false,
        "md5_digest": "5f26cdf36bf8762e416a585177942925",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8",
        "size": 153730,
        "upload_time": "2022-08-31T21:07:32",
        "upload_time_iso_8601": "2022-08-31T21:07:32.960782Z",
        "url": "https://files.pythonhosted.org/packages/2d/0d/36dca4e26842a7b023ca9126da129d173b5ead56d992fb37a8e1f3b78030/robust_deid-0.3.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5d7deaf2f9b9ad4d467106401c4febf41cdef042291269d20a5fdbd0e71e088a",
        "md5": "993d36099436bf128e0df56942cb38ea",
        "sha256": "d911f0ad4e8224aa8118d9e71e8623ff6c8a70de58076667abde447a50ef4f82"
      },
      "downloads": -1,
      "filename": "robust_deid-0.3.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "993d36099436bf128e0df56942cb38ea",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": ">=3.8",
      "size": 170733,
      "upload_time": "2022-08-31T21:07:30",
      "upload_time_iso_8601": "2022-08-31T21:07:30.253642Z",
      "url": "https://files.pythonhosted.org/packages/5d/7d/eaf2f9b9ad4d467106401c4febf41cdef042291269d20a5fdbd0e71e088a/robust_deid-0.3.0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2d0d36dca4e26842a7b023ca9126da129d173b5ead56d992fb37a8e1f3b78030",
        "md5": "5f26cdf36bf8762e416a585177942925",
        "sha256": "71d1139ce25768c8849a2b0b3d89653a8c9a2486ad5a2641bd4bf6b912191c0e"
      },
      "downloads": -1,
      "filename": "robust_deid-0.3.0.tar.gz",
      "has_sig": false,
      "md5_digest": "5f26cdf36bf8762e416a585177942925",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8",
      "size": 153730,
      "upload_time": "2022-08-31T21:07:32",
      "upload_time_iso_8601": "2022-08-31T21:07:32.960782Z",
      "url": "https://files.pythonhosted.org/packages/2d/0d/36dca4e26842a7b023ca9126da129d173b5ead56d992fb37a8e1f3b78030/robust_deid-0.3.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}