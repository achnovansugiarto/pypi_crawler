{
  "info": {
    "author": "Andrea Favia",
    "author_email": "drewlinguistics01@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# NLD - Natural Language Decorators\n\nThis is a package to carry out common text preprocessing tasks in NLTK using dedicated decorators from a class that can also help keep track of the preprocessing steps taken, time it took preprocessing and build simple pipelines faster, especially for when simple exploratory analysis is being carried out.\n\n\n## Install\n\nTo install the package, from the root directory of the repository, run the following:\n\n`python3 setup.py install --user`\n\nThis will also install nltk 3.4.5\n\n## Examples\n\nThe followings are some examples of the preprocessing steps that can be applied, including mistakes that may go undetected.\n\n```python\nfrom nltk.tokenize import word_tokenize\n\n# Jane Austen book Emma obtained from the Gutenberg Project, which is for some weird reason banned in Italy.\nwith open(\"~/Documents/austen.txt\") as raw_text:\n    raw_text = raw_text.read()\n\n# Instantiate the NLD object, you can set the logger on and store all the timings for each run if you want\nnldecorator = nld.NLD(logger=True, store_all_process_times=True)\n\n@nldecorator.timeit\n@nldecorator.freq_dist()\n@nldecorator.stem\n@nldecorator.remove_stopwords(punct=True) # this key argument must be specified\n@nldecorator.substitute([(\"emma\", \"peppa\")])\n@nldecorator.lower\ndef tokenize(_input: str) -> list:\n    # This single function with a pipeline of decorators is a single run\n    return word_tokenize(_input)\n\nfirst_result = tokenize(raw_text)\n\nprint(\"1\\n\", first_result, \"\\n\")\nprint(\"PROCESS TIME:\", nldecorator.process_time)\nprint(\"Run ID: \", nldecorator.id)\nprint(\"Decorators used:\", nldecorator.chain[nldecorator.id])\n```\n```\n1\n [('mr.', 1080), ('peppa', 860), ('could', 834), (\"'s\", 831), ('would', 815)] \n\nPROCESS TIME: 2.593135118484497\nRun ID:  2d1c614f-e5ba-41d3-aea7-ffab8e5e7a7b\nDecorators used: tokenize-lower_wrapper-sub_wrapper-rm_stopwords_wrapper-stem_wrapper-freq_dist_wrapper-\n```\n\n\n```python\n# Wrong Order, stopwords with capitals still present\n@nldecorator.timeit\n@nldecorator.n_grams(3)\n@nldecorator.stem\n@nldecorator.lower\n@nldecorator.remove_stopwords()\ndef tokenize(_input):\n    return word_tokenize(_input)\n\nthird_result = tokenize(raw_text[:])\n\nprint(\"3\\n\", list(third_result)[:20], \"\\n\")\nprint(\"PROCESS TIME:\", nldecorator.process_time)\nprint(\"Run ID: \", nldecorator.id)\nprint(\"Decorators used:\", nldecorator.chain[nldecorator.id])\n```\n\n```\n3\n [('\\ufeffthe', 'project', 'gutenberg'), ('project', 'gutenberg', 'ebook'), ('gutenberg', 'ebook', 'emma'), ('ebook', 'emma', ','), ('emma', ',', 'jane'), (',', 'jane', 'austen'), ('jane', 'austen', 'this'), ('austen', 'this', 'ebook'), ('this', 'ebook', 'use'), ('ebook', 'use', 'anyon'), ('use', 'anyon', 'anywher'), ('anyon', 'anywher', 'cost'), ('anywher', 'cost', 'almost'), ('cost', 'almost', 'restrict'), ('almost', 'restrict', 'whatsoev'), ('restrict', 'whatsoev', '.'), ('whatsoev', '.', 'you'), ('.', 'you', 'may'), ('you', 'may', 'copi'), ('may', 'copi', ',')] \n\nPROCESS TIME: 2.7864768505096436\nRun ID:  6f42acfa-0407-4ddd-8f91-40309db5f08b\nDecorators used: tokenize-rm_stopwords_wrapper-lower_wrapper-stem_wrapper-ngrams_wrapper-\n```\n\nThe following is an example using the `itarator` decorator and then the `open_from_path` decorator  \n\n```python\nnldecorator = nld.NLD(logger=True, store_all_process_times=True)\n\n# With the open_from_path decorator you can run through the pipeline all the files from a given directory or a single file\n\n@nldecorator.timeit\n@nldecorator.lower\n@nldecorator.word_tokenizer\n@nldecorator.iterator()\n@nldecorator.open_from_path\ndef return_directory():\n    # there are three files of the same book in this directory.\n    return \"~/Documents/books/\"\n\nreturn_directory()\nprint(nldecorator.all_process_times)\nreturn_directory()\nprint(nldecorator.all_process_times)\nreturn_directory()\nprint(nldecorator.all_process_times)\n```\n\n```\n[('lower_wrapper', 1.363213062286377)]\n[('lower_wrapper', 1.363213062286377), ('lower_wrapper', 1.3505115509033203)]\n[('lower_wrapper', 1.363213062286377), ('lower_wrapper', 1.3505115509033203), ('lower_wrapper', 1.2218332290649414)]\n```\n\n### Build Dataframes\n\n```python\n@nldecorator.build_df(column=\"tags\")\n@nldecorator.pos_tagger\n@nldecorator.iterator()\ndef preprocess_tags(sents):\n    return sents\n\n@nldecorator.build_df(\"tokens\")\n@nldecorator.stem\n@nldecorator.remove_stopwords()\n@nldecorator.word_tokenizer\n@nldecorator.lower\n@nldecorator.iterator()\ndef preprocess_tokens_iter(sents):\n    return sents\n\n\nsents = [\"This one is my awesome string, written by myself personally.\", \n         \"This two is my awesome string, written by myself personally 2.\",\n         \"This three is my awesome string, written by myself personally 3.\"]\n\nfor i in range(3):\n    preprocess_tokens_iter(sents)\n    preprocess_tags(sents)\n\nnldecorator.df\n```\n\n\n| \t| tokens | \ttags |\n| --- | ------ | ----- |\n| 0 \t| [one, awesom, string, ,, written, person, .] |    [(This, DT), (one, CD), (is, VBZ), (my, PRP$),... |\n| 1 \t| [two, awesom, string, ,, written, person, 2, .] | [(This, DT), (two, CD), (is, VBZ), (my, PRP$),... |\n| 2 \t| [three, awesom, string, ,, written, person, 3, .] |   [(This, DT), (three, CD), (is, VBZ), (my, PRP$... |\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/andcarnivorous/nld",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "nld",
    "package_url": "https://pypi.org/project/nld/",
    "platform": "",
    "project_url": "https://pypi.org/project/nld/",
    "project_urls": {
      "Homepage": "https://github.com/andcarnivorous/nld"
    },
    "release_url": "https://pypi.org/project/nld/0.0.1/",
    "requires_dist": [
      "nltk (==3.4.5)",
      "pandas (>=0.25.3)",
      "numpy (==1.19.0)"
    ],
    "requires_python": ">=3.6",
    "summary": "A small package that provides decorators for text preprocessing with nltk",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8833285,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a61403c785a0390fe98f3ead9eb4340d0e3c0dc13c7003f06f01abfbbd38bc7b",
          "md5": "0b05909590e5b36f702fd56598afb32f",
          "sha256": "33ec2b48999aab0730feb050e565d4794b111df02fdd99d328190088f20e4aa5"
        },
        "downloads": -1,
        "filename": "nld-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0b05909590e5b36f702fd56598afb32f",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 8524,
        "upload_time": "2020-12-06T16:08:40",
        "upload_time_iso_8601": "2020-12-06T16:08:40.445151Z",
        "url": "https://files.pythonhosted.org/packages/a6/14/03c785a0390fe98f3ead9eb4340d0e3c0dc13c7003f06f01abfbbd38bc7b/nld-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5e060f57b10d968ccc0d504476f76ead7ac0cc2fe59ff8c869c6022d3f826efc",
          "md5": "dfcc61d6cc4e11db94030aef72f9ad9c",
          "sha256": "393b0b6331fc653bd072f5ae443e075d0456b42c729f154620bad9d45931a374"
        },
        "downloads": -1,
        "filename": "nld-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "dfcc61d6cc4e11db94030aef72f9ad9c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 9603,
        "upload_time": "2020-12-06T16:08:41",
        "upload_time_iso_8601": "2020-12-06T16:08:41.979809Z",
        "url": "https://files.pythonhosted.org/packages/5e/06/0f57b10d968ccc0d504476f76ead7ac0cc2fe59ff8c869c6022d3f826efc/nld-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a61403c785a0390fe98f3ead9eb4340d0e3c0dc13c7003f06f01abfbbd38bc7b",
        "md5": "0b05909590e5b36f702fd56598afb32f",
        "sha256": "33ec2b48999aab0730feb050e565d4794b111df02fdd99d328190088f20e4aa5"
      },
      "downloads": -1,
      "filename": "nld-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "0b05909590e5b36f702fd56598afb32f",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 8524,
      "upload_time": "2020-12-06T16:08:40",
      "upload_time_iso_8601": "2020-12-06T16:08:40.445151Z",
      "url": "https://files.pythonhosted.org/packages/a6/14/03c785a0390fe98f3ead9eb4340d0e3c0dc13c7003f06f01abfbbd38bc7b/nld-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5e060f57b10d968ccc0d504476f76ead7ac0cc2fe59ff8c869c6022d3f826efc",
        "md5": "dfcc61d6cc4e11db94030aef72f9ad9c",
        "sha256": "393b0b6331fc653bd072f5ae443e075d0456b42c729f154620bad9d45931a374"
      },
      "downloads": -1,
      "filename": "nld-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "dfcc61d6cc4e11db94030aef72f9ad9c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 9603,
      "upload_time": "2020-12-06T16:08:41",
      "upload_time_iso_8601": "2020-12-06T16:08:41.979809Z",
      "url": "https://files.pythonhosted.org/packages/5e/06/0f57b10d968ccc0d504476f76ead7ac0cc2fe59ff8c869c6022d3f826efc/nld-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}