{
  "info": {
    "author": "Thomas Tumiel",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 3 - Alpha",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<div align=\"center\">\n<h1>PyTorch Interpret</h1>\n</div>\n\nA simple to use PyTorch library for interpreting your deep learning results, using both visualisations and attributions. Inspired by [TensorFlow Lucid](https://github.com/tensorflow/lucid).\n\n[![Build Status](https://travis-ci.org/ttumiel/interpret.svg?branch=master)](https://travis-ci.org/ttumiel/interpret)\n[![Coverage Status](https://coveralls.io/repos/github/ttumiel/interpret/badge.svg?branch=master)](https://coveralls.io/github/ttumiel/interpret?branch=master)\n\n<br/>\n\n## Installation\n\nInstall from PyPI:\n\n```bash\npip install interpret-pytorch\n```\n\nOr, install the latest code from GitHub:\n\n```bash\npip install git+https://github.com/ttumiel/interpret\n```\n\n### Dependencies\n\n`interpret` requires a working installation of PyTorch.\n\n\n\n## Contents\n\n- [Tutorials](#tutorials)\n- [Visualisation](#visualisation)\n  - [Quickstart](#vis-quickstart)\n  - [Parameterisations](#parameterisations)\n  - [Objectives](#objectives)\n- [Attribution](#attribution)\n  - [Quickstart](#attr-quickstart)\n- [Miscellaneous Interpretations](#misc-interpretations)\n  - [Plot Top Losses](#plot-top-losses)\n  - [Plot Confusion Matrix](#plot-confusion-matrix)\n  - [Plot Dataset Examples](#plot-dataset-examples)\n\n\n\n## Tutorials\n\nRun the tutorials in the browser using Google Colab.\n\nTutorial | Link\n---      | ---\nIntroduction to `interpret` | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttumiel/interpret/blob/master/nbs/Interpret-Intro.ipynb)\nVisualisation Tutorial | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttumiel/interpret/blob/master/nbs/Visualisation-Tutorial.ipynb)\nMiscellaneous Methods Tutorial | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ttumiel/interpret/blob/master/nbs/Misc-Interpretations-Tutorial.ipynb)\n\n\n<br/>\n<div align=\"center\">\n<h2 id='visualisation'>Visualisation</h2>\n</div>\n\n![Channel visualisations using pytorch interpret.](./images/collage-vis.jpg)\n\nVisualisation is a technique that generates inputs that optimise a particular objective within a trained network. By using visualisations, we can understand what it is that a network is looking for. For an in-depth explanation of visualisation, see [Feature Visualisation](https://distill.pub/2017/feature-visualization/).\n\n<h3 id=\"vis-quickstart\">Quickstart</h3>\n\nGenerating visualisations is done by loading a trained network, selecting the objective to optimise for and running the optimisation. An example using a pretrained network from `torchvision` is shown.\n\n\n```python\nfrom interpret import OptVis\nimport torchvision\n\n# Get the PyTorch neural network\nnetwork = torchvision.models.vgg11(pretrained=True)\n\n# Select a layer from the network. Use get_layer_names()\n# to see a list of layer names and sizes.\nlayer = 'features/18'\nchannel = 12\n\n# Create an OptVis object from a PyTorch model\noptvis = OptVis.from_layer(network, layer=layer, channel=channel)\n\n# Create visualisation\noptvis.vis()\n```\n\n### Parameterisations\n\nImages can be parameterised in several different ways. As long as the parameterisation is differentiable, the input can be optimised for a particular layer. For code examples, see the [Visualisation Tutorial Notebook](#tutorials).\n\nThe default parameterisation is in spatial and colour decorrelated space.\n\n![Decorrelated visualisations](./images/channel.jpg)\n\nWe can also parameterise in regular pixel space but the visualisations tend to be worse.\n\n![Pixel space parameterised visualisations](./images/pixels.jpg)\n\nAnother parameterisation is a compositional pattern producing network (CPPN) which can generate infinite resolution images that have the effect of \"light paintings.\"\n\n![CPPN visualisations](./images/cppn.jpg)\n\n\n### Objectives\n\nThe objective on which to optimise can also be manipulated to create different visualisations. We can add objectives together to get compound objectives or negate them to get negative neurons. See the [Visualisation Tutorial Notebook](#tutorials) for examples.\n\n#### Layer Objective\n\nA LayerObjective can be created easily using the `from_layer` OptVis class method. In this function, we can choose the layer, channel and neuron to optimise for. Here we can optimise for a particular neuron:\n\n![Neuron visualisations](./images/neuron.jpg)\n\nWe can also manually create two objectives and add them together to get a compound objective:\n\n![Compound activations between more than one objective](./images/compound.jpg)\n\nOr we can find the negated objective that minimises a particular neuron:\n\n![Negative neurons minimise a particular activation](./images/negatives.jpg)\n\nLayer objectives are fairly flexible. You can select any layer in the network and capture the output of that particular layer. We can visualise the last layer of the network, generating class visualisations of the different classes in ImageNet.\n\n![Class Visualisations](./images/class_vis.png)\n\n#### Deep Dream Objective\n\nThe deep dream objective optimises for \"interestingness\" across an entire layer. We can create this objective from an input image and select a layer using the `from_dream` class method.\n\n![Deep dream objective visualisations](./images/dream.jpg)\n\n<br/>\n<div align=\"center\">\n<h2 id='attribution'>Attribution</h2>\n</div>\n\n![Attribution methods show where a neural network is looking when it makes a certain prediction.](./images/attr.jpg)\n\nNetwork attribution is done by feeding a particular input into the trained network and generating a saliency map that shows the parts of the image that the network activates highly on.\n\n<h3 id=\"attr-quickstart\">Quickstart</h3>\n\n```python\nfrom interpret import Gradcam, norm\nfrom PIL import Image\nimport torchvision\n\nnetwork = torchvision.models.vgg11(pretrained=True)\ninput_img = Image.open('image.jpg')\n\n# Normalise the input image and turn it into a tensor\ninput_data = norm(input_img)\n\n# Select the class that we are attributing to\nclass_number = 207\n\n# Choose a layer for Grad-CAM\nlayer = 'features/20'\n\n# Generate a Grad-CAM attribution map\nsaliency_map = Gradcam(network, input_data, im_class=class_number, layer=layer)\nsaliency_map.show()\n```\n\n<br/>\n<div align=\"center\">\n<h2 id='misc-interpretations'>Miscellaneous Interpretations</h2>\n</div>\n\nIncluded in `interpret` are a few additional interpretation methods that don't neatly fit into visualisation or attribution methods.\n\n### Plot Top Losses\n\nPlot the inputs that result in the largest loss. Useful for identifying where your network is most unsure or where the inputs actually don't fit the label given (a mislabelled image). You can also enable a Grad-CAM attribution overlay for each image so that you can tell where the network is looking.\n\n<img src=\"./images/top_losses.png\" alt=\"Top losses plotted with Grad-CAM attribution overlay.\" width=\"500px\"/>\n\n### Plot Confusion Matrix\n\nPlot a confusion matrix for a multi-class classification or binned regression objective.\n\n![Confusion matrix on 10 classes](./images/confusion_mat.png)\n\n### Plot Dataset Examples\n\nPlot some dataset examples that maximise a particular `LayerObjective` from the visualisation objectives described above. Useful for identifying clear examples of what the network is looking for in a particular visualisation using real examples.\n\n![Comparison between a layer visualisation and dataset examples that also activate the same layer.](./images/dataset_examples.jpg)\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ttumiel/interpret",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "interpret-pytorch",
    "package_url": "https://pypi.org/project/interpret-pytorch/",
    "platform": "",
    "project_url": "https://pypi.org/project/interpret-pytorch/",
    "project_urls": {
      "Homepage": "https://github.com/ttumiel/interpret"
    },
    "release_url": "https://pypi.org/project/interpret-pytorch/0.2.1/",
    "requires_dist": [
      "numpy",
      "matplotlib",
      "pandas",
      "Pillow",
      "tqdm",
      "torchvision",
      "torch",
      "coverage ; extra == 'test'",
      "pytest ; extra == 'test'",
      "scikit-learn ; extra == 'test'"
    ],
    "requires_python": ">=3.6",
    "summary": "Interpreting deep learning models in PyTorch.",
    "version": "0.2.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 8269939,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3b1cc7d932983bccafddc1a0a2f05ad2f2e9d4a5ab4b215ce4ed1460ff070d9e",
          "md5": "3e2c635c73810bbe5c43da81400f2162",
          "sha256": "848923fa5e17771cdcf7bad5a851f1e891427e85c48fb57c003c87bea6758407"
        },
        "downloads": -1,
        "filename": "interpret_pytorch-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3e2c635c73810bbe5c43da81400f2162",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 36803,
        "upload_time": "2019-10-31T13:07:23",
        "upload_time_iso_8601": "2019-10-31T13:07:23.226847Z",
        "url": "https://files.pythonhosted.org/packages/3b/1c/c7d932983bccafddc1a0a2f05ad2f2e9d4a5ab4b215ce4ed1460ff070d9e/interpret_pytorch-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "32302ebc24b59bc72c86b7e9a21706f39643470329854aeb18e3e4bd1c9822bb",
          "md5": "a2de971d2dedeff0820839b4b0d4f2e6",
          "sha256": "6df75fa0e4609a8e27bf6fe67550e2d253b6c7b744f8c019fe23176a66795cf0"
        },
        "downloads": -1,
        "filename": "interpret-pytorch-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a2de971d2dedeff0820839b4b0d4f2e6",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 33277,
        "upload_time": "2019-10-31T13:07:25",
        "upload_time_iso_8601": "2019-10-31T13:07:25.812234Z",
        "url": "https://files.pythonhosted.org/packages/32/30/2ebc24b59bc72c86b7e9a21706f39643470329854aeb18e3e4bd1c9822bb/interpret-pytorch-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "57bb6ce5c943873d912be6b9fad2b2072fa1ca2b99a41c55cdb00b250737b490",
          "md5": "89485fed8fbecccbbd28bee9669c272d",
          "sha256": "7be6b864a4e485dc05a49bc355256c009e4f6cb740f558e3b1ecf2cdf04beea3"
        },
        "downloads": -1,
        "filename": "interpret_pytorch-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "89485fed8fbecccbbd28bee9669c272d",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 39891,
        "upload_time": "2020-05-26T14:04:24",
        "upload_time_iso_8601": "2020-05-26T14:04:24.954778Z",
        "url": "https://files.pythonhosted.org/packages/57/bb/6ce5c943873d912be6b9fad2b2072fa1ca2b99a41c55cdb00b250737b490/interpret_pytorch-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "084a2ac6c2f3cbba45c717f4780a494ecc9dc28ad6facc780157aea76f018c7f",
          "md5": "cc802f98c59ce1df93dfc62b6228284f",
          "sha256": "94e7a71a8e7840335f33c84c08fc36cd7dc27c527459d5a5f9baec67043821dc"
        },
        "downloads": -1,
        "filename": "interpret-pytorch-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "cc802f98c59ce1df93dfc62b6228284f",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 35680,
        "upload_time": "2020-05-26T14:04:26",
        "upload_time_iso_8601": "2020-05-26T14:04:26.934533Z",
        "url": "https://files.pythonhosted.org/packages/08/4a/2ac6c2f3cbba45c717f4780a494ecc9dc28ad6facc780157aea76f018c7f/interpret-pytorch-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eac0898a60e80037b972f973e1f181b3e7ba6697c3204f2644b3e2aab8206e3b",
          "md5": "2e0b40a67714a157a74c12bc11e1dfba",
          "sha256": "1ef8cb80cc31fc9293d4af22823f07535c050e50dd4d20ab7b729a53e1c4a030"
        },
        "downloads": -1,
        "filename": "interpret_pytorch-0.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2e0b40a67714a157a74c12bc11e1dfba",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 47958,
        "upload_time": "2020-09-25T09:45:56",
        "upload_time_iso_8601": "2020-09-25T09:45:56.357762Z",
        "url": "https://files.pythonhosted.org/packages/ea/c0/898a60e80037b972f973e1f181b3e7ba6697c3204f2644b3e2aab8206e3b/interpret_pytorch-0.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a41aa1a32768f57cecf9e404d13f5bbda93df96b8cd86cd3df95d8a2360a556f",
          "md5": "3ba925db509375b52c67a6b7416d3616",
          "sha256": "26d0890c78a2ad605be5bd83dec202789ad4b0512b65fa121787a243a53826d6"
        },
        "downloads": -1,
        "filename": "interpret-pytorch-0.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "3ba925db509375b52c67a6b7416d3616",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 40913,
        "upload_time": "2020-09-25T09:45:58",
        "upload_time_iso_8601": "2020-09-25T09:45:58.056949Z",
        "url": "https://files.pythonhosted.org/packages/a4/1a/a1a32768f57cecf9e404d13f5bbda93df96b8cd86cd3df95d8a2360a556f/interpret-pytorch-0.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "eac0898a60e80037b972f973e1f181b3e7ba6697c3204f2644b3e2aab8206e3b",
        "md5": "2e0b40a67714a157a74c12bc11e1dfba",
        "sha256": "1ef8cb80cc31fc9293d4af22823f07535c050e50dd4d20ab7b729a53e1c4a030"
      },
      "downloads": -1,
      "filename": "interpret_pytorch-0.2.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "2e0b40a67714a157a74c12bc11e1dfba",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 47958,
      "upload_time": "2020-09-25T09:45:56",
      "upload_time_iso_8601": "2020-09-25T09:45:56.357762Z",
      "url": "https://files.pythonhosted.org/packages/ea/c0/898a60e80037b972f973e1f181b3e7ba6697c3204f2644b3e2aab8206e3b/interpret_pytorch-0.2.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a41aa1a32768f57cecf9e404d13f5bbda93df96b8cd86cd3df95d8a2360a556f",
        "md5": "3ba925db509375b52c67a6b7416d3616",
        "sha256": "26d0890c78a2ad605be5bd83dec202789ad4b0512b65fa121787a243a53826d6"
      },
      "downloads": -1,
      "filename": "interpret-pytorch-0.2.1.tar.gz",
      "has_sig": false,
      "md5_digest": "3ba925db509375b52c67a6b7416d3616",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 40913,
      "upload_time": "2020-09-25T09:45:58",
      "upload_time_iso_8601": "2020-09-25T09:45:58.056949Z",
      "url": "https://files.pythonhosted.org/packages/a4/1a/a1a32768f57cecf9e404d13f5bbda93df96b8cd86cd3df95d8a2360a556f/interpret-pytorch-0.2.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}