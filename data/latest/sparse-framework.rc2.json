{
  "info": {
    "author": "",
    "author_email": "Antero Vainio <anterongugl@gmail.com>",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "# Sparse\n\nThis repository contains source code for Stream Processing Architecture for Resource Subtle Environments (or just\nSparse for short). Additionally, sample applications utilizing Sparse for deep learning can be found in examples\ndirectory.\n\n## Quick start with deep learning\n\nInstall sparse framework from PyPi:\n\n```\npip install sparse-framework\n```\n\nCreate a sparse worker node which trains a neural network using data sent by master:\n```model_trainer.py\n\"\"\"model_trainer.py\n\"\"\"\nimport torch\nfrom torch import nn\n\nfrom sparse_framework.node.worker import Worker\nfrom sparse_framework.dl.gradient_calculator import GradientCalculator\n\n# PyTorch model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n# Sparse node\nclass ModelTrainer(Worker):\n    def __init__(self):\n        model = NeuralNetwork()\n        loss_fn = nn.CrossEntropyLoss()\n        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n        Worker.__init__(self,\n                        task_executor = GradientCalculator(model=model,\n                                                           loss_fn=loss_fn,\n                                                           optimizer=optimizer))\n\nif __name__ == '__main__':\n    ModelTrainer().start()\n```\n\nThen create the corresponding sparse master node:\n```data_source.py\n\"\"\"data_source.py\n\"\"\"\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nimport asyncio\n\nfrom sparse_framework.dl.serialization import encode_offload_request, decode_offload_response\nfrom sparse_framework.node.master import Master\n\n# Sparse node\nclass TrainingDataSource(Master):\n    async def train(self, batch_size = 64, epochs = 1):\n        # torchvision dataset\n        training_data = datasets.FashionMNIST(\n            root=\"data\",\n            train=True,\n            download=True,\n            transform=ToTensor(),\n        )\n        for t in range(epochs):\n            for batch, (X, y) in enumerate(DataLoader(training_data, batch_size)):\n                input_data = encode_offload_request(X, y)\n                result_data = await self.task_deployer.deploy_task(input_data)\n                split_grad, loss = decode_offload_response(result_data)\n                print('Loss: {}'.format(loss))\n\nif __name__ == '__main__':\n    asyncio.run(TrainingDataSource().train())\n```\n\nTo run training, start the worker and the master processes (in separate terminal sessions):\n```\npython model_trainer.py\npython data_source.py\n```\n\n## Compatibility\n\nThe software has been tested, and thus can be considered compatible with, the following devices and the following\nsoftware:\n\n| Device            | JetPack version | Python version | PyTorch version | Docker version | Base image                                     | Docker tag suffix |\n| ----------------- | --------------- | -------------- | --------------- | -------------- | ---------------------------------------------- | ------------------ |\n| Jetson AGX Xavier | 5.0 preview     | 3.8.10         | 1.12.0a0        | 20.10.12       | nvcr.io/nvidia/l4t-pytorch:r34.1.0-pth1.12-py3 | jp50               |\n| Lenovo ThinkPad   | -               | 3.8.12         | 1.11.0          | 20.10.15       | pytorch/pytorch:1.11.0-cuda11.3-cudnn8-runtime | amd64              |\n\n## Install from source\n\nThe repository uses PyTorch as the primary Deep Learning framework. Software dependencies can be installed with pip or\nby using Docker.\n\n### Make\n\nSoftware can be installed with make utility, by running the following command:\n```\nmake all\n```\n\n## Run training\n\n### All-In-One\n\nTo test that the program was installed correctly, run the training suite with an unsplit model with the following\ncommand:\n\n```\nmake run-learning-aio\n```\n\n### Unsplit offloaded\n\nFirst start the unsplit training server with the following command:\n```\nmake run-learning-unsplit\n```\n\nThen start the data source with the following command:\n```\nmake run-learning-data-source\n```\n\n### Split offloaded\n\nFirst start the split training nodes with the following command:\n```\nmake run-learning-split\n```\n\nThen start the data source with the following command:\n```\nmake run-learning-data-source\n```\n\n## Run Inference\n\n### All-In-One\n\nTo test that the program was installed correctly, run the inference suite with an unsplit model with the following\ncommand:\n\n```\nmake run-inference-aio\n```\n\n### Unsplit offloaded\n\nFirst start the unsplit inference server with the following command:\n```\nmake run-inference-unsplit\n```\n\nThen start the data source with the following command:\n```\nmake run-inference-data-source\n```\n\n### Split offloaded\n\nFirst start the split inference nodes with the following command:\n```\nmake run-inference-split\n```\n\n\n### Statistics\n\nIn order to collect benchmark statistics for training or inference, before running a suite with the above instructions,\nfirst start the monitor server by running the following command:\n\n```\nmake run-sparse-monitor\n```\n\n## Configuration\n\nNodes can be configured with environment variables. Environment variables can be specified inline, or with a dotenv\nfile in the data directory.\n\nWhen using the Make scripts for running the software, the dotenv file should be `./data/.env`:\n```\nmkdir data\ntouch data/.env\n```\n\n### Configuration Options\n\nParameters prefixed with MASTER are used by master nodes, and the ones prefixed with WORKER by worker nodes. When not\nspecified, default configuration parameters are used.\n\n| Configuration parameter | Environment variable  | Default value |\n| ----------------------- | --------------------- | ------------- |\n| Master upstream host    | MASTER_UPSTREAM_HOST  | 127.0.0.1     |\n| Master upstream port    | MASTER_UPSTREAM_PORT  | 50007         |\n| Worker listen address   | WORKER_LISTEN_ADDRESS | 127.0.0.1     |\n| Worker listen port      | WORKER_LISTEN_PORT    | 50007         |\n\nBy convention, the port 50007 will be used for workers that expect raw data, i.e. unsplit workers, and the first\nsplits. The port 50008 is used by workers that expect the first task split output data, i.e. final split nodes. While\nthis port mapping is not a technical requirement, the Make scripts follow it.\n\n## Multi-node deployment\n\nIn order to set up a pipeline on multiple hosts, make sure that the master nodes have IP connectivity to the worker\nnodes. Then, for each master node, specify the IP address of the worker that the task will be offloaded to. If using\nthe Make scripts to start nodes, this is the only configuration required.\n\n### Example: Three node split training\nThis is an example on how to configure split training across three nodes: a data source, an intermediate worker, and a\nfinal worker. The data source will send the feature vectors to the intermediate worker, which will process the first\nsplit of the task. The intermediate node will then send the results of the first split to the final worker which will\nrun the final split to finish the task.\n\nAssume that the nodes have the following IP addressing in place:\n\n| Node                  | IP address    |\n| --------------------- | ------------- |\n| Data source           | 10.49.2.1     |\n| Intermediate worker   | 10.49.2.2     |\n| Final worker          | 10.49.2.3     |\n\n1. Start the final worker node\n\nRun the following command to start the final training split in the *final worker* node:\n```\nmake run-learning-split-final\n```\n\n2. Configure and start the intermediate worker\n\nAdd a .env file with the following contents in the *intermediate worker* node:\n```\nMASTER_UPSTREAM_HOST=10.49.2.3\n```\n\nThen start the intermediate worker by running the following command:\n```\nmake run-learning-split-intermediate\n```\n\n3. Configure and start the data source\n\nAdd a .env file with the following contents in the *data source* node:\n```\nMASTER_UPSTREAM_HOST=10.49.2.2\n```\n\nThen start the data source by running the following command:\n```\nmake run-learning-data-source\n```\n\n## Uninstall\n\nThe locally stored assets can be removed by running the following command:\n```\nmake clean\n```\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "sparse-framework",
    "package_url": "https://pypi.org/project/sparse-framework/",
    "platform": null,
    "project_url": "https://pypi.org/project/sparse-framework/",
    "project_urls": {
      "Bug Tracker": "https://github.com/AnteronGitHub/sparse/issues",
      "Homepage": "https://github.com/AnteronGitHub/sparse"
    },
    "release_url": "https://pypi.org/project/sparse-framework/1.0.0.post2/",
    "requires_dist": [
      "python-dotenv (==0.21.1)",
      "torch (==1.11.0)",
      "torchvision (==0.12.0)"
    ],
    "requires_python": ">=3.7",
    "summary": "Stream Processing Architecture for Resource Subtle Environments",
    "version": "1.0.0.post2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17011060,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "99fb2e3e0b4e29391ac938c367566cf172d3640c441be5c3afc199eb34cfc3fa",
          "md5": "696bd9d08e5001ab6dc410c90b24d2ff",
          "sha256": "665b494fa88c7de58f9fae36c87e82ada3b91b796e4b81e140448cb79f73a0a5"
        },
        "downloads": -1,
        "filename": "sparse_framework-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "696bd9d08e5001ab6dc410c90b24d2ff",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 3578,
        "upload_time": "2023-02-23T09:43:02",
        "upload_time_iso_8601": "2023-02-23T09:43:02.765164Z",
        "url": "https://files.pythonhosted.org/packages/99/fb/2e3e0b4e29391ac938c367566cf172d3640c441be5c3afc199eb34cfc3fa/sparse_framework-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b18780e52526cb87653854220b6a606865c7255ef7a6392e7d4036f4b00dba04",
          "md5": "a07e93fe1e37611b408ba8b0575ddc23",
          "sha256": "e2bf21f3d92b0174638e6ae23a87b685f19548c7c8fe2072c63e3db6fe64d1ac"
        },
        "downloads": -1,
        "filename": "sparse-framework-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "a07e93fe1e37611b408ba8b0575ddc23",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 3429,
        "upload_time": "2023-02-23T09:43:04",
        "upload_time_iso_8601": "2023-02-23T09:43:04.927907Z",
        "url": "https://files.pythonhosted.org/packages/b1/87/80e52526cb87653854220b6a606865c7255ef7a6392e7d4036f4b00dba04/sparse-framework-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.0.post2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f2e88b67c2256dbfbfcd69eb75e7ebf85f10c883f131a057adf7d28353231f5a",
          "md5": "014eb8e5c3a52a9218adfd3c8104a54c",
          "sha256": "5de2593a8df61c8fb386c80919268836528b430cc3291f4b3f014232691f9aca"
        },
        "downloads": -1,
        "filename": "sparse_framework-1.0.0.post2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "014eb8e5c3a52a9218adfd3c8104a54c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 19126,
        "upload_time": "2023-02-24T13:14:08",
        "upload_time_iso_8601": "2023-02-24T13:14:08.047772Z",
        "url": "https://files.pythonhosted.org/packages/f2/e8/8b67c2256dbfbfcd69eb75e7ebf85f10c883f131a057adf7d28353231f5a/sparse_framework-1.0.0.post2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "f2e88b67c2256dbfbfcd69eb75e7ebf85f10c883f131a057adf7d28353231f5a",
        "md5": "014eb8e5c3a52a9218adfd3c8104a54c",
        "sha256": "5de2593a8df61c8fb386c80919268836528b430cc3291f4b3f014232691f9aca"
      },
      "downloads": -1,
      "filename": "sparse_framework-1.0.0.post2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "014eb8e5c3a52a9218adfd3c8104a54c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 19126,
      "upload_time": "2023-02-24T13:14:08",
      "upload_time_iso_8601": "2023-02-24T13:14:08.047772Z",
      "url": "https://files.pythonhosted.org/packages/f2/e8/8b67c2256dbfbfcd69eb75e7ebf85f10c883f131a057adf7d28353231f5a/sparse_framework-1.0.0.post2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}