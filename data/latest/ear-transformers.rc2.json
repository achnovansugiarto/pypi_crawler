{
  "info": {
    "author": "Giuseppe Attanasio",
    "author_email": "giuseppeattanasio6@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "# Entropy-based Attention Regularization Frees Unintended Bias Mitigation from Lists\n\nEAR is a regularization technique to mitigate uninteded bias while reducing lexical overfitting. It is based on attention entropy maximization. In practice, EAR adds a regularization term at training time to learn tokens with maximal self-attention entropy.\n\n## Project structure\n\nThe data used in this study is in `data`. Please note that we are not allowed to distribute all the data sets. For some of those, you will need to download it yourselves (instructions below).\nThe code is organized in python scripts (training and evaluation of models), bash scripts to run experiments, and jupyter notebooks.\n\nThe main files are the following:\n- `train_bert.py`: use this script to train any bert-based model starting from HuggingFace checkpoints.\n- `evaluate_model.py`: use this script to evaluate a model either on a test set or a synthetic evaluation set.\n\nPlease find all the accepted parameters running `python <script_name> --help`.\n\n## Getting started\n\nThe following are the basic steps to setup our environment and replicate our results.\n\n## Getting the data sets\n\nPlease follow these instructions to retrive the presented dataset:\n\n- Misogyny (EN): the dataset is not publicly available. Please fill [this form](https://docs.google.com/forms/d/e/1FAIpQLSevs4Ji3dNmK5CxyulYG-PxX3U10-RgDrPpMKPRjtI81f0yaQ/viewform) to submit a request to the authors.\n- Misogyny (IT): the dataset is not publicly available. Please fill [this form](https://forms.gle/uFF3sAtMMqayiDiz9) to submit a request to the authors.\n- Multilingual and Multi-Aspect (MlMA): the dataset is available online. In `data`, we provide our splitfiles with the additional binary \"hate\" column used in our experiments.\n\nFor the sake of simplicty, we have assigned short names to each data set. Please find them and how to use them in [dataset.py](./dataset.py).\n\n## Dependencies\n\nYou'll need a working Python environment to run the code. \nThe required dependencies are specified in the file `environment.yml`.\nWe use `conda` virtual environments to manage the project dependencies in\nisolation.\n\nRun the following command in the repository folder to create a separate environment \nand install all required dependencies in it:\n\n    conda create -n ear python==3.8\n    conda activate ear\n    pip install -r requirements.txt\n\n## Example\n\nEAR can be plugged very easily to HuggingFace models.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport ear\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\nitem = tokenizer(\"Today it's a good day!\")\noutputs = model(**item, output_attentions=True)\n\nreg_strength = 0.01\nneg_entropy = ear.compute_negative_entropy(\n    inputs=outputs.attentions,\n    attention_mask=item[\"attention_mask\"]\n)\nreg_loss = reg_strength * neg_entropy\nloss = reg_loss + output.loss\n\n```\n\n## Reproducing Hate Speech Detection results\n\nThe [`bash`](bash) folder contains some utility bash scripts useful to run multiple experiments sequentially. They cover the training and evaluation pipeline of all the models tested in the paper. To let everything work as expected, please run them from the parent directory.\n\n### Training\n\nPlease check out your disk size, these scripts will save two model checkpoints (best and the last one) for every seed.\n\nTrain **BERT** on the Misogyny (EN) dataset:\n\n```bash\n./bash/train_model_10_seeds.sh bert-base-uncased <output_dir> <training_dataset>\n```\n\ne.g., `./bash/train_model_10_seeds.sh bert-base-uncased . miso`\n\n\nTrain **BERT+EAR** on the Multilingual and Multi-Aspect dataset:\n\n```bash\n./bash/train_model_EAR_10_seeds.sh bert-base-uncased <output_dir> <training_dataset>\n```\n\ne.g., `./bash/train_model_EAR_10_seeds.sh bert-base-uncased . mlma`\n\n\nNote that:\n- if you want to take into account class imbalance, you should add the `--balanced_loss` to the parameters passed as command line arguments to python;\n- for BERT+SOC (Kennedy et al. 2020), we re-use the authors's implementation. Therefore, no\ntraining scripts are provided here.\n\n## Testing\n\nTo evaluate a model, or a folder with several models (different seeds), you have to:\n1. run the evaluation on synthetic data.\n2. run the evaluation on test data \n\n### Evaluation of bias metrics on synthetic data\n\nHere we provide an example to run the evaluation on Madlibs77K synthetic data using a specific checkpoint name (`last.ckpt` in this case).\n\n```bash\n./bash/evaluate_folder_madlibs_pattern.sh <in_dir> <out_dir> last.ckpt\n```\n\nAnalogous script for the other synthetic sets are stored in the folder `./bash`. Namely:\n- `evaluate_folder_miso_synt.sh` Run the evaluation of all the models within a specified parent directory on Misogyny (EN), synthetic set.\n- `evaluate_folder_miso-ita_synt.sh` Run the evaluation of all the models within a specified parent directory on Misogyny (IT), synthetic set.\n\n### Evaluation on test data\n\nHere we provide an example to run the evaluation on the test set of MlMA.\n\n```bash\n./bash/test_folder.sh <in_dir> <out_dir> mlma <src_tokenizer> <ckpt_pattern>\n```\nNote that evaluation on Misogyny (IT) requires the parameter `--src_tokenizer dbmdz/bert-base-italian-uncased`\n\n## EAR for Biased Term Extraction\n\nWe provide a Jupyter Notebook where we show how to extract terms with the lowest contextualization, which\nmay induce most of the bias in the model.\n\nAfter having trained at least one model (i.e., you have a model checkpoint), the notebook [`term_extraction.ipynb`](term_extraction.ipynb) will guide you through the discovery of biased terms.\n\n### ðŸš¨ Ethical considerations\n\nThe process of building the list remains a data-driven approach, which is strongly dependent on the task, collected corpus, term frequencies, and the chosen model.\nTherefore, the list might either lack specific terms that instead need to be attentioned, or include some that do not strictly perpetrate harm.\nBecause of these twin issues, the resulting lists should not be read as complete or absolute. We would therefore discourage users from simply building and developing models based solely on the extracted terms. We want, instead, the terms to stand as a starting point for debugging and searching for potential bias issues in the task at hand. \n\n## License\n\nAll source code is made available under a MIT license. See `LICENSE.md` for the full license text.\n\nThe manuscript text is not open source. The authors reserve the rights to the article content, which is currently submitted for publication.\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/g8a9/ear",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ear-transformers",
    "package_url": "https://pypi.org/project/ear-transformers/",
    "platform": null,
    "project_url": "https://pypi.org/project/ear-transformers/",
    "project_urls": {
      "Homepage": "https://github.com/g8a9/ear"
    },
    "release_url": "https://pypi.org/project/ear-transformers/1.0.1/",
    "requires_dist": [
      "transformers"
    ],
    "requires_python": "",
    "summary": "Entropy-based Attention Regularization",
    "version": "1.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 13391224,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c9838d03299455f3f8eb92b209fe52888d15141b7184f39409d45c7a53c0795e",
          "md5": "80f17f2464f80c95263b9f577dc8f4c5",
          "sha256": "5634fa19024161d0512811bacb259f88e51d9800e309e73b5a702374b61c31ce"
        },
        "downloads": -1,
        "filename": "ear_transformers-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "80f17f2464f80c95263b9f577dc8f4c5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 6003,
        "upload_time": "2022-04-03T09:50:51",
        "upload_time_iso_8601": "2022-04-03T09:50:51.323630Z",
        "url": "https://files.pythonhosted.org/packages/c9/83/8d03299455f3f8eb92b209fe52888d15141b7184f39409d45c7a53c0795e/ear_transformers-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "58a2f99f9e1c4caf6942f0ef7a0ca2541ac9e80bb39b0d6cbc167e852edd75b1",
          "md5": "ece1d1eb66a31f12cb4d075b46ec089a",
          "sha256": "12f103149bd10f45640a9f48b7784f163b1a58ad7c3ae07b7055b302c01b0526"
        },
        "downloads": -1,
        "filename": "ear-transformers-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ece1d1eb66a31f12cb4d075b46ec089a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5934,
        "upload_time": "2022-04-03T09:50:53",
        "upload_time_iso_8601": "2022-04-03T09:50:53.219029Z",
        "url": "https://files.pythonhosted.org/packages/58/a2/f99f9e1c4caf6942f0ef7a0ca2541ac9e80bb39b0d6cbc167e852edd75b1/ear-transformers-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "02e2b7685da63c0100833b6972bda24959d81819ee4fdfe2158a1d429a91cdeb",
          "md5": "42f58aecb989186b08b7073e31e8ec41",
          "sha256": "0bc7f140a711d6b61617fe5e4d9885e40ce133e692bd89a5b6a5c3641d49e2ff"
        },
        "downloads": -1,
        "filename": "ear_transformers-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "42f58aecb989186b08b7073e31e8ec41",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 6000,
        "upload_time": "2022-04-03T10:07:56",
        "upload_time_iso_8601": "2022-04-03T10:07:56.552689Z",
        "url": "https://files.pythonhosted.org/packages/02/e2/b7685da63c0100833b6972bda24959d81819ee4fdfe2158a1d429a91cdeb/ear_transformers-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "830c2cb38a03c8c6560381b58b7793230c614927a4d194cdff901c8c88e10d51",
          "md5": "ce18ff30f89d3bbea923317417273a2c",
          "sha256": "63daa43cb91895bf0e47b82abab82a10352f5e747954cfa043eb179c41e3c510"
        },
        "downloads": -1,
        "filename": "ear-transformers-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ce18ff30f89d3bbea923317417273a2c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 5908,
        "upload_time": "2022-04-03T10:07:58",
        "upload_time_iso_8601": "2022-04-03T10:07:58.160635Z",
        "url": "https://files.pythonhosted.org/packages/83/0c/2cb38a03c8c6560381b58b7793230c614927a4d194cdff901c8c88e10d51/ear-transformers-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "02e2b7685da63c0100833b6972bda24959d81819ee4fdfe2158a1d429a91cdeb",
        "md5": "42f58aecb989186b08b7073e31e8ec41",
        "sha256": "0bc7f140a711d6b61617fe5e4d9885e40ce133e692bd89a5b6a5c3641d49e2ff"
      },
      "downloads": -1,
      "filename": "ear_transformers-1.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "42f58aecb989186b08b7073e31e8ec41",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 6000,
      "upload_time": "2022-04-03T10:07:56",
      "upload_time_iso_8601": "2022-04-03T10:07:56.552689Z",
      "url": "https://files.pythonhosted.org/packages/02/e2/b7685da63c0100833b6972bda24959d81819ee4fdfe2158a1d429a91cdeb/ear_transformers-1.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "830c2cb38a03c8c6560381b58b7793230c614927a4d194cdff901c8c88e10d51",
        "md5": "ce18ff30f89d3bbea923317417273a2c",
        "sha256": "63daa43cb91895bf0e47b82abab82a10352f5e747954cfa043eb179c41e3c510"
      },
      "downloads": -1,
      "filename": "ear-transformers-1.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "ce18ff30f89d3bbea923317417273a2c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 5908,
      "upload_time": "2022-04-03T10:07:58",
      "upload_time_iso_8601": "2022-04-03T10:07:58.160635Z",
      "url": "https://files.pythonhosted.org/packages/83/0c/2cb38a03c8c6560381b58b7793230c614927a4d194cdff901c8c88e10d51/ear-transformers-1.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}