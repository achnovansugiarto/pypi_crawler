{
  "info": {
    "author": "Alvaro Avila",
    "author_email": "almiavicas@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: POSIX",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3",
      "Topic :: Software Development :: Libraries"
    ],
    "description": "# as-scraper-airflow\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/as-scraper-airflow.svg)](https://pypi.org/project/as-scraper-airflow/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/as-scraper-airflow)](https://pypi.org/project/as-scraper-airflow/)\n\nPython library for scraping inside Airflow.\n\n# Installation\n\nThe **as-scraper** library uses Geckodriver (Firefox) for scraping with the Selenium library.\nIn order to use it, you need to have an airflow image having the Geckodriver dependency.\n\nWe have the [as-airflow](https://github.com/Avila-Systems/as-airflow) Docker image for you to have airflow ready with the Geckodriver dependency.\n\nTo use this library follow the next steps:\n\n### 1. Download the `docker-compose.yml` file from the Airflow docs.\n\nAirflow provides the [docker-compose.yml](https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#docker-compose-yaml) file you need for this library.\n\nYou can directly copy the `docker-compose.yml` file from [here](https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml) or run the following command to download it:\n\n```bash\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.3.4/docker-compose.yaml'\n```\n\n### 2. Modify the `docker-compose.yml` file to use the `as-airflow` image.\n\nThere are two ways of configuring the required docker image for this library.\n\n#### Option a. Create a Dockerfile that extends from the *almiavicas/as-airflow* image.\n\nTo do this, simply go into the *docker-compose.yml* file, comment the `image` line and uncomment the `build` tag:\n\n```yaml\n...\nversion: '3'\nx-airflow-common:\n  &airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.3.4}\n  build: .\n  ...\n```\n\nThen create your **Dockerfile** and copy and paste the following lines:\n\n```Dockerfile\nFROM almiavicas/as-airflow:2.2.3\n\nRUN pip install --no-cache-dir as-scraper-airflow\n\n```\n\n#### Option b. Modify the *docker-compose.yaml* to install the library.\n\nTo do this, go to the *docker-compose.yml* file and make the following changes:\n\n```yaml\n...\nversion: '3'\nx-airflow-common:\n  &airflow-common\n  # In order to add custom dependencies or upgrade provider packages you can use your extended image.\n  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml\n  # and uncomment the \"build\" line below, Then run `docker-compose build` to build the images.\n  image: ${AIRFLOW_IMAGE_NAME:-almiavicas/as-airflow:2.2.3}\n  # build: .\n  environment:\n    ...\n    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-as-scraper-airflow}\n```\n\nAnd that's it! You can now start using the as-scraper library.\n\n# Usage\n\nIf you are starting a new Airflow project, before running your containers you need to run the following command to configure volumes:\n\n```bash\nmkdir dags/ logs/ plugins/\n```\n\nYou can now run `docker-compose up` and you'll have your Airflow environment up & running.\n\n## Creating a simple scraper\n\nLets say that we want to scrap [yellowpages.com](https://www.yellowpages.com). Our target data would be the popular cities that we can find in the [sitemap](https://www.yellowpages.com/sitemap) url.\n\nOur output data will have two columns: `name` of the city and `url` which is linked to the city. For example, for *Houston*, we would want the following output:\n\n| name | url |\n|:-----|:----|\n|Houston|https://www.yellowpages.com/houston-tx|\n\n### Declaring our Scraper Class\n\nSo first we create a scraper that extends from the Scraper class, and define the `COLUMNS` variable to `['name', 'url']`.\n\nCreate the *dags/scrapers/yellowpages.py* file and type the following code into it:\n\n```python\nfrom as_scraper.scraper import Scraper\n\n\nclass YellowPagesScraper(Scraper):\n    COLUMNS = ['name', 'url']\n\n```\n\n### Deciding wether to load javascript or not\n\nNow, there are two execution options when running scrapers. We can either *load javascript* which uses the **Selenium** library, or not load javascript and use the *requests* library for http requests.\n\nFor this example, let's go ahead and use the **Selenium** library. To configure this, simply add the following variable to your scraper:\n\n```python\nfrom as_scraper.scraper import Scraper\n\n\nclass YellowPagesScraper(Scraper):\n    COLUMNS = ['name', 'url']\n    LOAD_JAVASCRIPT = True\n\n```\n\n### Defining the `scrape_handler`\n\nAnd the magic comes in the next step. We will define the `scrape_handler` method in our class, which will have the responsibility to scrape a given url and extract the data from it.\n\n> All scrapers must define the `scrape_handler` method.\n\n```python\nfrom typing import Optional\nfrom selenium.webdriver import Firefox\nfrom selenium.webdriver.common.by import By\nimport pandas as pd\nfrom as_scraper.scraper import Scraper\n\n\nclass YellowPagesScraper(Scraper):\n    COLUMNS = ['name', 'url']\n    LOAD_JAVASCRIPT = True\n\n    def scrape_handler(self, url: str, html: Optional[str] = None, driver: Optional[Firefox] = None, **kwargs) -> pd.DataFrame:\n        rows = []\n        div_tag = driver.find_element(By.CLASS_NAME, \"row-content\")\n        div_tag = div_tag.find_element(By.CLASS_NAME, \"row\")\n        section_tags = div_tag.find_elements(By.TAG_NAME, \"section\")\n        for section_tag in section_tags:\n            a_tags = section_tag.find_elements(By.TAG_NAME, \"a\")\n            for a_tag in a_tags:\n                city_name = a_tag.text\n                city_url = a_tag.get_attribute(\"href\")\n                rows.append({\"name\": city_name, \"url\": city_url})\n        df = pd.DataFrame(rows, columns=self.COLUMNS)\n        return df\n\n```\n\n### Creating the DAG.\n\nNow we want to create a DAG that will trigger the scraper. For that we will use the **ScraperToLogsOperator**.\n\nAs we mentioned before, the target url for our scraper is the https://www.yellowpages.com/sitemap. In the Dag definition file we will define the url that we want to scrape.\n\n> There are other ways of specifying urls based on a discovery strategy. However, for this example it's not required.\n\nCreate the *dags/yellowpages.py* file and copy the following content into it:\n\n```python\nfrom datetime import datetime, timedelta\nfrom airflow.models import DAG\nfrom scrapers.yellowpages import YellowPagesScraper\nfrom as_scraper_airflow.operators import ScraperToLogsOperator\n\n\nwith DAG(\n    dag_id=\"yellow_pages\",\n    catchup=False,\n    default_args={\n        'depends_on_past': False,\n        'email': ['airflow@example.com'],\n        'email_on_failure': False,\n        'email_on_retry': False,\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5),\n    },\n    description=\"A simple Scraper DAG\",\n    schedule_interval=timedelta(days=1),\n    start_date=datetime(2022, 8, 4),\n    catchup=False,\n) as dag:\n    t1 = ScraperToLogsOperator(\n        scraper_cls=YellowPagesScraper,\n        urls=['https://www.yellowpages.com/sitemap'],\n        task_id='scrape',\n        save_errors=True,\n    )\n\n```\n\nAnd that's it! Head to the airflow webserver to run your DAG!\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/Avila-Systems/as-scraper-airflow",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "as-scraper-airflow",
    "package_url": "https://pypi.org/project/as-scraper-airflow/",
    "platform": null,
    "project_url": "https://pypi.org/project/as-scraper-airflow/",
    "project_urls": {
      "Github Project": "https://github.com/Avila-Systems/as-scraper-airflow",
      "Homepage": "https://github.com/Avila-Systems/as-scraper-airflow",
      "Issue Tracker": "https://github.com/Avila-Systems/as-scraper-airflow/issues"
    },
    "release_url": "https://pypi.org/project/as-scraper-airflow/1.2.0/",
    "requires_dist": [
      "apache-airflow (>=2.2.3)",
      "apache-airflow-providers-google",
      "as-scraper"
    ],
    "requires_python": ">=3.6",
    "summary": "Python library for scraping inside Airflow.",
    "version": "1.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15309013,
  "releases": {
    "1.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "08e4bac38c94503e63224342f8bd91aeae48bbb562de924a59223fadaa3e9e01",
          "md5": "f5e2bd5d68dc8ad84d6463e78ba8963b",
          "sha256": "7ad6de92509f37cf68c98f79ef11d387dd69054a806a0e3be4e60fc9c03629df"
        },
        "downloads": -1,
        "filename": "as_scraper_airflow-1.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f5e2bd5d68dc8ad84d6463e78ba8963b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9180,
        "upload_time": "2022-10-05T04:12:46",
        "upload_time_iso_8601": "2022-10-05T04:12:46.721859Z",
        "url": "https://files.pythonhosted.org/packages/08/e4/bac38c94503e63224342f8bd91aeae48bbb562de924a59223fadaa3e9e01/as_scraper_airflow-1.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cfaaee980eb8c5d4fcaa41231e7b0973f89c45e9eee5d0e9564566d994d42352",
          "md5": "4bdb77260222cd8662af8e83d4a87aff",
          "sha256": "2630ccfd982acafe0086f0cddbf1b8dd484c56d59f03eba034e80b89909b9438"
        },
        "downloads": -1,
        "filename": "as-scraper-airflow-1.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "4bdb77260222cd8662af8e83d4a87aff",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 10069,
        "upload_time": "2022-10-05T04:12:48",
        "upload_time_iso_8601": "2022-10-05T04:12:48.090585Z",
        "url": "https://files.pythonhosted.org/packages/cf/aa/ee980eb8c5d4fcaa41231e7b0973f89c45e9eee5d0e9564566d994d42352/as-scraper-airflow-1.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cb7baba7ed42275b800ac3692aaefdfe6c8dea3a981eae0c31c68bd2e1d84c65",
          "md5": "9590755aa582abadd422908a8b6b66a6",
          "sha256": "4f22cd33512bd4ce9f047d736d14c71d804c57a23d3ad9488da63e900ce32763"
        },
        "downloads": -1,
        "filename": "as_scraper_airflow-1.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "9590755aa582abadd422908a8b6b66a6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9180,
        "upload_time": "2022-10-05T04:14:19",
        "upload_time_iso_8601": "2022-10-05T04:14:19.633036Z",
        "url": "https://files.pythonhosted.org/packages/cb/7b/aba7ed42275b800ac3692aaefdfe6c8dea3a981eae0c31c68bd2e1d84c65/as_scraper_airflow-1.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f7dac32319a5b5fa2d5c727be3b89ed17a291cf9c1825a98b56776de5ddf5c0d",
          "md5": "b7fbb9116fca191b6119eb94d0abcc3c",
          "sha256": "6b34a5fcc195b6b2d7e6f0c1615c4e3237d4cfb4cb5ea0230b203adf591f331f"
        },
        "downloads": -1,
        "filename": "as-scraper-airflow-1.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "b7fbb9116fca191b6119eb94d0abcc3c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 10109,
        "upload_time": "2022-10-05T04:14:21",
        "upload_time_iso_8601": "2022-10-05T04:14:21.358228Z",
        "url": "https://files.pythonhosted.org/packages/f7/da/c32319a5b5fa2d5c727be3b89ed17a291cf9c1825a98b56776de5ddf5c0d/as-scraper-airflow-1.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "58f7c9cd76975f920f485e9ae6c8a31bca24ba003c4c636761f5c520995d8bf0",
          "md5": "97218eab56c1d0d4fa2cd75516d97ce6",
          "sha256": "9ed5de84d29b6d21db0ba9165f75267bbe122e9f30bca90581a9d9d1f659c97f"
        },
        "downloads": -1,
        "filename": "as_scraper_airflow-1.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "97218eab56c1d0d4fa2cd75516d97ce6",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9182,
        "upload_time": "2022-10-05T04:15:26",
        "upload_time_iso_8601": "2022-10-05T04:15:26.512430Z",
        "url": "https://files.pythonhosted.org/packages/58/f7/c9cd76975f920f485e9ae6c8a31bca24ba003c4c636761f5c520995d8bf0/as_scraper_airflow-1.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cba3aaa33d091a4f3b62e52c139368f4376819c503c4453368e9d183d29b7239",
          "md5": "38f87b91f76d6a58e2feeef9482716bc",
          "sha256": "29500a8f9c370d55f26407929085252c25570d9f6709371f4446366524199849"
        },
        "downloads": -1,
        "filename": "as-scraper-airflow-1.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "38f87b91f76d6a58e2feeef9482716bc",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 10083,
        "upload_time": "2022-10-05T04:15:27",
        "upload_time_iso_8601": "2022-10-05T04:15:27.959886Z",
        "url": "https://files.pythonhosted.org/packages/cb/a3/aaa33d091a4f3b62e52c139368f4376819c503c4453368e9d183d29b7239/as-scraper-airflow-1.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "19caa3290f8a4ad5309fb4179ba5ec2234e79b3017e689e9bf78d09ac6bdd878",
          "md5": "6c223b45df55335662422160abe5c11b",
          "sha256": "3858bc9c94a5fb541c3391591a755e17fb94cf6e9508bb0b4a6c969ab796725f"
        },
        "downloads": -1,
        "filename": "as_scraper_airflow-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6c223b45df55335662422160abe5c11b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9180,
        "upload_time": "2022-10-05T04:35:14",
        "upload_time_iso_8601": "2022-10-05T04:35:14.908460Z",
        "url": "https://files.pythonhosted.org/packages/19/ca/a3290f8a4ad5309fb4179ba5ec2234e79b3017e689e9bf78d09ac6bdd878/as_scraper_airflow-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "eb25561a0cafda93947a5c3d4cf987bcce0804df6f4a752c69bd26e4a8c4e42b",
          "md5": "aac15e985089ceb6ad1c24e1523053a4",
          "sha256": "69776f3989718e9b0ec0a64b206f82a50cb6fd2c911d942fffb0a5d2aca56d18"
        },
        "downloads": -1,
        "filename": "as-scraper-airflow-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "aac15e985089ceb6ad1c24e1523053a4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 10072,
        "upload_time": "2022-10-05T04:35:17",
        "upload_time_iso_8601": "2022-10-05T04:35:17.034191Z",
        "url": "https://files.pythonhosted.org/packages/eb/25/561a0cafda93947a5c3d4cf987bcce0804df6f4a752c69bd26e4a8c4e42b/as-scraper-airflow-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "685fb42c9533d4e753ef3e31c8a019c593b740ce26818f411c09ddf34a89d265",
          "md5": "476965817308f0e56c4ed6c3cad26325",
          "sha256": "773633445d27b4122f650071a51c4448cf448399e4749f4c93a4db65af0c16b5"
        },
        "downloads": -1,
        "filename": "as_scraper_airflow-1.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "476965817308f0e56c4ed6c3cad26325",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9180,
        "upload_time": "2022-10-05T04:44:35",
        "upload_time_iso_8601": "2022-10-05T04:44:35.525486Z",
        "url": "https://files.pythonhosted.org/packages/68/5f/b42c9533d4e753ef3e31c8a019c593b740ce26818f411c09ddf34a89d265/as_scraper_airflow-1.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8b53f8efc2b8b8e8014f3dfcba639d35c8cdcd1b89f43e62bdbe4089287691bc",
          "md5": "39ba16147df31b111b1a1581e45b5c58",
          "sha256": "5d61e0ebdbc5c784328f8b1f21d44be18c5aa076d3866a9bc1c976d64531ab6b"
        },
        "downloads": -1,
        "filename": "as-scraper-airflow-1.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "39ba16147df31b111b1a1581e45b5c58",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 10075,
        "upload_time": "2022-10-05T04:44:37",
        "upload_time_iso_8601": "2022-10-05T04:44:37.051710Z",
        "url": "https://files.pythonhosted.org/packages/8b/53/f8efc2b8b8e8014f3dfcba639d35c8cdcd1b89f43e62bdbe4089287691bc/as-scraper-airflow-1.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "685fb42c9533d4e753ef3e31c8a019c593b740ce26818f411c09ddf34a89d265",
        "md5": "476965817308f0e56c4ed6c3cad26325",
        "sha256": "773633445d27b4122f650071a51c4448cf448399e4749f4c93a4db65af0c16b5"
      },
      "downloads": -1,
      "filename": "as_scraper_airflow-1.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "476965817308f0e56c4ed6c3cad26325",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 9180,
      "upload_time": "2022-10-05T04:44:35",
      "upload_time_iso_8601": "2022-10-05T04:44:35.525486Z",
      "url": "https://files.pythonhosted.org/packages/68/5f/b42c9533d4e753ef3e31c8a019c593b740ce26818f411c09ddf34a89d265/as_scraper_airflow-1.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "8b53f8efc2b8b8e8014f3dfcba639d35c8cdcd1b89f43e62bdbe4089287691bc",
        "md5": "39ba16147df31b111b1a1581e45b5c58",
        "sha256": "5d61e0ebdbc5c784328f8b1f21d44be18c5aa076d3866a9bc1c976d64531ab6b"
      },
      "downloads": -1,
      "filename": "as-scraper-airflow-1.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "39ba16147df31b111b1a1581e45b5c58",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 10075,
      "upload_time": "2022-10-05T04:44:37",
      "upload_time_iso_8601": "2022-10-05T04:44:37.051710Z",
      "url": "https://files.pythonhosted.org/packages/8b/53/f8efc2b8b8e8014f3dfcba639d35c8cdcd1b89f43e62bdbe4089287691bc/as-scraper-airflow-1.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}