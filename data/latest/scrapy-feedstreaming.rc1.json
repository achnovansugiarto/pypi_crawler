{
  "info": {
    "author": "Alexander Berkovich",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 5 - Production/Stable",
      "Environment :: Console",
      "Framework :: Scrapy",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: BSD License",
      "Natural Language :: English",
      "Operating System :: Microsoft :: Windows",
      "Operating System :: OS Independent",
      "Operating System :: POSIX",
      "Operating System :: Unix",
      "Programming Language :: Python",
      "Programming Language :: Python :: 3 :: Only",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7",
      "Programming Language :: Python :: 3.8",
      "Programming Language :: Python :: 3.9",
      "Programming Language :: Python :: Implementation :: CPython",
      "Topic :: Software Development :: Libraries :: Python Modules",
      "Topic :: Utilities"
    ],
    "description": "## scrapy-feedstreaming\n\nScrapy live Streaming data. `scrapy.extensions.feedexport.FeedExporter` fork to export item during scraping. See \n[https://medium.com/@alex_ber/scrapy-streaming-data-cdf97434dc15]\n\nSee CHANGELOG.md for detail description.\n\n\n\n### Getting Help\n\n\n### QuickStart\n```bash\npython3 -m pip install -U scrapy-feedstreaming\n```\n\n\n### Installing from Github\n\n```bash\npython3 -m pip install -U https://github.com/alex-ber/scrapy-feedstreaming/archive/master.zip\n```\nOptionally installing tests requirements.\n\n```bash\npython3 -m pip install -U https://github.com/alex-ber/scrapy-feedstreaming/archive/master.zip#egg=alex-ber-utils[tests]\n```\n\nOr explicitly:\n\n```bash\nwget https://github.com/alex-ber/scrapy-feedstreaming/archive/master.zip -O master.zip; unzip master.zip; rm master.zip\n```\nAnd then installing from source (see below).\n\n\n### Installing from source\n```bash\npython3 -m pip install -r req.txt # only installs \"required\" (relaxed)\n```\n```bash\npython3 -m pip install . # only installs \"required\"\n```\n```bash\npython3 -m pip install .[tests] # installs dependencies for tests\n```\n\n#### Alternatively you install install from requirements file:\n```bash\npython3 -m pip install -r requirements.txt # only installs \"required\"\n```\n```bash\npython3 -m pip install -r requirements-tests.txt # installs dependencies for tests\n```\n\n##\n\nFrom the directory with setup.py\n```bash\npython3 setup.py test #run all tests\n```\n\nor\n\n```bash\n\npytest\n```\n\n## Installing new version\nSee https://docs.python.org/3.1/distutils/uploading.html \n\n```bash\npython3 setup.py sdist upload\n```\n\n## Requirements\n\n\nscrapy-feedstreaming requires the following modules.\n\n* Python 3.6+\n\n\n\n# Changelog\n\nScrapy live Streaming data. `scrapy.extensions.feedexport.FeedExporter` fork to export item during scraping. See \n[https://medium.com/@alex_ber/scrapy-streaming-data-cdf97434dc15]\n\nAll notable changes to this project will be documented in this file.\n\n\\#https://pypi.org/manage/project/scrapy-feedstreaming/releases/\n\n## [Unrelased]\n\n\n## [0.0.1] - 12/07/2020\n\n### Added\n* Buffering was added to `item_scraped()`.\n* S3FeedStorage: you can specify `ACL` as query part of URI.\n* S3FeedStorage: support of `region` is added. \n* FEEDS: `slot_key_param`: New (not available in Scrapy itself) specify (global) function which takes item and spider as parameter \nand `slot_key`. Given the item that is passed through the pipeline to what URI you want to send it.\nFall back to noop method â€” method that does nothing.\n* FEEDS: `buff_capacity`: New (not available in Scrapy itself) â€” after what amount of item you want to export them. \nThe fall back value is 1. \n* `_FeedSlot` instances are created from your settings. They are created per supplied URI. \nSome extra (compare to Scraping) information is stored, namely:\n- `uri_template` â€” it is available through public API get_slots() method, see below.\n- `spider_name` â€” is used in public API get_slots() method to restrict returned slots for requested spider.\n- `buff_capacity` â€”bufferâ€™s capacity, if the number of item exceed this number the buffer is flushed\n- `buff` â€” buffer where all items pending export are stored.\n* `FeedExported` there is 1 extra public method \n- `get_slots()` â€” this method is used to get feed slotâ€™s information (see implementation note above). It is populated from the settings. For example, you can retrieve to either URI you will export the items.\nNote:\n1. `slot_key` is slot identifier as described above. If you have only 1 URI you can supply None for this value.\n2. You can retrieve feed slotâ€™s information only from your spider.\n3. It has optional `force_create=True` parameter. \nIf youâ€™re calling this method early in the Scrapy life-cycle feed slotâ€™s information may be not yet created. \nIn this case, the default behavior is to create this information and return it for you. \nIf `force_create=False` is supplied you will receive an empty collection of feed slotâ€™s information.\n* On `S3FeedStorage` there couple of public methods:\n\n- `botocore_session`\n- `botocore_client`\n- `botocore_base_kwargs` â€” dict of minimal parameters for `botocore_client.put_object()` method as supplied in settings.\n- `botocore_kwargs` â€” dict of all supplied parameters `for botocore_client.put_object()` method as supplied in settings. \nFor example, if supplied, it will contain `ACL` parameter while `botocore_base_kwargs` will not contain it.\n\n\n### Changed\n* You can have multiple URI for exports.\n* Logic of sending the item was moved from the `close_spider()` to `item_scraped()`.\n* back-port Fix missing `storage.store()` calls in `FeedExporter.close_spider()` [https://github.com/scrapy/scrapy/pull/4626]\n* back-port Fix duplicated feed logs [https://github.com/scrapy/scrapy/pull/4629]\n\n\n### Removed\n* removed deprecated: fallback to `boto` library if `botocore` is not found\n* removed deprecated: implicit retrieval of settings from the project â€” settings is passed explicitly now\n\n\n<!--\n### Added \n### Changed\n### Removed\n-->\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/alex-ber/scrapy-feedstreaming",
    "keywords": "Scrapy Feed Exports S3 streaming data online extension plugin",
    "license": "Apache 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "scrapy-feedstreaming",
    "package_url": "https://pypi.org/project/scrapy-feedstreaming/",
    "platform": "",
    "project_url": "https://pypi.org/project/scrapy-feedstreaming/",
    "project_urls": {
      "Homepage": "https://github.com/alex-ber/scrapy-feedstreaming"
    },
    "release_url": "https://pypi.org/project/scrapy-feedstreaming/0.0.1/",
    "requires_dist": [
      "Scrapy",
      "botocore",
      "alex-ber-utils (==0.5.2) ; extra == 'tests'",
      "atomicwrites (==1.3.0) ; extra == 'tests'",
      "attrs (==19.1.0) ; extra == 'tests'",
      "colorama (==0.4.1) ; extra == 'tests'",
      "mock (==2.0.0) ; extra == 'tests'",
      "more-itertools (==6.0.0) ; extra == 'tests'",
      "pbr (==5.1.3) ; extra == 'tests'",
      "pluggy (==0.9.0) ; extra == 'tests'",
      "py (==1.8.0) ; extra == 'tests'",
      "pytest (==4.3.1) ; extra == 'tests'",
      "pytest-assume (==1.2.2) ; extra == 'tests'",
      "pytest-mock (==1.10.1) ; extra == 'tests'",
      "PyYAML (==5.1) ; extra == 'tests'",
      "six (==1.12.0) ; extra == 'tests'"
    ],
    "requires_python": ">=3.6",
    "summary": "Based on scrapy.extensions.feedexport.FeedExporter to live stream data",
    "version": "0.0.1",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 7683721,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ea92a8227b524fdf61ae3698de79c4e2a17ffb7dd93a5426680a76252239a2bc",
          "md5": "eb319c3bac8111ceb83241d4f9d4db4b",
          "sha256": "89ee2ea3ac994843fdeb381b1fda60712aa5d16f763d478c94cc0035763252c3"
        },
        "downloads": -1,
        "filename": "scrapy_feedstreaming-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "eb319c3bac8111ceb83241d4f9d4db4b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 9416,
        "upload_time": "2020-07-12T06:40:38",
        "upload_time_iso_8601": "2020-07-12T06:40:38.992736Z",
        "url": "https://files.pythonhosted.org/packages/ea/92/a8227b524fdf61ae3698de79c4e2a17ffb7dd93a5426680a76252239a2bc/scrapy_feedstreaming-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ba13d59a17821a748ff273e478478020a86253e1775b38609643ea3dde32afa9",
          "md5": "52bb8100fe7379b77677c40858cb4cdd",
          "sha256": "18fcd13db5c441f640f06ff36e2eaae1b1e74d24ac436f645ea9a1dd04a1576c"
        },
        "downloads": -1,
        "filename": "scrapy-feedstreaming-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "52bb8100fe7379b77677c40858cb4cdd",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 11923,
        "upload_time": "2020-07-12T06:40:40",
        "upload_time_iso_8601": "2020-07-12T06:40:40.341299Z",
        "url": "https://files.pythonhosted.org/packages/ba/13/d59a17821a748ff273e478478020a86253e1775b38609643ea3dde32afa9/scrapy-feedstreaming-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ea92a8227b524fdf61ae3698de79c4e2a17ffb7dd93a5426680a76252239a2bc",
        "md5": "eb319c3bac8111ceb83241d4f9d4db4b",
        "sha256": "89ee2ea3ac994843fdeb381b1fda60712aa5d16f763d478c94cc0035763252c3"
      },
      "downloads": -1,
      "filename": "scrapy_feedstreaming-0.0.1-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "eb319c3bac8111ceb83241d4f9d4db4b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 9416,
      "upload_time": "2020-07-12T06:40:38",
      "upload_time_iso_8601": "2020-07-12T06:40:38.992736Z",
      "url": "https://files.pythonhosted.org/packages/ea/92/a8227b524fdf61ae3698de79c4e2a17ffb7dd93a5426680a76252239a2bc/scrapy_feedstreaming-0.0.1-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ba13d59a17821a748ff273e478478020a86253e1775b38609643ea3dde32afa9",
        "md5": "52bb8100fe7379b77677c40858cb4cdd",
        "sha256": "18fcd13db5c441f640f06ff36e2eaae1b1e74d24ac436f645ea9a1dd04a1576c"
      },
      "downloads": -1,
      "filename": "scrapy-feedstreaming-0.0.1.tar.gz",
      "has_sig": false,
      "md5_digest": "52bb8100fe7379b77677c40858cb4cdd",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 11923,
      "upload_time": "2020-07-12T06:40:40",
      "upload_time_iso_8601": "2020-07-12T06:40:40.341299Z",
      "url": "https://files.pythonhosted.org/packages/ba/13/d59a17821a748ff273e478478020a86253e1775b38609643ea3dde32afa9/scrapy-feedstreaming-0.0.1.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}