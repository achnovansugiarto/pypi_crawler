{
  "info": {
    "author": "",
    "author_email": "kushdubey63@gmail.com",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "# CAPPr: zero-shot text classification using autoregressive language models\r\n\r\n[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)\r\n[![Documentation Status](https://readthedocs.org/projects/cappr/badge/?version=latest)](https://cappr.readthedocs.io/en/latest/?badge=latest)\r\n[![tests](https://github.com/kddubey/cappr/actions/workflows/test.yml/badge.svg)](https://github.com/kddubey/cappr/actions/workflows/test.yml)\r\n[![codecov](https://codecov.io/gh/kddubey/cappr/branch/main/graph/badge.svg?token=NYIL076PSM)](https://codecov.io/gh/kddubey/cappr)\r\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) \r\n[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\r\n\r\nPerform zero-shot text classification by estimating the probability that an inputted\r\ncompletion comes after an inputted prompt. Hence the name:\r\n\r\n> **C**ompletion<br>\r\n  **A**fter<br>\r\n  **P**rompt<br>\r\n  **Pr**obability<br>\r\n\r\nThe method is fleshed out in my [question on CrossValidated](https://stats.stackexchange.com/q/601159/337906).\r\n\r\n## Usage\r\n\r\n<details>\r\n<summary>Use a model from the OpenAI API</summary>\r\n\r\nSpecifically, this model must be compatible with the\r\n[/v1/completions](https://platform.openai.com/docs/models/model-endpoint-compatibility)\r\nendpoint.\r\n\r\nLet's classify\r\n[this sentiment example](https://platform.openai.com/docs/guides/completion/classification)\r\nfrom the OpenAI text completion docs.\r\n\r\n```python\r\nfrom cappr.openai.classify import predict\r\n\r\ntweet = 'I loved the new Batman movie!'\r\nprompt = f'Tweet: {tweet}\\nSentiment:'\r\n\r\nclass_names = ('positive', 'neutral', 'negative')\r\nprior       = (   1/8    ,    1/8   ,     3/4   )\r\n\r\npreds = predict(prompts=[prompt],\r\n                completions=class_names,\r\n                model='text-ada-001',\r\n                prior=prior)\r\npreds\r\n# ['positive']\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>Use a model from the HuggingFace model hub</summary>\r\n\r\nSpecifically, this model must be able to be loaded using\r\n`transformers.AutoModelForCausalLM.from_pretrained(model)`.\r\n\r\nSmaller LMs may not work well. But there will likely be better ones in the hub soon.\r\n\r\n```python\r\nfrom cappr.huggingface.classify import predict\r\n\r\nprompt = 'Which planet is closer to the Sun: Mercury or Earth?'\r\n\r\nclass_names = ('Mercury', 'Earth')\r\nprior = None  # uniform prior\r\n\r\npreds = predict(prompts=[prompt],\r\n                completions=class_names,\r\n                model='gpt2',\r\n                prior=prior)\r\npreds\r\n# ['Mercury']\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>Run in batches</summary>\r\n\r\nLet's use `huggingface` for this example cuz it's free. And let's predict probabilities\r\ninstead of the class.\r\n\r\n```python\r\nfrom cappr.huggingface.classify import predict_proba\r\n\r\nprompts = [\r\n    'Stephen Curry is a',\r\n    'Martina Navratilova was a',\r\n    \"Dexter, from the TV Series Dexter's Laboratory, is a\",\r\n    'LeBron James is a',    \r\n]\r\n\r\n# each of the prompts could be completed with one of these:\r\nclass_names = (\r\n    'basketball player',\r\n    'tennis player',\r\n    'scientist'\r\n)\r\n\r\nprior = (\r\n    1/6,  # few\r\n    1/6,  # few\r\n    2/3   # there are more\r\n)\r\n\r\npred_probs = predict_proba(prompts=prompts,\r\n                           completions=class_names,\r\n                           model='gpt2',\r\n                           batch_size=32,  # whatever fits on your CPU/GPU\r\n                           prior=prior)\r\n\r\n# pred_probs[i,j] = probability that prompts[i] is classified as class_names[j]\r\nprint(pred_probs.round(1))\r\n# [[0.5 0.3 0.2]\r\n#  [0.3 0.6 0.2]\r\n#  [0.1 0.1 0.8]\r\n#  [0.8 0.2 0. ]]\r\n\r\n# for each prompt, which completion is most likely?\r\npred_class_idxs = pred_probs.argmax(axis=1)\r\nprint([class_names[pred_class_idx] for pred_class_idx in pred_class_idxs])\r\n# ['basketball player',\r\n#  'tennis player',\r\n#  'scientist',\r\n#  'basketball player']\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>Run in batches, where each prompt has a different set of possible completions\r\n</summary>\r\n\r\nAgain, let's use `huggingface` to predict probabilities. And this time, let's pass in an \r\ninstantiated model and tokenizer instead of its name. That way, the model isn't\r\nre-loaded every time you wanna use it.\r\n\r\n```python\r\nimport numpy as np\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\n\r\nfrom cappr import Example\r\nfrom cappr.huggingface.classify import predict_proba_examples\r\n\r\nexamples = [\r\n    Example(prompt='Jodie Foster played',\r\n            completions=('Clarice Starling', 'Trinity in The Matrix')),\r\n    Example(prompt='Batman, from Batman: The Animated Series, was played by',\r\n            completions=('Pete Holmes', 'Kevin Conroy', 'Spongebob!'),\r\n            prior=      (     1/3      ,      2/3     ,      0      ))\r\n]\r\n\r\nmodel_name = 'gpt2'\r\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\npred_probs = predict_proba_examples(examples,\r\n                                    model_and_tokenizer=(model, tokenizer))\r\n\r\n# pred_probs[i][j] = probability that examples[i].prompt is classified as\r\n# examples[i].completions[j]\r\nprint([example_pred_probs.round(2)\r\n       for example_pred_probs in pred_probs])\r\n# [array([0.7, 0.3]),\r\n#  array([0.03, 0.97, 0.  ])]\r\n\r\n# for each example, which completion is most likely?\r\npred_class_idxs = [np.argmax(example_pred_probs)\r\n                   for example_pred_probs in pred_probs]\r\nprint([example.completions[pred_class_idx]\r\n       for example, pred_class_idx in zip(examples, pred_class_idxs)])\r\n# ['Clarice Starling',\r\n#  'Kevin Conroy']\r\n```\r\n</details>\r\n\r\nMore examples are linked [here in the\r\ndocumentation](https://cappr.readthedocs.io/en/latest/5_examples.html).\r\n\r\nSee [`demos/copa.ipynb`](https://github.com/kddubey/cappr/blob/main/demos/copa.ipynb)\r\nfor a demonstration of a slightly harder classification task.\r\n\r\n\r\n## Documentation\r\n\r\nhttps://cappr.readthedocs.io/en/latest/\r\n\r\nPlease let me know if you find the writing too dense. The main motivation behind this\r\nproject is simplicity :-)\r\n\r\n\r\n## Setup\r\n\r\nIf you intend on using OpenAI models, [sign up for the OpenAI API\r\nhere](https://platform.openai.com/signup), and then set the environment variable\r\n`OPENAI_API_KEY`. For zero-shot classification, OpenAI models are currently far ahead of\r\nothers. But using them will cost ya ðŸ’°!\r\n\r\nInstall with `pip`:\r\n\r\n```\r\npython -m pip install cappr\r\n```\r\n\r\n<details>\r\n<summary>(Optional) Install requirements for HuggingFace models</summary>\r\n\r\n```\r\npython -m pip install cappr[hf]\r\n```\r\n</details>\r\n\r\n<details>\r\n<summary>(Optional) Set up to run demos</summary>\r\n\r\n```\r\npython -m pip install cappr[demos]\r\n```\r\n</details>\r\n\r\n\r\n## Motivation\r\n\r\nCreate a more usable zero-shot text classification interface than\r\n[classification via sampling (CVS)](https://platform.openai.com/docs/guides/completion/classification).\r\n\r\n<details>\r\n<summary>Short</summary>\r\n\r\nIn CVS, your job is to write up your classification task in a `prompt` string, and then\r\nwrite custom code to post-process arbitrary `completion`/output strings.\r\n\r\nIn CAPPr, your job starts and stops at writing up your classification task as a\r\n`{prompt}{end_of_prompt}{completion}` string.\r\n</details>\r\n\r\n<details>\r\n<summary>Long</summary>\r\n\r\nPlease see [this page of the\r\ndocumentation](https://cappr.readthedocs.io/en/latest/2_motivation.html).\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>Unstudied</summary>\r\n\r\nI'm curious to see how much easier estimation/discrimination is than generation.\r\nIn [`demos/copa.ipynb`](https://github.com/kddubey/cappr/blob/main/demos/copa.ipynb),\r\nCVS using OpenAI's `text-curie-001` is less than 50% accurate, while CAPPr is 80%\r\naccurate.\r\n\r\n</details>\r\n\r\n<details>\r\n<summary>Honest</summary>\r\n\r\nKeep myself busy\r\n\r\n</details>\r\n\r\n\r\n## Results\r\n\r\n<details>\r\n<summary>\r\nStatistical performance\r\n</summary>\r\nPerforms ok based on 2 datasets, when compared to classification via sampling (CVS).\r\nI need to run it on more ofc. Will update\r\n\r\n  * [`demos/copa.ipynb`](https://github.com/kddubey/cappr/blob/main/demos/copa.ipynb)\r\n  * [`demos/wsc.ipynb`](https://github.com/kddubey/cappr/blob/main/demos/wsc.ipynb)\r\n</details>\r\n\r\n\r\n<details>\r\n<summary>\r\nComputational performance\r\n</summary>\r\n\r\nOne concern was that CAPPr requires as many `model()` calls as there are classes. But in\r\nthe CAPPr scheme, we can simply cache each attention block's keys and values for the\r\nprompts. This feature is already supported by `AutoModelForCausalLM`s. See [this\r\ncode](https://github.com/kddubey/cappr/blob/main/src/cappr/huggingface/classify.py) for\r\nthe implementation. Note that this caching is not implemented for OpenAI models, as I\r\ncan't control their backend. **This means that when running `cappr.openai` functions,\r\nyou'll be on the *cappr (slow)* line** :-(\r\n\r\n![](/docs/source/_static/scaling_classes/batch_size_32.png)\r\n\r\n*Figure 1: [COPA](https://people.ict.usc.edu/~gordon/copa.html) dataset, repeating the\r\nchoices to simulate multi-class classification tasks. [GPT-2\r\n(small)](https://huggingface.co/gpt2) was run on a Tesla K80 GPU (whatever was free in\r\nGoogle Colab in March 2023, I'm not hardware savvy). 96 classification inputs were\r\nprocessed in batches of size 32. Each point in the graph is a median of 5 runs. For\r\nclassification via sampling (CVS), exactly 4 tokens were generated for each prompt,\r\nwhich is the number of tokens in `'\\n\\nAnswer A'`. 1-token times are also shown. But for\r\nCOPA (and other multiple-choice style prompts), that may result in lower zero-shot\r\naccuracy, as most of the sampled choices come after the first token.*\r\n\r\nSee the [`demos/computational_analysis.ipynb`\r\nnotebook](https://github.com/kddubey/cappr/blob/main/demos/computational_analysis.ipynb).\r\n\r\n</details>\r\n\r\n\r\n## Related work\r\n\r\nWhile [benchmarking this\r\nmethod](https://github.com/kddubey/cappr/blob/main/demos/wsc.ipynb) on the Winograd\r\nSchema Challenge, I found that [this paper](https://arxiv.org/abs/1806.02847) is very\r\nsimilar:\r\n\r\n> Trinh, Trieu H., and Quoc V. Le. \"A simple method for commonsense reasoning.\" arXiv preprint arXiv:1806.02847 (2018).\r\n\r\n[PET with multiple masks](https://arxiv.org/abs/2009.07118) also aggregates token\r\nprobabilities to do prompt-completion classification, but these probabilities are\r\nassumed to come from masked language models like BERT.\r\n\r\n> Schick, Timo, and Hinrich SchÃ¼tze. \"It's not just size that matters: Small language models are also few-shot learners.\" arXiv preprint arXiv:2009.07118 (2020).\r\n\r\n\r\n## Contributing\r\n\r\nTODO\r\n\r\n\r\n## Testing\r\n\r\n### Setup\r\n\r\n1. Clone the repo\r\n\r\n   ```\r\n   git clone https://github.com/kddubey/cappr.git\r\n   ```\r\n\r\n2. Create a new Python 3.8+ environment\r\n\r\n3. Install this package in editable mode, along with development requirements\r\n\r\n   ```\r\n   python -m pip install -e .[dev]\r\n   ```\r\n\r\n### Run tests\r\n\r\n```\r\npytest\r\n```\r\n\r\nDumping VS code extensions for development:\r\n  * [autoDocstring](https://marketplace.visualstudio.com/items?itemName=njpwerner.autodocstring).\r\n  Use the numpy format.\r\n  * [Set Python formatting to\r\n    `black`](https://dev.to/adamlombard/how-to-use-the-black-python-code-formatter-in-vscode-3lo0).\r\n  * [Rewrap](https://stkb.github.io/Rewrap/). Enable Auto Wrap.\r\n  * [TOML Language\r\n    Support](https://marketplace.visualstudio.com/items?itemName=be5invis.toml)\r\n\r\n\r\n## Todo\r\n\r\n(**) = I'm currently working on this or will work on it really soon\r\n\r\n<details>\r\n<summary>Code</summary>\r\n\r\n- [ ] Testing\r\n  - [ ] Increase test cases\r\n  - [ ] Some more standardization b/t openai and huggingface tests\r\n  - [x] Add code coverage badge to look cool\r\n  - [ ] Test input checks\r\n- [x] Small CPU speed-ups\r\n  - [x] For constant-completions input, vectorize `agg_log_probs`\r\n  - [x] For `examples` input, if # completions per prompt is constant, vectorize\r\n  `posterior_prob`\r\n- [ ] Add getLogger, basic logging\r\n- [ ] Make progress bars optional, since inference often isn't batched\r\n- [ ] Factor out input checks (on prompts and completions)\r\n- [x] De-automate overzealous auto-docstring stuff :-(\r\n- [ ] HuggingFace `transformers.AutoModelForCausalLM`\r\n  - [x] Optimize backend to enable greater scaling wrt # completions/classes\r\n  - [x] Get it working on single-GPU, check that it's faster than sampling assuming\r\n  batching\r\n    - [ ] Get to the bottom of why it's slower w/o batching\r\n  - [ ] Allow non-`' '` `end_of_prompt`! I'll have to go back to the drawing board I\r\n  think\r\n  - [ ] Consider batchifying the completions again, since they technically don't go in\r\n  batches of `batch_size`; the actual batch size is the sum of the number of completions\r\n  corresponding to the batch of prompts! Not a huge memory issue I think b/c completions\r\n  are usually half as long. But it should be configurable at the very least.\r\n  - [ ] Factor out repeated code b/t `classify` and `classify_no_cache`\r\n  - [ ] Support [Inference\r\n    Endpoints](https://huggingface.co/docs/inference-endpoints/index)?\r\n  - [ ] Support TensorFlow models if it's easy\r\n- [x] (for me) Auto-enforced code formatting b/c it's getting time-consuming\r\n- [ ] Allow for multi-label classification\r\n  - [ ] Pass `normalize` as an argument to predict_proba functions\r\n  - [ ] For `huggingface`, add note that you'll get faster results by passing all\r\n  labels at once (assuming prompt is identical for each label)\r\n- [ ] Create a notebook template\r\n- [ ] Fill in missing or non-numpy docstrings\r\n</details>\r\n\r\n<details>\r\n<summary>Research</summary>\r\n\r\nEvaluate on more datasets, and understand its relative advantages and disadvantages vs\r\nother classification methods.\r\n\r\n- [ ] More real world or harder datasets (**)\r\n  - [ ] See [this benchmark](https://arxiv.org/abs/2209.11055)\r\n  - [ ] Probably no good way to check train-test dependence unfortunately. Do I have to\r\n  buy datasets lol\r\n  - [ ] Mutli-class, multi-token labels, non-uniform prior\r\n- [ ] Re-run COPA demo w/ left-stripped completions (there are a few which aren't)\r\n- [ ] Create a user guide, build a table of results comparing competing\r\n  approaches on statistical performance, cost, and computation\r\n- [ ] Make a computational comparison to sampling (**)\r\n  - [x] Assume I have full freedom to decide how inference works. Demo w/\r\n  GPT-2. Process inputs in batches.\r\n  - [ ] Process inputs 1-by-1\r\n- [ ] More SuperGLUE tasks?\r\n- [ ] Calibration\r\n  - [ ] Is the prior actually effective? Downsample and see\r\n  - [ ] curves\r\n- [ ] Compare against few-shot embeddings\r\n- [ ] Finetune smaller, cheaper model and compare against zero-shot w/ davinci\r\n  - [ ] e.g., GPT-2 from huggingface, `text-ada-001`\r\n  - [ ] Again, compare against sampling\r\n- [ ] Evaluate a bigger model like GPT-J\r\n- [ ] Evaluate different aggregation functions. Currently taking mean, but\r\nthere was no good theory for that\r\n- [ ] A bit ambitious: support insertion and backwards-completion. Quite ambitious b/c\r\nmanipulating position IDs isn't sufficient (I think).\r\n</details>\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/kddubey/cappr/",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "cappr",
    "package_url": "https://pypi.org/project/cappr/",
    "platform": null,
    "project_url": "https://pypi.org/project/cappr/",
    "project_urls": {
      "Homepage": "https://github.com/kddubey/cappr/"
    },
    "release_url": "https://pypi.org/project/cappr/0.2.0/",
    "requires_dist": [
      "numpy (>=1.21.0)",
      "tqdm (>=4.27.0)",
      "openai (>=0.26.0)",
      "tiktoken (>=0.2.0)",
      "torch (>=1.12.1) ; extra == 'demos'",
      "transformers (>=4.26.1) ; extra == 'demos'",
      "datasets (>=2.10.0) ; extra == 'demos'",
      "jupyter (>=1.0.0) ; extra == 'demos'",
      "pandas (>=1.5.3) ; extra == 'demos'",
      "torch (>=1.12.1) ; extra == 'dev'",
      "transformers (>=4.26.1) ; extra == 'dev'",
      "datasets (>=2.10.0) ; extra == 'dev'",
      "jupyter (>=1.0.0) ; extra == 'dev'",
      "pandas (>=1.5.3) ; extra == 'dev'",
      "black (>=23.1.0) ; extra == 'dev'",
      "docutils (<0.19) ; extra == 'dev'",
      "pydata-sphinx-theme (>=0.13.1) ; extra == 'dev'",
      "pytest (>=7.2.1) ; extra == 'dev'",
      "pytest-cov (>=4.0.0) ; extra == 'dev'",
      "sphinx (>=6.1.3) ; extra == 'dev'",
      "sphinx-togglebutton (>=0.3.2) ; extra == 'dev'",
      "sphinxcontrib-napoleon (>=0.7) ; extra == 'dev'",
      "twine (>=4.0.2) ; extra == 'dev'",
      "torch (>=1.12.1) ; extra == 'hf'",
      "transformers (>=4.26.1) ; extra == 'hf'"
    ],
    "requires_python": ">=3.8.0",
    "summary": "Zero-shot text classification using autoregressive language models.",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17551075,
  "releases": {
    "0.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7e0672e494eaed8692774e255802b6ac3333407ddd8b0306092b3ce1f1e0b1d6",
          "md5": "0327583a9d5c81f40fc9913802d62795",
          "sha256": "e2948328b7680db1caa7700561a08dc837ae560cdbd261323bdbd4d3a4f4aa37"
        },
        "downloads": -1,
        "filename": "cappr-0.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0327583a9d5c81f40fc9913802d62795",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8.0",
        "size": 35393,
        "upload_time": "2023-03-30T18:38:54",
        "upload_time_iso_8601": "2023-03-30T18:38:54.192156Z",
        "url": "https://files.pythonhosted.org/packages/7e/06/72e494eaed8692774e255802b6ac3333407ddd8b0306092b3ce1f1e0b1d6/cappr-0.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "4bc75a89b622d4772ebbf8a92844cb3e311a52bda7c9e61dba775a82e680bde3",
          "md5": "aeaf769ba2c6aa6160b7d38a53a21ad4",
          "sha256": "2a9a997ef0148a0e73b1374549c2efe049546ab49ced9c4d2df3868b672ff74f"
        },
        "downloads": -1,
        "filename": "cappr-0.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "aeaf769ba2c6aa6160b7d38a53a21ad4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8.0",
        "size": 35329,
        "upload_time": "2023-03-30T18:38:57",
        "upload_time_iso_8601": "2023-03-30T18:38:57.270460Z",
        "url": "https://files.pythonhosted.org/packages/4b/c7/5a89b622d4772ebbf8a92844cb3e311a52bda7c9e61dba775a82e680bde3/cappr-0.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5b2100e9e6f71847707a1dd18d70c56ad46ecc74686133b9b697b344463af642",
          "md5": "aa877980b940f8a5297847b404236a1b",
          "sha256": "135bc1f0f20a21982f2b0c6a5a72a0be924dc59dcddf4903b049e4b48e47c428"
        },
        "downloads": -1,
        "filename": "cappr-0.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "aa877980b940f8a5297847b404236a1b",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.8.0",
        "size": 38425,
        "upload_time": "2023-04-02T21:45:59",
        "upload_time_iso_8601": "2023-04-02T21:45:59.593934Z",
        "url": "https://files.pythonhosted.org/packages/5b/21/00e9e6f71847707a1dd18d70c56ad46ecc74686133b9b697b344463af642/cappr-0.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "edcc3c511f574633dd8b009081804386f919a62651d15b9f950af49104d511b2",
          "md5": "974ede6c6fe26bd5b25d449371be15b1",
          "sha256": "1f2b032fa6224acd8fe1dd5b4e404da30b1df9ce12de4b2d044a4aff8793c0ce"
        },
        "downloads": -1,
        "filename": "cappr-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "974ede6c6fe26bd5b25d449371be15b1",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.8.0",
        "size": 36817,
        "upload_time": "2023-04-02T21:46:01",
        "upload_time_iso_8601": "2023-04-02T21:46:01.372901Z",
        "url": "https://files.pythonhosted.org/packages/ed/cc/3c511f574633dd8b009081804386f919a62651d15b9f950af49104d511b2/cappr-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "5b2100e9e6f71847707a1dd18d70c56ad46ecc74686133b9b697b344463af642",
        "md5": "aa877980b940f8a5297847b404236a1b",
        "sha256": "135bc1f0f20a21982f2b0c6a5a72a0be924dc59dcddf4903b049e4b48e47c428"
      },
      "downloads": -1,
      "filename": "cappr-0.2.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "aa877980b940f8a5297847b404236a1b",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.8.0",
      "size": 38425,
      "upload_time": "2023-04-02T21:45:59",
      "upload_time_iso_8601": "2023-04-02T21:45:59.593934Z",
      "url": "https://files.pythonhosted.org/packages/5b/21/00e9e6f71847707a1dd18d70c56ad46ecc74686133b9b697b344463af642/cappr-0.2.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "edcc3c511f574633dd8b009081804386f919a62651d15b9f950af49104d511b2",
        "md5": "974ede6c6fe26bd5b25d449371be15b1",
        "sha256": "1f2b032fa6224acd8fe1dd5b4e404da30b1df9ce12de4b2d044a4aff8793c0ce"
      },
      "downloads": -1,
      "filename": "cappr-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "974ede6c6fe26bd5b25d449371be15b1",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.8.0",
      "size": 36817,
      "upload_time": "2023-04-02T21:46:01",
      "upload_time_iso_8601": "2023-04-02T21:46:01.372901Z",
      "url": "https://files.pythonhosted.org/packages/ed/cc/3c511f574633dd8b009081804386f919a62651d15b9f950af49104d511b2/cappr-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}