{
  "info": {
    "author": "Ashan Liyanage, Ryan Dubnicek, HathiTrust Research Center",
    "author_email": "ashan8k@gmail.com, rdubnic2@illinois.edu, htrc-help@hathitrust.org",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: University of Illinois/NCSA Open Source License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3"
    ],
    "description": "<p align=\"center\">\n<a href=\"https://www.hathitrust.org/htrc\"><img src=\"https://wiki.htrc.illinois.edu/download/attachments/1769480/logo_googleForm.png?version=1&modificationDate=1584514252000&api=v2\" width=\"600\" title=\"HathiTrust Reseach Center\" alt=\"HTRC\"></a>\n</p>\n\n# ht_text_prep\n\n## Table of Contents\n1. [About ht_text_prep Library](#about)\n2. [Installation](#install)\n3. [Usage](#usage)\n\n\n## About `ht_text_prep` Library<a name=\"about\"></a>\nPython library to assist in processing HathiTrust full-text data outside of HathiTrust Research Center environments. This library helps manage the data that is stored and arrives in [pairtree](https://confluence.ucop.edu/display/Curation/PairTree) format.\n\nThis library also has tools for finding and removing running headers and footers in a given volume, removing them, and concatenating page-level text files into a single text file per HathiTrust volume.\n\n## Installation <a name=\"install\"></a>\n\nTo install,\n```bash\npip install ht_text_prep\n```\nThat's it! This library is written for Python 3.6+. For Python beginners, you'll need [pip](https://pip.pypa.io/en/stable/installing/).\n\n\n## Usage <a name=\"usage\"></a>\n\n### Import the library for use:\n\n```python\nimport ht_text_prep as htp\n```\n\n### Function: `get_zips(data_dir: str, output_dir: str, delete_zips=False)` \n\nA function that will traverse the pairtree directory structure, find the zip files that contain full text data from HathiTrust, expand them, and move the files to an output directory.\n\nReturns a new directory with one folder of page text files per volume\n\n#### Inputs:\n\n`data_dir`: path to folder holding the HathiTrust data to be cleaned/processed.\n\n`output_dir`: path to new output folder that will hold the cleaned/processed data. Will return error if folder already exists.\n\n`delete_zips`: default False, provide value True to delete original zipfiles after expansion, False to keep them.\n\n#### Examples:\n\n* Find and move zips for HathiTrust dataset, deleting zips after expanded:\n\n```python\nimport ht_text_prep as htp\n\ndata_dir = '/Users/rdubnic2/Desktop/data_download'\noutput_dir = '/Users/rdubnic2/Desktop/data'\n\nhtp.get_zips(data_dir, output_dir, delete_zips=True)\n```\n\n* Find and move zips for HathiTrust dataset, keeping original zips after expanded:\n\n```python\ndata_dir = '/Users/rdubnic2/Desktop/data_download'\noutput_dir = '/Users/rdubnic2/Desktop/data'\n\nhtp.get_zips(data_dir, output_dir, delete_zips=False)\n```\n\n* Using paths rather than variables, find and move zips for HathiTrust dataset, keeping original zips after expanded:\n\n```python \nhtp.get_zips('/Users/rdubnic2/Desktop/data_download', '/Users/rdubnic2/Desktop/data', delete_zips=False)\n```\n\n### Function: `normalize_txt_file_names(directory_path: str, prnt=False)`\n\nGiven an input path to a single directory holding page text files, this function will normalize irregular page text file names in HathiTrust data, converting all page text files names to an 8-digit sequence number in format '00000001.txt' in ascending numerical order based on original file names. For example:\n\n\t`0000000001.txt` becomes `00000001.txt`\n\n\t`ark+=13960=t3mw3px6k_00000001.txt` becomes `00000001.txt`\n\nThis function will also normalize jumps in page numbers greater than +1 between files sorted in ascending numerical order. For example, given this file list, names would be normalized to:\n\n\t`00000009.txt`  becomes  `00000009.txt`\n\n\t`00000010.txt`  becomes  `00000010.txt`\n\n\t`00000015.txt`  becomes  `00000011.txt`\n\n\t`00000016.txt`  becomes  `00000012.txt`\n\n\nThe function returns nothing explicitly, but yields normalized file names within the input directory.\n\n#### Inputs:\n\n`directory_path`: path to folder holding text files with filenames to be normalized\n\n`prnt`: parameter that determines if print messages are returned for each successfully normalized file. By default, this value is `False`.\n\n#### Examples:\n\n* Normalize file names for one volume's text files in one directory, without print messages:\n\n```python \nimport ht_text_prep as htp\ntest_directory = '/Users/username/Desktop/data_download/ark+=13960=t3mw3px6k'\nhtp.normalize_txt_file_names(test_directory)\n```\n\n* To normalize page file names for multiple volumes held in one top directory, use iteratively:\n\n```python\ntop_dir = ['/Users/rdubnic2/Desktop/data_download/ark+=13960=t3mw3px6k',\n\t'/Users/rdubnic2/Desktop/data_download/ark+=23200=t5mw3px1j',\n\t'/Users/rdubnic2/Desktop/data_download/ark+=53960=t4mp1qr7x']\n\nfor folder in top_dir:\n\thtp.normalize_txt_file_names(folder, prnt=True)\n```\n\n### Function: `load_vol(path: str, num_pages: int)`\n\nA function to load a HathiTrust volume, in the format of a folder of text files, and parse the page structure in advance of removing running headers and footers. \n\nReturns a list of HtrcPage(*) objects (indexed lines of text), ready as input for clean_vol.\n\n(*) See https://github.com/htrc/HTRC-Tools-RunningHeaders-Python/blob/develop/htrc/models.py\n\n#### Inputs:\n\n`path`: path to folder of text files for a given HathiTrust volume\n\n`num_pages`: the number of page text files in the directory for the volume\n\n#### Examples:\n\n* Load a HathiTrust volume using explicit parameters:\n\n```python\nimport ht_text_prep as htp\n\nhtp.load_vol('/Users/rdubnic2/Desktop/data_download/ark+=13960=t3mw3px6k', 7)\n```\n\n* Load a HathiTrust volume using variables, generating a list of paths using `glob`:\n\n```python\nimport glob\n\nvol_path = '/Users/rdubnic2/Desktop/data_download/ark+=13960=t3mw3px6k'\nnum_pages = len(glob.glob(str(vol_path)+'/**'))\n\nhtp.load_vol(vol_path, num_pages)\n```  \t\n\n### Function: `check_vol(vol_dir_path_list: list, clean_dir_path: str)`\n\nFunction to check an input directory to identify which volumes have already been processed by clean_vol. \nIntended as a helpful for very large worksets, where processing may be interrupted/stopped. This function\nwill return a list of volume paths that can be used to incrementally resume volume processing.\n\nReturns a list of volume directory paths that still require processing.\n\n#### Inputs:\n\n`vol_dir_path_list`: a list containing universal paths to directories containing HathiTrust page text\nfiles.\n\n`out_dir`: path to folder containing cleaned, concatenated text files.\n\n#### Examples:\n\n* Return a list of paths to volumes that have not yet been processed by `clean_vol()`:\n\n```python\nimport ht_text_prep as htp\n\ndata_dir = ['/Users/rdubnic2/Desktop/data_download/ark+=13960=t3mw3px6k',\n\t'/Users/rdubnic2/Desktop/data_download/ark+=23200=t5mw3px6k',\n\t'/Users/rdubnic2/Desktop/data_download/ark+=53960=t4mp1qr7x']\n\nout_dir = '/Users/rdubnic2/Desktop/clean_volumes/'\n\nhtp.check_vol(data_dir, out_dir)\n\n> ['/Users/rdubnic2/Desktop/data_download/ark+=53960=t4mp1qr7x']\n```\n\n* Use `check_vol()` as part of removing running headers/footers workflow, with `clean_vol()`:\n\n```python\ndata_dir = ['/Users/rdubnic2/Desktop/data_download/ark+=13960=t3mw3px6k',\n\t'/Users/rdubnic2/Desktop/data_download/ark+=23200=t5mw3px6k',\n\t'/Users/rdubnic2/Desktop/data_download/ark+=53960=t4mp1qr7x']\n\nout_dir = '/Users/rdubnic2/Desktop/clean_volumes/'\n\nto_be_cleaned = htp.check_vol(data_dir, out_dir)\n\nhtp.clean_vol(to_be_cleaned, out_dir)\n```\n\n### Function: `clean_vol(vol_dir_path_list: list, out_dir: str)`:\n\nFunction to parse the page structure of a HathiTrust volume, and write out only the page body text, removing running headers and footers.\n\nReturns nothing explicitly, but yields a single concatenated text file made up of input pages with running headers and footers removed, located in out_dir.\n\n#### Inputs:\n\n`vol_dir_path_list`: a list containing universal paths to directories containing HathiTrust page text\nfiles.\n\n`out_dir`: path to folder to contain cleaned, concatenated text files.\n\n#### Examples:\n\n* Remove running headers/footers for a directory containing sub-directories holding page text:\n\n```python\nimport ht_text_prep as htp\n\nvol_dir = ['/Users/rdubnic2/Desktop/data_download/ark+=13960=t3mw3px6k',\n\t'/Users/rdubnic2/Desktop/data_download/ark+=23200=t5mw3px1jk',\n\t'/Users/rdubnic2/Desktop/data_download/ark+=53960=t4mp1qr7x']\n\nout_dir = '/Users/rdubnic2/Desktop/final_vols/'\n\nhtp.clean_vol(vol_dir, out_dir)\n\n> 'Cleaned 3 volume(s)'\n```\n\n**Questions?** Contact htrc-help@hathitrust.org\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/htrc/ht-text-prep",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ht-text-prep",
    "package_url": "https://pypi.org/project/ht-text-prep/",
    "platform": "",
    "project_url": "https://pypi.org/project/ht-text-prep/",
    "project_urls": {
      "Homepage": "https://github.com/htrc/ht-text-prep"
    },
    "release_url": "https://pypi.org/project/ht-text-prep/1.0/",
    "requires_dist": null,
    "requires_python": ">=3.6",
    "summary": "Tools for management and processing of HathiTrust text data",
    "version": "1.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 9951257,
  "releases": {
    "1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ca2e834864d9aae0ad28109c49445e89ff945f1e9e91a7da0a26b74628364ae1",
          "md5": "c3b57d5dff2fa540555671349f7a3ed1",
          "sha256": "649dd7e6196079367cf691776c104efa27e6ac4379d08db5cf906b138869a8fe"
        },
        "downloads": -1,
        "filename": "ht_text_prep-1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c3b57d5dff2fa540555671349f7a3ed1",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 13450,
        "upload_time": "2021-04-01T21:59:38",
        "upload_time_iso_8601": "2021-04-01T21:59:38.265368Z",
        "url": "https://files.pythonhosted.org/packages/ca/2e/834864d9aae0ad28109c49445e89ff945f1e9e91a7da0a26b74628364ae1/ht_text_prep-1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e66d1cb460f63ad2de675540691b3ed444d7b4fda1f7d4ac4f41fe8d1bdf244a",
          "md5": "09d01d9d34f344b5da3045bd557a5b9a",
          "sha256": "cdf77d573e104e389db3009fdf21898030ec176dd60bff5c4a0bfcd307fa19f9"
        },
        "downloads": -1,
        "filename": "ht-text-prep-1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "09d01d9d34f344b5da3045bd557a5b9a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 13370,
        "upload_time": "2021-04-01T21:59:39",
        "upload_time_iso_8601": "2021-04-01T21:59:39.891102Z",
        "url": "https://files.pythonhosted.org/packages/e6/6d/1cb460f63ad2de675540691b3ed444d7b4fda1f7d4ac4f41fe8d1bdf244a/ht-text-prep-1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ca2e834864d9aae0ad28109c49445e89ff945f1e9e91a7da0a26b74628364ae1",
        "md5": "c3b57d5dff2fa540555671349f7a3ed1",
        "sha256": "649dd7e6196079367cf691776c104efa27e6ac4379d08db5cf906b138869a8fe"
      },
      "downloads": -1,
      "filename": "ht_text_prep-1.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "c3b57d5dff2fa540555671349f7a3ed1",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 13450,
      "upload_time": "2021-04-01T21:59:38",
      "upload_time_iso_8601": "2021-04-01T21:59:38.265368Z",
      "url": "https://files.pythonhosted.org/packages/ca/2e/834864d9aae0ad28109c49445e89ff945f1e9e91a7da0a26b74628364ae1/ht_text_prep-1.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e66d1cb460f63ad2de675540691b3ed444d7b4fda1f7d4ac4f41fe8d1bdf244a",
        "md5": "09d01d9d34f344b5da3045bd557a5b9a",
        "sha256": "cdf77d573e104e389db3009fdf21898030ec176dd60bff5c4a0bfcd307fa19f9"
      },
      "downloads": -1,
      "filename": "ht-text-prep-1.0.tar.gz",
      "has_sig": false,
      "md5_digest": "09d01d9d34f344b5da3045bd557a5b9a",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 13370,
      "upload_time": "2021-04-01T21:59:39",
      "upload_time_iso_8601": "2021-04-01T21:59:39.891102Z",
      "url": "https://files.pythonhosted.org/packages/e6/6d/1cb460f63ad2de675540691b3ed444d7b4fda1f7d4ac4f41fe8d1bdf244a/ht-text-prep-1.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}