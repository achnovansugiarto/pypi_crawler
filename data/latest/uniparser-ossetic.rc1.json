{
  "info": {
    "author": "Timofey Arkhangelskiy",
    "author_email": "timarkh@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "Intended Audience :: Science/Research",
      "License :: OSI Approved :: MIT License",
      "Operating System :: OS Independent",
      "Programming Language :: Python :: 3",
      "Topic :: Text Processing :: Linguistic"
    ],
    "description": "# Ossetic (Iron) morphological analyzer\r\n\r\nThis is a rule-based morphological analyzer for Ossetic (``oss``). It is based on a formalized description of the morphology of literary Ossetic, which is based on the Iron dialect, and uses [uniparser-morph](https://github.com/timarkh/uniparser-morph) for parsing. It performs full morphological analysis of Ossetic words (lemmatization, POS tagging, grammatical tagging).\r\n\r\n## How to use\r\n### Python package\r\nThe analyzer is available as a Python package. If you want to analyze Ossetic texts in Python, install the module:\r\n\r\n```\r\npip3 install uniparser-ossetic\r\n```\r\n\r\nImport the module and create an instance of ``OsseticAnalyzer`` class. Set ``mode='strict'`` if you are going to process text in standard orthography, or ``mode='nodiacritics'`` if you expect the ``ӕ`` character to be misrepresented in some words (either as an identically looking Latin character or as *ае*). After that, you can either parse tokens or lists of tokens with ``analyze_words()``, or parse a frequency list with ``analyze_wordlist()``. Here is a simple example:\r\n\r\n```python\r\nfrom uniparser_ossetic import OsseticAnalyzer\r\na = OsseticAnalyzer(mode='strict')\r\n\r\nanalyses = a.analyze_words('ӕвзаджы')\r\n# The parser is initialized before first use, so expect\r\n# some delay here (usually several seconds)\r\n\r\n# You will get a list of Wordform objects\r\n# The analysis attributes are stored in its properties\r\n# as string values, e.g.:\r\nfor ana in analyses:\r\n        print(ana.wf, ana.lemma, ana.gramm)\r\n\r\n# You can also pass lists (even nested lists) and specify\r\n# output format ('xml', 'json' or 'conll')\r\n# If you pass a list, you will get a list of analyses\r\n# with the same structure\r\nanalyses = a.analyze_words([['Фӕлӕ'], ['Æз', 'дæ', 'уарзын', '.']],\r\n\t                       format='xml')\r\nanalyses = a.analyze_words([['Фӕлӕ'], ['Æз', 'дæ', 'уарзын', '.']],\r\n\t                       format='conll')\r\nanalyses = a.analyze_words(['ӕвзаджы', [['Фӕлӕ'], ['Æз', 'дæ', 'уарзын', '.']]],\r\n\t                       format='json')\r\n```\r\n\r\nRefer to the [uniparser-morph documentation](https://uniparser-morph.readthedocs.io/en/latest/) for the full list of options.\r\n\r\n<!---\r\n### Disambiguation\r\nApart from the analyzer, this repository contains a set of [Constraint Grammar](https://visl.sdu.dk/constraint_grammar.html) rules that can be used for partial disambiguation of analyzed Ossetic texts. If you want to use them, set ``disambiguation=True`` when calling ``analyze_words``:\r\n\r\n```python\r\nanalyses = a.analyze_words(['Æз', 'дæ', 'уарзын'], disambiguate=True)\r\n```\r\n\r\nIn order for this to work, you have to install the ``cg3`` executable separately. On Ubuntu/Debian, you can use ``apt-get``:\r\n\r\n```\r\nsudo apt-get install cg3\r\n```\r\n\r\nOn Windows, download the binary and add the path to the ``PATH`` environment variable. See [the documentation](https://visl.sdu.dk/cg3/single/#installation) for other options.\r\n\r\nNote that each time you call ``analyze_words()`` with ``disambiguate=True``, the CG grammar is loaded and compiled from scratch, which makes the analysis even slower. If you are analyzing a large text, it would make sense to pass the entire text contents in a single function call rather than do it sentence-by-sentence, for optimal performance.\r\n-->\r\n\r\n### Word lists\r\nAlternatively, you can use a preprocessed word list. The ``wordlists`` directory contains a list of words from a 12-million-word [Ossetic National Corpus](http://corpus.ossetic-studies.org/search/?interface_language=en) (``wordlist.csv``) with 438,000 unique tokens, list of analyzed tokens (``wordlist_analyzed.txt``; each line contains all possible analyses for one word in an XML format), and list of tokens the parser could not analyze (``wordlist_unanalyzed.txt``). The recall of the analyzer on the corpus texts is about 90%.\r\n\r\n## Description format\r\nThe description is carried out in the ``uniparser-morph`` format and involves a description of the inflection (oss_paradigms.txt), a grammatical dictionary (oss_lexemes.txt), a list of productive lemma-changing derivations (derivations.txt), and a short list of analyses that should be avoided (bad_analyses.txt). The dictionary contains descriptions of individual lexemes, each of which is accompanied by information about its stem, its part-of-speech tag and some other grammatical information, its inflectional type (paradigm), and Russian and/or English translation. See more about the format [in the uniparser-morph documentation](https://uniparser-morph.readthedocs.io/en/latest/format.html).\r\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/timarkh/uniparser-grammar-ossetic",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "uniparser-ossetic",
    "package_url": "https://pypi.org/project/uniparser-ossetic/",
    "platform": null,
    "project_url": "https://pypi.org/project/uniparser-ossetic/",
    "project_urls": {
      "Bug Tracker": "https://github.com/timarkh/uniparser-grammar-ossetic/issues",
      "Homepage": "https://github.com/timarkh/uniparser-grammar-ossetic"
    },
    "release_url": "https://pypi.org/project/uniparser-ossetic/2.0.0/",
    "requires_dist": [
      "uniparser-morph (>=2.7.2)",
      "importlib-resources"
    ],
    "requires_python": ">=3.7",
    "summary": "Rule-based morphological analysis for Ossetic (Iron)",
    "version": "2.0.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16730494,
  "releases": {
    "2.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6c9ce2b8d829b7523fb2861303a96cc45c485c29ec9e43a33b2e2e12d10be64a",
          "md5": "1748e9a2d6c82e5063124ca34f061806",
          "sha256": "b814f5f9ec100764d805c8b20dfb77b8fb723c3463ac3cdc2abb0c9bb026012d"
        },
        "downloads": -1,
        "filename": "uniparser_ossetic-2.0.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "1748e9a2d6c82e5063124ca34f061806",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.7",
        "size": 1359074,
        "upload_time": "2023-02-07T16:43:31",
        "upload_time_iso_8601": "2023-02-07T16:43:31.510689Z",
        "url": "https://files.pythonhosted.org/packages/6c/9c/e2b8d829b7523fb2861303a96cc45c485c29ec9e43a33b2e2e12d10be64a/uniparser_ossetic-2.0.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "d0d1153ef4080e57ebf8f66eaf5992beb481fba1996eb389e2d26cba971d00f9",
          "md5": "ec7c6c81b2eed22057b2c929d3d20190",
          "sha256": "ec19fdd2421f2215c0458ae99eeab38fd77bdac639e13a3ea5573fceaed2aeb6"
        },
        "downloads": -1,
        "filename": "uniparser-ossetic-2.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "ec7c6c81b2eed22057b2c929d3d20190",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.7",
        "size": 1339273,
        "upload_time": "2023-02-07T16:43:33",
        "upload_time_iso_8601": "2023-02-07T16:43:33.006668Z",
        "url": "https://files.pythonhosted.org/packages/d0/d1/153ef4080e57ebf8f66eaf5992beb481fba1996eb389e2d26cba971d00f9/uniparser-ossetic-2.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "6c9ce2b8d829b7523fb2861303a96cc45c485c29ec9e43a33b2e2e12d10be64a",
        "md5": "1748e9a2d6c82e5063124ca34f061806",
        "sha256": "b814f5f9ec100764d805c8b20dfb77b8fb723c3463ac3cdc2abb0c9bb026012d"
      },
      "downloads": -1,
      "filename": "uniparser_ossetic-2.0.0-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "1748e9a2d6c82e5063124ca34f061806",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.7",
      "size": 1359074,
      "upload_time": "2023-02-07T16:43:31",
      "upload_time_iso_8601": "2023-02-07T16:43:31.510689Z",
      "url": "https://files.pythonhosted.org/packages/6c/9c/e2b8d829b7523fb2861303a96cc45c485c29ec9e43a33b2e2e12d10be64a/uniparser_ossetic-2.0.0-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "d0d1153ef4080e57ebf8f66eaf5992beb481fba1996eb389e2d26cba971d00f9",
        "md5": "ec7c6c81b2eed22057b2c929d3d20190",
        "sha256": "ec19fdd2421f2215c0458ae99eeab38fd77bdac639e13a3ea5573fceaed2aeb6"
      },
      "downloads": -1,
      "filename": "uniparser-ossetic-2.0.0.tar.gz",
      "has_sig": false,
      "md5_digest": "ec7c6c81b2eed22057b2c929d3d20190",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.7",
      "size": 1339273,
      "upload_time": "2023-02-07T16:43:33",
      "upload_time_iso_8601": "2023-02-07T16:43:33.006668Z",
      "url": "https://files.pythonhosted.org/packages/d0/d1/153ef4080e57ebf8f66eaf5992beb481fba1996eb389e2d26cba971d00f9/uniparser-ossetic-2.0.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}