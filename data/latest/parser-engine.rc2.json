{
  "info": {
    "author": "huangzhen",
    "author_email": "huangzhen@baixing.com",
    "bugtrack_url": null,
    "classifiers": [
      "Environment :: Web Environment",
      "Intended Audience :: Developers",
      "License :: OSI Approved :: MIT License",
      "Operating System :: MacOS",
      "Operating System :: Microsoft",
      "Operating System :: POSIX",
      "Operating System :: Unix",
      "Programming Language :: Python :: 2.6",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3.3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Topic :: Internet",
      "Topic :: Multimedia :: Video",
      "Topic :: Software Development :: Libraries :: Python Modules"
    ],
    "description": "## Parser Engine \n代号PE，为[scrapy](https://scrapy.org/)框架量身定制的 **\"可配置化的响应解析器引擎\"**。\n\n主要支持以下特性：\n- [x] 基于xpath、jsonpath等规则解析html和json格式的http请求响应体\n- [x] 输出值基于`scrapy.Item`，自动定位并返回实例化的`Item`\n- [x] 支持 **父节点**、**列表for循环解析**\n- [x] 字段枚举值的映射\n- [x] 设置字段为必有字段，缺失时丢弃整个`Item`\n- [x] 将list类型的字段拼接成字符串\n\n### 安装\n- 安装最新体验版: \n    >`pip install git+https://github.com/Danceiny/parser_engine`\n    \n- 安装稳定版：\n    >`pip install -U parser_engine`\n\n### 更新日志\n请移步[CHANGELOG.md](CHANGELOG.md)。\n\n### 速览\n>如何使用PE从零开始快速编写一个网站的爬虫，并持久化数据？可移步[快速开始](./TUTORIAL.md)。\n\n- 极简版，使用`CrawlSpider`的rules机制。\n```python\nfrom parser_engine import TemplateAnnotation\nfrom scrapy.spiders.crawl import CrawlSpider\n@TemplateAnnotation(tpls=\"demo\")\nclass DemoSpider4(CrawlSpider):\n    name = \"demo4\"\n    start_urls = [\n        \"http://github.cannot.cc/baixing-helper\"\n    ]\n```\n\n- 使用scrapy_redis，解析start_urls的响应。\n```python\nfrom parser_engine import TemplateAnnotation\nfrom parser_engine.clue.spider import ClueSpider\n@TemplateAnnotation(start_url_tpl=({\n    \"name\": \"zhongguozhongqi_xiaoshouwangluo\",\n    \"itemname\": \"HuocheDealerItem\",\n    \"parent\": {\n        \"xpath\": \"//tr[@class=\\\"bgcolor2\\\"]\"\n    },\n    \"fields\": [\n        {\n            \"key\": \"area\",\n            \"xpath\": \"td[1]/text()\",\n            \"value_type\": \"stripped_string\"\n        }, {\n            \"key\": \"leads_name\",\n            \"xpath\": \"td[2]/text()\",\n            \"value_type\": \"stripped_string\"\n        }, {\n            \"key\": \"address\",\n            \"xpath\": \"td[3]/text()\",\n            \"value_type\": \"stripped_string\"\n        }, {\n            \"key\": \"phone\",\n            \"xpath\": \"td[5]/text()\",\n            \"value_type\": \"stripped_string\"\n        }\n    ]\n}), channel='zhongguozhongqi', leads_src='中国重汽')\nclass ZhongguozhongqiSpider(ClueSpider):\n    name = 'zhongguozhongqi'\n    def parse(self, response):\n        items = self._parse_start_url(response)\n        for item in items:\n            phone = item.get('phone')\n            if phone:\n                item['phone'] = phone.replace('、', ',')\n            yield item\n        self.finish_clue(response, len(items))\n```\n\n- 使用scrapy_redis，灵活运用多种PE特性。\n```python\nfrom parser_engine.clue.spider import ClueSpider\nfrom parser_engine import TemplateAnnotation\nfrom parser_engine.clue.items import ClueItem\nfrom parser_engine.request import TaskRequest\nfrom scrapy import Request\n@TemplateAnnotation(start_url_tpl=({\n                                       \"name\": \"youka_shop_listing_api\",\n                                       \"parent\": {\n                                           \"json_key\": \"data\",\n                                       },\n                                       \"fields\": [{\n                                           \"key\": \"totalPage\",\n                                           \"json_key\": \"totalPage\",\n\n                                       }, {\n                                           \"key\": \"ids\",\n                                           \"json_path\": \"dataList[*].id\"\n                                       }]\n                                   },),\n    tpls=({\n        \"name\": \"youka_shop_detail_api\",\n        \"itemname\": \"HuocheDealerItem\",\n        \"parent\": {\n            \"json_key\": \"data\",\n        },\n        \"fields\": [{\n            \"key\": \"company_type\",\n            \"json_key\": \"category\",\n            \"mapper\": {\n                1: \"二手车直营店\",\n                2: \"4S店\"\n            }\n        }, {\n            \"key\": \"dealer_id\",\n            \"json_key\": \"id\",\n            \"required\": 1,\n        }, {\n            \"key\": \"leads_name\",\n            \"json_key\": \"shopName\",\n        }, {\n            \"key\": \"area\",\n            \"json_path\": \"districtDto.districtName\",\n            \"value_type\": \"singleton\"\n        }, {\n            \"key\": \"city\",\n            \"json_path\": \"cityDto.cityName\",\n            \"value_type\": \"singleton\"\n        }, {\n            \"key\": \"service_phone\",\n            \"default_value\": \"\",\n        }, {\n            \"key\": \"wechat\",\n            \"json_key\": \"wechat\",\n        },  {\n            \"key\": \"tags\",\n            \"json_key\": \"tags\",\n            \"join\": \",\"\n        }]\n    }), channel='youka', leads_src='优卡')\nclass YoukaSpider(ClueSpider):\n    name = 'youka'\n    custom_settings = {\n        'CONCURRENT_REQUESTS': 2,\n        'CONCURRENT_REQUESTS_PER_DOMAIN': 1\n    }\n    def parse(self, response):\n        items = self._parse_start_url(response)\n        meta = response.meta\n        clue_id = meta.get('clue_id')\n        from_url = response.request.url\n        if meta.get('open_pages'):\n            total_page = items[0]['totalPage']\n            import re\n            current_page = int(re.findall('page=(\\\\d+)', from_url)[0])\n            for i in range(1, total_page + 1):\n                if current_page == i:\n                    continue\n                url = \"http://www.china2cv.com/truck-foton-web/api/shop/v1/getShopList?page=%d&pageSize=10\" % i\n                yield ClueItem({\"project\": \"huoche\", \"spider\": self.name, \"req\": TaskRequest(\n                    url=url,\n                    meta={\"from_clue_id\": clue_id}\n                )})\n        for item in items:\n            for id in item['ids']:\n                r = Request(url=\"http://www.china2cv.com/truck-foton-web/api/shop/v1/getShopInfo?shopId=%d\" % int(id),\n                            callback=self._response_downloaded)\n                r.meta.update(rule=0, from_clue_id=clue_id)\n                yield r\n\n    def process_results(self, response, results):\n        for item in results:\n            item['url'] = 'http://www.china2cv.com/storeDetail.html?typess=1&shopId=' + str(item['dealer_id'])\n        return results\n```\n\n完整示例请参考：[examples](./examples)。\n\n### 原理\n- 解析器\n    >PE向调用方提供一套简单、易懂的参数，实际会将其`编译`成较为复杂的xpath表达式，再借助scrapy封装的解析器将所需内容提取出来。\n\n- 返回值\n    >通过配置`itemname`参数，PE将`反射`得到所需的`Item`类，按照配置的映射关系，从提取出的值创建一个`Item`实例（或者多个），并返回一个可迭代的`Item`实例列表。\n    \n### 已知问题\n\n- 如果提取规则较为复杂，建议直接使用xpath和css参数，因为PE的参数编译可能存在问题。\n- 如果对性能有比较强的需求，不建议使用。\n\n### 特性介绍\n- [x] 支持表格、列表等形式的批量解析\n    >通过在模板中定义一个父节点，从html页面中的表格、列表等组件中，批量抓取多个同类item\n    \n    >用法示例：使用`{\"parent\": {}, \"fields\": []}`这样的配置，将首先查找匹配`parent`的节点，然后遍历其每个子节点，对每个子节点应用`fields`规则，生成一个item。\n    \n- [x] Clue Mechanism\n    > 基于scrapy_redis的线索机制，可持久化（目前支持mysql）线索，方便追踪。\n\n- [x] 值映射\n    >一个简单的需求场景：API返回的性别字段是0和1，但是需要将其转换成\"男\"和\"女\"。\n    \n### 待做清单\n- 优化\n    - [ ] 支持直接在`Item`的类定义中定义模板\n        >用法示例：原模板的`itemname`参数通过注解传参，其他的模板参数定义在`Item`类中，如下所示。\n        ```\n        class MyItem(scrapy.Item):\n            tpl = {\"parent\": {\"xpath\":\"//div[@id=\\\"contentDiv\\\"]//table/tbody/tr[position()>1]\"}\n            name = Field(xpath=\"//a[@href]/text()\")\n        ```\n\n### scrapy配置参数\n\n- PARSER_ENGINE_CONFIG_FILE \n    > 模板配置文件的位置。默认是`parser_engine.json`，与`scrapy.cfg`文件同级。\n- MYSQL\n    > MySQL配置信息，dict类型，包含以下字段：\n        - DATABASE\n        - USER\n        - PASSWORD\n        - PORT 默认3306\n        - HOST 默认127.0.0.1\n\n下面的字段在MYSQL配置缺失时生效：\n- MYSQL_USER\n- MYSQL_PASSWORD\n- MYSQL_PORT\n- MYSQL_DATABASE\n\n### 模板参数\n模板配置文件，如`parser_engine.json`，其构成结构如下：\n```json\n{\n  \"templates\": [\n    {\n      \"name\": \"tpl1\"   \n    },\n    {\n      \"name\": \"tpl2\"\n    }\n  ]\n\n}\n\n```\n模板配置文件中`templates`列表中的每一项，即为一个模板。\n\n所谓模板，对应的类是[PETemplate](./parser_engine/template.py)，其构成结构如下：\n\n- name 必要。相当于该模板在该模板文件中的id\n- parent 不必要。如果指定，将按照其指定的规则，从响应中取出某节点作为后续提取规则的根节点。\n- itemname 不必要。如果指定，将尝试加载实例化该item类；如果没有找到类或者没有指定，则返回`dict`类型的原始数据。\n- extract_keys 不必要。json格式专属，用于直接从某个json对象中提取一组键值，通常可以搭配`parent`参数使用，较为高效。\n- extract_keys_map 不必要。类似`extract_keys_map`，适用于需要转换原json对象中的键名的场景。\n- fields 不必要。是一个`字段`的数组。\n\n所谓字段，对应的类是[PEField](./parser_engine/template.py)，属于PE的核心，其构成结构如下：\n- key 必要。提取结果保存的键名，通常是item的某个`Field`的变量名。\n- xpath\n- css\n- tags 不必要。一组有序的html标签。如`[\"div\",\"a\"]`会被翻译成`//div/a`的xpath。\n- attr_name 不必要。对于html来说，经常需要获取某个tag的某个属性值。\n- attributes 不必要。支持多种结构。\n    - string 直接作为`//div/a[{attributes}]`中的`{attributes}`\n    - map `{\"type\": \"password\", \"id\": \"id1\"}` => `//div/a[@type=\\\"password\\\" and @id=\\\"id1\\\"]`\n    - list `[[\"type\",\"=\",\"password\"],[\"id\", \"!=\",\"id1\"]]` => `//div/a[@type=\\\"password\\\" and @id!=\\\"id1\\\"]`\n- json_path 不必要。`json_path`完全遵循[json_path协议](https://goessner.net/articles/JsonPath/)，[json_path在线调试](http://jsonpath.com/)。\n- json_key 不必要。直接作为json取值的键名。\n- value_type 不必要。**但很有用**，主要是因为不管是xpath还是json_path，提取出来都是一个list，尽管有时字段明明是完全确定的，如果设置value_type=`singleton`，则PE将提取list的第一个元素。\n- position 不必要。\n- mapper 不必要。dict类型，用于实现值映射，例如：`{1: \"male\", 2: \"femail\"}`。\n- join 不必要。str类型，用于实现`','.join([])`。\n\n### TemplateAnnotation参数说明\nTemplateAnnotation注解中传进来的参数，除了下面列出的，其他的参数都会被塞到返回值中（当然，如果通过定义`itemname`，实例化item的时候会静默抛弃那些不属于item的值）。\n\n- start_url_tpl: 模板的数组，或者模板id的数组。\n    >对应于start_urls的模板，会生成一个`_parse_start_url`方法绑定到spider类上，该方法有两个参数（不包括self）：\n    - response \n    - tpl_index_or_id，默认是None\n    \n- tpls: 模板的数组，或者模板id的数组\n\n具体请参考[decorator.py](./parser_engine/decorator.py)中的注释及源代码。\n\n#### Html格式\n举个简单的例子。\n\n目标：抓取[抖音用户关键字搜索抓包数据分析脚本使用指南](http://github.cannot.cc/baixing-helper/抖音用户关键字搜索抓包数据分析脚本使用指南.html)页面的几个步骤标题，每个步骤是一个`h3`标签，步骤标题在`id`属性里，并且需要去掉形如`1-`的前缀。\n那么相应的配置文件是：\n```json\n    {\n      \"name\": \"demo\",\n      \"fields\": [\n        {\n          \"dom_id\": null,\n          \"_css\": null,\n          \"xpath\": null,\n          \"tags\": [\n            \"h3\"\n          ],\n          \"classes\": [],\n          \"attributes\": null,\n          \"position\": null,\n          \"key\": \"步骤\",\n          \"value_type\": null,\n          \"regexp\": \"[\\\\d]{1,2}-(\\\\w+)\",\n          \"attr_name\": \"id\"\n        }\n      ]\n    }\n```\n输出\n```\n{'步骤': ['准备工作', '找到电脑的ip地址和端口', '确保手机与电脑建立连接', '抖音搜索关键词', '抓包数据导出', '提取用户信息', '推荐在线转换工具', 'python脚本导出']}\n```\n\n如果只需要第二个步骤，将json配置中的`position`参数改为`2`，即可得到如下输出：\n```\n{'步骤': ['找到电脑的ip地址和端口']}\n```\n\n#### JSON格式\n```json\n    {\n      \"name\": \"json-api-demo\",\n      \"fields\": [\n        {\n          \"key\": \"poi_id\",\n          \"json_path\": \"$.pois[:1].id\"\n        },\n        {\n          \"key\": \"地名\",\n          \"json_path\": \"$.data.name\",\n          \"value_type\": \"singleton\"\n        },\n        {\n          \"key\": \"下级\",\n          \"json_path\": \"$.data.children[*].name\"\n        }\n      ]\n    }\n```\n\n\n由于`json_path`解析总是返回一个list，对于一些确定的字段，比如通过调用API`http://172.31.1.4:30815/api/dict/area/0?childrenDepth=1`，想拿到该地区的name字段，则可以设置`value_type`为`singleton`，则PE会做一次转换。",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://gitlab.baixing.cn/spider/parser_engine",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "parser-engine",
    "package_url": "https://pypi.org/project/parser-engine/",
    "platform": "",
    "project_url": "https://pypi.org/project/parser-engine/",
    "project_urls": {
      "Homepage": "https://gitlab.baixing.cn/spider/parser_engine"
    },
    "release_url": "https://pypi.org/project/parser-engine/0.1.4/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "template-driven parser engine for scrapy",
    "version": "0.1.4",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 5149258,
  "releases": {
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a91878e2b550329b05e6f72e10b914302109064ac71890a048654ba04483eae1",
          "md5": "a3e4d6b91ccfa80c462ff81f79e63bce",
          "sha256": "b05d431b9247d7f7bb5352615aad6ebfd8ecdef1a344d0025fef963b69a290ae"
        },
        "downloads": -1,
        "filename": "parser_engine-0.1.2-py3.6.egg",
        "has_sig": false,
        "md5_digest": "a3e4d6b91ccfa80c462ff81f79e63bce",
        "packagetype": "bdist_egg",
        "python_version": "3.6",
        "requires_python": null,
        "size": 60769,
        "upload_time": "2019-03-28T15:57:43",
        "upload_time_iso_8601": "2019-03-28T15:57:43.265387Z",
        "url": "https://files.pythonhosted.org/packages/a9/18/78e2b550329b05e6f72e10b914302109064ac71890a048654ba04483eae1/parser_engine-0.1.2-py3.6.egg",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2bab18d9295fc2c92889a98b10156919d4635bb1b8ccade4d4eef177cfd60f8b",
          "md5": "d54e17b9e35514eef9dac2fb9ef44961",
          "sha256": "97f39f3ec3c341782fc933751650e5a8aa36b63b0eff4c26b5d29dc664fd9d1e"
        },
        "downloads": -1,
        "filename": "parser_engine-0.1.4-py3.6.egg",
        "has_sig": false,
        "md5_digest": "d54e17b9e35514eef9dac2fb9ef44961",
        "packagetype": "bdist_egg",
        "python_version": "3.6",
        "requires_python": null,
        "size": 63578,
        "upload_time": "2019-04-16T10:01:25",
        "upload_time_iso_8601": "2019-04-16T10:01:25.436889Z",
        "url": "https://files.pythonhosted.org/packages/2b/ab/18d9295fc2c92889a98b10156919d4635bb1b8ccade4d4eef177cfd60f8b/parser_engine-0.1.4-py3.6.egg",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "2bab18d9295fc2c92889a98b10156919d4635bb1b8ccade4d4eef177cfd60f8b",
        "md5": "d54e17b9e35514eef9dac2fb9ef44961",
        "sha256": "97f39f3ec3c341782fc933751650e5a8aa36b63b0eff4c26b5d29dc664fd9d1e"
      },
      "downloads": -1,
      "filename": "parser_engine-0.1.4-py3.6.egg",
      "has_sig": false,
      "md5_digest": "d54e17b9e35514eef9dac2fb9ef44961",
      "packagetype": "bdist_egg",
      "python_version": "3.6",
      "requires_python": null,
      "size": 63578,
      "upload_time": "2019-04-16T10:01:25",
      "upload_time_iso_8601": "2019-04-16T10:01:25.436889Z",
      "url": "https://files.pythonhosted.org/packages/2b/ab/18d9295fc2c92889a98b10156919d4635bb1b8ccade4d4eef177cfd60f8b/parser_engine-0.1.4-py3.6.egg",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}