{
  "info": {
    "author": "Caglar Demir",
    "author_email": "caglardemir8@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3.9"
    ],
    "description": "# DICE Embeddings: Hardware-agnostic Framework for Large-scale Knowledge Graph Embeddings\n\nKnowledge graph embedding research has mainly focused on learning continuous representations of knowledge graphs towards the link prediction problem. \nRecently developed frameworks can be effectively applied in a wide range of research-related applications.\nYet, using these frameworks in real-world applications becomes more challenging as the size of the knowledge graph grows.\n\nWe developed the DICE Embeddings framework (dicee) to compute embeddings for large-scale knowledge graphs in a hardware-agnostic manner.\nTo achieve this goal, we rely on\n1. **[Pandas](https://pandas.pydata.org/) & Co.** to use parallelism at preprocessing a large knowledge graph,\n2. **[PyTorch](https://pytorch.org/) & Co.** to learn knowledge graph embeddings via multi-CPUs, GPUs, TPUs or computing cluster, and\n3. **[Huggingface](https://huggingface.co/)** to ease the deployment of pre-trained models.\n\n**Why [Pandas](https://pandas.pydata.org/) & Co. ?**\nA large knowledge graph can be read and preprocessed (e.g. removing literals) by pandas, modin, or polars in parallel.\nThrough polars, a knowledge graph having more than 1 billion triples can be read in parallel fashion. \nImportantly, using these frameworks allow us to perform all necessary computations on a single CPU as well as a cluster of computers.\n\n**Why [PyTorch](https://pytorch.org/) & Co. ?**\nPyTorch is one of the most popular machine learning frameworks available at the time of writing. \nPytorchLightning facilitates scaling the training procedure of PyTorch without boilerplate.\nIn our framework, we combine [PyTorch](https://pytorch.org/) & [PytorchLightning](https://www.pytorchlightning.ai/).\nUsers can choose the trainer class (e.g., DDP by Pytorch) to train large knowledge graph embedding models with billions of parameters.\nPytorchLightning allows us to use state-of-the-art model parallelism techniques (e.g. Fully Sharded Training, FairScale, or DeepSpeed)\nwithout extra effort.\nWith our framework, practitioners can directly use PytorchLightning for model parallelism to train gigantic embedding models.\n\n**Why [Hugging-face Gradio](https://huggingface.co/gradio)?**\nDeploy a pre-trained embedding model without writing a single line of code.\n\n\n## Installation\n```\npip install dicee\n```\nor\n```\ngit clone https://github.com/dice-group/dice-embeddings.git\nconda create -n dice python=3.9.12 --no-default-packages\nconda activate dice\npip3 install pandas==1.5.1 \npip3 install polars==0.15.13 \npip3 install modin[ray]==0.16.2 \npip3 install pyarrow==8.0.0\npip3 install torch==1.13.0 \npip3 install pytorch-lightning==1.6.4\npip3 install scikit-learn==1.1.1\npip3 install pytest==6.2.5\npip3 install gradio==3.0.17\npip3 install matplotlib==3.6.2\n```\nTo test the Installation\n```\nwget https://hobbitdata.informatik.uni-leipzig.de/KG/KGs.zip\nunzip KGs.zip\npytest -p no:warnings -x # it takes circa 15 minutes\npytest -p no:warnings --lf # run only the last failed test\npytest -p no:warnings --ff # to run the failures first and then the rest of the tests.\n```\nTo see the software architecture, execute the following command\n```\npyreverse dicee/ && dot -Tpng -x classes.dot -o dice_software.png && eog dice_software.png\n# or\npyreverse dicee/trainer && dot -Tpng -x classes.dot -o trainer.png && eog trainer.png\n```\n## Applications\n### Description Logic Concept Learning (soon)\n```python\nfrom dicee import KGE\n# (1) Load a pretrained KGE model on KGs/Family\npretrained_model = KGE(path='Experiments/2022-12-08 11:46:33.654677')\npretrained_model.learn_concepts(pos={''},neg={''},topk=1)\n```\n### Conjunctive Query/Question Answering\n```python\nfrom dicee import KGE\n# (1) Load a pretrained KGE model on KGs/Family\npretrained_model = KGE(path='Experiments/2022-12-08 11:46:33.654677')\n# (2) Answer the following conjunctive query question: To whom a sibling of F9M167 is married to?\n# (3) Decompose (2) into two query\n# (3.1) Who is a sibling of F9M167? => {F9F141,F9M157}\n# (3.2) To whom a results of (3.1) is married to ? {F9M142, F9F158}\npretrained_model.predict_conjunctive_query(entity='<http://www.benchmark.org/family#F9M167>',\n                                          relations=['<http://www.benchmark.org/family#hasSibling>',\n                                                     '<http://www.benchmark.org/family#married>'], topk=1)\n```\n### Triple Classification\n#### Using pre-trained ConEx on DBpedia 03-2022\n```bash\n# To download a pretrained ConEx\nmkdir ConEx && cd ConEx && wget -r -nd -np https://hobbitdata.informatik.uni-leipzig.de/KGE/DBpedia/ConEx/ && cd ..\n```\n```python\nfrom dicee import KGE\n# (1) Load a pretrained ConEx on DBpedia \npre_trained_kge = KGE(path='ConEx')\n\npre_trained_kge.triple_score(head_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"],relation=[\"http://dbpedia.org/ontology/birthPlace\"],tail_entity=[\"http://dbpedia.org/resource/Ulm\"]) # tensor([0.9309])\npre_trained_kge.triple_score(head_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"],relation=[\"http://dbpedia.org/ontology/birthPlace\"],tail_entity=[\"http://dbpedia.org/resource/German_Empire\"]) # tensor([0.9981])\npre_trained_kge.triple_score(head_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"],relation=[\"http://dbpedia.org/ontology/birthPlace\"],tail_entity=[\"http://dbpedia.org/resource/Kingdom_of_WÃ¼rttemberg\"]) # tensor([0.9994])\npre_trained_kge.triple_score(head_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"],relation=[\"http://dbpedia.org/ontology/birthPlace\"],tail_entity=[\"http://dbpedia.org/resource/Germany\"]) # tensor([0.9498])\npre_trained_kge.triple_score(head_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"],relation=[\"http://dbpedia.org/ontology/birthPlace\"],tail_entity=[\"http://dbpedia.org/resource/France\"]) # very low\npre_trained_kge.triple_score(head_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"],relation=[\"http://dbpedia.org/ontology/birthPlace\"],tail_entity=[\"http://dbpedia.org/resource/Italy\"]) # very low\n```\n### Relation Prediction\n```python\nfrom dicee import KGE\npre_trained_kge = KGE(path='ConEx')\npre_trained_kge.predict_topk(head_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"],tail_entity=[\"http://dbpedia.org/resource/Ulm\"])\n```\n### Entity Prediction\n```python\nfrom dicee import KGE\npre_trained_kge = KGE(path='ConEx')\npre_trained_kge.predict_topk(head_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"],relation=[\"http://dbpedia.org/ontology/birthPlace\"]) \npre_trained_kge.predict_topk(relation=[\"http://dbpedia.org/ontology/birthPlace\"],tail_entity=[\"http://dbpedia.org/resource/Albert_Einstein\"]) \n```\n### Finding Missing Triples\n```python\nfrom dicee import KGE\npre_trained_kge = KGE(path='ConEx')\nmissing_triples = pre_trained_kge.find_missing_triples(confidence=0.95, entities=[''], relations=[''])\n```\n\n## How to Train a KGE model \n> How to use the framework:`examples`.\n\n## How to Deploy\nAny pretrained model can be deployed with an ease. Moreover, anyone on the internet can use the pretrained model with ```--share``` parameter.\n```\npython deploy.py --path_of_experiment_folder 'ConEx' --share True\nLoading Model...\nModel is loaded!\nRunning on local URL:  http://127.0.0.1:7860/\nRunning on public URL: https://54886.gradio.app\n\nThis share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n```\n![alt text](dicee/figures/deploy_qmult_family.png)\n## Pre-trained Models\nPlease contact:  ```caglar.demir@upb.de ``` or ```caglardemir8@gmail.com ``` , if you lack hardware resources to obtain embeddings of a specific knowledge Graph.\n- [DBpedia version: 06-2022 Embeddings](https://hobbitdata.informatik.uni-leipzig.de/KGE/DBpediaQMultEmbeddings_03_07):\n  - Models: ConEx, QMult\n- [YAGO3-10 ConEx embeddings](https://hobbitdata.informatik.uni-leipzig.de/KGE/conex/YAGO3-10.zip)\n- [FB15K-237 ConEx embeddings](https://hobbitdata.informatik.uni-leipzig.de/KGE/conex/FB15K-237.zip)\n- [WN18RR ConEx embeddings](https://hobbitdata.informatik.uni-leipzig.de/KGE/conex/WN18RR.zip)\n- For more please look at [Hobbit Data](https://hobbitdata.informatik.uni-leipzig.de/KGE/)\n### Documentation\nIn documents folder, we explained many details about knowledge graphs, knowledge graph embeddings, training strategies and many more background knowledge.\nWe continuously work on documenting each and every step to increase the readability of our code.\n## How to cite\nCurrently, we are working on our manuscript describing our framework. \nIf you really like our work and want to cite it now, feel free to chose one :) \n```\n# DICE Embedding Framework\n@article{demir2022hardware,\n  title={Hardware-agnostic computation for large-scale knowledge graph embeddings},\n  author={Demir, Caglar and Ngomo, Axel-Cyrille Ngonga},\n  journal={Software Impacts},\n  year={2022},\n  publisher={Elsevier}\n}\n# KronE\n@article{demir2022kronecker,\n  title={Kronecker Decomposition for Knowledge Graph Embeddings},\n  author={Demir, Caglar and Lienen, Julian and Ngomo, Axel-Cyrille Ngonga},\n  journal={arXiv preprint arXiv:2205.06560},\n  year={2022}\n}\n# QMult, OMult, ConvQ, ConvO\n@InProceedings{pmlr-v157-demir21a,\n  title = \t {Convolutional Hypercomplex Embeddings for Link Prediction},\n  author =       {Demir, Caglar and Moussallem, Diego and Heindorf, Stefan and Ngonga Ngomo, Axel-Cyrille},\n  booktitle = \t {Proceedings of The 13th Asian Conference on Machine Learning},\n  pages = \t {656--671},\n  year = \t {2021},\n  editor = \t {Balasubramanian, Vineeth N. and Tsang, Ivor},\n  volume = \t {157},\n  series = \t {Proceedings of Machine Learning Research},\n  month = \t {17--19 Nov},\n  publisher =    {PMLR},\n  pdf = \t {https://proceedings.mlr.press/v157/demir21a/demir21a.pdf},\n  url = \t {https://proceedings.mlr.press/v157/demir21a.html},\n}\n# ConEx\n@inproceedings{demir2021convolutional,\ntitle={Convolutional Complex Knowledge Graph Embeddings},\nauthor={Caglar Demir and Axel-Cyrille Ngonga Ngomo},\nbooktitle={Eighteenth Extended Semantic Web Conference - Research Track},\nyear={2021},\nurl={https://openreview.net/forum?id=6T45-4TFqaX}}\n# Shallom\n@inproceedings{demir2021shallow,\n  title={A shallow neural model for relation prediction},\n  author={Demir, Caglar and Moussallem, Diego and Ngomo, Axel-Cyrille Ngonga},\n  booktitle={2021 IEEE 15th International Conference on Semantic Computing (ICSC)},\n  pages={179--182},\n  year={2021},\n  organization={IEEE}\n```\nFor any questions or wishes, please contact:  ```caglar.demir@upb.de``` or ```caglardemir8@gmail.com```\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/dice-group/dice-embeddings",
    "keywords": "",
    "license": "",
    "maintainer": "",
    "maintainer_email": "",
    "name": "dicee",
    "package_url": "https://pypi.org/project/dicee/",
    "platform": null,
    "project_url": "https://pypi.org/project/dicee/",
    "project_urls": {
      "Homepage": "https://github.com/dice-group/dice-embeddings"
    },
    "release_url": "https://pypi.org/project/dicee/0.0.2/",
    "requires_dist": [
      "pandas (>=1.5.1)",
      "modin[ray] (>=0.16.2)",
      "polars (>=0.15.13)",
      "pyarrow (>=8.0.0)",
      "torch (>=1.13.0)",
      "pytorch-lightning (==1.6.4)",
      "scikit-learn (>=1.1.1)",
      "pytest (>=6.2.5)",
      "gradio (>=3.0.17)",
      "matplotlib (>=3.6.2)"
    ],
    "requires_python": ">==3.9.12",
    "summary": "Dice embedding is an hardware-agnostic framework for large-scale knowledge graph embedding applications",
    "version": "0.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 17381735,
  "releases": {
    "0.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8069099cffd6340646b7377a6858839072d0a6115e70d0dbd2eb23f7eb50dead",
          "md5": "c922ff2d57049e080732526ad5343372",
          "sha256": "9ff17457da227c31ccaf620aeaab887080951d49d515dfb28f70a0e218f44b70"
        },
        "downloads": -1,
        "filename": "dicee-0.0.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "c922ff2d57049e080732526ad5343372",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">==3.9.12",
        "size": 117800,
        "upload_time": "2023-02-16T12:15:51",
        "upload_time_iso_8601": "2023-02-16T12:15:51.543799Z",
        "url": "https://files.pythonhosted.org/packages/80/69/099cffd6340646b7377a6858839072d0a6115e70d0dbd2eb23f7eb50dead/dicee-0.0.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e085bcf6662a3c3573412fd3affc68ebe4179cde6dda1927de70675d0182401e",
          "md5": "8dd63923f101d1bdb73c2e4a6187674a",
          "sha256": "e95eceb26f4aab0fc8de82efaf5f20abc230de2247d4f1357ae348e1df4bd9c0"
        },
        "downloads": -1,
        "filename": "dicee-0.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "8dd63923f101d1bdb73c2e4a6187674a",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">==3.9.12",
        "size": 298097,
        "upload_time": "2023-02-16T12:15:53",
        "upload_time_iso_8601": "2023-02-16T12:15:53.669428Z",
        "url": "https://files.pythonhosted.org/packages/e0/85/bcf6662a3c3573412fd3affc68ebe4179cde6dda1927de70675d0182401e/dicee-0.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "479d2ac708a51e8e4f5af9c784069f6ce1c58e3bfc8b0f57d0d482c1b3b7a8bb",
          "md5": "bc3646f26b717e69046242898bd6f0a3",
          "sha256": "6069cfb12670b581b032dab03dc6da3dff44c79fe2a4edbd7be84c1f0bc09d69"
        },
        "downloads": -1,
        "filename": "dicee-0.0.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bc3646f26b717e69046242898bd6f0a3",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">==3.9.12",
        "size": 110367,
        "upload_time": "2023-03-21T15:36:27",
        "upload_time_iso_8601": "2023-03-21T15:36:27.583138Z",
        "url": "https://files.pythonhosted.org/packages/47/9d/2ac708a51e8e4f5af9c784069f6ce1c58e3bfc8b0f57d0d482c1b3b7a8bb/dicee-0.0.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "529f2f35c3ffd1d97e022b8b486751c1eab7d5de9b34b43e9e21a8379b57ce1c",
          "md5": "bcdc52c1c8501aaaf71dc80326d2259c",
          "sha256": "81028e1156151e986218a4cc2f8485710f79bedd474cba6b33ca47bc8137beda"
        },
        "downloads": -1,
        "filename": "dicee-0.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "bcdc52c1c8501aaaf71dc80326d2259c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">==3.9.12",
        "size": 302951,
        "upload_time": "2023-03-21T15:36:30",
        "upload_time_iso_8601": "2023-03-21T15:36:30.329542Z",
        "url": "https://files.pythonhosted.org/packages/52/9f/2f35c3ffd1d97e022b8b486751c1eab7d5de9b34b43e9e21a8379b57ce1c/dicee-0.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "479d2ac708a51e8e4f5af9c784069f6ce1c58e3bfc8b0f57d0d482c1b3b7a8bb",
        "md5": "bc3646f26b717e69046242898bd6f0a3",
        "sha256": "6069cfb12670b581b032dab03dc6da3dff44c79fe2a4edbd7be84c1f0bc09d69"
      },
      "downloads": -1,
      "filename": "dicee-0.0.2-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "bc3646f26b717e69046242898bd6f0a3",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">==3.9.12",
      "size": 110367,
      "upload_time": "2023-03-21T15:36:27",
      "upload_time_iso_8601": "2023-03-21T15:36:27.583138Z",
      "url": "https://files.pythonhosted.org/packages/47/9d/2ac708a51e8e4f5af9c784069f6ce1c58e3bfc8b0f57d0d482c1b3b7a8bb/dicee-0.0.2-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "529f2f35c3ffd1d97e022b8b486751c1eab7d5de9b34b43e9e21a8379b57ce1c",
        "md5": "bcdc52c1c8501aaaf71dc80326d2259c",
        "sha256": "81028e1156151e986218a4cc2f8485710f79bedd474cba6b33ca47bc8137beda"
      },
      "downloads": -1,
      "filename": "dicee-0.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "bcdc52c1c8501aaaf71dc80326d2259c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">==3.9.12",
      "size": 302951,
      "upload_time": "2023-03-21T15:36:30",
      "upload_time_iso_8601": "2023-03-21T15:36:30.329542Z",
      "url": "https://files.pythonhosted.org/packages/52/9f/2f35c3ffd1d97e022b8b486751c1eab7d5de9b34b43e9e21a8379b57ce1c/dicee-0.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}