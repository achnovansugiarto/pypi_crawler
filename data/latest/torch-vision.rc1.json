{
  "info": {
    "author": "PyTorch Core Team",
    "author_email": "soumith@pytorch.org",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "torch-vision\n============\n\nThis repository consists of:\n\n-  `vision.datasets <#datasets>`__ : Data loaders for popular vision\n   datasets\n-  `vision.models <#models>`__ : Definitions for popular model\n   architectures, such as AlexNet, VGG, and ResNet and pre-trained\n   models.\n-  `vision.transforms <#transforms>`__ : Common image transformations\n   such as random crop, rotations etc.\n-  `vision.utils <#utils>`__ : Useful stuff such as saving tensor (3 x H\n   x W) as image to disk, given a mini-batch creating a grid of images,\n   etc.\n\nInstallation\n============\n\nBinaries:\n\n.. code:: bash\n\n    conda install torchvision -c https://conda.anaconda.org/t/6N-MsQ4WZ7jo/soumith\n\nFrom Source:\n\n.. code:: bash\n\n    pip install -r requirements.txt\n    pip install .\n\nDatasets\n========\n\nThe following dataset loaders are available:\n\n-  `COCO (Captioning and Detection) <#coco>`__\n-  `LSUN Classification <#lsun>`__\n-  `ImageFolder <#imagefolder>`__\n-  `Imagenet-12 <#imagenet-12>`__\n-  `CIFAR10 and CIFAR100 <#cifar>`__\n\nDatasets have the API: - ``__getitem__`` - ``__len__`` They all subclass\nfrom ``torch.utils.data.Dataset`` Hence, they can all be multi-threaded\n(python multiprocessing) using standard torch.utils.data.DataLoader.\n\nFor example:\n\n``torch.utils.data.DataLoader(coco_cap, batch_size=args.batchSize, shuffle=True, num_workers=args.nThreads)``\n\nIn the constructor, each dataset has a slightly different API as needed,\nbut they all take the keyword args:\n\n-  ``transform`` - a function that takes in an image and returns a\n   transformed version\n-  common stuff like ``ToTensor``, ``RandomCrop``, etc. These can be\n   composed together with ``transforms.Compose`` (see transforms section\n   below)\n-  ``target_transform`` - a function that takes in the target and\n   transforms it. For example, take in the caption string and return a\n   tensor of word indices.\n\nCOCO\n~~~~\n\nThis requires the `COCO API to be\ninstalled <https://github.com/pdollar/coco/tree/master/PythonAPI>`__\n\nCaptions:\n^^^^^^^^^\n\n``dset.CocoCaptions(root=\"dir where images are\", annFile=\"json annotation file\", [transform, target_transform])``\n\nExample:\n\n.. code:: python\n\n    import torchvision.datasets as dset\n    import torchvision.transforms as transforms\n    cap = dset.CocoCaptions(root = 'dir where images are',\n                            annFile = 'json annotation file',\n                            transform=transforms.ToTensor())\n\n    print('Number of samples: ', len(cap))\n    img, target = cap[3] # load 4th sample\n\n    print(\"Image Size: \", img.size())\n    print(target)\n\nOutput:\n\n::\n\n    Number of samples: 82783\n    Image Size: (3L, 427L, 640L)\n    [u'A plane emitting smoke stream flying over a mountain.',\n    u'A plane darts across a bright blue sky behind a mountain covered in snow',\n    u'A plane leaves a contrail above the snowy mountain top.',\n    u'A mountain that has a plane flying overheard in the distance.',\n    u'A mountain view with a plume of smoke in the background']\n\nDetection:\n^^^^^^^^^^\n\n``dset.CocoDetection(root=\"dir where images are\", annFile=\"json annotation file\", [transform, target_transform])``\n\nLSUN\n~~~~\n\n``dset.LSUN(db_path, classes='train', [transform, target_transform])``\n\n-  db\\_path = root directory for the database files\n-  classes =\n-  'train' - all categories, training set\n-  'val' - all categories, validation set\n-  'test' - all categories, test set\n-  ['bedroom\\_train', 'church\\_train', ...] : a list of categories to\n   load\n\nCIFAR\n~~~~~\n\n``dset.CIFAR10(root, train=True, transform=None, target_transform=None, download=False)``\n\n``dset.CIFAR100(root, train=True, transform=None, target_transform=None, download=False)``\n\n-  ``root`` : root directory of dataset where there is folder\n   ``cifar-10-batches-py``\n-  ``train`` : ``True`` = Training set, ``False`` = Test set\n-  ``download`` : ``True`` = downloads the dataset from the internet and\n   puts it in root directory. If dataset already downloaded, does not do\n   anything.\n\nImageFolder\n~~~~~~~~~~~\n\nA generic data loader where the images are arranged in this way:\n\n::\n\n    root/dog/xxx.png\n    root/dog/xxy.png\n    root/dog/xxz.png\n\n    root/cat/123.png\n    root/cat/nsdf3.png\n    root/cat/asd932_.png\n\n``dset.ImageFolder(root=\"root folder path\", [transform, target_transform])``\n\nIt has the members:\n\n-  ``self.classes`` - The class names as a list\n-  ``self.class_to_idx`` - Corresponding class indices\n-  ``self.imgs`` - The list of (image path, class-index) tuples\n\nImagenet-12\n~~~~~~~~~~~\n\nThis is simply implemented with an ImageFolder dataset.\n\nThe data is preprocessed `as described\nhere <https://github.com/facebook/fb.resnet.torch/blob/master/INSTALL.md#download-the-imagenet-dataset>`__\n\n`Here is an\nexample <https://github.com/pytorch/examples/blob/27e2a46c1d1505324032b1d94fc6ce24d5b67e97/imagenet/main.py#L48-L62>`__.\n\nModels\n======\n\nThe models subpackage contains definitions for the following model\narchitectures:\n\n-  `AlexNet <https://arxiv.org/abs/1404.5997>`__: AlexNet variant from\n   the \"One weird trick\" paper.\n-  `VGG <https://arxiv.org/abs/1409.1556>`__: VGG-11, VGG-13, VGG-16,\n   VGG-19 (with and without batch normalization)\n-  `ResNet <https://arxiv.org/abs/1512.03385>`__: ResNet-18, ResNet-34,\n   ResNet-50, ResNet-101, ResNet-152\n\nYou can construct a model with random weights by calling its\nconstructor:\n\n.. code:: python\n\n    import torchvision.models as models\n    resnet18 = models.resnet18()\n    alexnet = models.alexnet()\n\nWe provide pre-trained models for the ResNet variants and AlexNet, using\nthe PyTorch `model zoo <http://pytorch.org/docs/model_zoo.html>`__.\nThese can be constructed by passing ``pretrained=True``:\n\n``python  import torchvision.models as models  resnet18 = models.resnet18(pretrained=True)  alexnet = models.alexnet(pretrained=True)``\n\nTransforms\n==========\n\nTransforms are common image transforms. They can be chained together\nusing ``transforms.Compose``\n\n``transforms.Compose``\n~~~~~~~~~~~~~~~~~~~~~~\n\nOne can compose several transforms together. For example.\n\n.. code:: python\n\n    transform = transforms.Compose([\n        transforms.RandomSizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean = [ 0.485, 0.456, 0.406 ],\n                              std = [ 0.229, 0.224, 0.225 ]),\n    ])\n\nTransforms on PIL.Image\n~~~~~~~~~~~~~~~~~~~~~~~\n\n``Scale(size, interpolation=Image.BILINEAR)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRescales the input PIL.Image to the given 'size'. 'size' will be the\nsize of the smaller edge.\n\nFor example, if height > width, then image will be rescaled to (size \\*\nheight / width, size) - size: size of the smaller edge - interpolation:\nDefault: PIL.Image.BILINEAR\n\n``CenterCrop(size)`` - center-crops the image to the given size\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCrops the given PIL.Image at the center to have a region of the given\nsize. size can be a tuple (target\\_height, target\\_width) or an integer,\nin which case the target will be of a square shape (size, size)\n\n``RandomCrop(size, padding=0)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nCrops the given PIL.Image at a random location to have a region of the\ngiven size. size can be a tuple (target\\_height, target\\_width) or an\ninteger, in which case the target will be of a square shape (size, size)\nIf ``padding`` is non-zero, then the image is first zero-padded on each\nside with ``padding`` pixels.\n\n``RandomHorizontalFlip()``\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRandomly horizontally flips the given PIL.Image with a probability of\n0.5\n\n``RandomSizedCrop(size, interpolation=Image.BILINEAR)``\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nRandom crop the given PIL.Image to a random size of (0.08 to 1.0) of the\noriginal size and and a random aspect ratio of 3/4 to 4/3 of the\noriginal aspect ratio\n\nThis is popularly used to train the Inception networks - size: size of\nthe smaller edge - interpolation: Default: PIL.Image.BILINEAR\n\n``Pad(padding, fill=0)``\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nPads the given image on each side with ``padding`` number of pixels, and\nthe padding pixels are filled with pixel value ``fill``. If a ``5x5``\nimage is padded with ``padding=1`` then it becomes ``7x7``\n\nTransforms on torch.\\*Tensor\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``Normalize(mean, std)``\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nGiven mean: (R, G, B) and std: (R, G, B), will normalize each channel of\nthe torch.\\*Tensor, i.e. channel = (channel - mean) / std\n\nConversion Transforms\n~~~~~~~~~~~~~~~~~~~~~\n\n-  ``ToTensor()`` - Converts a PIL.Image (RGB) or numpy.ndarray (H x W x\n   C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W)\n   in the range [0.0, 1.0]\n-  ``ToPILImage()`` - Converts a torch.\\*Tensor of range [0, 1] and\n   shape C x H x W or numpy ndarray of dtype=uint8, range[0, 255] and\n   shape H x W x C to a PIL.Image of range [0, 255]\n\nGeneric Transofrms\n~~~~~~~~~~~~~~~~~~\n\n``Lambda(lambda)``\n^^^^^^^^^^^^^^^^^^\n\nGiven a Python lambda, applies it to the input ``img`` and returns it.\nFor example:\n\n.. code:: python\n\n    transforms.Lambda(lambda x: x.add(10))\n\nUtils\n=====\n\nmake\\_grid(tensor, nrow=8, padding=2)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nGiven a 4D mini-batch Tensor of shape (B x C x H x W), makes a grid of\nimages\n\nsave\\_image(tensor, filename, nrow=8, padding=2)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSaves a given Tensor into an image file.\n\nIf given a mini-batch tensor, will save the tensor as a grid of images.\n",
    "description_content_type": null,
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/pytorch/vision",
    "keywords": "",
    "license": "BSD",
    "maintainer": "",
    "maintainer_email": "",
    "name": "torch-vision",
    "package_url": "https://pypi.org/project/torch-vision/",
    "platform": "",
    "project_url": "https://pypi.org/project/torch-vision/",
    "project_urls": {
      "Homepage": "https://github.com/pytorch/vision"
    },
    "release_url": "https://pypi.org/project/torch-vision/0.1.6.dev0/",
    "requires_dist": null,
    "requires_python": "",
    "summary": "image and video datasets and models for torch deep learning",
    "version": "0.1.6.dev0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 2583772,
  "releases": {
    "0.1.6.dev0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ea134942860c32f6877def97c0b432348adce870ae613ed4eb1de10cae0bb018",
          "md5": "ebcdb6e89dbad46c39514023e6c24464",
          "sha256": "e5292127b8d9e4b211fdf1a030e61fa56a2f537fe8306963618be6b61d65a80d"
        },
        "downloads": -1,
        "filename": "torch_vision-0.1.6.dev0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "ebcdb6e89dbad46c39514023e6c24464",
        "packagetype": "bdist_wheel",
        "python_version": "3.5",
        "requires_python": null,
        "size": 23322,
        "upload_time": "2017-01-19T00:25:18",
        "upload_time_iso_8601": "2017-01-19T00:25:18.694631Z",
        "url": "https://files.pythonhosted.org/packages/ea/13/4942860c32f6877def97c0b432348adce870ae613ed4eb1de10cae0bb018/torch_vision-0.1.6.dev0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e95fc3f0dcafaffd9a481b009eaafd076e7d09cf7e044e204bbd79910672e918",
          "md5": "726f4a66c3953ba3971f938cf42b20ea",
          "sha256": "c3f682bc21ef59da0543aeba8191c6a1bbe95ccf18747b636769078ab3fe86d1"
        },
        "downloads": -1,
        "filename": "torch-vision-0.1.6.dev0.tar.gz",
        "has_sig": false,
        "md5_digest": "726f4a66c3953ba3971f938cf42b20ea",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 18495,
        "upload_time": "2017-01-19T00:25:17",
        "upload_time_iso_8601": "2017-01-19T00:25:17.043872Z",
        "url": "https://files.pythonhosted.org/packages/e9/5f/c3f0dcafaffd9a481b009eaafd076e7d09cf7e044e204bbd79910672e918/torch-vision-0.1.6.dev0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "ea134942860c32f6877def97c0b432348adce870ae613ed4eb1de10cae0bb018",
        "md5": "ebcdb6e89dbad46c39514023e6c24464",
        "sha256": "e5292127b8d9e4b211fdf1a030e61fa56a2f537fe8306963618be6b61d65a80d"
      },
      "downloads": -1,
      "filename": "torch_vision-0.1.6.dev0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "ebcdb6e89dbad46c39514023e6c24464",
      "packagetype": "bdist_wheel",
      "python_version": "3.5",
      "requires_python": null,
      "size": 23322,
      "upload_time": "2017-01-19T00:25:18",
      "upload_time_iso_8601": "2017-01-19T00:25:18.694631Z",
      "url": "https://files.pythonhosted.org/packages/ea/13/4942860c32f6877def97c0b432348adce870ae613ed4eb1de10cae0bb018/torch_vision-0.1.6.dev0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e95fc3f0dcafaffd9a481b009eaafd076e7d09cf7e044e204bbd79910672e918",
        "md5": "726f4a66c3953ba3971f938cf42b20ea",
        "sha256": "c3f682bc21ef59da0543aeba8191c6a1bbe95ccf18747b636769078ab3fe86d1"
      },
      "downloads": -1,
      "filename": "torch-vision-0.1.6.dev0.tar.gz",
      "has_sig": false,
      "md5_digest": "726f4a66c3953ba3971f938cf42b20ea",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 18495,
      "upload_time": "2017-01-19T00:25:17",
      "upload_time_iso_8601": "2017-01-19T00:25:17.043872Z",
      "url": "https://files.pythonhosted.org/packages/e9/5f/c3f0dcafaffd9a481b009eaafd076e7d09cf7e044e204bbd79910672e918/torch-vision-0.1.6.dev0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}