{
  "info": {
    "author": "Robert Sachunsky",
    "author_email": "sachunsky@informatik.uni-leipzig.de",
    "bugtrack_url": null,
    "classifiers": [],
    "description": "[![CircleCI](https://circleci.com/gh/ASVLeipzig/cor-asv-ann.svg?style=svg)](https://circleci.com/gh/ASVLeipzig/cor-asv-ann)\n[![PyPI version](https://badge.fury.io/py/ocrd-cor-asv-ann.svg)](https://badge.fury.io/py/ocrd-cor-asv-ann)\n\n# cor-asv-ann\n    OCR post-correction with encoder-attention-decoder LSTMs\n\nContents:\n  * [Introduction](#introduction)\n     * [Architecture](#architecture)\n     * [Multi-OCR input](#multi-ocr-input)\n     * [Decoder feedback](#decoder-feedback)\n     * [Decoder modes](#decoder-modes)\n        * [<em>fast</em>](#fast)\n        * [<em>greedy</em>](#greedy)\n        * [<em>beamed</em>](#default)\n     * [Rejection](#rejection)\n     * [Underspecification and gap](#underspecification-and-gap)\n     * [Training](#training)\n     * [Processing PAGE annotations](#processing-page-annotations)\n     * [Evaluation](#evaluation)\n  * [Installation](#installation)\n  * [Usage](#usage)\n     * [command line interface cor-asv-ann-train](#command-line-interface-cor-asv-ann-train)\n     * [command line interface cor-asv-ann-repl](#command-line-interface-cor-asv-ann-repl)\n     * [command line interface cor-asv-ann-proc](#command-line-interface-cor-asv-ann-proc)\n     * [command line interface cor-asv-ann-eval](#command-line-interface-cor-asv-ann-eval)\n     * [command line interface cor-asv-ann-compare](#command-line-interface-cor-asv-ann-compare)\n     * [OCR-D processor interface ocrd-cor-asv-ann-process](#ocr-d-processor-interface-ocrd-cor-asv-ann-process)\n     * [OCR-D processor interface ocrd-cor-asv-ann-evaluate](#ocr-d-processor-interface-ocrd-cor-asv-ann-evaluate)\n     * [OCR-D processor interface ocrd-cor-asv-ann-align](#ocr-d-processor-interface-ocrd-cor-asv-ann-align)\n     * [OCR-D processor interface ocrd-cor-asv-ann-join](#ocr-d-processor-interface-ocrd-cor-asv-ann-join)\n     * [OCR-D processor interface ocrd-cor-asv-ann-mark](#ocr-d-processor-interface-ocrd-cor-asv-ann-mark)\n  * [Testing](#testing)\n\n\n## Introduction\n\nThis is a tool for automatic OCR _post-correction_ (reducing optical character recognition errors) with recurrent neural networks. It uses sequence-to-sequence transduction on the _character level_ with a model architecture akin to neural machine translation, i.e. a stacked **encoder-decoder** network with attention mechanism. \n\n### Architecture\n\nThe **attention model** always applies to full lines (in a _local, monotonic_ configuration), and uses a linear _additive_ alignment model. (This transfers information between the encoder and decoder hidden layer states, and calculates a _soft alignment_ between input and output characters. It is imperative for character-level processing, because with a simple final-initial transfer, models tend to start \"forgetting\" the input altogether at some point in the line and behave like unconditional LM generators. Local alignment is necessary to prevent snapping back to earlier states during long sequences.)\n\nThe **network architecture** is as follows: \n\n![network architecture](https://asvleipzig.github.io/cor-asv-ann/scheme.svg?sanitize=true \"topology for depth=1 width=3\")\n\n0. The input characters are represented as unit vectors (or as a probability distribution in case of uncertainty and ambiguity). These enter a dense projection layer to be picked up by the encoder.\n1. The bottom hidden layer of the encoder is a bi-directional LSTM. \n2. The next encoder layers are forward LSTMs stacked on top of each other. \n3. The outputs of the top layer enter the attention model as constants (both in raw form to be weighted with the decoder state recurrently, and in a pre-calculated dense projection).\n4. The hidden layers of the decoder are forward LSTMs stacked on top of each other.\n5. The top hidden layer of the decoder has double width and contains the attention model:\n   - It reads the attention constants from 3. and uses the alignment as attention state (to be input as initial and output as final state). \n   - The attention model masks a window around the center of the previous alignment plus 1 character, calculates a new alignment between encoder outputs and current decoder state, and superimposes this with the encoder outputs to yield a context vector.\n   - The context vector is concatenated to the previous layers output and enters the LSTM.\n6. The decoder outputs enter a dense projection and get normalized to a probability distribution (softmax) for each character. (The output projection weights are the transpose of the input projection weights in 0. – weight tying.)\n7. Depending on the decoder mode, the decoder output is fed back directly (greedy) or indirectly (beamed) into the decoder input. (The first position is fed with a start symbol. Decoding ends on receiving a stop symbol.)\n8. The result is the character sequences corresponding to the argmax probabilities of the decoder outputs.\n\nHL depth and width, as well as many other topology and training options can be configured:\n- residual connections between layers in encoder and decoder?\n- deep bidirectional encoder (with fw/bw cross-summarization)?\n- LM loss/prediction as secondary output (multi-task learning, dual scoring)?\n\n(cf. [training options](#Training))\n\n### Multi-OCR input\n\nnot yet!\n\n### Decoder feedback\n\nOne important empirical finding is that the softmax output (full probability distribution) of the decoder can carry important information for the next state when input directly. This greatly improves the accuracy of both alignments and predictions. (This is in part attributable to exposure bias.) Therefore, instead of following the usual convention of feeding back argmax unit vectors, this implementation feeds back the softmax output directly.\n\nThis can even be done for beam search (which normally splits up the full distribution into a few select explicit candidates, represented as unit vectors) by simply resetting maximum outputs for lower-scoring candidates successively.\n\n### Decoder modes\n\nWhile the _encoder_ can always be run in parallel over a batch of lines and by passing the full sequence of characters in one tensor (padded to the longest line in the batch), which is very efficient with Keras backends like Tensorflow, a **beam-search** _decoder_ requires passing initial/final states character-by-character, with parallelism employed to capture multiple history hypotheses of a single line. However, one can also **greedily** use the best output only for each position (without beam search). This latter option also allows to run in parallel over lines, which is much faster – consuming up to ten times less CPU time.\n\nThererfore, the backend function can operate the decoder network in either of the following modes:\n\n#### _fast_\n\nDecode greedily, but feeding back the full softmax distribution in batch mode (lines-parallel).\n\n#### _greedy_\n\nDecode greedily, but feeding back the full softmax distribution for each line separately.\n\n#### _default_\n\nDecode beamed, selecting the best output candidates of the best history hypotheses for each line and feeding back their (successively reset) partial softmax distributions in batch mode (hypotheses-parallel). More specifically:\n\n> Start decoder with start-of-sequence, then keep decoding until\n> end-of-sequence is found or output length is way off, repeatedly.\n> Decode by using the best predicted output characters and several next-best\n> alternatives (up to some degradation threshold) as next input.\n> Follow-up on the N best overall candidates (estimated by accumulated\n> score, normalized by length and prospective cost), i.e. do A*-like\n> breadth-first search, with N equal `batch_size`.\n> Pass decoder initial/final states from character to character,\n> for each candidate respectively.\n\n### Rejection\n\nDuring beam search (default decoder mode), whenever the input and output is in good alignment (i.e. the attention model yields an alignment approximately 1 character after their predecessor's alignment on average), it is possible to estimate the current position in the source string. This input character's predicted output score, when smaller than a given (i.e. variable) probability threshold can be clipped to that minimum. This effectively adds a candidate which _rejects_ correction at that position (keeping the input unchanged).\n\n![rejection example](./rejection.png \"soft alignment and probabilities, greedy and beamed (red is the rejection candidate)\")\n\nThat probability is called _rejection threshold_ as a runtime parameter. But while 0.0 _will_ disable rejection completely (i.e. the input hypothesis, if at all identifiable, will keep its predicted score), 1.0 will _not_ disable correction completely (because the input hypothesis might not be found if alignment is too bad). \n\n### Underspecification and gap\n\nInput characters that have not been seen during training must be well-behaved at inference time: They must be represented by a reserved index, and should behave like **neutral/unknown** characters instead of spoiling HL states and predictions in a longer follow-up context. This is achieved by dedicated leave-one-out training and regularization to optimize for interpolation of all known characters. At runtime, the encoder merely shows a warning of the previously unseen character.\n\nThe same device is useful to fill a known **gap** in the input (the only difference being that no warning is shown).\n\nAs an additional facility, characters that are known in advance to not fit well with the model can be mapped prior to correction with the `charmap` parameter.\n\n### Training\n\nPossibilities:\n- incremental training and pretraining (on clean-only text)\n- scheduled sampling (mixed teacher forcing and decoder feedback)\n- LM transfer (initialization of the decoder weights from a language model of the same topology)\n- shallow transfer (initialization of encoder/decoder weights from a model of lesser depth)\n\nFor existing models, cf. [models subrepository](https://github.com/ASVLeipzig/cor-asv-ann-models/).\n\nFor tools and datasets, cf. [data processing subrepository](https://github.com/ASVLeipzig/cor-asv-ann-data-processing/).\n\n### Processing PAGE annotations\n\nWhen applied on [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) (as [OCR-D workspace processor](https://ocr-d.github.io/cli), cf. [usage](#ocr-d-processor-interface-ocrd-cor-asv-ann-process)), this component also allows processing below the `TextLine` hierarchy level, i.e. on `Word` or `Glyph` level.\n\nFor that one needs to distribute the line-level transduction onto the lower level elements (keeping their coordinates and other meta-data), while changing Word segmentation if necessary (i.e. merging and splitting tokens). To calculate an optimal **hard alignment** path for characters, we _could_ use the soft alignment scores – but in practise, the quality of an independent, a-posteriori string alignment (i.e. Needleman-Wunsch algorithm) is better.\n\n### Evaluation\n\nText lines can be compared (by aligning and computing a distance under some metric) across multiple inputs. (This would typically be GT and OCR vs post-correction.) This can be done both on plain text files (`cor-asv-ann-eval`) and PAGE-XML annotations (`ocrd-cor-asv-ann-evaluate`). \n\nDistances are accumulated (as micro-averages) as character error rate (CER) mean and stddev, but only on the character level.\n\nThere are a number of distance metrics available (all but the first operating on grapheme clusters, not mere codepoints, and using the alignment path length as denominator, not just the maximum string length):\n- `Levenshtein-fast`:  \n  simple unweighted edit distance (fastest, standard; GT level 3; no regard for combining sequences; max-length norm)\n- `Levenshtein`:  \n  simple unweighted edit distance (GT level 3)\n- `NFC`:  \n  like `Levenshtein`, but apply Unicode normal form with canonical composition before (i.e. less than GT level 2)\n- `NFKC`:  \n  like `Levenshtein`, but apply Unicode normal form with compatibility composition before (i.e. less than GT level 2, except for `ſ`, which is already normalized to `s`)\n- `historic_latin`:  \n  like `Levenshtein`, but decomposing non-vocalic ligatures before and treating as equivalent (i.e. zero distances) confusions of certain semantically close characters often found in historic texts (e.g. umlauts with combining letter `e` as in `Wuͤſte` instead of  to `Wüſte`, `ſ` vs `s`, or quotation/citation marks; GT level 1)\n\n...perplexity measurement...\n\n## Installation\n\nBesides [OCR-D](https://github.com/OCR-D/core), this builds on Keras/Tensorflow.\n\nRequired Ubuntu packages:\n\n* Python (`python` or `python3`)\n* pip (`python-pip` or `python3-pip`)\n* venv (`python-venv` or `python3-venv`)\n\nCreate and activate a virtual environment as usual.\n\nTo install Python dependencies:\n\n    make deps\n\nWhich is the equivalent of:\n\n    pip install -r requirements.txt\n\n\nTo install this module, then do:\n\n    make install\n\nWhich is the equivalent of:\n\n    pip install .\n\n\nThe module can use CUDA-enabled GPUs (when sufficiently installed), but can also run on CPU only. Models are always interchangable.\n\n> Note: Models and code are still based on Keras 2.3 / Tensorflow 1.15, which are already end-of-life. You might need an extra venv just for this module to avoid conflicts with other packages. Also, Python >= 3.8 and CUDA toolkit >= 11.0 might not work with prebuilt Tensorflow versions (but see [installation](./INSTALL.md) in that case).\n\n## Usage\n\nThis packages has the following user interfaces:\n\n### command line interface `cor-asv-ann-train`\n\nTo be used with TSV files (tab-delimited source-target lines),\nor pickle dump files (source-target tuple lists).\n\n```\nUsage: cor-asv-ann-train [OPTIONS] [DATA]...\n\n  Train a correction model on GT files.\n\n  Configure a sequence-to-sequence model with the given parameters.\n\n  If given `load_model`, and its configuration matches the current\n  parameters, then load its weights. If given `init_model`, then transfer\n  its mapping and matching layer weights. (Also, if its configuration has 1\n  less hidden layers, then fixate the loaded weights afterwards.) If given\n  `reset_encoder`, re-initialise the encoder weights afterwards.\n\n  Then, regardless, train on the `data` files using early stopping.\n\n  (Supported file formats are:\n   - * (tab-separated values), with source-target lines\n   - *.pkl (pickle dumps), with source-target lines, where source is either\n     - a single string, or\n     - a sequence of character-probability tuples.)\n\n  If no `valdata` were given, split off a random fraction of lines for\n  validation. Otherwise, use only those files for validation.\n\n  If the training has been successful, save the model under `save_model`.\n\nOptions:\n  -m, --save-model FILE      model file for saving\n  --load-model FILE          model file for loading (incremental/pre-training)\n  --init-model FILE          model file for initialisation (transfer from LM\n                             or shallower model)\n  --reset-encoder            reset encoder weights after load/init\n  -w, --width INTEGER RANGE  number of nodes per hidden layer\n  -d, --depth INTEGER RANGE  number of stacked hidden layers\n  -v, --valdata FILE         file to use for validation (instead of random\n                             split)\n  -h, --help                 Show this message and exit.\n```\n\n### command line interface `cor-asv-ann-proc`\n\nTo be used with plain-text files, TSV files (tab-delimited source-target lines\n– where target is ignored), or pickle dump files (source-target tuple lists –\nwhere target is ignored).\n\n```\nUsage: cor-asv-ann-proc [OPTIONS] [DATA]...\n\n  Apply a correction model on GT or text files.\n\n  Load a sequence-to-sequence model from the given path.\n\n  Then open the `data` files, (ignoring target side strings, if any) and\n  apply the model to its (source side) strings in batches, accounting for\n  input file names line by line.\n\n  (Supported file formats are:\n   - * (plain-text), with source lines,\n   - * (tab-separated values), with source-target lines,\n   - *.pkl (pickle dumps), with source-target lines, where source is either\n     - a single string, or\n     - a sequence of character-probability tuples.)\n\n  For each input file, open a new output file derived from its file name by\n  removing `old_suffix` (or the last extension) and appending `new_suffix`.\n  Write the resulting lines to that output file.\n\nOptions:\n  -m, --load-model FILE        model file to load\n  -f, --fast                   only decode greedily\n  -r, --rejection FLOAT RANGE  probability of the input characters in all\n                               hypotheses (set 0 to use raw predictions)\n  -C, --charmap TEXT           mapping for input characters before passing to\n                               correction; can be used to adapt to character\n                               set mismatch between input and model (without\n                               relying on underspecification alone)\n  -S, --old-suffix TEXT        Suffix to remove from input files for output\n                               files\n  -s, --new-suffix TEXT        Suffix to append to input files for output\n                               files\n  -h, --help                   Show this message and exit.\n```\n\n### command line interface `cor-asv-ann-eval`\n\nTo be used with TSV files (tab-delimited source-target lines),\nor pickle dump files (source-target tuple lists).\n\n```\nUsage: cor-asv-ann-eval [OPTIONS] [DATA]...\n\n  Evaluate a correction model on GT files.\n\n  Load a sequence-to-sequence model from the given path.\n\n  Then apply on the file paths `data`, comparing predictions (both greedy\n  and beamed) with GT target, and measuring error rates.\n\n  (Supported file formats are:\n   - * (tab-separated values), with source-target lines\n   - *.pkl (pickle dumps), with source-target lines, where source is either\n     - a single string, or\n     - a sequence of character-probability tuples.)\n\nOptions:\n  -m, --load-model FILE           model file to load\n  -f, --fast                      only decode greedily\n  -r, --rejection FLOAT RANGE     probability of the input characters in all\n                                  hypotheses (set 0 to use raw predictions)\n  -n, --normalization [Levenshtein|NFC|NFKC|historic_latin]\n                                  normalize character sequences before\n                                  alignment/comparison (set Levenshtein for\n                                  none)\n  -C, --charmap TEXT              mapping for input characters before passing\n                                  to correction; can be used to adapt to\n                                  character set mismatch between input and\n                                  model (without relying on underspecification\n                                  alone)\n  -l, --gt-level INTEGER RANGE    GT transcription level to use for\n                                  historic_latin normlization (1: strongest,\n                                  3: none)\n  -c, --confusion INTEGER RANGE   show this number of most frequent (non-\n                                  identity) edits (set 0 for none)\n  -H, --histogram                 aggregate and compare character histograms\n  -h, --help                      Show this message and exit.\n```\n\n### command line interface `cor-asv-ann-compare`\n\nTo be used with PAGE-XML files, plain-text files, or plain-text file lists\n(of PAGE-XML or plain-text files), 1 for GT and N for predictions (OCR or COR).\n\n```\nUsage: cor-asv-ann-compare [OPTIONS] GT_FILE [OCR_FILES]...\n\n  Compare text lines by aligning and computing the textual distance and\n  character error rate.\n\n  This compares 1:n given PAGE-XML or plain text files.\n\n  If `--file-lists` is given and files are plain text, then they will be\n  interpreted as (newline-separated) lists of path names for single-line\n  text files (for Ocropus convention).\n\n  Writes a JSON report file to `--output-file`. (No error aggregation across\n  files in this CLI.)\n\nOptions:\n  -o, --output-file FILE          path name of generated report (default:\n                                  stdout)\n  -n, --normalization [Levenshtein-fast|Levenshtein|NFC|NFKC|historic_latin]\n                                  normalize character sequences before\n                                  alignment/comparison (set Levenshtein for\n                                  none)\n  -l, --gt-level INTEGER RANGE    GT transcription level to use for\n                                  historic_latin normlization (1: strongest,\n                                  3: none)\n  -c, --confusion INTEGER RANGE   show this number of most frequent (non-\n                                  identity) edits (set 0 for none)\n  -H, --histogram                 aggregate and compare character histograms\n  -F, --file-lists                interpret files as plain text files with one\n                                  file path per line\n  -h, --help                      Show this message and exit.\n```\n\n\n### command line interface `cor-asv-ann-repl`\n\nThis tool provides a Python read-eval-print-loop for interactive usage (including some visualization):\n\n```\nUsage: cor-asv-ann-repl [OPTIONS]\n\n  Try a correction model interactively.\n\n  Import Sequence2Sequence, instantiate `s2s`, then enter REPL. Also,\n  provide function `transcode_line` for single line correction.\n\nOptions:\n  --help  Show this message and exit.\n```\n\nHere is what you see after starting up the interpreter:\n```\nusage example:\n>>> s2s.load_config('model')\n>>> s2s.configure()\n>>> s2s.load_weights('model')\n>>> s2s.evaluate(['filename'])\n\n>>> transcode_line('hello world!')\nnow entering REPL...\n\nPython 3.6.7 (default, Oct 22 2018, 11:32:17) \n[GCC 8.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(InteractiveConsole)\n```\n\n\n### [OCR-D processor](https://ocr-d.de/en/spec/cli) interface `ocrd-cor-asv-ann-process`\n\nTo be used with [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.de/about/) annotation workflow. \n\nInput could be anything with a textual annotation (`TextEquiv` on the given `textequiv_level`). \n\nPretrained model files are contained in the [models subrepository](https://github.com/ASVLeipzig/cor-asv-ann-models/). At runtime, you can use both absolute and relative paths for model files. The latter are searched for in the installation directory, under the path in the environment variable `CORASVANN_DATA` (if given), and in the default paths of the OCR-D resource manager (i.e. you can do `ocrd resmgr download -na ocrd-cor-asv-ann-process https://github.com/ASVLeipzig/cor-asv-ann-models/blob/master/s2s.dta19.Fraktur4.d2.w0512.adam.attention.stateless.variational-dropout.char.pretrained+retrained-conf.h5` and then `ocrd resmgr list-installed -e ocrd-cor-asv-ann-process` tells you that `s2s.dta19.Fraktur4.d2.w0512.adam.attention.stateless.variational-dropout.char.pretrained+retrained-conf.h5` will resolve as `model_file`).\n\n\n```\nUsage: ocrd-cor-asv-ann-process [OPTIONS]\n\n  Improve text annotation by character-level encoder-attention-decoder ANN model\n\n  > Perform OCR post-correction with encoder-attention-decoder ANN on\n  > the workspace.\n\n  > Open and deserialise PAGE input files, then iterate over the element\n  > hierarchy down to the requested `textequiv_level`, making sequences\n  > of TextEquiv objects as lists of lines. Concatenate their string\n  > values, obeying rules of implicit whitespace, and map the string\n  > positions where the objects start.\n\n  > Next, transcode the input lines into output lines in parallel, and\n  > use the retrieved soft alignment scores to calculate hard alignment\n  > paths between input and output string via Viterbi decoding. Then use\n  > those to map back the start positions and overwrite each TextEquiv\n  > with its new content, paying special attention to whitespace:\n\n  > Distribute edits such that whitespace objects cannot become more\n  > than whitespace (or be deleted) and that non-whitespace objects must\n  > not start or end with whitespace (but may contain new whitespace in\n  > the middle).\n\n  > Subsequently, unless processing on the `line` level, make the Word\n  > segmentation consistent with that result again: merge around deleted\n  > whitespace tokens and split at whitespace inside non-whitespace\n  > tokens.\n\n  > Finally, make the levels above `textequiv_level` consistent with\n  > that textual result (via concatenation joined by whitespace).\n\n  > Produce new output files by serialising the resulting hierarchy.\n\nOptions:\n  -I, --input-file-grp USE        File group(s) used as input\n  -O, --output-file-grp USE       File group(s) used as output\n  -g, --page-id ID                Physical page ID(s) to process\n  --overwrite                     Remove existing output pages/images\n                                  (with --page-id, remove only those)\n  -p, --parameter JSON-PATH       Parameters, either verbatim JSON string\n                                  or JSON file path\n  -P, --param-override KEY VAL    Override a single JSON object key-value pair,\n                                  taking precedence over --parameter\n  -m, --mets URL-PATH             URL or file path of METS to process\n  -w, --working-dir PATH          Working directory of local workspace\n  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]\n                                  Log level\n  -C, --show-resource RESNAME     Dump the content of processor resource RESNAME\n  -L, --list-resources            List names of processor resources\n  -J, --dump-json                 Dump tool description as JSON and exit\n  -h, --help                      This help message\n  -V, --version                   Show version\n\nParameters:\n   \"model_file\" [string - REQUIRED]\n    path of h5py weight/config file for model trained with cor-asv-ann-\n    train\n   \"textequiv_level\" [string - \"glyph\"]\n    PAGE XML hierarchy level to read/write TextEquiv input/output on\n    Possible values: [\"line\", \"word\", \"glyph\"]\n   \"charmap\" [object - {}]\n    mapping for input characters before passing to correction; can be\n    used to adapt to character set mismatch between input and model\n    (without relying on underspecification alone)\n   \"rejection_threshold\" [number - 0.5]\n    minimum probability of the candidate corresponding to the input\n    character in each hypothesis during beam search, helps balance\n    precision/recall trade-off; set to 0 to disable rejection (max\n    recall) or 1 to disable correction (max precision)\n   \"relative_beam_width\" [number - 0.2]\n    minimum fraction of the best candidate's probability required to\n    enter the beam in each hypothesis; controls the quality/performance\n    trade-off\n   \"fixed_beam_width\" [number - 15]\n    maximum number of candidates allowed to enter the beam in each\n    hypothesis; controls the quality/performance trade-off\n   \"fast_mode\" [boolean - false]\n    decode greedy instead of beamed, with batches of parallel lines\n    instead of parallel alternatives; also disables rejection and beam\n    parameters; enable if performance is far more important than quality\n```\n\n### [OCR-D processor](https://ocr-d.de/en/spec/cli) interface `ocrd-cor-asv-ann-evaluate`\n\nTo be used with [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.de/about/) annotation workflow.\n\nInputs could be anything with a textual annotation (`TextEquiv` on the line level), but at least 2. The first in the list of input file groups will be regarded as reference/GT.\n\nThere are various evaluation [metrics](#Evaluation) available.\n\nThe tool can also aggregate and show the most frequent character confusions.\n\n```\nUsage: ocrd-cor-asv-ann-evaluate [OPTIONS]\n\n  Align different textline annotations and compute distance\n\n  > Align textlines of multiple file groups and calculate distances.\n\n  > Find files in all input file groups of the workspace for the same\n  > pageIds. The first file group serves as reference annotation (ground\n  > truth).\n\n  > Open and deserialise PAGE input files, then iterate over the element\n  > hierarchy down to the TextLine level, looking at each first\n  > TextEquiv. Align character sequences in all pairs of lines for the\n  > same TextLine IDs, and calculate the distances using the error\n  > metric `metric`. Accumulate distances and sequence lengths per file\n  > group globally and per file, and show each fraction as a CER rate in\n  > the log.\n\nOptions:\n  -I, --input-file-grp USE        File group(s) used as input\n  -O, --output-file-grp USE       File group(s) used as output\n  -g, --page-id ID                Physical page ID(s) to process\n  --overwrite                     Remove existing output pages/images\n                                  (with --page-id, remove only those)\n  -p, --parameter JSON-PATH       Parameters, either verbatim JSON string\n                                  or JSON file path\n  -P, --param-override KEY VAL    Override a single JSON object key-value pair,\n                                  taking precedence over --parameter\n  -m, --mets URL-PATH             URL or file path of METS to process\n  -w, --working-dir PATH          Working directory of local workspace\n  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]\n                                  Log level\n  -C, --show-resource RESNAME     Dump the content of processor resource RESNAME\n  -L, --list-resources            List names of processor resources\n  -J, --dump-json                 Dump tool description as JSON and exit\n  -h, --help                      This help message\n  -V, --version                   Show version\n\nParameters:\n   \"metric\" [string - \"Levenshtein-fast\"]\n    Distance metric to calculate and aggregate: `historic_latin` for GT\n    level 1-3, `NFKC` for roughly GT level 2 (but including reduction of\n    `ſ/s` and superscript numerals etc), `Levenshtein` for GT level 3\n    (or `Levenshtein-fast` for faster alignment but using maximum\n    sequence length instead of path length as CER denominator).\n    Possible values: [\"Levenshtein-fast\", \"Levenshtein\", \"NFC\", \"NFKC\",\n    \"historic_latin\"]\n   \"gt_level\" [number - 1]\n    When `metric=historic_latin`, normalize and equate at this GT\n    transcription level.\n    Possible values: [1, 2, 3]\n   \"confusion\" [number - 0]\n    Count edits and show that number of most frequent confusions (non-\n    identity) in the end.\n   \"histogram\" [boolean - false]\n    Aggregate and show mutual character histograms.\n```\n\nThe output file group for the evaluation tool will contain a JSON report of the CER distances of each text line per page, and an aggregated JSON report with the totals and the confusion table. It also makes extensive use of logging.\n\n### [OCR-D processor](https://ocr-d.de/en/spec/cli) interface `ocrd-cor-asv-ann-align`\n\nTo be used with [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.de/about/) annotation workflow.\n\nInputs could be anything with a textual annotation (`TextEquiv` on the line level), but at least 2 fileGrps (or 3 for `method=majority`). No input will be priviledged regarding text content, but the first input fileGrp will serve as the base annotation for the output.\n\n```\nUsage: ocrd-cor-asv-ann-align [OPTIONS]\n\n  Align different textline annotations and pick best\n\n  > Align textlines of multiple file groups and choose the 'best'\n  > characters.\n\n  > Find files in all input file groups of the workspace for the same\n  > pageIds.\n\n  > Open and deserialise PAGE input files, then iterate over the element\n  > hierarchy down to the TextLine level, looking at each first\n  > TextEquiv. Align character sequences in all pairs of lines for the\n  > same TextLine IDs, and for each position pick the 'best' character\n  > hypothesis among the inputs.\n\n  > Choice depends on ``method``:\n  > - if `majority`, then use a majority rule over the inputs\n  >   (requires at least 3 input fileGrps),\n  > - if `confidence`, then use the candidate with the highest confidence\n  >   (requires input with per-character or per-line confidence annotations),\n  > - if `combined`, then try a heuristic combination of both approaches\n  >   (requires both conditions).\n\n  > Then concatenate those character choices to new TextLines (without\n  > segmentation at lower levels).\n\n  > Finally, make the parent regions (higher levels) consistent with\n  > that textual result (via concatenation joined by whitespace), and\n  > remove the child words/glyphs (lower levels) altogether.\n\n  > Produce new output files by serialising the resulting hierarchy.\n\nOptions:\n  -I, --input-file-grp USE        File group(s) used as input\n  -O, --output-file-grp USE       File group(s) used as output\n  -g, --page-id ID                Physical page ID(s) to process\n  --overwrite                     Remove existing output pages/images\n                                  (with --page-id, remove only those)\n  -p, --parameter JSON-PATH       Parameters, either verbatim JSON string\n                                  or JSON file path\n  -P, --param-override KEY VAL    Override a single JSON object key-value pair,\n                                  taking precedence over --parameter\n  -m, --mets URL-PATH             URL or file path of METS to process\n  -w, --working-dir PATH          Working directory of local workspace\n  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]\n                                  Log level\n  -C, --show-resource RESNAME     Dump the content of processor resource RESNAME\n  -L, --list-resources            List names of processor resources\n  -J, --dump-json                 Dump tool description as JSON and exit\n  -h, --help                      This help message\n  -V, --version                   Show version\n\nParameters:\n   \"method\" [string - \"majority\"]\n    decide by majority of OCR hypotheses, by highest confidence of OCRs\n    or by a combination thereof\n    Possible values: [\"majority\", \"confidence\", \"combined\"]\n```\n\n### [OCR-D processor](https://ocr-d.de/en/spec/cli) interface `ocrd-cor-asv-ann-join`\n\nTo be used with [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.de/about/) annotation workflow.\n\nInputs could be anything with a textual annotation (`TextEquiv` on the line level), but at least 2 fileGrps. No input will be priviledged regarding text content, but the first input fileGrp will become the first TextEquiv (which is usually the preferred annotation in OCR-D processors for consumption).\n\n```\nUsage: ocrd-cor-asv-ann-join [OPTIONS]\n\n  Join different textline annotations by concatenation\n\n  > Join textlines of multiple file groups by concatenating their text\n  > results.\n\n  > Find files in all input file groups of the workspace for the same\n  > pageIds.\n\n  > Open and deserialise PAGE input files, then iterate over the element\n  > hierarchy down to the TextLine level. Concatenate the TextEquivs for\n  > all lines with the same TextLine IDs, optionally differentiating\n  > them by adding their original fileGrp name in @comments.\n\n  > Produce new output files by serialising the resulting hierarchy.\n\nOptions:\n  -I, --input-file-grp USE        File group(s) used as input\n  -O, --output-file-grp USE       File group(s) used as output\n  -g, --page-id ID                Physical page ID(s) to process\n  --overwrite                     Remove existing output pages/images\n                                  (with --page-id, remove only those)\n  -p, --parameter JSON-PATH       Parameters, either verbatim JSON string\n                                  or JSON file path\n  -P, --param-override KEY VAL    Override a single JSON object key-value pair,\n                                  taking precedence over --parameter\n  -m, --mets URL-PATH             URL or file path of METS to process\n  -w, --working-dir PATH          Working directory of local workspace\n  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]\n                                  Log level\n  -C, --show-resource RESNAME     Dump the content of processor resource RESNAME\n  -L, --list-resources            List names of processor resources\n  -J, --dump-json                 Dump tool description as JSON and exit\n  -h, --help                      This help message\n  -V, --version                   Show version\n\nParameters:\n   \"add-filegrp-comments\" [boolean - false]\n    set @comments of each TextEquiv to the fileGrp it came from\n```\n\n### [OCR-D processor](https://ocr-d.de/en/spec/cli) interface `ocrd-cor-asv-ann-mark`\n\nTo be used with [PAGE-XML](https://github.com/PRImA-Research-Lab/PAGE-XML) documents in an [OCR-D](https://ocr-d.de/about/) annotation workflow.\n\nInputs could be anything with a textual annotation (`TextEquiv` on the word level).\n\n```\nUsage: ocrd-cor-asv-ann-mark [OPTIONS]\n\n  mark words not found by a spellchecker\n\n  > Mark words that are not recognized by a spellchecker\n\n  > Open and deserialise PAGE input files, then iterate over the element\n  > hierarchy down to the word level. If there is no text or empty text,\n  > continue. Otherwise, normalize the text content by apply the\n  > character-wise `normalization`, and stripping any non-letters. Pass\n  > that string into `command`: if the output is not empty, then mark\n  > the word according to `format`.\n\n  > Produce new output files by serialising the resulting hierarchy.\n\nOptions:\n  -I, --input-file-grp USE        File group(s) used as input\n  -O, --output-file-grp USE       File group(s) used as output\n  -g, --page-id ID                Physical page ID(s) to process\n  --overwrite                     Remove existing output pages/images\n                                  (with --page-id, remove only those)\n  -p, --parameter JSON-PATH       Parameters, either verbatim JSON string\n                                  or JSON file path\n  -P, --param-override KEY VAL    Override a single JSON object key-value pair,\n                                  taking precedence over --parameter\n  -m, --mets URL-PATH             URL or file path of METS to process\n  -w, --working-dir PATH          Working directory of local workspace\n  -l, --log-level [OFF|ERROR|WARN|INFO|DEBUG|TRACE]\n                                  Log level\n  -C, --show-resource RESNAME     Dump the content of processor resource RESNAME\n  -L, --list-resources            List names of processor resources\n  -J, --dump-json                 Dump tool description as JSON and exit\n  -h, --help                      This help message\n  -V, --version                   Show version\n\nParameters:\n   \"command\" [string - REQUIRED]\n    external tool to query word forms, e.g. 'hunspell -i utf-8 -d\n    de_DE,en_US -w'\n   \"normalization\" [object - {}]\n    mapping of characters prior to spellcheck, e.g. {'ſ': 's', 'aͤ': 'ä'}\n   \"format\" [string - \"conf\"]\n    how unknown words should be marked; if 'conf', then writes\n    @conf=0.123, otherwise writes that value into @comments\n```\n\n\n## Testing\n\nnot yet!\n...\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/ASVLeipzig/cor-asv-ann",
    "keywords": "",
    "license": "Apache License 2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "ocrd-cor-asv-ann",
    "package_url": "https://pypi.org/project/ocrd-cor-asv-ann/",
    "platform": null,
    "project_url": "https://pypi.org/project/ocrd-cor-asv-ann/",
    "project_urls": {
      "Homepage": "https://github.com/ASVLeipzig/cor-asv-ann"
    },
    "release_url": "https://pypi.org/project/ocrd-cor-asv-ann/0.1.14/",
    "requires_dist": [
      "ocrd (>=2.22.0)",
      "click",
      "keras (==2.3.*)",
      "numpy",
      "tensorflow-gpu (==1.15.*)",
      "h5py (<3.0.0)",
      "editdistance",
      "matplotlib"
    ],
    "requires_python": "",
    "summary": "sequence-to-sequence translator for noisy channel error correction",
    "version": "0.1.14",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 15243578,
  "releases": {
    "0.1.10": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "659494195f6dbd29f3c5c7d0de25308af60cfe09be63a2eb7b3df07ab2c59a22",
          "md5": "8b7751e10b4bb7705bce8a9a17b02d80",
          "sha256": "9cd3499164ffc87e3a6796d943100ac0125d69563dee34af454fb16c02a558dd"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.10-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8b7751e10b4bb7705bce8a9a17b02d80",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 81566,
        "upload_time": "2021-05-20T16:40:51",
        "upload_time_iso_8601": "2021-05-20T16:40:51.268848Z",
        "url": "https://files.pythonhosted.org/packages/65/94/94195f6dbd29f3c5c7d0de25308af60cfe09be63a2eb7b3df07ab2c59a22/ocrd_cor_asv_ann-0.1.10-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "f256a1251e1559c1d451c0f52ed3628a91ab1c619a20c8ad7c1ad166ae53047c",
          "md5": "97dd6c4da542f640236a83b1f6fbad98",
          "sha256": "ebd79df84bfa23f23790256ac5b14632913de79ae0c3c344d9bb0c50db9bb3fa"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.10.tar.gz",
        "has_sig": false,
        "md5_digest": "97dd6c4da542f640236a83b1f6fbad98",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 75296,
        "upload_time": "2021-05-20T16:40:53",
        "upload_time_iso_8601": "2021-05-20T16:40:53.490396Z",
        "url": "https://files.pythonhosted.org/packages/f2/56/a1251e1559c1d451c0f52ed3628a91ab1c619a20c8ad7c1ad166ae53047c/ocrd_cor_asv_ann-0.1.10.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "db11ea3fea74a8551e386ad858e15bb69a3754001bcaf4d44c7df1a33feeb41e",
          "md5": "d893375168f4c20b2c1129b2d18f8072",
          "sha256": "4e05582dd6b9132a1fda8790ab76374423e715002acc4e7276baf020d2879f5b"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.11-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d893375168f4c20b2c1129b2d18f8072",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 83160,
        "upload_time": "2021-05-31T12:17:22",
        "upload_time_iso_8601": "2021-05-31T12:17:22.441244Z",
        "url": "https://files.pythonhosted.org/packages/db/11/ea3fea74a8551e386ad858e15bb69a3754001bcaf4d44c7df1a33feeb41e/ocrd_cor_asv_ann-0.1.11-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7659593b3bb847929246adc0664fdfd6c3ce5d2b12cbeffc08f44cdc77d7f7ac",
          "md5": "f75e6a74deaecbee4c7fa010e19fe0c7",
          "sha256": "d55de3bfbbd46eecbb6927a44a834d197cac6330e626e4223d8bc9718784b61c"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.11.tar.gz",
        "has_sig": false,
        "md5_digest": "f75e6a74deaecbee4c7fa010e19fe0c7",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 83347,
        "upload_time": "2021-05-31T12:17:24",
        "upload_time_iso_8601": "2021-05-31T12:17:24.470782Z",
        "url": "https://files.pythonhosted.org/packages/76/59/593b3bb847929246adc0664fdfd6c3ce5d2b12cbeffc08f44cdc77d7f7ac/ocrd_cor_asv_ann-0.1.11.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.12": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "69fdb8efd7b0639e649ab4e2ad90932271892657d41efb5e24a3b8f9639a6e3e",
          "md5": "87b03f942943139f963520bd02915289",
          "sha256": "f85fe010b14f37518e3a72703b9da87c669b2657388079ef6517fbfee6f4b6b7"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.12-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "87b03f942943139f963520bd02915289",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 84095,
        "upload_time": "2021-06-23T23:10:45",
        "upload_time_iso_8601": "2021-06-23T23:10:45.878327Z",
        "url": "https://files.pythonhosted.org/packages/69/fd/b8efd7b0639e649ab4e2ad90932271892657d41efb5e24a3b8f9639a6e3e/ocrd_cor_asv_ann-0.1.12-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "989b53869e14661488a1572fa6ea028e4e1a244c8019544627aba9b149b9a3ea",
          "md5": "676675c5345021c5e84f1b3dc00c98fe",
          "sha256": "266e6d9780ea3ab2b07eaffb347b9d171ba99019c0890275b638ec85d7e2e09f"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.12.tar.gz",
        "has_sig": false,
        "md5_digest": "676675c5345021c5e84f1b3dc00c98fe",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 88030,
        "upload_time": "2021-06-23T23:10:47",
        "upload_time_iso_8601": "2021-06-23T23:10:47.943801Z",
        "url": "https://files.pythonhosted.org/packages/98/9b/53869e14661488a1572fa6ea028e4e1a244c8019544627aba9b149b9a3ea/ocrd_cor_asv_ann-0.1.12.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.13": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "bb73f5e80c2fc7a718e52c29d8be2b819c224eda833354f97a678aae33bb1207",
          "md5": "6cab105a21a6ba5a050e899f0580286a",
          "sha256": "f516dc6ed1af37ed197a636caca87f155d2bf4c71b7467294d9640fbc87cdad4"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.13-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6cab105a21a6ba5a050e899f0580286a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 92564,
        "upload_time": "2021-12-21T12:18:30",
        "upload_time_iso_8601": "2021-12-21T12:18:30.920936Z",
        "url": "https://files.pythonhosted.org/packages/bb/73/f5e80c2fc7a718e52c29d8be2b819c224eda833354f97a678aae33bb1207/ocrd_cor_asv_ann-0.1.13-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b337e966bee44aedc2019baa1bbc3fde3be390775eb826d482eed73fd4082c8d",
          "md5": "83fbcd8744c181182014a42ef2a389f8",
          "sha256": "b5cc2b24458e82d2e7b140a1177b6d7c755562c9d9f31f27048e1effa90b88ad"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.13.tar.gz",
        "has_sig": false,
        "md5_digest": "83fbcd8744c181182014a42ef2a389f8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 98077,
        "upload_time": "2021-12-21T12:18:33",
        "upload_time_iso_8601": "2021-12-21T12:18:33.195484Z",
        "url": "https://files.pythonhosted.org/packages/b3/37/e966bee44aedc2019baa1bbc3fde3be390775eb826d482eed73fd4082c8d/ocrd_cor_asv_ann-0.1.13.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.14": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "79ecfec93e659c49a23695fe830f9d8a675776523d5c494c02a986f3bb54670b",
          "md5": "b44de64c62600ceba76c9b299db11d30",
          "sha256": "e995c76a51cf8fdeec8a2617c00c5d79ae99f8d792d55ec8503b8accfcb67c8c"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.14-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b44de64c62600ceba76c9b299db11d30",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 97274,
        "upload_time": "2022-09-28T15:49:09",
        "upload_time_iso_8601": "2022-09-28T15:49:09.365915Z",
        "url": "https://files.pythonhosted.org/packages/79/ec/fec93e659c49a23695fe830f9d8a675776523d5c494c02a986f3bb54670b/ocrd_cor_asv_ann-0.1.14-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "68bc6c06042c47dfab9023c84b896c0a5ce9170561e9f7e11ddd666697996791",
          "md5": "64ae32ea72736329dc00a9d41f195710",
          "sha256": "0f9fb2cd7168d4c13ec508d54916b4815ff9b13d670bcf33889baf1d3a9f59be"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.14.tar.gz",
        "has_sig": false,
        "md5_digest": "64ae32ea72736329dc00a9d41f195710",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 102248,
        "upload_time": "2022-09-28T15:49:12",
        "upload_time_iso_8601": "2022-09-28T15:49:12.844779Z",
        "url": "https://files.pythonhosted.org/packages/68/bc/6c06042c47dfab9023c84b896c0a5ce9170561e9f7e11ddd666697996791/ocrd_cor_asv_ann-0.1.14.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "65923c23e6ec93846cb3c7da6c789ebf4f861dfe7c7a5c75b98d55f02ece3443",
          "md5": "af3ad3baf28e72b896b83470a267fef0",
          "sha256": "4dc1d1aa49be07730ca788f2ce060f43f9e266badf4f30dbd2faa08f608fc912"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.2-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "af3ad3baf28e72b896b83470a267fef0",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 74568,
        "upload_time": "2020-02-10T23:42:06",
        "upload_time_iso_8601": "2020-02-10T23:42:06.515439Z",
        "url": "https://files.pythonhosted.org/packages/65/92/3c23e6ec93846cb3c7da6c789ebf4f861dfe7c7a5c75b98d55f02ece3443/ocrd_cor_asv_ann-0.1.2-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8e283db473a5a1ad65930633f7fb05e9519ca0fc971035b7488ddec1f5787837",
          "md5": "76f75131ba098c9807c65ea5c5a69200",
          "sha256": "c10bbd76c64d4f2e155b220a8ecd58dbed9fd8e040b8763d176d139215e02569"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "76f75131ba098c9807c65ea5c5a69200",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 66983,
        "upload_time": "2020-02-10T23:42:09",
        "upload_time_iso_8601": "2020-02-10T23:42:09.311626Z",
        "url": "https://files.pythonhosted.org/packages/8e/28/3db473a5a1ad65930633f7fb05e9519ca0fc971035b7488ddec1f5787837/ocrd_cor_asv_ann-0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "10d82693513d9048d18f4ad8fe25f9ba27dab589ba1da41c1f0c3e350fdefee6",
          "md5": "bbbf7133609da87137b356f48d31498a",
          "sha256": "7e001009c102e4183951f44f8d546af882008e3e03076691aa65718055a660b8"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bbbf7133609da87137b356f48d31498a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 76302,
        "upload_time": "2020-08-22T12:48:37",
        "upload_time_iso_8601": "2020-08-22T12:48:37.989141Z",
        "url": "https://files.pythonhosted.org/packages/10/d8/2693513d9048d18f4ad8fe25f9ba27dab589ba1da41c1f0c3e350fdefee6/ocrd_cor_asv_ann-0.1.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "13fcad6e66bf574121908bc058dc77e833293a456c0305a806ab9c95c8f7a29e",
          "md5": "dbe9a2c03cbcac0df4a51fea7c2dfa82",
          "sha256": "7183b8e44a784546c6b8a75ff15326fdd6fb6f337b442a28ffb6a6e7f86dabc6"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "dbe9a2c03cbcac0df4a51fea7c2dfa82",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70782,
        "upload_time": "2020-08-22T12:48:39",
        "upload_time_iso_8601": "2020-08-22T12:48:39.751730Z",
        "url": "https://files.pythonhosted.org/packages/13/fc/ad6e66bf574121908bc058dc77e833293a456c0305a806ab9c95c8f7a29e/ocrd_cor_asv_ann-0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9a8db72a92f8efab91a8de5d83b4ba076be95d3e948f0487b28d9ee71f637364",
          "md5": "215e19962eb17f41021bc30b82de0696",
          "sha256": "e6071794ef90cf71865f4f5a150e680ac0fbc7cfc9bbcd834e29ba0c9a4816d4"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.4-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "215e19962eb17f41021bc30b82de0696",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 76148,
        "upload_time": "2020-08-22T20:55:55",
        "upload_time_iso_8601": "2020-08-22T20:55:55.967479Z",
        "url": "https://files.pythonhosted.org/packages/9a/8d/b72a92f8efab91a8de5d83b4ba076be95d3e948f0487b28d9ee71f637364/ocrd_cor_asv_ann-0.1.4-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e9f9b3a591f635be850d644b9e31acce65f1d0ba6651bdf297464bfd25d07691",
          "md5": "965a1d7ca6a22fb0e4c84e9410a124e0",
          "sha256": "32023caf5db51aa9e98e9e16d1ce0216cda41c5ec1c235c26fb15fddf021ef21"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "965a1d7ca6a22fb0e4c84e9410a124e0",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70637,
        "upload_time": "2020-08-22T20:55:57",
        "upload_time_iso_8601": "2020-08-22T20:55:57.865926Z",
        "url": "https://files.pythonhosted.org/packages/e9/f9/b3a591f635be850d644b9e31acce65f1d0ba6651bdf297464bfd25d07691/ocrd_cor_asv_ann-0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.5": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6cab235e1c540bee94b7831086ebb92b68104ef5daae59e7a1038463711aec76",
          "md5": "8539d5a9ebceb5871f96ef38b849a5ac",
          "sha256": "173d65f3be3e6b52a200bcaf33bac53e2d6bfbfe151975e0094781a846c37a4c"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.5-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "8539d5a9ebceb5871f96ef38b849a5ac",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 76170,
        "upload_time": "2020-10-22T07:31:11",
        "upload_time_iso_8601": "2020-10-22T07:31:11.015192Z",
        "url": "https://files.pythonhosted.org/packages/6c/ab/235e1c540bee94b7831086ebb92b68104ef5daae59e7a1038463711aec76/ocrd_cor_asv_ann-0.1.5-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5c47cd74f0eefdf8d41fcea9b242b10700a80ab100ed9d2f996076b13ead6775",
          "md5": "6da5e11d54ba25a67ab7685dabb466ae",
          "sha256": "4f34299136a2f47b2be1f3b5e5b2d26c0ab9a686f3bde2728eba3118e02bcc96"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.5.tar.gz",
        "has_sig": false,
        "md5_digest": "6da5e11d54ba25a67ab7685dabb466ae",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70660,
        "upload_time": "2020-10-22T07:31:13",
        "upload_time_iso_8601": "2020-10-22T07:31:13.264087Z",
        "url": "https://files.pythonhosted.org/packages/5c/47/cd74f0eefdf8d41fcea9b242b10700a80ab100ed9d2f996076b13ead6775/ocrd_cor_asv_ann-0.1.5.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3ec4bd6e9f55511084f8a033968b58e1b9e405e3a8f8398fd3457008c53b5e79",
          "md5": "244fcdc2f717079af4897d0f7992a461",
          "sha256": "0db0fb146b72a4b66376e7b16022f2dbf9a2943411edb2b2bcb35773353d3639"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.6-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "244fcdc2f717079af4897d0f7992a461",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 76033,
        "upload_time": "2020-10-22T07:32:14",
        "upload_time_iso_8601": "2020-10-22T07:32:14.601432Z",
        "url": "https://files.pythonhosted.org/packages/3e/c4/bd6e9f55511084f8a033968b58e1b9e405e3a8f8398fd3457008c53b5e79/ocrd_cor_asv_ann-0.1.6-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "ae219e5c6f654519d13845bfb261483497d53fc7b0720c98f816c92196874ae1",
          "md5": "5ab3ebd0eb69a6909091dcbff34ac03e",
          "sha256": "be38445d1167611a86c7b1ac84f2da5e9c8a4d4fa94746197f73aa4797eaaf75"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "5ab3ebd0eb69a6909091dcbff34ac03e",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 70557,
        "upload_time": "2020-10-22T07:32:16",
        "upload_time_iso_8601": "2020-10-22T07:32:16.450017Z",
        "url": "https://files.pythonhosted.org/packages/ae/21/9e5c6f654519d13845bfb261483497d53fc7b0720c98f816c92196874ae1/ocrd_cor_asv_ann-0.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9dc398965170e2253559014e493bda315db3bc97e46d44e7fd3bbe833905815a",
          "md5": "b44d45a6b9ada4dabad6f0885b7af55e",
          "sha256": "d8340d9b0a11d1a21f3795cc8a7503c37c307cfdc11916a3f1ff21e1927c8887"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.7-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "b44d45a6b9ada4dabad6f0885b7af55e",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 79130,
        "upload_time": "2021-05-12T00:46:50",
        "upload_time_iso_8601": "2021-05-12T00:46:50.129346Z",
        "url": "https://files.pythonhosted.org/packages/9d/c3/98965170e2253559014e493bda315db3bc97e46d44e7fd3bbe833905815a/ocrd_cor_asv_ann-0.1.7-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e92f13b23d44727d92224bf87db7be48d5b38b8f1a75db88e3dff2f4ef17bf93",
          "md5": "a7c254c5cf7d2b74bc04c5d87e2fd7d2",
          "sha256": "0d7d0e7da42f72611325d41f632bb94b5c6d7d5844f572b6d19ec9f67270c85a"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "a7c254c5cf7d2b74bc04c5d87e2fd7d2",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 73047,
        "upload_time": "2021-05-12T00:46:52",
        "upload_time_iso_8601": "2021-05-12T00:46:52.406081Z",
        "url": "https://files.pythonhosted.org/packages/e9/2f/13b23d44727d92224bf87db7be48d5b38b8f1a75db88e3dff2f4ef17bf93/ocrd_cor_asv_ann-0.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.8": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "701246e1945b68fa77b34f2206b885685d1808000f8db0dbafafc95c8915927c",
          "md5": "2f23cf050a810eb4b227091f9ceba674",
          "sha256": "d2dd35d211625c9ae3bf24c9cdd9af42d582ca081455d4edb78885726dd36b18"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.8-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "2f23cf050a810eb4b227091f9ceba674",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 80930,
        "upload_time": "2021-05-19T10:03:32",
        "upload_time_iso_8601": "2021-05-19T10:03:32.431846Z",
        "url": "https://files.pythonhosted.org/packages/70/12/46e1945b68fa77b34f2206b885685d1808000f8db0dbafafc95c8915927c/ocrd_cor_asv_ann-0.1.8-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1c579523891d3e03f9b9fd6377475b3caedf3f68e271d4e8bc436ce9cf8896ea",
          "md5": "468f02efca38e364b0a2df907b3f073c",
          "sha256": "2ed1ff704fa16d2098dc20c76581bf51f3ed090d593378fe576819fbb7cd8771"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.8.tar.gz",
        "has_sig": false,
        "md5_digest": "468f02efca38e364b0a2df907b3f073c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 74879,
        "upload_time": "2021-05-19T10:03:34",
        "upload_time_iso_8601": "2021-05-19T10:03:34.637588Z",
        "url": "https://files.pythonhosted.org/packages/1c/57/9523891d3e03f9b9fd6377475b3caedf3f68e271d4e8bc436ce9cf8896ea/ocrd_cor_asv_ann-0.1.8.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.1.9": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1715214ff1824004992b0484fa6f9d22b19653ddb28b2cbf219fb3f1b52f2d53",
          "md5": "a763cafea97dc440448d158277a94fe5",
          "sha256": "930cea2320ddca3b0363f8d28d2d1187ed0b8532547818edbeb07991cb3ac9bd"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.9-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a763cafea97dc440448d158277a94fe5",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": null,
        "size": 81558,
        "upload_time": "2021-05-20T09:50:39",
        "upload_time_iso_8601": "2021-05-20T09:50:39.610714Z",
        "url": "https://files.pythonhosted.org/packages/17/15/214ff1824004992b0484fa6f9d22b19653ddb28b2cbf219fb3f1b52f2d53/ocrd_cor_asv_ann-0.1.9-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "702dc2cd1390ffa91a2bd2f32ac713c11c5f431f7fb1caf585f22ca80d744c1c",
          "md5": "763b4536cb7b922fd99a59cea66f58cf",
          "sha256": "8c62197303ecc7cfb80ec4d67a47517ebe264564767a42972f990cbbc1b12728"
        },
        "downloads": -1,
        "filename": "ocrd_cor_asv_ann-0.1.9.tar.gz",
        "has_sig": false,
        "md5_digest": "763b4536cb7b922fd99a59cea66f58cf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 75291,
        "upload_time": "2021-05-20T09:50:41",
        "upload_time_iso_8601": "2021-05-20T09:50:41.803858Z",
        "url": "https://files.pythonhosted.org/packages/70/2d/c2cd1390ffa91a2bd2f32ac713c11c5f431f7fb1caf585f22ca80d744c1c/ocrd_cor_asv_ann-0.1.9.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "79ecfec93e659c49a23695fe830f9d8a675776523d5c494c02a986f3bb54670b",
        "md5": "b44de64c62600ceba76c9b299db11d30",
        "sha256": "e995c76a51cf8fdeec8a2617c00c5d79ae99f8d792d55ec8503b8accfcb67c8c"
      },
      "downloads": -1,
      "filename": "ocrd_cor_asv_ann-0.1.14-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "b44de64c62600ceba76c9b299db11d30",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": null,
      "size": 97274,
      "upload_time": "2022-09-28T15:49:09",
      "upload_time_iso_8601": "2022-09-28T15:49:09.365915Z",
      "url": "https://files.pythonhosted.org/packages/79/ec/fec93e659c49a23695fe830f9d8a675776523d5c494c02a986f3bb54670b/ocrd_cor_asv_ann-0.1.14-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "68bc6c06042c47dfab9023c84b896c0a5ce9170561e9f7e11ddd666697996791",
        "md5": "64ae32ea72736329dc00a9d41f195710",
        "sha256": "0f9fb2cd7168d4c13ec508d54916b4815ff9b13d670bcf33889baf1d3a9f59be"
      },
      "downloads": -1,
      "filename": "ocrd_cor_asv_ann-0.1.14.tar.gz",
      "has_sig": false,
      "md5_digest": "64ae32ea72736329dc00a9d41f195710",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 102248,
      "upload_time": "2022-09-28T15:49:12",
      "upload_time_iso_8601": "2022-09-28T15:49:12.844779Z",
      "url": "https://files.pythonhosted.org/packages/68/bc/6c06042c47dfab9023c84b896c0a5ce9170561e9f7e11ddd666697996791/ocrd_cor_asv_ann-0.1.14.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}