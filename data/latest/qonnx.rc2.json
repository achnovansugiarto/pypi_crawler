{
  "info": {
    "author": "",
    "author_email": "",
    "bugtrack_url": null,
    "classifiers": [
      "Development Status :: 4 - Beta",
      "Programming Language :: Python"
    ],
    "description": "# QONNX: Arbitrary-Precision Quantized Neural Networks in ONNX\n\n[![ReadTheDocs](https://readthedocs.org/projects/qonnx/badge/?version=latest&style=plastic)](http://qonnx.readthedocs.io/)\n[![GitHub Discussions](https://img.shields.io/github/discussions/fastmachinelearning/qonnx)](https://github.com/fastmachinelearning/qonnx/discussions)\n![Tests](https://github.com/fastmachinelearning/qonnx/actions/workflows/test.yml/badge.svg)\n![Code style](https://img.shields.io/badge/code%20style-black-000000.svg)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7622236.svg)](https://doi.org/10.5281/zenodo.7622236)\n\n<img align=\"left\" src=\"https://xilinx.github.io/finn/img/TFC_1W2A.onnx.png\" alt=\"QONNX example\" style=\"margin-right: 20px\" width=\"200\"/>\n\n\nQONNX (Quantized ONNX) introduces three new custom operators -- [`Quant`](docs/qonnx-custom-ops/quant_op.md), [`BipolarQuant`](docs/qonnx-custom-ops/bipolar_quant_op.md), and [`Trunc`](docs/qonnx-custom-ops/trunc_op.md) -- in order to represent arbitrary-precision uniform quantization in ONNX. This enables:\n* Representation of binary, ternary, 3-bit, 4-bit, 6-bit or any other quantization.\n* Quantization is an operator itself, and can be applied to any parameter or layer input.\n* Flexible choices for scaling factor and zero-point granularity.\n* Quantized values are carried using standard `float` datatypes to remain ONNX protobuf-compatible.\n\nThis repository contains a set of Python utilities to work with QONNX models, including but not limited to:\n* executing QONNX models for (slow) functional verification\n* shape inference, constant folding and other basic optimizations\n* summarizing the inference cost of a QONNX model in terms of mixed-precision MACs, parameter and activation volume\n* Python infrastructure for writing transformations and defining executable, shape-inferencable custom ops\n* (experimental) data layout conversion from standard ONNX NCHW to custom QONNX NHWC ops\n\n## Quickstart\n\n### Operator definitions\n\n* [Quant](docs/qonnx-custom-ops/quant_op.md) for 2-to-arbitrary-bit quantization, with scaling and zero-point\n* [BipolarQuant](docs/qonnx-custom-ops/bipolar_quant_op.md)  for 1-bit (bipolar) quantization, with scaling and zero-point\n* [Trunc](docs/qonnx-custom-ops/trunc_op.md) for truncating to a specified number of bits, with scaling and zero-point\n\n### Installation\n\n`pip install qonnx`\n\n### Export, Import and Model Zoo\n\nThe following quantization-aware training (QAT) frameworks support exporting to QONNX:\n\n* [Brevitas](https://github.com/Xilinx/brevitas)\n* [QKeras](https://github.com/google/qkeras) (beta, see [this PR](https://github.com/fastmachinelearning/qonnx/pull/7))\n* [HAWQ](https://github.com/Zhen-Dong/HAWQ/tree/main/utils/export)\n* [<your NN quantization framework here? please get in touch!>](https://github.com/fastmachinelearning/qonnx/discussions)\n\nThe following NN inference frameworks support importing QONNX models for deployment:\n\n* [FINN](https://github.com/Xilinx/finn) (FPGA dataflow-style)\n* [hls4ml](https://github.com/fastmachinelearning/hls4ml) (FPGA dataflow-style)\n* [<your NN deployment framework here? please get in touch!>](https://github.com/fastmachinelearning/qonnx/discussions)\n\nHead to the [QONNX model zoo](https://github.com/fastmachinelearning/QONNX_model_zoo) to download pre-trained QONNX models on various datasets.\n\n### Model Visualization\n\nWe recommend [Netron](https://netron.app/) for visualizing QONNX models.\n\n### Executing ONNX graph with QONNX custom nodes\n\nUsing the `qonnx-exec` command line utility, with top-level inputs supplied from `in0.npy` and `in1.npy`:\n\n`qonnx-exec my-qonnx-model.onnx in0.npy in1.npy`\n\nUsing the Python API:\n\n```\nfrom qonnx.core.modelwrapper import ModelWrapper\nfrom qonnx.core.onnx_exec import execute_onnx\n\nmodel = ModelWrapper(\"my-qonnx-model.onnx\")\nidict = {\"in0\" : np.load(\"in0.npy), \"in1\" : np.load(\"in1.npy\")}\nodict = execute_onnx(idict)\n```\n\n### Calculate inference cost for QONNX model\n\nUsing the `qonnx-inference-cost` command line utility:\n\n`qonnx-inference-cost CNV_2W2A.onnx`\n\nWhich will print a inference cost dictionary like the following:\n\n```\nInference cost for CNV_2W2A.onnx\n{\n  \"discount_sparsity\": true,    # discount MAC counts by layer sparsity (disregard zero-valued MACs)\n  # mem_o_X: number of layer outputs with datatype X\n  \"mem_o_FLOAT32\": 57600.0,     # number of FLOAT32 output elements\n  \"mem_o_INT32\": 85002.0,       # number of INT32 output elements\n  # mem_o_X: number of layer parameters (weights) with datatype X\n  \"mem_w_INT2\": 1144512.0,      # number of INT2 parameters (weights)\n  # op_mac_X_Y: number of MAC operations, datatype X by datatype Y\n  \"op_mac_FLOAT32_INT2\": 1555200.0, # number of float32 x int2 MACs\n  \"op_mac_INT2_INT2\": 57906176.0,   # number of int2 x int2 MACs\n  \"total_bops\": 331157504.0,        # total number of MACs normalized to bit-ops (BOPS)\n  \"total_mem_o_bits\": 4563264.0,    # total number of bits for layer outputs\n  \"total_mem_w_bits\": 2289024.0,    # total number of bits for layer parameters\n  \"unsupported\": \"set()\"\n}\n```\n\nYou can read more about the BOPS metric in [this paper](https://www.frontiersin.org/articles/10.3389/frai.2021.676564/full), Section 4.2 Bit Operations.\n\n### Development\n\nInstall in editable mode in a venv:\n\n```\ngit clone https://github.com/fastmachinelearning/qonnx\ncd qonnx\nvirtualenv -p python3.8 venv\nsource venv/bin/activate\npip install -e .[qkeras,testing]\n```\n\nRun entire test suite, parallelized across CPU cores:\n```\npytest -n auto --verbose\n```\n\nRun a particular test and fall into pdb if it fails:\n```\npytest --pdb -k \"test_extend_partition.py::test_extend_partition[extend_id1-2]\"\n```\n\n## Why QONNX?\n\nThe QONNX representation has several advantages compared to other alternatives, as summarized in the table below.\nThese include a compact but flexible, single-node quantization representation that avoids operator duplication\nand can support arbitrary precision up to the container datatype limit.\n\n<img align=\"left\" src=\"https://raw.githubusercontent.com/fastmachinelearning/qonnx/main/docs/qonnx-comparison.png\" alt=\"QONNX comparison table\" style=\"margin-right: 20px\" />\n\n## Community\n\nThe QONNX efforts were started by the FINN and hls4ml communities working together to create a common, arbitrary-precision representation that both frameworks could ingest. However, QONNX aims to build an open-source community for practitioners and researchers working with mixed-precision quantized neural networks by providing useful tools and a [discussion forum](https://github.com/fastmachinelearning/qonnx/discussions).\n\n<div>\n<img src=https://raw.githubusercontent.com/Xilinx/finn/github-pages/docs/img/finn-logo.png height=100/>\n<img src=\"https://fastmachinelearning.github.io/hls4ml/img/logo.jpg\" alt=\"hls4ml\" height=\"128\"/>\n</div>\n\n## Resources\n\nYou can read more about QONNX in [this paper](https://arxiv.org/abs/2206.07527). If you find QONNX useful in your work, please consider citing:\n\n```bibtex\n@inproceedings{Pappalardo:2022nxk,\n    author = \"Pappalardo, Alessandro and Umuroglu, Yaman and Blott, Michaela and Mitrevski, Jovan and Hawks, Ben and Tran, Nhan and Loncar, Vladimir and Summers, Sioni and Borras, Hendrik and Muhizi, Jules and Trahms, Matthew and Hsu, Shih-Chieh Hsu and Hauck, Scott and Duarte, Javier\"\n    title = \"{QONNX: Representing Arbitrary-Precision Quantized Neural Networks}\",\n    booktitle = \"{4th Workshop on Accelerated Machine Learning (AccML) at HiPEAC 2022 Conference}\",\n    eprint = \"2206.07527\",\n    archivePrefix = \"arXiv\",\n    primaryClass = \"cs.LG\",\n    reportNumber = \"FERMILAB-CONF-22-471-SCD\",\n    month = \"6\",\n    year = \"2022\",\n    url = \"https://accml.dcs.gla.ac.uk/papers/2022/4thAccML_paper_1(12).pdf\"\n}\n\n@software{yaman_umuroglu_2023_7622236,\n  author       = \"Umuroglu, Yaman and Borras, Hendrik and Loncar, Vladimir, and Summers, Sioni and Duarte, Javier\",\n  title        = \"fastmachinelearning/qonnx\",\n  month        = {06},\n  year         = 2022,\n  publisher    = {Zenodo},\n  doi          = {10.5281/zenodo.7622236},\n  url          = {https://github.com/fastmachinelearning/qonnx}\n}\n```\n\n\n",
    "description_content_type": "text/markdown; charset=UTF-8",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/fastmachinelearning/qonnx",
    "keywords": "",
    "license": "Apache-2.0",
    "maintainer": "",
    "maintainer_email": "",
    "name": "qonnx",
    "package_url": "https://pypi.org/project/qonnx/",
    "platform": "any",
    "project_url": "https://pypi.org/project/qonnx/",
    "project_urls": {
      "Documentation": "https://pyscaffold.org/",
      "Homepage": "https://github.com/fastmachinelearning/qonnx"
    },
    "release_url": "https://pypi.org/project/qonnx/0.2.0/",
    "requires_dist": [
      "bitstring (>=3.1.7)",
      "clize (==4.1.1)",
      "numpy (==1.24.1)",
      "onnx (==1.13.0)",
      "onnxruntime (==1.11.1)",
      "protobuf (==3.20.3)",
      "sigtools (==2.0.3)",
      "toposort (==1.7.0)",
      "importlib-metadata ; python_version < \"3.8\"",
      "jupyter ; extra == 'notebooks'",
      "netron ; extra == 'notebooks'",
      "QKeras (==0.9.0) ; extra == 'qkeras'",
      "pyparsing ; extra == 'qkeras'",
      "tensorflow (==2.7.0) ; extra == 'qkeras'",
      "tf2onnx (>=1.12.1) ; extra == 'qkeras'",
      "pytest ; extra == 'testing'",
      "pytest-cov ; extra == 'testing'",
      "pytest-randomly ; extra == 'testing'",
      "pytest-xdist ; extra == 'testing'",
      "setuptools ; extra == 'testing'"
    ],
    "requires_python": "",
    "summary": "Frontend and utilities for QONNX",
    "version": "0.2.0",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 16890100,
  "releases": {
    "0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b729cbc363fd0f28b69d67b361912c35c931fef84d4553b1d358d0530db58119",
          "md5": "aabd7048bd55a7702a69756048597a61",
          "sha256": "86762582c8afd5cf18a09f855a1b2d74e1dd8b133bb2a97b72261fd513580e1a"
        },
        "downloads": -1,
        "filename": "qonnx-0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "aabd7048bd55a7702a69756048597a61",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 490667,
        "upload_time": "2022-06-24T16:41:37",
        "upload_time_iso_8601": "2022-06-24T16:41:37.334576Z",
        "url": "https://files.pythonhosted.org/packages/b7/29/cbc363fd0f28b69d67b361912c35c931fef84d4553b1d358d0530db58119/qonnx-0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "0.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7c837e20005fffc404fbbf9c5d0d00134f105ee560816fb29f0402b5eb617a85",
          "md5": "d1c7af339df8e8d0d010063c8c7f037a",
          "sha256": "6f955430a46bb8ecf46c39227ffe26d82261c9c1fce276baa8d309520c3b8621"
        },
        "downloads": -1,
        "filename": "qonnx-0.2.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "d1c7af339df8e8d0d010063c8c7f037a",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 169068,
        "upload_time": "2023-02-16T16:03:08",
        "upload_time_iso_8601": "2023-02-16T16:03:08.185965Z",
        "url": "https://files.pythonhosted.org/packages/7c/83/7e20005fffc404fbbf9c5d0d00134f105ee560816fb29f0402b5eb617a85/qonnx-0.2.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "31c327d6d895bd2f3279d024c35611584c0b8ce204bc87eaa28dd7b004d9b15b",
          "md5": "91c0c49aa3494b6b0c89cc630caf1cae",
          "sha256": "3347e6cec0b51cf369d45b64818d9127f608872347e8c7cb5824c94c0b1ce3b0"
        },
        "downloads": -1,
        "filename": "qonnx-0.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "91c0c49aa3494b6b0c89cc630caf1cae",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 510568,
        "upload_time": "2023-02-16T16:03:10",
        "upload_time_iso_8601": "2023-02-16T16:03:10.583460Z",
        "url": "https://files.pythonhosted.org/packages/31/c3/27d6d895bd2f3279d024c35611584c0b8ce204bc87eaa28dd7b004d9b15b/qonnx-0.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "7c837e20005fffc404fbbf9c5d0d00134f105ee560816fb29f0402b5eb617a85",
        "md5": "d1c7af339df8e8d0d010063c8c7f037a",
        "sha256": "6f955430a46bb8ecf46c39227ffe26d82261c9c1fce276baa8d309520c3b8621"
      },
      "downloads": -1,
      "filename": "qonnx-0.2.0-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "d1c7af339df8e8d0d010063c8c7f037a",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 169068,
      "upload_time": "2023-02-16T16:03:08",
      "upload_time_iso_8601": "2023-02-16T16:03:08.185965Z",
      "url": "https://files.pythonhosted.org/packages/7c/83/7e20005fffc404fbbf9c5d0d00134f105ee560816fb29f0402b5eb617a85/qonnx-0.2.0-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "31c327d6d895bd2f3279d024c35611584c0b8ce204bc87eaa28dd7b004d9b15b",
        "md5": "91c0c49aa3494b6b0c89cc630caf1cae",
        "sha256": "3347e6cec0b51cf369d45b64818d9127f608872347e8c7cb5824c94c0b1ce3b0"
      },
      "downloads": -1,
      "filename": "qonnx-0.2.0.tar.gz",
      "has_sig": false,
      "md5_digest": "91c0c49aa3494b6b0c89cc630caf1cae",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 510568,
      "upload_time": "2023-02-16T16:03:10",
      "upload_time_iso_8601": "2023-02-16T16:03:10.583460Z",
      "url": "https://files.pythonhosted.org/packages/31/c3/27d6d895bd2f3279d024c35611584c0b8ce204bc87eaa28dd7b004d9b15b/qonnx-0.2.0.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}