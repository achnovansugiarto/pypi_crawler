{
  "info": {
    "author": "irshadbhat",
    "author_email": "bhatirshad127@gmail.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 2",
      "Programming Language :: Python :: 2.7",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.4",
      "Programming Language :: Python :: 3.5",
      "Programming Language :: Python :: 3.6",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "Polyglot Tokenizer\n==================\n\n\nTokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.\n\n\nInstallation\n------------\n\n::\n\n    pip install polyglot-tokenizer\n\nExamples\n--------\n\nWithin Python\n^^^^^^^^^^^^^\n\n\n.. code:: python\n\n    >>> from __future__ import unicode_literals\n    >>> from polyglot_tokenizer import Tokenizer\n    >>> tk = Tokenizer(lang='en', smt=True) #smt is a flag for social-media-text\n    >>> text = \"RT @BJP_RSS Crack down on Black money.India slides to 75th slot on Swiss bank money list #ModiForeignAchievements @RituRathaur https://t.c…\"\n    >>> tk.tokenize(text)\n    ['RT', '@BJP_RSS', 'Crack', 'down', 'on', 'Black', 'money', '.', 'India', 'slides', 'to', '75th', 'slot', 'on', 'Swiss', 'bank', 'money', 'list', '#ModiForeignAchievements', '@RituRathaur', 'https://t.c…']\n    >>> tk = Tokenizer(lang='hi')\n    >>> tk.tokenize(\"22 साल के लंबे इंतजार के बाद आखिरकार हॉलीवुड स्टार लियोनार्डो डिकैप्रियो को अपनी पहली ऑस्कर ट्रॉफी\"\n    ...             \" मिल चुकी है। उन्हें ये अवॉर्ड अपनी फिल्म ‘द रेवेनेंट’ में ह्यूज ग्लास के किरदार के लिए मिला, लेकिन उनके\"\n    ...             \" के लिए रोल निभाना आसान नहीं था।\")\n    ['22', 'साल', 'के', 'लंबे', 'इंतजार', 'के', 'बाद', 'आखिरकार', 'हॉलीवुड', 'स्टार', 'लियोनार्डो', 'डिकैप्रियो', 'को', 'अपनी', 'पहली', 'ऑस्कर', 'ट्रॉफी', 'मिल', 'चुकी', 'है', '।', 'उन्हें', 'ये', 'अवॉर्ड', 'अपनी', 'फिल्म', \"'\", 'द', 'रेवेनेंट', \"'\", 'में', 'ह्यूज', 'ग्लास', 'के', 'किरदार', 'के', 'लिए', 'मिला', ',', 'लेकिन', 'उनके', 'के', 'लिए', 'रोल', 'निभाना', 'आसान', 'नहीं', 'था', '।']\n    >>> tk = Tokenizer(lang='hi', split_sen=True)\n    >>> tk.tokenize(\"22 साल के लंबे इंतजार के बाद आखिरकार हॉलीवुड स्टार लियोनार्डो डिकैप्रियो को अपनी पहली ऑस्कर ट्रॉफी\"\n    ...             \" मिल चुकी है। उन्हें ये अवॉर्ड अपनी फिल्म ‘द रेवेनेंट’ में ह्यूज ग्लास के किरदार के लिए मिला, लेकिन उनके\"\n    ...             \" के लिए रोल निभाना आसान नहीं था। फिल्म एक सीन के लिए लियोनार्डो को भैंस का कच्चा लीवर खाना\"\n    ...             \" पड़ा था। जबकि असल जिंदगी में वो पूरी तरह शाकाहारी हैं। हालांकि इस सीन के लिए पहले लियोनार्डो को\"\n    ...             \" मांस जैसे दिखने वाली चीज दी गई थी, लेकिन उन्हें लगा कि ऐसा करना गलत होगा। फिल्म के लिए इम्पोर्ट\"\n    ...             \" की गई चीटियां...\")\n    [['22', 'साल', 'के', 'लंबे', 'इंतजार', 'के', 'बाद', 'आखिरकार', 'हॉलीवुड', 'स्टार', 'लियोनार्डो', 'डिकैप्रियो', 'को', 'अपनी', 'पहली', 'ऑस्कर', 'ट्रॉफी', 'मिल', 'चुकी', 'है', '।'], ['उन्हें', 'ये', 'अवॉर्ड', 'अपनी', 'फिल्म', \"'\", 'द', 'रेवेनेंट', \"'\", 'में', 'ह्यूज', 'ग्लास', 'के', 'किरदार', 'के', 'लिए', 'मिला', ',', 'लेकिन', 'उनके', 'के', 'लिए', 'रोल', 'निभाना', 'आसान', 'नहीं', 'था', '।'], ['फिल्म', 'एक', 'सीन', 'के', 'लिए', 'लियोनार्डो', 'को', 'भैंस', 'का', 'कच्चा', 'लीवर', 'खाना', 'पड़ा', 'था', '।'], ['जबकि', 'असल', 'जिंदगी', 'में', 'वो', 'पूरी', 'तरह', 'शाकाहारी', 'हैं', '।'], ['हालांकि', 'इस', 'सीन', 'के', 'लिए', 'पहले', 'लियोनार्डो', 'को', 'मांस', 'जैसे', 'दिखने', 'वाली', 'चीज', 'दी', 'गई', 'थी', ',', 'लेकिन', 'उन्हें', 'लगा', 'कि', 'ऐसा', 'करना', 'गलत', 'होगा', '।'], ['फिल्म', 'के', 'लिए', 'इम्पोर्ट', 'की', 'गई', 'चीटियां', '...']]\n\n\nFrom Console\n^^^^^^^^^^^^\n\n.. parsed-literal::\n\n    polyglot-tokenizer --h\n\n    usage: polyglot-tokenizer [-h] [-v] [-i] [-s] [-t] [-o] [-l]\n    \n    Tokenizer for world's most spoken languages\n\n    \n    optional arguments:\n      -h, --help            show this help message and exit\n      -v, --version         show program's version number and exit\n      -i , --input          <input-file>\n      -s, --split-sentences\n                            set this flag to apply sentence segmentation\n      -t, --social-media-test\n                            set this flag if the input file contains social media\n                            text like twitter, facebook and whatsapp\n      -o , --output         <output-file>\n      -l , --language       select language (2 letter ISO-639 code) {hi, ur, bn,\n                            as, gu, ml, pa, te, ta, kn, or, mr, cu, myv, nn, yi,\n                            ne, bo, br, ks, en, es, ca, cs, de, el, en, fi, da,\n                            eu, kok, nb, uz, fr, ga, hu, is, it, lt, lv, nl, pl,\n                            pt, ro, ru, sk, bm, yue, mk, ku, sl, sv, zh, et, fo,\n                            gl, hsb, af, ar, be, hy, bg, ka, ug, hr, mn, tk, kk,\n                            ky, la, no, fa, uk, tl, tr, vi, yo, ko, got, ckb, he,\n                            id, sr}\n\n    Example ::\n\n    polyglot-tokenizer < raw_file.txt -l en -s > tokenized.txt\n\n\n\n\n",
    "description_content_type": "text/x-rst",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/irshadbhat/polyglot-tokenizer",
    "keywords": "nlp,polyglot,tokenizer",
    "license": "MIT",
    "maintainer": "irshadbhat",
    "maintainer_email": "bhatirshad127@gmail.com",
    "name": "polyglot-tokenizer",
    "package_url": "https://pypi.org/project/polyglot-tokenizer/",
    "platform": "",
    "project_url": "https://pypi.org/project/polyglot-tokenizer/",
    "project_urls": {
      "Homepage": "https://github.com/irshadbhat/polyglot-tokenizer",
      "Repository": "https://github.com/irshadbhat/polyglot-tokenizer"
    },
    "release_url": "https://pypi.org/project/polyglot-tokenizer/2.0.2/",
    "requires_dist": [
      "six (>=1.12,<2.0)",
      "pbr (>=2.0,<3.0)"
    ],
    "requires_python": "",
    "summary": "Tokenizer for world's most spoken languages and social media texts like Facebook, Twitter etc.",
    "version": "2.0.2",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 10596923,
  "releases": {
    "2.0.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e69fa27bf3a14f49633b8873fba28da4cc7b2d5e601794f16c2a2724f6177d0a",
          "md5": "da6284e6406086b338fea30c41290f20",
          "sha256": "8128557aebb16029e53801b9ed57800ee2cbcb102e160559d9a5016ab673eda0"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.0-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "da6284e6406086b338fea30c41290f20",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 131697,
        "upload_time": "2021-06-03T15:45:26",
        "upload_time_iso_8601": "2021-06-03T15:45:26.504667Z",
        "url": "https://files.pythonhosted.org/packages/e6/9f/a27bf3a14f49633b8873fba28da4cc7b2d5e601794f16c2a2724f6177d0a/polyglot_tokenizer-2.0.0-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "262e3be7828e741f8b0434432b021c8ca561ed19e969f300a2e8416d6a06c393",
          "md5": "5a5a4a60ae02601a9d3c001e14b39409",
          "sha256": "90b8fae690ea30268541619e43307d2f33343cbcba94a3fce23755294c5477bc"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.0.tar.gz",
        "has_sig": false,
        "md5_digest": "5a5a4a60ae02601a9d3c001e14b39409",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 100807,
        "upload_time": "2018-12-16T17:08:31",
        "upload_time_iso_8601": "2018-12-16T17:08:31.786098Z",
        "url": "https://files.pythonhosted.org/packages/26/2e/3be7828e741f8b0434432b021c8ca561ed19e969f300a2e8416d6a06c393/polyglot-tokenizer-2.0.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "71658189f4ba5238f1072655408af4179205c0ed5a224d10da3af487786f1dbd",
          "md5": "5b0ab3722c221d532cd41406add6e028",
          "sha256": "0e95e57e50079db8c952288d8d5d5c5a8bb2655d43b8c25f5e8f2bbae7566636"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5b0ab3722c221d532cd41406add6e028",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 319691,
        "upload_time": "2019-01-18T09:57:07",
        "upload_time_iso_8601": "2019-01-18T09:57:07.805642Z",
        "url": "https://files.pythonhosted.org/packages/71/65/8189f4ba5238f1072655408af4179205c0ed5a224d10da3af487786f1dbd/polyglot_tokenizer-2.0.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "2abd04d80180c8271df82241507d9c8001af03379e725fb8e78a725fd3ca0d70",
          "md5": "ab4f17076f0c2e3ab35aafacd98b3a4b",
          "sha256": "48e55f45cb2d22657b8fdff934492350d67ded44acfc38b0d619e39d0b3404be"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.1.tar.gz",
        "has_sig": false,
        "md5_digest": "ab4f17076f0c2e3ab35aafacd98b3a4b",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 96643,
        "upload_time": "2019-01-18T09:57:05",
        "upload_time_iso_8601": "2019-01-18T09:57:05.230212Z",
        "url": "https://files.pythonhosted.org/packages/2a/bd/04d80180c8271df82241507d9c8001af03379e725fb8e78a725fd3ca0d70/polyglot-tokenizer-2.0.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "07b2f6aadcb2f08b4f9f92d51a5869a6f52ec552b44e7a29edde85255cfd63ca",
          "md5": "56ee44a235561c836b7eb399d604ba2c",
          "sha256": "754773355dbabd6f0d42cf5b62144585d3bffc83921304e0d3076873ad9b7574"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.1.1-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "56ee44a235561c836b7eb399d604ba2c",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 321574,
        "upload_time": "2019-01-18T10:14:48",
        "upload_time_iso_8601": "2019-01-18T10:14:48.705688Z",
        "url": "https://files.pythonhosted.org/packages/07/b2/f6aadcb2f08b4f9f92d51a5869a6f52ec552b44e7a29edde85255cfd63ca/polyglot_tokenizer-2.0.1.1-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "7b7bd25a8c3da0a313f14822698b6d83d19e316d8a11a2251bd07224dad1377d",
          "md5": "f30fea389792e48a6ede2e70dd2d659c",
          "sha256": "b24ccff698adae30fffbc7eaa45007f13a6972be6bb5ab7584390dc65a29ad3f"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.1.1.tar.gz",
        "has_sig": false,
        "md5_digest": "f30fea389792e48a6ede2e70dd2d659c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 100837,
        "upload_time": "2019-01-18T10:14:45",
        "upload_time_iso_8601": "2019-01-18T10:14:45.870405Z",
        "url": "https://files.pythonhosted.org/packages/7b/7b/d25a8c3da0a313f14822698b6d83d19e316d8a11a2251bd07224dad1377d/polyglot-tokenizer-2.0.1.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "39c6a6d50d8ef3f6e0bb9e793b8eabc493f15347bd67459fcdf64dcbb53322d7",
          "md5": "6a79b6ca38fab53dd26190a0e554082e",
          "sha256": "f262a2747ac4f2fc736ddc3e684f97a40651b1119ca7bd1881d95599b39ff1ff"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.1.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "6a79b6ca38fab53dd26190a0e554082e",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 321577,
        "upload_time": "2019-01-18T10:41:30",
        "upload_time_iso_8601": "2019-01-18T10:41:30.530110Z",
        "url": "https://files.pythonhosted.org/packages/39/c6/a6d50d8ef3f6e0bb9e793b8eabc493f15347bd67459fcdf64dcbb53322d7/polyglot_tokenizer-2.0.1.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "563f4bbbf9d6b8be7b30f484bd75e4d4f6fa721caf052edbc8b8d45d441128cb",
          "md5": "e80581235a90382e74a11b15bb8b4cf9",
          "sha256": "bcf65d0f26d05af231f9f2ab92d9891c2e42e3a98b0e12c2917977a04122c7a9"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.1.2.tar.gz",
        "has_sig": false,
        "md5_digest": "e80581235a90382e74a11b15bb8b4cf9",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 101291,
        "upload_time": "2019-01-18T10:41:27",
        "upload_time_iso_8601": "2019-01-18T10:41:27.716681Z",
        "url": "https://files.pythonhosted.org/packages/56/3f/4bbbf9d6b8be7b30f484bd75e4d4f6fa721caf052edbc8b8d45d441128cb/polyglot-tokenizer-2.0.1.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "cb440786a527c83e772216e3b05970e89d8ec0c300b18fa24d834b276833d2c1",
          "md5": "f1ba3cba7c092cc0eac684658c5882c0",
          "sha256": "9b611fc8fc3305b174916c65e034079741f804d434b4ba1149f6ce4820c665db"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.1.3-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "f1ba3cba7c092cc0eac684658c5882c0",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 321887,
        "upload_time": "2019-01-18T15:38:29",
        "upload_time_iso_8601": "2019-01-18T15:38:29.143316Z",
        "url": "https://files.pythonhosted.org/packages/cb/44/0786a527c83e772216e3b05970e89d8ec0c300b18fa24d834b276833d2c1/polyglot_tokenizer-2.0.1.3-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "5802123c1bc468777501162021a02a17d90362a7def63057b5f73909e1df987a",
          "md5": "bb622e51f73b32dc9459662b2309a366",
          "sha256": "4bcf3f2bc3fe7852b221c31b163d743c4d04e1e12ab04f2dffbc97f30f537c40"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.1.3.tar.gz",
        "has_sig": false,
        "md5_digest": "bb622e51f73b32dc9459662b2309a366",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 101075,
        "upload_time": "2019-01-18T15:38:25",
        "upload_time_iso_8601": "2019-01-18T15:38:25.492964Z",
        "url": "https://files.pythonhosted.org/packages/58/02/123c1bc468777501162021a02a17d90362a7def63057b5f73909e1df987a/polyglot-tokenizer-2.0.1.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1.4": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "aa3037ce725b650240b390bcf4c509c9edb80e754810e361153e93447dd2e703",
          "md5": "aa9222de23bf17ff0590b7f335e52671",
          "sha256": "9f6f3de28fb52d0161acadb6c072229eabcbb3c0a3c46c278c93b1da3dccfa08"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.1.4-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "aa9222de23bf17ff0590b7f335e52671",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 321877,
        "upload_time": "2019-01-18T16:32:45",
        "upload_time_iso_8601": "2019-01-18T16:32:45.023674Z",
        "url": "https://files.pythonhosted.org/packages/aa/30/37ce725b650240b390bcf4c509c9edb80e754810e361153e93447dd2e703/polyglot_tokenizer-2.0.1.4-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a621bc881d90d6f499c886412938a36391044585db10fea0f539af3d3d39588a",
          "md5": "2297566db6440a51702299616395cc4c",
          "sha256": "22db75fc256286d74dc4f9260bdfac5f81ab906b6db5f6128d6be05790f0eae3"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.1.4.tar.gz",
        "has_sig": false,
        "md5_digest": "2297566db6440a51702299616395cc4c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 101052,
        "upload_time": "2019-01-18T16:32:41",
        "upload_time_iso_8601": "2019-01-18T16:32:41.403933Z",
        "url": "https://files.pythonhosted.org/packages/a6/21/bc881d90d6f499c886412938a36391044585db10fea0f539af3d3d39588a/polyglot-tokenizer-2.0.1.4.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1.6": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "8a1f16623d6c6f72d87a0f4643f6277eaeff6902086ec51982e65d31a1096436",
          "md5": "3d1e3c9a0a8a792b9653b978f6b6a4d3",
          "sha256": "1f7d58b29488507228bcf00269d7402b2b16c1a5a297a8d683be45baefb7c577"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.1.6-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3d1e3c9a0a8a792b9653b978f6b6a4d3",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": ">=2.7,<3.0",
        "size": 107015,
        "upload_time": "2021-06-04T06:51:48",
        "upload_time_iso_8601": "2021-06-04T06:51:48.349704Z",
        "url": "https://files.pythonhosted.org/packages/8a/1f/16623d6c6f72d87a0f4643f6277eaeff6902086ec51982e65d31a1096436/polyglot_tokenizer-2.0.1.6-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "af0dea9d4f80a84f443dcb313efbe953d013f90dd72747027ca679d66384c9a6",
          "md5": "3635348b5ad48cba8fa256666d66c4b8",
          "sha256": "9cf84c3d81d597a8ad2837db510fa2d8250b7e89a0e0ebdc7cf64a491c333adb"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.1.6.tar.gz",
        "has_sig": false,
        "md5_digest": "3635348b5ad48cba8fa256666d66c4b8",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=2.7,<3.0",
        "size": 97302,
        "upload_time": "2021-06-04T06:51:44",
        "upload_time_iso_8601": "2021-06-04T06:51:44.762788Z",
        "url": "https://files.pythonhosted.org/packages/af/0d/ea9d4f80a84f443dcb313efbe953d013f90dd72747027ca679d66384c9a6/polyglot-tokenizer-2.0.1.6.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.1.7": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e9fae6eb12a916e0519722101a2f88a3449c53ed0137d481c3ca2186076e0e20",
          "md5": "0ef32c94f2da31b30c9de60774cc86fb",
          "sha256": "2905b89bc263a4717e2e33569211e7cf99f4e456a7c3aa707b141eba7d79d453"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.1.7-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "0ef32c94f2da31b30c9de60774cc86fb",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 107017,
        "upload_time": "2021-06-09T05:01:58",
        "upload_time_iso_8601": "2021-06-09T05:01:58.330071Z",
        "url": "https://files.pythonhosted.org/packages/e9/fa/e6eb12a916e0519722101a2f88a3449c53ed0137d481c3ca2186076e0e20/polyglot_tokenizer-2.0.1.7-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "6e237a6961dabebbc8612c29ebb32e40ac0c9c2d90bd6b4850170bb235464507",
          "md5": "f072dfdff96d1edc685f5815e1fb0936",
          "sha256": "4a40e9405661fcc13da1ce95bec41cd51d536e06acb2ccf78a0d8ab4d4e2f372"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.1.7.tar.gz",
        "has_sig": false,
        "md5_digest": "f072dfdff96d1edc685f5815e1fb0936",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 97294,
        "upload_time": "2021-06-09T05:01:53",
        "upload_time_iso_8601": "2021-06-09T05:01:53.520465Z",
        "url": "https://files.pythonhosted.org/packages/6e/23/7a6961dabebbc8612c29ebb32e40ac0c9c2d90bd6b4850170bb235464507/polyglot-tokenizer-2.0.1.7.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "2.0.2": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "e4d21338a204f5035015d54caa765478a0ceb8d806dceed150f45263f9d3ff98",
          "md5": "944fefbadb4c7afdd67e45ad8684451b",
          "sha256": "0ddb1101fa212c35795ca916adc02436e4ea7723182449fab5185f8f94caf873"
        },
        "downloads": -1,
        "filename": "polyglot_tokenizer-2.0.2-py2.py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "944fefbadb4c7afdd67e45ad8684451b",
        "packagetype": "bdist_wheel",
        "python_version": "py2.py3",
        "requires_python": null,
        "size": 323283,
        "upload_time": "2021-06-09T05:31:30",
        "upload_time_iso_8601": "2021-06-09T05:31:30.196447Z",
        "url": "https://files.pythonhosted.org/packages/e4/d2/1338a204f5035015d54caa765478a0ceb8d806dceed150f45263f9d3ff98/polyglot_tokenizer-2.0.2-py2.py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "a29e94e64554280a3636d4e9ad1185d7aed3d1919981885b9f3756578dee19d9",
          "md5": "5e1f8a2b33dfb120fef68752adab103c",
          "sha256": "8094a9a4b271cb0ac0ae81ace892d804b6806030bb7c2327810251e33ddafd95"
        },
        "downloads": -1,
        "filename": "polyglot-tokenizer-2.0.2.tar.gz",
        "has_sig": false,
        "md5_digest": "5e1f8a2b33dfb120fef68752adab103c",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": null,
        "size": 101320,
        "upload_time": "2021-06-09T05:31:25",
        "upload_time_iso_8601": "2021-06-09T05:31:25.069288Z",
        "url": "https://files.pythonhosted.org/packages/a2/9e/94e64554280a3636d4e9ad1185d7aed3d1919981885b9f3756578dee19d9/polyglot-tokenizer-2.0.2.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "e4d21338a204f5035015d54caa765478a0ceb8d806dceed150f45263f9d3ff98",
        "md5": "944fefbadb4c7afdd67e45ad8684451b",
        "sha256": "0ddb1101fa212c35795ca916adc02436e4ea7723182449fab5185f8f94caf873"
      },
      "downloads": -1,
      "filename": "polyglot_tokenizer-2.0.2-py2.py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "944fefbadb4c7afdd67e45ad8684451b",
      "packagetype": "bdist_wheel",
      "python_version": "py2.py3",
      "requires_python": null,
      "size": 323283,
      "upload_time": "2021-06-09T05:31:30",
      "upload_time_iso_8601": "2021-06-09T05:31:30.196447Z",
      "url": "https://files.pythonhosted.org/packages/e4/d2/1338a204f5035015d54caa765478a0ceb8d806dceed150f45263f9d3ff98/polyglot_tokenizer-2.0.2-py2.py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "a29e94e64554280a3636d4e9ad1185d7aed3d1919981885b9f3756578dee19d9",
        "md5": "5e1f8a2b33dfb120fef68752adab103c",
        "sha256": "8094a9a4b271cb0ac0ae81ace892d804b6806030bb7c2327810251e33ddafd95"
      },
      "downloads": -1,
      "filename": "polyglot-tokenizer-2.0.2.tar.gz",
      "has_sig": false,
      "md5_digest": "5e1f8a2b33dfb120fef68752adab103c",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": null,
      "size": 101320,
      "upload_time": "2021-06-09T05:31:25",
      "upload_time_iso_8601": "2021-06-09T05:31:25.069288Z",
      "url": "https://files.pythonhosted.org/packages/a2/9e/94e64554280a3636d4e9ad1185d7aed3d1919981885b9f3756578dee19d9/polyglot-tokenizer-2.0.2.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}