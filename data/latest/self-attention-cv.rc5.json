{
  "info": {
    "author": "Adaloglou Nikolas",
    "author_email": "nikolas@theaiusummer.com",
    "bugtrack_url": null,
    "classifiers": [
      "License :: OSI Approved :: MIT License",
      "Programming Language :: Python :: 3",
      "Programming Language :: Python :: 3.7"
    ],
    "description": "<div align=\"center\">\n<img src=\"feat_img.png\"/>\n</div>\n\n# Self-attention building blocks for computer vision applications in PyTorch\n\nImplementation of self attention mechanisms for computer vision in PyTorch with einsum and einops.\nFocused on computer vision self-attention modules. \n\n#### Install it via pip \n\n```$ pip install self-attention-cv``` \n\nIt would be nice to pre-install pytorch in your environment, in case you don't have a GPU. To run the tests from the terminal \n```$ pytest``` you may need to run ``` export PYTHONPATH=$PATHONPATH:`pwd` ``` before.\n\n\n## Related articles\n- [How Attention works in Deep Learning](https://theaisummer.com/attention/)\n- [How Transformers work in deep learning and NLP](https://theaisummer.com/transformer/)\n- [How the Vision Transformer (ViT) works in 10 minutes: an image is worth 16x16 words](https://theaisummer.com/vision-transformer/)\n- [Understanding einsum for Deep learning: implement a transformer with multi-head self-attention from scratch](https://theaisummer.com/einsum-attention/)\n- [How Positional Embeddings work in Self-Attention](https://theaisummer.com/positional-embeddings/)\n- [Why multi-head self attention works: math, intuitions and 10+1 hidden insights](https://theaisummer.com/self-attention/)\n\n\n## Code Examples\n\n\n#### Multi-head attention\n\n```python\nimport torch\nfrom self_attention_cv import MultiHeadSelfAttention\n\nmodel = MultiHeadSelfAttention(dim=64)\nx = torch.rand(16, 10, 64)  # [batch, tokens, dim]\nmask = torch.zeros(10, 10)  # tokens X tokens\nmask[5:8, 5:8] = 1\ny = model(x, mask)\n```\n\n#### Axial attention\n\n```python\nimport torch\nfrom self_attention_cv import AxialAttentionBlock\nmodel = AxialAttentionBlock(in_channels=256, dim=64, heads=8)\nx = torch.rand(1, 256, 64, 64)  # [batch, tokens, dim, dim]\ny = model(x)\n```\n\n#### Vanilla Transformer Encoder\n```python\nimport torch\nfrom self_attention_cv import TransformerEncoder\nmodel = TransformerEncoder(dim=64,blocks=6,heads=8)\nx = torch.rand(16, 10, 64)  # [batch, tokens, dim]\nmask = torch.zeros(10, 10)  # tokens X tokens\nmask[5:8, 5:8] = 1\ny = model(x,mask)\n```\n#### Vision Transformer with/without ResNet50 backbone for image classification\n```python\nimport torch\nfrom self_attention_cv import ViT, ResNet50ViT\n\nmodel1 = ResNet50ViT(img_dim=128, pretrained_resnet=False, \n                        blocks=6, num_classes=10, \n                        dim_linear_block=256, dim=256)\n# or\nmodel2 = ViT(img_dim=256, in_channels=3, patch_dim=16, num_classes=10,dim=512)\nx = torch.rand(2, 3, 256, 256)\ny = model2(x) # [2,10]\n```\n\n#### A re-implementation of Unet with the Vision Transformer encoder\n\n```python\nimport torch\nfrom self_attention_cv.transunet import TransUnet\na = torch.rand(2, 3, 128, 128)\nmodel = TransUnet(in_channels=3, img_dim=128, vit_blocks=8,\nvit_dim_linear_mhsa_block=512, classes=5)\ny = model(a) # [2, 5, 128, 128]\n```\n\n#### Bottleneck Attention block \n```python\nimport torch\nfrom self_attention_cv.bottleneck_transformer import BottleneckBlock\ninp = torch.rand(1, 512, 32, 32)\nbottleneck_block = BottleneckBlock(in_channels=512, fmap_size=(32, 32), heads=4, out_channels=1024, pooling=True)\ny = bottleneck_block(inp)\n```\n\n\n### Position embeddings are also available\n\n#### 1D Positional Embeddings \n\n```python\nimport torch\nfrom self_attention_cv.pos_embeddings import AbsPosEmb1D,RelPosEmb1D\n\nmodel = AbsPosEmb1D(tokens=20, dim_head=64)\n# batch heads tokens dim_head\nq = torch.rand(2, 3, 20, 64)\ny1 = model(q)\n\nmodel = RelPosEmb1D(tokens=20, dim_head=64, heads=3)\nq = torch.rand(2, 3, 20, 64)\ny2 = model(q)\n```\n\n#### 2D Positional Embeddings\n```python\nimport torch\nfrom self_attention_cv.pos_embeddings import RelPosEmb2D\ndim = 32  # spatial dim of the feat map\nmodel = RelPosEmb2D(\n    feat_map_size=(dim, dim),\n    dim_head=128)\n\nq = torch.rand(2, 4, dim*dim, 128)\ny = model(q)\n```\n\n## Acknowledgments\nThanks to Alex Rogozhnikov [@arogozhnikov](https://github.com/arogozhnikov) for the awesome einops package. \nFor my re-implementations I have studied and borrowed code from many repositories of Phil Wang [@lucidrains](https://github.com/lucidrains). \nBy studying  his code I have managed to grasp self-attention, discover nlp stuff that are never\nreferred in the papers, and learn from his clean coding style.\n\n### Cited as\n\n```\n@article{adaloglou2021transformer,\n    title   = \"Transformers in Computer Vision\",\n    author  = \"Adaloglou, Nikolas\",\n    journal = \"https://theaisummer.com/\",\n    year    = \"2021\",\n    howpublished = {https://github.com/The-AI-Summer/self-attention-cv},\n  }\n```\n \n## References\n\n1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\n2. Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., & Chen, L. C. (2020, August). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In European Conference on Computer Vision (pp. 108-126). Springer, Cham.\n3. Srinivas, A., Lin, T. Y., Parmar, N., Shlens, J., Abbeel, P., & Vaswani, A. (2021). Bottleneck Transformers for Visual Recognition. arXiv preprint arXiv:2101.11605.  \n4. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n5. Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019). Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909.\n6. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., ... & Zhou, Y. (2021). Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306.\n7. Wang, S., Li, B., Khabsa, M., Fang, H., & Ma, H. (2020). Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768.\n8. Bertasius, G., Wang, H., & Torresani, L. (2021). Is Space-Time Attention All You Need for Video Understanding?. arXiv preprint arXiv:2102.05095.\n9. Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position representations. arXiv preprint arXiv:1803.02155.\n\n## Support\nIf you really like this repository and find it useful, please consider (â˜…) starring it, so that it can reach a broader audience of like-minded people. It would be highly appreciated :) !\n\n\n\n\n\n",
    "description_content_type": "text/markdown",
    "docs_url": null,
    "download_url": "",
    "downloads": {
      "last_day": -1,
      "last_month": -1,
      "last_week": -1
    },
    "home_page": "https://github.com/The-AI-Summer/self_attention_cv",
    "keywords": "",
    "license": "MIT",
    "maintainer": "",
    "maintainer_email": "",
    "name": "self-attention-cv",
    "package_url": "https://pypi.org/project/self-attention-cv/",
    "platform": "",
    "project_url": "https://pypi.org/project/self-attention-cv/",
    "project_urls": {
      "Homepage": "https://github.com/The-AI-Summer/self_attention_cv"
    },
    "release_url": "https://pypi.org/project/self-attention-cv/1.2.3/",
    "requires_dist": [
      "torch (>=1.7)",
      "torchvision (>=0.8)",
      "einops (>=0.3)",
      "numpy (>=1.19)",
      "pytest (>=6.2)"
    ],
    "requires_python": ">=3.6",
    "summary": "Self-attention building blocks for computer vision applications in PyTorch",
    "version": "1.2.3",
    "yanked": false,
    "yanked_reason": null
  },
  "last_serial": 11003924,
  "releases": {
    "1.0.0rc11": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "36ec3c0822215b6dcfad1af89328d6997e655e0b328b77209dbe6c249ed7bf47",
          "md5": "a7e7815bb492bd27253b07bdefb3d6fd",
          "sha256": "38505612746b431cf7310e05a7ff4212e1d819249f61a9817dbf504a17e91b4f"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.0.0rc11-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "a7e7815bb492bd27253b07bdefb3d6fd",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 23974,
        "upload_time": "2021-02-15T13:28:53",
        "upload_time_iso_8601": "2021-02-15T13:28:53.634521Z",
        "url": "https://files.pythonhosted.org/packages/36/ec/3c0822215b6dcfad1af89328d6997e655e0b328b77209dbe6c249ed7bf47/self_attention_cv-1.0.0rc11-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "dcc028036a4810831dd3c84c73a3abc7302c4db9705fe2d830587c685cd3234d",
          "md5": "2a181ca12d240ac303dd1388b049c21d",
          "sha256": "0d92e40a347eb857d14f01fbdd65230b425c54de923d5a325e14819aa58efc4f"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.0.0rc11.tar.gz",
        "has_sig": false,
        "md5_digest": "2a181ca12d240ac303dd1388b049c21d",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 15865,
        "upload_time": "2021-02-15T13:28:55",
        "upload_time_iso_8601": "2021-02-15T13:28:55.325913Z",
        "url": "https://files.pythonhosted.org/packages/dc/c0/28036a4810831dd3c84c73a3abc7302c4db9705fe2d830587c685cd3234d/self_attention_cv-1.0.0rc11.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.1.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "695b4163230c657f80a5f4123af111a410257df5d0c3d26027bd150c40b75fec",
          "md5": "bb1234d7588ff66ac312e3d81d0e3a23",
          "sha256": "6ae0138447b9547f0886bd589f254babeca93dcb18448c61643d481982b41c3d"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.1.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "bb1234d7588ff66ac312e3d81d0e3a23",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 37501,
        "upload_time": "2021-03-02T19:18:47",
        "upload_time_iso_8601": "2021-03-02T19:18:47.711574Z",
        "url": "https://files.pythonhosted.org/packages/69/5b/4163230c657f80a5f4123af111a410257df5d0c3d26027bd150c40b75fec/self_attention_cv-1.1.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "b17e82a7237710b6b233776610606855ac3a81fa5d29ea1053ef7e37184e3b42",
          "md5": "8642b179a55228e75340666a2260dcf4",
          "sha256": "f28669868df25f20d79ed2c3fe97b9590df78363027879078182a545af83af09"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.1.0.tar.gz",
        "has_sig": false,
        "md5_digest": "8642b179a55228e75340666a2260dcf4",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 25672,
        "upload_time": "2021-03-02T19:18:49",
        "upload_time_iso_8601": "2021-03-02T19:18:49.147346Z",
        "url": "https://files.pythonhosted.org/packages/b1/7e/82a7237710b6b233776610606855ac3a81fa5d29ea1053ef7e37184e3b42/self_attention_cv-1.1.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.0": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "304a1ec193d8c26d3990a5c8ea18df58f1157d41b1aac811fcb8de389bcae6a6",
          "md5": "3195612d8f43072e00b8f0f60c2c3299",
          "sha256": "1418abe7ae4d6fbfc144d091042f151d688186cd93b56c5b41a540a53d146bef"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.2.0-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "3195612d8f43072e00b8f0f60c2c3299",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 41907,
        "upload_time": "2021-06-30T10:44:02",
        "upload_time_iso_8601": "2021-06-30T10:44:02.752375Z",
        "url": "https://files.pythonhosted.org/packages/30/4a/1ec193d8c26d3990a5c8ea18df58f1157d41b1aac811fcb8de389bcae6a6/self_attention_cv-1.2.0-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "c7f060ddad21e30c01b5c5ba38d4a89b14049940342f2ade1761306a3040d783",
          "md5": "f6fe7b36914f90899d4726f174848d15",
          "sha256": "37fe4a8be4724aaaa4f6bc02ad5cd3080c06e4b677d1252bad94c6969bc8aaf0"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.2.0.tar.gz",
        "has_sig": false,
        "md5_digest": "f6fe7b36914f90899d4726f174848d15",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 29682,
        "upload_time": "2021-06-30T10:44:04",
        "upload_time_iso_8601": "2021-06-30T10:44:04.254195Z",
        "url": "https://files.pythonhosted.org/packages/c7/f0/60ddad21e30c01b5c5ba38d4a89b14049940342f2ade1761306a3040d783/self_attention_cv-1.2.0.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.1": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "9fcf0a30d3d5c739e4d12b7c0d1d62773ae637da8f8b359ac45ea301f0c4a4da",
          "md5": "5dcdde80b55f854fa2657a51c214440a",
          "sha256": "7b1d82617d771ddfb7aa6ad543162ca8ce3d5981900125e06e85fc98c06e3eec"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.2.1-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "5dcdde80b55f854fa2657a51c214440a",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 42225,
        "upload_time": "2021-07-22T14:50:25",
        "upload_time_iso_8601": "2021-07-22T14:50:25.939077Z",
        "url": "https://files.pythonhosted.org/packages/9f/cf/0a30d3d5c739e4d12b7c0d1d62773ae637da8f8b359ac45ea301f0c4a4da/self_attention_cv-1.2.1-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "68384d09433f4390ed599523a29bd23cbf449626eee9f2889b9008d1677dc1ed",
          "md5": "d42238f2f64c29a8a785e0df4ca741bf",
          "sha256": "52d9c4d2dab0c19571ed2fdbebef1a300dd961fdce405dfeebbdd97e28c20147"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.2.1.tar.gz",
        "has_sig": false,
        "md5_digest": "d42238f2f64c29a8a785e0df4ca741bf",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 29915,
        "upload_time": "2021-07-22T14:50:30",
        "upload_time_iso_8601": "2021-07-22T14:50:30.115011Z",
        "url": "https://files.pythonhosted.org/packages/68/38/4d09433f4390ed599523a29bd23cbf449626eee9f2889b9008d1677dc1ed/self_attention_cv-1.2.1.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ],
    "1.2.3": [
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "3fe30b0a26317c657e8c8ed2830926db60cb2239812016f6d0aefa6859b58fab",
          "md5": "cfc294a4d08bc9e942e7ac5217ed0f5c",
          "sha256": "3577c422c666c14234089900e620d701f15c4a8f06ffd86adb86843f63a19518"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.2.3-py3-none-any.whl",
        "has_sig": false,
        "md5_digest": "cfc294a4d08bc9e942e7ac5217ed0f5c",
        "packagetype": "bdist_wheel",
        "python_version": "py3",
        "requires_python": ">=3.6",
        "size": 42388,
        "upload_time": "2021-07-26T09:58:15",
        "upload_time_iso_8601": "2021-07-26T09:58:15.243164Z",
        "url": "https://files.pythonhosted.org/packages/3f/e3/0b0a26317c657e8c8ed2830926db60cb2239812016f6d0aefa6859b58fab/self_attention_cv-1.2.3-py3-none-any.whl",
        "yanked": false,
        "yanked_reason": null
      },
      {
        "comment_text": "",
        "digests": {
          "blake2b_256": "1caaac0d804a07ea37cca283e093ef70097b84e7372ed697b9ebd2e6a818ff86",
          "md5": "5155f1895aecb2a325c5536cf5799c71",
          "sha256": "ab348f60c44bb7cd28efbd57cf0145b47bda494e33f52b2665e02565d7251095"
        },
        "downloads": -1,
        "filename": "self_attention_cv-1.2.3.tar.gz",
        "has_sig": false,
        "md5_digest": "5155f1895aecb2a325c5536cf5799c71",
        "packagetype": "sdist",
        "python_version": "source",
        "requires_python": ">=3.6",
        "size": 25214,
        "upload_time": "2021-07-26T09:58:17",
        "upload_time_iso_8601": "2021-07-26T09:58:17.034983Z",
        "url": "https://files.pythonhosted.org/packages/1c/aa/ac0d804a07ea37cca283e093ef70097b84e7372ed697b9ebd2e6a818ff86/self_attention_cv-1.2.3.tar.gz",
        "yanked": false,
        "yanked_reason": null
      }
    ]
  },
  "urls": [
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "3fe30b0a26317c657e8c8ed2830926db60cb2239812016f6d0aefa6859b58fab",
        "md5": "cfc294a4d08bc9e942e7ac5217ed0f5c",
        "sha256": "3577c422c666c14234089900e620d701f15c4a8f06ffd86adb86843f63a19518"
      },
      "downloads": -1,
      "filename": "self_attention_cv-1.2.3-py3-none-any.whl",
      "has_sig": false,
      "md5_digest": "cfc294a4d08bc9e942e7ac5217ed0f5c",
      "packagetype": "bdist_wheel",
      "python_version": "py3",
      "requires_python": ">=3.6",
      "size": 42388,
      "upload_time": "2021-07-26T09:58:15",
      "upload_time_iso_8601": "2021-07-26T09:58:15.243164Z",
      "url": "https://files.pythonhosted.org/packages/3f/e3/0b0a26317c657e8c8ed2830926db60cb2239812016f6d0aefa6859b58fab/self_attention_cv-1.2.3-py3-none-any.whl",
      "yanked": false,
      "yanked_reason": null
    },
    {
      "comment_text": "",
      "digests": {
        "blake2b_256": "1caaac0d804a07ea37cca283e093ef70097b84e7372ed697b9ebd2e6a818ff86",
        "md5": "5155f1895aecb2a325c5536cf5799c71",
        "sha256": "ab348f60c44bb7cd28efbd57cf0145b47bda494e33f52b2665e02565d7251095"
      },
      "downloads": -1,
      "filename": "self_attention_cv-1.2.3.tar.gz",
      "has_sig": false,
      "md5_digest": "5155f1895aecb2a325c5536cf5799c71",
      "packagetype": "sdist",
      "python_version": "source",
      "requires_python": ">=3.6",
      "size": 25214,
      "upload_time": "2021-07-26T09:58:17",
      "upload_time_iso_8601": "2021-07-26T09:58:17.034983Z",
      "url": "https://files.pythonhosted.org/packages/1c/aa/ac0d804a07ea37cca283e093ef70097b84e7372ed697b9ebd2e6a818ff86/self_attention_cv-1.2.3.tar.gz",
      "yanked": false,
      "yanked_reason": null
    }
  ],
  "vulnerabilities": []
}